# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TRAK: Attributing Model Behavior at Scale.](http://arxiv.org/abs/2303.14186) | TRAK是一种适用于大规模、可微模型的数据归因方法，既有效又计算量可行。 |
| [^2] | [How many dimensions are required to find an adversarial example?.](http://arxiv.org/abs/2303.14173) | 本文研究了对抗性漏洞如何取决于受限于高维输入空间中的子空间维数，同时针对标准PGD攻击的对抗性成功率提出了单调递增函数表达式。 |
| [^3] | [Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis.](http://arxiv.org/abs/2303.14157) | 提出了一种高效尺度不变的生成模型，不使用空间卷积或分层架构，通过将层内特征图分解为局部和全局特征并通过插值这些特征生成图像，使得模型具有较小的内存占用和更快的推理速度。 |
| [^4] | [Local Contrastive Learning for Medical Image Recognition.](http://arxiv.org/abs/2303.14153) | 本文提出了一种局部对比学习的微调框架LRCLR，该框架能够识别医学图像中显著的局部图像区域，提供有意义的图像和文本的解释，并显著提高了零样本性能。 |
| [^5] | [Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle.](http://arxiv.org/abs/2303.14151) | 双重下降是机器学习中一个令人惊讶的现象，数据量、数据维度和模型参数是影响双重下降的关键因素。研究者找到了制造双重下降的三个解释性因素，并证明这些因素可以在简单神经网络中关闭。 |
| [^6] | [CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images.](http://arxiv.org/abs/2303.14126) | 本文提出了一个名为CIFAKE的方法，可以通过计算机视觉将真实照片与AI生成图像进行二元分类。该方法使用了卷积神经网络（CNN）对图像进行分类。 |
| [^7] | [Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives.](http://arxiv.org/abs/2303.14116) | 这篇论文总结了作者关于注意力机制在提高深度学习模型解释性和性能中的潜力。这是基础与应用研究的重要问题，尤其在医学领域，而注意力机制可以成为解决这个问题的一种方法。 |
| [^8] | [Interpretable Anomaly Detection via Discrete Optimization.](http://arxiv.org/abs/2303.14111) | 该论文提出了一个通过学习有限自动机进行异常检测的框架，并通过约束优化算法和新的正则化方案提高了可解释性。 |
| [^9] | [Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck.](http://arxiv.org/abs/2303.14096) | 本论文提出了在互信息约束下的对抗学习训练模型，针对数据在获取方式上可能存在的偏差，扩展了信息瓶颈的模型来应对这种问题。该方案结合了卷积和变压器的混合判别-生成训练，在不使用领域特定知识的情况下，显著提高了学习表征的鲁棒性。 |
| [^10] | [Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter.](http://arxiv.org/abs/2303.14090) | 本文介绍了一种新的方法，将物理启发的神经网络应用于重建流体力学模拟中，通过将理论知识注入模型损失函数并结合新的性能评估指标，成功实现了对重子散射的重建。 |
| [^11] | [Differentially Private Synthetic Control.](http://arxiv.org/abs/2303.14084) | 本文提供了首个具有显式误差界限的差分隐私合成控制算法，具有广泛的应用前景。 |
| [^12] | [Online Learning for the Random Feature Model in the Student-Teacher Framework.](http://arxiv.org/abs/2303.14083) | 本研究考虑了一种两层神经网络模型的在线学习，研究发现，当学生的隐藏层大小呈指数增长时，完美泛化是可行的，但对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化。 |
| [^13] | [Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing.](http://arxiv.org/abs/2303.14077) | 本文提出了一种新的对抗训练方法(Instance-adaptive Adversarial Training, IAAT)通过平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本，取得了在各种数据集下的最新、最佳结果，并在白盒和黑盒攻击下均优于以前的方法。 |
| [^14] | [A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data.](http://arxiv.org/abs/2303.14068) | 本文提出了一种基于1D CNN-LSTM结构的船舶轨迹关联算法，采用多变量时间序列问题来解决 |
| [^15] | [Euler Characteristic Tools For Topological Data Analysis.](http://arxiv.org/abs/2303.14040) | 本文研究了欧拉特征技术在拓扑数据分析中的应用，利用点运算欧拉特征得到欧拉特征轮廓，在监督和无监督任务中实现了最先进性能，并提供了欧拉轮廓及其混合变换捕捉信息的启发式方法。 |
| [^16] | [PENTACET data -- 23 Million Contextual Code Comments and 500,000 SATD comments.](http://arxiv.org/abs/2303.14029) | 本文介绍了PENTACET数据集，该数据集包含了2300万个上下文代码注释和50万个自我承认技术债务注释，并提供了详细的上下文数据，将进一步推动使用人工智能技术的SATD研究。 |
| [^17] | [ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale.](http://arxiv.org/abs/2303.14006) | 本文介绍了ASTRA-sim2.0，一种分布式训练模拟基础设施，可模拟当代分布式训练模型和平台。ASTRA-sim2.0易于使用、灵活，支持任意模型并行化策略、多维异构拓扑、任意分解内存系统和精细的仪器和事件跟踪。 |
| [^18] | [Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation.](http://arxiv.org/abs/2303.13974) | 本文提出了一种通过知识蒸馏技术，将复杂预训练模型的知识转移到轻量级模型上，从而使低内存设备也能进行复杂缺陷分类，且无需大量标记数据。 |
| [^19] | [Uncovering Energy-Efficient Practices in Deep Learning Training: Preliminary Steps Towards Green AI.](http://arxiv.org/abs/2303.13972) | 本文旨在从可持续性的角度研究深度学习管道的训练阶段，并通过研究超参数调整策略和模型复杂性来降低能耗。研究发现，贝叶斯优化在超参数调整中明显优于其他策略。同时，对卷积神经网络的架构进行分析，并考虑了卷积、线性层和池化层的能量消耗。 |
| [^20] | [Optimal Transport for Offline Imitation Learning.](http://arxiv.org/abs/2303.13971) | 本文提出了一种名为“最优输运奖励标记（OTR）”的算法，可用于离线模仿学习中为无标记的轨迹分配奖励，通过使用最优输运计算数据集中未注释的轨迹和专家演示之间的最佳对齐，得出可解释为奖励的相似性度量。在D4RL基准测试中，OTR只使用一个演示时可以持续匹配性能。 |
| [^21] | [Gradient scarcity with Bilevel Optimization for Graph Learning.](http://arxiv.org/abs/2303.13964) | 本研究研究了二层优化算法在图学习中存在的梯度稀缺问题，并通过提出潜在图学习的方法来解决这一问题。 |
| [^22] | [Factorizers for Distributed Sparse Block Codes.](http://arxiv.org/abs/2303.13957) | 本文提出了一种用于分解分布式稀疏块编码（SBC）的GSBC，该方法引入了基于阈值的非线性激活、条件随机采样和$\ell_\infty$基于相似性度量，并能够分析确定预期的解的质量，解决了由于感知不确定性和近似而放松的噪声SBC中符号表示的挑战。 |
| [^23] | [Topological Reconstruction of Particle Physics Processes using Graph Neural Networks.](http://arxiv.org/abs/2303.13937) | Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。 |
| [^24] | [Particle Mean Field Variational Bayes.](http://arxiv.org/abs/2303.13930) | 本论文提出了一种新的基于粒子的MFVB方法，有效扩展了其适用范围，可应用于贝叶斯逻辑回归、随机波动和深度神经网络。 |
| [^25] | [Removing confounding information from fetal ultrasound images.](http://arxiv.org/abs/2303.13918) | 本文讨论了如何针对胎儿筛查超声扫描图像中的文本和卡尺去除混淆信息，以训练更加准确的超声诊断深度学习算法。 |
| [^26] | [Convolutional Neural Networks for the classification of glitches in gravitational-wave data streams.](http://arxiv.org/abs/2303.13917) | 本文使用卷积神经网络对引力波数据流中的毛刺信号进行分类，并探索使用自监督和Fine-tune预训练模型的方法。最佳模型F1分数达到97.18％，表明卷积神经网络用于分类引力波数据流中的毛刺信号具有潜在的应用价值。 |
| [^27] | [Benchmarking the Impact of Noise on Deep Learning-based Classification of Atrial Fibrillation in 12-Lead ECG.](http://arxiv.org/abs/2303.13915) | 本文评估了基于深度学习的房颤分类方法在12导联心电图噪声存在的情况下的准确性，结果显示该方法能够高效识别房颤并具有很大的潜力。 |
| [^28] | [Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis.](http://arxiv.org/abs/2303.13909) | 本文提出了一种Wave-U-Net鉴别器，它是一个单独但表达力强的鉴别器，可以以逐个样本的方式评估波形，同时提取多级特征，为生成器提供足够的信息，从而在不增加模型大小和计算时间的情况下提高语音合成质量。 |
| [^29] | [Remind of the Past: Incremental Learning with Analogical Prompts.](http://arxiv.org/abs/2303.13898) | 本文提出了一种新颖的增量学习方法，通过类比制作机制将新数据重新映射到旧类，并利用所学提示估计和对抗表示偏移，极大提高了增量学习性能。 |
| [^30] | [Regularization of polynomial networks for image recognition.](http://arxiv.org/abs/2303.13896) | 本论文介绍了一种新型的多项式网络结构及其正则化方案，能够在六项基准测试中达到 ResNet 的性能水平，并提出了 D-PolyNets 以进一步推动正则化方案和提高参数效率。 |
| [^31] | [Learning Causal Attributions in Neural Networks: Beyond Direct Effects.](http://arxiv.org/abs/2303.13850) | 本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。 |
| [^32] | [Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation.](http://arxiv.org/abs/2303.13830) | 为了评估自动驾驶汽车在不同社交特征的交互交通场景下采取安全有效的机动方式，该论文提出了一种社交可控行为生成（SCBG）模型，通过学习真实驾驶数据实现了逼真而类人的轨迹生成，并允许用户指定轨迹的礼貌程度。 |
| [^33] | [Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers.](http://arxiv.org/abs/2303.13827) | 本文提出了一种紧凑型可形变卷积变换器（DC Transformer），能够高效地识别单一和混合型晶圆缺陷，聚焦于全局特征，准确预测缺陷的数目和类型。 |
| [^34] | [Factor Decomposed Generative Adversarial Networks for Text-to-Image Synthesis.](http://arxiv.org/abs/2303.13821) | 本文提出一种因子分解生成对抗网络（FDGAN）用于文本转图像合成，能够将句子嵌入和噪声向量分解为不同的因子，并通过加性规范层来对齐和融合文本-图像特征，实验结果表明FDGAN可以实现更好的性能，同时使用更少的参数。 |
| [^35] | [marl-jax: Multi-agent Reinforcement Leaning framework for Social Generalization.](http://arxiv.org/abs/2303.13808) | marl-jax是一个基于DeepMind的JAX生态系和RL生态系的多智能体RL软件包，可以训练和评估代理在多样性背景下的社会普适性，提供命令行界面，适用于合作与竞争游戏环境。 |
| [^36] | [UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning.](http://arxiv.org/abs/2303.13804) | UniTS是一个带自监督表示学习的通用时间序列分析框架，能够解决部分标记和领域转移等实际问题，并在多个任务和设置中实现了优秀的性能表现。 |
| [^37] | [Toward Open-domain Slot Filling via Self-supervised Co-training.](http://arxiv.org/abs/2303.13801) | 本文提出了一种名为SCot的自监督协同训练框架，通过使用BERT模型和伪标签，实现开放域槽填充任务的零样本学习，克服了传统监督学习方法需要大量手动标注数据的问题。 |
| [^38] | [Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function.](http://arxiv.org/abs/2303.13797) | 本研究提出了一种个性化任务导向对话系统的新框架P-ToD，它使用零-shot泛化奖励函数进行无监督训练，并取得了优于现有技术的实验结果。 |
| [^39] | [Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance.](http://arxiv.org/abs/2303.13794) | 本文提出了一种特征点裁剪算法(MKPC)来定位、提取和裁剪包含重要信息的共视区域，并提出了一个通用的二阶段管线用于图像匹配，该算法具有高效和准确性，能够提高在户外环境下的图像匹配性能，且比最先进的算法更为优秀。 |
| [^40] | [Forecasting Competitions with Correlated Events.](http://arxiv.org/abs/2303.13793) | 此论文研究了带有相关事件的预测竞赛，并引入了一个块相关的概念。证明了在具有块相关性的分布下，基于follow-the-regularized-leader(FTRL)的竞赛机制仍然保留了它的$\epsilon$-最优保证。 |
| [^41] | [Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint.](http://arxiv.org/abs/2303.13790) | 本文提出了一个公平的病人-试验匹配框架，通过生成病人准则级别的公平性约束，考虑了包含和排除标准的嵌入差异，有效地解决了病人-试验匹配中的不公平性问题。 |
| [^42] | [GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism.](http://arxiv.org/abs/2303.13775) | 本文提出了一种新的并行小批量训练方法，即分裂并行，应用在图神经网络训练上，能有效缓解数据并行方法的性能瓶颈，同时在大规模图上的性能表现优越。 |
| [^43] | [A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models.](http://arxiv.org/abs/2303.13773) | 本研究提出基于GNN的纳米卫星任务调度方法，以更好地优化服务质量，解决ONTS问题的复杂性。 |
| [^44] | [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects.](http://arxiv.org/abs/2303.13769) | 本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。 |
| [^45] | [Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs.](http://arxiv.org/abs/2303.13763) | 本文提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。 |
| [^46] | [Structural Imbalance Aware Graph Augmentation Learning.](http://arxiv.org/abs/2303.13757) | 本文提出了一种选择性图增强方法 (SAug)，通过识别图中的中心节点和尾节点，设计了一种选择性增强策略，可以显著提高图机器学习模型的性能，特别是在学习尾节点时。 |
| [^47] | [Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers.](http://arxiv.org/abs/2303.13755) | 本文分析了ViT算法的计算成本高的问题，并且通过学习实例相关的注意力模式，来提高计算效率 |
| [^48] | [Leveraging Old Knowledge to Continually Learn New Classes in Medical Images.](http://arxiv.org/abs/2303.13752) | 本研究提出了一个框架，利用动态架构和训练过程交替两个目标的方式，使得机器学习算法在学习新类别时不会灾难性地遗忘已经学习的内容。该研究在医学数据集上取得了优秀的性能表现。 |
| [^49] | [LONGNN: Spectral GNNs with Learnable Orthonormal Basis.](http://arxiv.org/abs/2303.13750) | 本研究提出了一种谱图神经网络LONGNN，它采用可学习正交标准基，并解决了固定多项式基和非归一化基础所带来的缺陷，经实验证明其在各种图数据集上具有优异的表现。 |
| [^50] | [FixFit: using parameter-compression to solve the inverse problem in overdetermined models.](http://arxiv.org/abs/2303.13746) | FixFit是一种使用神经网络进行参数压缩的方法，可以解决复杂非线性模型中由于参数之间的相互作用导致的多参数集问题。 |
| [^51] | [EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms.](http://arxiv.org/abs/2303.13745) | EdgeTran框架旨在设计适用于移动边缘设备且在准确性、延迟、能耗和波峰功耗方面表现优秀的Transformer模型。同时，作者提出了块稀疏训练算法GPTran以帮助优化模型的内存占用和推理效率。 |
| [^52] | [An investigation of licensing of datasets for machine learning based on the GQM model.](http://arxiv.org/abs/2303.13735) | 机器学习数据集的许可证是一个问题，公开数据集质量不佳，缺乏商业可用性。当前数据集缺少许可证，需要更科学和系统的方法来调查。 |
| [^53] | [How Does Attention Work in Vision Transformers? A Visual Analytics Attempt.](http://arxiv.org/abs/2303.13731) | 本文采用视觉分析方法回答了ViT中头的重要性、不同头对空间邻居的关注强度、以及每个头学习的注意力模式等问题。 |
| [^54] | [A Survey on Secure and Private Federated Learning Using Blockchain: Theory and Application in Resource-constrained Computing.](http://arxiv.org/abs/2303.13727) | 基于区块链的安全与隐私联邦学习的综述与应用，旨在探讨如何防止隐私与安全威胁，并减少由资源不足引起的瓶颈问题。 |
| [^55] | [High Fidelity Image Synthesis With Deep VAEs In Latent Space.](http://arxiv.org/abs/2303.13714) | 该论文提出了一种使用深度VAE在潜空间中进行高保真地图像生成的方法，通过两个阶段的训练，在避免对大部分细节进行建模的基础上，重点学习图像的结构组成，并取得了较好的效果。 |
| [^56] | [End-to-End Diffusion Latent Optimization Improves Classifier Guidance.](http://arxiv.org/abs/2303.13703) | 本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。 |
| [^57] | [OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search.](http://arxiv.org/abs/2303.13683) | OFA$^2$是一个基于多目标优化的神经架构搜索模型，它通过将搜索阶段构想为一个多目标优化问题，并使用已经训练好的神经网络，从而在多个权衡目标之间找到高效和多样化的子网络。 |
| [^58] | [Clustering based on Mixtures of Sparse Gaussian Processes.](http://arxiv.org/abs/2303.13665) | 本文提出了一种基于稀疏高斯过程混合的聚类方法，同时实现了降维，并且相比于现有方法更具优势。 |
| [^59] | [Policy Evaluation in Distributional LQR.](http://arxiv.org/abs/2303.13657) | 本文提出了一种新的分布式LQR方法，通过提供一种有效逼近无限多个随机变量的有限一组时刻，解决了DRL中策略评估的挑战，其效果优于竞争方法。 |
| [^60] | [Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture.](http://arxiv.org/abs/2303.13651) | 本文提供了关于构建领域通用人工智能的思路和原则，通过研究生物神经网络和系统级分布网络通信、递归和短期拓扑变化，为建立人工神经网络提供宝贵指导。 |
| [^61] | [Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection.](http://arxiv.org/abs/2303.13649) | 本研究发现，对于驾驶员疲劳检测，最可靠的模型是XGB，最佳窗口时间为120至150秒之间。本研究使用SHAP方法选择了18个最具影响力的特征，并训练出新的较小模型以获得与最初模型相同的性能。尽管所有模型都容易受到对抗性攻击的影响，但对抗性训练使它们能够保持更高的对抗鲁棒性。 |
| [^62] | [Efficient and Direct Inference of Heart Rate Variability using Both Signal Processing and Machine Learning.](http://arxiv.org/abs/2303.13637) | 本论文使用信号处理和机器学习结合的方法，直接推断出心率变异性。在大数据集的评估中，该方法表现出比单独使用信号处理或机器学习更高的准确性。 |
| [^63] | [PPG-based Heart Rate Estimation with Efficient Sensor Sampling and Learning Models.](http://arxiv.org/abs/2303.13636) | 本文研究了如何将PPG心率估计技术应用于低功耗和资源受限的嵌入式设备中，在结合信号处理和机器学习的基础上，成功将PPG采样频率降至仅25Hz，并提高了心率估计的精度，同时也减小了机器学习模型的特征大小，使得模型更小。 |
| [^64] | [Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training.](http://arxiv.org/abs/2303.13635) | 本文着重研究了低秩优化为基础的深度学习技术，可通过低秩逼近压缩深度神经网络，既减少存储要求又可实现高效快速的训练。通过综合技术，可将以上方法集成到LRI-Net框架中，以共同降低存储和计算成本，且不影响模型准确性。 |
| [^65] | [Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity.](http://arxiv.org/abs/2303.13634) | 本文提出了一种物理学指导的神经网络模型PIPN，它能利用稀疏标记数据同时预测所需偏微分方程在数百个不同的几何体上的解，有望在工业界进行快速的几何设计。 |
| [^66] | [Associated Random Neural Networks for Collective Classification of Nodes in Botnet Attacks.](http://arxiv.org/abs/2303.13627) | 本文介绍了一种基于Associated Random Neural Networks的Botnet攻击集成分类技术，它能够识别被攻陷的节点并在个别节点分类方法上进行更多有效的补充。 |
| [^67] | [Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging.](http://arxiv.org/abs/2303.13610) | 本文研究了一种基于人工智能的光学成像技术，可以快速、无标记的对弥漫性胶质瘤进行分子诊断，为其治疗提供更加准确的指导。 |
| [^68] | [Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback.](http://arxiv.org/abs/2303.13604) | 本论文研究了具有随机次模收益和全赌徒延迟反馈的组合多臂赌博机问题，研究了三种延迟反馈模型并导出了后悔上限。研究结果表明，算法能够在考虑延迟组合匿名反馈时胜过其他全赌徒方法。 |
| [^69] | [Une comparaison des algorithmes d'apprentissage pour la survie avec donn\'ees manquantes.](http://arxiv.org/abs/2303.13590) | 本文研究了基于神经网络的生存任务学习算法在缺失数据处理方面的表现，结果显示对于所有情况没有单一的数据插补方法优于其他方法，提出的方法可用于比较其他缺失数据模式和(或)生存模型。 |
| [^70] | [A Closer Look at Scoring Functions and Generalization Prediction.](http://arxiv.org/abs/2303.13589) | 本文研究了广义误差预测器的有效性，探讨了置信度、局部流形平滑度和模型一致性评分函数的优缺点，发现在复杂机制缺失的情况下，最先进的评分无法在分布转移和损坏下超越简单的模型一致性。同时，在受损训练数据的情况下，模型一致性打分仍然表现良好，并且集成多样性有助于提高泛化性能。 |
| [^71] | [Efficient Symbolic Reasoning for Neural-Network Verification.](http://arxiv.org/abs/2303.13588) | 本文提出了一种高效的符号推理技术，用于解决神经网络的验证问题。该技术可以编码许多验证问题为二次程序，并将其松弛为半定程序以高效地解决。该框架可以验证各种神经网络属性，并为解决神经网络对对抗攻击的脆弱性问题提供了改进。 |
| [^72] | [Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings.](http://arxiv.org/abs/2303.13570) | 本研究提出了一种使用残差循环神经网络的新型模型，实现了可逆的句子嵌入。与其他神经机器翻译模型不同，该方法使用基于回归的输出层重建输入序列的单词向量，其具有高准确度和快速训练速度。这种方法适合各种自然语言处理应用，特别是对需要高质量句嵌入的神经网络系统的使用具有潜在优势。 |
| [^73] | [TinyML: Tools, Applications, Challenges, and Future Research Directions.](http://arxiv.org/abs/2303.13569) | TinyML是一种嵌入式ML技术，可在多种廉价，资源有限和功率有限的设备上实现ML应用。但是，在其实现过程中，需要及时解决多种挑战，例如处理能力优化，提高可靠性以及维护学习模型的准确性。 |
| [^74] | [Extracting real estate values of rental apartment floor plans using graph convolutional networks.](http://arxiv.org/abs/2303.13568) | 本文使用改进后的方法从大量平面图像中提取访问图，利用图卷积神经网络模型“平面图价值”估计访问图的房地产价值，大幅提高了租金估计的精度。该模型为全面估计平面图的价值提供了新思路。 |
| [^75] | [Federated Learning on Heterogenous Data using Chest CT.](http://arxiv.org/abs/2303.13567) | 本研究使用联邦学习技术在全球21家医院的10,000多位COVID患者的胸部CT扫描图像数据集上进行了研究，提出了三种联邦学习策略，并提出了结合合成生成数据的联邦学习策略，为医学AI在异构数据上的应用提供了新的解决方案。 |
| [^76] | [Enhancing Embedding Representations of Biomedical Data using Logic Knowledge.](http://arxiv.org/abs/2303.13566) | 本文提出使用逻辑规则增强生物医学数据的嵌入表示，应用于最具挑战性的PharmKG数据集，并能够处理复杂的实验环境中关系事实之间的依赖性。 |
| [^77] | [Graph Tensor Networks: An Intuitive Framework for Designing Large-Scale Neural Learning Systems on Multiple Domains.](http://arxiv.org/abs/2303.13565) | 张量数学已经被广泛用于现代深度学习，但在形式化设计和描述神经网络上，还未被充分利用。本文引入了图张量网络（GTN）框架，提供了一个直观而严谨的图形化框架，用于在规则和不规则领域上的大规模神经学习系统上系统地设计和实现，且通用性和灵活性很强。 |
| [^78] | [Skip Connections in Spiking Neural Networks: An Analysis of Their Effect on Network Training.](http://arxiv.org/abs/2303.13563) | 本文研究了跳跃连接在脉冲神经网络中的作用，并提出了一种优化技术。通过优化跳跃连接的位置、类型和数量等超参数，可以显著提高SNN的准确性和效率，使其收敛更快、信息流更畅。 |
| [^79] | [Learning unidirectional coupling using echo-state network.](http://arxiv.org/abs/2303.13562) | 本文介绍了利用回声状态网络（ESN）学习单向耦合的能力。仅使用系统的几个时间序列数据进行训练后，ESN可以预测不同驱动信号下响应系统的动态，该方法仅需要一些$A-B$型的时间序列训练数据就足以让ESN学习这种耦合方式。 |
| [^80] | [Enhancing Unsupervised Speech Recognition with Diffusion GANs.](http://arxiv.org/abs/2303.13559) | 本论文提出了一种基于扩散GAN的增强方法，可用于无监督语音识别任务，实验结果表明该方法在多个数据集上都取得了良好的效果。 |
| [^81] | [Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach.](http://arxiv.org/abs/2303.13555) | 本研究提出了一种机器学习方法，可用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型的吸附动力学定律结构。 |
| [^82] | [CH-Go: Online Go System Based on Chunk Data Storage.](http://arxiv.org/abs/2303.13553) | 该论文提出了一种基于分块数据存储方法的在线围棋游戏系统 CH-Go，可以高效地处理海量数据，并使用神经网络来进行训练。 |
| [^83] | [Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh.](http://arxiv.org/abs/2303.13549) | 本文介绍了一种名为DaToBS的方法，用于从自然环境中的照片中检测和转录Tifinagh字符，以提高非洲低资源语言阿马齐语在教育、研究和网络应用等方面的支持。 |
| [^84] | [Hey Dona! Can you help me with student course registration?.](http://arxiv.org/abs/2303.13548) | 本文演示了智能个人助手Dona，用于学生选课的自动化操作，采用语音输入、任务规划优化和语言翻译等技术，使学生不需要自己完成复杂的选课表格。 |
| [^85] | [Labeled Subgraph Entropy Kernel.](http://arxiv.org/abs/2303.13543) | 本文提出了一种新的标记子图熵图核，它具有较好的结构相似性评估性能，采用了动态规划子图枚举算法有效减少了时间复杂度，同时提出了标记子图来丰富子结构拓扑，用全局图熵来描述网络，已应用于多个真实数据集，并能对不同任务产生良好效果。 |
| [^86] | [Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision.](http://arxiv.org/abs/2303.13540) | 本论文在可持续性方面的主要贡献是使用深度学习技术提高产品生产和使用的可持续性，通过计算机视觉技术检测产品的磨损状态并用于改进智能产品-服务系统的集成和结果取向。 |
| [^87] | [Decentralized Multi-Agent Reinforcement Learning for Continuous-Space Stochastic Games.](http://arxiv.org/abs/2303.13539) | 本研究提出了一种分布式的多智能体强化学习算法，证明了其在连续空间随机博弈中的策略更新接近最优，同时推导了一般类算法的全局策略更新动态。 |
| [^88] | [Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices.](http://arxiv.org/abs/2303.13538) | 该论文捕获了首个公开可访问的商用设备真实射频指纹数据集，这对于识别非法或未授权发射器具有重要意义。 |
| [^89] | [Towards risk-informed PBSHM: Populations as hierarchical systems.](http://arxiv.org/abs/2303.13533) | PBSHM是一种新的结构健康监测方法，它通过对结构群体进行监测，将有价值的知识在结构实例之间传递，从而提高了决策的准确性和泛化能力。 |
| [^90] | [Enhancing Peak Network Traffic Prediction via Time-Series Decomposition.](http://arxiv.org/abs/2303.13529) | 本文通过将时间序列数据分解为趋势、季节性和噪声三个部分，并分别预测峰值流量，提高了峰值网络流量预测的能力。 |
| [^91] | [Uncertainty-Aware Workload Prediction in Cloud Computing.](http://arxiv.org/abs/2303.13525) | 本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。 |
| [^92] | [Neural Preset for Color Style Transfer.](http://arxiv.org/abs/2303.13511) | 本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。 |
| [^93] | [Adaptive Regularization for Class-Incremental Learning.](http://arxiv.org/abs/2303.13113) | 本文研究了适应性正则化在类增量学习中的应用，通过根据任务复杂度动态调整正则化强度，在学习新类别同时防止遗忘之前学习的类别。实验表明适应性正则化可以实现更加准确和不易遗忘的视觉增量学习。 |
| [^94] | [Lower Bound on the Bayesian Risk via Information Measure.](http://arxiv.org/abs/2303.12497) | 新提出一种方法计算贝叶斯风险下界，允许使用几乎任何信息度量，能提供与估计器无关的不可能结果。已应用于离散和连续参数问题，与最先进的技术进行了比较。 |
| [^95] | [Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms.](http://arxiv.org/abs/2303.11699) | 本研究提出了一种使用合成晶体数据集，通过神经网络从ICSD粉末X射线衍射图中提取结构信息的方法，并且在空间群分类任务上，取得了比直接在ICSD晶体上进行训练更高的准确度。 |
| [^96] | [Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation.](http://arxiv.org/abs/2303.11602) | 本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。 |
| [^97] | [Dynamic-Aware Loss for Learning with Label Noise.](http://arxiv.org/abs/2303.11562) | 本文提出一种动态感知损失函数，采用增强拟合能力，逐渐增加鲁棒性的权重来处理标签噪声。在后期阶段，引入自举项，让DNN更加重视容易的样例，证明了这种方法的优越性。 |
| [^98] | [ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time Social Ambiance Measurement.](http://arxiv.org/abs/2303.10727) | 本文提出了一种面向能源高效性和实时性的社交氛围测量神经网络架构搜索框架ERSAM。该框架可以自动搜索适合SAM任务的神经网络架构，并满足能源效率、实时处理和有限标签数据的要求。在基准数据集上，该框架优于现有解决方案，具有更好的精度和效率。 |
| [^99] | [DBSCAN of Multi-Slice Clustering for three-order Tensor.](http://arxiv.org/abs/2303.07768) | 本文提出了 MSC-DBSCAN扩展算法，可以在三元聚类中从数据中提取不同子空间的不同切片聚类，并可以获得与 MSC 算法在处理秩一张量数据时相同的解决方案。 |
| [^100] | [Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective.](http://arxiv.org/abs/2302.11963) | 本文首次将单步对抗示例分解为数据信息和自信息，揭示了一种有趣的现象，称为“自我拟合”，即网络学习单步扰动中嵌入的自信息，导致灾难性过拟合。 |
| [^101] | [An Operational Perspective to Fairness Interventions: Where and How to Intervene.](http://arxiv.org/abs/2302.01574) | 该论文提出了一个平衡预测性能、跨组差异、保护敏感群组属性和工程成本的全面框架，探讨了公平性干预的“何时”和“如何”，同时通过预测平等的案例研究，展示了一种新的方法。 |
| [^102] | [Real-Time Evaluation in Online Continual Learning: A New Hope.](http://arxiv.org/abs/2302.01047) | 该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。 |
| [^103] | [One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER.](http://arxiv.org/abs/2301.10410) | 本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。 |
| [^104] | [Deep Conditional Measure Quantization.](http://arxiv.org/abs/2301.06907) | 提出了一种名为DCMQ的方法，该方法结合了基于Huber能量核的方法和深度神经网络架构，用于条件测度量化，并在多个实例中取得了有希望的结果。 |
| [^105] | [Detection of out-of-distribution samples using binary neuron activation patterns.](http://arxiv.org/abs/2212.14268) | 本文提出了一种检测样本分布情况的新方法，该方法利用神经元激活模式，通过二进制表示提取卷积层的激活模式，具有更高的检测准确度。 |
| [^106] | [Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models.](http://arxiv.org/abs/2212.12380) | 本论文研究了物理一致神经网络(PCNNs) 在模拟建筑温度动态方面的扩展性和准确性。结果发现，PCNNs既确保了物理一致性，同时又能在复杂的多区域热建筑模型中取得高精度的性能表现，且在可用数据量有限的情况下超越经典灰盒模型，具有可扩展性优势。 |
| [^107] | [POTATO: The Portable Text Annotation Tool.](http://arxiv.org/abs/2212.08620) | POTATO是一个免费、开源的便携式文本注释工具，支持多种类型的文本和多模态数据的标注，提供易于配置的功能以最大化生产力，特别是对于长文档和复杂任务。 |
| [^108] | [FlexiViT: One Model for All Patch Sizes.](http://arxiv.org/abs/2212.08013) | 本文介绍了一种名为FlexiViT的模型，它可以使用一组权重表现良好，并且适用于各种补丁大小，从而使其在部署时容易根据不同的计算预算来定制模型。 |
| [^109] | [Misspecification in Inverse Reinforcement Learning.](http://arxiv.org/abs/2212.03201) | 本文研究了逆强化学习中的错误规范问题，探讨了不同模型与演示策略之间的偏差，以及在何种情况下模型会导致误导性推断。 |
| [^110] | [Convolution, aggregation and attention based deep neural networks for accelerating simulations in mechanics.](http://arxiv.org/abs/2212.01386) | 本研究探讨了基于卷积、聚合和注意力的深度神经网络架构，为学习固体变形的高度非线性变化提供了有效解决方案，并在两个基准示例上取得了良好性能。 |
| [^111] | [MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series.](http://arxiv.org/abs/2212.01141) | 本文提出了一种名为MHCCL的对比学习模型，可以从多元时间序列数据中学习语义丰富的表示，并利用层次聚类结构中的多粒度信息来滤除虚假负样本和补充正样本。 |
| [^112] | [Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test.](http://arxiv.org/abs/2211.16596) | 该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。 |
| [^113] | [RUST: Latent Neural Scene Representations from Unposed Imagery.](http://arxiv.org/abs/2211.14306) | 该论文提出了一种无需摆姿势的新视角合成方法RUST，通过训练一个姿势编码器学习图像中的潜在姿态嵌入，可有效地推广到多个场景和提高视觉质量，优于最先进的无监督方法。 |
| [^114] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^115] | [Less Emphasis on Difficult Layer Regions: Curriculum Learning for Singularly Perturbed Convection-Diffusion-Reaction Problems.](http://arxiv.org/abs/2210.12685) | 提出了一种基于课程学习的新的优化方法，让神经网络优先学习容易的区域，减少对难的区域的学习，从而提高对奇异摄动对流扩散反应问题的预测精度。 |
| [^116] | [When and why vision-language models behave like bags-of-words, and what to do about it?.](http://arxiv.org/abs/2210.01936) | 针对大型视觉语言模型在编码组合信息方面的表现存在问题，我们创建了Attribution、Relation和Order（ARO）基准，并提出了对VLMs进行Attention机制和对抗训练等修改以提高其组合理解能力。 |
| [^117] | [Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions.](http://arxiv.org/abs/2209.15055) | 本文研究了全连接神经网络的表示成本和深度之间的关系，发现其会收敛到非线性函数的秩的概念。同时，发现在一定的深度范围内，全局最小值可以恢复真实的数据秩，并探讨了分类器秩对类边界拓扑结构的影响。 |
| [^118] | [Continuous Mixtures of Tractable Probabilistic Models.](http://arxiv.org/abs/2209.10584) | 本文研究了一种连续混合的方法，将可计算概率模型和基于连续潜空间的模型结合起来，从而实现了高效的概率推断。 |
| [^119] | [TrojViT: Trojan Insertion in Vision Transformers.](http://arxiv.org/abs/2208.13049) | 本文提出了一种针对视觉Transformer的隐秘实用的后门攻击方法TrojViT，通过补丁级触发器将一些易受攻击位组成的参数建立为特洛伊木马。 |
| [^120] | [Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors.](http://arxiv.org/abs/2208.11356) | 本文提出了一种在基于Transformer的目标检测器中，通过两个新的设计实现了多尺度特征的高效利用，其核心思想是通过稀疏采样多尺度特征，以减少计算成本，同时保证了目标检测性能。 |
| [^121] | [The Integration of Machine Learning into Automated Test Generation: A Systematic Mapping Study.](http://arxiv.org/abs/2206.10210) | 该论文研究了将机器学习应用于自动化测试生成的方法，并总结了目前的研究进展、应用和挑战。其中监督学习和强化学习是主要的技术方法。 |
| [^122] | [Efficient decentralized multi-agent learning in asymmetric bipartite queueing systems.](http://arxiv.org/abs/2206.03324) | 该论文提出了一种简单的学习算法，使得在非对称二分排队系统中实现了高效的性能，并具备附加的鲁棒性质，并且提供了该问题的集中式情况下的首个经过证明的有效的UCB算法。 |
| [^123] | [On the Symmetries of Deep Learning Models and their Internal Representations.](http://arxiv.org/abs/2205.14258) | 本文研究了深度学习模型及其内部表示的对称性，发现网络的对称性传播到数据的表示中的对称性，为更好地理解架构影响学习和预测过程提供了一种方法。 |
| [^124] | [Continual Spatio-Temporal Graph Convolutional Networks.](http://arxiv.org/abs/2203.11009) | 本文提出了一种连续时空图卷积神经网络（CoST-GCN）来解决基于图的方法在在线推理设置中应用时重复计算的问题，相比之前的方法最多可将时间复杂度减少109倍。 |
| [^125] | [Euler State Networks: Non-dissipative Reservoir Computing.](http://arxiv.org/abs/2203.09382) | 本文提出了一种新型水库计算模型EuSN，其利用前向欧拉离散化和反对称循环矩阵来设计水库动力学，具有接近稳定边缘的饱和有效谱半径和零局部李雅普诺夫指数。在长期记忆任务和时间序列分类基准测试中表现出了优异的性能。 |
| [^126] | [Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models.](http://arxiv.org/abs/2202.00091) | 研究提出了基于决策的稀疏攻击方法SparseEvo，通过最小化扰动的像素数量，能够有效地欺骗黑盒深度学习模型，而不需要了解它的内部构造。 |
| [^127] | [Coordinate Descent Methods for Fractional Minimization.](http://arxiv.org/abs/2201.12691) | 本文提出两种坐标下降方法，用于解决具有结构化约束的分数规划问题，并保证收敛到逐坐标静态点。 |
| [^128] | [Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems.](http://arxiv.org/abs/2201.10542) | 本文提出一种增强式RBMLE-UCB算法，将RBMLE方法的惩罚与UCB方法的约束相结合，解决了自适应LQ控制问题。在噪声较高或样本数较少时，该算法表现更优。 |
| [^129] | [Decompositional Quantum Graph Neural Network.](http://arxiv.org/abs/2201.05158) | 本论文提出了一种新型的基于量子计算的神经网络，能够处理基于图的数据，通过使用自我图和张量积降低模型参数数量，提出一种从现实世界数据到希尔伯特空间的新型映射方式。 |
| [^130] | [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing.](http://arxiv.org/abs/2111.09543) | 本论文介绍了一种新的预训练语言模型DeBERTaV3，使用更加样本有效的替换令牌检测（RTD）取代了掩码语言建模（MLM）并提出了一种新的梯度去耦合嵌入共享方法，避免了“拔河”动态，提高了预训练模型的训练效率和质量。在多个下游自然语言理解任务中，DeBERTaV3表现出优秀的性能。 |
| [^131] | [Efficient Algorithms for Learning from Coarse Labels.](http://arxiv.org/abs/2108.09805) | 本文研究了从粗糙数据中学习的问题，提出了一种高效的算法，只需足够信息的粗标签即可在从细标签中学习的任何问题上进行学习。 |
| [^132] | [Local Clustering in Contextual Multi-Armed Bandits.](http://arxiv.org/abs/2103.00063) | 本文提出了一种基于未知赌博机参数的本地聚类策略，用于上下文多臂赌博机中的用户聚类问题。该算法在内容推荐和定向广告中具有重要应用价值，并在实验中表现出较高的性能。 |
| [^133] | [Near Optimal Adversarial Attack on UCB Bandits.](http://arxiv.org/abs/2008.09312) | 本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。 |

# 详细

[^1]: TRAK: 刻画大规模模型行为

    TRAK: Attributing Model Behavior at Scale. (arXiv:2303.14186v1 [stat.ML])

    [http://arxiv.org/abs/2303.14186](http://arxiv.org/abs/2303.14186)

    TRAK是一种适用于大规模、可微模型的数据归因方法，既有效又计算量可行。

    

    数据归因的目标是追踪模型预测结果的原始训练数据。虽然已经有很多工作致力于实现这一目标，但现有方法往往要求用户在计算效率和准确性之间做出选择。也就是说，在非凸场景（例如，深度神经网络领域）中，计算量可行的方法可能难以准确地归因模型预测结果，而在这类场景中有效的方法则需要训练数千个模型，这使得它们在大型模型或数据集中实际应用具有不可行性。在本文中，我们介绍了TRAK（随机投影核追踪），这是一种数据归因方法，适用于大规模、可微模型，既有效又计算量可行。具体来说，通过仅使用少量训练模型，TRAK 可以匹配需要训练数千模型才能得到的归因方法的性能。我们论证了TRAK 在各种模式和规模上的实用性。

    The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.  In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scal
    
[^2]: 找到对抗样本需要多少维度？

    How many dimensions are required to find an adversarial example?. (arXiv:2303.14173v1 [cs.LG])

    [http://arxiv.org/abs/2303.14173](http://arxiv.org/abs/2303.14173)

    本文研究了对抗性漏洞如何取决于受限于高维输入空间中的子空间维数，同时针对标准PGD攻击的对抗性成功率提出了单调递增函数表达式。

    

    过去探索对抗性漏洞的研究都着眼于对手可以扰动模型输入的所有维度的情况。另一方面，许多最近的研究考虑以下情况：（i）对手可以扰动有限数量的输入参数或（ii）多模态问题中的模态子集。在这两种情况下，对抗性样本有效地受限于高维输入空间中的子空间$V$。出于这个动机，我们在本文中研究了对抗性漏洞如何取决于$V$的维数。特别地，我们展示了标准PGD攻击的对抗性成功率如何表现为$\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$的单调递增函数，其中$\epsilon$是扰动预算，$\frac{1}{p}+\frac{q}{q}=1$，只要$p>1$（当$p=1$时会出现额外的细微差别，我们对此进行了详细的分析）。这个函数形式可以很容易地推导。

    Past work exploring adversarial vulnerability have focused on situations where an adversary can perturb all dimensions of model input. On the other hand, a range of recent works consider the case where either (i) an adversary can perturb a limited number of input parameters or (ii) a subset of modalities in a multimodal problem. In both of these cases, adversarial examples are effectively constrained to a subspace $V$ in the ambient input space $\mathcal{X}$. Motivated by this, in this work we investigate how adversarial vulnerability depends on $\dim(V)$. In particular, we show that the adversarial success of standard PGD attacks with $\ell^p$ norm constraints behaves like a monotonically increasing function of $\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$ where $\epsilon$ is the perturbation budget and $\frac{1}{p} + \frac{1}{q} =1$, provided $p > 1$ (the case $p=1$ presents additional subtleties which we analyze in some detail). This functional form can be easily deriv
    
[^3]: 一种具有行列交错像素合成的高效尺度不变生成器

    Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis. (arXiv:2303.14157v1 [cs.CV])

    [http://arxiv.org/abs/2303.14157](http://arxiv.org/abs/2303.14157)

    提出了一种高效尺度不变的生成模型，不使用空间卷积或分层架构，通过将层内特征图分解为局部和全局特征并通过插值这些特征生成图像，使得模型具有较小的内存占用和更快的推理速度。

    

    任意比例图像合成为合成在任意比例下合成逼真图像提供了一种高效且可扩展的解决方案，即使超出了2K分辨率范围。然而，现有的基于GAN的解决方案过度依赖于卷积和分层架构，在缩放输出分辨率时会引入不一致性和“纹理粘连”问题。从另一个角度来看，基于INR的生成器从设计上是尺度等变的，但它们巨大的内存占用和缓慢的推理妨碍了这些网络在大规模或实时系统中的应用。在本研究中，我们提出了一种新的生成模型：具有行列交错像素合成的列-行交错像素合成（$\textbf{CREPS}$）。不使用任何空间卷积或从粗到细的设计。为了节省内存占用并使系统可扩展，我们采用了一种新颖的双线表示法，将层内特征图分解为局部和全局特征的独立“厚条”插值这些条带的融合来生成图像。我们的实验表明，$\textbf{CREPS}$在图像质量和可扩展性方面优于现有技术，在保持更小的内存占用和更快的推理速度的同时。

    Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the $``$texture sticking$"$ issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose $\textbf{C}$olumn-$\textbf{R}$ow $\textbf{E}$ntangled $\textbf{P}$ixel $\textbf{S}$ynthesis ($\textbf{CREPS}$), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate $``$thick
    
[^4]: 医学图像识别的局部对比学习

    Local Contrastive Learning for Medical Image Recognition. (arXiv:2303.14153v1 [cs.CV])

    [http://arxiv.org/abs/2303.14153](http://arxiv.org/abs/2303.14153)

    本文提出了一种局部对比学习的微调框架LRCLR，该框架能够识别医学图像中显著的局部图像区域，提供有意义的图像和文本的解释，并显著提高了零样本性能。

    

    基于深度学习的放射学图像分析方法的普及已经创造了对专家标记的放射学数据的巨大需求。最近的自监督框架通过从相关放射学报告中获取监督来缓解对专家标注的需求。然而，这些框架往往难以区分医学图像中不同病理的细微差别。此外，它们中的许多不提供图像区域和文本之间的解释，使诊断医师很难评估模型预测。在本研究中，我们提出了局部对比学习（LRCLR）的灵活微调框架，该框架添加了显着的图像区域选择层以及跨模态交互。我们的结果对一个外部的胸透验证集表明，LRCLR识别了显著的局部图像区域，并在改善了几项胸透分类任务的零样本性能的同时提供了有意义的与放射学文本的解释。

    The proliferation of Deep Learning (DL)-based methods for radiographic image analysis has created a great demand for expert-labeled radiology data. Recent self-supervised frameworks have alleviated the need for expert labeling by obtaining supervision from associated radiology reports. These frameworks, however, struggle to distinguish the subtle differences between different pathologies in medical images. Additionally, many of them do not provide interpretation between image regions and text, making it difficult for radiologists to assess model predictions. In this work, we propose Local Region Contrastive Learning (LRCLR), a flexible fine-tuning framework that adds layers for significant image region selection as well as cross-modality interaction. Our results on an external validation set of chest x-rays suggest that LRCLR identifies significant local image regions and provides meaningful interpretation against radiology text while improving zero-shot performance on several chest x-
    
[^5]: 双重下降的谜团：辨识、解释和消解深度学习之谜

    Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle. (arXiv:2303.14151v1 [cs.LG])

    [http://arxiv.org/abs/2303.14151](http://arxiv.org/abs/2303.14151)

    双重下降是机器学习中一个令人惊讶的现象，数据量、数据维度和模型参数是影响双重下降的关键因素。研究者找到了制造双重下降的三个解释性因素，并证明这些因素可以在简单神经网络中关闭。

    

    双重下降是机器学习中一个令人惊讶的现象，即随着模型参数数量相对于数据量的增长，测试误差在模型不断扩大而进入高度超参数化（数据未充分采样）阶段时下降。测试误差下降的这种情况与传统的关于过度拟合的学习理论相悖，可谓承载了机器学习中大型模型的成功。这种非单调的测试误差变化行为取决于数据的数量、数据的维度和模型参数的数量。在本文中，我们简要描述了双重下降现象，然后用易于理解和接受的方式对为何出现双重下降进行了解释，只需要了解线性代数和概率论的知识。我们使用多项式回归提供可视化的直观感受，然后通过普通线性回归数学分析双重下降，确定了三个解释性因素，当同时存在时，可以共同制造双重下降现象。随后，我们展示了这些因素如何被关闭在输出单个标量的简单神经网络中。最后，我们通过控制消融实验测试了这些因素在现代深度神经网络中的重要性，并证明双重下降现象不仅仅是超参数化模型所特有的，也出现在欠参数化模型和插值阈值中。

    Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double des
    
[^6]: CIFAKE: AI生成图像的分类和可解释识别

    CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. (arXiv:2303.14126v1 [cs.CV])

    [http://arxiv.org/abs/2303.14126](http://arxiv.org/abs/2303.14126)

    本文提出了一个名为CIFAKE的方法，可以通过计算机视觉将真实照片与AI生成图像进行二元分类。该方法使用了卷积神经网络（CNN）对图像进行分类。

    

    最近合成数据技术的进步使得生成的图像质量如此之高，以至于人类无法区分真实照片和人工智能生成的图像。鉴于数据可靠性和认证的至关重要性，本文提出通过计算机视觉增强我们识别AI生成图像的能力。首先，通过潜在扩散生成合成数据集，该数据集与已有的CIFAR-10数据集中的十个类别相似，提供与真实照片对比的不同类型的图像。该模型能够生成复杂的视觉属性，例如水中逼真的反射。这两组数据之间存在二元分类问题，即照片是真实的还是由AI生成的。本研究随后提出使用卷积神经网络（CNN）将图像分类为两个类别：真实或伪造。

    Recent technological advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion which provides a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following
    
[^7]: 从基础与应用研究视角提高预测性能和模型可解释性：注意力机制

    Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])

    [http://arxiv.org/abs/2303.14116](http://arxiv.org/abs/2303.14116)

    这篇论文总结了作者关于注意力机制在提高深度学习模型解释性和性能中的潜力。这是基础与应用研究的重要问题，尤其在医学领域，而注意力机制可以成为解决这个问题的一种方法。

    

    随着深度学习技术的快速发展，机器学习研究关注于提高模型预测的可解释性和性能，涵盖基础与应用研究的各个领域。虽然深度学习模型比传统机器学习模型具有更高的预测性能，但具体预测过程仍难以解释和说明，这被称为机器学习模型的黑盒化，并被广泛认为是许多研究领域的一个特别重要的问题，包括制造业、商业、机器人和其他行业等普遍使用该技术，以及医学领域，在这些领域中错误是不可容忍的。本文基于作者论文的摘要，该论文的核心研究关注于近年来备受关注的注意力机制，并探讨了其在基础研究和应用研究中的潜力。

    With the dramatic advances in deep learning technology, machine learning research is focusing on improving the interpretability of model predictions as well as prediction performance in both basic and applied research. While deep learning models have much higher prediction performance than traditional machine learning models, the specific prediction process is still difficult to interpret and/or explain. This is known as the black-boxing of machine learning models and is recognized as a particularly important problem in a wide range of research fields, including manufacturing, commerce, robotics, and other industries where the use of such technology has become commonplace, as well as the medical field, where mistakes are not tolerated. This bulletin is based on the summary of the author's dissertation. The research summarized in the dissertation focuses on the attention mechanism, which has been the focus of much attention in recent years, and discusses its potential for both basic res
    
[^8]: 通过离散优化实现可解释性异常检测

    Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])

    [http://arxiv.org/abs/2303.14111](http://arxiv.org/abs/2303.14111)

    该论文提出了一个通过学习有限自动机进行异常检测的框架，并通过约束优化算法和新的正则化方案提高了可解释性。

    

    异常检测在许多应用领域中都是必不可少的，例如网络安全、执法、医学和欺诈保护。然而，目前深度学习方法的决策过程往往难以理解，这通常限制了它们的实际应用性。为了克服这个限制，我们提出了一个学习框架，可以从序列数据中学习可解释性的异常检测器。具体来说，我们考虑从给定的未标记序列多重集中学习确定性有限自动机 （DFA）的任务。我们证明了这个问题是计算难题，并基于约束优化开发了两个学习算法。此外，我们为优化问题引入了新的正则化方案，以提高我们的DFA的整体可解释性。通过原型实现，我们证明我们的方法在准确性和F1分数方面表现出有望的结果。

    Anomaly detection is essential in many application domains, such as cyber security, law enforcement, medicine, and fraud protection. However, the decision-making of current deep learning approaches is notoriously hard to understand, which often limits their practical applicability. To overcome this limitation, we propose a framework for learning inherently interpretable anomaly detectors from sequential data. More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences. We show that this problem is computationally hard and develop two learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate that our approach shows promising results in terms of accuracy and F1 score.
    
[^9]: 通过扰动增广信息瓶颈，提升多重可靠性指标

    Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck. (arXiv:2303.14096v1 [cs.LG])

    [http://arxiv.org/abs/2303.14096](http://arxiv.org/abs/2303.14096)

    本论文提出了在互信息约束下的对抗学习训练模型，针对数据在获取方式上可能存在的偏差，扩展了信息瓶颈的模型来应对这种问题。该方案结合了卷积和变压器的混合判别-生成训练，在不使用领域特定知识的情况下，显著提高了学习表征的鲁棒性。

    

    在数据训练有限的实际场景中，许多预测信号往往来自于数据获取的某些偏差（即不够具有一般性），因此不能防止模型在这些（所谓的）“快捷”信号上进行共适应，这使得模型在各种分布转移方面变得脆弱。为了避免这种失效模式，我们考虑了对互信息约束下的对抗威胁模型，以涵盖更广泛的训练扰动类别。这促使我们将标准信息瓶颈扩展到另外模拟有害信息。我们提出基于自编码器的训练来实现这一目标，以及实用的编码器设计，以便于基于卷积和变压器的架构的混合判别-生成训练。实验结果表明，所提出的方案显著提高了所学表征的鲁棒性（显著地没有使用任何领域特定的知识）。

    In practical scenarios where training data is limited, many predictive signals in the data can be rather from some biases in data acquisition (i.e., less generalizable), so that one cannot prevent a model from co-adapting on such (so-called) "shortcut" signals: this makes the model fragile in various distribution shifts. To bypass such failure modes, we consider an adversarial threat model under a mutual information constraint to cover a wider class of perturbations in training. This motivates us to extend the standard information bottleneck to additionally model the nuisance information. We propose an autoencoder-based training to implement the objective, as well as practical encoder designs to facilitate the proposed hybrid discriminative-generative training concerning both convolutional- and Transformer-based architectures. Our experimental results show that the proposed scheme improves robustness of learned representations (remarkably without using any domain-specific knowledge), w
    
[^10]: 物理启发的神经网络在利用暗物质重建流体力学模拟中的应用

    Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v1 [astro-ph.CO])

    [http://arxiv.org/abs/2303.14090](http://arxiv.org/abs/2303.14090)

    本文介绍了一种新的方法，将物理启发的神经网络应用于重建流体力学模拟中，通过将理论知识注入模型损失函数并结合新的性能评估指标，成功实现了对重子散射的重建。

    

    物理启发的神经网络已经成为一个合理的框架，用于构建将统计模式与领域知识相结合的预测模型。其基本理念是通过已知关系来丰富优化损失函数以限制可能解决方案的空间。水动力学模拟是现代宇宙学的核心组成部分，而所需的计算既昂贵又耗时。与此同时，快速模拟暗物质需要更少的资源，这导致了机器学习算法成为研究的一个活跃领域;在这里，重建流体力学模拟中发现的散射是一个持续的挑战。本文提出了将物理启发的神经网络应用于重建流体力学模拟中的新方法，它结合了神经网络架构的进步和物理约束，将关于重子转化效率的理论注入模型损失函数。我们还介绍了一种新的性能评估指标，基于结果图像中动力学功率谱中的误差，这使得可以量化网络对宇宙学参数推断的适用性。

    Physics-informed neural networks have emerged as a coherent framework for building predictive models that combine statistical patterns with domain knowledge. The underlying notion is to enrich the optimization loss function with known relationships to constrain the space of possible solutions. Hydrodynamic simulations are a core constituent of modern cosmology, while the required computations are both expensive and time-consuming. At the same time, the comparatively fast simulation of dark matter requires fewer resources, which has led to the emergence of machine learning algorithms for baryon inpainting as an active area of research; here, recreating the scatter found in hydrodynamic simulations is an ongoing challenge. This paper presents the first application of physics-informed neural networks to baryon inpainting by combining advances in neural network architectures with physical constraints, injecting theory on baryon conversion efficiency into the model loss function. We also in
    
[^11]: 差分隐私合成控制

    Differentially Private Synthetic Control. (arXiv:2303.14084v1 [cs.LG])

    [http://arxiv.org/abs/2303.14084](http://arxiv.org/abs/2303.14084)

    本文提供了首个具有显式误差界限的差分隐私合成控制算法，具有广泛的应用前景。

    

    合成控制是一种因果推断工具，用于通过创建合成对照数据来估计干预的治疗效果。这种方法结合了来自其他相似观察（即，捐赠者池）的测量结果，通过分析干预前目标和捐赠者池之间的关系来预测感兴趣的反事实时间序列（即，目标单元）。随着合成控制工具被越来越应用于敏感或专有数据，形式化的隐私保护通常是必需的。在这项工作中，我们提供了首个具有显式误差界限的差分隐私合成控制算法。我们的方法基于非私有合成控制和差分隐私经验风险最小化的工具。我们提供关于合成控制查询敏感性的上下界，并提供有关我们的私有合成控制算法准确性的显式误差界限。我们展示了我们的算法产生准确的预测结果。

    Synthetic control is a causal inference tool used to estimate the treatment effects of an intervention by creating synthetic counterfactual data. This approach combines measurements from other similar observations (i.e., donor pool ) to predict a counterfactual time series of interest (i.e., target unit) by analyzing the relationship between the target and the donor pool before the intervention. As synthetic control tools are increasingly applied to sensitive or proprietary data, formal privacy protections are often required. In this work, we provide the first algorithms for differentially private synthetic control with explicit error bounds. Our approach builds upon tools from non-private synthetic control and differentially private empirical risk minimization. We provide upper and lower bounds on the sensitivity of the synthetic control query and provide explicit error bounds on the accuracy of our private synthetic control algorithms. We show that our algorithms produce accurate pre
    
[^12]: 学生-教师框架下随机特征模型的在线学习

    Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])

    [http://arxiv.org/abs/2303.14083](http://arxiv.org/abs/2303.14083)

    本研究考虑了一种两层神经网络模型的在线学习，研究发现，当学生的隐藏层大小呈指数增长时，完美泛化是可行的，但对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化。

    

    深度神经网络是一种广泛应用的预测算法，随着权重数量的增加，其性能通常会提高，导致过度参数化。我们考虑一种两层神经网络，其第一层是冻结的，而最后一层是可训练的，称为随机特征模型。我们在学生-教师框架下研究了过度参数化，通过导出一组学习动态的微分方程。对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化，并计算非零渐近泛化误差。只有当学生的隐藏层大小呈指数增长时，才有可能实现完美泛化。

    Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
    
[^13]: 自适应实例级损失平滑改进对抗训练

    Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])

    [http://arxiv.org/abs/2303.14077](http://arxiv.org/abs/2303.14077)

    本文提出了一种新的对抗训练方法(Instance-adaptive Adversarial Training, IAAT)通过平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本，取得了在各种数据集下的最新、最佳结果，并在白盒和黑盒攻击下均优于以前的方法。

    

    通过对输入进行对抗扰动：即人类难以察觉的人造噪声，可以轻易地迷惑深度神经网络从而做出不正确的预测。目前对抗训练已成为最成功的对抗攻击防御方法，本文致力于改进对抗训练以提升对抗鲁棒性。首先从实例级别的角度分析了对抗训练期间对抗性脆弱性的演变。发现在训练期间，通过牺牲相当比例的训练样本来提高对抗攻击的脆弱性，从而实现对抗性损失的整体降低，这导致了不同数据的对抗性脆弱性分布不均衡。这种“不均衡脆弱性”在几种流行的鲁棒性训练方法中普遍存在，并且与对抗训练中的过拟合相关。基于此观察，我们提出了一种新的对抗训练方法：Instance-adaptive Adversarial Training (IAAT)。该方法在训练过程中平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本。本方法在各种数据集下都取得了最新的最佳结果，并在白盒和黑盒攻击下均优于以前的方法。

    Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt
    
[^14]: 基于CNN-LSTM架构的 AIS 数据船舶轨迹关联算法

    A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data. (arXiv:2303.14068v1 [cs.LG])

    [http://arxiv.org/abs/2303.14068](http://arxiv.org/abs/2303.14068)

    本文提出了一种基于1D CNN-LSTM结构的船舶轨迹关联算法，采用多变量时间序列问题来解决

    

    在海洋监测中，区分正常和异常的船舶运动模式对于及时识别潜在威胁至关重要。为了实现该目标，需要使用轨迹关联算法，将由运动参数组成的时序观测结果与相应的船只关联。本文提出了一种基于1D CNN-LSTM结构的轨迹关联框架，将这一追踪任务视为多变量时间序列问题来解决。

    In marine surveillance, distinguishing between normal and anomalous vessel movement patterns is critical for identifying potential threats in a timely manner. Once detected, it is important to monitor and track these vessels until a necessary intervention occurs. To achieve this, track association algorithms are used, which take sequential observations comprising geological and motion parameters of the vessels and associate them with respective vessels. The spatial and temporal variations inherent in these sequential observations make the association task challenging for traditional multi-object tracking algorithms. Additionally, the presence of overlapping tracks and missing data can further complicate the trajectory tracking process. To address these challenges, in this study, we approach this tracking task as a multivariate time series problem and introduce a 1D CNN-LSTM architecture-based framework for track association. This special neural network architecture can capture the spat
    
[^15]: 拓扑数据分析中的欧拉特征工具

    Euler Characteristic Tools For Topological Data Analysis. (arXiv:2303.14040v1 [cs.LG])

    [http://arxiv.org/abs/2303.14040](http://arxiv.org/abs/2303.14040)

    本文研究了欧拉特征技术在拓扑数据分析中的应用，利用点运算欧拉特征得到欧拉特征轮廓，在监督和无监督任务中实现了最先进性能，并提供了欧拉轮廓及其混合变换捕捉信息的启发式方法。

    

    本文研究了拓扑数据分析中的欧拉特征技术。从数据构建的一族单纯复合体的点运算欧拉特征，得到所谓的欧拉特征轮廓。我们展示了这个简单描述符以极低的计算成本在监督任务中实现了最先进性能。受信号分析的启发，我们计算欧拉特征轮廓的混合变换。这些积分变换将欧拉特征技术与勒贝格积分混合，提供高效压缩拓扑信号。因此，它们在无监督设置中表现出卓越的性能。在定性方面，我们提供了关于欧拉轮廓及其混合变换所捕捉的拓扑和几何信息的众多启发式方法。最后，我们证明了这些描述符的稳定性结果以及在随机设置下的渐近保证。

    In this article, we study Euler characteristic techniques in topological data analysis. Pointwise computing the Euler characteristic of a family of simplicial complexes built from data gives rise to the so-called Euler characteristic profile. We show that this simple descriptor achieve state-of-the-art performance in supervised tasks at a very low computational cost. Inspired by signal analysis, we compute hybrid transforms of Euler characteristic profiles. These integral transforms mix Euler characteristic techniques with Lebesgue integration to provide highly efficient compressors of topological signals. As a consequence, they show remarkable performances in unsupervised settings. On the qualitative side, we provide numerous heuristics on the topological and geometric information captured by Euler profiles and their hybrid transforms. Finally, we prove stability results for these descriptors as well as asymptotic guarantees in random settings.
    
[^16]: PENTACET数据——2300万个上下文代码注释和50万个自我承认技术债务注释

    PENTACET data -- 23 Million Contextual Code Comments and 500,000 SATD comments. (arXiv:2303.14029v1 [cs.SE])

    [http://arxiv.org/abs/2303.14029](http://arxiv.org/abs/2303.14029)

    本文介绍了PENTACET数据集，该数据集包含了2300万个上下文代码注释和50万个自我承认技术债务注释，并提供了详细的上下文数据，将进一步推动使用人工智能技术的SATD研究。

    

    大多数自我承认技术债务（SATD）研究使用如“TODO”和“FIXME”之类的显式SATD特征进行SATD检测。更仔细地观察发现，一些SATD研究使用简单的SATD（“易于发现的”）代码注释而没有上下文数据（前文和后文源代码上下文）。本研究通过PENTACET（或5C数据集）数据填补了这一空白。PENTACET是一个由贡献者进行筛选的大型上下文代码注释数据库，是最全面的SATD数据。我们从9,096个开源软件Java项目中挖掘了总共435百万行代码。结果是一个数据集，包含了2300万个代码注释，并为每个注释提供了前后源代码上下文，以及50万个被标记为SATD的注释，包括“易于发现的”和“难于发现的” SATD。我们相信PENTACET数据集将进一步推动使用人工智能技术的SATD研究。

    Most Self-Admitted Technical Debt (SATD) research utilizes explicit SATD features such as 'TODO' and 'FIXME' for SATD detection. A closer look reveals several SATD research uses simple SATD ('Easy to Find') code comments without the contextual data (preceding and succeeding source code context). This work addresses this gap through PENTACET (or 5C dataset) data. PENTACET is a large Curated Contextual Code Comments per Contributor and the most extensive SATD data. We mine 9,096 Open Source Software Java projects with a total of 435 million LOC. The outcome is a dataset with 23 million code comments, preceding and succeeding source code context for each comment, and more than 500,000 comments labeled as SATD, including both 'Easy to Find' and 'Hard to Find' SATD. We believe PENTACET data will further SATD research using Artificial Intelligence techniques.
    
[^17]: ASTRA-sim2.0: 模拟分层网络和分解系统，实现大型模型训练的规模化

    ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale. (arXiv:2303.14006v1 [cs.DC])

    [http://arxiv.org/abs/2303.14006](http://arxiv.org/abs/2303.14006)

    本文介绍了ASTRA-sim2.0，一种分布式训练模拟基础设施，可模拟当代分布式训练模型和平台。ASTRA-sim2.0易于使用、灵活，支持任意模型并行化策略、多维异构拓扑、任意分解内存系统和精细的仪器和事件跟踪。

    

    随着深度学习模型和输入数据以前所未有的速度扩展，采用分布式训练平台来适应模型并提高训练吞吐量是不可避免的。最先进的方法和技术，如晶圆级节点、多维网络拓扑、分解式内存系统和并行化策略，已经被新兴的分布式训练系统积极采用。这导致了一个复杂的分布式训练软硬件协同设计堆栈，并需要建立一个模拟基础设施以进行设计空间探索。本文在开源的 ASTRA-sim 基础设施上扩展了功能，并赋予其模拟当代分布式训练模型和平台的能力。具体而言，(i) 我们通过基于图的训练循环实现了对任意模型并行化策略的支持，(ii) 我们实现了一个参数化的多维异构拓扑生成模块，(iii) 我们提供了分层内存抽象接口，支持任意分解内存系统，(iv) 我们启用了精细的仪器和事件跟踪。得到的建模基础设施，称为 ASTRA-sim2.0，易于使用且足够灵活，可用于建模各种分布式训练系统，包括利用新兴硬件加速器（如NVM设备和稀疏矩阵加速器）的系统。

    As deep learning models and input data are scaling at an unprecedented rate, it is inevitable to move towards distributed training platforms to fit the model and increase training throughput. State-of-the-art approaches and techniques, such as wafer-scale nodes, multi-dimensional network topologies, disaggregated memory systems, and parallelization strategies, have been actively adopted by emerging distributed training systems. This results in a complex SW/HW co-design stack of distributed training, necessitating a modeling/simulation infrastructure for design-space exploration. In this paper, we extend the open-source ASTRA-sim infrastructure and endow it with the capabilities to model state-of-the-art and emerging distributed training models and platforms. More specifically, (i) we enable ASTRA-sim to support arbitrary model parallelization strategies via a graph-based training-loop implementation, (ii) we implement a parameterizable multi-dimensional heterogeneous topology generatio
    
[^18]: 通过知识蒸馏进行低内存设备的混合硅片分类

    Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation. (arXiv:2303.13974v1 [cs.LG])

    [http://arxiv.org/abs/2303.13974](http://arxiv.org/abs/2303.13974)

    本文提出了一种通过知识蒸馏技术，将复杂预训练模型的知识转移到轻量级模型上，从而使低内存设备也能进行复杂缺陷分类，且无需大量标记数据。

    

    制造硅片是一个复杂的任务，涉及数千个步骤。硅片地图的缺陷模式识别对于确定生产缺陷的根本原因至关重要，这可能进一步为硅片工厂的产量提高提供见解。在制造过程中，各种缺陷可能单独出现在硅片中，也可能以不同的组合形式出现。识别硅片中的多个缺陷通常比识别单个缺陷更难。最近，深度学习方法在混合类型DPR方面获得了显着的进展。然而，这些缺陷的复杂性需要大型复杂模型，使它们很难在通常用于制造实验室的低内存嵌入式设备上运行。另一个常见问题是缺乏标记数据来训练复杂网络。在这项工作中，我们提出了一种无监督训练程序，将复杂预训练模型的知识蒸馏到轻量级可部署模型中。我们凭经验证明，这种方法导致分类模型具有与最先进模型相当的性能，同时也足够高效，可在低内存设备上运行。

    Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial for determining the root cause of production defects, which may further provide insight for yield improvement in wafer foundry. During manufacturing, various defects may appear standalone in the wafer or may appear as different combinations. Identifying multiple defects in a wafer is generally harder compared to identifying a single defect. Recently, deep learning methods have gained significant traction in mixed-type DPR. However, the complexity of defects requires complex and large models making them very difficult to operate on low-memory embedded devices typically used in fabrication labs. Another common issue is the unavailability of labeled data to train complex networks. In this work, we propose an unsupervised training routine to distill the knowledge of complex pre-trained models to lightweight deployment-ready models. We empirically show that this 
    
[^19]: 揭示深度学习训练中的节能实践：绿色人工智能的初步步骤。

    Uncovering Energy-Efficient Practices in Deep Learning Training: Preliminary Steps Towards Green AI. (arXiv:2303.13972v1 [cs.LG])

    [http://arxiv.org/abs/2303.13972](http://arxiv.org/abs/2303.13972)

    本文旨在从可持续性的角度研究深度学习管道的训练阶段，并通过研究超参数调整策略和模型复杂性来降低能耗。研究发现，贝叶斯优化在超参数调整中明显优于其他策略。同时，对卷积神经网络的架构进行分析，并考虑了卷积、线性层和池化层的能量消耗。

    

    现代AI实践的目标都是相同的：更好的结果。在深度学习的背景下，“结果”通常指完成某个竞争性问题集时达到的准确性。我们采用绿色人工智能这一新兴领域的思想，将能源消耗作为一个同等重要的指标来考虑，并减少任何无关的任务或能量使用。我们从可持续性的角度研究了深度学习管道的训练阶段，通过研究超参数调整策略和模型复杂性这两个对整个管道能源消耗有巨大影响的因素。首先，我们研究了超参数调整期间的网格搜索，随机搜索和贝叶斯优化的有效性，并发现贝叶斯优化明显优于其他策略。此外，我们分析了卷积神经网络的架构，并考虑了三种主要层类型：卷积，线性层和池化层的能量消耗。

    Modern AI practices all strive towards the same goal: better results. In the context of deep learning, the term "results" often refers to the achieved accuracy on a competitive problem set. In this paper, we adopt an idea from the emerging field of Green AI to consider energy consumption as a metric of equal importance to accuracy and to reduce any irrelevant tasks or energy usage. We examine the training stage of the deep learning pipeline from a sustainability perspective, through the study of hyperparameter tuning strategies and the model complexity, two factors vastly impacting the overall pipeline's energy consumption. First, we investigate the effectiveness of grid search, random search and Bayesian optimisation during hyperparameter tuning, and we find that Bayesian optimisation significantly dominates the other strategies. Furthermore, we analyse the architecture of convolutional neural networks with the energy consumption of three prominent layer types: convolutional, linear a
    
[^20]: 离线模仿学习的最优输运方法

    Optimal Transport for Offline Imitation Learning. (arXiv:2303.13971v1 [cs.LG])

    [http://arxiv.org/abs/2303.13971](http://arxiv.org/abs/2303.13971)

    本文提出了一种名为“最优输运奖励标记（OTR）”的算法，可用于离线模仿学习中为无标记的轨迹分配奖励，通过使用最优输运计算数据集中未注释的轨迹和专家演示之间的最佳对齐，得出可解释为奖励的相似性度量。在D4RL基准测试中，OTR只使用一个演示时可以持续匹配性能。

    

    随着大型数据集的出现，离线强化学习（RL）是学习良好决策策略的有前途的框架，无需与真实环境进行交互。然而，在线下RL中需要数据集进行奖励注释，这在奖励工程困难或获得奖励注释需要大量劳动力时，会带来实际挑战。本文提出了一种名为“最优输运奖励标记（OTR）”的算法，它使用最优输运来计算数据集中未注释的轨迹和专家演示之间的最佳对齐，从而得出一种可以解释为奖励的相似性度量，该度量可以由离线RL算法用于学习策略。OTR易于实现且计算效率高。在D4RL基准测试中，我们展示了OTR只使用一个演示时可以持续匹配性能。

    With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the perfo
    
[^21]: 基于二层优化的梯度稀缺问题在图学习中的应用

    Gradient scarcity with Bilevel Optimization for Graph Learning. (arXiv:2303.13964v1 [cs.LG])

    [http://arxiv.org/abs/2303.13964](http://arxiv.org/abs/2303.13964)

    本研究研究了二层优化算法在图学习中存在的梯度稀缺问题，并通过提出潜在图学习的方法来解决这一问题。

    

    在半监督学习下，图学习中普遍存在的问题是梯度稀缺现象。即学习一部分节点的损失会导致未标记节点之间的边缘收到零梯度。本文对这种现象进行了精确的数学刻画，并证明了它也存在于二层优化中，其中问题的参数存在额外的依赖关系。我们探讨了解决这个问题的几种方法，并提出了通过使用图到图模型（G2G）进行潜在图学习的方法。

    A common issue in graph learning under the semi-supervised setting is referred to as gradient scarcity. That is, learning graphs by minimizing a loss on a subset of nodes causes edges between unlabelled nodes that are far from labelled ones to receive zero gradients. The phenomenon was first described when optimizing the graph and the weights of a Graph Neural Network (GCN) with a joint optimization algorithm. In this work, we give a precise mathematical characterization of this phenomenon, and prove that it also emerges in bilevel optimization, where additional dependency exists between the parameters of the problem. While for GCNs gradient scarcity occurs due to their finite receptive field, we show that it also occurs with the Laplacian regularization model, in the sense that gradients amplitude decreases exponentially with distance to labelled nodes. To alleviate this issue, we study several solutions: we propose to resort to latent graph learning using a Graph-to-Graph model (G2G)
    
[^22]: 分布式稀疏块编码的分解器

    Factorizers for Distributed Sparse Block Codes. (arXiv:2303.13957v1 [cs.CV])

    [http://arxiv.org/abs/2303.13957](http://arxiv.org/abs/2303.13957)

    本文提出了一种用于分解分布式稀疏块编码（SBC）的GSBC，该方法引入了基于阈值的非线性激活、条件随机采样和$\ell_\infty$基于相似性度量，并能够分析确定预期的解的质量，解决了由于感知不确定性和近似而放松的噪声SBC中符号表示的挑战。

    

    分布式稀疏块编码（SBC）利用固定宽度的向量对符号数据结构进行编码和操作，具有紧凑的表示形式。然而，一个主要的挑战是在不必搜寻所有可能的组合的情况下将这些数据结构拆分成其组成部分。当使用现代神经网络生成查询向量时，噪声SBC中的符号表示由于感知不确定性和近似而放松，这使得这种分解变得更加具有挑战性。为了解决这些挑战，我们首先提出了一种快速且高精度的方法来分解一种更灵活、因此更普遍的SBC形式，称为GSBC。我们的迭代因子引入了基于阈值的非线性激活、条件随机采样和$\ell_\infty$基于相似性度量。它的随机采样机制与叠加搜索相结合，可以分析确定预期的解的质量。

    Distributed sparse block codes (SBCs) exhibit compact representations for encoding and manipulating symbolic data structures using fixed-with vectors. One major challenge however is to disentangle, or factorize, such data structures into their constituent elements without having to search through all possible combinations. This factorization becomes more challenging when queried by noisy SBCs wherein symbol representations are relaxed due to perceptual uncertainty and approximations made when modern neural networks are used to generate the query vectors. To address these challenges, we first propose a fast and highly accurate method for factorizing a more flexible and hence generalized form of SBCs, dubbed GSBCs. Our iterative factorizer introduces a threshold-based nonlinear activation, a conditional random sampling, and an $\ell_\infty$-based similarity metric. Its random sampling mechanism in combination with the search in superposition allows to analytically determine the expected 
    
[^23]: 使用图神经网络重建粒子物理过程的拓扑结构

    Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])

    [http://arxiv.org/abs/2303.13937](http://arxiv.org/abs/2303.13937)

    Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。

    

    我们提出了一种新的方法，称为Topograph，它利用粒子物理衰变的本质和信息传递图神经网络的灵活性，重建了包括中介粒子在内的底层物理过程。Topograph不仅解决了观测到的末态对象的组合指派问题，将它们与它们原来的母粒子关联起来，而且直接预测了硬散射过程中中间粒子的性质及其后续衰变。与标准的组合方法或现代图神经网络方法相比，它的复杂度与重构对象的数量成线性关系。我们应用Topograph于全强子衰变模式下的顶夸克对产生问题，相对标准方法，我们的方法表现更优，与最先进的机器学习技术相当。

    We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
    
[^24]: 粒子平均场变分贝叶斯方法

    Particle Mean Field Variational Bayes. (arXiv:2303.13930v1 [stat.CO])

    [http://arxiv.org/abs/2303.13930](http://arxiv.org/abs/2303.13930)

    本论文提出了一种新的基于粒子的MFVB方法，有效扩展了其适用范围，可应用于贝叶斯逻辑回归、随机波动和深度神经网络。

    

    平均场变分贝叶斯（MFVB）方法是一种计算效率最高的贝叶斯推断技术之一，然而其使用仅限于具有共轭先验或需要解析计算的模型。本文提出了一种新的基于粒子的MFVB方法，大大扩展了MFVB方法的适用范围。我们通过利用Wasserstein梯度流与Langevin扩散动力学之间的联系构建了新方法的理论基础，并使用贝叶斯逻辑回归、随机波动和深度神经网络证明了这种方法的有效性。

    The Mean Field Variational Bayes (MFVB) method is one of the most computationally efficient techniques for Bayesian inference. However, its use has been restricted to models with conjugate priors or those that require analytical calculations. This paper proposes a novel particle-based MFVB approach that greatly expands the applicability of the MFVB method. We establish the theoretical basis of the new method by leveraging the connection between Wasserstein gradient flows and Langevin diffusion dynamics, and demonstrate the effectiveness of this approach using Bayesian logistic regression, stochastic volatility, and deep neural networks.
    
[^25]: 去除胎儿超声图像中的混淆信息。

    Removing confounding information from fetal ultrasound images. (arXiv:2303.13918v1 [cs.CV])

    [http://arxiv.org/abs/2303.13918](http://arxiv.org/abs/2303.13918)

    本文讨论了如何针对胎儿筛查超声扫描图像中的文本和卡尺去除混淆信息，以训练更加准确的超声诊断深度学习算法。

    

    医学图像中嵌入的文本或标记等混淆信息可能会严重影响诊断深度学习算法的训练。然而，为临床目的收集的数据通常拥有这样的标记。在皮肤学中，已知存在将图像中恶性病变过度展示的绘画或标尺等标记。本文中，我们发现了嵌入在国家数据库中包含胎儿筛查超声扫描图像中的文本和卡尺，这些图像与需要预测的标准平面相关。为了利用这些数据库中可用的大量数据，我们开发并验证了一系列最小化嵌入文本和卡尺对超声诊断深度学习算法影响的方法，以标准平面分类作为测试案例。

    Confounding information in the form of text or markings embedded in medical images can severely affect the training of diagnostic deep learning algorithms. However, data collected for clinical purposes often have such markings embedded in them. In dermatology, known examples include drawings or rulers that are overrepresented in images of malignant lesions. In this paper, we encounter text and calipers placed on the images found in national databases containing fetal screening ultrasound scans, which correlate with standard planes to be predicted. In order to utilize the vast amounts of data available in these databases, we develop and validate a series of methods for minimizing the confounding effects of embedded text and calipers on deep learning algorithms designed for ultrasound, using standard plane classification as a test case.
    
[^26]: 用卷积神经网络分类引力波数据流中的毛刺信号

    Convolutional Neural Networks for the classification of glitches in gravitational-wave data streams. (arXiv:2303.13917v1 [gr-qc])

    [http://arxiv.org/abs/2303.13917](http://arxiv.org/abs/2303.13917)

    本文使用卷积神经网络对引力波数据流中的毛刺信号进行分类，并探索使用自监督和Fine-tune预训练模型的方法。最佳模型F1分数达到97.18％，表明卷积神经网络用于分类引力波数据流中的毛刺信号具有潜在的应用价值。

    

    本文探索了使用卷积神经网络（包括现代的ConvNeXt网络系列）来分类来自Advanced LIGO探测器的数据中的短暂噪声信号（即毛刺信号）和引力波。首先，我们使用监督学习方法的模型，使用Gravity Spy数据集从头开始训练和Fine-tune预训练模型。其次，我们还探索了一种自监督的方法，使用自动生成的伪标签预训练模型。我们的发现与同一数据集的现有结果非常接近，最佳的监督（自监督）模型的F1分数达到了97.18％（94.15％）。我们进一步使用LIGO-Virgo O3运行的实际引力波信号测试了模型。虽然是使用前几次运行 （O1和O2）的数据进行训练，但模型表现良好，特别是使用了迁移学习。我们发现，迁移学习可以提高分数，而无需大量的注释数据。我们的工作展示了卷积神经网络在分类引力波数据流中的毛刺信号方面的潜力。

    We investigate the use of Convolutional Neural Networks (including the modern ConvNeXt network family) to classify transient noise signals (i.e.~glitches) and gravitational waves in data from the Advanced LIGO detectors. First, we use models with a supervised learning approach, both trained from scratch using the Gravity Spy dataset and employing transfer learning by fine-tuning pre-trained models in this dataset. Second, we also explore a self-supervised approach, pre-training models with automatically generated pseudo-labels. Our findings are very close to existing results for the same dataset, reaching values for the F1 score of 97.18% (94.15%) for the best supervised (self-supervised) model. We further test the models using actual gravitational-wave signals from LIGO-Virgo's O3 run. Although trained using data from previous runs (O1 and O2), the models show good performance, in particular when using transfer learning. We find that transfer learning improves the scores without the n
    
[^27]: 基于深度学习的房颤分类对噪声的影响评估：一个12导联心电图的基准测试

    Benchmarking the Impact of Noise on Deep Learning-based Classification of Atrial Fibrillation in 12-Lead ECG. (arXiv:2303.13915v1 [eess.SP])

    [http://arxiv.org/abs/2303.13915](http://arxiv.org/abs/2303.13915)

    本文评估了基于深度学习的房颤分类方法在12导联心电图噪声存在的情况下的准确性，结果显示该方法能够高效识别房颤并具有很大的潜力。

    

    心电图分析在各种临床应用中被广泛使用，深度学习模型用于分类任务目前是研究的重点。由于其数据驱动的特性，深度学习模型能够高效地处理信号噪声，但其对这些方法准确性的影响仍不清楚。因此，我们在一个公共可用数据集（PTBXL）的子集上进行了一个基于深度学习的方法的房颤检测精度在四种类型的噪声下的评估。我们使用由人工专家提供的关于噪声的元数据来为每个心电图分配信号质量。此外，我们为每个心电图计算了定量的信噪比。我们分析了深度学习模型的准确性与这两个度量标准的关系，并观察到该方法能够在某些信号被标记为多导噪声情况下稳健地识别房颤。我们的结果表明，深度学习方法在信号噪声存在的情况下具有很大的潜力。

    Electrocardiography analysis is widely used in various clinical applications and Deep Learning models for classification tasks are currently in the focus of research. Due to their data-driven character, they bear the potential to handle signal noise efficiently, but its influence on the accuracy of these methods is still unclear. Therefore, we benchmark the influence of four types of noise on the accuracy of a Deep Learning-based method for atrial fibrillation detection in 12-lead electrocardiograms. We use a subset of a publicly available dataset (PTBXL) and use the metadata provided by human experts regarding noise for assigning a signal quality to each electrocardiogram. Furthermore, we compute a quantitative signal-to-noise ratio for each electrocardiogram. We analyze the accuracy of the Deep Learning model with respect to both metrics and observe that the method can robustly identify atrial fibrillation, even in cases signals are labelled by human experts as being noisy on multipl
    
[^28]: Wave-U-Net鉴别器：快速轻量级的生成对抗网络语音合成判别器

    Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis. (arXiv:2303.13909v1 [cs.SD])

    [http://arxiv.org/abs/2303.13909](http://arxiv.org/abs/2303.13909)

    本文提出了一种Wave-U-Net鉴别器，它是一个单独但表达力强的鉴别器，可以以逐个样本的方式评估波形，同时提取多级特征，为生成器提供足够的信息，从而在不增加模型大小和计算时间的情况下提高语音合成质量。

    

    在语音合成中，生成对抗网络（GAN）被广泛用于提高语音质量，该网络训练生成器（语音合成器）和鉴别器，使它们在一个极小极大博弈中协同对抗。近期的神经语音编码器（例如HiFi-GAN）和端到端的文本到语音系统（例如VITS）通常使用多个鉴别器的集合来从多个角度仔细地检查波形。这种鉴别器可以使合成语音充分接近实际语音，但是随着鉴别器数量的增加，需要增加模型大小和计算时间。相反，本研究提出了一种Wave-U-Net鉴别器，这是一个单独的但具有Wave-U-Net架构的表达力强的鉴别器。这个鉴别器独特之处在于，它可以以与输入信号相同的分辨率以逐个样本的方式评估波形，同时通过编码器和解码器提取多级特征，并使用跳跃连接。这种架构为生成器提供了足够的信息，

    In speech synthesis, a generative adversarial network (GAN), training a generator (speech synthesizer) and a discriminator in a min-max game, is widely used to improve speech quality. An ensemble of discriminators is commonly used in recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS) systems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such discriminators allow synthesized speech to adequately approach real speech; however, they require an increase in the model size and computation time according to the increase in the number of discriminators. Alternatively, this study proposes a Wave-U-Net discriminator, which is a single but expressive discriminator with Wave-U-Net architecture. This discriminator is unique; it can assess a waveform in a sample-wise manner with the same resolution as the input signal, while extracting multilevel features via an encoder and decoder with skip connections. This architecture provides a generator with sufficie
    
[^29]: 过去的提醒: 带类比提示的增量学习

    Remind of the Past: Incremental Learning with Analogical Prompts. (arXiv:2303.13898v1 [cs.CV])

    [http://arxiv.org/abs/2303.13898](http://arxiv.org/abs/2303.13898)

    本文提出了一种新颖的增量学习方法，通过类比制作机制将新数据重新映射到旧类，并利用所学提示估计和对抗表示偏移，极大提高了增量学习性能。

    

    虽然无数据增量学习方法对存储友好，但在缺乏历史数据的情况下准确估计和对抗表示偏移是具有挑战性的。本文通过提出一种新颖的增量学习方法来解决这个棘手的问题，该方法受到人类类比能力的启发。具体来说，我们设计了一种类比制作机制，通过提示调整将新数据重新映射到旧类。它仅使用新类的样本模拟了旧模型上目标旧类的特征分布。所学习的提示进一步用于估计和对抗由于对历史样本进行微调而导致的表示偏移。所提出的方法在课程和领域增量学习设置下在四个增量学习基准测试中取得了新的最佳性能。它通过仅保存每个类别的特征原型不断优于数据重放方法。通过联合训练，它已经几乎达到了实证的上限。

    Although data-free incremental learning methods are memory-friendly, accurately estimating and counteracting representation shifts is challenging in the absence of historical data. This paper addresses this thorny problem by proposing a novel incremental learning method inspired by human analogy capabilities. Specifically, we design an analogy-making mechanism to remap the new data into the old class by prompt tuning. It mimics the feature distribution of the target old class on the old model using only samples of new classes. The learnt prompts are further used to estimate and counteract the representation shift caused by fine-tuning for the historical prototypes. The proposed method sets up new state-of-the-art performance on four incremental learning benchmarks under both the class and domain incremental learning settings. It consistently outperforms data-replay methods by only saving feature prototypes for each class. It has almost hit the empirical upper bound by joint training on
    
[^30]: 用于图像识别的多项式网络正则化

    Regularization of polynomial networks for image recognition. (arXiv:2303.13896v1 [cs.CV])

    [http://arxiv.org/abs/2303.13896](http://arxiv.org/abs/2303.13896)

    本论文介绍了一种新型的多项式网络结构及其正则化方案，能够在六项基准测试中达到 ResNet 的性能水平，并提出了 D-PolyNets 以进一步推动正则化方案和提高参数效率。

    

    深度神经网络在任务中表现出色，但仍然是黑盒子，例如难以理论分析。同时，多项式网络作为一种替代方法出现，具有良好的性能和可解释性，但仍未达到强大的深度神经网络水平。在这项工作中，我们旨在缩小这种性能差距。我们引入一类多项式网络，能够在六项基准测试中达到 ResNet 的性能水平。我们证明强正则化至关重要，并对确切的正则化方案进行了广泛研究，以达到相同的性能。为了进一步推动正则化方案，我们引入了 D-PolyNets，这些网络比以前提出的多项式网络有更高的扩展度。D-PolyNets 在实现类似其他多项式网络的性能的同时更具参数效率。我们期望我们的新模型能够带来一个开放的、正则化的多项式网络结构，并更好地解释神经网络的表现。

    Deep Neural Networks (DNNs) have obtained impressive performance across tasks, however they still remain as black boxes, e.g., hard to theoretically analyze. At the same time, Polynomial Networks (PNs) have emerged as an alternative method with a promising performance and improved interpretability but have yet to reach the performance of the powerful DNN baselines. In this work, we aim to close this performance gap. We introduce a class of PNs, which are able to reach the performance of ResNet across a range of six benchmarks. We demonstrate that strong regularization is critical and conduct an extensive study of the exact regularization schemes required to match performance. To further motivate the regularization schemes, we introduce D-PolyNets that achieve a higher-degree of expansion than previously proposed polynomial networks. D-PolyNets are more parameter-efficient while achieving a similar performance as other polynomial networks. We expect that our new models can lead to an un
    
[^31]: 神经网络中的因果归因学习：超越直接影响

    Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])

    [http://arxiv.org/abs/2303.13850](http://arxiv.org/abs/2303.13850)

    本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。

    

    近年来，捕捉和维护神经网络模型中的因果关系引起了越来越多的关注。本文研究了用因果方法估计和维护神经网络模型中的输入-输出属性。特别地，现有的研究仅假设输入变量独立（由于神经网络架构），因此仅研究直接影响。我们将神经网络视为结构性因果模型，并提出在输入特征中引入边缘以捕捉和维护直接和间接因果效应的简单而有效的方法，同时训练神经网络模型。我们还提出有效的近似策略来量化高维数据的因果归因。我们在合成和真实数据集上进行了各种实验，结果表明，所提出的方法学习了接近基本事实效果的直接和间接因果归因。

    There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
    
[^32]: 交通模拟中的可编辑驾驶角色：社交可控行为生成

    Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation. (arXiv:2303.13830v1 [cs.RO])

    [http://arxiv.org/abs/2303.13830](http://arxiv.org/abs/2303.13830)

    为了评估自动驾驶汽车在不同社交特征的交互交通场景下采取安全有效的机动方式，该论文提出了一种社交可控行为生成（SCBG）模型，通过学习真实驾驶数据实现了逼真而类人的轨迹生成，并允许用户指定轨迹的礼貌程度。

    

    交通模拟在评估和改进自动驾驶规划系统中起着至关重要的作用。在公共道路上部署自动驾驶汽车后，需要与具有不同社交偏好（例如，自私或彬彬有礼的人类驾驶员）的人类道路参与者进行交互。为了确保自动驾驶汽车在不同的交互交通场景中采取安全有效的机动方式，我们应该能够在模拟环境中评估自动驾驶汽车与带有不同社交特征的反应代理之间的差异。为此，我们提出了一种社交可控行为生成（SCBG）模型，它允许用户指定生成轨迹的礼貌程度，并通过从真实驾驶数据中学习来确保逼真和类人的轨迹生成。具体而言，我们定义了一种新颖的可微度量，用于量化驾驶行为的礼貌程度，并利用边际和条件行为预测模型。

    Traffic simulation plays a crucial role in evaluating and improving autonomous driving planning systems. After being deployed on public roads, autonomous vehicles need to interact with human road participants with different social preferences (e.g., selfish or courteous human drivers). To ensure that autonomous vehicles take safe and efficient maneuvers in different interactive traffic scenarios, we should be able to evaluate autonomous vehicles against reactive agents with different social characteristics in the simulation environment. We propose a socially-controllable behavior generation (SCBG) model for this purpose, which allows the users to specify the level of courtesy of the generated trajectory while ensuring realistic and human-like trajectory generation through learning from real-world driving data. Specifically, we define a novel and differentiable measure to quantify the level of courtesy of driving behavior, leveraging marginal and conditional behavior prediction models t
    
[^33]: 高效混合型晶圆缺陷图案识别使用紧凑型可形变卷积变换器

    Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers. (arXiv:2303.13827v1 [cs.CV])

    [http://arxiv.org/abs/2303.13827](http://arxiv.org/abs/2303.13827)

    本文提出了一种紧凑型可形变卷积变换器（DC Transformer），能够高效地识别单一和混合型晶圆缺陷，聚焦于全局特征，准确预测缺陷的数目和类型。

    

    晶圆制造是一项复杂的任务，涉及数千个步骤。晶圆缺陷图案识别（DPR）对于找到问题的根本原因并进一步提高晶圆铸造的产量至关重要。与单一类型DPR相比，混合类型DPR由于空间特征的多样性，缺陷的不确定性和存在数目等原因更加复杂。为了准确预测缺陷的数目和类型，我们提出了一种新颖的紧凑型可形变卷积变换器（DC Transformer）。具体来说，DC Transformer通过可学习的可形变内核和多头注意力，聚焦于晶圆图中存在的全局特征。所提出的方法简洁地模拟了晶圆图和缺陷之间的内部关系。在一个包含38种缺陷模式的真实数据集上评估了DC Transformer。实验结果表明，DC Transformer在识别单一以及混合型晶圆缺陷方面表现得异常优秀。

    Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial to find the root cause of the issue and further improving the yield in the wafer foundry. Mixed-type DPR is much more complicated compared to single-type DPR due to varied spatial features, the uncertainty of defects, and the number of defects present. To accurately predict the number of defects as well as the types of defects, we propose a novel compact deformable convolutional transformer (DC Transformer). Specifically, DC Transformer focuses on the global features present in the wafer map by virtue of learnable deformable kernels and multi-head attention to the global features. The proposed method succinctly models the internal relationship between the wafer maps and the defects. DC Transformer is evaluated on a real dataset containing 38 defect patterns. Experimental results show that DC Transformer performs exceptionally well in recognizing both single 
    
[^34]: 基于因子分解的生成对抗网络用于文本转图像合成

    Factor Decomposed Generative Adversarial Networks for Text-to-Image Synthesis. (arXiv:2303.13821v1 [cs.MM])

    [http://arxiv.org/abs/2303.13821](http://arxiv.org/abs/2303.13821)

    本文提出一种因子分解生成对抗网络（FDGAN）用于文本转图像合成，能够将句子嵌入和噪声向量分解为不同的因子，并通过加性规范层来对齐和融合文本-图像特征，实验结果表明FDGAN可以实现更好的性能，同时使用更少的参数。

    

    先前的文本转图像合成工作通常是将句子嵌入与噪声向量拼接在一起，而句子嵌入和噪声向量是控制生成的不同方面的两个不同的因子。简单地将它们拼接在一起会使潜在的因子纠缠在一起，阻碍生成模型。在本文中，我们尝试分解这两个因子，提出了一种因子分解生成对抗网络（FDGAN）。为了实现这一点，我们首先从噪声向量生成图像，然后在生成器和判别器的归一化层中应用句子嵌入。我们还设计了一个加性规范层来对齐和融合文本-图像特征。实验结果表明，在文本转图像合成中分解噪声和句子嵌入可以解开潜在的因子，并使生成模型更加高效。与基线相比，FDGAN可以实现更好的性能，同时使用更少的参数。

    Prior works about text-to-image synthesis typically concatenated the sentence embedding with the noise vector, while the sentence embedding and the noise vector are two different factors, which control the different aspects of the generation. Simply concatenating them will entangle the latent factors and encumber the generative model.  In this paper, we attempt to decompose these two factors and propose Factor Decomposed Generative Adversarial Networks~(FDGAN). To achieve this, we firstly generate images from the noise vector and then apply the sentence embedding in the normalization layer for both generator and discriminators. We also design an additive norm layer to align and fuse the text-image features. The experimental results show that decomposing the noise and the sentence embedding can disentangle latent factors in text-to-image synthesis, and make the generative model more efficient. Compared with the baseline, FDGAN can achieve better performance, while fewer parameters are u
    
[^35]: marl-jax：用于社会普适性的多智能体强化学习框架

    marl-jax: Multi-agent Reinforcement Leaning framework for Social Generalization. (arXiv:2303.13808v1 [cs.MA])

    [http://arxiv.org/abs/2303.13808](http://arxiv.org/abs/2303.13808)

    marl-jax是一个基于DeepMind的JAX生态系和RL生态系的多智能体RL软件包，可以训练和评估代理在多样性背景下的社会普适性，提供命令行界面，适用于合作与竞争游戏环境。

    

    最近，强化学习领域的进展促进了许多令人兴奋的应用。这些进展是由算法和工程方面的改进驱动的，导致RL代理的训练速度更快。我们提出了marl-jax，这是一个用于训练和评估代理的社会普适性的多智能体强化学习软件包。该包旨在训练多智能体环境中的一组代理，并评估其对多样化背景代理的泛化能力。它建立在DeepMind的JAX生态系统上，并利用由DeepMind开发的RL生态系统。我们的marl-jax框架能够在多个代理的合作和竞争、同时行动的环境中工作。该包提供了一个直观且用户友好的命令行界面，用于训练一组代理并评估其泛化能力。总之，marl-jax为研究人员提供了有价值的资源。

    Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents. We present marl-jax, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework marl-jax is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities. In conclusion, marl-jax provides a valuable resource for researchers
    
[^36]: UniTS: 一种带自监督表示学习的通用时间序列分析框架

    UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning. (arXiv:2303.13804v1 [cs.LG])

    [http://arxiv.org/abs/2303.13804](http://arxiv.org/abs/2303.13804)

    UniTS是一个带自监督表示学习的通用时间序列分析框架，能够解决部分标记和领域转移等实际问题，并在多个任务和设置中实现了优秀的性能表现。

    

    机器学习已经成为时间序列分析的强有力工具。现有方法通常针对不同分析任务进行定制，并面临着处理部分标记和领域转移等实际问题的挑战。为了实现通用分析并解决上述问题，我们开发了UniTS，这是一个新颖的框架，它集成了自监督表示学习（或预训练）。 UniTS的组件使用类似于sklearn的API进行设计，以允许灵活的扩展。我们演示了用户如何使用用户友好的GUI执行分析任务，并展示了UniTS在五个主流任务和两个实际设置中相较于传统特定任务方法没有自监督预训练的卓越性能。

    Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To achieve universal analysis and address the aforementioned problems, we develop UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings.
    
[^37]: 通过自监督协同训练实现开放域槽填充

    Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])

    [http://arxiv.org/abs/2303.13801](http://arxiv.org/abs/2303.13801)

    本文提出了一种名为SCot的自监督协同训练框架，通过使用BERT模型和伪标签，实现开放域槽填充任务的零样本学习，克服了传统监督学习方法需要大量手动标注数据的问题。

    

    槽填充是现代会话系统中的关键任务之一。现有大部分文献采用监督学习方法，需要每个新域的标记训练数据。零样本学习和弱监督方法等已表现出替代手动标注的前景，但是这些学习范例在性能方面明显逊于监督学习方法。为了最小化这种性能差距并展示开放域槽填充的可能性，我们提出了一种自监督协同训练框架，称为SCot，它不需要领域内手动标记训练示例并分为三个阶段进行。

    Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both
    
[^38]: 零-shot泛化奖励函数个性化任务导向对话系统

    Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function. (arXiv:2303.13797v1 [cs.CL])

    [http://arxiv.org/abs/2303.13797](http://arxiv.org/abs/2303.13797)

    本研究提出了一种个性化任务导向对话系统的新框架P-ToD，它使用零-shot泛化奖励函数进行无监督训练，并取得了优于现有技术的实验结果。

    

    任务导向的对话系统使用户能够使用自然语言完成任务。现有技术的系统无论用户如何，都会用相同的方式回应，但是自定义对话可能会提高采用率和更好的用户体验。构建个性化对话系统是一项重要但具有挑战性的任务，只有少数几项工作面对了这一挑战。大部分现有工作依赖于监督学习方法，并需要每个用户资料进行繁琐和昂贵的标记训练数据。此外，为每个用户档案收集和标记数据几乎是不可能的。在本文中，我们提出了一个新的框架，P-ToD，通过零-shot泛化奖励函数，个性化任务导向的对话系统，适应了广泛的用户资料，以无监督的方式进行。我们的实验结果表明，在基准数据集上，P-ToD在任务成功率和用户满意度方面显著优于现有技术。

    Task-oriented dialog systems enable users to accomplish tasks using natural language. State-of-the-art systems respond to users in the same way regardless of their personalities, although personalizing dialogues can lead to higher levels of adoption and better user experiences. Building personalized dialog systems is an important, yet challenging endeavor and only a handful of works took on the challenge. Most existing works rely on supervised learning approaches and require laborious and expensive labeled training data for each user profile. Additionally, collecting and labeling data for each user profile is virtually impossible. In this work, we propose a novel framework, P-ToD, to personalize task-oriented dialog systems capable of adapting to a wide range of user profiles in an unsupervised fashion using a zero-shot generalizable reward function. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three phases. Phase one performs task-specific training. Phase two kicks 
    
[^39]: 基于特征点裁剪的高效与准确共视区域定位算法(MKPC)：一个二阶段管线用于提高图像匹配性能。

    Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance. (arXiv:2303.13794v1 [cs.CV])

    [http://arxiv.org/abs/2303.13794](http://arxiv.org/abs/2303.13794)

    本文提出了一种特征点裁剪算法(MKPC)来定位、提取和裁剪包含重要信息的共视区域，并提出了一个通用的二阶段管线用于图像匹配，该算法具有高效和准确性，能够提高在户外环境下的图像匹配性能，且比最先进的算法更为优秀。

    

    图像匹配在计算机视觉领域是一个经典且基础的任务。本文假设共视区域外部的区域几乎不包含有用的信息，通过提出一个特征点裁剪算法(MKPC)，定位、提取和裁剪包含重要信息的共视区域，该算法具有高效和准确性。此外，我们基于MKPC提出了一个通用的二阶段管线用于图像匹配，它对于任何图像匹配模型或组合都适用。我们在此基础上，通过将SuperPoint + SuperGlue作为图像匹配模型，进行了实验，结果显示我们的方法在户外位姿估计方面提高了性能。此外，在公平的比较条件下，我们的方法超越了2022年图像匹配挑战赛基准，该基准代表目前最难的户外图像匹配任务。

    Image matching is a classic and fundamental task in computer vision. In this paper, under the hypothesis that the areas outside the co-visible regions carry little information, we propose a matching key-points crop (MKPC) algorithm. The MKPC locates, proposes and crops the critical regions, which are the co-visible areas with great efficiency and accuracy. Furthermore, building upon MKPC, we propose a general two-stage pipeline for image matching, which is compatible to any image matching models or combinations. We experimented with plugging SuperPoint + SuperGlue into the two-stage pipeline, whose results show that our method enhances the performance for outdoor pose estimations. What's more, in a fair comparative condition, our method outperforms the SOTA on Image Matching Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of image matching currently.
    
[^40]: 带有相关事件的预测竞赛。

    Forecasting Competitions with Correlated Events. (arXiv:2303.13793v1 [cs.LG])

    [http://arxiv.org/abs/2303.13793](http://arxiv.org/abs/2303.13793)

    此论文研究了带有相关事件的预测竞赛，并引入了一个块相关的概念。证明了在具有块相关性的分布下，基于follow-the-regularized-leader(FTRL)的竞赛机制仍然保留了它的$\epsilon$-最优保证。

    

    针对常见的赢家通吃机制中存在的激励问题，最近的预测竞赛研究从Witkowski等人[2022]开始。Frongillo等人[2021]提出了一种基于follow-the-regularized-leader(FTRL)的竞赛机制，这是一种在线学习框架。他们证明他们的机制仅使用$O(\log(n)/\epsilon^2)$个事件就能高概率选择一个$\epsilon$-最优的预测者。这些工作以及之前针对这个问题的所有工作都假设事件是独立的。我们开始研究带有相关事件的预测竞赛。为了量化相关性，我们引入了一个块相关的概念，它允许每个事件与最多$b$个事件强相关。我们证明，在具有这种相关性的分布下，FTRL机制仍然使用$O(b^2 \log(n)/\epsilon^2)$个事件保留了它的$\epsilon$-最优保证。我们的证明涉及到一种新的相关随机变量浓度界，这可能是重要的。

    Beginning with Witkowski et al. [2022], recent work on forecasting competitions has addressed incentive problems with the common winner-take-all mechanism. Frongillo et al. [2021] propose a competition mechanism based on follow-the-regularized-leader (FTRL), an online learning framework. They show that their mechanism selects an $\epsilon$-optimal forecaster with high probability using only $O(\log(n)/\epsilon^2)$ events. These works, together with all prior work on this problem thus far, assume that events are independent. We initiate the study of forecasting competitions for correlated events. To quantify correlation, we introduce a notion of block correlation, which allows each event to be strongly correlated with up to $b$ others. We show that under distributions with this correlation, the FTRL mechanism retains its $\epsilon$-optimal guarantee using $O(b^2 \log(n)/\epsilon^2)$ events. Our proof involves a novel concentration bound for correlated random variables which may be of br
    
[^41]: 通过病人准则级别的公平性约束实现公平的病人-试验匹配

    Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint. (arXiv:2303.13790v1 [cs.LG])

    [http://arxiv.org/abs/2303.13790](http://arxiv.org/abs/2303.13790)

    本文提出了一个公平的病人-试验匹配框架，通过生成病人准则级别的公平性约束，考虑了包含和排除标准的嵌入差异，有效地解决了病人-试验匹配中的不公平性问题。

    

    临床试验在新型治疗方法的开发中不可或缺，但由于招募和留存病人的难度，往往难以招募足够数量的参与者。为了解决这些挑战，已经创建了基于深度学习框架的病人-试验匹配方法。这些框架会计算病人和临床试验的相似度，考虑包含和排除标准之间的差异。近期的研究表明这些框架的性能优于早期的方法。然而，深度学习模型可能会导致病人-试验匹配中的公平性问题，当某些敏感人群在临床试验中被低估时，可能会导致数据不完整或不准确，从而对患者造成潜在危害。针对这个问题，本文提出了一个公平的病人-试验匹配框架，通过生成一个病人准则级别的公平性约束来解决不公平性问题。所提出的框架考虑了包含和排除标准的嵌入差异，基于嵌入差异制定了公平性约束。该框架的有效性在一个真实的临床试验数据集上得到了验证。

    Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and e
    
[^42]: GSplit: 通过分裂并行实现大规模图神经网络训练的扩展

    GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism. (arXiv:2303.13775v1 [cs.DC])

    [http://arxiv.org/abs/2303.13775](http://arxiv.org/abs/2303.13775)

    本文提出了一种新的并行小批量训练方法，即分裂并行，应用在图神经网络训练上，能有效缓解数据并行方法的性能瓶颈，同时在大规模图上的性能表现优越。

    

    在许多行业、科学和工程领域（如推荐系统、社交图分析、知识库、材料科学和生物学）中，拥有数十亿个边的大规模图形是普遍存在的。图神经网络（GNN）作为一种新兴的机器学习模型，由于在各种图分析任务中具有卓越的性能，因此越来越多地被采用来学习这些图形。在大型图形上训练通常采用小批量训练，并且数据并行是将小批量训练扩展到多个 GPU 的标准方法。本文认为，GNN 训练系统的几个基本性能瓶颈与数据并行方法的固有限制有关。我们提出了一种新的并行小批量训练范式- 分裂并行，并将其实现在一个名为gsplit的新系统中。实验表明，gsplit 的性能优于DGL、Quiver和PaGraph等现有的系统。

    Large-scale graphs with billions of edges are ubiquitous in many industries, science, and engineering fields such as recommendation systems, social graph analysis, knowledge base, material science, and biology. Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to learn on these graphs due to their superior performance in various graph analytics tasks. Mini-batch training is commonly adopted to train on large graphs, and data parallelism is the standard approach to scale mini-batch training to multiple GPUs. In this paper, we argue that several fundamental performance bottlenecks of GNN training systems have to do with inherent limitations of the data parallel approach. We then propose split parallelism, a novel parallel mini-batch training paradigm. We implement split parallelism in a novel system called gsplit and show that it outperforms state-of-the-art systems such as DGL, Quiver, and PaGraph.
    
[^43]: 基于图神经网络的纳米卫星任务调度方法：学习混合整数模型的洞见

    A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])

    [http://arxiv.org/abs/2303.13773](http://arxiv.org/abs/2303.13773)

    本研究提出基于GNN的纳米卫星任务调度方法，以更好地优化服务质量，解决ONTS问题的复杂性。

    

    本研究探讨如何利用图神经网络（GNN）更有效地调度纳米卫星任务。在离线纳米卫星任务调度（ONTS）问题中，目标是找到在轨道上执行任务的最佳安排，同时考虑服务质量（QoS）方面的考虑因素，如优先级，最小和最大激活事件，执行时间框架，周期和执行窗口，以及卫星电力资源和能量收集和管理的复杂性的约束。ONTS问题已经使用传统的数学公式和精确方法进行了处理，但是它们在问题的挑战性案例中的适用性有限。本研究考察了在这种情况下使用GNN的方法，该方法已经成功应用于许多优化问题，包括旅行商问题，调度问题和设施放置问题。在本文中，我们将ONTS问题的MILP实例完全表示成二分图网络结构来应用GNN。

    This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
    
[^44]: 未知嗅探器用于目标检测：不要对未知对象视而不见

    Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])

    [http://arxiv.org/abs/2303.13769](http://arxiv.org/abs/2303.13769)

    本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。

    

    近期提出的开放世界目标和开放集检测在寻找从未见过的物体并将其与已知类别区分开方面取得了突破。然而，他们对从已知类别向未知类别的知识传递的研究需要更深入，从而导致探测隐藏在背景中的未知物体的能力不足。本文中，我们提出了未知嗅探器(UnSniffer)来寻找未知和已知的目标。首先，引入广义物体置信度(GOC)分数，仅使用已知类别样本进行监督和避免在背景中不适当地压制未知物体。值得注意的是，从已知物体学习到的这种置信度分数可以推广到未知物体。此外，我们提出了负能量抑制损失来进一步限制背景中非物体样本。接下来，在推断过程中由于缺乏它们在训练中的语义信息，难以获得每个未知目标的最佳框。为了解决这个问题，

    The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
    
[^45]: 无需边缘但具有结构感知性：从GNN到MLP的原型引导知识蒸馏。

    Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])

    [http://arxiv.org/abs/2303.13763](http://arxiv.org/abs/2303.13763)

    本文提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。

    

    将高精度的图神经网络（GNN）在图任务中压缩成低延迟的多层感知器（MLP）已成为热门研究课题。以前的方法会将图的边缘处理成额外的输入给MLP，但这样的图结构对于各种场景可能无法获得。因此，我们提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。具体而言，我们分析了GNN教师中的图形结构信息，并通过原型在无边缘设置中从GNN到MLP进行了知识蒸馏。在流行的图形基准实验中的实验结果表明了所提出的PGKD方法的有效性和鲁棒性。

    Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
    
[^46]: 结构不平衡感知的图增强学习

    Structural Imbalance Aware Graph Augmentation Learning. (arXiv:2303.13757v1 [cs.LG])

    [http://arxiv.org/abs/2303.13757](http://arxiv.org/abs/2303.13757)

    本文提出了一种选择性图增强方法 (SAug)，通过识别图中的中心节点和尾节点，设计了一种选择性增强策略，可以显著提高图机器学习模型的性能，特别是在学习尾节点时。

    

    图机器学习 (GML) 在节点分类、链接预测、图分类等方面取得了巨大的进展。然而，现实中的图往往具有结构不平衡性，即只有少数几个中心节点具有更密集的局部结构和更高的影响力。这种不平衡可能会损害现有 GML 模型的鲁棒性，特别是在学习尾节点时。本文提出了一种选择性图增强方法 (SAug) 来解决这个问题。首先，设计了一种基于Pagerank的采样策略来识别图中的中心节点和尾节点。其次，提出了一种选择性增强策略，将中心节点的噪声邻居放弃在一侧，并在另一侧发现潜在的邻居和为尾节点生成伪邻居。它还可以减轻两种类型节点之间的结构不平衡。最后，在增强后的图上重新训练 GNN 模型。广泛的实验表明，SAug 可以显著提高现有模型的性能。

    Graph machine learning (GML) has made great progress in node classification, link prediction, graph classification and so on. However, graphs in reality are often structurally imbalanced, that is, only a few hub nodes have a denser local structure and higher influence. The imbalance may compromise the robustness of existing GML models, especially in learning tail nodes. This paper proposes a selective graph augmentation method (SAug) to solve this problem. Firstly, a Pagerank-based sampling strategy is designed to identify hub nodes and tail nodes in the graph. Secondly, a selective augmentation strategy is proposed, which drops the noisy neighbors of hub nodes on one side, and discovers the latent neighbors and generates pseudo neighbors for tail nodes on the other side. It can also alleviate the structural imbalance between two types of nodes. Finally, a GNN model will be retrained on the augmented graph. Extensive experiments demonstrate that SAug can significantly improve the backb
    
[^47]: Sparsifiner：学习稀疏的实例相关注意力用于高效的视觉Transformer

    Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers. (arXiv:2303.13755v1 [cs.CV])

    [http://arxiv.org/abs/2303.13755](http://arxiv.org/abs/2303.13755)

    本文分析了ViT算法的计算成本高的问题，并且通过学习实例相关的注意力模式，来提高计算效率

    

    视觉Transformer相比卷积神经网络在性能方面具有竞争优势，但往往伴随着高计算成本。为此，先前的方法通过限制一定数量的空间相邻令牌来探索不同的注意力模式，以加速ViT的多头自注意力（MHSA）操作。但是，这种结构化的注意力模式将令牌与其空间相关性的令牌之间的令牌 - 令牌连接限制在了一定范围内，这不考虑从完整的注意力掩码中学习的语义连接。在这项工作中，我们提出了一种新方法，通过设计一个轻量级的连接性预测模块来学习实例相关的注意力模式。直观的说，如果认为特征在空间或语义上是相关的，则两个标记具有高的连接得分。由于每个标记只与少量其他标记相关，因此二元化连接掩码通常是有效的 。

    Vision Transformers (ViT) have shown their competitive advantages performance-wise compared to convolutional neural networks (CNNs) though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose a novel approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module to estimate the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often ver
    
[^48]: 利用旧知识在医疗影像中持续学习新类别

    Leveraging Old Knowledge to Continually Learn New Classes in Medical Images. (arXiv:2303.13752v1 [cs.LG])

    [http://arxiv.org/abs/2303.13752](http://arxiv.org/abs/2303.13752)

    本研究提出了一个框架，利用动态架构和训练过程交替两个目标的方式，使得机器学习算法在学习新类别时不会灾难性地遗忘已经学习的内容。该研究在医学数据集上取得了优秀的性能表现。

    

    类别增量持续学习是开发人工智能系统的核心步骤，能够通过学习新概念而不遗忘先前学到的内容，从而不断适应环境变化。在医学领域，这尤其需要不断从新的输入数据中学习，以对扩展的疾病集进行分类。本文重点研究如何利用旧知识来学习新类别，同时避免灾难性的遗忘。我们提出了一个框架，包括两个主要组成部分：（1）一个具有扩展表示的动态架构，以保留先前学习的特征并容纳新特征； （2）一个交替两个目标的训练过程，以在维护模型对旧类别的性能的同时平衡学习新特征。多个医学数据集的实验结果表明，我们的解决方案能够在性能方面优于现有技术基线。

    Class-incremental continual learning is a core step towards developing artificial intelligence systems that can continuously adapt to changes in the environment by learning new concepts without forgetting those previously learned. This is especially needed in the medical domain where continually learning from new incoming data is required to classify an expanded set of diseases. In this work, we focus on how old knowledge can be leveraged to learn new classes without catastrophic forgetting. We propose a framework that comprises of two main components: (1) a dynamic architecture with expanding representations to preserve previously learned features and accommodate new features; and (2) a training procedure alternating between two objectives to balance the learning of new features while maintaining the model's performance on old classes. Experiment results on multiple medical datasets show that our solution is able to achieve superior performance over state-of-the-art baselines in terms
    
[^49]: LONGNN: 具有可学习正交标准基的谱图神经网络

    LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])

    [http://arxiv.org/abs/2303.13750](http://arxiv.org/abs/2303.13750)

    本研究提出了一种谱图神经网络LONGNN，它采用可学习正交标准基，并解决了固定多项式基和非归一化基础所带来的缺陷，经实验证明其在各种图数据集上具有优异的表现。

    

    近年来，大量的谱图神经网络（GNN）方法利用可学习系数的多项式基在许多节点级任务上实现了顶级性能。虽然已经探索了各种多项式基，但是每种方法都采用了固定的多项式基，可能不是给定图形的最佳选择。此外，我们确定了这些方法所谓的越界问题，并表明这在它们不太系统化的正则化策略和非归一化基础上有所根源。在本文中，我们首次尝试解决这两个问题。利用雅各比多项式，我们设计了一种新的具有可学习正交标准基的谱GNN，LON-GNN，并证明了正则化系数现在等效于正则化所学滤波函数的范数。我们在多样的图数据集上进行了广泛的实验，以评估LON-GNN的拟合和泛化能力，结果表明其优于几种最先进的方法。

    In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
    
[^50]: FixFit：使用参数压缩解决超定模型中的逆问题

    FixFit: using parameter-compression to solve the inverse problem in overdetermined models. (arXiv:2303.13746v1 [cs.LG])

    [http://arxiv.org/abs/2303.13746](http://arxiv.org/abs/2303.13746)

    FixFit是一种使用神经网络进行参数压缩的方法，可以解决复杂非线性模型中由于参数之间的相互作用导致的多参数集问题。

    

    所有科学领域都依赖于数学模型。使用复杂的非线性模型的一个基本问题是，基于数据的参数估计经常失败，因为模型参数之间的相互作用导致多个参数集同样适合数据。在这里，我们开发了一种解决这个问题的新方法FixFit，将给定数学模型的参数压缩为独特于模型输出的潜在表示。我们通过在模型参数和模型输出的数据对上训练带有瓶颈层的神经网络来获得此表示。瓶颈层节点对应于唯一的潜在参数，其维度表示模型的信息内容。训练后的神经网络可以在瓶颈层分裂成编码器来描述冗余信息和解码器来从测量中唯一推断潜在参数。我们在来自经典物理学和神经科学的两个应用案例中展示了FixFit。

    All fields of science depend on mathematical models. One of the fundamental problems with using complex nonlinear models is that data-driven parameter estimation often fails because interactions between model parameters lead to multiple parameter sets fitting the data equally well. Here, we develop a new method to address this problem, FixFit, which compresses a given mathematical model's parameters into a latent representation unique to model outputs. We acquire this representation by training a neural network with a bottleneck layer on data pairs of model parameters and model outputs. The bottleneck layer nodes correspond to the unique latent parameters, and their dimensionality indicates the information content of the model. The trained neural network can be split at the bottleneck layer into an encoder to characterize the redundancies and a decoder to uniquely infer latent parameters from measurements. We demonstrate FixFit in two use cases drawn from classical physics and neurosci
    
[^51]: EdgeTran：在移动边缘平台上共同设计Transformer以实现高效推理

    EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms. (arXiv:2303.13745v1 [cs.LG])

    [http://arxiv.org/abs/2303.13745](http://arxiv.org/abs/2303.13745)

    EdgeTran框架旨在设计适用于移动边缘设备且在准确性、延迟、能耗和波峰功耗方面表现优秀的Transformer模型。同时，作者提出了块稀疏训练算法GPTran以帮助优化模型的内存占用和推理效率。

    

    自动化设计高效Transformer模型近来广受行业和学术界关注。但大多数工作只关注某些度量标准，而搜索最佳性能的Transformer架构。此外，在低计算边缘平台上运行传统的复杂大型Transformer模型是一个难题。本文提出了一种名为ProTran的框架，通过对Transformer体系结构的设计空间和不同种类的边缘设备进行硬件性能度量来削减延迟、能耗和波峰功耗，从而获得在给定任务上具有高准确性的最佳性能模型。我们称共同优化准确性和硬件性能度量的框架为EdgeTran。最后，我们提出了GPTran，一种多阶段的块稀疏训练算法，以帮助优化模型的内存占用和推理效率。

    Automated design of efficient transformer models has recently attracted significant attention from industry and academia. However, most works only focus on certain metrics while searching for the best-performing transformer architecture. Furthermore, running traditional, complex, and large transformer models on low-compute edge platforms is a challenging problem. In this work, we propose a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices. We use this profiler in conjunction with the proposed co-design technique to obtain the best-performing models that have high accuracy on the given task and minimize latency, energy consumption, and peak power draw to enable edge deployment. We refer to our framework for co-optimizing accuracy and hardware performance measures as EdgeTran. It searches for the best transformer model and edge device pair. Finally, we propose GPTran, a multi-stage blo
    
[^52]: 基于GQM模型的机器学习数据集许可证调查

    An investigation of licensing of datasets for machine learning based on the GQM model. (arXiv:2303.13735v1 [cs.SE])

    [http://arxiv.org/abs/2303.13735](http://arxiv.org/abs/2303.13735)

    机器学习数据集的许可证是一个问题，公开数据集质量不佳，缺乏商业可用性。当前数据集缺少许可证，需要更科学和系统的方法来调查。

    

    数据集许可证目前在机器学习系统开发中是一个问题。而在机器学习系统开发中，最常用的是公开可用的数据集。然而，由于公开可用的数据集中的图像主要是从互联网上获取的，因此一些图像并不是商业可用的。此外，当使用数据集训练机器学习模型时，开发人员通常不关心数据集的许可证。总结而言，在这个阶段，机器学习系统的数据集许可证在所有方面都处于不完整的状态。我们调查了两个收集的数据集，发现大多数当前的数据集缺乏许可证，缺乏许可证使得无法确定数据集的商业可用性。因此，我们决定采取更科学和系统的方法来调查数据集的许可证和使用数据的机器学习系统的许可证。

    Dataset licensing is currently an issue in the development of machine learning systems. And in the development of machine learning systems, the most widely used are publicly available datasets. However, since the images in the publicly available dataset are mainly obtained from the Internet, some images are not commercially available. Furthermore, developers of machine learning systems do not often care about the license of the dataset when training machine learning models with it. In summary, the licensing of datasets for machine learning systems is in a state of incompleteness in all aspects at this stage.  Our investigation of two collection datasets revealed that most of the current datasets lacked licenses, and the lack of licenses made it impossible to determine the commercial availability of the datasets. Therefore, we decided to take a more scientific and systematic approach to investigate the licensing of datasets and the licensing of machine learning systems that use the data
    
[^53]: 注意力在视觉Transformer中是如何工作的？一次视觉分析尝试

    How Does Attention Work in Vision Transformers? A Visual Analytics Attempt. (arXiv:2303.13731v1 [cs.LG])

    [http://arxiv.org/abs/2303.13731](http://arxiv.org/abs/2303.13731)

    本文采用视觉分析方法回答了ViT中头的重要性、不同头对空间邻居的关注强度、以及每个头学习的注意力模式等问题。

    

    视觉Transformer（ViT）扩展了将Transformer模型从序列数据应用到图像的成功。该模型将图像分解为许多较小的patch，然后将它们排列成一个序列。接下来对序列应用多头自注意力机制以学习patch之间的关注。尽管已经有很多成功解释序列数据上Transformer的研究，但对ViT的解释却鲜有研究，许多问题依然没有得到明确的回答。例如，在众多的注意力头中，哪个更重要？不同的头对其空间邻居的特定patch进行的关注有多强？每个注意力头学习了哪些关注模式？本文通过一种视觉分析方法来回答这些问题。具体而言，我们首先通过引入多个基于剪枝的度量来确定ViT中哪些头更重要。接着，我们对每个注意力头内部的patch之间的注意力强度进行空间分布分析，同时分析了每个头学习的注意力模式。

    Vision transformer (ViT) expands the success of transformer models from sequential data to images. The model decomposes an image into many smaller patches and arranges them into a sequence. Multi-head self-attentions are then applied to the sequence to learn the attention between patches. Despite many successful interpretations of transformers on sequential data, little effort has been devoted to the interpretation of ViTs, and many questions remain unanswered. For example, among the numerous attention heads, which one is more important? How strong are individual patches attending to their spatial neighbors in different heads? What attention patterns have individual heads learned? In this work, we answer these questions through a visual analytics approach. Specifically, we first identify what heads are more important in ViTs by introducing multiple pruning-based metrics. Then, we profile the spatial distribution of attention strengths between patches inside individual heads, as well as
    
[^54]: 基于区块链的安全与隐私联邦学习综述：在资源受限计算中的理论与应用

    A Survey on Secure and Private Federated Learning Using Blockchain: Theory and Application in Resource-constrained Computing. (arXiv:2303.13727v1 [cs.CR])

    [http://arxiv.org/abs/2303.13727](http://arxiv.org/abs/2303.13727)

    基于区块链的安全与隐私联邦学习的综述与应用，旨在探讨如何防止隐私与安全威胁，并减少由资源不足引起的瓶颈问题。

    

    最近几年来，随着先进机器学习和人工智能的快速发展和新兴安全和隐私威胁的出现，联邦学习（FL）已广受欢迎。 FL使边缘设备的本地数据存储能够高效生成模型，而不需将敏感数据透露给任何实体。尽管这种范例在一定程度上缓解了用户敏感数据的隐私问题，但在不断增长的网络威胁和隐私侵犯技术下，FL过程的性能可能会受到威胁并达到瓶颈。为了促进FL过程的普及，区块链与FL环境的集成已经引起了学术界和工业界的广泛关注。区块链具有分散化、不可变性、共识和透明度等特性，有潜力防止安全和隐私威胁。但是，如果区块链机制需要昂贵的计算资源，那么资源受限的FL可能会成为瓶颈。

    Federated Learning (FL) has gained widespread popularity in recent years due to the fast booming of advanced machine learning and artificial intelligence along with emerging security and privacy threats. FL enables efficient model generation from local data storage of the edge devices without revealing the sensitive data to any entities. While this paradigm partly mitigates the privacy issues of users' sensitive data, the performance of the FL process can be threatened and reached a bottleneck due to the growing cyber threats and privacy violation techniques. To expedite the proliferation of FL process, the integration of blockchain for FL environments has drawn prolific attention from the people of academia and industry. Blockchain has the potential to prevent security and privacy threats with its decentralization, immutability, consensus, and transparency characteristic. However, if the blockchain mechanism requires costly computational resources, then the resource-constrained FL cli
    
[^55]: 深度VAE在潜空间中高保真图像合成技术

    High Fidelity Image Synthesis With Deep VAEs In Latent Space. (arXiv:2303.13714v1 [cs.CV])

    [http://arxiv.org/abs/2303.13714](http://arxiv.org/abs/2303.13714)

    该论文提出了一种使用深度VAE在潜空间中进行高保真地图像生成的方法，通过两个阶段的训练，在避免对大部分细节进行建模的基础上，重点学习图像的结构组成，并取得了较好的效果。

    

    我们提出了一种使用分层变分自编码器（VAE）在确定性自编码器的潜空间上进行训练的快速、逼真的高分辨率、多模态数据集图像生成方法。在这个两阶段的设置中，自编码器将图像压缩为其语义特征，然后用深度VAE对其进行建模。通过这种方法，VAE避免了对构成图像大部分代码长度的细节进行建模，而是专注于学习其结构组件。我们展示了我们的两阶段方法的有效性，在ImageNet-256数据集上实现了9.34的FID，这与BigGAN相当。我们的实现代码已经在线上公开。

    We present fast, realistic image generation on high-resolution, multimodal datasets using hierarchical variational autoencoders (VAEs) trained on a deterministic autoencoder's latent space. In this two-stage setup, the autoencoder compresses the image into its semantic features, which are then modeled with a deep VAE. With this method, the VAE avoids modeling the fine-grained details that constitute the majority of the image's code length, allowing it to focus on learning its structural components. We demonstrate the effectiveness of our two-stage approach, achieving a FID of 9.34 on the ImageNet-256 dataset which is comparable to BigGAN. We make our implementation available online.
    
[^56]: 端到端扩散潜在优化提高分类器引导能力。

    End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])

    [http://arxiv.org/abs/2303.13703](http://arxiv.org/abs/2303.13703)

    本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。

    

    分类器指导——利用图像分类器的梯度来引导扩散模型的生成——有潜力大幅扩展对图像生成和编辑的创造性控制。然而，目前分类器指导要么需要训练新的噪声感知模型以获得精确的梯度，要么使用一步去噪的近似最终生成物，并导致梯度不对齐和次优控制。我们强调了这种近似的缺点，并提出了一种新的指导方法：直接优化扩散潜变（DOODL），它通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，使用可逆扩散过程实现内存有效的反向传递。展示了更精确指导潜力的 DOODL 在不同形式的指导的计算和人类评估度量上优于一步分类器指导。

    Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
    
[^57]: OFA$^2$: 一种基于多目标的Once-for-All神经架构搜索

    OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search. (arXiv:2303.13683v1 [cs.NE])

    [http://arxiv.org/abs/2303.13683](http://arxiv.org/abs/2303.13683)

    OFA$^2$是一个基于多目标优化的神经架构搜索模型，它通过将搜索阶段构想为一个多目标优化问题，并使用已经训练好的神经网络，从而在多个权衡目标之间找到高效和多样化的子网络。

    

    Once-for-All（OFA）是一个神经架构搜索（NAS）框架，旨在通过分离训练和搜索阶段来解决为不同资源约束的设备搜索高效架构的问题。 Ofa神经网络的训练过程只需要进行一次，然后可以根据每个部署方案从此训练好的网络中提取多个子网络进行多次搜索。本文旨在通过将搜索阶段明确构想为多目标优化问题，进一步寻求效率。 然后使用任何多目标进化算法（例如NSGA-II和SMS-EMOA）在搜索阶段填充Pareto前沿，其中包含具有不同权衡的高效预训练神经结构。换句话说，神经网络只需训练一次，然后以多目标优化的形式执行子网搜索，并获得一组高效、预训练且多样化的子网络。

    Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed to address the problem of searching efficient architectures for devices with different resources constraints by decoupling the training and the searching stages. The computationally expensive process of training the OFA neural network is done only once, and then it is possible to perform multiple searches for subnetworks extracted from this trained network according to each deployment scenario. In this work we aim to give one step further in the search for efficiency by explicitly conceiving the search stage as a multi-objective optimization problem. A Pareto frontier is then populated with efficient, and already trained, neural architectures exhibiting distinct trade-offs among the conflicting objectives. This could be achieved by using any multi-objective evolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA. In other words, the neural network is trained once, the searching for subnetwo
    
[^58]: 基于稀疏高斯过程混合的聚类方法

    Clustering based on Mixtures of Sparse Gaussian Processes. (arXiv:2303.13665v1 [cs.LG])

    [http://arxiv.org/abs/2303.13665](http://arxiv.org/abs/2303.13665)

    本文提出了一种基于稀疏高斯过程混合的聚类方法，同时实现了降维，并且相比于现有方法更具优势。

    

    在许多机器学习应用中，创建高维数据集的低维表示是一个重要的组成部分。如何使用低维嵌入空间聚类数据仍然是机器学习中的一个具有挑战性的问题。本文将重点放在提出聚类和降维的联合表述上。当需要概率模型时，一种可能的解决方案是使用混合模型，在其中可以学习到聚类指示器和低维度空间。我们的算法是基于稀疏高斯过程混合的方法，称为稀疏高斯过程混合聚类（SGP-MIC）。与现有方法相比，我们方法的主要优点在于，概率模型的性质提供了比现有的确定性方法更多的优势，构造模型的非线性推广非常直接，同时应用稀疏模型和高效变分EM逼近有助于加速算法并降低计算成本。

    Creating low dimensional representations of a high dimensional data set is an important component in many machine learning applications. How to cluster data using their low dimensional embedded space is still a challenging problem in machine learning. In this article, we focus on proposing a joint formulation for both clustering and dimensionality reduction. When a probabilistic model is desired, one possible solution is to use the mixture models in which both cluster indicator and low dimensional space are learned. Our algorithm is based on a mixture of sparse Gaussian processes, which is called Sparse Gaussian Process Mixture Clustering (SGP-MIC). The main advantages to our approach over existing methods are that the probabilistic nature of this model provides more advantages over existing deterministic methods, it is straightforward to construct non-linear generalizations of the model, and applying a sparse model and an efficient variational EM approximation help to speed up the alg
    
[^59]: 分布式LQR中的策略评估

    Policy Evaluation in Distributional LQR. (arXiv:2303.13657v1 [math.OC])

    [http://arxiv.org/abs/2303.13657](http://arxiv.org/abs/2303.13657)

    本文提出了一种新的分布式LQR方法，通过提供一种有效逼近无限多个随机变量的有限一组时刻，解决了DRL中策略评估的挑战，其效果优于竞争方法。

    

    分布式强化学习增强了对环境中随机性影响的理解，通过让代理学习随机回报的分布，而不是像标准强化学习一样学习期望值。但是，在分布式强化学习中，策略评估通常依赖于对回报分布的表示，需要仔细设计。本文针对一类基于线性二次调节器（LQR）进行控制的DRL问题的这一挑战提出了一种新的分布式方法，称为“分布式LQR”。我们提供了随机回报分布的闭合表达式，适用于所有外部扰动的动力学。虽然所提出的精确回报分布包含无限多个随机变量，但我们表明这种分布可以通过有限的一组时刻有效逼近，并且我们证明这些逼近是准确且可计算的。最后，我们通过提供实证证据表明，在控制具有随机扰动的线性定常系统时，分布式LQR的效果优于竞争方法。

    Distributional reinforcement learning (DRL) enhances the understanding of the effects of the randomness in the environment by letting agents learn the distribution of a random return, rather than its expected value as in standard RL. At the same time, a main challenge in DRL is that policy evaluation in DRL typically relies on the representation of the return distribution, which needs to be carefully designed. In this paper, we address this challenge for a special class of DRL problems that rely on linear quadratic regulator (LQR) for control, advocating for a new distributional approach to LQR, which we call \emph{distributional LQR}. Specifically, we provide a closed-form expression of the distribution of the random return which, remarkably, is applicable to all exogenous disturbances on the dynamics, as long as they are independent and identically distributed (i.i.d.). While the proposed exact return distribution consists of infinitely many random variables, we show that this distri
    
[^60]: 建立人工神经电路用于领域通用认知：脑启发式系统级架构入门。

    Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture. (arXiv:2303.13651v1 [cs.NE])

    [http://arxiv.org/abs/2303.13651](http://arxiv.org/abs/2303.13651)

    本文提供了关于构建领域通用人工智能的思路和原则，通过研究生物神经网络和系统级分布网络通信、递归和短期拓扑变化，为建立人工神经网络提供宝贵指导。

    

    目前有一系列的努力致力于建立能够解决广泛认知任务且无需在各个问题空间和领域进行精细调整的通用神经网络模型，以构建领域通用人工智能。为了做到这一点，模型需要适当的先验及归纳偏见，训练得到的模型可以推广到寻找新问题空间的例子。我们提供了生物神经网络赋予其柔性认知功能的标志性概述，并讨论特定的系统层分布网络通信及递归的作用，此外还讨论了短期拓扑变化在高效局部计算方面的作用。在机器学习模型变得越来越复杂的时候，这些原则可能会对这个复杂且动态的领域提供有价值的指导。

    There is a concerted effort to build domain-general artificial intelligence in the form of universal neural network models with sufficient computational flexibility to solve a wide variety of cognitive tasks but without requiring fine-tuning on individual problem spaces and domains. To do this, models need appropriate priors and inductive biases, such that trained models can generalise to out-of-distribution examples and new problem sets. Here we provide an overview of the hallmarks endowing biological neural networks with the functionality needed for flexible cognition, in order to establish which features might also be important to achieve similar functionality in artificial systems. We specifically discuss the role of system-level distribution of network communication and recurrence, in addition to the role of short-term topological changes for efficient local computation. As machine learning models become more complex, these principles may provide valuable directions in an otherwis
    
[^61]: 驾驶员疲劳检测的对抗鲁棒性和特征影响分析

    Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection. (arXiv:2303.13649v1 [cs.LG])

    [http://arxiv.org/abs/2303.13649](http://arxiv.org/abs/2303.13649)

    本研究发现，对于驾驶员疲劳检测，最可靠的模型是XGB，最佳窗口时间为120至150秒之间。本研究使用SHAP方法选择了18个最具影响力的特征，并训练出新的较小模型以获得与最初模型相同的性能。尽管所有模型都容易受到对抗性攻击的影响，但对抗性训练使它们能够保持更高的对抗鲁棒性。

    

    疲劳驾驶是道路事故的主要原因之一，但司机们往往低估了疲劳对他们反应时间的影响。为了在发生任何障碍之前检测到疲劳，一个很有前途的策略是使用机器学习(Machine Learning, ML)来监测心率变异(Heart Rate Variability, HRV)信号。本研究提出了多个实验，采用不同的HRV时间窗口和ML模型，使用Shapley Additive Explanations (SHAP)进行特征影响分析，并进行了对抗鲁棒性分析，以评估它们在处理错误输入数据和受干扰的HRV信号时的可靠性。最可靠的模型是Extreme Gradient Boosting (XGB)，最佳时间窗口为120至150秒之间。此外，SHAP允许选择18个最具影响力的特征，并训练新的较小模型，其表现与最初的模型一样好。尽管所有模型都容易受到对抗性攻击的影响，但对抗性训练使它们能够保持更高的对抗鲁棒性。

    Drowsy driving is a major cause of road accidents, but drivers are dismissive of the impact that fatigue can have on their reaction times. To detect drowsiness before any impairment occurs, a promising strategy is using Machine Learning (ML) to monitor Heart Rate Variability (HRV) signals. This work presents multiple experiments with different HRV time windows and ML models, a feature impact analysis using Shapley Additive Explanations (SHAP), and an adversarial robustness analysis to assess their reliability when processing faulty input data and perturbed HRV signals. The most reliable model was Extreme Gradient Boosting (XGB) and the optimal time window had between 120 and 150 seconds. Furthermore, SHAP enabled the selection of the 18 most impactful features and the training of new smaller models that achieved a performance as good as the initial ones. Despite the susceptibility of all models to adversarial attacks, adversarial training enabled them to preserve significantly higher r
    
[^62]: 利用信号处理和机器学习高效直接推断心率变异性

    Efficient and Direct Inference of Heart Rate Variability using Both Signal Processing and Machine Learning. (arXiv:2303.13637v1 [cs.LG])

    [http://arxiv.org/abs/2303.13637](http://arxiv.org/abs/2303.13637)

    本论文使用信号处理和机器学习结合的方法，直接推断出心率变异性。在大数据集的评估中，该方法表现出比单独使用信号处理或机器学习更高的准确性。

    

    心率变异性(HRV)测量连续心跳时间的变化，是身心健康的主要指标。最近的研究表明，可以使用光电容积描记仪(PPG)传感器推断HRV。然而，许多先前的研究存在较高的误差，因为它们仅使用信号处理或机器学习(ML)，或者因为它们间接推断HRV，或者因为缺少大数据集。许多先前的研究也可能需要大的ML模型。低的准确率和大的模型大小限制了它们在小型嵌入式设备和未来可能在医疗保健中的应用。为了解决上述问题，我们首先收集了一个大的PPG信号和HRV的基本事实数据集。通过这个数据集，我们开发了结合信号处理和ML的HRV模型，直接推断HRV。评估结果表明，我们的方法的误差在3.5%到25.7%之间，并且优于单独使用信号处理或ML的方法。我们还探讨了不同的模型的大小和计算复杂度的平衡。

    Heart Rate Variability (HRV) measures the variation of the time between consecutive heartbeats and is a major indicator of physical and mental health. Recent research has demonstrated that photoplethysmography (PPG) sensors can be used to infer HRV. However, many prior studies had high errors because they only employed signal processing or machine learning (ML), or because they indirectly inferred HRV, or because there lacks large training datasets. Many prior studies may also require large ML models. The low accuracy and large model sizes limit their applications to small embedded devices and potential future use in healthcare. To address the above issues, we first collected a large dataset of PPG signals and HRV ground truth. With this dataset, we developed HRV models that combine signal processing and ML to directly infer HRV. Evaluation results show that our method had errors between 3.5% to 25.7% and outperformed signal-processing-only and ML-only methods. We also explored differe
    
[^63]: 基于高效传感器采样和学习模型的PPG心率估计

    PPG-based Heart Rate Estimation with Efficient Sensor Sampling and Learning Models. (arXiv:2303.13636v1 [cs.LG])

    [http://arxiv.org/abs/2303.13636](http://arxiv.org/abs/2303.13636)

    本文研究了如何将PPG心率估计技术应用于低功耗和资源受限的嵌入式设备中，在结合信号处理和机器学习的基础上，成功将PPG采样频率降至仅25Hz，并提高了心率估计的精度，同时也减小了机器学习模型的特征大小，使得模型更小。

    

    最近的研究表明，在可穿戴设备中嵌入的PPG传感器可以高精度地估计心率。然而，尽管之前的研究努力，将基于PPG传感器的心率估计应用于嵌入式设备仍面临着高能耗的高频PPG采样和资源密集型的机器学习模型的挑战。为此，我们旨在探索更适合低功耗和资源受限的嵌入式设备的心率估计技术。具体来说，我们希望设计出可以提供高精度心率估计的技术，其采样频率低，模型尺寸小且推断时间快。首先，我们展示了通过结合信号处理和机器学习，可以将PPG采样频率从125 Hz降至仅25 Hz，同时提供更高的心率估计精度。这种组合还有助于减小机器学习模型的特征大小，导致模型更小。此外，我们还呈现了一个全面的

    Recent studies showed that Photoplethysmography (PPG) sensors embedded in wearable devices can estimate heart rate (HR) with high accuracy. However, despite of prior research efforts, applying PPG sensor based HR estimation to embedded devices still faces challenges due to the energy-intensive high-frequency PPG sampling and the resource-intensive machine-learning models. In this work, we aim to explore HR estimation techniques that are more suitable for lower-power and resource-constrained embedded devices. More specifically, we seek to design techniques that could provide high-accuracy HR estimation with low-frequency PPG sampling, small model size, and fast inference time. First, we show that by combining signal processing and ML, it is possible to reduce the PPG sampling frequency from 125 Hz to only 25 Hz while providing higher HR estimation accuracy. This combination also helps to reduce the ML model feature size, leading to smaller models. Additionally, we present a comprehensiv
    
[^64]: 提高效率的低秩优化：使紧凑结构和快速训练达到平衡的深度学习研究

    Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training. (arXiv:2303.13635v1 [cs.LG])

    [http://arxiv.org/abs/2303.13635](http://arxiv.org/abs/2303.13635)

    本文着重研究了低秩优化为基础的深度学习技术，可通过低秩逼近压缩深度神经网络，既减少存储要求又可实现高效快速的训练。通过综合技术，可将以上方法集成到LRI-Net框架中，以共同降低存储和计算成本，且不影响模型准确性。

    

    深度神经网络在许多数据处理应用中取得了巨大的成功。然而，高计算复杂度和存储成本使得深度学习难以应用于资源受限的设备上，也不环保，需要更多的能耗。本文聚焦于提高效率的低秩优化技术，可以通过网络参数的低秩逼近来压缩空间域，减少网络参数数量，从而减少存储要求，同时也可以在时间域内通过在少数子空间中训练网络参数，实现高效的快速收敛训练。本文总结了空间域中的三种模型压缩方法：预训练、预设置和压缩感知方法。通过对可整合技术进行综合讨论，例如稀疏剪枝、量化和熵编码，我们可以将它们集成到一个名为低秩集成网络（LRI-Net）的集成框架中，以共同降低存储和计算成本。基准数据集上的实验结果表明，LRI-Net可以在不牺牲准确性的前提下显著减小模型大小和计算量，使得低功耗设备和大规模神经网络模型受益。

    Deep neural networks have achieved great success in many data processing applications. However, the high computational complexity and storage cost makes deep learning hard to be used on resource-constrained devices, and it is not environmental-friendly with much power cost. In this paper, we focus on low-rank optimization for efficient deep learning techniques. In the space domain, deep neural networks are compressed by low rank approximation of the network parameters, which directly reduces the storage requirement with a smaller number of network parameters. In the time domain, the network parameters can be trained in a few subspaces, which enables efficient training for fast convergence. The model compression in the spatial domain is summarized into three categories as pre-train, pre-set, and compression-aware methods, respectively. With a series of integrable techniques discussed, such as sparse pruning, quantization, and entropy coding, we can ensemble them in an integration framew
    
[^65]: 物理学指导的PointNet：它能同时解决多少不规则几何体的反问题？以线弹性为例。

    Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity. (arXiv:2303.13634v1 [cs.LG])

    [http://arxiv.org/abs/2303.13634](http://arxiv.org/abs/2303.13634)

    本文提出了一种物理学指导的神经网络模型PIPN，它能利用稀疏标记数据同时预测所需偏微分方程在数百个不同的几何体上的解，有望在工业界进行快速的几何设计。

    

    常规的物理学指导的神经网络（PINN）利用稀疏标记数据预测偏微分方程的解，但只限于单一的域。相反，完全监督学习模型通常是首先在已知解（即标记数据）的几千个域上进行训练，然后预测在一些未知域上的解。物理学指导的PointNet（PIPN）主要旨在填补PINN（作为弱监督学习模型）和完全监督学习模型之间的差距。在本文中，我们展示了PIPN能够同时预测所需偏微分方程在数百个域上的解，而只使用稀疏标记数据。这个框架有助于在工业界进行快速的几何设计，尤其当只有稀疏标记数据可用时。特别地，我们展示了PIPN能够同时预测平面应力问题在500多个不同几何体上的解。

    Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pio
    
[^66]: 关于Botnet攻击中节点的集成分类的Associated Random Neural Networks（论文翻译）

    Associated Random Neural Networks for Collective Classification of Nodes in Botnet Attacks. (arXiv:2303.13627v1 [cs.NI])

    [http://arxiv.org/abs/2303.13627](http://arxiv.org/abs/2303.13627)

    本文介绍了一种基于Associated Random Neural Networks的Botnet攻击集成分类技术，它能够识别被攻陷的节点并在个别节点分类方法上进行更多有效的补充。

    

    Botnet攻击对网络系统构成了重大威胁，因为它们能够将它们攻陷的网络节点变成额外的攻击者，导致高容量攻击在长时间内蔓延。检测这种Botnet攻击的复杂性在于，多个网络IP地址将同时被攻陷。为此，除了已有的针对个别节点的传统方法外，对于 compromised nodes 的集体分类也很有用。因此，这项工作引入了一种集体Botnet攻击分类技术，它对一个n节点IP网络的流量进行操作，并使用一种新颖的Associated Random Neural Network（ARNN）来识别被攻陷的节点。ARNN是一种循环体架构，它包含两个相互关联、互相连接并且结构相同的n神经元随机神经网络，它们同时作为互相批判的评判者来确定哪些n个节点被攻击了。

    Botnet attacks are a major threat to networked systems because of their ability to turn the network nodes that they compromise into additional attackers, leading to the spread of high volume attacks over long periods. The detection of such Botnets is complicated by the fact that multiple network IP addresses will be simultaneously compromised, so that Collective Classification of compromised nodes, in addition to the already available traditional methods that focus on individual nodes, can be useful. Thus this work introduces a collective Botnet attack classification technique that operates on traffic from an n-node IP network with a novel Associated Random Neural Network (ARNN) that identifies the nodes which are compromised. The ARNN is a recurrent architecture that incorporates two mutually associated, interconnected and architecturally identical n-neuron random neural networks, that act simultneously as mutual critics to reach the decision regarding which of n nodes have been compr
    
[^67]: 基于人工智能的快速、无标记光学成像分子分类诊断的应用于弥漫性胶质瘤研究

    Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging. (arXiv:2303.13610v1 [cs.CV])

    [http://arxiv.org/abs/2303.13610](http://arxiv.org/abs/2303.13610)

    本文研究了一种基于人工智能的光学成像技术，可以快速、无标记的对弥漫性胶质瘤进行分子诊断，为其治疗提供更加准确的指导。

    

    分子分类的应用使得脑肿瘤的治疗得到转变，使诊断更加准确，治疗更加个性化。然而，对于患有脑肿瘤的病人，及时进行分子诊断测试仍然存在限制，使得手术和副辅助治疗更为复杂，阻撓了临床试验的报名。本文构建了DeepGlioma诊断筛选系统，该系统是一种基于人工智能的快速（<90秒）、无标记的光学成像诊断技术。DeepGlioma使用多模态数据集进行训练，包括刺激拉曼组织学（SRH）和大型、公共基因组数据。在153例进行SRH成像的弥漫性胶质瘤患者组成的前瞻性、多中心、国际范围的测试队列中，我们展示了DeepGlioma可以预测世界卫生组织用于定义成人型弥漫性胶质瘤分类的分子改变（IDH mut）。

    Molecular classification has transformed the management of brain tumors by enabling more accurate prognostication and personalized treatment. However, timely molecular diagnostic testing for patients with brain tumors is limited, complicating surgical and adjuvant treatment and obstructing clinical trial enrollment. In this study, we developed DeepGlioma, a rapid ($< 90$ seconds), artificial-intelligence-based diagnostic screening system to streamline the molecular diagnosis of diffuse gliomas. DeepGlioma is trained using a multimodal dataset that includes stimulated Raman histology (SRH); a rapid, label-free, non-consumptive, optical imaging method; and large-scale, public genomic data. In a prospective, multicenter, international testing cohort of patients with diffuse glioma ($n=153$) who underwent real-time SRH imaging, we demonstrate that DeepGlioma can predict the molecular alterations used by the World Health Organization to define the adult-type diffuse glioma taxonomy (IDH mut
    
[^68]: 带有延迟组合匿名赌徒反馈的随机次模赌博算法

    Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback. (arXiv:2303.13604v1 [cs.LG])

    [http://arxiv.org/abs/2303.13604](http://arxiv.org/abs/2303.13604)

    本论文研究了具有随机次模收益和全赌徒延迟反馈的组合多臂赌博机问题，研究了三种延迟反馈模型并导出了后悔上限。研究结果表明，算法能够在考虑延迟组合匿名反馈时胜过其他全赌徒方法。

    

    本文研究了组合多臂赌博机问题，其中包含了期望下的随机次模收益和全赌徒延迟反馈，延迟反馈被假定为组合和匿名。也就是说，延迟反馈是由过去行动的奖励组成的，这些奖励由子组件构成，其未知的分配方式。研究了三种延迟反馈模型：有界对抗模型、随机独立模型和随机条件独立模型，并针对每种延迟模型导出了后悔界。忽略问题相关参数，我们证明了所有延迟模型的后悔界为 $\tilde{O}(T^{2/3} + T^{1/3} \nu)$，其中 $T$ 是时间范围，$\nu$ 是三种情况下不同定义的延迟参数，因此展示了带有延迟的补偿项。所考虑的算法被证明能够胜过其他考虑了延迟组合匿名反馈的全赌徒方法。

    This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedbac
    
[^69]: 缺失数据的生存学习算法比较研究

    Une comparaison des algorithmes d'apprentissage pour la survie avec donn\'ees manquantes. (arXiv:2303.13590v1 [stat.ML])

    [http://arxiv.org/abs/2303.13590](http://arxiv.org/abs/2303.13590)

    本文研究了基于神经网络的生存任务学习算法在缺失数据处理方面的表现，结果显示对于所有情况没有单一的数据插补方法优于其他方法，提出的方法可用于比较其他缺失数据模式和(或)生存模型。

    

    生存分析是研究健康数据的重要工具。然而，缺失数据是这种数据的固有组成部分。近年来，研究人员提出了基于神经网络的新型生存任务学习算法。本研究对采用不同缺失数据处理方法的此类算法在模拟数据上的预测能力进行了研究，这些数据反映了现实情况，即个体属于未被观察到的群体。我们研究了不同的缺失数据模式，结果显示，没有单一的数据插补方法能够适用于所有情况而不需要进一步的特征工程。所提出的方法可用于比较其他缺失数据模式和(或)生存模型。Python代码通过survivalsim包可以访问。

    Survival analysis is an essential tool for the study of health data. An inherent component of such data is the presence of missing values. In recent years, researchers proposed new learning algorithms for survival tasks based on neural networks. Here, we studied the predictive performance of such algorithms coupled with different methods for handling missing values on simulated data that reflect a realistic situation, i.e., when individuals belong to unobserved clusters. We investigated different patterns of missing data. The results show that, without further feature engineering, no single imputation method is better than the others in all cases. The proposed methodology can be used to compare other missing data patterns and/or survival models. The Python code is accessible via the package survivalsim.  - L'analyse de survie est un outil essentiel pour l'\'etude des donn\'ees de sant\'e. Une composante inh\'erente \`a ces donn\'ees est la pr\'esence de valeurs manquantes. Ces derni\
    
[^70]: Scoring Functions 和 Generalization Prediction 的详细研究

    A Closer Look at Scoring Functions and Generalization Prediction. (arXiv:2303.13589v1 [cs.LG])

    [http://arxiv.org/abs/2303.13589](http://arxiv.org/abs/2303.13589)

    本文研究了广义误差预测器的有效性，探讨了置信度、局部流形平滑度和模型一致性评分函数的优缺点，发现在复杂机制缺失的情况下，最先进的评分无法在分布转移和损坏下超越简单的模型一致性。同时，在受损训练数据的情况下，模型一致性打分仍然表现良好，并且集成多样性有助于提高泛化性能。

    

    本文研究了广义误差预测器（GEPs）的效果，这些 GEPs 旨在通过从样本级分数中推导出数据集级误差估计值，从而预测模型在未见分布上的表现。然而，GEPs 常常利用不同的机制（例如，回归器、阈值函数、校准数据集等），来推导这种误差估计值，这会混淆特定评分函数的优点。因此，本文在机制选择独立的情况下，深入研究了流行的评分函数的有效性（置信度、局部流形平滑度、模型一致性）。我们发现，在复杂机制缺失的情况下，当估计分布转移和损坏下的误差时，最先进的置信度和平滑度基础评分无法超越简单的模型一致性。此外，在实际情况下，当训练数据受到损害时（例如标签噪声、测量噪声、欠采样），我们发现模型一致性打分仍然表现良好，并且集成多样性有助于提高泛化性能。

    Generalization error predictors (GEPs) aim to predict model performance on unseen distributions by deriving dataset-level error estimates from sample-level scores. However, GEPs often utilize disparate mechanisms (e.g., regressors, thresholding functions, calibration datasets, etc), to derive such error estimates, which can obfuscate the benefits of a particular scoring function. Therefore, in this work, we rigorously study the effectiveness of popular scoring functions (confidence, local manifold smoothness, model agreement), independent of mechanism choice. We find, absent complex mechanisms, that state-of-the-art confidence- and smoothness- based scores fail to outperform simple model-agreement scores when estimating error under distribution shifts and corruptions. Furthermore, on realistic settings where the training data has been compromised (e.g., label noise, measurement noise, undersampling), we find that model-agreement scores continue to perform well and that ensemble diversi
    
[^71]: 神经网络验证的高效符号推理技术

    Efficient Symbolic Reasoning for Neural-Network Verification. (arXiv:2303.13588v1 [cs.AI])

    [http://arxiv.org/abs/2303.13588](http://arxiv.org/abs/2303.13588)

    本文提出了一种高效的符号推理技术，用于解决神经网络的验证问题。该技术可以编码许多验证问题为二次程序，并将其松弛为半定程序以高效地解决。该框架可以验证各种神经网络属性，并为解决神经网络对对抗攻击的脆弱性问题提供了改进。

    

    神经网络已成为现代软件系统中不可或缺的一部分。然而，他们仍然存在许多问题，特别是对对抗攻击的脆弱性。在这项工作中，我们提出了一个新的神经网络验证程序推理框架，称为符号推理。我们框架的关键组成部分是符号域和二次关系的使用。符号域具有非常灵活的语义，而二次关系则非常表达能力强。它们允许我们将许多神经网络验证问题编码为二次程序。然后，我们的方案将二次程序松弛为半定程序，可以高效地解决。这个框架让我们能够在不同的场景下验证各种神经网络属性，特别是那些对非符号域具有挑战性的属性。此外，它还引入了新的表示和验证任务的观点。我们相信，我们的框架可以为神经网络验证带来重要的改进，特别是解决它们对对抗攻击的脆弱性。

    The neural network has become an integral part of modern software systems. However, they still suffer from various problems, in particular, vulnerability to adversarial attacks. In this work, we present a novel program reasoning framework for neural-network verification, which we refer to as symbolic reasoning. The key components of our framework are the use of the symbolic domain and the quadratic relation. The symbolic domain has very flexible semantics, and the quadratic relation is quite expressive. They allow us to encode many verification problems for neural networks as quadratic programs. Our scheme then relaxes the quadratic programs to semidefinite programs, which can be efficiently solved. This framework allows us to verify various neural-network properties under different scenarios, especially those that appear challenging for non-symbolic domains. Moreover, it introduces new representations and perspectives for the verification tasks. We believe that our framework can bring
    
[^72]: RNN 的回归：用可逆句嵌入的残差循环神经网络

    Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])

    [http://arxiv.org/abs/2303.13570](http://arxiv.org/abs/2303.13570)

    本研究提出了一种使用残差循环神经网络的新型模型，实现了可逆的句子嵌入。与其他神经机器翻译模型不同，该方法使用基于回归的输出层重建输入序列的单词向量，其具有高准确度和快速训练速度。这种方法适合各种自然语言处理应用，特别是对需要高质量句嵌入的神经网络系统的使用具有潜在优势。

    

    本研究提出了一种新型模型，使用残差循环神经网络在无监督编码任务上进行训练，以生成可逆的句子嵌入。相比于神经机器翻译模型中常见的概率输出，我们的方法采用基于回归的输出层来重建输入序列的单词向量。该模型在使用 ADAM 优化器进行快速训练的同时，取得了高准确度的结果。我们引入了残差连接和“match drop”技术，即只计算错误单词的梯度。我们的方法在各种自然语言处理应用中表现出潜在优势，特别是在需要高质量句嵌入的神经网络系统中。

    This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a "match drop" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.
    
[^73]: TinyML：工具，应用，挑战和未来研究方向

    TinyML: Tools, Applications, Challenges, and Future Research Directions. (arXiv:2303.13569v1 [cs.LG])

    [http://arxiv.org/abs/2303.13569](http://arxiv.org/abs/2303.13569)

    TinyML是一种嵌入式ML技术，可在多种廉价，资源有限和功率有限的设备上实现ML应用。但是，在其实现过程中，需要及时解决多种挑战，例如处理能力优化，提高可靠性以及维护学习模型的准确性。

    

    近年来，人工智能（AI）和机器学习（ML）在工业和学术界都受到了极大的关注。然而，传统的ML技术需要大量的计算资源才能满足所需的准确性，这限制了它们主要用于高性能设备（如网络节点）。然而，随着物联网和边缘计算等技术的不断发展，将ML技术纳入到资源受限的嵌入式设备上以实现分布式和普适性智能也变得十分有必要。这促使出现了TinyML范 paradigm，它是一种嵌入式ML技术，可在多种廉价，资源有限和功率有限的设备上实现ML应用。然而，在向TinyML技术进行适当实现的过程中，需要及时解决多个挑战，例如处理能力优化，提高可靠性以及维护学习模型的准确性。

    In recent years, Artificial Intelligence (AI) and Machine learning (ML) have gained significant interest from both, industry and academia. Notably, conventional ML techniques require enormous amounts of power to meet the desired accuracy, which has limited their use mainly to high-capability devices such as network nodes. However, with many advancements in technologies such as the Internet of Things (IoT) and edge computing, it is desirable to incorporate ML techniques into resource-constrained embedded devices for distributed and ubiquitous intelligence. This has motivated the emergence of the TinyML paradigm which is an embedded ML technique that enables ML applications on multiple cheap, resource- and power-constrained devices. However, during this transition towards appropriate implementation of the TinyML technology, multiple challenges such as processing capacity optimization, improved reliability, and maintenance of learning models' accuracy require timely solutions. In this art
    
[^74]: 利用图卷积神经网络提取租赁公寓平面图的房地产价值

    Extracting real estate values of rental apartment floor plans using graph convolutional networks. (arXiv:2303.13568v1 [cs.LG])

    [http://arxiv.org/abs/2303.13568](http://arxiv.org/abs/2303.13568)

    本文使用改进后的方法从大量平面图像中提取访问图，利用图卷积神经网络模型“平面图价值”估计访问图的房地产价值，大幅提高了租金估计的精度。该模型为全面估计平面图的价值提供了新思路。

    

    本文从大量位于日本大阪的家庭式租赁公寓的平面图像中提取反映房间流线邻接关系的几何图形，并使用改进后的访问图提取方法定义和实现访问图的图卷积网络（GCN）模型，提出了一种估计访问图的房地产价值的方法，即“平面图价值”。该模型包括平面图价值、使用其他一般性解释变量的享受方法，并用于估计租金，比较估计精度。此外，通过学习卷积网络，分析了解释租金的平面图的特征。因此，本文提出并验证了一种全面估计房地产平面图价值的新模型。结果表明，所提出的方法显著提高了租金估计的准确性。

    Access graphs that indicate adjacency relationships from the perspective of flow lines of rooms are extracted automatically from a large number of floor plan images of a family-oriented rental apartment complex in Osaka Prefecture, Japan, based on a recently proposed access graph extraction method with slight modifications. We define and implement a graph convolutional network (GCN) for access graphs and propose a model to estimate the real estate value of access graphs as the floor plan value. The model, which includes the floor plan value and hedonic method using other general explanatory variables, is used to estimate rents and their estimation accuracies are compared. In addition, the features of the floor plan that explain the rent are analyzed from the learned convolution network. Therefore, a new model for comprehensively estimating the value of real estate floor plans is proposed and validated. The results show that the proposed method significantly improves the accuracy of ren
    
[^75]: 使用胸部CT进行异构数据联邦学习

    Federated Learning on Heterogenous Data using Chest CT. (arXiv:2303.13567v1 [cs.LG])

    [http://arxiv.org/abs/2303.13567](http://arxiv.org/abs/2303.13567)

    本研究使用联邦学习技术在全球21家医院的10,000多位COVID患者的胸部CT扫描图像数据集上进行了研究，提出了三种联邦学习策略，并提出了结合合成生成数据的联邦学习策略，为医学AI在异构数据上的应用提供了新的解决方案。

    

    大规模数据推动了人工智能的快速发展。虽然众所周知，来自遗传、性别、种族、饮食和各种环境因素的人群差异对疾病贡献显著，但是，在医学中进行的AI研究主要集中在具有较少数据来源和较少样本差异的区域患者队列中。这种限制源于在医学中实现大规模数据共享的障碍以及对数据隐私的道德担忧。联邦学习（FL）是一种潜在的AI发展途径，可以在医院之间进行学习，而无需进行数据共享。在本研究中，我们展示了使用三种技术（联邦平均（FedAvg）、增量制度学习（IIL）和循环增量制度学习（CIIL））在涵盖5个大陆的21家参与医院中进行的联邦学习策略在COVID-19胸部CT数据集上的结果，该数据集是最大和最多样化的，包括>10,000位患者和>1,000,000张图像。我们还提出了一种利用合成生成数据和FL策略相结合的方法。

    Large data have accelerated advances in AI. While it is well known that population differences from genetics, sex, race, diet, and various environmental factors contribute significantly to disease, AI studies in medicine have largely focused on locoregional patient cohorts with less diverse data sources. Such limitation stems from barriers to large-scale data share in medicine and ethical concerns over data privacy. Federated learning (FL) is one potential pathway for AI development that enables learning across hospitals without data share. In this study, we show the results of various FL strategies on one of the largest and most diverse COVID-19 chest CT datasets: 21 participating hospitals across five continents that comprise >10,000 patients with >1 million images. We present three techniques: Fed Averaging (FedAvg), Incremental Institutional Learning (IIL), and Cyclical Incremental Institutional Learning (CIIL). We also propose an FL strategy that leverages synthetically generated 
    
[^76]: 使用逻辑知识增强生物医学数据的嵌入表示

    Enhancing Embedding Representations of Biomedical Data using Logic Knowledge. (arXiv:2303.13566v1 [cs.AI])

    [http://arxiv.org/abs/2303.13566](http://arxiv.org/abs/2303.13566)

    本文提出使用逻辑规则增强生物医学数据的嵌入表示，应用于最具挑战性的PharmKG数据集，并能够处理复杂的实验环境中关系事实之间的依赖性。

    

    知识图谱嵌入（KGE）已经成为一种流行的模型，专门设计用于处理本体论和图形结构数据，因为它们可以隐式地在潜在空间中编码实体和关系之间的统计依赖关系。 KGE技术在生物医学领域特别有效，因为在生物和化学对象之间存在复杂的相互作用，因此处理大型知识图谱非常常见。最近，PharmKG数据集被提出作为最具挑战性的知识图谱生物医学基准之一，其中涉及基因，疾病和化学物质之间数十万个关系事实。尽管 KGE 可以处理非常大的关系域，但它们通常无法表示关系事实之间更复杂的关系依赖性，例如逻辑规则，在复杂的实验环境中可能是基本的。在本文中，我们利用逻辑规则增强KGE在PharmKG数据集上的嵌入表示。

    Knowledge Graph Embeddings (KGE) have become a quite popular class of models specifically devised to deal with ontologies and graph structure data, as they can implicitly encode statistical dependencies between entities and relations in a latent space. KGE techniques are particularly effective for the biomedical domain, where it is quite common to deal with large knowledge graphs underlying complex interactions between biological and chemical objects. Recently in the literature, the PharmKG dataset has been proposed as one of the most challenging knowledge graph biomedical benchmark, with hundreds of thousands of relational facts between genes, diseases and chemicals. Despite KGEs can scale to very large relational domains, they generally fail at representing more complex relational dependencies between facts, like logic rules, which may be fundamental in complex experimental settings. In this paper, we exploit logic rules to enhance the embedding representations of KGEs on the PharmKG
    
[^77]: 图张量网络：在多个领域上设计大规模神经学习系统的直观框架

    Graph Tensor Networks: An Intuitive Framework for Designing Large-Scale Neural Learning Systems on Multiple Domains. (arXiv:2303.13565v1 [cs.LG])

    [http://arxiv.org/abs/2303.13565](http://arxiv.org/abs/2303.13565)

    张量数学已经被广泛用于现代深度学习，但在形式化设计和描述神经网络上，还未被充分利用。本文引入了图张量网络（GTN）框架，提供了一个直观而严谨的图形化框架，用于在规则和不规则领域上的大规模神经学习系统上系统地设计和实现，且通用性和灵活性很强。

    

    尽管张量及其运算在现代深度学习中普遍存在，但在深度学习社区内，使用张量数学来形式化设计和描述神经网络仍未被充分探索。因此，我们引入了图张量网络（GTN）框架，这是一个直观而严谨的图形化框架，用于系统地设计和实现在规则和不规则领域上的大规模神经学习系统。所提出的框架被证明足够通用，可以包括许多流行的体系结构作为特殊情况，并且足够灵活，可处理任何和许多数据域上的数据。该框架的强大和灵活性通过真实数据实验得到证明，通过张量代数的优点，结果在极低的复杂度成本下实现了改进的性能。

    Despite the omnipresence of tensors and tensor operations in modern deep learning, the use of tensor mathematics to formally design and describe neural networks is still under-explored within the deep learning community. To this end, we introduce the Graph Tensor Network (GTN) framework, an intuitive yet rigorous graphical framework for systematically designing and implementing large-scale neural learning systems on both regular and irregular domains. The proposed framework is shown to be general enough to include many popular architectures as special cases, and flexible enough to handle data on any and many data domains. The power and flexibility of the proposed framework is demonstrated through real-data experiments, resulting in improved performance at a drastically lower complexity costs, by virtue of tensor algebra.
    
[^78]: 跳跃连接在脉冲神经网络中的作用：对网络训练影响的分析。

    Skip Connections in Spiking Neural Networks: An Analysis of Their Effect on Network Training. (arXiv:2303.13563v1 [cs.NE])

    [http://arxiv.org/abs/2303.13563](http://arxiv.org/abs/2303.13563)

    本文研究了跳跃连接在脉冲神经网络中的作用，并提出了一种优化技术。通过优化跳跃连接的位置、类型和数量等超参数，可以显著提高SNN的准确性和效率，使其收敛更快、信息流更畅。

    

    由于脉冲神经网络（SNN）具有潜在的能量效率和模拟生物系统尖峰行为的能力，因此已引起人们的关注，成为传统人工神经网络（ANN）的有力替代。然而，SNN的训练仍然是一个具有挑战性的问题，需要新的技术来提高其性能。在本文中，我们研究了跳跃连接对SNN的影响，并提出了一种超参数优化技术，将ANN模型调整为SNN模型。我们证明了优化跳跃连接的位置、类型和数量可以显著提高SNN的准确性和效率，通过加快收敛速度和增加网络中的信息流。我们的结果表明，在CIFAR-10-DVS和DVS128手势数据集上，多种最先进模型的平均准确率提高了8%以上。

    Spiking neural networks (SNNs) have gained attention as a promising alternative to traditional artificial neural networks (ANNs) due to their potential for energy efficiency and their ability to model spiking behavior in biological systems. However, the training of SNNs is still a challenging problem, and new techniques are needed to improve their performance. In this paper, we study the impact of skip connections on SNNs and propose a hyperparameter optimization technique that adapts models from ANN to SNN. We demonstrate that optimizing the position, type, and number of skip connections can significantly improve the accuracy and efficiency of SNNs by enabling faster convergence and increasing information flow through the network. Our results show an average +8% accuracy increase on CIFAR-10-DVS and DVS128 Gesture datasets adaptation of multiple state-of-the-art models.
    
[^79]: 利用回声状态网络学习单向耦合

    Learning unidirectional coupling using echo-state network. (arXiv:2303.13562v1 [cs.LG])

    [http://arxiv.org/abs/2303.13562](http://arxiv.org/abs/2303.13562)

    本文介绍了利用回声状态网络（ESN）学习单向耦合的能力。仅使用系统的几个时间序列数据进行训练后，ESN可以预测不同驱动信号下响应系统的动态，该方法仅需要一些$A-B$型的时间序列训练数据就足以让ESN学习这种耦合方式。

    

    沉积计算已经在复杂动力学领域找到了许多潜在的应用。在这篇文章中，我们利用回声状态网络（ESN）模型的异常能力，仅使用系统的几个时间序列数据，就使其学习单向耦合模式。我们展示了，一旦该模型通过一些驱动响应系统的动态训练之后，即使我们将驱动系统$A$替换为其他系统$C$，回声状态网络也能仅仅用新的驱动系统$C$的动态预测响应系统$B$的动态，我们仅仅需要一些$A-B$型的时间序列训练数据就足以让ESN学习这种耦合方式。

    Reservoir Computing has found many potential applications in the field of complex dynamics. In this article, we exploit the exceptional capability of the echo-state network (ESN) model to make it learn a unidirectional coupling scheme from only a few time series data of the system. We show that, once trained with a few example dynamics of a drive-response system, the machine is able to predict the response system's dynamics for any driver signal with the same coupling. Only a few time series data of an $A-B$ type drive-response system in training is sufficient for the ESN to learn the coupling scheme. After training even if we replace drive system $A$ with a different system $C$, the ESN can reproduce the dynamics of response system $B$ using the dynamics of new drive system $C$ only.
    
[^80]: 基于扩散GAN的无监督语音识别的增强

    Enhancing Unsupervised Speech Recognition with Diffusion GANs. (arXiv:2303.13559v1 [cs.CL])

    [http://arxiv.org/abs/2303.13559](http://arxiv.org/abs/2303.13559)

    本论文提出了一种基于扩散GAN的增强方法，可用于无监督语音识别任务，实验结果表明该方法在多个数据集上都取得了良好的效果。

    

    我们通过扩散GAN增强了用于无监督自动语音识别(ASR)的普通对抗训练方法。我们的模型(1)注入了强度不同的实例噪声到生成器的输出和未标记的参考文本，这些文本是从带有长度约束的预训练音素语言模型中采样的，(2)请求扩散时间步骤相关的判别器将它们分开，(3)反向传播梯度以更新生成器。在Librispeech(测试干净和测试其他的误差率分别为3.1%和5.6%)、TIMIT和MLS数据集下，与wav2vec-U进行单词/音素错误率比较表明，我们的增强策略是有效的。

    We enhance the vanilla adversarial training method for unsupervised Automatic Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance noises of various intensities to the generator's output and unlabeled reference text which are sampled from pretrained phoneme language models with a length constraint, (2) asks diffusion timestep-dependent discriminators to separate them, and (3) back-propagates the gradients to update the generator. Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our enhancement strategies work effectively.
    
[^81]: 非线性平流-扩散-吸附系统的高效混合建模和吸附模型探索：系统科学机器学习方法

    Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach. (arXiv:2303.13555v1 [cs.CE])

    [http://arxiv.org/abs/2303.13555](http://arxiv.org/abs/2303.13555)

    本研究提出了一种机器学习方法，可用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型的吸附动力学定律结构。

    

    本研究提出了一种系统化的机器学习方法，用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型。它演示了使用基于梯度的优化器、伴随灵敏度分析和JIT编译的向量雅各布乘积，结合空间离散和自适应积分器来训练这些复杂系统的有效方法。稀疏和符号回归被用来识别人工神经网络中缺失的函数。该方法的鲁棒性在一个模拟数据集上得到了测试，该数据集包含固定床吸附的噪声突破曲线观测结果，得出了拟合良好的混合模型。该研究成功地利用稀疏和符号回归重建了吸附摄取动力学，并利用确定的多项式准确地预测了突破曲线，突显了该框架发现吸附动力学定律结构的潜力。

    This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradientbased optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.
    
[^82]: 基于分块数据存储的在线围棋系统CH-Go

    CH-Go: Online Go System Based on Chunk Data Storage. (arXiv:2303.13553v1 [cs.LG])

    [http://arxiv.org/abs/2303.13553](http://arxiv.org/abs/2303.13553)

    该论文提出了一种基于分块数据存储方法的在线围棋游戏系统 CH-Go，可以高效地处理海量数据，并使用神经网络来进行训练。

    

    构建一个有效的数据管理系统来处理海量数据，如初始围棋游戏记录，表示学习获得的特征数据集，自我对弈的经验数据集，随机采样的蒙特卡罗树等，是实现在线围棋系统的训练和运行所必需的。先前的工作很少提到这个问题，但数据管理系统的能力和效率决定了围棋系统的准确性和速度。为了解决这个问题，我们提出了一种基于分块数据存储方法的在线围棋游戏系统 (CH-Go)，它处理了Kiseido Go Server (KGS)发布的160k格式的围棋游戏数据，并设计了一个具有11个平面的围棋编码器、一个并行处理器和一个生成器，以实现更好的内存性能。具体而言，我们把数据存储在块中，以1024为批处理大小，并将每个块的特征和标签保存为二进制文件。然后每次随机抽取一小组数据进行神经网络训练。

    The training and running of an online Go system require the support of effective data management systems to deal with vast data, such as the initial Go game records, the feature data set obtained by representation learning, the experience data set of self-play, the randomly sampled Monte Carlo tree, and so on. Previous work has rarely mentioned this problem, but the ability and efficiency of data management systems determine the accuracy and speed of the Go system. To tackle this issue, we propose an online Go game system based on the chunk data storage method (CH-Go), which processes the format of 160k Go game data released by Kiseido Go Server (KGS) and designs a Go encoder with 11 planes, a parallel processor and generator for better memory performance. Specifically, we store the data in chunks, take the chunk size of 1024 as a batch, and save the features and labels of each chunk as binary files. Then a small set of data is randomly sampled each time for the neural network training
    
[^83]: 从低资源语言阿马齐语的图像中进行Tifinagh字符的光学字符识别和转录

    Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])

    [http://arxiv.org/abs/2303.13549](http://arxiv.org/abs/2303.13549)

    本文介绍了一种名为DaToBS的方法，用于从自然环境中的照片中检测和转录Tifinagh字符，以提高非洲低资源语言阿马齐语在教育、研究和网络应用等方面的支持。

    

    柏柏尔语是一种低资源的北非土语，在摩洛哥、阿尔及利亚等地的柏柏尔社区中使用自己独特的字母表Tifinagh。这种非洲亚细亚语言是由1400万人使用的，但缺乏足够的教育、研究、网络应用等方面的支持。我们提出了一种DaToBS的有监督方法，用于检测和转录Berber字符，旨在实现从自然环境的照片中自动识别和转录Tifinagh字符。

    The Berber, or Amazigh language family is a low-resource North African vernacular language spoken by the indigenous Berber ethnic group. It has its own unique alphabet called Tifinagh used across Berber communities in Morocco, Algeria, and others. The Afroasiatic language Berber is spoken by 14 million people, yet lacks adequate representation in education, research, web applications etc. For instance, there is no option of translation to or from Amazigh / Berber on Google Translate, which hosts over 100 languages today. Consequently, we do not find specialized educational apps, L2 (2nd language learner) acquisition, automated language translation, and remote-access facilities enabled in Berber. Motivated by this background, we propose a supervised approach called DaToBS for Detection and Transcription of Berber Signs. The DaToBS approach entails the automatic recognition and transcription of Tifinagh characters from signs in photographs of natural environments. This is achieved by sel
    
[^84]: 嘿，Dona！你能帮我处理学生选课吗？

    Hey Dona! Can you help me with student course registration?. (arXiv:2303.13548v1 [cs.HC])

    [http://arxiv.org/abs/2303.13548](http://arxiv.org/abs/2303.13548)

    本文演示了智能个人助手Dona，用于学生选课的自动化操作，采用语音输入、任务规划优化和语言翻译等技术，使学生不需要自己完成复杂的选课表格。

    

    本文演示了一种名为Hey Dona（或仅称为Dona）的智能个人助手，它具有虚拟语音助手，用于学生选课的自动化操作，这是一个旨在应用于教育领域的项目。Hey Dona 适用于各种口音，并能够进行任务规划优化和语言翻译，接受可以通过麦克风（蓝牙、有线麦克风）的语音输入，并按照用户命令执行查询处理，连接网络搜索回答，建立任务依赖关系，确保成功注册。Dona 避免了学生自己输入、点击和浏览复杂的选课表格的需求。

    In this paper, we present a demo of an intelligent personal agent called Hey Dona (or just Dona) with virtual voice assistance in student course registration. It is a deployed project in the theme of AI for education. In this digital age with a myriad of smart devices, users often delegate tasks to agents. While pointing and clicking supersedes the erstwhile command-typing, modern devices allow users to speak commands for agents to execute tasks, enhancing speed and convenience. In line with this progress, Dona is an intelligent agent catering to student needs by automated, voice-operated course registration, spanning a multitude of accents, entailing task planning optimization, with some language translation as needed. Dona accepts voice input by microphone (Bluetooth, wired microphone), converts human voice to computer understandable language, performs query processing as per user commands, connects with the Web to search for answers, models task dependencies, imbibes quality control
    
[^85]: 标记子图熵核

    Labeled Subgraph Entropy Kernel. (arXiv:2303.13543v1 [cs.LG])

    [http://arxiv.org/abs/2303.13543](http://arxiv.org/abs/2303.13543)

    本文提出了一种新的标记子图熵图核，它具有较好的结构相似性评估性能，采用了动态规划子图枚举算法有效减少了时间复杂度，同时提出了标记子图来丰富子结构拓扑，用全局图熵来描述网络，已应用于多个真实数据集，并能对不同任务产生良好效果。

    

    近年来，核方法在相似性测量任务中得到广泛应用，特别是图核在生物信息学、化学和金融数据分析等领域都被广泛使用。然而，现有方法，特别是基于熵的图核存在计算复杂度大和忽略节点级信息的问题。在本文中，我们提出了一种新的标记子图熵图核，在结构相似性评估中表现良好。我们设计了一种动态规划子图枚举算法，有效减少了时间复杂度。特别地，我们提出了标记子图，它用语义信息丰富了子结构拓扑。类比于统计力学中气团簇扩展的过程，我们重新推导了配分函数，并计算全局图熵来描述网络。为了测试我们的方法，我们应用了几个真实数据集，并评估了不同任务的效果。

    In recent years, kernel methods are widespread in tasks of similarity measuring. Specifically, graph kernels are widely used in fields of bioinformatics, chemistry and financial data analysis. However, existing methods, especially entropy based graph kernels are subject to large computational complexity and the negligence of node-level information. In this paper, we propose a novel labeled subgraph entropy graph kernel, which performs well in structural similarity assessment. We design a dynamic programming subgraph enumeration algorithm, which effectively reduces the time complexity. Specially, we propose labeled subgraph, which enriches substructure topology with semantic information. Analogizing the cluster expansion process of gas cluster in statistical mechanics, we re-derive the partition function and calculate the global graph entropy to characterize the network. In order to test our method, we apply several real-world datasets and assess the effects in different tasks. To captu
    
[^86]: 人工智能在可持续性方面的应用: 利用计算机视觉促进可持续智能产品-服务系统。

    Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision. (arXiv:2303.13540v1 [cs.LG])

    [http://arxiv.org/abs/2303.13540](http://arxiv.org/abs/2303.13540)

    本论文在可持续性方面的主要贡献是使用深度学习技术提高产品生产和使用的可持续性，通过计算机视觉技术检测产品的磨损状态并用于改进智能产品-服务系统的集成和结果取向。

    

    目前使用深度学习来实现清洁生产和可持续性目的的研究还非常有限。本论文展示了如何利用深度学习技术提高产品生产和使用的可持续性，尤其是采用深度学习的计算机视觉技术来识别产品的磨损状态，并将这些结果用于改进智能产品-服务系统的集成和结果取向。此外，这些成果预计将促进产品使用的改进和研发创新。我们在两种产品上演示了我们的方法:加工工具和旋转X射线阳极。

    The usage and impact of deep learning for cleaner production and sustainability purposes remain little explored. This work shows how deep learning can be harnessed to increase sustainability in production and product usage. Specifically, we utilize deep learning-based computer vision to determine the wear states of products. The resulting insights serve as a basis for novel product-service systems with improved integration and result orientation. Moreover, these insights are expected to facilitate product usage improvements and R&D innovations. We demonstrate our approach on two products: machining tools and rotating X-ray anodes. From a technical standpoint, we show that it is possible to recognize the wear state of these products using deep-learning-based computer vision. In particular, we detect wear through microscopic images of the two products. We utilize a U-Net for semantic segmentation to detect wear based on pixel granularity. The resulting mean dice coefficients of 0.631 and
    
[^87]: 连续空间随机博弈的分布式多智能体强化学习

    Decentralized Multi-Agent Reinforcement Learning for Continuous-Space Stochastic Games. (arXiv:2303.13539v1 [cs.LG])

    [http://arxiv.org/abs/2303.13539](http://arxiv.org/abs/2303.13539)

    本研究提出了一种分布式的多智能体强化学习算法，证明了其在连续空间随机博弈中的策略更新接近最优，同时推导了一般类算法的全局策略更新动态。

    

    随机博弈是研究多智能体强化学习的流行框架。最近，多智能体强化学习的进展主要集中在有限状态的游戏上。在本文中，我们研究带有通用状态空间和信息结构的随机博弈中的多智能体学习。在这种情况下，我们提出了一种分布式的多智能体强化学习算法，并证明了其策略更新的近最优性。此外，我们研究基于纳什均衡的一般类算法的全局策略更新动态，并推导了联合策略空间内收敛概率的闭式表征。

    Stochastic games are a popular framework for studying multi-agent reinforcement learning (MARL). Recent advances in MARL have focused primarily on games with finitely many states. In this work, we study multi-agent learning in stochastic games with general state spaces and an information structure in which agents do not observe each other's actions. In this context, we propose a decentralized MARL algorithm and we prove the near-optimality of its policy updates. Furthermore, we study the global policy-updating dynamics for a general class of best-reply based algorithms and derive a closed-form characterization of convergence probabilities over the joint policy space.
    
[^88]: 用于商用设备真实射频指纹的蓝牙和WiFi数据集

    Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices. (arXiv:2303.13538v1 [cs.NI])

    [http://arxiv.org/abs/2303.13538](http://arxiv.org/abs/2303.13538)

    该论文捕获了首个公开可访问的商用设备真实射频指纹数据集，这对于识别非法或未授权发射器具有重要意义。

    

    射频指纹作为一种物理层安全方案，可用于识别在共享RF频谱的非法或未授权发射器。然而，由于缺乏公开可访问的真实世界数据集，大多数研究都集中在使用软件定义无线电（SDR）生成合成波形，这不适用于实际部署环境。另一方面，现有的有限数据集仅关注生成一种波形的芯片组。商用现成（COTS）组合芯片组支持使用共享双频段天线的两种无线标准（例如WiFi和蓝牙），如在笔记本电脑、适配器、无线充电器、树莓派等IoT设备中广泛使用。因此，为跟上现代IoT环境的步伐，迫切需要捕获这些组合芯片组发射异构通信协议的真实世界开放数据集。为此，我们捕获了第一个已知的此类公开可访问数据集，其中包括来自各种商业设备的COTS Wi-Fi和蓝牙组合芯片组的RF发射。

    RF fingerprinting is emerging as a physical layer security scheme to identify illegitimate and/or unauthorized emitters sharing the RF spectrum. However, due to the lack of publicly accessible real-world datasets, most research focuses on generating synthetic waveforms with software-defined radios (SDRs) which are not suited for practical deployment settings. On other hand, the limited datasets that are available focus only on chipsets that generate only one kind of waveform. Commercial off-the-shelf (COTS) combo chipsets that support two wireless standards (for example WiFi and Bluetooth) over a shared dual-band antenna such as those found in laptops, adapters, wireless chargers, Raspberry Pis, among others are becoming ubiquitous in the IoT realm. Hence, to keep up with the modern IoT environment, there is a pressing need for real-world open datasets capturing emissions from these combo chipsets transmitting heterogeneous communication protocols. To this end, we capture the first kno
    
[^89]: 基于风险的群体结构健康监视理论：以层次系统中的人口为例

    Towards risk-informed PBSHM: Populations as hierarchical systems. (arXiv:2303.13533v1 [cs.AI])

    [http://arxiv.org/abs/2303.13533](http://arxiv.org/abs/2303.13533)

    PBSHM是一种新的结构健康监测方法，它通过对结构群体进行监测，将有价值的知识在结构实例之间传递，从而提高了决策的准确性和泛化能力。

    

    在结构健康监测（SHM）系统的发展中，有一种基于概率风险的决策框架已经被提出。然而，为了学习决策所需的统计模型，需要从感兴趣的结构获得测量数据。不幸的是，这些数据很少在必要的环境和操作条件下涵盖足够的范围，以确保模型的良好泛化。最近，出现了可以将SHM扩展到结构群体的技术，以便在足够相似的结构实例之间传递有价值的知识。这个新方法被称为基于群体的结构健康监测(PBSHM)。本文提出了人群结构的正式表达，以进行基于风险的决策的表示。

    The prospect of informed and optimal decision-making regarding the operation and maintenance (O&M) of structures provides impetus to the development of structural health monitoring (SHM) systems. A probabilistic risk-based framework for decision-making has already been proposed. However, in order to learn the statistical models necessary for decision-making, measured data from the structure of interest are required. Unfortunately, these data are seldom available across the range of environmental and operational conditions necessary to ensure good generalisation of the model.  Recently, technologies have been developed that overcome this challenge, by extending SHM to populations of structures, such that valuable knowledge may be transferred between instances of structures that are sufficiently similar. This new approach is termed population-based structural heath monitoring (PBSHM).  The current paper presents a formal representation of populations of structures, such that risk-based d
    
[^90]: 通过时间序列分解提高峰值网络流量预测能力

    Enhancing Peak Network Traffic Prediction via Time-Series Decomposition. (arXiv:2303.13529v1 [cs.NI])

    [http://arxiv.org/abs/2303.13529](http://arxiv.org/abs/2303.13529)

    本文通过将时间序列数据分解为趋势、季节性和噪声三个部分，并分别预测峰值流量，提高了峰值网络流量预测的能力。

    

    对于网络管理和维护，预测网络将接收到的高峰流量的时间是至关重要的，以便可以为服务器服务请求分配足够的资源。如果没有为服务器分配足够的资源，则它们可能会容易发生故障和安全漏洞。然而，由于其通常包含趋势、季节性和噪声等不同特征，因此常见的预测模型如自回归移动平均模型(ARIMA)只能预测时间序列数据的一般情况，无法准确预测时间序列中的峰值数据。本文提出了一种方法，将时间序列数据分解为趋势、季节性和噪声三个部分，并分别预测峰值流量。我们将这种方法与现有的流行预测模型进行比较，实验结果表明，我们的方法在预测峰值流量方面具有更好的优势。

    For network administration and maintenance, it is critical to anticipate when networks will receive peak volumes of traffic so that adequate resources can be allocated to service requests made to servers. In the event that sufficient resources are not allocated to servers, they can become prone to failure and security breaches. On the contrary, we would waste a lot of resources if we always allocate the maximum amount of resources. Therefore, anticipating peak volumes in network traffic becomes an important problem. However, popular forecasting models such as Autoregressive Integrated Moving Average (ARIMA) forecast time-series data generally, thus lack in predicting peak volumes in these time-series. More than often, a time-series is a combination of different features, which may include but are not limited to 1) Trend, the general movement of the traffic volume, 2) Seasonality, the patterns repeated over some time periods (e.g. daily and monthly), and 3) Noise, the random changes in 
    
[^91]: 云计算下的不确定性工作负载预测

    Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])

    [http://arxiv.org/abs/2303.13525](http://arxiv.org/abs/2303.13525)

    本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。

    

    在云计算中预测未来的资源需求对于管理云数据中心并保证客户最低服务质量（QoS）水平至关重要。建模未来需求的不确定性可以提高预测的质量并减少由于资源过度分配而带来的浪费。本文提出了单变量和双变量贝叶斯深度学习模型，用于预测未来资源需求的分布及其不确定性。我们设计了不同的训练情景来训练这些模型，其中每个过程是在多个数据集配置上进行预训练和微调步骤的不同组合。我们还将双变量模型与其单变量对应模型进行比较，用单个或多个数据集进行训练，以研究不同组成部分如何影响预测的准确性并影响QoS。最后，我们研究了我们的模型是否具有迁移学习能力。广泛的实验表明，使用多个数据集进行预训练可以提高性能。

    Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
    
[^92]: 神经预设：用于颜色风格转移的新技术

    Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])

    [http://arxiv.org/abs/2303.13511](http://arxiv.org/abs/2303.13511)

    本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。

    

    本文提出了一种名为神经预设的技术，用于解决现有颜色风格转移方法的限制，包括可视化伪影、大量内存需求和缓慢的风格切换速度。本方法基于两个核心设计。首先，我们提出了一种确定性神经颜色映射方法（DNCM），通过自适应的颜色映射矩阵在每个像素上进行一致的操作，避免了伪影，并支持具有小内存占用的高分辨率输入。其次，我们通过将任务分为颜色归一化和风格化两个阶段来开发一个两阶段流水线，可以通过将颜色风格作为预设提取，并在归一化的输入图像上重复使用它们来实现有效的风格切换。由于存在成对数据集的问题，我们描述了如何通过自监督策略训练神经预设模型。通过全面的评估展示了神经预设相对于现有方法的各种优势。此外，我们展示了我们训练的模型可以自然地支持多个风格，并在定量和定性性能上实现了最新的表现。

    In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
    
[^93]: 适应性正则化在类增量学习中的应用

    Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])

    [http://arxiv.org/abs/2303.13113](http://arxiv.org/abs/2303.13113)

    本文研究了适应性正则化在类增量学习中的应用，通过根据任务复杂度动态调整正则化强度，在学习新类别同时防止遗忘之前学习的类别。实验表明适应性正则化可以实现更加准确和不易遗忘的视觉增量学习。

    

    类增量学习是指在维持先前学习的分类准确度的同时，更新具有新类别的深度分类器。在学习新类别的同时，通过正则化神经网络权重来防止遗忘之前学习的类别是常见的方法。然而，现有的正则化方法在整个增量学习过程中使用恒定的强度，可能无法反映所遇到的任务难度的变化。因此，本研究探讨了适应性正则化在类增量学习中的必要性，该方法根据手头任务的复杂度动态调整正则化强度。我们提出了一种基于贝叶斯优化的方法，自动确定每个学习任务的最佳正则化强度。通过两个数据集上的两种正则化方法的实验，结果表明适应性正则化对于实现更加准确和不易遗忘的视觉增量学习非常重要。

    Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
    
[^94]: 通过信息度量的下界贝叶斯风险

    Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])

    [http://arxiv.org/abs/2303.12497](http://arxiv.org/abs/2303.12497)

    新提出一种方法计算贝叶斯风险下界，允许使用几乎任何信息度量，能提供与估计器无关的不可能结果。已应用于离散和连续参数问题，与最先进的技术进行了比较。

    

    本文关注参数估计，介绍了一种新的方法来计算贝叶斯风险下界。该方法允许使用几乎任何信息度量，包括Rényi的α，φ-分歧和Sibson的α-互信息。该 方法将分歧视为度量的函数，并利用度量空间和函数空间之间的对偶性。特别地，我们展示了通过马尔可夫不等式对其对偶进行上界限制，就可以用任何信息度量计算风险的下界。因此，由于分歧满足数据处理不等式，我们能够提供与估计器无关的不可能结果。然后将这些结果应用于涉及离散和连续参数的有趣问题，包括“捉迷藏”问题，并与最先进的技术进行比较。重要的观察是下界在样本数上的行为受到t的影响。

    This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \emph{any} information measure, including R\'enyi's $\alpha$, $\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t
    
[^95]: 在合成晶体数据集上训练的神经网络可以从ICSD粉末X射线衍射图中提取结构信息

    Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms. (arXiv:2303.11699v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2303.11699](http://arxiv.org/abs/2303.11699)

    本研究提出了一种使用合成晶体数据集，通过神经网络从ICSD粉末X射线衍射图中提取结构信息的方法，并且在空间群分类任务上，取得了比直接在ICSD晶体上进行训练更高的准确度。

    

    机器学习技术已成功地应用于从粉末X射线衍射图中提取结构信息，如晶体空间群。然而，直接在ICSD等数据库的模拟衍射图上进行训练存在挑战，原因是其规模有限、类别不均匀并且偏向某些结构类型。我们提出了一种替代方法，即通过利用每个空间群的对称操作，在随机坐标下生成合成晶体。基于这种方法，我们演示了使用深度ResNet类模型进行在线训练，每小时最多可生成少量百万个唯一的合成衍射图。针对我们选择的空间群分类任务，在大多数空间群的未见ICSD结构类型上，我们实现了79.9%的测试准确性。这超过了直接在ICSD晶体上进行训练的当前最先进方法的56.1%准确性。我们的结果证明合成晶体数据集可用于通过神经网络提取ICSD粉末X射线衍射图中的结构信息。

    Machine learning techniques have successfully been used to extract structural information such as the crystal space group from powder X-ray diffractograms. However, training directly on simulated diffractograms from databases such as the ICSD is challenging due to its limited size, class-inhomogeneity, and bias toward certain structure types. We propose an alternative approach of generating synthetic crystals with random coordinates by using the symmetry operations of each space group. Based on this approach, we demonstrate online training of deep ResNet-like models on up to a few million unique on-the-fly generated synthetic diffractograms per hour. For our chosen task of space group classification, we achieved a test accuracy of 79.9% on unseen ICSD structure types from most space groups. This surpasses the 56.1% accuracy of the current state-of-the-art approach of training on ICSD crystals directly. Our results demonstrate that synthetically generated crystals can be used to extract
    
[^96]: 带应用于变分蒙特卡罗模拟的参数化球的随机梯度下降的收敛性分析。

    Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])

    [http://arxiv.org/abs/2303.11602](http://arxiv.org/abs/2303.11602)

    本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。

    

    本文分析了在由神经网络参数化为常数倍的高维球上使用随机梯度下降（SGD）类型算法。我们为有监督学习提供了一种新算法，并在理论和数值上证明了其收敛性。我们还首次提供了无监督设置的收敛证明，该设置对应于量子物理学中广泛使用的变分蒙特卡罗（VMC）方法。

    We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
    
[^97]: 动态感知损失函数用于标签噪声学习

    Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])

    [http://arxiv.org/abs/2303.11562](http://arxiv.org/abs/2303.11562)

    本文提出一种动态感知损失函数，采用增强拟合能力，逐渐增加鲁棒性的权重来处理标签噪声。在后期阶段，引入自举项，让DNN更加重视容易的样例，证明了这种方法的优越性。

    

    标签噪声对深度神经网络(DNN)构成严重威胁。使用既能拟合又具有鲁棒性的强健损失函数是处理此问题的一种简单而有效的策略。然而，这两个因素之间的静态权衡与DNN学习标签噪声的动态性相矛盾，导致性能不佳。因此，我们提出动态感知损失(DAL)来解决这个问题。考虑到DNN倾向于先学习一般化的模式，然后逐渐过拟合标签噪声，DAL最初增强了拟合能力，然后逐渐增加了鲁棒性的权重。此外，在后期阶段，我们让DNN更加重视容易的样例，这些样例更容易标记为正确的标签，并引入自举项来进一步减少标签噪声的负面影响。详细的理论分析和广泛的实验结果都证明了我们方法的优越性。

    Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.
    
[^98]: ERSAM: 面向能源高效和实时社交氛围测量的神经架构搜索

    ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time Social Ambiance Measurement. (arXiv:2303.10727v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.10727](http://arxiv.org/abs/2303.10727)

    本文提出了一种面向能源高效性和实时性的社交氛围测量神经网络架构搜索框架ERSAM。该框架可以自动搜索适合SAM任务的神经网络架构，并满足能源效率、实时处理和有限标签数据的要求。在基准数据集上，该框架优于现有解决方案，具有更好的精度和效率。

    

    社交氛围描述了社交互动发生的背景，可以使用语音音频通过计算同时发言者的数量来测量。这种测量已经实现了各种心理健康跟踪和面向人类的物联网应用。虽然设备上的社交氛围测量 (SAM) 非常理想，以确保用户隐私并促进上述应用的广泛采用，但最先进的深度神经网络（DNNs）驱动的SAM解决方案所需的计算复杂度与移动设备上的常见资源相矛盾。此外，在临床设置下，由于各种隐私限制和所需的人力劳动，只有有限的标记数据可用或实际可行，这进一步挑战了设备上SAM解决方案的可实现准确性。为此，我们提出了一个专门的神经架构搜索框架，用于面向能源高效和实时SAM的ERSAM。具体而言，我们的ERSAM框架可以自动搜索适合SAM任务的神经网络架构，并满足能源效率、实时处理和有限标签数据的严格要求。我们在基准数据集上展示了我们提出的ERSAM框架的有效性，它优于最先进的SAM解决方案，并提高了精度和效率。

    Social ambiance describes the context in which social interactions happen, and can be measured using speech audio by counting the number of concurrent speakers. This measurement has enabled various mental health tracking and human-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is highly desirable to ensure user privacy and thus facilitate wide adoption of the aforementioned applications, the required computational complexity of state-of-the-art deep neural networks (DNNs) powered SAM solutions stands at odds with the often constrained resources on mobile devices. Furthermore, only limited labeled data is available or practical when it comes to SAM under clinical settings due to various privacy constraints and the required human effort, further challenging the achievable accuracy of on-device SAM solutions. To this end, we propose a dedicated neural architecture search framework for Energy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework can
    
[^99]: 多维数组的多切片聚类中的DBSCAN算法

    DBSCAN of Multi-Slice Clustering for three-order Tensor. (arXiv:2303.07768v1 [cs.LG])

    [http://arxiv.org/abs/2303.07768](http://arxiv.org/abs/2303.07768)

    本文提出了 MSC-DBSCAN扩展算法，可以在三元聚类中从数据中提取不同子空间的不同切片聚类，并可以获得与 MSC 算法在处理秩一张量数据时相同的解决方案。

    

    对于三维数据的三元聚类，现有的几种方法需要指定每个维度的聚类大小或聚类数量。为了解决这个问题，三元聚类(MSC)算法可以在低维子空间中找到保留信号的切片以便基于相似度阈值找到聚类。本文提出了 MSC-DBSCAN扩展算法以从数据中提取位于不同子空间的不同切片聚类(如果数据集是r个秩一张量(r>1)的总和)。我们的算法使用和 MSC 算法相同的输入，可以在处理秩一张量数据时与 MSC 算法获得相同的解决方案。

    Several methods for triclustering three-dimensional data require the cluster size or the number of clusters in each dimension to be specified. To address this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal slices that lie in a low dimensional subspace for a rank-one tensor dataset in order to find a cluster based on the threshold similarity. We propose an extension algorithm called MSC-DBSCAN to extract the different clusters of slices that lie in the different subspaces from the data if the dataset is a sum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC algorithm and can find the same solution for rank-one tensor data as MSC.
    
[^100]: 快速对抗训练中的灾难性过拟合的自我拟合视角研究

    Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective. (arXiv:2302.11963v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11963](http://arxiv.org/abs/2302.11963)

    本文首次将单步对抗示例分解为数据信息和自信息，揭示了一种有趣的现象，称为“自我拟合”，即网络学习单步扰动中嵌入的自信息，导致灾难性过拟合。

    

    虽然快速对抗训练提供了一种构建强健网络的高效方法，但它可能会面临一个严重问题，即灾难性过拟合（CO），其中多步强健准确率突然降至零。本文首次将单步对抗示例分解为数据信息和自信息，揭示了一种有趣的现象，称为“自我拟合”。

    Although fast adversarial training provides an efficient approach for building robust networks, it may suffer from a serious problem known as catastrophic overfitting (CO), where multi-step robust accuracy suddenly collapses to zero. In this paper, we for the first time decouple single-step adversarial examples into data-information and self-information, which reveals an interesting phenomenon called "self-fitting". Self-fitting, i.e., the network learns the self-information embedded in single-step perturbations, naturally leads to the occurrence of CO. When self-fitting occurs, the network experiences an obvious "channel differentiation" phenomenon that some convolution channels accounting for recognizing self-information become dominant, while others for data-information are suppressed. In this way, the network can only recognize images with sufficient self-information and loses generalization ability to other types of data. Based on self-fitting, we provide new insights into the exi
    
[^101]: 公平性干预的操作视角：何时和如何干预

    An Operational Perspective to Fairness Interventions: Where and How to Intervene. (arXiv:2302.01574v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01574](http://arxiv.org/abs/2302.01574)

    该论文提出了一个平衡预测性能、跨组差异、保护敏感群组属性和工程成本的全面框架，探讨了公平性干预的“何时”和“如何”，同时通过预测平等的案例研究，展示了一种新的方法。

    

    随着基于人工智能的决策系统的不断增加，它们的成功运营需要在预测性能、跨组差异、保护敏感群组属性（如种族）和工程成本之间取得平衡。我们提出了一个全面的框架来评估和将公平性干预与上述期望相结合。实践中需要考虑的两个关键点是干预的“何时”（预处理、处理过程中、后处理）和“如何”（敏感群组数据使用的方法）。我们通过对预测平等性的案例研究来展示我们的框架。在其中，我们首先提出了一种通过分布式鲁棒优化实现预测平等的新方法，而无需在推断时使用群组数据。然后，我们展示了这些方法在近400种不同变体的两个主要模型类型（XGBoost vs. Neural Net）、十个数据集和二十多种独特方法的基准研究中的有效性。

    As AI-based decision systems proliferate, their successful operationalization requires balancing multiple desiderata: predictive performance, disparity across groups, safeguarding sensitive group attributes (e.g., race), and engineering cost. We present a holistic framework for evaluating and contextualizing fairness interventions with respect to the above desiderata. The two key points of practical consideration are \emph{where} (pre-, in-, post-processing) and \emph{how} (in what way the sensitive group data is used) the intervention is introduced. We demonstrate our framework with a case study on predictive parity. In it, we first propose a novel method for achieving predictive parity fairness without using group data at inference time via distibutionally robust optimization. Then, we showcase the effectiveness of these methods in a benchmarking study of close to 400 variations across two major model types (XGBoost vs. Neural Net), ten datasets, and over twenty unique methodologies.
    
[^102]: 在在线连续学习中进行实时评估：一个新希望

    Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01047](http://arxiv.org/abs/2302.01047)

    该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。

    

    当前连续学习方法的评估通常假设在训练时间和计算方面没有限制。这对于任何实际世界的环境都是不现实的。因此我们提出了一种实时评估连续学习的方法，其中流不等待模型完成训练即揭示下一个数据进行预测。为了实现这一点，我们从计算成本的角度评估当前的CL方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了广泛的实验。结果表明，在这种评估下，一个简单的基线模型胜过了最先进的CL方法，这对现有方法在现实环境中的适用性提出了质疑。另外，我们还探讨了文献中常用的各种CL组件，包括记忆采样策略和正则化方法。我们发现，所有考虑的方法都无法与我们的简单基线模型竞争。

    Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
    
[^103]: 适用于所有领域的一个模型：基于协作域前缀调整的跨领域实体识别

    One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10410](http://arxiv.org/abs/2301.10410)

    本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。

    

    解决实际场景中低资源问题是跨领域实体识别的一个挑战性任务。先前典型的解决方案主要通过使用来自丰富资源领域的数据进行预训练语言模型(PLMs)获得NER模型并将其适应于目标领域。由于不同领域实体类型之间的不匹配问题，先前的方法通常调整所有PLMs的参数，从而为每个领域结束一个全新的NER模型。此外，当前的模型只关注于利用一个普通来源领域中的知识，而未能成功地将来自多个来源领域的知识转移到目标上。为了解决这些问题，我们基于文本到文本生成的PLM引入了协作域前缀调整跨领域NER(CP-NER)。具体来说，我们呈现了用于文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务而无需结构修改。我们利用冻结的PLMs并进行协作域前缀调整。

    Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
    
[^104]: 深度条件测度量化

    Deep Conditional Measure Quantization. (arXiv:2301.06907v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06907](http://arxiv.org/abs/2301.06907)

    提出了一种名为DCMQ的方法，该方法结合了基于Huber能量核的方法和深度神经网络架构，用于条件测度量化，并在多个实例中取得了有希望的结果。

    

    概率测度的量化意味着使用一组有限的狄拉克分布来近似表示输入分布（在一些概率测度度量空间中）。有各种各样的方法可以实现这一点，但可能会存在条件法的量化需要探索的情况。我们提出了一种称为DCMQ的方法，它涉及到基于Huber能量核的方法和深度神经网络体系结构的耦合。该方法在多个实例上进行了测试，并获得了有希望的结果。

    Quantization of a probability measure means representing it with a finite set of Dirac masses that approximates the input distribution well enough (in some metric space of probability measures). Various methods exists to do so, but the situation of quantizing a conditional law has been less explored. We propose a method, called DCMQ, involving a Huber-energy kernel-based approach coupled with a deep neural network architecture. The method is tested on several examples and obtains promising results.
    
[^105]: 使用二进制神经元激活模式检测样本的分布情况

    Detection of out-of-distribution samples using binary neuron activation patterns. (arXiv:2212.14268v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14268](http://arxiv.org/abs/2212.14268)

    本文提出了一种检测样本分布情况的新方法，该方法利用神经元激活模式，通过二进制表示提取卷积层的激活模式，具有更高的检测准确度。

    

    深度神经网络在各种应用中具有卓越的性能。尽管研究人员作出了大量的努力，但是样本的分布情况仍然是深度神经网络分类器的重要局限。在自动驾驶汽车、无人机和机器人等安全关键应用中，将以前未见过的输入识别为新颖是至关重要的。现有的检测方法将深度神经网络视为黑盒，并评估其输出预测的置信度得分。不幸的是，由于深度神经网络没有被训练来减少OOD输入的置信度，因此该方法经常失败。在本研究中，我们介绍了一种新的OOD检测方法。我们的方法受到ReLU-based架构中神经元激活模式（NAP）的理论分析的启发。由于从卷积层提取的激活模式的二进制表示，所以该方法没有引入高计算负担。在多个数据集上进行的广泛实证评估证实了我们的方法优于现有的最先进技术。

    Deep neural networks (DNN) have outstanding performance in various applications. Despite numerous efforts of the research community, out-of-distribution (OOD) samples remain a significant limitation of DNN classifiers. The ability to identify previously unseen inputs as novel is crucial in safety-critical applications such as self-driving cars, unmanned aerial vehicles, and robots. Existing approaches to detect OOD samples treat a DNN as a black box and evaluate the confidence score of the output predictions. Unfortunately, this method frequently fails, because DNNs are not trained to reduce their confidence for OOD inputs. In this work, we introduce a novel method for OOD detection. Our method is motivated by theoretical analysis of neuron activation patterns (NAP) in ReLU-based architectures. The proposed method does not introduce a high computational overhead due to the binary representation of the activation patterns extracted from convolutional layers. The extensive empirical eval
    
[^106]: 面向可扩展物理一致的神经网络：在数据驱动多区域热建筑模型中的应用研究

    Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12380](http://arxiv.org/abs/2212.12380)

    本论文研究了物理一致神经网络(PCNNs) 在模拟建筑温度动态方面的扩展性和准确性。结果发现，PCNNs既确保了物理一致性，同时又能在复杂的多区域热建筑模型中取得高精度的性能表现，且在可用数据量有限的情况下超越经典灰盒模型，具有可扩展性优势。

    

    随着越来越多的数据被收集，数据驱动建模方法近年来越来越受欢迎。虽然经典灰盒模型在物理上是可靠的，但通常很难识别和扩展，并且受其有限的表现力影响可能会影响其准确性。另一方面，常常依赖神经网络 (NNs) 的经典黑盒方法通常能够从数据中推导出统计模式，即使在扩展方面也能取得令人印象深刻的性能。然而，它们对潜在的物理定律完全无视，如果基于它们做决策用于实际物理系统，可能会导致潜在的灾难性后果。最近开发了物理一致神经网络 (PCNNs) 来解决这些问题，确保物理一致性，同时利用 NNs 实现最先进的准确性。在这项工作中，我们将 PCNN 扩展到建筑温度动态建模，并提出与经典灰盒和黑盒方法的彻底比较。特别是，我们研究多区域建筑模型，其中每个区域的热行为由能量平衡方程式统治，其参数必须通过测量数据进行识别。所得结果表明，即使涉及许多相互作用的组件构成的复杂和动态系统，PCNNs 也可以在确保物理一致性的同时实现高精度。此外，我们证明 PCNN 在经典灰盒模型上提供了可扩展性优势，在有限的可用训练数据下表现出色。

    With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
    
[^107]: POTATO: 便携式文本注释工具

    POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08620](http://arxiv.org/abs/2212.08620)

    POTATO是一个免费、开源的便携式文本注释工具，支持多种类型的文本和多模态数据的标注，提供易于配置的功能以最大化生产力，特别是对于长文档和复杂任务。

    

    我们介绍了POTATO，即便携式文本注释工具，这是一个完全免费、开源的注释系统，支持标注多种类型的文本和多模态数据，提供易于配置的功能以最大化部署和注释者的生产力（便捷的机器学习/自然语言处理任务模板、主动学习、按键缩写键、关键字高亮、提示工具），并支持高度定制化（可编辑的UI，插入预筛选问题，注意力和资格测试）。两项注释任务的实验表明，POTATO通过其特别设计的生产力功能，特别是长文档和复杂任务，提高了标注速度。POTATO可在 https://github.com/davidjurgens/potato 上获取，并将继续更新。

    We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated.
    
[^108]: FlexiViT：适用于所有补丁大小的模型

    FlexiViT: One Model for All Patch Sizes. (arXiv:2212.08013v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08013](http://arxiv.org/abs/2212.08013)

    本文介绍了一种名为FlexiViT的模型，它可以使用一组权重表现良好，并且适用于各种补丁大小，从而使其在部署时容易根据不同的计算预算来定制模型。

    

    Vision Transformer将图像切成补丁以将其转换为序列。这些补丁的大小控制速度/准确性权衡，补丁越小，精度越高，但计算成本也越高，但更改补丁大小通常需要重新训练模型。本文证明，简单随机化训练时的补丁大小会导致一组权重表现良好，可在各种补丁大小范围内使用，从而使其在部署时容易根据不同的计算预算来定制模型。我们在许多任务上对生成的模型进行了全面评估，包括分类，图像-文本检索，开放式检测，全景分割和语义分割，得出结论：它通常可以匹配，并且有时会优于以单个补丁大小训练的标准ViT模型。因此，在ViT中使用FlexiViT进行训练是一种简单易行的改进方法。

    Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to a
    
[^109]: 逆强化学习中的错误规范问题

    Misspecification in Inverse Reinforcement Learning. (arXiv:2212.03201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03201](http://arxiv.org/abs/2212.03201)

    本文研究了逆强化学习中的错误规范问题，探讨了不同模型与演示策略之间的偏差，以及在何种情况下模型会导致误导性推断。

    

    逆强化学习的目标是从策略π中推断出奖励函数R。为此，我们需要一个表明π与R关系的模型。目前文献中最常见的模型是最优性、Boltzmann合理性和因果熵最大化。IRL的一个主要动机是从人类行为中推断出人类偏好。然而，人类偏好与人类行为之间的真实关系比IRL目前使用的任何模型都更加复杂。这意味着它们被规范错误，这引发了这样的担心：如果应用于真实世界的数据，它们可能会导致不良推断。

    The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\pi$. To do this, we need a model of how $\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecificatio
    
[^110]: 基于卷积、聚合和注意力的深度神经网络用于加速力学模拟

    Convolution, aggregation and attention based deep neural networks for accelerating simulations in mechanics. (arXiv:2212.01386v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01386](http://arxiv.org/abs/2212.01386)

    本研究探讨了基于卷积、聚合和注意力的深度神经网络架构，为学习固体变形的高度非线性变化提供了有效解决方案，并在两个基准示例上取得了良好性能。

    

    深度学习代理模型作为昂贵传统数值技术的替代品，在加速科学模拟方面的应用越来越多。然而，当处理复杂真实世界的样例时，它们的使用仍然是一个重大挑战。在这项工作中，我们展示了三种神经网络架构，用于有效地学习固体变形的高度非线性变化。前两种架构基于最近提出的CNN U-NET和MAgNET（图形U-NET）框架，已表现出在基于网格的数据上学习的良好性能。第三种架构是Perceiver IO，它是最近的一种架构，属于基于注意力的神经网络家族，这个类已经在各种工程领域中产生了革命性的影响，在计算力学领域仍然未被探索。我们研究并比较了这三个网络在两个基准示例上的性能，并展示了它们准确预测非线性力学响应的能力。

    Deep learning surrogate models are being increasingly used in accelerating scientific simulations as a replacement for costly conventional numerical techniques. However, their use remains a significant challenge when dealing with real-world complex examples. In this work, we demonstrate three types of neural network architectures for efficient learning of highly non-linear deformations of solid bodies. The first two architectures are based on the recently proposed CNN U-NET and MAgNET (graph U-NET) frameworks which have shown promising performance for learning on mesh-based data. The third architecture is Perceiver IO, a very recent architecture that belongs to the family of attention-based neural networks--a class that has revolutionised diverse engineering fields and is still unexplored in computational mechanics. We study and compare the performance of all three networks on two benchmark examples, and show their capabilities to accurately predict the non-linear mechanical responses 
    
[^111]: MHCCL：用于多元时间序列的层次掩蔽聚类对比学习

    MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01141](http://arxiv.org/abs/2212.01141)

    本文提出了一种名为MHCCL的对比学习模型，可以从多元时间序列数据中学习语义丰富的表示，并利用层次聚类结构中的多粒度信息来滤除虚假负样本和补充正样本。

    

    从原始无标签时间序列数据中学习语义丰富的表示对于分类和预测等下游任务至关重要。对比学习最近展示了在缺乏专家注释的情况下具有良好的表示学习能力。然而，现有的对比学习方法通常独立处理每个实例，导致共享相同语义的假负样本。为了解决这个问题，我们提出了MHCCL，一种层次掩蔽聚类对比学习模型，它利用由多个潜在分区组成的层次结构获得的语义信息来为多元时间序列建模。受到细粒度聚类保留更高纯度，而粗粒度聚类反映更高级别语义的观察的启发，我们提出了一种新颖的向下掩蔽策略，通过结合聚类层次结构中的多粒度信息，过滤掉虚假负面实例并补充正面实例。

    Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
    
[^112]: 面向稀有事件的动态因果发现：一种非参数条件独立性检验

    Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16596](http://arxiv.org/abs/2211.16596)

    该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。

    

    与稀有事件相关联的因果现象在许多工程问题中都存在，例如针对风险的安全分析、事故分析和预防以及极值理论等。然而，当前的因果发现方法往往无法发现在随机变量之间的原因联系，特别是在变动环境下，仅在变量第一次经历低概率实现时才会显现。为了解决这个问题，我们引入了一种新的统计独立性检验方法，用于从发生稀有但具有重要影响的时间不变动态系统收集的数据中进行因果探索。具体而言，我们利用底层数据的时间不变性来构建一个叠加的数据集，其中包括在不同时间步骤之前稀有事件发生前系统状态的数据。然后我们设计了一个在重新组织的数据上进行条件独立性检验的方法。我们提供了我们方法一致性的非渐近样本复杂度界限，并验证了它在各种模拟和真实世界数据集上的性能。

    Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
    
[^113]: RUST：从未摆姿势的图像中学习潜在的神经场景表示

    RUST: Latent Neural Scene Representations from Unposed Imagery. (arXiv:2211.14306v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14306](http://arxiv.org/abs/2211.14306)

    该论文提出了一种无需摆姿势的新视角合成方法RUST，通过训练一个姿势编码器学习图像中的潜在姿态嵌入，可有效地推广到多个场景和提高视觉质量，优于最先进的无监督方法。

    

    推断三维场景的结构是计算机视觉中的一个基本挑战。基于神经场景表示的方法近年来广受关注，并被应用于各种应用中。这个领域中的一个主要的挑战是训练一个能够提供潜在表示并且能够有效地推广到多个场景的单一模型。场景表示变换器（SRT）在这方面显示出了潜力，但将其扩展到更大数量的多样化场景是具有挑战性的，并需要准确的摆姿数据来进行训练。为了解决这个问题，我们提出了RUST（真正的未摆姿势场景表示变换器），这是一种基于RGB图像进行训练的无姿势新视角合成方法。我们的主要见解是，可以训练一个姿势编码器，它可以查看目标图像并学习一个潜在的姿势嵌入，该姿势被解码器用于视角合成。我们在三个具有挑战性的数据集上对RUST进行了实证研究，并证明我们的方法在视觉质量和泛化到新场景方面均显著优于最先进的无监督方法。

    Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation
    
[^114]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^115]: 基于课程学习的奇异摄动对流扩散反应问题中的PINN优化方法

    Less Emphasis on Difficult Layer Regions: Curriculum Learning for Singularly Perturbed Convection-Diffusion-Reaction Problems. (arXiv:2210.12685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12685](http://arxiv.org/abs/2210.12685)

    提出了一种基于课程学习的新的优化方法，让神经网络优先学习容易的区域，减少对难的区域的学习，从而提高对奇异摄动对流扩散反应问题的预测精度。

    

    物理信息神经网络(PINN)已成功应用于各种科学和工程领域，但在稍具挑战的对流扩散反应问题中，往往无法准确预测基本解。本文从区域分布角度研究了这种失败的原因，并发现同时学习多尺度场使神经网络无法提高训练效果，并容易陷入较差的局域极小值。我们发现在高损失层区域采样更多碰撞点的广泛经验几乎不能帮助优化，并且甚至会使结果变差。这些发现促使我们开发了一种新的课程学习方法，该方法鼓励神经网络优先学习更容易的非层区域，降低对难度更大的层区域的学习重点。所提出的方法可以帮助PINNs自动调整学习重点，从而促进优化过程。

    Although Physics-Informed Neural Networks (PINNs) have been successfully applied in a wide variety of science and engineering fields, they can fail to accurately predict the underlying solution in slightly challenging convection-diffusion-reaction problems. In this paper, we investigate the reason of this failure from a domain distribution perspective, and identify that learning multi-scale fields simultaneously makes the network unable to advance its training and easily get stuck in poor local minima. We show that the widespread experience of sampling more collocation points in high-loss layer regions hardly help optimize and may even worsen the results. These findings motivate the development of a novel curriculum learning method that encourages neural networks to prioritize learning on easier non-layer regions while downplaying learning on harder layer regions. The proposed method helps PINNs automatically adjust the learning emphasis and thereby facilitate the optimization procedur
    
[^116]: 视觉语言模型何时以及为何行为像词袋，以及如何解决？

    When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.01936](http://arxiv.org/abs/2210.01936)

    针对大型视觉语言模型在编码组合信息方面的表现存在问题，我们创建了Attribution、Relation和Order（ARO）基准，并提出了对VLMs进行Attention机制和对抗训练等修改以提高其组合理解能力。

    

    尽管大型视觉语言模型（VLM）在许多下游应用中取得了成功，但它们编码组合信息的能力尚不清楚。为此，我们创建了Attribution、Relation和Order（ARO）基准来系统评估VLM理解不同类型关系、属性和顺序的能力。ARO由Visual Genome Attribution测试对象属性的理解能力；Visual Genome Relation测试关系理解能力；以及COCO＆Flickr30k-Order测试顺序敏感性。ARO比以前的组合性基准大多个数量级，包括50,000多个测试用例。我们展示了最先进的VLM在哪些方面存在关系理解问题，当链接对象和属性时容易出错，并展示了其明显的缺乏顺序敏感性。VLM主要在具有丰富组合结构的图像和标题的大型数据集上进行训练和评估。然而，仅仅在这些数据集上训练并不能保证在需要组合理解的任务上表现良好。为了解决这些问题，我们提出了几种修改VLMs的方法，包括强调组成关系的注意机制和使用对抗训练来改善属性预测。我们提出的修改可以显著提高VLM在ARO基准上的组合理解能力。

    Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d
    
[^117]: 大深度网络的隐式偏见：非线性函数的秩定义。

    Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15055](http://arxiv.org/abs/2209.15055)

    本文研究了全连接神经网络的表示成本和深度之间的关系，发现其会收敛到非线性函数的秩的概念。同时，发现在一定的深度范围内，全局最小值可以恢复真实的数据秩，并探讨了分类器秩对类边界拓扑结构的影响。

    

    我们展示了具有齐次非线性函数的全连接神经网络的表示成本——描述了具有$L_2$正则化或交叉熵等损失网络在函数空间中的隐式偏见——随着网络深度趋近于无穷大，会收敛到非线性函数的秩的概念。然后，我们探究了全局最小值在哪些条件下可以恢复“真实”数据秩：我们展示出，对于太大的深度，全局最小值会近似为秩1（低估秩）；我们随后论证了有一系列深度，随着数据点数量增加，可以恢复真实秩。最后，我们讨论了分类器秩对结果类边界的拓扑结构的影响，并展示了具有最佳非线性秩的自编码器具有自然去噪的特性。

    We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.
    
[^118]: 可计算概率模型的连续混合

    Continuous Mixtures of Tractable Probabilistic Models. (arXiv:2209.10584v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10584](http://arxiv.org/abs/2209.10584)

    本文研究了一种连续混合的方法，将可计算概率模型和基于连续潜空间的模型结合起来，从而实现了高效的概率推断。

    

    基于连续潜空间的概率模型，如变分自编码器，可以理解为在潜变量上连续依赖的不可数混合模型。它们已被证明是表达生成和概率建模的有效工具，但与能够计算所表示的概率分布的较为简单的可计算概率推断方法存在矛盾。与此同时，可计算概率模型，如概率电路(PCs)，可以理解为层次离散混合模型，因此能够高效地进行准确推断，但在与连续潜空间模型相比时通常表现不佳。在本文中，我们研究了一种混合方法，即具有小潜变量维度的可计算模型的连续混合。尽管这些模型在解析上是难以处理的，但它们适合于基于有限积分点集的数值积分方案。通过足够大的积分点集，我们可以紧密地近似概率模型并进行高效推断。

    Probabilistic models based on continuous latent spaces, such as variational autoencoders, can be understood as uncountable mixture models where components depend continuously on the latent code. They have proven to be expressive tools for generative and probabilistic modelling, but are at odds with tractable probabilistic inference, that is, computing marginals and conditionals of the represented probability distribution. Meanwhile, tractable probabilistic models such as probabilistic circuits (PCs) can be understood as hierarchical discrete mixture models, and thus are capable of performing exact inference efficiently but often show subpar performance in comparison to continuous latent-space models. In this paper, we investigate a hybrid approach, namely continuous mixtures of tractable models with a small latent dimension. While these models are analytically intractable, they are well amenable to numerical integration schemes based on a finite set of integration points. With a large 
    
[^119]: TrojViT: 视觉Transformer中的后门插入

    TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13049](http://arxiv.org/abs/2208.13049)

    本文提出了一种针对视觉Transformer的隐秘实用的后门攻击方法TrojViT，通过补丁级触发器将一些易受攻击位组成的参数建立为特洛伊木马。

    

    视觉Transformer（ViT）在各种与视觉相关的任务中展示了最先进的性能。ViTs的成功激励了攻击者对其进行后门攻击。虽然传统的卷积神经网络（CNNs）对后门攻击的脆弱性是众所周知的，但ViTs的后门攻击很少被研究。与通过卷积捕捉像素级局部特征的CNNs相比，ViTs通过补丁和关注机制提取全局上下文信息。将CNN特定的后门攻击天真地移植到ViTs只能产生低干净数据准确性和低攻击成功率。在本文中，我们提出了一个隐秘而实用的ViT特定后门攻击TrojViT。TrojViT生成一个补丁级的触发器，旨在通过补丁显著性排序和关注目标损失，在DRAM存储器中的一些易受攻击位组成的ViT的参数上建立特洛伊木马。 TrojViT还使用最小化调整参数的方法。

    Vision Transformers (ViTs) have demonstrated the state-of-the-art performance in various vision-related tasks. The success of ViTs motivates adversaries to perform backdoor attacks on ViTs. Although the vulnerability of traditional CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are seldom-studied. Compared to CNNs capturing pixel-wise local features by convolutions, ViTs extract global context information through patches and attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs yields only a low clean data accuracy and a low attack success rate. In this paper, we propose a stealth and practical ViT-specific backdoor attack $TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor attacks, TrojViT generates a patch-wise trigger designed to build a Trojan composed of some vulnerable bits on the parameters of a ViT stored in DRAM memory through patch salience ranking and attention-target loss. TrojViT further uses minimum-tuned paramet
    
[^120]: 基于Transformer的目标检测器中多尺度特征的有效利用方法研究

    Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.11356](http://arxiv.org/abs/2208.11356)

    本文提出了一种在基于Transformer的目标检测器中，通过两个新的设计实现了多尺度特征的高效利用，其核心思想是通过稀疏采样多尺度特征，以减少计算成本，同时保证了目标检测性能。

    

    多尺度特征已被证明对于目标检测非常有效，但往往需要巨大甚至是禁止性的额外计算成本，尤其是对于最近的基于Transformer的检测器。本文提出了一种迭代多尺度特征聚合（IMFA）的通用方法，以实现在基于Transformer的目标检测器中高效使用多尺度特征。其核心思想是利用来自仅有几个关键位置的稀疏多尺度特征，并通过两个新的设计实现。首先，IMFA重新排列Transformer编码器-解码器管道，以便根据检测预测迭代更新编码特征。其次，IMFA在以先前的检测预测作为指导的情况下，从仅有几个关键点采用稀疏的尺度自适应特征用于精细检测。因此，采样的多尺度特征是稀疏的，但对于目标检测仍然非常有益。大量实验证明，所提出的IMFA技术可以在保证目标检测性能的同时显著减少计算成本。

    Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA bo
    
[^121]: 将机器学习应用于自动化测试生成的融合：系统性映射研究

    The Integration of Machine Learning into Automated Test Generation: A Systematic Mapping Study. (arXiv:2206.10210v4 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2206.10210](http://arxiv.org/abs/2206.10210)

    该论文研究了将机器学习应用于自动化测试生成的方法，并总结了目前的研究进展、应用和挑战。其中监督学习和强化学习是主要的技术方法。

    

    背景：机器学习（ML）可能实现有效的自动化测试生成。目标：我们研究测试实践、研究者目标、应用的ML技术、评估和挑战等方面的兴起研究。方法：我们在102篇论文中进行了系统性映射。结果：ML生成系统、GUI、单元、性能和组合测试的输入或改进现有的生成方法。ML还用于生成测试裁决、基于属性的和期望输出oracle。监督学习，通常基于神经网络和强化学习，通常基于Q-learning，是常见的，有些出版物还采用了无监督或半监督学习。(半/无)监督方法使用传统测试指标和ML相关指标（例如准确性）进行评估，而强化学习通常使用与奖励函数相关的测试指标进行评估。结论：迄今为止的工作显示了巨大的潜力，同时还揭示了挑战并提出了新的研究方向。

    Context: Machine learning (ML) may enable effective automated test generation.  Objective: We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges.  Methods: We perform a systematic mapping on a sample of 102 publications.  Results: ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property-based, and expected output oracles. Supervised learning - often based on neural networks and reinforcement learning - often based on Q-learning - are common, and some publications also employ unsupervised or semi-supervised learning. (Semi-/Un-)Supervised approaches are evaluated using both traditional testing metrics and ML-related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function.  Conclusion: Work-to-date shows grea
    
[^122]: 非对称二分排队系统中的高效分散式多智能体学习

    Efficient decentralized multi-agent learning in asymmetric bipartite queueing systems. (arXiv:2206.03324v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03324](http://arxiv.org/abs/2206.03324)

    该论文提出了一种简单的学习算法，使得在非对称二分排队系统中实现了高效的性能，并具备附加的鲁棒性质，并且提供了该问题的集中式情况下的首个经过证明的有效的UCB算法。

    

    我们研究了二分排队系统中的分散式多智能体学习，这是一个服务系统的标准模型。特别地，N个代理以完全分散的方式从K个服务器请求服务，即不进行通信地运行相同的算法。先前的分散式算法仅局限于对称系统，其性能随着服务器数量指数级降低，需要通过共享随机性和唯一的代理身份进行通信，且计算量巨大。相比之下，我们提供了一种简单的学习算法，当每个代理分散地运行时，在普通的非对称二分排队系统中实现了高效的性能，同时还具备附加的鲁棒性质。同时，我们为该问题的集中式情况提供了首个经过证明的有效的UCB算法。

    We study decentralized multi-agent learning in bipartite queueing systems, a standard model for service systems. In particular, N agents request service from K servers in a fully decentralized way, i.e, by running the same algorithm without communication. Previous decentralized algorithms are restricted to symmetric systems, have performance that is degrading exponentially in the number of servers, require communication through shared randomness and unique agent identities, and are computationally demanding. In contrast, we provide a simple learning algorithm that, when run decentrally by each agent, leads the queueing system to have efficient performance in general asymmetric bipartite queueing systems while also having additional robustness properties. Along the way, we provide the first provably efficient UCB-based algorithm for the centralized case of the problem.
    
[^123]: 深度学习模型及其内部表示的对称性研究

    On the Symmetries of Deep Learning Models and their Internal Representations. (arXiv:2205.14258v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14258](http://arxiv.org/abs/2205.14258)

    本文研究了深度学习模型及其内部表示的对称性，发现网络的对称性传播到数据的表示中的对称性，为更好地理解架构影响学习和预测过程提供了一种方法。

    

    对称性是研究复杂系统的基本工具。在机器学习中，对称性已经在模型和数据中得到了研究。本文旨在通过计算模型的一组基本对称群，即模型的intertwiner groups，将模型架构的对称性与该模型对数据的内部表示的对称性相联系。我们通过一系列实验将intertwiner groups连接到具有相同架构的模型的隐藏状态之间的相似之处。我们的研究表明，网络的对称性传播到该网络对数据的表示中的对称性，更好地理解架构如何影响学习和预测过程。最后，我们推测，对于ReLU网络，intertwiner groups可能为常见的堆叠网络结构提供了正当性。

    Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of con
    
[^124]: 持续的时空图卷积神经网络

    Continual Spatio-Temporal Graph Convolutional Networks. (arXiv:2203.11009v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.11009](http://arxiv.org/abs/2203.11009)

    本文提出了一种连续时空图卷积神经网络（CoST-GCN）来解决基于图的方法在在线推理设置中应用时重复计算的问题，相比之前的方法最多可将时间复杂度减少109倍。

    

    基于图的推理在骨骼数据的应用已成为人体动作识别的有希望的方法。然而，以整个时间序列作为输入的先前基于图的方法在在线推理设置中应用会导致重复的计算。本文通过将时空图卷积神经网络重新构建为连续推理网络来解决这个问题，这个网络可以在时间上逐步进行预测，而无需重复帧处理。为了评估我们的方法，我们创建了ST-GCN网络的连续版本CoST-GCN，以及两种具有不同自我注意机制的衍生方法——CoAGCN和CoS-TR。我们研究了权重传递策略和架构修改以加速推理，并在NTU RGB+D 60、NTU RGB+D 120和Kinetics Skeleton 400数据集上进行了实验。在保持类似的预测准确性条件下，我们观察到时间复杂度最多减少了109倍。

    Graph-based reasoning over skeleton data has emerged as a promising approach for human action recognition. However, the application of prior graph-based methods, which predominantly employ whole temporal sequences as their input, to the setting of online inference entails considerable computational redundancy. In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph Convolutional Neural Network as a Continual Inference Network, which can perform step-by-step predictions in time without repeat frame processing. To evaluate our method, we create a continual version of ST-GCN, CoST-GCN, alongside two derived methods with different self-attention mechanisms, CoAGCN and CoS-TR. We investigate weight transfer strategies and architectural modifications for inference acceleration, and perform experiments on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar predictive accuracy, we observe up to 109x reduction in time complexity, on-hard
    
[^125]: Euler State Networks: 非耗散性水库计算

    Euler State Networks: Non-dissipative Reservoir Computing. (arXiv:2203.09382v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09382](http://arxiv.org/abs/2203.09382)

    本文提出了一种新型水库计算模型EuSN，其利用前向欧拉离散化和反对称循环矩阵来设计水库动力学，具有接近稳定边缘的饱和有效谱半径和零局部李雅普诺夫指数。在长期记忆任务和时间序列分类基准测试中表现出了优异的性能。

    

    本文受到常微分方程的数值解启发，提出了一种新型水库计算(EuSN)模型，该方法利用前向欧拉离散化和反对称循环矩阵来设计水库动力学。该模型的数学分析表明，其具有接近稳定边缘的饱和有效谱半径和零局部李雅普诺夫指数。对长期记忆任务的实验表明，与标准水库计算模型相比，所提出的方法在需要多个时步有效传播输入信息的问题上具有明显的优势。此外，时间序列分类基准测试结果表明，EuSN能够匹配甚至超过可训练循环神经网络的准确性，同时保持水库计算家族的训练效率。

    Inspired by the numerical solution of ordinary differential equations, in this paper we propose a novel Reservoir Computing (RC) model, called the Euler State Network (EuSN). The presented approach makes use of forward Euler discretization and antisymmetric recurrent matrices to design reservoir dynamics that are both stable and non-dissipative by construction.  Our mathematical analysis shows that the resulting model is biased towards a unitary effective spectral radius and zero local Lyapunov exponents, intrinsically operating near to the edge of stability. Experiments on long-term memory tasks show the clear superiority of the proposed approach over standard RC models in problems requiring effective propagation of input information over multiple time-steps. Furthermore, results on time-series classification benchmarks indicate that EuSN is able to match (or even exceed) the accuracy of trainable Recurrent Neural Networks, while retaining the training efficiency of the RC family, res
    
[^126]: 基于决策的稀疏攻击黑盒深度学习模型的有效查询

    Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models. (arXiv:2202.00091v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00091](http://arxiv.org/abs/2202.00091)

    研究提出了基于决策的稀疏攻击方法SparseEvo，通过最小化扰动的像素数量，能够有效地欺骗黑盒深度学习模型，而不需要了解它的内部构造。

    

    尽管我们已经做了最大的努力，但深度学习模型仍然容易受到微小的对抗性扰动。从机器学习模型的输出中提取信息以制作黑盒模型的对抗扰动是现实世界系统（例如自动驾驶汽车或作为服务（MLaaS）公开的机器学习模型）面临的威胁。 稀疏攻击尤其受到关注。 稀疏攻击在黑盒模型中的实现表明，机器学习模型比我们想象的更容易受到攻击。这项研究提出了一种基于决策的稀疏攻击方法，通过最小化扰动的像素数量，有效地欺骗黑盒深度学习模型，而不需要了解它的内部构造。我们开发了一种基于进化算法的SparseEvo，用于解决NP难的问题。

    Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as autonomous cars or machine learning models exposed as a service (MLaaS). Of particular interest are sparse attacks. The realization of sparse attacks in black-box models demonstrates that machine learning models are more vulnerable than we believe. Because these attacks aim to minimize the number of perturbed pixels measured by l_0 norm-required to mislead a model by solely observing the decision (the predicted label) returned to a model query; the so-called decision-based attack setting. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm-SparseEvo-for the problem and evaluate against both convolutional deep 
    
[^127]: 分数规划的坐标下降方法

    Coordinate Descent Methods for Fractional Minimization. (arXiv:2201.12691v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2201.12691](http://arxiv.org/abs/2201.12691)

    本文提出两种坐标下降方法，用于解决具有结构化约束的分数规划问题，并保证收敛到逐坐标静态点。

    

    我们考虑一类结构化的分数规划问题，其中目标函数的分子部分是一个可微的凸函数和一个凸非光滑函数的和，而分母部分是一个凸或凹函数。由于该问题是非凸的，所以很难解决。通过利用问题的结构，我们提出了两种坐标下降方法来解决这个问题。所提出的方法迭代地解决一个一维子问题，而且保证收敛到逐坐标静止点。在分母为凸函数的情况下，在弱局部有界非凸条件下，我们证明了逐坐标静态点的最优性比标准临界点和方向性点的最优性更强。在满足适当的条件下，坐标下降算法以Q-线性方式收敛到逐坐标静态点。在分母为凹函数的情况下，我们展示了任何临界点都是逐坐标最优点的充分条件。

    We consider a class of structured fractional minimization problems, in which the numerator part of the objective is the sum of a differentiable convex function and a convex non-smooth function, while the denominator part is a convex or concave function. This problem is difficult to solve since it is non-convex. By exploiting the structure of the problem, we propose two Coordinate Descent (CD) methods for solving this problem. The proposed methods iteratively solve a one-dimensional subproblem \textit{globally}, and they are guaranteed to converge to coordinate-wise stationary points. In the case of a convex denominator, under a weak \textit{locally bounded non-convexity condition}, we prove that the optimality of coordinate-wise stationary point is stronger than that of the standard critical point and directional point. Under additional suitable conditions, CD methods converge Q-linearly to coordinate-wise stationary points. In the case of a concave denominator, we show that any critic
    
[^128]: 基于增强式RBMLE-UCB方法的线性二次系统自适应控制研究

    Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems. (arXiv:2201.10542v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2201.10542](http://arxiv.org/abs/2201.10542)

    本文提出一种增强式RBMLE-UCB算法，将RBMLE方法的惩罚与UCB方法的约束相结合，解决了自适应LQ控制问题。在噪声较高或样本数较少时，该算法表现更优。

    

    本文考虑了控制一个未知随机线性系统，该系统具有二次代价，即自适应LQ控制问题。我们重新审视了一个称为“奖励偏向最大似然估计”(RBMLE) 的方法，该方法提出了40多年前，在“上限置信区间法”(UCB)方法和针对赌博机问题的“遗憾”的定义之前。它简单地为参数估计的标准添加了一个倾向于具有更大奖励的参数的项。我们展示了如何协调RBMLE和UCB方法，并因此提出了一种增强的RBMLE-UCB算法，它将RBMLE方法的惩罚与UCB方法的约束相结合，将两种面对不确定性的方法统一了起来。我们理论上证明了，这种方法保持了迄今为止最好的$\Tilde{\mathcal{O}}(\sqrt{T})$遗憾值。我们进一步比较了所提出的增强式RBMLE-UCB和标准RBMLE(不带增强UCB方法)在模拟系统上的实证表现，并证明了增强算法在噪声较高或样本数较少时表现更优。

    We consider the problem of controlling an unknown stochastic linear system with quadratic costs - called the adaptive LQ control problem. We re-examine an approach called ''Reward Biased Maximum Likelihood Estimate'' (RBMLE) that was proposed more than forty years ago, and which predates the ''Upper Confidence Bound'' (UCB) method as well as the definition of ''regret'' for bandit problems. It simply added a term favoring parameters with larger rewards to the criterion for parameter estimation. We show how the RBMLE and UCB methods can be reconciled, and thereby propose an Augmented RBMLE-UCB algorithm that combines the penalty of the RBMLE method with the constraints of the UCB method, uniting the two approaches to optimism in the face of uncertainty. We establish that theoretically, this method retains $\Tilde{\mathcal{O}}(\sqrt{T})$ regret, the best-known so far. We further compare the empirical performance of the proposed Augmented RBMLE-UCB and the standard RBMLE (without the augm
    
[^129]: 分解式量子图神经网络

    Decompositional Quantum Graph Neural Network. (arXiv:2201.05158v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2201.05158](http://arxiv.org/abs/2201.05158)

    本论文提出了一种新型的基于量子计算的神经网络，能够处理基于图的数据，通过使用自我图和张量积降低模型参数数量，提出一种从现实世界数据到希尔伯特空间的新型映射方式。

    

    量子机器学习是一个快速发展的领域，旨在利用量子算法和量子计算解决机器学习问题。由于物理量子比特缺乏和缺乏有效的将现实世界数据从欧几里得空间映射到希尔伯特空间的方法，大多数方法都是基于量子类比或过程模拟，而不是基于量子比特的具体体系结构设计。在本文中，我们提出了一种新型的混合量子-经典算法，用于处理基于图的数据，我们将其称为基于自我图的量子图神经网络（egoQGNN）。egoQGNN使用张量积和单位矩阵表示实现了GNN理论框架，大大减少了所需的模型参数数量。当被经典计算机控制时，egoQGNN可以通过从输入图的自我图中处理来容纳任意大小的图，使用一个中等大小的量子设备。该体系结构基于从现实世界数据到希尔伯特空间的新型映射方式。

    Quantum machine learning is a fast-emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This m
    
[^130]: DeBERTaV3：使用梯度去耦合嵌入共享的ELECTRA风格预训练来改进DeBERTa模型

    DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.09543](http://arxiv.org/abs/2111.09543)

    本论文介绍了一种新的预训练语言模型DeBERTaV3，使用更加样本有效的替换令牌检测（RTD）取代了掩码语言建模（MLM）并提出了一种新的梯度去耦合嵌入共享方法，避免了“拔河”动态，提高了预训练模型的训练效率和质量。在多个下游自然语言理解任务中，DeBERTaV3表现出优秀的性能。

    

    本文介绍了一种新的预训练语言模型DeBERTaV3，它通过将掩码语言建模（MLM）替换为更加样本有效的替换令牌检测（RTD）来改进原始的DeBERTa模型。我们的分析表明，ELECTRA中的香草嵌入共享会影响训练效率和模型性能，因为判别器和生成器的训练损失将令牌嵌入拉向不同的方向，会造成“拔河”动态。因此，我们提出了一种新的梯度去耦合嵌入共享方法，避免了“拔河”动态，提高了预训练模型的训练效率和质量。我们使用与DeBERTa相同的设置预训练了DeBERTaV3，以展示其在各种下游自然语言理解（NLU）任务中的优秀性能。以八项任务为例的GLUE基准测试中，DeBERTaV3 Large模型平均得分为91.37％，比D高1.37％。

    This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over D
    
[^131]: 学习粗标签的高效算法

    Efficient Algorithms for Learning from Coarse Labels. (arXiv:2108.09805v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.09805](http://arxiv.org/abs/2108.09805)

    本文研究了从粗糙数据中学习的问题，提出了一种高效的算法，只需足够信息的粗标签即可在从细标签中学习的任何问题上进行学习。

    

    对于许多学习问题，我们可能无法访问细粒度标签信息；例如，根据注释者的专业知识，一幅图像可以被标记为哈士奇、狗甚至动物。本文对这些情况进行了形式化，并研究了从此类粗数据学习的问题。我们不是观察来自集合 $\mathcal{Z}$ 的实际标签，而是观察对应于 $\mathcal{Z}$ 的划分（或多种划分的混合）的粗标签。我们的主要算法结果是：几乎任何可从细粒度标签中学习的问题，当粗数据足够信息时也可以高效地学习。我们通过泛化减少回答统计查询（SQ) 的方式来获得结果，只给出粗标签而不是细标签。所需粗标签数量与粗糙化信息失真和细标签数量 $|\mathcal{Z}|$ 成多项式关系。我们还研究了（无限多个）实值标签的情况。

    For many learning problems one may not have access to fine grained label information; e.g., an image can be labeled as husky, dog, or even animal depending on the expertise of the annotator. In this work, we formalize these settings and study the problem of learning from such coarse data. Instead of observing the actual labels from a set $\mathcal{Z}$, we observe coarse labels corresponding to a partition of $\mathcal{Z}$ (or a mixture of partitions).  Our main algorithmic result is that essentially any problem learnable from fine grained labels can also be learned efficiently when the coarse data are sufficiently informative. We obtain our result through a generic reduction for answering Statistical Queries (SQ) over fine grained labels given only coarse labels. The number of coarse labels required depends polynomially on the information distortion due to coarsening and the number of fine labels $|\mathcal{Z}|$.  We also investigate the case of (infinitely many) real valued labels foc
    
[^132]: 上下文多臂赌博机中的本地聚类

    Local Clustering in Contextual Multi-Armed Bandits. (arXiv:2103.00063v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.00063](http://arxiv.org/abs/2103.00063)

    本文提出了一种基于未知赌博机参数的本地聚类策略，用于上下文多臂赌博机中的用户聚类问题。该算法在内容推荐和定向广告中具有重要应用价值，并在实验中表现出较高的性能。

    

    我们研究了在上下文多臂赌博机（MAB）中识别用户聚类的问题。上下文MAB是许多实际应用的有效工具，例如内容推荐和在线广告。在实践中，用户依赖关系在用户的行为和奖励中起着至关重要的作用。对类似用户进行聚类可以提高奖励估计的质量，从而实现更有效的内容推荐和定向广告。不同于传统聚类设置，我们基于未知的赌博机参数对用户进行聚类，这些参数将被逐步估计。特别地，我们定义了上下文MAB中的聚类检测问题，并提出了一种嵌入本地聚类过程的赌博算法LOCB。我们对LOCB的聚类正确性和效率以及其遗憾界进行了理论分析。最后，我们从各个方面评估了所提出的算法，结果表明其优于最先进的基线算法。

    We study identifying user clusters in contextual multi-armed bandits (MAB). Contextual MAB is an effective tool for many real applications, such as content recommendation and online advertisement. In practice, user dependency plays an essential role in the user's actions, and thus the rewards. Clustering similar users can improve the quality of reward estimation, which in turn leads to more effective content recommendation and targeted advertising. Different from traditional clustering settings, we cluster users based on the unknown bandit parameters, which will be estimated incrementally. In particular, we define the problem of cluster detection in contextual MAB, and propose a bandit algorithm, LOCB, embedded with local clustering procedure. And, we provide theoretical analysis about LOCB in terms of the correctness and efficiency of clustering and its regret bound. Finally, we evaluate the proposed algorithm from various aspects, which outperforms state-of-the-art baselines.
    
[^133]: UCB Bandits在对抗攻击中的近乎最优攻击策略

    Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.09312](http://arxiv.org/abs/2008.09312)

    本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。

    

    本文考虑了一种随机多臂赌博问题，其中奖励受到对抗性破坏。我们提出了一种新颖的攻击策略，通过操作UCB原则来拉动一些非最优目标臂$T-o(T)$次，累积成本的标度为$\sqrt{\log T}$，其中$T$为回合数。我们还证明了累积攻击成本的第一个下界。我们的下界与我们的上界匹配，除了$\log\log T$因子，表明我们的攻击策略近乎是最优的。

    We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
    

