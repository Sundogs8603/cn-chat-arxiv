# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Graph Neural Networks for Treatment Effect Prediction](https://arxiv.org/abs/2403.19289) | 提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性 |
| [^2] | [An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations](https://arxiv.org/abs/2403.13748) | 不同的散度排序可以通过它们的变分近似误估不确定性的各种度量，并且因子化近似无法同时匹配这些度量中的任意两个 |
| [^3] | [Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark](https://arxiv.org/abs/2403.13502) | 该研究通过评估在自动控制系统中部署的深度学习模型对抗性攻击的脆弱性和不同防御策略的有效性，提出了结合多种防御方法的新颖保护方法，并为确保工业中的稳健故障诊断提供了见解。 |
| [^4] | [Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://arxiv.org/abs/2403.07294) | 通过自表达图结构重建的方法解决了图数据压缩中的问题 |
| [^5] | [Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](https://arxiv.org/abs/2403.04696) | 提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。 |
| [^6] | [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/abs/2402.16310) | 该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。 |
| [^7] | [Deep Networks Always Grok and Here is Why](https://arxiv.org/abs/2402.15555) | 深度神经网络存在延迟泛化和延迟鲁棒性现象，在各种实际环境中普遍存在，并基于新的局部复杂度度量提供了解释。 |
| [^8] | [Transformers are Expressive, But Are They Expressive Enough for Regression?](https://arxiv.org/abs/2402.15478) | Transformer在逼近连续函数方面存在困难，是否真正是通用函数逼近器仍有待考证 |
| [^9] | [NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks](https://arxiv.org/abs/2402.15393) | NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。 |
| [^10] | [On Minimal Depth in Neural Networks](https://arxiv.org/abs/2402.15315) | 本研究研究了神经网络中关于最小深度的问题，特别关注了ReLU神经网络的表达能力和最小深度与CPWL函数的关系。 |
| [^11] | [Unintended Impacts of LLM Alignment on Global Representation](https://arxiv.org/abs/2402.15018) | 对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。 |
| [^12] | [DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents](https://arxiv.org/abs/2402.14865) | 本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。 |
| [^13] | [Dealing with unbounded gradients in stochastic saddle-point optimization](https://arxiv.org/abs/2402.13903) | 提出一种简单而有效的正则化技术，稳定了随机鞍点优化过程中的梯度不断增长的问题，能够在无界梯度和噪声的情况下提供有意义的性能保证 |
| [^14] | [Neural Control System for Continuous Glucose Monitoring and Maintenance](https://arxiv.org/abs/2402.13852) | 引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。 |
| [^15] | [When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting](https://arxiv.org/abs/2402.12767) | 提出了一种名为IDEA的模型，通过学习可识别的潜在状态检测时间序列数据中的分布变迁，并进一步分离平稳和非平稳的潜在状态。 |
| [^16] | [Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement](https://arxiv.org/abs/2402.11224) | 本文揭示了神经网络的多项式逼近作为一种独立对象的研究，发现PANN对某些类型的逼近误差... |
| [^17] | [CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback](https://arxiv.org/abs/2402.10980) | 通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索 |
| [^18] | [Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412) | 提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。 |
| [^19] | [Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning](https://arxiv.org/abs/2402.09542) | 这项工作针对在线连续学习中经验回放造成的优化不稳定问题进行了改进，提出了一种逐层近端回放（LPR）方法，通过优化几何的修改来平衡新数据和回放数据的学习，从而改善了回放式在线连续学习方法的准确性。 |
| [^20] | [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679) | 本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。 |
| [^21] | [Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations](https://arxiv.org/abs/2402.07933) | 本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。 |
| [^22] | [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://arxiv.org/abs/2402.06627) | 与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。 |
| [^23] | [S$\Omega$I: Score-based O-INFORMATION Estimation](https://arxiv.org/abs/2402.05667) | S$\Omega$I是一种新的信息论量度，可以计算多变量系统中的协同-冗余平衡，突破了传统量度的局限性，并在合成数据上得到了验证。 |
| [^24] | [Meta-learning the mirror map in policy mirror descent](https://arxiv.org/abs/2402.05187) | 该论文通过实证研究发现，传统的镜像映射选择（NPG）在标准基准环境中常常导致不理想的结果。通过元学习方法，找到了更高效的镜像映射，提升了性能。 |
| [^25] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^26] | [Estimating Barycenters of Distributions with Neural Optimal Transport](https://arxiv.org/abs/2402.03828) | 本研究借助最优传输的对偶形式，提出了一种新的可扩展方法，用于估计分布的重心。通过神经最优传输求解器，我们实现了双层对抗学习并适用于一般成本函数。这个方法的关键优势在于不需要使用三层优化，并且适用于多种成本函数。我们还建立了理论误差界，并在示例场景和图像数据上展示了方法的应用性和有效性。 |
| [^27] | [$C^*$-Algebraic Machine Learning: Moving in a New Direction](https://arxiv.org/abs/2402.02637) | $C^*$-代数机器学习是将$C^*$-代数与机器学习结合的新研究方向，它通过统一现有的学习策略，并构建更多元化和信息丰富的数据模型的新框架，为机器学习提供了一种新的方法。 |
| [^28] | [TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling](https://arxiv.org/abs/2402.02475) | TimeSiam是一种用于时间序列建模的预训练框架，通过使用Siamese网络和简单的数据增强方法，能够捕捉时间序列数据的内在时间相关性，并学习内部时序依赖的表示。 |
| [^29] | [Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training](https://arxiv.org/abs/2402.02225) | 本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。 |
| [^30] | [A Manifold Representation of the Key in Vision Transformers](https://arxiv.org/abs/2402.00534) | 该论文提出了一种在视觉转换器中将关键信息与查询和数值分离并采用流形表示的方法，实验证明这种方法能够提升模型性能，并在ImageNet-1K和COCO数据集上取得了积极的结果。 |
| [^31] | [Merging Multi-Task Models via Weight-Ensembling Mixture of Experts](https://arxiv.org/abs/2402.00433) | 通过权重集成专家的方法可以将训练在不同任务上的Transformer模型合并为一个统一的模型，通过动态整合共享和特定任务的知识来提供更灵活的解决方案。 |
| [^32] | [The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise](https://arxiv.org/abs/2401.07844) | 本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。 |
| [^33] | [RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair](https://arxiv.org/abs/2312.15698) | 高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。 |
| [^34] | [FRAPP\'E: A Group Fairness Framework for Post-Processing Everything](https://arxiv.org/abs/2312.02592) | 提出了一个将任何正则化的处理中方法转化为后处理方法的框架，适用于更广泛的问题设置，保持了良好的公平错误权衡，并且可能提高之前方法的效力 |
| [^35] | [HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction](https://arxiv.org/abs/2310.10565) | 本文提出了HelmFluid，一个精确且可解释的流体预测器。通过学习Helmholtz动力学，将流体动力学分解为更可解的无旋和无散部分，并结合多尺度多头积分架构进行集成，HelmFluid能够更准确地预测流体的未来行为。 |
| [^36] | [Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras](https://arxiv.org/abs/2310.04521) | 本文提出了一种Lie神经元网络，能够以任何半单Lie代数数据为输入，通过伴随操作使其具有等变性。通过推广向量神经元网络和引入新的层，该网络在各个领域具有广泛的适用性和竞争性能。 |
| [^37] | [Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics](https://arxiv.org/abs/2212.07892) | 提出了一种基于多模态变分自动编码器的算法框架，用于联合生成建模复杂动力学，引导重建模型训练。 |
| [^38] | [Benchmarking the Robustness of Image Watermarks.](http://arxiv.org/abs/2401.08573) | 本研究提出了一种新颖的基准测试方法WAVES，用于评估图像水印的鲁棒性。通过整合检测和识别任务，并进行多样化压力测试，我们揭示了以前未被发现的水印脆弱性。 |
| [^39] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^40] | [WinNet:time series forecasting with a window-enhanced period extracting and interacting.](http://arxiv.org/abs/2311.00214) | WinNet是一种用于时间序列预测的CNN-based模型，通过窗口增强的周期提取和交互，在捕捉长期和短期周期性方面具有高准确度和简单结构。在九个基准数据集上的实验结果表明，WinNet可以实现优于CNN、MLP、Transformer方法的最新性能，并具有较低的计算复杂度。 |
| [^41] | [Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution.](http://arxiv.org/abs/2310.16834) | 本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。 |
| [^42] | [Causal Representation Learning Made Identifiable by Grouping of Observational Variables.](http://arxiv.org/abs/2310.15709) | 该论文研究了因果表示学习（CRL），提出了一种基于观测变量分组的新型弱约束的可辨识性方法，该方法不依赖于时间结构、干预或监督，具有实际应用的意义。 |
| [^43] | [Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives.](http://arxiv.org/abs/2310.12324) | 适应性实验为持续课程改进提供了机会，通过动态部署最有效的条件以满足学生需求。 |
| [^44] | [GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers.](http://arxiv.org/abs/2310.10375) | 提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。 |
| [^45] | [Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?.](http://arxiv.org/abs/2310.10012) | 本文研究了对于文本到图像合成的扩散模型中的潜在滥用问题的安全措施的有效性，并提出了一个用于评估的新颖概念检索算法。我们引入了一个名为Ring-A-Bell的模型无关的红队工具，可以事先准备整个评估过程，而无需先验知识。 |
| [^46] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^47] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^48] | [Exploiting Transformer Activation Sparsity with Dynamic Inference.](http://arxiv.org/abs/2310.04361) | 本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。 |
| [^49] | [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2307.06887) | 通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。 |
| [^50] | [End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments.](http://arxiv.org/abs/2306.16978) | 本文提出了一种基于端到端强化学习的在线覆盖路径规划方法，能处理未知环境并结合全局地图和局部感知输入，同时考虑长期路径规划和短期障碍物检测。 |
| [^51] | [Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning.](http://arxiv.org/abs/2306.08590) | 本文通过实际数据的全面实证分析证明了在在线学习中，大学习率和小批量大小并不会带来任何隐性偏差的优势。在线学习中SGD噪音的好处只是计算上的便利，可以促进更大或更具成本效益的梯度步骤。 |
| [^52] | [Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning.](http://arxiv.org/abs/2306.05101) | Sy-CON是一种新的损失函数，用于持续自监督学习，它由两个损失组成，可以自然地找到良好的可塑性和稳定性之间的权衡，超过了现有持续学习方法。 |
| [^53] | [BOtied: Multi-objective Bayesian optimization with tied multivariate ranks.](http://arxiv.org/abs/2306.00344) | BOtied 是一种带有相关多元等级的多目标贝叶斯优化算法，相较于现有的方法具有更高的样本效率和较好的近似质量。 |
| [^54] | [PDExplain: Contextual Modeling of PDEs in the Wild.](http://arxiv.org/abs/2303.15827) | 我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。 |
| [^55] | [Spiking Neural Networks for event-based action recognition: A new task to understand their advantage.](http://arxiv.org/abs/2209.14915) | 本文通过提出新任务DVS-GC并进行实证研究，展示了脉冲神经网络在事件驱动动作识别中的优势，包括实现时间特征提取和对事件顺序的理解。 |
| [^56] | [Gauge Invariant and Anyonic Symmetric Autoregressive Neural Networks for Quantum Lattice Models.](http://arxiv.org/abs/2101.07243) | 该论文提出了一种针对量子晶格模型的自回归神经网络，并明确遵守规范对称性或任意子约束，能够提供精确表示量子晶格模型基态和激发态的方法。 |

# 详细

[^1]: 用于治疗效果预测的图神经网络

    Graph Neural Networks for Treatment Effect Prediction

    [https://arxiv.org/abs/2403.19289](https://arxiv.org/abs/2403.19289)

    提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性

    

    在电子商务中估计因果效应往往涉及昂贵的治疗分配，这在大规模设置中可能是不切实际的。利用机器学习来预测这种治疗效果而无需实际干预是减少风险的一种标准做法。然而，现有的治疗效果预测方法往往依赖于大规模实验构建的训练集，因此从根本上存在风险。在这项工作中，我们提出了一种图神经网络，以减少所需的训练集大小，依赖于电子商务数据中常见的图。具体地，我们将问题视为具有有限数量标记实例的节点回归，开发了一个类似于先前因果效应估计器的双模型神经架构，并测试了不同的消息传递层进行编码。此外，作为额外步骤，我们将模型与获取函数相结合，以引导信息传递。

    arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
    
[^2]: 变分推断中因子化高斯近似的差异排序

    An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations

    [https://arxiv.org/abs/2403.13748](https://arxiv.org/abs/2403.13748)

    不同的散度排序可以通过它们的变分近似误估不确定性的各种度量，并且因子化近似无法同时匹配这些度量中的任意两个

    

    在变分推断（VI）中，给定一个难以处理的分布$p$，问题是从一些更易处理的族$\mathcal{Q}$中计算最佳近似$q$。通常情况下，这种近似是通过最小化Kullback-Leibler (KL)散度来找到的。然而，存在其他有效的散度选择，当$\mathcal{Q}$不包含$p$时，每个散度都支持不同的解决方案。我们分析了在高斯的密集协方差矩阵被对角协方差矩阵的高斯近似所影响的VI结果中，散度选择如何影响VI结果。在这种设置中，我们展示了不同的散度可以通过它们的变分近似误估不确定性的各种度量，如方差、精度和熵，进行\textit{排序}。我们还得出一个不可能定理，表明无法通过因子化近似同时匹配这些度量中的任意两个；因此

    arXiv:2403.13748v1 Announce Type: cross  Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to compute the best approximation $q$ from some more tractable family $\mathcal{Q}$. Most commonly the approximation is found by minimizing a Kullback-Leibler (KL) divergence. However, there exist other valid choices of divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence champions a different solution. We analyze how the choice of divergence affects the outcome of VI when a Gaussian with a dense covariance matrix is approximated by a Gaussian with a diagonal covariance matrix. In this setting we show that different divergences can be \textit{ordered} by the amount that their variational approximations misestimate various measures of uncertainty, such as the variance, precision, and entropy. We also derive an impossibility theorem showing that no two of these measures can be simultaneously matched by a factorized approximation; henc
    
[^3]: 在自动控制系统中的对抗性攻击与防御：一项综合基准研究

    Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark

    [https://arxiv.org/abs/2403.13502](https://arxiv.org/abs/2403.13502)

    该研究通过评估在自动控制系统中部署的深度学习模型对抗性攻击的脆弱性和不同防御策略的有效性，提出了结合多种防御方法的新颖保护方法，并为确保工业中的稳健故障诊断提供了见解。

    

    将机器学习整合到自动控制系统（ACS）中增强了工业过程管理的决策能力。其中一项限制工业普遍采用这些技术的是神经网络对对抗性攻击的脆弱性。本研究探讨了使用Tennessee Eastman过程数据集在ACS中部署深度学习模型进行故障诊断时的威胁。通过评估三种不同架构的神经网络，我们对其进行了六种对抗性攻击，并探讨了五种不同的防御方法。我们的结果突出了模型对对抗性样本的强大脆弱性以及防御策略的不同有效性。我们还提出了一种新颖的保护方法，将多种防御方法结合起来，并展示了其有效性。这项研究对确保工业中机器学习的安全性提供了一些见解，确保了工业中稳健的故障诊断。

    arXiv:2403.13502v1 Announce Type: new  Abstract: Integrating machine learning into Automated Control Systems (ACS) enhances decision-making in industrial process management. One of the limitations to the widespread adoption of these technologies in industry is the vulnerability of neural networks to adversarial attacks. This study explores the threats in deploying deep learning models for fault diagnosis in ACS using the Tennessee Eastman Process dataset. By evaluating three neural networks with different architectures, we subject them to six types of adversarial attacks and explore five different defense methods. Our results highlight the strong vulnerability of models to adversarial samples and the varying effectiveness of defense strategies. We also propose a novel protection approach by combining multiple defense methods and demonstrate it's efficacy. This research contributes several insights into securing machine learning within ACS, ensuring robust fault diagnosis in industrial 
    
[^4]: 通过自表达图结构重建对图数据进行压缩

    Graph Data Condensation via Self-expressive Graph Structure Reconstruction

    [https://arxiv.org/abs/2403.07294](https://arxiv.org/abs/2403.07294)

    通过自表达图结构重建的方法解决了图数据压缩中的问题

    

    随着训练大规模图神经网络（GNNs）需求的增加，图数据压缩已经成为在训练阶段减轻存储和时间成本的关键技术。它旨在将原始大规模图压缩为一个更小的合成图，同时保留训练下游GNN所需的基本信息。然而，现有方法要么集中于仅优化节点特征，要么努力独立学习节点特征和图结构生成器。它们无法明确利用原始图结构的信息，并未能为合成数据集构建可解释的图结构。为了解决这些问题，我们引入了一种名为\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})的新型框架。我们的方法突出之处在于

    arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
    
[^5]: 通过标记级别的不确定性量化检验大型语言模型的输出

    Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

    [https://arxiv.org/abs/2403.04696](https://arxiv.org/abs/2403.04696)

    提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。

    

    大型语言模型(LLMs)以产生错误的声明而臭名昭著。这种幻觉可能很危险，因为在生成的文本中偶尔出现的事实不准确可能会被整体上是事实的文本掩盖，这使得用户极其难以发现。利用LLMs的当前服务通常不提供检测不可靠生成的方式。在这里，我们旨在弥补这一空白。具体而言，我们提出了一种基于标记级别的不确定性量化的新型事实核查和幻觉检测流程。不确定性分数利用了神经网络或其层输出中包含的信息来检测不可靠的预测，并我们展示它们可以用于核查LLM输出中的各种声明。此外，我们提出了一种新型的标记级别不确定性量化方法，消除了对事实提出怀疑的影响。

    arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
    
[^6]: REPLAY: 对稀疏轨迹进行位置预测的人类移动时间变化规律建模

    REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories

    [https://arxiv.org/abs/2402.16310](https://arxiv.org/abs/2402.16310)

    该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。

    

    位置预测是根据历史用户移动轨迹来预测用户位置的技术。为了应对真实世界用户移动轨迹的固有稀疏问题，时空上下文被证明是非常有用的。现有的解决方案主要是将位置之间的时空距离纳入到移动轨迹中，要么通过将其作为附加输入提供给递归神经网络（RNNs），要么通过利用它们来寻找有信息的过去隐藏状态进行预测。然而，这种基于距离的方法未能捕捉人类移动的时间变化规律，例如，人类移动在早晨通常比其他时间更有规律；这暗示了实际时间戳的有用性。基于这一背景，我们提出了REPLAY，是一种通用的RNN架构，旨在捕捉时间变化的人类移动时间规律以进行位置预测。

    arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
    
[^7]: 深度神经网络总是理解并且这就是原因

    Deep Networks Always Grok and Here is Why

    [https://arxiv.org/abs/2402.15555](https://arxiv.org/abs/2402.15555)

    深度神经网络存在延迟泛化和延迟鲁棒性现象，在各种实际环境中普遍存在，并基于新的局部复杂度度量提供了解释。

    

    Grokking，或者延迟泛化，是指深度神经网络（DNN）在达到接近于零的训练误差后很长时间内才发生泛化的现象。先前的研究报告了在特定受控环境中出现grokking的情况，例如使用大范数参数初始化的DNN或者在算法数据集上训练的transformers。我们展示了grokking实际上更加普遍，并且在广泛的实际环境中呈现，例如在CIFAR10上训练的卷积神经网络（CNN）或者在Imagenette上训练的Resnet。我们引入了延迟鲁棒性的新概念，即DNN在插值和/或泛化之后对抗示例进行理解并变得鲁棒。我们针对DNN的输入-输出映射的局部复杂度提出了出现延迟泛化和延迟鲁棒性的解释。我们的局部复杂度衡量了DNN输入-输出映射的复杂程度。

    arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
    
[^8]: Transformer是表现力强大的，但是对于回归任务来说表现力足够吗？

    Transformers are Expressive, But Are They Expressive Enough for Regression?

    [https://arxiv.org/abs/2402.15478](https://arxiv.org/abs/2402.15478)

    Transformer在逼近连续函数方面存在困难，是否真正是通用函数逼近器仍有待考证

    

    Transformer已成为自然语言处理中至关重要的技术，在机器翻译和摘要等应用中表现出色。随着它们的广泛应用，一些研究尝试分析Transformer的表现力。神经网络的表现力指的是它能够逼近的函数类。一个神经网络是完全表现力的，如果它可以充当通用函数逼近器。我们尝试分析Transformer的表现力。与现有观点相反，我们的研究结果表明，Transformer在可靠逼近连续函数方面存在困难，依赖于具有可观区间的分段常数逼近。关键问题是：“Transformer是否真正是通用函数逼近器？”为了解决这个问题，我们进行了彻底的调查，通过实验提供理论见解和支持证据。我们的贡献包括了一个理论分析……（摘要未完整）

    arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
    
[^9]: NeuralThink: 在一般任务中进行外推的算法综合

    NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks

    [https://arxiv.org/abs/2402.15393](https://arxiv.org/abs/2402.15393)

    NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。

    

    虽然机器学习方法擅长模式识别，但在可扩展的算法方式上处理复杂的推理任务时仍然面临困难。最近的深度思维方法展现了学习可以外推的算法的潜力：在较小的环境中学习并在较大的环境中执行学到的算法。然而，这些工作局限于对称任务，即输入和输出的维度相同。为了填补这一空白，我们提出了 NeuralThink，一种新的递归架构，可以一贯地对对称和不对称任务进行外推，其中输入和输出的维度不同。我们提供了一个新颖的不对称任务外推基准。我们展示了 NeuralThink 在稳定地从较小的训练规模对大观测进行外推方面一直优于之前的最先进的深度思维架构。

    arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
    
[^10]: 关于神经网络中的最小深度

    On Minimal Depth in Neural Networks

    [https://arxiv.org/abs/2402.15315](https://arxiv.org/abs/2402.15315)

    本研究研究了神经网络中关于最小深度的问题，特别关注了ReLU神经网络的表达能力和最小深度与CPWL函数的关系。

    

    通过对ReLU神经网络表达能力以及与表示任何连续分段线性函数（CPWL）所需的最小深度相关的猜想的关系进行研究，本研究探讨了神经网络的表达能力特性。研究重点包括对求和和最大运算的最小深度表示，以及对多面体神经网络的探索。实验结果表明，对于求和运算，我们建立了关于操作数最小深度的充分条件以找到运算的最小深度。相反，关于最大运算，我们提供了全面的例子，证明仅依赖于操作数深度的充分条件，并不会暗示运算的最小深度。研究还考察了凸CPWL函数之间的最小深度关系。

    arXiv:2402.15315v1 Announce Type: new  Abstract: A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence. This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL). The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks. For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation. The study also examine the minimal depth relationship between convex CPWL functions. On polytope neural ne
    
[^11]: LLM对全球表示的意外影响

    Unintended Impacts of LLM Alignment on Global Representation

    [https://arxiv.org/abs/2402.15018](https://arxiv.org/abs/2402.15018)

    对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。

    

    在为面向用户的应用程序部署之前，开发人员通过各种程序（如从人类反馈中学习强化学习（RLHF）和直接偏好优化（DPO））将大型语言模型（LLMs）与用户偏好进行对齐。目前对这些程序的评估侧重于遵循指导、推理和真实性的基准。然而，人类偏好并非普遍，对特定偏好集进行对齐可能会产生意外影响。我们探讨了对三个全球表示维度：英语方言、多语言能力和全球各国意见的影响。我们的结果显示，当前的对齐程序在英语方言和全球意见之间产生差异。我们发现对齐提高了多种语言的能力。最后，我们讨论了导致这些意外影响的设计决策，并为更公平的建议。

    arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
    
[^12]: DyVal 2: 元探测代理动态评估大型语言模型

    DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents

    [https://arxiv.org/abs/2402.14865](https://arxiv.org/abs/2402.14865)

    本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。

    

    大型语言模型（LLMs）的评估引起了社区的极大关注，因为存在数据污染问题。现有工作设计了使用针对特定任务的明确定义算法的评估协议，这些协议无法轻松扩展到不同的场景。此外，当前的评估基准只能提供整体基准结果，不能支持对LLMs能力进行细粒度和多方面的分析。在本文中，我们提出了元探测代理（MPA），这是一种受心理测量学启发的通用动态评估协议，用于评估LLMs。 MPA 是 DyVal 2 的关键组件，自然地扩展了先前的 DyVal。 MPA 设计了探测和评判代理，以自动将原始评估问题转化为一个新问题，遵循心理测量理论在三个基本认知能力上的应用: 语言理解、问题解决和领域知识。

    arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
    
[^13]: 处理随机鞍点优化中的无界梯度

    Dealing with unbounded gradients in stochastic saddle-point optimization

    [https://arxiv.org/abs/2402.13903](https://arxiv.org/abs/2402.13903)

    提出一种简单而有效的正则化技术，稳定了随机鞍点优化过程中的梯度不断增长的问题，能够在无界梯度和噪声的情况下提供有意义的性能保证

    

    我们研究了用于寻找凸凹函数鞍点的随机一阶方法的性能。这类方法面临的一个举世闻名的挑战是，在优化过程中梯度可能会任意增长，这可能导致不稳定性和发散。在本文中，我们提出了一种简单而有效的正则化技术，稳定了迭代并产生了有意义的性能保证，即使定义域和梯度噪声随迭代的规模线性变化（因此可能是无界的）。除了提供一系列一般性结果外，我们还将我们的算法应用到强化学习中的一个具体问题，该问题导致在不需要有关偏置跨度先验知识的情况下，找到平均奖励MDP中接近最优策略的性能保证。

    arXiv:2402.13903v1 Announce Type: new  Abstract: We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.
    
[^14]: 连续葡萄糖监测和维护的神经控制系统

    Neural Control System for Continuous Glucose Monitoring and Maintenance

    [https://arxiv.org/abs/2402.13852](https://arxiv.org/abs/2402.13852)

    引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。

    

    精确的葡萄糖水平管理对于糖尿病患者至关重要，可以避免严重并发症。本研究引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，利用微分预测控制。我们的系统受到复杂神经策略和可区分建模的指导，实时动态调整胰岛素输送，增强葡萄糖优化。这种端到端方法最大化效率，确保个性化护理和改善健康结果，如经验发现所证实。

    arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
    
[^15]: 在何时以及如何：学习可识别的潜在状态进行非平稳时间序列预测

    When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting

    [https://arxiv.org/abs/2402.12767](https://arxiv.org/abs/2402.12767)

    提出了一种名为IDEA的模型，通过学习可识别的潜在状态检测时间序列数据中的分布变迁，并进一步分离平稳和非平稳的潜在状态。

    

    时间分布的转移在时间序列数据中是普遍存在的。其中一种最流行的方法假定时间分布的转移是均匀发生的，以区分平稳和非平稳的依赖关系。然而，这个假设很难满足，因为我们不知道分布何时发生转移。为了解决这个问题，我们提出了学习可识别的潜在状态（IDEA）来检测分布何时发生转移。除此之外，我们进一步通过充分观察假设来分离平稳和非平稳的潜在状态，学习潜在状态的变化方式。具体来说，我们将因果过程形式化为与环境不相关的稳定变量和与环境相关的非平稳变量。在温和的条件下，我们展示了潜在环境和稳定/非稳定变量是可识别的。基于这些理论，我们设计了IDEA模型，该模型结合了自回归隐马尔科夫模型。

    arXiv:2402.12767v1 Announce Type: new  Abstract: Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated station- ary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov m
    
[^16]: 具有（低精度）多项式逼近的神经网络：准确性提高的新见解和技术

    Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement

    [https://arxiv.org/abs/2402.11224](https://arxiv.org/abs/2402.11224)

    本文揭示了神经网络的多项式逼近作为一种独立对象的研究，发现PANN对某些类型的逼近误差...

    

    在神经网络中，用多项式逼近替换非多项式函数（例如非线性激活函数，如ReLU）是隐私保护机器学习中的标准做法。本文中称之为神经网络的多项式逼近（PANN）的结果神经网络与先进的密码系统兼容，实现隐私保护模型推断。利用“高精度”逼近，最先进的PANN提供了与基础骨干模型相似的推断准确性。然而，关于逼近的影响知之甚少，并且现有文献通常是通过实证确定所需的逼近精度。在本文中，我们开始对PANN进行独立对象的研究。具体来说，我们的贡献是双重的。首先，我们解释了PANN中近似误差的影响。特别是，我们发现PANN容易受到某种类型的...

    arXiv:2402.11224v1 Announce Type: new  Abstract: Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type o
    
[^17]: CHEMREASONER：使用量子化学反馈在大型语言模型的知识空间中进行启发式搜索

    CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback

    [https://arxiv.org/abs/2402.10980](https://arxiv.org/abs/2402.10980)

    通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索

    

    arXiv:2402.10980v1 类型公告：跨领域 摘要：发现新的催化剂对于设计新的更高效的化学过程至关重要，以实现向可持续未来的过渡。我们引入了一种人工智能引导的计算筛选框架，将语言推理与基于量子化学的三维原子表示的反馈统一起来。我们的方法将催化剂发现构建为一个不确定环境，其中一个代理通过大型语言模型（LLM）推导的假设与基于原子图神经网络（GNN）的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤确定的催化剂经过基于空间定向、反应途径和稳定性的结构评估。基于吸附能和势垒的评分函数引导在LLM的知识空间中向能量有利、高效的催化剂探索。我们引入了可以自动规划的方法

    arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
    
[^18]: 通过专家加权来衡量和减少LLM在没有黄金标准答案的情况下的虚构

    Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting

    [https://arxiv.org/abs/2402.10412](https://arxiv.org/abs/2402.10412)

    提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。

    

    LLM幻觉，即生成事实不正确但看似令人信服的答案，目前是LLM可信度和可靠性的主要威胁。解决这一复杂问题的第一步是对其进行衡量。然而，现有的幻觉度量标准需要具有具有黄金标准答案的基准数据集，即人类编写的“最佳”或“正确”答案。这种要求使幻觉测量成本高昂，并容易出现人为误差。在这项工作中，我们提出了通过加权LLM对事实性进行评估（FEWL），这是第一个专门为金标准答案缺失时设计的幻觉度量标准。FEWL利用了现成的LLM答案作为黄金标准答案的代理。关键挑战是如何有效地量化参考LLM的专业知识。我们展示FEWL具有一定的理论保证，并在实证中证明它更准确。度量虚构。

    arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
    
[^19]: 逐层近端回放：一种在线连续学习的近端点方法

    Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning

    [https://arxiv.org/abs/2402.09542](https://arxiv.org/abs/2402.09542)

    这项工作针对在线连续学习中经验回放造成的优化不稳定问题进行了改进，提出了一种逐层近端回放（LPR）方法，通过优化几何的修改来平衡新数据和回放数据的学习，从而改善了回放式在线连续学习方法的准确性。

    

    在在线连续学习中，神经网络逐步从非独立同分布的数据流中学习。几乎所有的在线连续学习方法都使用经验回放来同时防止灾难性遗忘和过度拟合先前的数据。我们的工作展示了这种方法的一个局限性：使用经验回放训练的网络往往具有不稳定的优化轨迹，影响其整体准确度。令人惊讶的是，即使回放缓冲区存储了所有先前的训练样本，这些不稳定性仍然存在，这表明这个问题与灾难性遗忘是无关的。我们通过对优化几何的简单修改来最小化这些不稳定性。我们的解决方案，逐层近端回放（LPR），在只允许逐渐改变过去数据的隐藏激活的同时，平衡了从新数据和回放数据中的学习。我们证明了LPR在基于回放的在线连续学习方法中持续改进。

    arXiv:2402.09542v1 Announce Type: new  Abstract: In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning me
    
[^20]: COLD-Attack: 用于具有隐秘性和可控性的LLM越狱

    COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

    [https://arxiv.org/abs/2402.08679](https://arxiv.org/abs/2402.08679)

    本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。

    

    最近对大型语言模型（LLMs）进行越狱的注意力越来越多。为了全面评估LLM的安全性，有必要考虑具有不同属性的越狱，例如上下文连贯性以及情感/风格变化，因此研究可控性越狱是有益的，即如何对LLM攻击进行控制。在本文中，我们正式形式化了可控性攻击生成问题，并建立了该问题与可控文本生成之间的新型关联，这是自然语言处理中一个被广泛探索的主题。基于这种关联，我们改进了能量限制解码与Langevin动力学（COLD）的算法，这是一种在可控文本生成中的高效算法，并引入了COLD-Attack框架，该框架统一且自动化地搜索各种控制要求下的对抗性LLM攻击，例如流畅性、隐秘性、情感和左右连贯性。

    Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
    
[^21]: 人本智能产品原型设计中的无代码AutoML：概念框架、潜力与限制

    Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations

    [https://arxiv.org/abs/2402.07933](https://arxiv.org/abs/2402.07933)

    本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。

    

    本文评估了无代码AutoML作为解决人工智能产品原型设计中的挑战的方案，这些产品的特点是不可预测性和对非专家的不可访问性，并提出了一个概念框架。人工智能产品的复杂性阻碍了无缝执行和跨学科合作，这对于人本智能产品至关重要。与行业和创新相关，它影响战略决策和投资风险的降低。目前的方法对人工智能产品想法的潜力和可行性提供了有限的见解。本研究采用设计科学研究的方法，识别挑战，并通过提出无代码AutoML的概念框架来解决这些挑战。案例研究证实了其对非专家的支持潜力，提供了一种结构化的人工智能产品开发方法。该框架有助于实现可访问和可解释的原型设计，使学术界、管理者和决策者受益。无代码AutoML的战略整合

    This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
    
[^22]: 与语言模型的反馈循环推动上下文内奖励欺骗

    Feedback Loops With Language Models Drive In-Context Reward Hacking

    [https://arxiv.org/abs/2402.06627](https://arxiv.org/abs/2402.06627)

    与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。

    

    语言模型对外部世界产生影响：它们查询可以读写网页的API，生成能够影响人类行为的内容，以及作为自主代理运行系统命令。这些互动形成了反馈循环：语言模型的输出影响世界，反过来又影响后续的语言模型输出。在这项工作中，我们展示了反馈循环可能导致上下文内奖励欺骗(ICRH)，即测试时的语言模型在优化（可能隐含的）目标的同时，产生负面副作用。例如，考虑一个被部署用于增加Twitter参与度的语言模型代理；语言模型可能在上下文窗口中检索其以前的推文，并使推文更具争议性，从而增加参与度，但也增加了有毒性。我们确定并研究了导致ICRH的两个过程：输出优化和策略优化。对于这些过程，静态数据集上的评估是不足够的-他们无法捕捉到反馈效应，也不能捕捉到最有害的行为。为此，我们提供了...

    Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
    
[^23]: S$\Omega$I: 基于分数的O-INFORMATION估计

    S$\Omega$I: Score-based O-INFORMATION Estimation

    [https://arxiv.org/abs/2402.05667](https://arxiv.org/abs/2402.05667)

    S$\Omega$I是一种新的信息论量度，可以计算多变量系统中的协同-冗余平衡，突破了传统量度的局限性，并在合成数据上得到了验证。

    

    科学数据和复杂的多变量系统的分析需要捕捉多个随机变量之间关系的信息量。最近，新的信息论量度已被发展出来，以克服传统量度（如互信息）的局限性，后者只考虑成对交互作用。其中，信息协同和冗余的概念对于理解变量之间的高阶依赖关系至关重要。基于这一概念的最著名和多用途的量度之一是O-information，它提供了一种清晰且可扩展的方式来量化多变量系统中的协同-冗余平衡。然而，它在实际应用中受限于简化情况。在本研究中，我们引入了S$\Omega$I，该方法首次允许计算O-information而不受对系统的限制性假设。我们的实验证实了我们的方法在合成数据上的有效性，证明了S$的有效性。

    The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$
    
[^24]: 在策略镜像下降中元学习镜像映射

    Meta-learning the mirror map in policy mirror descent

    [https://arxiv.org/abs/2402.05187](https://arxiv.org/abs/2402.05187)

    该论文通过实证研究发现，传统的镜像映射选择（NPG）在标准基准环境中常常导致不理想的结果。通过元学习方法，找到了更高效的镜像映射，提升了性能。

    

    策略镜像下降（PMD）是强化学习中的一种流行框架，作为一种统一视角，它包含了许多算法。这些算法是通过选择一个镜像映射而导出的，并且具有有限时间的收敛保证。尽管它很受欢迎，但对PMD的全面潜力的探索是有限的，大部分研究集中在一个特定的镜像映射上，即负熵，从而产生了著名的自然策略梯度（NPG）方法。目前的理论研究还不确定镜像映射的选择是否会对PMD的有效性产生重大影响。在我们的工作中，我们进行了实证研究，证明了传统的镜像映射选择（NPG）在几个标准基准环境中经常产生不理想的结果。通过应用元学习方法，我们确定了更高效的镜像映射，提高了性能，无论是平均性能还是最佳性能。

    Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along th
    
[^25]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^26]: 使用神经最优传输估计分布的重心

    Estimating Barycenters of Distributions with Neural Optimal Transport

    [https://arxiv.org/abs/2402.03828](https://arxiv.org/abs/2402.03828)

    本研究借助最优传输的对偶形式，提出了一种新的可扩展方法，用于估计分布的重心。通过神经最优传输求解器，我们实现了双层对抗学习并适用于一般成本函数。这个方法的关键优势在于不需要使用三层优化，并且适用于多种成本函数。我们还建立了理论误差界，并在示例场景和图像数据上展示了方法的应用性和有效性。

    

    鉴于一组概率分布，从业者有时需要找到一种“平均”分布，以充分聚合参考分布。一个理论上有吸引力的平均分布概念是Wasserstein重心，而我们的工作重点就是它。通过建立在最优传输的对偶形式之上，我们提出了一种新的可扩展方法来解决Wasserstein重心问题。我们的方法基于最近的神经最优传输求解器：它具有双层对抗学习目标，并适用于一般的成本函数。这些是我们方法的关键优势，因为典型的利用重心任务的对抗性算法往往利用三层优化，并且主要集中在二次成本上。我们还为我们提出的方法建立了理论误差界，并展示了它在示例场景和图像数据设置上的适用性和有效性。

    Given a collection of probability measures, a practitioner sometimes needs to find an "average" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.
    
[^27]: $C^*$-代数机器学习：迈向新的方向

    $C^*$-Algebraic Machine Learning: Moving in a New Direction

    [https://arxiv.org/abs/2402.02637](https://arxiv.org/abs/2402.02637)

    $C^*$-代数机器学习是将$C^*$-代数与机器学习结合的新研究方向，它通过统一现有的学习策略，并构建更多元化和信息丰富的数据模型的新框架，为机器学习提供了一种新的方法。

    

    机器学习与数学的几个领域（如统计学、概率论和线性代数）有着长期的合作传统。我们提出了机器学习研究的一个新方向：$C^*$-代数机器学习，这是$C^*$-代数和机器学习之间的交流和相互滋养。$C^*$-代数是复数空间的自然推广的数学概念，它使我们能够统一现有的学习策略，并构建一个更多元化和信息丰富的数据模型的新框架。我们解释了在机器学习中使用$C^*$-代数的原因和方法，并提供了在核方法和神经网络背景下设计$C^*$-代数学习模型的技术考虑。此外，我们讨论了$C^*$-代数机器学习中的开放问题和挑战，并提出了我们对未来发展和应用的思考。

    Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications.
    
[^28]: TimeSiam：一种用于孪生时间序列建模的预训练框架

    TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling

    [https://arxiv.org/abs/2402.02475](https://arxiv.org/abs/2402.02475)

    TimeSiam是一种用于时间序列建模的预训练框架，通过使用Siamese网络和简单的数据增强方法，能够捕捉时间序列数据的内在时间相关性，并学习内部时序依赖的表示。

    

    最近，时间序列的预训练引起了广泛关注，因为它能够降低标注的成本并受益于各种下游任务。先前的方法主要基于在视觉或语言领域广为认可的预训练技术，如遮蔽建模和对比学习。然而，随机遮蔽时间序列或计算序列之间的相似性将导致丢失或忽视时间序列数据中关键的内在时间相关性。为了强调时间相关性建模，本文提出了一种名为TimeSiam的简单但有效的自监督预训练框架，用于基于孪生网络的时间序列。具体而言，TimeSiam对孪生编码器进行预训练，以捕捉随机采样的过去和当前子序列之间的内在时间相关性。通过简单的数据增强方法（例如遮蔽），TimeSiam可以从多样化的增强子序列中受益，并通过从过去到当前的重建来学习内部时序依赖的表示。此外，可学习的...

    Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnabl
    
[^29]: 重思出发点：通过协作预训练增强联邦学习的性能和公平性

    Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training

    [https://arxiv.org/abs/2402.02225](https://arxiv.org/abs/2402.02225)

    本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。

    

    大多数现有的联邦学习方法假设训练从一个随机初始化的模型开始。最近的研究实证了利用预训练模型可以为联邦学习提供有益的初始化。在本文中，我们提出了一种协作预训练方法CoPreFL，该方法通过策略性地设计一个预训练模型，为任何下游联邦学习任务提供良好的初始化。我们的预训练算法的关键思想是模仿下游分布式场景的元学习过程，使其能够适应任何未知的联邦学习任务。CoPreFL的预训练优化过程也在平均性能和公平性之间取得了平衡，旨在通过智能初始化来解决下游联邦学习任务中的竞争挑战。大量实验结果验证了我们的预训练方法为任何未知的下游联邦学习任务提供了可靠的初始化，从而提高了平均性能。

    Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe
    
[^30]: 视觉转换器中关键信息的流形表示

    A Manifold Representation of the Key in Vision Transformers

    [https://arxiv.org/abs/2402.00534](https://arxiv.org/abs/2402.00534)

    该论文提出了一种在视觉转换器中将关键信息与查询和数值分离并采用流形表示的方法，实验证明这种方法能够提升模型性能，并在ImageNet-1K和COCO数据集上取得了积极的结果。

    

    视觉转换器通过堆叠多个注意力块实现了多头自注意力（MSA）。在这些块中，查询、关键信息和数值通常纠缠在一起，并通过单个共享线性变换生成。本文探索了将关键信息从查询和数值中解耦，并为关键信息采用流形表示的概念。我们的实验表明，解耦并赋予关键信息流形结构可以提升模型性能。具体而言，ViT-B在ImageNet-1K数据集上的top-1准确率提高了0.87％，而Swin-T则在top-1准确率上提高了0.52％，使用了八个流形关键图。我们的方法在COCO数据集上的目标检测和实例分割任务中也取得了积极的结果。通过详细的消融研究，我们证明了这些性能提升不仅仅是由于增加了更多参数和计算的简单性。未来的研究可以探索减少计算复杂度的策略。

    Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the
    
[^31]: 通过权重集成专家的多任务模型合并

    Merging Multi-Task Models via Weight-Ensembling Mixture of Experts

    [https://arxiv.org/abs/2402.00433](https://arxiv.org/abs/2402.00433)

    通过权重集成专家的方法可以将训练在不同任务上的Transformer模型合并为一个统一的模型，通过动态整合共享和特定任务的知识来提供更灵活的解决方案。

    

    将训练在不同任务上的各种特定任务的Transformer模型合并为一个统一的模型，可以同时执行所有任务。以任务算术为例的先前方法已被证明既有效又可扩展。现有方法主要集中在寻找原始模型参数空间内的静态最优解。一个显著的挑战是减轻不同模型参数之间的干扰，这可能会严重削弱性能。在本文中，我们提出了一种方法，将大多数参数合并在一起，同时将Transformer层的MLP扩展为权重集成专家（MoE）模块，该模块可以根据输入动态地整合共享和特定任务的知识，从而提供一个更灵活的解决方案，可以适应每个实例的特定需求。我们的关键见解是通过识别和分离共享知识和特定任务知识，然后动态地整合它们，

    Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, 
    
[^32]: 使用ODE方法进行带有马尔可夫噪声的随机逼近和强化学习

    The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise

    [https://arxiv.org/abs/2401.07844](https://arxiv.org/abs/2401.07844)

    本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。

    

    随机逼近是一类通过迭代、增量和随机更新向量的算法，包括随机梯度下降和时序差分学习。分析随机逼近算法的一个主要挑战是确保其稳定性，即证明随机向量迭代几乎必定有界。本文将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，大大提高了其在强化学习中的适用性，特别是那些具有线性函数逼近和资格迹的离策略强化学习算法。我们的分析的核心在于少数函数的渐进变化速率下降，这一点由大数定律和常用的V4 Lyapunov漂移条件隐含，并在马尔可夫链是有限且不可约时显然成立。

    Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
    
[^33]: RepairLLaMA：高效表示和微调适配器用于程序修复

    RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair

    [https://arxiv.org/abs/2312.15698](https://arxiv.org/abs/2312.15698)

    高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。

    

    自动程序修复（APR）随着大型语言模型（LLMs）的出现已有了显著发展。对于程序修复进行LLMs的微调是最近研究的一个新领域，有许多未被探索的维度。现有工作大多使用简单的代码表示对LLMs进行微调，并在能够微调更大型LLMs的能力方面存在根本性局限。为解决这个问题，我们提出了RepairLLaMA，一个结合了1）用于APR的代码表示和2）最先进的参数高效的LLM微调技术LoRA的新型程序修复方法。这使得RepairLLaMA产生了一个高效的“程序修复适配器”，用于使用语言模型修复错误。我们的实验证明了这两个概念的有效性。首先，使用具有程序修复特定代码表示的微调适配器使模型能够使用有意义的修复信号。其次，参数高效的微调有助于微调...

    arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
    
[^34]: FRAPP'E：一个用于后处理的群体公平性框架

    FRAPP\'E: A Group Fairness Framework for Post-Processing Everything

    [https://arxiv.org/abs/2312.02592](https://arxiv.org/abs/2312.02592)

    提出了一个将任何正则化的处理中方法转化为后处理方法的框架，适用于更广泛的问题设置，保持了良好的公平错误权衡，并且可能提高之前方法的效力

    

    尽管在处理中实现了有前途的公平错误权衡，但群体公平性的处理方法无法在许多计算资源有限或无法访问预测模型训练管道的实际应用中使用。在这些情况下，后处理是一个可行的替代方法。然而，当前方法专为特定问题设置和公平性定义而设计，因此不如处理中方法广泛适用。在这项工作中，我们提出了一个框架，将任何正则化的处理中方法转化为后处理方法。这一方法提供了一种获得更广泛问题设置的后处理技术的途径，远超过以前的后处理文献。我们理论上并通过大量实验展示，我们的框架保留了处理中实现的良好公平错误权衡，并且能够提高先前方法的效力。

    arXiv:2312.02592v2 Announce Type: replace  Abstract: Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior p
    
[^35]: HelmFluid：学习Helmholtz动力学进行可解释的流体预测

    HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction

    [https://arxiv.org/abs/2310.10565](https://arxiv.org/abs/2310.10565)

    本文提出了HelmFluid，一个精确且可解释的流体预测器。通过学习Helmholtz动力学，将流体动力学分解为更可解的无旋和无散部分，并结合多尺度多头积分架构进行集成，HelmFluid能够更准确地预测流体的未来行为。

    

    流体预测是一个长期存在的挑战，由于固有的高维非线性动力学。先前的方法通常利用深度模型的非线性建模能力直接估计未来预测的速度场。然而，直接学习表面速度场而跳过固有的物理特性将导致模型难以生成精确或具有物理可靠性的结果。本文提出了HelmFluid，针对流体的精确和可解释的预测器。受Helmholtz定理的启发，我们设计了HelmDynamics模块，用于学习Helmholtz动力学，将流体动力学分解为更可解的无旋和无散部分，物理上对应于流体的势函数和流函数。通过将HelmDynamics模块嵌入到多尺度多头积分架构中，HelmFluid可以在多个空间尺度上沿时间维度集成学到的Helmholtz动力学，从而产生未来的流体。与先前的方法相比，HelmFluid能够更好地捕捉流体的物理特性，并实现准确的流体预测。

    Fluid prediction is a long-standing challenge due to the intrinsic high-dimensional non-linear dynamics. Previous methods usually utilize the non-linear modeling capability of deep models to directly estimate velocity fields for future prediction. However, skipping over inherent physical properties but directly learning superficial velocity fields will overwhelm the model from generating precise or physics-reliable results. In this paper, we propose the HelmFluid toward an accurate and interpretable predictor for fluid. Inspired by the Helmholtz theorem, we design a HelmDynamics block to learn Helmholtz dynamics, which decomposes fluid dynamics into more solvable curl-free and divergence-free parts, physically corresponding to potential and stream functions of fluid. By embedding the HelmDynamics block into a Multiscale Multihead Integral Architecture, HelmFluid can integrate learned Helmholtz dynamics along temporal dimension in multiple spatial scales to yield future fluid. Compared 
    
[^36]: Lie神经元：半单Lie代数的伴随等变神经网络

    Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras

    [https://arxiv.org/abs/2310.04521](https://arxiv.org/abs/2310.04521)

    本文提出了一种Lie神经元网络，能够以任何半单Lie代数数据为输入，通过伴随操作使其具有等变性。通过推广向量神经元网络和引入新的层，该网络在各个领域具有广泛的适用性和竞争性能。

    

    本文提出了一种等变神经网络，将数据作为输入，该数据存在于任何半单Lie代数中。对应的群通过伴随操作作用于Lie代数上，使得我们提出的网络具有伴随等变性。我们的框架将简单的$\mathrm{SO}(3)$-等变网络——向量神经元从3维欧几里得空间推广到Lie代数空间，利用Killing形式的不变性质。此外，我们还提出了新颖的Lie括号层和几何通道混合层来扩展建模能力。实验在多个任务上对$\mathfrak{so}(3)$和$\mathfrak{sl}(3)$ Lie代数进行了验证，包括拟合等变和不变函数、学习系统动力学、点云配准和基于单应性的形状分类。我们提出的等变网络在各个领域都表现出广泛的适用性和竞争性能。

    This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
    
[^37]: 整合多模态数据用于复杂动力学的联合生成建模

    Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics

    [https://arxiv.org/abs/2212.07892](https://arxiv.org/abs/2212.07892)

    提出了一种基于多模态变分自动编码器的算法框架，用于联合生成建模复杂动力学，引导重建模型训练。

    

    许多科学中感兴趣的系统本质上是非线性动力系统。通常，我们通过时间序列测量来访问这些系统。这些时间序列可能由离散随机变量而非连续测量组成，或者可能由同时观察到的多个数据模态的测量组成。在神经科学中，我们可能除了脉冲计数和连续生理记录外，还有行为标签。虽然现在关于深度学习用于动态系统重建的文献正在蓬勃发展，但多模态数据集成在这个背景下几乎没有被考虑。在这里，我们提供了一个基于多模态变分自动编码器的高效灵活算法框架，用于生成稀疏教师信号，指导重建模型的训练，利用了DSR训练技术的最新进展。

    arXiv:2212.07892v2 Announce Type: replace  Abstract: Many, if not most, systems of interest in science are naturally described as nonlinear dynamical systems. Empirically, we commonly access these systems through time series measurements. Often such time series may consist of discrete random variables rather than continuous measurements, or may be composed of measurements from multiple data modalities observed simultaneously. For instance, in neuroscience we may have behavioral labels in addition to spike counts and continuous physiological recordings. While by now there is a burgeoning literature on deep learning for dynamical systems reconstruction (DSR), multimodal data integration has hardly been considered in this context. Here we provide such an efficient and flexible algorithmic framework that rests on a multimodal variational autoencoder for generating a sparse teacher signal that guides training of a reconstruction model, exploiting recent advances in DSR training techniques. 
    
[^38]: 图像水印鲁棒性的基准测试

    Benchmarking the Robustness of Image Watermarks. (arXiv:2401.08573v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.08573](http://arxiv.org/abs/2401.08573)

    本研究提出了一种新颖的基准测试方法WAVES，用于评估图像水印的鲁棒性。通过整合检测和识别任务，并进行多样化压力测试，我们揭示了以前未被发现的水印脆弱性。

    

    本文研究了图像水印技术的弱点。我们提出了一种新颖的基准测试方法WAVES（通过增强的压力测试进行水印分析），用于评估水印的鲁棒性，克服了现有评估方法的局限性。WAVES整合了检测和识别任务，并建立了一个由多样化压力测试组成的标准化评估协议。WAVES中的攻击范围从传统的图像失真到扩散和对抗攻击的高级和新颖变体。我们的评估考察了两个关键维度：图像质量降低程度和攻击后水印检测的有效性。我们开发了一系列性能与质量2D图，变化基于几种突出的图像相似度度量，然后用一种启发式的新颖方法将它们聚合，从而为水印的鲁棒性和攻击能力提供一个全面的画面。我们的综合评估揭示了以前未被发现的脆弱性。

    This paper investigates the weaknesses of image watermarking techniques. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel benchmark for assessing watermark robustness, overcoming the limitations of current evaluation methods.WAVES integrates detection and identification tasks, and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced and novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. We develop a series of Performance vs. Quality 2D plots, varying over several prominent image similarity metrics, which are then aggregated in a heuristically novel manner to paint an overall picture of watermark robustness and attack potency. Our comprehensive evaluation reveals previously undetected vulnerabilities of 
    
[^39]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^40]: WinNet:具有窗口增强周期提取和交互的时间序列预测

    WinNet:time series forecasting with a window-enhanced period extracting and interacting. (arXiv:2311.00214v1 [cs.LG])

    [http://arxiv.org/abs/2311.00214](http://arxiv.org/abs/2311.00214)

    WinNet是一种用于时间序列预测的CNN-based模型，通过窗口增强的周期提取和交互，在捕捉长期和短期周期性方面具有高准确度和简单结构。在九个基准数据集上的实验结果表明，WinNet可以实现优于CNN、MLP、Transformer方法的最新性能，并具有较低的计算复杂度。

    

    最近，基于Transformer的方法显著提高了时序预测的最新结果，但它们面临高计算成本和无法捕捉时间序列的长期和短期周期的问题。我们提出了一种高度准确且简单结构的基于CNN的模型WinNet，用于长期时间序列预测任务，包括：(i) Inter-Intra Period Encoder (I2PE) 将1D序列转换为二维张量，根据预定义的周期窗口具有长期和短期周期性，(ii) Two-Dimensional Period Decomposition (TDPD) 对模型进行周期-趋势和振荡项建模，(iii) Decomposition Correlation Block (DCB) 利用周期-趋势和振荡项的相关性，通过CNNs支持预测任务。在九个基准数据集上的结果表明，WinNet在CNN、MLP和Transformer方法上可以实现SOTA性能和较低的计算复杂性。WinNet为基于CNN的元方法提供了潜在的可能性。

    Recently, Transformer-based methods have significantly improved state-of-the-art time series forecasting results, but they suffer from high computational costs and the inability to capture the long and short periodicity of time series. We present a highly accurate and simply structured CNN-based model for long-term time series forecasting tasks, called WinNet, including (i) Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with long and short periodicity according to the predefined periodic window, (ii) Two-Dimensional Period Decomposition (TDPD) to model period-trend and oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage the correlations of the period-trend and oscillation terms to support the prediction tasks by CNNs. Results on nine benchmark datasets show that the WinNet can achieve SOTA performance and lower computational complexity over CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the CNN-based met
    
[^41]: 通过估计数据分布比例的离散扩散语言建模

    Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])

    [http://arxiv.org/abs/2310.16834](http://arxiv.org/abs/2310.16834)

    本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。

    

    尽管扩散模型在许多生成建模任务中具有突破性的性能，但在自然语言等离散数据领域中却表现不佳。关键是，标准的扩散模型依赖于成熟的得分匹配理论，但是将其推广到离散结构并没有取得相同的经验收益。在本文中，我们通过提出得分熵，一种新颖的离散得分匹配损失，来弥补这个差距，它比现有方法更稳定，可以形成最大似然训练的ELBO，并且可以通过去噪变体高效优化。我们将我们的得分熵离散扩散模型（SEDD）扩展到GPT-2的实验设置中，实现了极具竞争力的似然度，同时引入了独特的算法优势。特别是，在比较大小相似的SEDD和GPT-2模型时，SEDD达到了可比较的困惑度（通常在基线的+$10\%$内，并且有时超过基线）。此外，SEDD模型学到了...

    Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
    
[^42]: 通过观测变量的分组使因果表示学习可辨识化

    Causal Representation Learning Made Identifiable by Grouping of Observational Variables. (arXiv:2310.15709v1 [stat.ML])

    [http://arxiv.org/abs/2310.15709](http://arxiv.org/abs/2310.15709)

    该论文研究了因果表示学习（CRL），提出了一种基于观测变量分组的新型弱约束的可辨识性方法，该方法不依赖于时间结构、干预或监督，具有实际应用的意义。

    

    当今很有意义的话题是因果表示学习（Causal Representation Learning，简称CRL），其目标是以数据驱动的方式学习隐藏特征的因果模型。不幸的是，CRL存在严重的不适问题，因为它结合了表示学习和因果发现这两个容易不适问题。然而，要找到能保证唯一解的实际可识别性条件对于其实际应用非常重要。目前大多数方法都基于对潜在因果机制的假设，比如时间因果性、监督或干预的存在；在实际应用中，这些假设可能过于限制性。在这里，我们通过对观测混合表现出合适的变量分组的新型弱约束，展示了可辨识性。我们还提出了一种与模型一致的新型自我监督估计框架。

    A topic of great current interest is Causal Representation Learning (CRL), whose goal is to learn a causal model for hidden features in a data-driven manner. Unfortunately, CRL is severely ill-posed since it is a combination of the two notoriously ill-posed problems of representation learning and causal discovery. Yet, finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most approaches so far have been based on assumptions on the latent causal mechanisms, such as temporal causality, or existence of supervision or interventions; these can be too restrictive in actual applications. Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision. The approach is based assuming the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework consistent with the model, 
    
[^43]: 通过平衡教师和研究者的激励，探索适应性实验促进持续改进的机会

    Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])

    [http://arxiv.org/abs/2310.12324](http://arxiv.org/abs/2310.12324)

    适应性实验为持续课程改进提供了机会，通过动态部署最有效的条件以满足学生需求。

    

    随机实验比较不同教学策略的机会可以为教师的决策提供有用的经验证据。然而，传统实验缺乏清晰简明的使用数据快速增加实验学生获得最佳条件机会的途径。受领先科技公司在产品开发中使用机器学习和实验的启示，我们探讨了如何利用适应性实验来持续改进课程。在适应性实验中，不同的条件将被应用于学生身上，数据将被分析并用于改变未来学生的学习体验。可以使用机器学习算法来识别哪些行动可以更有希望改善学生的体验或结果。然后，该算法可以动态地将最有效的条件应用于未来的学生，从而更好地满足学生的需求。我们通过实例说明了这种方法的应用。

    Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
    
[^44]: GTA：一种面向几何的多视图Transformer的注意力机制

    GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])

    [http://arxiv.org/abs/2310.10375](http://arxiv.org/abs/2310.10375)

    提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。

    

    随着transformers对输入标记的排列具有等变性，对标记的位置信息进行编码对许多任务是必要的。然而，由于现有的位置编码方案最初是为自然语言处理任务设计的，对于通常在其数据中表现出不同结构特性的视觉任务来说，它们的适用性值得怀疑。我们认为现有的位置编码方案对于3D视觉任务来说是次优的，因为它们不尊重其底层的3D几何结构。基于这个假设，我们提出了一种面向几何的注意力机制，它将标记的几何结构编码为由查询和键值对之间的几何关系所确定的相对变换。通过在稀疏宽基线多视图设置中评估多个新颖视图合成（NVS）数据集，我们展示了我们的注意力机制——几何变换注意力（GTA）如何提高了最先进的Transformer的学习效率和性能。

    As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
    
[^45]: 响铃！概念去除方法在扩散模型中的可靠性如何？

    Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?. (arXiv:2310.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10012](http://arxiv.org/abs/2310.10012)

    本文研究了对于文本到图像合成的扩散模型中的潜在滥用问题的安全措施的有效性，并提出了一个用于评估的新颖概念检索算法。我们引入了一个名为Ring-A-Bell的模型无关的红队工具，可以事先准备整个评估过程，而无需先验知识。

    

    文本到图像(T2I)合成的扩散模型，如稳定的扩散(SD)，最近展示出了生成高质量内容的卓越能力。然而，这一进展引发了对潜在滥用的几个关注，特别是在创建受版权限制、禁止和受限内容，或者不适宜工作的(NSFW)图片方面。虽然已经采取了一些措施来缓解这些问题，例如在评估阶段实施安全过滤器或通过微调模型来消除不受欢迎的概念或风格，但这些安全措施在处理各种提示方面的有效性仍然很少被探索。在这项工作中，我们旨在通过提出一种用于评估的新颖概念检索算法来研究这些安全机制。我们引入了Ring-A-Bell，这是一个面向T2I扩散模型的模型无关的红队工具，整个评估可以在没有目标模型先验知识的情况下提前准备好。具体来说，Ring-A-Bell首先对激励进行分解，然后通过去除候选概念和计算特定概念的相关度来设计筛选机制。

    Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first pe
    
[^46]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^47]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^48]: 利用动态推理来利用Transformer激活稀疏性

    Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])

    [http://arxiv.org/abs/2310.04361](http://arxiv.org/abs/2310.04361)

    本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。

    

    Transformer模型尽管表现出色，但由于其高计算需求，常面临实际限制。与此同时，先前的研究揭示了这些模型中的显著激活稀疏性，表明存在冗余计算。本文提出了一种称为动态稀疏化Transformer推理（DSTI）的方法，通过强制激活稀疏性并将密集模型转换为其稀疏的专家混合（MoE）版本，从而极大地降低Transformer模型的推理成本。我们证明，在推理过程中可以训练出成功预测每个专家相对贡献的小型门控网络。此外，我们引入了一种动态确定每个令牌执行的专家数量的机制。DSTI可以应用于任何基于Transformer的体系结构，并对准确性影响微乎其微。针对BERT-base分类模型，我们降低了推理成本。

    Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
    
[^49]: 通过双层ReLU神经网络实现可证明的多任务表示学习

    Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])

    [http://arxiv.org/abs/2307.06887](http://arxiv.org/abs/2307.06887)

    通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。

    

    特征学习是神经网络实际成功的关键，然而如何以及为何发生特征学习仍然难以解释。最近的理论研究表明，在用梯度下降方法优化的浅层神经网络上可以学习有意义的特征，扩展了我们对于神经切向核或随机特征范例中微不足道的特征学习的了解。然而，在实践中，神经网络越来越经常地同时训练多个具有不同损失函数的任务，并且这些先前的分析并不适用于这种情况。在多任务学习设置中，各种研究已经表明简单线性模型可以有效地进行特征学习。然而，通过非线性模型进行多任务学习，这在实践中是最常见的学习范式，仍然存在许多未知。在这项工作中，我们首次提出了一种可证明的多任务表示学习方法，通过双层ReLU神经网络实现。

    Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
    
[^50]: 未知环境中的在线覆盖路径规划的端到端强化学习

    End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])

    [http://arxiv.org/abs/2306.16978](http://arxiv.org/abs/2306.16978)

    本文提出了一种基于端到端强化学习的在线覆盖路径规划方法，能处理未知环境并结合全局地图和局部感知输入，同时考虑长期路径规划和短期障碍物检测。

    

    覆盖路径规划是寻找覆盖给定封闭区域整个自由空间的最短路径的问题，应用范围从机器人割草和吸尘到地雷清除和搜救任务。虽然离线方法可以为已知环境找到可证明完备且在某些情况下是最优的路径，但在在线场景下，环境事先未知，特别是在存在非静态障碍物的情况下，其价值有限。我们提出了一种基于连续状态和动作空间的端到端强化学习方法，用于处理未知环境的在线覆盖路径规划问题。我们从全局地图和局部感知输入构建观察空间，使代理能够规划长期路径，并同时对短期障碍物进行行动。为了考虑大规模环境，我们提出使用多尺度地图输入表示。此外，我们提出了一种新颖的总变差正则化方法以减少路径偏离问题。

    Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
    
[^51]: 超越隐性偏差：SGD噪声在在线学习中的不重要性

    Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning. (arXiv:2306.08590v1 [cs.LG])

    [http://arxiv.org/abs/2306.08590](http://arxiv.org/abs/2306.08590)

    本文通过实际数据的全面实证分析证明了在在线学习中，大学习率和小批量大小并不会带来任何隐性偏差的优势。在线学习中SGD噪音的好处只是计算上的便利，可以促进更大或更具成本效益的梯度步骤。

    

    先前的研究认为，SGD在深度学习中的成功归因于高学习率或小批量大小所引起的隐性偏差（“SGD噪声”）。而我们研究了SGD噪声在在线（即单个epoch）学习中的影响，通过对图像和语言数据的全面实证分析，我们证明了在在线学习中，大学习率和小批量大小并不会带来任何隐性偏差的优势。与离线学习相反，在线学习中SGD噪声的好处严格来说只是计算上的便利，可以促进更大或更具成本效益的梯度步骤。我们的研究表明，SGD在在线模式下可以被视为是在“无噪声梯度流算法”的“黄金路径”上踩踏嘈杂步伐。通过减少训练期间的SGD噪声和测量模型之间的逐点功能距离，我们提供了支持此假设的证据。

    The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size ("SGD noise"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the "golden path" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models 
    
[^52]: Sy-CON：用于持续自监督表示学习的对称对比损失

    Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning. (arXiv:2306.05101v1 [cs.LG])

    [http://arxiv.org/abs/2306.05101](http://arxiv.org/abs/2306.05101)

    Sy-CON是一种新的损失函数，用于持续自监督学习，它由两个损失组成，可以自然地找到良好的可塑性和稳定性之间的权衡，超过了现有持续学习方法。

    

    我们引入了一种新颖的通用损失函数，称为Symmetric Contrastive（Sy-CON）损失，用于有效的持续自监督学习（CSSL）。我们首先认为，传统的持续学习损失形式由单个任务特定损失（用于可塑性）和一个正则化器（用于稳定性）组成，对于基于对比损失的CSSL来说可能不理想，因为在对比学习方法中，任务特定损失会遭受负样本多样性降低的困扰，而正则化器可能会阻碍学习新的有区别的表示。为此，我们提出Sy-CON，它由两个损失（一个用于可塑性，另一个用于稳定性）组成，对当前和过去模型的负样本嵌入具有对称依赖关系。我们认为，我们的模型可以自然地找到良好的可塑性和稳定性之间的权衡，而无需任何显式的超参数调整。我们通过对几种持续学习基准的外部和内部实验验证了我们方法的有效性，并表明Sy-CON在稳定性和表征质量方面始终优于现有的持续学习方法。

    We introduce a novel and general loss function, called Symmetric Contrastive (Sy-CON) loss, for effective continual self-supervised learning (CSSL). We first argue that the conventional loss form of continual learning which consists of single task-specific loss (for plasticity) and a regularizer (for stability) may not be ideal for contrastive loss based CSSL that focus on representation learning. Our reasoning is that, in contrastive learning based methods, the task-specific loss would suffer from decreasing diversity of negative samples and the regularizer may hinder learning new distinctive representations. To that end, we propose Sy-CON that consists of two losses (one for plasticity and the other for stability) with symmetric dependence on current and past models' negative sample embeddings. We argue our model can naturally find good trade-off between the plasticity and stability without any explicit hyperparameter tuning. We validate the effectiveness of our approach through exte
    
[^53]: BOtied: 带有相关多元等级的多目标贝叶斯优化

    BOtied: Multi-objective Bayesian optimization with tied multivariate ranks. (arXiv:2306.00344v1 [cs.LG])

    [http://arxiv.org/abs/2306.00344](http://arxiv.org/abs/2306.00344)

    BOtied 是一种带有相关多元等级的多目标贝叶斯优化算法，相较于现有的方法具有更高的样本效率和较好的近似质量。

    

    许多科学和工业应用需要同时优化多个潜在的相互冲突的目标。多目标贝叶斯优化 (MOBO) 是一种高效地识别 Pareto 最优解的框架。我们展示了无支配解和最高多元等级之间的自然联系，它与联合累积分布函数的最外层等高线重合。我们提出了 CDF indicator，这是一种 Pareto 合规的度量，用于评估近似 Pareto 集合的质量，它补充了流行的 hypervolume indicator。MOBO 的核心是采集函数，它通过导航目标之间的最佳折中来确定下一个要评估的候选项。 基于盒子分解目标空间的多目标采集函数（例如期望的 hypervolume 改进（EHVI）和熵搜索）在存在大量目标时的性能缩放很差。我们提出一种采集函数，称为 BOtied，它利用相关多元等级来高效搜索 Pareto frontier。我们的实验表明，BOtied 在样本效率和近似质量方面优于现有方法。

    Many scientific and industrial applications require joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. We show a natural connection between non-dominated solutions and the highest multivariate rank, which coincides with the outermost level line of the joint cumulative distribution function (CDF). We propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets that complements the popular hypervolume indicator. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Multi-objective acquisition functions that rely on box decomposition of the objective space, such as the expected hypervolume improvement (EHVI) and entropy search, scale poorly to a large number of objectives. We propose an acquisition function, call
    
[^54]: PDExplain：PDEs 在实际应用中的情境建模

    PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])

    [http://arxiv.org/abs/2303.15827](http://arxiv.org/abs/2303.15827)

    我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。

    

    我们提出了一种解释性的方法PDExplain用于解决偏微分方程。在训练阶段，我们的方法通过一个操作员定义的PDE家族的数据以及这个家族的一般形式进行馈送。在推断阶段，提供了一个从现象中收集到的最小样本，其中样本与 PDE 家族相关，但不一定属于训练阶段看到的具体 PDE 集合。我们展示了算法如何预测未来时间步的PDE解。此外，我们的方法提供了PDE的可解释形式，这种特征可以协助通过物理科学数据来对现象进行建模。为了验证我们的方法，我们进行了大量实验，考察了其在预测误差和可解释性方面的质量。

    We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
    
[^55]: 基于脉冲神经网络的事件驱动动作识别：理解其优势的新任务

    Spiking Neural Networks for event-based action recognition: A new task to understand their advantage. (arXiv:2209.14915v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14915](http://arxiv.org/abs/2209.14915)

    本文通过提出新任务DVS-GC并进行实证研究，展示了脉冲神经网络在事件驱动动作识别中的优势，包括实现时间特征提取和对事件顺序的理解。

    

    脉冲神经网络具有独特的时间动态特性，但是这种计算的性质和优势仍然不太被了解。为了提供答案，本文展示了如何在前馈神经网络中利用脉冲神经元实现时间特征提取，无需递归突触，并展示了它们与传统神经元的差异。通过提出一个新任务DVS-Gesture-Chain（DVS-GC），我们首次对现实中的事件驱动动作识别数据集中的时间依赖进行了评估。我们的研究证明了广泛使用的DVS Gesture基准可以被不进行时间特征提取的网络解决，而新的DVS-GC则需要对事件的顺序进行理解。

    Spiking Neural Networks (SNN) are characterised by their unique temporal dynamics, but the properties and advantages of such computations are still not well understood. In order to provide answers, in this work we demonstrate how Spiking neurons can enable temporal feature extraction in feed-forward neural networks without the need for recurrent synapses, showing how their bio-inspired computing principles can be successfully exploited beyond energy efficiency gains and evidencing their differences with respect to conventional neurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain (DVS-GC), which allows, for the first time, to evaluate the perception of temporal dependencies in a real event-based action recognition dataset. Our study proves how the widely used DVS Gesture benchmark could be solved by networks without temporal feature extraction, unlike the new DVS-GC which demands an understanding of the ordering of the events. Furthermore, this setup allowed us to un
    
[^56]: 量子晶格模型的规范不变性和任意子对称自回归神经网络

    Gauge Invariant and Anyonic Symmetric Autoregressive Neural Networks for Quantum Lattice Models. (arXiv:2101.07243v3 [cond-mat.str-el] UPDATED)

    [http://arxiv.org/abs/2101.07243](http://arxiv.org/abs/2101.07243)

    该论文提出了一种针对量子晶格模型的自回归神经网络，并明确遵守规范对称性或任意子约束，能够提供精确表示量子晶格模型基态和激发态的方法。

    

    对称性，如规范不变性和任意子对称性，在量子多体物理中起着至关重要的作用。我们开发了一种通用方法来构建针对量子晶格模型的规范不变性或任意子对称性自回归神经网络，包括 Transformer 和循环神经网络等各种架构。这些网络可以高效地采样，并明确地遵守规范对称性或任意子约束。我们证明了我们的方法可以提供精确表示2D和3D扭曲码以及X-立方体分形模型的基态和激发态。我们变分优化了我们的对称性合并的自回归神经网络，用于各种模型的基态以及实时动力学。我们模拟了 $\text{U(1)}$ 网格规范理论的量子链接模型的动力学和基态，得到了 2D $\mathbb{Z}_2$ 规范理论的相图，确定了 $\text{SU(2)}$ 临界链的相变和中心荷，以及研究了 2D 三角晶格的自旋模型的热化动力学。

    Symmetries such as gauge invariance and anyonic symmetry play a crucial role in quantum many-body physics. We develop a general approach to constructing gauge invariant or anyonic symmetric autoregressive neural networks, including a wide range of architectures such as Transformer and recurrent neural network, for quantum lattice models. These networks can be efficiently sampled and explicitly obey gauge symmetries or anyonic constraint. We prove that our methods can provide exact representation for the ground and excited states of the 2D and 3D toric codes, and the X-cube fracton model. We variationally optimize our symmetry incorporated autoregressive neural networks for ground states as well as real-time dynamics for a variety of models. We simulate the dynamics and the ground states of the quantum link model of $\text{U(1)}$ lattice gauge theory, obtain the phase diagram for the 2D $\mathbb{Z}_2$ gauge theory, determine the phase transition and the central charge of the $\text{SU(2
    

