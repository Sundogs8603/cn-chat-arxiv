# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation.](http://arxiv.org/abs/2306.12422) | 本文提出了一种优先使用单调非增函数进行时间步采样的NeRF优化方法，以解决NeRF优化过程和得分蒸馏中均匀时间步采样之间的冲突。实验表明这种方法能够显著提高文本到3D内容的质量和多样性。 |
| [^2] | [Addressing Discontinuous Root-Finding for Subsequent Differentiability in Machine Learning, Inverse Problems, and Control.](http://arxiv.org/abs/2306.12413) | 本文解决了机器学习、逆问题和控制中碰撞响应函数阶跃形成的不连续根查找问题，并提供了平滑的解决方案。 |
| [^3] | [One-shot Imitation Learning via Interaction Warping.](http://arxiv.org/abs/2306.12392) | 本研究提出了一种交互变形的方法实现了从单个演示中学习SE（3）机器人操纵策略，并在实验中取得了成功效果。 |
| [^4] | [Probing the limit of hydrologic predictability with the Transformer network.](http://arxiv.org/abs/2306.12384) | 本文探究了采用Transformer网络对水文预测问题的建模效果，发现相比LSTM模型缺乏优势，可能是由于水文预测问题的马尔可夫特性所导致。 |
| [^5] | [Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms.](http://arxiv.org/abs/2306.12383) | 本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。 |
| [^6] | [On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets and their Aggregation.](http://arxiv.org/abs/2306.12380) | 本论文分析了 Gibbs 算法对训练数据的相关性，并介绍了用于评估 GA 泛化能力的不同性能指标，以及训练误差和测试误差之间的关联。 |
| [^7] | [PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning.](http://arxiv.org/abs/2306.12370) | 提出了一种针对深度学习量身定制的超参数优化算法PriorBand，能够同时利用专家信念和廉价的代理任务，具有高效性和抗干扰能力。 |
| [^8] | [Attention Hybrid Variational Net for Accelerated MRI Reconstruction.](http://arxiv.org/abs/2306.12365) | 提出了一种应用于加速MRI重建的注意力混合变分网络，可以同时利用k-空间和图像域中的信息进行学习，实验结果表明其在MRI重建中具有较好的性能。 |
| [^9] | [Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via Optimization and Data-driven Approach for Dynamic Systems.](http://arxiv.org/abs/2306.12361) | 本文提出了一种不需要假设未知输入为线性的方法，结合非线性优化和数据驱动方法可以实现对未知输入的估计，并通过联合 sigma-point 变换方案将状态和未知输入的不确定性纳入估计中，确保其稳定性。这个方法适用于许多智能自主系统。 |
| [^10] | [Protein Discovery with Discrete Walk-Jump Sampling.](http://arxiv.org/abs/2306.12360) | 本文提出了一种离散行走跳跃采样的方法，通过学习平滑能量函数、使用 Langevin Markov 链蒙特卡罗 (MCMC) 算法和一步去噪的投射技术，可以解决离散生成模型的采样难题。同时在抗体蛋白质的生成建模上进行了应用和测试，并引入了分布一致性得分作为基准测试标准。 |
| [^11] | [Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP.](http://arxiv.org/abs/2306.12356) | 本文研究了在部分可观察马尔可夫决策过程中的可证明高效表示学习，提出了可解码和$\gamma$-可观察两种算法，达到高效样本复杂度。 |
| [^12] | [ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data.](http://arxiv.org/abs/2306.12330) | 本文提出了ProtoGate，一种基于原型的神经模型，该模型通过考虑样本内和样本间的同质性和异质性来引入归纳偏差，并以全局到本地的方式选择特征，从而提高预测精度并使预测具有可解释性。 |
| [^13] | [Introspective Action Advising for Interpretable Transfer Learning.](http://arxiv.org/abs/2306.12314) | 本文提出了一种基于行动建议的迁移学习方法，可以更加解释性和细粒度地传输知识，并在一些情况下可以取得比标准权重复制方法更好的结果。 |
| [^14] | [Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift.](http://arxiv.org/abs/2306.12306) | 该论文在多个具有挑战性的分类和回归任务上对现代BDL算法进行了系统性评估，重点关注了在分布偏移下的泛化能力和校准能力，并研究了一种带符号的期望校准误差版本。 |
| [^15] | [StarVQA+: Co-training Space-Time Attention for Video Quality Assessment.](http://arxiv.org/abs/2306.12298) | 本文提出了一个针对视频质量评估的联合时空注意力协同训练模型StarVQA+，通过将主观平均分数编码为概率向量并嵌入一个特殊令牌作为可学习变量设计了一个向量化的回归损失函数，使用图像和视频来联合训练空间和时间注意权重，在野外视频质量评估数据集上实验，实验结果表明StarVQA+比现有的最先进方法更优秀。 |
| [^16] | [Diffusion Posterior Sampling for Informed Single-Channel Dereverberation.](http://arxiv.org/abs/2306.12286) | 本文提出了一种基于扩散模型的强化单通道去混响方法，鲁棒性更强，能有效地处理非平稳噪声，并在大混响时间下表现出优越性。 |
| [^17] | [Resilient Sparse Array Radar with the Aid of Deep Learning.](http://arxiv.org/abs/2306.12285) | 本文提出了两种基于机器学习的方法来应对稀疏阵列中传感器失效问题，显著提高了雷达的性能，同时保持方向估计的高分辨率特点。 |
| [^18] | [Online Resource Allocation with Convex-set Machine-Learned Advice.](http://arxiv.org/abs/2306.12282) | 该论文提出了一个框架，使用凸集机器学习建议来增强在线资源分配决策。该算法类在一致比率和鲁棒比率之间平衡，并在实验中表现出优异的性能。 |
| [^19] | [From structure mining to unsupervised exploration of atomic octahedral networks.](http://arxiv.org/abs/2306.12272) | 该论文提出了一种自动化的化学直觉算法，可以对配位八面体网络进行几何解析、量化和分类，并利用无监督机器学习对无机框架聚型进行分类。作者发现了ABO$_{3}$钙钛矿多形体系中的轴向倾斜趋势，并揭示了Pauling的第三条规则违反和设计原则的更新。 |
| [^20] | [A Finite Expression Method for Solving High-Dimensional Committor Problems.](http://arxiv.org/abs/2306.12268) | 本文提出了一种用于解决高维Committor问题的有限表达式方法(FEX)，该方法通过深度神经网络学习最优非线性函数和系数值，能够显著提高计算效果。 |
| [^21] | [Combining multi-spectral data with statistical and deep-learning models for improved exoplanet detection in direct imaging at high contrast.](http://arxiv.org/abs/2306.12266) | 本论文提出了一种利用统计和深度学习模型与多光谱数据相结合的方法，对直接成像的外行星探测进行改进，通过建立统计模型并进行卷积神经网络训练，在精度和召回之间实现更好的权衡。 |
| [^22] | [Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation.](http://arxiv.org/abs/2306.12259) | 该论文提出了一种VC模型，可使用仅两个增强函数自动将语音分解为四个部分，而无需多个手工制作的特征或费力的瓶颈调整，并且实证结果表明，相对于基线，模型在解脱有效性和语音自然度方面表现更好。 |
| [^23] | [GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection.](http://arxiv.org/abs/2306.12251) | GADBench是一个全面的基准系统，发现了树集成和简单邻域汇聚方法胜过所有23个模型，包括最新的针对GAD任务量身定制的GNN模型。 |
| [^24] | [Knowledge-based Multimodal Music Similarity.](http://arxiv.org/abs/2306.12249) | 本文研究了基于知识的多模态音乐相似性，旨在开发一个完全可解释和可解释的系统，为最终用户提供更多对音乐相似性和分类系统的控制和理解。 |
| [^25] | [Concurrent ischemic lesion age estimation and segmentation of CT brain using a Transformer-based network.](http://arxiv.org/abs/2306.12242) | 提出了一种基于Transformer网络的新方法，能够同时标记和估算脑部CT图像中的缺血性病变的成像时长，该方法在病灶分割和病灶年龄估计方面实现了最先进的性能，有望成为支持中风护理决策的重要工具。 |
| [^26] | [Predicting protein variants with equivariant graph neural networks.](http://arxiv.org/abs/2306.12231) | 本文比较了等变图神经网络和基于序列的方法在预测蛋白质变体方面的能力，并发现我们提出的结构方法在训练更少的分子的情况下可以达到与基于序列的方法相竞争的性能。 |
| [^27] | [Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training.](http://arxiv.org/abs/2306.12230) | 该论文对动态稀疏训练中的剪枝位置进行了实证分析，发现在低密度范围内，最简单的大小方法提供了最佳性能。 |
| [^28] | [Automated Machine Learning for Remaining Useful Life Predictions.](http://arxiv.org/abs/2306.12215) | 本文介绍了一种自动化的机器学习方法，名为AutoRUL，用于自动预测工程系统的剩余使用寿命（RUL）。该方法将微调的标准回归方法与高预测能力的集成相结合，并通过八个真实世界的和合成数据集的评估，证明AutoML提供了一种可行的选择。 |
| [^29] | [More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity.](http://arxiv.org/abs/2306.12214) | 本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。 |
| [^30] | [MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates.](http://arxiv.org/abs/2306.12212) | 本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。 |
| [^31] | [Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks.](http://arxiv.org/abs/2306.12198) | 本论文揭示了预训练语言模型在非语言任务中的内部运作，具体使用约束算术问题探索模型的注意力权重分数和隐藏状态，并发现了有前途的结果，该模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。 |
| [^32] | [Split Learning in 6G Edge Networks.](http://arxiv.org/abs/2306.12194) | Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks. |
| [^33] | [Quantifying lottery tickets under label noise: accuracy, calibration, and complexity.](http://arxiv.org/abs/2306.12190) | 本文使用双稀疏下降方法识别和表征与分类任务相关的修剪模型，展示了这些模型的大小与任务难度呈现振荡态势；同时，相较于全网络，这些修剪模型更好地捕捉到真实条件概率分布。 |
| [^34] | [Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand Edge Resource.](http://arxiv.org/abs/2306.12185) | 本研究提出了一个新颖的分散式DNN手术框架（DDS），并提出了一种资源自适应动态协商（R-ADB）方法，以便自适应地适应变化的资源情况。 |
| [^35] | [Mixture Encoder for Joint Speech Separation and Recognition.](http://arxiv.org/abs/2306.12173) | 本论文提出了一种中间方法，同时利用显式语音分离和直接在ASR模块中合并混合语音信息，通过交换跨说话者上下文信息的层，实现了在SMS-WSJ任务上相对于纯模块化设置的单词错误率7%的相对改进。 |
| [^36] | [Post-hoc Selection of Pareto-Optimal Solutions in Search and Recommendation.](http://arxiv.org/abs/2306.12165) | 本文提出了一种名为“从乌托邦的人口距离”（PDU）的后选择策略，用于确定和选择 Pareto-最优解中的最佳解。该方法分析点的分布，通过估计 PDU 分数的点的平均位置来确定最佳解的可能位置，实验结果表明 PDU 在准确性和稳定性方面表现优异。 |
| [^37] | [Adversarial Attacks Neutralization via Data Set Randomization.](http://arxiv.org/abs/2306.12161) | 本文提出一种新的防御机制，将原始数据集伪随机投影到一个新的数据集中，在多个有不同决策边界的训练分类器中随机选择一个来测试输入，能够中和对抗攻击，提高了分类器的安全性和可靠性。 |
| [^38] | [Joint Dense-Point Representation for Contour-Aware Graph Segmentation.](http://arxiv.org/abs/2306.12155) | 该论文提出了一种利用点检测和像素分割技术联合学习的图形分割方法，从而克服了传统方法中的一些缺陷，提高了分割的稳定性和准确性。 |
| [^39] | [Benchmark data to study the influence of pre-training on explanation performance in MR image classification.](http://arxiv.org/abs/2306.12150) | 本研究提出了一个MRI分类任务的基准数据集，用于评估不同模型的解释性能。实验结果表明，XAI方法并不一定比简单模型提供更好的解释，且CNN的解释能力取决于底层数据的复杂性和标签的质量。 |
| [^40] | [Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors.](http://arxiv.org/abs/2306.12129) | 本文展示了一种基于机器学习的针织力传感器不一致性补偿方法，通过使用指数平滑滤波器进行预处理，并使用最小的人工神经网络来提高传感器读数和执行力之间的映射。 |
| [^41] | [Mass-Producing Failures of Multimodal Systems with Language Models.](http://arxiv.org/abs/2306.12105) | 本文介绍了一种MultiMon系统，可以自动识别多模态系统中的系统性失败，揭示CLIP文本编码器的14个系统性失败，每个都由数百个不同的输入组成，这些输入会导致其他大多数最先进的多模态系统的失败。 |
| [^42] | [Efficient ResNets: Residual Network Design.](http://arxiv.org/abs/2306.12100) | 本文介绍了一个在CIFAR-10图像分类任务上可训练参数小于5百万的改进版ResNet模型，达到了96.04%的测试准确率，远优于其可训练参数大于1100万的基准架构ResNet18。 |
| [^43] | [MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for 12-Lead ECG Classification.](http://arxiv.org/abs/2306.12098) | 提出了一种称为MSW-Transformer的单层变压器网络，使用多窗口滑动注意机制和不同尺度来捕捉不同维度的特征，有效地从12导ECG信号中区分病理特征差异，实现了最先进的性能。 |
| [^44] | [Understanding human mobility patterns in Chicago: an analysis of taxi data using clustering techniques.](http://arxiv.org/abs/2306.12094) | 本研究使用芝加哥的出租车数据，通过聚类技术分析社区之间的联系，提供了新的公共交通发展以及交通或道路污染缓解工作的指导。 |
| [^45] | [Edge Devices Inference Performance Comparison.](http://arxiv.org/abs/2306.12093) | 本研究比较了多种边缘设备上常用的深度学习模型的推理性能，发现Google平台对于新型模型表现最佳，而Intel Neural Stick是最通用的加速器。 |
| [^46] | [Structure-Aware DropEdge Towards Deep Graph Convolutional Networks.](http://arxiv.org/abs/2306.12091) | 本文提出了一种结构感知的DropEdge++方法，其中包含基于层的采样器和基于特征的采样器。研究还发现，从底层采样边比逐渐减少的采样边以及DropEdge更能提高性能，证明了这一现象与过度平滑密切相关。 |
| [^47] | [An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning.](http://arxiv.org/abs/2306.12088) | 本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。 |
| [^48] | [What Constitutes Good Contrastive Learning in Time-Series Forecasting?.](http://arxiv.org/abs/2306.12086) | 本文通过对比分析各种训练变量(包括不同的SSCL算法、学习策略，模型体系结构以及它们之间的相互作用)的有效性，研究了SSCL在时间序列预测中的影响及具体好处。 |
| [^49] | [FLGo: A Fully Customizable Federated Learning Platform.](http://arxiv.org/abs/2306.12079) | 提出了一个名为FLGo的联邦学习平台，用于定制模拟以满足特定应用环境的需求。平台包含40+基准、20+算法和2个系统模拟器，并提供用户友好的API以用于快速开发新的插件，以提高共享性和可重复性。 |
| [^50] | [Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers.](http://arxiv.org/abs/2306.12077) | 该文提出了一种从高维经验数据中学习动力系统的方法，基于不变分解和（空间-）时间变换器，能够自动分离出一种实例特定的编码和一种潜在动态模型，使用经验数据作为模型的输入，通过在任意连续时间推断系统行为，与显式的神经ODE公式不同，高效可扩展。 |
| [^51] | [Task-Robust Pre-Training for Worst-Case Downstream Adaptation.](http://arxiv.org/abs/2306.12070) | 本文提出了一种任务鲁棒的预训练方法，将上游任务分成几个代表性任务并应用极小极大损失进行预训练，以保证模型能够在下游任务中具有均匀良好的性能。 |
| [^52] | [Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension.](http://arxiv.org/abs/2306.12069) | 提出了一个基于全局图网络的方法，既处理话语层面又处理单词层面的上下文，并通过分层交互机制对节点级别和类型级别的关系进行建模，以提供更细粒度的关系提取，该方法在逻辑推理QA数据集和自然语言推理数据集上优于现有的最先进方法。 |
| [^53] | [Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions.](http://arxiv.org/abs/2306.12067) | 本文提出了一种新型随机双层优化的最优算法，避免了使用高阶光滑性假设，能够更好地适应非凸设置。 |
| [^54] | [EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations.](http://arxiv.org/abs/2306.12059) | 本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。 |
| [^55] | [Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems.](http://arxiv.org/abs/2306.12047) | 本文提供了一种基于Corrector操作符的框架，以增强神经算子代理非线性变分边界值问题的准确度和可靠性。使用该方案对于PCANet型神经算子的二维非线性扩散模型的数值实验结果显示，逼近的准确度近乎提高了两个数量级，并且还在涉及非线性d之上的拓扑优化问题中得到了探讨。 |
| [^56] | [Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes.](http://arxiv.org/abs/2306.12045) | 本研究提出 TeCoS-LVM 模型，使用脉冲神经元以模拟自然视觉刺激的神经响应。该模型能够自适应地探索和利用刺激序列中的时间依赖关系，避免丢失脉冲列中的信息。 |
| [^57] | [Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors.](http://arxiv.org/abs/2306.12041) | 提出了一种基于轻量级遮蔽自编码器的高效异常事件检测模型，通过引入基于运动梯度的令牌加权方法，整合教师解码器和学生解码器以及生成合成异常事件，实现共同重构原始帧和对应的像素级异常映射。在三个基准测试中，实现出色的速度和准确性的权衡。 |
| [^58] | [End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection.](http://arxiv.org/abs/2306.12033) | 这项研究提出了一种名为ST-SSAD的新方法，可以系统地调整数据增强的超参数，从而有助于提高自我监督异常检测（SSAD）的性能。 |
| [^59] | [Comparative analysis of various web crawler algorithms.](http://arxiv.org/abs/2306.12027) | 本文介绍了网络爬虫和网页排名算法在处理Web数据方面的重要性；在评估五种不同的爬取算法后，旨在确定最有效的爬取算法。 |
| [^60] | [Continual Learners are Incremental Model Generalizers.](http://arxiv.org/abs/2306.12026) | 本文研究了continual learners作为预训练器对下游任务的影响，提出了一种新的无监督CL框架和微调方案GLAD，可用作较好的预训练模型本身。通过学习改善通用性特征，在容易忘记特定任务知识时，CL模型可逐步提高表示的转移质量而不影响微调性能。 |
| [^61] | [3HAN: A Deep Neural Network for Fake News Detection.](http://arxiv.org/abs/2306.12014) | 3HAN是一种基于深度学习的虚假新闻自动检测器。通过三层分层注意网络，它可以构建一个对新闻进行完整有效表示的新闻向量，并给文章不同部分分配不同的重要性，实现了高准确率的虚假新闻检测，并且提供了可理解的输出结果。 |
| [^62] | [Learning Homogenization for Elliptic Operators.](http://arxiv.org/abs/2306.12006) | 本文提出了一种新的数据驱动方法，可以学习用于椭圆算子的齐次化映射，以建立考虑接口的齐次化本构定律，并且证明了该方法的有效性。 |
| [^63] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^64] | [Training Transformers with 4-bit Integers.](http://arxiv.org/abs/2306.11987) | 本文提出了一种使用INT4算术训练Transformer的方法，并细致地分析了转换器中激活和梯度的特定结构，为它们提出了专用的量化器。算法在多个任务中达到了竞争性的准确性。 |
| [^65] | [Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?.](http://arxiv.org/abs/2306.11985) | 该论文评估了两种流行的解释人工智能方法，分析它们是否为临床决策提供可信的解释和适当的领域表示。 |
| [^66] | [Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture.](http://arxiv.org/abs/2306.11982) | 本研究分析了在CIFAR10数据集上不同池化配置下的模型性能表现，并发现预定义的下采样配置不一定是最优的。为了解决这个问题，提出了一个新的均衡混合超网络（BMSN）来优化下采样配置。 |
| [^67] | [Universal adversarial perturbations for multiple classification tasks with quantum classifiers.](http://arxiv.org/abs/2306.11974) | 本文探讨了量子通用对抗扰动，并发现一个精心制作的通用扰动可以成功地欺骗两个不同分类任务上达到最先进准确性的量子分类器，这为构建安全的量子机器学习系统带来潜在威胁。 |
| [^68] | [AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization.](http://arxiv.org/abs/2306.11971) | AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。 |
| [^69] | [Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning.](http://arxiv.org/abs/2306.11967) | 本文提出了一种基于互补学习子网络的无重演类增量学习方法，通过联合优化可塑性CNN特征提取器和分析前馈分类器来达到在不访问历史数据的情况下缓解灾难性遗忘的目的。 |
| [^70] | [Sampling Individually-Fair Rankings that are Always Group Fair.](http://arxiv.org/abs/2306.11964) | 该论文提出了一种有效算法，从个体公平分布中采样排名，同时确保每个输出的排名都满足群体公平性约束。输出排名的期望效用至少是最优公平解的效用的$\alpha$倍，其中$\alpha$是一个量化公平约束紧度的参数。 |
| [^71] | [TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings.](http://arxiv.org/abs/2306.11955) | 本文提出了TADIL方法，使用Transformer最近中心嵌入技术，能够在任务无关的条件下，对域增量学习中的任务进行识别，并训练出一个轻量级的增量任务分类器。 |
| [^72] | [On the Optimal Bounds for Noisy Computing.](http://arxiv.org/abs/2306.11951) | 本文改进了有噪音计算问题的自适应采样和非自适应采样模型下所有四个函数的下界，并且大多数下界与上界符合，差异不超过一个常数因子。 |
| [^73] | [Mitigating Communication Costs in Neural Networks: The Role of Dendritic Nonlinearity.](http://arxiv.org/abs/2306.11950) | 本研究发现，在神经网络中整合非线性树突结构可以显著提高模型的容量和性能，同时控制信号通信成本，这对于未来神经网络的发展具有重要的意义。 |
| [^74] | [Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets using Machine Learning.](http://arxiv.org/abs/2306.11946) | 本研究在多个异构数据集上采用机器学习方法预测冬小麦的产量，结果表明数据质量对于机器学习策略的决定性作用。 |
| [^75] | [Towards Understanding What Code Language Models Learned.](http://arxiv.org/abs/2306.11943) | 本研究探究了预先训练的代码语言模型的能力，证明其能够超越表面形式特征，学习精确而形式化定义的代码的计算语义。 |
| [^76] | [A Deep Learning Model for Heterogeneous Dataset Analysis -- Application to Winter Wheat Crop Yield Prediction.](http://arxiv.org/abs/2306.11942) | 本文提出了一种能够处理异构数据集的深度学习模型，并在数字农业领域的真实数据集上展示其表现优于传统机器学习模型。 |
| [^77] | [Open Problem: Learning with Variational Objectives on Measures.](http://arxiv.org/abs/2306.11928) | 本文探讨了在测度上编写变分目标的动机，提出通过此类目标推导实用算法，以解决超出分布的泛化和弱监督学习等问题的开放问题。 |
| [^78] | [No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths.](http://arxiv.org/abs/2306.11922) | 本文研究了样本梯度沿优化路径的基本几何性质，发现这些量在训练期间表现出可预测、一致的行为。我们的发现表明，优化轨迹不仅从未遇到显著的障碍，而且在大多数训练期间能够保持稳定的动力学。 |
| [^79] | [Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback.](http://arxiv.org/abs/2306.11918) | 本研究提出了自适应集成 Q 学习算法，通过误差反馈来减小集成方法中估计偏差的影响，并结合模型识别自适应控制（MIAC）来实现集成大小自适应。 |
| [^80] | [Structure-Aware Robustness Certificates for Graph Classification.](http://arxiv.org/abs/2306.11915) | 该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。 |
| [^81] | [Randomized Quantization is All You Need for Differential Privacy in Federated Learning.](http://arxiv.org/abs/2306.11913) | 该论文提出了一种基于随机量化的机制，通过两级量化实现了差分隐私的保证，这可以用于处理联邦学习中的隐私问题，并实现了接近最优的隐私效用平衡。 |
| [^82] | [Copula-Based Deep Survival Models for Dependent Censoring.](http://arxiv.org/abs/2306.11912) | 论文提出了一种基于Copula的参数化生存模型，通过放宽条件独立性的假设，扩展现代非线性生存分析，从而显著改进了对生存分布的估计。 |
| [^83] | [Accelerating Generalized Random Forests with Fixed-Point Trees.](http://arxiv.org/abs/2306.11908) | 本文提出一种新的树生长规则，使广义随机森林在无梯度优化的情况下大大节省了时间。 |
| [^84] | [Deep Fusion: Efficient Network Training via Pre-trained Initializations.](http://arxiv.org/abs/2306.11903) | 本文提出了Deep Fusion，一种基于预训练初始化的高效网络训练方法。通过加速训练过程、降低计算要求，并导致改进的泛化性能，使得该方法在维持传统训练方法的性能甚至超越其性能的同时，减少了训练时间和资源消耗。 |
| [^85] | [Closing the loop: Autonomous experiments enabled by machine-learning-based online data analysis in synchrotron beamline environments.](http://arxiv.org/abs/2306.11899) | 本研究将机器学习技术应用于同步辐射实验中的X射线反射测量，形成闭环反馈系统，并实现了基于在线数据分析的实时决策。 |
| [^86] | [Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings.](http://arxiv.org/abs/2306.11898) | 本文研究旨在回答ARDR算法的收敛性问题，我们将ARDR方法与传统的降维技术联系起来，可以将PCA嵌入完全恢复，通过稍加修改可以用LLE复现ARDR嵌入，并形式化了一系列猜想，如果成立，可以将2D嵌入中的结构归因于输入分布。 |
| [^87] | [Learning Costs for Structured Monge Displacements.](http://arxiv.org/abs/2306.11895) | 本文提出了一种新的方法，即学习合适的成本结构来鼓励映射沿着特定的工程特征来传送点，其在扩展 Monge-Bregman-Occam 管道方面做出了重大贡献，并在匹配直方图方面表现出了优异的性能。 |
| [^88] | [Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy.](http://arxiv.org/abs/2306.11890) | 本文提出了一种新的方法干预风格转移（IST），通过生成干预训练分布，从而显着改善了单细胞显微镜下外域泛化的问题。 |
| [^89] | [Reward Shaping via Diffusion Process in Reinforcement Learning.](http://arxiv.org/abs/2306.11885) | 本研究使用扩散过程来进行奖励塑形，提供了解决探索 - 利用权衡的优雅框架。同时，阐明了信息熵、随机系统动力学以及它们对熵产生的影响之间的关系。 |
| [^90] | [Personalized Federated Learning with Feature Alignment and Classifier Collaboration.](http://arxiv.org/abs/2306.11867) | 该研究提出了一种带有特征对齐和分类器协作的个性化联邦学习方法，通过利用全局语义知识进行显式的局部-全局特征对齐，并量化了每个客户端分类器组合的收益，作为组合权重的函数，对各种异构数据情形的基准数据集进行了广泛的评估。 |
| [^91] | [Unsupervised Deep Unfolded PGD for Transmit Power Allocation in Wireless Systems.](http://arxiv.org/abs/2306.11865) | 本论文通过深度神经网络展开算法提出了一种无监督学习方法用于优化DNN权重，以实现传输功率控制。在密集D2D通信环境下经过性能评估，将迭代算法所需迭代次数(>2倍)大大降低。 |
| [^92] | [A Model-free Closeness-of-influence Test for Features in Supervised Learning.](http://arxiv.org/abs/2306.11855) | 本文提出了一种无模型方法，评估响应值上两个特征的影响力，并提供一种新的测试方法。 |
| [^93] | [Generalization Across Experimental Parameters in Machine Learning Analysis of High Resolution Transmission Electron Microscopy Datasets.](http://arxiv.org/abs/2306.11853) | 该研究探讨了训练数据集中元数据特征的选择如何影响神经网络对纳米颗粒分割等任务的性能，并发现神经网络对显微镜参数不具有鲁棒性，但可以在某些样本参数上泛化。 |
| [^94] | [Using super-resolution for enhancing visual perception and segmentation performance in veterinary cytology.](http://arxiv.org/abs/2306.11848) | 该研究通过整合超分辨率技术，在细胞学图像的语义分割质量上取得显着进展，为细胞学图像分析的发展提出了巨大的发展前景。 |
| [^95] | [Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features.](http://arxiv.org/abs/2306.11847) | 本研究通过XGBoost机器学习模型揭示了社会人口统计学、建筑环境特征和环境危害曝露特征是影响城市癌症患病率的关键，结合因果推断实验提出了增加绿化空间、减少开发区和总排放量可以缓解癌症的发生。 |
| [^96] | [Discovering Causality for Efficient Cooperation in Multi-Agent Environments.](http://arxiv.org/abs/2306.11846) | 本文研究了因果关系在多智能体强化学习中的应用，通过对懒惰智能体进行惩罚以帮助团队取得更好的效果。此外，研究了如何利用因果关系估计改善对智能体的信用分配，以及如何使用Amortized Causal Discovery自动检测多智能体环境中的因果关系。 |
| [^97] | [Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations.](http://arxiv.org/abs/2306.11839) | 本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。 |
| [^98] | [Topological Parallax: A Geometric Specification for Deep Perception Models.](http://arxiv.org/abs/2306.11835) | 拓扑视差是一种比较训练模型和参考数据集多尺度几何结构相似性的理论和计算工具，它可以估计模型中的拓扑特征，有助于理解深度学习模型的行为和性能。 |
| [^99] | [UMM: Unsupervised Mean-difference Maximization.](http://arxiv.org/abs/2306.11830) | 提出了一种无需更改实验范例的无监督方法来检测事件相关电位 (ERPs) 中被关注的字母，并在两个公开数据集上优于现有的无监督和监督方法。 |
| [^100] | [Any Deep ReLU Network is Shallow.](http://arxiv.org/abs/2306.11827) | 该论文证明了任何深度的ReLU网络都可以被重写为一个具有透明性的浅层网络。这一结论有助于解释模型行为。 |
| [^101] | [Learning to Generate Better Than Your LLM.](http://arxiv.org/abs/2306.11816) | 本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。 |
| [^102] | [DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization.](http://arxiv.org/abs/2306.11800) | DynaQuant通过动态量化实现对各种最先进模型的显着压缩，几乎不影响模型准确性。 |
| [^103] | [Time-Varying Transition Matrices with Multi-task Gaussian Processes.](http://arxiv.org/abs/2306.11772) | 本文介绍了一种基于核的多任务高斯过程模型，用于逼近个体的移动状态的潜在函数，并考虑了转移概率之间的相关性和时间可变性，通过强制执行随机性和非负性约束来实现研究目标。 |
| [^104] | [Designing Explainable Predictive Machine Learning Artifacts: Methodology and Practical Demonstration.](http://arxiv.org/abs/2306.11771) | 本文提出了一个解释性预测机器学习工件的设计方法学，以解决决策者不愿使用现代机器学习算法应用程序的问题，该方法学包括五个阶段。通过设计并实施一个工件，预测诊断为新冠肺炎的患者是否将来会出现花粉热症状来证明其适用性。 |
| [^105] | [A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design.](http://arxiv.org/abs/2306.11768) | 本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。 |
| [^106] | [About some compression algorithms.](http://arxiv.org/abs/2306.11765) | 该论文针对图像压缩方法开展神经网络算法研究，以迭代函数系统框架实现相关创新。 |
| [^107] | [On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers.](http://arxiv.org/abs/2306.11764) | 该论文研究了如何在音频频谱转换中通过频率归一化来提高录音设备普适性。实验证明，在AST模型中使用频率归一化可以降低不同录音设备之间的差异，提高模型鲁棒性。 |
| [^108] | [MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing.](http://arxiv.org/abs/2306.11758) | MRFI是一个高度可配置的神经网络故障注入工具，用户可以修改独立的故障配置文件进行注入和漏洞分析。 |
| [^109] | [Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification.](http://arxiv.org/abs/2306.11754) | 该研究提出了一种新的训练范式，使用预修剪和梯度下降技巧来减少参数空间和提高可扩展性，从而显著改善差分隐私图像分类的隐私-准确性折衷问题。 |
| [^110] | [A survey on deep learning approaches for data integration in autonomous driving system.](http://arxiv.org/abs/2306.11740) | 本文综述了自主驾驶系统感知模块的最新深度学习集成技术。其提出了一个新的集成分类系统，总结了集成操作及其优缺点，提供了新的见解，阐明了“理想”数据集成方法的特性，可减轻现有方法的局限性。本文总结了优化数据集成方法的关键特点。 |
| [^111] | [Neural Shape Diameter Function for Efficient Mesh Segmentation.](http://arxiv.org/abs/2306.11737) | 该论文提出了一种利用深度学习在网格分割之前编码映射函数的数据驱动方法，可以用于多种应用，不受分辨率影响。 |
| [^112] | [Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events.](http://arxiv.org/abs/2306.11547) | 这篇论文介绍了Event Stream GPT (ESGPT)——一种用于构建连续时间事件序列的GPT模型的开源库。该库支持在医疗记录等具有内部依赖性的复杂事件上进行预测，具有高效易用且能达到最佳性能的特点。 |
| [^113] | [Top-down machine learning of coarse-grained protein force-fields.](http://arxiv.org/abs/2306.11375) | 通过分子动力学模拟和可微分轨迹重加权训练神经网络势能，实现了自上而下的粗粒化蛋白质力场建模，仅需蛋白质的天然构象即可展示其外推能力。 |
| [^114] | [RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks.](http://arxiv.org/abs/2306.11335) | 该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。 |
| [^115] | [Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs.](http://arxiv.org/abs/2306.11330) | 本论文介绍了一种基于FPGA的GNN架构，在粒子跟踪中实现了低延迟和资源高效性，并实现了对现有CPU和GPU的数千倍上的性能提升。 |
| [^116] | [Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method.](http://arxiv.org/abs/2306.11307) | 本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。 |
| [^117] | [Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding.](http://arxiv.org/abs/2306.11066) | 本文针对几种FSL方法的鲁棒性指出，与全微调模型相比，纯FSL模型面对对抗扰动会带来不可忽视的任务性能下降。但使用无标签数据生成提示或使用多个提示可以显著提高鲁棒性，在某些情况下甚至胜过全微调模型。 |
| [^118] | [Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork.](http://arxiv.org/abs/2306.10698) | 人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。 |
| [^119] | [MARBLE: Music Audio Representation Benchmark for Universal Evaluation.](http://arxiv.org/abs/2306.10548) | 本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。 |
| [^120] | [Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions.](http://arxiv.org/abs/2306.10456) | 图表示学习（GRL）已经成为一个重要领域，在生物医学领域有着广泛的应用，未来研究方向是解决GRL当前面临的挑战。 |
| [^121] | [Samplet basis pursuit.](http://arxiv.org/abs/2306.10180) | 本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。 |
| [^122] | [Matrix Diagonalization as a Board Game: Teaching an Eigensolver the Fastest Path to Solution.](http://arxiv.org/abs/2306.10075) | 本文展示了如何使用强化学习加速矩阵对角化，将选择最快的解决方案的过程视为棋盘游戏，为数值线性代数的性能提供了一种有前途的工具。 |
| [^123] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^124] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^125] | [Structured Cooperative Learning with Graphical Model Priors.](http://arxiv.org/abs/2306.09595) | 本文提出了结构化协作学习算法，在不同设备之间通过协作完成分散任务。通过图模型先验生成的协作图，算法可以自动捕捉设备之间的跨任务相关性。 |
| [^126] | [FedMultimodal: A Benchmark For Multimodal Federated Learning.](http://arxiv.org/abs/2306.09486) | FedMultimodal 是面向多模态联邦学习的基准测试，为此我们提出了第一个覆盖五种不同应用领域和十个社区的FL基准测试，以便促进多模态FL研究。 |
| [^127] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^128] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^129] | [SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization.](http://arxiv.org/abs/2306.01981) | SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。 |
| [^130] | [MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators.](http://arxiv.org/abs/2306.01697) | MutateNN是一种用于探索硬件加速器上深度学习图像识别模型鲁棒性的工具，提供突变测试和分析能力，且有效性已在多种预训练深度神经网络模型中得到验证。 |
| [^131] | [Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE Surrogates.](http://arxiv.org/abs/2305.18944) | 该论文提出了一种使用神经网络的代理模型来进行快速铁托等离子体模拟的方法，解决了实时应用或详尽的参数扫描中速度太慢的问题。 |
| [^132] | [Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers.](http://arxiv.org/abs/2305.16863) | 该论文提出了一种自动化的增强算法，以适当改变新增输入的标签，从而最小化误导性相关性，并提高了少数群体的准确性，同时保持了总体准确性。 |
| [^133] | [GUARD: A Safe Reinforcement Learning Benchmark.](http://arxiv.org/abs/2305.13681) | GUARD是一个广义统一安全强化学习开发基准测试平台，是目前广泛遍布且包含各种RL代理、任务和安全约束规范的一站式基准测试，能够全面涵盖最先进的安全RL算法，并具有高度的可自定义性。 |
| [^134] | [Survey of Malware Analysis through Control Flow Graph using Machine Learning.](http://arxiv.org/abs/2305.08993) | 本文介绍了最新基于控制流图和机器学习的恶意软件检测方法，重点关注了从CFG中提取、表示、分类的不同方法。 |
| [^135] | [Integrating nearest neighbors on neural network models for treatment effect estimation.](http://arxiv.org/abs/2305.06789) | 本论文提出了一种新的方法NNCI，用于将最近邻居信息集成到神经网络模型中，以更准确地估计治疗效果。 |
| [^136] | [Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model.](http://arxiv.org/abs/2304.11332) | 本文介绍如何使用大型的通用分割模型SAM来提升医学图像分割，展示了如何通过使用SAM生成的掩模、特征和稳定性分数来构建和训练更好的医学图像分割模型，并在两个数据集上进行了验证。 |
| [^137] | [SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM.](http://arxiv.org/abs/2304.05622) | 介绍了Segment Any Medical Model (SAMM)，它是用于3D Slicer的SAM的扩展。SAMM在医学图像分割上表现良好，在实时性和通用性方面都有很好的性能，可以推断出掩模。 |
| [^138] | [Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression.](http://arxiv.org/abs/2304.01561) | 本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。 |
| [^139] | [Exploring Vision-Language Models for Imbalanced Learning.](http://arxiv.org/abs/2304.01457) | 本文探索了如何通过向视觉-语言模型添加轻量级解码器和利用不平衡算法来改进性能，实验表明改进后的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上分类准确度显著提高，特别是对于少数类，性能提升很大。 |
| [^140] | [Visual Chain-of-Thought Diffusion Models.](http://arxiv.org/abs/2303.16187) | 本文提出了一种两阶段采样过程，使用可视化思维传递模型来缩小条件和无条件模型之间的差距，相对标准无条件生成，FID提高25-50%。 |
| [^141] | [Finding Competence Regions in Domain Generalization.](http://arxiv.org/abs/2303.09989) | 该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。 |
| [^142] | [Contrastive Hierarchical Clustering.](http://arxiv.org/abs/2303.03389) | 本文提出了 CoHiClust 模型，一种基于深度神经网络的对比层次聚类模型，通过自监督学习方法，生成与我们直觉和图像语义相符的合理聚类结构，且在大部分图像数据集上的聚类准确性超过了最先进的平面聚类模型。 |
| [^143] | [Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization.](http://arxiv.org/abs/2303.03108) | 研究提出了一阶平坦度的概念，使用梯度范数感知最小化算法寻找在所有方向上具有均匀小曲率的极小值，提高了广义化能力和测试损失。 |
| [^144] | [CHGNet: Pretrained universal neural network potential for charge-informed atomistic modeling.](http://arxiv.org/abs/2302.14231) | CHGNet是一种预训练的通用神经网络势，能够准确预测不同化学组成和晶体结构的各种材料的属性，并且将电荷信息并入原子表示中，能够用于描述带电系统中的复杂电子相互作用。 |
| [^145] | [Aligned Diffusion Schr\"odinger Bridges.](http://arxiv.org/abs/2302.11419) | 本文提出一种新的算法框架，首次能够在考虑对齐数据的同时解决扩散薛定谔桥问题，相对于之前的方法，有更简单、方差更低的训练过程，并使用原则性的正则化方案，在实验中取得了显着的改进。 |
| [^146] | [PAC Prediction Sets for Large Language Models of Code.](http://arxiv.org/abs/2302.08703) | 本文针对标签空间大小呈指数增长的结构化预测问题，提出了一种考虑到受限制的预测集的解决方案，可以紧凑地表示为部分程序，并利用了编程语言的抽象语法树，使得正确的程序以高置信度出现在集合中。应用方面包括Codex风格的代码生成器。 |
| [^147] | [Constrained Decision Transformer for Offline Safe Reinforcement Learning.](http://arxiv.org/abs/2302.07351) | 本文研究了离线安全强化学习问题，提出了约束决策Transformer方法。该方法可以在部署过程中动态调整权衡，具有学习自适应、安全、鲁棒且高报酬的优势表现。 |
| [^148] | [PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction.](http://arxiv.org/abs/2302.05040) | PATCorrect是一种基于音素增强的非自回归变换器，利用文本和音素模态的表示来最大限度地降低ASR系统中的单词错误率，并在低延迟需求的实际生产系统中表现出鲁棒性。 |
| [^149] | [Algorithmic Collective Action in Machine Learning.](http://arxiv.org/abs/2302.04262) | 本文研究了机器学习中的算法集体行动的理论模型，并在大量实验中验证了该算法可以大大提高分类准确性，特别是在数据结构复杂和集体规模大的情况下。 |
| [^150] | [Efficient Graph Field Integrators Meet Point Clouds.](http://arxiv.org/abs/2302.00942) | 本文提出了两种算法用于非欧几里得空间中的高效图场积分，适用于点云网格图或ε-最近邻图的表征方法，具有很强的实用性。 |
| [^151] | [Bayes-optimal Learning of Deep Random Networks of Extensive-width.](http://arxiv.org/abs/2302.00375) | 本文研究了深度随机网络的学习问题，提出了 Bayes 最优测试误差的闭式表达式。岭回归和核回归能够达到最优表现，而神经网络的测试误差也可以从平方级的样本数量中获得接近于零的结果。 |
| [^152] | [Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models.](http://arxiv.org/abs/2301.13104) | 本文提出等变差分隐私深度学习，利用数据中的群体对称性来实现更稀疏的模型，并保持隐私保证，从而解决了DP-SGD在更具挑战性的任务上所面临网络规模困难的问题。 |
| [^153] | [On the Connection Between MPNN and Graph Transformer.](http://arxiv.org/abs/2301.11956) | 本文研究了MPNN与图转换器之间的连接，并证明了带有虚拟节点的MPNN可以任意逼近GT的自注意力层。我们的工作阐明了两种图学习范式之间的关系和能力平衡。 |
| [^154] | [PLay: Parametrically Conditioned Layout Generation using Latent Diffusion.](http://arxiv.org/abs/2301.11529) | 本文提出了一种有条件的潜在扩散模型 PLay，能够根据用户指定的指南在矢量图空间中生成参数化条件布局。相比以前的工作，在三个数据集和用户研究中表现更好，并为专业布局设计带来了新颖和互动的体验。 |
| [^155] | [Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge.](http://arxiv.org/abs/2301.11214) | 本文研究了如何将有向无环图中的概率因果知识应用于回归问题，提出了一种碰撞回归框架，并证明了其在假设空间为再生核希尔伯特空间时具有严格正的广义收益，在合成和气候模型数据上的实验结果表明其可以提高预测性能。 |
| [^156] | [Neighborhood Homophily-based Graph Convolutional Network.](http://arxiv.org/abs/2301.09851) | 本文提出了一种基于邻居同质性的图卷积网络 (NHGCN) 模型，利用新指标 Neighborhood Homophily (NH) 测量节点邻域中的标签复杂度或纯度。实验证明 NHGCN 模型在节点分类和图分类任务中表现优异并显著超过最先进模型。 |
| [^157] | [Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors.](http://arxiv.org/abs/2211.13224) | 本文介绍了一种名为Peekaboo的技术，可以用于使用现成的文本到图像扩散模型进行无监督语义细分并基础，而无需任何重新培训。这项技术的推理时间优化过程可以在与自然语言提示相关联的情况下生成分割掩模。 |
| [^158] | [Data-Driven Modeling of Landau Damping by Physics-Informed Neural Networks.](http://arxiv.org/abs/2211.01021) | 本研究提出了一种基于物理启发式神经网络的数据驱动方法，成功构建了一个包含隐式流体闭合项的多矩便流体模型，能够重现Landau阻尼的动力学特征，并且引入了一个表征流体-细结构相互作用的新变量，该方法有效地将流体模型的应用范围扩展到多尺度系统，为研究等离子体动力学提供了强大工具。 |
| [^159] | [Back to the Source: Diffusion-Driven Test-Time Adaptation.](http://arxiv.org/abs/2207.03442) | 本文提出了基于扩散驱动的测试时自适应方法 DDA，通过将测试输入投影到生成扩散模型的源域中来更新目标数据，相比以往的模型适应方法更为稳健，适用于各种损坏、架构和数据情况。 |
| [^160] | [On Scaled Methods for Saddle Point Problems.](http://arxiv.org/abs/2206.08303) | 本文对解决鞍点问题的缩放方法进行了理论分析，并通过实验研究发现这些方法在解决GAN训练问题中具有较好的适用性。 |
| [^161] | [Decentralized Training of Foundation Models in Heterogeneous Environments.](http://arxiv.org/abs/2206.01288) | 本文研究了在分布式异构环境下进行模型并行训练大型基础模型的技术，提出了一种可以利用更多分布式、异构和低带宽互联计算资源的训练方案。 |
| [^162] | [Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing.](http://arxiv.org/abs/2205.06978) | 本文提出了一种QHD离线策略，它基于超维强化学习与脑启发计算，实现稳健和实时学习。QHD相比于DQN具有更高效率且适用于高效的强化学习，具有在线和实时学习潜力。 |
| [^163] | [Value Gradient weighted Model-Based Reinforcement Learning.](http://arxiv.org/abs/2204.01464) | 本文提出了一种基于价值梯度加权的模型驱动强化学习方法，用于提高模型学习的性能。 |
| [^164] | [Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals.](http://arxiv.org/abs/2203.13887) | 本文提出了一种自动去偏机器学习方法，通过递归Riesz表征嵌套均值回归，避免了在动态治疗方案中需要解决辅助倾向模型的问题。 |
| [^165] | [RiskNet: Neural Risk Assessment in Networks of Unreliable Resources.](http://arxiv.org/abs/2201.12263) | 本文提出了一种基于GNN算法的方法，用于在通信网络中处理连接故障引起的罚款分布，并可以准确地模拟各种现有拓扑结构中的惩罚，在实践中还获得了超过12,000倍的速度提高。 |
| [^166] | [Self-Supervised Graph Representation Learning for Neuronal Morphologies.](http://arxiv.org/abs/2112.12482) | 本文提出了GraphDINO，一种自监督的图形表示学习方法，可用于从未标记的大规模数据集中学习三维神经元形态的低维表示。该方法使用了一系列数据增强策略和新型的注意力机制AC-Attention，在多个大脑区域内，GraphDINO 显示出了优于其它最先进方法的表现。 |
| [^167] | [Generalization in the Face of Adaptivity: A Bayesian Perspective.](http://arxiv.org/abs/2106.10761) | 本文提出面对自适应选择数据样本引起的过度拟合问题，使用噪声加算法可以提供不依赖于查询规模的误差保证，这一结果表明适应数据分析的问题在于新查询与过去查询的协方差。 |
| [^168] | [Missing Value Imputation on Multidimensional Time Series.](http://arxiv.org/abs/2103.01600) | 本文提出了DeepMVI，一种用于填充多维时间序列数据中缺失值的深度学习方法。该方法结合时间序列中的精细和粗粒度模式，以及跨分类维度的相关系列趋势。经过改进，DeepMVI表现出比其他算法更优秀的结果。 |
| [^169] | [LEAD: Least-Action Dynamics for Min-Max Optimization.](http://arxiv.org/abs/2010.13846) | 本文提出了一种名为LEAD的优化器，它基于物理学中的动力学特性来改进博弈优化的收敛问题，并在二次极小-极大博弈中展示了线性收敛到纳什均衡的特性。 |
| [^170] | [An extension of McDiarmid's inequality.](http://arxiv.org/abs/1511.05240) | 本文推广了McDiarmid不等式，使其适用于具有有界差异的函数，并进一步将结果推广到一般度量空间的集中性。 |

# 详细

[^1]: DreamTime: 一种改进的文本到3D内容创作优化策略

    DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. (arXiv:2306.12422v1 [cs.CV])

    [http://arxiv.org/abs/2306.12422](http://arxiv.org/abs/2306.12422)

    本文提出了一种优先使用单调非增函数进行时间步采样的NeRF优化方法，以解决NeRF优化过程和得分蒸馏中均匀时间步采样之间的冲突。实验表明这种方法能够显著提高文本到3D内容的质量和多样性。

    

    近期，预先训练了数十亿个图像-文本对的文本到图像扩散模型通过使用得分蒸馏来优化随机初始化的神经辐射场（NeRF），从而实现了文本到3D内容的创建。 然而，所得到的3D模型存在两个局限性：（a）质量问题，例如饱和的颜色和Janus问题；（b）与文本引导的图像合成相比，极低的多样性。本文表明，NeRF优化过程和得分蒸馏中均匀时间步采样之间的冲突是这些局限性的主要原因。为了解决这种冲突，我们建议优先使用单调非增函数进行时间步采样，这使得NeRF优化与扩散模型的采样过程相一致。大量实验表明，我们的简单重设计能够显著提高文本到3D内容的质量和多样性。

    Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled text-to-3D content creation by optimizing a randomly initialized Neural Radiance Fields (NeRF) with score distillation. However, the resultant 3D models exhibit two limitations: (a) quality concerns such as saturated color and the Janus problem; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between NeRF optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns NeRF optimization with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves text-to-3D content creation with higher quality and diversity.
    
[^2]: 处理机器学习、逆问题和控制中不连续的根查找问题

    Addressing Discontinuous Root-Finding for Subsequent Differentiability in Machine Learning, Inverse Problems, and Control. (arXiv:2306.12413v1 [cs.LG])

    [http://arxiv.org/abs/2306.12413](http://arxiv.org/abs/2306.12413)

    本文解决了机器学习、逆问题和控制中碰撞响应函数阶跃形成的不连续根查找问题，并提供了平滑的解决方案。

    

    许多物理过程的数学公式具有固有的不连续性，例如刚体或可变形体之间的碰撞。因为碰撞的阶跃响应在计算时需要对参数求导数，这导致了在机器学习、逆问题和控制领域的数值计算中存在困难。本文理论上和数值上证明了碰撞时间对参数求导数在趋近于分离碰撞状态和未碰撞状态的界面时会变得无穷大，并使用复化技术来处理分界面上的极限值。我们同时修正了导数的不连续性，从而使得计算更加平滑可靠。

    There are many physical processes that have inherent discontinuities in their mathematical formulations. This paper is motivated by the specific case of collisions between two rigid or deformable bodies and the intrinsic nature of that discontinuity. The impulse response to a collision is discontinuous with the lack of any response when no collision occurs, which causes difficulties for numerical approaches that require differentiability which are typical in machine learning, inverse problems, and control. We theoretically and numerically demonstrate that the derivative of the collision time with respect to the parameters becomes infinite as one approaches the barrier separating colliding from not colliding, and use lifting to complexify the solution space so that solutions on the other side of the barrier are directly attainable as precise values. Subsequently, we mollify the barrier posed by the unbounded derivatives, so that one can tunnel back and forth in a smooth and reliable fas
    
[^3]: 交互变形的一次性模仿学习

    One-shot Imitation Learning via Interaction Warping. (arXiv:2306.12392v1 [cs.RO])

    [http://arxiv.org/abs/2306.12392](http://arxiv.org/abs/2306.12392)

    本研究提出了一种交互变形的方法实现了从单个演示中学习SE（3）机器人操纵策略，并在实验中取得了成功效果。

    

    在开放式应用中，通过少量示范学习机器人策略是至关重要的。我们提出了一种新的方法，即交互变形，用于从单个演示中学习SE（3）机器人操作策略。我们使用形状变形技术推断环境中每个物体的三维网格，并将操纵动作表示为物体上的关键点，可以用物体的形状进行变形。我们在三个模拟和真实世界的物体重新排列任务中成功地进行了一次性模仿学习。我们还展示了我们的方法在野外预测物体网格和机器人抓取的能力。

    Imitation learning of robot policies from few demonstrations is crucial in open-ended applications. We propose a new method, Interaction Warping, for learning SE(3) robotic manipulation policies from a single demonstration. We infer the 3D mesh of each object in the environment using shape warping, a technique for aligning point clouds across object instances. Then, we represent manipulation actions as keypoints on objects, which can be warped with the shape of the object. We show successful one-shot imitation learning on three simulated and real-world object re-arrangement tasks. We also demonstrate the ability of our method to predict object meshes and robot grasps in the wild.
    
[^4]: 用Transformer网络探究水文预测的极限（arXiv:2306.12384v1 [cs.LG]）

    Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])

    [http://arxiv.org/abs/2306.12384](http://arxiv.org/abs/2306.12384)

    本文探究了采用Transformer网络对水文预测问题的建模效果，发现相比LSTM模型缺乏优势，可能是由于水文预测问题的马尔可夫特性所导致。

    

    自其引入水文学领域以来，循环神经网络（如LSTM）在已知的可比基准上的日水文图表指标方面一直证明难以超越。除水文学外，Transformer现在已成为连续预测任务的首选模型，这使得它成为一个有趣的架构进行研究。在本文中，我们首先展示了香草Transformer架构在广泛使用的CAMELS数据集上与LSTM相比毫无竞争力，由于短期过程而特别滞后于高流量指标。然而，不需要递归的变体Transformer可以获得与LSTM相混合的比较，产生相同的Kling-Gupta效率系数（KGE）以及其他指标。Transformer没有优势与水文预测问题的马尔可夫特性有关。与LSTM类似，Transformer也可以合并多个强制数据集来改善模型性能。然而，虽然将水文预测问题的关键特征转换为更描述性的空间，Transformer模型似乎无法超越LSTM模型的预测性能。

    For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While t
    
[^5]: 二次型赌臂机的样本复杂度：Hessian相关性界限和最优算法

    Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])

    [http://arxiv.org/abs/2306.12383](http://arxiv.org/abs/2306.12383)

    本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。

    

    在随机零阶优化中，了解如何充分利用底层目标函数的局部几何结构是一个实际相关的问题。我们考虑一种基本情况，即目标函数是二次型的，并且提供了最优Hessian相关样本复杂度的第一个紧密刻画。我们的贡献具有双重性质。首先，从信息论的角度出发，通过引入一种称为能量分配的概念来捕捉搜索算法和目标函数几何结构之间的交互，证明了Hessian相关复杂度的紧密下界。通过解决最优能量谱，得到了配套的上限。其次，算法方面，我们展示了存在一种Hessian无关的算法，能够普遍实现所有Hessian实例的渐近最优样本复杂度。我们算法能够实现的渐近最优样本复杂度对于重尾噪声分布仍然有效。

    In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
    
[^6]: 关于 Gibbs 算法的验证：训练数据集、测试数据集及其聚合

    On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets and their Aggregation. (arXiv:2306.12380v1 [cs.LG])

    [http://arxiv.org/abs/2306.12380](http://arxiv.org/abs/2306.12380)

    本论文分析了 Gibbs 算法对训练数据的相关性，并介绍了用于评估 GA 泛化能力的不同性能指标，以及训练误差和测试误差之间的关联。

    

    本论文分析了 Gibbs 算法（GA）对训练数据的相关性，采用期望经验风险作为性能指标，并获得了 GA 在封闭形式下的灵敏度。通过运用这个描述，可以生成与不同训练集训练的 GA 的测试误差和训练误差相关的显式表达式。利用这些工具，探讨了数据集的聚合，并介绍了用于评估GA泛化能力的不同性能指标。对于这些数据集的特定大小和GA的参数，建立了 Jeffrey's divergence、训练误差和测试误差之间的关联。

    The dependence on training data of the Gibbs algorithm (GA) is analytically characterized. By adopting the expected empirical risk as the performance metric, the sensitivity of the GA is obtained in closed form. In this case, sensitivity is the performance difference with respect to an arbitrary alternative algorithm. This description enables the development of explicit expressions involving the training errors and test errors of GAs trained with different datasets. Using these tools, dataset aggregation is studied and different figures of merit to evaluate the generalization capabilities of GAs are introduced. For particular sizes of such datasets and parameters of the GAs, a connection between Jeffrey's divergence, training and test errors is established.
    
[^7]: PriorBand: 深度学习下的实用超参数优化方法。

    PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning. (arXiv:2306.12370v1 [cs.LG])

    [http://arxiv.org/abs/2306.12370](http://arxiv.org/abs/2306.12370)

    提出了一种针对深度学习量身定制的超参数优化算法PriorBand，能够同时利用专家信念和廉价的代理任务，具有高效性和抗干扰能力。

    

    深度学习（DL）流程中的超参数对其下游性能至关重要。尽管已经开发了许多超参数优化方法，但其代价往往对现代深度学习不可行。因此，手动实验仍是优化超参数的主要方法，依赖于研究人员的直觉、领域知识和廉价的初步探索。为了解决HPO算法和DL研究人员之间的这种不匹配，我们提出了PriorBand，这是一种针对DL量身定制的HPO算法，能够利用专家信念和廉价的代理任务。实验证明，PriorBand在一系列DL基准测试中的效率，以及在提供有效专家输入和抗击不良专家信念方面的收益。

    Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs
    
[^8]: 应用于加速MRI重建的注意力混合变分网络

    Attention Hybrid Variational Net for Accelerated MRI Reconstruction. (arXiv:2306.12365v1 [eess.IV])

    [http://arxiv.org/abs/2306.12365](http://arxiv.org/abs/2306.12365)

    提出了一种应用于加速MRI重建的注意力混合变分网络，可以同时利用k-空间和图像域中的信息进行学习，实验结果表明其在MRI重建中具有较好的性能。

    

    压缩感知（CS）数据重建在加速磁共振成像（MRI）中的应用仍然是一个具有挑战性的问题。原因是来自加速掩模中在k-空间中丢失的信息使得重建出类似于完全采样图像的图像变得困难。已经提出了多种基于深度学习的结构，通过使用k-空间和图像域以及使用展开的优化方法，实现了MRI重建。然而，这些结构的缺点是它们并没有完全利用来自两个域（k-空间和图像）的信息。在这里，我们提出了一种基于深度学习的注意力混合变分网络，它在k-空间和图像域中执行学习。我们评估了我们的方法在一个着名的开源MRI数据集和我们机构的一个诊断为卒中患者的临床MRI数据集上的表现。

    The application of compressed sensing (CS)-enabled data reconstruction for accelerating magnetic resonance imaging (MRI) remains a challenging problem. This is due to the fact that the information lost in k-space from the acceleration mask makes it difficult to reconstruct an image similar to the quality of a fully sampled image. Multiple deep learning-based structures have been proposed for MRI reconstruction using CS, both in the k-space and image domains as well as using unrolled optimization methods. However, the drawback of these structures is that they are not fully utilizing the information from both domains (k-space and image). Herein, we propose a deep learning-based attention hybrid variational network that performs learning in both the k-space and image domain. We evaluate our method on a well-known open-source MRI dataset and a clinical MRI dataset of patients diagnosed with strokes from our institution to demonstrate the performance of our network. In addition to quantitat
    
[^9]: 基于优化和数据驱动的 sigma-point 卡尔曼滤波器与非线性未知输入估计器

    Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via Optimization and Data-driven Approach for Dynamic Systems. (arXiv:2306.12361v1 [eess.SY])

    [http://arxiv.org/abs/2306.12361](http://arxiv.org/abs/2306.12361)

    本文提出了一种不需要假设未知输入为线性的方法，结合非线性优化和数据驱动方法可以实现对未知输入的估计，并通过联合 sigma-point 变换方案将状态和未知输入的不确定性纳入估计中，确保其稳定性。这个方法适用于许多智能自主系统。

    

    多数关于状态和未知输入(UI)估计的文献都要求UI是线性的，这个限制可能太严格了，因为它并不适用于许多智能自主系统。为了克服这一限制，我们提出了一种无导数未知输入 Sigma-point 卡尔曼滤波器(SPKE-nUI)，其中 SPKF 与普通非线性 UI 估计器相互连接，可以通过非线性优化和数据驱动方法实现。非线性 UI 估计器使用后验状态估计，这对状态预测误差不太敏感。此外，我们引入了联合 sigma-point 变换方案，将状态和 UI 的不确定性纳入 SPKF-nUI 的估计中。深入的随机稳定性分析证明了在合理的假设下，所提出的 SPKF-nUI 可以产生指数级收敛的估计误差界限。最后，我们在基于模拟的路面车辆控制问题上进行了两个案例研究。

    Most works on joint state and unknown input (UI) estimation require the assumption that the UIs are linear; this is potentially restrictive as it does not hold in many intelligent autonomous systems. To overcome this restriction and circumvent the need to linearize the system, we propose a derivative-free Unknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is interconnected with a general nonlinear UI estimator that can be implemented via nonlinear optimization and data-driven approaches. The nonlinear UI estimator uses the posterior state estimate which is less susceptible to state prediction error. In addition, we introduce a joint sigma-point transformation scheme to incorporate both the state and UI uncertainties in the estimation of SPKF-nUI. An in-depth stochastic stability analysis proves that the proposed SPKF-nUI yields exponentially converging estimation error bounds under reasonable assumptions. Finally, two case studies are carried out on a simulation-based ri
    
[^10]: 离散行走跳跃采样用于蛋白质发现

    Protein Discovery with Discrete Walk-Jump Sampling. (arXiv:2306.12360v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.12360](http://arxiv.org/abs/2306.12360)

    本文提出了一种离散行走跳跃采样的方法，通过学习平滑能量函数、使用 Langevin Markov 链蒙特卡罗 (MCMC) 算法和一步去噪的投射技术，可以解决离散生成模型的采样难题。同时在抗体蛋白质的生成建模上进行了应用和测试，并引入了分布一致性得分作为基准测试标准。

    

    我们通过学习平滑能量函数、使用 Langevin Markov 链蒙特卡罗 (MCMC) 从平滑数据流形中采样，并使用一步去噪投射回真实数据流形，解决了离散生成模型的训练和采样困难问题。我们的离散行走跳跃采样形式化方法结合了基于能量的模型的最大似然训练和基于分数的模型的改进样本质量，同时通过仅需要一个噪声水平来简化训练和采样。我们在抗体蛋白质的生成建模上评估了我们的方法的鲁棒性，并引入了分布一致性得分以对蛋白质生成模型进行基准测试。通过优化和采样我们的模型用于提议的分布一致性得分，97-100%的生成样品可以成功表达和纯化，并且35%的功能设计在第一次尝试中显示出与已知功能抗体相同或更好的结合亲和力。

    We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the maximum likelihood training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100% of generated samples are successfully expressed and purified and 35% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a sing
    
[^11]: 可证明高效的低秩POMDP规划中表示学习

    Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP. (arXiv:2306.12356v1 [cs.LG])

    [http://arxiv.org/abs/2306.12356](http://arxiv.org/abs/2306.12356)

    本文研究了在部分可观察马尔可夫决策过程中的可证明高效表示学习，提出了可解码和$\gamma$-可观察两种算法，达到高效样本复杂度。

    

    本文研究在部分可观察马尔可夫决策过程（POMDP）中的表示学习，其中代理学习一个解码器函数，将一系列高维原始观察映射到一个紧凑的表示中，并用于更高效的探索和规划。我们关注\textit{ $\gamma$-可观察}和 \textit{可解码POMDP}子类，已经证明了在这些子类中，统计上可处理的学习是可能的，但尚未有计算上高效的算法。我们首先提出了一种可解码POMDP的算法，该算法结合最大似然估计（MLE）和不确定性乐观性（OFU）进行表示学习，实现了高效的样本复杂度，同时只调用了监督学习计算神经元。然后，我们展示了如何将此算法调整为在更广泛的$\gamma$-可观察POMDP类中运行。

    In this paper, we study representation learning in partially observable Markov Decision Processes (POMDPs), where the agent learns a decoder function that maps a series of high-dimensional raw observations to a compact representation and uses it for more efficient exploration and planning.  We focus our attention on the sub-classes of \textit{$\gamma$-observable} and \textit{decodable POMDPs}, for which it has been shown that statistically tractable learning is possible, but there has not been any computationally efficient algorithm. We first present an algorithm for decodable POMDPs that combines maximum likelihood estimation (MLE) and optimism in the face of uncertainty (OFU) to perform representation learning and achieve efficient sample complexity, while only calling supervised learning computational oracles. We then show how to adapt this algorithm to also work in the broader class of $\gamma$-observable POMDPs.
    
[^12]: ProtoGate：面向表格型生物医学数据的原型神经网络与本地特征选择

    ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data. (arXiv:2306.12330v1 [cs.LG])

    [http://arxiv.org/abs/2306.12330](http://arxiv.org/abs/2306.12330)

    本文提出了ProtoGate，一种基于原型的神经模型，该模型通过考虑样本内和样本间的同质性和异质性来引入归纳偏差，并以全局到本地的方式选择特征，从而提高预测精度并使预测具有可解释性。

    

    表格型生物医学数据在机器学习中面临挑战，因为它往往是高维的，但样本量又相对较小。先前的研究试图通过特征选择方法来应对这些挑战，但这种方法可能在真实数据上表现不稳定。这表明当前方法缺乏适当的归纳偏差，不能捕捉不同样本之间的共同模式。在本文中，我们提出ProtoGate，这是一种基于原型的神经模型，通过考虑样本内和样本间的同质性和异质性来引入归纳偏差。ProtoGate以全局到本地的方式选择特征，并利用它们来生产可解释的预测，而原型模型使预测具有可解释性。我们进行了全面的实验来评估ProtoGate在合成和真实世界数据集上的性能。我们的结果表明，利用数据中的同质性和异质性模式可以提高预测精度，同时原型赋予了预测可解释性。

    Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size. Previous research has attempted to address these challenges via feature selection approaches, which can lead to unstable performance on real-world data. This suggests that current methods lack appropriate inductive biases that capture patterns common to different samples. In this paper, we propose ProtoGate, a prototype-based neural model that introduces an inductive bias by attending to both homogeneity and heterogeneity across samples. ProtoGate selects features in a global-to-local manner and leverages them to produce explainable predictions via an interpretable prototype-based model. We conduct comprehensive experiments to evaluate the performance of ProtoGate on synthetic and real-world datasets. Our results show that exploiting the homogeneous and heterogeneous patterns in the data can improve prediction accuracy while prototypes imbue interpretability.
    
[^13]: 可解释的迁移学习中的内省行动建议

    Introspective Action Advising for Interpretable Transfer Learning. (arXiv:2306.12314v1 [cs.LG])

    [http://arxiv.org/abs/2306.12314](http://arxiv.org/abs/2306.12314)

    本文提出了一种基于行动建议的迁移学习方法，可以更加解释性和细粒度地传输知识，并在一些情况下可以取得比标准权重复制方法更好的结果。

    

    迁移学习可以应用于深度强化学习中，通过将从相关源任务中学习到的策略的知识传输到目标任务中，加速目标任务策略的训练。通常，这是通过在训练之前将预训练权重从源策略复制到目标策略来实现的，并且要求它们使用相同的模型架构。然而，这不仅需要在广泛状态分布中学习到的强健表示——通常难以在针对单个任务训练的专业模型之间进行转移——而且很大程度上不可解释，提供很少有关迁移的知识。在本文中，我们提出了一种基于行动建议的任务之间的迁移学习的替代方法，在这种方法中，源任务中训练的教师积极指导目标任务中学生的探索。通过内省，教师能够确定何时有利于学生给予建议，在不确定时，为更可解释和细粒度的知识传输提供了可能性。我们在一组Atari游戏上评估了我们的方法，表明它在某些情况下可以胜过标准的权重复制方法，同时也提供了有关传输的知识。

    Transfer learning can be applied in deep reinforcement learning to accelerate the training of a policy in a target task by transferring knowledge from a policy learned in a related source task. This is commonly achieved by copying pretrained weights from the source policy to the target policy prior to training, under the constraint that they use the same model architecture. However, not only does this require a robust representation learned over a wide distribution of states -- often failing to transfer between specialist models trained over single tasks -- but it is largely uninterpretable and provides little indication of what knowledge is transferred. In this work, we propose an alternative approach to transfer learning between tasks based on action advising, in which a teacher trained in a source task actively guides a student's exploration in a target task. Through introspection, the teacher is capable of identifying when advice is beneficial to the student and should be given, an
    
[^14]: 超越深度集成——基于分布偏移下贝叶斯深度学习的大规模评估

    Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])

    [http://arxiv.org/abs/2306.12306](http://arxiv.org/abs/2306.12306)

    该论文在多个具有挑战性的分类和回归任务上对现代BDL算法进行了系统性评估，重点关注了在分布偏移下的泛化能力和校准能力，并研究了一种带符号的期望校准误差版本。

    

    贝叶斯深度学习（BDL）是实现在分布偏移数据上进行良好校准预测的有前途的方法。然而，缺乏大规模调查，以系统方式评估最近的 SOTA 方法在多样、现实和具有挑战性的基准任务上的表现。为了清晰了解BDL研究的当前状况，我们在来自WILDS集合的现实世界数据集上评估现代BDL算法，包含具有挑战性的分类和回归任务，重点关注在分布偏移下的泛化性能和校准能力。我们比较了一系列大型，卷积和基于 transformer 的神经网络结构上的算法，并研究了一个带符号的期望校准误差版本，揭示出方法是过度自信还是低振幅，进一步深入研究方法的行为。此外，我们为了首次系统评估BDL在微调大型预训练模型上表现，做了更多的工作。

    Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, 
    
[^15]: StarVQA+: 针对视频质量评估的联合时空注意力协同训练

    StarVQA+: Co-training Space-Time Attention for Video Quality Assessment. (arXiv:2306.12298v1 [cs.CV])

    [http://arxiv.org/abs/2306.12298](http://arxiv.org/abs/2306.12298)

    本文提出了一个针对视频质量评估的联合时空注意力协同训练模型StarVQA+，通过将主观平均分数编码为概率向量并嵌入一个特殊令牌作为可学习变量设计了一个向量化的回归损失函数，使用图像和视频来联合训练空间和时间注意权重，在野外视频质量评估数据集上实验，实验结果表明StarVQA+比现有的最先进方法更优秀。

    

    基于自注意力机制的Transformer在许多计算机视觉任务中取得了巨大的成功。然而，其在视频质量评估（VQA）中的应用迄今为止并不令人满意。评估野外视频的质量对原始参考和拍摄畸变的未知情况表示出了挑战。本文提出了一个针对VQA问题的联合时空注意网络，称为StarVQA+。具体而言，我们首先通过交替连接分割的时空注意力来构建StarVQA+。然后，为了促进StarVQA+的训练，我们通过将主观平均分数（MOS）编码为概率向量并嵌入一个特殊令牌作为MOS的可学习变量，设计了一个向量化的回归损失函数，从而更好地拟合人类评分过程。最后，为了解决Transformer的数据需求问题，我们建议使用图像和视频来联合训练空间和时间注意权重。本文对业已公认的野外视频质量评估数据集进行了各种实验，实验结果表明，StarVQA+比现有的最先进方法更优秀。

    Self-attention based Transformer has achieved great success in many computer vision tasks. However, its application to video quality assessment (VQA) has not been satisfactory so far. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a co-trained Space-Time Attention network for the VQA problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately concatenating the divided space-time attention. Then, to facilitate the training of StarVQA+, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to better fitting of human's rating process. Finally, to solve the data hungry problem with Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild vi
    
[^16]: 基于扩散模型的强化单通道去混响方法

    Diffusion Posterior Sampling for Informed Single-Channel Dereverberation. (arXiv:2306.12286v1 [eess.AS])

    [http://arxiv.org/abs/2306.12286](http://arxiv.org/abs/2306.12286)

    本文提出了一种基于扩散模型的强化单通道去混响方法，鲁棒性更强，能有效地处理非平稳噪声，并在大混响时间下表现出优越性。

    

    本文提出了一种基于条件生成的扩散模型的强化单通道去混响方法。通过对房间冲激响应的了解，利用测量一致性准则和代表清晰语音先验的神经网络，反向扩散产生消混响语音。与最先进的强化单通道去混响方法相比，该方法对测量噪声更加鲁棒，特别是对于非平稳噪声。此外，我们使用扩散模型与其他盲去混响方法进行比较，并展示了该方法在大混响时间下的优越性。我们通过引入盲去混响的扩展来提出所呈现的算法，允许同时估计房间冲激响应和消混响语音。可在网上找到音频样本和代码（https://uhh.de/inf-sp-derev-dps）。

    We present in this paper an informed single-channel dereverberation method based on conditional generation with diffusion models. With knowledge of the room impulse response, the anechoic utterance is generated via reverse diffusion using a measurement consistency criterion coupled with a neural network that represents the clean speech prior. The proposed approach is largely more robust to measurement noise compared to a state-of-the-art informed single-channel dereverberation method, especially for non-stationary noise. Furthermore, we compare to other blind dereverberation methods using diffusion models and show superiority of the proposed approach for large reverberation times. We motivate the presented algorithm by introducing an extension for blind dereverberation allowing joint estimation of the room impulse response and anechoic speech. Audio samples and code can be found online (https://uhh.de/inf-sp-derev-dps).
    
[^17]: 基于深度学习的弹性稀疏阵列雷达

    Resilient Sparse Array Radar with the Aid of Deep Learning. (arXiv:2306.12285v1 [cs.LG])

    [http://arxiv.org/abs/2306.12285](http://arxiv.org/abs/2306.12285)

    本文提出了两种基于机器学习的方法来应对稀疏阵列中传感器失效问题，显著提高了雷达的性能，同时保持方向估计的高分辨率特点。

    

    本文针对稀疏阵列中传感器失效的多目标方向估计问题进行研究。我们提出了两种基于机器学习的方法来减轻传感器失效的影响并保持方向估计的性能和分辨率。数值结果表明，这两种方法都可以显著提高阵列的性能，即使两个传感器失败。

    In this paper, we address the problem of direction of arrival (DOA) estimation for multiple targets in the presence of sensor failures in a sparse array. Generally, sparse arrays are known with very high-resolution capabilities, where N physical sensors can resolve up to $\mathcal{O}(N^2)$ uncorrelated sources. However, among the many configurations introduced in the literature, the arrays that provide the largest hole-free co-array are the most susceptible to sensor failures. We propose here two machine learning (ML) methods to mitigate the effect of sensor failures and maintain the DOA estimation performance and resolution. The first method enhances the conventional spatial smoothing using deep neural network (DNN), while the second one is an end-to-end data-driven method. Numerical results show that both approaches can significantly improve the performance of MRA with two failed sensors. The data-driven method can maintain the performance of the array with no failures at high signal
    
[^18]: 凸集机器学习建议下的在线资源分配

    Online Resource Allocation with Convex-set Machine-Learned Advice. (arXiv:2306.12282v1 [cs.DS])

    [http://arxiv.org/abs/2306.12282](http://arxiv.org/abs/2306.12282)

    该论文提出了一个框架，使用凸集机器学习建议来增强在线资源分配决策。该算法类在一致比率和鲁棒比率之间平衡，并在实验中表现出优异的性能。

    

    在线决策者通常会使用机器学习预测需求，称为建议，该建议可以在资源分配的在线决策过程中潜在地被利用。但是，由于其潜在的不准确性，利用这样的建议会带来挑战。为了解决这个问题，我们提出了一个框架，通过潜在不可靠的机器学习建议增强在线资源分配决策。我们假设该建议由需求向量的一般凸不确定性集表示。我们介绍了一种参数化的 Pareto 最优在线资源分配算法类，该算法在一致比率和鲁棒比率之间平衡。一致比率是指当机器学习建议准确时，算法相对于最优的后见之明解的表现，而鲁棒比率则捕获了建议不准确时对抗性需求过程下的表现。具体而言，我们在 C-Pareto 最优情况下，最大化资源利用，同时在一致比率和鲁棒比率之间实现平衡。我们的实验表明，我们提出的框架在合成和真实数据集上均优于基线。

    Decision-makers often have access to a machine-learned prediction about demand, referred to as advice, which can potentially be utilized in online decision-making processes for resource allocation. However, exploiting such advice poses challenges due to its potential inaccuracy. To address this issue, we propose a framework that enhances online resource allocation decisions with potentially unreliable machine-learned (ML) advice. We assume here that this advice is represented by a general convex uncertainty set for the demand vector.  We introduce a parameterized class of Pareto optimal online resource allocation algorithms that strike a balance between consistent and robust ratios. The consistent ratio measures the algorithm's performance (compared to the optimal hindsight solution) when the ML advice is accurate, while the robust ratio captures performance under an adversarial demand process when the advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize the r
    
[^19]: 从结构挖掘到无监督探索八面体原子网络

    From structure mining to unsupervised exploration of atomic octahedral networks. (arXiv:2306.12272v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2306.12272](http://arxiv.org/abs/2306.12272)

    该论文提出了一种自动化的化学直觉算法，可以对配位八面体网络进行几何解析、量化和分类，并利用无监督机器学习对无机框架聚型进行分类。作者发现了ABO$_{3}$钙钛矿多形体系中的轴向倾斜趋势，并揭示了Pauling的第三条规则违反和设计原则的更新。

    

    原子中心的配位八面体网络通常出现在无机和混合固态材料中。表征它们的空间排列和特性对于许多材料系列将结构与性质联系起来至关重要。传统的逐案例检查方法在发现大型数据集中的趋势和相似之处方面变得不可行。在这里，我们运用化学直觉自动化地进行几何解析、量化和分类配位八面体网络。我们在ABO$_{3}$钙钛矿多形体系中发现了轴向倾斜趋势，有助于检测氧化态变化。此外，我们开发了一个尺度不变的编码方案来表示这些网络，并与人类辅助的无监督机器学习相结合，可以对混合碘铅酸盐(A$_x$Pb$_y$I$_z$)中的无机框架聚型进行分类。因此，我们揭示了Pauling的第三条规则违反和设计原则ereotype0的更新。

    Networks of atom-centered coordination octahedra commonly occur in inorganic and hybrid solid-state materials. Characterizing their spatial arrangements and characteristics is crucial for relating structures to properties for many materials families. The traditional method using case-by-case inspection becomes prohibitive for discovering trends and similarities in large datasets. Here, we operationalize chemical intuition to automate the geometric parsing, quantification, and classification of coordination octahedral networks. We find axis-resolved tilting trends in ABO$_{3}$ perovskite polymorphs, which assist in detecting oxidation state changes. Moreover, we develop a scale-invariant encoding scheme to represent these networks, which, combined with human-assisted unsupervised machine learning, allows us to taxonomize the inorganic framework polytypes in hybrid iodoplumbates (A$_x$Pb$_y$I$_z$). Consequently, we uncover a violation of Pauling's third rule and the design principles und
    
[^20]: 一种用于解决高维Committor问题的有限表达式方法

    A Finite Expression Method for Solving High-Dimensional Committor Problems. (arXiv:2306.12268v1 [math.NA])

    [http://arxiv.org/abs/2306.12268](http://arxiv.org/abs/2306.12268)

    本文提出了一种用于解决高维Committor问题的有限表达式方法(FEX)，该方法通过深度神经网络学习最优非线性函数和系数值，能够显著提高计算效果。

    

    转移路径理论（TPT）是一种数学框架，用于量化从选定的亚稳态$A$到$B$之间的稀有转移事件。TPT的核心是Committor函数，其描述了从相空间的任何起始点到达亚稳态$B$之前到达$A$的概率。计算出Committor之后，可以立即找到转换通道和转换速率。Committor是具有适当边界条件的反向Kolmogorov方程的解。然而，在高维情况下，由于需要网格化整个环境空间，解决Committor是一项具有挑战性的任务。在这项工作中，我们探索了有限表达式方法（FEX，Liang和Yang（2022））作为计算Committor的工具。FEX通过涉及一定数量的非线性函数和二进制算术运算的固定有限代数表达式来逼近Committor。最佳的非线性函数、二进制运算和数值系数值通过深度神经网络从训练数据中学习到。我们通过解决多个高维Committor问题，其中包括高达400个维度，展示了FEX的有效性，并且表明FEX显著优于传统的数值方法，如有限元方法和有限差分方法。

    Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coeffi
    
[^21]: 利用统计和深度学习模型与多光谱数据相结合，以改善直接成像高对比度外行星探测

    Combining multi-spectral data with statistical and deep-learning models for improved exoplanet detection in direct imaging at high contrast. (arXiv:2306.12266v1 [astro-ph.IM])

    [http://arxiv.org/abs/2306.12266](http://arxiv.org/abs/2306.12266)

    本论文提出了一种利用统计和深度学习模型与多光谱数据相结合的方法，对直接成像的外行星探测进行改进，通过建立统计模型并进行卷积神经网络训练，在精度和召回之间实现更好的权衡。

    

    直接成像的外行星探测是一项艰巨的任务：有兴趣的物体的微弱信号被主星引起的空间结构繁殖的干扰成分所掩盖。只有将几次观测和专用探测算法相结合才能识别外行星信号。与大多数现有方法不同，我们建议从观测数据中直接学习干扰信号的空间、时间和光谱特征的模型。在预处理步骤中，本地建立它们之间相关性的统计模型，并对数据进行中心化和白化，以改善它们的平稳性和信噪比（SNR）。然后，在监督模式下对卷积神经网络（CNN）进行训练，以检测预处理图像中合成源的残余特征。我们的方法在精度和召回之间实现了更好的权衡，比领域内标准方法表现更好。它也优于基于单纯的吸收特征的最新算法。

    Exoplanet detection by direct imaging is a difficult task: the faint signals from the objects of interest are buried under a spatially structured nuisance component induced by the host star. The exoplanet signals can only be identified when combining several observations with dedicated detection algorithms. In contrast to most of existing methods, we propose to learn a model of the spatial, temporal and spectral characteristics of the nuisance, directly from the observations. In a pre-processing step, a statistical model of their correlations is built locally, and the data are centered and whitened to improve both their stationarity and signal-to-noise ratio (SNR). A convolutional neural network (CNN) is then trained in a supervised fashion to detect the residual signature of synthetic sources in the pre-processed images. Our method leads to a better trade-off between precision and recall than standard approaches in the field. It also outperforms a state-of-the-art algorithm based sole
    
[^22]: 使用排名模块和语音增强的语音转换自动语音解缠

    Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation. (arXiv:2306.12259v1 [cs.SD])

    [http://arxiv.org/abs/2306.12259](http://arxiv.org/abs/2306.12259)

    该论文提出了一种VC模型，可使用仅两个增强函数自动将语音分解为四个部分，而无需多个手工制作的特征或费力的瓶颈调整，并且实证结果表明，相对于基线，模型在解脱有效性和语音自然度方面表现更好。

    

    语音转换（VC）可以将源语音的声音转换为目标语音，同时保持源语音的内容。语音主要可分解为四个部分：内容、音色、节奏和音高。不幸的是，大多数相关工作只考虑了内容和音色，结果导致语音不够自然。最近一些研究可以将语音解脱成几个组成部分，但它们需要费力的瓶颈调整或各种手工制作的特征，每个特征都假定包含解脱的语音信息。在本文中，我们提出了一种VC模型，可使用仅两个增强函数自动将语音分解为四个部分，而无需多个手工制作的特征或费力的瓶颈调整。所提出的模型简单而高效，并且实证结果表明，相对于基线，我们的模型在解脱有效性和语音自然度方面表现更好。

    Voice Conversion (VC) converts the voice of a source speech to that of a target while maintaining the source's content. Speech can be mainly decomposed into four components: content, timbre, rhythm and pitch. Unfortunately, most related works only take into account content and timbre, which results in less natural speech. Some recent works are able to disentangle speech into several components, but they require laborious bottleneck tuning or various hand-crafted features, each assumed to contain disentangled speech information. In this paper, we propose a VC model that can automatically disentangle speech into four components using only two augmentation functions, without the requirement of multiple hand-crafted features or laborious bottleneck tuning. The proposed model is straightforward yet efficient, and the empirical results demonstrate that our model can achieve a better performance than the baseline, regarding disentanglement effectiveness and speech naturalness.
    
[^23]: GADBench：重新审视和对监督图形异常检测进行基准测试

    GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection. (arXiv:2306.12251v1 [cs.LG])

    [http://arxiv.org/abs/2306.12251](http://arxiv.org/abs/2306.12251)

    GADBench是一个全面的基准系统，发现了树集成和简单邻域汇聚方法胜过所有23个模型，包括最新的针对GAD任务量身定制的GNN模型。

    

    长期以来，传统图形异常检测（GAD）算法和最近流行的图形神经网络（GNN）一直存在。目前尚不清楚它们在标准综合设置下的性能如何，GNN是否优于传统算法（如树集成）以及它们在大规模图表上的效率如何。为了解决这些问题，我们提出了GADBench - 一个静态图形监督异常节点检测的全面基准。GADBench在从数千到数百万节点（约6M）的十个真实GAD数据集上提供了23种不同模型的彻底比较。我们的主要发现是，具有简单邻域汇聚的树集成胜过所有其他基线，包括最新的针对GAD任务量身定制的GNN。通过将GADBench作为开源工具提供，我们提供了有关GAD当前进展的关键见解，并为未来研究奠定了坚实的基础。

    With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs outperform traditional algorithms such as tree ensembles, and (3) their efficiency on large-scale graphs. In response, we present GADBench -- a comprehensive benchmark for supervised anomalous node detection on static graphs. GADBench provides a thorough comparison across 23 distinct models on ten real-world GAD datasets ranging from thousands to millions of nodes ($\sim$6M). Our main finding is that tree ensembles with simple neighborhood aggregation outperform all other baselines, including the latest GNNs tailored for the GAD task. By making GADBench available as an open-source tool, we offer pivotal insights into the current advancements of GAD and establish a solid foundation for future research. Our code is available at https://github.com/squareRoot3/GADBen
    
[^24]: 基于知识的多模态音乐相似性研究

    Knowledge-based Multimodal Music Similarity. (arXiv:2306.12249v1 [cs.SD])

    [http://arxiv.org/abs/2306.12249](http://arxiv.org/abs/2306.12249)

    本文研究了基于知识的多模态音乐相似性，旨在开发一个完全可解释和可解释的系统，为最终用户提供更多对音乐相似性和分类系统的控制和理解。

    

    音乐相似性是音乐检索、推荐系统和音乐分析的重要方面。而且，对于音乐专家来说，相似性可以研究作曲家和历史时期之间的类比和影响。目前，针对音乐相似性的方法主要依赖符号内容，这可能成本高昂且不总是易于获得。相比之下，使用音频信号的方法通常无法提供有关观察到的相似性背后原因的任何见解。本研究通过使用符号和音频内容来研究音乐相似性，解决了当前方法的局限性。本研究的目的是开发一个完全可解释和可解释的系统，可以为最终用户提供更多对音乐相似性和分类系统的控制和理解。

    Music similarity is an essential aspect of music retrieval, recommendation systems, and music analysis. Moreover, similarity is of vital interest for music experts, as it allows studying analogies and influences among composers and historical periods. Current approaches to musical similarity rely mainly on symbolic content, which can be expensive to produce and is not always readily available. Conversely, approaches using audio signals typically fail to provide any insight about the reasons behind the observed similarity. This research addresses the limitations of current approaches by focusing on the study of musical similarity using both symbolic and audio content. The aim of this research is to develop a fully explainable and interpretable system that can provide end-users with more control and understanding of music similarity and classification systems.
    
[^25]: 基于Transformer网络的脑部CT图像缺血损伤同时标记和估算成像时长的研究

    Concurrent ischemic lesion age estimation and segmentation of CT brain using a Transformer-based network. (arXiv:2306.12242v1 [eess.IV])

    [http://arxiv.org/abs/2306.12242](http://arxiv.org/abs/2306.12242)

    提出了一种基于Transformer网络的新方法，能够同时标记和估算脑部CT图像中的缺血性病变的成像时长，该方法在病灶分割和病灶年龄估计方面实现了最先进的性能，有望成为支持中风护理决策的重要工具。

    

    脑卒中护理的核心是迅速处理，这取决于中风发作的时间。因此，临床决策需要准确知道时间，通常需要放射医师解读脑部CT扫描以确认事件的发生和年龄。由于急性缺血性病变表现微妙且外观动态，因此这些任务特别具有挑战性。自动化的努力还没有将深度学习应用于估算病灶年龄，并将这两个任务独立处理，因此忽略了它们固有的互补关系。为了利用这一点，我们提出了一种新颖的端到端多任务Transformer网络，优化了同时标记和估计脑部缺血性病变的年龄。通过利用门控位置自我关注和CT特定数据增强，所提出的方法可以捕获长距离的空间依赖关系，同时保持其可传输到其他脑部CT成像数据集的能力。结果表明，我们的方法在病灶分割和准确估计病灶年龄方面实现了最先进的性能，成为支持中风护理决策的有前途的工具。

    The cornerstone of stroke care is expedient management that varies depending on the time since stroke onset. Consequently, clinical decision making is centered on accurate knowledge of timing and often requires a radiologist to interpret Computed Tomography (CT) of the brain to confirm the occurrence and age of an event. These tasks are particularly challenging due to the subtle expression of acute ischemic lesions and the dynamic nature of their appearance. Automation efforts have not yet applied deep learning to estimate lesion age and treated these two tasks independently, so, have overlooked their inherent complementary relationship. To leverage this, we propose a novel end-to-end multi-task transformer-based network optimized for concurrent segmentation and age estimation of cerebral ischemic lesions. By utilizing gated positional self-attention and CT-specific data augmentation, the proposed method can capture long-range spatial dependencies while maintaining its ability to be tr
    
[^26]: 利用等变图神经网络预测蛋白质变体

    Predicting protein variants with equivariant graph neural networks. (arXiv:2306.12231v1 [cs.LG])

    [http://arxiv.org/abs/2306.12231](http://arxiv.org/abs/2306.12231)

    本文比较了等变图神经网络和基于序列的方法在预测蛋白质变体方面的能力，并发现我们提出的结构方法在训练更少的分子的情况下可以达到与基于序列的方法相竞争的性能。

    

    预训练模型在许多蛋白质工程任务中取得了成功。其中，基于序列的模型在蛋白质适应性预测方面取得了最先进的性能，而基于结构的模型则已被实验性地用于开发具有增强功能的蛋白质。然而，在预测优于野生型蛋白质的蛋白质变体方面，基于结构和基于序列的方法比较这一研究领域存在研究空白。本文旨在通过比较等变图神经网络（EGNN）和基于序列的方法识别有前途的氨基酸突变的能力，来解决这一空白。结果表明，我们提出的结构方法在训练更少的分子的情况下，可以达到与基于序列的方法相竞争的性能。此外，我们发现将测定标记数据与结构预训练模型相结合，与序列预训练模型的效果类似。

    Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models.
    
[^27]: 奇妙的权重及其查找方法：动态稀疏训练中的剪枝位置

    Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])

    [http://arxiv.org/abs/2306.12230](http://arxiv.org/abs/2306.12230)

    该论文对动态稀疏训练中的剪枝位置进行了实证分析，发现在低密度范围内，最简单的大小方法提供了最佳性能。

    

    动态稀疏训练（DST）是一个快速发展的研究领域，旨在通过在训练过程中调整神经网络的拓扑结构来优化其稀疏初始化。已经证明，在特定条件下，DST能够胜过密集模型。该框架的关键组成部分是剪枝和生长标准，这些标准在训练过程中被反复应用以调整网络的稀疏连接。虽然生长标准对DST性能的影响相对较好地研究了，但剪枝标准的影响仍然被忽视。为解决这个问题，我们设计并进行了对各种剪枝标准的广泛实证分析，以更好地了解它们对 DST 解决方案动态的影响。令人惊讶的是，我们发现大多数研究方法都产生类似的结果。在低密度范围内，最简单的技术——基于大小的方法，提供了最佳性能。

    Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
    
[^28]: 面向剩余使用寿命预测的自动化机器学习

    Automated Machine Learning for Remaining Useful Life Predictions. (arXiv:2306.12215v1 [cs.LG])

    [http://arxiv.org/abs/2306.12215](http://arxiv.org/abs/2306.12215)

    本文介绍了一种自动化的机器学习方法，名为AutoRUL，用于自动预测工程系统的剩余使用寿命（RUL）。该方法将微调的标准回归方法与高预测能力的集成相结合，并通过八个真实世界的和合成数据集的评估，证明AutoML提供了一种可行的选择。

    

    预测工程系统的剩余使用寿命（RUL）是预测与健康管理中的重要任务。最近，数据驱动的方法在RUL预测中普及，相比模型驱动的方法不需要工程系统的物理知识。但是，这只是将需要的物理专业知识替换成机器学习（ML）专业知识，而这种专业知识通常也不可得。自动化机器学习（AutoML）承诺自动构建端到端的ML管道，使领域专家而非ML专家能够创建自己的模型。本文介绍了AutoRUL，一种AutoML驱动的端到端方法，用于自动RUL预测。AutoRUL将微调的标准回归方法与高预测能力的集成相结合。通过将所提出的方法用于八个真实世界的和合成数据集，与最先进的手工模型进行比较，我们表明AutoML提供了一种可行的选择。

    Being able to predict the remaining useful life (RUL) of an engineering system is an important task in prognostics and health management. Recently, data-driven approaches to RUL predictions are becoming prevalent over model-based approaches since no underlying physical knowledge of the engineering system is required. Yet, this just replaces required expertise of the underlying physics with machine learning (ML) expertise, which is often also not available. Automated machine learning (AutoML) promises to build end-to-end ML pipelines automatically enabling domain experts without ML expertise to create their own models. This paper introduces AutoRUL, an AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL combines fine-tuned standard regression methods to an ensemble with high predictive power. By evaluating the proposed method on eight real-world and synthetic datasets against state-of-the-art hand-crafted models, we show that AutoML provides a viable alternative to 
    
[^29]: 更多的PAC-Bayes Bounds：从有界损失到具有一般性尾部行为的损失，到任何时间均有效的损失。

    More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])

    [http://arxiv.org/abs/2306.12214](http://arxiv.org/abs/2306.12214)

    本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。

    

    本文针对不同类型的损失提出了新的高概率PAC-Bayes界限。首先，针对有界范围的损失，我们提出了Catoni界的加强版本，适用于所有参数值的统一界。这导致了新的快速速率和混合速率上限，这些上限可解释性强且比文献中先前界限更紧。其次，针对更一般的尾部行为的损失，我们引入了两个新的无参数上限：当损失的累积生成函数有界时，我们引入了一个PAC-Bayes Chernoff类比，另一个上限是损失的二阶矩有界。这两个上限是利用一种基于可能事件空间的离散化的新技术获得的，“在概率”参数优化问题。最后，我们使用一种适用于任何现有界限的简单技术将所有先前结果扩展到任何时间有效的上限。

    In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
    
[^30]: MimiC：模仿中心更新解决联邦学习中的客户端退出问题

    MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])

    [http://arxiv.org/abs/2306.12212](http://arxiv.org/abs/2306.12212)

    本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。

    

    联邦学习是一种有前途的隐私保护协作学习框架。在联邦学习中，模型训练任务分发给客户端，只需要在中央服务器收集模型更新。然而，在移动边缘网络中部署时，客户端（如智能手机和可穿戴设备）可能会无预警地退出任何一次训练迭代，这会阻碍联邦学习达到收敛。本文解决了联邦学习中这一关键挑战，设计出一种名为 MimiC 的新型训练算法，该算法在中心服务器修改其更新以模仿缺失客户端更新，通过实验结果显示，MimiC 相对现有方法在多个基准数据集上均取得了更高的测试准确率和更低的通信成本。

    Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
    
[^31]: 揭开黑匣子：分析预训练语言模型在非语言任务中的注意力权重和隐藏状态

    Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])

    [http://arxiv.org/abs/2306.12198](http://arxiv.org/abs/2306.12198)

    本论文揭示了预训练语言模型在非语言任务中的内部运作，具体使用约束算术问题探索模型的注意力权重分数和隐藏状态，并发现了有前途的结果，该模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。

    

    由于大多数先进模型的“黑匣子”特性，研究深度学习语言模型一直是一个重要的研究领域。随着基于transformers的预训练语言模型的最近进展及其在日常生活中的不断集成，解决这个问题变得更加紧迫。为了实现可解释的AI模型，必须理解涉及的过程步骤，并将其与人类思维过程进行比较。因此，在本文中，我们使用简单易懂的非语言任务来探索这些模型的内部运作。具体来说，我们将预训练语言模型应用于具有分层结构的约束算术问题，以分析其注意力权重分数和隐藏状态。调查结果显示出令人鼓舞的结果，模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。

    Investigating deep learning language models has always been a significant research area due to the ``black box" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye
    
[^32]: 6G边缘网络中的Split Learning

    Split Learning in 6G Edge Networks. (arXiv:2306.12194v1 [cs.LG])

    [http://arxiv.org/abs/2306.12194](http://arxiv.org/abs/2306.12194)

    Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.

    

    随着分布式边缘计算资源的普及，6G移动网络将发展成为一个连接智能的网络。在这条线路上，将联邦学习纳入移动边缘的提议近年来引起了相当大的兴趣。然而，联邦学习的部署面临着重大的挑战，因为庞大的资源受限的物联网设备几乎无法支持设备上的模型训练。这导致了Split Learning (SL)的出现，它使服务器处理主要的训练工作负载，同时增强数据隐私。在本文中，我们简要概述了SL的关键发展，并阐述了其与无线边缘网络的无缝集成。我们首先说明了定制的6G体系结构，以支持边缘SL。然后，我们研究了边缘SL的关键设计问题，包括创新的资源高效学习框架和在单个边缘服务器下的资源管理策略。此外，我们扩展了Split Learning模型的多边缘服务器方案。最后，我们提出了全面的未来研究方向和一些刺激人心的SL在6G边缘网络中的应用。

    With the proliferation of distributed edge computing resources, the 6G mobile network will evolve into a network for connected intelligence. Along this line, the proposal to incorporate federated learning into the mobile edge has gained considerable interest in recent years. However, the deployment of federated learning faces substantial challenges as massive resource-limited IoT devices can hardly support on-device model training. This leads to the emergence of split learning (SL) which enables servers to handle the major training workload while still enhancing data privacy. In this article, we offer a brief overview of key advancements in SL and articulate its seamless integration with wireless edge networks. We begin by illustrating the tailored 6G architecture to support edge SL. Then, we examine the critical design issues for edge SL, including innovative resource-efficient learning frameworks and resource management strategies under a single edge server. Additionally, we expand t
    
[^33]: 在标签噪声下量化彩票中奖号码：准确性、校准度和复杂度

    Quantifying lottery tickets under label noise: accuracy, calibration, and complexity. (arXiv:2306.12190v1 [cs.LG])

    [http://arxiv.org/abs/2306.12190](http://arxiv.org/abs/2306.12190)

    本文使用双稀疏下降方法识别和表征与分类任务相关的修剪模型，展示了这些模型的大小与任务难度呈现振荡态势；同时，相较于全网络，这些修剪模型更好地捕捉到真实条件概率分布。

    

    对深度神经网络进行修剪是减轻机器学习计算负担的一种广泛使用的策略。压倒性的经验证据表明，修剪模型即使只有极少量的参数，仍然能保持非常高的准确性。然而，相对少量的工作用于表征所获得的小型修剪网络，仅仅依靠精度的度量。本文使用稀疏双下降方法识别、表征与分类任务相关的修剪模型。我们经验证实，对于给定的任务，迭代幅值剪枝(IMP)倾向于从大小跨度几个数量级的全网络开始收敛到可比大小的网络。我们在一个可控实验环境中分析最佳修剪模型，并展示他们的参数数量反映了任务难度，而且他们比全网络更好的捕捉到真实条件概率分布。

    Pruning deep neural networks is a widely used strategy to alleviate the computational burden in machine learning. Overwhelming empirical evidence suggests that pruned models retain very high accuracy even with a tiny fraction of parameters. However, relatively little work has gone into characterising the small pruned networks obtained, beyond a measure of their accuracy. In this paper, we use the sparse double descent approach to identify univocally and characterise pruned models associated with classification tasks. We observe empirically that, for a given task, iterative magnitude pruning (IMP) tends to converge to networks of comparable sizes even when starting from full networks with sizes ranging over orders of magnitude. We analyse the best pruned models in a controlled experimental setup and show that their number of parameters reflects task difficulty and that they are much better than full networks at capturing the true conditional probability distribution of the labels. On re
    
[^34]: 自私推理加速下的自适应深度神经网络手术与按需边缘资源

    Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand Edge Resource. (arXiv:2306.12185v1 [cs.GT])

    [http://arxiv.org/abs/2306.12185](http://arxiv.org/abs/2306.12185)

    本研究提出了一个新颖的分散式DNN手术框架（DDS），并提出了一种资源自适应动态协商（R-ADB）方法，以便自适应地适应变化的资源情况。

    

    深度神经网络（DNN）显著提高了移动设备的智能应用准确性。DNN手术将DNN处理分割在移动设备和多接入边缘计算（MEC）服务器之间，可以在移动设备计算能力有限的情况下实现实时推理。然而，DNN手术面临一个关键挑战：确定服务器的最佳计算资源需求和相应的分区策略，同时考虑推理延迟和MEC服务器使用成本。本文引入了一个新颖的分散式DNN手术（DDS）框架。我们将分区策略表述为最小割，并提出一种资源自适应动态协商 (R-ADB) 方法，以便自适应地适应变化的资源情况。

    Deep Neural Networks (DNNs) have significantly improved the accuracy of intelligent applications on mobile devices. DNN surgery, which partitions DNN processing between mobile devices and multi-access edge computing (MEC) servers, can enable real-time inference despite the computational limitations of mobile devices. However, DNN surgery faces a critical challenge: determining the optimal computing resource demand from the server and the corresponding partition strategy, while considering both inference latency and MEC server usage costs. This problem is compounded by two factors: (1) the finite computing capacity of the MEC server, which is shared among multiple devices, leading to inter-dependent demands, and (2) the shift in modern DNN architecture from chains to directed acyclic graphs (DAGs), which complicates potential solutions.  In this paper, we introduce a novel Decentralized DNN Surgery (DDS) framework. We formulate the partition strategy as a min-cut and propose a resource 
    
[^35]: 联合语音分离与识别的混合编码器

    Mixture Encoder for Joint Speech Separation and Recognition. (arXiv:2306.12173v1 [cs.CL])

    [http://arxiv.org/abs/2306.12173](http://arxiv.org/abs/2306.12173)

    本论文提出了一种中间方法，同时利用显式语音分离和直接在ASR模块中合并混合语音信息，通过交换跨说话者上下文信息的层，实现了在SMS-WSJ任务上相对于纯模块化设置的单词错误率7%的相对改进。

    

    多说话人自动语音识别对于许多现实世界的应用程序至关重要，但需要特定的建模技术。现有的方法可以分为模块化和端到端方法。模块化方法使用单说话人ASR系统分离说话人并识别他们。端到端模型直接在一个强大的神经网络中处理重叠的语音。本文提出了一种介于两者之间的方法，利用类似于模块化方法的显式语音分离，但也直接在ASR模块中合并混合语音信息，以减轻语音分离器造成的错误传播。我们还探索了一种通过结合个体说话人信息的层来交换跨说话者上下文信息的方法。我们的系统通过单独和联合训练阶段进行优化，在SMS-WSJ任务上相对于纯模块化设置的单词错误率实现了7%的相对改进。

    Multi-speaker automatic speech recognition (ASR) is crucial for many real-world applications, but it requires dedicated modeling techniques. Existing approaches can be divided into modular and end-to-end methods. Modular approaches separate speakers and recognize each of them with a single-speaker ASR system. End-to-end models process overlapped speech directly in a single, powerful neural network. This work proposes a middle-ground approach that leverages explicit speech separation similarly to the modular approach but also incorporates mixture speech information directly into the ASR module in order to mitigate the propagation of errors made by the speech separator. We also explore a way to exchange cross-speaker context information through a layer that combines information of the individual speakers. Our system is optimized through separate and joint training stages and achieves a relative improvement of 7% in word error rate over a purely modular setup on the SMS-WSJ task.
    
[^36]: 搜索和推荐中 Pareto-最优解后选择策略研究

    Post-hoc Selection of Pareto-Optimal Solutions in Search and Recommendation. (arXiv:2306.12165v1 [cs.IR])

    [http://arxiv.org/abs/2306.12165](http://arxiv.org/abs/2306.12165)

    本文提出了一种名为“从乌托邦的人口距离”（PDU）的后选择策略，用于确定和选择 Pareto-最优解中的最佳解。该方法分析点的分布，通过估计 PDU 分数的点的平均位置来确定最佳解的可能位置，实验结果表明 PDU 在准确性和稳定性方面表现优异。

    

    信息检索（IR）和推荐系统（RS）任务从基于单一度量计算最终结果的排名过渡为多目标问题。解决这些问题会得到一组 Pareto-最优解，称为 Pareto frontier，其中没有目标可以进一步改善而不损害其他目标。原则上，Pareto frontier 上的所有点都有可能代表着基于两个或多个度量相结合选择的最佳模型候选者。我们提出了一种名为“从乌托邦的人口距离”（PDU）的新颖后选择策略，采用理论上的正当化技术来确定和选择 Pareto-最优解中的最佳解。具体而言，PDU 通过研究每个点与其乌托邦点（目标的理想性能）之间的距离来分析点的分布。在一定阈值范围内，通过估计 PDU 分数的点的平均位置来确定最佳解的可能位置。我们在合成和真实数据集上评估 PDU 并与其他知名的选择策略进行比较，结果表明 PDU 在准确性和稳定性方面表现优异。

    Information Retrieval (IR) and Recommender Systems (RS) tasks are moving from computing a ranking of final results based on a single metric to multi-objective problems. Solving these problems leads to a set of Pareto-optimal solutions, known as Pareto frontier, in which no objective can be further improved without hurting the others. In principle, all the points on the Pareto frontier are potential candidates to represent the best model selected with respect to the combination of two, or more, metrics. To our knowledge, there are no well-recognized strategies to decide which point should be selected on the frontier. In this paper, we propose a novel, post-hoc, theoretically-justified technique, named "Population Distance from Utopia" (PDU), to identify and select the one-best Pareto-optimal solution from the frontier. In detail, PDU analyzes the distribution of the points by investigating how far each point is from its utopia point (the ideal performance for the objectives). The possib
    
[^37]: 数据集随机化的对抗攻击中和方法

    Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])

    [http://arxiv.org/abs/2306.12161](http://arxiv.org/abs/2306.12161)

    本文提出一种新的防御机制，将原始数据集伪随机投影到一个新的数据集中，在多个有不同决策边界的训练分类器中随机选择一个来测试输入，能够中和对抗攻击，提高了分类器的安全性和可靠性。

    

    对深度学习模型的对抗攻击对它们的可靠性和安全性构成了严重的威胁。现有的防御机制要么只针对某一特定类型的攻击，要么容易受到复杂攻击的影响。我们提出了一种新的防御机制，虽然着眼于基于图像的分类器，但与引用类别相关的特征是一般的。它植根于超空间投影，提供了原始数据集的伪随机投影到一个新的数据集。在训练过程中，我们使用创建了各种投影数据集的一套生成器，每个生成器训练一个特定的分类器，从而得到具有不同决策边界的不同训练分类器。在测试过程中，选择一个分类器来测试输入。我们的方法不会损害对于合法输入的准确性。除了详细阐述和提供我们的防御机制的全面特征化外，我们还提供了四个实验的概念证明。

    Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four 
    
[^38]: 基于密集点表示的轮廓感知图形分割方法

    Joint Dense-Point Representation for Contour-Aware Graph Segmentation. (arXiv:2306.12155v1 [cs.CV])

    [http://arxiv.org/abs/2306.12155](http://arxiv.org/abs/2306.12155)

    该论文提出了一种利用点检测和像素分割技术联合学习的图形分割方法，从而克服了传统方法中的一些缺陷，提高了分割的稳定性和准确性。

    

    我们提出了一种新的方法，通过联合学习点和像素轮廓表示，将图形和密集分割技术相结合，从而发挥每种方法的优点。这样可以解决典型图形分割方法中目标错位限制网络学习区分性顶点和轮廓特征的问题。我们的联合学习策略允许编码丰富和多样的语义特征，同时缓解了在基于密集方法中常见的轮廓不稳定性问题，其中像素级目标可能会导致解剖学上不合理的拓扑结构。此外，我们确定了正确预测仍落在轮廓边界上的情况，并通过新的混合轮廓距离损失来解决这个问题。我们的方法在多个胸部X线数据集上进行验证，对比了多种密集和点检测方法，证明了分割稳定性和准确性的明显改善。我们的源代码是免费提供的。

    We present a novel methodology that combines graph and dense segmentation techniques by jointly learning both point and pixel contour representations, thereby leveraging the benefits of each approach. This addresses deficiencies in typical graph segmentation methods where misaligned objectives restrict the network from learning discriminative vertex and contour features. Our joint learning strategy allows for rich and diverse semantic features to be encoded, while alleviating common contour stability issues in dense-based approaches, where pixel-level objectives can lead to anatomically implausible topologies. In addition, we identify scenarios where correct predictions that fall on the contour boundary are penalised and address this with a novel hybrid contour distance loss. Our approach is validated on several Chest X-ray datasets, demonstrating clear improvements in segmentation stability and accuracy against a variety of dense- and point-based methods. Our source code is freely ava
    
[^39]: 基于预训练的影响因素研究医学图像分类解释性能的基准数据

    Benchmark data to study the influence of pre-training on explanation performance in MR image classification. (arXiv:2306.12150v1 [cs.CV])

    [http://arxiv.org/abs/2306.12150](http://arxiv.org/abs/2306.12150)

    本研究提出了一个MRI分类任务的基准数据集，用于评估不同模型的解释性能。实验结果表明，XAI方法并不一定比简单模型提供更好的解释，且CNN的解释能力取决于底层数据的复杂性和标签的质量。

    

    卷积神经网络（CNN）常常在医学预测任务中被成功地应用，通常与迁移学习相结合，在训练数据不足时能够提高性能。然而，由于CNN产生的模型高度复杂且通常不提供任何有关其预测机制的信息，这促使了“可解释性”人工智能（XAI）领域的研究。本文提出了一个基准数据集，用于在MRI分类任务中定量评估解释性能。通过这个基准数据集，我们可以了解迁移学习对解释质量的影响。实验结果表明，应用于基于迁移学习的CNN的流行XAI方法并不一定比简单模型提供更好的解释，并且CNN提供有意义解释的能力严重依赖于底层数据的复杂性和标签的质量。

    Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to t
    
[^40]: 基于机器学习的针织力传感器不一致性补偿

    Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors. (arXiv:2306.12129v1 [eess.SY])

    [http://arxiv.org/abs/2306.12129](http://arxiv.org/abs/2306.12129)

    本文展示了一种基于机器学习的针织力传感器不一致性补偿方法，通过使用指数平滑滤波器进行预处理，并使用最小的人工神经网络来提高传感器读数和执行力之间的映射。

    

    针织传感器经常受到固有效应（如偏移、松弛和漂移）的影响而产生不一致性。这些属性的结合使得从传感器数据到物理执行具有挑战性。在本文中，我们展示了一种利用最小人工神经网络（ANN）结合简单预处理方法来进行处理的方法来对抗这种不一致性。我们在重新采样过的传感器信号上应用了多个指数平滑滤波器，以产生保留不同历史传感器数据水平的特征，并结合表示以前传感器执行充分状态的特征。通过训练一个三层ANN，总共有8个神经元，我们成功地提高了传感器读数和执行力之间的映射。我们的研究发现，我们的技术对于材料和结构相当不同的传感器也是适用的，而且还可以应用于相关的物理特征。

    Knitted sensors frequently suffer from inconsistencies due to innate effects such as offset, relaxation, and drift. These properties, in combination, make it challenging to reliably map from sensor data to physical actuation. In this paper, we demonstrate a method for counteracting this by applying processing using a minimal artificial neural network (ANN) in combination with straightforward pre-processing. We apply a number of exponential smoothing filters on a re-sampled sensor signal, to produce features that preserve different levels of historical sensor data and, in combination, represent an adequate state of previous sensor actuation. By training a three-layer ANN with a total of 8 neurons, we manage to significantly improve the mapping between sensor reading and actuation force. Our findings also show that our technique translates to sensors of reasonably different composition in terms of material and structure, and it can furthermore be applied to related physical features such
    
[^41]: 通过语言模型批量生产多模态系统的失败

    Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])

    [http://arxiv.org/abs/2306.12105](http://arxiv.org/abs/2306.12105)

    本文介绍了一种MultiMon系统，可以自动识别多模态系统中的系统性失败，揭示CLIP文本编码器的14个系统性失败，每个都由数百个不同的输入组成，这些输入会导致其他大多数最先进的多模态系统的失败。

    

    部署的多模态系统可能以评估人员未曾预见的方式失败。为了在部署前找到这些失败，我们介绍了MultiMon，这是一个能够自动识别系统性失败的系统，能够提供可推广的、自然语言描述模型失败模式的例子。为了揭示系统性失败，MultiMon从语料库中抓取错误协议的示例：输入产生相同的输出，但不应该如此。然后它会激活一个语言模型（例如GPT-4）来查找系统性失败的模式并用自然语言描述它们。我们使用MultiMon找到了CLIP文本编码器的14个系统性失败（例如“忽略量词”，每个都由数百个不同的输入组成（例如“一个带有一些/许多书的书架”）。因为CLIP是大多数最先进的多模态系统的基础，这些输入会导致Midjourney 5.1、DALL-E、VideoFusion等系统失败。MultiMon也可以指导针对特定用例的相关故障，例如自驾车系统。

    Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
    
[^42]: 高效ResNets: 残差网络设计

    Efficient ResNets: Residual Network Design. (arXiv:2306.12100v1 [cs.CV])

    [http://arxiv.org/abs/2306.12100](http://arxiv.org/abs/2306.12100)

    本文介绍了一个在CIFAR-10图像分类任务上可训练参数小于5百万的改进版ResNet模型，达到了96.04%的测试准确率，远优于其可训练参数大于1100万的基准架构ResNet18。

    

    ResNets（残差网络）是最常用的图像分类模型之一。本项目设计和训练了一个修改后的ResNet模型，用于CIFAR-10图像分类。特别地，我们旨在在将我们的ResNet模型大小保持在指定的5百万可训练参数的固定预算以下的同时，最大化在CIFAR-10基准测试上的测试准确性。模型大小通常作为可训练参数的数量进行衡量，当模型需要存储在存储容量有限的设备上时（例如IoT/edge设备），这一点很重要。在本文中，我们介绍了低于5百万参数的残差网络设计。我们展示了我们的ResNet在CIFAR-10上达到了96.04%的测试准确率，这比ResNet18（可训练参数大于1100万）高得多，当配备了一些训练策略和合适的ResNet超参数时。模型和代码可在https://github.com/Nikunj-Gupta/Efficient_ResN上获得。

    ResNets (or Residual Networks) are one of the most commonly used models for image classification tasks. In this project, we design and train a modified ResNet model for CIFAR-10 image classification. In particular, we aimed at maximizing the test accuracy on the CIFAR-10 benchmark while keeping the size of our ResNet model under the specified fixed budget of 5 million trainable parameters. Model size, typically measured as the number of trainable parameters, is important when models need to be stored on devices with limited storage capacity (e.g. IoT/edge devices). In this article, we present our residual network design which has less than 5 million parameters. We show that our ResNet achieves a test accuracy of 96.04% on CIFAR-10 which is much higher than ResNet18 (which has greater than 11 million trainable parameters) when equipped with a number of training strategies and suitable ResNet hyperparameters. Models and code are available at https://github.com/Nikunj-Gupta/Efficient_ResN
    
[^43]: MSW-Transformer: 多尺度移位窗口变压器网络用于12导心电图分类

    MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for 12-Lead ECG Classification. (arXiv:2306.12098v1 [eess.SP])

    [http://arxiv.org/abs/2306.12098](http://arxiv.org/abs/2306.12098)

    提出了一种称为MSW-Transformer的单层变压器网络，使用多窗口滑动注意机制和不同尺度来捕捉不同维度的特征，有效地从12导ECG信号中区分病理特征差异，实现了最先进的性能。

    

    自动分类心电图（ECG）信号对于心血管疾病的早期预防和诊断至关重要。尽管ECG信号可用于诊断各种疾病，但其病理特征仅存在最小差异，这给自动分类模型提出了挑战。现有方法主要使用卷积神经网络来提取ECG信号特征进行分类，但可能无法完全捕捉不同疾病的病理特征差异。变压器网络在序列数据的特征提取方面具有优势，但完整的网络复杂，依赖大规模数据集。为应对这些挑战，我们提出了一种称为多尺度移位窗口变压器网络（MSW-Transformer）的单层Transformer网络，它使用不同尺度的多窗口滑动注意机制来捕捉不同维度的特征。自我注意被限制在非重叠和移位窗口内，以有效地捕捉局部特征。我们的方法在公开可用的12导ECG分类数据集上实现了最先进的性能，证明了MSW-Transformer在捕捉病理特征差异方面的有效性。

    Automatic classification of electrocardiogram (ECG) signals plays a crucial role in the early prevention and diagnosis of cardiovascular diseases. While ECG signals can be used for the diagnosis of various diseases, their pathological characteristics exhibit minimal variations, posing a challenge to automatic classification models. Existing methods primarily utilize convolutional neural networks to extract ECG signal features for classification, which may not fully capture the pathological feature differences of different diseases. Transformer networks have advantages in feature extraction for sequence data, but the complete network is complex and relies on large-scale datasets. To address these challenges, we propose a single-layer Transformer network called Multi-Scale Shifted Windows Transformer Networks (MSW-Transformer), which uses a multi-window sliding attention mechanism at different scales to capture features in different dimensions. The self-attention is restricted to non-ove
    
[^44]: 理解芝加哥的人类移动模式：使用聚类技术分析出租车数据

    Understanding human mobility patterns in Chicago: an analysis of taxi data using clustering techniques. (arXiv:2306.12094v1 [cs.SI])

    [http://arxiv.org/abs/2306.12094](http://arxiv.org/abs/2306.12094)

    本研究使用芝加哥的出租车数据，通过聚类技术分析社区之间的联系，提供了新的公共交通发展以及交通或道路污染缓解工作的指导。

    

    理解人类移动模式在城市规划、公共卫生和政治组织等应用中都很重要。出租车数据是人类移动的丰富数据来源。以芝加哥为例，我们研究了2016年出租车行程的数据，旨在了解各个社区之间的相互联系。该分析将提供人们使用出租车来往哪些社区的信息，为新的公共交通发展提供了重要指导。此外，该分析还将绘制交通循环模式图，了解人们从城市的哪个地方出发以及他们前往的目的地，从而更好地指导交通或道路污染的缓解工作。对于第一个应用，将数据表示为无向图就足够了。运输线路向两个方向延伸，因此仅知道哪些社区之间出租车的使用率较高就足以为公众交通发展提供了支持。

    Understanding human mobility patterns is important in applications as diverse as urban planning, public health, and political organizing. One rich source of data on human mobility is taxi ride data. Using the city of Chicago as a case study, we examine data from taxi rides in 2016 with the goal of understanding how neighborhoods are interconnected. This analysis will provide a sense of which neighborhoods individuals are using taxis to travel between, suggesting regions to focus new public transit development efforts. Additionally, this analysis will map traffic circulation patterns and provide an understanding of where in the city people are traveling from and where they are heading to perhaps informing traffic or road pollution mitigation efforts. For the first application, representing the data as an undirected graph will suffice. Transit lines run in both directions so simply a knowledge of which neighborhoods have high rates of taxi travel between them provides an argument for p
    
[^45]: 边缘设备推理性能比较

    Edge Devices Inference Performance Comparison. (arXiv:2306.12093v1 [cs.LG])

    [http://arxiv.org/abs/2306.12093](http://arxiv.org/abs/2306.12093)

    本研究比较了多种边缘设备上常用的深度学习模型的推理性能，发现Google平台对于新型模型表现最佳，而Intel Neural Stick是最通用的加速器。

    

    本研究探讨了MobileNet家族、EfficientNet V1和V2家族、VGG模型、Resnet家族和InceptionV3在四个边缘平台上的推理时间。具体而言，这些平台包括NVIDIA Jetson Nano、Intel Neural Stick、Google Coral USB Dongle和Google Coral PCIe。我们的主要贡献是对多种设置下所述模型的深入分析，特别是在输入大小、分类头的存在、其大小以及模型规模的影响下进行分析。由于在整个行业中，这些架构主要用作特征提取器，因此我们的重点是分析它们的特征提取性能。我们发现，Google平台提供了最快的平均推理时间，尤其是对于像MobileNet或EfficientNet家族这样的较新模型，而Intel Neural Stick是最通用的加速器，可以运行大多数架构。这些结果应为AI边缘系统开发的工程师提供指导。

    In this work, we investigate the inference time of the MobileNet family, EfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on four edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick, Google Coral USB Dongle, and Google Coral PCIe. Our main contribution is a thorough analysis of the aforementioned models in multiple settings, especially as a function of input size, the presence of the classification head, its size, and the scale of the model. Since throughout the industry, those architectures are mainly utilized as feature extractors we put our main focus on analyzing them as such. We show that Google platforms offer the fastest average inference time, especially for newer models like MobileNet or EfficientNet family, while Intel Neural Stick is the most universal accelerator allowing to run most architectures. These results should provide guidance for engineers in the early stages of AI edge systems development. All of them are accessible at htt
    
[^46]: 针对深度图卷积网络的结构感知DropEdge方法

    Structure-Aware DropEdge Towards Deep Graph Convolutional Networks. (arXiv:2306.12091v1 [cs.LG])

    [http://arxiv.org/abs/2306.12091](http://arxiv.org/abs/2306.12091)

    本文提出了一种结构感知的DropEdge++方法，其中包含基于层的采样器和基于特征的采样器。研究还发现，从底层采样边比逐渐减少的采样边以及DropEdge更能提高性能，证明了这一现象与过度平滑密切相关。

    

    研究发现，当堆积多个层时，图卷积网络（GCNs）的性能会显著下降。导致深层GCNs失败的主要因素在于过度平滑，随着网络深度的增加，使输出的网络与输入隔离，削弱了表达能力和可训练性。本文从DropEdge这一现有的简单而有效的技术入手，提出了一种新方法——DropEdge++。与DropEdge相比，DropEdge++具有两个结构感知采样器：基于层的采样器和基于特征的采样器。关于基于层的采样器，有趣的是，我们发现越来越多地从底层采样边比逐渐减少的采样边以及DropEdge更能提高性能。我们通过一个与过度平滑密切相关的指标——平均边数(MEN)来理论上揭示了这一现象。对于基于特征的采样器，我们将其与DropEdge++的其余部分相结合...

    It has been discovered that Graph Convolutional Networks (GCNs) encounter a remarkable drop in performance when multiple layers are piled up. The main factor that accounts for why deep GCNs fail lies in over-smoothing, which isolates the network output from the input with the increase of network depth, weakening expressivity and trainability. In this paper, we start by investigating refined measures upon DropEdge -- an existing simple yet effective technique to relieve over-smoothing. We term our method as DropEdge++ for its two structure-aware samplers in contrast to DropEdge: layer-dependent sampler and feature-dependent sampler. Regarding the layer-dependent sampler, we interestingly find that increasingly sampling edges from the bottom layer yields superior performance than the decreasing counterpart as well as DropEdge. We theoretically reveal this phenomenon with Mean-Edge-Number (MEN), a metric closely related to over-smoothing. For the feature-dependent sampler, we associate th
    
[^47]: 一种减少联邦学习通信的高效虚拟数据生成方法

    An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])

    [http://arxiv.org/abs/2306.12088](http://arxiv.org/abs/2306.12088)

    本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。

    

    通信开销是联邦学习中的主要挑战之一。一些经典的方案假设服务器可以从本地模型中提取参与者训练数据的辅助信息来构建中央虚拟数据集。服务器使用虚拟数据集来微调聚合的全局模型，以在较少的通信轮次内达到目标测试精度。本文将上述解决方案概括为基于数据的通信高效联邦学习框架。提出框架的关键是设计一个有效的提取模块（EM），它确保虚拟数据集对微调聚合的全局模型产生积极影响。与现有方法使用生成器来设计EM不同，我们提出的方法FedINIBoost借鉴了梯度匹配的思想来构建EM。具体而言，FedINIBoost在每个通信轮次的每个参与者中使用两个步骤构建真实数据集的代理数据集。然后服务器聚合所有的代理数据集来构建中央虚拟数据集。

    Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr
    
[^48]: 时间序列预测中的对比学习的重要性是什么？

    What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])

    [http://arxiv.org/abs/2306.12086](http://arxiv.org/abs/2306.12086)

    本文通过对比分析各种训练变量(包括不同的SSCL算法、学习策略，模型体系结构以及它们之间的相互作用)的有效性，研究了SSCL在时间序列预测中的影响及具体好处。

    

    近年来，自监督对比学习(Self-Supervised Contrastive Learning, SSCL)在各个领域中(包括自然语言处理和计算机视觉等)的引入已经展示了在表示学习方面的显著提升。通过利用自监督的潜在优势，SSCL使用大量无标签数据进行了表示模型的预训练。尽管这些进展，但仍然存在一个显著的差距——即我们对于不同的SSCL策略对时间序列预测性能的影响以及SSCL所带来的具体好处理解不足。本文旨在通过对各种训练变量的有效性进行全面分析来解决这些差距，其中包括不同的SSCL算法、学习策略、模型体系结构以及它们之间的相互作用。此外，为了深入了解SSCL在时间序列预测背景下带来的改进，我们还进行了经验感受野的定性分析。

    In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field i
    
[^49]: FLGo：一个全可定制的联邦学习平台

    FLGo: A Fully Customizable Federated Learning Platform. (arXiv:2306.12079v1 [cs.LG])

    [http://arxiv.org/abs/2306.12079](http://arxiv.org/abs/2306.12079)

    提出了一个名为FLGo的联邦学习平台，用于定制模拟以满足特定应用环境的需求。平台包含40+基准、20+算法和2个系统模拟器，并提供用户友好的API以用于快速开发新的插件，以提高共享性和可重复性。

    

    联邦学习在医疗保健、金融和物联网等领域有着广泛的应用。许多现有的联邦学习框架提供了一系列基准来评估联邦学习在真实情况下的性能。然而，定制模拟以适应特定应用环境、数据异构性和系统异构性的过程通常仍然过于复杂。这给传统的机器学习研究人员在探索联邦学习的使用时带来了重重的困难，同时也削弱了代码在联邦学习框架间的共享性。为了解决这个问题，我们提出了一个称之为FLGo的新型轻量联邦学习平台，以促进高度可共享的跨应用联邦学习研究。我们的平台提供了超过40种基准、20种算法和2种系统模拟器作为开箱即用的插件。我们还提供了用户友好的API，用于快速定制新的插件，使其可以轻松共享和重复使用，以提高可再现性。最后，我们开展了一系列实验，以展示我们平台的灵活性和适用性，包括医疗保健领域基于电子病历的诊断案例研究。

    Federated learning (FL) has found numerous applications in healthcare, finance, and IoT scenarios. Many existing FL frameworks offer a range of benchmarks to evaluate the performance of FL under realistic conditions. However, the process of customizing simulations to accommodate application-specific settings, data heterogeneity, and system heterogeneity typically remains unnecessarily complicated. This creates significant hurdles for traditional ML researchers in exploring the usage of FL, while also compromising the shareability of codes across FL frameworks. To address this issue, we propose a novel lightweight FL platform called FLGo, to facilitate cross-application FL studies with a high degree of shareability. Our platform offers 40+ benchmarks, 20+ algorithms, and 2 system simulators as out-of-the-box plugins. We also provide user-friendly APIs for quickly customizing new plugins that can be readily shared and reused for improved reproducibility. Finally, we develop a range of ex
    
[^50]: 通过不变分解和（空间-）时间变换器学习潜在动态

    Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])

    [http://arxiv.org/abs/2306.12077](http://arxiv.org/abs/2306.12077)

    该文提出了一种从高维经验数据中学习动力系统的方法，基于不变分解和（空间-）时间变换器，能够自动分离出一种实例特定的编码和一种潜在动态模型，使用经验数据作为模型的输入，通过在任意连续时间推断系统行为，与显式的神经ODE公式不同，高效可扩展。

    

    我们提出了一种利用变分自编码器和（空间-）时间注意力的方法，通过一个设计旨在强制实施某些科学动机的不变性的框架，从高维经验数据中学习动力学系统。我们专注于这样一种情况，即来自一个系统的多个不同实例的数据可用，其潜在动力学模型在开始时完全不知道。该方法基于一个分离，即一种实例特定的编码（捕获初始条件、常数等）和一种潜在动态模型，该模型本身在系统的所有实例/实现中都是通用的。这种分离是以自动化、数据驱动的方式实现的，只需要经验数据作为模型的输入。该方法允许在任何连续时间有效地推断系统行为，但不需要显式的神经ODE公式，这使得它高效且高度可扩展。我们通过简单的理论分析和实验结果研究了这种行为。

    We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and ex
    
[^51]: 对最坏情况下游任务适应性的任务鲁棒预训练

    Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])

    [http://arxiv.org/abs/2306.12070](http://arxiv.org/abs/2306.12070)

    本文提出了一种任务鲁棒的预训练方法，将上游任务分成几个代表性任务并应用极小极大损失进行预训练，以保证模型能够在下游任务中具有均匀良好的性能。

    

    预训练在转移到下游任务时取得了显着的成功。在机器学习中，我们关心模型不仅具有良好的性能，而且在合理的条件变化下的行为。当预训练基础模型时，同样的哲学也适用。然而，基础模型可能并不会在一系列相关下游任务中均匀地表现良好。本文考虑预训练一个模型，保证其在下游任务中具有均匀良好的性能，我们称此目标为下游任务鲁棒性。我们的方法首先将上游任务分成几个代表性任务，并应用简单的minimax loss 进行预训练，然后设计了一个高效的算法来解决极小极大问题，并表明我们的方法优于先前的基线。

    Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim
    
[^52]: 将话语单元和关键词联系起来对阅读理解进行层次推理链建模

    Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension. (arXiv:2306.12069v1 [cs.CL])

    [http://arxiv.org/abs/2306.12069](http://arxiv.org/abs/2306.12069)

    提出了一个基于全局图网络的方法，既处理话语层面又处理单词层面的上下文，并通过分层交互机制对节点级别和类型级别的关系进行建模，以提供更细粒度的关系提取，该方法在逻辑推理QA数据集和自然语言推理数据集上优于现有的最先进方法。

    

    机器阅读理解面临逻辑推理方面的新挑战，旨在理解给定上下文中所涉及的隐含逻辑关系并对其进行推理。由于逻辑复杂性，逻辑关系存在于不同的粒度级别。然而，大多数现有的推理方法分别关注实体感知或话语为基础的信息，但忽略了可能具有相互影响的层次关系。本文提出了一个基于全局图网络（HGN）的方法，既处理话语层面又处理单词层面的上下文，作为逻辑推理的基础，以提供更细粒度的关系提取。具体而言，通过分层交互机制对节点级别和类型级别的关系进行建模，这些关系可以解释为推理过程中的桥梁，以改进MRC系统的解释能力。实验结果表明，我们提出的方法在逻辑推理QA数据集（ReClor和LogiQA）和自然语言推理数据集（RACE和SWAG）上优于现有的最先进方法。

    Machine reading comprehension (MRC) poses new challenges over logical reasoning, which aims to understand the implicit logical relations entailed in the given contexts and perform inference over them. Due to the complexity of logic, logical relations exist at different granularity levels. However, most existing methods of logical reasoning individually focus on either entity-aware or discourse-based information but ignore the hierarchical relations that may even have mutual effects. In this paper, we propose a holistic graph network (HGN) which deals with context at both discourse level and word level, as the basis for logical reasoning, to provide a more fine-grained relation extraction. Specifically, node-level and type-level relations, which can be interpreted as bridges in the reasoning process, are modeled by a hierarchical interaction mechanism to improve the interpretation of MRC systems. Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and natural langu
    
[^53]: 松弛光滑条件下随机双层优化的最优算法

    Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions. (arXiv:2306.12067v1 [math.OC])

    [http://arxiv.org/abs/2306.12067](http://arxiv.org/abs/2306.12067)

    本文提出了一种新型随机双层优化的最优算法，避免了使用高阶光滑性假设，能够更好地适应非凸设置。

    

    随机双层优化通常涉及最小化依赖于强凸下层函数的下层函数（LL）最小值的上层（UL）函数。几种算法利用Neumann级数来近似估计隐式梯度（超梯度）的矩阵逆。最先进的随机双层算法（SOBA）[16]改用随机梯度下降步骤来解决与显式矩阵反演相关的线性系统。该修改使SOBA能够在非凸设置中匹配单层对应项的样本复杂度下限。不幸的是，当前SOBA的分析依赖于UL和LL函数的高阶光滑性假设以实现最优性。本文介绍了随机双层优化的一种新型完全单循环且无Hessian反演算法框架，并在标准光滑性假设下提供了更紧密的分析。

    Stochastic Bilevel optimization usually involves minimizing an upper-level (UL) function that is dependent on the arg-min of a strongly-convex lower-level (LL) function. Several algorithms utilize Neumann series to approximate certain matrix inverses involved in estimating the implicit gradient of the UL function (hypergradient). The state-of-the-art StOchastic Bilevel Algorithm (SOBA) [16] instead uses stochastic gradient descent steps to solve the linear system associated with the explicit matrix inversion. This modification enables SOBA to match the lower bound of sample complexity for the single-level counterpart in non-convex settings. Unfortunately, the current analysis of SOBA relies on the assumption of higher-order smoothness for the UL and LL functions to achieve optimality. In this paper, we introduce a novel fully single-loop and Hessian-inversion-free algorithmic framework for stochastic bilevel optimization and present a tighter analysis under standard smoothness assumpti
    
[^54]: EquiformerV2: 改进的等变Transformer，用于扩展到更高次表示

    EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])

    [http://arxiv.org/abs/2306.12059](http://arxiv.org/abs/2306.12059)

    本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。

    

    等变Transformer（例如Equiformer）已经证明了将Transformer应用于3D原子系统领域的功效。但是，由于计算复杂性，它们仍然局限于小数次等变表示。在本文中，我们调查了这些架构是否能够很好地扩展到更高的次数。从Equiformer开始，我们首先用eSCN卷积替换了$SO(3)$卷积，以有效地合并更高次的张量。然后，为了更好地利用更高次的能力，我们提出了三个架构改进——注意力重标准化、可分离的$S^2$激活和可分离层归一化。将这一切放在一起，我们提出了EquiformerV2，在大型OC20数据集上的表现优于以前的最先进方法，在力上提高了最多$12\%$，能量上提高了$4\%$，提供更好的速度-准确性权衡，并且在计算吸附能所需的DFT计算量方面缩减了2倍。

    Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
    
[^55]: 使用Corrector操作符增强神经算子代理非线性变分边界值问题的准确度和可靠性

    Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems. (arXiv:2306.12047v1 [math.NA])

    [http://arxiv.org/abs/2306.12047](http://arxiv.org/abs/2306.12047)

    本文提供了一种基于Corrector操作符的框架，以增强神经算子代理非线性变分边界值问题的准确度和可靠性。使用该方案对于PCANet型神经算子的二维非线性扩散模型的数值实验结果显示，逼近的准确度近乎提高了两个数量级，并且还在涉及非线性d之上的拓扑优化问题中得到了探讨。

    

    本文旨在开发一类参数偏微分方程解算符的逼近方法，即通过神经算子的方式。神经算子有几个挑战，包括生成适当的训练数据、成本-准确度权衡和非平凡的超参数调整问题。神经算子准确度的不可预测性影响了它们在推理、优化和控制等后续问题中的应用。本文提出了一个基于线性变分问题的框架，给出了神经算子预测结果的校正值。与校正问题相关的算子称为校正算子。通过使用提出的方案对采用PCANet型神经算子的二维非线性扩散模型进行的数值实验结果显示，当使用校正方法对神经算子进行校正时，逼近的准确度近乎提高了两个数量级。此外，涉及非线性d之上的拓扑优化问题也得到了探讨。

    This work focuses on developing methods for approximating the solution operators of a class of parametric partial differential equations via neural operators. Neural operators have several challenges, including the issue of generating appropriate training data, cost-accuracy trade-offs, and nontrivial hyperparameter tuning. The unpredictability of the accuracy of neural operators impacts their applications in downstream problems of inference, optimization, and control. A framework is proposed based on the linear variational problem that gives the correction to the prediction furnished by neural operators. The operator associated with the corrector problem is referred to as the corrector operator. Numerical results involving a nonlinear diffusion model in two dimensions with PCANet-type neural operators show almost two orders of increase in the accuracy of approximations when neural operators are corrected using the proposed scheme. Further, topology optimization involving a nonlinear d
    
[^56]: 处理自然视觉场景神经响应的时间条件脉冲潜变量模型

    Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.12045](http://arxiv.org/abs/2306.12045)

    本研究提出 TeCoS-LVM 模型，使用脉冲神经元以模拟自然视觉刺激的神经响应。该模型能够自适应地探索和利用刺激序列中的时间依赖关系，避免丢失脉冲列中的信息。

    

    发展神经响应的计算模型对于理解感知处理和神经计算至关重要。目前最先进的神经网络方法使用时间过滤器来处理时间依赖性，导致处理流程不现实且不灵活。同时，这些方法针对试验平均发放率，未能捕捉到脉冲列中的重要特征。本研究提出时间条件脉冲潜变量模型（TeCoS-LVM）来模拟自然视觉刺激的神经响应。我们使用脉冲神经元产生直接匹配记录脉冲列的脉冲输出。这种方法有助于避免丢失嵌入在原始脉冲列中的信息。我们从模型参数空间中排除时间维度，并引入时间条件操作，使模型能够在自然范式中自适应地探索和利用刺激序列中的时间依赖关系。我们展示了 TeCoS-LVM 模型能够产生...

    Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
    
[^57]: 自学习遮蔽自编码器是高效视频异常检测器

    Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors. (arXiv:2306.12041v1 [cs.CV])

    [http://arxiv.org/abs/2306.12041](http://arxiv.org/abs/2306.12041)

    提出了一种基于轻量级遮蔽自编码器的高效异常事件检测模型，通过引入基于运动梯度的令牌加权方法，整合教师解码器和学生解码器以及生成合成异常事件，实现共同重构原始帧和对应的像素级异常映射。在三个基准测试中，实现出色的速度和准确性的权衡。

    

    我们提出了一种基于轻量级遮蔽自编码器（AE）的高效异常事件检测模型，该模型应用于视频帧级别。我们的模型的三个创新点：其一，我们引入了基于运动梯度的令牌加权方法，因此避免了学习重构静态背景场景。其二，我们将教师解码器和学生解码器整合到我们的架构中，利用两个解码器给出的输出之间的差异来改善异常检测。其三，我们生成合成异常事件以增强培训视频，并将遮蔽AE模型任务设置为共同重构原始帧（无异常）和对应的像素级异常映射。我们的设计导致高效和有效的模型，通过对三个基准测试进行的广泛实验证明。实验结果显示，我们的模型在速度和准确性之间实现了出色的权衡。

    We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus avoiding learning to reconstruct the static background scene. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy
    
[^58]: 自我监督异常检测的端到端增强超参数调整

    End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])

    [http://arxiv.org/abs/2306.12033](http://arxiv.org/abs/2306.12033)

    这项研究提出了一种名为ST-SSAD的新方法，可以系统地调整数据增强的超参数，从而有助于提高自我监督异常检测（SSAD）的性能。

    

    自我监督学习（SSL）已经成为一个有前途的范例，它为现实问题提供自产生的监督信号，避免了繁琐的手动标注工作。SSL对于无监督任务，如异常检测尤其具有吸引力，因为标记的异常通常不存在或难以获得。虽然自我监督异常检测（SSAD）近年来受到了广泛关注，但文献却未将数据增强视为超参数。同时，最近的研究表明，增强选择对检测性能有重要影响。在本文中，我们介绍了ST-SSAD（自我调整自我监督异常检测），这是一种关于严格调整增强的SSAD的第一个系统方法。为此，我们的工作提出了两个关键贡献。第一是一种新的无监督验证损失函数，量化增强训练数据与（无标签）测试数据之间的对齐程度。在原则上，我们采用了最近高效的有监督学习方法借鉴的无监督验证方案和增强数据搜索策略，并将其适应于SSAD。我们进一步提出了一种新的增强搜索方法，通过贝叶斯优化的形式，将轻量级数据增强搜索器的简单集成。在各种异常检测基准数据集上的实验表明，我们的增强调整方法相对于以前的最新结果可以获得一致的性能提升，并且相对于最近的有监督方法具有竞争性的结果。

    Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop
    
[^59]: 多种网络爬虫算法的比较分析

    Comparative analysis of various web crawler algorithms. (arXiv:2306.12027v1 [cs.IR])

    [http://arxiv.org/abs/2306.12027](http://arxiv.org/abs/2306.12027)

    本文介绍了网络爬虫和网页排名算法在处理Web数据方面的重要性；在评估五种不同的爬取算法后，旨在确定最有效的爬取算法。

    

    本文论述了网络爬虫和网页排名算法在处理世界各地网络数据方面的重要性。随着Web的急剧增长，高效的搜索和检索方法变得至关重要。网络爬虫是将非结构化数据转换为结构化数据的过程，从而实现有效的信息检索。此外，网页排名算法在评估网页的质量和受欢迎程度方面发挥着重要作用。本文探讨了这些算法的背景，并评估了五种不同的爬取算法：鲨鱼搜索，基于优先级的队列，朴素贝叶斯，广度优先和深度优先。本文的目标是确定最有效的网络爬虫算法。通过了解这些算法，我们可以提高自己在Web上导航和提取有价值信息的能力。

    This presentation focuses on the importance of web crawling and page ranking algorithms in dealing with the massive amount of data present on the World Wide Web. As the web continues to grow exponentially, efficient search and retrieval methods become crucial. Web crawling is a process that converts unstructured data into structured data, enabling effective information retrieval. Additionally, page ranking algorithms play a significant role in assessing the quality and popularity of web pages. The presentation explores the background of these algorithms and evaluates five different crawling algorithms: Shark Search, Priority-Based Queue, Naive Bayes, Breadth-First, and Depth-First. The goal is to identify the most effective algorithm for crawling web pages. By understanding these algorithms, we can enhance our ability to navigate the web and extract valuable information efficiently.
    
[^60]: Continual学习是渐进的模型泛化器

    Continual Learners are Incremental Model Generalizers. (arXiv:2306.12026v1 [cs.LG])

    [http://arxiv.org/abs/2306.12026](http://arxiv.org/abs/2306.12026)

    本文研究了continual learners作为预训练器对下游任务的影响，提出了一种新的无监督CL框架和微调方案GLAD，可用作较好的预训练模型本身。通过学习改善通用性特征，在容易忘记特定任务知识时，CL模型可逐步提高表示的转移质量而不影响微调性能。

    

    本论文通过大量研究Continual Learning（CL）模型作为预训练器对下游任务的影响，探讨了其高效性和快速收敛的动机。在有监督和无监督的CL中，我们发现表示的转移质量通常会逐渐提高而不会影响微调性能的下降。这是因为当容易忘记特定任务的知识时，CL模型能够学习到更好的任务一般特征。基于这一观察，本文提出一种新的无监督CL框架，使用掩蔽建模旨在在训练期间捕获流畅的任务通用表示。此外，我们提出了一种新的微调方案GLobal Attention Discretization（GLAD），在解决下游任务时保留丰富的任务通用表示。使用GLAD微调的模型具有竞争性能，并且本身也可以用作较好的预训练模型。我们相信本文打破了p之间的障碍。

    Motivated by the efficiency and rapid convergence of pre-trained models for solving downstream tasks, this paper extensively studies the impact of Continual Learning (CL) models as pre-trainers. In both supervised and unsupervised CL, we find that the transfer quality of the representation often increases gradually without noticeable degradation in fine-tuning performance. This is because CL models can learn improved task-general features when easily forgetting task-specific knowledge. Based on this observation, we suggest a new unsupervised CL framework with masked modeling, which aims to capture fluent task-generic representation during training. Furthermore, we propose a new fine-tuning scheme, GLobal Attention Discretization (GLAD), that preserves rich task-generic representation during solving downstream tasks. The model fine-tuned with GLAD achieves competitive performance and can also be used as a good pre-trained model itself. We believe this paper breaks the barriers between p
    
[^61]: 3HAN：基于深度神经网络的虚假新闻检测模型

    3HAN: A Deep Neural Network for Fake News Detection. (arXiv:2306.12014v1 [cs.LG])

    [http://arxiv.org/abs/2306.12014](http://arxiv.org/abs/2306.12014)

    3HAN是一种基于深度学习的虚假新闻自动检测器。通过三层分层注意网络，它可以构建一个对新闻进行完整有效表示的新闻向量，并给文章不同部分分配不同的重要性，实现了高准确率的虚假新闻检测，并且提供了可理解的输出结果。

    

    虚假新闻的迅速传播是一个需要AI解决方案的严重问题。我们使用基于深度学习的三层分层注意网络（3HAN）构建一个自动检测器，用于快速、准确地检测虚假新闻。 3HAN通过逐层自下而上的方式构建新闻向量：对输入新闻文章的有效表示。在新闻中，标题是虚假新闻的一个区别特征，同时文章中的相对较少的单词和句子比其余部分更为重要。3HAN由于具有三层注意力的机制，给文章的不同部分分配不同的重要性。通过对大型真实数据集的实验，我们观察到了3HAN的高效性，准确率达到了96.77％。与其他深度学习模型不同，3HAN通过对文章不同部分的注意权重提供了可理解的输出结果。

    The rapid spread of fake news is a serious problem calling for AI solutions. We employ a deep learning based automated detector through a three level hierarchical attention network (3HAN) for fast, accurate detection of fake news. 3HAN has three levels, one each for words, sentences, and the headline, and constructs a news vector: an effective representation of an input news article, by processing an article in an hierarchical bottom-up manner. The headline is known to be a distinguishing feature of fake news, and furthermore, relatively few words and sentences in an article are more important than the rest. 3HAN gives a differential importance to parts of an article, on account of its three layers of attention. By experiments on a large real-world data set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike some other deep learning models, 3HAN provides an understandable output through the attention weights given to different parts of an article, which can be visu
    
[^62]: 学习小波对椭圆算子的齐次化

    Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v1 [math.NA])

    [http://arxiv.org/abs/2306.12006](http://arxiv.org/abs/2306.12006)

    本文提出了一种新的数据驱动方法，可以学习用于椭圆算子的齐次化映射，以建立考虑接口的齐次化本构定律，并且证明了该方法的有效性。

    

    多尺度偏微分方程在各种应用中都有出现。齐次化理论是消除小尺度依赖的强有力方法，可以得到简化的方程以及计算上的便利。在连续介质力学领域，齐次化对于导出包含微观物理学的本构定律以制定感兴趣的宏观量的平衡方程很关键。然而，获得齐次化本构定律通常是具有挑战性的，因为它们通常没有解析形式，并且可以展现在微观尺度上不存在的现象。针对这个问题，提出了数据驱动的学习本构定律方法。本文提出了一种新的基于数据驱动的学习椭圆算子齐次化映射的方法，可以说明与这种接口有关的齐次化建立。我们的方法通过构造适当的特征空间，以考虑底层几何学和微观结构，自适应地学习将这些接口合并到齐次化本构定律中。我们的数字结果表明，所提出的方法可以准确地捕捉有效的宏观行为，优于现有方法。

    Multiscale partial differential equations (PDEs) arise in various applications, and several schemes have been developed to solve them efficiently. Homogenization theory is a powerful methodology that eliminates the small-scale dependence, resulting in simplified equations that are computationally tractable. In the field of continuum mechanics, homogenization is crucial for deriving constitutive laws that incorporate microscale physics in order to formulate balance laws for the macroscopic quantities of interest. However, obtaining homogenized constitutive laws is often challenging as they do not in general have an analytic form and can exhibit phenomena not present on the microscale. In response, data-driven learning of the constitutive law has been proposed as appropriate for this task. However, a major challenge in data-driven learning approaches for this problem has remained unexplored: the impact of discontinuities and corner interfaces in the underlying material. These discontinui
    
[^63]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^64]: 使用4位整数训练Transformer

    Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])

    [http://arxiv.org/abs/2306.11987](http://arxiv.org/abs/2306.11987)

    本文提出了一种使用INT4算术训练Transformer的方法，并细致地分析了转换器中激活和梯度的特定结构，为它们提出了专用的量化器。算法在多个任务中达到了竞争性的准确性。

    

    将激活、权重和梯度量化为4位有望加速神经网络训练。然而，现有的4位训练方法需要自定义数值格式，而这些格式不受当代硬件支持。本文提出一种使用INT4算术实现所有矩阵乘法的Transformer训练方法。以极低的INT4精度训练是一项具有挑战性的任务。为了实现这一点，我们仔细分析了Transformer中激活和梯度的特定结构，为它们提出了专用量化器。对于前向传播，我们识别了离群值的挑战，并提出了一种哈达玛量化器来抑制离群值。对于反向传播，我们利用梯度的结构稀疏性，提出位分裂和杠杆得分采样技术来精确量化梯度。我们的算法在包括自然语言理解、机器翻译和图像分类在内的一系列任务上取得了竞争性的精度。

    Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and ima
    
[^65]: 应用于临床预测模型的热门XAI的评估：它们可信吗？

    Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?. (arXiv:2306.11985v1 [cs.LG])

    [http://arxiv.org/abs/2306.11985](http://arxiv.org/abs/2306.11985)

    该论文评估了两种流行的解释人工智能方法，分析它们是否为临床决策提供可信的解释和适当的领域表示。

    

    缺乏透明度和可解释性阻碍了机器学习（ML）算法在临床上的采用。本研究评估了用于解释医疗领域预测模型的两种流行的可解释人工智能（XAI）方法，以确定它们是否产生特定应用任务相关的领域适当的表示，是否会影响临床工作流程并保持一致性。因此，分析了在群体和患者水平上生成的解释。本文报道了第一次将XAI方法应用于风险预测模型的基准测试，评估了生成的解释与未来临床恶化事件的触发器之间的一致性。

    The absence of transparency and explainability hinders the clinical adoption of Machine learning (ML) algorithms. Although various methods of explainable artificial intelligence (XAI) have been suggested, there is a lack of literature that delves into their practicality and assesses them based on criteria that could foster trust in clinical environments. To address this gap this study evaluates two popular XAI methods used for explaining predictive models in the healthcare context in terms of whether they (i) generate domain-appropriate representation, i.e. coherent with respect to the application task, (ii) impact clinical workflow and (iii) are consistent. To that end, explanations generated at the cohort and patient levels were analysed. The paper reports the first benchmarking of the XAI methods applied to risk prediction models obtained by evaluating the concordance between generated explanations and the trigger of a future clinical deterioration episode recorded by the data colle
    
[^66]: 用均衡混合的超网络学习CNN池化结构

    Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture. (arXiv:2306.11982v1 [cs.CV])

    [http://arxiv.org/abs/2306.11982](http://arxiv.org/abs/2306.11982)

    本研究分析了在CIFAR10数据集上不同池化配置下的模型性能表现，并发现预定义的下采样配置不一定是最优的。为了解决这个问题，提出了一个新的均衡混合超网络（BMSN）来优化下采样配置。

    

    下采样层是卷积神经网络结构的关键组成部分，决定图像特征分析的粒度/尺度以及给定层的感受野大小。为了深入了解这个问题，我们使用ResNet20网络分析了在CIFAR10上各个池化配置单独训练的模型的性能，并且表明下采样层的位置可以极大地影响网络的性能，预定义的下采样配置并不是最优的。网络架构搜索（NAS）可以用作优化下采样配置的超参数。但是，我们发现基于单个超网络的常见一次性 NAS 在这个问题上不起作用。我们认为这是因为为了找到最优池化配置而训练的超网络将其参数完全共享于所有池化配置。这使它的训练非常困难，因为学习率的调整不当会导致不稳定的反向传播。我们提出一个新的均衡混合超网络（BMSN）来解决这个问题。

    Downsampling layers, including pooling and strided convolutions, are crucial components of the convolutional neural network architecture that determine both the granularity/scale of image feature analysis as well as the receptive field size of a given layer. To fully understand this problem, we analyse the performance of models independently trained with each pooling configurations on CIFAR10, using a ResNet20 network, and show that the position of the downsampling layers can highly influence the performance of a network and predefined downsampling configurations are not optimal. Network Architecture Search (NAS) might be used to optimize downsampling configurations as an hyperparameter. However, we find that common one-shot NAS based on a single SuperNet does not work for this problem. We argue that this is because a SuperNet trained for finding the optimal pooling configuration fully shares its parameters among all pooling configurations. This makes its training hard, because learnin
    
[^67]: 用于多分类任务的量子分类器的通用对抗扰动

    Universal adversarial perturbations for multiple classification tasks with quantum classifiers. (arXiv:2306.11974v1 [quant-ph])

    [http://arxiv.org/abs/2306.11974](http://arxiv.org/abs/2306.11974)

    本文探讨了量子通用对抗扰动，并发现一个精心制作的通用扰动可以成功地欺骗两个不同分类任务上达到最先进准确性的量子分类器，这为构建安全的量子机器学习系统带来潜在威胁。

    

    量子对抗机器学习是一门新兴的领域，它研究了量子学习系统对抗扰动的脆弱性并开发了可能的防御策略。量子通用对抗扰动是一种小的扰动，可以使不同的输入样本成为误导给定量子分类器的对抗示例。尽管此领域之前鲜有探究，但是通用扰动可能极大地简化恶意攻击，对量子机器学习模型造成意想不到的破坏。本文在异构分类任务的背景下，进一步探讨量子通用对抗扰动。特别地，我们发现，几乎在两个不同分类任务上达到最先进准确性的量子分类器，都可以被一个精心制作的通用扰动所诱导成功地欺骗。这一结果已经在计算机视觉和量子机器学习社区中广泛使用的数据集CIFAR-10和Iris中得到明确的证明。我们的发现表明，通用对抗扰动是量子机器学习模型的潜在威胁，并可能给构建安全的量子机器学习系统带来巨大挑战。

    Quantum adversarial machine learning is an emerging field that studies the vulnerability of quantum learning systems against adversarial perturbations and develops possible defense strategies. Quantum universal adversarial perturbations are small perturbations, which can make different input samples into adversarial examples that may deceive a given quantum classifier. This is a field that was rarely looked into but worthwhile investigating because universal perturbations might simplify malicious attacks to a large extent, causing unexpected devastation to quantum machine learning models. In this paper, we take a step forward and explore the quantum universal perturbations in the context of heterogeneous classification tasks. In particular, we find that quantum classifiers that achieve almost state-of-the-art accuracy on two different classification tasks can be both conclusively deceived by one carefully-crafted universal perturbation. This result is explicitly demonstrated with well-
    
[^68]: AdCraft：一种用于搜索引擎营销优化的高级强化学习基准环境

    AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])

    [http://arxiv.org/abs/2306.11971](http://arxiv.org/abs/2306.11971)

    AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。

    

    本文介绍了一种新的强化学习基准环境—— AdCraft，其具有随机和非静态特性。该环境模拟了搜索引擎营销中出价和预算的动态变化。SEM是一种利用付费广告来增加网站在搜索引擎结果页面上的可见性的数字营销技术。SEM广告活动的表现取决于多个因素，包括关键字选择、广告设计、出价管理、预算调整和表现监控。深度强化学习最近被认为是一种优化SEM广告投放活动的潜在策略，但需要大量数据，在实践中可能成本高昂或不可行。我们的可定制环境使从业者能够评估和提高与SEM出价和预算管理相关的RL算法的鲁棒性，而无需付出这些成本。通过在AdCraft环境下进行一系列实验，

    We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
    
[^69]: 基于互补学习子网络的参数高效类增量学习

    Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning. (arXiv:2306.11967v1 [cs.LG])

    [http://arxiv.org/abs/2306.11967](http://arxiv.org/abs/2306.11967)

    本文提出了一种基于互补学习子网络的无重演类增量学习方法，通过联合优化可塑性CNN特征提取器和分析前馈分类器来达到在不访问历史数据的情况下缓解灾难性遗忘的目的。

    

    在类增量学习(CIL)的场景下，深度神经网络必须适应非静态数据分布，例如随时间出现的新类。然而，CIL模型面临着臭名昭著的灾难性遗忘现象。传统的基于重演的方法依赖于存储旧类的样本来缓解灾难性遗忘，这限制了考虑内存资源和隐私问题的实际应用。本文提出了一种新颖的无重演CIL方法，通过两个互补学习子网络之间的协同学习不断地进行学习。我们的方法涉及联合优化可塑性卷积神经网络特征提取器和分析前馈分类器。历史数据的不可访问性通过完整地控制训练良好的模型的参数来处理，确保所学习的决策边界适合新类，同时保留先前学习的类的识别能力。

    In the scenario of class-incremental learning (CIL), deep neural networks have to adapt their model parameters to non-stationary data distributions, e.g., the emergence of new classes over time. However, CIL models are challenged by the well-known catastrophic forgetting phenomenon. Typical methods such as rehearsal-based ones rely on storing exemplars of old classes to mitigate catastrophic forgetting, which limits real-world applications considering memory resources and privacy issues. In this paper, we propose a novel rehearsal-free CIL approach that learns continually via the synergy between two Complementary Learning Subnetworks. Our approach involves jointly optimizing a plastic CNN feature extractor and an analytical feed-forward classifier. The inaccessibility of historical data is tackled by holistically controlling the parameters of a well-trained model, ensuring that the decision boundary learned fits new classes while retaining recognition of previously learned classes. Spe
    
[^70]: 采样个体公平且满足群体公平的排名

    Sampling Individually-Fair Rankings that are Always Group Fair. (arXiv:2306.11964v1 [cs.CY])

    [http://arxiv.org/abs/2306.11964](http://arxiv.org/abs/2306.11964)

    该论文提出了一种有效算法，从个体公平分布中采样排名，同时确保每个输出的排名都满足群体公平性约束。输出排名的期望效用至少是最优公平解的效用的$\alpha$倍，其中$\alpha$是一个量化公平约束紧度的参数。

    

    在线平台上的排名可以帮助用户快速找到相关信息，如人物、新闻、媒体和产品。公平排名是一种为了满足群体公平性约束而优化一组项目排名的任务，已经在算法公平性、信息检索和机器学习领域引起了广泛关注。然而，近期的研究表明项目效用的不确定性是不公平的主要原因，并建议在输出中引入随机性。这种随机性经过仔细选择，以确保对每个项目进行充分且合理的代表（同时考虑不确定性）。然而，由于这种随机性，输出的排名可能会违反群体公平性约束。我们提出了一个有效的算法，从一个个体公平分布中抽样排名，同时确保每个输出的排名都满足群体公平性约束。输出排名的期望效用至少是最优公平解的效用的 $\alpha$ 倍，其中 $\alpha$ 是一个量化公平约束紧度的参数。我们在真实世界数据集上进行实验，证明了我们算法的高效性和有效性。

    Rankings on online platforms help their end-users find the relevant information -- people, news, media, and products -- quickly. Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in the Algorithmic Fairness, Information Retrieval, and Machine Learning literature. Recent works, however, identify uncertainty in the utilities of items as a primary cause of unfairness and propose introducing randomness in the output. This randomness is carefully chosen to guarantee an adequate representation of each item (while accounting for the uncertainty). However, due to this randomness, the output rankings may violate group fairness constraints. We give an efficient algorithm that samples rankings from an individually-fair distribution while ensuring that every output ranking is group fair. The expected utility of the output ranking is at least $\alpha$ times the utility of the optimal fair solut
    
[^71]: TADIL：使用Transformer最近中心嵌入进行任务无关增量域学习中的任务识别

    TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings. (arXiv:2306.11955v1 [cs.LG])

    [http://arxiv.org/abs/2306.11955](http://arxiv.org/abs/2306.11955)

    本文提出了TADIL方法，使用Transformer最近中心嵌入技术，能够在任务无关的条件下，对域增量学习中的任务进行识别，并训练出一个轻量级的增量任务分类器。

    

    机器学习模型在面对随时间或领域变化的数据时会出现困难，而人类可以从这些非独立同分布的数据中学习。因此，连续学习是不可或缺的，特别是域增量学习。本文提出了一个新颖的流程，在不需要监督的情况下识别增量域学习场景中的任务。该流程包含四个步骤：首先，使用现有的基于Transformer的模型从原始数据中获取基础嵌入。其次，基于它们与集群中心的相似性，分组嵌入密度以获取每个集群中心最近的点。第三，只使用这些点训练增量任务分类器。最后，利用流程的轻量级计算要求，设计了一种算法，在在线情况下使用任务分类器来决定何时学习新任务并选择该任务的数据。

    Machine Learning (ML) models struggle with data that changes over time or across domains due to factors such as noise, occlusion, illumination, or frequency, unlike humans who can learn from such non independent and identically distributed data. Consequently, a Continual Learning (CL) approach is indispensable, particularly, Domain-Incremental Learning. In this paper, we propose a novel pipeline for identifying tasks in domain-incremental learning scenarios without supervision. The pipeline comprises four steps. First, we obtain base embeddings from the raw data using an existing transformer-based model. Second, we group the embedding densities based on their similarity to obtain the nearest points to each cluster centroid. Third, we train an incremental task classifier using only these few points. Finally, we leverage the lightweight computational requirements of the pipeline to devise an algorithm that decides in an online fashion when to learn a new task using the task classifier an
    
[^72]: 论如何处理有噪音的计算问题的最优界限

    On the Optimal Bounds for Noisy Computing. (arXiv:2306.11951v1 [cs.DS])

    [http://arxiv.org/abs/2306.11951](http://arxiv.org/abs/2306.11951)

    本文改进了有噪音计算问题的自适应采样和非自适应采样模型下所有四个函数的下界，并且大多数下界与上界符合，差异不超过一个常数因子。

    

    我们重新考虑了Feige等人1994年考虑的使用有噪声的信息进行计算的问题，其中包括从有噪声的查询中计算广义 OR 函数，以及从有噪声的成对比较中计算 MAX、SEARCH 和 SORT 函数。对于给定的 $K$ 个元素，目标是在每个查询的结果以概率 $p$ 翻转时，以至少 $1-\delta$ 的概率正确地恢复所需函数。我们考虑了自适应采样设置和非自适应采样设置。先前的工作提供了关于最坏情况下查询复杂度的紧密边界，这些边界是以 $K$ 依赖性为基础的。然而，在依赖于 $\delta$ 和 $p$ 方面，上下界并不匹配。我们提高了自适应和非自适应查询模型下所有四个函数的下界。我们的大多数下界与上界符合，差异不超过一个常数因子。

    We revisit the problem of computing with noisy information considered in Feige et al. 1994, which includes computing the OR function from noisy queries, and computing the MAX, SEARCH and SORT functions from noisy pairwise comparisons. For $K$ given elements, the goal is to correctly recover the desired function with probability at least $1-\delta$ when the outcome of each query is flipped with probability $p$. We consider both the adaptive sampling setting where each query can be adaptively designed based on past outcomes, and the non-adaptive sampling setting where the query cannot depend on past outcomes. The prior work provides tight bounds on the worst-case query complexity in terms of the dependence on $K$. However, the upper and lower bounds do not match in terms of the dependence on $\delta$ and $p$. We improve the lower bounds for all the four functions under both adaptive and non-adaptive query models. Most of our lower bounds match the upper bounds up to constant factors when
    
[^73]: 缓解神经网络中的通信成本：树突非线性的作用

    Mitigating Communication Costs in Neural Networks: The Role of Dendritic Nonlinearity. (arXiv:2306.11950v1 [cs.NE])

    [http://arxiv.org/abs/2306.11950](http://arxiv.org/abs/2306.11950)

    本研究发现，在神经网络中整合非线性树突结构可以显著提高模型的容量和性能，同时控制信号通信成本，这对于未来神经网络的发展具有重要的意义。

    

    生物神经网络的理解深刻地影响了人工神经网络（ANNs）的发展。然而，ANN中使用的神经元与其生物模型存在明显偏差，主要是由于缺少包含局部非线性的复杂树突。尽管存在这样的差异，先前的研究表明点神经元可以在执行计算任务方面在功能上替代树突神经元。在本研究中，我们审查了神经网络中非线性树突的重要性。通过使用机器学习方法，我们评估了树突结构非线性对神经网络性能的影响。我们的发现表明，整合树突结构可以在保持信号通信成本有效抑制的同时，显著增强模型容量和性能。这项研究提供了重要的见解，对未来神经网络的发展具有重要的意义。

    Our comprehension of biological neuronal networks has profoundly influenced the evolution of artificial neural networks (ANNs). However, the neurons employed in ANNs exhibit remarkable deviations from their biological analogs, mainly due to the absence of complex dendritic trees encompassing local nonlinearity. Despite such disparities, previous investigations have demonstrated that point neurons can functionally substitute dendritic neurons in executing computational tasks. In this study, we scrutinized the importance of nonlinear dendrites within neural networks. By employing machine-learning methodologies, we assessed the impact of dendritic structure nonlinearity on neural network performance. Our findings reveal that integrating dendritic structures can substantially enhance model capacity and performance while keeping signal communication costs effectively restrained. This investigation offers pivotal insights that hold considerable implications for the development of future neur
    
[^74]: 利用机器学习在多个异构数据集上预测冬小麦的产量

    Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets using Machine Learning. (arXiv:2306.11946v1 [cs.LG])

    [http://arxiv.org/abs/2306.11946](http://arxiv.org/abs/2306.11946)

    本研究在多个异构数据集上采用机器学习方法预测冬小麦的产量，结果表明数据质量对于机器学习策略的决定性作用。

    

    冬小麦是英国最重要的作物之一，而作物产量的预测对国家的粮食安全至关重要。已有多项研究使用机器学习技术在县或基于农场的层面上预测作物产量。本研究的主要目的是利用机器学习模型在多个异构数据集上（即区域层面上的土壤和天气）预测冬小麦的产量。实验结果展示了数据单独和结合使用的影响。此外，我们采用了多种机器学习算法，以强调数据质量在任何机器学习策略中的重要性。

    Winter wheat is one of the most important crops in the United Kingdom, and crop yield prediction is essential for the nation's food security. Several studies have employed machine learning (ML) techniques to predict crop yield on a county or farm-based level. The main objective of this study is to predict winter wheat crop yield using ML models on multiple heterogeneous datasets, i.e., soil and weather on a zone-based level. Experimental results demonstrated their impact when used alone and in combination. In addition, we employ numerous ML algorithms to emphasize the significance of data quality in any machine-learning strategy.
    
[^75]: 探究代码语言模型所学习的内容

    Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])

    [http://arxiv.org/abs/2306.11943](http://arxiv.org/abs/2306.11943)

    本研究探究了预先训练的代码语言模型的能力，证明其能够超越表面形式特征，学习精确而形式化定义的代码的计算语义。

    

    预先训练的语言模型在各种自然语言任务中都非常有效，但有人认为它们的能力不足以完全学习语言的意义或理解语言。为了了解语言模型能够学习某种形式的意义的程度，我们研究它们捕捉代码语义的能力，超越表层频率和共现的限制。与以往研究模型语言特征的探究相比，我们在一种可以客观地、简单明了地评估模型学习语义能力的环境下研究预训练模型。本文研究了这样的模型是否能捕捉精确而形式化定义的代码的语义。通过对代码片段的操纵实验，我们展示了代码预先训练模型学习了代码的计算语义的强有力的表征，超越了代码表面特征。

    Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone
    
[^76]: 一种用于异构数据集分析的深度学习模型——以冬小麦产量预测为例

    A Deep Learning Model for Heterogeneous Dataset Analysis -- Application to Winter Wheat Crop Yield Prediction. (arXiv:2306.11942v1 [cs.LG])

    [http://arxiv.org/abs/2306.11942](http://arxiv.org/abs/2306.11942)

    本文提出了一种能够处理异构数据集的深度学习模型，并在数字农业领域的真实数据集上展示其表现优于传统机器学习模型。

    

    西方国家严重依赖小麦，产量预测十分关键。已经有人探索并应用了用于产量预测的长短期记忆（LSTM）等时间序列深度学习模型，并且已有文献报告显示它们比传统机器学习模型表现更佳。然而，现有的 LSTMs 无法处理异构数据集（由时变和保持不变的数据组合而成）。在本文中，我们提出了一个能够处理异构数据集的高效深度学习模型。我们开发了系统架构并将其应用于数字农业领域中真实的数据集，证明了该模型表现优于现有的机器学习模型。

    Western countries rely heavily on wheat, and yield prediction is crucial. Time-series deep learning models, such as Long Short Term Memory (LSTM), have already been explored and applied to yield prediction. Existing literature reported that they perform better than traditional Machine Learning (ML) models. However, the existing LSTM cannot handle heterogeneous datasets (a combination of data which varies and remains static with time). In this paper, we propose an efficient deep learning model that can deal with heterogeneous datasets. We developed the system architecture and applied it to the real-world dataset in the digital agriculture area. We showed that it outperforms the existing ML models.
    
[^77]: 开放问题：基于变分目标的测度学习 (arXiv:2306.11928v1 [stat.ML])

    Open Problem: Learning with Variational Objectives on Measures. (arXiv:2306.11928v1 [stat.ML])

    [http://arxiv.org/abs/2306.11928](http://arxiv.org/abs/2306.11928)

    本文探讨了在测度上编写变分目标的动机，提出通过此类目标推导实用算法，以解决超出分布的泛化和弱监督学习等问题的开放问题。

    

    统计学习理论关注的是基于函数的变分目标。本文讨论了在测度上编写类似目标的动机，特别是讨论了超出分布的泛化和弱监督学习。这引发了一个自然的问题：能否将通常的统计学习结果转化为基于测量表达的目标？结果构建是否会导致新的实用算法？

    The theory of statistical learning has focused on variational objectives expressed on functions. In this note, we discuss motivations to write similar objectives on measures, in particular to discuss out-of-distribution generalization and weakly-supervised learning. It raises a natural question: can one cast usual statistical learning results to objectives expressed on measures? Does the resulting construction lead to new algorithms of practical interest?
    
[^78]: 没有错误的转弯：神经网络优化路径的简单几何学

    No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths. (arXiv:2306.11922v1 [cs.LG])

    [http://arxiv.org/abs/2306.11922](http://arxiv.org/abs/2306.11922)

    本文研究了样本梯度沿优化路径的基本几何性质，发现这些量在训练期间表现出可预测、一致的行为。我们的发现表明，优化轨迹不仅从未遇到显著的障碍，而且在大多数训练期间能够保持稳定的动力学。

    

    了解神经网络的优化动态对于弥合理论与实践之间的差距是必要的。随机一阶优化算法已被证明在深度神经网络中能够有效地定位有利的极小值。然而，这种效率与神经网络损失景观的非凸和看似复杂的结构形成了对比。在本研究中，我们深入研究了样本梯度沿优化路径的基本几何性质。我们专注于两个关键量，这些量出现在受限割线不等式和误差界限中。这两个量对于一阶优化具有高度重要性。我们的分析揭示了这些量在训练期间表现出可预测、一致的行为，尽管采样小批量引起的随机性。我们的发现表明，优化轨迹不仅从未遇到显著的障碍，而且在大多数训练期间能够保持稳定的动力学。这些观察到的特性已经足够

    Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, which appear in the restricted secant inequality and error bound. Both hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are suffi
    
[^79]: 自适应集成 Q 学习：通过误差反馈来减小估计偏差

    Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback. (arXiv:2306.11918v1 [cs.LG])

    [http://arxiv.org/abs/2306.11918](http://arxiv.org/abs/2306.11918)

    本研究提出了自适应集成 Q 学习算法，通过误差反馈来减小集成方法中估计偏差的影响，并结合模型识别自适应控制（MIAC）来实现集成大小自适应。

    

    集成方法是缓解 Q-learning 中过度估计问题的一种有效方法，其中使用多个函数逼近器来估计动作值。估计偏误严重依赖于集成大小（即目标中使用的 Q 函数逼近器数量），因此决定“正确”的集成大小非常困难，因为在学习过程中函数逼近误差会发生变化。针对这一挑战，我们首先根据估计偏差导出了上限和下限，并根据这些界对集成大小进行自适应，从而将偏差驱动到接近零，因此可以相应地处理时间变化逼近误差的影响。我们将集成方法与模型识别自适应控制（MIAC）相结合，提出了自适应集成 Q 学习（AdaEQ），是一种广义的 Q 学习算法，它可以有效地自适应集成大小。

    The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the `right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized 
    
[^80]: 图分类问题中结构感知的鲁棒性认证

    Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])

    [http://arxiv.org/abs/2306.11915](http://arxiv.org/abs/2306.11915)

    该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。

    

    对于基于图的机器学习模型进行鲁棒性认证是保证安全性的一个至关重要的挑战。目前用于图分类器的鲁棒性证明保证与节点对翻转（添加或删除边缘）的总数有关，这相当于以邻接矩阵为中心的l0球。尽管从理论上看很有吸引力，但这种各向同性的结构噪声在实际场景中可能过于严格，因为有些节点对于确定分类器的输出更为关键。在这种情况下，证书给出了对图模型鲁棒性的悲观描述。为了解决这个问题，我们开发了一种基于随机平滑的方法，将非各向同性的噪声分布添加到输入图结构中。我们展示了我们的过程为分类器生成了结构感知的证书，因此鲁棒性证书的大小可以在图的不同预定义结构之间变化。我们在几个基准图分类任务上展示了我们方法的优势，在对抗性攻击的鲁棒性方面取得了最先进的结果。

    Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
    
[^81]: 随机量化，是联邦学习中实现差分隐私的唯一所需。

    Randomized Quantization is All You Need for Differential Privacy in Federated Learning. (arXiv:2306.11913v1 [cs.LG])

    [http://arxiv.org/abs/2306.11913](http://arxiv.org/abs/2306.11913)

    该论文提出了一种基于随机量化的机制，通过两级量化实现了差分隐私的保证，这可以用于处理联邦学习中的隐私问题，并实现了接近最优的隐私效用平衡。

    

    联邦学习是一种常见和实用的机器学习模型，用于在分散的环境中学习。分散学习的一个主要动机是数据隐私，保证学习者永远不会查看每个本地源的数据。最大的挑战是处理服务器和大量数据源之间的可能复杂的模型更新，以及本地更新本身可能会泄露有关数据源的信息。为了解决这些问题，我们考虑将量化和差分隐私相结合的联邦学习方法。我们开发了一种新的算法RQM，通过两个级别的随机量化获得隐私性。该机制为传输的更新和每个数据源的局部差分隐私提供差分隐私保证。我们进一步为在FL中执行RQM的总隐私成本提供了上限，并表明该机制在隐私效用平衡方面实现了接近最优的结果。

    Federated learning (FL) is a common and practical framework for learning a machine model in a decentralized fashion. A primary motivation behind this decentralized approach is data privacy, ensuring that the learner never sees the data of each local source itself. Federated learning then comes with two majors challenges: one is handling potentially complex model updates between a server and a large number of data sources; the other is that de-centralization may, in fact, be insufficient for privacy, as the local updates themselves can reveal information about the sources' data. To address these issues, we consider an approach to federated learning that combines quantization and differential privacy. Absent privacy, Federated Learning often relies on quantization to reduce communication complexity. We build upon this approach and develop a new algorithm called the \textbf{R}andomized \textbf{Q}uantization \textbf{M}echanism (RQM), which obtains privacy through a two-levels of randomizat
    
[^82]: 基于Copula的依赖截尾深度生存模型

    Copula-Based Deep Survival Models for Dependent Censoring. (arXiv:2306.11912v1 [cs.LG])

    [http://arxiv.org/abs/2306.11912](http://arxiv.org/abs/2306.11912)

    论文提出了一种基于Copula的参数化生存模型，通过放宽条件独立性的假设，扩展现代非线性生存分析，从而显著改进了对生存分布的估计。

    

    生存数据集描述了一组实例（例如患者），并为每个实例提供了事件发生的时间（例如死亡）或截尾时间（例如失去随访的时间 - 这是事件发生时间的下限）。本文提出了一种基于Copula的参数化生存模型，通过放宽条件独立性的假设，扩展现代非线性生存分析，从而显著改进了对生存分布的估计。

    A survival dataset describes a set of instances (e.g. patients) and provides, for each, either the time until an event (e.g. death), or the censoring time (e.g. when lost to follow-up - which is a lower bound on the time until the event). We consider the challenge of survival prediction: learning, from such data, a predictive model that can produce an individual survival distribution for a novel instance. Many contemporary methods of survival prediction implicitly assume that the event and censoring distributions are independent conditional on the instance's covariates - a strong assumption that is difficult to verify (as we observe only one outcome for each instance) and which can induce significant bias when it does not hold. This paper presents a parametric model of survival that extends modern non-linear survival analysis by relaxing the assumption of conditional independence. On synthetic and semi-synthetic data, our approach significantly improves estimates of survival distributi
    
[^83]: 基于定点树的广义随机森林加速

    Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])

    [http://arxiv.org/abs/2306.11908](http://arxiv.org/abs/2306.11908)

    本文提出一种新的树生长规则，使广义随机森林在无梯度优化的情况下大大节省了时间。

    

    广义随机森林建立在传统随机森林的基础上，通过将其作为自适应核加权算法来构建估算器，并通过基于梯度的树生长过程来实现。我们提出了一种新的树生长规则，基于定点迭代近似表示梯度近似，实现了无梯度优化，并为此开发了渐近理论。这有效地节省了时间，尤其是在目标量的维度适中时。

    Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
    
[^84]: Deep Fusion：基于预训练初始化的高效网络训练

    Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])

    [http://arxiv.org/abs/2306.11903](http://arxiv.org/abs/2306.11903)

    本文提出了Deep Fusion，一种基于预训练初始化的高效网络训练方法。通过加速训练过程、降低计算要求，并导致改进的泛化性能，使得该方法在维持传统训练方法的性能甚至超越其性能的同时，减少了训练时间和资源消耗。

    

    近年来，深度学习在众多领域中取得了显著进展，尤其是对自然语言处理任务产生了显著影响。训练深度神经网络的挑战之一是需要大量的计算资源和时间。本文提出了Deep Fusion，一种利用较小网络的预训练初始化的高效网络训练方法。我们展示了Deep Fusion在各种NLP任务和T5模型大小上加速训练过程，降低计算要求，并导致改进的泛化性能。我们的实验表明，Deep Fusion是一种实用和有效的方法，可以在维持传统训练方法的性能甚至超越其性能的同时，减少训练时间和资源消耗。

    In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.
    
[^85]: 闭环反馈：基于机器学习的同步辐射实验自主化在线数据分析。

    Closing the loop: Autonomous experiments enabled by machine-learning-based online data analysis in synchrotron beamline environments. (arXiv:2306.11899v1 [physics.data-an])

    [http://arxiv.org/abs/2306.11899](http://arxiv.org/abs/2306.11899)

    本研究将机器学习技术应用于同步辐射实验中的X射线反射测量，形成闭环反馈系统，并实现了基于在线数据分析的实时决策。

    

    近年来，将机器学习技术应用于X射线散射实验已受到重视，这证明了它在研究涉及大量数据或快速产生数据集的价值。机器学习允许自动解释实验结果，特别是那些从同步辐射或中子设施获得的结果。机器学习模型处理数据的速度提供了一个重要的机会，建立闭环反馈系统，实现基于在线数据分析的实时决策。在这项研究中，我们描述了如何将机器学习融入X射线反射测量（XRR）的闭环工作流程中，以有机薄膜的生长为例。我们的重点在于机器学习基于在线数据分析和闭环反馈在光束线中的集成。我们提出了在实验过程中提供基本数据分析的解决方案，而不会在光束线控制中引入其他软件依赖。

    Recently, there has been significant interest in applying machine learning (ML) techniques to X-ray scattering experiments, which proves to be a valuable tool for enhancing research that involves large or rapidly generated datasets. ML allows for the automated interpretation of experimental results, particularly those obtained from synchrotron or neutron facilities. The speed at which ML models can process data presents an important opportunity to establish a closed-loop feedback system, enabling real-time decision-making based on online data analysis. In this study, we describe the incorporation of ML into a closed-loop workflow for X-ray reflectometry (XRR), using the growth of organic thin films as an example. Our focus lies on the beamline integration of ML-based online data analysis and closed-loop feedback. We present solutions that provide an elementary data analysis in real time during the experiment without introducing the additional software dependencies in the beamline contr
    
[^86]: 无法解释的解释：解释tSNE和UMAP嵌入的方法

    Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings. (arXiv:2306.11898v1 [cs.LG])

    [http://arxiv.org/abs/2306.11898](http://arxiv.org/abs/2306.11898)

    本文研究旨在回答ARDR算法的收敛性问题，我们将ARDR方法与传统的降维技术联系起来，可以将PCA嵌入完全恢复，通过稍加修改可以用LLE复现ARDR嵌入，并形式化了一系列猜想，如果成立，可以将2D嵌入中的结构归因于输入分布。

    

    将神经网络的潜在空间解释为吸引/排斥降维（ARDR）方法（如tSNE和UMAP）已成为标准。这取决于2D表示中的结构与模型潜在空间中的结构一致的先决条件。然而，这是一个未经证明的假设，我们不知道ARDR算法是否有任何收敛保证。本文旨在通过将ARDR方法与传统的降维技术联系起来，来回答这个问题。具体来说，我们展示了可以通过在随机初始化的数据集上应用吸引和排斥来完全恢复PCA嵌入。我们还展示了如果稍加修改，局部线性嵌入（LLE）可以复现ARDR嵌入的方法。最后，我们系统化了一系列猜想，如果这些猜想成立，就可以将2D嵌入中的结构归因于输入分布。

    It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.
    
[^87]: 学习结构蒙日位移的成本

    Learning Costs for Structured Monge Displacements. (arXiv:2306.11895v1 [stat.ML])

    [http://arxiv.org/abs/2306.11895](http://arxiv.org/abs/2306.11895)

    本文提出了一种新的方法，即学习合适的成本结构来鼓励映射沿着特定的工程特征来传送点，其在扩展 Monge-Bregman-Occam 管道方面做出了重大贡献，并在匹配直方图方面表现出了优异的性能。

    

    最优输运理论为机器学习提供了多种推断样本间密度前向映射的工具。尽管最近在机器学习领域中该理论已经见证了许多方法的发展，但其实际实现仍然极其困难，因为它同时面临计算和统计上的挑战。现有方法很少有不使用默认选择来估计这些映射的情况，其中简单的平方欧氏距离作为地面费用$c(x,y)=\|x-y\|^2_2$。我们在本文中采取不同的方法，以\emph{学习}合适的成本结构，鼓励映射沿着特定的工程特征来传送点。我们将最近提出的 Monge-Bregman-Occam 管道~\citep{cuturi2023monge} 的范式进行了扩展，该范式基于替代的成本公式$c(x,y)=h(x-y)$ ，它也是成本不变的，但采用更一般的形式$h=\tfrac12 \ell_2^2+\tau$，其中$\tau$是适当的凸规则项。

    Optimal transport theory has provided machine learning with several tools to infer a push-forward map between densities from samples. While this theory has recently seen tremendous methodological developments in machine learning, its practical implementation remains notoriously difficult, because it is plagued by both computational and statistical challenges. Because of such difficulties, existing approaches rarely depart from the default choice of estimating such maps with the simple squared-Euclidean distance as the ground cost, $c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the motivation of \emph{learning} a suitable cost structure to encourage maps to transport points along engineered features. We extend the recently proposed Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is an approp
    
[^88]: 单细胞显微镜下，干预风格转移实现外域泛化

    Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy. (arXiv:2306.11890v1 [cs.CV])

    [http://arxiv.org/abs/2306.11890](http://arxiv.org/abs/2306.11890)

    本文提出了一种新的方法干预风格转移（IST），通过生成干预训练分布，从而显着改善了单细胞显微镜下外域泛化的问题。

    

    计算机视觉技术的真实世界应用，包括生物医学研究中的发现过程，需要对上下文干扰具有不变性和对新数据具有泛化性的因果表征。利用两个新的单细胞荧光显微镜数据集的内在复制结构，我们提出了一般适用的测试方法，以评估模型在越来越具挑战性的OOD泛化水平上学习因果表征的程度。我们发现，尽管根据其他已建立的度量衡量，既有的天真基线设计用于防止混淆，但这些基线在这些测试中都会失效。我们引入了一种新的方法，干预风格转移（IST），通过生成干预训练分布，在其中减轻生物原因和干扰因素之间的虚假相关性，从而显着改善了OOD泛化性能。我们发布了我们的代码和数据集。

    Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance, as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.
    
[^89]: 强化学习中的扩散过程奖励塑形

    Reward Shaping via Diffusion Process in Reinforcement Learning. (arXiv:2306.11885v1 [cs.LG])

    [http://arxiv.org/abs/2306.11885](http://arxiv.org/abs/2306.11885)

    本研究使用扩散过程来进行奖励塑形，提供了解决探索 - 利用权衡的优雅框架。同时，阐明了信息熵、随机系统动力学以及它们对熵产生的影响之间的关系。

    

    强化学习（RL）模型不断发展，以在不确定的马尔可夫决策过程（MDP）中平衡探索和利用。本研究利用随机热力学和系统动力学原理，通过扩散过程探索奖励塑形。这提供了一个优雅的框架来思考探索 - 利用权衡。本文阐明了信息熵，随机系统动力学及其对熵产生的影响之间的关系。此探索使我们可以构建一个双重框架，可以解释为派生有效策略的最大熵程序，或者是计算信息成本和收益的修正成本优化程序。这项工作提出了信息的物理本质及其对MDP中的在线学习的影响的新视角，从而更好地理解RL中的信息取向公式。

    Reinforcement Learning (RL) models have continually evolved to navigate the exploration - exploitation trade-off in uncertain Markov Decision Processes (MDPs). In this study, I leverage the principles of stochastic thermodynamics and system dynamics to explore reward shaping via diffusion processes. This provides an elegant framework as a way to think about exploration-exploitation trade-off. This article sheds light on relationships between information entropy, stochastic system dynamics, and their influences on entropy production. This exploration allows us to construct a dual-pronged framework that can be interpreted as either a maximum entropy program for deriving efficient policies or a modified cost optimization program accounting for informational costs and benefits. This work presents a novel perspective on the physical nature of information and its implications for online learning in MDPs, consequently providing a better understanding of information-oriented formulations in RL
    
[^90]: 带特征对齐和分类器协作的个性化联邦学习

    Personalized Federated Learning with Feature Alignment and Classifier Collaboration. (arXiv:2306.11867v1 [cs.LG])

    [http://arxiv.org/abs/2306.11867](http://arxiv.org/abs/2306.11867)

    该研究提出了一种带有特征对齐和分类器协作的个性化联邦学习方法，通过利用全局语义知识进行显式的局部-全局特征对齐，并量化了每个客户端分类器组合的收益，作为组合权重的函数，对各种异构数据情形的基准数据集进行了广泛的评估。

    

    数据异构性是联邦学习中最具挑战性的问题之一，这促使人们采用各种方法为参与客户端学习个性化模型。在深度神经网络任务中，一种方法是使用共享特征表示，并为每个客户端学习定制分类器。然而，先前的工作在本地表示学习期间未利用全局知识，也忽略了本地分类器之间的细粒度协作，这限制了模型的泛化能力。在本文中，我们通过利用全球语义知识进行显式的局部-全局特征对齐，实现了更好的表示学习。此外，我们量化了每个客户端分类器组合的收益，作为组合权重的函数，并导出估计最优权重的优化问题。最后，对各种异构数据情形的基准数据集进行广泛评估结果证明了方法的有效性。

    Data heterogeneity is one of the most challenging issues in federated learning, which motivates a variety of approaches to learn personalized models for participating clients. One such approach in deep neural networks based tasks is employing a shared feature representation and learning a customized classifier head for each client. However, previous works do not utilize the global knowledge during local representation learning and also neglect the fine-grained collaboration between local classifier heads, which limit the model generalization ability. In this work, we conduct explicit local-global feature alignment by leveraging global semantic knowledge for learning a better representation. Moreover, we quantify the benefit of classifier combination for each client as a function of the combining weights and derive an optimization problem for estimating optimal weights. Finally, extensive evaluation results on benchmark datasets with various heterogeneous data scenarios demonstrate the 
    
[^91]: 无监督深度展开PGD在无线系统传输功率分配中的应用

    Unsupervised Deep Unfolded PGD for Transmit Power Allocation in Wireless Systems. (arXiv:2306.11865v1 [cs.LG])

    [http://arxiv.org/abs/2306.11865](http://arxiv.org/abs/2306.11865)

    本论文通过深度神经网络展开算法提出了一种无监督学习方法用于优化DNN权重，以实现传输功率控制。在密集D2D通信环境下经过性能评估，将迭代算法所需迭代次数(>2倍)大大降低。

    

    传输功率控制（TPC）是管理无线系统中干扰、能量利用和连接性的关键机制。本文提出了一种基于深度神经网络将迭代投影梯度下降（PGD）算法展开为网络层并学习步长参数的简单低复杂度TPC算法。通过使用无监督学习方法进行在线学习或离线预训练来优化DNN的权重。在密集设备对设备（D2D）通信场景下的性能评估表明，所提出的方法可以比迭代算法更快地实现更好的性能，并且所需迭代次数可以减少超过2倍。

    Transmit power control (TPC) is a key mechanism for managing interference, energy utilization, and connectivity in wireless systems. In this paper, we propose a simple low-complexity TPC algorithm based on the deep unfolding of the iterative projected gradient descent (PGD) algorithm into layers of a deep neural network and learning the step-size parameters. An unsupervised learning method with either online learning or offline pretraining is applied for optimizing the weights of the DNN. Performance evaluation in dense device-to-device (D2D) communication scenarios showed that the proposed method can achieve better performance than the iterative algorithm with more than a factor of 2 lower number of iterations.
    
[^92]: 一种无模型检验监督学习中特征影响的方法

    A Model-free Closeness-of-influence Test for Features in Supervised Learning. (arXiv:2306.11855v1 [cs.LG])

    [http://arxiv.org/abs/2306.11855](http://arxiv.org/abs/2306.11855)

    本文提出了一种无模型方法，评估响应值上两个特征的影响力，并提供一种新的测试方法。

    

    理解特征向量$x \in \mathbb{R}^d$对响应值（标签）$y \in \mathbb{R}$的影响是许多统计学习问题的基石。本文提出了一种新的视角来研究如何评估两个给定特征在响应值上具有不同影响力的问题。首先提出了特征影响力的紧密度概念，并证明了它能够恢复参数模型中系数的幅度的熟悉概念。然后，提出了一种新的方法来测试监督学习中的特征影响紧密度。我们的测试方法可以用于有限数量的样本，并且可以控制假阳性率。

    Understanding the effect of a feature vector $x \in \mathbb{R}^d$ on the response value (label) $y \in \mathbb{R}$ is the cornerstone of many statistical learning problems. Ideally, it is desired to understand how a set of collected features combine together and influence the response value, but this problem is notoriously difficult, due to the high-dimensionality of data and limited number of labeled data points, among many others. In this work, we take a new perspective on this problem, and we study the question of assessing the difference of influence that the two given features have on the response value. We first propose a notion of closeness for the influence of features, and show that our definition recovers the familiar notion of the magnitude of coefficients in the parametric model. We then propose a novel method to test for the closeness of influence in general model-free supervised learning problems. Our proposed test can be used with finite number of samples with control on
    
[^93]: 机器学习分析高分辨率透射电子显微镜数据时实验参数的泛化

    Generalization Across Experimental Parameters in Machine Learning Analysis of High Resolution Transmission Electron Microscopy Datasets. (arXiv:2306.11853v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2306.11853](http://arxiv.org/abs/2306.11853)

    该研究探讨了训练数据集中元数据特征的选择如何影响神经网络对纳米颗粒分割等任务的性能，并发现神经网络对显微镜参数不具有鲁棒性，但可以在某些样本参数上泛化。

    

    神经网络是高通量和准确的纳米材料透射电子显微镜（TEM）分析的有前途的工具，但已知它们在来自训练数据之外的数据上的泛化能力较差。鉴于在高分辨率TEM成像中通常看到的图像特征集有限，尚不清楚哪些图像被认为是来自其他图像的外部分布。在这里，我们研究训练数据集中元数据特征的选择如何影响神经网络对纳米颗粒分割等示例任务的性能，重点关注策划、实施高分辨率TEM图像数据集中的实验参数，包括放大率，剂量，纳米颗粒直径和纳米颗粒材料。总体而言，我们发现我们的神经网络在显微镜参数上不具有鲁棒性，但可以在某些样本参数上泛化。此外，数据预处理和网络结构设计等因素还对泛化性能产生了显着影响。

    Neural networks are promising tools for high-throughput and accurate transmission electron microscopy (TEM) analysis of nanomaterials, but are known to generalize poorly on data that is "out-of-distribution" from their training data. Given the limited set of image features typically seen in high-resolution TEM imaging, it is unclear which images are considered out-of-distribution from others. Here, we investigate how the choice of metadata features in the training dataset influences neural network performance, focusing on the example task of nanoparticle segmentation. We train and validate neural networks across curated, experimentally-collected high-resolution TEM image datasets of nanoparticles under controlled imaging and material parameters, including magnification, dosage, nanoparticle diameter, and nanoparticle material. Overall, we find that our neural networks are not robust across microscope parameters, but do generalize across certain sample parameters. Additionally, data pre
    
[^94]: 在兽医细胞学中使用超分辨率技术提高视觉感知和分割性能

    Using super-resolution for enhancing visual perception and segmentation performance in veterinary cytology. (arXiv:2306.11848v1 [cs.CV])

    [http://arxiv.org/abs/2306.11848](http://arxiv.org/abs/2306.11848)

    该研究通过整合超分辨率技术，在细胞学图像的语义分割质量上取得显着进展，为细胞学图像分析的发展提出了巨大的发展前景。

    

    本研究的主要目标是通过整合超分辨率（SR）架构来提高细胞学图像的语义分割质量。另一个贡献是开发一个新的数据集，旨在提高在不准确的聚焦情况下的成像质量。我们的实验结果表明，将SR技术整合到分割流水线中可能会导致平均精度（mAP）分割指标提高高达25％的显着改进。这些发现表明，利用SR架构在细胞学图像分析方面有着巨大的发展前景。

    The primary objective of this research was to enhance the quality of semantic segmentation in cytology images by incorporating super-resolution (SR) architectures. An additional contribution was the development of a novel dataset aimed at improving imaging quality in the presence of inaccurate focus. Our experimental results demonstrate that the integration of SR techniques into the segmentation pipeline can lead to a significant improvement of up to 25% in the mean average precision (mAP) segmentation metric. These findings suggest that leveraging SR architectures holds great promise for advancing the state of the art in cytology image analysis.
    
[^95]: 城市健康联系的解码：可解释的机器学习揭示了基于错综复杂城市特征的癌症患病率。

    Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features. (arXiv:2306.11847v1 [cs.LG])

    [http://arxiv.org/abs/2306.11847](http://arxiv.org/abs/2306.11847)

    本研究通过XGBoost机器学习模型揭示了社会人口统计学、建筑环境特征和环境危害曝露特征是影响城市癌症患病率的关键，结合因果推断实验提出了增加绿化空间、减少开发区和总排放量可以缓解癌症的发生。

    

    本研究调查了社会人口统计学、建筑环境特征和环境危害曝露特征在决定社区级癌症患病率中的相互作用。利用美国五个大都市统计区的数据：芝加哥、达拉斯、休斯敦、洛杉矶和纽约，本研究采用XGBoost机器学习模型预测了癌症患病率的程度，并评估了不同特征的重要性。我们的模型表现出可靠的性能，结果表明年龄、少数民族身份和人口密度是影响癌症患病率的最重要因素之一。我们进一步探讨了城市发展和设计策略，以减少癌症患病率，重点关注绿地、开发区和总排放量。通过一系列基于因果推断的实验评估，结果表明增加绿化空间，减少开发区和总排放量可以缓解癌症的发生。

    This study investigates the interplay among social demographics, built environment characteristics, and environmental hazard exposure features in determining community level cancer prevalence. Utilizing data from five Metropolitan Statistical Areas in the United States: Chicago, Dallas, Houston, Los Angeles, and New York, the study implemented an XGBoost machine learning model to predict the extent of cancer prevalence and evaluate the importance of different features. Our model demonstrates reliable performance, with results indicating that age, minority status, and population density are among the most influential factors in cancer prevalence. We further explore urban development and design strategies that could mitigate cancer prevalence, focusing on green space, developed areas, and total emissions. Through a series of experimental evaluations based on causal inference, the results show that increasing green space and reducing developed areas and total emissions could alleviate can
    
[^96]: 在多智能体环境下发现因果关系以实现高效协作

    Discovering Causality for Efficient Cooperation in Multi-Agent Environments. (arXiv:2306.11846v1 [cs.AI])

    [http://arxiv.org/abs/2306.11846](http://arxiv.org/abs/2306.11846)

    本文研究了因果关系在多智能体强化学习中的应用，通过对懒惰智能体进行惩罚以帮助团队取得更好的效果。此外，研究了如何利用因果关系估计改善对智能体的信用分配，以及如何使用Amortized Causal Discovery自动检测多智能体环境中的因果关系。

    

    在合作的多智能体强化学习中，智能体需要作为一个团队学习行为以实现共同的目标。然而，在学习任务的过程中，一些智能体可能会学习到次优策略，从而未能对团队目标做出贡献。这些智能体被称为“懒惰智能体”，因为它们的非合作性行为可能来自于未能理解是否导致回报。本文中，我们调查了因果关系在合作学习中的应用，以及如何应用它来惩罚这些懒惰的智能体。我们观察到，因果关系估计可用于改进对智能体的信用分配，并展示了如何利用它来改进多智能体独立学习。此外，我们还研究了如何使用分摊因果发现来自动检测多智能体环境中的因果关系。

    In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required to learn behaviours as a team to achieve a common goal. However, while learning a task, some agents may end up learning sub-optimal policies, not contributing to the objective of the team. Such agents are called lazy agents due to their non-cooperative behaviours that may arise from failing to understand whether they caused the rewards. As a consequence, we observe that the emergence of cooperative behaviours is not necessarily a byproduct of being able to solve a task as a team. In this paper, we investigate the applications of causality in MARL and how it can be applied in MARL to penalise these lazy agents. We observe that causality estimations can be used to improve the credit assignment to the agents and show how it can be leveraged to improve independent learning in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used to automate causality detection within MARL environments. The r
    
[^97]: 是否应该停止：具有异质种群的早期停止方法

    Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])

    [http://arxiv.org/abs/2306.11839](http://arxiv.org/abs/2306.11839)

    本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。

    

    随机实验由于治疗造成意外的有害影响，因此往往需要提前停止。目前确定何时提前终止实验的现有方法通常适用于总体数据，不考虑治疗效应的异质性。本文研究了针对异质种群有害实验的早期停止方法。我们首先确定现有方法在治疗对少数参与者造成伤害时往往无法停止实验。然后使用因果机器学习开发了CLASH，这是首个广泛适用于异质早期停止的方法。我们在模拟和实际数据上展示了CLASH的表现，并证明它在临床试验和A/B测试中都能有效提前停止。

    Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
    
[^98]: 拓扑视差：深度感知模型的几何规范说明

    Topological Parallax: A Geometric Specification for Deep Perception Models. (arXiv:2306.11835v1 [cs.LG])

    [http://arxiv.org/abs/2306.11835](http://arxiv.org/abs/2306.11835)

    拓扑视差是一种比较训练模型和参考数据集多尺度几何结构相似性的理论和计算工具，它可以估计模型中的拓扑特征，有助于理解深度学习模型的行为和性能。

    

    为了保证人工智能系统的安全性和鲁棒性，我们引入拓扑视差作为比较已训练模型和参考数据集的多尺度几何结构相似性的理论和计算工具。我们的证明和例子表明，数据集和模型之间的这种几何相似性对于可信的插值和扰动至关重要，并且我们猜测，这个新概念将为深度学习应用中过拟合和泛化之间不明确的关系的当前讨论增添价值。在典型的DNN应用中，模型的显式几何描述是不可能的，但视差可以通过检查使用参考数据集的测地畸变对Rips复合体的影响来估计模型中的拓扑特征（组件、周期、空洞等）。因此，视差指示模型与数据集是否共享类似的多尺度几何特征。视差通过拓扑数据分析理论，提供了从不同角度观察数据的直观概念，并为理解深度感知模型的行为和性能提供了新的视角。

    For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between overfitting and generalization in applications of deep-learning. In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topolo
    
[^99]: UMM: 无监督的均差最大化方法

    UMM: Unsupervised Mean-difference Maximization. (arXiv:2306.11830v1 [cs.LG])

    [http://arxiv.org/abs/2306.11830](http://arxiv.org/abs/2306.11830)

    提出了一种无需更改实验范例的无监督方法来检测事件相关电位 (ERPs) 中被关注的字母，并在两个公开数据集上优于现有的无监督和监督方法。

    

    许多脑机接口利用对视觉、听觉或触觉刺激产生的大脑信号进行操作，这些信号被称为事件相关电位 (ERPs)。在视觉 ERP 拼写器应用中，在屏幕上随机闪烁一组字母，参与者关注他们想要拼写的目标字母。当这个字母闪烁时，产生的 ERP 与任何其他非目标字母闪烁时产生的 ERP 有所不同。我们提出了一种新的无监督方法来检测这个被关注的字母。在每次试验中，对于每个可用字母，我们的方法都假设这个字母实际上是关注的字母，并根据这些假设计算 ERPs。我们利用了只有真实假设才会在类均值之间产生最大差异的事实。值得注意的是，这种无监督方法不需要对基础实验范例进行任何更改，因此几乎可以应用于所有基于 ERP 的设置。为了处理有限的数据，我们使用了一个翻译模型来增加训练数据。我们的实验结果表明，我们的方法在两个公开数据集上优于现有的无监督和监督方法。

    Many brain-computer interfaces make use of brain signals that are elicited in response to a visual, auditory or tactile stimulus, so-called event-related potentials (ERPs). In visual ERP speller applications, sets of letters shown on a screen are flashed randomly, and the participant attends to the target letter they want to spell. When this letter flashes, the resulting ERP is different compared to when any other non-target letter flashes. We propose a new unsupervised approach to detect this attended letter. In each trial, for every available letter our approach makes the hypothesis that it is in fact the attended letter, and calculates the ERPs based on each of these hypotheses. We leverage the fact that only the true hypothesis produces the largest difference between the class means. Note that this unsupervised method does not require any changes to the underlying experimental paradigm and therefore can be employed in almost any ERP-based setup. To deal with limited data, we use a 
    
[^100]: 任何深度ReLU网络都是浅层网络

    Any Deep ReLU Network is Shallow. (arXiv:2306.11827v1 [cs.LG])

    [http://arxiv.org/abs/2306.11827](http://arxiv.org/abs/2306.11827)

    该论文证明了任何深度的ReLU网络都可以被重写为一个具有透明性的浅层网络。这一结论有助于解释模型行为。

    

    我们构造性地证明了每个深度的ReLU网络可以被重写为一个函数上等价的三层网络，其中权重值为延迟实数。基于此证明，我们提供了一个算法，可以给出一个深度ReLU网络对应的显式权重。由此得到的浅层网络是透明的，并用于生成模型行为的解释。

    We constructively prove that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals. Based on this proof, we provide an algorithm that, given a deep ReLU network, finds the explicit weights of the corresponding shallow network. The resulting shallow network is transparent and used to generate explanations of the model s behaviour.
    
[^101]: 学会生成比你的LMM更好的文本

    Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])

    [http://arxiv.org/abs/2306.11816](http://arxiv.org/abs/2306.11816)

    本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。

    

    强化学习(RL)已经成为一种强大的范例，用于优化大型语言模型 (LLM) 条件文本生成。特别地，最近的LLM，如ChatGPT和GPT - 4能够与用户进行流畅的对话，并融合了RL和人类反馈。本研究受到学习搜索算法的启发，并利用文本生成的关键特性，探索了超出通用RL算法如PPO之外的强化学习算法。特别地，我们扩展了RL算法，使其能够与动态黑匣子的指导LLM如GPT-3进行交互，并提出了具有引导反馈的RL(RLGF)，这是一套用于LLM微调的RL算法。我们在GRUE基准测试的IMDB正向评论和CommonGen文本生成任务上进行了实验。我们展示了我们的RL算法比监督学习(SL)和默认PPO基线表现更高，证明了与指导LLM互动的好处。

    Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
    
[^102]: DynaQuant: 通过动态量化压缩深度学习训练检查点

    DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization. (arXiv:2306.11800v1 [cs.LG])

    [http://arxiv.org/abs/2306.11800](http://arxiv.org/abs/2306.11800)

    DynaQuant通过动态量化实现对各种最先进模型的显着压缩，几乎不影响模型准确性。

    

    随着深度学习训练工作量在计算资源和时间消耗方面的增加，遇到训练失败的可能性显著增加，导致工作丢失和资源浪费。最新的方法涉及有损模型压缩机制，这会在模型质量（准确性）和压缩比之间产生权衡。我们提出了一个新颖的动态量化框架，称为DynaQuant，它可以根据训练期间模型权重的灵敏度变化来更新量化级别，从而实现对各种最先进模型的显着压缩，并且几乎不影响模型准确性。

    With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient searc
    
[^103]: 多任务高斯过程中的时间变化转移矩阵

    Time-Varying Transition Matrices with Multi-task Gaussian Processes. (arXiv:2306.11772v1 [stat.ML])

    [http://arxiv.org/abs/2306.11772](http://arxiv.org/abs/2306.11772)

    本文介绍了一种基于核的多任务高斯过程模型，用于逼近个体的移动状态的潜在函数，并考虑了转移概率之间的相关性和时间可变性，通过强制执行随机性和非负性约束来实现研究目标。

    

    本文提出了一种基于核的多任务高斯过程模型，用于逼近个体的移动状态的潜在函数，该模型使用具有两个状态（移动和停留）的时间不均匀马尔科夫过程。我们的方法通过创建任务之间的协方差矩阵来考虑转移概率之间的相关性。我们还引入了时间可变性，假设个体的转移概率随着外部变量而随时间变化。通过将约束点集成到高斯过程中，我们强制执行马尔科夫过程中概率的随机性和非负性约束。我们还讨论了利用Toeplitz和Kronecker乘积结构在此上下文中加速GP估计和推断的机会。我们的数值实验表明，我们的公式可以强制执行所需的约束条件，同时学习转移概率的函数形式。

    In this paper, we present a kernel-based, multi-task Gaussian Process (GP) model for approximating the underlying function of an individual's mobility state using a time-inhomogeneous Markov Process with two states: moves and pauses. Our approach accounts for the correlations between the transition probabilities by creating a covariance matrix over the tasks. We also introduce time-variability by assuming that an individual's transition probabilities vary over time in response to exogenous variables. We enforce the stochasticity and non-negativity constraints of probabilities in a Markov process through the incorporation of a set of constraint points in the GP. We also discuss opportunities to speed up GP estimation and inference in this context by exploiting Toeplitz and Kronecker product structures. Our numerical experiments demonstrate the ability of our formulation to enforce the desired constraints while learning the functional form of transition probabilities.
    
[^104]: 解释性预测机器学习工件的设计：方法和实用演示

    Designing Explainable Predictive Machine Learning Artifacts: Methodology and Practical Demonstration. (arXiv:2306.11771v1 [cs.SE])

    [http://arxiv.org/abs/2306.11771](http://arxiv.org/abs/2306.11771)

    本文提出了一个解释性预测机器学习工件的设计方法学，以解决决策者不愿使用现代机器学习算法应用程序的问题，该方法学包括五个阶段。通过设计并实施一个工件，预测诊断为新冠肺炎的患者是否将来会出现花粉热症状来证明其适用性。

    

    面向预测的机器学习在组织中越来越有价值，因为它可以在重要的业务领域推动应用。然而，来自各行各业公司的决策者仍然极为不愿意雇用基于现代机器学习算法的应用程序。我们把这个问题归因于对于先进的机器学习算法普遍持有的观点，即它们是“黑盒子”，其复杂性不允许揭示驱动相应系统输出的因素。为了有助于克服这种采用障碍，我们认为信息系统研究应更多地关注设计能向人类决策者解释其预测的原型预测导向机器学习应用程序（即工件）。然而，尽管最近出现了许多工具来促进这种工件的开发，但迄今为止对其开发的研究还很少。我们认为这种研究空白是由于缺乏已建立的设计方法学，因此在本文中提出这样的方法学。我们的方法学包括五个阶段，引导工程师完成工件的调查、规范、设计、验证和实施。我们通过设计并实施一个工件来证明其适用性，该工件预测诊断为新冠肺炎的患者是否将来会出现花粉热症状。

    Prediction-oriented machine learning is becoming increasingly valuable to organizations, as it may drive applications in crucial business areas. However, decision-makers from companies across various industries are still largely reluctant to employ applications based on modern machine learning algorithms. We ascribe this issue to the widely held view on advanced machine learning algorithms as "black boxes" whose complexity does not allow for uncovering the factors that drive the output of a corresponding system. To contribute to overcome this adoption barrier, we argue that research in information systems should devote more attention to the design of prototypical prediction-oriented machine learning applications (i.e., artifacts) whose predictions can be explained to human decision-makers. However, despite the recent emergence of a variety of tools that facilitate the development of such artifacts, there has so far been little research on their development. We attribute this research g
    
[^105]: 基于几何深度学习的结构药物设计系统综述

    A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.11768](http://arxiv.org/abs/2306.11768)

    本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。

    

    结构药物设计利用蛋白质的三维几何结构来识别潜在的药物候选物，在药物发现中变得越来越重要。然而，基于物理化学建模和专家领域知识的传统方法费时费力。近年来，几何深度学习的发展，可以处理和整合三维几何数据，加上类似AlphaFold的工具提供准确的蛋白质三维结构预测，极大地推动了结构药物设计的进展。在本文中，我们系统地回顾了几何深度学习在结构药物设计中的最新进展。我们从结构药物设计中的主流任务、常用的3D蛋白质表示和预测/生成模型入手，然后详细介绍每个任务的回顾（例如结合位点预测、结合构象生成、\emph{de novo} 分子设计等），并按不同的几何深度学习方法进行组织。最后，我们总结了该领域未来研究的挑战和前景。

    Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
    
[^106]: 关于一些压缩算法 (arXiv:2306.11765v1 [eess.IV])

    About some compression algorithms. (arXiv:2306.11765v1 [eess.IV])

    [http://arxiv.org/abs/2306.11765](http://arxiv.org/abs/2306.11765)

    该论文针对图像压缩方法开展神经网络算法研究，以迭代函数系统框架实现相关创新。

    

    我们利用神经网络算法在迭代函数系统框架中寻找图像压缩方法，该框架是一组使区间$(0, 1)$的变换满足适当性质的集合。

    We use neural network algorithms for finding compression methods of images in the framework of iterated function systems which is a collection of the transformations of the interval $(0, 1)$ satisfying suitable properties.
    
[^107]: 频率归一化在音频频谱转换中提高录音设备普适性方面的应用研究

    On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers. (arXiv:2306.11764v1 [eess.AS])

    [http://arxiv.org/abs/2306.11764](http://arxiv.org/abs/2306.11764)

    该论文研究了如何在音频频谱转换中通过频率归一化来提高录音设备普适性。实验证明，在AST模型中使用频率归一化可以降低不同录音设备之间的差异，提高模型鲁棒性。

    

    机器学习中的一个主要挑战是训练数据与应用时数据之间的变化条件不同。该论文从声学情景分类的角度研究了在存在不匹配录音设备时如何有效地利用神经网络减少录音设备之间的差异。研究表明，在卷积神经网络的输入和隐藏层激活中进行频率归一化能够降低录音设备之间的差异，并显著提高模型的鲁棒性。本文研究了基于音频频谱变换的 AST 模型中，录音设备特征如何在隐藏层激活中表达，发现频率维度最初编码录音设备信息，但是经过第一次自注意力块后，它大部分被转换为令牌维度。基于这一观察结果，本文推测在频率维度抑制录音设备特征可能更好地推广AST模型到不匹配录音设备中。我们在多个声学情景分类数据集上进行了实验，结果表明频率归一化确实提高了AST模型在处理不同录音设备的数据时的准确度和稳健性比没有归一化的基线模型更高。

    Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device character
    
[^108]: MRFI：神经网络处理的开源多分辨率故障注入框架

    MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])

    [http://arxiv.org/abs/2306.11758](http://arxiv.org/abs/2306.11758)

    MRFI是一个高度可配置的神经网络故障注入工具，用户可以修改独立的故障配置文件进行注入和漏洞分析。

    

    为了确保即使在不可靠的硬件上也能进行有弹性的神经网络处理，通常需要在深度神经网络模型部署之前进行各种硬件故障的全面可靠性分析，并且需要高效的错误注入工具。然而，大多数现有的故障注入工具仍然局限于对神经元的基本故障注入，并未提供细粒度漏洞分析能力。此外，许多故障注入工具仍需要更改神经网络模型并使故障注入与正常神经网络处理紧密耦合，这进一步增加了故障注入工具的使用难度并减慢了故障模拟。在这项工作中，我们提出了一个高度可配置的深度神经网络多分辨率故障注入工具MRFI。它使用户能够修改独立的故障配置文件，而不是修改神经网络模型进行故障注入和漏洞分析。

    To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
    
[^109]: 预修剪和梯度下降改进差分隐私图像分类

    Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification. (arXiv:2306.11754v1 [cs.CV])

    [http://arxiv.org/abs/2306.11754](http://arxiv.org/abs/2306.11754)

    该研究提出了一种新的训练范式，使用预修剪和梯度下降技巧来减少参数空间和提高可扩展性，从而显著改善差分隐私图像分类的隐私-准确性折衷问题。

    

    应用差分隐私到深度神经网络的训练时，可扩展性是一个重大的挑战。常用的DP-SGD算法在实现高准确性的同时保持高隐私保护水平方面存在困难，即使在中等大小的模型上也是如此。为了解决这个挑战，我们利用神经网络有过度参数化的特点，从而提高了差分隐私中神经网络的训练效果。具体而言，我们引入了一种新的训练范式，使用预修剪和梯度下降来减少参数空间并提高可扩展性。该过程始于对原始网络的参数进行修剪，以获取更小的模型，然后使用DP-SGD进行训练。在训练过程中，不重要的梯度被删除，只有选择的梯度被更新。我们的训练范式在预修剪和梯度下降速率、隐私损失和分类准确性之间引入了一种张力。过多或过少的任何一种都可能导致准确性或隐私保护的下降。通过对CIFAR-10和CIFAR-100进行大量实验证明，我们提出的训练范式可以显著改善隐私-准确性的折衷，并实现最先进的差分隐私图像分类结果。

    Scalability is a significant challenge when it comes to applying differential privacy to training deep neural networks. The commonly used DP-SGD algorithm struggles to maintain a high level of privacy protection while achieving high accuracy on even moderately sized models. To tackle this challenge, we take advantage of the fact that neural networks are overparameterized, which allows us to improve neural network training with differential privacy. Specifically, we introduce a new training paradigm that uses \textit{pre-pruning} and \textit{gradient-dropping} to reduce the parameter space and improve scalability. The process starts with pre-pruning the parameters of the original network to obtain a smaller model that is then trained with DP-SGD. During training, less important gradients are dropped, and only selected gradients are updated. Our training paradigm introduces a tension between the rates of pre-pruning and gradient-dropping, privacy loss, and classification accuracy. Too mu
    
[^110]: 自主驾驶系统中数据集成的深度学习方法综述

    A survey on deep learning approaches for data integration in autonomous driving system. (arXiv:2306.11740v1 [cs.RO])

    [http://arxiv.org/abs/2306.11740](http://arxiv.org/abs/2306.11740)

    本文综述了自主驾驶系统感知模块的最新深度学习集成技术。其提出了一个新的集成分类系统，总结了集成操作及其优缺点，提供了新的见解，阐明了“理想”数据集成方法的特性，可减轻现有方法的局限性。本文总结了优化数据集成方法的关键特点。

    

    自主驾驶汽车的感知模块依赖于多传感器系统来理解其环境。深度学习的最新进展促进了多感知测量的集成方法的迅速发展，以增强感知能力。本文对应用于自主驾驶系统感知模块的最新深度学习集成技术进行了调查，将集成方法分为“何时集成”，“如何集成”和“何时集成”三类。提出了一个新的集成分类系统，基于三个维度：多视角，多模态和多帧。总结了集成操作及其优缺点，提供了新的见解，阐明了“理想”数据集成方法的特性，可减轻现有方法的局限性。经过对上百篇相关论文的审查，本综述总结了优化数据集成方法的关键特点。

    The perception module of self-driving vehicles relies on a multi-sensor system to understand its environment. Recent advancements in deep learning have led to the rapid development of approaches that integrate multi-sensory measurements to enhance perception capabilities. This paper surveys the latest deep learning integration techniques applied to the perception module in autonomous driving systems, categorizing integration approaches based on "what, how, and when to integrate." A new taxonomy of integration is proposed, based on three dimensions: multi-view, multi-modality, and multi-frame. The integration operations and their pros and cons are summarized, providing new insights into the properties of an "ideal" data integration approach that can alleviate the limitations of existing methods. After reviewing hundreds of relevant papers, this survey concludes with a discussion of the key features of an optimal data integration approach.
    
[^111]: 用于高效网格分割的神经形状直径函数

    Neural Shape Diameter Function for Efficient Mesh Segmentation. (arXiv:2306.11737v1 [cs.GR])

    [http://arxiv.org/abs/2306.11737](http://arxiv.org/abs/2306.11737)

    该论文提出了一种利用深度学习在网格分割之前编码映射函数的数据驱动方法，可以用于多种应用，不受分辨率影响。

    

    将多边形网格分割为有意义的部分可能具有挑战性。许多应用程序需要将这些结构分解以进行计算机图形学中的进一步处理。在过去的十年中，提出了几种方法来解决这个问题，但代价是计算时间的大量消耗。最近，机器学习已被证明对于对3D结构的分割任务是有效的。然而，这些最先进的方法往往难以推广，并需要将学习的模型分成几个特定的对象类别，以避免过度拟合。我们提出了一种数据驱动的方法，利用深度学习在网格分割之前编码映射函数，以用于多种应用。我们的网络使用顶点邻域之间的相似性复现邻域图，利用我们对形状直径函数（SDF）方法的了解。我们的方法不受分辨率影响，因为我们对输入网格进行下采样，并仅针对邻居查询完整分辨率结构。

    Partitioning a polygonal mesh into meaningful parts can be challenging. Many applications require decomposing such structures for further processing in computer graphics. In the last decade, several methods were proposed to tackle this problem, at the cost of intensive computational times. Recently, machine learning has proven to be effective for the segmentation task on 3D structures. Nevertheless, these state-of-the-art methods are often hardly generalizable and require dividing the learned model into several specific classes of objects to avoid overfitting. We present a data-driven approach leveraging deep learning to encode a mapping function prior to mesh segmentation for multiple applications. Our network reproduces a neighborhood map using our knowledge of the \textsl{Shape Diameter Function} (SDF) method using similarities among vertex neighborhoods. Our approach is resolution-agnostic as we downsample the input meshes and query the full-resolution structure solely for neighbor
    
[^112]: 事件流GPT：用于连续时间序列的复杂事件的数据预处理和建模库

    Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events. (arXiv:2306.11547v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11547](http://arxiv.org/abs/2306.11547)

    这篇论文介绍了Event Stream GPT (ESGPT)——一种用于构建连续时间事件序列的GPT模型的开源库。该库支持在医疗记录等具有内部依赖性的复杂事件上进行预测，具有高效易用且能达到最佳性能的特点。

    

    生成式、预训练变压器（GPT）通过在多个下游任务中的多样性，改变了自然语言处理（NLP），但其潜力远不止于此。本文提供了一种软件工具，扩展了GPT的适用性，使其适用于内部依赖关系的连续时间序列的复杂事件，例如医疗记录数据集。

    Generative, pre-trained transformers (GPTs, a.k.a. "Foundation Models") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that i
    
[^113]: 自上而下的机器学习用于粗粒化蛋白质力场

    Top-down machine learning of coarse-grained protein force-fields. (arXiv:2306.11375v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2306.11375](http://arxiv.org/abs/2306.11375)

    通过分子动力学模拟和可微分轨迹重加权训练神经网络势能，实现了自上而下的粗粒化蛋白质力场建模，仅需蛋白质的天然构象即可展示其外推能力。

    

    开发准确和高效的蛋白质粗粒化表征对于理解它们的折叠、功能和在长时间尺度下的相互作用至关重要。我们的方法包括通过分子动力学模拟蛋白质，并利用得到的轨迹通过可微分轨迹重加权来训练神经网络势能。令人惊讶的是，该方法仅需要蛋白质的天然构象，消除了从广泛模拟或内存密集型端到端可微分模拟导出标记数据的需求。一旦训练完成，模型可以用于并行分子动力学模拟，并对训练分布内外的蛋白质进行折叠事件采样，展示其外推能力。通过应用马尔可夫状态模型，可以预测模拟蛋白质的与天然构象相似的构象。由于其理论可转移性和仅使用蛋白质的天然构象的能力，可以在不同尺度上应用该方法。

    Developing accurate and efficient coarse-grained representations of proteins is crucial for understanding their folding, function, and interactions over extended timescales. Our methodology involves simulating proteins with molecular dynamics and utilizing the resulting trajectories to train a neural network potential through differentiable trajectory reweighting. Remarkably, this method requires only the native conformation of proteins, eliminating the need for labeled data derived from extensive simulations or memory-intensive end-to-end differentiable simulations. Once trained, the model can be employed to run parallel molecular dynamics simulations and sample folding events for proteins both within and beyond the training distribution, showcasing its extrapolation capabilities. By applying Markov State Models, native-like conformations of the simulated proteins can be predicted from the coarse-grained simulations. Owing to its theoretical transferability and ability to use solely e
    
[^114]: RM-PRT: 真实的机器人操作模拟器和基于渐进推理任务的基准评估

    RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.11335](http://arxiv.org/abs/2306.11335)

    该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。

    

    最近，预训练的大规模语言模型（LLM），如ChatGPT和GPT-4的出现，显着推进了机器的自然语言理解能力。这一突破使我们能够将这些开源LLM无缝地集成到统一的机器人模拟器环境中，以帮助机器人准确理解和执行人类自然语言指令。为此，我们引入了一个逼真的机器人操作模拟器，并在此基础上构建了一个基于渐进推理任务的机器人操作基准（RM-PRT）。具体而言，RM-PRT基准评估基于Unreal Engine 5构建了一个新的高保真数字双胞胎场景，其中包括782个类别，2023个物体，并使用ChatGPT生成了15,000个自然语言指令，以详细评估机器人操作。我们提出了一个通用的RM-PRT基准评估流程，该流程接受包含自然语言指令的多模态提示作为输入，并自动输出行动。

    Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
    
[^115]: 基于FPGA的粒子轨迹跟踪的低延迟边缘分类GNN

    Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs. (arXiv:2306.11330v1 [cs.AR] CROSS LISTED)

    [http://arxiv.org/abs/2306.11330](http://arxiv.org/abs/2306.11330)

    本论文介绍了一种基于FPGA的GNN架构，在粒子跟踪中实现了低延迟和资源高效性，并实现了对现有CPU和GPU的数千倍上的性能提升。

    

    大型强子对撞机中实时粒子轨迹重建面临高碰撞率和众多粒子撞击的挑战。使用FPGA上的GNN（图神经网络）可以实现更高的准确率和灵活的轨迹分类。然而，现有的GNN架构在边缘分类方面资源利用效率低且并行性不足。本文在FPGA上引入了一种资源高效的GNN架构，用于低延迟粒子跟踪。模块化的架构便于设计扩展以支持大型图形。利用击中探测器的几何特性进一步减少了图形的复杂性和资源利用率。我们在Xilinx UltraScale+ VU9P上的实验结果表明，相对于CPU和GPU，性能提高了1625倍和1574倍。

    In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.
    
[^116]: 增强属性聚类的图形变换方法：一种创新的图形转换器方法

    Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11307](http://arxiv.org/abs/2306.11307)

    本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。

    

    图表示学习是一种有影响力的方法，它使得我们更深入地理解图结构化数据，并有助于图聚类，这是各个领域的一个关键任务。注意力机制最近已经进入了图学习的领域，这从根本上改变了研究趋势。因此，图注意力网络和图注意力自编码器已成为图聚类任务优选的工具。然而，这些方法主要采用局部注意机制，从而限制了它们理解图中节点之间复杂全局依赖性的能力。为了解决这些障碍，本研究介绍了一种称为图自编码器的图形变换自编码器的图形聚类（GTAGC）的创新方法。通过将图自编码器与图形变换器融合，GTAGC能够捕获节点之间的全局依赖关系。这种融合提高了性能，使得图形聚类任务有了显着的提升。

    Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
    
[^117]: 基于提示的少样本学习在自然语言理解中的对抗鲁棒性研究

    Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (arXiv:2306.11066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11066](http://arxiv.org/abs/2306.11066)

    本文针对几种FSL方法的鲁棒性指出，与全微调模型相比，纯FSL模型面对对抗扰动会带来不可忽视的任务性能下降。但使用无标签数据生成提示或使用多个提示可以显著提高鲁棒性，在某些情况下甚至胜过全微调模型。

    

    现有的少样本学习（Few-shot learning，FSL）方法通过基于提示的微调在自然语言理解（NLU）任务上取得了显著的结果。本文对几种最先进的FSL方法进行了广泛的研究，以评估它们对于对抗扰动的鲁棒性。研究表明，相对于全微调模型，纯FSL方法面对对抗扰动会出现明显的任务性能下降，但使用无标签数据进行提示生成或在训练中使用多个提示，可以在某些情况下显著提高FSL方法的鲁棒性，甚至超过全微调模型的性能表现。此外，我们发现模型大小和类型选择在对抗鲁棒性上也有显著的影响。

    State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for pro
    
[^118]: 基于任务条件化超网络的多任务记忆深度强化学习

    Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10698](http://arxiv.org/abs/2306.10698)

    人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。

    

    深度强化学习算法通常受到采样效率低下的限制，严重依赖与环境的多次交互才能获得准确的决策能力。相比之下，人类似乎依赖海马体从过去有关任务的经历中检索相关信息，在学习新任务时指导其决策，而不是仅仅依赖于环境交互。然而，为代理设计类似海马体的模块以将过去的经历融入既有的强化学习算法面临两个挑战。第一个挑战涉及选择当前任务最相关的过去经验，第二个是将这些经验与决策网络相结合。为了解决这些问题，我们提出了一种新算法，利用基于任务条件化超网络的检索网络，根据任务调整检索网络的参数。

    Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
    
[^119]: MARBLE：音乐音频表征通用评估基准

    MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10548](http://arxiv.org/abs/2306.10548)

    本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。

    

    在艺术与人工智能（AI）之间交叉的广泛时代中，例如图像生成和虚构共创，音乐的AI仍然相对初步，特别是在音乐理解方面。针对这个问题，本论文介绍了一个通用的音乐音频表征评估基准MARBLE，旨在提供各种音乐信息检索（MIR）任务的基准，通过定义包括声学，演奏，乐谱和高级描述在内的四个层次的综合分类法。然后，我们基于8个公共可用数据集上的14项任务建立了一个统一的协议，提供了所有基于音乐录音开发的开放源代码的预训练模型的表征的公平和标准的评估。此外，MARBLE提供了一个易于使用、可扩展和可重用的工具库，以支持社区驱动的客观基准评估。

    In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
    
[^120]: 用图表示学习推进生物医学：最新进展，挑战和未来方向综述

    Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions. (arXiv:2306.10456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10456](http://arxiv.org/abs/2306.10456)

    图表示学习（GRL）已经成为一个重要领域，在生物医学领域有着广泛的应用，未来研究方向是解决GRL当前面临的挑战。

    

    图表示学习（GRL）已经成为一个重要领域，在包括生物医学在内的多个领域都做出了重大贡献。本综述的目的是回顾GRL方法在生物医学领域的最新进展和应用。我们还强调了GRL当前面临的主要挑战，以及未来研究的潜在方向。

    Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.
    
[^121]: 基于Samplet基 Pursuit 的核学习方法

    Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])

    [http://arxiv.org/abs/2306.10180](http://arxiv.org/abs/2306.10180)

    本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。

    

    本文考虑了基于l1正则化的Samplet坐标下的核学习问题。在Samplet基的系数上，应用l1正则化项可以强制增加稀疏性。因此，我们称这种方法为Samplet基 Pursuit。Samplet基是波形类型的有符号测度，专门用于散乱数据。它们具有与小波相似的本地化、多分辨率分析和数据压缩性质。可以在Samplet基上稀疏地表示的信号类比单尺度基上能够表示稀疏的信号类别要大得多。特别地，仅用基函数映射的几个特征叠加即可表示的所有信号也可以在Samplet坐标下实现稀疏表示。我们提出了一种高效解决该问题的方法，将软阈值和半光滑牛顿法相结合，并将该方法与快速迭代收缩阈值算法进行了比较。实验结果表明了该方法在稀疏性和预测精度方面的优势。

    We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
    
[^122]: 将矩阵对角化作为一种棋盘游戏: 教授本征值求解器最快的解决路径

    Matrix Diagonalization as a Board Game: Teaching an Eigensolver the Fastest Path to Solution. (arXiv:2306.10075v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2306.10075](http://arxiv.org/abs/2306.10075)

    本文展示了如何使用强化学习加速矩阵对角化，将选择最快的解决方案的过程视为棋盘游戏，为数值线性代数的性能提供了一种有前途的工具。

    

    矩阵对角化是科学计算中许多领域的基础。对角化矩阵以解决特征值问题需要一系列迭代，最终达到所有特征值和特征向量的充分收敛和精确解决。这通常会导致高计算成本。在这里，我们展示了如何使用 AlphaZero 框架的强化学习，将选择最快的解决方案的过程视为棋盘游戏，以加速 Jacobi 对角化。为了证明我们方法的可行性，我们将 Jacobi 对角化算法应用于出现在量子化学计算中的对称哈密顿矩阵。我们发现通常可以实现显着加速。我们的发现突显了使用机器学习作为改进数值线性代数性能的一种有前途的工具。

    Matrix diagonalization is at the cornerstone of numerous fields of scientific computing. Diagonalizing a matrix to solve an eigenvalue problem requires a sequential path of iterations that eventually reaches a sufficiently converged and accurate solution for all the eigenvalues and eigenvectors. This typically translates into a high computational cost. Here we demonstrate how reinforcement learning, using the AlphaZero framework, can accelerate Jacobi matrix diagonalizations by viewing the selection of the fastest path to solution as a board game. To demonstrate the viability of our approach we apply the Jacobi diagonalization algorithm to symmetric Hamiltonian matrices that appear in quantum chemistry calculations. We find that a significant acceleration can often be achieved. Our findings highlight the opportunity to use machine learning as a promising tool to improve the performance of numerical linear algebra.
    
[^123]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^124]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^125]: 结构化图模型先验下的协作学习

    Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])

    [http://arxiv.org/abs/2306.09595](http://arxiv.org/abs/2306.09595)

    本文提出了结构化协作学习算法，在不同设备之间通过协作完成分散任务。通过图模型先验生成的协作图，算法可以自动捕捉设备之间的跨任务相关性。

    

    我们研究了如何在分散设备上对不同任务进行个性化建模，这些设备的局部数据受限。我们提出了“结构化协作学习（SCooL）”，其中一个跨设备的协作图由图模型先验生成，以自动协调设备之间的相互学习。通过选择施加不同结构的图模型，我们可以通过变分推断推导出一类丰富的现有和新型去中心化学习算法。特别地，我们展示了三种 SCooL 的示例，在其中以 Dirac 分布、随机块模型（SBM）和注意力作为生成协作图的先验。这些 EM 类型的算法通过更新协作图和协同本地模型学习之间进行交替，仅通过监视模型更新来优化协作图，从而可以自动捕捉设备之间的跨任务相关性。

    We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
    
[^126]: FedMultimodal：面向多模态联邦学习的基准测试

    FedMultimodal: A Benchmark For Multimodal Federated Learning. (arXiv:2306.09486v1 [cs.DC])

    [http://arxiv.org/abs/2306.09486](http://arxiv.org/abs/2306.09486)

    FedMultimodal 是面向多模态联邦学习的基准测试，为此我们提出了第一个覆盖五种不同应用领域和十个社区的FL基准测试，以便促进多模态FL研究。

    

    近几年来，联邦学习（FL）已成为一种应对数据隐私挑战的新兴机器学习技术。在FL算法中，客户端提交本地训练模型，服务器将这些参数进行聚合直至收敛。尽管在计算机视觉、音频和自然语言处理等领域，对FL进行了重大努力，但是利用多模态数据流的FL应用仍然没有得到广泛探索。众所周知，多模态学习在情感识别、医疗保健、多媒体和社交媒体等领域具有广泛的现实应用，而用户隐私仍然是一个重要问题。具体来说，目前还没有针对多模态应用或相关任务的现有FL基准测试。为了促进多模态FL研究，我们介绍了FedMultimodal，这是第一个覆盖十个社区中的五个代表性多模态应用的FL基准测试。

    Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten comm
    
[^127]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^128]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^129]: SGEM：通过序列级广义熵最小化实现自动语音识别的测试时自适应

    SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.01981](http://arxiv.org/abs/2306.01981)

    SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。

    

    在许多实际情况下，自动语音识别（ASR）模型经常暴露于数据分布的变化，导致错误的预测。为了解决这个问题，最近提出了一种现有的测试时自适应（TTA）方法，可以在没有源数据的情况下调整预训练的ASR模型以适应未标记的测试实例。尽管有了不错的性能提升，但这项工作仅依赖于简单的贪心解码，并在帧级别上跨越时间步长进行调整，这在模型输出的序列性质下可能不是最优的。出于这个动机，我们提出了一个新的TTA框架，称为SGEM，用于一般ASR模型。为了处理序列输出，SGEM首先利用波束搜索来探索候选输出标志，并选择最可信的标志。然后，它利用广义熵最小化和负抽样作为无监督目标来适应模型。在各种领域的转变下，SGEM实现了三种主流ASR模型的最新性能。

    Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
    
[^130]: MutateNN：用于硬件加速器上图像识别模型的突变测试

    MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])

    [http://arxiv.org/abs/2306.01697](http://arxiv.org/abs/2306.01697)

    MutateNN是一种用于探索硬件加速器上深度学习图像识别模型鲁棒性的工具，提供突变测试和分析能力，且有效性已在多种预训练深度神经网络模型中得到验证。

    

    随着人工智能的研究进展，解决现实世界问题并推动技术发展的新机遇应运而生。图像识别模型特别是被分配了感知任务，以解决复杂的现实世界挑战并导致新的解决方案。此外，这类模型的计算复杂度和资源需求也有所增加。为了解决这个问题，模型优化和硬件加速已成为关键技术，但有效整合这些概念是一个具有挑战性和容易出错的过程。为了让开发人员和研究人员能够探索在不同硬件加速设备上部署的深度学习图像识别模型的鲁棒性，我们提出了MutateNN，这是一个为此目的提供突变测试和分析能力的工具。为了展示其功能，我们对7个广为人知的预训练深度神经网络模型进行了21种变异。我们在4种不同类型的硬件加速器上部署了我们的变异体，分析了它们的行为，并评估了MutateNN在检测出不正确或不精确的模型行为方面的有效性。

    With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
    
[^131]: 神经PDE代理的快速动态1D勃然模拟

    Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE Surrogates. (arXiv:2305.18944v2 [physics.plasm-ph] UPDATED)

    [http://arxiv.org/abs/2305.18944](http://arxiv.org/abs/2305.18944)

    该论文提出了一种使用神经网络的代理模型来进行快速铁托等离子体模拟的方法，解决了实时应用或详尽的参数扫描中速度太慢的问题。

    

    管理铁托式核聚变设备中的铁托等离子体对于应对其热量和粒子通量限制至关重要。模拟是理解和控制这些等离子体的重要工具，然而，对于实时应用或详尽的参数扫描，目前只有简单的近似方法速度足够快。我们使用神经PDE代理来解决这种快速模拟器的缺乏，即使用基于数据驱动的神经网络的代理模型，该模型使用使用经典数值方法生成的解进行训练。该代理模型近似演化参考基于物理模型的完整空间解决方案的时间步进算子。我们使用DIV1D作为参考模型来生成数据，即从X点（上游）到目标处的1D热流管的DIV1D。我们模拟了一个真实的TCV铁托等离子体，其动态是由上游密度坎口引起的，并提供了快速暂态的探索性展望。

    Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural PDE surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D's domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. Stat
    
[^132]: 控制学习效应以降低文本分类器中的误导性相关性

    Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers. (arXiv:2305.16863v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16863](http://arxiv.org/abs/2305.16863)

    该论文提出了一种自动化的增强算法，以适当改变新增输入的标签，从而最小化误导性相关性，并提高了少数群体的准确性，同时保持了总体准确性。

    

    针对NLP分类器学习训练特征和目标标签之间误导性相关性的问题，一种常见的方法是使模型的预测对这些特征具有不变性。然而，当这些特征对目标标签有非零因果效应并且对预测很重要时，这种方法可能会产生反作用。因此，我们使用因果推断文献中的方法，提出了一种算法，将学习到的特征对模型预测的效应规范化为特征对标签的估计效应。这导致了一种自动化的增强方法，利用特征的估计效应适当地改变新增的输入的标签。在毒性和IMDB评论数据集上，所提出的算法最小化了误导性相关性，并提高了少数群体（即打破误导性相关性的样本）的准确性，同时也提高了总体准确性。

    To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model's predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training.
    
[^133]: GUARD: 一个安全强化学习基准测试平台

    GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])

    [http://arxiv.org/abs/2305.13681](http://arxiv.org/abs/2305.13681)

    GUARD是一个广义统一安全强化学习开发基准测试平台，是目前广泛遍布且包含各种RL代理、任务和安全约束规范的一站式基准测试，能够全面涵盖最先进的安全RL算法，并具有高度的可自定义性。

    

    由于试错的性质，将RL算法应用于安全关键的现实应用（例如自动驾驶、人机交互、机器人操作等）通常是具有挑战性的，因为这些错误是不可容忍的。最近，安全RL（即约束RL）已经在文献中迅速出现，其中代理在满足约束条件的同时，探索环境。由于算法和任务的多样性，比较现有的安全RL算法仍然很困难。为了填补这一空白，我们介绍了GUARD，一个广义统一安全强化学习开发基准测试平台。与现有基准相比，GUARD具有几个优点。首先，GUARD是一个广义基准测试平台，具有各种RL代理、任务和安全约束规范。其次，GUARD全面涵盖了最先进的安全RL算法，并具有自包含的实现。第三，GUARD在任务和算法方面具有高度的可自定义性。我们提供了状态下现有方法在GUARD上的基准测试结果。

    Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
    
[^134]: 基于机器学习的控制流图恶意软件分析综述

    Survey of Malware Analysis through Control Flow Graph using Machine Learning. (arXiv:2305.08993v1 [cs.CR])

    [http://arxiv.org/abs/2305.08993](http://arxiv.org/abs/2305.08993)

    本文介绍了最新基于控制流图和机器学习的恶意软件检测方法，重点关注了从CFG中提取、表示、分类的不同方法。

    

    恶意软件对计算机系统和网络的安全构成了重大威胁，需要先进的技术对其进行行为和功能分析以进行检测。传统的基于签名的恶意软件检测方法由于恶意软件的快速演化已经失效。其中最有希望克服基于签名检测方法局限的技术之一是使用控制流图（CFG）。CFG利用程序的结构信息将可执行路径表示为图形，其中节点表示指令，边表示控制流依赖关系。目前利用机器学习算法从CFG中提取这些特征并将其分类为恶意或良性的方法已经成为主流。在本综述中，我们旨在回顾一些基于CFG和机器学习的恶意软件检测的最新方法，重点关注从CFG中提取，表示和分类的不同方法。

    Malware is a significant threat to the security of computer systems and networks which requires sophisticated techniques to analyze the behavior and functionality for detection. Traditional signature-based malware detection methods have become ineffective in detecting new and unknown malware due to their rapid evolution. One of the most promising techniques that can overcome the limitations of signature-based detection is to use control flow graphs (CFGs). CFGs leverage the structural information of a program to represent the possible paths of execution as a graph, where nodes represent instructions and edges represent control flow dependencies. Machine learning (ML) algorithms are being used to extract these features from CFGs and classify them as malicious or benign. In this survey, we aim to review some state-of-the-art methods for malware detection through CFGs using ML, focusing on the different ways of extracting, representing, and classifying. Specifically, we present a comprehe
    
[^135]: 在神经网络模型中集成最近邻居以估计治疗效果

    Integrating nearest neighbors on neural network models for treatment effect estimation. (arXiv:2305.06789v1 [stat.ML])

    [http://arxiv.org/abs/2305.06789](http://arxiv.org/abs/2305.06789)

    本论文提出了一种新的方法NNCI，用于将最近邻居信息集成到神经网络模型中，以更准确地估计治疗效果。

    

    治疗效果估计对于许多科学和工业领域的研究人员和从业者来说具有高度重要性。观察数据的丰富性使它们越来越受到研究人员用于因果效应的估计。然而，这些数据存在偏差和其他弱点，导致如果不正确处理，估计因果效应会不准确。因此，提出了几种机器学习技术，其中大部分都专注于利用神经网络模型的预测能力，以达到更精确的因果效应估计。在本文中，我们提出了一种名为最近邻居信息用于因果推断（NNCI）的新方法，用于将有价值的最近邻居信息集成到基于神经网络的模型中，以估计治疗效果。提出的NNCI方法被应用于一些最广泛使用的基于神经网络的治疗效果估计模型，其使用观察数据。

    Treatment effect estimation is of high-importance for both researchers and practitioners across many scientific and industrial domains. The abundance of observational data makes them increasingly used by researchers for the estimation of causal effects. However, these data suffer from biases, from several weaknesses, leading to inaccurate causal effect estimations, if not handled properly. Therefore, several machine learning techniques have been proposed, most of them focusing on leveraging the predictive power of neural network models to attain more precise estimation of causal effects. In this work, we propose a new methodology, named Nearest Neighboring Information for Causal Inference (NNCI), for integrating valuable nearest neighboring information on neural network-based models for estimating treatment effects. The proposed NNCI methodology is applied to some of the most well established neural network-based models for treatment effect estimation with the use of observational data
    
[^136]: 利用SAM的输入增强技术: 以分割基础模型为基础提升医学图像分割

    Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])

    [http://arxiv.org/abs/2304.11332](http://arxiv.org/abs/2304.11332)

    本文介绍如何使用大型的通用分割模型SAM来提升医学图像分割，展示了如何通过使用SAM生成的掩模、特征和稳定性分数来构建和训练更好的医学图像分割模型，并在两个数据集上进行了验证。

    

    Segment Anything Model (SAM)是一个最近发展的通用分割模型，用于计算机视觉任务. SAM使用了超过1亿个掩模的1100万图像进行训练，可以为自然场景图像中的广泛对象生成分割结果。本研究展示了如何利用这样一个大型基础模型来进行医学图像分割，尽管SAM并没有立即为医学图像提供高质量的分割，但其生成的掩模、特征和稳定性分数对于构建和训练更好的医学图像分割模型非常有用。特别地，我们演示了如何使用SAM来增强经典的医学图像分割模型（如U-Net）的图像输入。对两个数据集的实验表明了我们所提出的方法的有效性。

    The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
    
[^137]: SAMM（Segment Any Medical Model）：用于SAM的3D Slicer集成

    SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])

    [http://arxiv.org/abs/2304.05622](http://arxiv.org/abs/2304.05622)

    介绍了Segment Any Medical Model (SAMM)，它是用于3D Slicer的SAM的扩展。SAMM在医学图像分割上表现良好，在实时性和通用性方面都有很好的性能，可以推断出掩模。

    

    Segment Anything Model（SAM）是一个新的图像分割工具，使用迄今为止最大的分割数据集进行训练。该模型表明它可以创建高质量的图像分割掩模，具有良好的实时性和通用性。然而，在医学图像上的性能需要进一步验证。为了协助在医学图像上开发，评估和利用SAM，我们介绍了Segment Any Medical Model（SAMM），它是SAM在3D Slicer上的扩展。3D Slicer是一个广泛使用于医学影像处理和可视化软件的开源软件。这个开源扩展程序及其演示已发布在GitHub上（https://github.com/bingogome/samm）。SAMM在完整周期中实现了0.6秒的延迟，并可以实时推断出图像掩模。

    The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
    
[^138]: Shallow ReLU$^k$神经网络的逼近速率及其在非参数回归中的应用

    Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])

    [http://arxiv.org/abs/2304.01561](http://arxiv.org/abs/2304.01561)

    本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。

    

    本研究探讨与Shallow ReLU$^k$神经网络相关的变异空间的逼近容量。结果表明，在有限变异范数下，容纳了足够平滑的函数。对于较少平滑的函数，根据变异范数建立了逼近速率。运用这些结果，我们可以证明Shallow ReLU$^k$神经网络的最优逼近速率。同时阐明了这些结果如何用于推导深层神经网络和卷积神经网络(CNNs)的逼近界限。为应用研究，我们使用了三种ReLU神经网络模型：浅层神经网络，超参数神经网络和CNN进行非参数回归收敛速率研究。特别地，我们展示了浅层神经网络可以实现学习H\"older函数的最优渐进速率，这补充了深层神经网络的最近结果。

    We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
    
[^139]: 探索视觉-语言模型在不平衡学习中的应用

    Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])

    [http://arxiv.org/abs/2304.01457](http://arxiv.org/abs/2304.01457)

    本文探索了如何通过向视觉-语言模型添加轻量级解码器和利用不平衡算法来改进性能，实验表明改进后的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上分类准确度显著提高，特别是对于少数类，性能提升很大。

    

    使用对比语言-图像预训练的视觉-语言模型（VLMs）已经显示出有希望的零样本分类表现。然而，在不平衡数据集上，它们的性能相对较差，在训练数据集中类的分布倾斜，导致在预测少数类方面性能不佳。我们提出向VLM添加轻量级解码器，以避免由于大量类别导致的内存不足问题，并捕捉尾部类别的微妙特征。然后，我们探索了利用提示调整、微调以及加入不平衡算法（例如Focal Loss、Balanced SoftMax和Distribution Alignment）来改进VLM。实验表明，在使用解码器和不平衡方法时，VLM的性能可以进一步提高。具体而言，我们改进的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上的分类准确度平均提高了6.58%、69.82%和10.43%。

    Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
    
[^140]: 可视化思维传递模型

    Visual Chain-of-Thought Diffusion Models. (arXiv:2303.16187v1 [cs.CV])

    [http://arxiv.org/abs/2303.16187](http://arxiv.org/abs/2303.16187)

    本文提出了一种两阶段采样过程，使用可视化思维传递模型来缩小条件和无条件模型之间的差距，相对标准无条件生成，FID提高25-50%。

    

    条件图像扩散模型的近期进展是惊人的，这适用于以文本描述、场景布局或素描为条件的模型。无条件图像扩散模型也在改进，但落后于条件模型，以类标签为条件的扩散模型亦是如此。本文提出使用两阶段采样过程，缩小条件和无条件模型之间的差距。在第一阶段中，我们采样描述图像语义内容的嵌入。在第二阶段中，我们在这个嵌入的条件下采样图像，然后丢弃这个嵌入。这样做让我们利用条件扩散模型的强大功能来进行无条件生成任务，我们证明相对于标准无条件生成，FID（Frechet inception distance）最多提高了25-50%。

    Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.
    
[^141]: 在领域泛化中找到能力区域

    Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])

    [http://arxiv.org/abs/2303.09989](http://arxiv.org/abs/2303.09989)

    该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。

    

    我们提出了一个“学习拒绝”框架来解决领域泛化中默默失败的问题，即测试分布与训练分布不同的情况。假设有一个温和的分布偏移，我们希望在模型估计的能力预示着可信响应时接受超出分布的数据，而不是直接拒绝超出分布的数据。可信度通过与分类器性能密切相关的代理无能分数进行预测。我们对分类的无能得分进行了全面的实验评估，并强调了拒绝率与准确率之间的权衡。为了与先前的工作进行比较，我们聚焦于标准领域泛化基准，并考虑在闭合和开放世界环境下通过不同的学习表示来衡量无能。我们的结果表明，增加无能分数确实预示着降低准确性，从而导致显着的...

    We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
    
[^142]: 对比层次聚类

    Contrastive Hierarchical Clustering. (arXiv:2303.03389v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03389](http://arxiv.org/abs/2303.03389)

    本文提出了 CoHiClust 模型，一种基于深度神经网络的对比层次聚类模型，通过自监督学习方法，生成与我们直觉和图像语义相符的合理聚类结构，且在大部分图像数据集上的聚类准确性超过了最先进的平面聚类模型。

    

    深度聚类一直被平面模型所占主导地位，这些模型将数据集分成预定义数量的组。尽管最近的方法在流行的基准测试中与基本事实的相似度非常高，但平面分区提供的信息有限。本文介绍了一种基于深度神经网络的对比层次聚类模型 CoHiClust，可应用于典型图像数据。通过采用自监督学习方法，CoHiClust 可以将基础网络蒸馏为二叉树，而不需要访问任何标记的数据。层次聚类结构可以用于分析聚类之间的关系以及衡量数据点之间的相似度。实验证明，CoHiClust 生成了一个合理的聚类结构，符合我们的直觉和图像语义。此外，它与最先进的平面聚类相比，在大多数图像数据集上获得了更高的聚类准确性。

    Deep clustering has been dominated by flat models, which split a dataset into a predefined number of groups. Although recent methods achieve an extremely high similarity with the ground truth on popular benchmarks, the information contained in the flat partition is limited. In this paper, we introduce CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. By employing a self-supervised learning approach, CoHiClust distills the base network into a binary tree without access to any labeled data. The hierarchical clustering structure can be used to analyze the relationship between clusters, as well as to measure the similarity between data points. Experiments demonstrate that CoHiClust generates a reasonable structure of clusters, which is consistent with our intuition and image semantics. Moreover, it obtains superior clustering accuracy on most of the image datasets compared to the state-of-the-art flat clusterin
    
[^143]: 梯度范数感知最小化在寻找一阶平坦度中改进了广义化能力

    Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03108](http://arxiv.org/abs/2303.03108)

    研究提出了一阶平坦度的概念，使用梯度范数感知最小化算法寻找在所有方向上具有均匀小曲率的极小值，提高了广义化能力和测试损失。

    

    最近，已经证明了平坦的极小值有效地提高了泛化能力，而锐度感知最小化 (SAM) 实现了最先进的性能。然而，SAM 及其后续讨论中当前关于平坦性的定义仅限于零阶平坦性 (即扰动半径内最坏损失)。我们表明，当存在单一最小值或给定扰动半径内的多个最小值时，零阶平坦度可能不足以区分具有低泛化误差和高泛化误差的极小值。因此，我们提出了一阶平坦度，这是一种更强的平坦度测量，重点关注扰动半径内的最大梯度范数，其限制了局部极小值的 Hessian 的最大特征值和 SAM 的正则化函数。我们还提出了一种名为 Gradient norm Aware Minimization (GAM) 的新型训练过程，以寻找所有方向上曲率均匀小的最小值。实验结果表明，GAM 显着改进了广义化能力和测试损失

    Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results s
    
[^144]: CHGNet：用于带电原子建模的预训练通用神经网络势

    CHGNet: Pretrained universal neural network potential for charge-informed atomistic modeling. (arXiv:2302.14231v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2302.14231](http://arxiv.org/abs/2302.14231)

    CHGNet是一种预训练的通用神经网络势，能够准确预测不同化学组成和晶体结构的各种材料的属性，并且将电荷信息并入原子表示中，能够用于描述带电系统中的复杂电子相互作用。

    

    复杂的电子相互作用大规模系统的模拟仍然是材料原子建模中最大的挑战之一。尽管经典力场通常无法描述电子状态和离子重排之间的耦合，但更准确的从头分子动力学由于计算复杂性而无法进行长时间和大规模的模拟，这对于研究许多技术相关现象非常重要，例如反应、离子迁移、相变和降解。在本文中，我们提出了Crystal Hamiltonian Graph神经网络（CHGNet）作为一种新的机器学习相互作用势（MLIP），使用基于图神经网络的力场来建模通用的势能面。 CHGNet以材料项目轨迹数据集中的能量、力、应力和磁矩为先验进行预训练，该数据集由超过10年的密度泛函理论静态和松弛计算组成，涵盖了超过86,000种材料，并因此能够准确预测不同化学组成和晶体结构的各种材料的属性。重要的是，CHGNet还将电荷信息并入原子表示中，使得描述带电系统中的复杂电子相互作用成为可能。我们通过将其应用于几个模型系统的相稳定性、表面松弛和表面分离的预测，并将其精度和效率与其他MLIP进行比较，展示了CHGNet的多功能性。我们的结果表明，CHGNet是用于带电原子建模的有希望的工具，可以加速新材料的设计和发现。

    The simulation of large-scale systems with complex electron interactions remains one of the greatest challenges for the atomistic modeling of materials. Although classical force fields often fail to describe the coupling between electronic states and ionic rearrangements, the more accurate \textit{ab-initio} molecular dynamics suffers from computational complexity that prevents long-time and large-scale simulations, which are essential to study many technologically relevant phenomena, such as reactions, ion migrations, phase transformations, and degradation.  In this work, we present the Crystal Hamiltonian Graph neural Network (CHGNet) as a novel machine-learning interatomic potential (MLIP), using a graph-neural-network-based force field to model a universal potential energy surface. CHGNet is pretrained on the energies, forces, stresses, and magnetic moments from the Materials Project Trajectory Dataset, which consists of over 10 years of density functional theory static and relaxat
    
[^145]: 对齐扩散薛定谔桥

    Aligned Diffusion Schr\"odinger Bridges. (arXiv:2302.11419v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11419](http://arxiv.org/abs/2302.11419)

    本文提出一种新的算法框架，首次能够在考虑对齐数据的同时解决扩散薛定谔桥问题，相对于之前的方法，有更简单、方差更低的训练过程，并使用原则性的正则化方案，在实验中取得了显着的改进。

    

    最近，通过在不同时间点的边际观察恢复随机动态的扩散薛定谔桥(DSB)已经成为一个强大的框架。尽管有许多成功的应用，但现有算法尚未利用多种生物现象中自然出现的对齐数据结构来解决DSB。在本文中，我们提出了一个新的算法框架，首次在考虑对齐数据的同时解决了DSB问题。我们的方法依靠两个二十年历史的思想结合起来：经典的薛定谔桥理论和Doob的$h$-变换。相对于之前的方法，我们的方法导致训练过程更简单，方差更低，并且我们还使用了原则性的正则化方案。这最终在合成和真实数据的实验中导致了显着的改进，包括刚性蛋白质对接和细胞分化过程的时间演化等任务。

    Diffusion Schr\"odinger bridges (DSB) have recently emerged as a powerful framework for recovering stochastic dynamics via their marginal observations at different time points. Despite numerous successful applications, existing algorithms for solving DSBs have so far failed to utilize the structure of aligned data, which naturally arises in many biological phenomena. In this paper, we propose a novel algorithmic framework that, for the first time, solves DSBs while respecting the data alignment. Our approach hinges on a combination of two decades-old ideas: The classical Schr\"odinger bridge theory and Doob's $h$-transform. Compared to prior methods, our approach leads to a simpler training procedure with lower variance, which we further augment with principled regularization schemes. This ultimately leads to sizeable improvements across experiments on synthetic and real data, including the tasks of rigid protein docking and temporal evolution of cellular differentiation processes.
    
[^146]: 大型语言模型的PAC预测集

    PAC Prediction Sets for Large Language Models of Code. (arXiv:2302.08703v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08703](http://arxiv.org/abs/2302.08703)

    本文针对标签空间大小呈指数增长的结构化预测问题，提出了一种考虑到受限制的预测集的解决方案，可以紧凑地表示为部分程序，并利用了编程语言的抽象语法树，使得正确的程序以高置信度出现在集合中。应用方面包括Codex风格的代码生成器。

    

    预测集最近被证明是量化深度神经网络不确定性的一种有前途的策略，同时也提供了理论保证。然而，现有的技术大多针对标签空间简单的设置，因此预测集可以是任意子集。对于标签空间大小呈指数增长的结构化预测问题，即使包含所有标签的预测集也可能呈指数级增长。在代码生成的背景下，我们提出了一种解决方案，考虑到一组受限制的预测集，可以紧凑地表示为部分程序，这些部分程序中的部分已经被替换为占位符。鉴于训练好的代码生成模型，我们的算法利用了编程语言的抽象语法树来生成一组程序，以便正确的程序以高置信度出现在集合中。我们的算法有很多有价值的应用场景，包括Codex风格的代码生成器。

    Prediction sets have recently been shown to be a promising strategy for quantifying the uncertainty of deep neural networks in a way that provides theoretical guarantees. However, existing techniques have largely targeted settings where the space of labels is simple, so prediction sets can be arbitrary subsets of labels. For structured prediction problems where the space of labels is exponential in size, even prediction sets containing a small fraction of all labels can be exponentially large. In the context of code generation, we propose a solution that considers a restricted set of prediction sets that can compactly be represented as partial programs, which are programs with portions replaced with holes. Given a trained code generation model, our algorithm leverages a programming language's abstract syntax tree to generate a set of programs such that the correct program is in the set with high-confidence. Valuable applications of our algorithm include a Codex-style code generator wit
    
[^147]: 离线安全强化学习中的约束决策Transformer

    Constrained Decision Transformer for Offline Safe Reinforcement Learning. (arXiv:2302.07351v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07351](http://arxiv.org/abs/2302.07351)

    本文研究了离线安全强化学习问题，提出了约束决策Transformer方法。该方法可以在部署过程中动态调整权衡，具有学习自适应、安全、鲁棒且高报酬的优势表现。

    

    安全强化学习通过与环境交互训练约束满足策略。本文致力于解决更具挑战性的问题：从离线数据集中学习安全策略。我们从多目标优化的角度研究了离线安全强化学习问题，并提出了可ε-可减概念来表征问题难度。安全和任务绩效之间的固有权衡激发了我们提出约束决策Transformer（CDT）方法，该方法可以在部署过程中动态调整权衡。大量实验证明了所提出的方法在学习自适应、安全、鲁棒且高报酬的策略方面的优势。与所有任务相同的超参数下，CDT在保持零-shot适应能力的同时，以大幅超过其变体和强离线安全RL基线的性能表现，使我们的方法更适用于约束条件下的现实RL。

    Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the $\epsilon$-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy. CDT outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, while keeping the zero-shot adaptation capability to different constraint thresholds, making our approach more suitable for real-world RL under constraints. The code
    
[^148]: PATCorrect：基于音素增强的非自回归变换器用于ASR错误修正

    PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction. (arXiv:2302.05040v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.05040](http://arxiv.org/abs/2302.05040)

    PATCorrect是一种基于音素增强的非自回归变换器，利用文本和音素模态的表示来最大限度地降低ASR系统中的单词错误率，并在低延迟需求的实际生产系统中表现出鲁棒性。

    

    自动语音识别(ASR)系统产生的语音到文本错误会对下游模型造成负面影响。最近，已经开发了错误修正模型作为后处理文本编辑方法，以优化ASR输出。然而，目前尚未对符合工业级生产系统低延迟需求的高效模型进行充分研究。我们提出了PATCorrect这一新型非自回归(NAR)方法，基于多模态融合，利用文本和音素模态的表示，以降低单词错误率(WER)，并在不同质量的输入转录情况下表现出鲁棒性。我们证明PATCorrect在英语语料上表现优于最先进的NAR方法，与仅使用文本模态的方法相比，总体WER降低了11.62%，而其他方法的WER降低率为9.46%。此外，其推理延迟在毫秒级别，非常适合需要低延迟系统的使用。

    Speech-to-text errors made by automatic speech recognition (ASR) systems negatively impact downstream models. Error correction models as a post-processing text editing method have been recently developed for refining the ASR outputs. However, efficient models that meet the low latency requirements of industrial grade production systems have not been well studied. We propose PATCorrect-a novel non-autoregressive (NAR) approach based on multi-modal fusion leveraging representations from both text and phoneme modalities, to reduce word error rate (WER) and perform robustly with varying input transcription quality. We demonstrate that PATCorrect consistently outperforms state-of-the-art NAR method on English corpus across different upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to 9.46% WERR achieved by other methods using text only modality. Besides, its inference latency is at tens of milliseconds, making it ideal for systems with low latency requirements.
    
[^149]: 机器学习中算法集体行动的研究

    Algorithmic Collective Action in Machine Learning. (arXiv:2302.04262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04262](http://arxiv.org/abs/2302.04262)

    本文研究了机器学习中的算法集体行动的理论模型，并在大量实验中验证了该算法可以大大提高分类准确性，特别是在数据结构复杂和集体规模大的情况下。

    

    我们对在采用机器学习算法的数字平台上进行算法集体行动进行了原则性研究。我们提出了一个简单的理论模型，描述了一群人与公司的学习算法进行交互的情况。集体汇聚参与个体的数据并通过一种算法策略指导参与者修改自己的数据以实现集体目标。我们在三种基本的学习理论设置下研究了这种模型的结果：非参数最优学习算法，参数风险最小化和梯度下降优化。在每个设置中，我们提出了协调的算法策略，并根据集体规模的大小来表征自然的成功标准。为了补充我们的理论，我们对涉及数以万计自由职业平台简历的技能分类任务进行了系统实验。通过 BERT 模型的两千多次训练运行，我们证明了我们的算法集体行动可以显著提高分类准确性，比集中式学习算法和独立修改数据的非协调方法要好得多。我们的实验表明，算法集体行动的有效性至关重要的依赖于集体的规模和数据的基本结构。

    We initiate a principled study of algorithmic collective action on digital platforms that deploy machine learning algorithms. We propose a simple theoretical model of a collective interacting with a firm's learning algorithm. The collective pools the data of participating individuals and executes an algorithmic strategy by instructing participants how to modify their own data to achieve a collective goal. We investigate the consequences of this model in three fundamental learning-theoretic settings: the case of a nonparametric optimal learning algorithm, a parametric risk minimizer, and gradient-based optimization. In each setting, we come up with coordinated algorithmic strategies and characterize natural success criteria as a function of the collective's size. Complementing our theory, we conduct systematic experiments on a skill classification task involving tens of thousands of resumes from a gig platform for freelancers. Through more than two thousand model training runs of a BERT
    
[^150]: 高效图场积分器在点云中的应用

    Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00942](http://arxiv.org/abs/2302.00942)

    本文提出了两种算法用于非欧几里得空间中的高效图场积分，适用于点云网格图或ε-最近邻图的表征方法，具有很强的实用性。

    

    文章提出了两种新的算法类别，用于对编码点云的图形进行高效场积分。第一类算法使用点云网格图的有界亏格，第二类算法则使用点云的流行的ε-最近邻图表示方法。两种算法都可以被看作 Fast Multipole Methods(FMMs) 的可行替代，但适用于非欧几里得空间。文章重点研究基于点之间步长分布（如最短路径距离）所引发的几何学。通过提供详细的理论分析，文章获得了结构图论的新结果。文章还进行了全面的实证评估，包括对刚性和可变形物体的表面插值（特别是用于网格动态建模），点云的Wasserstein距离计算以及Gromov-Wasserstein变体等。

    We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
    
[^151]: 深度随机网络的 Bayes 最优学习

    Bayes-optimal Learning of Deep Random Networks of Extensive-width. (arXiv:2302.00375v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00375](http://arxiv.org/abs/2302.00375)

    本文研究了深度随机网络的学习问题，提出了 Bayes 最优测试误差的闭式表达式。岭回归和核回归能够达到最优表现，而神经网络的测试误差也可以从平方级的样本数量中获得接近于零的结果。

    

    本文考虑学习一个深度广度非线性神经网络的目标函数，其具有随机高斯权重。我们研究了样本数量、输入维数和网络宽度成比例增加时的渐近情况，并为回归和分类任务提出了 Bayes 最优测试误差的闭式表达式。此外，我们还计算了岭回归、核函数和随机特征回归的测试误差的闭式表达式。我们发现，最优正则化的岭回归以及核回归可以达到 Bayes 最优表现，而逻辑损失函数对于分类问题几乎能达到最优的测试误差。我们通过数字实验证明，当样本数量增长速度快于维数时，岭回归和核方法变得次优，而神经网络可以从平方级的样本数量中获得接近于零的测试误差。

    We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random Gaussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large. We propose a closed-form expression for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test error close to zero from quadratically many samples.
    
[^152]: 等变差分隐私深度学习：DP-SGD为什么需要更稀疏的模型

    Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models. (arXiv:2301.13104v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13104](http://arxiv.org/abs/2301.13104)

    本文提出等变差分隐私深度学习，利用数据中的群体对称性来实现更稀疏的模型，并保持隐私保证，从而解决了DP-SGD在更具挑战性的任务上所面临网络规模困难的问题。

    

    差分隐私随机梯度下降（DP-SGD）通过修剪和添加噪声来限制深度学习模型在训练过程中可以记忆的私有信息量。因此，具有更多参数的网络需要相应更强的扰动，导致大型模型难以学习有用信息，使得在更具挑战性的训练任务上使用DP-SGD变得非常困难。最近的研究集中在通过训练适应性技术，如大规模数据增强和大批量大小，来克服这一挑战。但是，这些技术进一步增加了DP-SGD的计算开销，并降低了其实际适用性。在本文中，我们提出了使用稀疏模型设计原则来解决这些复杂任务，具有更少的参数、更高的准确性和更短的时间，从而成为DP-SGD的一种有前途的方向。我们通过引入等变差分隐私深度学习来实现这种稀疏性设计，利用数据中的群体对称性来获得更稀疏的模型同时保持隐私保证。

    Differentially Private Stochastic Gradient Descent (DP-SGD) limits the amount of private information deep learning models can memorize during training. This is achieved by clipping and adding noise to the model's gradients, and thus networks with more parameters require proportionally stronger perturbation. As a result, large models have difficulties learning useful information, rendering training with DP-SGD exceedingly difficult on more challenging training tasks. Recent research has focused on combating this challenge through training adaptations such as heavy data augmentation and large batch sizes. However, these techniques further increase the computational overhead of DP-SGD and reduce its practical applicability. In this work, we propose using the principle of sparse model design to solve precisely such complex tasks with fewer parameters, higher accuracy, and in less time, thus serving as a promising direction for DP-SGD. We achieve such sparsity by design by introducing equiv
    
[^153]: MPNN与图转换器之间的连接

    On the Connection Between MPNN and Graph Transformer. (arXiv:2301.11956v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11956](http://arxiv.org/abs/2301.11956)

    本文研究了MPNN与图转换器之间的连接，并证明了带有虚拟节点的MPNN可以任意逼近GT的自注意力层。我们的工作阐明了两种图学习范式之间的关系和能力平衡。

    

    最近出现了一种新的图学习算法范例——图转换器（GT），在多个基准测试中表现优于之前流行的消息传递神经网络（MPNN）。以前的工作表明，通过适当的位置嵌入，GT可以任意逼近MPNN，这意味着GT至少与MPNN一样强大。本文研究了反向连接，并展示了带有虚拟节点（VN）的MPNN足够强大，可以任意逼近GT的自注意力层。特别地，我们首先展示了如果考虑一种线性变换器——所谓的表现者/线性变换器，则具有O（1）深度和O（1）宽度的MPNN + VN可以逼近表现者/线性变换器的自我注意层。接下来，通过MPNN + VN与DeepSets之间的联系，我们证明了具有O(n^d)宽度和O(1)深度的MPNN + VN可以逼近GT的任何层，其中n是图中的节点数，d是图的直径。我们的工作阐明了两种图学习范式之间的关系和能力平衡。

    Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT.  In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n^d) width and O(1)
    
[^154]: PLay：使用潜在扩散生成参数条件化的布局设计

    PLay: Parametrically Conditioned Layout Generation using Latent Diffusion. (arXiv:2301.11529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11529](http://arxiv.org/abs/2301.11529)

    本文提出了一种有条件的潜在扩散模型 PLay，能够根据用户指定的指南在矢量图空间中生成参数化条件布局。相比以前的工作，在三个数据集和用户研究中表现更好，并为专业布局设计带来了新颖和互动的体验。

    

    布局设计是各种设计领域（包括用户界面、文档和图形设计）中的重要任务。由于此任务需要设计师付出繁琐的手动努力，先前的工作尝试使用生成模型自动化此过程，但通常无法提供直观的用户控制和实现设计目标。在本文中，我们构建了一个有条件的潜在扩散模型 PLay，它从用户指定的指南中生成矢量图空间中的参数化条件布局。这些指南通常由设计师在当前实践中用于表示他们的设计意图。我们的方法在包括 FID 和 FD-VG 的三个数据集和用户研究中优于以前的工作。此外，它为专业布局设计流程带来了一种新颖和互动的体验。

    Layout design is an important task in various design fields, including user interface, document, and graphic design. As this task requires tedious manual effort by designers, prior works have attempted to automate this process using generative models, but commonly fell short of providing intuitive user controls and achieving design objectives. In this paper, we build a conditional latent diffusion model, PLay, that generates parametrically conditioned layouts in vector graphic space from user-specified guidelines, which are commonly used by designers for representing their design intents in current practices. Our method outperforms prior works across three datasets on metrics including FID and FD-VG, and in user study. Moreover, it brings a novel and interactive experience to professional layout design processes.
    
[^155]: 回报恩惠：回归如何从概率因果知识中受益

    Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge. (arXiv:2301.11214v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11214](http://arxiv.org/abs/2301.11214)

    本文研究了如何将有向无环图中的概率因果知识应用于回归问题，提出了一种碰撞回归框架，并证明了其在假设空间为再生核希尔伯特空间时具有严格正的广义收益，在合成和气候模型数据上的实验结果表明其可以提高预测性能。

    

    有向无环图(DAG)提供了有价值的先验知识，但在机器学习中回归任务中经常被忽略。本文表明，在DAG中由碰撞结构引起的独立性提供了有意义的归纳偏差，这可以限制回归假设空间并提高预测性能。我们介绍了碰撞回归，一种将一个碰撞中的概率因果知识纳入回归问题的框架。当假设空间为再生核希尔伯特空间时，我们证明在温和的假设下，具有严格正的广义收益，并提供了经验风险最小化的闭合形式估计量。在合成和气候模型数据上的实验表明，所提出的技术可以提高预测性能。

    A directed acyclic graph (DAG) provides valuable prior knowledge that is often discarded in regression tasks in machine learning. We show that the independences arising from the presence of collider structures in DAGs provide meaningful inductive biases, which constrain the regression hypothesis space and improve predictive performance. We introduce collider regression, a framework to incorporate probabilistic causal knowledge from a collider in a regression problem. When the hypothesis space is a reproducing kernel Hilbert space, we prove a strictly positive generalisation benefit under mild assumptions and provide closed-form estimators of the empirical risk minimiser. Experiments on synthetic and climate model data demonstrate performance gains of the proposed methodology.
    
[^156]: 基于邻居同质性的图卷积网络

    Neighborhood Homophily-based Graph Convolutional Network. (arXiv:2301.09851v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09851](http://arxiv.org/abs/2301.09851)

    本文提出了一种基于邻居同质性的图卷积网络 (NHGCN) 模型，利用新指标 Neighborhood Homophily (NH) 测量节点邻域中的标签复杂度或纯度。实验证明 NHGCN 模型在节点分类和图分类任务中表现优异并显著超过最先进模型。

    

    图神经网络在图形任务中表现出强大的能力。然而，许多真实世界的图形是异构的，这挑战了经典图神经网络的同质性假设。为了解决这个普适性问题，许多研究加深网络或连接中间表示，但这并不会本质上改变邻居聚合并引入噪声。最近的研究提出了新的指标来表征同质性，但很少考虑所提出指标和模型之间的相关性。本文首先设计了一种新的指标 Neighborhood Homophily (NH)，用于测量节点邻域中的标签复杂度或纯度。此外，我们将该指标融入到经典的图卷积网络 (GCN) 结构中，提出了 NHGCN 模型。在该框架中，邻居被预估的 NH 值分组，从不同通道进行聚合。多项基准数据集上的实验证明，NHGCN 在节点分类和图分类任务中始终优于最先进的模型。

    Graph neural networks (GNNs) have been proved powerful in graph-oriented tasks. However, many real-world graphs are heterophilous, challenging the homophily assumption of classical GNNs. To solve the universality problem, many studies deepen networks or concatenate intermediate representations, which does not inherently change neighbor aggregation and introduces noise. Recent studies propose new metrics to characterize the homophily, but rarely consider the correlation of the proposed metrics and models. In this paper, we first design a new metric, Neighborhood Homophily (\textit{NH}), to measure the label complexity or purity in node neighborhoods. Furthermore, we incorporate the metric into the classical graph convolutional network (GCN) architecture and propose \textbf{N}eighborhood \textbf{H}omophily-based \textbf{G}raph \textbf{C}onvolutional \textbf{N}etwork (\textbf{NHGCN}). In this framework, neighbors are grouped by estimated \textit{NH} values and aggregated from different ch
    
[^157]: Peekaboo：文本到图像扩散模型是零-shot细分器

    Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors. (arXiv:2211.13224v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13224](http://arxiv.org/abs/2211.13224)

    本文介绍了一种名为Peekaboo的技术，可以用于使用现成的文本到图像扩散模型进行无监督语义细分并基础，而无需任何重新培训。这项技术的推理时间优化过程可以在与自然语言提示相关联的情况下生成分割掩模。

    

    最近，文本到图像扩散模型展示了在从自然语言提示中创建逼真图像方面的显着能力。然而，鲜有研究探讨如何利用这些模型进行语义定位或基础。在这项工作中，我们探讨了一个现成的文本到图像扩散模型，它没有接触到本地化信息的训练如何在无需细分特定重新训练的情况下，以自然语言提示为条件建立各种语义短语。我们引入了一个推理时间优化过程，能够在与自然语言提示相关联的情况下生成分割掩模。我们的提议——Peekaboo，是一种首款无训练开放词汇无监督语义接地技术，利用扩散模型。我们在Pascal VOC数据集上对Peekaboo进行了评估，用于无监督语义细分，以及在RefCOCO数据集上对其进行了评估，用于引用细分，显示具有有希望的结果的竞争优势。我们还展示了如何使用Peekaboo通过条件自然语言提示来生成带透明背景的图像。

    Recently, text-to-image diffusion models have shown remarkable capabilities in creating realistic images from natural language prompts. However, few works have explored using these models for semantic localization or grounding. In this work, we explore how an off-the-shelf text-to-image diffusion model, trained without exposure to localization information, can ground various semantic phrases without segmentation-specific re-training. We introduce an inference time optimization process capable of generating segmentation masks conditioned on natural language prompts. Our proposal, Peekaboo, is a first-of-its-kind zero-shot, open-vocabulary, unsupervised semantic grounding technique leveraging diffusion models without any training. We evaluate Peekaboo on the Pascal VOC dataset for unsupervised semantic segmentation and the RefCOCO dataset for referring segmentation, showing results competitive with promising results. We also demonstrate how Peekaboo can be used to generate images with tr
    
[^158]: 基于物理启发式神经网络的Landau阻尼数据驱动建模

    Data-Driven Modeling of Landau Damping by Physics-Informed Neural Networks. (arXiv:2211.01021v2 [physics.plasm-ph] UPDATED)

    [http://arxiv.org/abs/2211.01021](http://arxiv.org/abs/2211.01021)

    本研究提出了一种基于物理启发式神经网络的数据驱动方法，成功构建了一个包含隐式流体闭合项的多矩便流体模型，能够重现Landau阻尼的动力学特征，并且引入了一个表征流体-细结构相互作用的新变量，该方法有效地将流体模型的应用范围扩展到多尺度系统，为研究等离子体动力学提供了强大工具。

    

    在微观等离子体物理问题中，动力学方法一般很准确，但对于大尺度或多尺度系统而言计算开销较大。等离子体物理学中长期存在的问题之一是如何将动力学物理学整合到流体模型中，通常通过复杂的解析闭合项实现。本研究成功构建了一个多矩便流体模型，其中神经网络包含隐式流体闭合项，采用机器学习的物理引导神经网络（PINN）和梯度增强的物理引导神经网络（gPINN）对少量稀疏采样的动力学仿真数据进行训练。使用PINN或gPINN构建的多矩便流体模型能够重现Landau阻尼的电场能量时变演化，包括其阻尼率，以及动力学仿真中的等离子体动力学。我们首次引入了一个新变量，即多矩便雷诺应力张量，该变量表征流体-细结构相互作用，是理解等离子体多尺度效应的关键。我们的方法提供了一种有效的数据驱动方法，将流体模型的有效性扩展到多尺度系统，为研究等离子体动力学提供了显著优势。

    Kinetic approaches are generally accurate in dealing with microscale plasma physics problems but are computationally expensive for large-scale or multiscale systems. One of the long-standing problems in plasma physics is the integration of kinetic physics into fluid models, which is often achieved through sophisticated analytical closure terms. In this study, we successfully construct a multi-moment fluid model with an implicit fluid closure included in the neural network using machine learning. The multi-moment fluid model is trained with a small fraction of sparsely sampled data from kinetic simulations of Landau damping, using the physics-informed neural network (PINN) and the gradient-enhanced physics-informed neural network (gPINN). The multi-moment fluid model constructed using either PINN or gPINN reproduces the time evolution of the electric field energy, including its damping rate, and the plasma dynamics from the kinetic simulations. For the first time, we introduce a new var
    
[^159]: 回到源头: 扩散驱动的测试时自适应

    Back to the Source: Diffusion-Driven Test-Time Adaptation. (arXiv:2207.03442v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03442](http://arxiv.org/abs/2207.03442)

    本文提出了基于扩散驱动的测试时自适应方法 DDA，通过将测试输入投影到生成扩散模型的源域中来更新目标数据，相比以往的模型适应方法更为稳健，适用于各种损坏、架构和数据情况。

    

    测试时自适应利用测试输入来提高源数据训练的模型在经过移动的目标数据时的准确性。现有的方法通过在每个目标域中重复训练来更新源模型。虽然有效，但重新训练对于数据量和顺序以及优化的超参数非常敏感。相反，我们通过将所有测试输入投影到具有生成扩散模型的源域中来更新目标数据。我们的扩散驱动适应方法 DDA 在所有域中共享其用于分类和生成的模型。这两个模型在源域中训练，然后在测试期间固定。我们通过图像引导和自学习来增强扩散，以自动决定适应程度。相比以前的模型适应方法，DDA 的输入适应对于 ImageNet-C 数据集中各种损坏、架构和数据情况更为稳健。在输入逐个更新的情况下，DDA 成功地克服了模型适应所面临的问题。

    Test-time adaptation harnesses test inputs to improve the accuracy of a model trained on source data when tested on shifted target data. Existing methods update the source model by (re-)training on each target domain. While effective, re-training is sensitive to the amount and order of the data and the hyperparameters for optimization. We instead update the target data, by projecting all test inputs toward the source domain with a generative diffusion model. Our diffusion-driven adaptation method, DDA, shares its models for classification and generation across all domains. Both models are trained on the source domain, then fixed during testing. We augment diffusion with image guidance and self-ensembling to automatically decide how much to adapt. Input adaptation by DDA is more robust than prior model adaptation approaches across a variety of corruptions, architectures, and data regimes on the ImageNet-C benchmark. With its input-wise updates, DDA succeeds where model adaptation degrad
    
[^160]: 关于解决鞍点问题的缩放方法论文研究

    On Scaled Methods for Saddle Point Problems. (arXiv:2206.08303v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08303](http://arxiv.org/abs/2206.08303)

    本文对解决鞍点问题的缩放方法进行了理论分析，并通过实验研究发现这些方法在解决GAN训练问题中具有较好的适用性。

    

    具有不同特征的自适应缩放方法在解决鞍点问题方面发挥着关键作用，主要是由于Adam在解决对抗性机器学习问题，包括GAN训练方面的流行。本文对以下解决SPP的缩放技术进行了理论分析：众所周知的Adam和RmsProp缩放以及基于Hutchison近似公式的较新的AdaHessian和OASIS。我们使用Extra Gradient及其带有负动量的改进版本作为基本方法。GAN的实验研究表明，不仅Adam，而且其他不太流行的方法也具有良好的适用性。

    Methods with adaptive scaling of different features play a key role in solving saddle point problems, primarily due to Adam's popularity for solving adversarial machine learning problems, including GANS training. This paper carries out a theoretical analysis of the following scaling techniques for solving SPPs: the well-known Adam and RmsProp scaling and the newer AdaHessian and OASIS based on Hutchison approximation. We use the Extra Gradient and its improved version with negative momentum as the basic method. Experimental studies on GANs show good applicability not only for Adam, but also for other less popular methods.
    
[^161]: 分布式环境下基础模型的去中心化训练

    Decentralized Training of Foundation Models in Heterogeneous Environments. (arXiv:2206.01288v4 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2206.01288](http://arxiv.org/abs/2206.01288)

    本文研究了在分布式异构环境下进行模型并行训练大型基础模型的技术，提出了一种可以利用更多分布式、异构和低带宽互联计算资源的训练方案。

    

    训练基础模型（如GPT-3和PaLM）可能非常昂贵，往往需要数万个GPU连续运行数月。这些模型通常在专用集群中进行训练，这些集群拥有快速的同构互联和使用精心设计的软件系统，支持数据并行和模型/管道并行。这样的专用集群成本高昂且难以获得。我们能否利用分布式、异构和带宽较低的计算资源呢？以往研究异构、分布式场景主要关注小型模型，可以以纯数据并行方式进行训练。用于模型并行基础模型训练的最先进方案（如Megatron）仅涉及同构数据中心环境。在本文中，我们首次提出了在分布式异构环境中使用模型并行进行大型基础模型的训练的研究。

    Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous ne
    
[^162]: 基于脑启发计算的高效离线强化学习

    Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing. (arXiv:2205.06978v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.06978](http://arxiv.org/abs/2205.06978)

    本文提出了一种QHD离线策略，它基于超维强化学习与脑启发计算，实现稳健和实时学习。QHD相比于DQN具有更高效率且适用于高效的强化学习，具有在线和实时学习潜力。

    

    强化学习（RL）已经开创了增强现有智能系统的新机会，这些系统通常包括复杂的决策过程。然而，现代RL算法，如Deep Q-Networks（DQN），基于深度神经网络，导致计算成本高昂。本文提出了QHD，一种基于超维强化学习的离线策略，模仿大脑的属性，实现稳健和实时学习。QHD依靠轻量级的脑启发模型，在未知环境中学习最佳策略。在桌面和功率限制的嵌入式平台上，QHD的整体效率显著优于DQN，同时提供更高或可比较的回报。QHD也适用于高度有效的强化学习，具有极大的在线和实时学习潜力。我们的解决方案支持小的经验重放批量，与DQN相比提供12.3倍的加速，并确保最小的质量损失。我们的评估显示，QHD与先进的RL算法相比，具有更少的计算成本和更高的效率。

    Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation sho
    
[^163]: 基于价值梯度加权的模型驱动强化学习

    Value Gradient weighted Model-Based Reinforcement Learning. (arXiv:2204.01464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.01464](http://arxiv.org/abs/2204.01464)

    本文提出了一种基于价值梯度加权的模型驱动强化学习方法，用于提高模型学习的性能。

    

    模型驱动强化学习（MBRL）是一种高效获取控制策略的技术，但模型误差往往会导致性能下降。MBRL中的模型通常仅用于重建动态，特别是状态观察值，而模型误差对策略的影响不会被训练目标捕捉到。这导致MBRL的目标与实际使用的损失函数的目标不匹配，从而影响策略和价值的学习效果。本文提出了一种基于价值梯度加权的模型驱动强化学习方法（VaGraM），用于解决这个问题。

    Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the perfo
    
[^164]: 动态治疗效应和一般嵌套函数的自动去偏机器学习

    Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals. (arXiv:2203.13887v5 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2203.13887](http://arxiv.org/abs/2203.13887)

    本文提出了一种自动去偏机器学习方法，通过递归Riesz表征嵌套均值回归，避免了在动态治疗方案中需要解决辅助倾向模型的问题。

    

    我们将自动去偏机器学习的思想扩展到了动态治疗方案和更一般的嵌套函数上。我们展示了动态治疗方案离散处理的多重稳健公式可以用嵌套均值回归的递归 Riesz 表示来重新表述。然后，我们应用一种递归 Riesz 表示估计学习算法，估计去偏转化的修正，而无需描述校正项的形式，例如倒数概率加权项的乘积，如在动态机制中进行的双重稳健估计中所做的那样。我们的方法定义了一系列损失最小化问题，其最小化器是去偏转化的修正的乘数，从而避免了需要解决辅助倾向模型，并直接优化目标去偏性修正的均方误差。我们进一步将我们的方法应用于一些实际问题中。

    We extend the idea of automated debiased machine learning to the dynamic treatment regime and more generally to nested functionals. We show that the multiply robust formula for the dynamic treatment regime with discrete treatments can be re-stated in terms of a recursive Riesz representer characterization of nested mean regressions. We then apply a recursive Riesz representer estimation learning algorithm that estimates de-biasing corrections without the need to characterize how the correction terms look like, such as for instance, products of inverse probability weighting terms, as is done in prior work on doubly robust estimation in the dynamic regime. Our approach defines a sequence of loss minimization problems, whose minimizers are the mulitpliers of the de-biasing correction, hence circumventing the need for solving auxiliary propensity models and directly optimizing for the mean squared error of the target de-biasing correction. We provide further applications of our approach to
    
[^165]: RiskNet:不可靠资源网络的神经风险评估

    RiskNet: Neural Risk Assessment in Networks of Unreliable Resources. (arXiv:2201.12263v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2201.12263](http://arxiv.org/abs/2201.12263)

    本文提出了一种基于GNN算法的方法，用于在通信网络中处理连接故障引起的罚款分布，并可以准确地模拟各种现有拓扑结构中的惩罚，在实践中还获得了超过12,000倍的速度提高。

    

    我们提出了一种基于图神经网络（GNN）的方法，用于预测通信网络中的故障引起的罚款分布，其中连接受共享于工作路径和备用路径之间的资源保护。该GNN算法仅使用通过Barab\'asi-Albert模型生成的随机图进行训练。尽管如此，所得的测试结果表明我们可以准确地模拟各种现有拓扑结构中的惩罚。GNN消除了在研究网络拓扑时模拟复杂故障场景的需要。在实践中，整个设计操作仅受现代硬件的4ms的限制。这样，我们可以获得超过12,000倍的速度提高。

    We propose a graph neural network (GNN)-based method to predict the distribution of penalties induced by outages in communication networks, where connections are protected by resources shared between working and backup paths. The GNN-based algorithm is trained only with random graphs generated with the Barab\'asi-Albert model. Even though, the obtained test results show that we can precisely model the penalties in a wide range of various existing topologies. GNNs eliminate the need to simulate complex outage scenarios for the network topologies under study. In practice, the whole design operation is limited by 4ms on modern hardware. This way, we can gain as much as over 12,000 times in the speed improvement.
    
[^166]: 自监督的图形表示学习在神经元形态学中的应用

    Self-Supervised Graph Representation Learning for Neuronal Morphologies. (arXiv:2112.12482v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.12482](http://arxiv.org/abs/2112.12482)

    本文提出了GraphDINO，一种自监督的图形表示学习方法，可用于从未标记的大规模数据集中学习三维神经元形态的低维表示。该方法使用了一系列数据增强策略和新型的注意力机制AC-Attention，在多个大脑区域内，GraphDINO 显示出了优于其它最先进方法的表现。

    

    无监督的图形表示学习在多个应用领域如神经科学中吸引了越来越多的关注，其中对大脑中细胞类型的多样形态进行建模是其中的关键挑战之一。本文提出了GraphDINO，一种用于从未标记的大规模数据集中学习三维神经元形态的低维表示的数据驱动方法。GraphDINO 是一种新的基于Transformer模型的用于空间嵌入式图形表示学习的方法。为了使Transformer模型能够进行自监督学习，我们 (1)开发了针对空间嵌入式图形的数据增强策略， (2) 对位置编码进行了修改， (3)引入了新型的注意力机制AC-Attention，它结合了节点间基于注意力的全局交互和传统的图形卷积处理。我们在两个不同种类的、跨越多个大脑区域的神经元数据上测试，结果表明，GraphDINO 在神经元形态学表示学习方面表现出色，优于目前最先进的方法。研究结果表明，所提出的自监督方法可以学习到捕捉神经元形态学相关特征的表示。

    Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain
    
[^167]: 面对适应性泛化：贝叶斯视角下的研究

    Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.10761](http://arxiv.org/abs/2106.10761)

    本文提出面对自适应选择数据样本引起的过度拟合问题，使用噪声加算法可以提供不依赖于查询规模的误差保证，这一结果表明适应数据分析的问题在于新查询与过去查询的协方差。

    

    自适应选择样本可能导致过度拟合，简单的噪声加算法可以避免这一问题。本文证明了噪声加算法可以提供不依赖于查询规模的误差保证，这一结果来源于更好地理解自适应数据分析的核心问题。我们表明适应数据分析的问题在于新查询与过去查询的协方差。

    Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries. However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.  In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a 
    
[^168]: 多维时间序列的缺失值填充方法DeepMVI研究

    Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.01600](http://arxiv.org/abs/2103.01600)

    本文提出了DeepMVI，一种用于填充多维时间序列数据中缺失值的深度学习方法。该方法结合时间序列中的精细和粗粒度模式，以及跨分类维度的相关系列趋势。经过改进，DeepMVI表现出比其他算法更优秀的结果。

    

    本文提出了DeepMVI，一种用于填充多维时间序列数据中缺失值的深度学习方法。由于来自不同来源的长时间数据聚合会产生大量缺失数据，因此对于可靠的数据分析，需要仔细处理缺失数据。我们的方法使用神经网络来结合时间序列中的精细和粗粒度模式，以及跨分类维度的相关系列趋势。在经过多次不成功的尝试之后，我们设计了自己的网络，其中包括具有新型卷积窗口特征和核回归的时间变形器。在各种真实数据集上的填充设置中，我们展示了比几种最先进的算法更好的结果。

    We present DeepMVI, a deep learning method for missing value imputation in multidimensional time-series datasets. Missing values are commonplace in decision support platforms that aggregate data over long time stretches from disparate sources, and reliable data analytics calls for careful handling of missing data. One strategy is imputing the missing values, and a wide variety of algorithms exist spanning simple interpolation, matrix factorization methods like SVD, statistical models like Kalman filters, and recent deep learning methods. We show that often these provide worse results on aggregate analytics compared to just excluding the missing data. DeepMVI uses a neural network to combine fine-grained and coarse-grained patterns along a time series, and trends from related series across categorical dimensions. After failing with off-the-shelf neural architectures, we design our own network that includes a temporal transformer with a novel convolutional window feature, and kernel regr
    
[^169]: LEAD：用于极小-极大优化的最小作用动力学

    LEAD: Least-Action Dynamics for Min-Max Optimization. (arXiv:2010.13846v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.13846](http://arxiv.org/abs/2010.13846)

    本文提出了一种名为LEAD的优化器，它基于物理学中的动力学特性来改进博弈优化的收敛问题，并在二次极小-极大博弈中展示了线性收敛到纳什均衡的特性。

    

    对抗性建模（如生成性对抗网络（GAN））重新燃起了人们对双人极小-极大博弈的兴趣。该类博弈优化的一个主要难题是旋转动力学阻碍其收敛。本文表明，博弈优化共享粒子系统受多重力的动力学特性，可以利用物理学中的工具来改进优化动力学。受物理框架启发，我们提出一个用于极小-极大博弈的优化器LEAD。接下来，我们使用李亚普诺夫稳定性理论和谱分析，在连续和离散时间设置下研究了LEAD在一类二次极小-极大博弈中的收敛特性，展示了线性收敛到纳什均衡。最后，我们在合成数据集和CIFAR-10图像生成上对我们的方法进行了实证评估，证明了GAN训练中的改进效果。

    Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. In this paper, we show that game optimization shares dynamic properties with particle systems subject to multiple forces, and one can leverage tools from physics to improve optimization dynamics. Inspired by the physical framework, we propose LEAD, an optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete time settings for a class of quadratic min-max games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements in GAN training.
    
[^170]: McDiarmid不等式的推广

    An extension of McDiarmid's inequality. (arXiv:1511.05240v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1511.05240](http://arxiv.org/abs/1511.05240)

    本文推广了McDiarmid不等式，使其适用于具有有界差异的函数，并进一步将结果推广到一般度量空间的集中性。

    

    我们使用推广论证推广McDiarmid不等式，使它适用于具有有界差异的函数，并且适用于高概率集合。这些函数集中于它们的条件期望周围。我们进一步将结果推广到一般度量空间的集中性。

    We generalize McDiarmid's inequality for functions with bounded differences on a high probability set, using an extension argument. Those functions concentrate around their conditional expectations. We further extend the results to concentration in general metric spaces.
    

