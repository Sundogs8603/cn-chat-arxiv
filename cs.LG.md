# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [3D Adversarial Augmentations for Robust Out-of-Domain Predictions.](http://arxiv.org/abs/2308.15479) | 本文提出了一种用于改善模型对域外数据的泛化能力的方法，通过增加训练集中的对抗样本来实现。在3D语义分割任务中，该方法有效地防止了对非标准对象点的错误关联。这种方法通过学习一组向量来对对象进行对抗性变形，并通过约束保持它们的合理性和平滑性。 |
| [^2] | [An Adaptive Tangent Feature Perspective of Neural Networks.](http://arxiv.org/abs/2308.15478) | 本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。 |
| [^3] | [Policy composition in reinforcement learning via multi-objective policy optimization.](http://arxiv.org/abs/2308.15470) | 通过多目标政策优化方法，我们将预先存在的教师策略引入到强化学习中，证明了教师策略在无形状奖励下能够加速学习过程。我们的智能体成功地组合教师策略并扩展教师的策略以解决任务。 |
| [^4] | [Input margins can predict generalization too.](http://arxiv.org/abs/2308.15466) | 本研究发现当搜索空间适当约束时，输入界限可预测泛化性能，且与隐藏表示界限相比取得了极具竞争力的结果。 |
| [^5] | [A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios.](http://arxiv.org/abs/2308.15464) | 本文通过研究多种受重尾分析和不平衡分类问题启发的损失函数，提出了改进交通预测中拥堵情况的准确性的方法，并分别提出了针对不同优化目标的最佳选择。 |
| [^6] | [Canonical Factors for Hybrid Neural Fields.](http://arxiv.org/abs/2308.15461) | 该研究对混合神经网络领域的规范因素进行了研究，发现因子特征容量虽然简单高效，但存在不良偏差。通过学习一组规范化变换，该方法成功消除偏差，并在图像、距离和辐射场重建任务中实现了质量、鲁棒性和运行时间的改进。 |
| [^7] | [From SMOTE to Mixup for Deep Imbalanced Classification.](http://arxiv.org/abs/2308.15457) | 本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。 |
| [^8] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^9] | [Random feature approximation for general spectral methods.](http://arxiv.org/abs/2308.15434) | 本论文研究了随机特征逼近在一般谱方法中的应用，分析了与随机特征相结合的谱正则化方法的泛化性质，并获得了在不同正则性类别中的最佳学习速率。 |
| [^10] | [Probabilistic solar flare forecasting using historical magnetogram data.](http://arxiv.org/abs/2308.15410) | 这项研究使用历时太阳磁场图数据，结合卷积神经网络和逻辑回归模型，进行了概率性太阳耀斑预测，发现包含历史数据可以提高预测准确性和可靠性。 |
| [^11] | [Robust Long-Tailed Learning via Label-Aware Bounded CVaR.](http://arxiv.org/abs/2308.15405) | 本文提出了两种基于CVaR的方法来改进长尾学习的性能，并具备坚实的理论基础。第一种方法是引入了一种标签感知有界CVaR（LAB-CVaR）损失函数，解决了原始CVaR的悲观结果，通过设计最优权重上下界进行改进；第二种方法是提出了一种带有logit调整的LAB-CVaR（LAB-CVaR-logit）损失函数，以稳定优化过程。 |
| [^12] | [The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data.](http://arxiv.org/abs/2308.15395) | CausalBench挑战赛是一个机器学习竞赛，旨在构建基因网络推断的最新方法。参与者利用大规模遗传干扰数据提升了先进方法的性能。 |
| [^13] | [Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System.](http://arxiv.org/abs/2308.15394) | 本文提出了一种基于分散式多智能体强化学习的电荷平衡策略，通过分布式方法解决了分布式能量存储系统中的电荷平衡问题，并通过平均一致性算法来进行优化。 |
| [^14] | [Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation and Diagnosis.](http://arxiv.org/abs/2308.15386) | 本文提出了一种基于形状边缘知识增强网络（SkaNet）的甲状腺结节分割和诊断方法，通过探索甲状腺结节分割和诊断之间的关系，实现了两个任务的一体化。该方法基于甲状腺影像报告和数据系统（TI-RADS），利用形状和边缘特征来区分良性和恶性甲状腺结节。 |
| [^15] | [Multi-Response Heteroscedastic Gaussian Process Models and Their Inference.](http://arxiv.org/abs/2308.15370) | 本文介绍了多响应异方差高斯过程模型，将其应用于回归、分类和状态空间模型，并提出了一种利用变分推断来近似后验的方法，解决了高斯过程模型在捕捉函数平滑性的突变和适应异方差错误方面的局限性。 |
| [^16] | [Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation.](http://arxiv.org/abs/2308.15367) | pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL. |
| [^17] | [Heterogeneous Multi-Task Gaussian Cox Processes.](http://arxiv.org/abs/2308.15364) | 本文提出了一种用于联合建模多个异构相关任务的扩展方法，通过多输出高斯过程（MOGP）促进任务之间的信息共享和非参数参数估计，并应用数据增广技术来解决非共轭贝叶斯推断问题。 |
| [^18] | [Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation.](http://arxiv.org/abs/2308.15363) | 本文提出了一个大规模语言模型(LLMs)赋能的文本到SQL任务的基准评估，并基于实验结果提出了一种新的集成解决方案DAIL-SQL，刷新了Spider榜单并实现了86.6%的执行准确率。同时，强调了在提示工程中的词汇效率以实现高效经济的LLM-based文本到SQL解决方案，此外还对在上下文学习中应用开源LLMs进行了研究，并进行了任务特定的性能优化。 |
| [^19] | [Lie-Poisson Neural Networks (LPNets): Data-Based Computing of Hamiltonian Systems with Symmetries.](http://arxiv.org/abs/2308.15349) | 该论文提出了Lie-Poisson神经网络（LPNets），用于基于数据计算具有对称性的Hamiltonian系统。该网络通过准确保持Poisson括号和特殊函数来预测系统的长期演化。 |
| [^20] | [Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary.](http://arxiv.org/abs/2308.15344) | 本文提出了一种对深度神经网络进行的不可察觉对抗攻击，通过系统性地攻击输入图像边界来查找对抗性样本。实验结果表明，边界可以主导DNN模型的行为。 |
| [^21] | [Enhancing Robot Learning through Learned Human-Attention Feature Maps.](http://arxiv.org/abs/2308.15327) | 本文提出了一种通过学习人类关注特征图来增强机器人学习的方法。通过模拟人类的注意力机制，并将其作为辅助特征传递给下游的学习任务，可以提高模型对于分布之外样本的稳健性和学习速度。 |
| [^22] | [Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing.](http://arxiv.org/abs/2308.15323) | 本论文提出了一种通过均匀的Tanh变换进行面部解析的遮挡感知深度卷积神经网络。该方法解决了面部遮挡问题，并且能够融合更多上下文信息。同时，引入了遮挡感知损失，提高了边界的识别能力。 |
| [^23] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^24] | [On-Device Learning with Binary Neural Networks.](http://arxiv.org/abs/2308.15308) | 本文提出了一种在低功耗设备上使用二进制神经网络进行设备端学习的解决方案，该方法结合了最新的连续学习技术和二进制神经网络的高效性。实验证实了该方法的有效性和适用性。 |
| [^25] | [Towards quantitative precision for ECG analysis: Leveraging state space models, self-supervision and patient metadata.](http://arxiv.org/abs/2308.15291) | 这项研究利用状态空间模型(SSMs)和自监督学习来改善深度学习模型在自动心电图分析中的定量准确性。结果表明，使用SSMs可以捕捉时间序列数据中的长期依赖关系，并且在标准诊断任务中，较高的采样率和扩大输入尺寸并没有明显的改进效果。 |
| [^26] | [Structural Node Embeddings with Homomorphism Counts.](http://arxiv.org/abs/2308.15283) | 本论文提出了使用同态计数的结构节点嵌入，通过实验证明了其有效性。这种嵌入在解释和理解机器学习模型方面具有优势，并可应用于各种下游任务。 |
| [^27] | [Let There Be Sound: Reconstructing High Quality Speech from Silent Videos.](http://arxiv.org/abs/2308.15256) | 本文介绍了一个重建高质量语音的唇语转语音系统，通过解决一对多映射问题和细节精炼来显著改进生成质量。 |
| [^28] | [The Relative Gaussian Mechanism and its Application to Private Gradient Descent.](http://arxiv.org/abs/2308.15250) | 本文介绍了相对高斯机制(RGM)，它利用了相对L2敏感性假设，在保护隐私的同时能够更精确地界定隐私损失。 |
| [^29] | [Reliability Gaps Between Groups in COMPAS Dataset.](http://arxiv.org/abs/2308.15243) | 本文研究了在COMPAS数据集中不同群体之间的可靠性差异，并发现输出可靠性存在系统性差异，差异的方向取决于所使用的评估者间统计量的类型和是否进行了对群体预测概率的修正。 |
| [^30] | [Assessing Cyclostationary Malware Detection via Feature Selection and Classification.](http://arxiv.org/abs/2308.15237) | 该研究旨在通过循环平稳性检测恶意软件行为，并找到在网络入侵检测系统中最重要的循环平稳特征。 |
| [^31] | [Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification.](http://arxiv.org/abs/2308.15232) | 本文提出了一种结合可解释性分析的分类感知神经主题模型，用于冲突分类和主题发现。该模型提供了可靠的分类结果和发现的主题的解释，并通过优化模型的复杂度来提高分类性能。 |
| [^32] | [Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders.](http://arxiv.org/abs/2308.15230) | 本论文提出了一种使用变分自动编码器的新方法，通过限制人口统计信息的编码来减少推荐系统中的歧视，从而为以前未出现的用户提供公平推荐。 |
| [^33] | [Evaluating Explanation Methods for Multivariate Time Series Classification.](http://arxiv.org/abs/2308.15223) | 本文评估了用于多变量时间序列分类的解释方法，重点研究了基于显著性的方法来指示分类决策中最相关的通道和时间序列点。 |
| [^34] | [Ensemble of Counterfactual Explainers.](http://arxiv.org/abs/2308.15194) | 该论文提出了一种合奏反事实解释器，可以提升弱解释器的性能，实现对反事实实例的最小化、可操作性、稳定性、多样性、合理性和辨别力的全覆盖。 |
| [^35] | [Is visual explanation with Grad-CAM more reliable for deeper neural networks? a case study with automatic pneumothorax diagnosis.](http://arxiv.org/abs/2308.15172) | 本研究通过自动气胸诊断案例研究，探究了不同深度学习模型中Grad-CAM的鲁棒性和效果，结果表明深度神经网络并不一定会显著改善Grad-CAM的性能。 |
| [^36] | [ABS-SGD: A Delayed Synchronous Stochastic Gradient Descent Algorithm with Adaptive Batch Size for Heterogeneous GPU Clusters.](http://arxiv.org/abs/2308.15164) | ABS-SGD 是一种用于异构GPU集群的延迟同步随机梯度下降算法，实现了计算资源的充分利用并缓解了过时梯度问题。 |
| [^37] | [On the improvement of model-predictive controllers.](http://arxiv.org/abs/2308.15157) | 本文研究了模型预测控制器的改进问题，通过提高内部预测模型的精确性来自动改善整个控制器，结果表明提高预测模型总体上将改善控制器的质量。 |
| [^38] | [Uncertainty Aware Training to Improve Deep Learning Model Calibration for Classification of Cardiac MR Images.](http://arxiv.org/abs/2308.15141) | 本文研究了三种新的不确定性感知训练策略，比较了它们与两种最先进方法的效果，并在心脏磁共振图像分类的两个临床应用中进行了性能分析。 |
| [^39] | [Biquality Learning: a Framework to Design Algorithms Dealing with Closed-Set Distribution Shifts.](http://arxiv.org/abs/2308.15132) | 本论文提出了一种双质量学习的框架，旨在设计能够处理闭集分布转换的算法。该框架假设在训练时有一个可信数据集和一个带有数据集转换和弱监督的不可信数据集可用，可以应对任何分布转换。作者提出了两种方法来处理双质量学习，分别灵感来源于标签噪声和协变量转换，实验证明这些方法在真实世界数据集中具有较好的效果。 |
| [^40] | [Evaluation and Analysis of Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2308.15126) | 本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。 |
| [^41] | [Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators.](http://arxiv.org/abs/2308.15116) | 本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。 |
| [^42] | [Stochastic Graph Bandit Learning with Side-Observations.](http://arxiv.org/abs/2308.15107) | 本文研究了具有一般函数空间和图反馈的随机背景赌博学习问题，并提出了一种算法，填补了此前研究中的空白。该算法提供了奖励差距依赖的上界，并在遗憾上界方面提供了改进。数值实验证明了该方法的计算效率和有效性。 |
| [^43] | [How Faithful are Self-Explainable GNNs?.](http://arxiv.org/abs/2308.15096) | 自解释的GNN的可信度进行了分析，并发现了模型本身和评估指标的几个限制，提出了可能的解决办法。 |
| [^44] | [Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification.](http://arxiv.org/abs/2308.15094) | 通过分位数回归校准实现基于群体条件的符合预测框架，解决了深度学习预测模型的不确定性问题，并应用于实际农业计算机视觉中的作物与杂草分类问题。 |
| [^45] | [Can We Rely on AI?.](http://arxiv.org/abs/2308.15092) | 近年来，对抗性攻击算法揭示了人工智能工具的不稳定性，在高风险环境中引发了安全、可靠性和可解释性方面的问题。这篇论文提供了一个对该主题的概述，关注对应用和计算数学领域的研究人员可能感兴趣的方面。 |
| [^46] | [Using deep learning for an automatic detection and classification of the vascular bifurcations along the Circle of Willis.](http://arxiv.org/abs/2308.15088) | 该研究利用深度学习技术开发了一种自动检测和分类Willis环血管分叉的方法，帮助神经放射学家及时定位高颅内动脉瘤风险的区域。 |
| [^47] | [Exploring Model Transferability through the Lens of Potential Energy.](http://arxiv.org/abs/2308.15074) | 本文提出了一种名为PED的物理启发式方法，通过在受力驱动的物理模型中捕捉动态表示的运动来降低潜在能量，以解决模型选择时的挑战，并获得更强大和更稳定的观察结果。 |
| [^48] | [Advancing Adversarial Robustness Through Adversarial Logit Update.](http://arxiv.org/abs/2308.15072) | 本研究提出了一种基于对抗logit更新的新原则(ALU)，用于推断对抗样本的标签，并通过使用预处理和后处理的logit差异提高了模型的对抗鲁棒性。 |
| [^49] | [MadSGM: Multivariate Anomaly Detection with Score-based Generative Models.](http://arxiv.org/abs/2308.15069) | MadSGM是一种基于评分的生成模型的多变量时间序列异常检测器，考虑了重构、密度和梯度等全面的异常测量因素。实验证明MadSGM具有最强大和准确的预测能力。 |
| [^50] | [OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams.](http://arxiv.org/abs/2308.15059) | OEBench是一个用于评估关系数据流中开放环境挑战的开放环境基准，研究发现这种挑战在真实世界数据集中普遍存在。 |
| [^51] | [Taxonomic Loss for Morphological Glossing of Low-Resource Languages.](http://arxiv.org/abs/2308.15055) | 本文提出了一种利用形态信息的分类损失函数，在低资源语言的形态词义标注中提高了性能。 尽管在单标签预测准确性方面不如标准损失函数，但在前n个预测标签方面表现更好。 这个方法在人机协作标注方面具有潜力。 |
| [^52] | [iBARLE: imBalance-Aware Room Layout Estimation.](http://arxiv.org/abs/2308.15050) | 我们提出了平衡感知式房间布局估计（iBARLE）框架，它包括外观变化生成模块、复杂结构混合模块和梯度-based布局目标函数，旨在解决房间布局估计中的不平衡和泛化问题。 |
| [^53] | [Large language models converge toward human-like concept organization.](http://arxiv.org/abs/2308.15047) | 大型语言模型学会以类似于知识库的方式组织概念，这表明它们具备人类推理语义和世界知识的能力。 |
| [^54] | [Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs.](http://arxiv.org/abs/2308.15020) | 提出了一种基于GPU的大规模并行连续局部搜索方法来加速混合SAT求解器，该方法通过使用基于快速傅里叶变换的新型并行算法来计算基本对称多项式，并在搜索中使用重启启发式算法以提高效率。与以前的方法相比，实现了显著的改进。 |
| [^55] | [Exploiting Problem Geometry in Safe Linear Bandits.](http://arxiv.org/abs/2308.15006) | 通过利用安全线性赌博机问题的几何特征，我们提出了改进的遗憾保证算法，并将其推广到具有凸约束的情况。模拟结果显示，在各种随机采样的设置中，我们的算法表现出优越的性能。 |
| [^56] | [WSAM: Visual Explanations from Style Augmentation as Adversarial Attacker and Their Influence in Image Classification.](http://arxiv.org/abs/2308.14995) | 本文提出了一种使用基于随机抽样和噪声添加的风格增强算法，该算法在风格转移中表现出了非凡的鲁棒性，并且在STL-10数据集上超过了先前的方法和最先进的性能。 |
| [^57] | [Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence.](http://arxiv.org/abs/2308.14991) | 本研究引入了神经启发的适应性解决方案，以实现人工智能的持续学习。通过模拟果蝇学习系统，我们提出了一种可以灵活适应变化的通用方法，改善了学习可塑性，并确保解决方案的兼容性。 |
| [^58] | [Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation.](http://arxiv.org/abs/2308.14983) | 本文提出了一种基于增量构建学习和集成域适应的方法，用于滚动轴承故障诊断。该方法通过在随机配置网络上实施增量学习，结合云特征提取和小波包分解，提高了故障诊断的准确性和适应性。 |
| [^59] | [Sub-universal variational circuits for combinatorial optimization problems.](http://arxiv.org/abs/2308.14981) | 本研究提出了一种基于经典概率电路的变分电路，用于解决组合优化问题。通过对Max-Cut问题的数值研究，我们发现这种变分电路在多种图上表现出更好的性能，相比于量子近似优化算法。在评估量子变分电路的性能时，可以将其与具有子通用门集的变分电路进行比较，以识别量子变分电路的优势领域。 |
| [^60] | [Efficient labeling of solar flux evolution videos by a deep learning model.](http://arxiv.org/abs/2308.14976) | 通过粗略标注的天文视频训练卷积神经网络（CNN），可以提高数据标注质量和减少人为干预，减少标注过程中的时间消耗。 |
| [^61] | [Distributed multi-agent target search and tracking with Gaussian process and reinforcement learning.](http://arxiv.org/abs/2308.14971) | 本研究提出了一种基于强化学习和高斯过程的多智能体目标搜索和跟踪方法，在未知目标场景下通过数据驱动的方式实现目标探索和决策规划，提高效率和准确性。 |
| [^62] | [Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets.](http://arxiv.org/abs/2308.14969) | 重新审视彩票式转移的高效可靠性，研究了线性探测（LP）和视觉提示/重新编程（VP）方法在数据稀疏性和模型稀疏性方面的能力，并发现彩票式转移并非通用的重新编程器。 |
| [^63] | [Streaming Compression of Scientific Data via weak-SINDy.](http://arxiv.org/abs/2308.14962) | 本文开发了一种专门用于压缩流式科学数据的流式弱SINDy算法。与经典的离线压缩算法不同，流式压缩算法可以在数据生成过程中进行压缩，使其适用于科学数据压缩。流式弱SINDy算法利用了底层数据的特征来进行压缩。 |
| [^64] | [Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset.](http://arxiv.org/abs/2308.14951) | 本文实现了一种鲁棒的开放集言语识别系统，通过使用MFCC和音高特征，TDNN模型提取特征嵌入，设置置信度阈值，以及使用LDA和pLDA学习新的未知语言分类来实现。在经过训练的语言上，系统准确率达到91.76%，并且具备实时适应未知语言的能力。 |
| [^65] | [Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation.](http://arxiv.org/abs/2308.14949) | 本文提出了一种带有平滑感知的深度图神经网络的低比特量化解决方案，用于解决大规模和深层GNN中的效率和准确性问题。该方法通过压缩模型、学习量化范围和控制层间相似性的变化来实现高效处理和避免过度平滑问题。 |
| [^66] | [Improving Reinforcement Learning Training Regimes for Social Robot Navigation.](http://arxiv.org/abs/2308.14947) | 本研究提出了一种改进社交机器人导航的强化学习训练方法，通过使用课程学习，多样化环境和建模行人等技术，实现了更好的泛化性能。 |
| [^67] | [Reinforcement Learning for Sampling on Temporal Medical Imaging Sequences.](http://arxiv.org/abs/2308.14946) | 本论文研究了在动态图像重建中学习采样策略的问题，通过应用深度Q学习和REINFORCE算法，成功地发现了最佳采样模式，并验证了其对图像质量的改善。 |
| [^68] | [Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals.](http://arxiv.org/abs/2308.14945) | 本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。 |
| [^69] | [Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance.](http://arxiv.org/abs/2308.14938) | 本研究通过引入基于熵的损失项，通过测量神经网络处理数据时的熵变化，指导神经网络以更快速的收敛、更好的性能学习丰富的潜在数据表示。 |
| [^70] | [Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples.](http://arxiv.org/abs/2308.14930) | 本研究探讨了量子预处理滤波器（QPF）在二值图像分类中的应用，并通过在MNIST、EMNIST和CIFAR-10上提高了分类准确率，并在GTSRB上降低了分类准确率。 |
| [^71] | [Maestro: Uncovering Low-Rank Structures via Trainable Decomposition.](http://arxiv.org/abs/2308.14929) | 本研究提出了一种通过可训练分解揭示低秩结构的方法，解决了深度神经网络训练大模型时的耗时和资源消耗的问题。 |
| [^72] | [Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning.](http://arxiv.org/abs/2308.14924) | 本研究通过将西门子能源提供的热力学软件纳入环境模型，并模拟不确定性，揭示了使用深度强化学习进行经济燃气轮机调度优化的好处，并发现Deep Q-Networks (DQN) 在算法和基准方法中获得了最高的奖励。 |
| [^73] | [Gender bias and stereotypes in Large Language Models.](http://arxiv.org/abs/2308.14921) | 研究发现大型语言模型存在性别偏见和刻板印象，它们更倾向于选择与个人性别刻板印象一致的职业，并且放大了偏见，超过了现实情况。 |
| [^74] | [Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction.](http://arxiv.org/abs/2308.14920) | Matbench Discovery是一个评估机器学习晶体稳定性预测的框架，在热力学稳定性预测方面的测试中，CHGNet表现最佳。 |
| [^75] | [On Reward Structures of Markov Decision Processes.](http://arxiv.org/abs/2308.14919) | 该论文研究了马尔可夫决策过程中的奖励结构，提出了一种估计器用于估计单个状态值，并通过根据奖励代替常用的基于转移的常数，提供了对强化学习中技巧的理论解释。 |
| [^76] | [RecRec: Algorithmic Recourse for Recommender Systems.](http://arxiv.org/abs/2308.14916) | 本文提出了一个算法性补救框架，用于帮助理解推荐系统的模型并修改推荐结果。 |
| [^77] | [Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech.](http://arxiv.org/abs/2308.14909) | 本文提出了一种剪枝方法来提高文本到语音转换模型的泛化能力，通过剪枝自注意力层中的多余连接，并采用可微的剪枝方法以自动学习剪枝阈值。实验证实该方法在零唤醒多说话者TTS中具有有效性。 |
| [^78] | [BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition.](http://arxiv.org/abs/2308.14906) | BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。 |
| [^79] | [Maturity-Aware Active Learning for Semantic Segmentation with Hierarchically-Adaptive Sample Assessment.](http://arxiv.org/abs/2308.14904) | MADBAL是一种成熟度感知的主动学习方法，通过分层的方式综合考虑了不同样本定义，能够选择最有影响力的分割像素。它还采用了新的不确定性表达方式，并在早期学习阶段显著提升性能，减轻了训练负担。在Cityscapes和PASCAL VOC数据集上，MADBAL优于现有方法。 |
| [^80] | [Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks.](http://arxiv.org/abs/2308.14902) | Ad-Rec是一个利用高级特征交互技术解决推荐网络中协变量漂移问题的模型，通过利用掩码转换器实现高阶交叉特征的学习，提高了模型质量、加速了收敛速度并减少了训练时间。 |
| [^81] | [Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning.](http://arxiv.org/abs/2308.14897) | 本论文提出了一种使用双策略评估的离线序列建模和离线强化学习的RL算法，通过统计方法证明具有方差缩减的特性。 |
| [^82] | [Conformal Meta-learners for Predictive Inference of Individual Treatment Effects.](http://arxiv.org/abs/2308.14895) | 本文提出了一种新的一致性元学习器框架，通过在条件平均治疗效果元学习器上应用一致性预测程序，生成个体治疗效果的预测区间，同时证明了其有效性。 |
| [^83] | [When hard negative sampling meets supervised contrastive learning.](http://arxiv.org/abs/2308.14893) | 当前图像模型在训练时常用交叉熵损失，但其在泛化和稳定性方面存在问题。本文提出了一种新的监督对比学习目标，SCHaNe，通过加权困难的负样本挖掘来提高模型性能。实验结果显示，SCHaNe在各个基准测试中的Top-1准确率优于BEiT-3。 |
| [^84] | [CommunityFish: A Poisson-based Document Scaling With Hierarchical Clustering.](http://arxiv.org/abs/2308.14873) | 本文介绍了一种新的文本缩放方法CommunityFish，它利用层次聚类算法在词空间上聚类，从而揭示文本数据中独立词组（社区）的差异。 |
| [^85] | [NAS-X: Neural Adaptive Smoothing via Twisting.](http://arxiv.org/abs/2308.14864) | NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。 |
| [^86] | [Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams.](http://arxiv.org/abs/2308.14861) | 本研究评估了使用熔池图像流进行印刷轨迹异常分类的关键时空学习器，并介绍了一些领先的深度时空学习模型的实践应用。 |
| [^87] | [SMOClust: Synthetic Minority Oversampling based on Stream Clustering for Evolving Data Streams.](http://arxiv.org/abs/2308.14845) | 基于流聚类的合成少数类过采样方法SMOClust可以在演化数据流中解决类别不平衡和概念漂移的挑战，通过利用数据困难因素来适应漂移并生成合成样本。 |
| [^88] | [Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction.](http://arxiv.org/abs/2308.14841) | 该研究利用肌电图设备测量、建模和预测了VR用户在与虚拟环境交互时的颈部肌肉收缩水平，开发了一个生物物理启发式的计算模型，可以准确预测不同头部运动状态下的颈部收缩水平，并且可以预测仅通过目标头部姿势的潜在收缩需求。 |
| [^89] | [Tackling Diverse Minorities in Imbalanced Classification.](http://arxiv.org/abs/2308.14838) | 该研究提出了一种应对不平衡分类中多样的少数群体问题的方法，通过混合少数和多数群体的数据样本来生成合成样本，来解决分散分布的少数群体问题。 |
| [^90] | [Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates.](http://arxiv.org/abs/2308.14831) | 本文通过实证研究探究了持续学习中不同动态稀疏训练（DST）组件的影响，旨在填补关键研究空白并为持续学习下的DST提供最佳配置指导。 |
| [^91] | [Distributionally Robust Statistical Verification with Imprecise Neural Networks.](http://arxiv.org/abs/2308.14815) | 本文提出了一种使用不精确神经网络的分布鲁棒统计验证方法，通过结合主动学习、不确定性量化和神经网络验证，可以在大量的分布上提供对黑盒系统行为的保证。 |
| [^92] | [Scattering with Neural Operators.](http://arxiv.org/abs/2308.14789) | 这项研究使用神经算子进行散射预测，证明了在量子力学中的应用潜力，并展示了神经算子在两个具体问题中相比传统方法更高效的特点。 |
| [^93] | [A correlation-based fuzzy cluster validity index with secondary options detector.](http://arxiv.org/abs/2308.14785) | 本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。 |
| [^94] | [Generating tabular datasets under differential privacy.](http://arxiv.org/abs/2308.14784) | 该论文研究了在差分隐私的约束下生成表格数据集的问题，通过利用生成对抗网络（GAN），它解决了训练数据的记忆重复和隐私泄露的问题，并提出了与传统方法相比更好的解决方案。 |
| [^95] | [Distributed Dual Coordinate Ascent with Imbalanced Data on a General Tree Network.](http://arxiv.org/abs/2308.14783) | 本论文研究了树网络中不平衡数据对分布式双坐标上升算法收敛性的影响，并提出了延迟广义分布式双坐标上升方法来解决这个问题。实验结果表明，该方法有效提高了算法的收敛速度。 |
| [^96] | [Conflict-Aware Active Automata Learning.](http://arxiv.org/abs/2308.14781) | C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。 |
| [^97] | [Systematic reduction of Hyperspectral Images for high-throughput Plastic Characterization.](http://arxiv.org/abs/2308.14776) | 本研究系统性地降维高光谱图像，实现对高通量塑料特性的评估。同时应用化学统计学中的高科技发展，通过自动化和循证数据缩减，提高了非负矩阵分解（NMF）的速度和性能。 |
| [^98] | [XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples.](http://arxiv.org/abs/2308.14769) | XVir是一种基于Transformer的深度学习架构，用于可靠识别人类肿瘤中的病毒DNA。该架构通过对来自病毒和人类基因组的基因组测序读取进行训练，并可以与肿瘤序列信息一起使用以找到病毒DNA的证据。 |
| [^99] | [Unified Concept Editing in Diffusion Models.](http://arxiv.org/abs/2308.14761) | 本文提出了一种称为统一概念编辑（UCE）的方法，通过使用一个闭合解决方案对模型进行编辑，同时解决文本到图像模型中的偏见、版权和冒犯性内容等问题。实验证明了该方法的改进和可扩展性。 |
| [^100] | [May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations.](http://arxiv.org/abs/2308.14759) | 本论文提出了一种统一的力导向预训练模型用于3D分子构型，涵盖了平衡和非平衡数据。通过直接从原子力中学习非平衡数据和使用零力正则化和基于力的去噪技术近似近平衡力，我们获得了一个包含超过1500万个多样构型的预训练模型，相比于未预训练的模型，我们将力的准确性提高了大约3倍。 |
| [^101] | [Reinforcement Learning for Generative AI: A Survey.](http://arxiv.org/abs/2308.14328) | 该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。 |
| [^102] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^103] | [A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions.](http://arxiv.org/abs/2308.13821) | 本综述对图上不平衡学习进行了全面的审视，旨在纠正数据分布偏差，以获得更准确和代表性的学习结果。 |
| [^104] | [Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities.](http://arxiv.org/abs/2308.13420) | 本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。 |
| [^105] | [Beyond Document Page Classification: Design, Datasets, and Challenges.](http://arxiv.org/abs/2308.12896) | 本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。 |
| [^106] | [EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction.](http://arxiv.org/abs/2308.06564) | EquiDiff是一种基于条件扩散模型的轨迹预测深度生成模型，通过整合历史信息和随机噪声来生成未来车辆轨迹，并利用几何特性和社交交互提取技术来提高性能。 |
| [^107] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^108] | [Symmetry-Preserving Program Representations for Learning Code Semantics.](http://arxiv.org/abs/2308.03312) | 本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。 |
| [^109] | [Risk-optimized Outlier Removal for Robust Point Cloud Classification.](http://arxiv.org/abs/2307.10875) | 提出了一种面向稳健点云分类的风险优化异常值去除方法，利用普通训练的模型消除额外的异常值并恢复数据。方法通过归因分析确定每个点对模型输出的影响，使用条件风险价值优化高风险点的过滤过程。该方法在不需要额外训练的情况下能够产生出色的结果。 |
| [^110] | [Atlas-Based Interpretable Age Prediction.](http://arxiv.org/abs/2307.07439) | 本研究提出了一种基于图谱的可解释年龄预测方法，利用全身图像研究了各个身体部位的年龄相关变化。通过使用解释性方法和配准技术，确定了最能预测年龄的身体区域，并创下了整个身体年龄预测的最新水平。研究结果表明，脊柱、本原性背部肌肉和心脏区域是最重要的关注领域。 |
| [^111] | [inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data.](http://arxiv.org/abs/2307.03854) | inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。 |
| [^112] | [All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning.](http://arxiv.org/abs/2307.00290) | 本论文介绍了一种称为全能SAM的流程，通过在整个AI开发工作流程中使用SAM，并且在推理阶段无需手动提示，实现了从弱注释到基于提示的像素级细胞核分割的目标。 |
| [^113] | [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms.](http://arxiv.org/abs/2306.16740) | 本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。 |
| [^114] | [Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset.](http://arxiv.org/abs/2306.11167) | 这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。 |
| [^115] | [Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective.](http://arxiv.org/abs/2306.10033) | 该论文调查了跨越语音和语言处理学科的七个会议中，源代码可用性对于Interspeech会议要少于40%，并提出了建议以提高未来研究的可重复性。 |
| [^116] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^117] | [Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML.](http://arxiv.org/abs/2306.09297) | 本文提出了一个使用AutoML技术来减少偏见的新方法，该方法通过改进优化函数和搜索空间，并结合公平目标，在几乎不损失准确性的情况下减少基于ML的软件中的偏见。同时提出了一个适用于AutoML的公平感知搜索空间修剪方法，以减少计算成本和修复时间。 |
| [^118] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^119] | [Combining Primal and Dual Representations in Deep Restricted Kernel Machines Classifiers.](http://arxiv.org/abs/2306.07015) | 该论文提出了一种在深度学习和核机器分类器中结合原始和对偶表示的新方法，通过将可见单元和隐藏单元的核主成分分析和最小二乘支持向量机相结合，实现了多个级别的深度架构。该方法在计算效率上具有优势，适用于高维输入和大数据集。 |
| [^120] | [When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset.](http://arxiv.org/abs/2306.06826) | 标注者的背景对数据标注的影响很重要。通过POPQUORN数据集的分析，我们发现标注者的背景在他们的判断中起到了显著作用，并且应该考虑以前未考虑的背景因素。我们的研究建议理解标注者的背景，从具有人口统计学平衡的众包工作者中收集标签，以减少数据集的偏差。 |
| [^121] | [When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming.](http://arxiv.org/abs/2306.04930) | 本研究利用先前数据的干预措施提高基于AI的代码推荐系统的有效性，提出了一个CDHF框架来整合人类反馈，预测建议接受程度并决定何时展示哪些建议。 |
| [^122] | [Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities.](http://arxiv.org/abs/2306.03454) | 本篇论文研究了AI多传感器融合系统的鲁棒性问题，特别关注在安全关键的环境中的表现和可靠性，并提出了基于AI的MSF感知系统基准测试套件。 |
| [^123] | [On Optimal Caching and Model Multiplexing for Large Model Inference.](http://arxiv.org/abs/2306.02003) | 本文提出了最优缓存与模型复用两种方法来缓解大型模型推理中资源消耗和延迟挑战，经过实证模拟发现这种组合大大提高了传统模型推理方法的性能。 |
| [^124] | [Blockwise Parallel Transformer for Long Context Large Models.](http://arxiv.org/abs/2305.19370) | 本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。 |
| [^125] | [Reinforcement Learning With Reward Machines in Stochastic Games.](http://arxiv.org/abs/2305.17372) | 该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。 |
| [^126] | [Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration.](http://arxiv.org/abs/2305.17261) | 本论文介绍了一种基于机器学习和人工智能协作的高危孕产妇计划，提出了早期妊娠检测、准确识别高风险会员和提供可解释指标等三个挑战的解决方案，提高了孕期风险的预测准确率。 |
| [^127] | [Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time.](http://arxiv.org/abs/2305.17118) | Scissorhands是一个可以在不对模型进行微调的情况下，通过利用重要性持久性假设将LLM KV缓存的内存使用维持在固定预算内的系统。 |
| [^128] | [Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting.](http://arxiv.org/abs/2305.15786) | 本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。 |
| [^129] | [torchgfn: A PyTorch GFlowNet library.](http://arxiv.org/abs/2305.14594) | torchgfn是一个基于PyTorch构建的GFlowNet库，提供了简单的API和有用的抽象，解决了不同代码库的约定问题，并且重现了已发表的结果。 |
| [^130] | [A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction.](http://arxiv.org/abs/2305.09121) | 本文提出了一种名为VIC-DDPM的条件去噪扩散概率模型，在可见度数据和脏图像的帮助下，能够生成更细节的图像，同时消除噪声和伪影。相关实验证实，该算法在恢复微弱信号、保留细节结构和消除伪影等方面有很好的性能。 |
| [^131] | [Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM.](http://arxiv.org/abs/2304.12534) | 本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。 |
| [^132] | [Explainable AI Insights for Symbolic Computation: A case study on selecting the variable ordering for cylindrical algebraic decomposition.](http://arxiv.org/abs/2304.12154) | 本文研究了在符号计算中使用可解释的人工智能技术的案例，通过使用SHAP工具为圆柱代数分解的变量排序提供新的启发式方法，从而为符号计算提供了新的洞察。 |
| [^133] | [Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation.](http://arxiv.org/abs/2304.11705) | 本研究通过设计第一个实验设置，探讨了LiDAR语义分割在不同领域间的泛化能力。研究结果表明，现有方法在跨领域设置中存在显著差距。为了解决这个问题，我们提出了一种新的方法，通过结合稀疏-密集卷积网络，实现了在目标领域上的显著优化。 |
| [^134] | [Multi-label Node Classification On Graph-Structured Data.](http://arxiv.org/abs/2304.10398) | 该论文提出了一种新的图神经网络架构 MLGCN，用于处理多标签节点分类任务，并研究了多标签场景中同类偏好的语义。在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。 |
| [^135] | [Improving Few-Shot Prompts with Relevant Static Analysis Products.](http://arxiv.org/abs/2304.06815) | 本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。 |
| [^136] | [Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification.](http://arxiv.org/abs/2304.03981) | 提出了基于不确定性的开放集(UIOS)模型，用于处理训练中未见过的类别样本，该模型通过计算不确定性得分来表达其置信度，并在多个测试数据集中表现优异，为真实世界的视网膜异常筛查提供了一个强大的方法。 |
| [^137] | [The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting.](http://arxiv.org/abs/2303.15860) | 该论文提出了一个基于Wyner变分自编码器的多层指纹识别框架，通过利用多层特征共享的设备信息，在无监督情况下提高识别性能。 |
| [^138] | [Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach.](http://arxiv.org/abs/2303.15544) | 本研究提出DIAMOND算法用于无线干扰网络中的多流传输，该算法基于收敛图学习方法，既可以集中式计算多流传输策略，也可以分布式实现数据包的传输，相比现有方法提高了15-20％的网络性能。 |
| [^139] | [Generalized partitioned local depth.](http://arxiv.org/abs/2303.10167) | 本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。 |
| [^140] | [Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference.](http://arxiv.org/abs/2303.07122) | 该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。 |
| [^141] | [Physics-informed neural networks for solving forward and inverse problems in complex beam systems.](http://arxiv.org/abs/2303.01055) | 本文提出了一种使用物理信息神经网络（PINNs）解决复杂梁系统中正向和逆向问题的新框架，通过求解欧拉-伯努利和Timoshenko偏微分方程，高效地计算横向位移和横截面旋转，并稳健地确定未知模型参数和施加力，为工程结构和机械问题的解决提供了有前景的策略。 |
| [^142] | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective.](http://arxiv.org/abs/2302.12095) | 本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。 |
| [^143] | [Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize.](http://arxiv.org/abs/2302.05516) | 本文研究了深度学习中的随机步长和循环步长相对于常数步长的优势，通过考虑尾部指数如何随步长调度的变化而变化来推动理论研究，进一步揭示了它们对于泛化性能的改善机制。 |
| [^144] | [Fairness-aware Vision Transformer via Debiased Self-Attention.](http://arxiv.org/abs/2301.13803) | 这篇论文提出了一种基于去偏自注意力的公平感知视觉变换器框架，通过消除与敏感属性相关的虚假特征来减轻偏见，并利用对抗性示例来定位和屏蔽这些特征。 |
| [^145] | [Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets.](http://arxiv.org/abs/2301.03364) | 本文介绍了两个无线测量活动所提供的数据集，并将其与机器学习结合起来用于指纹识别、视线检测、服务质量预测或链路选择等任务。 |
| [^146] | [Discovery and Exploitation of Generalized Network Effects.](http://arxiv.org/abs/2301.00270) | 我们提出了NetEffect，一种图挖掘方法，能够识别和理解具有少量节点标签的大型图中的广义网络效应（如同质性、异质性或二者的组合），并利用这些效应来改进下游任务的准确性和效率。 |
| [^147] | [A Bayesian Framework for Digital Twin-Based Control, Monitoring, and Data Collection in Wireless Systems.](http://arxiv.org/abs/2212.01351) | 本文提出了一个基于贝叶斯框架的无线系统数字孪生控制、监测和数据采集。该框架可以在数字孪生中量化和处理由于数据量和质量的限制导致的模型不确定性。 |
| [^148] | [Safe and Efficient Reinforcement Learning Using Disturbance-Observer-Based Control Barrier Functions.](http://arxiv.org/abs/2211.17250) | 本论文提出了一种使用扰动观测器和控制屏障函数进行安全高效增强学习的方法，与现有方法相比，该方法不需要模型学习，利用扰动观测器准确估计不确定性的点值，并将其纳入鲁棒的CBF条件中生成安全动作。 |
| [^149] | [Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model.](http://arxiv.org/abs/2211.14573) | 本研究提出了一种名为DeCurvEd的新方法，以可交换的方式确定潜空间中的语义交错矢量场，从而提供了高质量的图像编辑方案。 |
| [^150] | [Efficient Representation of Natural Image Patches.](http://arxiv.org/abs/2210.13004) | 通过抽象模型，研究人员展示了如何通过非线性种群码实现自然图像块的高效表示，以实现早期视觉系统的信息传输和传感器概率分布建模的目标。 |
| [^151] | [Multi-Parameter Performance Modeling via Tensor Completion.](http://arxiv.org/abs/2210.10184) | 本论文采用低秩张量分解来建模应用程序性能，通过对应用程序执行时间的逼近，实现对未观测区域的精确外推，并应用张量完成算法优化低秩正交-多项式（CP）分解，从而提高了预测准确性和存储效率。 |
| [^152] | [Policy Gradient for Reinforcement Learning with General Utilities.](http://arxiv.org/abs/2210.00991) | 本文研究了强化学习中具有通用效用的策略梯度。传统的线性强化学习方法无法直接应用于非线性效用的问题，而本文推导出的策略梯度定理为解决这个问题提供了新的思路和方法。 |
| [^153] | [Deep Learning Based Residuals in Non-linear Factor Models: Precision Matrix Estimation of Returns with Low Signal-to-Noise Ratio.](http://arxiv.org/abs/2209.04512) | 本文介绍了在深度学习框架中使用非线性因子模型对大型投资组合中资产回报的精确矩阵进行一致估计和收敛速度。我们的方法不仅适用于金融市场典型的低信噪比环境，还与弱因子兼容，并且通过理论分析建立了统一的界限，同时提供了基于数据的一致误差协方差估计方法。模拟和实证结果显示我们的模型具有卓越的准确性。 |
| [^154] | [An Analysis of Abstracted Model-Based Reinforcement Learning.](http://arxiv.org/abs/2208.14407) | 本论文分析了抽象模型驱动的强化学习中的问题，揭示抽象状态会引入样本相关性，而使用鞅不等式可以解决这个问题，从而将现有MBRL算法的保证扩展到带有抽象的设置。 |
| [^155] | [Group Equality in Adaptive Submodular Maximization.](http://arxiv.org/abs/2207.03364) | 本论文研究了非自适应和自适应情况下，受群体平等约束的经典子模块最大化问题。研究发现，现有算法没有考虑公平性约束，导致一些特定群体的欠代表或过代表。因此，我们研究了在群体平等约束下选择一组项以最大化子模块效用函数。 |
| [^156] | [Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-\L{}ojasiewicz Functions when the Non-Convexity is Averaged-Out.](http://arxiv.org/abs/2206.11872) | 本研究开发了新的技术，通过分析Hessian矩阵的变化，实现了一类Polyak-\L{}ojasiewicz函数在非凸性被平均处理时的可证明加速，对于现代机器学习模型的训练具有重要意义。 |
| [^157] | [PRANC: Pseudo RAndom Networks for Compacting deep models.](http://arxiv.org/abs/2206.08464) | PRANC是一种用于压缩深度模型的框架，通过将深度模型重新参数化为多个随机初始化的基础网络的线性组合来实现。 PRANC可以显著减小深度模型的大小，并解决了存储和传输深度模型的挑战。在图像分类任务中，PRANC表现出比基线方法更好的性能。 |
| [^158] | [Strategic Coalition for Data Pricing in IoT Data Markets.](http://arxiv.org/abs/2206.07785) | 本文提出了一个智能物联网数据市场的战略联盟，通过博弈论的分析，基于数据价值控制数据价格，并通过高效算法解决了参与度的挑战，建立了数据市场，并最大化了社交效应。 |
| [^159] | [Recurrent segmentation meets block models in temporal networks.](http://arxiv.org/abs/2205.09862) | 本文通过将时间线分割成多个段并用泊松过程来建模边的参数，对时间网络中的循环活动进行建模。 |
| [^160] | [Differentially Private Sampling from Rashomon Sets, and the Universality of Langevin Diffusion for Convex Optimization.](http://arxiv.org/abs/2204.01585) | 本文提供了一种基于Langevin扩散的算法框架，可以同时实现差分隐私抽样和紧密均匀稳定性保证，从而对凸优化中的损失函数提供了最优的过度经验和总体风险保证。该框架还允许设计差分隐私均匀抽样器，应用于可解释和鲁棒机器学习等方向。 |
| [^161] | [Preserving Privacy and Security in Federated Learning.](http://arxiv.org/abs/2202.03402) | 该论文提出了一个原则性的框架，旨在在联合学习中实现用户隐私保证和防御攻击，并采用了安全的聚合协议和零知识证明协议来解决隐私和安全问题。 |
| [^162] | [AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization.](http://arxiv.org/abs/2201.06714) | AdaTerm是一种自适应T分布估计稳健矩的方法，提供了对优化算法的统一处理。 |
| [^163] | [Incomplete Multi-View Weak-Label Learning.](http://arxiv.org/abs/2201.01079) | 本文提出了一种解决多视图多标签学习中存在的不完全特征和标签、噪声视图和标签不平衡问题的新方法。该方法通过将不完全的视图和弱标签嵌入到低维子空间中，并通过自适应的权重和嵌入矩阵差异来减少冗余。实验证实了该方法的有效性。 |
| [^164] | [An Empirical Investigation of the Role of Pre-training in Lifelong Learning.](http://arxiv.org/abs/2112.09153) | 这项研究通过对大型预训练模型在多个任务上的性能评估，发现通用的前期训练可以在终身学习中减轻灾难性遗忘的影响。 |
| [^165] | [On the Existence of the Adversarial Bayes Classifier (Extended Version).](http://arxiv.org/abs/2112.01694) | 本篇论文研究了对抗训练健壮性下Bayes最优分类器的存在性问题，提出了一般性的充分条件，并可以为研究对抗性代理损失和其一致性属性提供有用的工具。 |
| [^166] | [Human-Inspired Multi-Agent Navigation using Knowledge Distillation.](http://arxiv.org/abs/2103.10000) | 本文提出了一种基于知识蒸馏的框架，用于在多智能体环境中训练智能体采取人类般的碰撞避免策略。实验证明，通过该方法训练的智能体可以超越专家和未经知识蒸馏训练的智能体，在碰撞避免和目标导向转向任务中表现出优异的能力。 |
| [^167] | [Variational Inference for Deblending Crowded Starfields.](http://arxiv.org/abs/2102.02409) | 我们提出了一种名为StarNet的贝叶斯方法，用于在拥挤星场中分离光源。StarNet利用了变分推断的最新进展，并在实验中比其他竞争方法更准确。 |
| [^168] | [Combinatorial Pure Exploration with Full-bandit Feedback and Beyond: Solving Combinatorial Optimization under Uncertainty with Limited Observation.](http://arxiv.org/abs/2012.15584) | 该论文研究了在组合优化问题中，当输入参数不确定或最初未知时，通过全带回馈的组合纯探索（CPE）来解决该不确定性的问题。以往的研究主要关注半赌博机反馈或假设每个边的结果始终可以访问，而这篇论文考虑了强反馈信息不一定可用的实际约束。 |
| [^169] | [Ballistocardiogram artifact removal in simultaneous EEG-fMRI using generative adversarial network.](http://arxiv.org/abs/2011.01710) | 本文提出了一种使用生成对抗网络在同时进行的EEG-fMRI中去除心动揭示现象的方法，通过优化网络模型的局部表示能力，改善整体性能，并获得一个可靠的生成器。该方法不需要额外的参考信号或复杂的硬件设备。 |
| [^170] | [Bayesian Feature Selection in Joint Quantile Time Series Analysis.](http://arxiv.org/abs/2010.01654) | 本文提出了一种贝叶斯特征选择方法，用于高维联合分位数时间序列分析。模型具有灵活性，可以根据每个时间序列的需要进行特征的选择，并且允许进行即时预测。 |
| [^171] | [Second-order Conditional Gradient Sliding.](http://arxiv.org/abs/2002.08907) | 提出了一种二阶条件梯度滑动（SOCGS）算法，可以高效解决约束二次凸优化问题，并在有限次线性收敛迭代后二次收敛于原始间隙。 |
| [^172] | [Logarithmic Regret in Multisecretary and Online Linear Programming Problems with Continuous Valuations.](http://arxiv.org/abs/1912.08917) | 本文研究了一个收益管理问题，其中顾客的到达是连续的，有限均值连续分布的效用值和有界离散或连续分布的库存量。研究发现，如果初始库存量与顾客数成线性比例，随着顾客数的增加，预期的遗憾将以对数的速度增长。 |
| [^173] | [Semi-supervised Vector-valued Learning: Improved Bounds and Algorithms.](http://arxiv.org/abs/1909.04883) | 该论文提出了一种针对半监督矢量值学习的改进算法，通过利用局部Rademacher复杂度和无标签数据，得出了更精确的超出风险界限。实验结果表明，该算法在多个领域中明显优于其他方法。 |

# 详细

[^1]: 用于鲁棒性域外预测的3D对抗增强

    3D Adversarial Augmentations for Robust Out-of-Domain Predictions. (arXiv:2308.15479v1 [cs.CV])

    [http://arxiv.org/abs/2308.15479](http://arxiv.org/abs/2308.15479)

    本文提出了一种用于改善模型对域外数据的泛化能力的方法，通过增加训练集中的对抗样本来实现。在3D语义分割任务中，该方法有效地防止了对非标准对象点的错误关联。这种方法通过学习一组向量来对对象进行对抗性变形，并通过约束保持它们的合理性和平滑性。

    

    由于真实世界的训练数据集无法正确采样基础数据分布的长尾部分，角落案例和稀有的域外样本会严重影响最先进模型的性能。对于稠密任务，例如3D语义分割，这个问题变得更加严重，因为非标准对象的点可以被错误地关联到其他类别。在这项工作中，我们着重改善对域外数据的泛化能力。我们通过增加训练集中的对抗样本来实现这一点。首先，我们学习一组向量，以对抗方式改变对象。为了防止对抗样本远离现有数据分布过远，我们通过一系列约束来保持它们的合理性，确保传感器感知和形状平滑性。然后，在训练模型时，我们将学习到的独立于样本的向量应用于可用的对象，进行对抗增强。我们进行了大量实验。

    Since real-world training datasets cannot properly sample the long tail of the underlying data distribution, corner cases and rare out-of-domain samples can severely hinder the performance of state-of-the-art models. This problem becomes even more severe for dense tasks, such as 3D semantic segmentation, where points of non-standard objects can be confidently associated to the wrong class. In this work, we focus on improving the generalization to out-of-domain data. We achieve this by augmenting the training set with adversarial examples. First, we learn a set of vectors that deform the objects in an adversarial fashion. To prevent the adversarial examples from being too far from the existing data distribution, we preserve their plausibility through a series of constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform adversarial augmentation by applying the learned sample-independent vectors to the available objects when training a model. We conduct extensive expe
    
[^2]: 神经网络的自适应切向特征视角

    An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])

    [http://arxiv.org/abs/2308.15478](http://arxiv.org/abs/2308.15478)

    本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。

    

    为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在切向特征空间中的线性模型，其中特征在训练过程中可以进行转换。我们考虑特征的线性变换，从而通过双线性插值约束在参数和变换上进行联合优化。我们证明了这个优化问题与具有结构化正则化的等价线性约束优化问题具有近似低秩解的关系。通过将其应用于神经网络结构，我们对特征以及核函数的变化获得了更深入的理解，为当目标函数在切向特征上表征不好时的核对齐现象提供了额外的细微差别。除了在简单回归问题上验证我们的理论观察结果外，我们还通过实验证明了切向特征分类的自适应特征实现方法。

    In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat
    
[^3]: 强化学习中的政策组合通过多目标政策优化

    Policy composition in reinforcement learning via multi-objective policy optimization. (arXiv:2308.15470v1 [cs.LG])

    [http://arxiv.org/abs/2308.15470](http://arxiv.org/abs/2308.15470)

    通过多目标政策优化方法，我们将预先存在的教师策略引入到强化学习中，证明了教师策略在无形状奖励下能够加速学习过程。我们的智能体成功地组合教师策略并扩展教师的策略以解决任务。

    

    我们通过使用相关的预先存在的教师策略，使强化学习智能体能够学习成功的行为策略。教师策略被引入作为目标，除了任务目标以外，在多目标政策优化的设置中。我们使用多目标最大后验政策优化算法，展示了教师策略能够加速学习的过程，尤其是在缺少形状奖励的情况下。在具有连续观察和行动空间的两个域中，我们的智能体成功地按顺序和并行地组合教师策略，并且还能够进一步扩展教师的策略以解决任务。根据任务和教师的指定组合，教师可能自然地限制智能体的最终性能。智能体需要遵守教师策略的程度由超参数决定，这些超参数确定了教师策略的影响程度。

    We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.  Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of te
    
[^4]: 输入界限也可以预测泛化性。

    Input margins can predict generalization too. (arXiv:2308.15466v1 [cs.LG])

    [http://arxiv.org/abs/2308.15466](http://arxiv.org/abs/2308.15466)

    本研究发现当搜索空间适当约束时，输入界限可预测泛化性能，且与隐藏表示界限相比取得了极具竞争力的结果。

    

    深度神经网络中的泛化性如何理解是一个积极研究的领域。一种有前景的探索方法是界限测量：给定样本或其在网络内的表示到决策边界的最短距离。虽然已经显示了在隐藏表示（隐藏界限）中测量时界限与模型的泛化能力相关，但尚未建立起输入界限与泛化之间的联系。我们表明，虽然输入界限通常不能预测泛化，但如果适当地约束搜索空间，它们可以起到预测作用。我们基于输入界限开发了这样一种度量，称之为“受限界限”。我们展示了这种新度量的预测能力，与隐藏表示界限进行了对比，并在“深度学习中预测泛化性”（PGDL）数据集上进行了验证。我们发现，受限界限取得了极具竞争力的分数和优势。

    Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and ou
    
[^5]: 损失函数的比较研究：常规和拥堵情景下的交通预测

    A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios. (arXiv:2308.15464v1 [cs.LG])

    [http://arxiv.org/abs/2308.15464](http://arxiv.org/abs/2308.15464)

    本文通过研究多种受重尾分析和不平衡分类问题启发的损失函数，提出了改进交通预测中拥堵情况的准确性的方法，并分别提出了针对不同优化目标的最佳选择。

    

    时空图神经网络在交通预测方面取得了最先进的性能。然而，由于传统损失函数的局限性，它们往往难以准确预测拥堵情况。准确预测常规交通条件至关重要，但可靠的人工智能系统还必须准确预测拥堵情景，以维持安全和高效的交通。在本文中，我们探讨了受重尾分析和不平衡分类问题启发的各种损失函数，以解决这个问题。我们在交通速度预测方面评估了这些损失函数的有效性，重点关注拥堵情况。通过对真实交通数据集进行广泛实验，我们发现在优化平均绝对误差（MAE）时，MAE-Focal Loss函数表现最为有效。在优化均方误差（MSE）时，Gumbel Loss被证明是更优的选择。这些选择可以有效地预测交通拥堵。

    Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic conges
    
[^6]: 混合神经网络领域的规范因素

    Canonical Factors for Hybrid Neural Fields. (arXiv:2308.15461v1 [cs.CV])

    [http://arxiv.org/abs/2308.15461](http://arxiv.org/abs/2308.15461)

    该研究对混合神经网络领域的规范因素进行了研究，发现因子特征容量虽然简单高效，但存在不良偏差。通过学习一组规范化变换，该方法成功消除偏差，并在图像、距离和辐射场重建任务中实现了质量、鲁棒性和运行时间的改进。

    

    因子特征容量提供了一种简单的方法来构建更紧凑、高效和可解释的神经网络领域，但也引入了不一定有益于真实世界数据的偏差。在这项工作中，我们（1）对这些体系结构对齐轴信号的不良偏差进行了表征 - 它们可以导致辐射场重建的差异高达2 PSNR - 并（2）探索了通过学习一组规范化变换来提高表示的方法，从而消除这些偏差。在一个二维模型问题中，我们证明同时学习这些变换以及场景外观可以以大大提高的效率成功。我们使用图像、有符号距离和辐射场重建任务验证了所得到的体系结构，我们观察到质量、鲁棒性、紧凑性和运行时间方面的改进。结果表明，TILTED可以实现与基线相当的能力，而基线是2倍大。

    Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x lar
    
[^7]: 从SMOTE到Mixup用于深度不平衡分类

    From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])

    [http://arxiv.org/abs/2308.15457](http://arxiv.org/abs/2308.15457)

    本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。

    

    鉴于不平衡的数据，使用深度学习训练好的分类器因为少数类的泛化能力差而困难重重。传统上，用于数据增强的知名少数类合成过采样技术（SMOTE），作为一种面向不平衡学习的数据挖掘方法，被用来改善这种泛化。然而，SMOTE在深度学习中是否也有益处仍不清楚。在这项工作中，我们研究了为什么原始的SMOTE对深度学习来说是不足的，并使用软标签增强了SMOTE。将得到的软SMOTE与Mixup，一种现代数据增强技术，连接在一起，形成了一个统一的框架，将传统和现代的数据增强技术纳入同一个范畴。在这个框架中进行系统研究表明，Mixup通过隐式地实现多数类和少数类之间的不平衡间隙来改善泛化能力。然后，我们提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。大量的实验表明，我们的方法在多个数据集上相对于最先进的算法都取得了最好的性能。

    Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
    
[^8]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^9]: 针对一般谱方法的随机特征逼近

    Random feature approximation for general spectral methods. (arXiv:2308.15434v1 [cs.LG])

    [http://arxiv.org/abs/2308.15434](http://arxiv.org/abs/2308.15434)

    本论文研究了随机特征逼近在一般谱方法中的应用，分析了与随机特征相结合的谱正则化方法的泛化性质，并获得了在不同正则性类别中的最佳学习速率。

    

    随机特征逼近被认为是在大规模算法中加速核方法的最流行技术之一，并且为深度神经网络的分析提供了一种理论方法。我们分析了与随机特征相结合的一大类谱正则化方法的泛化性质，其中包括使用隐式正则化（如梯度下降）或显式方法（如Tikhonov正则化）的核方法。对于我们的估计器，我们在适当的源条件下定义了正则性类别，从而获得了最佳的学习速率。这改进或完善了以前在特定核算法相关设置中获得的结果。

    Random feature approximation is arguably one of the most popular techniques to speed up kernel methods in large scale algorithms and provides a theoretical approach to the analysis of deep neural networks. We analyze generalization properties for a large class of spectral regularization methods combined with random features, containing kernel methods with implicit regularization such as gradient descent or explicit methods like Tikhonov regularization. For our estimators we obtain optimal learning rates over regularity classes (even for classes that are not included in the reproducing kernel Hilbert space), which are defined through appropriate source conditions. This improves or completes previous results obtained in related settings for specific kernel algorithms.
    
[^10]: 使用历史磁场图数据进行概率性太阳耀斑预测

    Probabilistic solar flare forecasting using historical magnetogram data. (arXiv:2308.15410v1 [astro-ph.SR])

    [http://arxiv.org/abs/2308.15410](http://arxiv.org/abs/2308.15410)

    这项研究使用历时太阳磁场图数据，结合卷积神经网络和逻辑回归模型，进行了概率性太阳耀斑预测，发现包含历史数据可以提高预测准确性和可靠性。

    

    使用机器学习进行太阳耀斑预测的研究集中在SDO/HMI时代覆盖太阳第24个周日和第25个周日开始的高分辨率磁场图数据上，一些研究还回顾了来自太阳第23个周日的SOHO/MDI数据。在本文中，我们考虑了来自多个仪器的历时4个太阳周期的每日历史磁场图数据。这是首次尝试利用这些历史数据进行基于机器学习的耀斑预测。我们使用卷积神经网络（CNN）从全球磁场图中提取特征，并结合逻辑回归模型将基于磁场图和耀斑历史的标量特征纳入模型。我们采用集成方法生成在接下来的24小时内M级或更大耀斑的校准概率预测。总体而言，我们发现包含历史数据可以提高耀斑预测的准确性和可靠性。我们还展示了单帧磁场图并不包含比我们的模型能够提供的更多相关信息。

    Solar flare forecasting research using machine learning (ML) has focused on high resolution magnetogram data from the SDO/HMI era covering Solar Cycle 24 and the start of Solar Cycle 25, with some efforts looking back to SOHO/MDI for data from Solar Cycle 23. In this paper, we consider over 4 solar cycles of daily historical magnetogram data from multiple instruments. This is the first attempt to take advantage of this historical data for ML-based flare forecasting. We apply a convolutional neural network (CNN) to extract features from full-disk magnetograms together with a logistic regression model to incorporate scalar features based on magnetograms and flaring history. We use an ensemble approach to generate calibrated probabilistic forecasts of M-class or larger flares in the next 24 hours. Overall, we find that including historical data improves forecasting skill and reliability. We show that single frame magnetograms do not contain significantly more relevant information than can
    
[^11]: 通过标签感知有界CVaR实现鲁棒的长尾学习

    Robust Long-Tailed Learning via Label-Aware Bounded CVaR. (arXiv:2308.15405v1 [cs.LG])

    [http://arxiv.org/abs/2308.15405](http://arxiv.org/abs/2308.15405)

    本文提出了两种基于CVaR的方法来改进长尾学习的性能，并具备坚实的理论基础。第一种方法是引入了一种标签感知有界CVaR（LAB-CVaR）损失函数，解决了原始CVaR的悲观结果，通过设计最优权重上下界进行改进；第二种方法是提出了一种带有logit调整的LAB-CVaR（LAB-CVaR-logit）损失函数，以稳定优化过程。

    

    在真实世界的分类问题中，数据往往是不平衡或长尾分布的，其中大多数类别拥有大部分样本，并主导模型训练。在这种情况下，普通模型往往在少数类上表现较差。之前，已经提出了各种损失函数修正方法来解决长尾学习问题，然而这些方法要么对同一类别的样本漠不关心，要么缺乏理论保证。本文提出了两种基于CVaR（条件价值-at-Risk）的新方法来提高长尾学习的性能并具备坚实的理论基础。具体而言，我们首先引入了一种标签感知有界CVaR（LAB-CVaR）损失函数来克服原始CVaR悲观的结果，并在理论上设计了LAB-CVaR的最优权重上下界。基于LAB-CVaR，我们进一步提出了一种带有logit调整的LAB-CVaR（LAB-CVaR-logit）损失函数，以稳定优化过程。

    Data in the real-world classification problems are always imbalanced or long-tailed, wherein the majority classes have the most of the samples that dominate the model training. In such setting, the naive model tends to have poor performance on the minority classes. Previously, a variety of loss modifications have been proposed to address the long-tailed leaning problem, while these methods either treat the samples in the same class indiscriminatingly or lack a theoretical guarantee. In this paper, we propose two novel approaches based on CVaR (Conditional Value at Risk) to improve the performance of long-tailed learning with a solid theoretical ground. Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization pro
    
[^12]: CausalBench挑战赛：基于单细胞干扰数据的基因网络推断的机器学习竞赛

    The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data. (arXiv:2308.15395v1 [cs.LG])

    [http://arxiv.org/abs/2308.15395](http://arxiv.org/abs/2308.15395)

    CausalBench挑战赛是一个机器学习竞赛，旨在构建基因网络推断的最新方法。参与者利用大规模遗传干扰数据提升了先进方法的性能。

    

    在药物发现中，绘制细胞系统内基因之间的相互作用关系是关键的早期步骤。这有助于制定关于可能被未来药物靶向的分子机制的假设。CausalBench挑战是一项邀请机器学习社区来推进构建基因-基因相互作用网络的最新技术的倡议。这些网络是从大规模真实世界的单细胞数据集中推导出来的，这些数据集经过不同类型的干扰。这些网络对于理解疾病生物学的因果机制至关重要。参与者利用CausalBench基准测试框架，任务是提升先进方法利用大规模遗传干扰数据的能力。本报告分析和总结了挑战赛期间提交的方法，以描绘竞赛期间技术发展的部分情况。

    In drug discovery, mapping interactions between genes within cellular systems is a crucial early step. This helps formulate hypotheses regarding molecular mechanisms that could potentially be targeted by future medicines. The CausalBench Challenge was an initiative to invite the machine learning community to advance the state of the art in constructing gene-gene interaction networks. These networks, derived from large-scale, real-world datasets of single cells under various perturbations, are crucial for understanding the causal mechanisms underlying disease biology. Using the framework provided by the CausalBench benchmark, participants were tasked with enhancing the capacity of the state of the art methods to leverage large-scale genetic perturbation data. This report provides an analysis and summary of the methods submitted during the challenge to give a partial image of the state of the art at the time of the challenge. The winning solutions significantly improved performance compa
    
[^13]: 分布式能量存储系统的分散式多智能体强化学习电荷平衡策略

    Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System. (arXiv:2308.15394v1 [cs.AI])

    [http://arxiv.org/abs/2308.15394](http://arxiv.org/abs/2308.15394)

    本文提出了一种基于分散式多智能体强化学习的电荷平衡策略，通过分布式方法解决了分布式能量存储系统中的电荷平衡问题，并通过平均一致性算法来进行优化。

    

    本文提出了一种分散式多智能体强化学习（Dec-MARL）方法，用于解决分布式能量存储系统中的电荷平衡问题。首先，将电荷平衡问题转化为一个有限马尔可夫决策过程，并根据需求平衡得到的动作约束，采用Dec-MARL来解决该问题。具体而言，利用一阶平均一致性算法以完全分散的方式扩展分布式能量存储系统状态的观测，并根据这些观测决定初始动作（即输出功率）。为了得到允许范围内的最终动作，提出了一个反事实需求平衡算法来平衡总需求和初始动作。然后，智能体执行最终动作并从环境中获得局部奖励，使系统进入下一个状态。最后，通过一阶平均一致性算法，智能体获得平均奖励，并通过迭代更新策略来不断优化电荷平衡策略。

    This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the avera
    
[^14]: 基于形状边缘知识增强网络的甲状腺结节分割和诊断

    Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation and Diagnosis. (arXiv:2308.15386v1 [eess.IV])

    [http://arxiv.org/abs/2308.15386](http://arxiv.org/abs/2308.15386)

    本文提出了一种基于形状边缘知识增强网络（SkaNet）的甲状腺结节分割和诊断方法，通过探索甲状腺结节分割和诊断之间的关系，实现了两个任务的一体化。该方法基于甲状腺影像报告和数据系统（TI-RADS），利用形状和边缘特征来区分良性和恶性甲状腺结节。

    

    甲状腺结节分割是医生和计算机辅助诊断系统诊断过程中的关键步骤。目前的研究大多将分割和诊断作为独立的任务，没有考虑这些任务之间的相关性。计算机辅助诊断系统中独立任务的顺序步骤可能会导致误差的累积。因此，值得通过探索甲状腺结节分割和诊断之间的关系将它们作为一个整体进行组合。根据甲状腺影像报告和数据系统（TI-RADS），形状和边缘特征的评估是区分良性和恶性甲状腺结节的先决条件。这些特征可以在甲状腺结节分割掩模中观察到。受到TI-RADS诊断过程的启发，本文提出了一种用于同时进行甲状腺结节分割和诊断的形状边缘知识增强网络（SkaNet）。

    Thyroid nodule segmentation is a crucial step in the diagnostic procedure of physicians and computer-aided diagnosis systems. Mostly, current studies treat segmentation and diagnosis as independent tasks without considering the correlation between these tasks. The sequence steps of these independent tasks in computer-aided diagnosis systems may lead to the accumulation of errors. Therefore, it is worth combining them as a whole through exploring the relationship between thyroid nodule segmentation and diagnosis. According to the thyroid imaging reporting and data system (TI-RADS), the assessment of shape and margin characteristics is the prerequisite for the discrimination of benign and malignant thyroid nodules. These characteristics can be observed in the thyroid nodule segmentation masks. Inspired by the diagnostic procedure of TI-RADS, this paper proposes a shape-margin knowledge augmented network (SkaNet) for simultaneously thyroid nodule segmentation and diagnosis. Due to the sim
    
[^15]: 多响应异方差高斯过程模型及其推断

    Multi-Response Heteroscedastic Gaussian Process Models and Their Inference. (arXiv:2308.15370v1 [stat.ML])

    [http://arxiv.org/abs/2308.15370](http://arxiv.org/abs/2308.15370)

    本文介绍了多响应异方差高斯过程模型，将其应用于回归、分类和状态空间模型，并提出了一种利用变分推断来近似后验的方法，解决了高斯过程模型在捕捉函数平滑性的突变和适应异方差错误方面的局限性。

    

    尽管高斯过程模型被广泛用于灵活的非参数建模，但它们在有效捕捉函数平滑性的突变和适应异方差错误方面存在局限性。为了解决这些问题，异方差高斯过程（HeGP）回归旨在通过承认回归模型中协变量间残差方差的可变性来引入灵活性。本文将HeGP概念扩展到分类和状态空间模型，并提出了一种新的框架，将高斯过程与协变量诱导的精度矩阵过程相结合，采用混合形式。这种方法使得可以对协变量之间的异方差协方差函数进行建模。为了解决采样带来的计算挑战，我们采用变分推断来近似后验并便利计算。

    Despite the widespread utilization of Gaussian process models for versatile nonparametric modeling, they exhibit limitations in effectively capturing abrupt changes in function smoothness and accommodating relationships with heteroscedastic errors. Addressing these shortcomings, the heteroscedastic Gaussian process (HeGP) regression seeks to introduce flexibility by acknowledging the variability of residual variances across covariates in the regression model. In this work, we extend the HeGP concept, expanding its scope beyond regression tasks to encompass classification and state-space models. To achieve this, we propose a novel framework where the Gaussian process is coupled with a covariate-induced precision matrix process, adopting a mixture formulation. This approach enables the modeling of heteroscedastic covariance functions across covariates. To mitigate the computational challenges posed by sampling, we employ variational inference to approximate the posterior and facilitate p
    
[^16]: 通过客户端特定的提示生成，在联邦学习中实现高效的模型个性化

    Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation. (arXiv:2308.15367v1 [cs.CV])

    [http://arxiv.org/abs/2308.15367](http://arxiv.org/abs/2308.15367)

    pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL.

    

    联邦学习（FL）作为一种分散的学习框架，可以在不共享数据以保护隐私的情况下，从多个分布式客户端训练模型。最近，大型预训练模型（如Vision Transformer）展示了从分布式客户端中获得稳健表示的强大能力。然而，客户端之间数据的异质性、有限的计算资源和通信带宽限制了大型模型在FL框架中的部署。为了利用大型模型的稳健表示，同时实现对异构客户端的高效模型个性化，我们提出了一种新的个性化FL框架，即客户端特定的提示生成（pFedPG），它学习在服务器端部署个性化提示生成器，用以产生适应本地数据分布的客户端特定视觉提示，从而有效地将冻结的骨干网络适应到本地数据分布。

    Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personal
    
[^17]: 异构多任务高斯Cox过程

    Heterogeneous Multi-Task Gaussian Cox Processes. (arXiv:2308.15364v1 [cs.LG])

    [http://arxiv.org/abs/2308.15364](http://arxiv.org/abs/2308.15364)

    本文提出了一种用于联合建模多个异构相关任务的扩展方法，通过多输出高斯过程（MOGP）促进任务之间的信息共享和非参数参数估计，并应用数据增广技术来解决非共轭贝叶斯推断问题。

    

    本文提出了一种新颖的多任务高斯Cox过程的扩展，用于联合建模多个异构相关任务，例如分类和回归，通过多输出高斯过程（MOGP）。通过对分类、回归和点过程任务的专用似然参数引入MOGP先验，可以促进异构任务之间的信息共享，同时允许非参数参数估计。为了克服MOGP调制的异构多任务框架中的非共轭贝叶斯推断问题，我们采用数据增广技术，并导出均场近似来实现对模型参数的闭式迭代更新。我们在1D合成数据和温哥华的2D城市数据上展示了性能和推断。

    This paper presents a novel extension of multi-task Gaussian Cox processes for modeling multiple heterogeneous correlated tasks jointly, e.g., classification and regression, via multi-output Gaussian processes (MOGP). A MOGP prior over the parameters of the dedicated likelihoods for classification, regression and point process tasks can facilitate sharing of information between heterogeneous tasks, while allowing for nonparametric parameter estimation. To circumvent the non-conjugate Bayesian inference in the MOGP modulated heterogeneous multi-task framework, we employ the data augmentation technique and derive a mean-field approximation to realize closed-form iterative updates for estimating model parameters. We demonstrate the performance and inference on both 1D synthetic data as well as 2D urban data of Vancouver.
    
[^18]: 大语言模型赋能文本到SQL的研究：一个基准评估

    Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])

    [http://arxiv.org/abs/2308.15363](http://arxiv.org/abs/2308.15363)

    本文提出了一个大规模语言模型(LLMs)赋能的文本到SQL任务的基准评估，并基于实验结果提出了一种新的集成解决方案DAIL-SQL，刷新了Spider榜单并实现了86.6%的执行准确率。同时，强调了在提示工程中的词汇效率以实现高效经济的LLM-based文本到SQL解决方案，此外还对在上下文学习中应用开源LLMs进行了研究，并进行了任务特定的性能优化。

    

    大语言模型(LLMs)已经成为文本到SQL任务的一种新范式。然而，缺乏一个系统性的基准阻碍了设计有效、高效和经济的LLM-based文本到SQL解决方案的发展。为了解决这一挑战，本文首先对现有的提示工程方法进行了系统性和广泛的比较，包括问题表示、示例选择和示例组织，并根据实验结果详细阐述了它们的优缺点。基于这些发现，我们提出了一种新的集成解决方案，名为DAIL-SQL，刷新了Spider榜单，达到了86.6%的执行准确率，建立了一个新的标杆。为了实现高效经济的LLM-based文本到SQL解决方案，我们强调提示工程中的词汇效率，并在此度量下比较了之前的研究。此外，我们还研究了上下文学习中的开源LLMs，并用任务特定的监督进行了进一步的性能优化。

    Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
    
[^19]: Lie-Poisson神经网络（LPNets）：基于数据的具有对称性的Hamiltonian系统的计算

    Lie-Poisson Neural Networks (LPNets): Data-Based Computing of Hamiltonian Systems with Symmetries. (arXiv:2308.15349v1 [cs.LG])

    [http://arxiv.org/abs/2308.15349](http://arxiv.org/abs/2308.15349)

    该论文提出了Lie-Poisson神经网络（LPNets），用于基于数据计算具有对称性的Hamiltonian系统。该网络通过准确保持Poisson括号和特殊函数来预测系统的长期演化。

    

    准确地基于数据预测Hamiltonian系统的长期演化需要一个在每个时间步保持适当结构的网络。每个Hamiltonian系统都包含两个基本要素：Poisson括号和Hamiltonian。具有对称性的Hamiltonian系统，其典型例子是Lie-Poisson系统，已被证明能描述从卫星运动到水下车辆、流体、地球物理应用、复杂流体和等离子物理等广泛的物理现象。在这些系统中，Poisson括号来自对称性，而Hamiltonian来自底层物理。我们将系统的对称性视为首要，因此Lie-Poisson括号被准确知晓，而Hamiltonian则被视为来自物理且可能不知晓或近似已知。基于这种方法，我们开发了一个基于变换的网络，能够准确保持Poisson括号和特殊函数。

    An accurate data-based prediction of the long-term evolution of Hamiltonian systems requires a network that preserves the appropriate structure under each time step. Every Hamiltonian system contains two essential ingredients: the Poisson bracket and the Hamiltonian. Hamiltonian systems with symmetries, whose paradigm examples are the Lie-Poisson systems, have been shown to describe a broad category of physical phenomena, from satellite motion to underwater vehicles, fluids, geophysical applications, complex fluids, and plasma physics. The Poisson bracket in these systems comes from the symmetries, while the Hamiltonian comes from the underlying physics. We view the symmetry of the system as primary, hence the Lie-Poisson bracket is known exactly, whereas the Hamiltonian is regarded as coming from physics and is considered not known, or known approximately. Using this approach, we develop a network based on transformations that exactly preserve the Poisson bracket and the special funct
    
[^20]: 从图像边界进行的对深度神经网络的不可察觉对抗攻击

    Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary. (arXiv:2308.15344v1 [cs.LG])

    [http://arxiv.org/abs/2308.15344](http://arxiv.org/abs/2308.15344)

    本文提出了一种对深度神经网络进行的不可察觉对抗攻击，通过系统性地攻击输入图像边界来查找对抗性样本。实验结果表明，边界可以主导DNN模型的行为。

    

    虽然深度神经网络（DNNs），例如卷积神经网络（CNN）和视觉转换器（ViTs），已成功应用于计算机视觉领域，但它们被证明容易被伪装成对抗性样本（AEs）所欺骗。对AE的研究一直很活跃，自从2014年发现以来已经提出了许多对抗性攻击和解释。AE存在的奥秘仍然是一个未解之谜，许多研究表明DNN训练算法存在盲区。显著对象通常不与边界重叠，因此边界不是DNN模型的关注点。然而，最近的研究表明，边界可以主导DNN模型的行为。因此，本研究旨在从不同的角度观察AE，并提出一种对输入图像边界进行系统性攻击以寻找AE的不可察觉的对抗攻击。实验结果表明，

    Although Deep Neural Networks (DNNs), such as the convolutional neural networks (CNN) and Vision Transformers (ViTs), have been successfully applied in the field of computer vision, they are demonstrated to be vulnerable to well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The research in AEs has been active, and many adversarial attacks and explanations have been proposed since they were discovered in 2014. The mystery of the AE's existence is still an open question, and many studies suggest that DNN training algorithms have blind spots. The salient objects usually do not overlap with boundaries; hence, the boundaries are not the DNN model's attention. Nevertheless, recent studies show that the boundaries can dominate the behavior of the DNN models. Hence, this study aims to look at the AEs from a different perspective and proposes an imperceptible adversarial attack that systemically attacks the input image boundary for finding the AEs. The experimental results ha
    
[^21]: 通过学习的人类关注特征图来增强机器人学习能力

    Enhancing Robot Learning through Learned Human-Attention Feature Maps. (arXiv:2308.15327v1 [cs.RO])

    [http://arxiv.org/abs/2308.15327](http://arxiv.org/abs/2308.15327)

    本文提出了一种通过学习人类关注特征图来增强机器人学习的方法。通过模拟人类的注意力机制，并将其作为辅助特征传递给下游的学习任务，可以提高模型对于分布之外样本的稳健性和学习速度。

    

    在机器人学习中，强大而高效的学习仍然是一个具有挑战性的问题，特别是对于复杂的视觉输入。受到人类注意机制的启发，我们认为将关注焦点的辅助信息嵌入到机器人学习中，将提高学习过程的效率和稳健性。在本文中，我们提出了一种新颖的方法，用一个近似预测模型来建模和模拟人类的注意力。然后，我们将这个输出作为结构化的辅助特征图传递给下游的学习任务。我们通过在真实世界的手动驾驶中，从人类注视记录中学习预测模型来验证这个想法。我们在两个学习任务中进行了测试——目标检测和模仿学习。我们的实验表明，将预测的人类关注引入到训练模型中，可以提高模型对于分布之外样本的稳健性，同时也能实现更快的学习速度。

    Robust and efficient learning remains a challenging problem in robotics, in particular with complex visual inputs. Inspired by human attention mechanism, with which we quickly process complex visual scenes and react to changes in the environment, we think that embedding auxiliary information about focus point into robot learning would enhance efficiency and robustness of the learning process. In this paper, we propose a novel approach to model and emulate the human attention with an approximate prediction model. We then leverage this output and feed it as a structured auxiliary feature map into downstream learning tasks. We validate this idea by learning a prediction model from human-gaze recordings of manual driving in the real world. We test our approach on two learning tasks - object detection and imitation learning. Our experiments demonstrate that the inclusion of predicted human attention leads to improved robustness of the trained models to out-of-distribution samples and faster
    
[^22]: 通过均匀的Tanh变换的面部解析的遮挡感知深度卷积神经网络

    Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing. (arXiv:2308.15323v1 [cs.CV])

    [http://arxiv.org/abs/2308.15323](http://arxiv.org/abs/2308.15323)

    本论文提出了一种通过均匀的Tanh变换进行面部解析的遮挡感知深度卷积神经网络。该方法解决了面部遮挡问题，并且能够融合更多上下文信息。同时，引入了遮挡感知损失，提高了边界的识别能力。

    

    面部解析是为每个语义面部组件推断像素级标签图的过程。以前的方法通常对无遮挡的面部效果良好，但在面部遮挡下忽略了遮挡和忽视了单个面部外一些上下文区域，尤其是在COVID-19流行期间，面部遮挡已经成为一种常见情况。受图像照明理论的启发，我们提出了一种新颖的图像预处理方法，即由四个Tanh变换组成的均匀Tanh变换，将中央视觉和周边视觉融合在一起。我们的方法解决了遮挡下面部解析的困境，并压缩了更多的周围上下文信息。基于均匀的Tanh变换，我们提出了一种适用于遮挡面部解析的遮挡感知卷积神经网络。它结合了Tanh极坐标空间和Tanh笛卡尔空间中的信息，能够增强感受野。此外，我们引入了一种遮挡感知损失，专注于遮挡边界。

    Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however overlook the facial occlusion and ignore some contextual area outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the illumination theory of image, we propose a novel homogeneous tanh-transforms for image preprocessing, which made up of four tanh-transforms, that fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information of surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines the information both in Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occ
    
[^23]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^24]: 使用二进制神经网络进行设备上的学习

    On-Device Learning with Binary Neural Networks. (arXiv:2308.15308v1 [cs.LG])

    [http://arxiv.org/abs/2308.15308](http://arxiv.org/abs/2308.15308)

    本文提出了一种在低功耗设备上使用二进制神经网络进行设备端学习的解决方案，该方法结合了最新的连续学习技术和二进制神经网络的高效性。实验证实了该方法的有效性和适用性。

    

    现有的连续学习解决方案在低功耗嵌入式CPU上部署时，仅部分解决了深度学习模型在功耗、内存和计算方面的限制。本文提出了一种连续学习解决方案，结合了连续学习领域的最新进展和二进制神经网络(BNN)的高效性，BNN使用1位用于权重和激活以高效执行深度学习模型。我们提出了一种CWR*的混合量化方法（一种有效的连续学习方法），该方法在前向传递和反向传递过程中分别考虑，以在梯度更新步骤中保持更高的精度，并同时最大限度地减少延迟开销。选择二进制网络作为骨干网络对于满足低功耗设备的限制至关重要，据作者所知，这是首次尝试证明使用BNN进行设备上的学习。进行的实验验证了该方法的有效性和适用性。

    Existing Continual Learning (CL) solutions only partially address the constraints on power, memory and computation of the deep learning models when deployed on low-power embedded CPUs. In this paper, we propose a CL solution that embraces the recent advancements in CL field and the efficiency of the Binary Neural Networks (BNN), that use 1-bit for weights and activations to efficiently execute deep learning models. We propose a hybrid quantization of CWR* (an effective CL approach) that considers differently forward and backward pass in order to retain more precision during gradient update step and at the same time minimizing the latency overhead. The choice of a binary network as backbone is essential to meet the constraints of low power devices and, to the best of authors' knowledge, this is the first attempt to prove on-device learning with BNN. The experimental validation carried out confirms the validity and the suitability of the proposed method.
    
[^25]: 迈向心电图分析的定量精确性：利用状态空间模型、自监督学习和患者元数据

    Towards quantitative precision for ECG analysis: Leveraging state space models, self-supervision and patient metadata. (arXiv:2308.15291v1 [eess.SP])

    [http://arxiv.org/abs/2308.15291](http://arxiv.org/abs/2308.15291)

    这项研究利用状态空间模型(SSMs)和自监督学习来改善深度学习模型在自动心电图分析中的定量准确性。结果表明，使用SSMs可以捕捉时间序列数据中的长期依赖关系，并且在标准诊断任务中，较高的采样率和扩大输入尺寸并没有明显的改进效果。

    

    深度学习已成为自动心电图分析的首选建模方法。本研究探讨了三个旨在提高这种系统定量准确性的要素。这些组件始终超越了基于卷积模型的现有最先进技术，并通过利用结构化状态空间模型(SSMs)来提高性能。这些模型在捕捉时间序列数据中的长期依赖方面显示了很大的潜力。通过将SSMs纳入我们的方法中，我们不仅实现了更好的性能，还对该领域的长期相关问题有了更深的理解。具体而言，对于标准诊断任务，我们发现与100Hz相比，使用500Hz等更高采样率没有优势。类似地，将模型的输入尺寸扩大到3秒以上也没有带来显著的改进。其次，我们证明了使用对比预测的自监督学习能够增强模型的性能，这是第二个要素。

    Deep learning has emerged as the preferred modeling approach for automatic ECG analysis. In this study, we investigate three elements aimed at improving the quantitative accuracy of such systems. These components consistently enhance performance beyond the existing state-of-the-art, which is predominantly based on convolutional models. Firstly, we explore more expressive architectures by exploiting structured state space models (SSMs). These models have shown promise in capturing long-term dependencies in time series data. By incorporating SSMs into our approach, we not only achieve better performance, but also gain insights into long-standing questions in the field. Specifically, for standard diagnostic tasks, we find no advantage in using higher sampling rates such as 500Hz compared to 100Hz. Similarly, extending the input size of the model beyond 3 seconds does not lead to significant improvements. Secondly, we demonstrate that self-supervised learning using contrastive predictive c
    
[^26]: 使用同态计数的结构节点嵌入

    Structural Node Embeddings with Homomorphism Counts. (arXiv:2308.15283v1 [cs.LG])

    [http://arxiv.org/abs/2308.15283](http://arxiv.org/abs/2308.15283)

    本论文提出了使用同态计数的结构节点嵌入，通过实验证明了其有效性。这种嵌入在解释和理解机器学习模型方面具有优势，并可应用于各种下游任务。

    

    图同态计数是1967年由Lov\'asz首次探索的，最近在基于图的机器学习中引起了人们的兴趣。Grohe (PODS 2020)提出了在机器学习的图级别和节点级别任务中使用同态计数的理论基础。由于其本质上捕捉了局部结构信息，这使得创建稳健的结构嵌入成为可能。虽然Nguyen和Maehara (ICML 2020)已经提出了用于图级任务的第一种方法，但我们通过实验证明了基于同态计数的节点嵌入的有效性。这些嵌入与节点标签、节点权重和边权重相结合，提供了对图数据的可解释性表示，增强了机器学习模型的可解释性。我们提出了一个理论框架，用于同构不变的基于同态计数的嵌入，适用于各种下游任务。我们的方法充分利用了高效计算的特点。

    Graph homomorphism counts, first explored by Lov\'asz in 1967, have recently garnered interest as a powerful tool in graph-based machine learning. Grohe (PODS 2020) proposed the theoretical foundations for using homomorphism counts in machine learning on graph level as well as node level tasks. By their very nature, these capture local structural information, which enables the creation of robust structural embeddings. While a first approach for graph level tasks has been made by Nguyen and Maehara (ICML 2020), we experimentally show the effectiveness of homomorphism count based node embeddings. Enriched with node labels, node weights, and edge weights, these offer an interpretable representation of graph data, allowing for enhanced explainability of machine learning models.  We propose a theoretical framework for isomorphism-invariant homomorphism count based embeddings which lend themselves to a wide variety of downstream tasks. Our approach capitalises on the efficient computability 
    
[^27]: 让声音存在：从无声视频中重建高质量语音

    Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])

    [http://arxiv.org/abs/2308.15256](http://arxiv.org/abs/2308.15256)

    本文介绍了一个重建高质量语音的唇语转语音系统，通过解决一对多映射问题和细节精炼来显著改进生成质量。

    

    本研究的目标是仅通过唇运动重建高质量的语音，也被称为唇语转语音。唇语转语音系统的一个关键挑战是由于同形异音和多样化语音变化而造成的一对多映射，导致发音错误和过度平滑的语音。在本文中，我们提出了一种新颖的唇语转语音系统，通过从多个角度缓解一对多映射问题，显著改进了生成质量。具体来说，我们结合了（1）自我监督的语音表示来消除同形异音，和（2）声学变异信息来建模多样化的语音风格。此外，为了更好地解决上述问题，我们采用了基于流的后处理网络，捕捉和精炼所生成语音的细节。我们进行了大量实验，并证明我们的方法实现了接近真实人类语音的生成质量，超过了现有方法。

    The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
    
[^28]: 相对高斯机制及其在私有梯度下降中的应用

    The Relative Gaussian Mechanism and its Application to Private Gradient Descent. (arXiv:2308.15250v1 [cs.LG])

    [http://arxiv.org/abs/2308.15250](http://arxiv.org/abs/2308.15250)

    本文介绍了相对高斯机制(RGM)，它利用了相对L2敏感性假设，在保护隐私的同时能够更精确地界定隐私损失。

    

    高斯机制(GM)是一种在发布之前向矢量查询添加高斯噪声的标准隐私保护机制。特别地，如果查询满足某种L2敏感性属性(任意两个相邻输入上输出之间的L2距离有界)，GM保证了Rényi差分隐私(RDP)。不幸的是，精确地界定L2敏感性可能很困难，从而导致松弛的隐私界限。在这项工作中，我们考虑了相对L2敏感性假设，在这种假设下，两个查询输出之间的距离界限也可能取决于它们的范数。利用这一假设，我们引入了相对高斯机制(RGM)，其中噪声的方差取决于输出的范数。我们在相对L2敏感性下证明了RDP参数的严格界限，并描述了因使用输出相关噪声而产生的隐私损失。特别地，我们展示了RGM可以自然地适应潜变量。

    The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a vector-valued query before releasing it, is a standard privacy protection mechanism. In particular, given that the query respects some L2 sensitivity property (the L2 distance between outputs on any two neighboring inputs is bounded), GM guarantees R\'enyi Differential Privacy (RDP). Unfortunately, precisely bounding the L2 sensitivity can be hard, thus leading to loose privacy bounds. In this work, we consider a Relative L2 sensitivity assumption, in which the bound on the distance between two query outputs may also depend on their norm. Leveraging this assumption, we introduce the Relative Gaussian Mechanism (RGM), in which the variance of the noise depends on the norm of the output. We prove tight bounds on the RDP parameters under relative L2 sensitivity, and characterize the privacy loss incurred by using output-dependent noise. In particular, we show that RGM naturally adapts to a latent variable that would
    
[^29]: COMPAS数据集中不同群体之间的可靠性差距

    Reliability Gaps Between Groups in COMPAS Dataset. (arXiv:2308.15243v1 [cs.CY])

    [http://arxiv.org/abs/2308.15243](http://arxiv.org/abs/2308.15243)

    本文研究了在COMPAS数据集中不同群体之间的可靠性差异，并发现输出可靠性存在系统性差异，差异的方向取决于所使用的评估者间统计量的类型和是否进行了对群体预测概率的修正。

    

    本文研究了风险评估工具（RAIs）的评估者间可靠性。主要问题是不同的社会显著群体是否受到RAIs评估者间可靠性不足的影响，即不同群体的错误是否会对他们产生不同的影响。通过对COMPAS数据集进行模拟研究来调查这个问题。在预测模型的输入数据中注入了控制的噪声，这种噪声可以解释为制造错误的合成评估者。主要发现是COMPAS数据集中不同群体之间的输出可靠性存在系统差异。差异的方向取决于使用的评估者间统计量的类型（Cohen's Kappa、Byrt's PABAK、ICC），特别是是否使用了对群体预测概率进行修正。

    This paper investigates the inter-rater reliability of risk assessment instruments (RAIs). The main question is whether different, socially salient groups are affected differently by a lack of inter-rater reliability of RAIs, that is, whether mistakes with respect to different groups affects them differently. The question is investigated with a simulation study of the COMPAS dataset. A controlled degree of noise is injected into the input data of a predictive model; the noise can be interpreted as a synthetic rater that makes mistakes. The main finding is that there are systematic differences in output reliability between groups in the COMPAS dataset. The sign of the difference depends on the kind of inter-rater statistic that is used (Cohen's Kappa, Byrt's PABAK, ICC), and in particular whether or not a correction of predictions prevalences of the groups is used.
    
[^30]: 通过特征选择和分类评估循环平稳恶意软件检测

    Assessing Cyclostationary Malware Detection via Feature Selection and Classification. (arXiv:2308.15237v1 [cs.CR])

    [http://arxiv.org/abs/2308.15237](http://arxiv.org/abs/2308.15237)

    该研究旨在通过循环平稳性检测恶意软件行为，并找到在网络入侵检测系统中最重要的循环平稳特征。

    

    循环平稳性涉及信号和进程中的周期性统计变化，通常用于信号分析和网络安全。在攻击情境中，循环平稳性有助于检测网络流量中的恶意行为，如分布式拒绝服务（DDoS）攻击中的流量模式或恶意软件中的隐藏通信通道。该方法通过识别异常模式并通知网络入侵检测系统（NIDSs）识别潜在攻击，增强对已知和新颖威胁的保护。本研究的重点是识别循环平稳的恶意软件行为及其检测。主要目标是确定在NIDSs中使用的关键循环平稳特征。这些特征使用Boruta和主成分分析（PCA）等算法提取，然后进行分类以找到最重要的循环平稳模式。本文旨在通过循环平稳性揭示恶意软件行为的周期性变化。

    Cyclostationarity involves periodic statistical variations in signals and processes, commonly used in signal analysis and network security. In the context of attacks, cyclostationarity helps detect malicious behaviors within network traffic, such as traffic patterns in Distributed Denial of Service (DDoS) attacks or hidden communication channels in malware. This approach enhances security by identifying abnormal patterns and informing Network Intrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing protection against both known and novel threats. This research focuses on identifying cyclostationary malware behavior and its detection. The main goal is to pinpoint essential cyclostationary features used in NIDSs. These features are extracted using algorithms such as Boruta and Principal Component Analysis (PCA), and then categorized to find the most significant cyclostationary patterns. The aim of this article is to reveal periodically changing malware behaviors thro
    
[^31]: 结合可解释性分析的分类感知神经主题模型——用于冲突分类

    Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification. (arXiv:2308.15232v1 [cs.LG])

    [http://arxiv.org/abs/2308.15232](http://arxiv.org/abs/2308.15232)

    本文提出了一种结合可解释性分析的分类感知神经主题模型，用于冲突分类和主题发现。该模型提供了可靠的分类结果和发现的主题的解释，并通过优化模型的复杂度来提高分类性能。

    

    世界上有大量的冲突事件一直在影响着我们。为了有效分析这些冲突事件，本文提出了一种用于冲突信息分类和主题发现的分类感知神经主题模型（CANTM-IA）。该模型通过引入可解释性分析来提供可靠的分类结果和发现的主题的解释。同时，将解释性引入模型架构中，以提高模型的分类性能，并使解释进一步关注数据的细节。最后，对模型架构进行优化，以降低模型的复杂度。

    A large number of conflict events are affecting the world all the time. In order to analyse such conflict events effectively, this paper presents a Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information Classification and Topic Discovery. The model provides a reliable interpretation of classification results and discovered topics by introducing interpretability analysis. At the same time, interpretation is introduced into the model architecture to improve the classification performance of the model and to allow interpretation to focus further on the details of the data. Finally, the model architecture is optimised to reduce the complexity of the model.
    
[^32]: 使用变分自动编码器为以前未出现的用户提供公平推荐

    Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders. (arXiv:2308.15230v1 [cs.IR])

    [http://arxiv.org/abs/2308.15230](http://arxiv.org/abs/2308.15230)

    本论文提出了一种使用变分自动编码器的新方法，通过限制人口统计信息的编码来减少推荐系统中的歧视，从而为以前未出现的用户提供公平推荐。

    

    机器学习中关于公平性的新定义要求模型对用户的人口统计信息不可见，例如，用户的性别或年龄不应影响模型。个性化推荐系统特别容易通过其显式的用户关注和用户建模来违反这个定义。显式的用户建模也是许多推荐系统无法为以前未出现的用户提供推荐的原因。我们提出了一种限制人口统计信息编码的新方法来减少基于变分自动编码器的推荐系统中的歧视。这些方法能够在评估中为未在训练数据中出现的用户提供公平推荐。

    An emerging definition of fairness in machine learning requires that models are oblivious to demographic user information, e.g., a user's gender or age should not influence the model. Personalized recommender systems are particularly prone to violating this definition through their explicit user focus and user modelling. Explicit user modelling is also an aspect that makes many recommender systems incapable of providing hitherto unseen users with recommendations. We propose novel approaches for mitigating discrimination in Variational Autoencoder-based recommender systems by limiting the encoding of demographic information. The approaches are capable of, and evaluated on, providing users that are not represented in the training data with fair recommendations.
    
[^33]: 评估用于多变量时间序列分类的解释方法

    Evaluating Explanation Methods for Multivariate Time Series Classification. (arXiv:2308.15223v1 [cs.LG])

    [http://arxiv.org/abs/2308.15223](http://arxiv.org/abs/2308.15223)

    本文评估了用于多变量时间序列分类的解释方法，重点研究了基于显著性的方法来指示分类决策中最相关的通道和时间序列点。

    

    多变量时间序列分类是一项重要的计算任务，出现在数据随时间和多个通道记录的应用中。例如，智能手表可以记录人体运动的加速度和方向，这些信号被记录为多变量时间序列。我们可以对这些数据进行分类，以了解和预测人体运动和各种属性，如健身水平。在许多应用中，仅靠分类是不够的，我们通常需要分类，同时还要理解模型学到了什么（例如，基于数据中的哪些信息给出了预测）。本文的重点是分析和评估专用于多变量时间序列分类（MTSC）的解释方法。我们着重研究了能指出分类决策中最相关通道和时间序列点的基于显著性的解释方法。我们分析了两种流行且准确的多变量时间序列分类器，ROCKET和...

    Multivariate time series classification is an important computational task arising in applications where data is recorded over time and over multiple channels. For example, a smartwatch can record the acceleration and orientation of a person's motion, and these signals are recorded as multivariate time series. We can classify this data to understand and predict human movement and various properties such as fitness levels. In many applications classification alone is not enough, we often need to classify but also understand what the model learns (e.g., why was a prediction given, based on what information in the data). The main focus of this paper is on analysing and evaluating explanation methods tailored to Multivariate Time Series Classification (MTSC). We focus on saliency-based explanation methods that can point out the most relevant channels and time series points for the classification decision. We analyse two popular and accurate multivariate time series classifiers, ROCKET and 
    
[^34]: 合奏反事实解释器

    Ensemble of Counterfactual Explainers. (arXiv:2308.15194v1 [cs.AI])

    [http://arxiv.org/abs/2308.15194](http://arxiv.org/abs/2308.15194)

    该论文提出了一种合奏反事实解释器，可以提升弱解释器的性能，实现对反事实实例的最小化、可操作性、稳定性、多样性、合理性和辨别力的全覆盖。

    

    在可解释的人工智能（XAI）领域中，已经提出了几种反事实解释器，每种解释器都关注反事实实例的一些可取特性：最小化、可操作性、稳定性、多样性、合理性、辨别力。我们提出了一种合奏反事实解释器，它能够增强弱解释器的性能，这些弱解释器仅提供这些特性的一个子集，使其成为一种强大的方法涵盖所有特性。该合奏解释器在一些实例和特征的样本上运行弱解释器，并通过利用多样性驱动的选择函数来合并其结果。该方法是模型无关的，并且通过基于自动编码器的封装方法，也是数据无关的。

    In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic.
    
[^35]: 深度神经网络中使用Grad-CAM的视觉解释更可靠吗？以自动气胸诊断为案例研究

    Is visual explanation with Grad-CAM more reliable for deeper neural networks? a case study with automatic pneumothorax diagnosis. (arXiv:2308.15172v1 [eess.IV])

    [http://arxiv.org/abs/2308.15172](http://arxiv.org/abs/2308.15172)

    本研究通过自动气胸诊断案例研究，探究了不同深度学习模型中Grad-CAM的鲁棒性和效果，结果表明深度神经网络并不一定会显著改善Grad-CAM的性能。

    

    尽管深度学习技术在各种临床任务中提供了最先进的性能，但关于其决策过程的解释性可以极大增强这些方法在安全和快速临床应用中的可信度。具有高灵活性的梯度加权类别激活映射（Grad-CAM）已被广泛采用，以提供各种深度学习模型在计算机辅助诊断中推理过程的直观视觉解释。然而，尽管该技术的流行，对Grad-CAM在不同深度学习架构上的性能仍缺乏系统研究。在本研究中，我们研究了其在不同流行的深度学习模型上的鲁棒性和效果，重点关注网络的深度和架构类型的影响，通过使用X光扫描中自动气胸诊断的案例研究。我们的结果表明，深度神经网络不一定会显著改善Grad-CAM的性能。

    While deep learning techniques have provided the state-of-the-art performance in various clinical tasks, explainability regarding their decision-making process can greatly enhance the credence of these methods for safer and quicker clinical adoption. With high flexibility, Gradient-weighted Class Activation Mapping (Grad-CAM) has been widely adopted to offer intuitive visual interpretation of various deep learning models' reasoning processes in computer-assisted diagnosis. However, despite the popularity of the technique, there is still a lack of systematic study on Grad-CAM's performance on different deep learning architectures. In this study, we investigate its robustness and effectiveness across different popular deep learning models, with a focus on the impact of the networks' depths and architecture types, by using a case study of automatic pneumothorax diagnosis in X-ray scans. Our results show that deeper neural networks do not necessarily contribute to a strong improvement of p
    
[^36]: ABS-SGD: 一种用于异构GPU集群的延迟同步随机梯度下降算法和自适应批量大小

    ABS-SGD: A Delayed Synchronous Stochastic Gradient Descent Algorithm with Adaptive Batch Size for Heterogeneous GPU Clusters. (arXiv:2308.15164v1 [cs.LG])

    [http://arxiv.org/abs/2308.15164](http://arxiv.org/abs/2308.15164)

    ABS-SGD 是一种用于异构GPU集群的延迟同步随机梯度下降算法，实现了计算资源的充分利用并缓解了过时梯度问题。

    

    随着模型和数据集的增长，以并行的方式训练模型变得越来越常见。然而，现有的分布式随机梯度下降（SGD）算法在异构集群中存在着计算资源利用不充分和收敛性差的问题。本文提出了一种用于异构GPU集群的延迟同步SGD算法和自适应批量大小（ABS-SGD）。在ABS-SGD中，工作节点进行全局同步以累积延迟梯度，并使用累积的延迟梯度来更新参数。在工作节点进行延迟梯度的全局同步时，它们在提前指定批量大小之前进行下一批次的计算，这样可以实现计算资源的充分利用。由于梯度延迟仅为一个迭代，可以缓解过时梯度问题。我们理论上证明了ABS-SGD在异构集群中的收敛性。

    As the size of models and datasets grows, it has become increasingly common to train models in parallel. However, existing distributed stochastic gradient descent (SGD) algorithms suffer from insufficient utilization of computational resources and poor convergence in heterogeneous clusters. In this paper, we propose a delayed synchronous SGD algorithm with adaptive batch size (ABS-SGD) for heterogeneous GPU clusters. In ABS-SGD, workers perform global synchronization to accumulate delayed gradients and use the accumulated delayed gradients to update parameters. While workers are performing global synchronization for delayed gradients, they perform the computation of the next batch without specifying batch size in advance, which lasts until the next global synchronization starts, realizing the full utilization of computational resources. Since the gradient delay is only one iteration, the stale gradient problem can be alleviated. We theoretically prove the convergence of ABS-SGD in hete
    
[^37]: 关于模型预测控制器的改进研究

    On the improvement of model-predictive controllers. (arXiv:2308.15157v1 [cs.LG])

    [http://arxiv.org/abs/2308.15157](http://arxiv.org/abs/2308.15157)

    本文研究了模型预测控制器的改进问题，通过提高内部预测模型的精确性来自动改善整个控制器，结果表明提高预测模型总体上将改善控制器的质量。

    

    本文研究了合成模型预测控制(MPC)问题，以证明内部预测模型(PM)的精确性提高会自动改进整个控制器。与强化学习(RL)不同，MPC使用PM来预测受控系统(CS)的后续状态，而不是直接推荐合适的动作。为了评估PM的精确性对模型预测控制器质量的影响，我们将基于DNN的PM与三个复杂程度不同的著名控制问题的最佳基准PM进行比较。基准PM通过访问CS本身的模拟实现了完美的准确性。根据所得结果，我们认为提高PM将总体上改善控制器，而无需考虑其他组件（本文中基于进化优化的动作选择）的影响。

    This article investigates synthetic model-predictive control (MPC) problems to demonstrate that an increased precision of the internal prediction model (PM) automatially entails an improvement of the controller as a whole. In contrast to reinforcement learning (RL), MPC uses the PM to predict subsequent states of the controlled system (CS), instead of directly recommending suitable actions. To assess how the precision of the PM translates into the quality of the model-predictive controller, we compare a DNN-based PM to the optimal baseline PM for three well-known control problems of varying complexity. The baseline PM achieves perfect accuracy by accessing the simulation of the CS itself. Based on the obtained results, we argue that an improvement of the PM will always improve the controller as a whole, without considering the impact of other components such as action selection (which, in this article, relies on evolutionary optimization).
    
[^38]: 提高心脏磁共振图像分类的深度学习模型校准的不确定性感知训练

    Uncertainty Aware Training to Improve Deep Learning Model Calibration for Classification of Cardiac MR Images. (arXiv:2308.15141v1 [eess.IV])

    [http://arxiv.org/abs/2308.15141](http://arxiv.org/abs/2308.15141)

    本文研究了三种新的不确定性感知训练策略，比较了它们与两种最先进方法的效果，并在心脏磁共振图像分类的两个临床应用中进行了性能分析。

    

    量化预测不确定性被认为是开发更可靠的人工智能模型的一种方式，超越传统性能指标报告。在考虑其在临床决策支持环境中的作用时，人工智能分类模型理想情况下应避免自信的错误预测并最大限度地提高正确预测的置信度。这样做的模型被认为在置信度方面具有良好的校准性。然而，在训练这些模型时，如何改善校准性，即使训练策略不确定性感知，却相对少受到关注。在这项工作中，我们评估了三种新颖的不确定性感知训练策略，并与两种最先进的方法进行比较。我们分析了两种不同的临床应用的性能：心脏复同步疗法（CRT）反应预测和冠状动脉疾病（CAD）的心脏磁共振（CMR）图像诊断。在性能方面表现最佳的模型是

    Quantifying uncertainty of predictions has been identified as one way to develop more trustworthy artificial intelligence (AI) models beyond conventional reporting of performance metrics. When considering their role in a clinical decision support setting, AI classification models should ideally avoid confident wrong predictions and maximise the confidence of correct predictions. Models that do this are said to be well-calibrated with regard to confidence. However, relatively little attention has been paid to how to improve calibration when training these models, i.e., to make the training strategy uncertainty-aware. In this work we evaluate three novel uncertainty-aware training strategies comparing against two state-of-the-art approaches. We analyse performance on two different clinical applications: cardiac resynchronisation therapy (CRT) response prediction and coronary artery disease (CAD) diagnosis from cardiac magnetic resonance (CMR) images. The best-performing model in terms of
    
[^39]: 两质量学习：处理闭集分布转换的算法设计框架

    Biquality Learning: a Framework to Design Algorithms Dealing with Closed-Set Distribution Shifts. (arXiv:2308.15132v1 [cs.LG])

    [http://arxiv.org/abs/2308.15132](http://arxiv.org/abs/2308.15132)

    本论文提出了一种双质量学习的框架，旨在设计能够处理闭集分布转换的算法。该框架假设在训练时有一个可信数据集和一个带有数据集转换和弱监督的不可信数据集可用，可以应对任何分布转换。作者提出了两种方法来处理双质量学习，分别灵感来源于标签噪声和协变量转换，实验证明这些方法在真实世界数据集中具有较好的效果。

    

    从弱监督和数据集转换的数据训练机器学习模型依然具有挑战性。在这两种情况下设计算法并没有得到充分探索，现有的算法也不能处理最复杂的分布转换。我们认为双质量数据设置是设计这种算法的合适框架。双质量学习假设在训练时有两个数据集可用：一个从感兴趣的分布中采样的可信数据集，以及带有数据集转换和弱监督的不可信数据集（即分布转换）。训练时可用的可信和不可信数据集使得设计能够处理任何分布转换的算法成为可能。我们提出了两种方法，一种灵感来自标签噪声文献，另一种灵感来自协变量转换文献，用于双质量学习。我们在真实世界的数据集中尝试了两种新方法来合成概念漂移和类条件转换。

    Training machine learning models from data with weak supervision and dataset shifts is still challenging. Designing algorithms when these two situations arise has not been explored much, and existing algorithms cannot always handle the most complex distributional shifts. We think the biquality data setup is a suitable framework for designing such algorithms. Biquality Learning assumes that two datasets are available at training time: a trusted dataset sampled from the distribution of interest and the untrusted dataset with dataset shifts and weaknesses of supervision (aka distribution shifts). The trusted and untrusted datasets available at training time make designing algorithms dealing with any distribution shifts possible. We propose two methods, one inspired by the label noise literature and another by the covariate shift literature for biquality learning. We experiment with two novel methods to synthetically introduce concept drift and class-conditional shifts in real-world datase
    
[^40]: 大型视觉语言模型中幻觉的评估与分析

    Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])

    [http://arxiv.org/abs/2308.15126](http://arxiv.org/abs/2308.15126)

    本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。

    

    最近，大型视觉语言模型（LVLMs）取得了显著的成功。然而，LVLMs仍然存在幻觉问题，这限制了在许多场景中的实用性。幻觉指的是LVLMs响应中不存在于视觉输入中的信息，这可能导致重大后果的潜在风险。目前对LVLMs中的幻觉评估的研究工作有限。在本文中，我们提出了基于大型语言模型（LLM）的幻觉评估框架HaELM。HaELM的性能近似于ChatGPT的95%，并具有低成本、可复现、保护隐私和本地部署等额外优势。利用HaELM，我们评估了当前LVLMs中的幻觉。此外，我们分析了导致LVLMs中幻觉的因素，并提出了缓解幻觉问题的有用建议。

    Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
    
[^41]: 通过Mixup增强的元学习方法实现蛋白质模拟器的高效微调

    Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])

    [http://arxiv.org/abs/2308.15116](http://arxiv.org/abs/2308.15116)

    本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。

    

    分子动力学模拟已经成为研究生物分子的基本工具。与此同时，我们希望在分子能够波动的各种条件下对一组粒子进行模拟。本文中，我们将软提示学习方法应用于分子动力学任务并进行了适应性探索。我们的模型可以在有限的训练数据下非常好地泛化到未见过的和超出分布的场景。虽然我们的工作以温度为测试案例，但我们的方法的多功能性使其可以通过任何连续的动态条件（如压力和体积）进行有效模拟。我们的框架有两个阶段：1）使用数据混合技术进行预训练，增强分子结构数据和温度提示，然后通过逐渐增加比例的方式应用课程学习方法。2）基于元学习的微调框架提高了微调过程的样本效率，并为软提示微调提供更好的表现。

    Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
    
[^42]: 带有侧面观测的随机图赌博学习

    Stochastic Graph Bandit Learning with Side-Observations. (arXiv:2308.15107v1 [cs.LG])

    [http://arxiv.org/abs/2308.15107](http://arxiv.org/abs/2308.15107)

    本文研究了具有一般函数空间和图反馈的随机背景赌博学习问题，并提出了一种算法，填补了此前研究中的空白。该算法提供了奖励差距依赖的上界，并在遗憾上界方面提供了改进。数值实验证明了该方法的计算效率和有效性。

    

    本文研究了具有一般函数空间和图反馈的随机背景赌博问题。我们提出了一种算法，通过适应潜在的图结构和奖励差距来解决这个问题。据我们所知，我们的算法是在这个随机设置下提供奖励差距依赖的上界的第一个算法，填补了[35]的研究空白。与[31,33,35]相比，我们的方法提供了改进的遗憾上界，并且不需要图形数量的知识。我们进行数值实验，以证明我们的方法在遗憾上界方面的计算效率和有效性。这些发现突显了我们算法在推进具有图反馈的随机背景赌博领域方面的重要性，并为各个领域的实际应用开辟了道路。

    In this paper, we investigate the stochastic contextual bandit with general function space and graph feedback. We propose an algorithm that addresses this problem by adapting to both the underlying graph structures and reward gaps. To the best of our knowledge, our algorithm is the first to provide a gap-dependent upper bound in this stochastic setting, bridging the research gap left by the work in [35]. In comparison to [31,33,35], our method offers improved regret upper bounds and does not require knowledge of graphical quantities. We conduct numerical experiments to demonstrate the computational efficiency and effectiveness of our approach in terms of regret upper bounds. These findings highlight the significance of our algorithm in advancing the field of stochastic contextual bandits with graph feedback, opening up avenues for practical applications in various domains.
    
[^43]: 自解释GNN（图神经网络）的可信度有多高？

    How Faithful are Self-Explainable GNNs?. (arXiv:2308.15096v1 [cs.LG])

    [http://arxiv.org/abs/2308.15096](http://arxiv.org/abs/2308.15096)

    自解释的GNN的可信度进行了分析，并发现了模型本身和评估指标的几个限制，提出了可能的解决办法。

    

    自解释的深度神经网络是一种能够输出模型推理的先验局部解释的最新模型，因此在表达能力和可解释性之间取得了进一步的进展。自解释的图神经网络旨在在图数据的背景下实现相同的功能。这引出了一个问题：这些模型是否在可信度方面实现了其隐式保证？在这个扩展摘要中，我们使用不同的可信度度量分析了几个自解释的GNN，并确定了几个限制-无论是在模型本身还是在评估指标上-并概述了可能的解决办法。

    Self-explainable deep neural networks are a recent class of models that can output ante-hoc local explanations that are faithful to the model's reasoning, and as such represent a step forward toward filling the gap between expressiveness and interpretability. Self-explainable graph neural networks (GNNs) aim at achieving the same in the context of graph data. This begs the question: do these models fulfill their implicit guarantees in terms of faithfulness? In this extended abstract, we analyze the faithfulness of several self-explainable GNNs using different measures of faithfulness, identify several limitations -- both in the models themselves and in the evaluation metrics -- and outline possible ways forward.
    
[^44]: 通过分位数回归校准实现基于群体条件的符合预测，用于作物与杂草分类

    Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification. (arXiv:2308.15094v1 [cs.CV])

    [http://arxiv.org/abs/2308.15094](http://arxiv.org/abs/2308.15094)

    通过分位数回归校准实现基于群体条件的符合预测框架，解决了深度学习预测模型的不确定性问题，并应用于实际农业计算机视觉中的作物与杂草分类问题。

    

    随着深度学习预测模型成为精准农业系统中不可或缺的一部分，一个阻碍采用这些自动化解决方案的障碍是用户对这些高度复杂、不透明和不确定的模型缺乏信任。实际上，深度神经网络没有任何明确的保证，可以用来证明系统的性能，在高度变化的以及无法控制的环境中，比如典型的农业计算机视觉中所面临的环境。幸运的是，其他领域开发的某些方法对农业应用非常重要。本文提出了符合预测框架，该框架对任何黑盒子预测机器的预测性能提供有效的统计保证，几乎不需要任何假设，应用于实际条件下的深度可视化杂草和作物分类问题。本文重点介绍了框架的实际方面，并特别注意了细节。

    As deep learning predictive models become an integral part of a large spectrum of precision agricultural systems, a barrier to the adoption of such automated solutions is the lack of user trust in these highly complex, opaque and uncertain models. Indeed, deep neural networks are not equipped with any explicit guarantees that can be used to certify the system's performance, especially in highly varying uncontrolled environments such as the ones typically faced in computer vision for agriculture.Fortunately, certain methods developed in other communities can prove to be important for agricultural applications. This article presents the conformal prediction framework that provides valid statistical guarantees on the predictive performance of any black box prediction machine, with almost no assumptions, applied to the problem of deep visual classification of weeds and crops in real-world conditions. The framework is exposed with a focus on its practical aspects and special attention accor
    
[^45]: 我们能依赖人工智能吗？

    Can We Rely on AI?. (arXiv:2308.15092v1 [math.NA])

    [http://arxiv.org/abs/2308.15092](http://arxiv.org/abs/2308.15092)

    近年来，对抗性攻击算法揭示了人工智能工具的不稳定性，在高风险环境中引发了安全、可靠性和可解释性方面的问题。这篇论文提供了一个对该主题的概述，关注对应用和计算数学领域的研究人员可能感兴趣的方面。

    

    在过去的十年里，对抗性攻击算法揭示了深度学习工具的不稳定性。这些算法引发了人工智能在安全性、可靠性和可解释性方面的问题，特别是在高风险环境中。从实际角度来看，攻击和防御策略的发展之间存在着一种升级的战争。在更加理论的层面上，研究人员也研究了攻击的存在性和可计算性等更大的问题。在这里，我们简要概述了这个主题，并重点关注对应用和计算数学领域的研究人员可能感兴趣的方面。

    Over the last decade, adversarial attack algorithms have revealed instabilities in deep learning tools. These algorithms raise issues regarding safety, reliability and interpretability in artificial intelligence; especially in high risk settings. From a practical perspective, there has been a war of escalation between those developing attack and defence strategies. At a more theoretical level, researchers have also studied bigger picture questions concerning the existence and computability of attacks. Here we give a brief overview of the topic, focusing on aspects that are likely to be of interest to researchers in applied and computational mathematics.
    
[^46]: 使用深度学习进行Circle of Willis血管分叉的自动检测和分类

    Using deep learning for an automatic detection and classification of the vascular bifurcations along the Circle of Willis. (arXiv:2308.15088v1 [eess.IV])

    [http://arxiv.org/abs/2308.15088](http://arxiv.org/abs/2308.15088)

    该研究利用深度学习技术开发了一种自动检测和分类Willis环血管分叉的方法，帮助神经放射学家及时定位高颅内动脉瘤风险的区域。

    

    大多数颅内动脉瘤发生在被称为Willis环的大脑血管树的特定部位上。特别地，它们主要出现在构成这个环形结构的十五个主要动脉分叉上。因此，为了进行高效和及时的诊断，开发一些能够准确识别每个感兴趣的分叉的方法至关重要。实际上，自动提取具有较高ICA发生风险的分叉将为神经放射学家提供对最令人担忧的区域的快速概述。由于人工智能的最近努力，深度学习成为许多模式识别任务中表现最好的技术。此外，各种方法被特别设计用于医学图像分析。这项研究意在帮助神经放射学家快速定位任何具有高ICA风险的分叉的发生。

    Most of the intracranial aneurysms (ICA) occur on a specific portion of the cerebral vascular tree named the Circle of Willis (CoW). More particularly, they mainly arise onto fifteen of the major arterial bifurcations constituting this circular structure. Hence, for an efficient and timely diagnosis it is critical to develop some methods being able to accurately recognize each Bifurcation of Interest (BoI). Indeed, an automatic extraction of the bifurcations presenting the higher risk of developing an ICA would offer the neuroradiologists a quick glance at the most alarming areas. Due to the recent efforts on Artificial Intelligence, Deep Learning turned out to be the best performing technology for many pattern recognition tasks. Moreover, various methods have been particularly designed for medical image analysis purposes. This study intends to assist the neuroradiologists to promptly locate any bifurcation presenting a high risk of ICA occurrence. It can be seen as a Computer Aided Di
    
[^47]: 通过潜在能量探索模型可迁移性

    Exploring Model Transferability through the Lens of Potential Energy. (arXiv:2308.15074v1 [cs.CV])

    [http://arxiv.org/abs/2308.15074](http://arxiv.org/abs/2308.15074)

    本文提出了一种名为PED的物理启发式方法，通过在受力驱动的物理模型中捕捉动态表示的运动来降低潜在能量，以解决模型选择时的挑战，并获得更强大和更稳定的观察结果。

    

    由于预训练深度学习模型的大量可用性，迁移学习在计算机视觉任务中变得至关重要。然而，针对特定下游任务从多种预训练模型中选择最佳模型仍然面临挑战。现有的衡量预训练模型可迁移性的方法依赖于编码的静态特征和任务标签之间的统计相关性，但它们忽视了在微调过程中底层表示动态的影响，导致结果不可靠，尤其对于自监督模型而言。在本文中，我们提出了一种名为PED的物理启发式方法来解决这些挑战。我们通过潜在能量的视角重新构筑了模型选择的挑战，并直接建模影响微调动态的相互作用力。通过在受力驱动的物理模型中捕捉动态表示的运动来降低潜在能量，我们可以获得更强大和更稳定的观察结果。

    Transfer learning has become crucial in computer vision tasks due to the vast availability of pre-trained deep learning models. However, selecting the optimal pre-trained model from a diverse pool for a specific downstream task remains a challenge. Existing methods for measuring the transferability of pre-trained models rely on statistical correlations between encoded static features and task labels, but they overlook the impact of underlying representation dynamics during fine-tuning, leading to unreliable results, especially for self-supervised models. In this paper, we present an insightful physics-inspired approach named PED to address these challenges. We reframe the challenge of model selection through the lens of potential energy and directly model the interaction forces that influence fine-tuning dynamics. By capturing the motion of dynamic representations to decline the potential energy within a force-driven physical model, we can acquire an enhanced and more stable observatio
    
[^48]: 通过对抗logit更新推进对抗鲁棒性

    Advancing Adversarial Robustness Through Adversarial Logit Update. (arXiv:2308.15072v1 [cs.LG])

    [http://arxiv.org/abs/2308.15072](http://arxiv.org/abs/2308.15072)

    本研究提出了一种基于对抗logit更新的新原则(ALU)，用于推断对抗样本的标签，并通过使用预处理和后处理的logit差异提高了模型的对抗鲁棒性。

    

    深度神经网络容易受到对抗性扰动的影响。对抗训练和对抗净化是广泛认可的防御策略之一。尽管这些方法有不同的基本逻辑，但都依赖于绝对logit值生成标签预测。在本研究中，从理论角度对成功的对抗攻击周围的logit差异进行了理论分析，并提出了一种新的原则，即对抗logit更新(ALU)，用于推断对抗样本的标签。基于ALU，我们引入了一种新的分类范式，利用预处理和后处理的logit差异来提高模型的对抗鲁棒性。我们的清洁数据合成模型不需要对抗或额外的数据用于模型训练，可以轻松应用于各种预训练模型，用于对抗样本检测和基于ALU的数据分类。

    Deep Neural Networks are susceptible to adversarial perturbations. Adversarial training and adversarial purification are among the most widely recognized defense strategies. Although these methods have different underlying logic, both rely on absolute logit values to generate label predictions. In this study, we theoretically analyze the logit difference around successful adversarial attacks from a theoretical point of view and propose a new principle, namely Adversarial Logit Update (ALU), to infer adversarial sample's labels. Based on ALU, we introduce a new classification paradigm that utilizes pre- and post-purification logit differences for model's adversarial robustness boost. Without requiring adversarial or additional data for model training, our clean data synthesis model can be easily applied to various pre-trained models for both adversarial sample detection and ALU-based data classification. Extensive experiments on both CIFAR-10, CIFAR-100, and tiny-ImageNet datasets show 
    
[^49]: MadSGM：基于评分的生成模型的多变量异常检测

    MadSGM: Multivariate Anomaly Detection with Score-based Generative Models. (arXiv:2308.15069v1 [cs.LG])

    [http://arxiv.org/abs/2308.15069](http://arxiv.org/abs/2308.15069)

    MadSGM是一种基于评分的生成模型的多变量时间序列异常检测器，考虑了重构、密度和梯度等全面的异常测量因素。实验证明MadSGM具有最强大和准确的预测能力。

    

    时间序列异常检测是时间序列中最基本的任务之一。与时间序列预测和分类不同，时间序列异常检测通常需要无监督（或自监督）训练，因为收集和标记异常观测是困难的。此外，大多数现有方法采用有限形式的异常测量，因此不清楚它们是否在所有情况下都是最优的。为此，我们提出了一种基于评分的生成模型的多变量时间序列异常检测器，称为MadSGM，它考虑了迄今为止最广泛的异常测量因素：i）基于重构的、ii）基于密度的和iii）基于梯度的异常测量。我们还设计了一个条件评分网络及其去噪评分匹配损失，用于时间序列异常检测。对五个真实世界基准数据集的实验表明，MadSGM实现了最强大和准确的预测。

    The time-series anomaly detection is one of the most fundamental tasks for time-series. Unlike the time-series forecasting and classification, the time-series anomaly detection typically requires unsupervised (or self-supervised) training since collecting and labeling anomalous observations are difficult. In addition, most existing methods resort to limited forms of anomaly measurements and therefore, it is not clear whether they are optimal in all circumstances. To this end, we present a multivariate time-series anomaly detector based on score-based generative models, called MadSGM, which considers the broadest ever set of anomaly measurement factors: i) reconstruction-based, ii) density-based, and iii) gradient-based anomaly measurements. We also design a conditional score network and its denoising score matching loss for the time-series anomaly detection. Experiments on five real-world benchmark datasets illustrate that MadSGM achieves the most robust and accurate predictions.
    
[^50]: OEBench: 研究现实世界中关系数据流中的开放环境挑战

    OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams. (arXiv:2308.15059v1 [cs.LG])

    [http://arxiv.org/abs/2308.15059](http://arxiv.org/abs/2308.15059)

    OEBench是一个用于评估关系数据流中开放环境挑战的开放环境基准，研究发现这种挑战在真实世界数据集中普遍存在。

    

    关系数据集在现实世界中非常普遍，并且通常以数据流的方式传递。这种类型的数据流可能存在一些特殊的挑战，例如分布漂移、异常值、新兴类别和特征变化，最近将这些挑战描述为机器学习中的开放环境挑战。虽然已经有一些关于数据流的增量学习的研究，但其评估主要是基于手动分割的数据集。此外，虽然有几个现实世界的数据流数据集可用，但这些开放环境挑战是否普遍存在以及现有的增量学习算法在真实数据集上的表现如何尚不确定。为了填补这个空白，我们开发了一个名为OEBench的开放环境基准，用于评估关系数据流中的开放环境挑战。具体而言，我们研究了55个真实世界的数据流数据集，并确立了开放环境场景在真实数据集中的普遍存在性，这在当前的研究中还没有被确定。

    Relational datasets are widespread in real-world scenarios and are usually delivered in a streaming fashion. This type of data stream can present unique challenges, such as distribution drifts, outliers, emerging classes, and changing features, which have recently been described as open environment challenges for machine learning. While some work has been done on incremental learning for data streams, their evaluations are mostly conducted with manually partitioned datasets. Moreover, while several real-world streaming datasets are available, it is uncertain whether these open environment challenges are prevalent and how existing incremental learning algorithms perform on real datasets. To fill this gap, we develop an Open Environment Benchmark named OEBench to evaluate open environment challenges in relational data streams. Specifically, we investigate 55 real-world streaming datasets and establish that open environment scenarios are indeed widespread in real-world datasets, which pre
    
[^51]: 低资源语言的形态词义标注中的分类丧失

    Taxonomic Loss for Morphological Glossing of Low-Resource Languages. (arXiv:2308.15055v1 [cs.CL])

    [http://arxiv.org/abs/2308.15055](http://arxiv.org/abs/2308.15055)

    本文提出了一种利用形态信息的分类损失函数，在低资源语言的形态词义标注中提高了性能。 尽管在单标签预测准确性方面不如标准损失函数，但在前n个预测标签方面表现更好。 这个方法在人机协作标注方面具有潜力。

    

    形态词义标注是自动语言文档中的关键任务，它可以极大地提高其他下游应用的效果。尽管目前最先进的标注系统在数据丰富的语言上表现非常好，但在低资源语言上创建有用的模型更加困难。在本文中，我们提出了一种利用形态信息的分类损失函数，在数据稀缺的情况下，使形态词义标注的性能更好。我们发现，尽管使用这种损失函数在单标签预测准确性方面表现不如标准损失函数，但在考虑前n个预测标签时，它产生更好的预测。我们认为这种性质使得分类损失函数在人机协作标注的环境中有用。

    Morpheme glossing is a critical task in automated language documentation and can benefit other downstream applications greatly. While state-of-the-art glossing systems perform very well for languages with large amounts of existing data, it is more difficult to create useful models for low-resource languages. In this paper, we propose the use of a taxonomic loss function that exploits morphological information to make morphological glossing more performant when data is scarce. We find that while the use of this loss function does not outperform a standard loss function with regards to single-label prediction accuracy, it produces better predictions when considering the top-n predicted labels. We suggest this property makes the taxonomic loss function useful in a human-in-the-loop annotation setting.
    
[^52]: iBARLE：平衡感知式房间布局估计

    iBARLE: imBalance-Aware Room Layout Estimation. (arXiv:2308.15050v1 [cs.CV])

    [http://arxiv.org/abs/2308.15050](http://arxiv.org/abs/2308.15050)

    我们提出了平衡感知式房间布局估计（iBARLE）框架，它包括外观变化生成模块、复杂结构混合模块和梯度-based布局目标函数，旨在解决房间布局估计中的不平衡和泛化问题。

    

    房间布局估计是从单个全景图预测布局。它需要具有大规模和多样化的房间形状的数据集来训练模型。然而，真实世界的数据集中存在显著的不平衡，包括布局复杂度的尺寸、相机位置和场景外观的变化。这些问题显著影响模型的训练性能。在这项工作中，我们提出了平衡感知式房间布局估计（iBARLE）框架来解决这些问题。iBARLE包括（1）外观变化生成（AVG）模块，促进视觉外观领域的泛化，（2）复杂结构混合（CSMix）模块，提高对房间结构的泛化能力，和（3）基于梯度的布局目标函数，更有效地考虑复杂布局中的遮挡。所有模块都是联合训练的，彼此帮助以实现最佳性能。基于ZInD~\cite{cruz2021zillow}数据集的实验和消融研究验证了我们的方法的有效性。

    Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose the imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD~\cite{cruz2021zillow} dat
    
[^53]: 大型语言模型在概念组织上趋向人类的水平

    Large language models converge toward human-like concept organization. (arXiv:2308.15047v1 [cs.LG])

    [http://arxiv.org/abs/2308.15047](http://arxiv.org/abs/2308.15047)

    大型语言模型学会以类似于知识库的方式组织概念，这表明它们具备人类推理语义和世界知识的能力。

    

    大型语言模型在知识提取、推理和对话方面展现出人类水平的表现，但这种表现是由于记忆和模式匹配，还是反映了类似于人类推理语义和世界知识的表现仍然存在争议。知识库（如WikiData）提供了推理语义和世界知识的大规模高质量表示。我们展示了大型语言模型学会以与这些知识库中概念的组织方式惊人相似的方式组织概念。知识库模拟了集体、机构化的知识，而大型语言模型似乎从原始文本中产生了这样的知识。我们展示了更大更好的模型在概念组织上表现出更加人类化的特点，涵盖了四个语言模型系列和三个知识图谱嵌入方法。

    Large language models show human-like performance in knowledge extraction, reasoning and dialogue, but it remains controversial whether this performance is best explained by memorization and pattern matching, or whether it reflects human-like inferential semantics and world knowledge. Knowledge bases such as WikiData provide large-scale, high-quality representations of inferential semantics and world knowledge. We show that large language models learn to organize concepts in ways that are strikingly similar to how concepts are organized in such knowledge bases. Knowledge bases model collective, institutional knowledge, and large language models seem to induce such knowledge from raw text. We show that bigger and better models exhibit more human-like concept organization, across four families of language models and three knowledge graph embeddings.
    
[^54]: 基于GPU的混合可满足性问题求解器的大规模并行连续局部搜索

    Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs. (arXiv:2308.15020v1 [cs.AI])

    [http://arxiv.org/abs/2308.15020](http://arxiv.org/abs/2308.15020)

    提出了一种基于GPU的大规模并行连续局部搜索方法来加速混合SAT求解器，该方法通过使用基于快速傅里叶变换的新型并行算法来计算基本对称多项式，并在搜索中使用重启启发式算法以提高效率。与以前的方法相比，实现了显著的改进。

    

    针对基于冲突驱动子句学习（CDCL）的最先进可满足性问题（SAT）求解器在工程上取得了显著成功，但其顺序性质限制了在图形处理单元（GPU）等平台上进行加速的并行性。在本研究中，我们提出了FastFourierSAT，一种基于梯度驱动连续局部搜索（CLS）的高度并行混合SAT求解器。这是通过一种受快速傅里叶变换（FFT）卷积启发的新型并行算法来实现的，用于计算以前的CLS方法中的主要计算任务——基本对称多项式（ESP）。我们的算法复杂度与最佳的以前结果相匹配。此外，我们算法固有的大规模并行性能够利用GPU进行加速，相比以前的CLS方法展现出显著的改进。我们还提出了将重启启发式算法融入CLS以提高搜索效率。我们将我们的应用与其他SAT求解器进行了比较。

    Although state-of-the-art (SOTA) SAT solvers based on conflict-driven clause learning (CDCL) have achieved remarkable engineering success, their sequential nature limits the parallelism that may be extracted for acceleration on platforms such as the graphics processing unit (GPU). In this work, we propose FastFourierSAT, a highly parallel hybrid SAT solver based on gradient-driven continuous local search (CLS). This is realized by a novel parallel algorithm inspired by the Fast Fourier Transform (FFT)-based convolution for computing the elementary symmetric polynomials (ESPs), which is the major computational task in previous CLS methods. The complexity of our algorithm matches the best previous result. Furthermore, the substantial parallelism inherent in our algorithm can leverage the GPU for acceleration, demonstrating significant improvement over the previous CLS approaches. We also propose to incorporate the restart heuristics in CLS to improve search efficiency. We compare our app
    
[^55]: 利用问题几何特征的安全线性赌博机问题

    Exploiting Problem Geometry in Safe Linear Bandits. (arXiv:2308.15006v1 [cs.LG])

    [http://arxiv.org/abs/2308.15006](http://arxiv.org/abs/2308.15006)

    通过利用安全线性赌博机问题的几何特征，我们提出了改进的遗憾保证算法，并将其推广到具有凸约束的情况。模拟结果显示，在各种随机采样的设置中，我们的算法表现出优越的性能。

    

    安全线性赌博机问题是经典线性赌博机问题的一个版本，其中学习器的行动必须在所有回合满足一个不确定的线性约束。由于其在许多实际场景中的适用性，近年来这个问题受到了相当大的关注。我们发现通过利用特定问题设置的几何特征，可以为相互分离的问题实例和有限星凸集的行动集提供改进的遗憾保证。此外，我们提出了一种新的算法，能够自适应地选择问题参数，并具有至少与现有算法相当的遗憾保证。最后，我们引入了安全线性赌博机设置的推广，其中约束是凸的，并利用了一种基于凸分析的新方法来调整我们的算法和分析。通过模拟结果显示，相对于现有算法，我们的算法在各种随机采样的设置中表现出改进的性能。

    The safe linear bandit problem is a version of the classic linear bandit problem where the learner's actions must satisfy an uncertain linear constraint at all rounds. Due its applicability to many real-world settings, this problem has received considerable attention in recent years. We find that by exploiting the geometry of the specific problem setting, we can achieve improved regret guarantees for both well-separated problem instances and action sets that are finite star convex sets. Additionally, we propose a novel algorithm for this setting that chooses problem parameters adaptively and enjoys at least as good regret guarantees as existing algorithms. Lastly, we introduce a generalization of the safe linear bandit setting where the constraints are convex and adapt our algorithms and analyses to this setting by leveraging a novel convex-analysis based approach. Simulation results show improved performance over existing algorithms for a variety of randomly sampled settings.
    
[^56]: WSAM: 从风格增强的可解释性作为对抗攻击者及其对图像分类的影响

    WSAM: Visual Explanations from Style Augmentation as Adversarial Attacker and Their Influence in Image Classification. (arXiv:2308.14995v1 [cs.CV])

    [http://arxiv.org/abs/2308.14995](http://arxiv.org/abs/2308.14995)

    本文提出了一种使用基于随机抽样和噪声添加的风格增强算法，该算法在风格转移中表现出了非凡的鲁棒性，并且在STL-10数据集上超过了先前的方法和最先进的性能。

    

    当前，由于卷积神经网络（CNN）对纹理而不是形状的强烈偏好，风格增强引起了人们的关注。大多数现有的风格化方法要么执行低保真度的风格转移，要么在嵌入向量中进行弱风格表示。本文提出一种使用基于随机抽样和噪声添加的风格增强算法，以改善对风格转移的一般线性变换的随机化。通过我们的增强策略，所有模型不仅具有令人难以置信的对图像风格化的鲁棒性，而且在STL-10数据集上表现出色，超过了所有先前的方法和最先进的性能。此外，我们还对不同风格变化下的模型解释进行了分析。同时，我们比较了在训练环境中应用于深度神经架构时的性能的全面实验。

    Currently, style augmentation is capturing attention due to convolutional neural networks (CNN) being strongly biased toward recognizing textures rather than shapes. Most existing styling methods either perform a low-fidelity style transfer or a weak style representation in the embedding vector. This paper outlines a style augmentation algorithm using stochastic-based sampling with noise addition to improving randomization on a general linear transformation for style transfer. With our augmentation strategy, all models not only present incredible robustness against image stylizing but also outperform all previous methods and surpass the state-of-the-art performance for the STL-10 dataset. In addition, we present an analysis of the model interpretations under different style variations. At the same time, we compare comprehensive experiments demonstrating the performance when applied to deep neural architectures in training settings.
    
[^57]: 在人工智能中引入神经启发的适应性，以实现持续学习

    Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])

    [http://arxiv.org/abs/2308.14991](http://arxiv.org/abs/2308.14991)

    本研究引入了神经启发的适应性解决方案，以实现人工智能的持续学习。通过模拟果蝇学习系统，我们提出了一种可以灵活适应变化的通用方法，改善了学习可塑性，并确保解决方案的兼容性。

    

    持续学习旨在赋予人工智能（AI）对真实世界的强大适应能力。为此，一个理想的解决方案应该在记忆稳定性和学习可塑性之间保持适当平衡，并获得足够的兼容性来捕捉观测到的分布。现有的进展主要集中在保持记忆稳定性以克服灾难性遗忘，但仍难以像生物智能（BI）那样灵活地适应增量变化。通过建模一个能够主动调节遗忘的稳健果蝇学习系统，并利用多个学习模块，我们提出了一种通用方法，通过在参数分布中适当衰减旧记忆来改善学习可塑性，并相应地协调多学习者架构来确保解决方案的兼容性。通过广泛的理论和实证验证，我们的方法不仅明显提高了持续学习的性能，特别是在突触调节方面。

    Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
    
[^58]: 基于增量构建学习和集成域适应的滚动轴承故障诊断

    Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation. (arXiv:2308.14983v1 [cs.AI])

    [http://arxiv.org/abs/2308.14983](http://arxiv.org/abs/2308.14983)

    本文提出了一种基于增量构建学习和集成域适应的方法，用于滚动轴承故障诊断。该方法通过在随机配置网络上实施增量学习，结合云特征提取和小波包分解，提高了故障诊断的准确性和适应性。

    

    鉴于滚动轴承故障诊断在各种工况下的实际问题普遍存在，样本的有限可用性增加了挑战。此外，外部环境的复杂性和滚动轴承的结构经常表现出具有随机性和模糊性的故障，从而阻碍了故障特征的有效提取，并限制了故障诊断的准确性。为了解决这些问题，本文提出了一种称为构造增量学习的集成域适应（CIL-EDA）方法。具体而言，它在随机配置网络（SCN）上实施，以在多个域中构建其自适应性能。具体地，采用云特征提取方法结合小波包分解（WPD）来捕捉来自多个分辨率方面的故障信息的不确定性。随后，采用增量构建学习的域适应方案。

    Given the prevalence of rolling bearing fault diagnosis as a practical issue across various working conditions, the limited availability of samples compounds the challenge. Additionally, the complexity of the external environment and the structure of rolling bearings often manifests faults characterized by randomness and fuzziness, hindering the effective extraction of fault characteristics and restricting the accuracy of fault diagnosis. To overcome these problems, this paper presents a novel approach termed constructive Incremental learning-based ensemble domain adaptation (CIL-EDA) approach. Specifically, it is implemented on stochastic configuration networks (SCN) to constructively improve its adaptive performance in multi-domains. Concretely, a cloud feature extraction method is employed in conjunction with wavelet packet decomposition (WPD) to capture the uncertainty of fault information from multiple resolution aspects. Subsequently, constructive Incremental learning-based domai
    
[^59]: 子通用变分电路用于组合优化问题的翻译论文

    Sub-universal variational circuits for combinatorial optimization problems. (arXiv:2308.14981v1 [quant-ph])

    [http://arxiv.org/abs/2308.14981](http://arxiv.org/abs/2308.14981)

    本研究提出了一种基于经典概率电路的变分电路，用于解决组合优化问题。通过对Max-Cut问题的数值研究，我们发现这种变分电路在多种图上表现出更好的性能，相比于量子近似优化算法。在评估量子变分电路的性能时，可以将其与具有子通用门集的变分电路进行比较，以识别量子变分电路的优势领域。

    

    由于其在量子近似优化算法和量子机器学习研究中的应用，量子变分电路引起了广泛的关注。本研究引入了一种新颖的经典概率电路，用于生成对由二位随机矩阵构建的组合优化问题的近似解。通过数值研究，我们调查了我们提出的变分电路在解决不断增加规模的各种图的Max-Cut问题中的性能。我们的经典算法在多种类型的图上表现出更好的性能，相比于量子近似优化算法。我们的发现表明，将量子变分电路的性能与具有子通用门集的变分电路进行评估，是识别量子变分电路可突出优势领域的有价值的基准。

    Quantum variational circuits have gained significant attention due to their applications in the quantum approximate optimization algorithm and quantum machine learning research. This work introduces a novel class of classical probabilistic circuits designed for generating approximate solutions to combinatorial optimization problems constructed using two-bit stochastic matrices. Through a numerical study, we investigate the performance of our proposed variational circuits in solving the Max-Cut problem on various graphs of increasing sizes. Our classical algorithm demonstrates improved performance for several graph types to the quantum approximate optimization algorithm. Our findings suggest that evaluating the performance of quantum variational circuits against variational circuits with sub-universal gate sets is a valuable benchmark for identifying areas where quantum variational circuits can excel.
    
[^60]: 一种通过深度学习模型高效对太阳通量演化视频进行标注的方法

    Efficient labeling of solar flux evolution videos by a deep learning model. (arXiv:2308.14976v1 [astro-ph.SR])

    [http://arxiv.org/abs/2308.14976](http://arxiv.org/abs/2308.14976)

    通过粗略标注的天文视频训练卷积神经网络（CNN），可以提高数据标注质量和减少人为干预，减少标注过程中的时间消耗。

    

    机器学习（ML）正成为对大型复杂数据进行问询的关键工具。标注，即添加有意义的注释的过程，是监督式ML的一个关键步骤。然而，标注数据集非常耗时。我们在这里展示了卷积神经网络（CNN）可以利用粗略标注的天文视频来提高数据标注质量，减少人为干预的需求。我们使用太阳磁场的视频，根据它们在太阳盘上首次检测到的双极磁区（BMRs）的出现或非出现将其粗略标记成两个类别。我们使用粗略标签训练CNN，手动验证和纠正CNN与标注的不一致之处，并重复此过程直至收敛。传统上，通量出现的标注是手动完成的。我们发现通过这个迭代过程得到的高质量标注数据集可以将人工验证的需求降低50%。此外，通过逐渐屏蔽掉标签，我们可以逐步提高CNN在准确标注过程中的自信度。

    Machine learning (ML) is becoming a critical tool for interrogation of large complex data. Labeling, defined as the process of adding meaningful annotations, is a crucial step of supervised ML. However, labeling datasets is time consuming. Here we show that convolutional neural networks (CNNs), trained on crudely labeled astronomical videos, can be leveraged to improve the quality of data labeling and reduce the need for human intervention. We use videos of the solar magnetic field, crudely labeled into two classes: emergence or non-emergence of bipolar magnetic regions (BMRs), based on their first detection on the solar disk. We train CNNs using crude labels, manually verify, correct labeling vs. CNN disagreements, and repeat this process until convergence. Traditionally, flux emergence labelling is done manually. We find that a high-quality labeled dataset, derived through this iterative process, reduces the necessary manual verification by 50%. Furthermore, by gradually masking the 
    
[^61]: 分布式多智能体目标搜索和跟踪：高斯过程与强化学习

    Distributed multi-agent target search and tracking with Gaussian process and reinforcement learning. (arXiv:2308.14971v1 [cs.RO])

    [http://arxiv.org/abs/2308.14971](http://arxiv.org/abs/2308.14971)

    本研究提出了一种基于强化学习和高斯过程的多智能体目标搜索和跟踪方法，在未知目标场景下通过数据驱动的方式实现目标探索和决策规划，提高效率和准确性。

    

    部署多个机器人用于目标搜索和跟踪在实际应用中有很多困难，特别是在未知或部分已知目标的情况下。最近深度学习的进展使得智能控制技术如强化学习能够让智能体在与环境交互中从零到有一定的自主学习能力。这种方法可以通过数据驱动的方式解决未知目标规划中的勘探和利用的权衡，消除传统方法中对启发式算法的依赖，并简化决策流程。本文提出了一种基于分布式高斯过程的多智能体强化学习技术，用于目标地图构建。我们利用分布式高斯过程对目标位置进行置信度编码，并高效地规划未知目标。我们评估了训练策略的性能和可迁移性

    Deploying multiple robots for target search and tracking has many practical applications, yet the challenge of planning over unknown or partially known targets remains difficult to address. With recent advances in deep learning, intelligent control techniques such as reinforcement learning have enabled agents to learn autonomously from environment interactions with little to no prior knowledge. Such methods can address the exploration-exploitation tradeoff of planning over unknown targets in a data-driven manner, eliminating the reliance on heuristics typical of traditional approaches and streamlining the decision-making pipeline with end-to-end training. In this paper, we propose a multi-agent reinforcement learning technique with target map building based on distributed Gaussian process. We leverage the distributed Gaussian process to encode belief over the target locations and efficiently plan over unknown targets. We evaluate the performance and transferability of the trained polic
    
[^62]: 受限制下的重新编程: 重新审视彩票式转移的高效可靠性

    Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])

    [http://arxiv.org/abs/2308.14969](http://arxiv.org/abs/2308.14969)

    重新审视彩票式转移的高效可靠性，研究了线性探测（LP）和视觉提示/重新编程（VP）方法在数据稀疏性和模型稀疏性方面的能力，并发现彩票式转移并非通用的重新编程器。

    

    在预训练预算巨大的基础模型时代，下游任务已经转向了高效快速适应的叙述。对于计算机视觉领域的基于分类的任务，最高效的方法是线性探测（LP）和视觉提示/重新编程（VP）; 前者旨在通过在预训练模型提取的特征上学习线性头部分类器，而后者将输入数据映射到最初在其上进行预训练的源数据领域。尽管广泛的研究已经证明了LP和VP在下游性能方面的差异，但我们通过稀疏度轴来探索这两种方法的能力: (a) 数据稀疏性: 少样本自适应的影响，以及 (b) 模型稀疏性: 彩票式转移的影响。我们证明了彩票式转移不是通用的重新编程器，即对于某些目标数据集，重新编程彩票式转移会产生显著效果。

    In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif
    
[^63]: 通过弱SINDy算法进行流式科学数据压缩

    Streaming Compression of Scientific Data via weak-SINDy. (arXiv:2308.14962v1 [cs.LG])

    [http://arxiv.org/abs/2308.14962](http://arxiv.org/abs/2308.14962)

    本文开发了一种专门用于压缩流式科学数据的流式弱SINDy算法。与经典的离线压缩算法不同，流式压缩算法可以在数据生成过程中进行压缩，使其适用于科学数据压缩。流式弱SINDy算法利用了底层数据的特征来进行压缩。

    

    本文开发了一种针对流式科学数据压缩的流式弱SINDy算法。科学数据的产生无论是通过模拟还是实验，都正在经历一段指数增长的阶段，这使得数据压缩对于存储和利用大规模科学数据集来说变得重要且常常是必要的。与经典的“离线”压缩算法相反，这些算法可以在可用数据集上执行压缩，流式压缩算法在模拟或实验生成的数据仍在系统中流动时进行数据压缩。这个特性使得流式压缩算法非常适用于科学数据压缩，因为在离线存储整个数据集往往是不可行的。本文提出了一种新的流式压缩算法，流式弱SINDy，它在压缩过程中利用了底层数据的特征。

    In this paper a streaming weak-SINDy algorithm is developed specifically for compressing streaming scientific data. The production of scientific data, either via simulation or experiments, is undergoing an stage of exponential growth, which makes data compression important and often necessary for storing and utilizing large scientific data sets. As opposed to classical ``offline" compression algorithms that perform compression on a readily available data set, streaming compression algorithms compress data ``online" while the data generated from simulation or experiments is still flowing through the system. This feature makes streaming compression algorithms well-suited for scientific data compression, where storing the full data set offline is often infeasible. This work proposes a new streaming compression algorithm, streaming weak-SINDy, which takes advantage of the underlying data characteristics during compression. The streaming weak-SINDy algorithm constructs feature matrices and 
    
[^64]: 鲁棒的开放集言语识别和CU MultiLang数据集

    Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset. (arXiv:2308.14951v1 [cs.CL])

    [http://arxiv.org/abs/2308.14951](http://arxiv.org/abs/2308.14951)

    本文实现了一种鲁棒的开放集言语识别系统，通过使用MFCC和音高特征，TDNN模型提取特征嵌入，设置置信度阈值，以及使用LDA和pLDA学习新的未知语言分类来实现。在经过训练的语言上，系统准确率达到91.76%，并且具备实时适应未知语言的能力。

    

    大多数最先进的言语识别模型是闭集的，即它们只能输出它们在训练时使用的类别集合中的语言标签。然而，开放集言语识别系统具备检测输入是否不属于原始语言的能力。本文实现了一种新颖的开放集言语识别方法，该方法使用MFCC和音高特征， TDNN模型提取有意义的特征嵌入，通过对softmax输出进行置信度阈值处理，以及使用LDA和pLDA学习对新的未知语言进行分类。我们提出了一个言语识别系统，其在经过训练的语言上实现了91.76%的准确率，并具备实时适应未知语言的能力。为此，我们还构建了CU MultiLang数据集，这是一个大规模多样化的多语言语音语料库，用于训练和评估我们的系统。

    Most state-of-the-art spoken language identification models are closed-set; in other words, they can only output a language label from the set of classes they were trained on. Open-set spoken language identification systems, however, gain the ability to detect when an input exhibits none of the original languages. In this paper, we implement a novel approach to open-set spoken language identification that uses MFCC and pitch features, a TDNN model to extract meaningful feature embeddings, confidence thresholding on softmax outputs, and LDA and pLDA for learning to classify new unknown languages. We present a spoken language identification system that achieves 91.76% accuracy on trained languages and has the capability to adapt to unknown languages on the fly. To that end, we also built the CU MultiLang Dataset, a large and diverse multilingual speech corpus which was used to train and evaluate our system.
    
[^65]: 带有平滑感知的深度图神经网络的低比特量化

    Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation. (arXiv:2308.14949v1 [cs.LG])

    [http://arxiv.org/abs/2308.14949](http://arxiv.org/abs/2308.14949)

    本文提出了一种带有平滑感知的深度图神经网络的低比特量化解决方案，用于解决大规模和深层GNN中的效率和准确性问题。该方法通过压缩模型、学习量化范围和控制层间相似性的变化来实现高效处理和避免过度平滑问题。

    

    图神经网络（GNN）的训练和推理面临着模型规模和层数增加导致效率和准确性降低的挑战，特别是在资源受限环境下的大规模和深层GNN中更为明显。我们提出了一个端到端的解决方案，旨在解决这些挑战，以实现在资源受限环境下高效的GNN，并避免深层GNN中的过度平滑问题。我们引入了一种基于量化的方法，用于GNN的各个阶段，从训练中的消息传递到节点分类，压缩模型并实现高效处理。所提出的GNN量化器学习量化范围，即使在低比特量化下也能减小模型大小并获得可比的准确性。为了随着层数的增加而扩展，我们设计了一种训练中的消息传播机制，控制相邻节点之间相似性的逐层变化。通过将这个目标纳入带约束的Lagrangian函数和差分多元最优化问题中实现。

    Graph Neural Network (GNN) training and inference involve significant challenges of scalability with respect to both model sizes and number of layers, resulting in degradation of efficiency and accuracy for large and deep GNNs. We present an end-to-end solution that aims to address these challenges for efficient GNNs in resource constrained environments while avoiding the oversmoothing problem in deep GNNs. We introduce a quantization based approach for all stages of GNNs, from message passing in training to node classification, compressing the model and enabling efficient processing. The proposed GNN quantizer learns quantization ranges and reduces the model size with comparable accuracy even under low-bit quantization. To scale with the number of layers, we devise a message propagation mechanism in training that controls layer-wise changes of similarities between neighboring nodes. This objective is incorporated into a Lagrangian function with constraints and a differential multiplie
    
[^66]: 改进社交机器人导航的强化学习训练方法

    Improving Reinforcement Learning Training Regimes for Social Robot Navigation. (arXiv:2308.14947v1 [cs.RO])

    [http://arxiv.org/abs/2308.14947](http://arxiv.org/abs/2308.14947)

    本研究提出了一种改进社交机器人导航的强化学习训练方法，通过使用课程学习，多样化环境和建模行人等技术，实现了更好的泛化性能。

    

    为了让自主移动机器人在人类空间中导航，它们必须遵守我们的社交规范。强化学习（RL）已经成为一种有效的训练机器人导航策略的方法，以便使其能够遵守这些规范。然而，该领域中现有工作的很大一部分在简化环境中进行RL训练和测试。这限制了这些模型在未知环境中的泛化能力和其结果的实质性。我们提出了一种使用课程学习方法来提高RL社交导航方法泛化性能的方法。通过使用多种环境类型和多个动力学模型来建模行人，我们能够逐步增加训练的多样性和难度。我们的结果表明，在训练过程中使用课程学习可以实现比以前的训练方法更好的泛化性能。我们还展示了许多现有统计结果的结果之一，并对模型进行了详细分析。

    In order for autonomous mobile robots to navigate in human spaces, they must abide by our social norms. Reinforcement learning (RL) has emerged as an effective method to train robot navigation policies that are able to respect these norms. However, a large portion of existing work in the field conducts both RL training and testing in simplistic environments. This limits the generalization potential of these models to unseen environments, and the meaningfulness of their reported results. We propose a method to improve the generalization performance of RL social navigation methods using curriculum learning. By employing multiple environment types and by modeling pedestrians using multiple dynamics models, we are able to progressively diversify and escalate difficulty in training. Our results show that the use of curriculum learning in training can be used to achieve better generalization performance than previous training methods. We also show that results presented in many existing stat
    
[^67]: 用于时间医学图像序列采样的强化学习

    Reinforcement Learning for Sampling on Temporal Medical Imaging Sequences. (arXiv:2308.14946v1 [cs.LG])

    [http://arxiv.org/abs/2308.14946](http://arxiv.org/abs/2308.14946)

    本论文研究了在动态图像重建中学习采样策略的问题，通过应用深度Q学习和REINFORCE算法，成功地发现了最佳采样模式，并验证了其对图像质量的改善。

    

    高速磁共振成像通过傅里叶域子采样或更好的重建算法来处理较少的测量数据，同时生成高质量的医学图像。在给定固定重建协议的情况下确定最佳采样策略通常具有组合复杂度。在本研究中，我们应用双重深度Q学习和REINFORCE算法来学习动态图像重建的采样策略。我们考虑的数据格式是时间序列，重建方法是预训练的自编码器类型的神经网络。我们提出了一个概念验证，即强化学习算法能够有效地发现潜在于预训练重构网络中的最佳采样模式（即环境中的动态）。可以在https://github.com/zhishenhuang/RLsamp找到复制实验的代码。

    Accelerated magnetic resonance imaging resorts to either Fourier-domain subsampling or better reconstruction algorithms to deal with fewer measurements while still generating medical images of high quality. Determining the optimal sampling strategy given a fixed reconstruction protocol often has combinatorial complexity. In this work, we apply double deep Q-learning and REINFORCE algorithms to learn the sampling strategy for dynamic image reconstruction. We consider the data in the format of time series, and the reconstruction method is a pre-trained autoencoder-typed neural network. We present a proof of concept that reinforcement learning algorithms are effective to discover the optimal sampling pattern which underlies the pre-trained reconstructor network (i.e., the dynamics in the environment). The code for replicating experiments can be found at https://github.com/zhishenhuang/RLsamp.
    
[^68]: 通过正则化Wasserstein Proximals实现无噪声的抽样算法

    Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])

    [http://arxiv.org/abs/2308.14945](http://arxiv.org/abs/2308.14945)

    本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。

    

    本文考虑由潜势函数控制的分布抽样问题。本文提出了一种显式的基于评分的确定性马尔科夫链蒙特卡洛方法，使得粒子的演化变为确定性的，而不是随机微分方程的演化。评分项由正则化的Wasserstein proximal以闭合形式给出，使用采样来近似核卷积。我们在不同问题上展示了快速收敛，并且与未调整Langevin算法和Metropolis调整Langevin算法相比，显示了高斯分布的混合时间边界的改善维度依赖性。我们还推导了二次潜势函数每次迭代的分布的闭合形式表达式，表征了方差降低。实证结果表明，粒子的行为是有组织的，位于潜势的等值线上。此外，后验均值估计结果显示了该方法的有效性。

    We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
    
[^69]: 基于熵的指导深度神经网络加速收敛和改善性能

    Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance. (arXiv:2308.14938v1 [cs.CV])

    [http://arxiv.org/abs/2308.14938](http://arxiv.org/abs/2308.14938)

    本研究通过引入基于熵的损失项，通过测量神经网络处理数据时的熵变化，指导神经网络以更快速的收敛、更好的性能学习丰富的潜在数据表示。

    

    神经网络极大地增加了我们从大规模、高维度数据集中学习的能力，跨越无数学科。然而，它们的决策不易解释，计算成本高，建立和训练它们是不确定的过程。为了给这些努力增加结构，我们推导出了新的数学结果，以高效地测量全连接和卷积神经网络处理数据时的熵变化，并引入了基于熵的损失项。在基准数据集上进行的图像压缩和图像分类实验表明，这些损失项指导神经网络以更少的维度学习丰富的潜在数据表示，收敛于更少的训练轮次，并取得更好的测试指标。

    Neural networks have dramatically increased our capacity to learn from large, high-dimensional datasets across innumerable disciplines. However, their decisions are not easily interpretable, their computational costs are high, and building and training them are uncertain processes. To add structure to these efforts, we derive new mathematical results to efficiently measure the changes in entropy as fully-connected and convolutional neural networks process data, and introduce entropy-based loss terms. Experiments in image compression and image classification on benchmark datasets demonstrate these losses guide neural networks to learn rich latent data representations in fewer dimensions, converge in fewer training epochs, and achieve better test metrics.
    
[^70]: 量子预处理滤波器在小样本二值图像分类中的应用

    Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples. (arXiv:2308.14930v1 [cs.CV])

    [http://arxiv.org/abs/2308.14930](http://arxiv.org/abs/2308.14930)

    本研究探讨了量子预处理滤波器（QPF）在二值图像分类中的应用，并通过在MNIST、EMNIST和CIFAR-10上提高了分类准确率，并在GTSRB上降低了分类准确率。

    

    过去几年来，量子机器学习（QML）在研究人员中引起了极大的兴趣，因为它有潜力改变机器学习领域。已开发出利用量子力学特性的几种模型用于实际应用。本研究探讨了我们之前提出的量子预处理滤波器（QPF）在二值图像分类中的应用。我们对四个数据集进行了QPF的评估：MNIST（手写数字）、EMNIST（手写数字和字母）、CIFAR-10（照片图像）和GTSRB（真实交通标志图像）。与我们之前的多类别分类结果类似，应用QPF使得使用神经网络对MNIST、EMNIST和CIFAR-10的二值图像分类准确率分别从98.9%提高到99.2%、从97.8%提高到98.3%和从71.2%提高到76.1%，但在GTSRB上的准确率下降了从93.5%到92.0%。然后我们将QPF应用于训练样本数量较少的情况下。

    Over the past few years, there has been significant interest in Quantum Machine Learning (QML) among researchers, as it has the potential to transform the field of machine learning. Several models that exploit the properties of quantum mechanics have been developed for practical applications. In this study, we investigated the application of our previously proposed quantum pre-processing filter (QPF) to binary image classification. We evaluated the QPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits and alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic sign images). Similar to our previous multi-class classification results, the application of QPF improved the binary image classification accuracy using neural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8% to 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from 93.5% to 92.0%. We then applied QPF in cases using a smaller number of training and 
    
[^71]: Maestro: 通过可训练分解揭示低秩结构

    Maestro: Uncovering Low-Rank Structures via Trainable Decomposition. (arXiv:2308.14929v1 [cs.LG])

    [http://arxiv.org/abs/2308.14929](http://arxiv.org/abs/2308.14929)

    本研究提出了一种通过可训练分解揭示低秩结构的方法，解决了深度神经网络训练大模型时的耗时和资源消耗的问题。

    

    深度神经网络(DNNs)近年来推动和促成了人工智能突破性进展。为了提高准确性并应对新兴的应用场景，包括AR/VR和智能助手，这些模型越来越大。然而，这些大模型的训练过程耗时费力，通常只能生成一个适应所有目标的模型。为了缓解这个问题，文献中提出了各种技术，包括模型权重和更新的剪枝、稀疏化或量化。虽然可以实现高压缩率，但往往会造成计算开销或准确性损失。另外，还通过因式分解方法将低秩压缩纳入训练过程中。然而，这些技术(例如SVD)常常依赖于计算昂贵的层次分解，对于非线性模型如DNNs可能不是最优解。在本研究中，我们采用了一种新的方法，通过可训练的分解揭示低秩结构。

    Deep Neural Networks (DNNs) have been a large driver and enabler for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs. In this work, we take 
    
[^72]: 使用深度强化学习进行经济燃气轮机调度优化

    Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning. (arXiv:2308.14924v1 [cs.LG])

    [http://arxiv.org/abs/2308.14924](http://arxiv.org/abs/2308.14924)

    本研究通过将西门子能源提供的热力学软件纳入环境模型，并模拟不确定性，揭示了使用深度强化学习进行经济燃气轮机调度优化的好处，并发现Deep Q-Networks (DQN) 在算法和基准方法中获得了最高的奖励。

    

    在现代电力网络中，燃气轮机的调度策略正在发生变化。与间歇性可再生能源的日益融入相比，燃气轮机需要更频繁地以更短周期和部分负载运行。深度强化学习（DRL）最近被提出作为可以应对这种发展并在经济上调度燃气轮机的工具。DRL的主要优势是无模型优化和处理不确定性的能力，比如由不同负载或可再生能源产生的变化。在本研究中，我们实现了三种流行的DRL算法来解决加拿大阿尔伯塔省的经济燃气轮机调度问题，并通过将西门子能源提供的现有热力学软件纳入环境模型并模拟不确定性（如电价、负载和环境条件的变化）来凸显DRL的优势。

    Dispatching strategies for gas turbines (GTs) are changing in modern electricity grids. A growing incorporation of intermittent renewable energy requires GTs to operate more but shorter cycles and more frequently on partial loads. Deep reinforcement learning (DRL) has recently emerged as a tool that can cope with this development and dispatch GTs economically. The key advantages of DRL are a model-free optimization and the ability to handle uncertainties, such as those introduced by varying loads or renewable energy production. In this study, three popular DRL algorithms are implemented for an economic GT dispatch problem on a case study in Alberta, Canada. We highlight the benefits of DRL by incorporating an existing thermodynamic software provided by Siemens Energy into the environment model and by simulating uncertainty via varying electricity prices, loads, and ambient conditions. Among the tested algorithms and baseline methods, Deep Q-Networks (DQN) obtained the highest rewards w
    
[^73]: 大型语言模型中的性别偏见和刻板印象

    Gender bias and stereotypes in Large Language Models. (arXiv:2308.14921v1 [cs.CL])

    [http://arxiv.org/abs/2308.14921](http://arxiv.org/abs/2308.14921)

    研究发现大型语言模型存在性别偏见和刻板印象，它们更倾向于选择与个人性别刻板印象一致的职业，并且放大了偏见，超过了现实情况。

    

    在过去几个月中，大型语言模型（LLMs）取得了显著的进展，在许多领域打破了最先进的测试基准。本文研究LLMs在性别刻板印象方面的行为，这是先前模型中已知的一个问题。我们使用一个简单的范例来测试性别偏见的存在，这一范例建立在但与WinoBias不同，后者是一个常用的性别偏见数据集，很可能包含在目前LLMs的训练数据中。我们测试了四个最近发布的LLMs，并证明它们在男性和女性职业方面表现出有偏见的假设。本文的贡献如下：（a）LLMs在选择与人的性别刻板印象一致的职业时的概率是3-6倍；（b）这些选择与人们的感知更加一致，而不是与官方职业统计数据的真实情况一致；（c）事实上，LLMs放大了偏见，超过了人们的感知或真实情况；（d）LLMs忽视了关键的歧义

    Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities 
    
[^74]: Matbench Discovery - 一种用于评估机器学习晶体稳定性预测的框架

    Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction. (arXiv:2308.14920v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2308.14920](http://arxiv.org/abs/2308.14920)

    Matbench Discovery是一个评估机器学习晶体稳定性预测的框架，在热力学稳定性预测方面的测试中，CHGNet表现最佳。

    

    Matbench Discovery通过模拟机器学习能源模型在高通量搜索稳定无机晶体方面的应用，解决了热力学稳定性和形成能之间的差异以及域内与域外性能之间的脱节问题。此外，我们还发布了一个Python包，以便于未来模型的提交，并提供了一个在线排行榜，进一步洞察各种性能指标之间的权衡。通过对热力学稳定性预测的测试集F1得分进行排名，我们发现CHGNet > M3GNet > MACE > ALIGNN > MEGNet > CGCNN > CGCNN+P > Wrenformer > BOWSR > Voronoi tessellation fingerprints with random forest。

    Matbench Discovery simulates the deployment of machine learning (ML) energy models in a high-throughput search for stable inorganic crystals. We address the disconnect between (i) thermodynamic stability and formation energy and (ii) in-domain vs out-of-distribution performance. Alongside this paper, we publish a Python package to aid with future model submissions and a growing online leaderboard with further insights into trade-offs between various performance metrics. To answer the question which ML methodology performs best at materials discovery, our initial release explores a variety of models including random forests, graph neural networks (GNN), one-shot predictors, iterative Bayesian optimizers and universal interatomic potentials (UIP). Ranked best-to-worst by their test set F1 score on thermodynamic stability prediction, we find CHGNet > M3GNet > MACE > ALIGNN > MEGNet > CGCNN > CGCNN+P > Wrenformer > BOWSR > Voronoi tessellation fingerprints with random forest. The top 3 mod
    
[^75]: 论马尔可夫决策过程的奖励结构

    On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])

    [http://arxiv.org/abs/2308.14919](http://arxiv.org/abs/2308.14919)

    该论文研究了马尔可夫决策过程中的奖励结构，提出了一种估计器用于估计单个状态值，并通过根据奖励代替常用的基于转移的常数，提供了对强化学习中技巧的理论解释。

    

    马尔可夫决策过程可以通过转移核与奖励函数参数化。这两个因素在强化学习研究中起着重要作用，正如它们在贝尔曼方程中的存在所证明的那样。针对机器人应用中的需求，我们研究了与强化学习相关的各种"成本"，奖励是理解马尔可夫决策过程结构的核心，奖励中心概念可以阐明强化学习中的重要概念。具体而言，我们研究了策略评估的样本复杂性，并开发了一种新的估计器，其实例特定的误差界为$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$，用于估计单个状态值。在在线遗憾最小化设置下，我们将基于转移的MDP常数，直径，改进为基于奖励的常数，最大预期到达成本，并通过该常数为一种广为人知的技术，基于潜力的奖励形状提供了理论解释。

    A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
    
[^76]: RecRec: 推荐系统的算法性补救措施

    RecRec: Algorithmic Recourse for Recommender Systems. (arXiv:2308.14916v1 [cs.IR])

    [http://arxiv.org/abs/2308.14916](http://arxiv.org/abs/2308.14916)

    本文提出了一个算法性补救框架，用于帮助理解推荐系统的模型并修改推荐结果。

    

    推荐系统在娱乐、购物、食物、新闻、就业和教育等领域的决策中起着至关重要的作用。这些推荐系统背后的机器学习模型对于用户、内容提供者和系统开发者来说通常都是巨大且不透明的。对于所有利益相关者来说，理解模型在进行某些预测和推荐时的原理至关重要。对于那些生计依赖于推荐系统的内容提供者来说尤其如此。在本研究中，我们从从实际需求出发，提出了一个面向内容提供者的推荐系统补救框架。推荐设置中的算法性补救是一组操作，如果执行，将以期望的方式修改项目的推荐（或排序）。补救措施提供的操作形式为：“如果一个特征从X变为Y，那么该项目的排名也会相应变化。”

    Recommender systems play an essential role in the choices people make in domains such as entertainment, shopping, food, news, employment, and education. The machine learning models underlying these recommender systems are often enormously large and black-box in nature for users, content providers, and system developers alike. It is often crucial for all stakeholders to understand the model's rationale behind making certain predictions and recommendations. This is especially true for the content providers whose livelihoods depend on the recommender system. Drawing motivation from the practitioners' need, in this work, we propose a recourse framework for recommender systems, targeted towards the content providers. Algorithmic recourse in the recommendation setting is a set of actions that, if executed, would modify the recommendations (or ranking) of an item in the desired manner. A recourse suggests actions of the form: "if a feature changes X to Y, then the ranking of that item for a s
    
[^77]: 剪枝自注意力实现零唤醒多说话者文本到语音转换

    Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech. (arXiv:2308.14909v1 [cs.SD])

    [http://arxiv.org/abs/2308.14909](http://arxiv.org/abs/2308.14909)

    本文提出了一种剪枝方法来提高文本到语音转换模型的泛化能力，通过剪枝自注意力层中的多余连接，并采用可微的剪枝方法以自动学习剪枝阈值。实验证实该方法在零唤醒多说话者TTS中具有有效性。

    

    为了进行个性化语音生成，必须在限定的目标说话者数据中成功实现神经文本到语音（TTS）模型。为了达到这个目标，基线TTS模型需要广泛适用于领域外的数据（即目标说话者的语音）。然而，解决TTS中这个领域外泛化的问题的方法尚未被深入研究。在这项工作中，我们提出了一种有效的剪枝方法，即稀疏注意力，以提高TTS模型的泛化能力。特别地，我们剪枝自注意力层中的多余连接，其注意力权重低于阈值。为了灵活地确定剪枝强度以搜索最佳的泛化程度，我们还提出了一种新的可微的剪枝方法，使得模型可以自动学习阈值。对零唤醒多说话者TTS的评估验证了我们方法在声音质量和说话者相似性方面的有效性。

    For personalized speech generation, a neural text-to-speech (TTS) model must be successfully implemented with limited data from a target speaker. To this end, the baseline TTS model needs to be amply generalized to out-of-domain data (i.e., target speaker's speech). However, approaches to address this out-of-domain generalization problem in TTS have yet to be thoroughly studied. In this work, we propose an effective pruning method for a transformer known as sparse attention, to improve the TTS model's generalization abilities. In particular, we prune off redundant connections from self-attention layers whose attention weights are below the threshold. To flexibly determine the pruning strength for searching optimal degree of generalization, we also propose a new differentiable pruning method that allows the model to automatically learn the thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness of our method in terms of voice quality and speaker similarity.
    
[^78]: BayOTIDE: 基于贝叶斯方法的在线多元时间序列插补与函数分解

    BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])

    [http://arxiv.org/abs/2308.14906](http://arxiv.org/abs/2308.14906)

    BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。

    

    在真实世界的场景中，如交通和能源，经常观察到具有缺失值和噪声的大规模时间序列数据，甚至是不规则采样。尽管已经提出了许多插补方法，但它们大多数只适用于局部视角，即将长序列拆分为适当大小的批次进行训练。这种局部视角可能使模型忽略全局趋势或周期性模式。更重要的是，几乎所有方法都假设观测值在规则的时间间隔进行采样，并且无法处理来自不同应用的复杂不规则采样时间序列。此外，大多数现有方法都是在离线状态下进行学习的。因此，对于那些有快速到达的流数据的应用来说，它们并不合适。为了克服这些局限性，我们提出了BayOTIDE：基于贝叶斯方法的在线多元时间序列插补与函数分解。

    In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
    
[^79]: 具有分层自适应样本评估的语义分割的成熟度感知主动学习

    Maturity-Aware Active Learning for Semantic Segmentation with Hierarchically-Adaptive Sample Assessment. (arXiv:2308.14904v1 [cs.CV])

    [http://arxiv.org/abs/2308.14904](http://arxiv.org/abs/2308.14904)

    MADBAL是一种成熟度感知的主动学习方法，通过分层的方式综合考虑了不同样本定义，能够选择最有影响力的分割像素。它还采用了新的不确定性表达方式，并在早期学习阶段显著提升性能，减轻了训练负担。在Cityscapes和PASCAL VOC数据集上，MADBAL优于现有方法。

    

    语义分割的主动学习(AL)由于严重的类别不平衡和不同方式定义的“样本”(像素、区域等)而具有挑战性，使得数据分布的解释模糊不清。我们提出“成熟度感知分布拆分的主动学习”(MADBAL)，一种从分层的角度定义多视图数据分布的主动学习方法，综合考虑了不同的“样本”定义，能够选择具有综合理解的最有影响力的分割像素。MADBAL还采用了一种新的不确定性表达方式，其中包括AL支持模块来感知特性的成熟度，其加权影响持续贡献于不确定性检测。通过这种方式，MADBAL即使在早期AL阶段也能取得显著的性能提升，从而显著减轻了训练负担。我们在Cityscapes和PASCAL VOC数据集上验证了MADBAL优于现有方法的性能。

    Active Learning (AL) for semantic segmentation is challenging due to heavy class imbalance and different ways of defining "sample" (pixels, areas, etc.), leaving the interpretation of the data distribution ambiguous. We propose "Maturity-Aware Distribution Breakdown-based Active Learning'' (MADBAL), an AL method that benefits from a hierarchical approach to define a multiview data distribution, which takes into account the different "sample" definitions jointly, hence able to select the most impactful segmentation pixels with comprehensive understanding. MADBAL also features a novel uncertainty formulation, where AL supporting modules are included to sense the features' maturity whose weighted influence continuously contributes to the uncertainty detection. In this way, MADBAL makes significant performance leaps even in the early AL stage, hence reducing the training burden significantly. It outperforms state-of-the-art methods on Cityscapes and PASCAL VOC datasets as verified in our e
    
[^80]: Ad-Rec: 高级特征交互来解决推荐网络中的协变量漂移问题

    Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks. (arXiv:2308.14902v1 [cs.IR])

    [http://arxiv.org/abs/2308.14902](http://arxiv.org/abs/2308.14902)

    Ad-Rec是一个利用高级特征交互技术解决推荐网络中协变量漂移问题的模型，通过利用掩码转换器实现高阶交叉特征的学习，提高了模型质量、加速了收敛速度并减少了训练时间。

    

    推荐模型通过利用多个输入特征之间的相关性，在提供个性化用户体验方面起到重要作用。然而，基于深度学习的推荐模型经常面临用户行为和物品特征不断变化导致的协变量漂移问题。在处理数据分布漂移和适应用户行为变化方面，有效的特征交互学习至关重要。传统的特征交互技术在这种情况下存在一些局限性。本文介绍了Ad-Rec，一个利用特征交互技术来解决协变量漂移问题的高级网络。这有助于消除推荐任务中的无关交互。Ad-Rec利用掩码转换器来实现高阶交叉特征的学习，同时减轻数据分布漂移的影响。我们的方法通过Area Under Curve（AUC）指标衡量，提高了模型质量，加快了收敛速度，减少了训练时间。

    Recommendation models are vital in delivering personalized user experiences by leveraging the correlation between multiple input features. However, deep learning-based recommendation models often face challenges due to evolving user behaviour and item features, leading to covariate shifts. Effective cross-feature learning is crucial to handle data distribution drift and adapting to changing user behaviour. Traditional feature interaction techniques have limitations in achieving optimal performance in this context.  This work introduces Ad-Rec, an advanced network that leverages feature interaction techniques to address covariate shifts. This helps eliminate irrelevant interactions in recommendation tasks. Ad-Rec leverages masked transformers to enable the learning of higher-order cross-features while mitigating the impact of data distribution drift. Our approach improves model quality, accelerates convergence, and reduces training time, as measured by the Area Under Curve (AUC) metric.
    
[^81]: 高效统计方差缩减的双策略评估用于序列建模强化学习中的离线策略评估

    Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning. (arXiv:2308.14897v1 [cs.LG])

    [http://arxiv.org/abs/2308.14897](http://arxiv.org/abs/2308.14897)

    本论文提出了一种使用双策略评估的离线序列建模和离线强化学习的RL算法，通过统计方法证明具有方差缩减的特性。

    

    离线强化学习旨在利用先前收集的环境-动作交互记录的数据集来学习一个无需访问真实环境的策略。最近的研究表明，离线强化学习可以被形式化为一个序列建模问题，并通过决策转换器等方法进行监督学习来解决。尽管这些基于序列的方法在回报率方法上取得了有竞争力的结果，尤其是在需要较长的回合或回报稀缺的任务上，但在处理离线数据时并未考虑重要性采样来校正策略偏差，主要原因是缺乏行为策略和使用确定性评估策略。为此，我们提出了DPE：一种将离线序列建模和离线强化学习与统计上证明具有方差缩减性质的双策略评估（DPE）融合在一个统一框架中的RL算法。我们在多个任务中验证了我们的方法。

    Offline reinforcement learning aims to utilize datasets of previously gathered environment-action interaction records to learn a policy without access to the real environment. Recent work has shown that offline reinforcement learning can be formulated as a sequence modeling problem and solved via supervised learning with approaches such as decision transformer. While these sequence-based methods achieve competitive results over return-to-go methods, especially on tasks that require longer episodes or with scarce rewards, importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies. To this end, we propose DPE: an RL algorithm that blends offline sequence modeling and offline reinforcement learning with Double Policy Estimation (DPE) in a unified framework with statistically proven properties on variance reduction. We validate our method in multiple tasks 
    
[^82]: 预测个体治疗效果的一致性元学习器

    Conformal Meta-learners for Predictive Inference of Individual Treatment Effects. (arXiv:2308.14895v1 [cs.LG])

    [http://arxiv.org/abs/2308.14895](http://arxiv.org/abs/2308.14895)

    本文提出了一种新的一致性元学习器框架，通过在条件平均治疗效果元学习器上应用一致性预测程序，生成个体治疗效果的预测区间，同时证明了其有效性。

    

    我们研究了基于机器学习的个体治疗效果预测推理问题。以往的工作主要集中在开发能够提供条件平均治疗效果（CATE）的点估计的元学习器；这些方法是把中间的无关要素估计集合起来产生CATE估计的模型无关方法。在本文中，我们开发了一种一致性元学习器，这是一个通用框架，通过在CATE元学习器之上应用标准的一致性预测（CP）程序来生成个体治疗效果的预测区间。我们关注基于两阶段伪结果回归的广泛类别的元学习器，并开发了一个随机排序的框架来研究其有效性。我们证明了一致性元学习器的推理在其（伪结果）一致性得分在未观察到的个体治疗效果上随机优于oracle一致性得分时是边际有效的。此外，我们证明了常用的CATE元学习器...

    We investigate the problem of machine learning-based (ML) predictive inference on individual treatment effects (ITEs). Previous work has focused primarily on developing ML-based meta-learners that can provide point estimates of the conditional average treatment effect (CATE); these are model-agnostic approaches for combining intermediate nuisance estimates to produce estimates of CATE. In this paper, we develop conformal meta-learners, a general framework for issuing predictive intervals for ITEs by applying the standard conformal prediction (CP) procedure on top of CATE meta-learners. We focus on a broad class of meta-learners based on two-stage pseudo-outcome regression and develop a stochastic ordering framework to study their validity. We show that inference with conformal meta-learners is marginally valid if their (pseudo outcome) conformity scores stochastically dominate oracle conformity scores evaluated on the unobserved ITEs. Additionally, we prove that commonly used CATE meta
    
[^83]: 当困难的负样本采样与监督对比学习相遇

    When hard negative sampling meets supervised contrastive learning. (arXiv:2308.14893v1 [cs.CV])

    [http://arxiv.org/abs/2308.14893](http://arxiv.org/abs/2308.14893)

    当前图像模型在训练时常用交叉熵损失，但其在泛化和稳定性方面存在问题。本文提出了一种新的监督对比学习目标，SCHaNe，通过加权困难的负样本挖掘来提高模型性能。实验结果显示，SCHaNe在各个基准测试中的Top-1准确率优于BEiT-3。

    

    目前的图像模型通常遵循两阶段策略：在大数据集上进行预训练，然后使用交叉熵损失进行微调。许多研究表明，使用交叉熵可能导致次优的泛化和稳定性。监督对比损失通过关注类内相似性和类间差异来解决交叉熵损失的一些限制，但它忽视了困难负样本挖掘的重要性。我们提出，通过根据负样本与正样本的不相似程度进行加权，模型将从性能改进中受益。在本文中，我们引入了一种新的监督对比学习目标，SCHaNe，在微调阶段引入了困难负样本采样。实验结果表明，SCHaNe在各个基准测试中的Top-1准确率上优于强基线模型BEiT-3，而无需专门的架构、额外的数据或计算资源。

    State-of-the-art image models predominantly follow a two-stage strategy: pre-training on large datasets and fine-tuning with cross-entropy loss. Many studies have shown that using cross-entropy can result in sub-optimal generalisation and stability. While the supervised contrastive loss addresses some limitations of cross-entropy loss by focusing on intra-class similarities and inter-class differences, it neglects the importance of hard negative mining. We propose that models will benefit from performance improvement by weighting negative samples based on their dissimilarity to positive counterparts. In this paper, we introduce a new supervised contrastive learning objective, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. Without requiring specialized architectures, additional data, or extra computational resources, experimental results indicate that SCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across various benchmarks, with signific
    
[^84]: CommunityFish: 一种基于泊松分布的层次聚类文本缩放方法

    CommunityFish: A Poisson-based Document Scaling With Hierarchical Clustering. (arXiv:2308.14873v1 [cs.CL])

    [http://arxiv.org/abs/2308.14873](http://arxiv.org/abs/2308.14873)

    本文介绍了一种新的文本缩放方法CommunityFish，它利用层次聚类算法在词空间上聚类，从而揭示文本数据中独立词组（社区）的差异。

    

    文本缩放是社会科学家和政治研究人员在文本数据应用中的关键组成部分，也是一种重要的研究领域。本文介绍了一种新的文本缩放方法CommunityFish，它基于层次聚类算法Louvain，在词空间上进行聚类，通过识别共现在文档中的独立词组（称为社区），从而揭示演讲者或政党之间的差异。

    Document scaling has been a key component in text-as-data applications for social scientists and a major field of interest for political researchers, who aim at uncovering differences between speakers or parties with the help of different probabilistic and non-probabilistic approaches. Yet, most of these techniques are either built upon the agnostically bag-of-word hypothesis or use prior information borrowed from external sources that might embed the results with a significant bias. If the corpus has long been considered as a collection of documents, it can also be seen as a dense network of connected words whose structure could be clustered to differentiate independent groups of words, based on their co-occurrences in documents, known as communities. This paper introduces CommunityFish as an augmented version of Wordfish based on a hierarchical clustering, namely the Louvain algorithm, on the word space to yield communities as semantic and independent n-grams emerging from the corpus
    
[^85]: NAS-X: 基于扭曲的神经自适应平滑方法

    NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])

    [http://arxiv.org/abs/2308.14864](http://arxiv.org/abs/2308.14864)

    NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。

    

    本文提出了一种名为NAS-X的神经自适应平滑方法，该方法基于重新加权的唤醒-睡眠算法进行顺序潜变量模型的学习和推断。NAS-X适用于离散和连续潜变量，并利用平滑SMC方法来拟合比传统的重新加权唤醒-睡眠方法更广泛的模型。我们在离散和连续任务上测试了NAS-X，并发现在推断和参数恢复方面，它明显优于先前的变分和基于重新加权唤醒-睡眠方法。

    We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
    
[^86]: 评估使用熔池图像流进行印刷轨迹异常分类的关键时空学习器

    Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams. (arXiv:2308.14861v1 [cs.LG])

    [http://arxiv.org/abs/2308.14861](http://arxiv.org/abs/2308.14861)

    本研究评估了使用熔池图像流进行印刷轨迹异常分类的关键时空学习器，并介绍了一些领先的深度时空学习模型的实践应用。

    

    最近在金属增材制造（MAM）中应用机器学习已经展现出了解决普遍采用MAM技术的关键障碍的巨大潜力。这个领域的最新研究强调了利用熔池特征进行实时缺陷预测的重要性。尽管高质量的熔池图像数据具有实现精确预测的潜力，但对于能够利用增材制造过程的瞬态和时序特征的尖端时空模型的利用仍然有限。本研究介绍并实践了一些领先的深度时空学习模型，可用于分类来自不同材料、系统和应用的熔池图像流。具体而言，它研究了由空间和时间流组成的两流网络、循环空间网络和因式分解3D卷积。

    Recent applications of machine learning in metal additive manufacturing (MAM) have demonstrated significant potential in addressing critical barriers to the widespread adoption of MAM technology. Recent research in this field emphasizes the importance of utilizing melt pool signatures for real-time defect prediction. While high-quality melt pool image data holds the promise of enabling precise predictions, there has been limited exploration into the utilization of cutting-edge spatiotemporal models that can harness the inherent transient and sequential characteristics of the additive manufacturing process. This research introduces and puts into practice some of the leading deep spatiotemporal learning models that can be adapted for the classification of melt pool image streams originating from various materials, systems, and applications. Specifically, it investigates two-stream networks comprising spatial and temporal streams, a recurrent spatial network, and a factorized 3D convoluti
    
[^87]: 基于流聚类的合成少数类过采样方法SMOClust用于演化数据流

    SMOClust: Synthetic Minority Oversampling based on Stream Clustering for Evolving Data Streams. (arXiv:2308.14845v1 [cs.LG])

    [http://arxiv.org/abs/2308.14845](http://arxiv.org/abs/2308.14845)

    基于流聚类的合成少数类过采样方法SMOClust可以在演化数据流中解决类别不平衡和概念漂移的挑战，通过利用数据困难因素来适应漂移并生成合成样本。

    

    许多现实世界的数据流应用不仅面临概念漂移的挑战，还面临类别不平衡的问题。然而，很少有研究同时探索这两个挑战。现有方法在学习类别不平衡的数据流时没有考虑到数据困难因素，而这些因素在类别不平衡的数据流中是关键挑战。在这项工作中，我们提出了一种适应漂移的过采样策略，根据流聚类来合成少数类样本。动机是流聚类方法不断更新自身以反映当前潜在概念的特性，包括数据困难因素。这个特性可以使用压缩过去信息来生成最新少数类样本区域内的合成样本，而不需要显式地缓存数据。通过人工和真实世界数据流的实验证明了该方法的有效性。

    Many real-world data stream applications not only suffer from concept drift but also class imbalance. Yet, very few existing studies investigated this joint challenge. Data difficulty factors, which have been shown to be key challenges in class imbalanced data streams, are not taken into account by existing approaches when learning class imbalanced data streams. In this work, we propose a drift adaptable oversampling strategy to synthesise minority class examples based on stream clustering. The motivation is that stream clustering methods continuously update themselves to reflect the characteristics of the current underlying concept, including data difficulty factors. This nature can potentially be used to compress past information without caching data in the memory explicitly. Based on the compressed information, synthetic examples can be created within the region that recently generated new minority class examples. Experiments with artificial and real-world data streams show that the
    
[^88]: 优化虚拟现实/增强现实人机工程学：建模和预测用户颈部肌肉收缩

    Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction. (arXiv:2308.14841v1 [cs.HC])

    [http://arxiv.org/abs/2308.14841](http://arxiv.org/abs/2308.14841)

    该研究利用肌电图设备测量、建模和预测了VR用户在与虚拟环境交互时的颈部肌肉收缩水平，开发了一个生物物理启发式的计算模型，可以准确预测不同头部运动状态下的颈部收缩水平，并且可以预测仅通过目标头部姿势的潜在收缩需求。

    

    对于大规模和长期采用VR/AR体验来说，人体工程学效率是至关重要的。虽然VR/AR头戴显示器在观看时解锁了用户自然的广泛头部运动，但由于增加的硬件重量，不可避免地影响了他们的颈部肌肉舒适度。不幸的是，到目前为止，还没有多少量化的知识来理解和解决这个问题。利用肌电图设备，我们测量、建模和预测VR用户在与虚拟环境交互时头部运动时的颈部肌肉收缩水平（MCL）。具体来说，通过学习收集的生理数据，我们开发了一个生物物理启发式的计算模型，可以预测在不同头部运动状态下的颈部MCL。除了量化完成的头部运动的累积MCL之外，我们的模型还可以预测仅通过目标头部姿势的潜在MCL需求。一系列客观评估和用户研究证明了其预测准确性和普适性。

    Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR experiences. While VR/AR head-mounted displays unlock users' natural wide-range head movements during viewing, their neck muscle comfort is inevitably compromised by the added hardware weight. Unfortunately, little quantitative knowledge for understanding and addressing such an issue is available so far.  Leveraging electromyography devices, we measure, model, and predict VR users' neck muscle contraction levels (MCL) while they move their heads to interact with the virtual environment. Specifically, by learning from collected physiological data, we develop a bio-physically inspired computational model to predict neck MCL under diverse head kinematic states. Beyond quantifying the cumulative MCL of completed head movements, our model can also predict potential MCL requirements with target head poses only. A series of objective evaluations and user studies demonstrate its prediction accuracy and generality, as
    
[^89]: 应对不平衡分类中多样的少数群体问题

    Tackling Diverse Minorities in Imbalanced Classification. (arXiv:2308.14838v1 [cs.LG])

    [http://arxiv.org/abs/2308.14838](http://arxiv.org/abs/2308.14838)

    该研究提出了一种应对不平衡分类中多样的少数群体问题的方法，通过混合少数和多数群体的数据样本来生成合成样本，来解决分散分布的少数群体问题。

    

    在各种实际应用中普遍存在不平衡数据集，在训练分类器时会带来重大挑战。当处理大型数据集时，不平衡问题可能进一步恶化，使得有效训练分类器变得异常困难。为了解决这个问题，已经开发出了过采样技术，通过在少数群体和其邻居之间线性插值数据实例。然而，在许多实际场景中，例如异常检测，少数群体实例通常在特征空间中分散分布而不是聚集在一起。受领域无关数据混合的启发，我们提出通过混合少数群体和多数群体的数据样本来迭代生成合成样本。开发这样一个框架是非常复杂的，挑战包括源样本选择、混合策略选择以及底层模型和混合策略之间的协调。为了应对这些挑战，我们提出了一种解决方法。

    Imbalanced datasets are commonly observed in various real-world applications, presenting significant challenges in training classifiers. When working with large datasets, the imbalanced issue can be further exacerbated, making it exceptionally difficult to train classifiers effectively. To address the problem, over-sampling techniques have been developed to linearly interpolating data instances between minorities and their neighbors. However, in many real-world scenarios such as anomaly detection, minority instances are often dispersed diversely in the feature space rather than clustered together. Inspired by domain-agnostic data mix-up, we propose generating synthetic samples iteratively by mixing data samples from both minority and majority classes. It is non-trivial to develop such a framework, the challenges include source sample selection, mix-up strategy selection, and the coordination between the underlying model and mix-up strategies. To tackle these challenges, we formulate th
    
[^90]: 使用动态稀疏训练的持续学习：探索有效模型更新的算法

    Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v1 [cs.LG])

    [http://arxiv.org/abs/2308.14831](http://arxiv.org/abs/2308.14831)

    本文通过实证研究探究了持续学习中不同动态稀疏训练（DST）组件的影响，旨在填补关键研究空白并为持续学习下的DST提供最佳配置指导。

    

    持续学习是指智能系统从连续的数据流中以尽可能少的计算开销顺序获取和保留知识的能力。为了实现这一目标，已在文献中引入了正则化、重放、架构和参数隔离等方法。使用稀疏网络进行参数隔离，可以将神经网络的不同部分分配给不同的任务，并允许在任务相似时共享参数。动态稀疏训练(DST)是发现这些稀疏网络并为每个任务进行隔离的一种重要方法。本文是首个对CL范式下不同DST组件效果进行实证研究的研究，旨在填补关键研究空白并为CL下的DST的最佳配置提供指导。因此，我们在著名的CIFAR100和miniImage数据集上进行了综合研究，探究了各种DST组件以找到每个任务的最佳拓扑结构。

    Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImag
    
[^91]: 使用不精确神经网络的分布鲁棒统计验证

    Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])

    [http://arxiv.org/abs/2308.14815](http://arxiv.org/abs/2308.14815)

    本文提出了一种使用不精确神经网络的分布鲁棒统计验证方法，通过结合主动学习、不确定性量化和神经网络验证，可以在大量的分布上提供对黑盒系统行为的保证。

    

    在AI安全领域，一个特别具有挑战性的问题是在高维自主系统的行为上提供保证。以可达性分析为中心的验证方法无法扩展，而纯粹的统计方法受到对采样过程的分布假设的限制。相反，我们提出了一个针对黑盒系统的分布鲁棒版本的统计验证问题，其中我们的性能保证适用于大量的分布。本文提出了一种基于主动学习、不确定性量化和神经网络验证的新方法。我们方法的一个核心部分是一种称为不精确神经网络的集成技术，它提供了不确定性以指导主动学习。主动学习使用了一种称为Sherlock的全面神经网络验证工具来收集样本。在openAI gym Mujoco环境中使用多个物理模拟器进行评估。

    A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
    
[^92]: 使用神经算子的散射

    Scattering with Neural Operators. (arXiv:2308.14789v1 [hep-th])

    [http://arxiv.org/abs/2308.14789](http://arxiv.org/abs/2308.14789)

    这项研究使用神经算子进行散射预测，证明了在量子力学中的应用潜力，并展示了神经算子在两个具体问题中相比传统方法更高效的特点。

    

    机器学习的最新进展确立了一类称为神经算子的神经网络架构能够近似函数空间之间的映射关系。受到将其应用于基础物理学的前景的启发，我们研究了其在量子力学中散射过程的应用。我们使用了傅里叶神经算子的迭代变体来学习薛定谔算子的物理学，该算子将初始波函数和势场映射到最终波函数。这些深度运算符学习的思想在两个具体问题上进行了测试：一个神经算子预测一个在$1+1$维度中与中心势场发生散射的波包的时间演化，以及$2+1$维度中的双缝实验。在推理过程中，与传统的有限差分求解器相比，神经算子可以提高数个数量级的效率。

    Recent advances in machine learning establish the ability of certain neural-network architectures called neural operators to approximate maps between function spaces. Motivated by a prospect of employing them in fundamental physics, we examine applications to scattering processes in quantum mechanics. We use an iterated variant of Fourier neural operators to learn the physics of Schr\"odinger operators, which map from the space of initial wave functions and potentials to the final wave functions. These deep operator learning ideas are put to test in two concrete problems: a neural operator predicting the time evolution of a wave packet scattering off a central potential in $1+1$ dimensions, and the double-slit experiment in $2+1$ dimensions. At inference, neural operators can become orders of magnitude more efficient compared to traditional finite-difference solvers.
    
[^93]: 基于相关性的模糊聚类有效性指标与次要选项检测器

    A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])

    [http://arxiv.org/abs/2308.14785](http://arxiv.org/abs/2308.14785)

    本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。

    

    应用聚类分析时，最佳聚类数量是主要关注点之一。已经引入了多个聚类有效性指标来解决这个问题。然而，在某些情况下，有多个选项可以作为最终的聚类数量。大多数现有工作在这个领域忽视了这一方面。在本研究中，我们引入了一种基于相关性的模糊聚类有效性指标，称为Wiroonsri-Preedasawakul（WP）指标。该指标根据一对数据点的实际距离与相应对的调整质心之间的距离之间的相关性来定义。我们评估并比较了我们的指标与Xie-Beni，Pakhira-Bandyopadhyay-Maulik，Tang，Wu-Li，广义C和Kwon2等几个现有指标的性能。我们在四种类型的数据集上进行了评估：人工数据集，现实世界数据集，带有等级的模拟数据集和图像数据集，使用模糊c-mea算法。

    The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
    
[^94]: 在差分隐私下生成表格数据集

    Generating tabular datasets under differential privacy. (arXiv:2308.14784v1 [cs.LG])

    [http://arxiv.org/abs/2308.14784](http://arxiv.org/abs/2308.14784)

    该论文研究了在差分隐私的约束下生成表格数据集的问题，通过利用生成对抗网络（GAN），它解决了训练数据的记忆重复和隐私泄露的问题，并提出了与传统方法相比更好的解决方案。

    

    机器学习在各个领域和行业中推动了进展，但其依赖于可访问和高质量的训练数据。一些最重要的数据集以表格和关系数据库的形式出现在生物医学和金融领域。但这些表格数据通常具有敏感性质。合成数据生成可以揭示敏感数据的潜力，但生成模型往往会记忆和重复训练数据，从而破坏隐私目标。为了解决这个问题，研究人员将差分隐私（DP）的数学框架融入深度神经网络的训练过程中。但这会在生成的数据的质量和隐私之间产生权衡。生成对抗网络（GAN）是在差分隐私下合成表格数据的主要范式，但受到不稳定的对抗训练和模式坍塌的困扰，这些问题在隐私约束和复杂的表格数据模态下更加严重。

    Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This 
    
[^95]: 在一般树网络上处理不平衡数据的分布式双坐标上升方法

    Distributed Dual Coordinate Ascent with Imbalanced Data on a General Tree Network. (arXiv:2308.14783v1 [cs.LG])

    [http://arxiv.org/abs/2308.14783](http://arxiv.org/abs/2308.14783)

    本论文研究了树网络中不平衡数据对分布式双坐标上升算法收敛性的影响，并提出了延迟广义分布式双坐标上升方法来解决这个问题。实验结果表明，该方法有效提高了算法的收敛速度。

    

    本论文研究了在分布式机器学习中，不平衡数据对于解决基于经验损失最小化问题的分布式双坐标上升算法在树网络中收敛性的影响。为了解决这个问题，我们提出了一种称为延迟广义分布式双坐标上升方法的方法，该方法考虑了不平衡数据的信息，并提供了所提算法的分析。数值实验证实了我们提出的方法在改善树网络中分布式双坐标上升算法的收敛速度方面的有效性。

    In this paper, we investigate the impact of imbalanced data on the convergence of distributed dual coordinate ascent in a tree network for solving an empirical loss minimization problem in distributed machine learning. To address this issue, we propose a method called delayed generalized distributed dual coordinate ascent that takes into account the information of the imbalanced data, and provide the analysis of the proposed algorithm. Numerical experiments confirm the effectiveness of our proposed method in improving the convergence speed of distributed dual coordinate ascent in a tree network.
    
[^96]: 冲突感知的主动有限状态机学习

    Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])

    [http://arxiv.org/abs/2308.14781](http://arxiv.org/abs/2308.14781)

    C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。

    

    主动有限状态机学习算法在处理观测数据中的冲突（同一输入对应不同输出）方面存在困难。这种固有的冲突恢复能力不足，影响了它们在存在噪声或学习中的系统变化场景中的有效应用。我们提出了冲突感知的主动有限状态机学习（C3AL）框架，以在学习过程中处理冲突信息。核心思想是将所谓的观测树视为学习过程的一等公民。尽管这个想法在最近的研究中得到了探索，但我们通过将其与任何现有的学习算法结合，并在面对冲突时最小化对正在学习的系统执行的测试次数，充分发挥了它的作用。我们在大量的基准测试中评估了C3AL，涵盖了30多个不同的真实目标和18,000多个不同的场景。评估结果表明，C3AL是一个合适的替代方法。

    Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
    
[^97]: 高通量塑料特性的系统化低维度高光谱图像

    Systematic reduction of Hyperspectral Images for high-throughput Plastic Characterization. (arXiv:2308.14776v1 [eess.IV])

    [http://arxiv.org/abs/2308.14776](http://arxiv.org/abs/2308.14776)

    本研究系统性地降维高光谱图像，实现对高通量塑料特性的评估。同时应用化学统计学中的高科技发展，通过自动化和循证数据缩减，提高了非负矩阵分解（NMF）的速度和性能。

    

    高光谱成像（HSI）结合显微镜和光谱学，用于评估物体中光谱活性化合物的空间分布，并在食品质量控制、制药过程和废物分类等领域具有广泛应用。然而，由于HSI数据集的体积庞大，在有限的数码基础设施中进行分析和存储可能具有挑战性，特别是在废物分类中，速度和数据存储资源有限。此外，像素和变量的选择对于保留化学信息至关重要，因为大多数光谱数据存在显著的冗余。近年来，化学统计学中的高科技发展使得自动化和循证数据缩减成为可能，这可以极大地提高非负矩阵分解（NMF）的速度和性能，NMF是一种广泛应用于HSI数据化学分析的算法。通过恢复分布化合物的纯贡献图和光谱特征，NMF可以提供证据。

    Hyperspectral Imaging (HSI) combines microscopy and spectroscopy to assess the spatial distribution of spectroscopically active compounds in objects, and has diverse applications in food quality control, pharmaceutical processes, and waste sorting. However, due to the large size of HSI datasets, it can be challenging to analyze and store them within a reasonable digital infrastructure, especially in waste sorting where speed and data storage resources are limited. Additionally, as with most spectroscopic data, there is significant redundancy, making pixel and variable selection crucial for retaining chemical information. Recent high-tech developments in chemometrics enable automated and evidence-based data reduction, which can substantially enhance the speed and performance of Non-Negative Matrix Factorization (NMF), a widely used algorithm for chemical resolution of HSI data. By recovering the pure contribution maps and spectral profiles of distributed compounds, NMF can provide evide
    
[^98]: XVir：一种用于从癌症样本中识别病毒读取序列的基于Transformer的架构

    XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples. (arXiv:2308.14769v1 [q-bio.GN])

    [http://arxiv.org/abs/2308.14769](http://arxiv.org/abs/2308.14769)

    XVir是一种基于Transformer的深度学习架构，用于可靠识别人类肿瘤中的病毒DNA。该架构通过对来自病毒和人类基因组的基因组测序读取进行训练，并可以与肿瘤序列信息一起使用以找到病毒DNA的证据。

    

    估计全球约15%的癌症与病毒感染有关。可以引起或增加癌症风险的病毒包括人乳头瘤病毒、乙肝和丙肝病毒、埃普斯坦-巴尔病毒和人类免疫缺陷病毒等。随着测序技术的最新进展，对大量肿瘤DNA数据的计算分析使得研究癌症和病毒病原体之间的潜在关联成为可能。然而，肿瘤病毒家族的高多样性使得可靠检测病毒DNA变得困难，从而使得此类分析具有挑战性。在本文中，我们介绍了一种名为XVir的数据管道，它依赖于基于Transformer的深度学习架构，可可靠地识别人类肿瘤中的病毒DNA。具体而言，XVir是通过对来自病毒和人类基因组的基因组测序读取进行训练的，并可以与肿瘤序列信息一起使用以找到病毒DNA的证据。

    It is estimated that approximately 15% of cancers worldwide can be linked to viral infections. The viruses that can cause or increase the risk of cancer include human papillomavirus, hepatitis B and C viruses, Epstein-Barr virus, and human immunodeficiency virus, to name a few. The computational analysis of the massive amounts of tumor DNA data, whose collection is enabled by the recent advancements in sequencing technologies, have allowed studies of the potential association between cancers and viral pathogens. However, the high diversity of oncoviral families makes reliable detection of viral DNA difficult and thus, renders such analysis challenging. In this paper, we introduce XVir, a data pipeline that relies on a transformer-based deep learning architecture to reliably identify viral DNA present in human tumors. In particular, XVir is trained on genomic sequencing reads from viral and human genomes and may be used with tumor sequence information to find evidence of viral DNA in hu
    
[^99]: 扩散模型中的统一概念编辑

    Unified Concept Editing in Diffusion Models. (arXiv:2308.14761v1 [cs.CV])

    [http://arxiv.org/abs/2308.14761](http://arxiv.org/abs/2308.14761)

    本文提出了一种称为统一概念编辑（UCE）的方法，通过使用一个闭合解决方案对模型进行编辑，同时解决文本到图像模型中的偏见、版权和冒犯性内容等问题。实验证明了该方法的改进和可扩展性。

    

    文本到图像模型存在各种安全问题，可能限制其适用性。先前的方法分别解决了文本到图像模型中的偏见、版权和冒犯性内容等各个问题。然而，在真实世界中，所有这些问题都同时出现在同一个模型中。我们提出了一种使用单一方法解决所有问题的方法，名为统一概念编辑（UCE）。我们的方法在不经过训练的情况下通过闭合解决方案对模型进行编辑，并可无缝地扩展到文本条件下的扩散模型上进行并行编辑。我们通过编辑文本到图像的投影来展示可扩展的同时去偏见、消除风格和内容调节，我们进行了大量实验证明了相对于先前方法的改进效果和可扩展性。我们的代码可以在https://unified.baulab.info上找到。

    Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at https://unified.baulab.info
    
[^100]: 愿原力与你同在：统一的力导向预训练用于3D分子构型

    May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations. (arXiv:2308.14759v1 [physics.chem-ph])

    [http://arxiv.org/abs/2308.14759](http://arxiv.org/abs/2308.14759)

    本论文提出了一种统一的力导向预训练模型用于3D分子构型，涵盖了平衡和非平衡数据。通过直接从原子力中学习非平衡数据和使用零力正则化和基于力的去噪技术近似近平衡力，我们获得了一个包含超过1500万个多样构型的预训练模型，相比于未预训练的模型，我们将力的准确性提高了大约3倍。

    

    最近的研究显示了学习预训练模型在3D分子表示方面的潜力。然而，现有的预训练模型主要关注平衡数据，很大程度上忽视了非平衡构型。将这些方法扩展到非平衡数据是具有挑战性的，因为它们的训练目标依赖于构型是局部能量极小值的假设。我们通过提出一种力导向的预训练模型来填补这一差距，用于3D分子构型的平衡和非平衡数据。对于非平衡数据，我们的模型直接从原子力中学习。对于平衡数据，我们引入零力正则化和基于力的去噪技术来近似近平衡力。我们获得了一个具有超过1500万个多样的构型的统一预训练模型。实验表明，相比于未预训练的模型，在我们的预训练目标下，我们将力的准确性提高了大约3倍。

    Recent works have shown the promise of learning pre-trained models for 3D molecular representation. However, existing pre-training models focus predominantly on equilibrium data and largely overlook off-equilibrium conformations. It is challenging to extend these methods to off-equilibrium data because their training objective relies on assumptions of conformations being the local energy minima. We address this gap by proposing a force-centric pretraining model for 3D molecular conformations covering both equilibrium and off-equilibrium data. For off-equilibrium data, our model learns directly from their atomic forces. For equilibrium data, we introduce zero-force regularization and forced-based denoising techniques to approximate near-equilibrium forces. We obtain a unified pre-trained model for 3D molecular representation with over 15 million diverse conformations. Experiments show that, with our pre-training objective, we increase forces accuracy by around 3 times compared to the un
    
[^101]: 强化学习在生成型人工智能中的应用：综述

    Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14328](http://arxiv.org/abs/2308.14328)

    该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。

    

    深度生成型人工智能是机器学习界中一个长期存在的重要主题，可以影响到诸多应用领域，如文本生成和计算机视觉。训练生成模型的主要范式是最大似然估计，通过减小模型分布和目标分布之间的差异来推动学习器捕捉并逼近目标数据分布。这种公式成功地建立了生成任务的目标，然而却无法满足使用者对生成模型的所有需求。强化学习作为一种竞争性选择，通过创建新的目标来注入新的训练信号，展示了它的强大和灵活性，可以从多个角度引入人类归纳偏好，如对抗学习、手动设计规则和学习奖励模型，以建立一个性能良好的模型。

    Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
    
[^102]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^103]: 图上不平衡学习的综述：问题、技术和未来方向

    A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])

    [http://arxiv.org/abs/2308.13821](http://arxiv.org/abs/2308.13821)

    本综述对图上不平衡学习进行了全面的审视，旨在纠正数据分布偏差，以获得更准确和代表性的学习结果。

    

    图表示与世界各种场景中普遍存在的相互连接的结构。有效的图分析技术，如图学习方法，使用户能够从图数据中获得深刻的洞察力，为节点分类和链路预测等各种任务提供支持。然而，这些方法常常面临数据不平衡的问题，即在图数据中某些片段拥有大量数据而其他数据稀缺，从而导致偏倚的学习结果。这就需要出现了图上不平衡学习的新兴领域，旨在纠正这些数据分布偏差，以获得更准确和代表性的学习结果。在本综述中，我们对图上不平衡学习的文献进行了全面的审视。我们首先提供了对该概念和相关术语的明确理解，为读者建立了扎实的基础知识。随后，我们提出了两个全面的分类法：（1）问题分类法（Problem Taxonomy）。

    Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
    
[^104]: 强化学习辅助进化算法：调查和研究机会

    Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2308.13420](http://arxiv.org/abs/2308.13420)

    本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。

    

    进化算法是一类基于自然进化原理的随机搜索方法，因其在各种实际优化问题中的卓越性能而广受赞誉。尽管全球的研究人员提出了各种各样的进化算法，但仍存在一些限制，如收敛速度慢和泛化能力差。因此，许多学者积极探索改进算法结构、操作符、搜索模式等方法，以提高其优化性能。近年来，将强化学习作为进化算法框架的一个组成部分，已经展示出超越性能。本文综述了将强化学习集成到进化算法中的最新研究进展，被称为强化学习辅助进化算法（RL-EA）。我们首先介绍了强化学习和进化算法的概念。然后，我们提供了一个对RL-EA中不同结构、操作符和搜索模式的分类方法。

    Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
    
[^105]: 超越文档页分类：设计、数据集和挑战

    Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])

    [http://arxiv.org/abs/2308.12896](http://arxiv.org/abs/2308.12896)

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。

    

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，即在测试数据的性质上（$X$：多通道、多页、多行业；$Y$：类别分布和标签集的多样性）和考虑的分类任务上（$f$：多页文档、页面流和文档捆绑分类，...）。我们确定了公共的多页文档分类数据集的缺乏，并规范了应用场景中产生的不同分类任务，并激发了以高效的多页文档表示为目标的价值。对提出的多页文档分类数据集进行的实验研究表明，当前的基准测试已经变得无关紧要，并需要更新以评估实际中自然发生的完整文档。这个现实情况检查也呼吁更成熟的评估方法，涵盖校准评估、推理复杂性（时间-内存）和一系列现实分散情况。

    This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
    
[^106]: EquiDiff:一种用于轨迹预测的条件等变扩散模型

    EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction. (arXiv:2308.06564v1 [cs.LG])

    [http://arxiv.org/abs/2308.06564](http://arxiv.org/abs/2308.06564)

    EquiDiff是一种基于条件扩散模型的轨迹预测深度生成模型，通过整合历史信息和随机噪声来生成未来车辆轨迹，并利用几何特性和社交交互提取技术来提高性能。

    

    准确的轨迹预测对于自动驾驶车辆的安全和高效运行至关重要。深度学习的盛行使得出现了许多轨迹预测方法。尽管确定性深度学习模型被广泛使用，但深度生成模型因其从训练数据中学习数据分布并考虑轨迹不确定性而受到欢迎。在本研究中，我们提出了EquiDiff，一种用于预测未来车辆轨迹的深度生成模型。EquiDiff基于条件扩散模型，通过结合历史信息和随机高斯噪声来生成未来轨迹。EquiDiff的骨干模型是一个SO(2)等变换器，充分利用了位置坐标的几何特性。此外，我们还使用循环神经网络和图注意力网络来提取历史轨迹中的社交交互。为了评估EquiDiff的性能，我们进行了实验。

    Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, w
    
[^107]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^108]: 保持对称性的程序表示法用于学习代码语义

    Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03312](http://arxiv.org/abs/2308.03312)

    本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。

    

    大型语言模型(LLMs)在自动程序推理方面显示出潜力，这是许多安全任务的关键方面。然而，现有的用于代码的LLM架构通常从其他领域（如自然语言处理）借用，引发对其泛化能力和对未知代码的健壮性的担忧。一个关键的泛化挑战是将代码语义的知识，包括控制和数据流，纳入LLM架构中。受到利用平移对称性的卷积层的启发，我们探索了代码对称性如何增强程序分析和建模的LLM架构。我们提供了一个严格的群论框架，形式化地定义了代码对称性作为保持语义的变换，并提供了在LLM架构中精确推理对称性保持的技术。利用这个框架，我们引入了一种保持程序对称性的新型自注意力变体，并展示了其有效性。

    Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
    
[^109]: 面向稳健点云分类的风险优化异常值去除方法

    Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])

    [http://arxiv.org/abs/2307.10875](http://arxiv.org/abs/2307.10875)

    提出了一种面向稳健点云分类的风险优化异常值去除方法，利用普通训练的模型消除额外的异常值并恢复数据。方法通过归因分析确定每个点对模型输出的影响，使用条件风险价值优化高风险点的过滤过程。该方法在不需要额外训练的情况下能够产生出色的结果。

    

    点云深度模型在安全关键任务中的使用越来越普遍，但是点云噪声可能会影响这些模型的可靠性和安全性。为了解决这个问题，我们提出了一种新颖的点云异常值去除方法(PointCVaR)，它可以使标准训练的模型消除额外的异常值并恢复数据。我们的方法首先进行归因分析，以确定每个点对模型输出的影响，我们将其称为点的风险。然后，我们使用条件风险价值 (CVaR) 作为目标，优化高风险点的过滤过程。这种方法的基本原理是观察到点云噪声点往往聚集在风险分布的尾部，频率低但风险水平高，从而对分类结果产生重大干扰。尽管不需要额外的训练，我们的方法却能产生出色的结果。

    The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
    
[^110]: 基于图谱的可解释年龄预测

    Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])

    [http://arxiv.org/abs/2307.07439](http://arxiv.org/abs/2307.07439)

    本研究提出了一种基于图谱的可解释年龄预测方法，利用全身图像研究了各个身体部位的年龄相关变化。通过使用解释性方法和配准技术，确定了最能预测年龄的身体区域，并创下了整个身体年龄预测的最新水平。研究结果表明，脊柱、本原性背部肌肉和心脏区域是最重要的关注领域。

    

    年龄预测是医学评估和研究的重要部分，可以通过突出实际年龄和生物年龄之间的差异来帮助检测疾病和异常衰老。为了全面了解各个身体部位的年龄相关变化，我们使用了全身图像进行研究。我们利用Grad-CAM解释性方法确定最能预测一个人年龄的身体区域。通过使用配准技术生成整个人群的解释性图，我们将分析扩展到个体之外。此外，我们以一个平均绝对误差为2.76年的模型，创下了整个身体年龄预测的最新水平。我们的研究结果揭示了三个主要的关注领域：脊柱、本原性背部肌肉和心脏区域，其中心脏区域具有最重要的作用。

    Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
    
[^111]: inTformer: 一种基于时间嵌入的关注机制Transformer用于使用连接车辆数据的路口事故可能性预测

    inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])

    [http://arxiv.org/abs/2307.03854](http://arxiv.org/abs/2307.03854)

    inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。

    

    实时事故可能性预测模型是主动交通安全管理系统的关键组成部分。多年来，许多研究尝试构建事故可能性预测模型，以提高交通安全，但主要集中在高速公路上。在大多数现有研究中，研究人员主要采用基于深度学习的框架来识别事故潜在风险。最近，Transformer已经成为一种潜在的深度神经网络，其基本原理是通过注意力机制来进行操作。Transformer在功能上比现有的深度学习模型（如LSTM，CNN等）具有几个优势。首先，Transformer可以轻松处理数据序列中的长期依赖性。其次，Transformer可以在训练期间并行处理数据序列中的所有元素。最后，Transformer不存在梯度消失的问题。认识到Transformer的巨大潜力，

    The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
    
[^112]: 全能SAM：从弱注释到基于提示的微观细胞核分割

    All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])

    [http://arxiv.org/abs/2307.00290](http://arxiv.org/abs/2307.00290)

    本论文介绍了一种称为全能SAM的流程，通过在整个AI开发工作流程中使用SAM，并且在推理阶段无需手动提示，实现了从弱注释到基于提示的像素级细胞核分割的目标。

    

    目前，Segment Anything Model (SAM)是一种使用提示的通用零样本分割模型。然而，该流程在推理阶段仍然需要手动提示，对于生物医学图像分割仍然资源密集。本文介绍了一种称为全能SAM的流程，它在整个AI开发工作流程中使用了SAM，并且在推理阶段无需手动提示。具体而言，SAM首先利用弱提示（例如点、边界框）生成像素级注释，然后使用像素级注释对SAM分割模型进行微调。实验结果表明了两个关键发现：1）所提出的pi

    The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pi
    
[^113]: 评估社交机器人导航算法的原则与指南

    Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])

    [http://arxiv.org/abs/2306.16740](http://arxiv.org/abs/2306.16740)

    本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。

    

    在人类居住环境中导航是部署机器人广泛应用的主要挑战，通常被称为社交机器人导航。虽然社交导航领域近年来取得了很大进展，但评估解决社交导航的算法仍然困难，因为它不仅涉及机器人在静态环境中移动，还涉及到动态的人类参与者及其对机器人行为的感知适应性。相比之下，清晰、可重复、易于获得的基准在计算机视觉、自然语言处理和传统机器人导航等领域加速了进展，使研究人员能够公平比较算法，揭示现有解决方案的局限性，并呈现有前途的新方向。我们相信相同的方法可以有助于社交导航。在本文中，我们为评估社交机器人导航建立了共同、广泛可用且可重复的基准标准，并提出了自己的创新点。

    A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
    
[^114]: 大型语言模型被误导：使用Only Connect Wall数据集探索创造性问题解决和Einstellung效应。

    Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11167](http://arxiv.org/abs/2306.11167)

    这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。

    

    自从人工智能诞生以来，对人类仿真智能的追求一直是人工智能研究的持久话题。最新一代的大型语言模型（LLM）的技术演进和新兴能力将这个主题从学术界带到了文化时代。尽管最近的NLP评估基准任务测试了人类仿真行为的一些方面（例如BIG-bench的“类人行为”任务），但几乎没有一个任务考察创造性问题解决能力。人类的创造性问题解决是认知神经科学中研究较为深入的主题，标准化测试主要使用将线索词之间的（异构）连接能力作为创造性的度量。在这样的任务中，暗示性的误导性刺激-被称为“诱导误解”的干扰因素-通过固定效应和Einstellung范式阻碍了人类的表现。在认知神经科学的研究中，通过事先让参与者接触到有相似拼写的错误因素来实验性地诱导这样的固定。

    The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
    
[^115]: 在Interspeech会议上研究可重复性：一种长期和比较的视角。

    Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective. (arXiv:2306.10033v1 [cs.DL])

    [http://arxiv.org/abs/2306.10033](http://arxiv.org/abs/2306.10033)

    该论文调查了跨越语音和语言处理学科的七个会议中，源代码可用性对于Interspeech会议要少于40%，并提出了建议以提高未来研究的可重复性。

    

    可重复性是横跨学科的科学进展的关键方面，降低开放科学的障碍是Interspeech 2023主题的焦点领域。源代码的可用性是促进可重复性的指标之一。然而，与领域内其他会议相比，Interspeech会议的再现率较低是我们鲜有了解的。为了填补这个空白，我们对跨越语音和语言处理学科的七个会议的27,717篇论文进行了调查。我们发现，尽管接受的论文数量与其他会议相近，但Interspeech的可用源代码较少，达到40%。除了报告我们在研究过程中遇到的困难，我们还提供了建议和可能的方向，以增加可重复性进行进一步研究。

    Reproducibility is a key aspect for scientific advancement across disciplines, and reducing barriers for open science is a focus area for the theme of Interspeech 2023. Availability of source code is one of the indicators that facilitates reproducibility. However, less is known about the rates of reproducibility at Interspeech conferences in comparison to other conferences in the field. In order to fill this gap, we have surveyed 27,717 papers at seven conferences across speech and language processing disciplines. We find that despite having a close number of accepted papers to the other conferences, Interspeech has up to 40% less source code availability. In addition to reporting the difficulties we have encountered during our research, we also provide recommendations and possible directions to increase reproducibility for further studies.
    
[^116]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^117]: 修复公平性，而不是破坏准确性：使用AutoML的性能感知公平修复

    Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML. (arXiv:2306.09297v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2306.09297](http://arxiv.org/abs/2306.09297)

    本文提出了一个使用AutoML技术来减少偏见的新方法，该方法通过改进优化函数和搜索空间，并结合公平目标，在几乎不损失准确性的情况下减少基于ML的软件中的偏见。同时提出了一个适用于AutoML的公平感知搜索空间修剪方法，以减少计算成本和修复时间。

    

    机器学习（ML）越来越多地被用于关键决策软件中，但事故引发了关于ML预测公平性的质疑。为了解决这个问题，需要新的工具和方法来减少基于ML的软件中的偏见。先前的研究提出了偏见缓解算法，但这些算法只能在特定情况下工作，并且通常会导致准确性损失。我们提出了一种新颖的方法，利用自动机器学习（AutoML）技术来减少偏见。我们的方法包括两个关键创新：一种新颖的优化函数和一个公平感知的搜索空间。通过改进AutoML的默认优化函数并结合公平目标，我们能够在几乎不损失准确性的情况下减少偏见。此外，我们还提出了一种适用于AutoML的公平感知搜索空间修剪方法，以减少计算成本和修复时间。我们的方法建立在最先进的Auto-Sklearn工具上，旨在减少偏见。

    Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias i
    
[^118]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^119]: 在深度受限核机器分类器中结合原始和对偶表示

    Combining Primal and Dual Representations in Deep Restricted Kernel Machines Classifiers. (arXiv:2306.07015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07015](http://arxiv.org/abs/2306.07015)

    该论文提出了一种在深度学习和核机器分类器中结合原始和对偶表示的新方法，通过将可见单元和隐藏单元的核主成分分析和最小二乘支持向量机相结合，实现了多个级别的深度架构。该方法在计算效率上具有优势，适用于高维输入和大数据集。

    

    在深度学习与核机器的背景下，深度受限核机器（DRKM）框架允许将多个级别的核主成分分析（KPCA）和最小二乘支持向量机（LSSVM）结合为一个使用可见单元和隐藏单元的深度架构。我们提出了一种新的DRKM分类方法，将KPCA和分类级别的目标相结合，隐藏特征矩阵位于Stiefel流形上。分类级别可以表示为LSSVM或MLP特征图，结合级别和层数的深度。分类级别在其原始形式中表达，而KPCA的深度级别在其对偶形式中可以将数据的最信息化组分嵌入到一个更低维的空间中。对偶设置独立于输入的维度，原始设置是参数化的，这使得所提出的方法在高维输入和大数据集上都具有计算效率。

    In the context of deep learning with kernel machines, the deep Restricted Kernel Machine (DRKM) framework allows multiple levels of kernel PCA (KPCA) and Least-Squares Support Vector Machines (LSSVM) to be combined into a deep architecture using visible and hidden units. We propose a new method for DRKM classification coupling the objectives of KPCA and classification levels, with the hidden feature matrix lying on the Stiefel manifold. The classification level can be formulated as an LSSVM or as an MLP feature map, combining depth in terms of levels and layers. The classification level is expressed in its primal formulation, as the deep KPCA levels, in their dual formulation, can embed the most informative components of the data in a much lower dimensional space. The dual setting is independent of the dimension of the inputs and the primal setting is parametric, which makes the proposed method computationally efficient for both high-dimensional inputs and large datasets. In the experi
    
[^120]: Annotator Demographics Matter - Measuring the Influence of Annotator Demographics with the POPQUORN Dataset

    When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset. (arXiv:2306.06826v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.06826](http://arxiv.org/abs/2306.06826)

    标注者的背景对数据标注的影响很重要。通过POPQUORN数据集的分析，我们发现标注者的背景在他们的判断中起到了显著作用，并且应该考虑以前未考虑的背景因素。我们的研究建议理解标注者的背景，从具有人口统计学平衡的众包工作者中收集标签，以减少数据集的偏差。

    

    标注者的背景和经历会影响他们对数据的标注，然而，自然语言处理领域只近期开始考虑标注者身份对他们决策的影响。本文介绍了POPQUORN（POtato-Prolific 数据集，用于问题回答、冒犯性、文本改写和礼貌评分，包含人口统计学细微差异）。POPQUORN包含1,484个标注者的45,000个标注，采用了美国人口中性别、年龄和种族的代表样本。通过一系列分析，我们展示了标注者背景在他们的判断中起到了重要作用。此外，我们的工作还表明，在自然语言处理中以前未考虑的背景因素（例如教育）是有意义且应该被考虑的。我们的研究表明，理解标注者的背景，并从具有人口统计学平衡的众包工作者中收集标签，对减少数据集偏差非常重要。

    Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering, Offensiveness, text Rewriting, and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators' background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and anno
    
[^121]: 何时展示建议？在AI辅助编程中整合人类反馈

    When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming. (arXiv:2306.04930v1 [cs.HC])

    [http://arxiv.org/abs/2306.04930](http://arxiv.org/abs/2306.04930)

    本研究利用先前数据的干预措施提高基于AI的代码推荐系统的有效性，提出了一个CDHF框架来整合人类反馈，预测建议接受程度并决定何时展示哪些建议。

    

    基于AI的代码推荐系统，如Copilot和CodeWhisperer，提供程序员环境（例如IDE）内的代码建议，旨在提高他们的生产力。由于在这些场景中，程序员接受和拒绝建议，因此理想情况下，该系统应使用此反馈以促进这一目标。本研究利用程序员与Copilot交互的先前数据，开发可以节省程序员时间的干预措施。我们提出了一个实用理论框架，用于建模与程序员的交互，并决定何时展示哪些建议。我们的框架“基于人类反馈的条件建议展示”（CDHF）基于对程序员操作的预测模型。使用535名程序员的数据，我们构建了可以预测建议接受程度的模型。在对通过AI辅助编程解决的真实世界编程任务的回顾性评估中，我们发现CDHF能够实现有利的权衡。我们的发现表明，整合人类反馈可以显著提高基于AI的代码推荐系统的有效性。

    AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim to improve their productivity. Since, in these scenarios, programmers accept and reject suggestions, ideally, such a system should use this feedback in furtherance of this goal. In this work we leverage prior data of programmers interacting with Copilot to develop interventions that can save programmer time. We propose a utility theory framework, which models this interaction with programmers and decides when and which suggestions to display. Our framework Conditional suggestion Display from Human Feedback (CDHF) is based on predictive models of programmer actions. Using data from 535 programmers we build models that predict the likelihood of suggestion acceptance. In a retrospective evaluation on real-world programming tasks solved with AI-assisted programming, we find that CDHF can achieve favorable tradeoffs. Our findings s
    
[^122]: AI多传感器融合系统鲁棒性基准测试：挑战和机遇

    Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities. (arXiv:2306.03454v1 [cs.SE])

    [http://arxiv.org/abs/2306.03454](http://arxiv.org/abs/2306.03454)

    本篇论文研究了AI多传感器融合系统的鲁棒性问题，特别关注在安全关键的环境中的表现和可靠性，并提出了基于AI的MSF感知系统基准测试套件。

    

    基于多传感器融合（MSF）的感知系统是支撑许多工业应用和领域的基础，例如自动驾驶汽车、机器人臂和无人机。近年来，数据驱动的人工智能（AI）在深度学习技术方面的快速进步为MSF系统的性能提高提供了快速增长的趋势，特别是在智能系统及其感知系统方面。尽管提出了许多基于AI的MSF感知系统和技术，但到目前为止，公开的专注于MSF感知的基准测试有限。鉴于许多智能系统（如自动驾驶汽车）在感知系统在安全关键的环境中运行，这就急需更深入地了解这些MSF系统的性能和可靠性。为了弥补这一差距，我们朝着这个方向迈出了早期的一步，并构建了一个基于AI的MSF感知系统基准测试套件，专注于对抗性攻击的鲁棒性和不同环境下的泛化性能。

    Multi-Sensor Fusion (MSF) based perception systems have been the foundation in supporting many industrial applications and domains, such as self-driving cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the fast progress in data-driven artificial intelligence (AI) has brought a fast-increasing trend to empower MSF systems by deep learning techniques to further improve performance, especially on intelligent systems and their perception systems. Although quite a few AI-enabled MSF perception systems and techniques have been proposed, up to the present, limited benchmarks that focus on MSF perception are publicly available. Given that many intelligent systems such as self-driving cars are operated in safety-critical contexts where perception systems play an important role, there comes an urgent need for a more in-depth understanding of the performance and reliability of these MSF systems. To bridge this gap, we initiate an early step in this direction and construc
    
[^123]: 关于大型模型推理中的最优缓存与模型复用

    On Optimal Caching and Model Multiplexing for Large Model Inference. (arXiv:2306.02003v1 [cs.LG])

    [http://arxiv.org/abs/2306.02003](http://arxiv.org/abs/2306.02003)

    本文提出了最优缓存与模型复用两种方法来缓解大型模型推理中资源消耗和延迟挑战，经过实证模拟发现这种组合大大提高了传统模型推理方法的性能。

    

    大型语言模型和其他大型基础模型已经取得了显著的成功，但其尺寸加剧了现有的资源消耗和延迟挑战。本文研究了两种方法来缓解这些挑战：利用缓存存储先前的查询和学习模型复用器来选择用于查询处理的模型。理论上，我们提供了一种最优算法来联合优化这两种方法，从而减少离线和在线制表环境中的推理成本。通过将缓存算法和模型复用器相结合，我们在离线和在线设置下都实现了最优性能。实证模拟表明，我们的缓存和模型复用算法的组合大大提高了传统模型推理方法的性能。

    Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference. In this paper, we study two approaches for mitigating these challenges: employing a cache to store previous queries and learning a model multiplexer to choose from an ensemble of models for query processing.  Theoretically, we provide an optimal algorithm for jointly optimizing both approaches to reduce the inference cost in both offline and online tabular settings. By combining a caching algorithm, namely Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we achieve optimal rates in both offline and online settings. Empirically, simulations show that the combination of our caching and model multiplexing algorithms greatly improves over the 
    
[^124]: 大型长序列模型的块级并行Transformer

    Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])

    [http://arxiv.org/abs/2305.19370](http://arxiv.org/abs/2305.19370)

    本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。

    

    Transformer已经成为最先进的自然语言处理模型的基石，在各种AI应用中展现出出色的性能。然而，Transformer中的自我注意机制和大型前馈网络所需的内存容量限制了它们处理长序列的能力，从而为涉及多个长序列或长期依赖的任务带来了挑战。我们提出了一种独特的方法，块级并行Transformer（BPT），它利用块级计算自我注意和前馈网络融合以最小化内存成本。通过在保持内存效率的同时处理更长的输入序列，BPT使训练序列的长度比原始的Transformer长32倍，比先前的内存高效方法长2到4倍。对语言建模和强化学习任务进行的大量实验证明了BPT在减少内存需求和提高性能方面的有效性。

    Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
    
[^125]: 在随机博弈中使用奖励机制的强化学习

    Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2305.17372](http://arxiv.org/abs/2305.17372)

    该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。

    

    我们研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习。我们利用奖励机制来整合高层次的复杂任务知识。我们开发了一种称为QRM-SG的算法，用于学习每个智能体在纳什均衡下的最佳应答策略。在QRM-SG中，我们在增广状态空间中定义了纳什均衡下的Q函数。增广状态空间整合了随机博弈的状态和奖励机制的状态。每个智能体学习了系统中所有智能体的Q函数。我们证明了在QRM-SG中学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是在学习过程中每个时间步的阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。我们使用Lemke-Howson方法来得出给定当前Q函数时的最佳应答策略。

    We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
    
[^126]: 机器学习和人工智能协作关闭高危孕产妇护理差距

    Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration. (arXiv:2305.17261v1 [cs.LG])

    [http://arxiv.org/abs/2305.17261](http://arxiv.org/abs/2305.17261)

    本论文介绍了一种基于机器学习和人工智能协作的高危孕产妇计划，提出了早期妊娠检测、准确识别高风险会员和提供可解释指标等三个挑战的解决方案，提高了孕期风险的预测准确率。

    

    健康保险公司通常使用算法来识别会受益于护理和状况管理计划的会员，该计划提供个性化的高端临床支持。算法识别与临床干预之间的及时、准确和无缝集成取决于系统设计师和护理管理员之间的有效协作。我们关注了一个旨在减少孕产妇不良产前、产期和产后事件的高危孕产妇计划，并描述了我们如何克服护理管理员所述的三个HRP计划的挑战：（1）早期检测妊娠，（2）准确识别有影响力的高风险会员，以及（3）提供可解释的指标来补充预测。我们提出了一种新的孕期识别算法，在回顾性研究中比之前基于代码的模型提前了57天识别妊娠。然后我们建立了一个模型来预测会影响孕期的并发症。

    Health insurers often use algorithms to identify members who would benefit from care and condition management programs, which provide personalized, high-touch clinical support. Timely, accurate, and seamless integration between algorithmic identification and clinical intervention depends on effective collaboration between the system designers and nurse care managers. We focus on a high-risk pregnancy (HRP) program designed to reduce the likelihood of adverse prenatal, perinatal, and postnatal events and describe how we overcome three challenges of HRP programs as articulated by nurse care managers; (1) early detection of pregnancy, (2) accurate identification of impactable high-risk members, and (3) provision of explainable indicators to supplement predictions. We propose a novel algorithm for pregnancy identification that identifies pregnancies 57 days earlier than previous code-based models in a retrospective study. We then build a model to predict impactable pregnancy complications 
    
[^127]: 剪刀手：利用重要性持久性假设在测试时对LLM KV缓存进行压缩

    Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. (arXiv:2305.17118v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17118](http://arxiv.org/abs/2305.17118)

    Scissorhands是一个可以在不对模型进行微调的情况下，通过利用重要性持久性假设将LLM KV缓存的内存使用维持在固定预算内的系统。

    

    大型语言模型（LLMs）引发了一波新的令人兴奋的人工智能应用。大规模托管这些模型需要大量的内存资源。部署过程中一个关键的内存瓶颈来自于上下文窗口。众所周知，模型权重占用大量内存；然而，在生成过程中存储的键值嵌入大小（KV缓存）往往超过了模型的大小。巨大的KV缓存大小对于关键字批处理大小的推理产生约束，这对于高吞吐量的推理工作负载至关重要。受到注意力分数的有趣观察的启发，我们提出了持久性重要性的假设：只有具有重要影响的关键标记，在一步中有实质性影响，才会在未来的生成中产生重大影响。基于对这一假设的经验验证和理论分析，我们提出了剪刀手，一个可以在不微调模型的情况下将KV缓存的内存使用维持在固定预算内的系统。

    Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In 
    
[^128]: 学习集合策略的理论保证及其在时间序列预测中的应用

    Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])

    [http://arxiv.org/abs/2305.15786](http://arxiv.org/abs/2305.15786)

    本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。

    

    集合是机器学习中最常用的工具之一，由于其能够有效地减少方差，从而提高泛化性能。针对黑盒基学习器的大多数集合方法都属于“叠加泛化”范畴，即训练一个接受基学习器推理作为输入的机器学习算法。虽然叠加泛化在实践中广泛应用，但其理论性质仍然不为人所知。本文证明了一个新的结果，表明选择基于交叉验证性能的“有限或有限维”叠加泛化中的最佳叠加泛化并不比最优解表现“差得多”。这一结果加强和大大扩展了Van der Laan等人（2007年）的结果。受到理论分析的启发，我们在概率预测的背景下进一步提出了一系列不同敏感性的叠加泛化模型。

    Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
    
[^129]: torchgfn：一个PyTorch GFlowNet库

    torchgfn: A PyTorch GFlowNet library. (arXiv:2305.14594v1 [cs.LG])

    [http://arxiv.org/abs/2305.14594](http://arxiv.org/abs/2305.14594)

    torchgfn是一个基于PyTorch构建的GFlowNet库，提供了简单的API和有用的抽象，解决了不同代码库的约定问题，并且重现了已发表的结果。

    

    随着生成流网络（GFlowNets或GFNs）的日益流行，代码来源也变得越来越多。这会妨碍实现新功能（例如训练损失），这些功能可以轻松地与现有功能进行比较，并在一组常见环境中进行使用。除了减缓GFlowNets领域的研究外，不同的代码库还使用不同的约定，可能会让新手感到困惑。torchgfn是一个基于PyTorch构建的库，旨在解决这两个问题。它为用户提供了简单的API和有用的抽象，以实现采样器和损失函数。提供了多个示例，重现了已发表的结果。该代码可在https://github.com/saleml/torchgfn中获取。

    The increasing popularity of generative flow networks (GFlowNets or GFNs) is accompanied with a proliferation of code sources. This hinders the implementation of new features, such as training losses, that can readily be compared to existing ones, on a set of common environments. In addition to slowing down research in the field of GFlowNets, different code bases use different conventions, that might be confusing for newcomers. `torchgfn` is a library built on top of PyTorch, that aims at addressing both problems. It provides user with a simple API for environments, and useful abstractions for samplers and losses. Multiple examples are provided, replicating published results. The code is available in https://github.com/saleml/torchgfn.
    
[^130]: 一种用于射电干涉成像重建的条件去噪扩散概率模型

    A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction. (arXiv:2305.09121v1 [astro-ph.IM])

    [http://arxiv.org/abs/2305.09121](http://arxiv.org/abs/2305.09121)

    本文提出了一种名为VIC-DDPM的条件去噪扩散概率模型，在可见度数据和脏图像的帮助下，能够生成更细节的图像，同时消除噪声和伪影。相关实验证实，该算法在恢复微弱信号、保留细节结构和消除伪影等方面有很好的性能。

    

    在射电天文学中，射电望远镜接收到的信号会转换为天体对象的图像。但这些图像通常会包含伪源和其他因素导致的伪影，称为“脏图像”。因此，需要对脏图像进行重建以获取更干净的图像并消除伪影。然而，现有的方法在恢复微弱信号，保留细节结构和消除伪影方面存在一定的局限性。本文提出了一种名为VIC-DDPM的可见度和图像条件下的去噪扩散概率模型。该模型同时利用时域和空域的信息来生成图像，以达到消除噪声和生成更细节的图像的目的。实验证实，我们的算法在峰值信噪比、视觉质量和恢复微弱信号的能力方面优于当前的最先进算法。

    In radio astronomy, signals from radio telescopes are transformed into images of observed celestial objects, or sources. However, these images, called dirty images, contain real sources as well as artifacts due to signal sparsity and other factors. Therefore, radio interferometric image reconstruction is performed on dirty images, aiming to produce clean images in which artifacts are reduced and real sources are recovered. So far, existing methods have limited success on recovering faint sources, preserving detailed structures, and eliminating artifacts. In this paper, we present VIC-DDPM, a Visibility and Image Conditioned Denoising Diffusion Probabilistic Model. Our main idea is to use both the original visibility data in the spectral domain and dirty images in the spatial domain to guide the image generation process with DDPM. This way, we can leverage DDPM to generate fine details and eliminate noise, while utilizing visibility data to separate signals from noise and retaining spat
    
[^131]: 通过随机行走随机交替方向乘法算法推动个性化联邦学习

    Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])

    [http://arxiv.org/abs/2304.12534](http://arxiv.org/abs/2304.12534)

    本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。

    

    在本研究中，我们探讨了在现实世界中实现联邦学习（FL）时存在的障碍，其中不能维护中央服务器与所有客户端之间的一致连接，并且数据分布是异构的。为了解决这些挑战，我们专注于动态联邦学习，其中服务器在相邻客户端组之间移动以学习本地模型。具体来说，我们提出了一种新算法，即随机行走随机交替方向乘法算法（RWSADMM），只要有足够数量的连接客户端用于模型训练，就能适应动态和即席网络条件。在RWSADMM中，服务器随机向一组客户端行走。它基于硬不等式约束形成相邻客户端之间的局部近似，而不是采用一致更新来解决数据异构性。我们提出的方法是收敛的，可以降低通信成本，通过减少训练时间提高可扩展性。

    In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th
    
[^132]: 可解释的人工智能洞察在符号计算中的应用：以圆柱代数分解变量排序为案例研究

    Explainable AI Insights for Symbolic Computation: A case study on selecting the variable ordering for cylindrical algebraic decomposition. (arXiv:2304.12154v2 [cs.SC] UPDATED)

    [http://arxiv.org/abs/2304.12154](http://arxiv.org/abs/2304.12154)

    本文研究了在符号计算中使用可解释的人工智能技术的案例，通过使用SHAP工具为圆柱代数分解的变量排序提供新的启发式方法，从而为符号计算提供了新的洞察。

    

    最近几年，在数学领域中对机器学习（ML）技术的使用有所增加，包括在符号计算中，可以安全地应用ML来优化或选择算法。本文探讨了在这些ML模型上使用可解释的人工智能（XAI）技术是否可以为符号计算提供新的洞察，以激发在计算机代数系统中实现新的方法，而不直接调用AI工具。我们提出了一个以ML为基础来选择圆柱代数分解变量排序的案例研究。已经证明ML可以进行很好的选择，但在这里，我们展示了如何利用可解释性工具SHAP来为符号计算中常用的人工设计启发式方法提供新的启示，其大小和复杂度类似。

    In recent years there has been increased use of machine learning (ML) techniques within mathematics, including symbolic computation where it may be applied safely to optimise or select algorithms. This paper explores whether using explainable AI (XAI) techniques on such ML models can offer new insight for symbolic computation, inspiring new implementations within computer algebra systems that do not directly call upon AI tools. We present a case study on the use of ML to select the variable ordering for cylindrical algebraic decomposition. It has already been demonstrated that ML can make the choice well, but here we show how the SHAP tool for explainability can be used to inform new heuristics of a size and complexity similar to those human-designed heuristics currently commonly used in symbolic computation.
    
[^133]: 多领域LiDAR语义分割中的方法研究和探讨

    Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation. (arXiv:2304.11705v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.11705](http://arxiv.org/abs/2304.11705)

    本研究通过设计第一个实验设置，探讨了LiDAR语义分割在不同领域间的泛化能力。研究结果表明，现有方法在跨领域设置中存在显著差距。为了解决这个问题，我们提出了一种新的方法，通过结合稀疏-密集卷积网络，实现了在目标领域上的显著优化。

    

    在开发智能机器人的过程中，能够安全地在多样化的环境中运行是至关重要的。我们在同一领域的LiDAR语义分割方面取得了巨大的进展。然而，这些方法是否能够在不同领域之间泛化是一个问题。为了回答这个问题，我们设计了第一个用于研究领域泛化（DG-LSS）的实验设置。我们的结果表明，在跨领域的设置中评估方法之间存在显著差距：例如，在源数据集（SemanticKITTI）上训练的模型在目标数据上获得了26.53的mIoU，而在目标域（nuScenes）上训练的模型则获得了48.49的mIoU。为了解决这个差距，我们提出了第一个专门为DG-LSS设计的方法，在目标领域上获得了34.88的mIoU，超越了所有其他基线模型。我们的方法通过将稀疏卷积编码器-解码器3D分割网络与额外的密集2D卷积相结合，

    The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional 
    
[^134]: 基于图结构的多标签节点分类

    Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])

    [http://arxiv.org/abs/2304.10398](http://arxiv.org/abs/2304.10398)

    该论文提出了一种新的图神经网络架构 MLGCN，用于处理多标签节点分类任务，并研究了多标签场景中同类偏好的语义。在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。

    

    图神经网络（GNN）在图中节点分类任务方面展示了最先进的改进。虽然这些进展在多类分类场景中得到了广泛的展示，但一个更普遍和现实的情况，在这种情况下，每个节点可能有多个标签，一直以来受到了很少关注。在进行关于多标签节点分类的重点研究的首要挑战是公开可用的多标签图数据集数量有限。因此，作为我们的第一个贡献，我们收集并发布了三个真实的生物数据集，并开发了一个多标签图生成器，以生成具有可调属性的数据集。虽然高标签相似性（高同类偏好）通常被归因于GNN的成功，但我们认为多标签场景并不遵循目前为多类场景定义的同类偏好和异类偏好的常规语义。作为我们的第二个贡献，我们除了为多标签场景定义同类偏好外，还开发了一种新颖的GNN体系结构，即MLGCN（多标签图卷积网络），来处理多标签节点分类任务。我们的实验表明，在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。

    Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
    
[^135]: 用相关静态分析产品改善少样本提示

    Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])

    [http://arxiv.org/abs/2304.06815](http://arxiv.org/abs/2304.06815)

    本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。

    

    大型语言模型是一类新型计算引擎，通过提示工程实现"编程"。我们仍在学习如何最好地"编程"这些大型语言模型以帮助开发人员。我们的研究从这样一种直觉出发，即开发人员在处理编码任务时会有一系列意识和无意识的语义事实。对于一个函数而言，这些语义事实可能包括参数和局部变量名称、返回表达式、简单的前置和后置条件以及基本的控制和数据流，等等。我们的目标是调查这个问题，使用代码摘要任务并评估是否使用显式添加信息能够帮助大型语言模型提取这些语义事实。

    Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto
    
[^136]: 基于不确定性的开放集学习用于视网膜异常识别

    Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification. (arXiv:2304.03981v1 [cs.LG])

    [http://arxiv.org/abs/2304.03981](http://arxiv.org/abs/2304.03981)

    提出了基于不确定性的开放集(UIOS)模型，用于处理训练中未见过的类别样本，该模型通过计算不确定性得分来表达其置信度，并在多个测试数据集中表现优异，为真实世界的视网膜异常筛查提供了一个强大的方法。

    

    人工智能在实际视网膜异常分类应用中的一个主要限制是无法识别训练过程中未见过的类别样本。为解决这一障碍，我们提出了一种基于不确定性的开放集(UIOS)模型，该模型使用了9个常见视网膜病变的眼底图像进行训练。除了每个类别的概率，UIOS还计算了不确定性得分来表达其置信度。我们的UIOS模型通过设置阈值策略，在内部测试数据集、外部测试数据集和非典型测试数据集中的F1分别达到了99.55％、97.01％和91.91％，相比标准人工智能模型的F1分别为92.20％、80.69％和64.74％。此外，UIOS正确预测了罕见视网膜疾病、低质量眼底图像和非眼底图像等数据集中的高不确定性分数，提示需要手动检查。这项工作为真实世界的视网膜异常筛查提供了一个强大的方法。

    Failure to recognize samples from the classes unseen during training is a major limit of artificial intelligence (AI) in real-world implementation of retinal anomaly classification. To resolve this obstacle, we propose an uncertainty-inspired open-set (UIOS) model which was trained with fundus images of 9 common retinal conditions. Besides the probability of each category, UIOS also calculates an uncertainty score to express its confidence. Our UIOS model with thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91% for the internal testing set, external testing set and non-typical testing set, respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by the standard AI model. Furthermore, UIOS correctly predicted high uncertainty scores, which prompted the need for a manual check, in the datasets of rare retinal diseases, low-quality fundus images, and non-fundus images. This work provides a robust method for real-world screening of retinal anomalies.
    
[^137]: Wyner变分自编码器用于无监督多层无线指纹识别

    The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting. (arXiv:2303.15860v1 [cs.IT])

    [http://arxiv.org/abs/2303.15860](http://arxiv.org/abs/2303.15860)

    该论文提出了一个基于Wyner变分自编码器的多层指纹识别框架，通过利用多层特征共享的设备信息，在无监督情况下提高识别性能。

    

    无线指纹识别是一种利用硬件不完善和无线信道变化作为标识的设备识别方法。除了物理层特征外，最近的研究表明可以通过网络流量（例如包长度）识别用户行为，而无需解密有效载荷。因此，我们提出了一个多层指纹识别框架，联合考虑多层特征以提高识别性能。与以前的工作不同，我们利用了最近的多视图机器学习范例，即多形式数据，可以在不需要监督的情况下聚合多层特征共享的设备信息。我们的信息论方法可以扩展到有监督和半监督设置，并通过直接推导获得简单的导出式。为了解决所制定的问题，我们使用变分推断获得紧密的代理界限，以进行有效的优化。在提取特征方面，我们使用Wyner变分自编码器来克服多视图数据分布间的分歧。

    Wireless fingerprinting refers to a device identification method leveraging hardware imperfections and wireless channel variations as signatures. Beyond physical layer characteristics, recent studies demonstrated that user behaviours could be identified through network traffic, e.g., packet length, without decryption of the payload. Inspired by these results, we propose a multi-layer fingerprinting framework that jointly considers the multi-layer signatures for improved identification performance. In contrast to previous works, by leveraging the recent multi-view machine learning paradigm, i.e., data with multiple forms, our method can cluster the device information shared among the multi-layer features without supervision. Our information-theoretic approach can be extended to supervised and semi-supervised settings with straightforward derivations. In solving the formulated problem, we obtain a tight surrogate bound using variational inference for efficient optimization. In extracting
    
[^138]: 无线干扰网络中的多流传输：基于收敛图学习方法的研究

    Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach. (arXiv:2303.15544v1 [cs.LG])

    [http://arxiv.org/abs/2303.15544](http://arxiv.org/abs/2303.15544)

    本研究提出DIAMOND算法用于无线干扰网络中的多流传输，该算法基于收敛图学习方法，既可以集中式计算多流传输策略，也可以分布式实现数据包的传输，相比现有方法提高了15-20％的网络性能。

    

    本文针对无线网络中多流传输的问题进行研究，其中不同流间的数据信号可能由于它们路径上的链路间相互干扰而使链路容量减小。其目的是开发一种多流传输策略，通过路由在无线干扰网络中传输流以最大化网络效用。然而，由于涉及大量状态和操作空间，获得最优解的计算代价是昂贵的。为应对这个挑战，我们引入了一种新的算法Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND)。DIAMOND算法的设计允许采用混合式的集中式-分布式实现，这是5G及更高技术中具有中央单元部署特征的一种。集中式阶段使用一种新颖的图神经网络 (GNN) 强化学习 (RL) 路由代理计算多流传输策略。GNN-RL代理已经训练好了以学习网络状态并设计一个考虑干扰的路由策略。然后，这个策略被用于指导沿无干扰路径传输流，随后分布式阶段则利用这个策略来传输数据包。模拟结果表明DIAMOND算法在网络性能方面比现有方法表现更好，提高了15-20％。

    We consider the problem of of multi-flow transmission in wireless networks, where data signals from different flows can interfere with each other due to mutual interference between links along their routes, resulting in reduced link capacities. The objective is to develop a multi-flow transmission strategy that routes flows across the wireless interference network to maximize the network utility. However, obtaining an optimal solution is computationally expensive due to the large state and action spaces involved. To tackle this challenge, we introduce a novel algorithm called Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. A centralized stage computes the multi-flow transmission strategy using a novel design of graph neural network (GNN) reinforcement learning (RL) routing ag
    
[^139]: 广义划分局部深度

    Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])

    [http://arxiv.org/abs/2303.10167](http://arxiv.org/abs/2303.10167)

    本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。

    

    本文提供了一个最近由Berenhaut、Moore和Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]提出的凝聚概念的概括。所提出的表述基于分区局部深度的技术并提炼了两个关键概率概念：局部相关性和支持分割。早期结果在新的背景下得到扩展，并包括在具有不确定性的数据中揭示社区的应用示例。

    In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
    
[^140]: 基于深度学习的时间序列因果推断量化北极放大的原因

    Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07122](http://arxiv.org/abs/2303.07122)

    该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。

    

    北极变暖，也称北极放大，由多种大气和海洋因素导致，但其基础热力因素的详细情况仍不清楚。使用固定治疗效应策略推断大气过程对海冰融化的因果效应会导致不现实的反事实估计。这样的模型也容易受到时间变化的混淆的影响而引起偏差。为了解决这些挑战，我们提出了TCINet - 一种基于循环神经网络的时间序列因果推断模型，以连续治疗方式推断因果关系。通过对合成和观测数据的实验，我们展示了我们的研究如何大大提高量化北极海冰融化的主要原因的能力。

    The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
    
[^141]: 使用物理信息神经网络在复杂梁系统中解决正向和逆向问题

    Physics-informed neural networks for solving forward and inverse problems in complex beam systems. (arXiv:2303.01055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01055](http://arxiv.org/abs/2303.01055)

    本文提出了一种使用物理信息神经网络（PINNs）解决复杂梁系统中正向和逆向问题的新框架，通过求解欧拉-伯努利和Timoshenko偏微分方程，高效地计算横向位移和横截面旋转，并稳健地确定未知模型参数和施加力，为工程结构和机械问题的解决提供了有前景的策略。

    

    本文提出了一种新的框架，使用物理信息神经网络（PINNs）来模拟由欧拉-伯努利和Timoshenko理论构成的复杂结构系统，其中双梁与Winkler基础相连。特别地，使用物理信息损失函数解决了欧拉-伯努利和Timoshenko偏微分方程的正向和逆向问题。高阶复杂梁偏微分方程在正向问题中被高效地解决，以计算横向位移和横截面旋转，误差小于1e-3％。此外，逆向问题稳健地解决了在整个时空域内确定未知无量纲模型参数和施加力的问题，甚至在数据噪声存在的情况下。结果表明，PINNs是解决涉及梁系统的工程结构和机械问题的一种有前景的策略。

    This paper proposes a new framework using physics-informed neural networks (PINNs) to simulate complex structural systems that consist of single and double beams based on Euler-Bernoulli and Timoshenko theory, where the double beams are connected with a Winkler foundation. In particular, forward and inverse problems for the Euler-Bernoulli and Timoshenko partial differential equations (PDEs) are solved using nondimensional equations with the physics-informed loss function. Higher-order complex beam PDEs are efficiently solved for forward problems to compute the transverse displacements and cross-sectional rotations with less than 1e-3 percent error. Furthermore, inverse problems are robustly solved to determine the unknown dimensionless model parameters and applied force in the entire space-time domain, even in the case of noisy data. The results suggest that PINNs are a promising strategy for solving problems in engineering structures and machines involving beam systems.
    
[^142]: 论ChatGPT的鲁棒性：对抗性和超出分布的视角。

    On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.12095](http://arxiv.org/abs/2302.12095)

    本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。

    

    ChatGPT是OpenAI最近发布的聊天机器人服务，并在过去几个月中受到越来越多的关注。虽然已对ChatGPT的各个方面进行了评估，但其鲁棒性，即对于未预期输入的表现，仍不清楚。鲁棒性在负责任的AI中特别受关注，特别是对于安全关键应用程序。在本文中，我们从对抗性和超出分布（OOD）的角度对ChatGPT的鲁棒性进行了彻底评估。为此，我们采用了AdvGLUE和ANLI基准来评估对抗性鲁棒性，采用Flipkart评论和DDXPlus医学诊断数据集进行OOD评估。我们选择了几个流行的基础模型作为基准。结果表明，ChatGPT在大多数对抗性和OOD分类和翻译任务上表现出一致的优势。但是，绝对的表现远非完美，这表明对抗性和OOD鲁棒性仍然是一个重要的威胁。

    ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
    
[^143]: 随机步长和循环步长引发了比常数步长更重的SGD尾部

    Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize. (arXiv:2302.05516v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05516](http://arxiv.org/abs/2302.05516)

    本文研究了深度学习中的随机步长和循环步长相对于常数步长的优势，通过考虑尾部指数如何随步长调度的变化而变化来推动理论研究，进一步揭示了它们对于泛化性能的改善机制。

    

    随机步长和循环步长在深度学习实践中被广泛使用，并且通常可以胜过常数步长选择，如SGD中的常数步长。尽管它们在经验上获得成功，但目前对于它们何时以及为什么能在理论上提高泛化性能的了解还不够。我们考虑了一类关于学习的马尔可夫步长，其中包含独立同分布的随机步长、循环步长以及常数步长等特殊情况。受到文献中显示的SGD迭代的尾部（通过所谓的“尾部指数”来衡量）的重尾现象与泛化的相关性，我们研究了尾部指数，并提供了一些理论结果，展示了尾部指数在步长调度上的变化。我们的结果为从尾部行为的角度上讨论循环和随机步长相对于常数步长的好处提供了新的理解。我们通过线性回归实验证明了我们的理论，并展示了结果。

    Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called "tail-index") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show thro
    
[^144]: 基于去偏自注意力的公平感知视觉变换器

    Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13803](http://arxiv.org/abs/2301.13803)

    这篇论文提出了一种基于去偏自注意力的公平感知视觉变换器框架，通过消除与敏感属性相关的虚假特征来减轻偏见，并利用对抗性示例来定位和屏蔽这些特征。

    

    最近，由于其提取信息特征和通过自我关注机制建模长距离依赖关系的能力，视觉变换器（ViT）在解决计算机视觉（CV）问题方面引起了广泛关注。为了充分发挥ViT在现实应用中的优势，最近的研究探索了ViT的可靠性和可解释性，包括其鲁棒性和可解释性。然而，另一个需求，公平性，在文献中尚未得到充分解决。我们证明了现有的公平感知算法（主要设计用于CNN）在ViT上表现不佳。这就需要我们通过去偏自注意（DSA）开发我们的新框架。DSA是一种通过盲目方法来强制ViT消除与敏感属性相关的虚假特征以减轻偏见的方法。值得注意的是，对抗性示例被用来定位和屏蔽输入图像块中的虚假特征。

    Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
    
[^145]: 走向AI-enabled连接产业: AGV通信和传感器测量数据集

    Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.03364](http://arxiv.org/abs/2301.03364)

    本文介绍了两个无线测量活动所提供的数据集，并将其与机器学习结合起来用于指纹识别、视线检测、服务质量预测或链路选择等任务。

    

    本文介绍了两个工业测试平台上的无线测量活动: 工业车辆间通信(iV2V)和工业车辆到基础设施加传感器(iV2I+)。提供了关于这两个捕获数据集的详细信息。iV2V涵盖了自动引导车(AGVs)之间的侧向链路通信场景，而iV2I+则是在工业设置中进行的，其中自主清洁机器人连接到私有蜂窝网络。不同通信技术的组合，连同共同的测量方法，提供了机器学习(ML)可以利用的洞察力，用于指纹识别、视线检测、服务质量预测或链路选择等任务。此外，数据集已标记和预过滤，以便快速启动和应用。对于两个数据集，还详细介绍了相应的测试平台和测量情况。

    This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
    
[^146]: 发现和利用广义网络效应

    Discovery and Exploitation of Generalized Network Effects. (arXiv:2301.00270v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2301.00270](http://arxiv.org/abs/2301.00270)

    我们提出了NetEffect，一种图挖掘方法，能够识别和理解具有少量节点标签的大型图中的广义网络效应（如同质性、异质性或二者的组合），并利用这些效应来改进下游任务的准确性和效率。

    

    针对具有少量节点标签的大型图，我们如何（a）确定图中是否存在广义网络效应（GNE），（b）估计GNE以解释节点类之间的相互关系，以及（c）利用GNE改进诸如准确高效地预测未知标签等下游任务？对于节点分类和定向广告等各种任务，了解GNE是很有价值的。然而，由于节点标签有限且边缘噪音，识别和理解GNE（如同质性、异质性或二者的组合）在现实世界的图中是具有挑战性的。我们提出了NetEffect，一种图挖掘方法来解决上述问题，具有以下特点：（i）基于原则：用统计测试确定带有少量节点标签的图中是否存在GNE；（ii）普遍且可解释：估计观察到的特定类型的GNE的闭式解决方案；（iii）准确且可扩展：集成GNE以实现准确且快速的结果。

    Given a large graph with few node labels, how can we (a) identify whether there is generalized network-effects (GNE) of the graph or not, (b) estimate GNE to explain the interrelations among node classes, and (c) exploit GNE to improve downstream tasks such as predicting the unknown labels accurately and efficiently? The knowledge of GNE is valuable for various tasks like node classification and targeted advertising. However, identifying and understanding GNE such as homophily, heterophily or their combination is challenging in real-world graphs due to limited availability of node labels and noisy edges. We propose NetEffect, a graph mining approach to address the above issues, enjoying the following properties: (i) Principled: a statistical test to determine the presence of GNE in a graph with few node labels; (ii) General and Explainable: a closed-form solution to estimate the specific type of GNE observed; and (iii) Accurate and Scalable: the integration of GNE for accurate and fast
    
[^147]: 基于贝叶斯框架的无线系统数字孪生控制、监测和数据采集

    A Bayesian Framework for Digital Twin-Based Control, Monitoring, and Data Collection in Wireless Systems. (arXiv:2212.01351v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.01351](http://arxiv.org/abs/2212.01351)

    本文提出了一个基于贝叶斯框架的无线系统数字孪生控制、监测和数据采集。该框架可以在数字孪生中量化和处理由于数据量和质量的限制导致的模型不确定性。

    

    数字孪生（DT）平台在制造业和航空航天业广泛应用，被视为控制、监测和分析基于软件的“开放”通信系统的一种有前景的范例。特别是，DT平台为测试通信系统的人工智能（AI）解决方案提供了沙盒，潜在地减少了在物理孪生（PT）上收集数据和测试算法的需求。DT系统部署中的一个关键挑战是确保DT上的虚拟控制优化、监测和分析是安全可靠的，避免“模型滥用”导致的错误决策。为解决这一挑战，本文提出了一个通用的贝叶斯框架，旨在量化和考虑在DT上由于在PT上可用的数据量和质量的限制而导致的模型不确定性。在该提议的框架中，DT建立了通信系统的贝叶斯模型。

    Commonly adopted in the manufacturing and aerospace sectors, digital twin (DT) platforms are increasingly seen as a promising paradigm to control, monitor, and analyze software-based, "open", communication systems. Notably, DT platforms provide a sandbox in which to test artificial intelligence (AI) solutions for communication systems, potentially reducing the need to collect data and test algorithms in the field, i.e., on the physical twin (PT). A key challenge in the deployment of DT systems is to ensure that virtual control optimization, monitoring, and analysis at the DT are safe and reliable, avoiding incorrect decisions caused by "model exploitation". To address this challenge, this paper presents a general Bayesian framework with the aim of quantifying and accounting for model uncertainty at the DT that is caused by limitations in the amount and quality of data available at the DT from the PT. In the proposed framework, the DT builds a Bayesian model of the communication system,
    
[^148]: 使用扰动观测器和控制屏障函数的安全高效增强学习

    Safe and Efficient Reinforcement Learning Using Disturbance-Observer-Based Control Barrier Functions. (arXiv:2211.17250v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.17250](http://arxiv.org/abs/2211.17250)

    本论文提出了一种使用扰动观测器和控制屏障函数进行安全高效增强学习的方法，与现有方法相比，该方法不需要模型学习，利用扰动观测器准确估计不确定性的点值，并将其纳入鲁棒的CBF条件中生成安全动作。

    

    近期关注越来越多的是在训练过程中保证硬状态约束满足的安全增强学习。安全过滤器，例如基于控制屏障函数(CBFs)的方法，通过即时修改增强学习代理的不安全动作来实现安全的增强学习。现有的基于安全过滤器的方法通常涉及学习不确定动力学和量化学习模型误差，这导致在收集足够的数据学习好模型之前，过滤器过于保守，从而影响了有效的探索。本文提出了一种使用扰动观测器和控制屏障函数进行安全高效增强学习的方法。与大多数现有的处理硬状态约束的安全增强学习方法不同，我们的方法不涉及模型学习，并利用扰动观测器准确估计不确定性的点值，然后将其纳入鲁棒的CBF条件生成安全动作。

    Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-bas
    
[^149]: 深度曲线编辑：针对预训练深度生成模型的可交换和非线性图像编辑

    Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model. (arXiv:2211.14573v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14573](http://arxiv.org/abs/2211.14573)

    本研究提出了一种名为DeCurvEd的新方法，以可交换的方式确定潜空间中的语义交错矢量场，从而提供了高质量的图像编辑方案。

    

    图像的语义编辑是计算机视觉的基本目标。尽管深度学习方法，如生成对抗网络（GAN），能够生成高质量的图像，但通常它们不具备对生成的图像进行语义编辑的内在方式。最近的研究探讨了通过操纵潜变量来确定要生成的图像的方法。然而，假设线性语义算术的方法在图像编辑方面具有某些局限性，而发现非线性的语义路径提供了不可交换的编辑，这在以不同的顺序应用时不一致。本研究提出了一种称为深度曲线编辑（DeCurvEd）的新方法，以确定潜空间中的语义交错矢量场。我们从理论上证明，由于可交换性，多个属性的编辑仅取决于数量而不是顺序。此外，我们还通过实验证明了该方法的成功应用。

    Semantic editing of images is the fundamental goal of computer vision. Although deep learning methods, such as generative adversarial networks (GANs), are capable of producing high-quality images, they often do not have an inherent way of editing generated images semantically. Recent studies have investigated a way of manipulating the latent variable to determine the images to be generated. However, methods that assume linear semantic arithmetic have certain limitations in terms of the quality of image editing, whereas methods that discover nonlinear semantic pathways provide non-commutative editing, which is inconsistent when applied in different orders. This study proposes a novel method called deep curvilinear editing (DeCurvEd) to determine semantic commuting vector fields on the latent space. We theoretically demonstrate that owing to commutativity, the editing of multiple attributes depends only on the quantities and not on the order. Furthermore, we experimentally demonstrate th
    
[^150]: 自然图像块的高效表示

    Efficient Representation of Natural Image Patches. (arXiv:2210.13004v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.13004](http://arxiv.org/abs/2210.13004)

    通过抽象模型，研究人员展示了如何通过非线性种群码实现自然图像块的高效表示，以实现早期视觉系统的信息传输和传感器概率分布建模的目标。

    

    在神经信息处理的复杂领域中，从次要细节中辨别出基本原理仍然是一个重大挑战。尽管我们已经对早期视觉系统的解剖学和生理学有了广泛的了解，但一个全面的计算理论仍然难以捉摸。我们能否通过抽象出系统的详细实现并专注于系统旨在解决的基本问题，来洞察生物系统的基本原理呢？利用基于最简化而又现实的假设的一个抽象模型，我们展示了如何实现早期视觉系统的两个最终目标：高效的信息传输和传感器概率分布建模。我们表明，为了优化信息传输，并不意味着获得了最优的概率分布建模。我们通过使用一个二维系统和图像块来说明，可以通过由两个类型驱动的非线性种群码实现一个高效的表示。

    In the complex domain of neural information processing, discerning fundamental principles from ancillary details remains a significant challenge. While there is extensive knowledge about the anatomy and physiology of the early visual system, a comprehensive computational theory remains elusive. Can we gain insights into the underlying principles of a biological system by abstracting away from its detailed implementation and focusing on the fundamental problems that the system is designed to solve? Utilizing an abstract model based on minimal yet realistic assumptions, we show how to achieve the early visual system's two ultimate objectives: efficient information transmission and sensor probability distribution modeling. We show that optimizing for information transmission does not yield optimal probability distribution modeling. We illustrate, using a two-pixel (2D) system and image patches, that an efficient representation can be realized via nonlinear population code driven by two ty
    
[^151]: 基于张量完成的多参数性能建模

    Multi-Parameter Performance Modeling via Tensor Completion. (arXiv:2210.10184v2 [cs.PF] UPDATED)

    [http://arxiv.org/abs/2210.10184](http://arxiv.org/abs/2210.10184)

    本论文采用低秩张量分解来建模应用程序性能，通过对应用程序执行时间的逼近，实现对未观测区域的精确外推，并应用张量完成算法优化低秩正交-多项式（CP）分解，从而提高了预测准确性和存储效率。

    

    预测应用程序性能是性能调整、软硬件协同设计和作业调度等许多任务所依赖的模型之一。我们提出并评估了低秩张量分解来建模应用程序性能。我们使用规则网格对应用程序的输入和配置域进行离散化。在网格单元中映射的应用程序执行时间被平均并表示为张量元素。我们表明，低秩的正交-多项式（CP）张量分解通过对这些张量进行逼近是有效的。我们进一步表明，这种分解使得对一个应用程序参数空间中未观测区域的精确外推成为可能。然后，我们应用张量完成来优化给定一组稀疏的观察的运行时间的CP分解。我们考虑了六个应用的替代分段/网格模型和监督学习模型，并证明使用张量完成功能优化的CP分解具有更高的预测准确性和存储效率。

    Performance tuning, software/hardware co-design, and job scheduling are among the many tasks that rely on models to predict application performance. We propose and evaluate low rank tensor decomposition for modeling application performance. We discretize the input and configuration domain of an application using regular grids. Application execution times mapped within grid-cells are averaged and represented by tensor elements. We show that low-rank canonical-polyadic (CP) tensor decomposition is effective in approximating these tensors. We further show that this decomposition enables accurate extrapolation of unobserved regions of an application's parameter space. We then employ tensor completion to optimize a CP decomposition given a sparse set of observed runtimes. We consider alternative piecewise/grid-based models and supervised learning models for six applications and demonstrate that CP decomposition optimized using tensor completion offers higher prediction accuracy and memory-e
    
[^152]: 强化学习中具有通用效用的策略梯度研究

    Policy Gradient for Reinforcement Learning with General Utilities. (arXiv:2210.00991v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00991](http://arxiv.org/abs/2210.00991)

    本文研究了强化学习中具有通用效用的策略梯度。传统的线性强化学习方法无法直接应用于非线性效用的问题，而本文推导出的策略梯度定理为解决这个问题提供了新的思路和方法。

    

    在强化学习中，智能体的目标是发现最大化期望累积奖励的最优策略。这个目标也可以看作是找到一个优化状态-动作占有度量的线性函数的策略，即线性强化学习。然而，许多监督学习和无监督学习的强化学习问题并不适用于线性强化学习框架，比如学徒学习、纯探索和变分内在控制，其中的目标是占有度量的非线性函数。非线性效用的强化学习看起来不那么方便，因为在线性强化学习中取得巨大成功的贝尔曼方程、值迭代、策略梯度、动态规划等方法无法直接泛化。在本文中，我们推导了处理具有通用效用的强化学习的策略梯度定理。策略梯度定理因其简洁易实现的特性而成为线性强化学习中的基石。

    In Reinforcement Learning (RL), the goal of agents is to discover an optimal policy that maximizes the expected cumulative rewards. This objective may also be viewed as finding a policy that optimizes a linear function of its state-action occupancy measure, hereafter referred as Linear RL. However, many supervised and unsupervised RL problems are not covered in the Linear RL framework, such as apprenticeship learning, pure exploration and variational intrinsic control, where the objectives are non-linear functions of the occupancy measures. RL with non-linear utilities looks unwieldy, as methods like Bellman equation, value iteration, policy gradient, dynamic programming that had tremendous success in Linear RL, fail to trivially generalize. In this paper, we derive the policy gradient theorem for RL with general utilities. The policy gradient theorem proves to be a cornerstone in Linear RL due to its elegance and ease of implementability. Our policy gradient theorem for RL with genera
    
[^153]: 基于深度学习的非线性因子模型中的残差：低信噪比下资产回报的精确矩阵估计

    Deep Learning Based Residuals in Non-linear Factor Models: Precision Matrix Estimation of Returns with Low Signal-to-Noise Ratio. (arXiv:2209.04512v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.04512](http://arxiv.org/abs/2209.04512)

    本文介绍了在深度学习框架中使用非线性因子模型对大型投资组合中资产回报的精确矩阵进行一致估计和收敛速度。我们的方法不仅适用于金融市场典型的低信噪比环境，还与弱因子兼容，并且通过理论分析建立了统一的界限，同时提供了基于数据的一致误差协方差估计方法。模拟和实证结果显示我们的模型具有卓越的准确性。

    

    本文介绍了在深度学习框架中使用非线性因子模型对大型投资组合中资产回报的精确矩阵进行一致估计和收敛速度。我们的估计方法即使在金融市场典型的信噪比低的环境中仍然有效，并且与弱因子兼容。我们的理论分析对于不断增加的资产数量，基于深度神经网络的预期估计风险建立了统一的界限。此外，我们提供了深度神经网络中基于数据的一致误差协方差估计方法。我们的模型在广泛的模拟和实证中表现出卓越的准确性。

    This paper introduces a consistent estimator and rate of convergence for the precision matrix of asset returns in large portfolios using a non-linear factor model within the deep learning framework. Our estimator remains valid even in low signal-to-noise ratio environments typical for financial markets and is compatible with weak factors. Our theoretical analysis establishes uniform bounds on expected estimation risk based on deep neural networks for an expanding number of assets. Additionally, we provide a new consistent data-dependent estimator of error covariance in deep neural networks. Our models demonstrate superior accuracy in extensive simulations and the empirics.
    
[^154]: 抽象模型驱动的强化学习的分析

    An Analysis of Abstracted Model-Based Reinforcement Learning. (arXiv:2208.14407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14407](http://arxiv.org/abs/2208.14407)

    本论文分析了抽象模型驱动的强化学习中的问题，揭示抽象状态会引入样本相关性，而使用鞅不等式可以解决这个问题，从而将现有MBRL算法的保证扩展到带有抽象的设置。

    

    在马尔可夫决策过程中，许多模型驱动的强化学习（MBRL）方法都能够提供对模型准确性和学习效率的保证。同时，状态抽象技术可以在保持与原问题有界损失的同时减少MDP的大小。然而，当将这两种技术结合起来时，即MBRL仅仅观察抽象状态时，却没有相应的保证可用。我们的理论分析表明，抽象可以引入在线采集样本之间的相关性（例如在真实世界中采集的样本）。这意味着，如果不考虑这种相关性，MBRL的结果不能直接推广到这个设置中。我们的结果表明，我们可以使用鞅不等式来克服这个问题。这个结果使得将现有MBRL算法的保证扩展到带有抽象的设置成为可能。

    Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate 
    
[^155]: 在自适应子模块最大化中的群体平等

    Group Equality in Adaptive Submodular Maximization. (arXiv:2207.03364v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03364](http://arxiv.org/abs/2207.03364)

    本论文研究了非自适应和自适应情况下，受群体平等约束的经典子模块最大化问题。研究发现，现有算法没有考虑公平性约束，导致一些特定群体的欠代表或过代表。因此，我们研究了在群体平等约束下选择一组项以最大化子模块效用函数。

    

    本论文研究了在非自适应和自适应设置下，受群体平等约束的经典子模块最大化问题。已经证明，许多机器学习应用的效用函数，包括数据汇总、社交网络中的影响最大化和个性化推荐，满足子模块性质。因此，在许多应用中，子模块最大化的核心是在各种约束下选择最具代表性的项（例如，数据点）。然而，大多数现有算法的设计没有考虑公平性约束，导致某些特定群体的欠代表或过代表。这激发了我们研究具有群体平等的子模块最大化问题，即在群体平等约束下选择一组项以最大化一个（可能非单调）的子模块效用函数。

    In this paper, we study the classic submodular maximization problem subject to a group equality constraint under both non-adaptive and adaptive settings. It has been shown that the utility function of many machine learning applications, including data summarization, influence maximization in social networks, and personalized recommendation, satisfies the property of submodularity. Hence, maximizing a submodular function subject to various constraints can be found at the heart of many of those applications. On a high level, submodular maximization aims to select a group of most representative items (e.g., data points). However, the design of most existing algorithms does not incorporate the fairness constraint, leading to under- or over-representation of some particular groups. This motivates us to study the submodular maximization problem with group equality, where we aim to select a group of items to maximize a (possibly non-monotone) submodular utility function subject to a group equ
    
[^156]: 一类Polyak-\L{}ojasiewicz函数在非凸性被平均处理时，Heavy Ball超越二次加速的可证明性

    Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-\L{}ojasiewicz Functions when the Non-Convexity is Averaged-Out. (arXiv:2206.11872v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.11872](http://arxiv.org/abs/2206.11872)

    本研究开发了新的技术，通过分析Hessian矩阵的变化，实现了一类Polyak-\L{}ojasiewicz函数在非凸性被平均处理时的可证明加速，对于现代机器学习模型的训练具有重要意义。

    

    Heavy Ball (HB)是当今非凸优化中最受欢迎的动量方法之一。广泛观察到，在梯度下降方法中引入Heavy Ball动力学可以加速现代机器学习模型的训练过程。然而，加速的理论基础在进展上显然远远落后于实证的成功。现有的可证明加速结果仅限于二次或接近二次函数，因为目前HB加速的技术局限于Hessian矩阵固定的情况。在本研究中，我们开发了一些新的技术，通过分析两个连续时间点上Hessian矩阵的变化对收敛速度的影响，实现了超越二次加速的证明。基于我们的技术结果，我们确定了一类Polyak-\L{}ojasiewicz（PL）优化问题，可以通过HB实现可证明的加速。此外，我们的分析证明了对于这个类别的问题，HB可以提供比传统方法更高的加速效果。

    Heavy Ball (HB) nowadays is one of the most popular momentum methods in non-convex optimization. It has been widely observed that incorporating the Heavy Ball dynamic in gradient-based methods accelerates the training process of modern machine learning models. However, the progress on establishing its theoretical foundation of acceleration is apparently far behind its empirical success. Existing provable acceleration results are of the quadratic or close-to-quadratic functions, as the current techniques of showing HB's acceleration are limited to the case when the Hessian is fixed. In this work, we develop some new techniques that help show acceleration beyond quadratics, which is achieved by analyzing how the change of the Hessian at two consecutive time points affects the convergence speed. Based on our technical results, a class of Polyak-\L{}ojasiewicz (PL) optimization problems for which provable acceleration can be achieved via HB is identified. Moreover, our analysis demonstrate
    
[^157]: PRANC：用于压缩深度模型的伪随机网络

    PRANC: Pseudo RAndom Networks for Compacting deep models. (arXiv:2206.08464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08464](http://arxiv.org/abs/2206.08464)

    PRANC是一种用于压缩深度模型的框架，通过将深度模型重新参数化为多个随机初始化的基础网络的线性组合来实现。 PRANC可以显著减小深度模型的大小，并解决了存储和传输深度模型的挑战。在图像分类任务中，PRANC表现出比基线方法更好的性能。

    

    我们展示了深度模型可以被重新参数化为在权重空间中几个随机初始化并冻结的深度模型的线性组合。在训练过程中，我们寻找存在于这些随机模型（即“基础”网络）所张成的子空间中的局部最小值。我们的框架PRANC能够显著压缩深度模型。使用一个单一的标量“种子”来生成伪随机的“基础”网络，再结合学习到的线性混合系数，可以重构模型。在实际应用中，PRANC解决了高效存储和传输深度模型的挑战，这在包括多智能体学习、持续学习、联邦系统和边缘设备等多种情况下都是一个常见的瓶颈。在本研究中，我们使用PRANC来压缩图像分类模型并通过压缩其相关的隐式神经网络来压缩图像。PRANC在图像分类任务上表现出比基线方法更好的性能。

    We demonstrate that a deep model can be reparametrized as a linear combination of several randomly initialized and frozen deep models in the weight space. During training, we seek local minima that reside within the subspace spanned by these random models (i.e., `basis' networks). Our framework, PRANC, enables significant compaction of a deep model. The model can be reconstructed using a single scalar `seed,' employed to generate the pseudo-random `basis' networks, together with the learned linear mixture coefficients.  In practical applications, PRANC addresses the challenge of efficiently storing and communicating deep models, a common bottleneck in several scenarios, including multi-agent learning, continual learners, federated systems, and edge devices, among others. In this study, we employ PRANC to condense image classification models and compress images by compacting their associated implicit neural networks. PRANC outperforms baselines with a large margin on image classificatio
    
[^158]: 智能物联网数据市场的战略联盟

    Strategic Coalition for Data Pricing in IoT Data Markets. (arXiv:2206.07785v4 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2206.07785](http://arxiv.org/abs/2206.07785)

    本文提出了一个智能物联网数据市场的战略联盟，通过博弈论的分析，基于数据价值控制数据价格，并通过高效算法解决了参与度的挑战，建立了数据市场，并最大化了社交效应。

    

    本文考虑了一个用于交易用于训练机器学习模型的物联网数据的市场。数据，无论是原始还是经过处理，通过网络提供给市场平台，并根据其对机器学习模型的价值来控制数据的价格。我们在博弈论的框架下探讨了数据的相关属性，最终得出了一个简化的分布式解决方案，该解决方案强调了设备和市场的相互利益。关键提议是一种用于市场的高效算法，共同应对参与度的可用性和异质性所带来的挑战，以及在物联网网络中的信任传递和数据交换的经济价值。所提出的方法通过加强具有相关数据的设备之间的协作机会，建立了数据市场，以避免信息泄露。在此基础上，我们提出了一个网络优化问题，最大化了社交效应。

    This paper considers a market for trading Internet of Things (IoT) data that is used to train machine learning models. The data, either raw or processed, is supplied to the market platform through a network and the price of such data is controlled based on the value it brings to the machine learning model. We explore the correlation property of data in a game-theoretical setting to eventually derive a simplified distributed solution for a data trading mechanism that emphasizes the mutual benefit of devices and the market. The key proposal is an efficient algorithm for markets that jointly addresses the challenges of availability and heterogeneity in participation, as well as the transfer of trust and the economic value of data exchange in IoT networks. The proposed approach establishes the data market by reinforcing collaboration opportunities between device with correlated data to avoid information leakage. Therein, we develop a network-wide optimization problem that maximizes the soc
    
[^159]: 基于循环分割和块模型的时间网络分析

    Recurrent segmentation meets block models in temporal networks. (arXiv:2205.09862v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2205.09862](http://arxiv.org/abs/2205.09862)

    本文通过将时间线分割成多个段并用泊松过程来建模边的参数，对时间网络中的循环活动进行建模。

    

    模型网络交互的一种流行方法是将其表示为一个节点是代理者而边是交互的网络。这些交互通常有时间戳，从而导致了带有时间戳的边。许多现实世界的时间网络具有循环或可能循环的行为。例如，社交网络活动可能在一天中的特定时间增加。在本文中，我们主要关注对这种时间网络中的循环活动进行建模。作为起点，我们使用了随机块模型，这是模拟静态网络的一种流行选择，其中节点被分成 R 组。我们通过使用泊松过程来将该模型扩展到时间网络中以建模边。我们通过将时间线分割成 K 个段来使过程的参数依赖于时间。为了实现循环活动，我们要求只能使用 H < K 个不同的参数集，也就是说，几个不一定连续的段必须共享参数。

    A popular approach to model interactions is to represent them as a network with nodes being the agents and the interactions being the edges. Interactions are often timestamped, which leads to having timestamped edges. Many real-world temporal networks have a recurrent or possibly cyclic behaviour. For example, social network activity may be heightened during certain hours of day. In this paper, our main interest is to model recurrent activity in such temporal networks. As a starting point we use stochastic block model, a popular choice for modelling static networks, where nodes are split into $R$ groups. We extend this model to temporal networks by modelling the edges with a Poisson process. We make the parameters of the process dependent on time by segmenting the time line into $K$ segments. To enforce the recurring activity we require that only $H < K$ different set of parameters can be used, that is, several, not necessarily consecutive, segments must share their parameters. We prov
    
[^160]: 来自拉什莫恩集合的差分隐私抽样以及Langevin扩散在凸优化中的普适性

    Differentially Private Sampling from Rashomon Sets, and the Universality of Langevin Diffusion for Convex Optimization. (arXiv:2204.01585v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.01585](http://arxiv.org/abs/2204.01585)

    本文提供了一种基于Langevin扩散的算法框架，可以同时实现差分隐私抽样和紧密均匀稳定性保证，从而对凸优化中的损失函数提供了最优的过度经验和总体风险保证。该框架还允许设计差分隐私均匀抽样器，应用于可解释和鲁棒机器学习等方向。

    

    在本文中，我们提供了一种基于Langevin扩散（LD）及其相应离散化的算法框架，可以同时实现：i）一种从指数机制中进行抽样的算法，其隐私分析不依赖于凸性，并且可以在任何时候停止而不损害隐私；ii）指数机制的紧密均匀稳定性保证。作为直接结果，在纯粹和近似差分隐私（DP）下，获得了（强）凸损失的最优过度经验和总体风险保证。该框架允许我们设计来自拉什莫恩集合的差分隐私均匀抽样器。拉什莫恩集合在可解释和鲁棒机器学习、理解变量重要性和表征公平性方面被广泛使用。

    In this paper we provide an algorithmic framework based on Langevin diffusion (LD) and its corresponding discretizations that allow us to simultaneously obtain: i) An algorithm for sampling from the exponential mechanism, whose privacy analysis does not depend on convexity and which can be stopped at anytime without compromising privacy, and ii) tight uniform stability guarantees for the exponential mechanism. As a direct consequence, we obtain optimal excess empirical and population risk guarantees for (strongly) convex losses under both pure and approximate differential privacy (DP). The framework allows us to design a DP uniform sampler from the Rashomon set. Rashomon sets are widely used in interpretable and robust machine learning, understanding variable importance, and characterizing fairness.
    
[^161]: 在联合学习中保护隐私和安全

    Preserving Privacy and Security in Federated Learning. (arXiv:2202.03402v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03402](http://arxiv.org/abs/2202.03402)

    该论文提出了一个原则性的框架，旨在在联合学习中实现用户隐私保证和防御攻击，并采用了安全的聚合协议和零知识证明协议来解决隐私和安全问题。

    

    联合学习已经被证实存在安全和隐私问题。现有研究要么致力于防止来自用户的攻击，要么致力于将本地模型更新对服务器进行隐藏，但并非两者兼得。然而，将这两个研究领域整合起来仍然是一个重要的挑战，因为它们在威胁模型方面常常相互冲突。在这项工作中，我们开发了一个原则性的框架，为用户提供隐私保证，并对它们的攻击进行检测。我们提出了一种安全的聚合协议，使用同态加密使服务器可以以隐私方式合并本地模型更新。然后，利用零知识证明协议，将检测攻击的任务从服务器转移到用户端。关键观察是，服务器不再需要访问本地模型以进行检测。

    Federated learning is known to be vulnerable to both security and privacy issues. Existing research has focused either on preventing poisoning attacks from users or on concealing the local model updates from the server, but not both. However, integrating these two lines of research remains a crucial challenge since they often conflict with one another with respect to the threat model. In this work, we develop a principle framework that offers both privacy guarantees for users and detection against poisoning attacks from them. With a new threat model that includes both an honest-but-curious server and malicious users, we first propose a secure aggregation protocol using homomorphic encryption for the server to combine local model updates in a private manner. Then, a zero-knowledge proof protocol is leveraged to shift the task of detecting attacks in the local models from the server to the users. The key observation here is that the server no longer needs access to the local models for a
    
[^162]: AdaTerm: 自适应T分布估计稳健矩用于噪声健壮随机梯度优化

    AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization. (arXiv:2201.06714v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.06714](http://arxiv.org/abs/2201.06714)

    AdaTerm是一种自适应T分布估计稳健矩的方法，提供了对优化算法的统一处理。

    

    随着深度学习应用的增加，从各种来源如测量误差、错误标记和估计代理输入/输出中受损的数据集不可避免地影响了优化结果，提高优化算法对噪声的稳健性已成为常见做法。之前的研究发现，Adam-like随机梯度下降优化器中使用的一阶矩可以基于学生t分布进行修改。然而，这种修改只影响了一阶矩，其他关联的统计量保持不变，导致了所假设模型的一致性问题。本文提出了AdaTerm，一种新颖方法，将学生t分布用于推导一阶矩和所有关联的统计量，从而提供了对优化算法的统一处理。

    With the increasing practicality of deep learning applications, practitioners are inevitably faced with datasets corrupted by noise from various sources such as measurement errors, mislabeling, and estimated surrogate inputs/outputs that can adversely impact the optimization results. It is a common practice to improve the optimization algorithm's robustness to noise, since this algorithm is ultimately in charge of updating the network parameters. Previous studies revealed that the first-order moment used in Adam-like stochastic gradient descent optimizers can be modified based on the Student's t-distribution. While this modification led to noise-resistant updates, the other associated statistics remained unchanged, resulting in inconsistencies in the assumed models. In this paper, we propose AdaTerm, a novel approach that incorporates the Student's t-distribution to derive not only the first-order moment but also all the associated statistics. This provides a unified treatment of the o
    
[^163]: 不完全的多视角弱标签学习

    Incomplete Multi-View Weak-Label Learning. (arXiv:2201.01079v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.01079](http://arxiv.org/abs/2201.01079)

    本文提出了一种解决多视图多标签学习中存在的不完全特征和标签、噪声视图和标签不平衡问题的新方法。该方法通过将不完全的视图和弱标签嵌入到低维子空间中，并通过自适应的权重和嵌入矩阵差异来减少冗余。实验证实了该方法的有效性。

    

    现代应用中存在多视图多标签学习问题，每个样本具有多个视图特征，多个标签通过共同的视图相关联。目前的方法通常不能直接处理每个样本只观察到部分特征和标签的情况，并忽略了真实世界问题中存在的噪声视图和不均衡标签。在本文中，我们提出了一种新方法来克服这些限制。它将不完全的视图和弱标签一起嵌入到自适应权重的低维子空间中，并通过自适应Hilbert-Schmidt独立准则（HSIC）的嵌入权重矩阵之间的差异来减少冗余。此外，它通过聚焦损失自适应学习视图的重要性以检测噪声视图，并通过聚焦损失减轻标签不平衡问题。对四个真实世界多视图多标签数据集的实验证明了所提方法的有效性。

    A variety of modern applications exhibit multi-view multi-label learning, where each sample has multi-view features, and multiple labels are correlated via common views. Current methods usually fail to directly deal with the setting where only a subset of features and labels are observed for each sample, and ignore the presence of noisy views and imbalanced labels in real-world problems. In this paper, we propose a novel method to overcome the limitations. It jointly embeds incomplete views and weak labels into a low-dimensional subspace with adaptive weights, and facilitates the difference between embedding weight matrices via auto-weighted Hilbert-Schmidt Independence Criterion (HSIC) to reduce the redundancy. Moreover, it adaptively learns view-wise importance for embedding to detect noisy views, and mitigates the label imbalance problem by focal loss. Experimental results on four real-world multi-view multi-label datasets demonstrate the effectiveness of the proposed method.
    
[^164]: 前期训练在终身学习中的作用的实证研究

    An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09153](http://arxiv.org/abs/2112.09153)

    这项研究通过对大型预训练模型在多个任务上的性能评估，发现通用的前期训练可以在终身学习中减轻灾难性遗忘的影响。

    

    机器学习中的终身学习范式不仅因其类似生物学习的特性而具有吸引力，而且因其通过避免过多的模型重新训练而减少能源浪费的潜力而备受关注。这一范式面临的关键挑战是灾难性遗忘现象。随着预训练模型在机器学习中的日益流行和成功，我们提出一个问题：在终身学习中，前期训练在灾难性遗忘方面扮演何种角色？我们在大型预训练模型的背景下研究现有方法，并在各种文本和图像分类任务中评估它们的性能，包括使用一个新颖的包含15个不同自然语言处理任务的数据集进行的大规模研究。在所有设置中，我们观察到与随机初始化模型相比，通用的前期训练在学习多个任务时隐含地缓解了灾难性遗忘的影响。

    The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo
    
[^165]: 关于对抗性Bayes分类器存在性的研究（扩展版）

    On the Existence of the Adversarial Bayes Classifier (Extended Version). (arXiv:2112.01694v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.01694](http://arxiv.org/abs/2112.01694)

    本篇论文研究了对抗训练健壮性下Bayes最优分类器的存在性问题，提出了一般性的充分条件，并可以为研究对抗性代理损失和其一致性属性提供有用的工具。

    

    对抗训练健壮性在现代机器学习应用中至关重要。虽然最近已经有多项理论研究，但与对抗训练健壮性相关的许多重要问题仍然未被解决。本文研究了一个关于对抗训练健壮性下Bayes最优分类器存在性的基本问题。我们提出了一般的充分条件，以保证存在对抗训练健壮性下的Bayes最优分类器。我们的结果可以为对后续对抗训练健壮性下代理损失和它们的一致性属性的研究提供有用的工具。本文是“关于对抗性Bayes分类器存在性”的矫正和扩展版本，该稿件已发表在NeurIPS 2021上。原始论文中有两处定理错误，一处是对伪可证健壮性的定义，另一处是针对任意度量空间的$A^\e$可测性。

    Adversarial robustness is a critical property in a variety of modern machine learning applications. While it has been the subject of several recent theoretical studies, many important questions related to adversarial robustness are still open. In this work, we study a fundamental question regarding Bayes optimality for adversarial robustness. We provide general sufficient conditions under which the existence of a Bayes optimal classifier can be guaranteed for adversarial robustness. Our results can provide a useful tool for a subsequent study of surrogate losses in adversarial robustness and their consistency properties. This manuscript is the extended and corrected version of the paper \emph{On the Existence of the Adversarial Bayes Classifier} published in NeurIPS 2021. There were two errors in theorem statements in the original paper -- one in the definition of pseudo-certifiable robustness and the other in the measurability of $A^\e$ for arbitrary metric spaces. In this version we 
    
[^166]: 基于知识蒸馏的人类启发多智能体导航

    Human-Inspired Multi-Agent Navigation using Knowledge Distillation. (arXiv:2103.10000v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2103.10000](http://arxiv.org/abs/2103.10000)

    本文提出了一种基于知识蒸馏的框架，用于在多智能体环境中训练智能体采取人类般的碰撞避免策略。实验证明，通过该方法训练的智能体可以超越专家和未经知识蒸馏训练的智能体，在碰撞避免和目标导向转向任务中表现出优异的能力。

    

    尽管多智能体导航领域取得了显著进展，但智能体在多智能体环境中仍缺乏人类所展示的复杂性和智能。本文提出了一种学习人类般普遍的碰撞避免策略的框架，用于处理完全分散的多智能体环境中的智能体-智能体交互。我们的方法使用强化学习的知识蒸馏，根据通过行为克隆从人类轨迹演示中提取的专家策略来确定奖励函数。我们证明，通过我们的方法训练的智能体可以在碰撞避免和目标导向转向任务中采取人类般的轨迹，超过了专家和未经知识蒸馏训练的基于学习的智能体。

    Despite significant advancements in the field of multi-agent navigation, agents still lack the sophistication and intelligence that humans exhibit in multi-agent settings. In this paper, we propose a framework for learning a human-like general collision avoidance policy for agent-agent interactions in fully decentralized, multi-agent environments. Our approach uses knowledge distillation with reinforcement learning to shape the reward function based on expert policies extracted from human trajectory demonstrations through behavior cloning. We show that agents trained with our approach can take human-like trajectories in collision avoidance and goal-directed steering tasks not provided by the demonstrations, outperforming the experts as well as learning-based agents trained without knowledge distillation.
    
[^167]: 变分推断用于分离拥挤星场

    Variational Inference for Deblending Crowded Starfields. (arXiv:2102.02409v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2102.02409](http://arxiv.org/abs/2102.02409)

    我们提出了一种名为StarNet的贝叶斯方法，用于在拥挤星场中分离光源。StarNet利用了变分推断的最新进展，并在实验中比其他竞争方法更准确。

    

    在天文调查中收集的图像中，星星和星系常常在视觉上重叠。分离是在调查图像中区分和表征单个光源的任务。我们提出了StarNet，一种用于分离拥挤星星区域天文图像中光源的贝叶斯方法。StarNet利用了变分推断的最新进展，包括摊销的变分分布和针对前向KL散度期望的优化目标。在我们对M2球状星团的SDSS图像的实验中，StarNet比两种竞争方法Probabilistic Cataloging (PCAT)（一种使用MCMC进行推断的方法）和SDSS用于分离的软件管线DAOPHOT要准确得多。此外，摊销推断方法赋予了StarNet在现代天文调查中进行贝叶斯推断所需的可扩展特性。

    In images collected by astronomical surveys, stars and galaxies often overlap visually. Deblending is the task of distinguishing and characterizing individual light sources in survey images. We propose StarNet, a Bayesian method to deblend sources in astronomical images of crowded star fields. StarNet leverages recent advances in variational inference, including amortized variational distributions and an optimization objective targeting an expectation of the forward KL divergence. In our experiments with SDSS images of the M2 globular cluster, StarNet is substantially more accurate than two competing methods: Probabilistic Cataloging (PCAT), a method that uses MCMC for inference, and DAOPHOT, a software pipeline employed by SDSS for deblending. In addition, the amortized approach to inference gives StarNet the scaling characteristics necessary to perform Bayesian inference on modern astronomical surveys.
    
[^168]: 通过全带回馈解决不确定性下的组合优化问题：组合纯探索和更多扩展

    Combinatorial Pure Exploration with Full-bandit Feedback and Beyond: Solving Combinatorial Optimization under Uncertainty with Limited Observation. (arXiv:2012.15584v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.15584](http://arxiv.org/abs/2012.15584)

    该论文研究了在组合优化问题中，当输入参数不确定或最初未知时，通过全带回馈的组合纯探索（CPE）来解决该不确定性的问题。以往的研究主要关注半赌博机反馈或假设每个边的结果始终可以访问，而这篇论文考虑了强反馈信息不一定可用的实际约束。

    

    组合优化是理论计算机科学和运筹学中广泛研究的基础研究领域之一。在开发组合优化算法时，通常假设输入的参数（例如边权重）是准确已知的。然而，在很多应用程序中，例如推荐系统、众包、通信网络和在线广告，输入参数往往是不确定的或者最初未知的。为了解决这样的不确定性，组合多臂赌博机的纯探索问题（CPE）及其变种引起了越来越多的关注。早期关于CPE的研究主要是研究半赌博机反馈或者假设每个边的结果在每轮中都是可以访问的。然而，由于实际约束（例如预算限制或隐私问题），近期的应用中并不总是有这么强的反馈信息可用。

    Combinatorial optimization is one of the fundamental research fields that has been extensively studied in theoretical computer science and operations research. When developing an algorithm for combinatorial optimization, it is commonly assumed that parameters such as edge weights are exactly known as inputs. However, this assumption may not be fulfilled since input parameters are often uncertain or initially unknown in many applications such as recommender systems, crowdsourcing, communication networks, and online advertisement. To resolve such uncertainty, the problem of combinatorial pure exploration of multi-armed bandits (CPE) and its variants have recieved increasing attention. Earlier work on CPE has studied the semi-bandit feedback or assumed that the outcome from each individual edge is always accessible at all rounds. However, due to practical constraints such as a budget ceiling or privacy concern, such strong feedback is not always available in recent applications. In this a
    
[^169]: 使用生成对抗网络在同时进行的EEG-fMRI中去除心动揭示现象

    Ballistocardiogram artifact removal in simultaneous EEG-fMRI using generative adversarial network. (arXiv:2011.01710v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.01710](http://arxiv.org/abs/2011.01710)

    本文提出了一种使用生成对抗网络在同时进行的EEG-fMRI中去除心动揭示现象的方法，通过优化网络模型的局部表示能力，改善整体性能，并获得一个可靠的生成器。该方法不需要额外的参考信号或复杂的硬件设备。

    

    由于同时进行的脑电图-功能磁共振成像（EEG-fMRI）的高时间和空间分辨率的优势，该技术吸引了广泛的关注，并被广泛应用于脑科学的各个研究领域。然而，在脑部的功能磁共振成像过程中，心动揭示现象（BCG）会严重污染脑电图（EEG）。作为一个非配对问题，BCG揭示现象的去除仍然是一个相当大的挑战。为了提供解决方案，本文提出了一种新颖的模块化生成对抗网络（GAN）及相应的训练策略，通过优化每个模块的参数来改进网络性能。通过这种方式，我们希望提高网络模型的局部表示能力，从而改善其整体性能，并获得一个可靠的用于去除BCG揭示现象的生成器。此外，所提出的方法不依赖于额外的参考信号或复杂的硬件设备。

    Due to its advantages of high temporal and spatial resolution, the technology of simultaneous electroencephalogram-functional magnetic resonance imaging (EEG-fMRI) acquisition and analysis has attracted much attention, and has been widely used in various research fields of brain science. However, during the fMRI of the brain, ballistocardiogram (BCG) artifacts can seriously contaminate the EEG. As an unpaired problem, BCG artifact removal now remains a considerable challenge. Aiming to provide a solution, this paper proposed a novel modular generative adversarial network (GAN) and corresponding training strategy to improve the network performance by optimizing the parameters of each module. In this manner, we hope to improve the local representation ability of the network model, thereby improving its overall performance and obtaining a reliable generator for BCG artifact removal. Moreover, the proposed method does not rely on additional reference signal or complex hardware equipment. E
    
[^170]: 贝叶斯特征选择在联合分位数时间序列分析中的应用

    Bayesian Feature Selection in Joint Quantile Time Series Analysis. (arXiv:2010.01654v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.01654](http://arxiv.org/abs/2010.01654)

    本文提出了一种贝叶斯特征选择方法，用于高维联合分位数时间序列分析。模型具有灵活性，可以根据每个时间序列的需要进行特征的选择，并且允许进行即时预测。

    

    对于相关的多变量时间序列数据进行分位数特征选择一直是一种方法上的挑战和一个未解决的问题。在本文中，我们提出了一种通用的贝叶斯降维方法，用于高维联合分位数时间序列分析中的特征选择，该方法被称为分位数特征选择时间序列（QFSTS）模型。QFSTS模型是一个通用的结构时间序列模型，每个组成部分对时间序列建模产生了可直接解释的加性贡献。其灵活性体现在用户可以为每个时间序列添加/减少组成部分，并且每个时间序列可以具有不同大小的特定值组成部分。特征选择是在分位数回归组件中进行的，每个时间序列都有自己的同时外部预测变量池，允许进行即时预测。基于贝叶斯方法将特征选择扩展到分位数时间序列研究领域。

    Quantile feature selection over correlated multivariate time series data has always been a methodological challenge and is an open problem. In this paper, we propose a general Bayesian dimension reduction methodology for feature selection in high-dimensional joint quantile time series analysis, under the name of the quantile feature selection time series (QFSTS) model. The QFSTS model is a general structural time series model, where each component yields an additive contribution to the time series modeling with direct interpretations. Its flexibility is compound in the sense that users can add/deduct components for each time series and each time series can have its own specific valued components of different sizes. Feature selection is conducted in the quantile regression component, where each time series has its own pool of contemporaneous external predictors allowing nowcasting. Bayesian methodology in extending feature selection to the quantile time series research area is developed
    
[^171]: 二阶条件梯度滑动

    Second-order Conditional Gradient Sliding. (arXiv:2002.08907v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2002.08907](http://arxiv.org/abs/2002.08907)

    提出了一种二阶条件梯度滑动（SOCGS）算法，可以高效解决约束二次凸优化问题，并在有限次线性收敛迭代后二次收敛于原始间隙。

    

    当需要高精度解决问题时，约束二阶凸优化算法是首选，因为它们具有局部二次收敛性。这些算法在每次迭代时需要解决一个约束二次子问题。我们提出了\emph{二阶条件梯度滑动}（SOCGS）算法，它使用一种无投影算法来近似解决约束二次子问题。当可行域是一个多面体时，该算法在有限次线性收敛迭代后二次收敛于原始间隙。进入二次收敛阶段后，SOCGS算法需通过$\mathcal{O}(\log(\log 1/\varepsilon))$次一阶和Hessian正交调用以及$\mathcal{O}(\log (1/\varepsilon) \log(\log1/\varepsilon))$次线性最小化正交调用来实现$\varepsilon$-最优解。当可行域只能通过线性优化正交调用高效访问时，此算法非常有用。

    Constrained second-order convex optimization algorithms are the method of choice when a high accuracy solution to a problem is needed, due to their local quadratic convergence. These algorithms require the solution of a constrained quadratic subproblem at every iteration. We present the \emph{Second-Order Conditional Gradient Sliding} (SOCGS) algorithm, which uses a projection-free algorithm to solve the constrained quadratic subproblems inexactly. When the feasible region is a polytope the algorithm converges quadratically in primal gap after a finite number of linearly convergent iterations. Once in the quadratic regime the SOCGS algorithm requires $\mathcal{O}(\log(\log 1/\varepsilon))$ first-order and Hessian oracle calls and $\mathcal{O}(\log (1/\varepsilon) \log(\log1/\varepsilon))$ linear minimization oracle calls to achieve an $\varepsilon$-optimal solution. This algorithm is useful when the feasible region can only be accessed efficiently through a linear optimization oracle, 
    
[^172]: 在多秘书问题和连续估值的在线线性规划问题中的对数遗憾

    Logarithmic Regret in Multisecretary and Online Linear Programming Problems with Continuous Valuations. (arXiv:1912.08917v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1912.08917](http://arxiv.org/abs/1912.08917)

    本文研究了一个收益管理问题，其中顾客的到达是连续的，有限均值连续分布的效用值和有界离散或连续分布的库存量。研究发现，如果初始库存量与顾客数成线性比例，随着顾客数的增加，预期的遗憾将以对数的速度增长。

    

    我研究了一个一般的收益管理问题，其中$n$个顾客在$n$个时期内顺序到达，您必须动态决定要满足哪个顾客。满足第$t$个时期的顾客可以获得效用$u_{t}\in \mathbb{R}_{+}$，并减少您的库存量$A_{t}\in \mathbb{R}_{+}^{M}$。顾客向量$(u_{t}, A_{t}')'$是独立同分布的，其中$u_{t}$是从有限均值连续分布中抽取的，$A_{t}$是从有界离散或连续分布中抽取的。我研究了该系统的遗憾，即如果您不需要即时决策，您可以获得的额外效用。我展示了如果您的初始库存资产与$n$成线性比例，那么当$n \rightarrow \infty$时，您的预期遗憾是$ \Theta(\log(n)) $。我提供了一种简单的策略，实现了$ \Theta(\log(n)) $的遗憾率。最后，我将这个结果扩展到Arlotto和Gurich（2019）的多秘书问题，其中秘书估值是均匀分布的。

    I study a general revenue management problem in which $ n $ customers arrive sequentially over $ n $ periods, and you must dynamically decide which to satisfy. Satisfying the period-$ t $ customer yields utility $ u_{t} \in \mathbb{R}_{+} $ and decreases your inventory holdings by $ A_{t} \in \mathbb{R}_{+}^{M} $. The customer vectors, $ (u_{t}, A_{t}')' $, are i.i.d., with $ u_{t} $ drawn from a finite-mean continuous distribution and $ A_{t} $ drawn from a bounded discrete or continuous distribution. I study this system's regret, which is the additional utility you could get if you didn't have to make decisions on the fly. I show that if your initial inventory endowment scales linearly with $ n $ then your expected regret is $ \Theta(\log(n)) $ as $ n \rightarrow \infty $. I provide a simple policy that achieves this $ \Theta(\log(n)) $ regret rate. Finally, I extend this result to Arlotto and Gurich's (2019) multisecretary problem with uniformly distributed secretary valuations.
    
[^173]: 半监督矢量值学习：改进的界限与算法

    Semi-supervised Vector-valued Learning: Improved Bounds and Algorithms. (arXiv:1909.04883v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1909.04883](http://arxiv.org/abs/1909.04883)

    该论文提出了一种针对半监督矢量值学习的改进算法，通过利用局部Rademacher复杂度和无标签数据，得出了更精确的超出风险界限。实验结果表明，该算法在多个领域中明显优于其他方法。

    

    矢量值学习是一个重要的问题，其输出空间具有矢量值结构，涵盖了许多重要领域，如多任务学习和迁移学习。我们利用局部Rademacher复杂度和无标签数据，从核函数和线性方法的角度为一般矢量值学习导出了新的半监督超出风险界限。这些界限比现有的界限更精确，收敛速度从标记样本大小的平方根改进为总样本大小的平方根或直接依赖于标记样本大小。在理论分析的基础上，我们提出了一种通用的半监督算法来高效学习矢量值函数，结合了局部Rademacher复杂度和拉普拉斯正则化。大量的实验结果表明，所提出的算法明显优于其他方法，并与我们的理论结果相吻合。

    Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.
    

