# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits](https://arxiv.org/abs/2403.03219) | 该研究提出了一种广义最佳双赢线性背景强化型赌博机算法，能够在次优性差距受到下界限制时遗憾为$O(\log(T))$。同时引入了边缘条件来描述次优性差距对问题难度的影响。 |
| [^2] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^3] | [Active Statistical Inference](https://arxiv.org/abs/2403.03208) | 主动推断是一种统计推断方法，通过利用机器学习模型确定最有利于标记的数据点来有效利用预算，实现比现有基线更少样本的相同准确性。 |
| [^4] | [Reliable, Adaptable, and Attributable Language Models with Retrieval](https://arxiv.org/abs/2403.03187) | 持续面临挑战的参数化语言模型LMs，作者主张使用具有检索功能的LMs作为下一代LMs，以提高可靠性、适应性和可追溯性。 |
| [^5] | [Preventing Reward Hacking with Occupancy Measure Regularization](https://arxiv.org/abs/2403.03185) | 用占用度测量正则化方法可以有效防止奖励欺骗，通过考虑代理与真实奖励之间大的状态占用度偏差来避免潜在的灾难后果。 |
| [^6] | [How Well Can Transformers Emulate In-context Newton's Method?](https://arxiv.org/abs/2403.03183) | Transformers能够实现高阶优化算法，线性注意力Transformer可以在 logistic 回归任务中近似实现二阶优化算法，并展示即使是线性注意力的Transformer也可以实现矩阵求逆的牛顿迭代。 |
| [^7] | [Behavior Generation with Latent Actions](https://arxiv.org/abs/2403.03181) | 这项工作介绍了一种名为矢量量化行为转换器（VQ-BeT）的多功能行为生成模型，通过对连续动作进行标记化处理多模态动作预测、条件生成和部分观察。 |
| [^8] | [Shuffling Momentum Gradient Algorithm for Convex Optimization](https://arxiv.org/abs/2403.03180) | 本研究将洗牌动量梯度方法扩展到有限和强凸优化问题，首次提供了针对强凸设置的洗牌动量方法的分析，达到了收敛速度为$O(1/nT^2)$。 |
| [^9] | [Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination](https://arxiv.org/abs/2403.03172) | 提出了一种基于模型的共识机制，使用多智能体目标想象框架引导智能体达成共识，从而提高合作多智能体强化学习的性能。 |
| [^10] | [Learning Explicitly Conditioned Sparsifying Transforms](https://arxiv.org/abs/2403.03168) | 该论文提出了一种新的稀疏变换模型，通过显式控制数据表示质量和条件数，有效地学习保证数据在稀疏域中具有良好表示的优化变换。 |
| [^11] | [PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning](https://arxiv.org/abs/2403.03161) | PalmProbNet 是一种利用迁移学习分析高分辨率无人机图像的概率方法，可以在厄瓜多尔雨林中检测棕榈树，实现自动化的棕榈树检测，有效确定棕榈树在混合热带雨林中的存在和位置。 |
| [^12] | [Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks](https://arxiv.org/abs/2403.03157) | 本研究探讨了在NOMA增强的无线网络中将聚类联邦学习与非独立同分布数据集相结合的优势，并提出了解决非IID条件挑战的解决方案。 |
| [^13] | [Deep-Learned Compression for Radio-Frequency Signal Classification](https://arxiv.org/abs/2403.03150) | 提出了一个深度学习压缩模型HQARF，用于对射频信号进行压缩和分类，以降低数据传输的带宽和延迟成本。 |
| [^14] | [Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks](https://arxiv.org/abs/2403.03149) | 本研究提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，用于防御客户端训练数据分布推断攻击。 |
| [^15] | [Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization](https://arxiv.org/abs/2403.03145) | 提出了一种新颖的半监督学习框架用于音频-视觉源定位，通过双重均值教师结构避免确认偏差问题，并利用有限标记数据和丰富无标记数据生成高质量伪标签。 |
| [^16] | [Emergent Equivariance in Deep Ensembles](https://arxiv.org/abs/2403.03103) | 深度集成模型通过简单使用数据增强即可实现在所有输入和训练时刻都保持等变性，这种等变性在离开流形时和无限宽度限制下的任何架构都能保持。 |
| [^17] | [KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents](https://arxiv.org/abs/2403.03101) | KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。 |
| [^18] | [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100) | NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音 |
| [^19] | [VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism](https://arxiv.org/abs/2403.03089) | VQSynergy通过矢量量化机制以及其他创新技术提高了药物协同预测的精度和泛化能力，在处理高斯噪声条件下表现出色。 |
| [^20] | [Recall-Oriented Continual Learning with Generative Adversarial Meta-Model](https://arxiv.org/abs/2403.03082) | 框架提出了召回导向的持续学习方法，通过生成对抗元模型(GAMM)在学习新任务时最大化过去知识的稳定性。 |
| [^21] | [On a Neural Implementation of Brenier's Polar Factorization](https://arxiv.org/abs/2403.03071) | 提出了Brenier的极分解定理的神经实现，探讨了在机器学习中的应用，并通过神经网络参数化潜在函数$u$，从最新神经最优输运领域的进展中汲取灵感。 |
| [^22] | [Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families](https://arxiv.org/abs/2403.03069) | 缺失数据增加了模型对潜在变量后验分布的复杂性，本文提出了两种策略——基于有限变分混合和基于填补的变分混合分布，有效改善了从不完整数据估计VAE的准确性。 |
| [^23] | [Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range](https://arxiv.org/abs/2403.03055) | 提出了一种在多智能体线性二次网络系统中收敛到近似最优解的可扩展分布式策略梯度方法，证明了随着通信和控制范围的增加，性能差距会指数级减小为零，并展示了增加通信范围如何增强系统稳定性和揭示关键权衡。 |
| [^24] | [A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives](https://arxiv.org/abs/2403.03037) | 提出了EgoPack，这是一个统一的视频理解方法，结合共享时间建模和最小开销，支持多个下游任务，并在学习新技能时进行合作，为智能机器提供全面的视频理解能力。 |
| [^25] | [SplAgger: Split Aggregation for Meta-Reinforcement Learning](https://arxiv.org/abs/2403.03020) | 本文展示了任务推断序列模型在元强化学习中的益处。 |
| [^26] | [CRISPR: Ensemble Model](https://arxiv.org/abs/2403.03018) | 提出了一种新颖的集成学习方法用于sgRNA设计，在准确性和泛化能力方面均优于现有方法 |
| [^27] | [Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees](https://arxiv.org/abs/2403.02995) | 本研究旨在利用集成树来缓解恶意URL检测器中的标签翻转攻击，从而在机器学习模型中集成防御机制以防范潜在攻击。 |
| [^28] | [Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks](https://arxiv.org/abs/2403.02983) | 该研究旨在探索计算机网络领域中数据中毒攻击的严重性，使用了两种类型的攻击：标签翻转和特征中毒，采用了新颖的方法。 |
| [^29] | [Online Learning of Human Constraints from Feedback in Shared Autonomy](https://arxiv.org/abs/2403.02974) | 该论文提出了一种学习人类约束模型的方法，用于共享自治方式下的实时协作，目的是支持人类操作者执行共享任务并最小化其不适感。 |
| [^30] | [Hamiltonian Property Testing](https://arxiv.org/abs/2403.02968) | 本文研究了Hamiltonian的本地性测试作为属性测试问题，重点在于确定未知的$n$比特Hamiltonian是否是$k$局部的，通过对$H$的时间演化进行访问来解决问题。 |
| [^31] | [Non-Convex Stochastic Composite Optimization with Polyak Momentum](https://arxiv.org/abs/2403.02967) | 本文研究了具有Polyak动量的随机近端梯度方法，在非凸复合优化问题中实现了最佳收敛速度，无论批量大小如何。 |
| [^32] | [Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering](https://arxiv.org/abs/2403.02966) | 提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。 |
| [^33] | [On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models](https://arxiv.org/abs/2403.02957) | 本论文通过严格证明一个特定的DPM去噪策略在大量扩散步数下收敛到均方误差最优条件均值估计器，突出了DPM由渐近最优的去噪器组成，同时具有强大生成器的独特视角。 |
| [^34] | [SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators](https://arxiv.org/abs/2403.02946) | 介绍了一种针对基于Systolic Array的DNN加速器的新型分层软件化硬件感知故障注入策略，以解决可靠性评估中的时间效率问题。 |
| [^35] | [Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?](https://arxiv.org/abs/2403.02945) | 通过研究不同数据集验证现有研究结果是否具有普适性，以测试是否存在共同的ICU患者亚组。 |
| [^36] | [Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity](https://arxiv.org/abs/2403.02944) | 该论文提出了一种新的文本引导图像压缩算法，实现了高感知和像素级准确度，并通过文本自适应编码和联合图像-文本损失训练，避免了像素级准确度下降的问题。 |
| [^37] | [AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models](https://arxiv.org/abs/2403.02938) | 本研究探讨了人类是否可以听到经过优化的语音，并提出了一种系统，可根据音素为单位自动调整播放速度，以确保语音可辨识度。 |
| [^38] | [AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators](https://arxiv.org/abs/2403.02936) | 提出了一种适用于ASIC-based DNN加速器的自适应容错近似乘法器架构。 |
| [^39] | [A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study](https://arxiv.org/abs/2403.02930) | 通过复制研究BASS框架，发现了与原始工作相比性能上的差异，并强调了撰写可复制论文的关键实践。 |
| [^40] | [From Spectra to Biophysical Insights: End-to-End Learning with a Biased Radiative Transfer Model](https://arxiv.org/abs/2403.02922) | 提出了将辐射传输模型集成到自动编码器架构中的端对端学习方法，不仅能够纠正RTMs中的偏倚，而且在变量提取方面表现优于传统技术。 |
| [^41] | [TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax](https://arxiv.org/abs/2403.02920) | TaylorShift通过引入TaylorSoftmax重新计算全记号之间的交互，将自注意力机制的复杂度由平方级降低到线性级，从而提高了处理长序列的效率。 |
| [^42] | [Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems](https://arxiv.org/abs/2403.02912) | 提出了具有差分隐私的随机鞍点问题的镜像下降算法，实现了几乎与维度无关的收敛速率，这种速率之前只针对双线性目标已知。 |
| [^43] | [Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects](https://arxiv.org/abs/2403.02906) | 通过结合公民科学和机器学习，加速欧亚猞猁等濒危物种监测数据的准备、标记和分析过程，应用在自然研究和保护中。 |
| [^44] | [In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889) | 提出了一种用于在大型语言模型中检测幻觉的新方法，解决了这些模型在各种现实场景中应用时遇到的关键问题，通过对多个数据集和LLMs进行广泛评估，展示了该方法的有效性。 |
| [^45] | [Revisiting Confidence Estimation: Towards Reliable Failure Prediction](https://arxiv.org/abs/2403.02886) | 大多数置信度估计方法对于检测错误分类错误是有害的，我们提出通过寻找平坦的最小值来扩大置信度间隔，从而取得了最先进的失败预测。 |
| [^46] | [MathScale: Scaling Instruction Tuning for Mathematical Reasoning](https://arxiv.org/abs/2403.02884) | MathScale提出了一种简单可扩展的方法来创建高质量的数学推理数据，展现出在数学数据集大小方面的有效可扩展性。 |
| [^47] | [Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization](https://arxiv.org/abs/2403.02882) | 通过在SUMO中随机化规则微观交通流的行为，利用深度强化学习算法训练自动驾驶车辆的决策策略，提高其在更真实交通场景中的性能。 |
| [^48] | [A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails](https://arxiv.org/abs/2403.02873) | 这种技术可以简化分析依赖轻尾随机源的算法，通过对较简单的算法变体进行分析，避免使用专门的集中不等式，并且适用于指数、亚高斯和更一般的快速衰减分布。 |
| [^49] | [Quantum Mixed-State Self-Attention Network](https://arxiv.org/abs/2403.02871) | 本论文介绍了一种新颖的量子混合态自注意力网络（QMSAN），结合了量子计算原理和经典机器学习算法，特别是自注意力网络，以增强处理NLP任务的效率和效果。 |
| [^50] | [Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices](https://arxiv.org/abs/2403.02870) | 该研究揭示了对深度学习模型进行精确提取的新型攻击方法，通过边缘/端点设备的侧信道攻击可以获取模型架构和图像维度等重要信息。 |
| [^51] | [Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation](https://arxiv.org/abs/2403.02867) | 将扩散过程视为连续时间动力系统，建立了可扩展且有效的框架以近似从级联中推断出基础网络结构，解决了网络推断和影响估计中存在的可扩展性问题 |
| [^52] | [FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models](https://arxiv.org/abs/2403.02846) | 通过对比模型集合，提出了FLGuard方法增强联邦学习的拜占庭-鲁棒性。 |
| [^53] | [SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix](https://arxiv.org/abs/2403.02833) | SOFIM利用正则化Fisher信息矩阵和Sherman-Morrison矩阵求逆改善了大规模随机优化中梯度更新的收敛速率，同时解决了数据异质性带来的非平稳目标问题。 |
| [^54] | [An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation](https://arxiv.org/abs/2403.02821) | 提出了一种使用自适应生态排放来保护生态系统的水电管理方法，并结合神经网络和优化算法，旨在推动水电厂兼顾环境保护和能源生产。 |
| [^55] | [InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting](https://arxiv.org/abs/2403.02814) | 提出了InjectTST这种将全局信息注入独立通道的变压器方法，用于改进多变量时间序列（MTS）预测性能。 |
| [^56] | [Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems](https://arxiv.org/abs/2403.02810) | 提出了一种新颖的操作学习算法，称为动态高斯图算子（DGGO），可以在任意离散力学问题中学习参数化PDEs。 |
| [^57] | [A Distance Metric Learning Model Based On Variational Information Bottleneck](https://arxiv.org/abs/2403.02794) | 本文首次将变分信息瓶颈与度量学习模型相结合，提出了一种新的度量学习模型 VIB-DML 用于评分预测，限制潜空间特征向量的互信息以提高模型鲁棒性并满足欧氏距离的假设。 |
| [^58] | [Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease](https://arxiv.org/abs/2403.02786) | 该研究在半监督学习框架内探索了图表示学习的潜力，通过人类中心解释提供了个性化特征重要性得分，以增强解释性和临床相关性。 |
| [^59] | [Data Collaboration Analysis Over Matrix Manifolds](https://arxiv.org/abs/2403.02780) | 本研究讨论了在矩阵流形上的数据协作分析，探讨了如何通过隐私保护机器学习来处理多来源数据的道德和隐私问题 |
| [^60] | [A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation](https://arxiv.org/abs/2403.02777) | 提出了一种零样本学习策略，可以在不重新训练的情况下应用于未见过的血管解剖结构。 |
| [^61] | [EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs](https://arxiv.org/abs/2403.02775) | EasyQuant是一种无需训练的、无需数据的仅针对权重的量化算法，旨在减少量化误差并保证LLM的泛化性能。 |
| [^62] | [Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models](https://arxiv.org/abs/2403.02774) | 通过学习一致性模型，在不需要重新训练的情况下高效、准确地降尺度任意地球系统模型模拟，并产生概率性降尺度场。 |
| [^63] | [Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives](https://arxiv.org/abs/2403.02772) | 这项研究引入了一种新的有监督对比学习框架，通过结合硬和软负样本，有效地解决了康复锻炼评估中样本稀缺的问题 |
| [^64] | [G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes](https://arxiv.org/abs/2403.02765) | 该论文提出了一种新的深度学习模型G4-Attention，应用注意力机制，用于预测DNA G-四链体，相较于现有算法，该模型具备更强的预测能力。 |
| [^65] | [Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels](https://arxiv.org/abs/2403.02746) | 提出了一个名为Paraformer的弱监督框架，通过低分辨率历史土地覆盖数据指导大规模高分辨率土地覆盖映射，设计了CNN-Transformer特征提取器来综合捕获局部和全局背景信息 |
| [^66] | [Neural Fractional Differential Equations](https://arxiv.org/abs/2403.02737) | 提出了神经FDE，一种新型深度神经网络架构，可调整FDE以适应数据动态，可能优于神经OD。 |
| [^67] | [A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks](https://arxiv.org/abs/2403.02730) | 描述了一种用于模拟受限系统的神经常微分方程的两阶段训练方法，在第一阶段寻找合适的神经网络参数，第二阶段找到最佳参数并保持在可行域内。 |
| [^68] | [Android in the Zoo: Chain-of-Action-Thought for GUI Agents](https://arxiv.org/abs/2403.02713) | 该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。 |
| [^69] | [Noise misleads rotation invariant algorithms on sparse targets](https://arxiv.org/abs/2403.02697) | 噪声添加后，稀疏线性问题上的旋转不变算法仍然次优，我们证明了这一点。 |
| [^70] | [Controllable Prompt Tuning For Balancing Group Distributional Robustness](https://arxiv.org/abs/2403.02695) | 引入了可控提示调整（CPT）技术，通过优化方案在不同组之间实现良好性能，避免牺牲任何一个组的性能，在虚假相关基准测试中取得了最先进的结果。 |
| [^71] | [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694) | MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。 |
| [^72] | [Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning](https://arxiv.org/abs/2403.02690) | 提出了基于Dirichlet分布的逐样本加权采样框架，提出了RENT方法来有效利用转移矩阵进行噪声标签学习。 |
| [^73] | [DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators](https://arxiv.org/abs/2403.02688) | 首次提出了轻量级的动态片上矫正框架DOCTOR，针对光子张量加速器中的时间漂移变化问题，实现自适应、原位准确度恢复 |
| [^74] | [Learning to Defer to a Population: A Meta-Learning Approach](https://arxiv.org/abs/2403.02683) | 通过元学习，本研究提出一种学习推迟对人群的方法，该方法可以在测试时适应前所未见的专家，从而更好地面对困难决策。 |
| [^75] | [Time Weaver: A Conditional Time Series Generation Model](https://arxiv.org/abs/2403.02682) | 引入了Time Weaver模型，利用异构元数据改善时间序列生成，并指出标准评估指标的朴素扩展是不够的。 |
| [^76] | [SGD with Partial Hessian for Deep Neural Networks Optimization](https://arxiv.org/abs/2403.02681) | 提出了一种深度神经网络优化方法，结合了部分Hessian信息的二阶优化器和一阶随机梯度下降(SGD)优化器，从而提高了性能稳定性 |
| [^77] | [Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad](https://arxiv.org/abs/2403.02648) | KATE是一种新的优化算法，提出了一种与AdaGrad标度不变的适应方法，并在广义线性模型和一般的非凸问题中证明了其标度不变性。数值实验结果表明，KATE在各种场景中均优于AdaGrad并与Adam性能匹配/超越。 |
| [^78] | [Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain](https://arxiv.org/abs/2403.02645) | 本文提出了一种在5G网络中检测干扰者的新型深度学习技术，通过引入双阈值深度学习干扰检测器，专注于SSB的RF领域特征，提高了网络的鲁棒性。 |
| [^79] | [False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy](https://arxiv.org/abs/2403.02639) | 本研究提出了一种名为虚假阳性采样的新增强技术，通过重新训练模型使用被识别为虚假阳性的点云，以提高3D物体检测模型的性能。 |
| [^80] | [FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling](https://arxiv.org/abs/2403.02630) | 该研究提出了FedHCDR框架，通过超图信号解耦的方式解决了联邦跨领域推荐中不同领域数据异质性的问题。 |
| [^81] | [Interactive Continual Learning: Fast and Slow Thinking](https://arxiv.org/abs/2403.02628) | 本文提出了一种基于交互的持续学习框架，通过多模型之间的合作交互，实现了更好的任务推导和内存检索。 |
| [^82] | [Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use](https://arxiv.org/abs/2403.02626) | 提出了一个新的框架，利用自然语言交互取代人工标注，将定义视觉概念所需的人力投入减少了一个数量级 |
| [^83] | [Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects](https://arxiv.org/abs/2403.02624) | 该论文提出了帕累托最优估计和策略学习的方法，用于确定如何在短期和长期治疗效果之间进行权衡从而实现最佳治疗。 |
| [^84] | [World Models for Autonomous Driving: An Initial Survey](https://arxiv.org/abs/2403.02622) | 世界模型在自主驾驶领域的重要性和作用，是通过准确预测未来事件和评估其影响来帮助决策过程，从而推动自主驾驶技术发展的革命性方法。 |
| [^85] | [Training Machine Learning models at the Edge: A Survey](https://arxiv.org/abs/2403.02619) | 这项调研深入探讨了边缘学习(EL)中优化机器学习模型训练的各种方法和方法论，旨在综合现有知识，识别挑战，并突出未来趋势。 |
| [^86] | [Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems](https://arxiv.org/abs/2403.02616) | 提出了一种细粒度自适应异常诊断方法（MAD-Transformer），通过构建时间状态矩阵和空间状态矩阵来揭示工业CPS工作状态的时空关联关系和演变机制 |
| [^87] | [Search Intenion Network for Personalized Query Auto-Completion in E-Commerce](https://arxiv.org/abs/2403.02609) | 个性化搜索意图网络解决了电子商务中查询自动补全系统面临的意图模糊性和意图转移问题。 |
| [^88] | [DNNLasso: Scalable Graph Learning for Matrix-Variate Data](https://arxiv.org/abs/2403.02608) | DNNLasso是一种对角非负图形套索模型，用于估计Kronecker-和结构的精度矩阵，在准确性和计算时间方面优于现有方法。 |
| [^89] | [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://arxiv.org/abs/2403.02600) | TESTAM模型通过引入混合的专家模型，并且针对时间建模、静态图的时空建模以及动态图的动态时空依赖建模，能够更好地捕捉交通数据的各种模式和情况。 |
| [^90] | [Pooling Image Datasets With Multiple Covariate Shift and Imbalance](https://arxiv.org/abs/2403.02598) | 本文从范畴论的角度提供了一个简单而有效的解决方案，完全避免了复杂的多阶段训练流程。 |
| [^91] | [What do we learn from inverting CLIP models?](https://arxiv.org/abs/2403.02580) | 通过反转CLIP模型，研究发现生成的图像与指定目标提示语义对齐，揭示了CLIP模型的混合概念能力、性别偏见以及可能出现的不安全内容图像。 |
| [^92] | [Geometric Dynamics of Signal Propagation Predict Trainability of Transformers](https://arxiv.org/abs/2403.02579) | 研究探索了深度transformers中信号传播和梯度反向传播的动力学特性，提出了初始化超参数条件以确保训练的可行性，并发现了在有MLP层的情况下粒子几何的定量演化规律，揭示了初始化函数的有序-混沌相变。 |
| [^93] | [AceMap: Knowledge Discovery through Academic Graph](https://arxiv.org/abs/2403.02576) | AceMap是一个面向知识发现的学术系统，通过构建全面的数据库和运用创新的可视化、量化和分析方法，解决了科学文献管理与价值提取的挑战。 |
| [^94] | [Learning-augmented Online Minimization of Age of Information and Transmission Costs](https://arxiv.org/abs/2403.02573) | 该论文提出了一种学习增强的在线算法，用于最小化传输和陈旧成本的总和，在保证最坏情况下性能的同时，兼顾典型情况下性能。 |
| [^95] | [DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training](https://arxiv.org/abs/2403.02571) | DPAdapter通过噪声容忍预训练的方法，旨在增强差分隐私深度学习模型性能，有效解决了DP引入的模型性能降低挑战。 |
| [^96] | [Wukong: Towards a Scaling Law for Large-Scale Recommendation](https://arxiv.org/abs/2403.02545) | Wukong通过堆叠因子分解机和协同增长策略，在推荐领域建立了一个标度律，并在质量上优于现有模型。 |
| [^97] | [Coronary artery segmentation in non-contrast calcium scoring CT images using deep learning](https://arxiv.org/abs/2403.02544) | 该论文介绍了一种利用深度学习算法在非对比度心脏CT图像中进行冠状动脉分割的方法，通过半自动生成Ground Truth的新框架。 |
| [^98] | [Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning](https://arxiv.org/abs/2403.02536) | 利用新数据集结合机器学习策略，预测太阳活动周期23和24期间的SEP事件，以减少对航空、太空电子设备和太空探索的辐射危害。 |
| [^99] | [Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?](https://arxiv.org/abs/2403.02534) | 通常需要对大量时间序列进行预测，但可能无法为每个序列单独训练模型，提出了建立基础时间序列模型的解决方案，并探讨使用合成数据作为训练集的优势。 |
| [^100] | [Density-based Isometric Mapping](https://arxiv.org/abs/2403.02531) | 该论文提出了一种基于密度的等距映射方法，通过修改最短路径算法，引入了受Parzen-Rosenblatt (PR)窗口启发的新约束，有助于处理高维数据中的不均匀性，使得构建的Isomap中最短路径图具有更好的均匀性。 |
| [^101] | [Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces](https://arxiv.org/abs/2403.02524) | 本文提出了一种基于装配再生核希尔伯特空间内在结构和jets几何概念的估计Koopman算子的新方法JetDMD，通过明确的误差界和收敛率证明其优越性，为Koopman算子的数值估计提供了更精确的方法，同时在装配希尔伯特空间框架内提出了扩展Koopman算子的概念，有助于深入理解估计的Koopman特征函数。 |
| [^102] | [HeAR -- Health Acoustic Representations](https://arxiv.org/abs/2403.02522) | HeAR是一个基于自监督学习的深度学习系统，通过大规模数据集训练，在33个健康声学任务上表现优越，有望推动健康声学研究的发展。 |
| [^103] | [Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation](https://arxiv.org/abs/2403.02514) | 提出了设定机器人目的的概念，以帮助机器人更加关注获取与目的相关的知识。 |
| [^104] | [Differentially Private Representation Learning via Image Captioning](https://arxiv.org/abs/2403.02506) | 通过图像字幕生成实现了有效的差分隐私表示学习，获得了高质量图像特征，可用于各种视觉和视觉语言任务。 |
| [^105] | [Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents](https://arxiv.org/abs/2403.02502) | 提出了一种面向LLM代理的基于探索的轨迹优化方法，通过允许代理从探索失败中学习，实现了性能的改进。 |
| [^106] | [RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction](https://arxiv.org/abs/2403.02500) | RVRAE是一种动态因子模型，利用变分递归自编码器，结合先验-后验学习方法，解决了市场数据中的时间依赖性和噪声问题，擅长风险调整。 |
| [^107] | [Encodings for Prediction-based Neural Architecture Search](https://arxiv.org/abs/2403.02484) | 预测器方法在神经架构搜索方面起到了显著作用，本文对不同类型的神经编码进行了分类和研究，并引入了统一编码，扩展了NAS预测器到多个搜索空间。 |
| [^108] | [A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation](https://arxiv.org/abs/2403.02476) | 通过一个新颖的两步论证，我们展示了在不实际执行投影步骤的情况下保留投影基础分析的简单性是可能的。 |
| [^109] | [Enhancing LLM Safety via Constrained Direct Preference Optimization](https://arxiv.org/abs/2403.02475) | 通过引入Constrained DPO（C-DPO）方法，我们提出了一种高效且轻量的微调大型语言模型（LLMs）的方法，能有效平衡有用性和安全性之间的权衡，为LLMs提供了安全保障。 |
| [^110] | [Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review](https://arxiv.org/abs/2403.02469) | 该论文综述了医学视觉-语言模型在医学报告生成和视觉问答领域的最新进展，重点讨论了模型架构、预训练策略、评估指标以及未来方向。 |
| [^111] | [Applied Causal Inference Powered by ML and AI](https://arxiv.org/abs/2403.02467) | 本书介绍了机器学习和因果推断的新兴融合，探讨了经典结构方程模型与现代AI等价物之间的联系，并涵盖了使用双重/去偏移机器学习方法进行推断的内容。 |
| [^112] | [On Latency Predictors for Neural Architecture Search](https://arxiv.org/abs/2403.02446) | 通过在训练设备上进行预训练，并将预测器转移到测试设备上，可以显著提高预测模型的样本效率。 |
| [^113] | [Anatomically Constrained Tractography of the Fetal Brain](https://arxiv.org/abs/2403.02444) | 提出了一种基于精确分割胎儿大脑组织的解剖约束纤维束追踪方法，并通过深度学习方法实现自动分割，有效改善了对胎儿大脑的纤维束追踪准确性。 |
| [^114] | [Root Causing Prediction Anomalies Using Explainable AI](https://arxiv.org/abs/2403.02439) | 本文介绍了在根本上解决个性化广告中模型性能下降问题的方法，通过解释人工智能技术分析预测异常。 |
| [^115] | [SoK: Challenges and Opportunities in Federated Unlearning](https://arxiv.org/abs/2403.02437) | 联邦学习引入了新的隐私要求，促使研究开始关注适用于联邦学习环境的反学习机制。 |
| [^116] | [On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation](https://arxiv.org/abs/2403.02432) | 通过测度预处理技术，研究了参数ML模型和领域自适应迁移学习中学习代理的收敛性，提出了伽玛收敛的类似法图引理的理解。 |
| [^117] | [Towards efficient deep autoencoders for multivariate time series anomaly detection](https://arxiv.org/abs/2403.02429) | 本文提出了面向多变量时间序列异常检测的深度自编码器的高效压缩方法，通过修剪减少权重数量并防止精度灾难性下降，以在有限时间和内存约束的实时系统中获得最佳结果。 |
| [^118] | [Digital Twins and Civil Engineering Phases: Reorienting Adoption Strategies](https://arxiv.org/abs/2403.02426) | 数字孪生技术在土木工程领域的采用策略需要重新定位，以解决不同阶段的挑战和应用分散性问题。 |
| [^119] | [Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems](https://arxiv.org/abs/2403.02419) | 本文研究了复合推理系统的扩展定律，发现投票推理系统的性能随LLM调用次数增加先增加后下降。 |
| [^120] | [From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima](https://arxiv.org/abs/2403.02418) | 局部曲率变化导致系统从良性且富有信息的局部景观逐渐陷入无信息的迷宫，关键转变与时间相关的Hessian的阈值有关。 |
| [^121] | [NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function](https://arxiv.org/abs/2403.02411) | 提出了一种新的计算块，称为NiNformer，具有令牌混合生成门控功能，以解决注意机制在深度学习中的计算成本高昂和数据集要求大的缺点。 |
| [^122] | [Classification of the Fashion-MNIST Dataset on a Quantum Computer](https://arxiv.org/abs/2403.02405) | 通过改进变分算法，使用渐近浅层电路对时尚-MNIST数据集进行编码，为未来的量子机器学习实证研究提供了新方法。 |
| [^123] | [OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport](https://arxiv.org/abs/2403.02372) | 使用最优输运理论的OTClean框架解决了在条件独立性（CI）约束下数据清洗的问题，通过将数据修复问题转化为正则化优化问题，并提出了受Sinkhorn矩阵缩放算法启发的迭代算法，克服了可伸缩性挑战。 |
| [^124] | [A Novel Hybrid Feature Importance and Feature Interaction Detection Framework for Predictive Optimization in Industry 4.0 Applications](https://arxiv.org/abs/2403.02368) | 该论文提出了一个新型混合框架，结合特征重要性检测和特征交互检测，以提高工业4.0应用中的预测优化准确性。 |
| [^125] | [Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity](https://arxiv.org/abs/2403.02363) | 提出了一种两阶段方法，通过软标签修复和多专家集成学习结合，解决了长尾嘈杂标签学习问题。 |
| [^126] | [Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling](https://arxiv.org/abs/2403.02360) | 通过对比云边模型解耦，提出了一种为异构联邦学习定制架构，该架构将深度神经网络分为捕获共享表示的主体和处理数据异构性的个性化头部，以优化联邦学习性能。 |
| [^127] | [Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space](https://arxiv.org/abs/2403.02355) | 该论文提出在超复数空间中利用更具表现力的四元数表示进行时间知识图补全，着重捕捉时间敏感关系，并理论上验证了方法可以建模各种关系模式，实验表明该方法达到了最新的性能水平。 |
| [^128] | [Spatio-Temporal Field Neural Networks for Air Quality Inference](https://arxiv.org/abs/2403.02354) | 该研究提出了基于时空场神经网络的新模型和金字塔推断框架，在空气质量推断中取得了最先进的性能。 |
| [^129] | [ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys](https://arxiv.org/abs/2403.02352) | 提出了一种新的注意机制ATP，通过关注顶级主要键而非每个标记，以实现对输入序列的快速处理，并能够在降低注意力复杂度的同时捕捉输入序列的语义关系。 |
| [^130] | [On the Convergence of Federated Learning Algorithms without Data Similarity](https://arxiv.org/abs/2403.02347) | 本文提出了一种无需数据相似性条件的联邦学习算法收敛性分析框架，通过推导出三种常用步长调度的精确表达式，实现了对算法收敛性能的全面评估。 |
| [^131] | [Neural Redshift: Random Networks are not Random Functions](https://arxiv.org/abs/2403.02241) | 本论文研究了未经训练的随机权重网络，发现即使简单的MLPs也具有强烈的归纳偏见，不同于传统观点的是，NNs并不具有固有的“简单偏见”，而是依赖于组件的作用。 |
| [^132] | [Mutual Information Estimation via Normalizing Flows](https://arxiv.org/abs/2403.02187) | 通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。 |
| [^133] | [Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification](https://arxiv.org/abs/2403.01944) | 提出了辅助傅立叶基增强（AFA）的方法，通过在频域进行增强，填补了视觉增强遗留的增强差距，提高了模型的鲁棒性。 |
| [^134] | [Matrix Completion with Convex Optimization and Column Subset Selection](https://arxiv.org/abs/2403.01919) | 该方法结合了列子集选择和低秩矩阵完成问题的理论基础，提出使用凸优化解决矩阵恢复问题，同时通过实验验证了算法的正确性和性能。 |
| [^135] | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548) | 本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。 |
| [^136] | [Mixed-Strategy Nash Equilibrium for Crowd Navigation](https://arxiv.org/abs/2403.01537) | 通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。 |
| [^137] | [LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation](https://arxiv.org/abs/2403.01131) | LLaMoCo是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架，通过全面指令集和新颖的两阶段学习策略，实现了优越的性能。 |
| [^138] | [Sensitivity Analysis On Loss Landscape](https://arxiv.org/abs/2403.01128) | 利用一、二和三阶导数进行损失景观分析，发现了与Spearman秩相关系数类似可视化的信息，以及损失函数和激活函数结合带来的非线性模式。 |
| [^139] | [Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling](https://arxiv.org/abs/2403.01053) | 提出了一种通过几何限制概率建模处理方法来解决生物医学数据中存在的非 i.i.d. 数据分布、类别不平衡等问题。 |
| [^140] | [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867) | 本文提出了一种名为Gradient Cuff的方法，通过探索拒绝损失地形图来检测对大语言模型的越狱攻击，成功设计了一种有效的两步检测策略。 |
| [^141] | [DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://arxiv.org/abs/2403.00818) | DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。 |
| [^142] | [Robust Policy Learning via Offline Skill Diffusion](https://arxiv.org/abs/2403.00225) | 提出了一种新颖的离线技能学习框架DuSkill，通过引导扩散模型生成通用技能，从而增强不同领域任务的策略学习鲁棒性。 |
| [^143] | [GraphPub: Generation of Differential Privacy Graph with High Availability](https://arxiv.org/abs/2403.00030) | 提出了一种名为GraphPub的新型图边保护框架，通过反向学习和编码器-解码器机制，在保护图拓扑结构的同时保证数据的可用性基本不变。 |
| [^144] | [Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models](https://arxiv.org/abs/2402.18946) | 本文提出了一种用于高阶不确定模型的实时自适应安全关键控制的方法，包括利用稀疏高斯过程进行在线学习和基于高阶控制屏障函数的安全过滤器。 |
| [^145] | [Training-set-free two-stage deep learning for Spectroscopic data de-noising](https://arxiv.org/abs/2402.18830) | 提出了一种无需训练集的两阶段深度学习方法，结合自适应先验和先进优化技术，实现了比先前工作快五倍的加速。 |
| [^146] | [FedUV: Uniformity and Variance for Heterogeneous Federated Learning](https://arxiv.org/abs/2402.18372) | 提出了FedUV框架，通过引入两种正则化项，促使局部模型在异构分布数据中表现得更均匀和稳定 |
| [^147] | [Learning method for S4 with Diagonal State Space Layers using Balanced Truncation](https://arxiv.org/abs/2402.15993) | 一种用于处理长序列数据的边缘智能应用的S4模型的学习方法，利用平衡截断技术降低计算成本，并通过改进初始化过程和优化准确度和效率指标来超越传统训练模型。 |
| [^148] | [Learning to See Through Dazzle](https://arxiv.org/abs/2402.15919) | 通过波前编码的相位掩模和三明治生成对抗网络(SGAN)来恢复机器视觉受到激光盛光影响下的图像，结合了傅里叶特征表示以改进神经网络对高频图像细节的学习。 |
| [^149] | [GraphEdit: Large Language Models for Graph Structure Learning](https://arxiv.org/abs/2402.15183) | 本研究提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习复杂的图结构化数据中的节点关系，通过在图结构上进行指导调整，增强LLMs的推理能力，从而提高图结构学习的可靠性。 |
| [^150] | [SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects](https://arxiv.org/abs/2402.14482) | SpanSeq 是一种用于生物数据序列的数据库分区方法，能够避免训练集和测试集之间的数据泄漏。 |
| [^151] | [Quantum Circuit Optimization with AlphaTensor](https://arxiv.org/abs/2402.14396) | 使用基于深度强化学习的AlphaTensor-Quantum方法，在容错量子计算中优化T门数量，显著减少电路的T计数。 |
| [^152] | [Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement](https://arxiv.org/abs/2402.14160) | 提出了递归推测解码(RSD)方法，通过无重复抽样最大化树的多样性，从而进一步加速LLM推理过程。 |
| [^153] | [Neural Control System for Continuous Glucose Monitoring and Maintenance](https://arxiv.org/abs/2402.13852) | 引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。 |
| [^154] | [Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews](https://arxiv.org/abs/2402.13613) | 该论文总结了VLSP 2023中ComOM任务的一个数据挑战，旨在推动自然语言处理领域通过开发从越南产品评论中提取比较意见的技术，参与者需提出能够提取比较"五元组"的模型并根据F1分数进行评估排名。 |
| [^155] | [Attacks on Node Attributes in Graph Neural Networks](https://arxiv.org/abs/2402.12426) | 该研究通过基于特征的对抗攻击，针对图神经网络中的节点属性展开研究，发现使用Projected Gradient Descent的决策时攻击比使用Mean Node Embeddings和Graph Contrastive Learning策略的毒化攻击更加有效。 |
| [^156] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^157] | [Online Local False Discovery Rate Control: A Resource Allocation Approach](https://arxiv.org/abs/2402.11425) | 该研究提出了一种在线局部虚发现率控制的资源分配方法，实现了$O(\sqrt{T})$的后悔率，并指出这种后悔率在一般情况下是不可改进的。 |
| [^158] | [Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like](https://arxiv.org/abs/2402.07383) | 本文提出了ELaTE，一种基于流匹配的零样本文本到语音系统，可以根据短音频提示以精确控制笑声时机和表情生成任何说话者的自然笑声。 |
| [^159] | [GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains](https://arxiv.org/abs/2402.07232) | GenSTL是一个通用的稀疏轨迹学习框架，通过自回归生成特征域来实现稀疏轨迹与密集轨迹之间的连接，从而消除了对大规模密集轨迹数据的依赖。 |
| [^160] | [Entropy-Regularized Token-Level Policy Optimization for Large Language Models](https://arxiv.org/abs/2402.06700) | 本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。 |
| [^161] | [The VampPrior Mixture Model](https://arxiv.org/abs/2402.04412) | 本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。 |
| [^162] | [Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities](https://arxiv.org/abs/2402.01831) | Audio Flamingo是一种新型音频语言模型，具备强大的音频理解能力、通过上下文学习和检索快速适应未见过的任务的能力以及强大的多轮对话能力，并且通过广泛的评估达到了最优成绩。 |
| [^163] | [From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models](https://arxiv.org/abs/2402.00421) | 本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。 |
| [^164] | [Swing: Short-cutting Rings for Higher Bandwidth Allreduce](https://arxiv.org/abs/2401.09356) | Swing算法通过摆动在螺旋方向之间保持通信节点之间的低距离，从而在螺旋网络上优化Allreduce操作的性能，相比现有算法提升高达3倍。 |
| [^165] | [scDiffusion: conditional generation of high-quality single-cell data using diffusion model](https://arxiv.org/abs/2401.03968) | scDiffusion是一种结合扩散模型和基础模型的生成模型，能准确地生成高质量的单细胞数据，具有受控条件下生成数据的能力。 |
| [^166] | [Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions](https://arxiv.org/abs/2312.12450) | 该研究评估了大型语言模型遵循代码编辑指令的能力，在指令式代码编辑任务上发现了开放和封闭模型之间的显著差距。 |
| [^167] | [RiskBench: A Scenario-based Benchmark for Risk Identification](https://arxiv.org/abs/2312.01659) | RiskBench是一种基于场景的风险识别基准，旨在解决当前使用独立数据集评估风险识别算法时的困难，为不同算法的直接比较和安全性能提升提供集体进展。 |
| [^168] | [Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data](https://arxiv.org/abs/2311.18377) | 本研究展示了利用药物样小分子和化学反应数据库预训练BERT模型的潜力，从而增强其在有机材料虚拟筛选中的性能。 |
| [^169] | [An Effective Universal Polynomial Basis for Spectral Graph Neural Networks](https://arxiv.org/abs/2311.18177) | 通过研究图滤波器的多项式基础与图异质性度量之间的关联，本研究提出了一种自适应的异质性基础，可以有效地适应不同图之间的异质性程度。 |
| [^170] | [TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection](https://arxiv.org/abs/2311.18061) | TransNAS-TSAD是一个结合了Transformer架构和神经架构搜索的框架，通过NSGA-II算法优化，成功解决了时间序列数据的复杂性，具有高效的搜索空间探索和定制架构适应性，显著提高了异常检测模型的性能。 |
| [^171] | [Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes](https://arxiv.org/abs/2311.17948) | 提出了一种名为Action-slot的方法，通过槽注意力学习视觉动作中心表示，在交通场景中实现了多标签原子活动识别。 |
| [^172] | [Mitigating Biases with Diverse Ensembles and Diffusion Models](https://arxiv.org/abs/2311.16176) | 通过利用扩散概率模型（DPMs）生成新特征组合的图像，可以在集成模型中增加模型多样性，并减轻捷径偏见，而无需额外监督信号。 |
| [^173] | [A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification](https://arxiv.org/abs/2311.11772) | 在弱监督整个切片图像分类中，不同于常规认知的观念，研究发现省略染色标准化和图像增强并不会影响下游切片级别的分类性能，同时还能节省大量内存和计算资源。 |
| [^174] | [Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation](https://arxiv.org/abs/2311.09354) | 本研究提出了一种图像处理算法，用于在三维培养中量化细胞的活力，无需基于试剂的指示物，并且展示了其与人类专家的表现类似。 |
| [^175] | [On Leakage in Machine Learning Pipelines](https://arxiv.org/abs/2311.04179) | 本论文旨在扩展对设计、实施和评估机器学习管道时导致信息泄漏的原因的理解，通过具体示例提供了各种可能在机器学习管道中出现的泄漏的全面概述和讨论。 |
| [^176] | [Estimating treatment effects from single-arm trials via latent-variable modeling](https://arxiv.org/abs/2311.03002) | 通过深度潜变量模型和摊销变分推断，我们提出了一种可用于单臂试验的治疗效果估计方法，可以处理缺失的协变量观察，实现患者匹配或直接治疗效果估计。 |
| [^177] | [Fast hyperboloid decision tree algorithms](https://arxiv.org/abs/2310.13841) | 快速双曲决策树算法hyperDT通过利用内积将欧几里得决策树算法调整到双曲空间，消除了对计算密集型黎曼优化和数值不稳定操作的需求。 |
| [^178] | [XRMDN: An Extended Recurrent Mixture Density Network for Short-Term Probabilistic Rider Demand Forecasting with High Volatility](https://arxiv.org/abs/2310.09847) | XRMDN是一种扩展循环混合密度网络，专门用于处理移动出行系统中高波动率的短期概率骑手需求预测，能够灵活整合内生和外生数据。 |
| [^179] | [A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks](https://arxiv.org/abs/2308.10664) | 提出一种新的安全深度强化学习方法，利用惩罚函数在训练时惩罚违反环境约束的策略，以确保无线通信网络中能源高效联邦学习的总能耗最小化。 |
| [^180] | [Machine Unlearning: Solutions and Challenges](https://arxiv.org/abs/2308.07061) | 本文提供了对机器遗忘解决方案的全面分类和分析，明确了完全遗忘方法和近似遗忘方法，并讨论了它们的优势和局限性，提出了推进机器遗忘的未来方向。 |
| [^181] | [Fast Training of Diffusion Models with Masked Transformers](https://arxiv.org/abs/2306.09305) | 该论文提出了一种使用Masked Transformers快速训练扩散模型的方法，首次利用Masked training显著降低了模型的训练成本，并引入了不对称的编码器-解码器架构和辅助任务，以提升对全patches的长程理解。 |
| [^182] | [Hierarchical Contrastive Learning Enhanced Heterogeneous Graph Neural Network](https://arxiv.org/abs/2304.12228) | 提出了一种名为HeCo的新型共同对比学习机制，用于自监督HGNNs，采用交叉视图对比机制以学习节点嵌入。 |
| [^183] | [Fine Robotic Manipulation without Force/Torque Sensor](https://arxiv.org/abs/2301.13413) | 利用神经网络方法，不需要力/扭矩传感器，通过特定训练数据结构准确估计外部力矩。 |
| [^184] | [Constrained Policy Optimization with Explicit Behavior Density for Offline Reinforcement Learning](https://arxiv.org/abs/2301.12130) | 提出了一种具有显式行为密度的约束策略优化方法，通过使用flow-GAN模型来准确识别安全区域，实现了更少保守的学习策略。 |
| [^185] | [Knowledge Distillation in Federated Edge Learning: A Survey](https://arxiv.org/abs/2301.05849) | 研究调查了知识蒸馏在联邦边缘学习中的应用，讨论了现有方法的局限性和问题，并提供了实际部署的指导。 |
| [^186] | [Directed Acyclic Graph Structure Learning from Dynamic Graphs](https://arxiv.org/abs/2211.17029) | 在动态图中，我们研究了节点特征生成机制的学习问题，通过同时估计节点特征之间的同时关系和时滞交互关系来构建有向无环图，有效地描述了特征生成过程 |
| [^187] | [Incremental Spatial and Spectral Learning of Neural Operators for Solving Large-Scale PDEs](https://arxiv.org/abs/2211.15188) | 提出了增量傅里叶神经算子（iFNO），通过逐步增加频率模式的数量来解决训练FNO中的两大挑战：计算高分辨率输入的傅里叶变换消耗大，选择谱层中的相关频率集合困难 |
| [^188] | [Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery](https://arxiv.org/abs/2211.13715) | 提出了一种基于梯度的干预目标定位方法，GIT，在因果发现中能够通过信号梯度估计器降低干预次数，在低数据量情况下优于竞争基线。 |
| [^189] | [Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks](https://arxiv.org/abs/2211.10209) | 通过使用自适应阈值来考虑数据集中敏感属性类别不平衡，我们提出了一种实用且有效的属性推断攻击方法。 |
| [^190] | [Kernel Normalized Convolutional Networks](https://arxiv.org/abs/2205.10089) | 提出了核规范化卷积网络（KNConvNets），通过替代BatchNorm层，在图像分类和语义分割中实现比BatchNorm更高或相媲美的性能，同时也在非私密和差分私密训练中显著优于基于层和组规范化的竞争对手。 |
| [^191] | [Generalizing Graph Neural Networks on Out-Of-Distribution Graphs](https://arxiv.org/abs/2111.10657) | 提出了一个名为StableGNN的通用因果表示框架，通过提取高级图表示并利用因果推断的区分能力，帮助模型在分布外图上实现稳定的泛化能力。 |
| [^192] | [Neural network relief: a pruning algorithm based on neural activity](https://arxiv.org/abs/2109.10795) | 提出了一种基于神经活动的迭代剪枝策略，通过引入简单的重要性评分指标来停用无关紧要的连接，解决了DNNs中的参数过多问题并实现了显著的参数压缩，在MNIST、CIFAR-10/100和Tiny-ImageNet数据集上达到了可比较的性能。 |
| [^193] | [Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning](https://arxiv.org/abs/2102.12920) | 联邦学习作为一种新的学习范式，不仅有助于改进算法和模型融合方法，还展示了与其他学习框架相结合的潜力，特别是在联邦 X 学习中，为多任务学习、元学习、迁移学习等提供了新的研究视角。 |
| [^194] | [Making deep neural networks right for the right scientific reasons by interacting with their explanations](https://arxiv.org/abs/2001.05371) | 通过“解释交互学习”(XIL)学习设置，研究者可以与深度神经网络进行交互，有助于避免其利用混淆因素而导致的高性能，同时增强对模型的信任。 |
| [^195] | [Contextual Text Denoising with Masked Language Models](https://arxiv.org/abs/1910.14080) | 提出了一种基于遮蔽语言模型的上下文文本去噪算法，可以在不重新训练模型的情况下纠正噪声文本，并在多个下游任务中提高性能 |
| [^196] | [Pair-Matching: Links Prediction with Adaptive Queries](https://arxiv.org/abs/1905.07342) | 本文展示如果图是根据随机块模型（SBM）生成的情况下，可以实现Pair-Matching问题中的次线性遗憾。 |
| [^197] | [Augmenting Replay in World Models for Continual Reinforcement Learning.](http://arxiv.org/abs/2401.16650) | 本研究通过在回放缓冲区中应用增强方法，成功地解决了增强连续强化学习中的内存限制问题，并在世界模型中有效防止灾难性遗忘。 |
| [^198] | [Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents.](http://arxiv.org/abs/2401.16461) | 通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。 |
| [^199] | [ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation.](http://arxiv.org/abs/2401.14074) | ProCNS是一种用于弱监督医学图像分割的新方法，采用渐进式原型校准和噪声抑制的原则来解决现有方法中的问题。 |
| [^200] | [Feature Selection for Functional Data Classification.](http://arxiv.org/abs/2401.05765) | 本文介绍了一种名为FSFC的新方法，它解决了在具有分类响应和纵向特征的情况下同时进行功能数据特征选择和分类的挑战。 |
| [^201] | [Masked Audio Generation using a Single Non-Autoregressive Transformer.](http://arxiv.org/abs/2401.04577) | MAGNeT是一种遮蔽生成序列建模方法，使用单一非自回归Transformer生成具有高质量的音频，并引入了一种新颖的重新评分方法来提高生成音频的质量。同时，MAGNeT还探索了混合版本，可在自回归模式和非自回归模式下生成序列。在实验中证明MAGNeT在文本到音乐和文本到音频生成任务中具有高效性。 |
| [^202] | [Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data.](http://arxiv.org/abs/2311.03131) | 该论文介绍了一种基于水库计算网络的计算模型，可以从电生理测量数据中解码神经元网络的时空信息，并在宏观领域内重建网络结构。实验证明该模型比其他常用方法更准确地预测了网络的连接图，并且能够预测网络对特定输入的响应能力。 |
| [^203] | [Robust Causal Bandits for Linear Models.](http://arxiv.org/abs/2310.19794) | 本文研究了线性模型的鲁棒因果强化学习算法，在复杂系统的情况下，现有方法无法保持遗憾次线性。 |
| [^204] | [Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection.](http://arxiv.org/abs/2310.17748) | OrionBench是一个以用户为中心的无监督时间序列异常检测基准测试，提供了通用抽象、可扩展性和发布频繁的基准测试。 |
| [^205] | [To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets.](http://arxiv.org/abs/2310.13061) | 本研究探讨了在深度学习中的泛化和记忆的问题，通过对模数算术任务上训练的神经网络进行实验，发现网络可以同时记住损坏的标签并实现100%的泛化，并且可以通过识别和剪枝记忆化的神经元来降低对损坏数据的准确率，提高对未损坏数据的准确率。 |
| [^206] | [Zipformer: A faster and better encoder for automatic speech recognition.](http://arxiv.org/abs/2310.11230) | Zipformer是一种更快速、更节省内存、性能更好的自动语音识别编码器，通过U-Net-like编码器结构、重新组织的块结构、改进的LayerNorm、新的激活函数和新的优化器等方式实现了优化。实验证明它在LibriSpeech、Aishell-1和Wenet等数据集上表现出更快的收敛和更好的性能。 |
| [^207] | [LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions.](http://arxiv.org/abs/2310.10096) | 本文提出了一个大规模表格LLP基准，填补了表格LLP领域的研究空白。在该基准中，我们可以创建特征bags，其中所有实例具有相同的特征值，从而更好地模拟实际应用场景。 |
| [^208] | [Identifying Representations for Intervention Extrapolation.](http://arxiv.org/abs/2310.04295) | 本文研究了干预外推的任务，证明了可识别的表示方法能够有效地解决这个任务，即使干预对结果产生非线性影响。 |
| [^209] | [Forecasting Tropical Cyclones with Cascaded Diffusion Models.](http://arxiv.org/abs/2310.01690) | 本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。 |
| [^210] | [Towards Poisoning Fair Representations.](http://arxiv.org/abs/2309.16487) | 这项研究提出了第一个针对公平表示学习（FRL）的数据中毒框架，该框架在对抗性场景下评估模型的健壮性，解决了FRL方法在面对数据中毒攻击时的脆弱性。 |
| [^211] | [Provable Training for Graph Contrastive Learning.](http://arxiv.org/abs/2309.13944) | 图对比学习中，我们发现训练存在不平衡的问题，为此我们提出了“节点紧凑性”度量来指导训练。 |
| [^212] | [Minimum width for universal approximation using ReLU networks on compact domain.](http://arxiv.org/abs/2309.10402) | 本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\mathbb R^{d_y}$所需的最小宽度为$\max\{d_x,d_y,2\}$，从而表明在紧致域上的逼近比在${\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\min}\ge d_y+1$（当$d_x<d_y\le2d_x$）。 |
| [^213] | [Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources.](http://arxiv.org/abs/2308.09766) | 本论文综述了机器学习在未监测站点的水资源预测中的应用，包括河流流量、水质等相关变量，并讨论了融合集水区特征的新方法，提升机器学习的使用。 |
| [^214] | [A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs.](http://arxiv.org/abs/2307.14940) | 本论文提出了一种自适应惩罚算法，用于将约束的自然系统集成到神经常微分方程中，通过引入先验知识提高了模型的可解释性，并通过数值实验证明了其有效性。 |
| [^215] | [METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation.](http://arxiv.org/abs/2307.13991) | 本文提出了METAVerse，一个用于在各种环境中准确可靠地预测地形可行性的元学习框架。通过自监督学习，利用稀疏的LiDAR点云生成密集连续值成本图，通过元学习训练全局模型，有效减小地形可行性估计的不确定性。 |
| [^216] | [Nonparametric Linear Feature Learning in Regression Through Regularisation.](http://arxiv.org/abs/2307.12754) | 本研究提出了一种新的非参数线性特征学习方法，对于监督学习中存在于低维线性子空间中的相关信息的预测和解释能力的提升是非常有帮助的。 |
| [^217] | [Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition.](http://arxiv.org/abs/2307.11333) | 本文利用信息论的部分信息分解（PID）方法，研究了在联邦学习中关于敏感属性的群体公平性权衡问题。通过分解发现了三种不公平来源，分别是唯一不平等性、冗余不平等性和掩盖不平等性，揭示了全局和局部公平性之间的基本限制和权衡。 |
| [^218] | [DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding.](http://arxiv.org/abs/2307.06924) | DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。 |
| [^219] | [Momentum Benefits Non-IID Federated Learning Simply and Provably.](http://arxiv.org/abs/2306.16504) | 本论文研究了在非独立同分布联邦学习中利用动量来提升FedAvg和SCAFFOLD算法的性能，证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。 |
| [^220] | [Mass-Producing Failures of Multimodal Systems with Language Models.](http://arxiv.org/abs/2306.12105) | 本文介绍了一种MultiMon系统，可以自动识别多模态系统中的系统性失败，揭示CLIP文本编码器的14个系统性失败，每个都由数百个不同的输入组成，这些输入会导致其他大多数最先进的多模态系统的失败。 |
| [^221] | [Optimal Inference in Contextual Stochastic Block Models.](http://arxiv.org/abs/2306.07948) | 本论文提出了一种基于信念传播的算法，用于半监督上下文随机块模型(cSBM)的推断问题。研究发现，相比于现有的图神经网络结构，这种算法达到的准确性更高，同时可用于建立性能基准。 |
| [^222] | [Soft-prompt Tuning for Large Language Models to Evaluate Bias.](http://arxiv.org/abs/2306.04735) | 本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。 |
| [^223] | [Input gradient diversity for neural network ensembles.](http://arxiv.org/abs/2306.02775) | 本文提出了一阶斥力深度集成 (FoRDE) 算法，它使用输入梯度来增强多样性以提高神经网络集成的表现。 |
| [^224] | [Synaptic Weight Distributions Depend on the Geometry of Plasticity.](http://arxiv.org/abs/2305.19394) | 计算神经科学的研究表明，突触权重分布取决于突触可塑性的几何形态，进而表明实验观测到的对数正态权重分布与标准的梯度下降模型不一致，可能说明大脑中使用的是非欧几里得距离。 |
| [^225] | [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.](http://arxiv.org/abs/2305.14342) | Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。 |
| [^226] | [Fine-tuning Language Models with Generative Adversarial Feedback.](http://arxiv.org/abs/2305.06176) | 本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。 |
| [^227] | [Application of Transformers for Nonlinear Channel Compensation in Optical Systems.](http://arxiv.org/abs/2304.13119) | 本文提出了一种利用Transformer进行光学系统非线性通道补偿的新方法，这种方法利用了Transformer的记忆关注能力和并行结构，实现了高效的非线性补偿。同时，作者还提出了一种物理学信息掩码，用于降低计算复杂度。 |
| [^228] | [On the lifting and reconstruction of nonlinear systems with multiple attractors.](http://arxiv.org/abs/2304.11860) | 本文研究了具有多个吸引子的非线性系统的Koopman算子提升和重构机制，通过利用吸引域之间的固有对称性，只需三个自由度的线性重构就可以全局线性化系统。 |
| [^229] | [A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology.](http://arxiv.org/abs/2304.04497) | META-CODE是一个统一的框架，通过探索学习和易于收集的节点元数据，在未知拓扑网络中检测重叠社区。实验结果证明了META-CODE的有效性和可扩展性。 |
| [^230] | [Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations.](http://arxiv.org/abs/2303.16618) | 本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。 |
| [^231] | [Generating Multidimensional Clusters With Support Lines.](http://arxiv.org/abs/2301.10327) | 提出了一个名为Clugen的模块化合成数据生成过程，能够使用任意分布创建支持线段的多维聚类，适用于评估聚类算法并有潜力成为广泛使用的框架。 |
| [^232] | [Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery.](http://arxiv.org/abs/2206.10540) | 本文重新审视并重新创建了符号回归数据集，以评估符号回归在科学发现中的潜力和能力，并提出使用归一化编辑距离进行度量。 |
| [^233] | [Confident Sinkhorn Allocation for Pseudo-Labeling.](http://arxiv.org/abs/2206.05880) | 本文提出了一种基于置信泰森堡分配的伪标签方法，通过最优传输仅对高置信度样本进行伪标签分配，在半监督学习方面取得了目前最好的表现。 |
| [^234] | [AutoGL: A Library for Automated Graph Learning.](http://arxiv.org/abs/2104.04987) | AutoGL是第一个专门用于自动图机器学习的开源库，使用方便且易于扩展。它提供了一个完整的自动图学习流程，并支持各种图应用。 |

# 详细

[^1]: LC-Tsalis-INF: 广义最佳双赢线性背景强化型赌博机

    LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits

    [https://arxiv.org/abs/2403.03219](https://arxiv.org/abs/2403.03219)

    该研究提出了一种广义最佳双赢线性背景强化型赌博机算法，能够在次优性差距受到下界限制时遗憾为$O(\log(T))$。同时引入了边缘条件来描述次优性差距对问题难度的影响。

    

    本研究考虑具有独立同分布（i.i.d.）背景的线性背景强化型赌博机问题。在这个问题中，现有研究提出了最佳双赢（BoBW）算法，其遗憾在随机区域中满足$O(\log^2(T))$，其中$T$为回合数，其次优性差距由正常数下界，同时在对抗性区域中满足$O(\sqrt{T})$。然而，对$T$的依赖仍有改进空间，并且次优性差距的假设可以放宽。针对这个问题，本研究提出了一个算法，当次优性差距受到下界限制时，其遗憾满足$O(\log(T))$。此外，我们引入了一个边缘条件，即对次优性差距的一个更温和的假设。该条件使用参数$\beta \in (0, \infty]$表征与次优性差距相关的问题难度。然后我们证明该算法的遗憾满足$O\left(\

    arXiv:2403.03219v1 Announce Type: new  Abstract: This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\beta \in (0, \infty]$. We then show that the algorithm's regret satisfies $O\left(\
    
[^2]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^3]: 主动统计推断

    Active Statistical Inference

    [https://arxiv.org/abs/2403.03208](https://arxiv.org/abs/2403.03208)

    主动推断是一种统计推断方法，通过利用机器学习模型确定最有利于标记的数据点来有效利用预算，实现比现有基线更少样本的相同准确性。

    

    受主动学习概念启发，我们提出了主动推断——一种利用机器学习辅助数据收集进行统计推断的方法。假设对可收集的标签数量有预算限制，该方法利用机器学习模型确定哪些数据点最有利于标记，从而有效利用预算。其运作方式基于一种简单而强大的直觉：优先收集模型表现出不确定性的数据点的标签，并在模型表现出自信时依赖于其预测。主动推断构建了可证明有效的置信区间和假设检验，同时利用任何黑盒机器学习模型并处理任何数据分布。关键点在于，它能以比依赖于非自适应收集数据的现有基线更少的样本达到相同水平的准确性。这意味着对于相同数量的样本，...

    arXiv:2403.03208v1 Announce Type: cross  Abstract: Inspired by the concept of active learning, we propose active inference$\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number 
    
[^4]: 具有检索功能的可靠、适应性强且可追溯的语言模型

    Reliable, Adaptable, and Attributable Language Models with Retrieval

    [https://arxiv.org/abs/2403.03187](https://arxiv.org/abs/2403.03187)

    持续面临挑战的参数化语言模型LMs，作者主张使用具有检索功能的LMs作为下一代LMs，以提高可靠性、适应性和可追溯性。

    

    参数化语言模型（LMs）在海量网络数据上训练，表现出卓越的灵活性和能力。然而，它们仍然面临着幻觉、难以适应新数据分布和缺乏可验证性等实际挑战。在这篇立场论文中，我们主张用具备检索功能的LMs取代参数化LMs作为下一代LMs。通过在推理过程中整合大规模数据存储，具有检索功能的LMs可以更加可靠、适应性更强、可追溯。

    arXiv:2403.03187v1 Announce Type: cross  Abstract: Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose
    
[^5]: 用占用度测量正则化防止奖励欺骗

    Preventing Reward Hacking with Occupancy Measure Regularization

    [https://arxiv.org/abs/2403.03185](https://arxiv.org/abs/2403.03185)

    用占用度测量正则化方法可以有效防止奖励欺骗，通过考虑代理与真实奖励之间大的状态占用度偏差来避免潜在的灾难后果。

    

    当代理根据一个“代理”奖励函数（可能是手动指定或学习的）表现出色，但相对于未知的真实奖励却表现糟糕时，就会发生奖励欺骗。由于确保代理和真实奖励之间良好对齐极为困难，预防奖励欺骗的一种方法是保守地优化代理。以往的研究特别关注于通过惩罚他们的行为分布之间的KL散度来强制让学习到的策略表现类似于“安全”策略。然而，行为分布的正则化并不总是有效，因为在单个状态下行为分布的微小变化可能导致潜在的灾难性后果，而较大的变化可能并不代表任何危险活动。我们的见解是，当奖励欺骗时，代理访问的状态与安全策略达到的状态截然不同，导致状态占用度的巨大偏差。

    arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
    
[^6]: Transformers能多好地模拟 Newton 方法上下文中的表现？

    How Well Can Transformers Emulate In-context Newton's Method?

    [https://arxiv.org/abs/2403.03183](https://arxiv.org/abs/2403.03183)

    Transformers能够实现高阶优化算法，线性注意力Transformer可以在 logistic 回归任务中近似实现二阶优化算法，并展示即使是线性注意力的Transformer也可以实现矩阵求逆的牛顿迭代。

    

    基于Transformer的模型展示了显著的上下文学习能力，引发了对其基础机制的广泛研究。最近的研究表明，Transformers可以实现一阶优化算法进行上下文学习，甚至对于线性回归的情况，可以实现二阶优化算法。在这项工作中，我们研究了Transformer是否能够执行高阶优化方法，超越了线性回归的情况。我们确定具有ReLU层的线性注意力Transformer可以近似实现二阶优化算法，用于逻辑回归任务，并且仅使用对数到错误更多的层可以达到$\epsilon$误差。作为副产品，我们展示了即使是仅具有线性注意力的Transformer也可以在仅两层的情况下实现矩阵求逆的牛顿迭代的单步。这些结果表明了Transformer架构实现的潜力。

    arXiv:2403.03183v1 Announce Type: cross  Abstract: Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to im
    
[^7]: 具有潜在动作的行为生成

    Behavior Generation with Latent Actions

    [https://arxiv.org/abs/2403.03181](https://arxiv.org/abs/2403.03181)

    这项工作介绍了一种名为矢量量化行为转换器（VQ-BeT）的多功能行为生成模型，通过对连续动作进行标记化处理多模态动作预测、条件生成和部分观察。

    

    从带标签的数据集中生成复杂行为的生成建模一直是决策制定中长期存在的问题。与语言或图像生成不同，决策制定需要建模动作 - 连续值向量，其在分布上是多模态的，可能来自未经筛选的来源，在顺序预测中生成误差可能会相互累积。最近一类称为行为转换器（BeT）的模型通过使用k-means聚类对动作进行离散化以捕捉不同模式来解决这个问题。然而，k-means在处理高维动作空间或长序列时存在困难，并且缺乏梯度信息，因此BeT在建模长距离动作时存在困难。在这项工作中，我们提出了一种名为矢量量化行为转换器（VQ-BeT）的多功能行为生成模型，用于处理多模态动作预测、条件生成和部分观察。VQ-BeT通过对连续动作进行标记化来增强BeT

    arXiv:2403.03181v1 Announce Type: cross  Abstract: Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous
    
[^8]: 用于凸优化的洗牌动量梯度算法

    Shuffling Momentum Gradient Algorithm for Convex Optimization

    [https://arxiv.org/abs/2403.03180](https://arxiv.org/abs/2403.03180)

    本研究将洗牌动量梯度方法扩展到有限和强凸优化问题，首次提供了针对强凸设置的洗牌动量方法的分析，达到了收敛速度为$O(1/nT^2)$。

    

    随机梯度下降方法（SGD）及其随机变体已成为解决机器学习和数据科学中由大规模应用和大数据集产生的有限和优化问题的首选方法，由于其能够处理大规模应用和大型数据集。过去几十年，研究人员已经付出了大量努力来研究SGD及其洗牌变体的理论性能。然而，只有有限的工作涉及了其洗牌动量变体，包括用于非凸问题的洗牌重量球动量方案和用于凸设置的Nesterov动量。在这项工作中，我们将Tran等人（2021年）所开发的洗牌动量梯度方法的分析拓展到有限和强凸优化问题，我们首次提供了针对强凸设置的洗牌动量方法的分析，达到了收敛速度为$O(1/nT^2)$，其中$n$是数量

    arXiv:2403.03180v1 Announce Type: cross  Abstract: The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number 
    
[^9]: 使用目标想象在合作多智能体强化学习中实现共识

    Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination

    [https://arxiv.org/abs/2403.03172](https://arxiv.org/abs/2403.03172)

    提出了一种基于模型的共识机制，使用多智能体目标想象框架引导智能体达成共识，从而提高合作多智能体强化学习的性能。

    

    达成一致意见对于多智能体协调至关重要。为了完成协作任务，智能体需要协调地选择最佳的联合动作，以最大化团队奖励。然而，当前的合作多智能体强化学习方法通常不明确考虑一致性，这可能导致协调问题。在本文中，我们提出了一种基于模型的共识机制，以明确协调多个智能体。提出的多智能体目标想象（MAGI）框架引导智能体通过想象出的共同目标达成一致。共同目标是一个具有高价值的可实现状态，通过从未来状态分布中采样获得。我们直接使用自我监督生成模型对此分布进行建模，从而缓解了模型方法中常用的多智能体多步骤策略展开引起的“维度灾难”问题。我们展示了这种高效的共识机制可以提高合作多智能体强化学习的性能。

    arXiv:2403.03172v1 Announce Type: new  Abstract: Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the "curse of dimensinality" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient c
    
[^10]: 学习显式条件化稀疏变换

    Learning Explicitly Conditioned Sparsifying Transforms

    [https://arxiv.org/abs/2403.03168](https://arxiv.org/abs/2403.03168)

    该论文提出了一种新的稀疏变换模型，通过显式控制数据表示质量和条件数，有效地学习保证数据在稀疏域中具有良好表示的优化变换。

    

    在过去的几十年里，稀疏化变换已经成为一种广为人知的工具，用于在某些变换域中找到信号的结构稀疏表示。尽管像DCT和小波这样的经典变换很受欢迎，但最近在一系列论文中已经分析了学习保证数据在稀疏域中具有良好表示的最优变换。学习方块变换的条件数和表示能力通常是互补的关键特征，可能在给定的优化模型中不能明确控制。与现有文献中的方法不同，在我们的论文中，我们考虑了一种新的稀疏变换模型，该模型强制在学习变换的数据表示质量和条件数上进行显式控制。我们通过数值实验确认，我们的模型比最先进的模型具有更好的数值行为。

    arXiv:2403.03168v1 Announce Type: cross  Abstract: Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.
    
[^11]: PalmProbNet：一种通过迁移学习理解厄瓜多尔热带雨林中棕榈分布的概率方法

    PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning

    [https://arxiv.org/abs/2403.03161](https://arxiv.org/abs/2403.03161)

    PalmProbNet 是一种利用迁移学习分析高分辨率无人机图像的概率方法，可以在厄瓜多尔雨林中检测棕榈树，实现自动化的棕榈树检测，有效确定棕榈树在混合热带雨林中的存在和位置。

    

    棕榈树在热带森林中发挥着重要作用，对于人类和野生动物来说是重要资源。热带生态系统中的一个核心问题是理解棕榈树的分布和丰度。然而，在热带雨林的图像中准确识别和定位棕榈树由于密集植被、重叠树冠以及混合森林景观中的光照条件变化而面临重大挑战。为解决这一问题，我们引入了PalmProbNet，这是一种利用迁移学习的概率方法，能够分析高分辨率的无人机获取的正射影像图像，从而在厄瓜多尔雨林的密集树冠中检测棕榈树。这种方法在自动棕榈树检测方面取得了重大进展，能够有效地确定混合热带雨林中的棕榈树存在和位置。我们的过程始于从无人机图像生成正射影像，并从中提取和标记棕榈树和非棕榈树图像块。

    arXiv:2403.03161v1 Announce Type: cross  Abstract: Palms play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding palm distribution and abundance. However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patch
    
[^12]: 在NOMA增强的无线网络中重新思考聚类联邦学习

    Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks

    [https://arxiv.org/abs/2403.03157](https://arxiv.org/abs/2403.03157)

    本研究探讨了在NOMA增强的无线网络中将聚类联邦学习与非独立同分布数据集相结合的优势，并提出了解决非IID条件挑战的解决方案。

    

    本研究探讨了将新型的聚类联邦学习（CFL）方法与非正交多址接入（NOMA）相结合在非独立同分布（non-IID）数据集下的优势，其中多个设备参与具有时间限制和有限子信道数量的聚合。详细的理论分析了衡量数据分布中非IID程度的泛化差距。在此之后，提出了解决非IID条件所带来挑战的解决方案，并分析了各项性质。具体地，用户的数据分布被参数化为集中参数，并使用谱聚类进行分组，Dirichlet分布作为先验。对泛化差距和收敛速率的探讨指导了通过基于匹配的算法设计子信道分配，并实现功率分配。

    arXiv:2403.03157v1 Announce Type: cross  Abstract: This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels. A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented. Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties. Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior. The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achie
    
[^13]: 深度学习压缩用于射频信号分类

    Deep-Learned Compression for Radio-Frequency Signal Classification

    [https://arxiv.org/abs/2403.03150](https://arxiv.org/abs/2403.03150)

    提出了一个深度学习压缩模型HQARF，用于对射频信号进行压缩和分类，以降低数据传输的带宽和延迟成本。

    

    下一代蜂窝概念依赖于处理大量的射频（RF）样本，包括连接基于软件定义无线电（SDR）的蜂窝前端的射频接入网络（RAN）和用于处理频谱相关数据的人工智能框架。我们提出了一个基于学习矢量量化（VQ）的深度学习压缩（DLC）模型HQARF，用于压缩由6种调制类别组成的射频信号的复值样本。我们正在评估HQARF对训练推断射频信号调制类别的AI模型性能的影响。窄带射频样本的压缩将允许有效进行训练和现场推断。

    arXiv:2403.03150v1 Announce Type: new  Abstract: Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples. This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data. The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making. Moving large amounts of data to AI agents may result in significant bandwidth and latency costs. We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes. We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal. Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient
    
[^14]: 强大的联邦学习方法缓解客户端训练数据分布推断攻击

    Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks

    [https://arxiv.org/abs/2403.03149](https://arxiv.org/abs/2403.03149)

    本研究提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，用于防御客户端训练数据分布推断攻击。

    

    近期研究揭示了联邦学习（FL）曾被认为安全的漏洞，因为客户端不向服务器共享其私有数据，然而这种方法容易遭受诸如客户端训练数据分布推断攻击等攻击，此攻击可以让恶意客户端重现受害者的数据。本文提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，旨在防御客户端训练数据分布推断攻击。在我们提出的InferGuard中，服务器首先计算其收到的所有模型更新的坐标中位数。如果客户端的模型更新与计算出的中位数更新显著偏离，则视为恶意。我们对我们的InferGuard在五个基准数据集上进行了彻底评估。

    arXiv:2403.03149v1 Announce Type: cross  Abstract: Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark dat
    
[^15]: 双重均值教师：用于音频-视觉源定位的无偏半监督框架

    Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization

    [https://arxiv.org/abs/2403.03145](https://arxiv.org/abs/2403.03145)

    提出了一种新颖的半监督学习框架用于音频-视觉源定位，通过双重均值教师结构避免确认偏差问题，并利用有限标记数据和丰富无标记数据生成高质量伪标签。

    

    音频-视觉源定位（AVSL）旨在在视频帧中定位声音对象，给定配对的音频剪辑。现有方法主要依赖于音频-视觉对应关系的自监督对比学习。在没有任何边界框注释的情况下，它们很难实现精确的定位，特别是对于小物体，并且容易出现边界模糊和误报。此外，朴素的半监督方法在充分利用丰富的无标记数据信息方面效果不佳。在本文中，我们提出了一种用于AVSL的新颖半监督学习框架，即双重均值教师（DMT），包括两个教师-学生结构，以规避确认偏差问题。具体地，两个在有限标记数据上预训练的教师被用来通过它们预测之间的一致性来过滤嘈杂样本，然后通过交叉它们的置信图生成高质量伪标签。

    arXiv:2403.03145v1 Announce Type: cross  Abstract: Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The
    
[^16]: 深度集成模型中的新型等变性

    Emergent Equivariance in Deep Ensembles

    [https://arxiv.org/abs/2403.03103](https://arxiv.org/abs/2403.03103)

    深度集成模型通过简单使用数据增强即可实现在所有输入和训练时刻都保持等变性，这种等变性在离开流形时和无限宽度限制下的任何架构都能保持。

    

    我们展示了深度集成模型是暗中等变的模型。更准确地说，我们表明通过简单使用数据增强，深度集成模型在所有输入和所有训练时刻都变得等变。至关重要的是，这种等变性在离开流形时和在无限宽度限制下的任何架构都保持。这种等变性是新兴的，因为单个集成成员的预测并非等变，但它们的集体预测是等变的。我们使用神经切线核理论推导了这一结果，并通过详细的数值实验验证了我们的理论见解。

    arXiv:2403.03103v1 Announce Type: new  Abstract: We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.
    
[^17]: KnowAgent: 知识增强规划用于基于LLM的Agent

    KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents

    [https://arxiv.org/abs/2403.03101](https://arxiv.org/abs/2403.03101)

    KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。

    

    大型语言模型(LLMs)在复杂推理任务中表现出巨大潜力，但在处理更复杂的挑战时仍有所不足，特别是与环境互动通过生成可执行动作时。这种不足主要来自于语言Agent中缺乏内置动作知识，导致在任务求解过程中无法有效引导规划轨迹，从而导致规划幻觉。为了解决这个问题，我们引入了KnowAgent，一种旨在通过整合显式动作知识来增强LLM规划能力的新方法。具体而言，KnowAgent采用了一个动作知识库和一个知识型自学习策略来限制规划过程中的行动路径，实现更合理的轨迹合成，进而提高语言Agent的计划性能。基于HotpotQA和ALFWorld的实验结果基于不同的主干模型。

    arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
    
[^18]: NaturalSpeech 3: 利用分解编解码器和扩散模型实现零-shot语音合成

    NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models

    [https://arxiv.org/abs/2403.03100](https://arxiv.org/abs/2403.03100)

    NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音

    

    近期大规模文本到语音（TTS）模型取得了显著进展，然而在语音质量、相似度和韵律方面仍存在不足。鉴于语音复杂地包含各种属性（例如内容、韵律、音色和声学细节），给生成带来了重大挑战，一个自然的想法是将语音因子分解为代表不同属性的各个子空间，并单独生成它们。在此基础上，我们提出了NaturalSpeech 3，这是一个具有新颖的分解扩散模型的TTS系统，可以以零-shot方式生成自然语音。具体来说，1) 我们设计了一个具有分解向量量化（FVQ）的神经编解码器，将语音波形分解为内容、韵律、音色和声学细节的子空间；2) 我们提出了一个分解扩散模型，根据其相应的提示生成每个子空间中的属性。借助这种分解设计，NaturalSpeech 3能够ef

    arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
    
[^19]: VQSynergy: 利用矢量量化机制进行稳健的药物协同预测

    VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism

    [https://arxiv.org/abs/2403.03089](https://arxiv.org/abs/2403.03089)

    VQSynergy通过矢量量化机制以及其他创新技术提高了药物协同预测的精度和泛化能力，在处理高斯噪声条件下表现出色。

    

    近年来高通量筛选和计算创新的出现引领了一种更高效的探索药物相互作用方法学的转变。本研究提出了VQSynergy，这是一个采用了矢量量化（VQ）机制、与门控残差和量身定制的注意机制相结合的新颖框架，以增强药物协同预测的精度和泛化能力。我们的研究结果表明，在高斯噪声条件下，VQSynergy在鲁棒性方面超过了现有模型，突显了其在复杂且常常嘈杂的药物协同研究领域中的卓越表现和实用性。

    arXiv:2403.03089v1 Announce Type: cross  Abstract: The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in rev
    
[^20]: 具有生成对抗元模型的召回导向的持续学习

    Recall-Oriented Continual Learning with Generative Adversarial Meta-Model

    [https://arxiv.org/abs/2403.03082](https://arxiv.org/abs/2403.03082)

    框架提出了召回导向的持续学习方法，通过生成对抗元模型(GAMM)在学习新任务时最大化过去知识的稳定性。

    

    稳定性-可塑性困境是持续学习中的主要挑战，因为它涉及在学习新任务的同时保持对以前任务性能的平衡。本文提出了召回导向的持续学习框架来解决这一挑战。灵感来自于人类大脑分离稳定性和可塑性机制的能力，我们的框架包括一个两级体系结构，其中推理网络有效地获取新知识，而生成网络在需要时回顾过去的知识。具体地，为了最大化过去知识的稳定性，我们研究了不同表示取决于知识复杂度，从而引入了增量学习任务特定参数而不是任务的输入数据样本的生成对抗元模型（GAMM）。通过实验证明，我们的框架不仅

    arXiv:2403.03082v1 Announce Type: cross  Abstract: The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only 
    
[^21]: 论Brenier的极分解的神经实现

    On a Neural Implementation of Brenier's Polar Factorization

    [https://arxiv.org/abs/2403.03071](https://arxiv.org/abs/2403.03071)

    提出了Brenier的极分解定理的神经实现，探讨了在机器学习中的应用，并通过神经网络参数化潜在函数$u$，从最新神经最优输运领域的进展中汲取灵感。

    

    在1991年，Brenier证明了一个定理，将$QR$分解（分为半正定矩阵$\times$酉矩阵）推广到任意矢量场$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$。这个被称为极分解定理的定理表明，任意场$F$都可以表示为凸函数$u$的梯度与保测度映射$M$的复合，即$F=\nabla u \circ M$。我们提出了这一具有深远理论意义的结果的实际实现，并探讨了在机器学习中可能的应用。该定理与最优输运（OT）理论密切相关，我们借鉴了神经最优输运领域的最新进展，将潜在函数$u$参数化为输入凸神经网络。映射$M$可以通过使用$u^*$，即$u$的凸共轭，逐点计算得到，即$M=\nabla u^* \circ F$，或者作为辅助网络学习得到。因为$M$在基因

    arXiv:2403.03071v1 Announce Type: cross  Abstract: In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\times$ unitary -- to any vector field $F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary network. Because $M$ is, in gene
    
[^22]: 用混合变分家族改进从不完整数据估计的变分自动编码器

    Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families

    [https://arxiv.org/abs/2403.03069](https://arxiv.org/abs/2403.03069)

    缺失数据增加了模型对潜在变量后验分布的复杂性，本文提出了两种策略——基于有限变分混合和基于填补的变分混合分布，有效改善了从不完整数据估计VAE的准确性。

    

    我们考虑了在训练数据不完整的情况下估计变分自动编码器（VAEs）的任务。我们证明了缺失数据会增加模型对潜在变量的后验分布的复杂性，与完全观测的情况相比。增加的复杂性可能会由于变分分布和模型后验分布之间的不匹配而对模型拟合产生不利影响。我们引入了两种基于（i）有限变分混合和（ii）基于填补的变分混合分布的策略，以解决增加的后验复杂性。通过对所提出方法的全面评估，我们表明变分混合在改进从不完整数据估计VAE的准确性方面是有效的。

    arXiv:2403.03069v1 Announce Type: new  Abstract: We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions. We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data.
    
[^23]: 具有有限通信范围的线性二次网络控制的分布式策略梯度

    Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range

    [https://arxiv.org/abs/2403.03055](https://arxiv.org/abs/2403.03055)

    提出了一种在多智能体线性二次网络系统中收敛到近似最优解的可扩展分布式策略梯度方法，证明了随着通信和控制范围的增加，性能差距会指数级减小为零，并展示了增加通信范围如何增强系统稳定性和揭示关键权衡。

    

    本文提出了一种可扩展的分布式策略梯度方法，并证明其在多智能体线性二次网络系统中收敛到近似最优解。智能体在特定网络中进行交互，受限于本地通信约束，意味着每个智能体只能与有限数量的相邻智能体交换信息。在网络的底层图中，每个智能体在线性二次控制设置中根据其附近邻居的状态实施其控制输入。我们展示了只使用局部信息即可近似精确梯度是可能的。与集中式最优控制器相比，随着通信和控制范围的增加，性能差距指数级减小为零。我们还展示了如何增加通信范围可以增强梯度下降过程中系统稳定性，从而阐明了一个关键的权衡。模拟结果验证了

    arXiv:2403.03055v1 Announce Type: cross  Abstract: This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify
    
[^24]: 一个充满技能的背包：具有多元任务视角的自我中心视频理解

    A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives

    [https://arxiv.org/abs/2403.03037](https://arxiv.org/abs/2403.03037)

    提出了EgoPack，这是一个统一的视频理解方法，结合共享时间建模和最小开销，支持多个下游任务，并在学习新技能时进行合作，为智能机器提供全面的视频理解能力。

    

    arXiv：2403.03037v1 公告类型：交叉摘要：人类对视频流的理解自然而然是广泛的：在短短的瞬间内，我们能够理解发生了什么，对象的相关性和关系，并预测接下来的将来，所有这些一次性完成。我们认为，要有效地将这样的整体感知转移到智能机器中，学习相关概念和提炼来自不同任务的知识之间的关系扮演了重要角色，以便在学习新技能时协同利用它们。为了实现这一点，我们寻求一种统一的视频理解方法，将人类行为的共享时间建模与最小开销相结合，以支持多个下游任务并在学习新技能时进行合作。然后，我们提出了EgoPack，这是一个解决方案，创建了在下游任务中可以携带的任务视角集合，并可用作额外见解的潜在来源，就像一个背包。

    arXiv:2403.03037v1 Announce Type: cross  Abstract: Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpac
    
[^25]: SplAgger：用于元强化学习的分割聚合

    SplAgger: Split Aggregation for Meta-Reinforcement Learning

    [https://arxiv.org/abs/2403.03020](https://arxiv.org/abs/2403.03020)

    本文展示了任务推断序列模型在元强化学习中的益处。

    

    强化学习的一个核心目标是创建能快速学习新任务的智能体。元强化学习旨在通过直接学习这些智能体来实现这一目标。一类元强化学习方法被称为黑盒方法，通过端到端训练现成的序列模型来实现这一目标。与之形成对比的是另一类方法，它们明确地推断出未知任务的后验分布。这些方法通常具有不同的目标和序列模型，旨在实现任务推断，因此被称为任务推断方法。本文提出了强有力的证据，证明任务推断序列模型仍然具有益处。

    arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
    
[^26]: CRISPR：集成模型

    CRISPR: Ensemble Model

    [https://arxiv.org/abs/2403.03018](https://arxiv.org/abs/2403.03018)

    提出了一种新颖的集成学习方法用于sgRNA设计，在准确性和泛化能力方面均优于现有方法

    

    Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)是一种革命性的基因编辑技术，已经彻底改变了生物学和医学领域。然而，使用CRISPR的挑战之一是预测单导RNA (sgRNAs)的靶向效力和非靶向敏感性。本文提出了一种新颖的集成学习方法用于sgRNA设计，该方法既准确又具有泛化能力。我们的方法结合了多个机器学习模型的预测结果，以生成单一且更健壮的预测。这种方法使我们能够从更广泛的数据中学习，从而提高了模型的泛化能力。我们在一个基准sgRNA设计数据集上评估了我们的方法，并发现在准确性和泛化能力方面均优于现有方法。我们的结果表明...

    arXiv:2403.03018v1 Announce Type: new  Abstract: Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine. However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability. In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable. Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction. This approach allows us to learn from a wider range of data, which improves the generalizability of our model. We evaluated our method on a benchmark dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability. Our results s
    
[^27]: 使用集成树在恶意URL检测器中缓解标签翻转攻击

    Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees

    [https://arxiv.org/abs/2403.02995](https://arxiv.org/abs/2403.02995)

    本研究旨在利用集成树来缓解恶意URL检测器中的标签翻转攻击，从而在机器学习模型中集成防御机制以防范潜在攻击。

    

    恶意URL提供了跨各行业（包括交通、医疗保健、能源和银行业）的对抗性机会，可能对业务运营造成重大损害。因此，检测这些URL的重要性不言而喻；然而，当前的机器学习（ML）模型容易受到后门攻击的影响。这些攻击涉及操纵少量训练数据标签，如标签翻转（LF），将良性标签更改为恶意标签，反之亦然。这种操纵导致误分类，并导致模型行为不正确。因此，在ML模型架构中集成防御机制成为加固潜在攻击的必要考虑因素。本研究关注在使用集成树进行URL检测背景下的后门攻击。通过阐明此类攻击背后的动机，突出攻击者的角色，并强调

    arXiv:2403.02995v1 Announce Type: cross  Abstract: Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizi
    
[^28]: 受攻击的联邦学习：通过数据中毒攻击揭示在计算机网络中的漏洞

    Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks

    [https://arxiv.org/abs/2403.02983](https://arxiv.org/abs/2403.02983)

    该研究旨在探索计算机网络领域中数据中毒攻击的严重性，使用了两种类型的攻击：标签翻转和特征中毒，采用了新颖的方法。

    

    arXiv:2403.02983v1 公告类型: 跨领域  摘要: 联邦学习(FL)是一种机器学习(ML)方法，使多个分散的设备或边缘服务器协作训练共享模型，而无需交换原始数据。在客户端和服务器之间的训练和模型更新共享过程中，数据和模型容易受到不同的数据中毒攻击。在本研究中，我们旨在探讨计算机网络领域中数据中毒攻击的严重性，因为它们很容易实施但很难检测。我们考虑了两种类型的数据中毒攻击，即标签翻转(LF)和特征中毒(FP)，并采用了一种新颖的方法。在LF中，我们随机翻转了良性数据的标签，并在操纵后的数据上训练模型。对于FP，我们随机操纵了使用随机森林算法确定的高贡献特征。该实验使用的数据集为与计算机网络相关的CIC和UNSW数据集。

    arXiv:2403.02983v1 Announce Type: cross  Abstract: Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We
    
[^29]: 通过反馈在线学习人类约束的共享自治

    Online Learning of Human Constraints from Feedback in Shared Autonomy

    [https://arxiv.org/abs/2403.02974](https://arxiv.org/abs/2403.02974)

    该论文提出了一种学习人类约束模型的方法，用于共享自治方式下的实时协作，目的是支持人类操作者执行共享任务并最小化其不适感。

    

    与人类实时协作面临挑战，因为不同的人类行为模式是受到不同物理约束的影响。现有研究通常关注于学习协作的安全约束，或者如何在参与的代理之间分配和分发子任务来执行主要任务。相比之下，我们提出学习人类约束模型，该模型考虑不同人类操作者的多样化行为。我们考虑一种在共享自治方式下的协作类型，其中人类操作者和辅助机器人同时在同一任务空间内行动，彼此的行动会相互影响。辅助代理的任务是通过尽可能支持人类，既降低工作量也最小化人类操作者的不适感，来增强人类执行共享任务的能力。因此，我们提出了一个增强型助手代理

    arXiv:2403.02974v1 Announce Type: cross  Abstract: Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints. Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task. In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators. We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions. The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator. Therefore, we propose an augmentative assistant agent c
    
[^30]: Hamiltonian性质测试

    Hamiltonian Property Testing

    [https://arxiv.org/abs/2403.02968](https://arxiv.org/abs/2403.02968)

    本文研究了Hamiltonian的本地性测试作为属性测试问题，重点在于确定未知的$n$比特Hamiltonian是否是$k$局部的，通过对$H$的时间演化进行访问来解决问题。

    

    本文研究了Hamiltonian本地性测试作为一个属性测试问题，即确定一个未知的$n$比特Hamiltonian $H$是否是$k$局部的，或者与所有$k$局部Hamiltonian都相距$\varepsilon$，并通过对$H$的时间演化进行访问来解决问题。

    arXiv:2403.02968v1 Announce Type: cross  Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2
    
[^31]: 具有Polyak动量的非凸随机复合优化

    Non-Convex Stochastic Composite Optimization with Polyak Momentum

    [https://arxiv.org/abs/2403.02967](https://arxiv.org/abs/2403.02967)

    本文研究了具有Polyak动量的随机近端梯度方法，在非凸复合优化问题中实现了最佳收敛速度，无论批量大小如何。

    

    随机近端梯度法是广泛使用的随机梯度下降（SGD）方法的一个强大泛化，在机器学习中已经被广泛应用。然而，众所周知，当随机噪声显著时（即仅使用小型或有界批量大小时），该方法在非凸环境中无法收敛。本文关注具有Polyak动量的随机近端梯度方法。我们证明了该方法对于非凸复合优化问题实现了最佳收敛速度，而批量大小大小无关。此外，我们对Polyak动量在复合优化环境中的方差减少效应进行了严格分析，并且我们证明了当近端步骤只能通过近似解来求解时，该方法也会收敛。最后，我们提供了数值实验来验证我们的理论结果。

    arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
    
[^32]: 面向证据的事实摘要化用于知识增强的零-shot问答

    Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering

    [https://arxiv.org/abs/2403.02966](https://arxiv.org/abs/2403.02966)

    提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。

    

    最近的研究探讨了利用知识图谱（KGs）来增强大语言模型（LLMs）的问答（QA）性能，然而结构化的KG形式化仍然具有挑战性。现有方法，如三元组形式或三元组事实的自由文本转换，遇到了一些问题。这些问题包括由于重复实体或关系而导致的证据密度降低，以及由于无法强调关键证据而导致的证据清晰度降低。为解决这些问题，我们提出了EFSum，一个面向证据的事实摘要化框架，用于通过知识增强的LLMs增强QA。我们通过蒸馏和偏好对齐来优化一个开源的LLM作为事实摘要器。我们的广泛实验证明，EFSum提高了LLM的零-shot QA性能，并且可以确保摘要的同时有益和忠实。

    arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
    
[^33]: 关于扩散概率模型渐近均方误差最优性的研究

    On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models

    [https://arxiv.org/abs/2403.02957](https://arxiv.org/abs/2403.02957)

    本论文通过严格证明一个特定的DPM去噪策略在大量扩散步数下收敛到均方误差最优条件均值估计器，突出了DPM由渐近最优的去噪器组成，同时具有强大生成器的独特视角。

    

    最近，扩散概率模型（DPMs）在去噪任务中展现出巨大潜力。尽管它们在实际应用中很有用，但它们的理论理解存在明显的差距。本文通过严格证明特定DPM去噪策略在大量扩散步数下收敛到均方误差（MSE）最优条件均值估计器（CME），为该领域提供了新的理论见解。研究的基于DPM的去噪器在训练过程中与DPMs共享，但在训练后的逆推理过程中仅传递条件均值。我们强调了DPM由渐近最优的去噪器组成的独特视角，同时通过在逆过程中切换重新采样的方式继承了一个强大的生成器。通过数值结果验证了理论发现。

    arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
    
[^34]: SAFFIRA: 一种评估基于Systolic Array的DNN加速器可靠性的框架

    SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators

    [https://arxiv.org/abs/2403.02946](https://arxiv.org/abs/2403.02946)

    介绍了一种针对基于Systolic Array的DNN加速器的新型分层软件化硬件感知故障注入策略，以解决可靠性评估中的时间效率问题。

    

    Systolic array已经成为深度神经网络(DNN)硬件加速器的显着架构，提供高吞吐量和低延迟性能，对于在各种应用中部署DNN至关重要。然而，在安全关键应用中使用时，必须进行可靠性评估以确保DNN加速器的正确行为。虽然故障注入作为一种成熟实用且稳健的可靠性评估方法，但仍然是一个非常耗时的过程。本文通过引入一种针对基于systolic array的DNN加速器量身定制的新型分层软件化硬件感知故障注入策略，解决了时间效率问题。

    arXiv:2403.02946v1 Announce Type: new  Abstract: Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications. However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators. While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process. This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators.
    
[^35]: 用于识别ICU患者亚组的无监督学习方法：结果是否具有普适性？

    Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?

    [https://arxiv.org/abs/2403.02945](https://arxiv.org/abs/2403.02945)

    通过研究不同数据集验证现有研究结果是否具有普适性，以测试是否存在共同的ICU患者亚组。

    

    使用无监督学习来识别患者亚组已经被提出作为改善重症监护病房（ICU）效率的潜在方向。通过识别具有类似医疗资源需求水平的患者亚组，ICUs可以重组为一系列较小的亚单元，每个单元对应一个特定组。然而，目前尚不清楚是否存在跨不同ICUs共同的患者亚组，这将决定ICU重组是否可以以标准化方式实施。在本文中，我们测试了一个假设：通过检查一个现有研究的结果是否可以推广到不同数据集，来验证是否存在共同的ICU患者亚组。我们提取了代表医疗资源需求的16个特征，并使用共识聚类来得出患者亚组，复制了先前的研究。我们发现我们的结果与先前研究的结果之间存在有限的相似性，提供了...

    arXiv:2403.02945v1 Announce Type: new  Abstract: The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs). By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group. However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner. In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset. We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study. We found limited similarities between our results and those of the previous study, providi
    
[^36]: 用文本引导编码的神经图像压缩技术实现像素级和感知准确度的双重提升

    Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity

    [https://arxiv.org/abs/2403.02944](https://arxiv.org/abs/2403.02944)

    该论文提出了一种新的文本引导图像压缩算法，实现了高感知和像素级准确度，并通过文本自适应编码和联合图像-文本损失训练，避免了像素级准确度下降的问题。

    

    最近在文本引导图像压缩方面取得的进展显示出了提高重建图像感知质量的巨大潜力。然而，这些方法往往会导致像素级准确度明显降低，限制了它们的实用性。为了填补这一差距，我们开发了一种新的文本引导图像压缩算法，实现了高感知和像素级准确度。具体来说，我们提出了一种压缩框架，主要通过文本自适应编码和联合图像-文本损失训练来利用文本信息。这样一来，我们避免了基于文本引导生成模型进行解码的问题，这些模型以高生成多样性而闻名，并有效利用了文本的语义信息。在各种数据集上的实验结果表明，我们的方法可以实现高像素级和感知质量，无论是人类生成的标题还是机器生成的标题。特别是，我们的方法在所有基线模型中表现最佳。

    arXiv:2403.02944v1 Announce Type: cross  Abstract: Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms
    
[^37]: AIx Speed：使用语音识别模型的听力理解优化回放速度

    AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models

    [https://arxiv.org/abs/2403.02938](https://arxiv.org/abs/2403.02938)

    本研究探讨了人类是否可以听到经过优化的语音，并提出了一种系统，可根据音素为单位自动调整播放速度，以确保语音可辨识度。

    

    由于人类可以以比实际观察到的速度更快地倾听音频和观看视频，因此我们经常以更高的播放速度倾听或观看这些内容的片段，以提高内容理解的时间效率。为了进一步利用这种能力，已经开发出了根据用户情况和内容类型自动调整播放速度的系统，以协助更高效地理解时间序列内容。然而，这些系统仍有进一步提高人类速听能力的空间，即生成已经针对更精细的时间单位优化过的语音，并将其提供给人类。在本研究中，我们确定人类能否听到优化过的语音，并提出了一种系统，该系统可根据音素为单位自动调整播放速度，同时确保语音可辨识度。系统使用语音识别得分作为衡量人类能否听到的代理。

    arXiv:2403.02938v1 Announce Type: new  Abstract: Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a
    
[^38]: AdAM: 适用于边缘DNN加速器的自适应容错近似乘法器

    AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators

    [https://arxiv.org/abs/2403.02936](https://arxiv.org/abs/2403.02936)

    提出了一种适用于ASIC-based DNN加速器的自适应容错近似乘法器架构。

    

    这篇论文提出了一种专为基于ASIC的DNN加速器定制的新型自适应容错近似乘法器的架构。

    arXiv:2403.02936v1 Announce Type: new  Abstract: In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators.
    
[^39]: BASS的再审视--利用统一语义图提升抽象摘要--一项复制研究

    A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study

    [https://arxiv.org/abs/2403.02930](https://arxiv.org/abs/2403.02930)

    通过复制研究BASS框架，发现了与原始工作相比性能上的差异，并强调了撰写可复制论文的关键实践。

    

    我们展示了对BASS框架的详细复制研究，这是一个基于统一语义图概念的抽象摘要系统。我们的调查包括复制关键组件时遇到的挑战，以及一个消融研究来系统地隔离在复制新颖组件时根源于错误来源。我们的发现揭示了与原始工作相比性能上的差异。我们强调了即使是被合理省略的细节对于复制像BASS这样的先进框架的重要性，并强调了撰写可复制论文的关键实践。

    arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
    
[^40]: 从光谱到生物物理学洞察：带有偏倚辐射传输模型的端对端学习

    From Spectra to Biophysical Insights: End-to-End Learning with a Biased Radiative Transfer Model

    [https://arxiv.org/abs/2403.02922](https://arxiv.org/abs/2403.02922)

    提出了将辐射传输模型集成到自动编码器架构中的端对端学习方法，不仅能够纠正RTMs中的偏倚，而且在变量提取方面表现优于传统技术。

    

    机器学习的进步推动了地球观测数据在气候变化研究中的应用。然而，机器学习表示的可解释性仍然是一个挑战，尤其是在理解森林对气候变化的生物物理反应方面。遥感中传统的逆辐射传输模型（RTMs）来从光谱数据中恢复生物物理变量的方法通常无法考虑RTM本身固有的偏倚，尤其是对于复杂的森林。我们建议将RTMs集成到自动编码器架构中，创建一个端到端学习方法。我们的方法不仅纠正了RTMs中的偏倚，而且在变量恢复方面优于神经网络回归等传统技术。此外，我们的框架在逆转偏倚的物理模型方面具有潜在的一般性。代码可在https://github.com/yihshe/ai-refined-rtm.git上获得。

    arXiv:2403.02922v1 Announce Type: new  Abstract: Advances in machine learning have boosted the use of Earth observation data for climate change research. Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change. Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests. We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach. Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression. Furthermore, our framework has potential generally for inverting biased physical models. The code is available on https://github.com/yihshe/ai-refined-rtm.git.
    
[^41]: TaylorShift：利用TaylorSoftmax将自注意力机制的复杂度从平方级转变为线性级（再转回去）

    TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax

    [https://arxiv.org/abs/2403.02920](https://arxiv.org/abs/2403.02920)

    TaylorShift通过引入TaylorSoftmax重新计算全记号之间的交互，将自注意力机制的复杂度由平方级降低到线性级，从而提高了处理长序列的效率。

    

    注意机制的二次复杂度是使用Transformer处理长序列时面临的最大障碍之一。当前的方法依赖于稀疏表示或有状态的循环，牺牲了记号之间的交互，最终导致性能上的妥协。本文介绍了TaylorShift，一种新颖的Taylor softmax 重构，能够在线性时间和空间内计算全体记号之间的交互。我们通过分析确定了使用TaylorShift比传统注意力更加高效的交叉点，这与实证测量结果密切匹配。具体来说，我们的研究结果表明，TaylorShift提高了对短至800个记号的序列的内存效率，并加速了对长达约1700个记号及以上输入的推断。对于较短的序列，TaylorShift与原始注意力的性能相当。此外，一种分类...

    arXiv:2403.02920v1 Announce Type: cross  Abstract: The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification
    
[^42]: 具有几乎与维度无关收敛速率的镜像下降算法，用于具有差分隐私的随机鞍点问题

    Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems

    [https://arxiv.org/abs/2403.02912](https://arxiv.org/abs/2403.02912)

    提出了具有差分隐私的随机鞍点问题的镜像下降算法，实现了几乎与维度无关的收敛速率，这种速率之前只针对双线性目标已知。

    

    我们研究了多面体设置中具有差分隐私（DP）的随机（凸凹）鞍点问题。我们提出了基于随机镜像下降的（ϵ，δ）-DP算法，其实现了预期对偶间隙的几乎与维度无关的收敛速率，这种保证在以前只针对双线性目标已知。对于凸凹和一阶平滑随机目标，我们的算法实现了一个率，即sqrt(log(d)/n) + (log(d)^{3/2}/[nϵ])^{1/3}，其中d是问题的维度，n是数据集大小。在额外的二阶平滑性假设下，我们将预期间隙的速率改进为sqrt(log(d)/n) + (log(d)^{3/2}/[nϵ])^{2/5}。在这种额外假设下，我们还通过使用偏差减少的梯度估计器，证明了对偶间隙受常数成功概率的界为log(d)/sqrt(n) + log(d)/[nϵ]^{1/2}。

    arXiv:2403.02912v1 Announce Type: cross  Abstract: We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\log(d)/\sqrt{n} + \log(d)/[n\varepsilon]^{1/2}$ with constant success pro
    
[^43]: 公民科学和机器学习在研究和自然保护中的应用：以欧亚猞猁、自由放养的啮齿动物和昆虫为例

    Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects

    [https://arxiv.org/abs/2403.02906](https://arxiv.org/abs/2403.02906)

    通过结合公民科学和机器学习，加速欧亚猞猁等濒危物种监测数据的准备、标记和分析过程，应用在自然研究和保护中。

    

    在全球的自然保护区和国家公园中，技术越来越多地被用于支持保护工作。濒危物种，如欧亚猞猁，通过一组自动照相陷阱进行监测。然而，这种方法产生大量数据，需要准备、分析和解释。因此，从事这一领域研究的研究人员越来越需要支持来处理这些数据。一种机会是寻求志愿公民科学家的支持，他们可以帮助标记数据，但很难保持他们的兴趣。另一种方法是利用卷积神经网络进行图像识别自动化处理。在讨论中，我们将探讨与自然研究和保护相关的考虑因素，以及利用公民科学和机器学习加快数据准备、标记和分析过程的机会。

    arXiv:2403.02906v1 Announce Type: cross  Abstract: Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.
    
[^44]: 在寻找真相：一种审问方法用于幻觉检测

    In Search of Truth: An Interrogation Approach to Hallucination Detection

    [https://arxiv.org/abs/2403.02889](https://arxiv.org/abs/2403.02889)

    提出了一种用于在大型语言模型中检测幻觉的新方法，解决了这些模型在各种现实场景中应用时遇到的关键问题，通过对多个数据集和LLMs进行广泛评估，展示了该方法的有效性。

    

    尽管大型语言模型（LLMs）取得了许多进展并且以前所未有的速度快速发展，但由于各种原因，它们对我们日常生活的各个方面的影响和整合仍然有限。一个阻碍它们广泛应用的关键因素是幻觉的发生，即LLMs创造出听起来真实但偏离事实真相的答案。在本文中，我们提出了一种新颖的方法用于检测大型语言模型中的幻觉，这解决了这些模型在各种现实场景中应用的一个关键问题。通过对多个数据集和LLMs进行广泛评估，包括Llama-2，我们研究了各种最新LLMs的幻觉水平，并展示了我们的方法在自动检测它们方面的有效性。值得注意的是，我们在一个特定实验中观察到Llama-2达到62%的幻觉水平，而我们的方法在没有依赖的情况下实现了87%的平衡准确率（B-ACC）。

    arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
    
[^45]: 重新审视置信度估计：朝向可靠的故障预测

    Revisiting Confidence Estimation: Towards Reliable Failure Prediction

    [https://arxiv.org/abs/2403.02886](https://arxiv.org/abs/2403.02886)

    大多数置信度估计方法对于检测错误分类错误是有害的，我们提出通过寻找平坦的最小值来扩大置信度间隔，从而取得了最先进的失败预测。

    

    可靠的置信度估计在许多风险敏感应用中是一个具有挑战性但基本的要求。然而，现代深度神经网络往往对于它们的错误预测过于自信，即，来自已知类别的被错误分类的样本和来自未知类别的超出分布（OOD）样本。近年来，许多置信度校准和OOD检测方法已经被开发出来。在本文中，我们发现了一个普遍存在但实际上被忽视的现象，即大多数置信度估计方法对于检测错误分类错误是有害的。我们调查了这个问题，并揭示了流行的校准和OOD检测方法通常导致更糟糕的置信度分离正确分类和错误分类的事例，从而使得难以决定是否信任一个预测。最后，我们提出通过寻找平坦的最小值来扩大置信度间隔，从而获得最先进的失败预测。

    arXiv:2403.02886v1 Announce Type: cross  Abstract: Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failu
    
[^46]: MathScale: 数学推理的指导优化尺度

    MathScale: Scaling Instruction Tuning for Mathematical Reasoning

    [https://arxiv.org/abs/2403.02884](https://arxiv.org/abs/2403.02884)

    MathScale提出了一种简单可扩展的方法来创建高质量的数学推理数据，展现出在数学数据集大小方面的有效可扩展性。

    

    大型语言模型（LLMs）在问题解决方面展现了出色的能力，但它们在解决数学问题方面的熟练程度仍然不足。我们提出了MathScale，这是一种简单且可扩展的方法，使用前沿的LLMs（例如GPT-3.5）创建高质量的数学推理数据。受人类数学学习中的认知机制启发，它首先从种子数学问题中提取主题和知识点，然后构建一个概念图，随后用于生成新的数学问题。MathScale在我们生成的数学数据集的大小方面展现出了有效的可扩展性。因此，我们创建了一个包含两百万数学问题-答案对的数学推理数据集（MathScaleQA）。为了全面评估LLMs的数学推理能力，我们构建了MwpBench，这是一个数学问题词汇问题基准，包括十个数据集。

    arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including 
    
[^47]: 通过以交通流随机化方式进行强化学习的自动驾驶车辆决策与控制

    Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization

    [https://arxiv.org/abs/2403.02882](https://arxiv.org/abs/2403.02882)

    通过在SUMO中随机化规则微观交通流的行为，利用深度强化学习算法训练自动驾驶车辆的决策策略，提高其在更真实交通场景中的性能。

    

    当前大多数关于自动驾驶车辆决策和控制任务基于强化学习的研究是在模拟环境中进行的。这些研究的训练和测试是在基于规则的微观交通流下进行的，很少考虑将它们迁移到真实或接近真实的环境中以测试它们的性能。本研究提出了一种方法，通过随机化SUMO中基于规则的微观交通流的车辆跟驰模型和换道模型的某些参数来随机化周围车辆的驾驶风格和行为。我们使用深度强化学习算法在公路和合并场景中的领域随机化规则微观交通流下训练策略，然后分别在基于规则的微观交通流和.

    arXiv:2403.02882v1 Announce Type: cross  Abstract: Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and h
    
[^48]: 关于具有指数、亚高斯和一般轻尾的高概率分析算法的注解

    A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails

    [https://arxiv.org/abs/2403.02873](https://arxiv.org/abs/2403.02873)

    这种技术可以简化分析依赖轻尾随机源的算法，通过对较简单的算法变体进行分析，避免使用专门的集中不等式，并且适用于指数、亚高斯和更一般的快速衰减分布。

    

    这篇简短的注解描述了一种分析概率算法的简单技术，该算法依赖于一个轻尾（但不一定有界）的随机化来源。我们展示了这样一个算法的分析可以通过黑盒方式减少，只在对数因子中有小量损失，转化为分析同一算法的一个更简单变体，该变体使用有界随机变量，通常更容易分析。这种方法同时适用于任何轻尾随机化，包括指数、亚高斯和更一般的快速衰减分布，而不需要调用专门的集中不等式。提供了对一般化Azuma不等式和具有一般轻尾噪声的随机优化的分析，以说明该技术。

    arXiv:2403.02873v1 Announce Type: new  Abstract: This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.
    
[^49]: 量子混合态自注意力网络

    Quantum Mixed-State Self-Attention Network

    [https://arxiv.org/abs/2403.02871](https://arxiv.org/abs/2403.02871)

    本论文介绍了一种新颖的量子混合态自注意力网络（QMSAN），结合了量子计算原理和经典机器学习算法，特别是自注意力网络，以增强处理NLP任务的效率和效果。

    

    量子计算的快速发展越来越突出了其在机器学习领域的潜力，特别是在自然语言处理（NLP）任务中。量子机器学习（QML）利用量子计算的独特能力为复杂数据处理和模式识别挑战提供新颖的视角和方法论。本文介绍了一种新颖的量子混合态注意力网络（QMSAN），它将量子计算原理与经典机器学习算法，特别是自注意力网络，相结合，以增强处理NLP任务的效率和效果。QMSAN模型采用基于混合态的量子注意力机制，实现了在量子领域内查询和键之间相似性的高效直接估计，从而实现更有效的注意力权重获取。此外，我们提出了一种创新的量子 posit

    arXiv:2403.02871v1 Announce Type: cross  Abstract: The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum posit
    
[^50]: 通过边缘/端点设备的侧信道攻击精确提取深度学习模型

    Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices

    [https://arxiv.org/abs/2403.02870](https://arxiv.org/abs/2403.02870)

    该研究揭示了对深度学习模型进行精确提取的新型攻击方法，通过边缘/端点设备的侧信道攻击可以获取模型架构和图像维度等重要信息。

    

    随着深度学习（DL）模型日益普及，模型规模越来越大，只有拥有庞大训练数据集和巨大计算能力的公司才能应对业务需求的这些大型模型。大多数DL模型是公司专有的，因此这些公司努力保护他们的私有模型，以免受到模型提取攻击（MEA）的侵害，该攻击目的是通过训练代理模型来窃取模型。如今，公司倾向于将模型从中央服务器转移到边缘/端点设备。正如最新研究揭示的那样，攻击者利用这一机会作为启动侧信道攻击（SCA）的新攻击向量，针对运行受害模型的设备发动攻击，获取模型信息的各种要点，例如模型架构（MA）和图像维度（ID）。我们的工作首次全面理解了这种关系，将有助于未来MEA研究在进攻和防御方面取得进展。

    arXiv:2403.02870v1 Announce Type: new  Abstract: With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and de
    
[^51]: 可扩展的连续时间扩散框架用于网络推断和影响估计

    Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation

    [https://arxiv.org/abs/2403.02867](https://arxiv.org/abs/2403.02867)

    将扩散过程视为连续时间动力系统，建立了可扩展且有效的框架以近似从级联中推断出基础网络结构，解决了网络推断和影响估计中存在的可扩展性问题

    

    最近几年，连续时间信息传播的研究已成为许多应用领域的重要研究领域。当只能访问传播跟踪（级联）时，基于级联的网络推断和影响估计是两个必须探讨的重要问题。然而，现有方法在推断和处理超过几千个节点的网络时表现出有限的能力，存在可扩展性问题。本文将扩散过程视为连续时间动力系统，基于此建立了一个连续时间扩散模型。随后，我们将该模型实例化为一个可扩展且有效的框架（FIM），以近似从可用级联中推断出基础网络结构的扩散传播。此外，我们对FIM在网络推断中的近似误差进行了分析。为了实现影响估计的所需可扩展性，我们设计了...

    arXiv:2403.02867v1 Announce Type: cross  Abstract: The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise 
    
[^52]: FLGuard: 通过对比模型集合实现拜占庭-鲁棒的联邦学习

    FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models

    [https://arxiv.org/abs/2403.02846](https://arxiv.org/abs/2403.02846)

    通过对比模型集合，提出了FLGuard方法增强联邦学习的拜占庭-鲁棒性。

    

    联邦学习（FL）通过仅共享本地模型的参数来训练全局模型，从而在训练具有私人训练数据集的众多客户时蓬勃发展。然而，最近的研究提出了投毒攻击，当对手假扮为良性客户并存在于一组客户中时，会导致全局模型准确性的灾难性损失。因此，最近的研究提出了拜占庭-鲁棒的FL方法，允许服务器即使在系统中存在对手也能训练准确的全局模型。然而，许多现有方法要求知道恶意客户的数量或辅助（干净）数据集，或者在私有数据集非独立同分布（non-IID）时其有效性报告明显降低。在这项工作中，我们提出

    arXiv:2403.02846v1 Announce Type: cross  Abstract: Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose
    
[^53]: SOFIM: 使用正则化Fisher信息矩阵的随机优化

    SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix

    [https://arxiv.org/abs/2403.02833](https://arxiv.org/abs/2403.02833)

    SOFIM利用正则化Fisher信息矩阵和Sherman-Morrison矩阵求逆改善了大规模随机优化中梯度更新的收敛速率，同时解决了数据异质性带来的非平稳目标问题。

    

    这篇论文介绍了一种新的基于正则化Fisher信息矩阵（FIM）的随机优化方法，称为SOFIM，可以有效利用FIM来逼近Hessian矩阵，以找到大规模随机优化机器学习模型中的牛顿梯度更新。可以视为自然梯度下降（NGD）的一种变体，通过使用正则化FIM和直接通过Sherman-Morrison矩阵求逆找到梯度更新方向来解决存储和计算完整FIM的挑战。此外，像广受欢迎的Adam方法一样，SOFIM利用梯度的第一时刻来处理由异构数据引起的跨小批次非平稳目标的问题。正则化FIM和Sherman-Morrison矩阵求逆的利用导致了收敛速率的改善，同时space和time复杂度与随机梯度下降相同。

    arXiv:2403.02833v1 Announce Type: new  Abstract: This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (
    
[^54]: 一种适应性水电管理方法用于下游生态系统保护

    An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation

    [https://arxiv.org/abs/2403.02821](https://arxiv.org/abs/2403.02821)

    提出了一种使用自适应生态排放来保护生态系统的水电管理方法，并结合神经网络和优化算法，旨在推动水电厂兼顾环境保护和能源生产。

    

    水电厂在推动清洁和可持续能源生产方面起着关键作用，对全球向可再生能源来源的过渡发挥着重要作用。然而，水电厂目前被视为既是可再生能源的来源，又是生态系统的破坏者。在这项工作中，我们强调了使用自适应生态排放作为保护生态系统的一种潜力被忽视了。为了提倡这一观点，我们提出使用神经网络在每个所需时间预测最小生态排放值。此外，我们提出了一种新颖框架，将其无缝集成到水电管理软件中，利用传统受限优化算法的成熟方法。这种新颖方法不仅可以保护生态系统免受气候变化的影响，还有可能增加电力产量。

    arXiv:2403.02821v1 Announce Type: new  Abstract: Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the elec
    
[^55]: InjectTST: 将全局信息注入独立通道的变压器方法用于长时间序列预测

    InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting

    [https://arxiv.org/abs/2403.02814](https://arxiv.org/abs/2403.02814)

    提出了InjectTST这种将全局信息注入独立通道的变压器方法，用于改进多变量时间序列（MTS）预测性能。

    

    变压器已成为多变量时间序列（MTS）预测中最流行的架构之一。最近基于Transformer的MTS模型通常倾向于具有通道独立结构，因为通道独立可以减轻噪声和分布漂移问题，从而更具鲁棒性。然而，值得注意的是，通道依赖性仍然是MTS的固有特性，蕴含着宝贵的信息。设计一个结合了通道独立和通道混合结构优点的模型是进一步改进MTS预测的关键，这带来了一个具有挑战性的难题。为了解决这个问题，在本文中提出了一种将全局信息注入通道无关Transformer的注入方法InjectTST。我们没有直接设计一个混合通道模型，而是保留了通道独立的框架，并逐渐将全局信息注入到单个通道中。

    arXiv:2403.02814v1 Announce Type: cross  Abstract: Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels
    
[^56]: 动态高斯图算子：在任意离散力学问题中学习参数化偏微分方程

    Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems

    [https://arxiv.org/abs/2403.02810](https://arxiv.org/abs/2403.02810)

    提出了一种新颖的操作学习算法，称为动态高斯图算子（DGGO），可以在任意离散力学问题中学习参数化PDEs。

    

    深度学习方法可以用于解决由参数化偏微分方程（PDEs）控制的物理系统，因为有大量科学数据可供使用。最近，已经发展了一种操作学习方法，专注于学习无限维函数空间之间的非线性映射，提供了从观测数据到解决方案的接口。然而，最先进的神经算子仅限于恒定和均匀离散化，从而导致在计算域的任意离散化方案上泛化不足。在这项工作中，我们提出了一种新颖的操作学习算法，称为动态高斯图算子（DGGO），将神经算子扩展到在任意离散力学问题中学习参数化PDEs。动态高斯图（DGG）核学习将在一般欧几里得空间中定义的观察向量映射到高维均匀度量中定义的度量向量。

    arXiv:2403.02810v1 Announce Type: cross  Abstract: Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data. It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions. However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain. In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric s
    
[^57]: 基于变分信息瓶颈的距离度量学习模型

    A Distance Metric Learning Model Based On Variational Information Bottleneck

    [https://arxiv.org/abs/2403.02794](https://arxiv.org/abs/2403.02794)

    本文首次将变分信息瓶颈与度量学习模型相结合，提出了一种新的度量学习模型 VIB-DML 用于评分预测，限制潜空间特征向量的互信息以提高模型鲁棒性并满足欧氏距离的假设。

    

    近年来，个性化推荐技术蓬勃发展，成为研究热点之一。先后提出的矩阵分解模型和度量学习模型已被广泛研究和应用。后者使用欧氏距离而非前者所使用的点积来衡量潜空间向量。本文首次将变分信息瓶颈与度量学习模型相结合，提出了一种新的度量学习模型 VIB-DML（变分信息瓶颈距离度量学习）用于评分预测，限制潜空间特征向量的互信息，提高模型的鲁棒性并满足欧氏距离的假设。

    arXiv:2403.02794v1 Announce Type: cross  Abstract: In recent years, personalized recommendation technology has flourished and become one of the hot research directions. The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied. The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector. While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited recommendation quality of the model. In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the mutual information of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean 
    
[^58]: 基于人类中心解释的半监督图表示学习用于预测脂肪肝病的研究

    Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease

    [https://arxiv.org/abs/2403.02786](https://arxiv.org/abs/2403.02786)

    该研究在半监督学习框架内探索了图表示学习的潜力，通过人类中心解释提供了个性化特征重要性得分，以增强解释性和临床相关性。

    

    在临床环境中，特别是在预测脂肪肝病方面，由于有限的标记数据，这项研究探索了在半监督学习框架内利用图表示学习的潜力。通过利用图神经网络（GNNs），我们的方法构建了一个主题相似性图，从健康检查数据中识别风险模式。我们展示了在这一背景下各种GNN方法的有效性，即使有最少的标记样本。我们方法的核心是通过可解释的GNNs包含人类中心解释，为增强的可解释性和临床相关性提供个性化特征重要性得分，从而强调了我们的方法在促进健康实践方面的潜力，重点关注图表示学习和人类中心解释。

    arXiv:2403.02786v1 Announce Type: cross  Abstract: Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of graph representation learning within a semi-supervised learning framework. Leveraging graph neural networks (GNNs), our approach constructs a subject similarity graph to identify risk patterns from health checkup data. The effectiveness of various GNN approaches in this context is demonstrated, even with minimal labeled samples. Central to our methodology is the inclusion of human-centric explanations through explainable GNNs, providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on graph representation learning and human-centric explanation.
    
[^59]: 矩阵流形上的数据协作分析

    Data Collaboration Analysis Over Matrix Manifolds

    [https://arxiv.org/abs/2403.02780](https://arxiv.org/abs/2403.02780)

    本研究讨论了在矩阵流形上的数据协作分析，探讨了如何通过隐私保护机器学习来处理多来源数据的道德和隐私问题

    

    机器学习(ML)算法的有效性与其训练数据集的质量和多样性密切相关。改进的数据集，标志着优越的质量，增强了预测的准确性，并扩展了模型在各种场景下的适用性。研究人员经常整合来自多个来源的数据，以减轻单一来源数据集的偏见和限制。然而，这种广泛的数据融合引发了重大的道德关切，特别是关于用户隐私和未经授权的数据披露风险。已建立了各种全球立法框架来解决这些隐私问题。虽然这些法规对保护隐私至关重要，但它们可能会使ML技术的实际部署变得复杂。隐私保护机器学习(PPML)通过保护从健康记录到地理位置数据等敏感信息，同时实现安全使用这些信息，来应对这一挑战。

    arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
    
[^60]: 一种用于自主导丝引导的零样本强化学习策略

    A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation

    [https://arxiv.org/abs/2403.02777](https://arxiv.org/abs/2403.02777)

    提出了一种零样本学习策略，可以在不重新训练的情况下应用于未见过的血管解剖结构。

    

    摘要：心血管疾病的治疗需要复杂且具有挑战性的导丝和导管导航。深度强化学习方法在学习这一任务方面表现出了潜力，可能是自动化导管导航在机器辅助手术中的关键。然而，现有的训练方法在泛化到未见过的血管解剖结构方面能力有限，需要每次几何结构变化时重新训练。

    arXiv:2403.02777v1 Announce Type: new  Abstract: Purpose: The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter. This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation. Deep Reinforcement Learning approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions. Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the geometry changes. Methods: In this paper, we propose a zero-shot learning strategy for three-dimensional autonomous endovascular navigation. Using a very small training set of branching patterns, our reinforcement learning algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining. Results: We demonstrate our method on 4 different vascular systems, with 
    
[^61]: EasyQuant: 一种用于LLM的高效无数据量化算法

    EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs

    [https://arxiv.org/abs/2403.02775](https://arxiv.org/abs/2403.02775)

    EasyQuant是一种无需训练的、无需数据的仅针对权重的量化算法，旨在减少量化误差并保证LLM的泛化性能。

    

    大型语言模型(LLMs)在各种任务中已被证明要比传统方法优越得多。然而，它们昂贵的计算和高内存需求使其难以部署。模型量化是减少这种开销的有效方法。然而，大多数先前的工作中，量化模型是使用少量训练数据样本进行校准的，这可能会影响前人工作里量化后的LLMs对未知情况和任务的泛化性能。因此，在这项工作中，我们探讨了一个重要问题：我们是否可以为LLM设计一种无数据量化方法以保证其泛化性能？在这项工作中，我们提出了EasyQuant，一种用于LLM的无需训练和无数据的仅针对权重的量化算法。我们的观察表明，权重和量化范围中的异常值是减少量化误差的关键因素。因此，在EasyQuant中，我们保留了这些异常值（待续）。

    arXiv:2403.02775v1 Announce Type: new  Abstract: Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (
    
[^62]: 快速、自适应尺度和具有不确定性意识的地球系统模型场降尺度与生成基础模型

    Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models

    [https://arxiv.org/abs/2403.02774](https://arxiv.org/abs/2403.02774)

    通过学习一致性模型，在不需要重新训练的情况下高效、准确地降尺度任意地球系统模型模拟，并产生概率性降尺度场。

    

    精确和高分辨率的地球系统模型(ESM)模拟对于评估人为气候变化对生态和社会经济影响至关重要，但计算成本过高。最近的机器学习方法在ESM模拟的降尺度中表现出色，优于最先进的统计方法。然而，现有方法对每个ESM都需要计算昂贵的重新训练，并且在训练期间未见过的气候预测效果差。我们通过学习一个一致性模型(CM)，以零样本方式高效准确地降尺度任意ESM模拟来解决这些缺点。我们的基础模型方法以只受观测参考数据限制的分辨率产生概率性降尺度场。我们展示了CM在维持高可控性的同时以较低的计算成本优于最先进的扩散模型。

    arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
    
[^63]: 通过有监督对比学习进行康复锻炼质量评估，结合硬负样本和软负样本

    Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives

    [https://arxiv.org/abs/2403.02772](https://arxiv.org/abs/2403.02772)

    这项研究引入了一种新的有监督对比学习框架，通过结合硬和软负样本，有效地解决了康复锻炼评估中样本稀缺的问题

    

    基于锻炼的康复计划已被证明在提高生活质量、降低死亡率和再住院率方面是有效的。利用人工智能驱动的虚拟康复，患者可以在家独立完成锻炼，利用AI算法分析锻炼数据，为患者提供反馈，并向临床医生更新他们的进展情况。这些计划通常会指定各种锻炼类型，这导致康复锻炼评估数据集面临独特挑战：虽然在整体训练样本中丰富，但这些数据集通常对每种具体锻练类型的样本数量有限。这种差异影响了现有方法训练具有小样本量的每种锻练的可泛化模型的能力。为了解决这个问题，我们的论文引入了一种新颖的带有硬负样本和软负样本的有监督对比学习框架，有效利用了整个

    arXiv:2403.02772v1 Announce Type: cross  Abstract: Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entir
    
[^64]: G4-Attention: 应用注意力机制的深度学习模型用于预测DNA G-四链体

    G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes

    [https://arxiv.org/abs/2403.02765](https://arxiv.org/abs/2403.02765)

    该论文提出了一种新的深度学习模型G4-Attention，应用注意力机制，用于预测DNA G-四链体，相较于现有算法，该模型具备更强的预测能力。

    

    G-四链体是由鸟嘌呤四聚体堆积排列形成的四链非规范核酸结构，因其独特的结构特性在生物学中扮演着重要角色。为了预测活跃的G4区域，研究人员近期开发了名为G4-seq和G4-ChIP-seq的测序技术，能够在体内和体外以数百个碱基的分辨率绘制G4s。然而，现有的机器学习方法虽然能够利用现有数据库来预测G4区域，但其预测模型较为简单。

    arXiv:2403.02765v1 Announce Type: new  Abstract: G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers. They are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics. After the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active G4s regions \textit{in vitro} based on the canonical G4 sequence elements, G-\textit{richness}, and G-\textit{skewness}, as well as the non-canonical sequence features. Recently, sequencing techniques like G4-seq and G4-ChIP-seq were developed to map the G4s \textit{in vitro}, and \textit{in vivo} respectively at a few hundred base resolution. Subsequently, several machine learning approaches were developed for predicting the G4 regions using the existing databases. However, their prediction models were simplistic, and the predictio
    
[^65]: 不需要精确指导的学习：从低分辨率历史标签更新大规模高分辨率土地覆盖图

    Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels

    [https://arxiv.org/abs/2403.02746](https://arxiv.org/abs/2403.02746)

    提出了一个名为Paraformer的弱监督框架，通过低分辨率历史土地覆盖数据指导大规模高分辨率土地覆盖映射，设计了CNN-Transformer特征提取器来综合捕获局部和全局背景信息

    

    大规模高分辨率（HR）土地覆盖映射是调查地球表面和解决人类面临的许多挑战的重要任务。然而，由于复杂的地面细节、各种地貌和广泛地理区域内准确训练标签的稀缺性，这仍然是一项非平凡的任务。在本文中，我们提出了一个高效的弱监督框架（Paraformer），即低到高网络（L2HNet）V2，用于在低分辨率（LR）的易获历史土地覆盖数据指导大规模高分辨率土地覆盖映射。具体来说，现有的土地覆盖映射方法显示了CNN在保留局部地面细节方面的优势，但仍然存在在各种地貌中全局建模不足的问题。因此，我们设计了Paraformer中的并行CNN-Transformer特征提取器，包括一个无降采样CNN分支和一个Transformer分支，来共同捕获局部和全局背景信息

    arXiv:2403.02746v1 Announce Type: cross  Abstract: Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual informatio
    
[^66]: 神经分数阶微分方程

    Neural Fractional Differential Equations

    [https://arxiv.org/abs/2403.02737](https://arxiv.org/abs/2403.02737)

    提出了神经FDE，一种新型深度神经网络架构，可调整FDE以适应数据动态，可能优于神经OD。

    

    分数阶微分方程（FDEs）是科学和工程中建模复杂系统的基本工具。 它们将传统的微分和积分概念扩展到非整数阶，使得能够更精确地表示具有非局部和记忆依赖行为特征的过程。在这个背景下，受神经常微分方程（Neural ODEs）的启发，我们提出了神经FDE，这是一种调整FDE以适应数据动态的新型深度神经网络架构。这项工作全面概述了神经FDE中采用的数值方法和神经FDE架构。数值结果表明，尽管计算要求更高，神经FDE可能优于神经OD。

    arXiv:2403.02737v1 Announce Type: new  Abstract: Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours.   This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions.   Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data.   This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural OD
    
[^67]: 用于建模受限系统的神经网络的两阶段训练方法

    A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks

    [https://arxiv.org/abs/2403.02730](https://arxiv.org/abs/2403.02730)

    描述了一种用于模拟受限系统的神经常微分方程的两阶段训练方法，在第一阶段寻找合适的神经网络参数，第二阶段找到最佳参数并保持在可行域内。

    

    现实世界中的系统经常被制定为受限制的优化问题。将约束引入神经网络（NN）的技术，如神经常微分方程（Neural ODEs），已经被使用。然而，这些方法会引入需要通过试错手动调整的超参数，这使得约束成功融入生成模型中产生疑虑。本文详细描述了神经常微分方程的两阶段训练方法，这是一种简单、有效且无需惩罚参数的方法，用于建模受限系统。在这种方法中，约束优化问题被重写为两个无约束的子问题，分两个阶段解决。第一阶段的目标是通过最小化约束违反程度来找到合适的NN参数。第二阶段的目标是通过最小化损失函数来找到最佳的NN参数，同时保持在可行域内。

    arXiv:2403.02730v1 Announce Type: new  Abstract: Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimenta
    
[^68]: Android在动物园中: GUI代理的动作思维链

    Android in the Zoo: Chain-of-Action-Thought for GUI Agents

    [https://arxiv.org/abs/2403.02713](https://arxiv.org/abs/2403.02713)

    该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。

    

    大型语言模型（LLM）导致智能手机上的大量自主GUI代理激增，这些代理通过预测API的一系列动作来完成由自然语言触发的任务。尽管该任务高度依赖于过去的动作和视觉观察，但现有研究通常很少考虑中间截图和屏幕操作传递的语义信息。为了解决这一问题，本文提出了动作思维链（CoAT），它考虑了先前动作的描述、当前屏幕，更重要的是分析应当执行的动作以及选择的动作带来的结果。我们证明，在使用现成LLM进行零次学习的情况下，CoAT相比于标准上下文建模显著提高了目标的完成情况。为了进一步促进这一研究领域的发展，我们构建了一个名为Android-In-The-Zoo（AitZ）的基准测试集，其中包含18,643个屏幕动作对。

    arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
    
[^69]: 噪声误导稀疏目标上的旋转不变算法

    Noise misleads rotation invariant algorithms on sparse targets

    [https://arxiv.org/abs/2403.02697](https://arxiv.org/abs/2403.02697)

    噪声添加后，稀疏线性问题上的旋转不变算法仍然次优，我们证明了这一点。

    

    众所周知，即使对于学习稀疏线性问题，当样本数低于问题的“维度”时，旋转不变算法也是次优的。这个类别包括任何使用全连接输入层的梯度下降训练的神经网络（初始化为旋转对称分布）。最简单的稀疏问题是学习$d$个特征中的一个特征。在这种情况下，分类误差或回归损失随着$1-k/n$增长，其中$k$是观察到的样本数。当样本数$k$达到维度$d$时，这些下界变得空泛。我们表明，当将噪声添加到这个稀疏线性问题时，即使在观察到$d$个或更多样本后，旋转不变算法仍然是次优的。我们通过针对一个旋转对称化问题的贝叶斯最优算法的一个下界证明了这一点。然后，我们证明了相同问题的更低的上界

    arXiv:2403.02697v1 Announce Type: cross  Abstract: It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the "dimension" of the problem. This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution). The simplest sparse problem is learning a single feature out of $d$ features. In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen. These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$.   We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples. We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much lower upper bounds on the same problem for 
    
[^70]: 可控提示调整用于平衡组分布鲁棒性

    Controllable Prompt Tuning For Balancing Group Distributional Robustness

    [https://arxiv.org/abs/2403.02695](https://arxiv.org/abs/2403.02695)

    引入了可控提示调整（CPT）技术，通过优化方案在不同组之间实现良好性能，避免牺牲任何一个组的性能，在虚假相关基准测试中取得了最先进的结果。

    

    在由不同组或领域组成的数据上训练的模型可能会在分布偏移下出现严重的性能下降。尽管最近的方法主要集中在优化最差组的目标上，但这往往是以牺牲其他组上的良好性能为代价的。为了解决这个问题，我们引入了一种优化方案，以实现组内良好性能，并找到一个良好的解决方案，而不会严重牺牲任何一个组的性能。然而，直接应用这种优化会涉及更新整个网络的参数，这既耗时又具有挑战性。因此，我们引入了可控提示调整（CPT），将我们的方法与提示调整技术相结合。在虚假相关基准测试中，我们的程序在变压器和非变压器架构以及单模态和多模态数据上均实现了最先进的结果，同时也需要

    arXiv:2403.02695v1 Announce Type: new  Abstract: Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring
    
[^71]: 面向大型语言模型的隐私感知语义缓存

    Privacy-Aware Semantic Cache for Large Language Models

    [https://arxiv.org/abs/2403.02694](https://arxiv.org/abs/2403.02694)

    MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。

    

    大型语言模型（LLMs）如ChatGPT、Google Bard、Claude和Llama 2彻底改变了自然语言处理和搜索引擎动态。然而，这些模型造成了异常高的计算成本。本文介绍了MeanCache，一种用于LLMs的语义缓存，它能够识别语义上相似的查询以确定缓存命中或未命中。

    arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
    
[^72]: 利用转移矩阵进行Dirichlet-based Per-Sample加权的噪声标签学习

    Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning

    [https://arxiv.org/abs/2403.02690](https://arxiv.org/abs/2403.02690)

    提出了基于Dirichlet分布的逐样本加权采样框架，提出了RENT方法来有效利用转移矩阵进行噪声标签学习。

    

    针对带有噪声标签的学习问题，转移矩阵被用来明确地建模噪声标签分布与干净标签分布之间的关系，以实现分类器或风险的统计一致性。我们提出了转移矩阵的良好利用至关重要，并提出了一种基于重新采样的新利用方法，称为RENT。具体来说，我们首先展示了当前利用方式在实现中可能存在潜在限制。作为Reweighting的延伸，我们提出了基于Dirichlet分布的逐样本加权采样（DWS）框架，并在DWS框架下比较了加权和重新采样。通过DWS的分析，我们提出了RENT，一种带有噪声转移矩阵的重新采样方法。在经验上，RENT一直胜过现有的转移矩阵方法。

    arXiv:2403.02690v1 Announce Type: new  Abstract: For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matr
    
[^73]: DOCTOR: 针对时间漂移热变化的动态芯片矫正方法，用于自我校正的光子张量加速器

    DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators

    [https://arxiv.org/abs/2403.02688](https://arxiv.org/abs/2403.02688)

    首次提出了轻量级的动态片上矫正框架DOCTOR，针对光子张量加速器中的时间漂移变化问题，实现自适应、原位准确度恢复

    

    Photonic computing作为加速计算密集型人工智能(AI)工作负载的一种有前途的解决方案已经出现，特别是在资源有限、延迟敏感的边缘计算环境中提供了无与伦比的速度和能量效率。然而，模拟光子张量加速器的部署遇到了可靠性挑战，由于硬件噪声和环境变化。虽然已经提出了脱机噪声感知训练和片上训练来增强对具有适度、静态噪声的光学神经加速器的变化容忍度，但我们观察到由于时间漂移变化导致的显著性能下降，这需要实时、原位校准机制。为了解决这些具有挑战性的可靠性问题，我们首次提出了一种轻量级的动态片上矫正框架，称为DOCTOR，提供适应性的、原位的准确度恢复，针对时间

    arXiv:2403.02688v1 Announce Type: cross  Abstract: Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally
    
[^74]: 学习推迟对人群的学习：一种元学习方法

    Learning to Defer to a Population: A Meta-Learning Approach

    [https://arxiv.org/abs/2403.02683](https://arxiv.org/abs/2403.02683)

    通过元学习，本研究提出一种学习推迟对人群的方法，该方法可以在测试时适应前所未见的专家，从而更好地面对困难决策。

    

    arXiv:2403.02683v1 公告类型：新 摘要：学习推迟（L2D）框架允许自主系统通过将困难决策委托给人类专家来保持安全和健壮。所有现有的关于L2D的工作都假设每个专家都可以很好地确定，并且如果任何专家发生变化，系统应该重新训练。在这项工作中，我们减轻了这一限制，制定了一个L2D系统，它可以在测试时应对前所未见的专家。我们通过使用元学习来实现这一点，考虑了基于优化和基于模型的变体。给定一个小的上下文集来描述当前可用的专家，我们的框架可以快速调整它的推迟策略。对于基于模型的方法，我们采用了一个注意力机制，能够寻找上下文集中与给定测试点相似的点，从而更精确地评估专家的能力。

    arXiv:2403.02683v1 Announce Type: new  Abstract: The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and s
    
[^75]: 时间编织者：一种条件时间序列生成模型

    Time Weaver: A Conditional Time Series Generation Model

    [https://arxiv.org/abs/2403.02682](https://arxiv.org/abs/2403.02682)

    引入了Time Weaver模型，利用异构元数据改善时间序列生成，并指出标准评估指标的朴素扩展是不够的。

    

    arXiv:2403.02682v1 通告类型：新的 摘要：想象根据天气、电动车存在和位置生成城市的电力需求模式，这可以用于冬季寒冻期间的容量规划。这些真实世界的时间序列常常包含配对的异构背景元数据（天气、位置等）。当前的时间序列生成方法通常忽略了这些配对的元数据，其异构性给将现有的条件生成方法从图像、音频和视频领域适应到时间序列领域中带来了几个实际挑战。为了填补这一空白，我们引入了时间编织者，这是一种基于扩散的新型模型，利用形式各异的元数据（分类、连续甚至时间变量）显著改善时间序列生成。此外，我们证明了从图像到时间序列领域的标准评估指标的朴素扩展是不够的。

    arXiv:2403.02682v1 Announce Type: new  Abstract: Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These m
    
[^76]: 使用部分Hessian的SGD优化深度神经网络

    SGD with Partial Hessian for Deep Neural Networks Optimization

    [https://arxiv.org/abs/2403.02681](https://arxiv.org/abs/2403.02681)

    提出了一种深度神经网络优化方法，结合了部分Hessian信息的二阶优化器和一阶随机梯度下降(SGD)优化器，从而提高了性能稳定性

    

    由于二阶算法在解决经典优化问题方面的有效性，设计二阶优化器来训练深度神经网络(DNNs)近年来吸引了很多研究兴趣。然而，由于DNN中间特征的非常高维度，直接计算和存储Hessian矩阵以进行网络优化是困难的。大多数先前的二阶方法对Hessian信息进行不精确近似，导致性能不稳定。在这项工作中，我们提出了一个复合优化器，它是一个将二阶优化器与用于更新通道参数的精确部分Hessian矩阵以及用于更新其他参数的一阶随机梯度下降(SGD)优化器相结合的优化器。我们证明了通道参数的相关Hessian矩阵是对角线型的，并且可以直接且精确地从无Hessian方法中提取。

    arXiv:2403.02681v1 Announce Type: new  Abstract: Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The pro
    
[^77]: 移除平方根：一种新的高效标度不变版本的AdaGrad

    Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad

    [https://arxiv.org/abs/2403.02648](https://arxiv.org/abs/2403.02648)

    KATE是一种新的优化算法，提出了一种与AdaGrad标度不变的适应方法，并在广义线性模型和一般的非凸问题中证明了其标度不变性。数值实验结果表明，KATE在各种场景中均优于AdaGrad并与Adam性能匹配/超越。

    

    自适应方法在机器学习中非常流行，因为它们可以降低学习速率调整的成本。本文引入了一种名为KATE的新型优化算法，它提出了一个著名的AdaGrad算法的标度不变适应。我们证明了KATE在广义线性模型案例中的标度不变性。此外，对于一般的光滑非凸问题，我们为KATE建立了一个收敛速率为$O \left(\frac{\log T}{\sqrt{T}} \right)$，与AdaGrad和Adam的最佳收敛速率相匹配。我们还通过不同问题的数值实验将KATE与其他最先进的自适应算法Adam和AdaGrad进行了比较，包括在真实数据上进行图像分类和文本分类等复杂机器学习任务。结果表明，在所有考虑到的场景中，KATE始终胜过AdaGrad，并且在性能上匹配/超越Adam。

    arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
    
[^78]: 在5G RF领域，用于干扰检测的空中双阈值深度学习器

    Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain

    [https://arxiv.org/abs/2403.02645](https://arxiv.org/abs/2403.02645)

    本文提出了一种在5G网络中检测干扰者的新型深度学习技术，通过引入双阈值深度学习干扰检测器，专注于SSB的RF领域特征，提高了网络的鲁棒性。

    

    随着5G无线通信的发展，同步信号块（SSB）在设备同步和服务可访问性中起着关键作用。然而，由于SSB传输具有可预测性，包括主要同步信号（PSS）和次要同步信号（SSS），干扰攻击是重要威胁。本文利用RF领域知识，提出了一种新颖的基于深度学习的5G网络干扰检测技术。与现有的大多依赖网络参数的干扰检测算法不同，我们通过专注于SSB引入了双阈值深度学习干扰检测器。该检测方法侧重于RF领域特征，提高了网络的鲁棒性，无需与现有网络基础设施集成。通过集成一个预处理块来提取PSS相关性和每个空闲资源元素的能量（EPNRE）

    arXiv:2403.02645v1 Announce Type: cross  Abstract: With the evolution of 5G wireless communications, the Synchronization Signal Block (SSB) plays a critical role in the synchronization of devices and accessibility of services. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. By leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in 5G networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block that extracts PSS correlation and energy per null resource elements (EPNRE) characte
    
[^79]: 基于虚假阳性采样的数据增强方法提高3D物体检测准确性

    False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy

    [https://arxiv.org/abs/2403.02639](https://arxiv.org/abs/2403.02639)

    本研究提出了一种名为虚假阳性采样的新增强技术，通过重新训练模型使用被识别为虚假阳性的点云，以提高3D物体检测模型的性能。

    

    近期的研究集中在提升3D物体检测模型的性能上。在各种方法中，地面真实数据采样被提出作为一种增强技术来解决有限的地面真实数据带来的挑战。然而，地面真实数据采样的一个固有问题是其倾向增加虚假阳性。因此，本研究旨在克服地面真实数据采样的局限，并通过开发一种名为虚假阳性采样的新增强技术来提高3D物体检测模型的性能。虚假阳性采样涉及重新训练模型，使用在模型预测中被识别为虚假阳性的点云。我们提出了一个同时利用地面真实数据和虚假阳性采样的算法，以及一个建立虚假阳性样本数据库的算法。此外，我们分析了由于虚假阳性采样导致的性能提升背后的原理。

    arXiv:2403.02639v1 Announce Type: cross  Abstract: Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampl
    
[^80]: FedHCDR: 具有超图信号解耦的联邦跨领域推荐

    FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling

    [https://arxiv.org/abs/2403.02630](https://arxiv.org/abs/2403.02630)

    该研究提出了FedHCDR框架，通过超图信号解耦的方式解决了联邦跨领域推荐中不同领域数据异质性的问题。

    

    近年来，跨领域推荐（CDR）备受关注，利用来自多个领域的用户数据来增强推荐性能。然而，当前的CDR方法需要跨领域共享用户数据，违反了《通用数据保护条例》（GDPR）。因此，已提出了许多联邦跨领域推荐（FedCDR）方法。然而，不同领域间的数据异质性不可避免地影响了联邦学习的整体性能。在这项研究中，我们提出了FedHCDR，一种具有超图信号解耦的新型联邦跨领域推荐框架。具体地，为了解决不同领域之间的数据异质性，我们引入一种称为超图信号解耦（HSD）的方法，将用户特征解耦为领域独有和领域共享特征。该方法采用高通和低通超图滤波器来进行解耦。

    arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
    
[^81]: 交互式持续学习: 快速与缓慢思考

    Interactive Continual Learning: Fast and Slow Thinking

    [https://arxiv.org/abs/2403.02628](https://arxiv.org/abs/2403.02628)

    本文提出了一种基于交互的持续学习框架，通过多模型之间的合作交互，实现了更好的任务推导和内存检索。

    

    高级生命形式通过神经认知机制的协同互动，终身不断地获取和传递知识。与此相反，当代机器学习范式在模拟持续学习的方面存在局限性。然而，大型语言模型的出现为通过与这些模型的交互实现持续学习提供了希望。本文基于补充学习系统理论，提出了一种新颖的交互式持续学习（ICL）框架，通过各种规模模型之间的协作交互实现。具体而言，我们将ViT模型指定为第一系统，将多模态LLM指定为第二系统。为了使内存模块能够从类信息中推导任务并增强Set2Set检索，我们提出了Class-Knowledge-Task Multi-Head Attention (CKT-MHA)。此外，为了通过增强的几何检索改进第一系统的内存检索

    arXiv:2403.02628v1 Announce Type: cross  Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric r
    
[^82]: 利用LLM工具实现主观视觉分类的协作建模:减少人力投入

    Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use

    [https://arxiv.org/abs/2403.02626](https://arxiv.org/abs/2403.02626)

    提出了一个新的框架，利用自然语言交互取代人工标注，将定义视觉概念所需的人力投入减少了一个数量级

    

    从内容审核到野生动物保护，需要模型识别微妙或主观视觉概念的应用数量正在增加。传统上，为这类概念开发分类器需要大量手动工作，需要以时、天甚至月来测量识别和注释训练所需数据。即使使用最近提出的敏捷建模技术，可以快速引导图像分类器的训练，用户仍需要花费30分钟甚至更多的单调重复数据标注时间来训练单个分类器。借鉴菲斯克的认知懒汉理论，我们提出了一个新框架，通过用自然语言交互取代人工标注，减少定义概念所需的总体投入一个数量级：从标记2,000张图像到仅需100张图像再加上一些自然语言交互。我们的框架利用了最近在f

    arXiv:2403.02626v1 Announce Type: cross  Abstract: From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in f
    
[^83]: Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects

    Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects

    [https://arxiv.org/abs/2403.02624](https://arxiv.org/abs/2403.02624)

    该论文提出了帕累托最优估计和策略学习的方法，用于确定如何在短期和长期治疗效果之间进行权衡从而实现最佳治疗。

    

    这篇论文专注于发展帕累托最优估计和策略学习，以确定最有效的治疗方法，从而最大化来自短期和长期效果的总奖励，这可能会相互冲突。 例如，药物剂量的增加可能会提高患者康复速度（短期），但也可能导致严重的长期副作用。虽然最近的研究已经探讨了有关短期或长期效应或两者的问题，但如何在它们之间取得平衡以实现最佳治疗仍然是一个悬而未决的挑战。此外，当使用传统因果表示学习直接估计多个目标时，各种任务之间的优化方向也可能发生冲突。在这篇论文中，我们系统地研究了这些问题，并引入了一个帕累托有效算法，包括帕累托最优估计（POE）和帕累托最优策略学习（POPL）。

    arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
    
[^84]: 自主驾驶的世界模型：一项初步调查

    World Models for Autonomous Driving: An Initial Survey

    [https://arxiv.org/abs/2403.02622](https://arxiv.org/abs/2403.02622)

    世界模型在自主驾驶领域的重要性和作用，是通过准确预测未来事件和评估其影响来帮助决策过程，从而推动自主驾驶技术发展的革命性方法。

    

    在自主驾驶领域不断发展的背景下，准确预测未来事件并评估其影响对于安全和效率至关重要，关键地帮助决策过程。世界模型已经成为一种革命性方法，使自主驾驶系统能够综合和解释大量传感器数据，从而预测潜在的未来情景并弥补信息缺口。本文对自主驾驶中世界模型的当前状态和未来发展进行了初步审查，涵盖了其理论基础、实际应用以及旨在克服现有限制的正在进行的研究工作。强调了世界模型在推动自主驾驶技术发展中的重要作用，本调查旨在成为研究社区的基础参考，便于快速获得和应用。

    arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
    
[^85]: 在边缘进行机器学习模型训练：一项调研

    Training Machine Learning models at the Edge: A Survey

    [https://arxiv.org/abs/2403.02619](https://arxiv.org/abs/2403.02619)

    这项调研深入探讨了边缘学习(EL)中优化机器学习模型训练的各种方法和方法论，旨在综合现有知识，识别挑战，并突出未来趋势。

    

    边缘计算(EC)近年来获得了显著关注，通过在边缘集成人工智能(AI)能力，承诺提高效率。虽然主要关注点在边缘部署和推断机器学习(ML)模型，但训练方面仍然较少被探讨。这项调研深入探讨了边缘学习(EL)，特别是在边缘优化ML模型训练的方面。其目标是全面探讨EL中的不同方法和方法论，综合现有知识，识别挑战，并突出未来趋势。利用Scopus的高级搜索，确定了关于EL的相关文献，显示了研究工作在分布式学习方法方面的聚焦，特别是联邦学习(FL)。此调研还提供了一个比较用于优化边缘学习的ML的技术的指南，以及对不同框架的探索。

    arXiv:2403.02619v1 Announce Type: new  Abstract: Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, 
    
[^86]: 工业物理系统细粒度自适应异常诊断的无监督时空状态估计

    Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems

    [https://arxiv.org/abs/2403.02616](https://arxiv.org/abs/2403.02616)

    提出了一种细粒度自适应异常诊断方法（MAD-Transformer），通过构建时间状态矩阵和空间状态矩阵来揭示工业CPS工作状态的时空关联关系和演变机制

    

    精确检测和诊断异常行为，如多变量时间序列（MTS）中的网络攻击，对于确保工业物理系统（CPS）的稳定有效运行至关重要。然而，现有研究很少关注系统工作状态之间的逻辑依赖关系，并且难以解释异常信号的演变机制。为了揭示工业CPS工作状态的时空关联关系和演变机制，本文提出了一种细粒度自适应异常诊断方法（即MAD-Transformer），用于识别和诊断MTS中的异常。

    arXiv:2403.02616v1 Announce Type: cross  Abstract: Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS). However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals. To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension. Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation rel
    
[^87]: 个性化电子商务查询自动补全的搜索意图网络

    Search Intenion Network for Personalized Query Auto-Completion in E-Commerce

    [https://arxiv.org/abs/2403.02609](https://arxiv.org/abs/2403.02609)

    个性化搜索意图网络解决了电子商务中查询自动补全系统面临的意图模糊性和意图转移问题。

    

    查询自动补全（QAC）作为现代搜索引擎的重要组成部分，在补充用户查询并帮助他们细化搜索意图方面发挥关键作用。然而，目前实际场景中的QAC系统面临两个主要挑战：1）意图模糊性（IE）：在用户输入过程中，前缀通常包含字符和子词的组合，这使得当前意图模糊且难以建模。2）意图转移（IT）：之前的工作基于用户的历史序列提供建议，但忽略了搜索意图的转移。然而，从前缀提取的当前意图可能与历史偏好相悖。

    arXiv:2403.02609v1 Announce Type: cross  Abstract: Query Auto-Completion(QAC), as an important part of the modern search engine, plays a key role in complementing user queries and helping them refine their search intentions.Today's QAC systems in real-world scenarios face two major challenges:1)intention equivocality(IE): during the user's typing process,the prefix often contains a combination of characters and subwords, which makes the current intention ambiguous and difficult to model.2)intention transfer (IT):previous works make personalized recommendations based on users' historical sequences, but ignore the search intention transfer.However, the current intention extracted from prefix may be contrary to the historical preferences.
    
[^88]: DNNLasso：面向矩阵变量数据的可扩展图学习

    DNNLasso: Scalable Graph Learning for Matrix-Variate Data

    [https://arxiv.org/abs/2403.02608](https://arxiv.org/abs/2403.02608)

    DNNLasso是一种对角非负图形套索模型，用于估计Kronecker-和结构的精度矩阵，在准确性和计算时间方面优于现有方法。

    

    我们考虑了联合学习矩阵变量观测的行和列之间的依赖关系的问题，这些观测分别由两个精度矩阵单独建模。由于常用矩阵变量高斯图模型中Kronecker乘积精度矩阵的复杂结构，最近提出了一个更稀疏的Kronecker和结构，基于图的笛卡尔乘积。然而，现有的用于估计Kronecker和结构精度矩阵的方法在大规模数据集上的扩展性并不好。本文介绍了DNNLasso，一种用于估计Kronecker和结构精度矩阵的对角非负图形套索模型，它在准确性和计算时间方面都大幅优于最先进的方法。我们的代码可在https://github.com/YangjingZhang/DNNLasso找到。

    arXiv:2403.02608v1 Announce Type: new  Abstract: We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices. Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of graphs. However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets. In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time. Our code is available at https://github.com/YangjingZhang/DNNLasso.
    
[^89]: TESTAM: 一种具有时间增强的时空注意力模型与专家混合模型

    TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts

    [https://arxiv.org/abs/2403.02600](https://arxiv.org/abs/2403.02600)

    TESTAM模型通过引入混合的专家模型，并且针对时间建模、静态图的时空建模以及动态图的动态时空依赖建模，能够更好地捕捉交通数据的各种模式和情况。

    

    准确的交通预测因对路网的复杂依赖、各种类型的道路以及由事件造成的速度突变而具有挑战性。最近的工作主要集中在具有自适应图嵌入或图注意力的动态空间建模，对于时间特性和就地建模的考虑较少。在本文中，我们提出了一种名为TESTAM的新型深度学习模型，通过三个专家模型（分别处理时间建模、具有静态图的时空建模和具有动态图的动态时空依赖建模）分别对周期性和非周期性交通模式进行建模。通过引入不同的专家并适当路由它们，TESTAM能更好地模拟各种情况，包括空间隔离节点、高度相关节点以及循环和非循环事件。为了进行适当的路由，我们将一个门控问题重新表述为一个分类问题。

    arXiv:2403.02600v1 Announce Type: new  Abstract: Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification p
    
[^90]: 具有多个协变量转移和不平衡的图像数据集聚合

    Pooling Image Datasets With Multiple Covariate Shift and Imbalance

    [https://arxiv.org/abs/2403.02598](https://arxiv.org/abs/2403.02598)

    本文从范畴论的角度提供了一个简单而有效的解决方案，完全避免了复杂的多阶段训练流程。

    

    许多学科中常见小样本大小，这需要跨多个机构汇总大致相似的数据集来研究图像与疾病结果之间的弱但相关关联。这些数据通常体现出协变量（即次要的非成像数据）的转移/不平衡。在标准统计分析中控制这些无用变量是常见的，但这些思想并不直接适用于参数过多的模型。因此，最近的工作表明，从不变表示学习中提供了一个有意义的起点，但目前的方法库仅限于一次考虑几个协变量的转移/不平衡。本文展示了如何从范畴论的角度看待这一问题，提供了一个简单而有效的解决方案，完全避免了原本需要复杂的多阶段训练流程。我们展示了该方法的效果。

    arXiv:2403.02598v1 Announce Type: new  Abstract: Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effect
    
[^91]: 从反转CLIP模型中我们学到了什么？

    What do we learn from inverting CLIP models?

    [https://arxiv.org/abs/2403.02580](https://arxiv.org/abs/2403.02580)

    通过反转CLIP模型，研究发现生成的图像与指定目标提示语义对齐，揭示了CLIP模型的混合概念能力、性别偏见以及可能出现的不安全内容图像。

    

    我们采用一个基于反转的方法来检验CLIP模型。我们的检验揭示，反转CLIP模型会生成展现与指定目标提示语义对齐的图像。我们利用这些反转图像来深入了解CLIP模型的各个方面，比如它们混合概念的能力和包含性别偏见。我们特别观察到在模型反转过程中出现不安全内容（NSFW）的图像实例。这种现象甚至发生在语义上无害的提示，比如“美丽的风景”，以及涉及名人姓名的提示中。

    arXiv:2403.02580v1 Announce Type: cross  Abstract: We employ an inversion-based approach to examine CLIP models. Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target prompts. We leverage these inverted images to gain insights into various aspects of CLIP models, such as their ability to blend concepts and inclusion of gender biases. We notably observe instances of NSFW (Not Safe For Work) images during model inversion. This phenomenon occurs even for semantically innocuous prompts, like "a beautiful landscape," as well as for prompts involving the names of celebrities.
    
[^92]: 信号传播的几何动力学预测transformers的可训练性

    Geometric Dynamics of Signal Propagation Predict Trainability of Transformers

    [https://arxiv.org/abs/2403.02579](https://arxiv.org/abs/2403.02579)

    研究探索了深度transformers中信号传播和梯度反向传播的动力学特性，提出了初始化超参数条件以确保训练的可行性，并发现了在有MLP层的情况下粒子几何的定量演化规律，揭示了初始化函数的有序-混沌相变。

    

    我们研究了深度、随机初始化的transformers中前向信号传播和梯度反向传播，得出了初始化超参数的简单必要和充分条件，确保深度transformers的可训练性。我们的方法将$n$个符号的表示随着它们通过transformer层传播视为$n$个相互作用粒子的离散时间动力系统。我们推导出这个粒子系统演化几何的简单更新方程，从一个置换对称单纯形开始。我们的更新方程表明，在没有MLP层的情况下，该系统将会坍缩成为一条线，与以前在transformers中秩坍缩的相关工作相一致。然而，不同于以往的研究，我们的演化方程能够定量跟踪粒子几何，包括非线性MLP层的额外存在，并且它揭示了作为初始化函数的有序-混沌相变。

    arXiv:2403.02579v1 Announce Type: cross  Abstract: We investigate forward signal propagation and gradient back propagation in deep, randomly initialized transformers, yielding simple necessary and sufficient conditions on initialization hyperparameters that ensure trainability of deep transformers. Our approach treats the evolution of the representations of $n$ tokens as they propagate through the transformer layers in terms of a discrete time dynamical system of $n$ interacting particles. We derive simple update equations for the evolving geometry of this particle system, starting from a permutation symmetric simplex. Our update equations show that without MLP layers, this system will collapse to a line, consistent with prior work on rank collapse in transformers. However, unlike prior work, our evolution equations can quantitatively track particle geometry in the additional presence of nonlinear MLP layers, and it reveals an order-chaos phase transition as a function of initializatio
    
[^93]: AceMap：通过学术图谱进行知识发现

    AceMap: Knowledge Discovery through Academic Graph

    [https://arxiv.org/abs/2403.02576](https://arxiv.org/abs/2403.02576)

    AceMap是一个面向知识发现的学术系统，通过构建全面的数据库和运用创新的可视化、量化和分析方法，解决了科学文献管理与价值提取的挑战。

    

    科学文献的指数增长需要有效管理和提取有价值的见解。尽管现有的科学搜索引擎在基于关系数据库提供搜索结果方面表现出色，但它们经常忽略了科学实体之间的合作以及思想演化的分析，以及对科学出版物内容的深入分析。异质图的表示以及这种图的有效测量、分析和挖掘带来了重大挑战。为了解决这些挑战，我们提出了AceMap，一个旨在通过学术图谱进行知识发现的学术系统。我们提出了先进的数据库构建技术，以构建包含丰富视觉、文本和数值信息的大规模学术出版物的全面AceMap数据库。AceMap还采用了创新的可视化、量化和分析方法

    arXiv:2403.02576v1 Announce Type: cross  Abstract: The exponential growth of scientific literature requires effective management and extraction of valuable insights. While existing scientific search engines excel at delivering search results based on relational databases, they often neglect the analysis of collaborations between scientific entities and the evolution of ideas, as well as the in-depth analysis of content within scientific publications. The representation of heterogeneous graphs and the effective measurement, analysis, and mining of such graphs pose significant challenges. To address these challenges, we present AceMap, an academic system designed for knowledge discovery through academic graph. We present advanced database construction techniques to build the comprehensive AceMap database with large-scale academic publications that contain rich visual, textual, and numerical information. AceMap also employs innovative visualization, quantification, and analysis methods to
    
[^94]: 学习增强的在线最小化信息时代和传输成本

    Learning-augmented Online Minimization of Age of Information and Transmission Costs

    [https://arxiv.org/abs/2403.02573](https://arxiv.org/abs/2403.02573)

    该论文提出了一种学习增强的在线算法，用于最小化传输和陈旧成本的总和，在保证最坏情况下性能的同时，兼顾典型情况下性能。

    

    我们考虑一个离散时间系统，一个资源受限的来源（例如，一个小型传感器）通过一个时变无线信道将其及时数据传输给目的地。每次传输会产生固定的传输成本（例如，能量成本），而没有传输会导致一个以信息时代表示的陈旧成本。来源必须在传输成本和陈旧成本之间取得平衡。为了解决这一挑战，我们开发了一个强大的在线算法，以最小化传输和陈旧成本的总和，确保最坏情况下的性能保证。尽管在线算法是稳健的，但它们通常过于保守，在典型情况下可能表现不佳。相反，通过利用历史数据和预测模型，机器学习（ML）算法在平均情况下表现良好。但是，它们通常缺乏最坏情况下的性能保证。为了实现最好的两种情况，我们提出了一种学习增强的在线最小化信息时代和传输成本的方法。

    arXiv:2403.02573v1 Announce Type: new  Abstract: We consider a discrete-time system where a resource-constrained source (e.g., a small sensor) transmits its time-sensitive data to a destination over a time-varying wireless channel. Each transmission incurs a fixed transmission cost (e.g., energy cost), and no transmission results in a staleness cost represented by the Age-of-Information. The source must balance the tradeoff between transmission and staleness costs. To address this challenge, we develop a robust online algorithm to minimize the sum of transmission and staleness costs, ensuring a worst-case performance guarantee. While online algorithms are robust, they are usually overly conservative and may have a poor average performance in typical scenarios. In contrast, by leveraging historical data and prediction models, machine learning (ML) algorithms perform well in average cases. However, they typically lack worst-case performance guarantees. To achieve the best of both worlds,
    
[^95]: DPAdapter: 通过噪声容忍预训练改进差分隐私深度学习

    DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training

    [https://arxiv.org/abs/2403.02571](https://arxiv.org/abs/2403.02571)

    DPAdapter通过噪声容忍预训练的方法，旨在增强差分隐私深度学习模型性能，有效解决了DP引入的模型性能降低挑战。

    

    近期的研究强调了差分隐私在保护个人数据训练机器学习模型中的关键作用。然而，集成差分隐私通常会导致模型性能严重降低，这是差分隐私机器学习领域面临的重要挑战。为此，提出了几项缓解措施，通常围绕制定新的差分隐私机器学习算法或放宽差分隐私定义以适应不同情境。尽管有这些努力，差分隐私对模型的减弱，特别是大规模模型，仍然很大，因此需要一种创新性解决方案，巧妙地规避模型效用的重要损害。为此，我们引入了DPAdapter，这是一个旨在提升DPML模型性能的开创性技术。

    arXiv:2403.02571v1 Announce Type: new  Abstract: Recent developments have underscored the critical role of \textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML al
    
[^96]: Wukong: 迈向大规模推荐的标度律

    Wukong: Towards a Scaling Law for Large-Scale Recommendation

    [https://arxiv.org/abs/2403.02545](https://arxiv.org/abs/2403.02545)

    Wukong通过堆叠因子分解机和协同增长策略，在推荐领域建立了一个标度律，并在质量上优于现有模型。

    

    缩放定律在提高模型质量方面起着关键作用。然而，迄今为止的推荐模型并没有展现出类似于大型语言模型领域观察到的定律，这是由于它们的升级机制的低效性。本文提出了一种基于纯堆叠因子分解机和协同增长策略的有效网络架构，统称为Wukong，以在推荐领域建立一个标度律。Wukong的独特设计使其能够通过更高更宽的层次简单捕获各种任意阶的交互。我们在六个公共数据集上进行了广泛评估，结果表明，与最先进的模型相比，Wukong在质量方面始终表现优越。此外，我们评估了Wuko

    arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
    
[^97]: 使用深度学习在非对比度钙评分CT图像中进行冠状动脉分割

    Coronary artery segmentation in non-contrast calcium scoring CT images using deep learning

    [https://arxiv.org/abs/2403.02544](https://arxiv.org/abs/2403.02544)

    该论文介绍了一种利用深度学习算法在非对比度心脏CT图像中进行冠状动脉分割的方法，通过半自动生成Ground Truth的新框架。

    

    在计算机断层扫描（CT）图像中精确定位冠状动脉对于评估冠状动脉疾病至关重要。本文介绍了一种深度学习算法，用于在多供应商的心电图门控非对比度心脏CT图像中分割冠状动脉，通过图像配准实现半自动生成Ground Truth（GT）的新框架。

    arXiv:2403.02544v1 Announce Type: cross  Abstract: Precise localization of coronary arteries in Computed Tomography (CT) scans is critical from the perspective of medical assessment of coronary artery disease. Although various methods exist that offer high-quality segmentation of coronary arteries in cardiac contrast-enhanced CT scans, the potential of less invasive, non-contrast CT in this area is still not fully exploited. Since such fine anatomical structures are hardly visible in this type of medical images, the existing methods are characterized by high recall and low precision, and are used mainly for filtering of atherosclerotic plaques in the context of calcium scoring. In this paper, we address this research gap and introduce a deep learning algorithm for segmenting coronary arteries in multi-vendor ECG-gated non-contrast cardiac CT images which benefits from a novel framework for semi-automatic generation of Ground Truth (GT) via image registration. We hypothesize that the pr
    
[^98]: 利用可解释的机器学习预测太阳活动周期23和24期间的SEP事件

    Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning

    [https://arxiv.org/abs/2403.02536](https://arxiv.org/abs/2403.02536)

    利用新数据集结合机器学习策略，预测太阳活动周期23和24期间的SEP事件，以减少对航空、太空电子设备和太空探索的辐射危害。

    

    预测太阳高能粒子(SEP)事件在太空任务逐渐扩展至地球保护磁层之外的情况下引起了越来越大的兴趣。这些事件大多是在太阳耀斑或快速日冕物质抛射驱动的冲击波期间产生的磁重连驱动过程的产物，对航空、太空电子设备以及尤其是太空探索构成了重要的辐射危害。在这项工作中，我们利用最近开发的数据集，结合了太阳动力学观测卫星/表面波和磁场成像仪(SDO/HMI)的空间天气HMI活跃区域补丁(SHARP)与太阳和日球联合观测卫星/米歇森多普勒成像仪(SoHO/MDI)的空间天气MDI活跃区域补丁(SMARPs)。我们采用一系列机器学习策略，包括支持向量机(SVM)和回归模型，评估这一新数据产品的预测潜力，用于预测太阳后期的空间天气活动。

    arXiv:2403.02536v1 Announce Type: cross  Abstract: Prediction of the Solar Energetic Particle (SEP) events garner increasing interest as space missions extend beyond Earth's protective magnetosphere. These events, which are, in most cases, products of magnetic reconnection-driven processes during solar flares or fast coronal-mass-ejection-driven shock waves, pose significant radiation hazards to aviation, space-based electronics, and particularly, space exploration. In this work, we utilize the recently developed dataset that combines the Solar Dynamics Observatory/Helioseismic and Magnetic Imager's (SDO/HMI) Space weather HMI Active Region Patches (SHARP) and the Solar and Heliospheric Observatory/Michelson Doppler Imager's (SoHO/MDI) Space Weather MDI Active Region Patches (SMARP). We employ a suite of machine learning strategies, including Support Vector Machines (SVM) and regression models, to evaluate the predictive potential of this new data product for a forecast of post-solar f
    
[^99]: 迈向基础时间序列模型：合成还是不合成？

    Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?

    [https://arxiv.org/abs/2403.02534](https://arxiv.org/abs/2403.02534)

    通常需要对大量时间序列进行预测，但可能无法为每个序列单独训练模型，提出了建立基础时间序列模型的解决方案，并探讨使用合成数据作为训练集的优势。

    

    产业中经常有需要同时对大量时间序列进行预测的情况。然而，我们可能会遇到无法为每个时间序列训练单独模型的情况。时间序列建模中这一问题一直未得到充分重视。针对这种情况的解决办法是建立一个基础模型。这种模型预计能在零样本和少样本情况下工作。然而，我们应该将什么作为这种模型的训练数据集呢？从NLP数据集的扩充中获益，我们可能希望借鉴他们的经验用于时间序列。与自然语言不同，合成时间序列数据生成过程更为有利，因为它提供了全面控制系列模式、时间跨度和样本数量的可能性。

    arXiv:2403.02534v1 Announce Type: new  Abstract: The industry is rich in cases when we are required to make forecasting for large amounts of time series at once. However, we might be in a situation where we can not afford to train a separate model for each of them. Such issue in time series modeling remains without due attention. The remedy for this setting is the establishment of a foundation model. Such a model is expected to work in zero-shot and few-shot regimes. However, what should we take as a training dataset for such kind of model?   Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series. In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples. In this work, we consider the essential question if it is advantageous to train a foundation mode
    
[^100]: 基于密度的等距映射

    Density-based Isometric Mapping

    [https://arxiv.org/abs/2403.02531](https://arxiv.org/abs/2403.02531)

    该论文提出了一种基于密度的等距映射方法，通过修改最短路径算法，引入了受Parzen-Rosenblatt (PR)窗口启发的新约束，有助于处理高维数据中的不均匀性，使得构建的Isomap中最短路径图具有更好的均匀性。

    

    同位映射方法利用最短路径算法来估计高维流形上点的欧氏距离。这可能不足以处理弱均匀的高维数据，因为它可能导致估计远处相邻点之间的距离过高，在投影过程中导致本征（局部）和外显（全局）距离之间的不一致。为解决此问题，我们通过为最短路径算法添加了受 Parzen-Rosenblatt（PR）窗口启发的新约束来修改该算法，有助于在等距映射中保持构建的最短路径图的均匀性。我们使用了总共72,236个案例的多个成像数据集，其中包括70,000个MINST数据，来自多个胸部X射线肺炎数据集的1596个数据，以及三个NSCLC CT/PET数据集中共640名肺癌患者的数据，用于基准测试和验证PR-Isomap。每种模态从中提取了431个成像生物标志物。我们的结果表明P

    arXiv:2403.02531v1 Announce Type: new  Abstract: The isometric mapping method employs the shortest path algorithm to estimate the Euclidean distance between points on High dimensional (HD) manifolds. This may not be sufficient for weakly uniformed HD data as it could lead to overestimating distances between far neighboring points, resulting in inconsistencies between the intrinsic (local) and extrinsic (global) distances during the projection. To address this issue, we modify the shortest path algorithm by adding a novel constraint inspired by the Parzen-Rosenblatt (PR) window, which helps to maintain the uniformity of the constructed shortest-path graph in Isomap. Multiple imaging datasets overall of 72,236 cases, 70,000 MINST data, 1596 from multiple Chest-XRay pneumonia datasets, and three NSCLC CT/PET datasets with a total of 640 lung cancer patients, were used to benchmark and validate PR-Isomap. 431 imaging biomarkers were extracted from each modality. Our results indicate that P
    
[^101]: 在装配再生核希尔伯特空间中具有内在可观测性的Koopman算子

    Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces

    [https://arxiv.org/abs/2403.02524](https://arxiv.org/abs/2403.02524)

    本文提出了一种基于装配再生核希尔伯特空间内在结构和jets几何概念的估计Koopman算子的新方法JetDMD，通过明确的误差界和收敛率证明其优越性，为Koopman算子的数值估计提供了更精确的方法，同时在装配希尔伯特空间框架内提出了扩展Koopman算子的概念，有助于深入理解估计的Koopman特征函数。

    

    本文提出了一种新颖的方法，用于估计装配再生核希尔伯特空间（RKHS）上定义的Koopman算子及其谱。我们提出了一种估计方法，称为Jet Dynamic Mode Decomposition（JetDMD），利用RKHS的内在结构和称为jets的几何概念来增强Koopman算子的估计。该方法在精确度上优化了传统的扩展动态模态分解（EDMD），特别是在特征值的数值估计方面。本文通过明确的误差界和特殊正定内核的收敛率证明了JetDMD的优越性，为其性能提供了坚实的理论基础。我们还深入探讨了Koopman算子的谱分析，在装配希尔伯特空间框架内提出了扩展Koopman算子的概念。这个概念有助于更深入地理解估计的Koopman特征函数并捕捉

    arXiv:2403.02524v1 Announce Type: cross  Abstract: This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and captu
    
[^102]: 健康声学表示：HeAR

    HeAR -- Health Acoustic Representations

    [https://arxiv.org/abs/2403.02522](https://arxiv.org/abs/2403.02522)

    HeAR是一个基于自监督学习的深度学习系统，通过大规模数据集训练，在33个健康声学任务上表现优越，有望推动健康声学研究的发展。

    

    健康声学声音，如咳嗽和呼吸声，已知包含有用的健康信号，具有监测健康和疾病的重要潜力，但在医疗机器学习社区中尚未得到充分探讨。现有的健康声学深度学习系统通常只针对单一任务进行狭窄训练和评估，这受到数据限制，可能限制了对其他任务的泛化能力。为了弥合这些差距，我们开发了HeAR，这是一个可伸缩的基于自监督学习的深度学习系统，使用了一个包含3.13亿个两秒长音频剪辑的大型数据集来训练掩码自编码器。通过线性探测，我们将HeAR确立为在六个数据集上的33个健康声学任务基准上的最先进的健康音频嵌入模型。通过引入这项工作，我们希望能够启用和加速进一步的健康声学研究。

    arXiv:2403.02522v1 Announce Type: cross  Abstract: Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and disease, yet are underexplored in the medical machine learning community. The existing deep learning systems for health acoustics are often narrowly trained and evaluated on a single task, which is limited by data and may hinder generalization to other tasks. To mitigate these gaps, we develop HeAR, a scalable self-supervised learning-based deep learning system using masked autoencoders trained on a large dataset of 313 million two-second long audio clips. Through linear probes, we establish HeAR as a state-of-the-art health audio embedding model on a benchmark of 33 health acoustic tasks across 6 datasets. By introducing this work, we hope to enable and accelerate further health acoustics research.
    
[^103]: 为开放式学习机器人设定目的：一个计算分类、定义和操作化

    Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation

    [https://arxiv.org/abs/2403.02514](https://arxiv.org/abs/2403.02514)

    提出了设定机器人目的的概念，以帮助机器人更加关注获取与目的相关的知识。

    

    arXiv:2403.02514v1 公告类型: 跨领域 摘要: 自主开放式学习(OEL)机器人能够通过与环境的直接交互累积获取新技能和知识，例如依靠内在动机和自动生成的目标的指导。OEL机器人对应用具有很高的相关性，因为它们可以使用自主获取的知识来完成对人类用户有关的任务。然而，OEL机器人面临一个重要限制：这可能导致获取的知识对完成用户任务并不那么重要。本文分析了这个问题的一个可能解决方案，它围绕“目的”这一新概念展开。目的表示设计者和/或用户希望机器人从中获得什么。机器人应使用目的的内部表征，这里称为“愿望”，来将其开放式探索集中于获取与其完成目的相关的知识。这项工作有助于发展一个共同

    arXiv:2403.02514v1 Announce Type: cross  Abstract: Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a co
    
[^104]: 通过图像字幕实现差分隐私表示学习

    Differentially Private Representation Learning via Image Captioning

    [https://arxiv.org/abs/2403.02506](https://arxiv.org/abs/2403.02506)

    通过图像字幕生成实现了有效的差分隐私表示学习，获得了高质量图像特征，可用于各种视觉和视觉语言任务。

    

    差分隐私（DP）机器学习被认为是从敏感数据中训练模型同时保护隐私的黄金标准解决方案。然而，实现这一理想的一个主要障碍是其次优的隐私-准确性权衡，在DP表示学习中特别明显。具体来说，已经证明在适度的隐私预算下，大多数模型学习的表示并不比手工特征显著更好。在这项工作中，我们展示了通过图像字幕和扩展到互联网规模的多模态数据集可以实现有效的DP表示学习。通过一系列工程技巧，我们成功地使用可观的计算量从头开始训练了DP图像字幕生成器（DP-Cap）在来自LAION-2B的233M子集上，并获得了前所未有的高质量图像特征，可用于各种下游视觉和视觉语言任务。

    arXiv:2403.02506v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For examp
    
[^105]: 试错法：面向LLM代理的基于探索的轨迹优化

    Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents

    [https://arxiv.org/abs/2403.02502](https://arxiv.org/abs/2403.02502)

    提出了一种面向LLM代理的基于探索的轨迹优化方法，通过允许代理从探索失败中学习，实现了性能的改进。

    

    大型语言模型（LLMs）已经成为各种自主代理系统中不可或缺的组成部分。在这项研究中，我们提出一种基于探索的轨迹优化方法，称为ETO。这种学习方法旨在提高开放LLM代理的性能。与先前专门训练成功专家轨迹的研究相反，我们的方法允许代理从其探索失败中学习。这通过迭代优化框架实现了性能的改进。在探索阶段，代理与环境互动，完成指定任务，收集失败轨迹以创建对比轨迹对。在随后的训练阶段，代理利用这些轨迹偏好对更新其策略，使用类似DPO的对比学习方法。这种探索和训练的迭代循环促进了代理的持续改进。

    arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
    
[^106]: 基于变分递归自编码器的动态因子模型用于股票收益预测

    RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction

    [https://arxiv.org/abs/2403.02500](https://arxiv.org/abs/2403.02500)

    RVRAE是一种动态因子模型，利用变分递归自编码器，结合先验-后验学习方法，解决了市场数据中的时间依赖性和噪声问题，擅长风险调整。

    

    近年来，动态因子模型作为经济学和金融领域的主要工具崭露头角，特别适用于投资策略。相比传统的静态因子模型，该模型在处理复杂、非线性和嘈杂的市场情况方面有所改进。机器学习的进步，特别是在处理非线性数据方面，进一步增强了资产定价方法。本文介绍了一种开创性的动态因子模型，名为RVRAE。该模型是一种概率方法，解决了市场数据中的时间依赖性和噪声问题。RVRAE巧妙地将动态因子建模原理与深度学习中的变分递归自编码器（VRAE）相结合。RVRAE的一个关键特点是其使用先验-后验学习方法。该方法通过寻找受未来数据启发的最佳后验因子模型，微调模型的学习过程。值得注意的是，RVRAE擅长风险调整。

    arXiv:2403.02500v1 Announce Type: cross  Abstract: In recent years, the dynamic factor model has emerged as a dominant tool in economics and finance, particularly for investment strategies. This model offers improved handling of complex, nonlinear, and noisy market conditions compared to traditional static factor models. The advancement of machine learning, especially in dealing with nonlinear data, has further enhanced asset pricing methodologies. This paper introduces a groundbreaking dynamic factor model named RVRAE. This model is a probabilistic approach that addresses the temporal dependencies and noise in market data. RVRAE ingeniously combines the principles of dynamic factor modeling with the variational recurrent autoencoder (VRAE) from deep learning. A key feature of RVRAE is its use of a prior-posterior learning method. This method fine-tunes the model's learning process by seeking an optimal posterior factor model informed by future data. Notably, RVRAE is adept at risk mod
    
[^107]: 基于预测的神经架构搜索的编码方式

    Encodings for Prediction-based Neural Architecture Search

    [https://arxiv.org/abs/2403.02484](https://arxiv.org/abs/2403.02484)

    预测器方法在神经架构搜索方面起到了显著作用，本文对不同类型的神经编码进行了分类和研究，并引入了统一编码，扩展了NAS预测器到多个搜索空间。

    

    预测器方法大大增强了神经架构搜索（NAS）优化的效果。这些预测器的有效性在很大程度上取决于神经网络架构的编码方法。本文对三种主要类型的神经编码进行了分类和研究：结构型、学习型和基于分数的。此外，我们扩展了这些编码，并引入了“统一编码”，将NAS预测器扩展到多个搜索空间。我们的分析来自于在NASBench-101（NB101）、NB201、NB301、网络设计空间（NDS）和TransNASBench-101等NAS空间上进行的超过150万个神经网络架构的实验。根据我们的研究，我们提出了

    arXiv:2403.02484v1 Announce Type: cross  Abstract: Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present 
    
[^108]: 使用线性函数逼近的TD学习的简单有限时间分析

    A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation

    [https://arxiv.org/abs/2403.02476](https://arxiv.org/abs/2403.02476)

    通过一个新颖的两步论证，我们展示了在不实际执行投影步骤的情况下保留投影基础分析的简单性是可能的。

    

    我们研究了在马尔可夫采样下使用线性函数逼近的TD学习的有限时间收敛性。此设置下现有的证明要么假定算法中存在投影步骤以简化分析，要么需要一个相当复杂的论证来确保迭代的稳定性。我们提出：\textit{在不实际执行投影步骤的情况下保留投影基础分析的简单性是否可能？}我们的主要贡献是通过一个新颖的两步论证来展示这是可能的。在第一步中，我们使用归纳证明，在标准选择常量步长$\alpha$下，由TD学习生成的迭代保持期望上的一致有界性。在第二步中，我们建立了一个递归，模拟了TD学习的稳态动态，受马尔可夫采样效果的$O(\alpha^2)$数量级上的有界摄动影响。

    arXiv:2403.02476v1 Announce Type: new  Abstract: We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads 
    
[^109]: 通过受限直接偏好优化增强LLM安全性

    Enhancing LLM Safety via Constrained Direct Preference Optimization

    [https://arxiv.org/abs/2403.02475](https://arxiv.org/abs/2403.02475)

    通过引入Constrained DPO（C-DPO）方法，我们提出了一种高效且轻量的微调大型语言模型（LLMs）的方法，能有效平衡有用性和安全性之间的权衡，为LLMs提供了安全保障。

    

    大型语言模型（LLMs）的快速增强能力提高了将人工智能系统与不同人类偏好相一致以同时增强其有用性和安全性的迫切需要，尽管这些目标常常相互冲突。为解决这一重要问题，一种有前途的方法是在微调阶段通过受限制的人类反馈强化学习（RLHF）框架施加安全约束。然而，这种方法计算成本高且常常不稳定。本文引入了受限制的DPO（C-DPO），这是对最近提出的直接偏好优化（DPO）方法的一种新颖扩展，用于优化LLMs的微调，具有高效和轻量的特点。通过融合双梯度下降和DPO，我们的方法在不使用强化学习的情况下确定了帮助性和无害性之间的几乎最佳折衷。从经验上看，我们的方法为LLMs提供了安全保障。

    arXiv:2403.02475v1 Announce Type: cross  Abstract: The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to L
    
[^110]: 视觉-语言模型在医学报告生成和视觉问答中的应用：一项综述

    Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review

    [https://arxiv.org/abs/2403.02469](https://arxiv.org/abs/2403.02469)

    该论文综述了医学视觉-语言模型在医学报告生成和视觉问答领域的最新进展，重点讨论了模型架构、预训练策略、评估指标以及未来方向。

    

    医学视觉-语言模型（VLMs）结合了计算机视觉和自然语言处理，用于分析医学领域中的视觉和文本数据。本文综述了最近在开发专门用于医疗领域的VLMs方面取得的进展，重点关注设计用于医学报告生成和视觉问答的模型。我们介绍了自然语言处理和计算机视觉的背景，解释了如何将这两个领域的技术结合到VLMs中，以实现从多模态数据中学习。我们讨论的关键领域包括对医学视觉-语言数据集的探索，对最近值得关注的医学VLMs中采用的架构和预训练策略进行深入分析，以及对评估VLMs在医学报告生成和视觉问答中表现的评估指标进行全面讨论。我们还强调了当前的挑战并提出了未来的方向，包括提高临床有效性和解决...

    arXiv:2403.02469v1 Announce Type: cross  Abstract: Medical vision-language models (VLMs) combine computer vision and natural language processing to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on models designed for medical report generation and visual question answering. We provide background on natural language processing and computer vision, explaining how techniques from both fields are integrated into VLMs to enable learning from multimodal data. Key areas we address include the exploration of medical vision-language datasets, in-depth analyses of architectures and pre-training strategies employed in recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs' performance in medical report generation and visual question answering. We also highlight current challenges and propose future directions, including enhancing clinical validity and addressing p
    
[^111]: 应用机器学习和人工智能推动的因果推断

    Applied Causal Inference Powered by ML and AI

    [https://arxiv.org/abs/2403.02467](https://arxiv.org/abs/2403.02467)

    本书介绍了机器学习和因果推断的新兴融合，探讨了经典结构方程模型与现代AI等价物之间的联系，并涵盖了使用双重/去偏移机器学习方法进行推断的内容。

    

    arXiv:2403.02467v1 公告类型：交叉摘要：介绍了机器学习和因果推断的新兴融合。该书介绍了经典结构方程模型（SEMs）的思想及其现代人工智能等价物，有向无环图（DAGs）和结构因果模型（SCMs），并涵盖了使用现代预测工具在这些模型中进行推断的双重/去偏移机器学习方法。

    arXiv:2403.02467v1 Announce Type: cross  Abstract: An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.
    
[^112]: 关于神经架构搜索的延迟预测器

    On Latency Predictors for Neural Architecture Search

    [https://arxiv.org/abs/2403.02446](https://arxiv.org/abs/2403.02446)

    通过在训练设备上进行预训练，并将预测器转移到测试设备上，可以显著提高预测模型的样本效率。

    

    高效部署神经网络（NN）需要同时优化准确性和延迟。例如，硬件感知神经架构搜索已被用于自动找到满足特定硬件设备上延迟约束的NN架构。这些搜索算法的核心是一个旨在为候选NN架构提供硬件延迟估计的预测模型。最近的研究表明，通过在一些\textit{训练}设备上进行大量样本的预训练，然后将预测器转移到\textit{测试}（目标）设备上，这些预测模型的样本效率可以极大地提高。迁移学习和元学习方法已被用于此，但往往表现出显著的性能变异性。此外，现有延迟预测器的评估主要是在手工定制的训练/测试设备集上进行的，这使得难以确定

    arXiv:2403.02446v1 Announce Type: new  Abstract: Efficient deployment of neural networks (NN) requires the co-optimization of accuracy and latency. For example, hardware-aware neural architecture search has been used to automatically find NN architectures that satisfy a latency constraint on a specific hardware device. Central to these search algorithms is a prediction model that is designed to provide a hardware latency estimate for a candidate NN architecture. Recent research has shown that the sample efficiency of these predictive models can be greatly improved through pre-training on some \textit{training} devices with many samples, and then transferring the predictor on the \textit{test} (target) device. Transfer learning and meta-learning methods have been used for this, but often exhibit significant performance variability. Additionally, the evaluation of existing latency predictors has been largely done on hand-crafted training/test device sets, making it difficult to ascertain
    
[^113]: 胎儿大脑解剖约束下的纤维束追踪

    Anatomically Constrained Tractography of the Fetal Brain

    [https://arxiv.org/abs/2403.02444](https://arxiv.org/abs/2403.02444)

    提出了一种基于精确分割胎儿大脑组织的解剖约束纤维束追踪方法，并通过深度学习方法实现自动分割，有效改善了对胎儿大脑的纤维束追踪准确性。

    

    弥敦路:2403.02444v1 公告类型：跨过摘要：扩散加权磁共振成像（dMRI）被越来越广泛地用于研究子宫内的胎儿大脑。 dMRI使得流线追踪成为可能的重要计算，其具有独特的应用，如对脑白质进行纤维束特异性分析和结构连接性评估。然而，由于胎儿dMRI数据质量较低且纤维束追踪具有挑战性，现有方法往往会产生高度不准确的结果。它们生成许多虚假的流线，同时未能重构构成主要白质纤维束的流线。在本文中，我们提倡在dMRI空间中直接对胎儿大脑组织进行准确分割的解剖约束纤维束追踪。我们开发了一种深度学习方法来自动计算分割。对独立测试数据的实验表明，该方法可以准确分割胎儿大脑组织，并显著改善tra

    arXiv:2403.02444v1 Announce Type: cross  Abstract: Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to study the fetal brain in utero. An important computation enabled by dMRI is streamline tractography, which has unique applications such as tract-specific analysis of the brain white matter and structural connectivity assessment. However, due to the low fetal dMRI data quality and the challenging nature of tractography, existing methods tend to produce highly inaccurate results. They generate many false streamlines while failing to reconstruct streamlines that constitute the major white matter tracts. In this paper, we advocate for anatomically constrained tractography based on an accurate segmentation of the fetal brain tissue directly in the dMRI space. We develop a deep learning method to compute the segmentation automatically. Experiments on independent test data show that this method can accurately segment the fetal brain tissue and drastically improve tra
    
[^114]: 使用可解释人工智能 (XAI) 分析预测异常根本原因

    Root Causing Prediction Anomalies Using Explainable AI

    [https://arxiv.org/abs/2403.02439](https://arxiv.org/abs/2403.02439)

    本文介绍了在根本上解决个性化广告中模型性能下降问题的方法，通过解释人工智能技术分析预测异常。

    

    本文介绍了可解释人工智能（XAI）在根因分析机器学习模型性能下降中的创新应用，该模型不断从用户参与数据中学习。在这些系统中，单个特征损坏可能导致级联特征、标签和概念漂移。我们已成功将这一技术应用于提高个性化广告中使用的模型的可靠性。在这种系统中，性能下降表现为模型中的预测异常。

    arXiv:2403.02439v1 Announce Type: cross  Abstract: This paper presents a novel application of explainable AI (XAI) for root-causing performance degradation in machine learning models that learn continuously from user engagement data. In such systems a single feature corruption can cause cascading feature, label and concept drifts. We have successfully applied this technique to improve the reliability of models used in personalized advertising. Performance degradation in such systems manifest as prediction anomalies in the models. These models are typically trained continuously using features that are produced by hundreds of real time data processing pipelines or derived from other upstream models. A failure in any of these pipelines or an instability in any of the upstream models can cause feature corruption, causing the model's predicted output to deviate from the actual output and the training data to become corrupted. The causal relationship between the features and the predicted ou
    
[^115]: SoK: 联邦反学习中的挑战与机遇

    SoK: Challenges and Opportunities in Federated Unlearning

    [https://arxiv.org/abs/2403.02437](https://arxiv.org/abs/2403.02437)

    联邦学习引入了新的隐私要求，促使研究开始关注适用于联邦学习环境的反学习机制。

    

    引入于2017年的联邦学习（FL）促进了不信任方之间的合作学习，无需各方明确共享其数据。这允许在尊重GDPR和CPRA等隐私规定的同时，在用户数据上训练模型。然而，新兴的隐私要求可能要求模型所有者能够“遗忘”一些已学习的数据，例如当数据所有者或执法机构要求时。这催生了一个名为“机器反学习”的活跃研究领域。在FL的背景下，许多为集中式环境开发的反学习技术并不容易应用！这是由于FL中集中式和分布式学习之间的独特差异，特别是互动性、随机性、异构性和有限可访问性。为应对这一挑战，最近的一系列研究工作聚焦于开发适用于FL的反学习机制。

    arXiv:2403.02437v1 Announce Type: cross  Abstract: Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK pape
    
[^116]: 关于测度预处理对通用参数ML模型和通过领域自适应进行迁移学习的影响

    On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation

    [https://arxiv.org/abs/2403.02432](https://arxiv.org/abs/2403.02432)

    通过测度预处理技术，研究了参数ML模型和领域自适应迁移学习中学习代理的收敛性，提出了伽玛收敛的类似法图引理的理解。

    

    我们研究了一种新技术，用于理解在数据略微修改的情况下学习代理的收敛性。 我们展示了这种收敛可以通过一种类似于法图引理的模拟理解，从而产生了伽玛收敛。 我们展示了这种技术在通用机器学习任务和领域自适应迁移学习中的相关性和应用。

    arXiv:2403.02432v1 Announce Type: cross  Abstract: We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.
    
[^117]: 面向高效多变量时间序列异常检测的深度自编码器

    Towards efficient deep autoencoders for multivariate time series anomaly detection

    [https://arxiv.org/abs/2403.02429](https://arxiv.org/abs/2403.02429)

    本文提出了面向多变量时间序列异常检测的深度自编码器的高效压缩方法，通过修剪减少权重数量并防止精度灾难性下降，以在有限时间和内存约束的实时系统中获得最佳结果。

    

    多变量时间序列异常检测在许多工业和研究应用中是一个关键问题。及时检测异常允许防止制造过程中的缺陷和网络物理系统中的故障。深度学习方法由于其对复杂多变量数据进行准确和稳健分析的特点而备受青睐。然而，一个关键方面是能够及时提取预测结果，以适应不同应用中的实时需求。在深度学习模型中，模型缩减对于在具有有限时间和内存约束的实时系统中实现最佳结果非常重要。在本文中，我们通过提出一种涉及三个关键因素的深度自动编码器新压缩方法来解决这个问题。首先，修剪减少权重数量，同时通过快速搜索过程防止精度的灾难性下降。

    arXiv:2403.02429v1 Announce Type: cross  Abstract: Multivariate time series anomaly detection is a crucial problem in many industrial and research applications. Timely detection of anomalies allows, for instance, to prevent defects in manufacturing processes and failures in cyberphysical systems. Deep learning methods are preferred among others for their accuracy and robustness for the analysis of complex multivariate data. However, a key aspect is being able to extract predictions in a timely manner, to accommodate real-time requirements in different applications. In the case of deep learning models, model reduction is extremely important to achieve optimal results in real-time systems with limited time and memory constraints. In this paper, we address this issue by proposing a novel compression method for deep autoencoders that involves three key factors. First, pruning reduces the number of weights, while preventing catastrophic drops in accuracy by means of a fast search process th
    
[^118]: 数字孪生和土木工程阶段：重新定位采用策略

    Digital Twins and Civil Engineering Phases: Reorienting Adoption Strategies

    [https://arxiv.org/abs/2403.02426](https://arxiv.org/abs/2403.02426)

    数字孪生技术在土木工程领域的采用策略需要重新定位，以解决不同阶段的挑战和应用分散性问题。

    

    数字孪生（DT）技术多年来受到极大关注，因为它为科学和工程各利益相关者带来了许多承诺。因此，人们探讨了数字孪生的不同主题领域。土木工程等特定领域也不例外，导致了针对特定领域应用的碎片化方法。土木工程行业在这方面进一步处于不利地位，因为它依赖其他工程领域的外部技术来进行数字孪生的采用。这些扩展的一个不断增加的后果是将数字孪生集中应用于运营和维护阶段。另一方面，建筑信息建模（BIM）被广泛用于规划/设计阶段，而施工阶段的瞬变性质对于数字孪生的采用构成了挑战。在本文中，我们提出了数字孪生在建筑设计...

    arXiv:2403.02426v1 Announce Type: cross  Abstract: Digital twin (DT) technology has received immense attention over the years due to the promises it presents to various stakeholders in science and engineering. As a result, different thematic areas of DT have been explored. This is no different in specific fields such as manufacturing, automation, oil and gas, and civil engineering, leading to fragmented approaches for field-specific applications. The civil engineering industry is further disadvantaged in this regard as it relies on external techniques by other engineering fields for its DT adoption. A rising consequence of these extensions is a concentrated application of DT to the operations and maintenance phase. On another spectrum, Building Information Modeling (BIM) are pervasively utilized in the planning/design phase, and the transient nature of the construction phase remains a challenge for its DT adoption. In this paper, we present a phase-based development of DT in the Archit
    
[^119]: 你需要更多LLM调用吗？走向复合推理系统的扩展定律

    Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems

    [https://arxiv.org/abs/2403.02419](https://arxiv.org/abs/2403.02419)

    本文研究了复合推理系统的扩展定律，发现投票推理系统的性能随LLM调用次数增加先增加后下降。

    

    许多最近语言任务中的最先进结果是通过执行多个大型语言模型（LLM）调用并汇总它们的响应的复合系统实现的。然而，对于LLM调用次数的影响 -- 例如，当要求LLM多次回答每个问题并取得共识时 -- 对于这种复合系统的性能了解甚少。在本文中，我们开始研究复合推理系统的扩展定律。我们从理论和实证的角度分析了LLM调用次数如何影响一个层级投票推理系统的性能 -- 这是最简单的复合系统之一，它通过多数投票聚合LLM的响应。我们实验证明，在多个语言任务中，令人惊讶的是，投票推理系统的性能随着LLM调用次数的增加而先增加后下降。我们的理论结果表明，这种非单调性是由于

    arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
    
[^120]: 从零到英雄：无知初值处的局部曲率如何远离糟糕的极小值

    From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima

    [https://arxiv.org/abs/2403.02418](https://arxiv.org/abs/2403.02418)

    局部曲率变化导致系统从良性且富有信息的局部景观逐渐陷入无信息的迷宫，关键转变与时间相关的Hessian的阈值有关。

    

    我们研究了梯度下降在非凸和高维设置中的优化动力学，重点关注相位恢复问题作为复杂损失景观的案例研究。通过分析局部曲率在优化过程中的变化，我们发现在中间信噪比下，Hessian在下降的第一个阶段显示出指向好极小值的下降方向，然后在结束时被困在糟糕的极小值中。因此，局部景观起初是良性且富有信息的，然后梯度下降将系统带入无信息的迷宫。两个阶段之间的转变与时间相关的Hessian的BBP类型阈值相关联。

    arXiv:2403.02418v1 Announce Type: new  Abstract: We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes. We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end. Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze. The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian. Through both theoretical analysis and numerical experiments, we show that in prac
    
[^121]: NiNformer: 一种具有令牌混合生成门控功能的网络中网络变压器

    NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function

    [https://arxiv.org/abs/2403.02411](https://arxiv.org/abs/2403.02411)

    提出了一种新的计算块，称为NiNformer，具有令牌混合生成门控功能，以解决注意机制在深度学习中的计算成本高昂和数据集要求大的缺点。

    

    注意机制是Transformer架构的主要组件，自引入以来，在深度学习领域取得了显著进展，跨越了许多领域和多个任务。该机制在计算机视觉中被应用为Vision Transformer ViT，并且其用途已扩展到视觉领域的许多任务，如分类、分割、目标检测和图像生成。尽管该机制非常具有表现力和能力，但其缺点是计算成本高昂，需要大规模数据集来有效优化。为了解决这些缺点，文献中提出了许多设计来减轻计算负担和缓解数据大小要求。在视觉领域的一些尝试的例子包括MLP-Mixer、Conv-Mixer、Perciver-IO等。本文介绍了一种新的计算块，作为一种

    arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
    
[^122]: 在量子计算机上对时尚-MNIST数据集进行分类

    Classification of the Fashion-MNIST Dataset on a Quantum Computer

    [https://arxiv.org/abs/2403.02405](https://arxiv.org/abs/2403.02405)

    通过改进变分算法，使用渐近浅层电路对时尚-MNIST数据集进行编码，为未来的量子机器学习实证研究提供了新方法。

    

    arXiv:2403.02405v1 公告类型：交叉 摘要：量子机器学习算法对工业应用的潜在影响仍然是一个令人振奋的悬而未决的问题。将经典数据编码到量子计算机的常规方法不仅对算法的潜在量子优势成本过高，而且严重限制了当前硬件上可行实验的规模。因此，尽管最近的研究声称他们的算法适用于近期，但却没有在标准机器学习数据集上进行实验基准测试。我们尝试通过改进最近提出的一种变分算法[1]来解决数据编码问题，该算法近似准备编码数据，使用与当前可用量子计算机的本机门集和拓扑相适应的渐近浅层电路。我们将改进的算法应用于对Fashion-MNIST数据集[2]进行编码，可以直接在未来的量子机器学习实证研究中使用。

    arXiv:2403.02405v1 Announce Type: cross  Abstract: The potential impact of quantum machine learning algorithms on industrial applications remains an exciting open question. Conventional methods for encoding classical data into quantum computers are not only too costly for a potential quantum advantage in the algorithms but also severely limit the scale of feasible experiments on current hardware. Therefore, recent works, despite claiming the near-term suitability of their algorithms, do not provide experimental benchmarking on standard machine learning datasets. We attempt to solve the data encoding problem by improving a recently proposed variational algorithm [1] that approximately prepares the encoded data, using asymptotically shallow circuits that fit the native gate set and topology of currently available quantum computers. We apply the improved algorithm to encode the Fashion-MNIST dataset [2], which can be directly used in future empirical studies of quantum machine learning al
    
[^123]: OTClean：使用最优输运进行条件独立性违规数据清洗

    OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport

    [https://arxiv.org/abs/2403.02372](https://arxiv.org/abs/2403.02372)

    使用最优输运理论的OTClean框架解决了在条件独立性（CI）约束下数据清洗的问题，通过将数据修复问题转化为正则化优化问题，并提出了受Sinkhorn矩阵缩放算法启发的迭代算法，克服了可伸缩性挑战。

    

    确保条件独立性（CI）约束对于公平和可信赖的机器学习模型的发展至关重要。本文介绍了一个名为OTClean的框架，利用最优输运理论在CI约束下进行数据修复。最优输运理论提供了一个严格的框架来衡量概率分布之间的差异，从而确保对数据效用的控制。我们将涉及CI的数据修复问题形式化为一个二次约束线性规划（QCLP），并提出了一种用于解决该问题的交替方法。然而，由于计算最优输运距离（如Wasserstein距离）所涉及的计算成本，这种方法面临可伸缩性问题。为了克服这些可伸缩性挑战，我们将问题重新构建为正则化优化问题，从而使我们能够开发一个受Sinkhorn矩阵缩放算法启发的迭代算法，从而使我们能够进行可伸缩的数据修复。

    arXiv:2403.02372v1 Announce Type: cross  Abstract: Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models. In this paper, we introduce \sys, a framework that harnesses optimal transport theory for data repair under CI constraints. Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility. We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution. However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance. To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which e
    
[^124]: 适用于工业4.0应用中预测优化的新型混合特征重要性和特征交互检测框架

    A Novel Hybrid Feature Importance and Feature Interaction Detection Framework for Predictive Optimization in Industry 4.0 Applications

    [https://arxiv.org/abs/2403.02368](https://arxiv.org/abs/2403.02368)

    该论文提出了一个新型混合框架，结合特征重要性检测和特征交互检测，以提高工业4.0应用中的预测优化准确性。

    

    先进的机器学习算法越来越被广泛应用于工业4.0中提供基于数据的预测和决策支持。然而，现有模型所达到的预测准确性不足以保证在现实应用中的实际实施。这是因为现实世界数据集中并非所有特征都与正在进行的预测分析直接相关。因此，精心选择特征的结合有潜力对结果产生重大积极影响。为了填补研究空白，本文提出了一个新颖的混合框架，将特征重要性检测器——局部可解释的模型无关解释（LIME）和特征交互检测器——神经交互检测（NID）相结合，以提高预测准确性。通过应用所提出的框架，可以消除不必要的特征，并对交互作用进行编码以生成

    arXiv:2403.02368v1 Announce Type: cross  Abstract: Advanced machine learning algorithms are increasingly utilized to provide data-based prediction and decision-making support in Industry 4.0. However, the prediction accuracy achieved by the existing models is insufficient to warrant practical implementation in real-world applications. This is because not all features present in real-world datasets possess a direct relevance to the predictive analysis being conducted. Consequently, the careful incorporation of select features has the potential to yield a substantial positive impact on the outcome. To address the research gap, this paper proposes a novel hybrid framework that combines the feature importance detector - local interpretable model-agnostic explanations (LIME) and the feature interaction detector - neural interaction detection (NID), to improve prediction accuracy. By applying the proposed framework, unnecessary features can be eliminated, and interactions are encoded to gene
    
[^125]: 解决长尾嘈杂标签学习问题：考虑标签稀有性的两阶段解决方案

    Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity

    [https://arxiv.org/abs/2403.02363](https://arxiv.org/abs/2403.02363)

    提出了一种两阶段方法，通过软标签修复和多专家集成学习结合，解决了长尾嘈杂标签学习问题。

    

    真实世界的数据集通常表现出嘈杂标签和类别不平衡，如长尾分布。先前的研究通过区分嘈杂和干净样本来解决这个问题，但依赖于基于嘈杂长尾数据的预测信息会引入潜在错误。为了克服先前研究的局限性，我们提出了一种有效的两阶段方法，将软标签修复与多专家集成学习相结合。在稳健的软标签修复的第一阶段中，我们通过对比学习获得无偏特征，使用经过精心设计的BAlanced Noise-tolerant Cross-entropy (BANC) 损失训练的分类器进行初步预测。在第二阶段，我们的标签修复方法被应用于为多专家集成学习获取软标签，为长尾嘈杂标签问题提供了一个原则性的解决方案。在多个基准测试中进行的实验表明，我们的方法在各个方面均优于现有技术。

    arXiv:2403.02363v1 Announce Type: cross  Abstract: Real-world datasets commonly exhibit noisy labels and class imbalance, such as long-tailed distributions. While previous research addresses this issue by differentiating noisy and clean samples, reliance on information from predictions based on noisy long-tailed data introduces potential errors. To overcome the limitations of prior works, we introduce an effective two-stage approach by combining soft-label refurbishing with multi-expert ensemble learning. In the first stage of robust soft label refurbishing, we acquire unbiased features through contrastive learning, making preliminary predictions using a classifier trained with a carefully designed BAlanced Noise-tolerant Cross-entropy (BANC) loss. In the second stage, our label refurbishment method is applied to obtain soft labels for multi-expert ensemble learning, providing a principled solution to the long-tail noisy label problem. Experiments conducted across multiple benchmarks v
    
[^126]: 面向异构联邦学习的定制架构研究: 对比云边模型解耦

    Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling

    [https://arxiv.org/abs/2403.02360](https://arxiv.org/abs/2403.02360)

    通过对比云边模型解耦，提出了一种为异构联邦学习定制架构，该架构将深度神经网络分为捕获共享表示的主体和处理数据异构性的个性化头部，以优化联邦学习性能。

    

    联邦学习作为一种有前途的分布式学习范例，实现了多个网络边缘客户端之间全局模型的协作训练，而无需进行中央数据收集。然而，边缘数据分布的异构性使得模型倾向于局部极小值，这些极小值可能远离全局最优。这种异构性通常导致收敛速度缓慢且通信开销巨大。为解决这些问题，我们提出了一种名为FedCMD的新型联邦学习框架，即适应云边支持的联邦学习的模型解耦，将深度神经网络分为用于获取云端共享表示的主体和用于迁移数据异构性的个性化头部。我们的动机是，通过深入研究选择不同神经网络层作为个性化头部的性能，发现将最后一层刚性分配为个性化头部可能无法最大化改善异构性数据性能，从而提出了一种自动化选择个性化头部的方法。

    arXiv:2403.02360v1 Announce Type: cross  Abstract: Federated learning, as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel federated learning framework called FedCMD, a model decoupling tailored to the Cloud-edge supported federated learning that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized he
    
[^127]: 在超复数空间中利用时间敏感关系进行时间知识图补全

    Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space

    [https://arxiv.org/abs/2403.02355](https://arxiv.org/abs/2403.02355)

    该论文提出在超复数空间中利用更具表现力的四元数表示进行时间知识图补全，着重捕捉时间敏感关系，并理论上验证了方法可以建模各种关系模式，实验表明该方法达到了最新的性能水平。

    

    时间知识图补全（TKGC）旨在在给定特定时间的时间知识图中填补缺失的事实。本文通过引入更具表现力的四元数表示在超复数空间中进行TKGC，超越了传统方法。我们的研究聚焦于捕捉时间敏感关系，而不是时间感知实体，通过时间感知的旋转和周期性时间平移来建模时间敏感关系，有效捕捉复杂的时间变化。此外，我们在理论上证明了我们的方法能够建模对称、非对称、逆向、组合和演变的关系模式。公共数据集上的综合实验验证了我们提出的方法实现了最新的性能。

    arXiv:2403.02355v1 Announce Type: cross  Abstract: Temporal knowledge graph completion (TKGC) aims to fill in missing facts within a given temporal knowledge graph at a specific time. Existing methods, operating in real or complex spaces, have demonstrated promising performance in this task. This paper advances beyond conventional approaches by introducing more expressive quaternion representations for TKGC within hypercomplex space. Unlike existing quaternion-based methods, our study focuses on capturing time-sensitive relations rather than time-aware entities. Specifically, we model time-sensitive relations through time-aware rotation and periodic time translation, effectively capturing complex temporal variability. Furthermore, we theoretically demonstrate our method's capability to model symmetric, asymmetric, inverse, compositional, and evolutionary relation patterns. Comprehensive experiments on public datasets validate that our proposed approach achieves state-of-the-art perform
    
[^128]: 基于时空场神经网络的空气质量推断

    Spatio-Temporal Field Neural Networks for Air Quality Inference

    [https://arxiv.org/abs/2403.02354](https://arxiv.org/abs/2403.02354)

    该研究提出了基于时空场神经网络的新模型和金字塔推断框架，在空气质量推断中取得了最先进的性能。

    

    空气质量推断问题旨在利用来自有限观测站的历史数据推断未知位置的空气质量指数。考虑到观测站高昂的维护成本导致数据稀疏性，良好的推断算法可以有效节约成本并细化数据粒度。尽管时空图神经网络在这个问题上取得了显著进展，但它们对现实的非欧几里得和离散数据结构建模限制了潜力。本文首次尝试通过提出一个新模型，即时空场神经网络，及其对应的新框架，金字塔推断，将两种不同的时空观点，场和图，相结合。大量实验证实我们的模型在中国大陆全国范围内的空气质量推断中实现了最新技术水平，展示了我们提出的模型的优越性。

    arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
    
[^129]: ATP: 通过对顶级主要键进行关注实现快速LLM服务

    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys

    [https://arxiv.org/abs/2403.02352](https://arxiv.org/abs/2403.02352)

    提出了一种新的注意机制ATP，通过关注顶级主要键而非每个标记，以实现对输入序列的快速处理，并能够在降低注意力复杂度的同时捕捉输入序列的语义关系。

    

    我们提出了一种具有线性复杂度的新型注意机制 ATP，该机制将注意力集中在顶级主要键上，而不是每个单独的标记上。特别地，ATP受到一个重要观察的驱动，即输入序列通常具有低秩结构，即输入序列可以由少量主要基表示。因此，ATP将输入转换为正交空间，并仅在顶级主要基上计算注意力。由于输入序列中观察到的低秩结构，ATP能够仅通过少量主要基捕捉输入序列中的语义关系。此外，注意复杂度从二次降低到线性，而不会引起明显的性能下降。ATP进一步为具有低秩输入的其他线性层减少了复杂度，与仅单纯进行注意力计算的先前工作相比，实现了更大的加速。

    arXiv:2403.02352v1 Announce Type: cross  Abstract: We propose a new attention mechanism with linear complexity, ATP, that fixates \textbf{A}ttention on \textbf{T}op \textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \emph{quadratic} to \emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely
    
[^130]: 关于无需数据相似性条件的联邦学习算法收敛性

    On the Convergence of Federated Learning Algorithms without Data Similarity

    [https://arxiv.org/abs/2403.02347](https://arxiv.org/abs/2403.02347)

    本文提出了一种无需数据相似性条件的联邦学习算法收敛性分析框架，通过推导出三种常用步长调度的精确表达式，实现了对算法收敛性能的全面评估。

    

    数据相似性假设传统上被广泛依赖于理解联邦学习方法的收敛行为。不幸的是，这种方法通常要求根据数据相似性程度微调步长。当数据相似性较低时，这些小步长会导致联邦方法的收敛速度不可接受地慢。本文提出了一种新颖和统一的框架，用于分析联邦学习算法的收敛性，无需数据相似性条件。我们的分析集中在一个不等式上，这个不等式捕捉了步长对算法收敛性能的影响。通过将我们的定理应用于众所周知的联邦算法，我们推导出了三种广泛使用的步长调度的精确表达式：固定步长、递减步长和步衰减步长，这些表达式独立于数据相似性条件。最后，我们对性能进行了全面评估。

    arXiv:2403.02347v1 Announce Type: new  Abstract: Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance 
    
[^131]: 神经红移：随机网络并非随机函数

    Neural Redshift: Random Networks are not Random Functions

    [https://arxiv.org/abs/2403.02241](https://arxiv.org/abs/2403.02241)

    本论文研究了未经训练的随机权重网络，发现即使简单的MLPs也具有强烈的归纳偏见，不同于传统观点的是，NNs并不具有固有的“简单偏见”，而是依赖于组件的作用。

    

    我们对神经网络（NNs）的泛化能力的理解仍不完整。目前的解释基于梯度下降（GD）的隐含偏见，但无法解释梯度自由方法中模型的能力，也无法解释最近观察到的未经训练网络的简单偏见。本文寻找NNs中的其他泛化源。为了独立于GD理解体系结构提供的归纳偏见，我们研究未经训练的随机权重网络。即使是简单的MLPs也表现出强烈的归纳偏见：在权重空间中进行均匀抽样会产生一个非常偏向于复杂性的函数分布。但与常规智慧不同，NNs并不具有固有的“简单偏见”。这一特性取决于组件，如ReLU、残差连接和层归一化。可利用替代体系结构构建偏向于任何复杂性水平的偏见。Transformers也具有这一特性。

    arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
    
[^132]: 通过正则化流进行互信息估计

    Mutual Information Estimation via Normalizing Flows

    [https://arxiv.org/abs/2403.02187](https://arxiv.org/abs/2403.02187)

    通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。

    

    我们提出了一种新颖的方法来解决互信息（MI）估计问题，即引入基于正则化流的估计器。该估计器将原始数据映射到具有已知互信息闭合形式表达式的目标分布。我们证明了我们的方法产生了原始数据的互信息估计。通过高维数据的实验结果展示了所提出估计器的优势。

    arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
    
[^133]: 用傅立叶基函数弥合增强差距：重新思考图像分类中的频率增强

    Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification

    [https://arxiv.org/abs/2403.01944](https://arxiv.org/abs/2403.01944)

    提出了辅助傅立叶基增强（AFA）的方法，通过在频域进行增强，填补了视觉增强遗留的增强差距，提高了模型的鲁棒性。

    

    计算机视觉模型通常在部署到现实场景中时性能下降，这是由于训练过程中未考虑到的输入出现了意外变化。数据增强通常用于解决这个问题，因为它旨在增加数据变化性并减少训练和测试数据之间的分布差距。然而，常见的视觉增强可能无法保证计算机视觉模型的广泛鲁棒性。在本文中，我们提出了辅助傅立叶基增强（AFA），这是一种针对频率域增强的补充技术，填补了视觉增强遗留的增强差距。我们通过简单高效的对抗设置展示了傅立叶基加性噪声增强的效用。我们的结果表明，AFA有助于模型对常见损坏、OOD泛化以及模型性能随着性能增强的一致性。

    arXiv:2403.01944v1 Announce Type: cross  Abstract: Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. Data augmentation is commonly used to address this issue, as it aims to increase data variety and reduce the distribution gap between training and test data. However, common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the augmentation gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing
    
[^134]: 使用凸优化和列子集选择的矩阵完成

    Matrix Completion with Convex Optimization and Column Subset Selection

    [https://arxiv.org/abs/2403.01919](https://arxiv.org/abs/2403.01919)

    该方法结合了列子集选择和低秩矩阵完成问题的理论基础，提出使用凸优化解决矩阵恢复问题，同时通过实验验证了算法的正确性和性能。

    

    我们介绍了一种用于矩阵恢复问题的两步方法。我们的方法结合了列子集选择和低秩矩阵完成问题的理论基础。提出的方法在每一步中解决一个凸优化任务。我们提出了两种实现我们的列选择矩阵完成（CSMC）方法的算法，每种算法针对不同规模的问题。我们对所提出的方法进行了正式分析，在分析中我们阐明了必要的假设和找到正确解的概率。在论文的第二部分，我们展示了实验工作的结果。数值实验验证了算法的正确性和性能。为了研究矩阵大小、秩和缺失元素比例对解的质量和计算时间的影响，我们在合成数据上进行了实验。所提出的方法被应用于两个真实世界的例子。

    arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
    
[^135]: 基于内部表征的上下文锐度作为警报：减少幻觉的一个视角

    In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation

    [https://arxiv.org/abs/2403.01548](https://arxiv.org/abs/2403.01548)

    本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。

    

    大型语言模型（LLMs）经常会产生幻觉并产生事实错误，然而我们对它们为什么会犯这些错误的理解仍然有限。在本研究中，我们从内部表征的角度深入探讨LLM幻觉的潜在机制，并发现与幻觉相关的一个突出模式：正确的生成在上下文标记的隐藏状态中具有更清晰的上下文激活，而不正确的生成则没有。利用这一见解，我们提出了一种基于熵的度量来量化上下文隐藏状态之间的“锐度”，并将其纳入解码过程中以制定一种受限解码方法。在各种知识寻求和幻觉基准测试上的实验证明了我们方法的一致有效性，例如，在TruthfulQA上实现了高达8.6点的改进。我们相信这项研究可以提高我们对幻觉的理解。

    arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
    
[^136]: 混合策略纳什均衡用于人群导航

    Mixed-Strategy Nash Equilibrium for Crowd Navigation

    [https://arxiv.org/abs/2403.01537](https://arxiv.org/abs/2403.01537)

    通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。

    

    我们解决了针对人群导航找到混合策略纳什均衡的问题。混合策略纳什均衡为机器人提供了一个严谨的模型，使其能够预测人群中不确定但合作的人类行为，但计算成本通常太高，无法进行可扩展和实时的决策制定。在这里，我们证明了一个简单的迭代贝叶斯更新方案收敛于混合策略社交导航游戏的纳什均衡。此外，我们提出了一个基于数据驱动的框架，通过将代理策略初始化为从人类数据集学习的高斯过程，来构建该游戏。基于所提出的混合策略纳什均衡模型，我们开发了一个基于采样的人群导航框架，可以集成到现有导航方法中，并可在笔记本电脑 CPU 上实时运行。我们通过模拟环境和真实世界的非结构化环境中人类数据集对我们的框架进行了评估。

    arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
    
[^137]: LLaMoCo：用于优化代码生成的大型语言模型指令调优

    LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation

    [https://arxiv.org/abs/2403.01131](https://arxiv.org/abs/2403.01131)

    LLaMoCo是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架，通过全面指令集和新颖的两阶段学习策略，实现了优越的性能。

    

    最近的研究探讨了使用大型语言模型（LLMs）进行优化，方法包括从LLMs迭代地寻找下一步解决方案，或直接提示LLMs以获取优化器。然而，这些方法存在固有限制，包括操作效率低、对提示设计敏感度高以及缺乏领域特定知识。我们介绍了LLaMoCo，这是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架。具体地，我们建立了一个包含清晰描述的问题提示和有效优化代码的全面指令集。然后我们开发了一种新颖的两阶段学习策略，在指令调优阶段之前，该策略整合了基于对比学习的热身过程，以增强模型微调期间的收敛行为。实验结果表明，通过我们的LLaMoCo精调的CodeGen（350M）模型达到了卓越的性能。

    arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
    
[^138]: 损失景观的灵敏度分析

    Sensitivity Analysis On Loss Landscape

    [https://arxiv.org/abs/2403.01128](https://arxiv.org/abs/2403.01128)

    利用一、二和三阶导数进行损失景观分析，发现了与Spearman秩相关系数类似可视化的信息，以及损失函数和激活函数结合带来的非线性模式。

    

    梯度可用于灵敏度分析。在这里，我们利用损失景观的优势，了解哪些自变量影响因变量。我们通过自动微分利用一阶、二阶和三阶导数来理解损失景观。我们知道Spearman秩相关系数可以检测两个变量之间的单调关系。然而，我发现在特定配置和参数下，二阶梯度提供的信息可以类似于Spearman的结果进行可视化。在我们的方法中，我们将损失函数与激活函数结合，导致非线性模式。通过重新训练对损失景观的每次探索都提供新的有价值信息。此外，一阶和三阶导数也很有用，因为它们显示了自变量对因变量的影响程度。

    arXiv:2403.01128v1 Announce Type: new  Abstract: Gradients can be employed for sensitivity analysis. Here, we leverage the advantages of the Loss Landscape to comprehend which independent variables impact the dependent variable. We seek to grasp the loss landscape by utilizing first, second, and third derivatives through automatic differentiation. we know that Spearman's rank correlation coefficient can detect the monotonic relationship between two variables. However, I have found that second-order gradients, with certain configurations and parameters, provide information that can be visualized similarly to Spearman's results.In our approach, we incorporate a loss function with an activation function, resulting in a non-linear pattern. Each exploration of the loss landscape through retraining yields new valuable information. Furthermore, the first and third derivatives are also beneficial, as they indicate the extent to which independent variables influence the dependent variable.
    
[^139]: 透过几何限制概率建模发现新生物医学概念

    Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling

    [https://arxiv.org/abs/2403.01053](https://arxiv.org/abs/2403.01053)

    提出了一种通过几何限制概率建模处理方法来解决生物医学数据中存在的非 i.i.d. 数据分布、类别不平衡等问题。

    

    arXiv:2403.01053v1 通告类型: 交叉  摘要: 机器学习以其数据驱动的特性，对科学发现的基本实践具有巨大的潜力改变。随着不断增加的研究数据收集，自动探索观测数据中的模式和见解，发现新的表型类别和概念将会变得更加吸引人。然而，在生物医学领域，累积数据中存在若干挑战，阻碍了新类发现的进展。非 i.i.d. 数据分布伴随着不同类别组之间的严重不平衡，本质上导致模糊和偏倚的语义表示。在这项工作中，我们提出了一种几何限制概率建模处理方法来解决所识别的问题。首先，我们建议将实例嵌入的近似后验参数化为边际 von Mises-Fisher 分布，以解决神经嵌入方案的模糊性与偏见性。

    arXiv:2403.01053v1 Announce Type: cross  Abstract: Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the int
    
[^140]: 梯度被罚：通过探索拒绝损失地形图来检测针对大语言模型的越狱攻击

    Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes

    [https://arxiv.org/abs/2403.00867](https://arxiv.org/abs/2403.00867)

    本文提出了一种名为Gradient Cuff的方法，通过探索拒绝损失地形图来检测对大语言模型的越狱攻击，成功设计了一种有效的两步检测策略。

    

    大型语言模型（LLMs）正成为一种突出的生成式AI工具，用户输入查询，LLM生成答案。为了减少伤害和滥用，人们通过使用先进的训练技术如来自人类反馈的强化学习（RLHF）来将这些LLMs与人类价值观保持一致。然而，最近的研究突显了LLMs对于试图颠覆嵌入的安全防护措施的对抗性越狱尝试的脆弱性。为了解决这一挑战，本文定义并调查了LLMs的拒绝损失，然后提出了一种名为Gradient Cuff的方法来检测越狱尝试。Gradient Cuff利用拒绝损失地形图中观察到的独特特性，包括功能值及其光滑性，设计了一种有效的两步检测策略。

    arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
    
[^141]: DenseMamba: 具有密集隐藏连接的状态空间模型，用于高效大型语言模型

    DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models

    [https://arxiv.org/abs/2403.00818](https://arxiv.org/abs/2403.00818)

    DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。

    

    大型语言模型(LLMs)面临着由普遍使用的Transformer架构过高的计算和内存需求而带来的巨大挑战。而状态空间模型(SSM)是一种新型基础网络架构，具有较低的计算复杂度，但其性能尚未完全能与Transformer相媲美。本文引入了DenseSSM，一种增强SSMs中各层之间隐藏信息流动的新方法。通过有选择地将浅层隐藏状态集成到更深层，DenseSSM保留了对最终输出至关重要的细粒度信息。密集连接增强的DenseSSM仍保持了训练的并行性和推理效率。该方法可以广泛适用于RetNet和Mamba等各种SSM类型。在相似的模型大小下，DenseSSM取得了显著的改进，例如DenseRetNet比原始RetNet提高了高达5%的准确率。

    arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
    
[^142]: 通过离线技能扩散实现稳健策略学习

    Robust Policy Learning via Offline Skill Diffusion

    [https://arxiv.org/abs/2403.00225](https://arxiv.org/abs/2403.00225)

    提出了一种新颖的离线技能学习框架DuSkill，通过引导扩散模型生成通用技能，从而增强不同领域任务的策略学习鲁棒性。

    

    基于技能的强化学习方法在解决长时域任务中表现出了相当大的潜力，尤其是通过分层结构。这些技能是从离线数据集中无关任务地学习的，可以加快针对新任务的策略学习过程。然而，由于这些技能在不同领域中的应用仍受限于对数据集的固有依赖，当尝试通过强化学习为不同于数据集领域的目标领域学习基于技能的策略时，这一挑战就变得困难。在本文中，我们提出了一个新颖的离线技能学习框架DuSkill，它采用了引导扩散模型来生成从数据集中有限技能扩展出的通用技能，从而增强了不同领域任务的策略学习鲁棒性。具体来说，我们设计了一个引导扩散技能解码器，结合分层编码，以解开技能嵌入。

    arXiv:2403.00225v1 Announce Type: new  Abstract: Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embeddi
    
[^143]: GraphPub: 具有高可用性的差分隐私图生成

    GraphPub: Generation of Differential Privacy Graph with High Availability

    [https://arxiv.org/abs/2403.00030](https://arxiv.org/abs/2403.00030)

    提出了一种名为GraphPub的新型图边保护框架，通过反向学习和编码器-解码器机制，在保护图拓扑结构的同时保证数据的可用性基本不变。

    

    近年来，随着图神经网络（GNN）的快速发展，越来越多的图数据集被用于GNN任务。然而，当上游数据所有者发布图数据时，往往会存在许多隐私问题，因为许多现实世界的图数据包含像个人的朋友列表等敏感信息。差分隐私（DP）是一种常用的保护隐私的方法，但由于图数据的复杂拓扑结构，将DP应用在图上往往会影响GNN模型的消息传递和聚合，导致模型准确性下降。本文提出了一种新颖的图边保护框架GraphPub，可以保护图拓扑结构同时确保数据的可用性基本不变。通过反向学习和编码器-解码器机制，我们搜索一些对节点特征聚合没有太大负面影响的虚假边。

    arXiv:2403.00030v1 Announce Type: cross  Abstract: In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, 
    
[^144]: 在高阶不确定模型中使用高斯过程进行实时自适应安全关键控制

    Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models

    [https://arxiv.org/abs/2402.18946](https://arxiv.org/abs/2402.18946)

    本文提出了一种用于高阶不确定模型的实时自适应安全关键控制的方法，包括利用稀疏高斯过程进行在线学习和基于高阶控制屏障函数的安全过滤器。

    

    本文提出了一种自适应在线学习框架，用于具有不确定参数的系统，以确保在非平稳环境中进行安全关键控制。我们的方法包括两个阶段。初始阶段集中在一个新颖的稀疏高斯过程（GP）框架上。我们首先集成了一个遗忘因子来优化变分稀疏GP算法，从而增强其适应性。随后，使用特殊复合核对高斯模型的超参数进行训练，并通过更新一个源自新样本的孤独感应点来加强高斯模型的在线推理能力和计算效率，同时与学习到的超参数相结合。在第二阶段，我们提出了基于高阶控制屏障函数（HOCBFs）的安全过滤器，与先前训练的学习模型协同工作。通过利用第一阶段的复合核，我们有效地解决了

    arXiv:2402.18946v1 Announce Type: new  Abstract: This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse Gaussian process (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian model's online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from new samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high-order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address 
    
[^145]: 无需训练集的两阶段深度学习用于光谱数据去噪

    Training-set-free two-stage deep learning for Spectroscopic data de-noising

    [https://arxiv.org/abs/2402.18830](https://arxiv.org/abs/2402.18830)

    提出了一种无需训练集的两阶段深度学习方法，结合自适应先验和先进优化技术，实现了比先前工作快五倍的加速。

    

    去噪是光谱后处理过程中的重要步骤。先前的基于机器学习的方法快速但大多基于监督学习，需要一个训练集，在实际实验测量中可能成本较高。基于无监督学习的算法速度慢，并且需要多次迭代才能收敛。本文通过提出一种无需训练集的两阶段深度学习方法，弥合了这一差距。我们展示了之前方法中模糊固定输入的改进，引入自适应先验。结合更先进的优化技术，我们的方法比前人工作快五倍。理论上，我们研究了相应非凸线性问题的潜在状态，并且结果表明，这个问题对于一阶算法收敛具有良好的几何性质。

    arXiv:2402.18830v1 Announce Type: cross  Abstract: De-noising is a prominent step in the spectra post-processing procedure. Previous machine learning-based methods are fast but mostly based on supervised learning and require a training set that may be typically expensive in real experimental measurements. Unsupervised learning-based algorithms are slow and require many iterations to achieve convergence. Here, we bridge this gap by proposing a training-set-free two-stage deep learning method. We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior. Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work. Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign geometry for first-order algorithms to converge.
    
[^146]: FedUV: 异构联邦学习的均匀性和方差

    FedUV: Uniformity and Variance for Heterogeneous Federated Learning

    [https://arxiv.org/abs/2402.18372](https://arxiv.org/abs/2402.18372)

    提出了FedUV框架，通过引入两种正则化项，促使局部模型在异构分布数据中表现得更均匀和稳定

    

    联邦学习是一种训练神经网络的有希望的框架，能够处理广泛分布的数据。然而，性能很大程度上会随着异构分布的数据而下降。最近的研究表明，这是由于网络的最终层最容易出现局部偏差，一些研究发现通过将最终层冻结为正交分类器可以取得成功。我们通过对权重应用奇异值分解来研究分类器的训练动态，这是受到冻结权重导致奇异值恒定的观察启发的。我们发现在IID和非IID设置下训练时存在差异。基于这一发现，我们引入两种局部训练的正则化项，以持续模拟IID设置：（1）分类器的维度概率分布方差和（2）编码器表示的超球均匀性。这些正则化促使局部模型表现得好像在IID设置中一样。

    arXiv:2402.18372v1 Announce Type: cross  Abstract: Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting
    
[^147]: 使用平衡截断技术学习带有对角状态空间层的S4模型的方法

    Learning method for S4 with Diagonal State Space Layers using Balanced Truncation

    [https://arxiv.org/abs/2402.15993](https://arxiv.org/abs/2402.15993)

    一种用于处理长序列数据的边缘智能应用的S4模型的学习方法，利用平衡截断技术降低计算成本，并通过改进初始化过程和优化准确度和效率指标来超越传统训练模型。

    

    我们引入了一种新颖的学习方法，用于结构化状态空间序列（S4）模型并且加入了对角状态空间（DSS）层，这种方法专门设计用于处理边缘智能应用中的长序列数据，包括传感器数据分析和实时分析。该方法利用平衡截断技术，在控制理论中很常见，特别应用于DSS层以降低推断过程中的计算成本。通过利用减少模型的参数，我们改进了S4模型的初始化过程，在性能方面优于广泛使用的Skew-HiPPo初始化。数值实验表明，我们训练的带有DSS层的S4模型在准确度和效率指标上超越了传统训练的模型。此外，我们的观察结果显示一个积极的相关性：原始模型中的更高准确度一致导致使用我们方法训练的模型的准确度增加，这表明

    arXiv:2402.15993v1 Announce Type: new  Abstract: We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggest
    
[^148]: 学习通过盛光看见

    Learning to See Through Dazzle

    [https://arxiv.org/abs/2402.15919](https://arxiv.org/abs/2402.15919)

    通过波前编码的相位掩模和三明治生成对抗网络(SGAN)来恢复机器视觉受到激光盛光影响下的图像，结合了傅里叶特征表示以改进神经网络对高频图像细节的学习。

    

    机器视觉容易受到激光遮蔽的影响，其中强烈的激光光可以通过过度饱和或对传感器像素造成永久性损坏来使其失明并扭曲其对环境的感知。本文利用波前编码的相位掩模来散射激光光的能量，并引入一种三明治生成对抗网络(SGAN)，用于从复杂的图像退化中恢复图像，例如不同的激光诱导的图像饱和度、掩模诱导的图像模糊、未知的光照条件和各种噪声污染。SGAN架构通过在一个可学习的图像反卷积模块周围封装两个GANs结合识别和生成方法。此外，我们利用傅里叶特征表示来减少神经网络的频谱偏差，并提高其对高频图像细节的学习。端到端训练包括基于现实物理学的综合，生成一系列公开可用的大规模训练数据。

    arXiv:2402.15919v2 Announce Type: replace-cross  Abstract: Machine vision is susceptible to laser dazzle, where intense laser light can blind and distort its perception of the environment through oversaturation or permanent damage to sensor pixels. Here we employ a wavefront-coded phase mask to diffuse the energy of laser light and introduce a sandwich generative adversarial network (SGAN) to restore images from complex image degradations, such as varying laser-induced image saturation, mask-induced image blurring, unknown lighting conditions, and various noise corruptions. The SGAN architecture combines discriminative and generative methods by wrapping two GANs around a learnable image deconvolution module. In addition, we make use of Fourier feature representations to reduce the spectral bias of neural networks and improve its learning of high-frequency image details. End-to-end training includes the realistic physics-based synthesis of a large set of training data from publicly avai
    
[^149]: GraphEdit：用于图结构学习的大型语言模型

    GraphEdit: Large Language Models for Graph Structure Learning

    [https://arxiv.org/abs/2402.15183](https://arxiv.org/abs/2402.15183)

    本研究提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习复杂的图结构化数据中的节点关系，通过在图结构上进行指导调整，增强LLMs的推理能力，从而提高图结构学习的可靠性。

    

    图结构学习（GSL）致力于通过生成新颖的图结构来捕捉图结构数据中节点之间的固有依赖性和相互作用。本文提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习图结构化数据中复杂的节点关系。通过在图结构上进行指导调整，增强LLMs的推理能力，我们旨在克服显式图结构信息带来的挑战，并提高图结构学习的可靠性。

    arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
    
[^150]: SpanSeq：用于改进深度学习项目开发和评估的基于相似度的序列数据拆分方法

    SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects

    [https://arxiv.org/abs/2402.14482](https://arxiv.org/abs/2402.14482)

    SpanSeq 是一种用于生物数据序列的数据库分区方法，能够避免训练集和测试集之间的数据泄漏。

    

    过去几年中，在计算生物学中使用深度学习模型的增加很大，并且随着诸如自然语言处理等领域的当前进展，预计将进一步增加。本文提出了SpanSeq，这是一种适用于大多数生物序列（基因、蛋白质和基因组）的机器学习数据库分区方法，旨在避免数据集之间的数据泄漏。

    arXiv:2402.14482v1 Announce Type: new  Abstract: The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing. These models, although able to draw complex relations between input and target, are also largely inclined to learn noisy deviations from the pool of data used during their development. In order to assess their performance on unseen data (their capacity to generalize), it is common to randomly split the available data in development (train/validation) and test sets. This procedure, although standard, has lately been shown to produce dubious assessments of generalization due to the existing similarity between samples in the databases used. In this work, we present SpanSeq, a database partition method for machine learning that can scale to most biological sequences (genes, proteins and genomes) in order to avoid data leakage between sets. We a
    
[^151]: 使用AlphaTensor进行量子电路优化

    Quantum Circuit Optimization with AlphaTensor

    [https://arxiv.org/abs/2402.14396](https://arxiv.org/abs/2402.14396)

    使用基于深度强化学习的AlphaTensor-Quantum方法，在容错量子计算中优化T门数量，显著减少电路的T计数。

    

    实现容错量子计算机的一个关键挑战是电路优化。我们专注于容错量子计算中最昂贵的门（即T门），解决T计数优化问题，即最小化实现给定电路所需的T门数量。为实现这一目标，我们开发了基于深度强化学习的AlphaTensor-Quantum方法，利用优化T计数与张量分解之间的关系。与现有的T计数优化方法不同，AlphaTensor-Quantum能够整合关于量子计算的领域特定知识并利用工具，显著减少了优化电路的T计数。AlphaTensor-Quantum在一组算术基准上优于现有的T计数优化方法（即使在不使用工具的情况下进行比较）。值得注意的是，它发现了一种类似Karat的高效算法。

    arXiv:2402.14396v1 Announce Type: cross  Abstract: A key challenge in realizing fault-tolerant quantum computers is circuit optimization. Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition. Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic benchmarks (even when compared without making use of gadgets). Remarkably, it discovers an efficient algorithm akin to Karat
    
[^152]: 递归推测解码：通过无重复抽样加速LLM推理

    Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement

    [https://arxiv.org/abs/2402.14160](https://arxiv.org/abs/2402.14160)

    提出了递归推测解码(RSD)方法，通过无重复抽样最大化树的多样性，从而进一步加速LLM推理过程。

    

    推测解码是一种用于大型语言模型(LLMs)的推理加速方法，其中一个小型语言模型生成一个草稿令牌序列，该序列进一步由目标LLM并行验证。最近的研究通过建立草稿令牌树推进了这种方法，实现了优于单序列推测解码的性能。然而，这些工作在树的每个级别独立生成令牌，没有利用整个树的多样性。此外，尽管固定序列长度已经显示出更好的性能，但这些作品在固定目标计算资源上并没有进行实证研究，这是对于资源受限设备至关重要的。我们提出了递归推测解码(RSD)，一种新的基于树的方法，它对不重复抽样的草稿令牌进行最大化，并最大限度地实现了多样性。

    arXiv:2402.14160v1 Announce Type: cross  Abstract: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the dive
    
[^153]: 连续葡萄糖监测和维护的神经控制系统

    Neural Control System for Continuous Glucose Monitoring and Maintenance

    [https://arxiv.org/abs/2402.13852](https://arxiv.org/abs/2402.13852)

    引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。

    

    精确的葡萄糖水平管理对于糖尿病患者至关重要，可以避免严重并发症。本研究引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，利用微分预测控制。我们的系统受到复杂神经策略和可区分建模的指导，实时动态调整胰岛素输送，增强葡萄糖优化。这种端到端方法最大化效率，确保个性化护理和改善健康结果，如经验发现所证实。

    arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
    
[^154]: VLSP 2023综述--ComOM任务：越南产品评论的比较意见挖掘数据挑战

    Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews

    [https://arxiv.org/abs/2402.13613](https://arxiv.org/abs/2402.13613)

    该论文总结了VLSP 2023中ComOM任务的一个数据挑战，旨在推动自然语言处理领域通过开发从越南产品评论中提取比较意见的技术，参与者需提出能够提取比较"五元组"的模型并根据F1分数进行评估排名。

    

    本文提供了越南语产品评论比较意见挖掘共享任务（ComOM）的综合概述，该任务作为第十届越南语言和语音处理国际研讨会（VLSP 2023）的一部分举行。此共享任务的主要目标是通过开发能够有效从越南产品评论中提取比较意见的技术来推动自然语言处理领域的发展。参与者被挑战提出能够从比较句中熟练提取比较“五元组”的模型，包括主题、客体、方面、谓词和比较类型标签。我们构建了一个包含120个文档的人工标记数据集，其中包括7427个非比较句和1798个句子中的2468个比较。参与的模型将根据准确匹配宏平均的五元组F1分数进行评估和排名。

    arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
    
[^155]: 图神经网络中节点属性的攻击

    Attacks on Node Attributes in Graph Neural Networks

    [https://arxiv.org/abs/2402.12426](https://arxiv.org/abs/2402.12426)

    该研究通过基于特征的对抗攻击，针对图神经网络中的节点属性展开研究，发现使用Projected Gradient Descent的决策时攻击比使用Mean Node Embeddings和Graph Contrastive Learning策略的毒化攻击更加有效。

    

    图经常用来模型化现代社交媒体和文献应用中的复杂网络。我们的研究通过基于特征的对抗攻击，重点关注决策时攻击和毒化攻击，探究这些图的脆弱性。与Net Attack和Meta Attack等最先进模型针对节点属性和图结构不同，我们的研究专门针对节点属性。我们利用Hellaswag文本数据集以及图数据集Cora和CiteSeer进行分析，为评估提供了多样的基础。我们的发现表明，使用Projected Gradient Descent (PGD)的决策时攻击比采用Mean Node Embeddings和Graph Contrastive Learning策略的毒化攻击更具威力。这为图数据安全提供了见解，指出了图基模型最脆弱的地方，从而为开发工作提供信息。

    arXiv:2402.12426v1 Announce Type: cross  Abstract: Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes. For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies. This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the develo
    
[^156]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^157]: 在线局部虚发现率控制：一种资源分配方法

    Online Local False Discovery Rate Control: A Resource Allocation Approach

    [https://arxiv.org/abs/2402.11425](https://arxiv.org/abs/2402.11425)

    该研究提出了一种在线局部虚发现率控制的资源分配方法，实现了$O(\sqrt{T})$的后悔率，并指出这种后悔率在一般情况下是不可改进的。

    

    我们考虑在线局部虚发现率（FDR）控制问题，其中多个测试被顺序进行，目标是最大化总期望的发现次数。我们将问题形式化为一种在线资源分配问题，涉及接受/拒绝决策，从高层次来看，这可以被视为一个带有额外不确定性的在线背包问题，即随机预算补充。我们从一般的到达分布开始，并提出了一个简单的策略，实现了$O(\sqrt{T})$的后悔。我们通过展示这种后悔率在一般情况下是不可改进的来补充这一结果。然后我们将焦点转向离散到达分布。我们发现许多现有的在线资源分配文献中的重新解决启发式虽然在典型设置中实现了有界的损失，但可能会造成$\Omega(\sqrt{T})$甚至$\Omega(T)$的后悔。通过观察到典型策略往往太过

    arXiv:2402.11425v1 Announce Type: cross  Abstract: We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too op
    
[^158]: 使基于流匹配的零样本文本到语音系统自由地产生笑声

    Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like

    [https://arxiv.org/abs/2402.07383](https://arxiv.org/abs/2402.07383)

    本文提出了ELaTE，一种基于流匹配的零样本文本到语音系统，可以根据短音频提示以精确控制笑声时机和表情生成任何说话者的自然笑声。

    

    笑声是人类语音中最表达性和自然的一部分，传达着情感、社交暗示和幽默。然而，大多数文本到语音(TTS)系统缺乏产生逼真且合适的笑声的能力，限制了其应用和用户体验。虽然之前有工作生成了自然的笑声，但在控制生成的笑声的时机和多样性方面仍存在不足。在这项工作中，我们提出了ELaTE，一种可以基于短音频提示以精确控制笑声时机和表情的零样本TTS系统，可以产生任何说话者的自然笑声。具体而言，ELaTE通过音频提示来模仿声音特征，通过文本提示来指示所生成语音的内容，通过输入来控制笑声表情，可以是笑声的起始和结束时间，或包含要模仿的笑声的另外音频提示。我们的模型基于找到的技术基础进行了开发。

    Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
    
[^159]: GenSTL: 通过特征域的自回归生成实现通用稀疏轨迹学习

    GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains

    [https://arxiv.org/abs/2402.07232](https://arxiv.org/abs/2402.07232)

    GenSTL是一个通用的稀疏轨迹学习框架，通过自回归生成特征域来实现稀疏轨迹与密集轨迹之间的连接，从而消除了对大规模密集轨迹数据的依赖。

    

    轨迹是时间戳位置样本的序列。在稀疏轨迹中，位置样本的采样是不频繁的；尽管这种轨迹在现实世界中很常见，但要使用它们来实现高质量的与交通相关的应用程序是具有挑战性的。当前的方法要么假设轨迹是密集采样的并且经过准确的地图匹配，要么依赖于两阶段方案，从而产生次优的应用程序。为了扩展稀疏轨迹的效用，我们提出了一种新颖的稀疏轨迹学习框架GenSTL。该框架经过预训练以使用特征域的自回归生成形成稀疏轨迹与密集轨迹之间的连接。GenSTL可以直接应用于下游任务，或者可以先进行微调。通过这种方式，GenSTL消除了对大规模密集和地图匹配轨迹数据的依赖。其中包括精心设计的特征域编码层和分层的...

    Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical m
    
[^160]: 熵正则化的令牌级策略优化用于大规模语言模型

    Entropy-Regularized Token-Level Policy Optimization for Large Language Models

    [https://arxiv.org/abs/2402.06700](https://arxiv.org/abs/2402.06700)

    本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。

    

    大规模语言模型（LLMs）在交互式决策任务中表现出了智能代理的潜力。传统方法通常依赖于精心设计的提示、高质量的示例或额外的奖励模型进行上下文学习、监督微调或RLHF。强化学习（RL）提供了一种动态的解决方案，使LLMs能够通过直接与任务特定环境进行交互来克服这些依赖关系。尽管如此，它面临着重重困难：1）由于巨大的动作空间需要探索而产生的不稳定性；2）基于动作级奖励信号分配令牌级学分的挑战，导致最大化奖励和准确建模语料库数据之间的冲突。为了应对这些挑战，我们引入了熵正则化的令牌级策略优化（ETPO），这是一种专为在令牌级优化LLMs而设计的熵增强强化学习方法。ETPO的核心是我们的一种新颖的逐令牌软Bellman更新算法，

    Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
    
[^161]: VampPrior混合模型

    The VampPrior Mixture Model

    [https://arxiv.org/abs/2402.04412](https://arxiv.org/abs/2402.04412)

    本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。

    

    当前用于深度潜变量模型（DLVMs）的聚类先验需要预先定义聚类的数量，并且容易受到较差的初始化的影响。解决这些问题可以通过同时执行集成和聚类的方式极大地改进基于深度学习的scRNA-seq分析。我们将VampPrior（Tomczak和Welling，2018）调整为Dirichlet过程高斯混合模型，得到VampPrior混合模型（VMM），这是一种新颖的DLVM先验。我们提出了一个推理过程，交替使用变分推理和经验贝叶斯，以清楚地区分变分和先验参数。在基准数据集上使用VMM的变分自动编码器获得了极具竞争力的聚类性能。将VMM与广受欢迎的scRNA-seq集成方法scVI（Lopez等，2018）相结合，显著改善了其性能，并自动将细胞分组为具有生物意义的聚类。

    Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
    
[^162]: Audio Flamingo: 一种具备弱监督学习和对话能力的新型音频语言模型

    Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities

    [https://arxiv.org/abs/2402.01831](https://arxiv.org/abs/2402.01831)

    Audio Flamingo是一种新型音频语言模型，具备强大的音频理解能力、通过上下文学习和检索快速适应未见过的任务的能力以及强大的多轮对话能力，并且通过广泛的评估达到了最优成绩。

    

    对大型语言模型（LLMs）进行增强，以理解音频——包括非语音声音和非言语的语音——对LLMs的多样化真实世界应用至关重要。本文提出了一种名为Audio Flamingo的新型音频语言模型，具备强大的音频理解能力、通过上下文学习和检索快速适应未见过的任务的能力以及强大的多轮对话能力。我们引入了一系列训练技术、架构设计和数据策略，以增强我们的模型具备这些功能。广泛的音频理解任务评估验证了我们方法的有效性，并创造了新的最优成绩基准。

    Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks.
    
[^163]: 从PARIS到LE-PARIS：通过推荐系统和协作大型语言模型实现专利响应自动化

    From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models

    [https://arxiv.org/abs/2402.00421](https://arxiv.org/abs/2402.00421)

    本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。

    

    在专利审查中，对于及时和有效地回应审查意见（OAs）对于获得专利至关重要，然而过去的自动化和人工智能研究很少涉及到这一方面。为了弥补这一空白，我们的研究介绍了专利审查意见响应智能系统（PARIS）及其先进版本LE-PARIS。这些系统旨在加快专利律师在协作处理OA回应方面的效率。系统的关键特征包括构建OA主题数据库，开发响应模板，以及实施推荐系统和基于LLM的响应生成。我们的验证涉及使用USPTO Office Action数据库和律师与我们系统的长期交互数据进行的多范式分析，为期六年。通过五个研究，我们利用主题建模和提出的Delphi过程来检验OA主题的建设性（研究1和2），还有使用推荐系统和基于LLM的响应生成来提高回应质量（研究3和4），以及经过训练的LLM对于回应自动生成的可行性（研究5）。

    In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
    
[^164]: 为实现更高带宽的Allreduce操作而设计的Swing算法

    Swing: Short-cutting Rings for Higher Bandwidth Allreduce

    [https://arxiv.org/abs/2401.09356](https://arxiv.org/abs/2401.09356)

    Swing算法通过摆动在螺旋方向之间保持通信节点之间的低距离，从而在螺旋网络上优化Allreduce操作的性能，相比现有算法提升高达3倍。

    

    Allreduce集体操作在分布式系统上运行的工作负载的运行时中占据了相当大的比例。其性能的一个影响因素是通信节点之间的距离，特别是在螺旋网络等网络中，较长的距离意味着多个消息在同一链路上转发，从而降低了Allreduce的带宽。为了提高螺旋网络上的Allreduce性能，我们引入了Swing，一种新算法，通过在螺旋方向之间摆动来保持通信节点之间的低距离。我们的分析和实验评估显示，Swing在不同类型的螺旋和类似螺旋拓扑上，对于从32B到128MiB的向量，优于现有的Allreduce算法多达3倍。

    arXiv:2401.09356v2 Announce Type: replace-cross  Abstract: The allreduce collective operation accounts for a significant fraction of the runtime of workloads running on distributed systems. One factor determining its performance is the distance between communicating nodes, especially on networks like torus, where a higher distance implies multiple messages being forwarded on the same link, thus reducing the allreduce bandwidth. Torus networks are widely used on systems optimized for machine learning workloads (e.g., Google TPUs and Amazon Trainium devices), as well as on some of the Top500 supercomputers. To improve allreduce performance on torus networks we introduce Swing, a new algorithm that keeps a low distance between communicating nodes by swinging between torus directions. Our analysis and experimental evaluation show that Swing outperforms by up to 3x existing allreduce algorithms for vectors ranging from 32B to 128MiB, on different types of torus and torus-like topologies, re
    
[^165]: scDiffusion：使用扩散模型有条件地生成高质量的单细胞数据

    scDiffusion: conditional generation of high-quality single-cell data using diffusion model

    [https://arxiv.org/abs/2401.03968](https://arxiv.org/abs/2401.03968)

    scDiffusion是一种结合扩散模型和基础模型的生成模型，能准确地生成高质量的单细胞数据，具有受控条件下生成数据的能力。

    

    单细胞RNA测序（scRNA-seq）数据对于研究单细胞水平生命规律至关重要。然而，获取足够高质量的scRNA-seq数据仍然具有挑战性。为了缓解数据受限的问题，已经提出了生成模型来计算生成合成scRNA-seq数据。然而，目前模型生成的数据尚不够真实，特别是在需要生成受控条件数据时。与此同时，扩散模型展示了其在高保真度生成数据方面的能力，为scRNA-seq生成提供了新机会。在这项研究中，我们开发了scDiffusion，这是一个将扩散模型和基础模型相结合的生成模型，可以以受控条件生成高质量的scRNA-seq数据。我们设计了多个分类器来同时指导扩散过程，使scDiffusion能够在多个条件下生成数据。

    arXiv:2401.03968v2 Announce Type: replace-cross  Abstract: Single-cell RNA sequencing (scRNA-seq) data are important for studying the laws of life at single-cell level. However, it is still challenging to obtain enough high-quality scRNA-seq data. To mitigate the limited availability of data, generative models have been proposed to computationally generate synthetic scRNA-seq data. Nevertheless, the data generated with current models are not very realistic yet, especially when we need to generate data with controlled conditions. In the meantime, the Diffusion models have shown their power in generating data at high fidelity, providing a new opportunity for scRNA-seq generation.   In this study, we developed scDiffusion, a generative model combining diffusion model and foundation model to generate high-quality scRNA-seq data with controlled conditions. We designed multiple classifiers to guide the diffusion process simultaneously, enabling scDiffusion to generate data under multiple con
    
[^166]: 大型语言模型能否进行编辑？评估其遵循代码编辑指令的能力

    Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions

    [https://arxiv.org/abs/2312.12450](https://arxiv.org/abs/2312.12450)

    该研究评估了大型语言模型遵循代码编辑指令的能力，在指令式代码编辑任务上发现了开放和封闭模型之间的显著差距。

    

    大量研究集中在开发和评估大型语言模型用于各种代码合成任务。这些任务包括从自然语言指令中合成代码，从代码中合成测试，以及从代码中合成解释。与此相反，使用LLMs进行指令式代码编辑的行为研究不足。这些任务要求模型按照提供的提示更新一块代码。编辑指令可能要求添加或删除功能，描述错误并要求修复，要求不同类型的解决方案，或者其他常见的代码编辑任务。我们引入了一个精心设计的代码编辑任务基准，并用它评估了几个最先进的LLMs。我们的评估展示了当前最先进的开放和封闭模型之间的显著差距。例如，即使是GPT-3.5-Turbo也比最好的开放模型在编辑代码方面好了8.8%。

    arXiv:2312.12450v4 Announce Type: replace-cross  Abstract: A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks.   We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing cod
    
[^167]: RiskBench：一种基于场景的风险识别基准

    RiskBench: A Scenario-based Benchmark for Risk Identification

    [https://arxiv.org/abs/2312.01659](https://arxiv.org/abs/2312.01659)

    RiskBench是一种基于场景的风险识别基准，旨在解决当前使用独立数据集评估风险识别算法时的困难，为不同算法的直接比较和安全性能提升提供集体进展。

    

    智能驾驶系统旨在实现零碰撞的出行体验，需要跨学科努力来提高安全性能。本文关注风险识别，即识别和分析源自动态交通参与者和意外事件的风险的过程。尽管社区取得了显著进展，但当前对不同风险识别算法的评估使用独立数据集，导致难以直接比较，并阻碍了朝着安全性能提升的集体进展。为解决这一局限性，我们引入了RiskBench，这是一种用于风险识别的大规模基于场景的基准。我们设计了一个场景分类法和增强管道，以便系统地收集不同场景下的地面真实风险。我们评估了十种算法的能力，包括（1）检测和定位风险，（2）预测风险，以及（3）促进

    arXiv:2312.01659v2 Announce Type: replace-cross  Abstract: Intelligent driving systems aim to achieve a zero-collision mobility experience, requiring interdisciplinary efforts to enhance safety performance. This work focuses on risk identification, the process of identifying and analyzing risks stemming from dynamic traffic participants and unexpected events. While significant advances have been made in the community, the current evaluation of different risk identification algorithms uses independent datasets, leading to difficulty in direct comparison and hindering collective progress toward safety performance enhancement. To address this limitation, we introduce \textbf{RiskBench}, a large-scale scenario-based benchmark for risk identification. We design a scenario taxonomy and augmentation pipeline to enable a systematic collection of ground truth risks under diverse scenarios. We assess the ability of ten algorithms to (1) detect and locate risks, (2) anticipate risks, and (3) faci
    
[^168]: 跨不同化学领域的迁移学习：预训练于小分子和化学反应数据的深度学习模型用于有机材料虚拟筛选

    Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data

    [https://arxiv.org/abs/2311.18377](https://arxiv.org/abs/2311.18377)

    本研究展示了利用药物样小分子和化学反应数据库预训练BERT模型的潜力，从而增强其在有机材料虚拟筛选中的性能。

    

    机器学习由于其在有机材料虚拟筛选中的成本效益优于传统的计算密集型技术，正变得越来越受欢迎。然而，有机材料标记数据的稀缺性对于训练先进的机器学习模型构成了重要挑战。本研究展示了利用药物样小分子和化学反应数据库预训练BERT模型的潜力，从而增强其在有机材料虚拟筛选中的性能。通过将BERT模型与来自五个虚拟筛选任务的数据微调，使用USPTO-SMILES数据集预训练的版本在三个任务中实现了超过0.94的R2得分，其余两个任务的得分超过0.81。这一性能超过了在小分子或有机材料数据库上预训练的模型，并胜过了直接在虚拟筛选数据上训练的三个传统机器学习模型。

    arXiv:2311.18377v2 Announce Type: replace-cross  Abstract: Machine learning is becoming a preferred method for the virtual screening of organic materials due to its cost-effectiveness over traditional computationally demanding techniques. However, the scarcity of labeled data for organic materials poses a significant challenge for training advanced machine learning models. This study showcases the potential of utilizing databases of drug-like small molecules and chemical reactions to pretrain the BERT model, enhancing its performance in the virtual screening of organic materials. By fine-tuning the BERT models with data from five virtual screening tasks, the version pretrained with the USPTO-SMILES dataset achieved R2 scores exceeding 0.94 for three tasks and over 0.81 for two others. This performance surpasses that of models pretrained on the small molecule or organic materials databases and outperforms three traditional machine learning models trained directly on virtual screening da
    
[^169]: 一种有效的用于谱图神经网络的通用多项式基础

    An Effective Universal Polynomial Basis for Spectral Graph Neural Networks

    [https://arxiv.org/abs/2311.18177](https://arxiv.org/abs/2311.18177)

    通过研究图滤波器的多项式基础与图异质性度量之间的关联，本研究提出了一种自适应的异质性基础，可以有效地适应不同图之间的异质性程度。

    

    谱图神经网络（GNNs），也称为图滤波器，在异构图上越来越普遍。最优图滤波器依赖于拉普拉斯特征分解进行傅立叶变换。为了避免繁琐的计算，许多多项式滤波器通过利用不同的多项式来近似所需的图滤波器。然而，大多数多项式滤波器中的多项式是预定义的，并且在所有图上保持不变，无法适应不同图之间的多样的异质性程度。为了解决这个问题，我们首先通过彻底的理论分析调查所需图滤波器的多项式基础与图异质性度量之间的相关性。随后，我们通过合并图异质性度量开发出一种自适应的异质性基础。随后，我们将这种异质性基础与同质性基础相结合，创建一个通用的多项式基础。

    arXiv:2311.18177v2 Announce Type: replace  Abstract: Spectral Graph Neural Networks (GNNs), also referred to as graph filters have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert the prohibitive computations, numerous polynomial filters by leveraging distinct polynomials have been proposed to approximate the desired graph filters. However, polynomials in the majority of polynomial filters are predefined and remain fixed across all graphs, failing to accommodate the diverse heterophily degrees across different graphs. To tackle this issue, we first investigate the correlation between polynomial bases of desired graph filters and the degrees of graph heterophily via a thorough theoretical analysis. Afterward, we develop an adaptive heterophily basis by incorporating graph heterophily degrees. Subsequently, we integrate this heterophily basis with the homophily basis, creating a u
    
[^170]: TransNAS-TSAD：利用Transformer进行多目标神经架构搜索在时间序列异常检测中的应用

    TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection

    [https://arxiv.org/abs/2311.18061](https://arxiv.org/abs/2311.18061)

    TransNAS-TSAD是一个结合了Transformer架构和神经架构搜索的框架，通过NSGA-II算法优化，成功解决了时间序列数据的复杂性，具有高效的搜索空间探索和定制架构适应性，显著提高了异常检测模型的性能。

    

    在各行业实时数据收集激增的背景下，强调了需要在一元和多元时间序列数据中进行先进异常检测。本文介绍了TransNAS-TSAD，这是一个将Transformer架构与神经架构搜索（NAS）相结合的框架，通过NSGA-II算法优化。这种方法有效地解决了时间序列数据的复杂性，平衡了计算效率与检测准确性。我们的评估显示，TransNAS-TSAD由于其定制的架构适应性和高效的复杂搜索空间探索，超越了传统的异常检测模型，在不同数据情境中取得了显著改进。我们还引入了Efficiency-Accuracy-Complexity Score（EACS）作为评估模型性能的新指标，强调了准确性和计算资源之间的平衡。TransNAS-TSAD树立了一个新的标准

    arXiv:2311.18061v3 Announce Type: replace  Abstract: The surge in real-time data collection across various industries has underscored the need for advanced anomaly detection in both univariate and multivariate time series data. This paper introduces TransNAS-TSAD, a framework that synergizes the transformer architecture with neural architecture search (NAS), enhanced through NSGA-II algorithm optimization. This approach effectively tackles the complexities of time series data, balancing computational efficiency with detection accuracy. Our evaluation reveals that TransNAS-TSAD surpasses conventional anomaly detection models due to its tailored architectural adaptability and the efficient exploration of complex search spaces, leading to marked improvements in diverse data scenarios. We also introduce the Efficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model performance, emphasizing the balance between accuracy and computational resources. TransNAS-TSAD sets a n
    
[^171]: 动作槽：交通场景中多标签原子活动识别的视觉动作中心表示

    Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes

    [https://arxiv.org/abs/2311.17948](https://arxiv.org/abs/2311.17948)

    提出了一种名为Action-slot的方法，通过槽注意力学习视觉动作中心表示，在交通场景中实现了多标签原子活动识别。

    

    在本文中，我们研究了多标签原子活动识别。尽管在动作识别方面取得了显著进展，但由于对多个道路用户动作及其上下文信息的整体理解不足，识别原子活动仍然具有挑战性。我们引入了一种基于槽注意力的方法——动作槽，它学习视觉动作中心表示，捕捉了动作和上下文信息。我们的关键思想是设计能够注意到原子活动发生位置的动作槽，而无需明确的感知引导。为了进一步增强槽的注意力，我们引入了一个背景槽，与动作槽竞争，有助于训练过程中避免不必要地关注缺乏活动的背景区域。然而，现有数据集中不平衡的类分布阻碍了对稀有活动的评估。

    arXiv:2311.17948v1 Announce Type: cross  Abstract: In this paper, we study multi-label atomic activity recognition. Despite the notable progress in action recognition, it is still challenging to recognize atomic activities due to a deficiency in a holistic understanding of both multiple road users' motions and their contextual information. In this paper, we introduce Action-slot, a slot attention-based approach that learns visual action-centric representations, capturing both motion and contextual information. Our key idea is to design action slots that are capable of paying attention to regions where atomic activities occur, without the need for explicit perception guidance. To further enhance slot attention, we introduce a background slot that competes with action slots, aiding the training process in avoiding unnecessary focus on background regions devoid of activities. Yet, the imbalanced class distribution in the existing dataset hampers the assessment of rare activities. To addre
    
[^172]: 通过多样化合成和扩散模型减轻偏见

    Mitigating Biases with Diverse Ensembles and Diffusion Models

    [https://arxiv.org/abs/2311.16176](https://arxiv.org/abs/2311.16176)

    通过利用扩散概率模型（DPMs）生成新特征组合的图像，可以在集成模型中增加模型多样性，并减轻捷径偏见，而无需额外监督信号。

    

    数据中的虚假相关性，即多个线索可以预测目标标签，常常导致一种称为捷径偏见的现象，即模型依赖于错误的、易学的线索，而忽略可靠的线索。在这项工作中，我们提出了一种利用扩散概率模型（DPMs）的集成多样化框架，用于减轻捷径偏见。我们展示了在特定的训练间隔中，DPMs可以生成具有新特征组合的图像，即使在显示相关输入特征的样本上进行训练。我们利用这一关键属性通过集成不一致性生成合成反事实来增加模型的多样性。我们展示了DPM引导的多样化足以消除对主要捷径线索的依赖，无需额外的监督信号。我们进一步在几个多样化目标上在实证上量化其有效性，并最终展示了改进的泛化性能。

    arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
    
[^173]: 一个良好的特征提取器就是你在弱监督病理学切片分类中所需的一切

    A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification

    [https://arxiv.org/abs/2311.11772](https://arxiv.org/abs/2311.11772)

    在弱监督整个切片图像分类中，不同于常规认知的观念，研究发现省略染色标准化和图像增强并不会影响下游切片级别的分类性能，同时还能节省大量内存和计算资源。

    

    常规认为染色标准化是计算病理学流程中关键的预处理步骤。我们在弱监督的整个切片图像分类环境中对这一信念提出质疑，这一信念是由训练在多样化病理学数据集上进行自监督学习的强大特征提取器的出现所激励的。为此，我们对迄今为止公开可获得的病理学特征提取器进行了最全面的评估，涉及九个任务、五个数据集、三个下游架构和各种预处理设置中的8000多个训练运行。值得注意的是，我们发现忽略染色标准化和图像增强并不会损害下游切片级别的分类性能，同时还会在内存和计算上带来大量节省。通过使用一种新的评估指标，促进了相对下游性能的比较，我们确定了最好的公开可获得的提取器，并展示。

    arXiv:2311.11772v4 Announce Type: replace-cross  Abstract: Stain normalisation is thought to be a crucial preprocessing step in computational pathology pipelines. We question this belief in the context of weakly supervised whole slide image classification, motivated by the emergence of powerful feature extractors trained using self-supervised learning on diverse pathology datasets. To this end, we performed the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 8,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Notably, we find that omitting stain normalisation and image augmentations does not compromise downstream slide-level classification performance, while incurring substantial savings in memory and compute. Using a new evaluation metric that facilitates relative downstream performance comparison, we identify the best publicly available extractors, and sho
    
[^174]: 使用机器学习图像分割进行三维组织培养的非破坏性定量活力分析

    Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation

    [https://arxiv.org/abs/2311.09354](https://arxiv.org/abs/2311.09354)

    本研究提出了一种图像处理算法，用于在三维培养中量化细胞的活力，无需基于试剂的指示物，并且展示了其与人类专家的表现类似。

    

    确定不同细胞培养条件下细胞的集体活力通常依赖于平均色度指标，并且通常以简单的二进制读数报告。最近的研究将活力评估技术与基于图像的深度学习模型相结合，以自动化特征化细胞属性。然而，需要进一步发展活力测量技术，以评估可能的细胞状态连续性和对培养条件下的干扰响应。在这项工作中，我们展示了一种图像处理算法，用于在三维培养中量化细胞的活力，无需基于试剂的指示物。我们展示了我们的算法在一系列天数和培养基组成下在全孔图像中的表现与一对人类专家类似。为了展示潜在的实用性，我们进行了一项纵向研究，调查已知疗法的影响

    arXiv:2311.09354v2 Announce Type: replace-cross  Abstract: Ascertaining the collective viability of cells in different cell culture conditions has typically relied on averaging colorimetric indicators and is often reported out in simple binary readouts. Recent research has combined viability assessment techniques with image-based deep-learning models to automate the characterization of cellular properties. However, further development of viability measurements to assess the continuity of possible cellular states and responses to perturbation across cell culture conditions is needed. In this work, we demonstrate an image processing algorithm for quantifying cellular viability in 3D cultures without the need for assay-based indicators. We show that our algorithm performs similarly to a pair of human experts in whole-well images over a range of days and culture matrix compositions. To demonstrate potential utility, we perform a longitudinal study investigating the impact of a known therap
    
[^175]: 机器学习管道中的信息泄漏问题

    On Leakage in Machine Learning Pipelines

    [https://arxiv.org/abs/2311.04179](https://arxiv.org/abs/2311.04179)

    本论文旨在扩展对设计、实施和评估机器学习管道时导致信息泄漏的原因的理解，通过具体示例提供了各种可能在机器学习管道中出现的泄漏的全面概述和讨论。

    

    机器学习（ML）提供了强大的预测建模工具，其受欢迎程度源自于在物理学、市场营销、医疗保健等各个领域中应用样本级别的预测的承诺。然而，如果未经适当实施和评估，ML管道可能包含泄漏，通常导致过度乐观的性能估计并且无法推广到新数据。这可能对财务和社会产生严重负面影响。我们的目标是在设计、实施和评估ML管道时扩展与导致泄漏相关的原因的理解。通过具体示例说明，我们提供了在ML管道中可能出现的各种类型泄漏的综合概述和讨论。

    arXiv:2311.04179v2 Announce Type: replace-cross  Abstract: Machine learning (ML) provides powerful tools for predictive modeling. ML's popularity stems from the promise of sample-level prediction with applications across a variety of fields from physics and marketing to healthcare. However, if not properly implemented and evaluated, ML pipelines may contain leakage typically resulting in overoptimistic performance estimates and failure to generalize to new data. This can have severe negative financial and societal implications. Our aim is to expand understanding associated with causes leading to leakage when designing, implementing, and evaluating ML pipelines. Illustrated by concrete examples, we provide a comprehensive overview and discussion of various types of leakage that may arise in ML pipelines.
    
[^176]: 通过潜变量建模从单臂试验中估计治疗效果

    Estimating treatment effects from single-arm trials via latent-variable modeling

    [https://arxiv.org/abs/2311.03002](https://arxiv.org/abs/2311.03002)

    通过深度潜变量模型和摊销变分推断，我们提出了一种可用于单臂试验的治疗效果估计方法，可以处理缺失的协变量观察，实现患者匹配或直接治疗效果估计。

    

    随机对照试验（RCTs）被接受为治疗效果估计的标准，但由于伦理原因和成本过高而不可行。单臂试验，所有患者属于治疗组，可能是一个可行的替代方案，但需要访问外部对照组。我们提出了一个可识别的深度潜变量模型，可以用于这种情况，并且还可以通过建模结构化缺失模式来考虑缺失的协变量观察。我们的方法使用摊销变分推断来学习既特定于组又可识别的共享潜在表示，随后可用于（i）患者匹配，如果治疗组的治疗结果不可用，或用于（ii）假定两组均有结果可用的直接治疗效果估计。我们在一个公共基准和一个由已发表的RCT组成的数据集上评估了该模型。

    arXiv:2311.03002v2 Announce Type: replace  Abstract: Randomized controlled trials (RCTs) are the accepted standard for treatment effect estimation but they can be infeasible due to ethical reasons and prohibitive costs. Single-arm trials, where all patients belong to the treatment group, can be a viable alternative but require access to an external control group. We propose an identifiable deep latent-variable model for this scenario that can also account for missing covariate observations by modeling their structured missingness patterns. Our method uses amortized variational inference to learn both group-specific and identifiable shared latent representations, which can subsequently be used for {\em (i)} patient matching if treatment outcomes are not available for the treatment group, or for {\em (ii)} direct treatment effect estimation assuming outcomes are available for both groups. We evaluate the model on a public benchmark as well as on a data set consisting of a published RCT s
    
[^177]: 快速双曲决策树算法

    Fast hyperboloid decision tree algorithms

    [https://arxiv.org/abs/2310.13841](https://arxiv.org/abs/2310.13841)

    快速双曲决策树算法hyperDT通过利用内积将欧几里得决策树算法调整到双曲空间，消除了对计算密集型黎曼优化和数值不稳定操作的需求。

    

    双曲几何在机器学习领域日益受到关注，因为它能有效捕捉现实世界数据中的层次结构。双曲空间中的邻域呈指数增长，提供了显著优势，并在各种应用中持续提供最先进的结果。然而，双曲分类器往往面临计算挑战。依赖黎曼优化方法的方法通常表现缓慢，其根源在于在黎曼流形上的操作对计算的增加需求。作为对这些挑战的应对，我们提出了hyperDT，这是决策树算法向双曲空间的新扩展。关键在于，hyperDT消除了对计算密集型黎曼优化、数值不稳定的指数和对数映射或点之间的成对比较的需求，通过利用内积将欧几里得决策树算法调整到双曲空间。

    arXiv:2310.13841v2 Announce Type: replace  Abstract: Hyperbolic geometry is gaining traction in machine learning for its effectiveness at capturing hierarchical structures in real-world data. Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial advantages and consistently deliver state-of-the-art results across diverse applications. However, hyperbolic classifiers often grapple with computational challenges. Methods reliant on Riemannian optimization frequently exhibit sluggishness, stemming from the increased computational demands of operations on Riemannian manifolds. In response to these challenges, we present hyperDT, a novel extension of decision tree algorithms into hyperbolic space. Crucially, hyperDT eliminates the need for computationally intensive Riemannian optimization, numerically unstable exponential and logarithmic maps, or pairwise comparisons between points by leveraging inner products to adapt Euclidean decision tree algorithms to hyperbolic sp
    
[^178]: XRMDN: 一种用于高波动率短期概率骑手需求预测的扩展循环混合密度网络

    XRMDN: An Extended Recurrent Mixture Density Network for Short-Term Probabilistic Rider Demand Forecasting with High Volatility

    [https://arxiv.org/abs/2310.09847](https://arxiv.org/abs/2310.09847)

    XRMDN是一种扩展循环混合密度网络，专门用于处理移动出行系统中高波动率的短期概率骑手需求预测，能够灵活整合内生和外生数据。

    

    在移动出行系统领域，骑手需求预测是运营决策和系统优化的基石。传统的预测方法主要产生点估计，从而忽略了需求预测中固有的不确定性。此外，出行需求受内生和外生因素的深刻影响，导致高动态波动性。这种波动性显著削弱了传统时间序列预测方法的有效性。为此，我们提出了一种扩展的循环混合密度网络（XRMDN），这是一个新颖的深度学习框架，旨在解决这些挑战。XRMDN利用一个复杂的架构来通过相关模块处理需求残差和方差，从而灵活地整合内生和外生数据。这种架构具有权重内的循环连接，m

    arXiv:2310.09847v2 Announce Type: replace  Abstract: In the realm of Mobility-on-Demand (MoD) systems, the forecasting of rider demand is a cornerstone for operational decision-making and system optimization. Traditional forecasting methodologies primarily yield point estimates, thereby neglecting the inherent uncertainty within demand projections. Moreover, MoD demand levels are profoundly influenced by both endogenous and exogenous factors, leading to high and dynamic volatility. This volatility significantly undermines the efficacy of conventional time series forecasting methods. In response, we propose an Extended Recurrent Mixture Density Network (XRMDN), a novel deep learning framework engineered to address these challenges. XRMDN leverages a sophisticated architecture to process demand residuals and variance through correlated modules, allowing for the flexible incorporation of endogenous and exogenous data. This architecture, featuring recurrent connections within the weight, m
    
[^179]: 一种用于无线通信网络中能源高效联邦学习的安全深度强化学习方法

    A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks

    [https://arxiv.org/abs/2308.10664](https://arxiv.org/abs/2308.10664)

    提出一种新的安全深度强化学习方法，利用惩罚函数在训练时惩罚违反环境约束的策略，以确保无线通信网络中能源高效联邦学习的总能耗最小化。

    

    向着人工智能（AI）赋能的无线网络新时代迈进，行业和学术界对AI的环境影响提出了关注。联邦学习（FL）作为一种关键的隐私保护的分散式AI技术已经出现。尽管目前在FL方面已经做出努力，但其环境影响仍然是一个尚未解决的问题。为了最小化FL过程的总能耗，我们提出了编排参与设备的计算和通信资源，以最小化所需的总能量，同时保证模型的一定性能。为此，我们提出了一种Soft Actor Critic Deep Reinforcement Learning (DRL)解决方案，在训练过程中引入了一种惩罚函数，惩罚违反环境约束的策略，有助于实现安全的RL过程。

    arXiv:2308.10664v3 Announce Type: replace-cross  Abstract: Progressing towards a new era of Artificial Intelligence (AI) - enabled wireless networks, concerns regarding the environmental impact of AI have been raised both in industry and academia. Federated Learning (FL) has emerged as a key privacy preserving decentralized AI technique. Despite efforts currently being made in FL, its environmental impact is still an open problem. Targeting the minimization of the overall energy consumption of an FL process, we propose the orchestration of computational and communication resources of the involved devices to minimize the total energy required, while guaranteeing a certain performance of the model. To this end, we propose a Soft Actor Critic Deep Reinforcement Learning (DRL) solution, where a penalty function is introduced during training, penalizing the strategies that violate the constraints of the environment, and contributing towards a safe RL process. A device level synchronization 
    
[^180]: 机器遗忘：解决方案与挑战

    Machine Unlearning: Solutions and Challenges

    [https://arxiv.org/abs/2308.07061](https://arxiv.org/abs/2308.07061)

    本文提供了对机器遗忘解决方案的全面分类和分析，明确了完全遗忘方法和近似遗忘方法，并讨论了它们的优势和局限性，提出了推进机器遗忘的未来方向。

    

    机器学习模型可能无意中记住敏感、未经授权或恶意数据，存在隐私泄露、安全漏洞和性能降级的风险。为了解决这些问题，机器遗忘已经成为一种重要的技术，可以有选择地消除特定训练数据点对训练模型的影响。本文对机器遗忘中的解决方案进行了全面分类和分析。我们将现有解决方案分为完全遗忘方法和有效减少数据影响的近似遗忘方法。通过全面回顾解决方案，我们确定并讨论它们的优势和局限性。此外，我们提出了未来的发展方向，以推进机器遗忘并将其建立为值得信赖和适应性机器学习模型的重要能力。本文为研究人员提供了一份路线图。

    arXiv:2308.07061v2 Announce Type: replace-cross  Abstract: Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy breaches, security vulnerabilities, and performance degradation. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of the solutions in machine unlearning. We categorize existing solutions into exact unlearning approaches that remove data influence thoroughly and approximate unlearning approaches that efficiently minimize data influence. By comprehensively reviewing solutions, we identify and discuss their strengths and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning models. This paper provides researchers with a roadmap
    
[^181]: 使用Masked Transformers快速训练扩散模型

    Fast Training of Diffusion Models with Masked Transformers

    [https://arxiv.org/abs/2306.09305](https://arxiv.org/abs/2306.09305)

    该论文提出了一种使用Masked Transformers快速训练扩散模型的方法，首次利用Masked training显著降低了模型的训练成本，并引入了不对称的编码器-解码器架构和辅助任务，以提升对全patches的长程理解。

    

    我们提出了一种有效的方法，使用Masked Transformers来训练大型扩散模型。尽管Masked Transformers已经被广泛探索用于表示学习，在视觉领域中，它们在生成学习方面的应用却较少被探讨。我们的工作是第一个利用Masked training显著降低扩散模型的训练成本。具体地，在训练过程中，我们随机屏蔽扩散输入图像中高比例（例如50%）的patches。为了进行Masked training，我们引入了一个不对称的编码器-解码器架构，其中包括仅在未屏蔽patches上运行的transformer编码器和在全部patches上运行的轻量级transformer解码器。为了提升对全patches的长程理解，我们加入了一个辅助任务，即重构屏蔽patches，这是为了denoising score matching目标学习未屏蔽patches的score。我们在ImageNet-256x256和ImageNet-512x...上进行了实验。

    arXiv:2306.09305v2 Announce Type: replace-cross  Abstract: We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x
    
[^182]: 分层对比学习增强异构图神经网络

    Hierarchical Contrastive Learning Enhanced Heterogeneous Graph Neural Network

    [https://arxiv.org/abs/2304.12228](https://arxiv.org/abs/2304.12228)

    提出了一种名为HeCo的新型共同对比学习机制，用于自监督HGNNs，采用交叉视图对比机制以学习节点嵌入。

    

    异构图神经网络（HGNNs）作为一种新兴技术已经展示出在处理异构信息网络（HIN）方面具有卓越的能力。然而，大多数HGNNs遵循半监督学习方式，这明显限制了它们在现实中的广泛应用，因为真实应用中标签通常很少。最近，自监督方法中的对比学习成为最令人兴奋的学习范式之一，并在没有标签时显示出巨大潜力。在本文中，我们研究了自监督HGNNs的问题，并提出了一种新颖的HGNNs的共同对比学习机制，称为HeCo。不同于传统的对比学习只关注对比正负样本，HeCo采用了交叉视图对比机制。具体而言，提出了HIN的两个视图（网络架构和元路径视图）以学习节点嵌入，从而捕捉局部和高阶结构。

    arXiv:2304.12228v2 Announce Type: replace  Abstract: Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-view contrastive mechanism. Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both of local and high-order structures sim
    
[^183]: 无需力/扭矩传感器的精细机器人操作

    Fine Robotic Manipulation without Force/Torque Sensor

    [https://arxiv.org/abs/2301.13413](https://arxiv.org/abs/2301.13413)

    利用神经网络方法，不需要力/扭矩传感器，通过特定训练数据结构准确估计外部力矩。

    

    力传感和力控对于许多工业应用至关重要。通常情况下，一个6轴力/扭矩（F/T）传感器被安装在机器人的手腕和末端执行器之间，以测量环境对机器人施加力和扭矩（外部力矩）。虽然典型的6轴F/T传感器可以提供高度准确的测量结果，但它昂贵且容易漂移和受到外部冲击。现有方法旨在仅使用机器人内部信号来估计外部力矩的范围有限：例如，力矩估计的准确性主要在自由空间运动和简单接触情况下得到验证，而不是像组装这样需要高精度力控的任务。在这里，我们提出了一种基于神经网络的方法，并认为通过特别关注训练数据结构，可以准确地估计各种情况下的外部力矩。

    arXiv:2301.13413v2 Announce Type: replace-cross  Abstract: Force Sensing and Force Control are essential to many industrial applications. Typically, a 6-axis Force/Torque (F/T) sensor is mounted between the robot's wrist and the end-effector in order to measure the forces and torques exerted by the environment onto the robot (the external wrench). Although a typical 6-axis F/T sensor can provide highly accurate measurements, it is expensive and vulnerable to drift and external impacts. Existing methods aiming at estimating the external wrench using only the robot's internal signals are limited in scope: for example, wrench estimation accuracy was mostly validated in free-space motions and simple contacts as opposed to tasks like assembly that require high-precision force control. Here we present a Neural Network based method and argue that by devoting particular attention to the training data structure, it is possible to accurately estimate the external wrench in a wide range of scenar
    
[^184]: 具有显式行为密度的约束策略优化用于线下强化学习

    Constrained Policy Optimization with Explicit Behavior Density for Offline Reinforcement Learning

    [https://arxiv.org/abs/2301.12130](https://arxiv.org/abs/2301.12130)

    提出了一种具有显式行为密度的约束策略优化方法，通过使用flow-GAN模型来准确识别安全区域，实现了更少保守的学习策略。

    

    由于无法与环境交互，线下强化学习（RL）方法面临着估计分布之外点（OOD）的挑战。现有方法要么控制策略以排除OOD动作，要么使$Q$函数变得悲观。然而，这些方法可能过于保守或无法准确识别OOD区域。为了克服这个问题，我们提出了一种利用flow-GAN模型显式估计行为策略密度的约束策略优化（CPED）方法。通过估计显式密度，CPED可以准确识别安全区域并在该区域内进行优化，从而得到更少保守的学习策略。我们进一步为flow-GAN估计器和CPED的性能保证提供了理论结果，表明CPED可以找到最优的$Q$函数值。

    arXiv:2301.12130v2 Announce Type: replace  Abstract: Due to the inability to interact with the environment, offline reinforcement learning (RL) methods face the challenge of estimating the Out-of-Distribution (OOD) points. Existing methods for addressing this issue either control policy to exclude the OOD action or make the $Q$ function pessimistic. However, these methods can be overly conservative or fail to identify OOD areas accurately. To overcome this problem, we propose a Constrained Policy optimization with Explicit Behavior density (CPED) method that utilizes a flow-GAN model to explicitly estimate the density of behavior policy. By estimating the explicit density, CPED can accurately identify the safe region and enable optimization within the region, resulting in less conservative learning policies. We further provide theoretical results for both the flow-GAN estimator and performance guarantee for CPED by showing that CPED can find the optimal $Q$-function value. Empirically,
    
[^185]: 知识蒸馏在联邦边缘学习中的应用：一项调查

    Knowledge Distillation in Federated Edge Learning: A Survey

    [https://arxiv.org/abs/2301.05849](https://arxiv.org/abs/2301.05849)

    研究调查了知识蒸馏在联邦边缘学习中的应用，讨论了现有方法的局限性和问题，并提供了实际部署的指导。

    

    随着对智能服务和移动设备以及物联网设备隐私保护要求的增加，促使了联邦边缘学习（FEL）的广泛应用，其中设备在不共享私人数据的情况下协同训练设备上的机器学习（ML）模型。由于设备硬件、用户行为和网络基础设施的多样性，FEL的算法设计面临着与资源、个性化和网络环境相关的挑战。幸运的是，知识蒸馏（KD）已被利用作为解决FEL中上述挑战的重要技术。本文调查了KD应用于FEL的研究成果，讨论了现有基于KD的FEL方法的局限性和未解决的问题，并为它们的实际部署提供指导。

    arXiv:2301.05849v3 Announce Type: replace  Abstract: The increasing demand for intelligent services and privacy protection of mobile and Internet of Things (IoT) devices motivates the wide application of Federated Edge Learning (FEL), in which devices collaboratively train on-device Machine Learning (ML) models without sharing their private data. Limited by device hardware, diverse user behaviors and network infrastructure, the algorithm design of FEL faces challenges related to resources, personalization and network environments. Fortunately, Knowledge Distillation (KD) has been leveraged as an important technique to tackle the above challenges in FEL. In this paper, we investigate the works that KD applies to FEL, discuss the limitations and open problems of existing KD-based FEL approaches, and provide guidance for their real deployment.
    
[^186]: 从动态图中学习有向无环图结构

    Directed Acyclic Graph Structure Learning from Dynamic Graphs

    [https://arxiv.org/abs/2211.17029](https://arxiv.org/abs/2211.17029)

    在动态图中，我们研究了节点特征生成机制的学习问题，通过同时估计节点特征之间的同时关系和时滞交互关系来构建有向无环图，有效地描述了特征生成过程

    

    估计特征（变量）的有向无环图（DAG）结构在揭示潜在数据生成过程和提供各种应用中的因果洞见方面发挥着至关重要的作用。虽然已经有许多关于不同数据类型结构学习的研究，但动态图上的结构学习尚未被探索，因此我们研究了这种无处不在的动态图数据上的节点特征生成机制的学习问题。在动态图中，我们提出同时估计节点特征之间的同时关系和时滞交互关系。这两种关系形成一个DAG，能够有效地简洁描述特征生成过程。为了学习这样的DAG，我们将学习问题建模为一个连续的基于分数的优化问题，其中包括一个可微分的得分函数来衡量有效性

    arXiv:2211.17029v2 Announce Type: replace-cross  Abstract: Estimating the structure of directed acyclic graphs (DAGs) of features (variables) plays a vital role in revealing the latent data generation process and providing causal insights in various applications. Although there have been many studies on structure learning with various types of data, the structure learning on the dynamic graph has not been explored yet, and thus we study the learning problem of node feature generation mechanism on such ubiquitous dynamic graph data. In a dynamic graph, we propose to simultaneously estimate contemporaneous relationships and time-lagged interaction relationships between the node features. These two kinds of relationships form a DAG, which could effectively characterize the feature generation process in a concise way. To learn such a DAG, we cast the learning problem as a continuous score-based optimization problem, which consists of a differentiable score function to measure the validity 
    
[^187]: 增量空间和谱神经算子学习用于解决大规模PDE的论文

    Incremental Spatial and Spectral Learning of Neural Operators for Solving Large-Scale PDEs

    [https://arxiv.org/abs/2211.15188](https://arxiv.org/abs/2211.15188)

    提出了增量傅里叶神经算子（iFNO），通过逐步增加频率模式的数量来解决训练FNO中的两大挑战：计算高分辨率输入的傅里叶变换消耗大，选择谱层中的相关频率集合困难

    

    Fourier神经算子（FNO）提供了一种解决挑战性偏微分方程（PDE）问题的原则方法，例如湍流流动。FNO的核心是一个谱层，利用傅里叶域中的收敛表示，并学习固定一组频率上的权重。然而，训练FNO存在两个主要挑战，特别是在大规模、高分辨率的应用中：（i）计算高分辨率输入的傅里叶变换计算量巨大，但是必要的，因为解决许多PDE问题（如流体流动）需要细节，（ii）在谱层中选择相关频率集合具有挑战性，太多模式会导致过度拟合，而太少会导致不足拟合。为了解决这些问题，我们引入了增量傅里叶神经算子（iFNO），它逐渐增加用于计算的频率模式数量

    arXiv:2211.15188v4 Announce Type: replace  Abstract: Fourier Neural Operators (FNO) offer a principled approach to solving challenging partial differential equations (PDE) such as turbulent flows. At the core of FNO is a spectral layer that leverages a discretization-convergent representation in the Fourier domain, and learns weights over a fixed set of frequencies. However, training FNO presents two significant challenges, particularly in large-scale, high-resolution applications: (i) Computing Fourier transform on high-resolution inputs is computationally intensive but necessary since fine-scale details are needed for solving many PDEs, such as fluid flows, (ii) selecting the relevant set of frequencies in the spectral layers is challenging, and too many modes can lead to overfitting, while too few can lead to underfitting. To address these issues, we introduce the Incremental Fourier Neural Operator (iFNO), which progressively increases both the number of frequency modes used by the
    
[^188]: 相信您的 $\nabla$: 基于梯度的干预目标定位用于因果发现

    Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery

    [https://arxiv.org/abs/2211.13715](https://arxiv.org/abs/2211.13715)

    提出了一种基于梯度的干预目标定位方法，GIT，在因果发现中能够通过信号梯度估计器降低干预次数，在低数据量情况下优于竞争基线。

    

    从数据中推断因果结构是科学中一项具有基础重要性的挑战性任务。观测数据通常不足以唯一确定系统的因果结构。虽然进行干预（即实验）可以改善可识别性，但这些样本通常难以获得且成本高昂。因此，因果发现的实验设计方法旨在通过估计最具信息性的干预目标来最小化干预次数。在这项工作中，我们提出了一种新颖的基于梯度的干预目标定位方法，简称为GIT，它‘相信’了基于梯度的因果发现框架的梯度估计器，以提供干预采集函数的信号。我们在模拟和真实世界数据集上进行了大量实验，并证明GIT在低数据量情况下表现与竞争基线相当，甚至在某些情况下超越它们。

    arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
    
[^189]: 利用算法公平性减轻黑盒属性推断攻击

    Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks

    [https://arxiv.org/abs/2211.10209](https://arxiv.org/abs/2211.10209)

    通过使用自适应阈值来考虑数据集中敏感属性类别不平衡，我们提出了一种实用且有效的属性推断攻击方法。

    

    机器学习（ML）模型已经被部署在高风险应用中，例如医疗保健和刑事司法。先前的研究表明，ML模型容易受到属性推断攻击的影响，攻击者利用一些背景知识训练ML攻击模型，通过利用可区分的模型预测来推断敏感属性。然而，一些先前的属性推断攻击对攻击者的背景知识（例如，敏感属性的边际分布）有很强的假设，并且不会带来比统计推断更多的隐私风险。此外，先前的攻击并未考虑来自真实应用程序的数据集中敏感属性的类别不平衡（例如，种族和性别）。在本文中，我们提出了一个考虑此不平衡的实用且有效的属性推断攻击，使用攻击模型预测的自适应阈值。我们对我们的方法进行了全面评估。

    arXiv:2211.10209v2 Announce Type: replace  Abstract: Machine learning (ML) models have been deployed for high-stakes applications, e.g., healthcare and criminal justice. Prior work has shown that ML models are vulnerable to attribute inference attacks where an adversary, with some background knowledge, trains an ML attack model to infer sensitive attributes by exploiting distinguishable model predictions. However, some prior attribute inference attacks have strong assumptions about adversary's background knowledge (e.g., marginal distribution of sensitive attribute) and pose no more privacy risk than statistical inference. Moreover, none of the prior attacks account for class imbalance of sensitive attribute in datasets coming from real-world applications (e.g., Race and Sex). In this paper, we propose an practical and effective attribute inference attack that accounts for this imbalance using an adaptive threshold over the attack model's predictions. We exhaustively evaluate our propo
    
[^190]: 核规范化卷积网络

    Kernel Normalized Convolutional Networks

    [https://arxiv.org/abs/2205.10089](https://arxiv.org/abs/2205.10089)

    提出了核规范化卷积网络（KNConvNets），通过替代BatchNorm层，在图像分类和语义分割中实现比BatchNorm更高或相媲美的性能，同时也在非私密和差分私密训练中显著优于基于层和组规范化的竞争对手。

    

    现有的卷积神经网络体系结构通常依赖于批量规范化（BatchNorm）来有效训练模型。然而，BatchNorm 在小批量大小时性能较差，并且不适用于差分隐私。为了解决这些限制，我们提出了核规范化（KernelNorm）和核规范化卷积层，并将它们作为主要构建模块融入核规范化卷积网络（KNConvNets）中。我们实现了相应于最先进的ResNets的KNConvNets，同时放弃了BatchNorm层。通过大量实验，我们展示了KNConvNets在图像分类和语义分割中相比于BatchNorm对应的模型能实现更高或相媲美的性能。在非私密和差分私密训练中，它们还显著优于基于层和组规范化的批量独立竞争对手。

    arXiv:2205.10089v4 Announce Type: replace  Abstract: Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private train
    
[^191]: 在分布外图上推广图神经网络

    Generalizing Graph Neural Networks on Out-Of-Distribution Graphs

    [https://arxiv.org/abs/2111.10657](https://arxiv.org/abs/2111.10657)

    提出了一个名为StableGNN的通用因果表示框架，通过提取高级图表示并利用因果推断的区分能力，帮助模型在分布外图上实现稳定的泛化能力。

    

    图神经网络（GNNs）在没有考虑训练和测试图之间的分布差异的情况下提出，导致GNNs在分布外（OOD）设置上的泛化能力下降。这种退化的根本原因是大多数GNNs是基于独立同分布假设开发的。在这种设置中，GNNs倾向于利用训练集中存在的细微统计相关性进行预测，即使这是一种伪相关性。然而，这种伪相关性在测试环境中可能会改变，导致GNNs失败。因此，消除伪相关性的影响对于稳定的GNNs至关重要。为此，我们提出了一个名为StableGNN的通用因果表示框架。主要思想是首先从图数据中提取高级表示，然后借助因果推断的区分能力来帮助模型获得稳定的预测效果。

    arXiv:2111.10657v3 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get ri
    
[^192]: 基于神经活动的剪枝算法：神经网络Relief

    Neural network relief: a pruning algorithm based on neural activity

    [https://arxiv.org/abs/2109.10795](https://arxiv.org/abs/2109.10795)

    提出了一种基于神经活动的迭代剪枝策略，通过引入简单的重要性评分指标来停用无关紧要的连接，解决了DNNs中的参数过多问题并实现了显著的参数压缩，在MNIST、CIFAR-10/100和Tiny-ImageNet数据集上达到了可比较的性能。

    

    当前深度神经网络（DNNs）存在参数过多，在每个任务的推断过程中使用大部分神经元连接。然而，人脑为不同任务发展了专门的区域，并仅使用其神经连接的一小部分进行推断。我们提出了一种迭代剪枝策略，引入了一种简单的重要性评分指标，将无关紧要的连接停用，解决了DNNs中的参数过多问题，并调节了神经元的激活模式。旨在找到解决给定任务的最小连接数，同时保持可比较的准确性，即一个更简单的子网络。我们在MNIST上的LeNet架构获得了可比较的性能，并在CIFAR-10/100和Tiny-ImageNet上相比最先进的算法实现了更高程度的参数压缩，也在考虑的两种不同优化器--Adam和SGD--上表现良好。

    arXiv:2109.10795v3 Announce Type: replace  Abstract: Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the firing patterns. The aim is to find the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and significantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two different optimizers considered -- Adam and SGD. 
    
[^193]: 新兴趋势：从模型融合到联邦 X 学习的研究

    Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning

    [https://arxiv.org/abs/2102.12920](https://arxiv.org/abs/2102.12920)

    联邦学习作为一种新的学习范式，不仅有助于改进算法和模型融合方法，还展示了与其他学习框架相结合的潜力，特别是在联邦 X 学习中，为多任务学习、元学习、迁移学习等提供了新的研究视角。

    

    联邦学习是一种新的学习范式，通过多方计算和模型聚合来解耦数据收集与模型训练。作为一种灵活的学习设置，联邦学习有潜力与其他学习框架整合。本文针对联邦学习与其他学习算法的结合进行了重点调查。具体来说，我们探索了各种学习算法以改进原始的联邦平均算法，并回顾了模型融合方法，例如自适应聚合、正则化、聚类方法和贝叶斯方法。根据新兴趋势，我们还讨论了联邦学习与其他学习范式的交集，称为联邦 X 学习，其中 X 包括多任务学习、元学习、迁移学习、无监督学习和强化学习。这项调查回顾了现状、挑战和未来方向。

    arXiv:2102.12920v4 Announce Type: replace  Abstract: Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed federated X learning, where X includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. This survey reviews the state of the art, challenges, and future directions.
    
[^194]: 通过与解释进行交互，使深度神经网络出于正确的科学原因

    Making deep neural networks right for the right scientific reasons by interacting with their explanations

    [https://arxiv.org/abs/2001.05371](https://arxiv.org/abs/2001.05371)

    通过“解释交互学习”(XIL)学习设置，研究者可以与深度神经网络进行交互，有助于避免其利用混淆因素而导致的高性能，同时增强对模型的信任。

    

    深度神经网络在许多现实应用中表现出色。不幸的是，它们可能会表现出“聪明的汉斯”式的行为，利用数据集中的混淆因素以实现高性能。本研究引入了“解释交互学习”(XIL)的新型学习设置，并在植物表型研究任务中展示了其好处。XIL将科学家引入训练循环，使她通过对模型解释的反馈进行交互式修订。我们的实验结果表明，XIL可以帮助避免机器学习中的“聪明汉斯”时刻，并鼓励（或鼓励，如果合适的话）对基础模型的信任。

    arXiv:2001.05371v4 Announce Type: replace-cross  Abstract: Deep neural networks have shown excellent performances in many real-world applications. Unfortunately, they may show "Clever Hans"-like behavior -- making use of confounding factors within datasets -- to achieve high performance. In this work, we introduce the novel learning setting of "explanatory interactive learning" (XIL) and illustrate its benefits on a plant phenotyping research task. XIL adds the scientist into the training loop such that she interactively revises the original model via providing feedback on its explanations. Our experimental results demonstrate that XIL can help avoiding Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust into the underlying model.
    
[^195]: 基于遮蔽语言模型的上下文文本去噪

    Contextual Text Denoising with Masked Language Models

    [https://arxiv.org/abs/1910.14080](https://arxiv.org/abs/1910.14080)

    提出了一种基于遮蔽语言模型的上下文文本去噪算法，可以在不重新训练模型的情况下纠正噪声文本，并在多个下游任务中提高性能

    

    最近，借助深度学习模型，在不同的自然语言处理（NLP）任务中取得了显著进展。不幸的是，最先进的模型容易受到嘈杂文本的影响。我们提出了一种基于现成遮蔽语言模型的新上下文文本去噪算法。提出的算法不需要对模型进行重新训练，可以集成到任何NLP系统中，而不需要对配对的清洗训练数据进行额外训练。我们在合成噪声和自然噪声下评估了我们的方法，并表明所提算法可以利用上下文信息来纠正噪声文本，并在多个下游任务中提高嘈杂输入的性能。

    arXiv:1910.14080v2 Announce Type: replace  Abstract: Recently, with the help of deep learning models, significant advances have been made in different Natural Language Processing (NLP) tasks. Unfortunately, state-of-the-art models are vulnerable to noisy texts. We propose a new contextual text denoising algorithm based on the ready-to-use masked language model. The proposed algorithm does not require retraining of the model and can be integrated into any NLP system without additional training on paired cleaning training data. We evaluate our method under synthetic noise and natural noise and show that the proposed algorithm can use context information to correct noise text and improve the performance of noisy inputs in several downstream tasks.
    
[^196]: Pair-Matching: Links Prediction with Adaptive Queries

    Pair-Matching: Links Prediction with Adaptive Queries

    [https://arxiv.org/abs/1905.07342](https://arxiv.org/abs/1905.07342)

    本文展示如果图是根据随机块模型（SBM）生成的情况下，可以实现Pair-Matching问题中的次线性遗憾。

    

    Pair-Matching问题出现在许多应用程序中，其中一个想要发现实体或个体之间良好匹配的情况。形式上，个体集合由图的节点表示，其中边，起初未被观察到，表示良好的匹配。算法查询节点对并观察边的存在/不存在。其目标是在固定的查询预算下尽可能多地发现边。Pair-Matching是多臂老虎机问题的一个特殊实例，其中手臂是个体对，奖励是连接这些对的边。尽管如此，这个老虎机问题是非标准的，因为每个手臂只能玩一次。 鉴于这最后一个约束，只有在图具有一定的基本结构时才可以预期次线性遗憾。本文表明，在图根据随机块模型（SBM）生成的情况下，可以实现次线性遗憾。

    arXiv:1905.07342v3 Announce Type: replace-cross  Abstract: The pair-matching problem appears in many applications where one wants to discover good matches between pairs of entities or individuals. Formally, the set of individuals is represented by the nodes of a graph where the edges, unobserved at first, represent the good matches. The algorithm queries pairs of nodes and observes the presence/absence of edges. Its goal is to discover as many edges as possible with a fixed budget of queries. Pair-matching is a particular instance of multi-armed bandit problem in which the arms are pairs of individuals and the rewards are edges linking these pairs. This bandit problem is non-standard though, as each arm can only be played once.   Given this last constraint, sublinear regret can be expected only if the graph presents some underlying structure. This paper shows that sublinear regret is achievable in the case where the graph is generated according to a Stochastic Block Model (SBM) with tw
    
[^197]: 增强连续强化学习中的回放在世界模型中

    Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])

    [http://arxiv.org/abs/2401.16650](http://arxiv.org/abs/2401.16650)

    本研究通过在回放缓冲区中应用增强方法，成功地解决了增强连续强化学习中的内存限制问题，并在世界模型中有效防止灾难性遗忘。

    

    在连续强化学习中，强化学习代理的环境会发生变化。成功的系统应该适当平衡保持已学习任务上的代理性能、稳定性和学习新任务的可塑性之间的矛盾要求。首进先出缓冲区通常用于增强此类设置中的学习，但需要大量内存。我们探索了将增强方法应用于此缓冲区中，以缓解内存限制，并与基于世界模型的强化学习算法一起使用，评估其在促进连续学习方面的效果。我们在Procgen和Atari强化学习基准测试中评估了我们方法的有效性，并证明了在潜在世界模型的背景下，回放缓冲区中的分布匹配增强可以成功防止灾难性遗忘，并显著降低计算开销。然而，我们也发现这种解决方案并非完全无懈可击，

    In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
    
[^198]: 温和的规范执行：更快的出现，更快乐的智能体

    Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])

    [http://arxiv.org/abs/2401.16461](http://arxiv.org/abs/2401.16461)

    通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。

    

    多智能体系统可视为一个自主智能体的社会，通过社会规范可以有效地调控智能体的交互。一般来说，一个社会的规范并不是硬编码的，而是从智能体的交互中产生的。具体来说，一个社会中的智能体对另一个智能体的行为作出的反应以及对他人反应的回应，决定了社会中出现哪些规范。我们将一个智能体对另一个智能体的满意或不满意行为的反应视为第一个智能体向第二个智能体的交流。理解这些交流是一种社会智能：这些交流通过推动智能体朝着某些行为进行，从而促进规范的出现。虽然众所周知惩罚可以导致规范的出现，但我们认为更宽泛的社会智能可能在促进多智能体系统中的合作方面更有效。因此，我们开发了一种被称为Ne的方法

    A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
    
[^199]: ProCNS: 用于弱监督医学图像分割的渐进式原型校准和噪声抑制

    ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])

    [http://arxiv.org/abs/2401.14074](http://arxiv.org/abs/2401.14074)

    ProCNS是一种用于弱监督医学图像分割的新方法，采用渐进式原型校准和噪声抑制的原则来解决现有方法中的问题。

    

    弱监督分割（WSS）作为缓解注释成本和模型性能之间冲突的解决方案而出现，采用稀疏的注释格式（例如点、涂鸦、块等）。典型的方法试图利用解剖和拓扑先验将稀疏注释直接扩展为伪标签。然而，由于对医学图像中模糊边缘的关注不足和对稀疏监督的不充分探索，现有方法往往会在噪声区域生成错误且过于自信的伪建议，导致模型误差累积和性能下降。在这项工作中，我们提出了一种新颖的WSS方法，名为ProCNS，它包含两个协同模块，设计原则是渐进式原型校准和噪声抑制。具体而言，我们设计了一种基于原型的区域空间相似性（PRSA）损失函数，最大化空间和语义元素之间的成对相似度，为我们感兴趣的模型提供了

    Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest 
    
[^200]: 功能数据分类的特征选择

    Feature Selection for Functional Data Classification. (arXiv:2401.05765v1 [stat.ML])

    [http://arxiv.org/abs/2401.05765](http://arxiv.org/abs/2401.05765)

    本文介绍了一种名为FSFC的新方法，它解决了在具有分类响应和纵向特征的情况下同时进行功能数据特征选择和分类的挑战。

    

    功能数据分析已经成为许多需要整合和解释复杂数据的当代科学领域中的关键工具。此外，新技术的出现促进了大量纵向变量的收集，使得特征选择对于避免过拟合和提高预测性能至关重要。本文介绍了一种名为FSFC（功能分类特征选择）的新方法，它解决了在具有分类响应和纵向特征的情况下同时进行功能数据特征选择和分类的挑战。我们的方法解决了一个新定义的优化问题，将逻辑损失和功能特征结合起来，以识别用于分类的最关键特征。为了解决最小化过程，我们使用功能主成分，并开发了一种新的自适应版本的双增广Lagrange算法，利用了。。。

    Functional data analysis has emerged as a crucial tool in many contemporary scientific domains that require the integration and interpretation of complex data. Moreover, the advent of new technologies has facilitated the collection of a large number of longitudinal variables, making feature selection pivotal for avoiding overfitting and improving prediction performance. This paper introduces a novel methodology called FSFC (Feature Selection for Functional Classification), that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and longitudinal features. Our approach tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial features for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm that leverages the 
    
[^201]: 使用单一非自回归Transformer生成遮蔽音频

    Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])

    [http://arxiv.org/abs/2401.04577](http://arxiv.org/abs/2401.04577)

    MAGNeT是一种遮蔽生成序列建模方法，使用单一非自回归Transformer生成具有高质量的音频，并引入了一种新颖的重新评分方法来提高生成音频的质量。同时，MAGNeT还探索了混合版本，可在自回归模式和非自回归模式下生成序列。在实验中证明MAGNeT在文本到音乐和文本到音频生成任务中具有高效性。

    

    我们介绍了一种名为MAGNeT的遮蔽生成序列建模方法，它直接操作多个音频令牌流。与以往的方法不同，MAGNeT由单阶段非自回归Transformer组成。在训练过程中，我们根据遮蔽计划器预测遮蔽令牌的范围，而在推断过程中，我们逐步构建输出序列使用多个解码步骤。为了进一步提高生成音频的质量，我们引入了一种新颖的重新评分方法，其中我们利用外部预训练模型来重新评分和排名MAGNeT的预测结果，这些结果将被用于后续的解码步骤。最后，我们探索了MAGNeT的混合版本，其中我们在自回归模式下生成前几秒钟，而其余的序列则以并行方式进行解码。我们展示了MAGNeT在文本到音乐和文本到音频生成任务中的效率，并进行了广泛的实验验证。

    We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi
    
[^202]: 从电生理数据中映射和预测神经元相互作用的水库计算模型

    Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data. (arXiv:2311.03131v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2311.03131](http://arxiv.org/abs/2311.03131)

    该论文介绍了一种基于水库计算网络的计算模型，可以从电生理测量数据中解码神经元网络的时空信息，并在宏观领域内重建网络结构。实验证明该模型比其他常用方法更准确地预测了网络的连接图，并且能够预测网络对特定输入的响应能力。

    

    神经元网络的电生理特性能够在非常短的时间尺度内揭示不同细胞单元之间的各种相互作用。在分析这些信号的过程中，一个挑战就是找出给定网络的形态和功能。在这项工作中，我们开发了一个基于水库计算网络（RCN）架构的计算模型，它可以解码神经元培养物的电生理测量数据的时空信息，并在宏观领域内重建表示神经元单元之间连接性的网络结构。我们证明该模型可以比交叉相关和传递熵等常用方法更准确地预测网络的连接图。此外，我们还通过实验展示了该模型预测网络对特定输入（如局部刺激）的响应能力。

    Electrophysiological nature of neuronal networks allows to reveal various interactions between different cell units at a very short time-scales. One of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. In this work we developed a computational model, based on Reservoir Computing Network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. We demonstrate that the model can predict the connectivity map of the network with higher accuracy than the common methods such as Cross-Correlation and Transfer-Entropy. In addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.
    
[^203]: 线性模型的鲁棒因果强化学习算法

    Robust Causal Bandits for Linear Models. (arXiv:2310.19794v1 [stat.ML])

    [http://arxiv.org/abs/2310.19794](http://arxiv.org/abs/2310.19794)

    本文研究了线性模型的鲁棒因果强化学习算法，在复杂系统的情况下，现有方法无法保持遗憾次线性。

    

    在因果系统中，优化回报函数的顺序实验设计可以有效地建模为因果强化学习中的顺序干预设计。在已有的因果强化学习文献中，一个重要的假设是因果模型在时间上保持不变。然而，在复杂系统中，数学模型常常发生时间上的波动，这个假设不一定成立。本文研究了因果强化学习在模型波动存在下的鲁棒性。重点研究了具有线性结构方程模型 (SEMs) 的因果系统。SEMs 和时间变化的干预前后统计模型均为未知。以累计遗憾为设计指标，在知道整个因果模型及其波动情况的神谕的基础上，设计一系列干预使得累计遗憾最小化。首先，通过实验证明了现有方法无法保持遗憾次线性。

    Sequential design of experiments for optimizing a reward function in causal systems can be effectively modeled by the sequential design of interventions in causal bandits (CBs). In the existing literature on CBs, a critical assumption is that the causal models remain constant over time. However, this assumption does not necessarily hold in complex systems, which constantly undergo temporal model fluctuations. This paper addresses the robustness of CBs to such model fluctuations. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown. Cumulative regret is adopted as the design criteria, based on which the objective is to design a sequence of interventions that incur the smallest cumulative regret with respect to an oracle aware of the entire causal model and its fluctuations. First, it is established that the existing approaches fail to maintain regret sub-linearity with 
    
[^204]: 让最终用户成为基准测试的重点：OrionBench用于无监督时间序列异常检测

    Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection. (arXiv:2310.17748v1 [cs.LG])

    [http://arxiv.org/abs/2310.17748](http://arxiv.org/abs/2310.17748)

    OrionBench是一个以用户为中心的无监督时间序列异常检测基准测试，提供了通用抽象、可扩展性和发布频繁的基准测试。

    

    时间序列异常检测是许多应用领域中的常见问题，例如医疗保健中的患者监测、金融中的预测或能源中的预测性维护。这导致了许多异常检测方法的出现，包括最近的基于深度学习的方法。虽然已经提出了几种用于比较新开发模型的基准测试，但它们通常依赖于对有限数据集的一次性执行，并且比较仅限于少数模型。我们提出了OrionBench——一个以用户为中心、持续维护的无监督时间序列异常检测基准测试。该框架提供了用于表示模型的通用抽象、添加新的流水线和数据集的可扩展性、超参数标准化、流水线验证以及发布基准测试的频繁版本。我们展示了OrionBench的用法，并展示了在三年时间内发布的15个版本中流水线的演化过程。

    Time series anomaly detection is a prevalent problem in many application domains such as patient monitoring in healthcare, forecasting in finance, or predictive maintenance in energy. This has led to the emergence of a plethora of anomaly detection methods, including more recently, deep learning based methods. Although several benchmarks have been proposed to compare newly developed models, they usually rely on one-time execution over a limited set of datasets and the comparison is restricted to a few models. We propose OrionBench -- a user centric continuously maintained benchmark for unsupervised time series anomaly detection. The framework provides universal abstractions to represent models, extensibility to add new pipelines and datasets, hyperparameter standardization, pipeline verification, and frequent releases with published benchmarks. We demonstrate the usage of OrionBench, and the progression of pipelines across 15 releases published over the course of three years. Moreover,
    
[^205]: 到底是理解还是记忆：解析算法数据集上的泛化和记忆

    To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. (arXiv:2310.13061v1 [cs.LG])

    [http://arxiv.org/abs/2310.13061](http://arxiv.org/abs/2310.13061)

    本研究探讨了在深度学习中的泛化和记忆的问题，通过对模数算术任务上训练的神经网络进行实验，发现网络可以同时记住损坏的标签并实现100%的泛化，并且可以通过识别和剪枝记忆化的神经元来降低对损坏数据的准确率，提高对未损坏数据的准确率。

    

    在深度学习中，强大的泛化能力是一个重要的挑战，特别是在可训练参数非常大的情况下。通常情况下，很难知道网络是否已经记住了一组特定的样本，还是理解了其中的基本规律（或者两者都有）。受到这个挑战的启发，我们研究了一个可解释的模型，其中的泛化表示可以通过分析来理解，并且很容易与记忆性表示区分开。具体而言，我们考虑了在模数算术任务上训练的两层神经网络，在这些任务中，（ξ·100%）的标签是被损坏的（即训练集中的一些模数运算结果是错误的）。我们展示了：（i）网络可以同时记住损坏的标签并实现100%的泛化；（ii）可以识别和剪枝记忆化的神经元，降低对损坏数据的准确率，提高对未损坏数据的准确率；（iii）正则化方法如w

    Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider two-layer neural networks trained on modular arithmetic tasks where ($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as w
    
[^206]: Zipformer：一种更快速、更好的自动语音识别编码器

    Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v1 [eess.AS])

    [http://arxiv.org/abs/2310.11230](http://arxiv.org/abs/2310.11230)

    Zipformer是一种更快速、更节省内存、性能更好的自动语音识别编码器，通过U-Net-like编码器结构、重新组织的块结构、改进的LayerNorm、新的激活函数和新的优化器等方式实现了优化。实验证明它在LibriSpeech、Aishell-1和Wenet等数据集上表现出更快的收敛和更好的性能。

    

    Conformer已成为自动语音识别（ASR）中最流行的编码器模型。它在变换器中加入了卷积模块以学习局部和全局依赖关系。本文介绍了一种更快速、更节省内存、性能更好的变换器——Zipformer。建模改变包括：1）类似U-Net的编码器结构，中间堆栈在较低的帧率下运行；2）重新组织的块结构，增加了更多的模块，其中我们重复使用注意力权重以提高效率；3）一种改进的LayerNorm形式，称为BiasNorm，允许我们保留一些长度信息；4）新的激活函数SwooshR和SwooshL的性能优于Swish。我们还提出了一种新的优化器，称为ScaledAdam，它通过当前张量的规模来缩放更新，以保持相对变化大致相同，并明确学习参数规模。与Adam相比，它实现了更快的收敛和更好的性能。在LibriSpeech、Aishell-1和Wenet上进行了大量实验。

    The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and Wenet
    
[^207]: LLP-Bench：一种用于从标签比例中学习的大规模表格基准

    LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions. (arXiv:2310.10096v1 [cs.LG])

    [http://arxiv.org/abs/2310.10096](http://arxiv.org/abs/2310.10096)

    本文提出了一个大规模表格LLP基准，填补了表格LLP领域的研究空白。在该基准中，我们可以创建特征bags，其中所有实例具有相同的特征值，从而更好地模拟实际应用场景。

    

    在学习从标签比例中学习（LLP）任务中，模型通过对实例组（称为bags）和相应的标签比例进行训练，以预测个体实例的标签。LLP主要应用于图像和表格两种数据集。在图像LLP中，通过从底层数据集中随机抽样实例来创建固定大小的bags。通过这种方法创建的bags称为随机bags。对图像LLP的实验主要集中在CIFAR-*和MNIST数据集的随机bags上。尽管表格LLP在隐私敏感应用中非常重要，但尚缺乏一个开放的、大规模的表格LLP基准。表格LLP的一个独特特性是能够创建特征bags，其中bag中的所有实例对于给定的特征具有相同的值。先前的研究表明，特征bags在实际的现实应用中非常常见。在本文中，我们解决了表格LLP的研究空白，提出了一个用于大规模表格LLP的基准。

    In the task of Learning from Label Proportions (LLP), a model is trained on groups (a.k.a bags) of instances and their corresponding label proportions to predict labels for individual instances. LLP has been applied pre-dominantly on two types of datasets - image and tabular. In image LLP, bags of fixed size are created by randomly sampling instances from an underlying dataset. Bags created via this methodology are called random bags. Experimentation on Image LLP has been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very crucial task in privacy sensitive applications, tabular LLP does not yet have a open, large scale LLP benchmark. One of the unique properties of tabular LLP is the ability to create feature bags where all the instances in a bag have the same value for a given feature. It has been shown in prior research that feature bags are very common in practical, real world applications [Chen et. al '23, Saket et. al. '22].  In this paper, we address the lac
    
[^208]: 识别干预外推的表示方法

    Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])

    [http://arxiv.org/abs/2310.04295](http://arxiv.org/abs/2310.04295)

    本文研究了干预外推的任务，证明了可识别的表示方法能够有效地解决这个任务，即使干预对结果产生非线性影响。

    

    可识别和因果关系表示学习的前提是改进当前的表示学习范式，以提高泛化性或鲁棒性。尽管在可识别性问题上取得了近期的进展，但仍需要更多理论结果来证明这些方法对下游任务的具体优势。在本文中，我们考虑干预外推的任务：预测干预如何影响结果，即使这些干预在训练时没有观察到，我们证明了可识别的表示能够为这个任务提供有效的解决方案，即使干预对结果产生非线性影响。我们的设置包括一个结果Y，观察到的特征X，这些特征是潜在特征Z的非线性转换，以及影响Z的外生行为变量A。干预外推的目标是预测位于训练支持之外的A上的干预如何影响Y。在这里，外推变得重要。

    The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becom
    
[^209]: 使用级联扩散模型进行热带气旋预测

    Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.01690](http://arxiv.org/abs/2310.01690)

    本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。

    

    随着气候变化，飓风变得更加强烈，基于人工智能模型的预测方法比基于数学模型的传统方法更加经济实惠和易于获取。本研究利用扩散模型通过整合卫星成像、遥感和大气数据，采用级联方法进行飓风轨迹和降水模式的预测，训练数据集包括来自六个主要盆地的51个飓风。实验证明，级联模型的最终预测在36小时内显示准确的预测结果，所有三项任务的结构相似性指数（SSIM）和峰值信噪比（PSNR）的值都超过了0.5和20dB。本研究还强调了扩散模型等人工智能方法在高性能需求（如飓风预测）方面的高效性和计算经济性，使其成为高度脆弱地区的理想选择。

    As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
    
[^210]: 对中毒公平表示的研究

    Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])

    [http://arxiv.org/abs/2309.16487](http://arxiv.org/abs/2309.16487)

    这项研究提出了第一个针对公平表示学习（FRL）的数据中毒框架，该框架在对抗性场景下评估模型的健壮性，解决了FRL方法在面对数据中毒攻击时的脆弱性。

    

    公平机器学习旨在减少对某些人口子群体（如老年人和女性）的模型预测偏见。最近，通过深度神经网络训练的公平表示学习（FRL）表现出优越的性能，从数据中推断出不包含人口统计信息的表示，然后将其用作分类或其他下游任务的输入。尽管FRL方法得到了发展，但它们在面对数据中毒攻击时的脆弱性，即在对抗性场景下评估模型的健壮性的流行协议中，尚未得到充分研究。数据中毒攻击已经针对将公平性约束纳入浅层模型分类器的经典公平机器学习方法进行了开发。然而，由于公平目标和模型架构有明显的差异，这些攻击在FRL中不够有效。本文提出了第一个针对FRL的数据中毒框架。我们诱使模型输出包含尽可能多公平表示的不公平表示。

    Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much 
    
[^211]: 图对比学习的可证明训练方法

    Provable Training for Graph Contrastive Learning. (arXiv:2309.13944v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13944](http://arxiv.org/abs/2309.13944)

    图对比学习中，我们发现训练存在不平衡的问题，为此我们提出了“节点紧凑性”度量来指导训练。

    

    图对比学习（GCL）已经成为一种从增强图中学习节点嵌入而无需标签的流行训练方法。尽管最大化正节点对之间的相似性并最小化负节点对之间的相似性的关键原则已经得到确认，但仍存在一些基本问题。考虑到复杂的图结构，是否有一些节点始终按照这一原则进行良好训练，即使在不同的图增强方法下也是如此？还是有一些节点更有可能在图增强中未经训练，并违反这一原则？如何区分这些节点并进一步指导GCL的训练？为了回答这些问题，我们首先提出了实验证据，表明GCL的训练在所有节点上确实存在不平衡。为了解决这个问题，我们提出了度量“节点紧凑性”，它是节点遵循GCL原则与增强范围相关的下界。我们进一步导出了

    Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the
    
[^212]: 使用ReLU网络在紧致域上进行通用逼近的最小宽度

    Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])

    [http://arxiv.org/abs/2309.10402](http://arxiv.org/abs/2309.10402)

    本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\mathbb R^{d_y}$所需的最小宽度为$\max\{d_x,d_y,2\}$，从而表明在紧致域上的逼近比在${\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\min}\ge d_y+1$（当$d_x<d_y\le2d_x$）。

    

    经过研究，限制宽度网络的通用逼近性质已经作为深度限制网络的经典通用逼近定理的对偶进行研究。已经有几次尝试来表征使得通用逼近性质成立的最小宽度$w_{\min}$，但只有很少几个找到了确切的值。在这项工作中，我们证明了对于从$[0,1]^{d_x}$到$\mathbb R^{d_y}$的$L^p$函数的通用逼近的最小宽度，如果激活函数是ReLU-Like（例如ReLU，GELU，Softplus），那么它的确切值是$\max\{d_x,d_y,2\}$。与已知的结果$w_{\min}=\max\{d_x+1,d_y\}$相比，当域为${\mathbb R^{d_x}}$时，我们的结果首次表明，在紧致域上的逼近要求比在${\mathbb R^{d_x}}$上的要求更小。我们接下来利用包括ReLU在内的一般激活函数进行一致逼近的最小宽度$w_{\min}$证明了一个下界：如果$d_x<d_y\le2d_x$，则$w_{\min}\ge d_y+1$。结合我们的第一个结果，这表明了一个二分法。

    The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x<d_y\le2d_x$. Together with our first result, this shows a dichotomy be
    
[^213]: 未监测站点中的时间序列预测：水资源中机器学习技术的综述

    Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources. (arXiv:2308.09766v1 [cs.LG])

    [http://arxiv.org/abs/2308.09766](http://arxiv.org/abs/2308.09766)

    本论文综述了机器学习在未监测站点的水资源预测中的应用，包括河流流量、水质等相关变量，并讨论了融合集水区特征的新方法，提升机器学习的使用。

    

    针对未监测站点中动态环境变量的预测仍然是水资源科学中长期存在的挑战。世界上大部分的淡水资源没有适当的监测关键环境变量的能力，而对河流流量和水质等水文变量进行广泛预测的需求由于气候和土地利用变化在过去几十年中越来越迫切，并影响着水资源。现代机器学习方法能够从大规模、多样化的数据集中提取信息，相对于基于过程和经验模型的方法在水文时间序列预测方面表现越来越优越。我们回顾了机器学习在河流流量、水质和其他水资源预测中的相关最新应用，并讨论了利用新兴方法改进机器学习在集水区特征融合方面的机会。

    Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics i
    
[^214]: 一种自适应惩罚方法用于将先验知识约束集成到神经常微分方程中

    A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])

    [http://arxiv.org/abs/2307.14940](http://arxiv.org/abs/2307.14940)

    本论文提出了一种自适应惩罚算法，用于将约束的自然系统集成到神经常微分方程中，通过引入先验知识提高了模型的可解释性，并通过数值实验证明了其有效性。

    

    使用神经常微分方程(Neural ODEs)，可以有效地对自然系统的连续动态进行建模。然而，为了实现准确而有意义的预测，模型必须遵循管理这些系统的基本规律或定律。在本论文中，我们提出了一种自适应惩罚算法，用于将约束的自然系统集成到神经常微分方程中。所提出的自适应惩罚函数可以动态调整惩罚参数。引入先验知识有助于增加基于神经常微分方程的模型的可解释性。我们通过模拟三个具有先验知识约束的自然系统(人口增长，化学反应演化和阻尼谐振运动)来验证所提出的方法。数值实验证明了所提出的自适应惩罚算法在神经常微分方程中的有效性，并与其他惩罚神经常微分方程方法和原始神经常微分方程进行了比较。

    The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural
    
[^215]: METAVerse：用于越野导航的元学习可行性成本图

    METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation. (arXiv:2307.13991v1 [cs.RO])

    [http://arxiv.org/abs/2307.13991](http://arxiv.org/abs/2307.13991)

    本文提出了METAVerse，一个用于在各种环境中准确可靠地预测地形可行性的元学习框架。通过自监督学习，利用稀疏的LiDAR点云生成密集连续值成本图，通过元学习训练全局模型，有效减小地形可行性估计的不确定性。

    

    越野环境中的自主导航需要准确估计地形可行性。然而，在非结构化环境中，地形可行性估计受到多个影响车辆与地形相互作用的因素的不确定性的影响。因此，获取一个能够准确预测各种环境中可行性的可推广模型是具有挑战性的。本文提出了METAVerse，这是一个用于在各种环境中准确可靠地预测地形可行性的元学习框架。我们通过自监督方式利用车辆与地形相互作用反馈来训练可行性预测网络，从稀疏的LiDAR点云生成密集连续值成本图。利用元学习从多个环境收集的驾驶数据训练全局模型，有效地减小估计不确定性。在部署过程中，进行在线适应。

    Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is p
    
[^216]: 非参数线性特征学习在回归中的应用通过正则化

    Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2307.12754](http://arxiv.org/abs/2307.12754)

    本研究提出了一种新的非参数线性特征学习方法，对于监督学习中存在于低维线性子空间中的相关信息的预测和解释能力的提升是非常有帮助的。

    

    表征学习在自动化特征选择中发挥着关键作用，特别是在高维数据的背景下，非参数方法常常很难应对。在本研究中，我们专注于监督学习场景，其中相关信息存在于数据的低维线性子空间中，即多指数模型。如果已知该子空间，将大大增强预测、计算和解释能力。为了解决这一挑战，我们提出了一种新颖的非参数预测的线性特征学习方法，同时估计预测函数和线性子空间。我们的方法采用经验风险最小化，并加上函数导数的惩罚项，以保证其多样性。通过利用Hermite多项式的正交性和旋转不变性特性，我们引入了我们的估计器RegFeaL。通过利用替代最小化，我们迭代地旋转数据以改善与线性子空间的对齐。

    Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
    
[^217]: 揭示联邦学习中局部和全局公平性权衡的信息分解方法

    Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition. (arXiv:2307.11333v1 [cs.LG])

    [http://arxiv.org/abs/2307.11333](http://arxiv.org/abs/2307.11333)

    本文利用信息论的部分信息分解（PID）方法，研究了在联邦学习中关于敏感属性的群体公平性权衡问题。通过分解发现了三种不公平来源，分别是唯一不平等性、冗余不平等性和掩盖不平等性，揭示了全局和局部公平性之间的基本限制和权衡。

    

    本文从信息论的角度，研究了在联邦学习中关于敏感属性（如性别、种族等）的群体公平性权衡问题。现有工作主要关注“全局公平性”（模型在所有客户端上的不平等程度）或“局部公平性”（模型在每个个体客户端上的不平等程度），而并不总是考虑它们之间的权衡。对于联邦学习中的全局公平性和局部公平性之间的相互作用，以及一个是否暗示另一个，我们缺乏理解。为了弥补这一空白，我们利用了信息论中的部分信息分解（PID）方法，首先确定了联邦学习中三种不公平来源，即“唯一不平等性”、“冗余不平等性”和“掩盖不平等性”。通过典型案例，我们演示了这三种不平等性如何影响全局和局部公平性。这种分解帮助我们推导出全局和局部公平性之间的基本限制和权衡。

    In this paper, we present an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works mostly focus on either \emph{global fairness} (overall disparity of the model across all clients) or \emph{local fairness} (disparity of the model at each individual client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, \emph{Unique Disparity}, \emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical examples, we demonstrate how these three disparities contribute to global and local fairness. This decomposition helps us derive fundamental limits and trade-offs between
    
[^218]: DRAGON: 一种基于对话的带有视觉语言关联的辅助导航机器人

    DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])

    [http://arxiv.org/abs/2307.06924](http://arxiv.org/abs/2307.06924)

    DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。

    

    视力受损者在理解和导航周围空间方面存在困难。目前的导航技术要么只关注导航，要么提供有限的关于环境的沟通。受到最近在视觉语言关联和语义导航方面的进展的启发，我们提出了DRAGON，一种由对话系统驱动的导航机器人，并具有将环境与自然语言关联的能力。通过理解用户的指令，DRAGON能够引导用户到地图上的目标地标，描述环境，并通过视觉观察回答问题。通过有效利用对话，机器人可以将用户的自由形式描述与环境中的地标关联起来，并通过口语提供语义信息给用户。我们在日常室内环境中进行了盲目参与者的用户研究。我们的结果表明，DRAGON能够与用户顺畅地沟通，

    Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
    
[^219]: 动量简单而可证实地增强非独立同分布联邦学习的效果

    Momentum Benefits Non-IID Federated Learning Simply and Provably. (arXiv:2306.16504v1 [cs.LG])

    [http://arxiv.org/abs/2306.16504](http://arxiv.org/abs/2306.16504)

    本论文研究了在非独立同分布联邦学习中利用动量来提升FedAvg和SCAFFOLD算法的性能，证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。

    

    联邦学习是一种用于大规模机器学习的强大范例，但由于不可靠的网络连接、缓慢的通信以及客户端之间存在的数据异质性，它面临着重大挑战。FedAvg和SCAFFOLD是两种解决这些挑战的基本算法。特别地，FedAvg在与中央服务器进行通信之前采用多个本地更新，而SCAFFOLD在其本地更新中维护每个客户端上的控制变量以补偿“客户端漂移”。文献中提出了各种方法来增强这两种算法的收敛性，但它们要么对算法结构进行不切实际的调整，要么依赖于有界数据异质性的假设。本文探讨了利用动量来增强FedAvg和SCAFFOLD性能的方法。当所有客户端参与训练过程时，我们证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。

    Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two fundamental algorithms to address these challenges. In particular, FedAvg employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for "client drift" in its local updates. Various methods have been proposed in literature to enhance the convergence of these two algorithms, but they either make impractical adjustments to algorithmic structure, or rely on the assumption of bounded data heterogeneity.  This paper explores the utilization of momentum to enhance the performance of FedAvg and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FedAvg to converge without relying on the assumption o
    
[^220]: 通过语言模型批量生产多模态系统的失败

    Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])

    [http://arxiv.org/abs/2306.12105](http://arxiv.org/abs/2306.12105)

    本文介绍了一种MultiMon系统，可以自动识别多模态系统中的系统性失败，揭示CLIP文本编码器的14个系统性失败，每个都由数百个不同的输入组成，这些输入会导致其他大多数最先进的多模态系统的失败。

    

    部署的多模态系统可能以评估人员未曾预见的方式失败。为了在部署前找到这些失败，我们介绍了MultiMon，这是一个能够自动识别系统性失败的系统，能够提供可推广的、自然语言描述模型失败模式的例子。为了揭示系统性失败，MultiMon从语料库中抓取错误协议的示例：输入产生相同的输出，但不应该如此。然后它会激活一个语言模型（例如GPT-4）来查找系统性失败的模式并用自然语言描述它们。我们使用MultiMon找到了CLIP文本编码器的14个系统性失败（例如“忽略量词”，每个都由数百个不同的输入组成（例如“一个带有一些/许多书的书架”）。因为CLIP是大多数最先进的多模态系统的基础，这些输入会导致Midjourney 5.1、DALL-E、VideoFusion等系统失败。MultiMon也可以指导针对特定用例的相关故障，例如自驾车系统。

    Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
    
[^221]: 上下文随机块模型中的最优推断

    Optimal Inference in Contextual Stochastic Block Models. (arXiv:2306.07948v1 [cs.SI])

    [http://arxiv.org/abs/2306.07948](http://arxiv.org/abs/2306.07948)

    本论文提出了一种基于信念传播的算法，用于半监督上下文随机块模型(cSBM)的推断问题。研究发现，相比于现有的图神经网络结构，这种算法达到的准确性更高，同时可用于建立性能基准。

    

    上下文随机块模型(cSBM)被提出来对具有节点标签相关性的属性图进行无监督社区检测，其中图形和高维节点信息都与节点标签相关。在图上机器学习的背景下，cSBM已被广泛用作合成数据集，用于评估半监督节点分类的图神经网络(GNN)的性能。

    The contextual stochastic block model (cSBM) was proposed for unsupervised community detection on attributed graphs where both the graph and the high-dimensional node information correlate with node labels. In the context of machine learning on graphs, the cSBM has been widely used as a synthetic dataset for evaluating the performance of graph-neural networks (GNNs) for semi-supervised node classification. We consider a probabilistic Bayes-optimal formulation of the inference problem and we derive a belief-propagation-based algorithm for the semi-supervised cSBM; we conjecture it is optimal in the considered setting and we provide its implementation. We show that there can be a considerable gap between the accuracy reached by this algorithm and the performance of the GNN architectures proposed in the literature. This suggests that the cSBM, along with the comparison to the performance of the optimal algorithm, readily accessible via our implementation, can be instrumental in the develo
    
[^222]: 大型语言模型的软提示调整方法用于评估偏差

    Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])

    [http://arxiv.org/abs/2306.04735](http://arxiv.org/abs/2306.04735)

    本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。

    

    近年来，大型语言模型的提示功能因无需标记数据即可产生良好结果而备受青睐。然而，这需要进行提示调整以获得引导更好模型性能的最佳提示。本文中，我们探讨了在情感分类任务中使用软提示调整来量化大型语言模型（LLMs）如Open Pre-trained Transformers（OPT）和Galactica语言模型中的偏差。由于这些模型是在可能偏向某些人群的真实数据上训练的，因此识别这些潜在问题非常重要。使用软提示来评估偏差给我们带来了额外的优势，可以避免手动设计提示导致的人为偏差注入。我们使用分组公平性（偏差）检查模型对不同敏感属性的偏见，并找到了有趣的偏差模式。由于LLMs已在各种应用中被用于工业中，因此我们对其进行的偏见评估具有实际意义。

    Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
    
[^223]: 神经网络集合的输入梯度多样性

    Input gradient diversity for neural network ensembles. (arXiv:2306.02775v1 [stat.ML])

    [http://arxiv.org/abs/2306.02775](http://arxiv.org/abs/2306.02775)

    本文提出了一阶斥力深度集成 (FoRDE) 算法，它使用输入梯度来增强多样性以提高神经网络集成的表现。

    

    深度集成 (DE) 通过它们的功能多样性在准确性、校准性和抵抗干扰方面表现出比单个神经网络更好的表现。基于粒子的变分推断 (ParVI) 方法通过基于网络相似性内核的排斥项来增强多样性。然而，由于过度参数化，权重空间排斥是低效的，而直接功能空间排斥被发现对 DE 的改进很小。为了避免这些困难，我们提出了基于 ParVI 的一阶斥力深度集成 (FoRDE)，这是一种基于输入梯度的集成学习方法。由于输入梯度唯一地确定了一个函数并且比权重小得多，所以这种方法保证了集合成员在功能上是不同的。直观地说，多样化输入梯度鼓励每个网络学习不同的特征，这有望改善神经网络集成的表现。

    Deep Ensembles (DEs) demonstrate improved accuracy, calibration and robustness to perturbations over single neural networks partly due to their functional diversity. Particle-based variational inference (ParVI) methods enhance diversity by formalizing a repulsion term based on a network similarity kernel. However, weight-space repulsion is inefficient due to over-parameterization, while direct function-space repulsion has been found to produce little improvement over DEs. To sidestep these difficulties, we propose First-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method based on ParVI, which performs repulsion in the space of first-order input gradients. As input gradients uniquely characterize a function up to translation and are much smaller in dimension than the weights, this method guarantees that ensemble members are functionally different. Intuitively, diversifying the input gradients encourages each network to learn different features, which is expected to improv
    
[^224]: 突触权重分布取决于可塑性的几何形态

    Synaptic Weight Distributions Depend on the Geometry of Plasticity. (arXiv:2305.19394v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.19394](http://arxiv.org/abs/2305.19394)

    计算神经科学的研究表明，突触权重分布取决于突触可塑性的几何形态，进而表明实验观测到的对数正态权重分布与标准的梯度下降模型不一致，可能说明大脑中使用的是非欧几里得距离。

    

    机器学习中的大多数学习算法都依赖于梯度下降调整模型参数，计算神经科学中日益增长的文献利用这些思想研究突触可塑性。然而，绝大部分此类研究忽略了一个关键的基本假设：突触变化的距离选择（即突触可塑性的几何形态）。梯度下降假定距离为欧几里得距离，但许多其他距离也是可能的，并且生物学不一定使用欧几里得几何形态。在这里，我们使用镜像下降提供的理论工具表明，无论最小化的损失为何，突触权重的分布都取决于突触可塑性的几何形态。我们利用这些结果表明，在几个大脑区域中发现的实验观测到的对数正态权重分布与标准的梯度下降（即欧几里得几何形态）不一致，而是与非欧几里得距离一致。

    Most learning algorithms in machine learning rely on gradient descent to adjust model parameters, and a growing literature in computational neuroscience leverages these ideas to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes (i.e. the geometry of synaptic plasticity). Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that, regardless of the loss being minimized, the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances.
    
[^225]: Sophia：一种用于语言模型预训练的可扩展的随机二阶优化器

    Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14342](http://arxiv.org/abs/2305.14342)

    Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。

    

    鉴于语言模型预训练的巨大成本，优化算法的微小改进将会大大降低训练的时间和成本。Adam及其变种一直是最先进的，而更复杂的二阶（基于Hessian的）优化器往往会带来太多的每步开销。在这篇论文中，我们提出了Sophia，一种简单可扩展的二阶优化器，它使用轻量级估计的对角Hessian作为预调节器。更新步骤是梯度的移动平均值除以估计Hessian的移动平均值，然后进行元素级别的裁剪。裁剪控制了最坏情况下的更新大小，并控制了Hessian在轨迹上的非凸性和快速变化的负面影响。Sophia只在每几次迭代中估计对角Hessian，这几乎没有平均每步的时间和内存开销。在使用GPT m进行语言建模时，

    Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
    
[^226]: 通过生成对抗反馈对语言模型进行微调

    Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])

    [http://arxiv.org/abs/2305.06176](http://arxiv.org/abs/2305.06176)

    本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。

    

    通过人类反馈的强化学习已经显著提高了大型语言模型(LLMs)的性能，使其输出与人类期望的价值观保持一致。然而，RLHF受到人类评估者的专业知识和生产力限制。在本研究中，我们研究了一种替代方法: 使用生成对抗反馈的强化学习(RLGAF)代替RLHF。我们的初步发现表明，RLGAF可以帮助对齐LLM的输出，同时不会受到RLHF固有的限制，为进一步自动化AI对齐的研究提供了有希望的途径。

    Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
    
[^227]: 利用Transformer进行光学系统非线性通道补偿的应用

    Application of Transformers for Nonlinear Channel Compensation in Optical Systems. (arXiv:2304.13119v1 [cs.IT])

    [http://arxiv.org/abs/2304.13119](http://arxiv.org/abs/2304.13119)

    本文提出了一种利用Transformer进行光学系统非线性通道补偿的新方法，这种方法利用了Transformer的记忆关注能力和并行结构，实现了高效的非线性补偿。同时，作者还提出了一种物理学信息掩码，用于降低计算复杂度。

    

    本文介绍了一种基于Transformer的新型非线性通道均衡方法，用于相干长距离传输。我们证明，由于其能够直接关注一系列符号之间的记忆，因此Transformer可以与并行结构有效地配合使用。我们展示了一个编码器部分的Transformer实现，用于非线性均衡，并分析了其在不同超参数范围内的性能。通过在每次迭代中处理符号块，并仔细选择要一起处理的编码器输出子集，可以实现高效的非线性补偿。我们还提出了一种基于非线性扰动理论的物理学信息掩码，用于降低Transformer非线性均衡的计算复杂度。

    In this paper, we introduce a new nonlinear channel equalization method for the coherent long-haul transmission based on Transformers. We show that due to their capability to attend directly to the memory across a sequence of symbols, Transformers can be used effectively with a parallelized structure. We present an implementation of encoder part of Transformer for nonlinear equalization and analyze its performance over a wide range of different hyper-parameters. It is shown that by processing blocks of symbols at each iteration and carefully selecting subsets of the encoder's output to be processed together, an efficient nonlinear compensation can be achieved. We also propose the use of a physic-informed mask inspired by nonlinear perturbation theory for reducing the computational complexity of Transformer nonlinear equalization.
    
[^228]: 关于多个吸引子非线性系统的提升和重构

    On the lifting and reconstruction of nonlinear systems with multiple attractors. (arXiv:2304.11860v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2304.11860](http://arxiv.org/abs/2304.11860)

    本文研究了具有多个吸引子的非线性系统的Koopman算子提升和重构机制，通过利用吸引域之间的固有对称性，只需三个自由度的线性重构就可以全局线性化系统。

    

    Koopman算子通过关注不变子空间中的观测量的演化，提供了非线性动力学的线性视角。感兴趣的观测量通常是从Koopman特征函数线性重构出来的。尽管Koopman算子在过去几年广泛使用，但对于具有多个稳定点的动力系统，关于Koopman算子的适用性存在一些误解。本研究解释了具有多个吸引子的非线性系统的Koopman算子提升机制。通过考虑Duffing振荡器的例子，我们表明通过利用吸引域之间的固有对称性，Koopman可观测空间中具有三个自由度的线性重构就足以全局线性化系统。

    The Koopman operator provides a linear perspective on non-linear dynamics by focusing on the evolution of observables in an invariant subspace. Observables of interest are typically linearly reconstructed from the Koopman eigenfunctions. Despite the broad use of Koopman operators over the past few years, there exist some misconceptions about the applicability of Koopman operators to dynamical systems with more than one fixed point. In this work, an explanation is provided for the mechanism of lifting for the Koopman operator of nonlinear systems with multiple attractors. Considering the example of the Duffing oscillator, we show that by exploiting the inherent symmetry between the basins of attraction, a linear reconstruction with three degrees of freedom in the Koopman observable space is sufficient to globally linearize the system.
    
[^229]: 未知拓扑网络中的探索学习辅助社区检测的统一框架

    A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology. (arXiv:2304.04497v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2304.04497](http://arxiv.org/abs/2304.04497)

    META-CODE是一个统一的框架，通过探索学习和易于收集的节点元数据，在未知拓扑网络中检测重叠社区。实验结果证明了META-CODE的有效性和可扩展性。

    

    在社交网络中，发现社区结构作为各种网络分析任务中的一个基本问题受到了广泛关注。然而，由于隐私问题或访问限制，网络结构通常是未知的，这使得现有的社区检测方法在没有昂贵的网络拓扑获取的情况下无效。为了解决这个挑战，我们提出了 META-CODE，这是一个统一的框架，通过探索学习辅助易于收集的节点元数据，在未知拓扑网络中检测重叠社区。具体而言，META-CODE 除了初始的网络推理步骤外，还包括三个迭代步骤：1) 基于图神经网络（GNNs）的节点级社区归属嵌入，通过我们的新重构损失进行训练，2) 基于社区归属的节点查询进行网络探索，3) 使用探索网络中的基于边连接的连体神经网络模型进行网络推理。通过实验结果证明了 META-CODE 的有效性和可扩展性。

    In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through e
    
[^230]: 使用丰富的元数据注释的屏幕角色的个性化语言建模

    Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])

    [http://arxiv.org/abs/2303.16618](http://arxiv.org/abs/2303.16618)

    本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。

    

    语言模型的个性化为对话敏感，能更好地捕捉特定特征的人员和/或特定环境中的说话模式。然而，丰富的角色注释难以得到和成功利用。在此工作中，我们发布并描述了一组新颖的手动注释，涵盖了来自流行的 Cornell 电影对话语料库的 863 名演讲者，包括特征引用和角色描述，以及超过 95％ 的特色电影的一组自动提取的六个元数据。我们对两个语料库进行了广泛的实验，并表明可以有效地使用此类注释来个性化语言模型，将困惑减少高达 8.5％。我们的方法甚至可以应用于零样本的演讲者，即对于没有先前培训数据的演讲者，依赖于角色的人口特征的组合。由于收集此类元数据成本高昂，因此我们还贡献了一项成本效益分析，以突出显示

    Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
    
[^231]: 生成支持线的多维聚类

    Generating Multidimensional Clusters With Support Lines. (arXiv:2301.10327v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10327](http://arxiv.org/abs/2301.10327)

    提出了一个名为Clugen的模块化合成数据生成过程，能够使用任意分布创建支持线段的多维聚类，适用于评估聚类算法并有潜力成为广泛使用的框架。

    

    合成数据对于评估聚类技术、补充和扩展真实数据以及允许更完整地覆盖给定问题空间至关重要。而合成数据生成器具有创建大量数据的潜力，这在真实世界数据稀缺时至关重要，同时提供了一个被充分理解的生成过程和一个可解释的工具，以系统地研究聚类分析算法。在这里，我们提出了一个名为Clugen的模块化合成数据生成过程，能够使用任意分布创建支持线段的多维聚类。Clugen是开源的，有全面的单元测试和文档，并且适用于Python、R、Julia和MATLAB/Octave生态系统。我们证明了我们的方法可以在各个维度上产生丰富多样的结果，适合用于评估聚类算法，并有潜力成为各个领域中广泛使用的框架。

    Synthetic data is essential for assessing clustering techniques, complementing and extending real data, and allowing for more complete coverage of a given problem's space. In turn, synthetic data generators have the potential of creating vast amounts of data -- a crucial activity when real-world data is at premium -- while providing a well-understood generation procedure and an interpretable instrument for methodically investigating cluster analysis algorithms. Here, we present Clugen, a modular procedure for synthetic data generation, capable of creating multidimensional clusters supported by line segments using arbitrary distributions. Clugen is open source, comprehensively unit tested and documented, and is available for the Python, R, Julia, and MATLAB/Octave ecosystems. We demonstrate that our proposal can produce rich and varied results in various dimensions, is fit for use in the assessment of clustering algorithms, and has the potential to be a widely used framework in diverse 
    
[^232]: 重新思考科学发现中符号回归数据集和基准

    Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10540](http://arxiv.org/abs/2206.10540)

    本文重新审视并重新创建了符号回归数据集，以评估符号回归在科学发现中的潜力和能力，并提出使用归一化编辑距离进行度量。

    

    本文重新审视符号回归（SR）的数据集和评估标准，特别关注其在科学发现中的潜力。针对现有数据集中基于费曼物理讲义的一组公式，我们重新创建了120个数据集，讨论符号回归在科学发现中的性能（SRSD）。对于这120个SRSD数据集，我们仔细审查了公式及其变量的属性，设计了合理的实值范围来采样值，以便我们的新SRSD数据集可用于评估SRSD的潜力，如SR方法是否能从这样的数据集中（重新）发现物理定律。我们还创建了另外120个包含虚拟变量的数据集，以检验SR方法是否能够仅选择必要变量。另外，我们提出使用预测方程与真实方程树之间的归一化编辑距离（NED）来解决现有SR度量存在的一个关键问题，即二元度量问题。

    This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
    
[^233]: 基于置信泰森堡分配的伪标签方法

    Confident Sinkhorn Allocation for Pseudo-Labeling. (arXiv:2206.05880v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05880](http://arxiv.org/abs/2206.05880)

    本文提出了一种基于置信泰森堡分配的伪标签方法，通过最优传输仅对高置信度样本进行伪标签分配，在半监督学习方面取得了目前最好的表现。

    

    半监督学习是减少机器学习对标记数据依赖的重要工具。它通过利用预训练模型或数据增强的内在空间和语义结构成功应用于结构化数据，例如图像和自然语言。然而，当数据没有适当的结构或不变性时，这些方法不适用。由于其简单性，伪标签（PL）方法可以在没有任何领域假设的情况下广泛使用。然而，PL 对阈值敏感，如果由于过度自信导致错误分配，则可能表现不佳。本文从理论上研究了不确定性对伪标签的作用，并提出了基于置信泰森堡分配的方法(CSA)，通过最优传输仅对高置信度样本进行伪标签分配。CSA 在半监督学习这一实际重要领域中胜过了当前的最先进方法。此外，所提出的方法具有可扩展性，高效性和易实现性。

    Semi-supervised learning is a critical tool in reducing machine learning's dependence on labeled data. It has been successfully applied to structured data, such as images and natural language, by exploiting the inherent spatial and semantic structure therein with pretrained models or data augmentation. These methods are not applicable, however, when the data does not have the appropriate structure, or invariances. Due to their simplicity, pseudo-labeling (PL) methods can be widely used without any domain assumptions. However, PL is sensitive to a threshold and can perform poorly if wrong assignments are made due to overconfidence. This paper studies theoretically the role of uncertainty to pseudo-labeling and proposes Confident Sinkhorn Allocation (CSA), which identifies the best pseudo-label allocation via optimal transport to only samples with high confidence scores. CSA outperforms the current state-of-the-art in this practically important area of semi-supervised learning. Additiona
    
[^234]: AutoGL：自动图学习库

    AutoGL: A Library for Automated Graph Learning. (arXiv:2104.04987v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.04987](http://arxiv.org/abs/2104.04987)

    AutoGL是第一个专门用于自动图机器学习的开源库，使用方便且易于扩展。它提供了一个完整的自动图学习流程，并支持各种图应用。

    

    近年来，图上的机器学习的研究兴趣和应用不断增加。然而，为不同的图数据集和任务手动设计最优的机器学习算法是不灵活、费时，并且需要专业知识，限制了其适应性和适用性。图上自动机器学习（AutoML）旨在为给定的图数据集和任务自动设计最优的机器学习算法，受到了相当大的关注。然而，现有的库中没有一个能完全支持图上的AutoML。为了填补这一空白，我们提出了自动图学习（AutoGL），这是第一个专门用于自动图机器学习的库。AutoGL是开源的，易于使用，而且易于扩展。具体而言，我们提出了一个包含与设备交互的后端、完整的自动图学习流程和支持的图应用的三层架构。

    Recent years have witnessed an upsurge in research interests and applications of machine learning on graphs. However, manually designing the optimal machine learning algorithms for different graph datasets and tasks is inflexible, labor-intensive, and requires expert knowledge, limiting its adaptivity and applicability. Automated machine learning (AutoML) on graphs, aiming to automatically design the optimal machine learning algorithm for a given graph dataset and task, has received considerable attention. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first dedicated library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose a three-layer architecture, consisting of backends to interface with devices, a complete automated graph learning pipeline, and supported graph applications. The automated machine learning
    

