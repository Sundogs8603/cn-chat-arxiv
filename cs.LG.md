# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks.](http://arxiv.org/abs/2306.17844) | 本研究发现，神经网络在已知算法任务中有时会发现质态不同的算法。即使对模型进行微小调整，也可能出现并行实现多个算法的情况。这一结论表明，即使是简单的学习问题，也可以有令人惊讶的多样解决方案。 |
| [^2] | [Resetting the Optimizer in Deep RL: An Empirical Study.](http://arxiv.org/abs/2306.17833) | 在深度强化学习中，当优化问题的风景在不同迭代中差异较大时，重置优化器的内部参数可以避免污染和提高性能。 |
| [^3] | [Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm.](http://arxiv.org/abs/2306.17829) | 本文研究了联合学习算法在目标检测中的应用，并将其与集中式训练方法进行性能比较。实验结果表明，使用联合学习训练的YOLOv5模型在生成准确的未见过目标的边界框方面具有显著优势。 |
| [^4] | [Understanding Unfairness via Training Concept Influence.](http://arxiv.org/abs/2306.17828) | 通过观察训练数据的作用，研究模型不公平性的来源和影响，并通过改变样本的属性来计算训练样本对模型的不公平性的影响。 |
| [^5] | [Scalable tensor methods for nonuniform hypergraphs.](http://arxiv.org/abs/2306.17825) | 本论文提出了一种可扩展的张量方法，用于处理非均匀超图。通过开发新的TTSV算法，我们能够在低于指数复杂度的情况下处理邻接张量，并应用于超图中心性和聚类等问题。这些方法不仅能提供与图缩减方法互补的信息，还能够探测到高阶结构。 |
| [^6] | [Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation.](http://arxiv.org/abs/2306.17817) | Act3D是一种基于Transformer的操作策略，将6自由度关键姿势预测作为3D检测任务，并以自适应空间计算的方式进行处理。它在高度精确的机器人操纵任务中取得了显著的性能改进。 |
| [^7] | [Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction.](http://arxiv.org/abs/2306.17815) | 本文提出了一种基于贝叶斯优化的方法，无论约束函数的特性如何，都能满足安全要求。 |
| [^8] | [Stay on topic with Classifier-Free Guidance.](http://arxiv.org/abs/2306.17806) | 本论文展示了分类器无关的指导（CFG）可以作为一种推断时间技术，显著提高了纯语言建模中各种任务的性能，并能够增强助手在具有挑战性的提示中的准确性和一致性。 |
| [^9] | [Hierarchical Bayesian Regression for Multi-Location Sales Transaction Forecasting.](http://arxiv.org/abs/2306.17795) | 本文介绍了一种层次贝叶斯回归模型，用于预测多地点销售交易，并通过共享推理结果在群组之间进行泛化。研究表明，该模型在预测商店特许经营店购买情况方面取得了进展。 |
| [^10] | [Vision Through the Veil: Differential Privacy in Federated Learning for Medical Image Classification.](http://arxiv.org/abs/2306.17794) | 本研究将差分隐私技术应用于医学图像分类的联邦学习中，通过引入一种新颖的差分隐私联邦学习模型，平衡模型准确性与隐私设置，为医疗保健领域的深度学习应用提供了隐私保护解决方案。 |
| [^11] | [Look, Remember and Reason: Visual Reasoning with Grounded Rationales.](http://arxiv.org/abs/2306.17778) | 在这项研究中，我们借鉴了人类视觉问题解决的方法，通过三个步骤（看、记住、推理）逐步提取视觉信息来解决复杂的视觉推理问题，从而使大型语言模型能够有效解决这些问题。 |
| [^12] | [Practical and Asymptotically Exact Conditional Sampling in Diffusion Models.](http://arxiv.org/abs/2306.17775) | 本论文提出了一种名为TDS的扭转式扩散采样器，它是一种针对扩散模型的顺序蒙特卡洛算法。该方法通过使用扭转技术结合启发式近似，能够在不需要特定训练的情况下在广泛的条件分布上提供精确的样本。 |
| [^13] | [Precision Anti-Cancer Drug Selection via Neural Ranking.](http://arxiv.org/abs/2306.17771) | 通过神经排序方法，准确选择和优先排序敏感药物来进行个性化抗癌治疗。 |
| [^14] | [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit.](http://arxiv.org/abs/2306.17759) | 在无限深度和宽度的比例极限下，我们通过修改Softmax-based注意力模型，研究了Transformer的协方差矩阵。我们发现在初始化时，极限分布可以用随机微分方程来描述。通过修改注意力机制并使用残差连接，我们可以控制网络的稳定性和协方差结构的行为。 |
| [^15] | [TD Convergence: An Optimization Perspective.](http://arxiv.org/abs/2306.17750) | 本研究从优化的视角研究了时差(TD)学习算法的收敛行为，在经典反例中确定了影响算法收敛或发散的两个力量，并在线性逼近和平方损失以外的情况下证明了TD的收敛性。这一研究为TD在强化学习领域的成功应用提供了理论解释。 |
| [^16] | [Why Deep Models Often cannot Beat Non-deep Counterparts on Molecular Property Prediction?.](http://arxiv.org/abs/2306.17702) | 本研究表明，尽管深度神经网络在分子属性预测任务上已取得进展，但在大多数情况下，它们仍无法击败传统的非深度模型。关键原因在于分子数据的不规则模式，而使用分子指纹作为输入的树模型在这方面表现更好。 |
| [^17] | [Beyond Neural-on-Neural Approaches to Speaker Gender Protection.](http://arxiv.org/abs/2306.17700) | 本文超越了基于神经网络的方法，提出了一种超越性别保护的研究方法，并强调了测试基于语音特征的性别推测攻击的重要性，以及与人类执行的声音适应进行比较。 |
| [^18] | [Thompson sampling for improved exploration in GFlowNets.](http://arxiv.org/abs/2306.17693) | 本文介绍了一种将生成流网络（GFlowNets）中的采样问题视为主动学习问题，并使用贝叶斯技术中的Thompson采样方法来解决的算法。实验结果表明该算法可以有效改进探索性能。 |
| [^19] | [Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering.](http://arxiv.org/abs/2306.17690) | 本文提出了一种泛化时间扭曲不变字典学习算法，用于处理时间序列数据的模式识别和分类。该算法通过使用连续基函数的线性组合来构建泛化时间扭曲算子，以实现连续的时间扭曲。通过联合优化扭曲路径、字典和稀疏系数，我们的算法在时间序列分类和聚类任务中取得了优秀的性能。 |
| [^20] | [Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings.](http://arxiv.org/abs/2306.17670) | 本文提出了一种新的离散时间算法，通过反向传播学习尖峰神经网络(SNNs)中的延迟，提高了SNNs在节能信息处理系统中的表达能力。 |
| [^21] | [Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies.](http://arxiv.org/abs/2306.17648) | 这项研究提出了一种基于域分解的预条件策略，用于增强物理信息神经网络的训练。该方法通过非线性预条件器改进了L-BFGS优化器的收敛性，并提供了更准确的偏微分方程解。加性预条件器还具有并行性，为模型并行提供了新的方法。 |
| [^22] | [Federated Object Detection for Quality Inspection in Shared Production.](http://arxiv.org/abs/2306.17645) | 本文提出了一个利用联邦学习进行质量检验任务中的目标检测的算法，该算法使用YOLOv5作为目标检测算法和Federated Averaging作为联邦学习算法。实验结果表明，该联邦学习方法在整体客户测试数据集上具有更好的泛化性能，并生成相对于使用本地客户数据集训练的模型更精确的边界框。 |
| [^23] | [Geometric Autoencoders -- What You See is What You Decode.](http://arxiv.org/abs/2306.17638) | 这篇论文介绍了一种名为几何自编码器的方法，通过从微分几何的角度对解码器进行优化，避免了可视化结果的失真，使得数据结构能够更准确地被捕捉到。 |
| [^24] | [Achieving RGB-D level Segmentation Performance from a Single ToF Camera.](http://arxiv.org/abs/2306.17636) | 本文使用单个ToF相机的红外和深度图像，在语义分割任务上表现出与RGB-D相机相当的准确性，通过引入深度专用卷积的多任务学习方法，展示了在车内分割数据集上的竞争力。 |
| [^25] | [Impact of Noise on Calibration and Generalisation of Neural Networks.](http://arxiv.org/abs/2306.17630) | 本研究研究了不同类型噪声对神经网络的校准和泛化的影响，发现激活噪声能最有效地提高泛化性能，而输入增强噪声则能显著改善分布外的校准。 |
| [^26] | [Design of Induction Machines using Reinforcement Learning.](http://arxiv.org/abs/2306.17626) | 该论文介绍了一种使用强化学习算法设计定制的感应电动机的方法，通过模拟多个电机设计实例进行离线训练，使得电机设计自动化并满足特定的操作要求。 |
| [^27] | [Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions.](http://arxiv.org/abs/2306.17624) | Sphere2Vec是一种多尺度位置编码器，用于在球面上编码点坐标时保持球面距离，解决了大规模真实世界GPS坐标数据集中的距离度量问题。 |
| [^28] | [Navigation of micro-robot swarms for targeted delivery using reinforcement learning.](http://arxiv.org/abs/2306.17598) | 通过强化学习算法控制微型机器人群体，实现了针对性的药物输送，具有很大的应用潜力。 |
| [^29] | [Variational principle to regularize machine-learned density functionals: the non-interacting kinetic-energy functional.](http://arxiv.org/abs/2306.17587) | 本文为机器学习的密度泛函引入了一种新的规范方法，其中包括非相互作用动能泛函。该方法通过使用基于深度神经网络的密度泛函训练，取得了在一维系统上的优秀结果。 |
| [^30] | [ChatGPT for Robotics: Design Principles and Model Abilities.](http://arxiv.org/abs/2306.17582) | 本文介绍了使用ChatGPT进行机器人应用的实验研究，通过设计原则和函数库的结合，ChatGPT能够适应不同的机器人任务，并展示了在各种机器人任务中的有效性和多样性。 |
| [^31] | [Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters.](http://arxiv.org/abs/2306.17575) | 这项研究通过机器学习模型的实证评估发现，在大学录取过程中排除受保护属性会导致预测表现下降，而通过使用文本信息可以部分恢复模型的性能。 |
| [^32] | [Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting.](http://arxiv.org/abs/2306.17563) | 本论文提出了一种名为PRP的新技术，通过使用两两排名提示来显著减轻大型语言模型（LLM）的负担，并首次在标准基准测试中实现了最先进的排名性能。 |
| [^33] | [Class-Incremental Learning using Diffusion Model for Distillation and Replay.](http://arxiv.org/abs/2306.17560) | 本文提出了一种使用预训练的扩散模型作为增量学习的附加数据源的方法，通过生成属于先前遇到的图像所属类别的合成样本，并在蒸馏损失和分类损失中使用这些样本，进一步提高了模型的性能。 |
| [^34] | [TTSWING: a Dataset for Table Tennis Swing Analysis.](http://arxiv.org/abs/2306.17550) | TTSWING是一种专为乒乓球挥拍分析设计的数据集，通过集成传感器获取详细信息并与运动员数据一起发布。对于乒乓球分析的创新研究具有巨大潜力，对科学界来说是宝贵的资源。 |
| [^35] | [DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place Recognition under Adverse Conditions.](http://arxiv.org/abs/2306.17536) | 本研究通过利用先验地图来改进动态车辆检测，无需使用3D地图或像素级地图查询对应。通过视觉场所识别和二进制分类神经网络，我们成功优化了初始的候选物体检测，产生了更准确的检测结果。该方法在恶劣的天气和光照条件下表现出优异的性能。 |
| [^36] | [Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization.](http://arxiv.org/abs/2306.17529) | 该论文提出了一种利用动态车辆施加的运动约束来改善视觉定位的方法，通过在自动驾驶车辆的背景下使用动态车辆在定位流程中提供有限姿态约束信息，优化姿态估计并计算未来姿态估计质量，从而提高了定位的鲁棒性和准确性。 |
| [^37] | [Efficient uniform approximation using Random Vector Functional Link networks.](http://arxiv.org/abs/2306.17501) | 本文研究了使用随机向量功能连接网络进行高效统一逼近的方法，证明了具有ReLU激活函数的RVFL网络可以逼近利普希茨连续函数，前提是隐藏层相对于输入维度是指数级宽度的。这是第一个证明了$L_\infty$逼近误差和高斯内部权重条件下的结果，给出了非渐进性的隐藏层节点数量下界。 |
| [^38] | [Empirical Interpretation of the Relationship Between Speech Acoustic Context and Emotion Recognition.](http://arxiv.org/abs/2306.17500) | 本研究通过使用基于注意力的方法，探讨了声学背景和音位边界对语音情感识别中局部标记的影响，并发现使用分布式方法进行语音情感理解的好处。 |
| [^39] | [The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks.](http://arxiv.org/abs/2306.17499) | 本文研究了在训练单隐藏层的多变量ReLU网络时，随机梯度下降收敛到何种解。我们发现线性稳定的最小值对应于具有有界$L^1$范数的预测器的二阶导数。我们将这个结果推广到多变量情况，并证明了与损失函数的稳定最小值对应的ReLU网络的逼近能力很低。 |
| [^40] | [Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers.](http://arxiv.org/abs/2306.17486) | 通过结合多网格求解器和卷积神经网络，该论文提出了一种用于解决离散异质Helmholtz方程的迭代深度学习方法，在可伸缩性和求解速度上优于传统方法。其中的三个主要创新包括引入隐式层来解决CNN中的视野问题、改进CNN预条件技术以提高性能，并提出了一种多尺度训练方法使网络能够处理不同尺寸的问题。 |
| [^41] | [Landmark Guided Active Exploration with Stable Low-level Policy Learning.](http://arxiv.org/abs/2306.17484) | 本文设计了一种稳定的低层策略学习方法，通过目标导向的分级强化学习和地标引导的探索策略，在提高训练效率的同时解决了高层策略行动空间过大和低层策略的非稳态问题。 |
| [^42] | [Graphtester: Exploring Theoretical Boundaries of GNNs on Graph Datasets.](http://arxiv.org/abs/2306.17482) | 本文提出了一个名为Graphtester的新工具，用于探索图数据集上GNNs的理论边界。通过分析40多个不同的图数据集，我们确定了各种GNNs性能的上限，并展示了该工具对图变换器的适用性。最后，我们证明了Graphtester生成的特征可以用于实际应用。 |
| [^43] | [FedBone: Towards Large-Scale Federated Multi-Task Learning.](http://arxiv.org/abs/2306.17465) | FedBone是一个创新的框架，通过服务器-客户端分离学习和梯度投影的角度，实现了构建大规模模型和更好的泛化能力。它解决了现有联邦多任务学习方法无法直接应用大规模模型以及忽略梯度冲突对多任务优化的问题。 |
| [^44] | [Provable Robust Watermarking for AI-Generated Text.](http://arxiv.org/abs/2306.17439) | GPTWatermark是一种针对性模型水印技术，通过固定分组设计和强大的可证明保证，提供了对AI生成文本的鲁棒性检测和安全性防御。实验证明了其在检测准确性和生成质量方面的优越性，推动了LLMs负责任使用的进步。 |
| [^45] | [ReLU Neural Networks, Polyhedral Decompositions, and Persistent Homolog.](http://arxiv.org/abs/2306.17418) | 通过ReLU神经网络，我们发现了将有限多面体分解与持续同调结合使用来检测输入空间中流形的同调信号的方法，并发现这种方法对于各种训练目的的网络都具有普适性。 |
| [^46] | [Physics-informed invertible neural network for the Koopman operator learning.](http://arxiv.org/abs/2306.17396) | 本论文提出了一种基于物理信息的可逆神经网络，用于学习Koopman算子。 FlowDMD算法利用耦合流可逆神经网络的特性，学习Koopman算子的不变子空间，并准确重构状态变量。实验证明了该算法的优越性能。 |
| [^47] | [Global Optimality in Bivariate Gradient-based DAG Learning.](http://arxiv.org/abs/2306.17378) | 本文研究了双变量基于梯度的有向无环图(DAG)学习问题，通过证明一种路径跟踪优化算法的全局收敛性，提供了该问题的全局最优解。 |
| [^48] | [$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces.](http://arxiv.org/abs/2306.17366) | 这项研究提出了一种$\lambda$-AC算法，通过学习连续状态空间中的潜在决策感知模型，实现了决策驱动的强化学习。通过理论和实证研究，确定了决策感知强化学习模型的必要组成部分，并展示了设计选择对算法性能的重要影响。 |
| [^49] | [Improving Federated Aggregation with Deep Unfolding Networks.](http://arxiv.org/abs/2306.17362) | 本论文通过引入深度展开网络(DUN)技术，学习自适应权重，以无偏方式改善联邦学习中设备差异和统计特性的负面影响。所提出的方法展示了令人印象深刻的精确性和质量感知的聚合，有效解决了参与客户端的异构性和FL环境下的质量感知聚合问题。 |
| [^50] | [iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models.](http://arxiv.org/abs/2306.17361) | 本文提出了一种识别非线性加性噪声模型中因果机制转变的方法，该方法专注于在相关的结构因果模型中识别功能机制的变化，而不需要估计整个有向无环图(DAG)的结构。 |
| [^51] | [A Survey on Blockchain-Based Federated Learning and Data Privacy.](http://arxiv.org/abs/2306.17338) | 本调查比较了基于区块链的联邦学习架构中采用的各种数据隐私机制的性能和安全性。 |
| [^52] | [Designing Stable Neural Networks using Convex Analysis and ODEs.](http://arxiv.org/abs/2306.17332) | 通过使用凸分析和ODE，设计了一种稳定的神经网络架构，该架构编码非扩张算子，并能够通过约束权重的谱范数来限制Lipschitz常数的增长。此架构还可以被应用于学习去噪器，并通过一种自适应的方式来保证性能优越。 |
| [^53] | [Kernel $\epsilon$-Greedy for Contextual Bandits.](http://arxiv.org/abs/2306.17329) | 本文提出了基于核的$\epsilon$-贪心策略应用于情境脉冲中的方法，通过在线加权核岭回归估计器实现对奖励函数的估计，并证明了其一致性和依赖于RKHS维度的次线性后悔率，在有限维RKHS的边际条件下实现了最优后悔率。 |
| [^54] | [Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version).](http://arxiv.org/abs/2306.17323) | 该论文通过状态空间缩减和输入分割提出了一个扩展DNN分析的模型检验框架，解决了模型检验的可扩展性问题。 |
| [^55] | [Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study.](http://arxiv.org/abs/2306.17301) | 本文通过数值研究探讨了浅层神经网络在逼近和学习高频率方面的困难，重点是通过分析激活函数的谱分析来理解问题的原因。 |
| [^56] | [Probabilistic Constraint for Safety-Critical Reinforcement Learning.](http://arxiv.org/abs/2306.17279) | 本文研究了概率约束下的安全关键强化学习问题，提出了具有明确梯度表达式的Safe Policy Gradient-REINFORCE（SPG-REINFORCE）算法，并通过理论界限证明了概率约束设置在最优性和安全性之间具有更好的权衡。 |
| [^57] | [DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios.](http://arxiv.org/abs/2306.17271) | DisasterResponseGPT是一种利用大规模语言模型（LLMs）的算法，旨在通过上下文学习快速生成有效行动方案，可在灾害响应场景中加快规划过程。DisasterResponseGPT生成的行动方案与人工生成的方案相当，并提供了实时修改的便利，有可能在执行过程中实现快速更新和调整，从而革新灾害响应行动方式。 |
| [^58] | [Fast and Robust State Estimation and Tracking via Hierarchical Learning.](http://arxiv.org/abs/2306.17267) | 本文通过使用分层系统架构和共识+创新算法，加快了状态估计和跟踪的收敛速度，并增强了鲁棒性。 |
| [^59] | [Subgraph Stationary Hardware-Software Inference Co-Design.](http://arxiv.org/abs/2306.17266) | 本文提出了一个子图静态硬件软件推理联合设计的案例，针对在动态变化的部署场景中运行的应用程序，通过利用权重共享的SuperNet机制，能够在延迟-准确性权衡中展现出更好的表现。 |
| [^60] | [Suffering Toasters.](http://arxiv.org/abs/2306.17258) | 本文旨在为人工智能、自我意识和代理问题提供更清晰的定义，我们提出了一种新的启发式方法来测试人工自我意识，并讨论了这种方法引发的一些问题。 |
| [^61] | [Prediction of COVID-19 Patients' Emergency Room Revisit using Multi-Source Transfer Learning.](http://arxiv.org/abs/2306.17257) | 本研究利用迁移学习和自然语言处理技术，预测COVID-19患者出院后在急诊室的再访情况，早期识别有助于医生专注于危及生命的病例。 |
| [^62] | [Towards Zero-Shot Scale-Aware Monocular Depth Estimation.](http://arxiv.org/abs/2306.17253) | 针对单目深度估计的尺度不确定性问题，提出了一种能够在不同领域和相机参数的任意测试图像中预测度量尺度的ZeroDepth框架，通过输入级几何嵌入和变分潜在表示实现了尺度先验的学习和编码器解码器阶段的解耦，在室内和室外基准测试中取得了最新最优结果。 |
| [^63] | [TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures.](http://arxiv.org/abs/2306.17248) | TemperatureGAN是一个生成对抗网络，使用地面以上2m的大气温度数据，能够生成具有良好空间表示和与昼夜周期一致的时间动态的高保真样本。 |
| [^64] | [The power of motifs as inductive bias for learning molecular distributions.](http://arxiv.org/abs/2306.17246) | 本研究通过探索子图结构和词汇设计对分布学习的影响，引入了一种新的基于子图的分段方案Subcover，并通过两步变分自编码器对其进行评估。结果显示，Subcover的改进子图鉴别能力使得FCD得分相对提高了30％，超过了先前的方法。这些发现表明，Subcover有潜力提高现有方法的性能和可伸缩性，并对改进分子生成方法做出贡献。 |
| [^65] | [Scattering Spectra Models for Physics.](http://arxiv.org/abs/2306.17210) | 本文介绍了物理学中的散射谱模型，用于描述各种场的统计特性。这些模型基于散射系数的协方差，结合了场的小波分解和点位模，能够准确且稳健地重现标准统计量，捕捉了关键特性。 |
| [^66] | [Learning Environment Models with Continuous Stochastic Dynamics.](http://arxiv.org/abs/2306.17204) | 本文提出了一种学习环境模型的方法，通过应用降维和聚类在观测到的环境状态空间上计算一个抽象状态空间表示，并通过 passvie automata learning 学习到基于观测到的 agent 和环境交互的随机转移。 |
| [^67] | [Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models.](http://arxiv.org/abs/2306.17203) | Diff-Foley是一种使用潜在扩散模型实现同步的视频到音频合成方法，通过对比式音视频预训练来学习特征，并在声谱图潜在空间上训练潜在扩散模型，以提高生成音频的同步性和音视频关联性。 |
| [^68] | [An end-to-end framework for gene expression classification by integrating a background knowledge graph: application to cancer prognosis prediction.](http://arxiv.org/abs/2306.17202) | 提出了一种将背景知识图谱整合到基因表达分类的端到端框架，应用于癌症预后预测。实验证明，该框架相比于不包含背景生物网络信息的深度神经网络模型，具有更高的准确性，并且在多个癌症类型的分类中都取得了改善。 |
| [^69] | [Residual Feature Pyramid Network for Enhancement of Vascular Patterns.](http://arxiv.org/abs/2306.17200) | 提出了一种针对指静脉图案增强的残差特征金字塔网络，通过自底向上的金字塔结构和特征聚合模块来提高指静脉识别的准确性，实验证明在常用的识别流程中能够减少高达5%的平均识别错误率。 |
| [^70] | [Guided Deep Generative Model-based Spatial Regularization for Multiband Imaging Inverse Problems.](http://arxiv.org/abs/2306.17197) | 本研究提出了一个通用框架，利用深度学习和高空间分辨率的辅助图像来指导多波段成像逆问题的空间正则化，从而提升重建图像的质量和准确性。 |
| [^71] | [On the Exploitability of Instruction Tuning.](http://arxiv.org/abs/2306.17194) | 该论文研究了如何利用指令调整技术来改变模型行为的问题，并提出了一种自动数据注入的方法AutoPoison。实验结果表明，通过少量的训练数据毒化，对手能够改变模型的行为。 |
| [^72] | [Limits of Machine Learning for Automatic Vulnerability Detection.](http://arxiv.org/abs/2306.17193) | 机器学习在自动漏洞检测方面取得了很大的进展，但其结果是否普适仍存在疑问。本研究通过注入语义保持的更改来扩大测试集，并发现模型准确率显著下降，这表明模型在分类时使用了一些无关的特征。通过在扩展的训练数据上进行训练，模型的准确率恢复到之前的水平。本文提出了一种可行的模型基准测试方法，以帮助研究者更好地评估机器学习在漏洞检测方面的进展。 |
| [^73] | [Classification and Explanation of Distributed Denial-of-Service (DDoS) Attack Detection using Machine Learning and Shapley Additive Explanation (SHAP) Methods.](http://arxiv.org/abs/2306.17190) | 本论文提出了一个框架，利用机器学习和SHAP方法实现对分布式拒绝服务（DDoS）攻击的分类和解释，以增加模型可信度。 |
| [^74] | [Steganographic Capacity of Deep Learning Models.](http://arxiv.org/abs/2306.17189) | 本研究考虑了几个学习模型的隐写能力，并发现这些模型的隐写能力非常高，且存在一个明显的门限，门限之后模型性能迅速下降。 |
| [^75] | [Why can neural language models solve next-word prediction? A mathematical perspective.](http://arxiv.org/abs/2306.17184) | 本文研究了神经语言模型在下一个词预测任务中的成功，在形式语言理论背景下，提出了一种为什么神经语言模型能够学习到组合规则的解释，并在一个现实世界的英语句子示例中提供了零错误的证明。 |
| [^76] | [Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis.](http://arxiv.org/abs/2306.17181) | 本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。 |
| [^77] | [Integrating Tick-level Data and Periodical Signal for High-frequency Market Making.](http://arxiv.org/abs/2306.17179) | 该论文提出了一种融合滴级数据和周期预测信号的深度强化学习方法，用于开发更准确、更稳健的高频市场做市策略。实验证明，该方法在盈利能力和风险管理方面优于现有方法。 |
| [^78] | [Optimal Execution Using Reinforcement Learning.](http://arxiv.org/abs/2306.17178) | 本论文研究了使用强化学习进行最优交易执行的问题，在考虑多个交易所信号的情况下，通过对齐数据提取交叉交易所信号，并发现它们对最优执行过程具有积极的影响。 |
| [^79] | [Enterprise Disk Drive Scrubbing Based on Mondrian Conformal Predictors.](http://arxiv.org/abs/2306.17169) | 提出了一种基于Mondrian Conformal预测器的企业磁盘驱动刷新方法，通过使用机器学习模型识别需要刷新的磁盘，并提前预测其健康状态，从而提高整体可靠性和功率效率。 |
| [^80] | [Elastically-Constrained Meta-Learner for Federated Learning.](http://arxiv.org/abs/2306.16703) | 这项研究提出了一种弹性约束的元学习方法，用于解决联邦学习中由于非独立同分布数据导致元学习的不稳定目标的收敛问题。 |
| [^81] | [Dynamic-Resolution Model Learning for Object Pile Manipulation.](http://arxiv.org/abs/2306.16700) | 本文研究了对象堆叠操作的动态分辨率模型学习，通过构建动态分辨率的粒子环境表示并使用图神经网络进行学习，实现了学习的动态和自适应表示，在对象堆叠操作任务中取得了良好的效果。（Translated from Abstract） |
| [^82] | [DUET: 2D Structured and Approximately Equivariant Representations.](http://arxiv.org/abs/2306.16058) | DUET是一种2D结构化且近似等变表示方法，相比于其他方法，可以在保留输入变换信息的同时具有更好的可控性和更高的准确性。 |
| [^83] | [See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging.](http://arxiv.org/abs/2306.15574) | 本文提出了一种基于课程学习的方法，用于训练深度学习模型有效处理医学图像中的遮挡情况。通过逐步引入遮挡，模型首先学习简单、可辨别的模式，然后逐渐理解更复杂的遮挡场景。 |
| [^84] | [Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints.](http://arxiv.org/abs/2306.12857) | 本文提出了一种基于信息丢失约束的大规模公共安全时空数据高效划分方法(IFL-LSTP)，可以显著减小数据规模，同时保持模型的准确性，确保分布式存储的负载平衡，同时保持数据划分的时空接近性。 |
| [^85] | [Online Learning with Set-Valued Feedback.](http://arxiv.org/abs/2306.06247) | 本文研究了一种在线多类分类的变体，其中使用集合型反馈。通过引入新的组合维度，该论文表明确定性和随机性的在线可学习性在实现设置下不等价，并将在线多标签排名和在线多标签分类等实际学习设置作为其特定实例。 |
| [^86] | [Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular Design.](http://arxiv.org/abs/2306.04620) | 本文研究了在多属性优化问题中，一种更好的，基于目标条件下的分子设计模型，以此解决现有标量化方案所无法解决的问题。 |
| [^87] | [Self-Adjusting Weighted Expected Improvement for Bayesian Optimization.](http://arxiv.org/abs/2306.04262) | 本文提出了一种新的自适应加权期望改进方法（SAWEI），可以自动平衡探索不确定区域和利用有承诺区域之间的权衡。在COCO基准测试中，该方法表现出有利的性能。 |
| [^88] | [Transfer learning for atomistic simulations using GNNs and kernel mean embeddings.](http://arxiv.org/abs/2306.01589) | 本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。 |
| [^89] | [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning.](http://arxiv.org/abs/2305.15912) | 本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。 |
| [^90] | [Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs.](http://arxiv.org/abs/2305.12334) | 本研究提出了一种基于学习的模拟模型，称为GNSTODE，通过利用统一的端到端框架描述了粒子系统中不同时间和不同空间条件下的变化。 |
| [^91] | [Robust Implicit Regularization via Weight Normalization.](http://arxiv.org/abs/2305.05448) | 本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。 |
| [^92] | [String Diagrams with Factorized Densities.](http://arxiv.org/abs/2305.02506) | 本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。 |
| [^93] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^94] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^95] | [GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning.](http://arxiv.org/abs/2304.04970) | 本文提出一种名为GRIL的方法，用于将拓扑特征表示散度到机器学习模型中，该方法可以稳定地用于不同的过滤函数。 |
| [^96] | [Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification.](http://arxiv.org/abs/2304.02836) | 本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。 |
| [^97] | [On the Existence of a Complexity in Fixed Budget Bandit Identification.](http://arxiv.org/abs/2303.09468) | 该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。 |
| [^98] | [Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?.](http://arxiv.org/abs/2303.06021) | 该论文研究了机器学习在体育博彩中的应用，提出了优化预测模型校准性比准确度更重要的假设，并通过实验证明了此假设的正确性。 |
| [^99] | [Uncertainty Estimation by Fisher Information-based Evidential Deep Learning.](http://arxiv.org/abs/2303.02045) | 本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。 |
| [^100] | [First-order ANIL learns linear representations despite misspecified latent dimension.](http://arxiv.org/abs/2303.01335) | 本研究表明，在存在架构误指定的情况下，初阶ANIL可以成功学习到线性的共享表示。这个结果是基于对无限数量任务的极限情况下的推导。 |
| [^101] | [Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures.](http://arxiv.org/abs/2302.12317) | 本文介绍了逐层相关传播（LRP）在不同的人工神经网络（ANN）架构上的修订和应用。LRP通过可视化相关性热图揭示模型预测的影响原因，但需要注意其中可能存在的人工制品。 |
| [^102] | [MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection.](http://arxiv.org/abs/2302.10739) | 本文提出了一种专门为恶意软件检测领域设计的状态防御技术MalProtect，它通过实现一种新颖的查询分类方法来检测查询攻击，实验结果证明其有效性。 |
| [^103] | [Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity.](http://arxiv.org/abs/2302.03407) | 本论文提出了一种无需下水平强凸性的双层优化均值乘法方法(sl-BAMM)，通过对上下层目标求平均值，实现了在大规模BLO中高效而简单的求解。与其他方法相比，本论文的分析不需要强梯度有界性假设，适用范围更广泛。实验结果表明了该方法的优越性。 |
| [^104] | [UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers.](http://arxiv.org/abs/2301.13741) | UPop是一种通用的视觉语言Transformer压缩框架，采用统一和渐进式剪枝方法，可自动分配剪枝比率，实现更高的压缩比率。 |
| [^105] | [Bounded (O(1)) Regret Recommendation Learning via Synthetic Controls Oracle.](http://arxiv.org/abs/2301.12571) | 通过合成控制理论，本论文提出了一种实现有界遗憾的推荐学习方法，并解决了线性模型的精确知识、潜在协变量的存在、不均匀的用户到达速率和用户选择退出私人数据跟踪等实践中的问题。 |
| [^106] | [WDC Products: A Multi-Dimensional Entity Matching Benchmark.](http://arxiv.org/abs/2301.09521) | 本研究提出了WDC产品作为一个多维实体匹配基准，通过对角落案例数量、未见实体的泛化能力以及开发集大小等三个维度进行评估，来评估实体匹配系统的鲁棒性。 |
| [^107] | [Off-Policy Evaluation with Out-of-Sample Guarantees.](http://arxiv.org/abs/2301.08649) | 本文提出了一种具有样本外保证的离线策略评估方法，可以在考虑模型配置错误的情况下，使用观察数据对决策策略的性能进行有效推断。 |
| [^108] | [Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions.](http://arxiv.org/abs/2301.06535) | 案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。 |
| [^109] | [Expressive architectures enhance interpretability of dynamics-based neural population models.](http://arxiv.org/abs/2212.03771) | 研究通过顺序自动编码器从神经数据集中恢复潜在的混沌吸引子，发现采用神经常微分方程为基础的 SAES 在准确率和维度方面优于采用循环神经网络的 SAES。 |
| [^110] | [Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training.](http://arxiv.org/abs/2211.10737) | 本文提出了一种基于时代驱动的混合尾数HBFP技术，通过对不同参数的探索和优化，实现了对DNN训练中算术操作的更小编码。使用分析模型表明，该方法能够将HBFP训练加速器的算术密度增加高达$21.3\times$。 |
| [^111] | [GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond.](http://arxiv.org/abs/2211.01962) | 本研究在交互式决策的框架下，提出了一种新的复杂度度量GEC，用于样本高效强化学习。该方法能够捕捉到探索和开发之间的权衡，将RL问题划分为低GEC和高GEC两个类别，并展示了低GEC类别的丰富性质。 |
| [^112] | [ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation.](http://arxiv.org/abs/2210.17375) | ERL-Re$^2$提出了双尺度状态表示和策略表示的进化强化学习方法，解决了现有工作中忽视共享知识和语义级行为进化的问题。 |
| [^113] | [Structure-based Drug Design with Equivariant Diffusion Models.](http://arxiv.org/abs/2210.13695) | 该论文提出了一个基于结构的药物设计方法，使用了等变扩散模型DiffSBDD来生成具有亲和力和特异性的新型药物配体。实验结果表明DiffSBDD在生成具有竞争性对接得分的多样化药物样配体方面具有高效和有效的性能。 |
| [^114] | [Private Online Prediction from Experts: Separations and Faster Rates.](http://arxiv.org/abs/2210.13537) | 这篇论文提出了新的算法，用于在在线预测从专家中解决隐私约束的问题，并改进了现有算法的遗憾界限。研究结果表明，在纯差分隐私和近似差分隐私设置下，对于愚蠢敌手，在高维范围内的遗憾可以达到亚线性水平，与自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。 |
| [^115] | [Cooperative Multi-Agent Deep Reinforcement Learning for Reliable and Energy-Efficient Mobile Access via Multi-UAV Control.](http://arxiv.org/abs/2210.00945) | 本文提出了一种基于合作多智能体深度强化学习的算法，用于多个无人机的定位和通信的协作。该算法旨在实现可靠和高效的移动接入网络，以支持智能交通系统的发展。 |
| [^116] | [MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier.](http://arxiv.org/abs/2209.11549) | 本论文提出了一种名为MAGIC的方法，通过反转准鲁棒分类器进行一次性掩码引导的图像合成。它通过聚合梯度并利用强空间先验的指导二进制掩码，实现了形状和位置控制、非刚性形状变形以及复制/移动操作，并可简单指定二进制引导掩码来提供强大的合成控制。 |
| [^117] | [Learning Bilinear Models of Actuated Koopman Generators from Partially-Observed Trajectories.](http://arxiv.org/abs/2209.09977) | 该论文提出了一种从部分观测轨迹中学习作用Koopman生成器的方法，克服了现有方法依赖于基函数选择和观测不完整的限制。 |
| [^118] | [Lifelong Learning for Neural powered Mixed Integer Programming.](http://arxiv.org/abs/2208.12226) | 本论文研究了混合整数规划的终身学习范例，并提出了一种名为LIMIP的方法，在嵌入空间中建模MIP实例，通过应用知识蒸馏来避免灾难性遗忘。 |
| [^119] | [A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization.](http://arxiv.org/abs/2208.00290) | 本文提出了一种具有截断柯西随机扰动的随机梯度算法用于非凸目标函数的优化，算法具有稳定性与快速收敛性。 |
| [^120] | [Interactive Volume Visualization via Multi-Resolution Hash Encoding based Neural Representation.](http://arxiv.org/abs/2207.11620) | 本文通过同时利用现代GPU张量核心、本地CUDA神经网络框架以及具有宏单元加速的渲染算法，实现了交互式的体积神经表示光线追踪。这种神经表示具有高保真度和紧凑性，同时还开发了高效的离核训练策略以支持极大规模的体积数据。 |
| [^121] | [Sequential Recommendation Model for Next Purchase Prediction.](http://arxiv.org/abs/2207.06225) | 本文提出了一种顺序推荐系统，考虑了用户的购买顺序以预测他们的下一次购买，该模型利用大规模的信用卡交易数据集进行了验证和排名，展现了其在准确性和效果上的优势。 |
| [^122] | [Algorithms for bounding contribution for histogram estimation under user-level privacy.](http://arxiv.org/abs/2206.03008) | 该论文提出了一种在用户级隐私条件下对直方图估计的贡献边界算法。该算法可以在有界和无界域设置下选择最佳用户贡献边界，并近似达到最佳贡献边界的两倍近似。 |
| [^123] | [Improving Expert Predictions with Conformal Prediction.](http://arxiv.org/abs/2201.12006) | 本研究开发了一种自动决策支持系统，通过使用符合预测构建的标签预测集合，精确地权衡了真实标签不在预测集合中的概率。 |
| [^124] | [Sufficient-Statistic Memory AMP.](http://arxiv.org/abs/2112.15327) | 该论文提出了一种在特定条件下解决AMP类型算法收敛性问题的充分统计记忆型AMP算法框架，通过充分统计约束和特定条件下的协方差矩阵性质，实现了有效的信号重构。 |
| [^125] | [LTD: Low Temperature Distillation for Robust Adversarial Training.](http://arxiv.org/abs/2111.02331) | 本文提出了一种名为低温蒸馏（LTD）的新方法，通过使用修改的知识蒸馏框架生成软标签，解决了对抗训练中常用的独热向量标签带来的学习困难问题，提高了模型的稳健性。 |
| [^126] | [Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting.](http://arxiv.org/abs/2110.05367) | 该论文提出了一种新方法GEEP，用于提高预训练语言模型的性别公平性，同时没有灾难性遗忘问题。透过性别中性数据学习性别相关的提示，GEEP实现了SOTA表现并在GLUE性能上取得了显著提高。 |
| [^127] | [The Bayesian Learning Rule.](http://arxiv.org/abs/2107.04562) | 许多机器学习算法都可以归结为贝叶斯学习规则，该规则通过利用自然梯度来逼近后验分布，从而得到广泛的算法应用。这一工作不仅统一了现有算法，还帮助我们设计新的算法。 |
| [^128] | [Approximating Probability Distributions by using Wasserstein Generative Adversarial Networks.](http://arxiv.org/abs/2103.10060) | 本文研究了Wasserstein生成对抗网络（WGANs），并使用GroupSort神经网络作为鉴别器。研究结果显示，生成器和鉴别器的容量对目标分布的逼近误差有影响，并且WGANs对鉴别器的容量要求高于生成器。此外，在鉴别器不足够强大时，低容量的生成器可能比过度深层和宽度的生成器效果更好。数值结果证实了理论结果。 |
| [^129] | [A method to integrate and classify normal distributions.](http://arxiv.org/abs/2012.14331) | 本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。 |
| [^130] | [High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient.](http://arxiv.org/abs/1806.04047) | 本文研究了在高维设置中的多任务学习问题，并引入了一个估计器来处理多连接线性回归问题，称为数据丰富/共享。我们通过凸函数来描述公共参数和个体参数的结构，并提出了一种具有几何收敛速度的迭代估计算法。 |

# 详细

[^1]: 时钟与披萨：神经网络的机械解释中的两个故事

    The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. (arXiv:2306.17844v1 [cs.LG])

    [http://arxiv.org/abs/2306.17844](http://arxiv.org/abs/2306.17844)

    本研究发现，神经网络在已知算法任务中有时会发现质态不同的算法。即使对模型进行微小调整，也可能出现并行实现多个算法的情况。这一结论表明，即使是简单的学习问题，也可以有令人惊讶的多样解决方案。

    

    神经网络在已知的算法任务上训练，能否可靠地重新发现解决这些任务的已知算法？最近的一些研究，涉及到从群算术到上下文线性回归的任务，表明答案是肯定的。我们以模块加法为典型问题，展示了神经网络中的算法发现有时更加复杂。对模型超参数和初始化进行微小的更改，可以导致从固定训练集中发现定性不同的算法，甚至是并行实现多个这样的算法。一些训练用于执行模块加法的网络实现了熟悉的时钟算法；其他实现了以前未描述过的、不太直观但可理解的过程，我们将其称为披萨算法，或者其他更复杂的过程。我们的结果表明，即使简单的学习问题也可以有令人惊讶的多样解决方案，促进了新工具的发展。

    Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools 
    
[^2]: 重置深度强化学习中的优化器：一个实证研究

    Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v1 [cs.LG])

    [http://arxiv.org/abs/2306.17833](http://arxiv.org/abs/2306.17833)

    在深度强化学习中，当优化问题的风景在不同迭代中差异较大时，重置优化器的内部参数可以避免污染和提高性能。

    

    我们关注的是在深度强化学习中近似计算最优值函数的任务。这个迭代过程包括在每个迭代中解决一系列不同迭代中目标函数可能改变的优化问题。解决这个问题的常见方法是使用现代变种的随机梯度下降算法，如Adam。这些优化器保持自己的内部参数，如梯度的一阶和二阶矩估计，并随时间更新这些参数。因此，之前迭代的信息被用来在当前迭代中解决优化问题。我们假设在之前迭代的优化风景与当前迭代相差较大的情况下，这可能会污染所使用优化器的内部参数。为了避免这种影响，一个简单的想法是在开始新的迭代时重置优化器的内部参数。

    We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of approximately solving a sequence of optimization problems where the objective function can change per iteration. The common approach to solving the problem is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first and the second moment of the gradient, and update these parameters over time. Therefore, information obtained in previous iterations is being used to solve the optimization problem in the current iteration. We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration. To hedge against this effect, a simple idea is to reset the internal parameters of the optimizer when starting a n
    
[^3]: 联合集成YOLOv5 - 一种更好的广义目标检测算法

    Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm. (arXiv:2306.17829v1 [cs.CV])

    [http://arxiv.org/abs/2306.17829](http://arxiv.org/abs/2306.17829)

    本文研究了联合学习算法在目标检测中的应用，并将其与集中式训练方法进行性能比较。实验结果表明，使用联合学习训练的YOLOv5模型在生成准确的未见过目标的边界框方面具有显著优势。

    

    联合学习（FL）作为一种保护隐私的算法已经获得了显著的关注，但是联合学习算法如联合平均（FED Avg）或联合SGD（FED SGD）与集成学习算法的相似性尚未完全探索。本文旨在研究FL在目标检测中的应用作为一种增强泛化性能的方法，并将其与集中式训练方法的目标检测算法进行性能比较。具体来说，我们研究了使用FL训练的YOLOv5模型在多个客户端上的性能，并采用无替换的随机抽样策略，使每个客户端持有一部分用于集中式训练的相同数据集。我们的实验结果展示了FL目标检测器的全局模型在生成准确的未见过目标的边界框方面的卓越效率，测试集是来自两个不同客户端的对象的混合。

    Federated learning (FL) has gained significant traction as a privacy-preserving algorithm, but the underlying resembles of federated learning algorithm like Federated averaging (FED Avg) or Federated SGD (FED SGD) to ensemble learning algorithms has not been fully explored. The purpose of this paper is to examine the application of FL to object detection as a method to enhance generalizability, and to compare its performance against a centralized training approach for an object detection algorithm. Specifically, we investigate the performance of a YOLOv5 model trained using FL across multiple clients and employ a random sampling strategy without replacement, so each client holds a portion of the same dataset used for centralized training. Our experimental results showcase the superior efficiency of the FL object detector's global model in generating accurate bounding boxes for unseen objects, with the test set being a mixture of objects from two distinct clients not represented in the 
    
[^4]: 通过训练概念影响理解不公平性

    Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])

    [http://arxiv.org/abs/2306.17828](http://arxiv.org/abs/2306.17828)

    通过观察训练数据的作用，研究模型不公平性的来源和影响，并通过改变样本的属性来计算训练样本对模型的不公平性的影响。

    

    了解模型不公平性的原因有助于从业人员更好地理解他们的数据和算法。我们通过培训数据这一主要不公平来源的视角来研究这个问题。我们提出以下问题：如果在训练数据中有些样本（1）来自不同的（例如人口统计学）群体，（2）标记方式不同，或者（3）某些特征发生了变化，那么模型的公平性表现会发生怎样的变化？换句话说，我们通过反事实地对基于预定义概念的样本进行干预和改变，量化训练样本对模型的不公平性的影响。计算训练样本对模型相对于概念的不公平性的影响时，我们首先基于概念生成反事实版本的样本，即如果概念发生变化，样本的反事实版本。然后我们计算重新

    Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
    
[^5]: 可扩展的非均匀超图的张量方法

    Scalable tensor methods for nonuniform hypergraphs. (arXiv:2306.17825v1 [math.NA])

    [http://arxiv.org/abs/2306.17825](http://arxiv.org/abs/2306.17825)

    本论文提出了一种可扩展的张量方法，用于处理非均匀超图。通过开发新的TTSV算法，我们能够在低于指数复杂度的情况下处理邻接张量，并应用于超图中心性和聚类等问题。这些方法不仅能提供与图缩减方法互补的信息，还能够探测到高阶结构。

    

    尽管多线性代数在研究由超图模拟的多方交互方面似乎很自然，但通用超图的张量方法受到理论和实际限制的阻碍。最近提出的邻接张量适用于非均匀超图，但在实践中形成和分析它是代价高昂的。我们开发了这个张量的张量乘相同向量（TTSV）算法，将复杂度从$O(n^r)$降低到$r$的低次多项式，其中$n$是顶点的数量，$r$是最大超边大小。我们的算法是隐式的，避免了形成$r$阶邻接张量。通过开发基于张量的超图中心性和聚类算法，我们展示了我们方法的灵活性和实用性。我们还展示了这些张量度量在数据上与类似的图缩减方法提供互补信息，并且还能够检测到许多现有基于矩阵的方法无法检测到的高阶结构。

    While multilinear algebra appears natural for studying the multiway interactions modeled by hypergraphs, tensor methods for general hypergraphs have been stymied by theoretical and practical barriers. A recently proposed adjacency tensor is applicable to nonuniform hypergraphs, but is prohibitively costly to form and analyze in practice. We develop tensor times same vector (TTSV) algorithms for this tensor which improve complexity from $O(n^r)$ to a low-degree polynomial in $r$, where $n$ is the number of vertices and $r$ is the maximum hyperedge size. Our algorithms are implicit, avoiding formation of the order $r$ adjacency tensor. We demonstrate the flexibility and utility of our approach in practice by developing tensor-based hypergraph centrality and clustering algorithms. We also show these tensor measures offer complementary information to analogous graph-reduction approaches on data, and are also able to detect higher-order structure that many existing matrix-based approaches p
    
[^6]: Act3D：无限分辨率的机器人操作检测Transformer

    Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])

    [http://arxiv.org/abs/2306.17817](http://arxiv.org/abs/2306.17817)

    Act3D是一种基于Transformer的操作策略，将6自由度关键姿势预测作为3D检测任务，并以自适应空间计算的方式进行处理。它在高度精确的机器人操纵任务中取得了显著的性能改进。

    

    3D感知表征非常适用于机器人操纵，因为它们可以轻松编码遮挡情况并简化空间推理。许多操纵任务需要对末端执行器姿势预测进行高空间精度，通常需要高分辨率的3D感知网格进行计算，这在处理上非常耗时。因此，大多数操作策略直接在2D中运作，放弃了3D的归纳偏差。本文提出了Act3D，一种将6自由度关键姿势预测视为自适应空间计算的操作策略Transformer。它以一个或多个摄像机视图的未投影3D特征云作为输入，以粗-精方式在自由空间中迭代采样3D点网格，使用相对空间注意力将其特征化为物理特征云，并选择最佳特征点进行末端执行器姿势预测。Act3D在已建立的操纵基准RLbench中取得了最新的最好成绩。我们的模型在该基准中实现了10%的绝对改进。

    3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
    
[^7]: 通过在线信心预测实现具备形式安全保证的贝叶斯优化

    Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction. (arXiv:2306.17815v1 [cs.LG])

    [http://arxiv.org/abs/2306.17815](http://arxiv.org/abs/2306.17815)

    本文提出了一种基于贝叶斯优化的方法，无论约束函数的特性如何，都能满足安全要求。

    

    黑盒零阶优化是金融、物理和工程等领域应用的核心基本操作。在这个问题的常见形式中，设计者顺序尝试候选解，并从系统中接收到关于每个尝试值的噪声反馈。本文研究了在这些场景中还提供了有关尝试解的安全性的反馈，并且优化器被限制在整个优化过程中尝试的不安全解的数量上。在基于贝叶斯优化（BO）的方法上，先前的研究引入了一种被称为SAFEOPT的优化方案，只要满足对安全约束函数的严格假设，就能够以可控的概率在反馈噪声上避免选择任何不安全的解。本文介绍了一种新的基于BO的方法，无论约束函数的特性如何，都能满足安全要求。

    Black-box zero-th order optimization is a central primitive for applications in fields as diverse as finance, physics, and engineering. In a common formulation of this problem, a designer sequentially attempts candidate solutions, receiving noisy feedback on the value of each attempt from the system. In this paper, we study scenarios in which feedback is also provided on the safety of the attempted solution, and the optimizer is constrained to limit the number of unsafe solutions that are tried throughout the optimization process. Focusing on methods based on Bayesian optimization (BO), prior art has introduced an optimization scheme -- referred to as SAFEOPT -- that is guaranteed not to select any unsafe solution with a controllable probability over feedback noise as long as strict assumptions on the safety constraint function are met. In this paper, a novel BO-based approach is introduced that satisfies safety requirements irrespective of properties of the constraint function. This s
    
[^8]: 不使用分类器的指导下保持话题的一致性

    Stay on topic with Classifier-Free Guidance. (arXiv:2306.17806v1 [cs.CL])

    [http://arxiv.org/abs/2306.17806](http://arxiv.org/abs/2306.17806)

    本论文展示了分类器无关的指导（CFG）可以作为一种推断时间技术，显著提高了纯语言建模中各种任务的性能，并能够增强助手在具有挑战性的提示中的准确性和一致性。

    

    分类器无关的指导（CFG）最近在文本到图像生成中出现，作为一种轻量级技术促进生成的立即遵循。在这项工作中，我们证明CFG可以广泛用作纯语言建模的推断时间技术。我们展示了CFG在一系列任务上提高了Pythia、GPT-2和LLaMA-family模型的性能：问答，推理，代码生成和机器翻译，在LAMBADA上使用LLaMA-7B超过PaLM-540B的SOTA；（2）带来了相当于双倍参数数的模型的改进；（3）可以与其他推断时间方法如Chain-of-Thought和Self-Consistency一起使用，在困难任务中取得进一步改进；（4）可以用于增加助手在具有挑战性的形式驱动和内容驱动提示中的忠实度和连贯性：在人类评估中，我们展示了75％的用户更喜欢使用CFG的GPT4All而不是基准方法。

    Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\% preference for GPT4All using CFG over baseline.
    
[^9]: 多地点销售交易预测的层次贝叶斯回归

    Hierarchical Bayesian Regression for Multi-Location Sales Transaction Forecasting. (arXiv:2306.17795v1 [stat.AP])

    [http://arxiv.org/abs/2306.17795](http://arxiv.org/abs/2306.17795)

    本文介绍了一种层次贝叶斯回归模型，用于预测多地点销售交易，并通过共享推理结果在群组之间进行泛化。研究表明，该模型在预测商店特许经营店购买情况方面取得了进展。

    

    许多预测模型中的特征自然地形成层次结构。较低层次代表个体或事件。这些个体自然地聚集成为地点、时间间隔或其他聚合，通常有多个层次。分组的层次可以交叉和合并，就像关系数据库表一样。除了表示数据的结构外，层次模型中的预测特征可以分配到适当的层次。这样的模型适合使用层次贝叶斯解决方法，通过将推理结果在群组之间进行泛化，实现共享。在本文中，我们展示了将层次贝叶斯模型应用于预测商店特许经营店每天的购买情况的工作进展，包括对地点和每周某些天的分组。我们使用\textsf{stan}软件包展示了在一年的时间内收集的个别销售交易数据。

    The features in many prediction models naturally take the form of a hierarchy. The lower levels represent individuals or events. These units group naturally into locations and intervals or other aggregates, often at multiple levels. Levels of groupings may intersect and join, much as relational database tables do. Besides representing the structure of the data, predictive features in hierarchical models can be assigned to their proper levels. Such models lend themselves to hierarchical Bayes solution methods that ``share'' results of inference between groups by generalizing over the case of individual models for each group versus one model that aggregates all groups into one.  In this paper we show our work-in-progress applying a hierarchical Bayesian model to forecast purchases throughout the day at store franchises, with groupings over locations and days of the week. We demonstrate using the \textsf{stan} package on individual sales transaction data collected over the course of a yea
    
[^10]: 透过面纱看“视觉”：差分隐私在医学图像分类的联邦学习中的应用

    Vision Through the Veil: Differential Privacy in Federated Learning for Medical Image Classification. (arXiv:2306.17794v1 [cs.LG])

    [http://arxiv.org/abs/2306.17794](http://arxiv.org/abs/2306.17794)

    本研究将差分隐私技术应用于医学图像分类的联邦学习中，通过引入一种新颖的差分隐私联邦学习模型，平衡模型准确性与隐私设置，为医疗保健领域的深度学习应用提供了隐私保护解决方案。

    

    在医疗保健领域深度学习应用的普及需要跨多个机构进行数据聚合，这常常涉及严重的隐私问题。在医学图像分析中，隐私保护机制至关重要，因为数据具有敏感性。联邦学习使得合作模型训练成为可能，而无需直接交换数据，提供了一个有希望的解决方案。然而，联邦学习的固有漏洞需要更多的隐私保护措施。本研究通过将差分隐私，一种领先的隐私保护技术，整合进医学图像分类的联邦学习框架中，解决了这一需求。我们引入了一种新颖的差分隐私联邦学习模型，并详细研究了其对隐私保护和模型性能的影响。我们的研究证实了模型准确性与隐私设置之间存在权衡。然而，我们也证明了通过合适的隐私设置，仍然可以在医学图像分类中实现较高的模型性能。

    The proliferation of deep learning applications in healthcare calls for data aggregation across various institutions, a practice often associated with significant privacy concerns. This concern intensifies in medical image analysis, where privacy-preserving mechanisms are paramount due to the data being sensitive in nature. Federated learning, which enables cooperative model training without direct data exchange, presents a promising solution. Nevertheless, the inherent vulnerabilities of federated learning necessitate further privacy safeguards. This study addresses this need by integrating differential privacy, a leading privacy-preserving technique, into a federated learning framework for medical image classification. We introduce a novel differentially private federated learning model and meticulously examine its impacts on privacy preservation and model performance. Our research confirms the existence of a trade-off between model accuracy and privacy settings. However, we demonstr
    
[^11]: 看看、记住和推理：基于机理的视觉推理

    Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])

    [http://arxiv.org/abs/2306.17778](http://arxiv.org/abs/2306.17778)

    在这项研究中，我们借鉴了人类视觉问题解决的方法，通过三个步骤（看、记住、推理）逐步提取视觉信息来解决复杂的视觉推理问题，从而使大型语言模型能够有效解决这些问题。

    

    近期，大型语言模型在各种推理任务上展示了与人类水平的表现。然而，这些模型在进行复杂的视觉推理方面的能力尚未得到详细研究。在许多视觉推理任务中，一个关键挑战是需要将视觉信息紧密融合到推理过程中。我们提出通过借鉴人类视觉问题解决的方法来解决这个挑战，这个方法依赖于多种低级视觉能力。它通常可以被看作是“看，记住，推理”的三个步骤过程：通过逐步进行低级视觉过程提取视觉信息，直到得出最终答案。我们遵循相同的范例，通过最小的架构更改，使现有的大型语言模型能够解决视觉推理问题。为此，我们引入了基于视觉输入的原理，允许我们集成低级视觉能力，如对象识别。

    Large language models have recently shown human level performance on a variety of reasoning tasks. However, the ability of these models to perform complex visual reasoning has not been studied in detail yet. A key challenge in many visual reasoning tasks is that the visual information needs to be tightly integrated in the reasoning process. We propose to address this challenge by drawing inspiration from human visual problem solving which depends on a variety of low-level visual capabilities. It can often be cast as the three step-process of ``Look, Remember, Reason'': visual information is incrementally extracted using low-level visual routines in a step-by-step fashion until a final answer is reached. We follow the same paradigm to enable existing large language models, with minimal changes to the architecture, to solve visual reasoning problems. To this end, we introduce rationales over the visual input that allow us to integrate low-level visual capabilities, such as object recogni
    
[^12]: 扩散模型中的实用和渐进精确条件采样

    Practical and Asymptotically Exact Conditional Sampling in Diffusion Models. (arXiv:2306.17775v1 [stat.ML])

    [http://arxiv.org/abs/2306.17775](http://arxiv.org/abs/2306.17775)

    本论文提出了一种名为TDS的扭转式扩散采样器，它是一种针对扩散模型的顺序蒙特卡洛算法。该方法通过使用扭转技术结合启发式近似，能够在不需要特定训练的情况下在广泛的条件分布上提供精确的样本。

    

    扩散模型在分子设计和文本到图像生成等条件生成任务中取得了成功。然而，这些成就主要依赖于任务特定的条件训练或容易出错的启发式近似。理想情况下，条件生成方法应该能够在不需要特定训练的情况下为广泛的条件分布提供精确的样本。为此，我们引入了扭转式扩散采样器(TDS)。TDS是一种针对扩散模型的顺序蒙特卡洛(SMC)算法。其主要思想是使用扭转，一种具有良好计算效率的SMC技术，来结合启发式近似而不影响渐进精确性。我们首先在模拟实验和MNIST图像修复以及类条件生成任务中发现，TDS提供了计算统计权衡，使用更多粒子得到更准确的近似结果，但同时需要更多计算资源。

    Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and on MNIST image inpainting and class-conditional generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but wi
    
[^13]: 通过神经排序实现精确的抗癌药物选择

    Precision Anti-Cancer Drug Selection via Neural Ranking. (arXiv:2306.17771v1 [cs.LG])

    [http://arxiv.org/abs/2306.17771](http://arxiv.org/abs/2306.17771)

    通过神经排序方法，准确选择和优先排序敏感药物来进行个性化抗癌治疗。

    

    个性化癌症治疗需要对药物与癌细胞系在不同的遗传和分子环境中的复杂相互作用有深入的理解。为了解决这个问题，高通量筛选已被用来生成大规模的药物反应数据，促进数据驱动的计算模型。这些模型可以完全以数据驱动的方式捕捉到不同环境下复杂的药物-细胞系相互作用。然而，准确地为每个细胞系优先选择最敏感的药物仍然是一个重大挑战。为了解决这个问题，我们开发了神经排序方法，利用来自不同癌症类型的多个细胞系的大规模药物反应数据。与现有方法主要使用回归和分类技术进行药物反应预测不同，我们将药物选择和优先级确定的目标形式化为一个药物排序问题。在这项工作中，我们提出了两种神经排序方法，可以学习潜在的表示来解决这个问题。

    Personalized cancer treatment requires a thorough understanding of complex interactions between drugs and cancer cell lines in varying genetic and molecular contexts. To address this, high-throughput screening has been used to generate large-scale drug response data, facilitating data-driven computational models. Such models can capture complex drug-cell line interactions across various contexts in a fully data-driven manner. However, accurately prioritizing the most sensitive drugs for each cell line still remains a significant challenge. To address this, we developed neural ranking approaches that leverage large-scale drug response data across multiple cell lines from diverse cancer types. Unlike existing approaches that primarily utilize regression and classification techniques for drug response prediction, we formulated the objective of drug selection and prioritization as a drug ranking problem. In this work, we proposed two neural listwise ranking methods that learn latent repres
    
[^14]: 受形状改变的Transformer：在无限深度和宽度极限中的注意力模型

    The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit. (arXiv:2306.17759v1 [stat.ML])

    [http://arxiv.org/abs/2306.17759](http://arxiv.org/abs/2306.17759)

    在无限深度和宽度的比例极限下，我们通过修改Softmax-based注意力模型，研究了Transformer的协方差矩阵。我们发现在初始化时，极限分布可以用随机微分方程来描述。通过修改注意力机制并使用残差连接，我们可以控制网络的稳定性和协方差结构的行为。

    

    在深度学习理论中，表示的协方差矩阵用作检查网络可训练性的代理。受Transformer的成功启发，我们研究了在无限深度和宽度的比例极限下，带有跳跃连接的修改Softmax-based注意力模型的协方差矩阵。我们展示了在初始化时，极限分布可以用深度与宽度比率为索引的随机微分方程（SDE）来描述。为了实现良定义的随机极限，Transformer的注意力机制通过将Softmax输出居中在单位矩阵上，并通过宽度相关的温度参数对Softmax logits进行缩放来进行修改。我们通过相应的SDE研究了网络的稳定性，展示了如何通过残差连接优雅地控制漂移和扩散的尺度。稳定SDE的存在意味着协方差结构是良 behaved 的，即使对于非常大的深度和宽度也是如此。

    In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and widt
    
[^15]: TD收敛性：一个优化的视角

    TD Convergence: An Optimization Perspective. (arXiv:2306.17750v1 [cs.LG])

    [http://arxiv.org/abs/2306.17750](http://arxiv.org/abs/2306.17750)

    本研究从优化的视角研究了时差(TD)学习算法的收敛行为，在经典反例中确定了影响算法收敛或发散的两个力量，并在线性逼近和平方损失以外的情况下证明了TD的收敛性。这一研究为TD在强化学习领域的成功应用提供了理论解释。

    

    我们研究了著名的时差(TD)学习算法的收敛特性。通过优化的视角来看待算法，我们首先论证了TD可以被视为一种迭代优化算法，其中每次迭代时要最小化的函数都会发生变化。通过仔细研究TD在经典反例中的发散行为，我们确定了决定算法收敛或发散行为的两个力量。我们还将这一优化视角推广到了比线性逼近和平方损失更广泛的设置中，证明了TD的收敛性取决于这两个力量之间的相互作用。我们的结果为TD在强化学习中的成功应用提供了理论上的解释。

    We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.
    
[^16]: 为什么深度模型在分子属性预测上经常无法打败非深度对手？

    Why Deep Models Often cannot Beat Non-deep Counterparts on Molecular Property Prediction?. (arXiv:2306.17702v1 [cs.LG])

    [http://arxiv.org/abs/2306.17702](http://arxiv.org/abs/2306.17702)

    本研究表明，尽管深度神经网络在分子属性预测任务上已取得进展，但在大多数情况下，它们仍无法击败传统的非深度模型。关键原因在于分子数据的不规则模式，而使用分子指纹作为输入的树模型在这方面表现更好。

    

    分子属性预测（MPP）是药物发现流程中的关键任务，最近由于深度神经网络的进展而受到了相当的关注。然而，最新研究表明，在MPP上，深度模型往往难以超过传统的非深度模型。在本研究中，我们对14种分子数据集上的12个代表性模型进行了基准测试（3个非深度模型和9个深度模型）。通过迄今为止最全面的研究，我们得出以下关键观察结果: \textbf{（罗马数字 1）} 深度模型通常无法超越非深度模型; \textbf{（罗马数字 2）} 深度模型在MPP上的失败不能仅归因于分子数据集的规模小。重要的是分子数据的不规则模式; \textbf{（罗马数字 3）} 特别是使用分子指纹作为输入的树模型往往比其他竞争对手表现更好。此外，我们对分子数据的独特模式进行了广泛的实证研究。

    Molecular property prediction (MPP) is a crucial task in the drug discovery pipeline, which has recently gained considerable attention thanks to advances in deep neural networks. However, recent research has revealed that deep models struggle to beat traditional non-deep ones on MPP. In this study, we benchmark 12 representative models (3 non-deep models and 9 deep models) on 14 molecule datasets. Through the most comprehensive study to date, we make the following key observations: \textbf{(\romannumeral 1)} Deep models are generally unable to outperform non-deep ones; \textbf{(\romannumeral 2)} The failure of deep models on MPP cannot be solely attributed to the small size of molecular datasets. What matters is the irregular molecule data pattern; \textbf{(\romannumeral 3)} In particular, tree models using molecular fingerprints as inputs tend to perform better than other competitors. Furthermore, we conduct extensive empirical investigations into the unique patterns of molecule data 
    
[^17]: 超越基于神经网络的方法保护演讲者性别的研究

    Beyond Neural-on-Neural Approaches to Speaker Gender Protection. (arXiv:2306.17700v1 [eess.AS])

    [http://arxiv.org/abs/2306.17700](http://arxiv.org/abs/2306.17700)

    本文超越了基于神经网络的方法，提出了一种超越性别保护的研究方法，并强调了测试基于语音特征的性别推测攻击的重要性，以及与人类执行的声音适应进行比较。

    

    最近的研究提出了一些修改语音以防止性别推测攻击的方法。这些保护算法的目标是控制关于演讲者性别这个隐私敏感属性的信息的可用性。目前，开发和测试性别保护算法的常见做法是 "神经网络之间的"，即通过神经网络生成和测试扰动。在本文中，我们提出超越这种做法以加强对性别保护的研究。首先，我们证明了测试基于语音科学家历史上开发的语音特征的性别推测攻击的重要性，同时还与传统的神经分类器进行比较。接下来，我们认为研究人员应该使用语音特征来洞察保护性修改如何改变语音信号。最后，我们指出性别保护算法应该与新型的 "语音对手"，即人类执行的声音适应进行比较。

    Recent research has proposed approaches that modify speech to defend against gender inference attacks. The goal of these protection algorithms is to control the availability of information about a speaker's gender, a privacy-sensitive attribute. Currently, the common practice for developing and testing gender protection algorithms is "neural-on-neural", i.e., perturbations are generated and tested with a neural network. In this paper, we propose to go beyond this practice to strengthen the study of gender protection. First, we demonstrate the importance of testing gender inference attacks that are based on speech features historically developed by speech scientists, alongside the conventionally used neural classifiers. Next, we argue that researchers should use speech features to gain insight into how protective modifications change the speech signal. Finally, we point out that gender-protection algorithms should be compared with novel "vocal adversaries", human-executed voice adaptati
    
[^18]: GFlowNets中的Thompson抽样用于改进探索

    Thompson sampling for improved exploration in GFlowNets. (arXiv:2306.17693v1 [cs.LG])

    [http://arxiv.org/abs/2306.17693](http://arxiv.org/abs/2306.17693)

    本文介绍了一种将生成流网络（GFlowNets）中的采样问题视为主动学习问题，并使用贝叶斯技术中的Thompson采样方法来解决的算法。实验结果表明该算法可以有效改进探索性能。

    

    生成流网络（GFlowNets）是一种用于组合对象分布采样的变分推理算法，将其视为可学习的动作策略的顺序决策问题。与其他优化变分界限的分层采样算法不同，GFlowNet算法可以稳定地进行离策略运行，这在发现目标分布的模式时有优势。尽管在行为策略的选择上存在灵活性，但目前还没有系统地探索有效选择轨迹进行训练的最佳方式。在本文中，我们将训练轨迹的选择视为主动学习问题，并使用受多臂老虎机方法启发的贝叶斯技术来处理。提出的算法Thompson采样GFlowNets（TS-GFN）通过维护策略的近似后验分布，并从该后验中采样轨迹进行训练。我们在两个领域的实验证明TS-GFN可以改进探索性能。

    Generative flow networks (GFlowNets) are amortized variational inference algorithms that treat sampling from a distribution over compositional objects as a sequential decision-making problem with a learnable action policy. Unlike other algorithms for hierarchical sampling that optimize a variational bound, GFlowNet algorithms can stably run off-policy, which can be advantageous for discovering modes of the target distribution. Despite this flexibility in the choice of behaviour policy, the optimal way of efficiently selecting trajectories for training has not yet been systematically explored. In this paper, we view the choice of trajectories for training as an active learning problem and approach it using Bayesian techniques inspired by methods for multi-armed bandits. The proposed algorithm, Thompson sampling GFlowNets (TS-GFN), maintains an approximate posterior distribution over policies and samples trajectories from this posterior for training. We show in two domains that TS-GFN yi
    
[^19]: 泛化时间扭曲不变字典学习用于时间序列分类和聚类

    Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering. (arXiv:2306.17690v1 [stat.ML])

    [http://arxiv.org/abs/2306.17690](http://arxiv.org/abs/2306.17690)

    本文提出了一种泛化时间扭曲不变字典学习算法，用于处理时间序列数据的模式识别和分类。该算法通过使用连续基函数的线性组合来构建泛化时间扭曲算子，以实现连续的时间扭曲。通过联合优化扭曲路径、字典和稀疏系数，我们的算法在时间序列分类和聚类任务中取得了优秀的性能。

    

    字典学习是时间序列数据模式识别和分类的有效工具。在各种字典学习技术中，动态时间扭曲（DTW）通常用于处理时间延迟、缩放、转换和其他各种时间不准确性问题。然而，由于DTW在对齐时间序列数据时是离散的性质，因此容易出现过拟合或信息损失的问题。为了解决这个问题，本文提出了一种泛化时间扭曲不变字典学习算法。我们的方法采用了泛化时间扭曲算子，该算子由连续基函数的线性组合构成，以便实现连续的时间扭曲。将所提出的算子与字典学习相结合，将其建模为一个优化问题，并采用块坐标下降法来联合优化扭曲路径、字典和稀疏系数。优化结果被用作特征提取和分类的提取。实验结果表明，我们的方法在时间序列分类和聚类任务中取得了优秀的性能。

    Dictionary learning is an effective tool for pattern recognition and classification of time series data. Among various dictionary learning techniques, the dynamic time warping (DTW) is commonly used for dealing with temporal delays, scaling, transformation, and many other kinds of temporal misalignments issues. However, the DTW suffers overfitting or information loss due to its discrete nature in aligning time series data. To address this issue, we propose a generalized time warping invariant dictionary learning algorithm in this paper. Our approach features a generalized time warping operator, which consists of linear combinations of continuous basis functions for facilitating continuous temporal warping. The integration of the proposed operator and the dictionary learning is formulated as an optimization problem, where the block coordinate descent method is employed to jointly optimize warping paths, dictionaries, and sparseness coefficients. The optimized results are then used as hy
    
[^20]: 使用可学习间距的膨胀卷积学习尖峰神经网络中的延迟

    Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v1 [cs.NE])

    [http://arxiv.org/abs/2306.17670](http://arxiv.org/abs/2306.17670)

    本文提出了一种新的离散时间算法，通过反向传播学习尖峰神经网络(SNNs)中的延迟，提高了SNNs在节能信息处理系统中的表达能力。

    

    尖峰神经网络(SNNs)是构建节能信息处理系统的一种有前途的研究方向，特别适用于如语音识别等时间任务。在SNNs中，延迟指的是从一个神经元到另一个神经元传播需要的时间。这些延迟很重要，因为它们影响脉冲到达时间，已知尖峰神经元对于重叠的输入脉冲有更强的响应。更正式地说，理论上已经证明可塑性延迟极大增加了SNNs的表达能力。然而，目前缺乏有效的算法来学习这些延迟。在这里，我们提出了一种新的离线离散时间算法，用于通过反向传播在深度前馈SNNs中解决这个问题。为了模拟连续层之间的延迟，我们使用了沿时间轴的一维卷积。卷积核仅包含少数非零权重 - 每个突触一个 - 它们的位置对应于延迟。这些位置与权重一起被学习。

    Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the wei
    
[^21]: 使用基于域分解的预条件策略增强物理信息神经网络的训练

    Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v1 [math.NA])

    [http://arxiv.org/abs/2306.17648](http://arxiv.org/abs/2306.17648)

    这项研究提出了一种基于域分解的预条件策略，用于增强物理信息神经网络的训练。该方法通过非线性预条件器改进了L-BFGS优化器的收敛性，并提供了更准确的偏微分方程解。加性预条件器还具有并行性，为模型并行提供了新的方法。

    

    我们提出了一种增强物理信息神经网络（PINNs）训练的方法。为此，我们引入了非线性加性和乘性预条件策略，用于广泛使用的L-BFGS优化器。非线性预条件器是通过利用Schwarz域分解框架构建的，其中网络的参数以逐层方式进行分解。通过一系列数值实验，我们证明了加性和乘性预条件器都能显著改善标准L-BFGS优化器的收敛性，同时提供了更准确的偏微分方程解。此外，加性预条件器本质上是并行的，因此为一种新的模型并行方法提供了可能。

    We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative preconditioners significantly improve the convergence of the standard L-BFGS optimizer, while providing more accurate solutions of the underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.
    
[^22]: 共享生产中的联邦目标检测用于质量检验

    Federated Object Detection for Quality Inspection in Shared Production. (arXiv:2306.17645v1 [cs.LG])

    [http://arxiv.org/abs/2306.17645](http://arxiv.org/abs/2306.17645)

    本文提出了一个利用联邦学习进行质量检验任务中的目标检测的算法，该算法使用YOLOv5作为目标检测算法和Federated Averaging作为联邦学习算法。实验结果表明，该联邦学习方法在整体客户测试数据集上具有更好的泛化性能，并生成相对于使用本地客户数据集训练的模型更精确的边界框。

    

    联邦学习（FL）已经成为一种在分散数据的条件下训练机器学习模型的有希望方法，而不会损害数据隐私。在本文中，我们提出了一种利用YOLOv5作为目标检测算法和联邦平均（FedAvg）作为FL算法的联邦学习算法，用于质量检验任务中的目标检测。我们将此方法应用于制造业中的一个使用案例，多个工厂/客户共享数据以训练全局目标检测模型，同时在非IID数据集上保护数据隐私。我们的实验表明，我们的联邦学习方法在整体客户测试数据集上实现了更好的泛化性能，并且生成的边界框相对于使用本地客户数据集训练的模型更加精确。这项工作展示了联邦学习在制造业质量检验任务中的潜力，并为利用YOLOv5和FedAvg进行联邦目标检测提供了有价值的见解。

    Federated learning (FL) has emerged as a promising approach for training machine learning models on decentralized data without compromising data privacy. In this paper, we propose a FL algorithm for object detection in quality inspection tasks using YOLOv5 as the object detection algorithm and Federated Averaging (FedAvg) as the FL algorithm. We apply this approach to a manufacturing use-case where multiple factories/clients contribute data for training a global object detection model while preserving data privacy on a non-IID dataset. Our experiments demonstrate that our FL approach achieves better generalization performance on the overall clients' test dataset and generates improved bounding boxes around the objects compared to models trained using local clients' datasets. This work showcases the potential of FL for quality inspection tasks in the manufacturing industry and provides valuable insights into the performance and feasibility of utilizing YOLOv5 and FedAvg for federated ob
    
[^23]: 几何自编码器 - 你所见即你所解码的

    Geometric Autoencoders -- What You See is What You Decode. (arXiv:2306.17638v1 [cs.LG])

    [http://arxiv.org/abs/2306.17638](http://arxiv.org/abs/2306.17638)

    这篇论文介绍了一种名为几何自编码器的方法，通过从微分几何的角度对解码器进行优化，避免了可视化结果的失真，使得数据结构能够更准确地被捕捉到。

    

    可视化是探索性数据分析的关键步骤之一。一种可能的方法是训练一个具有低维潜在空间的自编码器。大规模的网络深度和宽度可以帮助展示数据。然而，即使在潜在表示被扭曲的情况下，这种表达能力强的网络仍然可以实现低重构误差。为了避免这种误导性的可视化，我们首先提出了一种对解码器的微分几何视角，从而提供了对嵌入的失真的深入诊断，其次是一个新的正则化器来缓解这种失真。我们的“几何自编码器”避免了误造成嵌入的拉伸，以便可视化更准确地表达数据结构。它还标记了无法实现的小失真区域，从而防止了错误解读。

    Visualization is a crucial step in exploratory data analysis. One possible approach is to train an autoencoder with low-dimensional latent space. Large network depth and width can help unfolding the data. However, such expressive networks can achieve low reconstruction error even when the latent representation is distorted. To avoid such misleading visualizations, we propose first a differential geometric perspective on the decoder, leading to insightful diagnostics for an embedding's distortion, and second a new regularizer mitigating such distortion. Our ``Geometric Autoencoder'' avoids stretching the embedding spuriously, so that the visualization captures the data structure more faithfully. It also flags areas where little distortion could not be achieved, thus guarding against misinterpretation.
    
[^24]: 用单个ToF相机实现与RGB-D相机相当的分割性能

    Achieving RGB-D level Segmentation Performance from a Single ToF Camera. (arXiv:2306.17636v1 [cs.CV])

    [http://arxiv.org/abs/2306.17636](http://arxiv.org/abs/2306.17636)

    本文使用单个ToF相机的红外和深度图像，在语义分割任务上表现出与RGB-D相机相当的准确性，通过引入深度专用卷积的多任务学习方法，展示了在车内分割数据集上的竞争力。

    

    深度是计算机视觉中非常重要的一种模态，通常作为RGB的补充信息由RGB-D相机提供。在本文中，我们展示了使用单个ToF相机的红外（IR）和深度图像可以在语义分割任务上获得与RGB-D相机相同水平的准确性。为了融合ToF相机的红外和深度模态，我们引入了一种在多任务学习框架中利用深度专用卷积的方法。在我们对车内分割数据集的评估中，我们展示了我们的方法与更昂贵的RGB-D方法的竞争力。

    Depth is a very important modality in computer vision, typically used as complementary information to RGB, provided by RGB-D cameras. In this work, we show that it is possible to obtain the same level of accuracy as RGB-D cameras on a semantic segmentation task using infrared (IR) and depth images from a single Time-of-Flight (ToF) camera. In order to fuse the IR and depth modalities of the ToF camera, we introduce a method utilizing depth-specific convolutions in a multi-task learning framework. In our evaluation on an in-car segmentation dataset, we demonstrate the competitiveness of our method against the more costly RGB-D approaches.
    
[^25]: 噪声对神经网络的校准和泛化的影响

    Impact of Noise on Calibration and Generalisation of Neural Networks. (arXiv:2306.17630v1 [cs.LG])

    [http://arxiv.org/abs/2306.17630](http://arxiv.org/abs/2306.17630)

    本研究研究了不同类型噪声对神经网络的校准和泛化的影响，发现激活噪声能最有效地提高泛化性能，而输入增强噪声则能显著改善分布外的校准。

    

    噪声注入和数据增强策略对提升神经网络的泛化性能和鲁棒性有效。某些类型的噪声，如标签平滑和MixUp，也被证明能改善校准。由于噪声可以在神经网络的训练的不同阶段添加，这引发了噪声在何时何地最有效的问题。我们研究了各种噪声类型，以确定它们对校准和泛化的改进程度以及在什么条件下起作用。具体而言，我们评估了在分布内（ID）和分布外（OOD）场景中的各种噪声注入策略。研究结果表明，激活噪声对于提高泛化性能是最具传递性和有效性的，而输入增强噪声在改善分布外校准上很显著，但不一定适用于分布内数据。

    Noise injection and data augmentation strategies have been effective for enhancing the generalisation and robustness of neural networks (NNs). Certain types of noise such as label smoothing and MixUp have also been shown to improve calibration. Since noise can be added in various stages of the NN's training, it motivates the question of when and where the noise is the most effective. We study a variety of noise types to determine how much they improve calibration and generalisation, and under what conditions. More specifically we evaluate various noise-injection strategies in both in-distribution (ID) and out-of-distribution (OOD) scenarios. The findings highlight that activation noise was the most transferable and effective in improving generalisation, while input augmentation noise was prominent in improving calibration on OOD but not necessarily ID data.
    
[^26]: 使用强化学习设计感应电机

    Design of Induction Machines using Reinforcement Learning. (arXiv:2306.17626v1 [cs.LG])

    [http://arxiv.org/abs/2306.17626](http://arxiv.org/abs/2306.17626)

    该论文介绍了一种使用强化学习算法设计定制的感应电动机的方法，通过模拟多个电机设计实例进行离线训练，使得电机设计自动化并满足特定的操作要求。

    

    由于电磁和热约束的不同，感应电机的设计是一项具有挑战性的任务。在销售工具中快速估算机器的尺寸对于根据特定要求给客户提供快速报价非常重要。这个过程的关键部分是选择不同的设计参数，如长度、直径、齿尖高度和绕组匝数，以实现机器的特定扭矩、电流和温度。电机设计师通过他们的经验知道如何改变不同的机器设计参数，以满足客户的特定操作要求。我们提出了一种强化学习算法来设计定制的感应电动机。神经网络模型通过模拟电机设计游戏的不同实例进行离线训练，当做出良好或不良设计选择时，使用奖励或惩罚函数。结果表明，所提出的方法可以自动化电机设计，而无需应用任何人为制约。

    The design of induction machine is a challenging task due to different electromagnetic and thermal constraints. Quick estimation of machine's dimensions is important in the sales tool to provide quick quotations to customers based on specific requirements. The key part of this process is to select different design parameters like length, diameter, tooth tip height and winding turns to achieve certain torque, current and temperature of the machine. Electrical machine designers, with their experience know how to alter different machine design parameters to achieve a customer specific operation requirements. We propose a reinforcement learning algorithm to design a customised induction motor. The neural network model is trained off-line by simulating different instances of of electrical machine design game with a reward or penalty function when a good or bad design choice is made. The results demonstrate that the suggested method automates electrical machine design without applying any hu
    
[^27]: Sphere2Vec：一种适用于大规模地理空间预测的球面上通用位置表示学习方法

    Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])

    [http://arxiv.org/abs/2306.17624](http://arxiv.org/abs/2306.17624)

    Sphere2Vec是一种多尺度位置编码器，用于在球面上编码点坐标时保持球面距离，解决了大规模真实世界GPS坐标数据集中的距离度量问题。

    

    在机器学习中，为空间中的点生成适合学习的表示是一个基本且长期存在的问题。最近，提出了多尺度编码方案（如Space2Vec和NeRF），可以直接将二维/三维欧几里得空间中的任意点编码为高维向量，并成功应用于各种地理空间预测和生成任务。然而，目前所有的二维和三维位置编码器都是设计用来模拟欧几里得空间中的点距离。因此，在应用于需要在球面上进行距离度量学习的大规模真实世界GPS坐标数据集时，这两种类型的模型都会出现问题，原因是地图投影失真问题（2D）和球面到欧几里得距离近似误差（3D）。为了解决这些问题，我们提出了一种称为Sphere2Vec的多尺度位置编码器，可以在球面上编码点坐标时保持球面距离。我们在球面上的位置编码的距离保持编码的统一视角上进行了探索。

    Generating learning-friendly representations for points in space is a fundamental and long-standing problem in ML. Recently, multi-scale encoding schemes (such as Space2Vec and NeRF) were proposed to directly encode any point in 2D/3D Euclidean space as a high-dimensional vector, and has been successfully applied to various geospatial prediction and generative tasks. However, all current 2D and 3D location encoders are designed to model point distances in Euclidean space. So when applied to large-scale real-world GPS coordinate datasets, which require distance metric learning on the spherical surface, both types of models can fail due to the map projection distortion problem (2D) and the spherical-to-Euclidean distance approximation error (3D). To solve these problems, we propose a multi-scale location encoder called Sphere2Vec which can preserve spherical distances when encoding point coordinates on a spherical surface. We developed a unified view of distance-reserving encoding on sph
    
[^28]: 微型机器人群体导航以实现有针对性的药物输送的强化学习方法

    Navigation of micro-robot swarms for targeted delivery using reinforcement learning. (arXiv:2306.17598v1 [cs.RO])

    [http://arxiv.org/abs/2306.17598](http://arxiv.org/abs/2306.17598)

    通过强化学习算法控制微型机器人群体，实现了针对性的药物输送，具有很大的应用潜力。

    

    微型机器人在有针对性的药物输送中具有很大的潜力。然而，由于其微小的尺寸，单独控制每个机器人是困难的。因此，使用单一控制器控制多个机器人非常重要，而人工智能可以帮助我们成功完成这个任务。在本研究中，我们使用强化学习算法Proximal Policy Optimization (PPO)和Robust Policy Optimization (RPO)来控制一群微型游泳机器人，在受水动力学效应控制的情况下，将它们的方向朝向一个圆形吸收目标。我们考查了PPO和RPO在有限状态信息情景下的表现，并测试了它们在随机目标位置和大小的情况下的鲁棒性。我们使用课程学习来提高性能，并在学习控制一群25个游泳机器人并将它们导航到一个展示目标的实验中证明了这一点。

    Micro robotics is quickly emerging to be a promising technological solution to many medical treatments with focus on targeted drug delivery. They are effective when working in swarms whose individual control is mostly infeasible owing to their minute size. Controlling a number of robots with a single controller is thus important and artificial intelligence can help us perform this task successfully. In this work, we use the Reinforcement Learning (RL) algorithms Proximal Policy Optimization (PPO) and Robust Policy Optimization (RPO) to navigate a swarm of 4, 9 and 16 microswimmers under hydrodynamic effects, controlled by their orientation, towards a circular absorbing target. We look at both PPO and RPO performances with limited state information scenarios and also test their robustness for random target location and size. We use curriculum learning to improve upon the performance and demonstrate the same in learning to navigate a swarm of 25 swimmers and steering the swarm to exempli
    
[^29]: 用变分原理来规范机器学习的密度泛函：非相互作用动能泛函

    Variational principle to regularize machine-learned density functionals: the non-interacting kinetic-energy functional. (arXiv:2306.17587v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.17587](http://arxiv.org/abs/2306.17587)

    本文为机器学习的密度泛函引入了一种新的规范方法，其中包括非相互作用动能泛函。该方法通过使用基于深度神经网络的密度泛函训练，取得了在一维系统上的优秀结果。

    

    实际密度泛函理论 (DFT) 的成功归功于 Kohn 和 Sham 的开创性工作，他们引入了使用辅助均场系统计算非相互作用动能的准确方法。然而，DFT 的全部潜力将无法释放，直到找到电子密度与非相互作用动能之间的准确关系。已经尝试了各种方法来近似这个泛函，类似于交换关联泛函，但由于动能的贡献更大且更非局域，成功程度较低。在本研究中，我们提出了一种新的高效规范方法，以深度神经网络为基础来训练密度泛函，尤其是动能泛函。该方法在（有效的）一维系统上进行了测试，包括氢链、非相互作用电子和前两个周期元素的原子，取得了出色的结果。

    Practical density functional theory (DFT) owes its success to the groundbreaking work of Kohn and Sham that introduced the exact calculation of the non-interacting kinetic energy of the electrons using an auxiliary mean-field system. However, the full power of DFT will not be unleashed until the exact relationship between the electron density and the non-interacting kinetic energy is found. Various attempts have been made to approximate this functional, similar to the exchange--correlation functional, with much less success due to the larger contribution of kinetic energy and its more non-local nature. In this work we propose a new and efficient regularization method to train density functionals based on deep neural networks, with particular interest in the kinetic-energy functional. The method is tested on (effectively) one-dimensional systems, including the hydrogen chain, non-interacting electrons, and atoms of the first two periods, with excellent results. For the atomic systems, t
    
[^30]: ChatGPT用于机器人技术：设计原则和模型能力

    ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])

    [http://arxiv.org/abs/2306.17582](http://arxiv.org/abs/2306.17582)

    本文介绍了使用ChatGPT进行机器人应用的实验研究，通过设计原则和函数库的结合，ChatGPT能够适应不同的机器人任务，并展示了在各种机器人任务中的有效性和多样性。

    

    本文介绍了使用OpenAI的ChatGPT进行机器人应用的实验研究。我们概述了一种策略，将提示工程的设计原则与高级函数库的创建相结合，使ChatGPT能够适应不同的机器人任务、模拟器和形态。我们重点评估了不同的提示工程技术和对话策略对执行各种类型机器人任务的效果。我们探讨了ChatGPT使用自由形式对话、解析XML标记和合成代码的能力，以及使用任务特定提示函数和通过对话进行闭环推理的能力。我们的研究涵盖了机器人领域的一系列任务，从基本的逻辑、几何和数学推理到复杂的领域，如空中导航、操纵和具身代理。我们证明了ChatGPT在解决这些任务方面可以取得有效结果，同时使我们能够进行探索。

    This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
    
[^31]: 使用自然语言处理增强大学录取中的整体评估，以分析论文和推荐信

    Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters. (arXiv:2306.17575v1 [cs.CL])

    [http://arxiv.org/abs/2306.17575](http://arxiv.org/abs/2306.17575)

    这项研究通过机器学习模型的实证评估发现，在大学录取过程中排除受保护属性会导致预测表现下降，而通过使用文本信息可以部分恢复模型的性能。

    

    在许多高度选择性的机构中，大学录取采用全面评估过程，考虑申请的所有方面，包括隐私属性（如种族、性别）、成绩、论文和推荐信，以组成一支优秀和多样化的班级。在本研究中，我们使用机器学习（ML）模型实证评估受保护属性对预测录取决策的影响，并探讨文本信息（如个人论文、教师推荐信）在模型中代替受保护属性的程度。通过使用2022-2023学年在一所具有选择性的美国本科入学办公室的14,915名申请人的数据，我们发现从ML模型中排除受保护属性会显著降低预测录取表现。通过TF-IDF表示和隐狄利克雷分配（LDA）模型，文本信息的包含部分恢复了模型的性能。

    University admission at many highly selective institutions uses a holistic review process, where all aspects of the application, including protected attributes (e.g., race, gender), grades, essays, and recommendation letters are considered, to compose an excellent and diverse class. In this study, we empirically evaluate how influential protected attributes are for predicting admission decisions using a machine learning (ML) model, and in how far textual information (e.g., personal essay, teacher recommendation) may substitute for the loss of protected attributes in the model. Using data from 14,915 applicants to an undergraduate admission office at a selective U.S. institution in the 2022-2023 cycle, we find that the exclusion of protected attributes from the ML model leads to substantially reduced admission-prediction performance. The inclusion of textual information via both a TF-IDF representation and a Latent Dirichlet allocation (LDA) model partially restores model performance, b
    
[^32]: 大型语言模型是有效的文本排序器，具有两两排名提示

    Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])

    [http://arxiv.org/abs/2306.17563](http://arxiv.org/abs/2306.17563)

    本论文提出了一种名为PRP的新技术，通过使用两两排名提示来显著减轻大型语言模型（LLM）的负担，并首次在标准基准测试中实现了最先进的排名性能。

    

    使用大型语言模型（LLM）通过直接将查询和候选文档输入提示进行文档排序是一个有趣且实用的问题。然而，迄今为止取得了有限的成功，研究人员发现很难在基准数据集上超越精调基准排序器。我们分析了现有方法使用的点对点和列表排序提示，并认为现成的LLM没有完全理解这些排序公式，可能是由于LLM的训练方式的特性。在本文中，我们提出了一种名为两两排名提示（PRP）的新技术，大大减轻了LLM的负担。我们的结果是文献中首次使用中等规模的开源LLM在标准基准测试中实现了最先进的排名性能。在TREC-DL2020上，基于20B参数的Flan-UL2模型的PRP超过了文献中基于商业黑盒GPT-4的最佳方法。

    Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
    
[^33]: 使用扩散模型进行蒸馏和重播的增量学习

    Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])

    [http://arxiv.org/abs/2306.17560](http://arxiv.org/abs/2306.17560)

    本文提出了一种使用预训练的扩散模型作为增量学习的附加数据源的方法，通过生成属于先前遇到的图像所属类别的合成样本，并在蒸馏损失和分类损失中使用这些样本，进一步提高了模型的性能。

    

    增量学习旨在以增量的方式学习新类别，而不会忘记先前学习的类别。多个研究表明，增量模型可以利用附加数据来帮助减轻灾难性遗忘。本文提出了使用预训练的稳定扩散模型作为增量学习的附加数据源。与依赖于外部、通常是无标签的真实图像数据集的竞争方法相比，我们的方法可以生成属于先前遇到的图像所属类别的合成样本。这使我们不仅可以在蒸馏损失中使用这些附加数据样本，还可以在分类损失中进行重播。在CIFAR100、ImageNet-Subset和ImageNet等竞争基准上的实验表明，这种新方法可以进一步提高模型的性能。

    Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pretrained Stable Diffusion model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples belonging to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of s
    
[^34]: TTSWING：一种用于乒乓球挥拍分析的数据集

    TTSWING: a Dataset for Table Tennis Swing Analysis. (arXiv:2306.17550v1 [cs.LG])

    [http://arxiv.org/abs/2306.17550](http://arxiv.org/abs/2306.17550)

    TTSWING是一种专为乒乓球挥拍分析设计的数据集，通过集成传感器获取详细信息并与运动员数据一起发布。对于乒乓球分析的创新研究具有巨大潜力，对科学界来说是宝贵的资源。

    

    我们介绍了TTSWING，这是一个专门用于乒乓球挥拍分析的新型数据集。该数据集通过集成到定制的球拍握把上的9轴传感器获取了详细的挥拍信息，并附带了运动员的匿名人口统计数据。我们详细介绍了数据收集和注释的过程。此外，我们还利用多种机器学习模型进行了初步的挥拍分析研究。TTSWING在促进乒乓球分析的创新研究方面具有巨大潜力，并且对科学界来说是一个宝贵的资源。我们在https://github.com/DEPhantom/TTSWING上发布了数据集和实验代码。

    We introduce TTSWING, a novel dataset designed for table tennis swing analysis. This dataset comprises comprehensive swing information obtained through 9-axis sensors integrated into custom-made racket grips, accompanied by anonymized demographic data of the players. We detail the data collection and annotation procedures. Furthermore, we conduct pilot studies utilizing diverse machine learning models for swing analysis. TTSWING holds tremendous potential to facilitate innovative research in table tennis analysis and is a valuable resource for the scientific community. We release the dataset and experimental codes at https://github.com/DEPhantom/TTSWING.
    
[^35]: 通过恶劣情况下的视觉场所识别改进动态车辆检测——迁移对象位置

    DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place Recognition under Adverse Conditions. (arXiv:2306.17536v1 [cs.CV])

    [http://arxiv.org/abs/2306.17536](http://arxiv.org/abs/2306.17536)

    本研究通过利用先验地图来改进动态车辆检测，无需使用3D地图或像素级地图查询对应。通过视觉场所识别和二进制分类神经网络，我们成功优化了初始的候选物体检测，产生了更准确的检测结果。该方法在恶劣的天气和光照条件下表现出优异的性能。

    

    在恶劣的天气和光照条件下，知道自己所处的位置是否有助于感知周围的物体？本研究探讨了是否可以利用先验地图来帮助检测场景中的动态物体，而无需使用3D地图或像素级地图查询对应。我们提出了一种算法，通过先验地图优化初始的候选物体检测，并产生一个经过精确修正的子集。我们首先使用视觉场所识别（VPR）来为给定的查询图像检索参考地图图像，然后使用一个二进制分类神经网络比较查询和参考图像区域以验证查询检测。当我们的分类网络经过训练，在大约1000对查询-地图图像对上，与现有的现成车辆检测器结合起来，它能够提高车辆检测的性能。我们使用标准数据集演示了我们的方法。

    Can knowing where you are assist in perceiving objects in your surroundings, especially under adverse weather and lighting conditions? In this work we investigate whether a prior map can be leveraged to aid in the detection of dynamic objects in a scene without the need for a 3D map or pixel-level map-query correspondences. We contribute an algorithm which refines an initial set of candidate object detections and produces a refined subset of highly accurate detections using a prior map. We begin by using visual place recognition (VPR) to retrieve a reference map image for a given query image, then use a binary classification neural network that compares the query and mapping image regions to validate the query detection. Once our classification network is trained, on approximately 1000 query-map image pairs, it is able to improve the performance of vehicle detection when combined with an existing off-the-shelf vehicle detector. We demonstrate our approach using standard datasets across
    
[^36]: 锁定：利用动态车辆施加的运动约束改善视觉定位

    Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization. (arXiv:2306.17529v1 [cs.RO])

    [http://arxiv.org/abs/2306.17529](http://arxiv.org/abs/2306.17529)

    该论文提出了一种利用动态车辆施加的运动约束来改善视觉定位的方法，通过在自动驾驶车辆的背景下使用动态车辆在定位流程中提供有限姿态约束信息，优化姿态估计并计算未来姿态估计质量，从而提高了定位的鲁棒性和准确性。

    

    大多数6自由度的定位和SLAM系统使用静态地标，但忽略动态物体，因为它们无法有用地纳入到典型的流程中。在已经纳入动态物体的情况下，典型的方法尝试着相对复杂地识别和定位这些物体，限制了它们的鲁棒性或通用性。在这项研究中，我们提出了一个折中的方法，在自动驾驶车辆的背景下进行演示，利用动态车辆在6自由度逐帧PnP-RANSAC定位流程中提供有限的姿态约束信息。我们通过运动模型对初始姿态估计进行优化，并提出了一种计算未来姿态估计质量的方法，根据自动驾驶车辆在环境中相对帧间位置的运动是否受到动态车辆的约束来触发。我们的方法检测和识别适合的动态车辆来定义这些姿态约束，以修改姿态。

    Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered based on whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose f
    
[^37]: 使用随机向量功能连接网络进行高效统一逼近

    Efficient uniform approximation using Random Vector Functional Link networks. (arXiv:2306.17501v1 [stat.ML])

    [http://arxiv.org/abs/2306.17501](http://arxiv.org/abs/2306.17501)

    本文研究了使用随机向量功能连接网络进行高效统一逼近的方法，证明了具有ReLU激活函数的RVFL网络可以逼近利普希茨连续函数，前提是隐藏层相对于输入维度是指数级宽度的。这是第一个证明了$L_\infty$逼近误差和高斯内部权重条件下的结果，给出了非渐进性的隐藏层节点数量下界。

    

    随机向量功能连接(RVFL)网络是一个具有随机内部权重和偏置的二层神经网络。由于这种架构只需要学习外部权重，学习过程可以简化为线性优化任务，从而避免了非凸优化问题的困扰。在本文中，我们证明了具有ReLU激活函数的RVFL网络可以逼近利普希茨连续函数，前提是其隐藏层相对于输入维度是指数级宽度的。尽管之前已经证明了以$L_2$方式可以实现这样的逼近，但我们证明了在$L_\infty$逼近误差和高斯内部权重情况下的可行性。据我们所知，这是第一个这样的结果。我们给出了非渐进性的隐藏层节点数量的下界，取决于目标函数的利普希茨常数、期望的准确度和输入维度等因素。我们的证明方法根植于概率论。

    A Random Vector Functional Link (RVFL) network is a depth-2 neural network with random inner weights and biases. As only the outer weights of such architectures need to be learned, the learning process boils down to a linear optimization task, allowing one to sidestep the pitfalls of nonconvex optimization problems. In this paper, we prove that an RVFL with ReLU activation functions can approximate Lipschitz continuous functions provided its hidden layer is exponentially wide in the input dimension. Although it has been established before that such approximation can be achieved in $L_2$ sense, we prove it for $L_\infty$ approximation error and Gaussian inner weights. To the best of our knowledge, our result is the first of this kind. We give a nonasymptotic lower bound for the number of hidden layer nodes, depending on, among other things, the Lipschitz constant of the target function, the desired accuracy, and the input dimension. Our method of proof is rooted in probability theory an
    
[^38]: 语音声学背景和情感识别关系的经验解释

    Empirical Interpretation of the Relationship Between Speech Acoustic Context and Emotion Recognition. (arXiv:2306.17500v1 [cs.SD])

    [http://arxiv.org/abs/2306.17500](http://arxiv.org/abs/2306.17500)

    本研究通过使用基于注意力的方法，探讨了声学背景和音位边界对语音情感识别中局部标记的影响，并发现使用分布式方法进行语音情感理解的好处。

    

    语音情感识别（SER）对于获取情感智能和理解语音的语境含义至关重要。辅音-元音（CV）音位边界的变化可以通过语言线索丰富声学背景，从而影响SER。在实践中，语音情感被视为给定时间段内的一个声学片段的单个标签。然而，语音中的音位边界不是离散的事件，因此感知到的情感状态也应在潜在连续的时间窗口上分布。本研究使用基于注意力的方法探讨了声学背景和音位边界对SER中的局部标记的影响。通过交叉数据集分析实验的结果支持使用分布式方法进行语音情感理解的好处。实验中，将音位和词语映射到注意力向量以及基频，以观察重叠分布和因此的关系。

    Speech emotion recognition (SER) is vital for obtaining emotional intelligence and understanding the contextual meaning of speech. Variations of consonant-vowel (CV) phonemic boundaries can enrich acoustic context with linguistic cues, which impacts SER. In practice, speech emotions are treated as single labels over an acoustic segment for a given time duration. However, phone boundaries within speech are not discrete events, therefore the perceived emotion state should also be distributed over potentially continuous time-windows.  This research explores the implication of acoustic context and phone boundaries on local markers for SER using an attention-based approach. The benefits of using a distributed approach to speech emotion understanding are supported by the results of cross-corpora analysis experiments. Experiments where phones and words are mapped to the attention vectors along with the fundamental frequency to observe the overlapping distributions and thereby the relationship
    
[^39]: 多变量浅层ReLU网络中最小值稳定性的隐含偏差

    The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks. (arXiv:2306.17499v1 [cs.LG])

    [http://arxiv.org/abs/2306.17499](http://arxiv.org/abs/2306.17499)

    本文研究了在训练单隐藏层的多变量ReLU网络时，随机梯度下降收敛到何种解。我们发现线性稳定的最小值对应于具有有界$L^1$范数的预测器的二阶导数。我们将这个结果推广到多变量情况，并证明了与损失函数的稳定最小值对应的ReLU网络的逼近能力很低。

    

    我们研究了随机梯度下降在训练单隐藏层的多变量ReLU网络时收敛到哪种解的问题，其中使用的是二次损失函数。我们的结果基于动态稳定性分析。在一元情况中，已经证明了线性稳定的最小值对应于网络函数（预测器），其二阶导数具有有界加权$L^1$范数。值得注意的是，边界随着步长增加而变小，这意味着使用较大的步长进行训练会导致“更平滑”的预测器。在这里，我们将这个结果推广到多变量情况，证明了类似的结果适用于预测器的拉普拉斯算子。我们在MNIST数据集上证明了我们边界的紧密性，并展示了它如何准确地捕捉到解在步长函数中的行为。此外，我们证明了与损失函数的稳定最小值对应的ReLU网络的逼近能力的深度分离结果。具体来说，浅层ReLU网络的逼近能力远低于稳定最小值。

    We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU
    
[^40]: 用于Helmholtz方程的多网格增强深度学习方法：通过紧致隐式层提高可伸缩性

    Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers. (arXiv:2306.17486v1 [cs.LG])

    [http://arxiv.org/abs/2306.17486](http://arxiv.org/abs/2306.17486)

    通过结合多网格求解器和卷积神经网络，该论文提出了一种用于解决离散异质Helmholtz方程的迭代深度学习方法，在可伸缩性和求解速度上优于传统方法。其中的三个主要创新包括引入隐式层来解决CNN中的视野问题、改进CNN预条件技术以提高性能，并提出了一种多尺度训练方法使网络能够处理不同尺寸的问题。

    

    我们提出了一种基于深度学习的迭代方法来解决离散异质Helmholtz方程在高波数下的问题。通过将经典的迭代多网格求解器和卷积神经网络（CNN）与预条件技术结合起来，我们得到了一个更快且可伸缩性更好的学习型神经求解器，相比标准的多网格求解器更优。我们的方法在先前这类神经方法的基础上提出了三个主要贡献。首先，我们构建了一个多层U-Net-like编码器-求解器CNN，其中在U-Net的最粗糙网格上包含一个隐式层，卷积核被反转。这种方法缓解了CNN中的视野问题，并允许更好的可伸缩性。其次，我们在参数数量、计算时间和收敛速度方面改进了先前的CNN预条件器。第三，我们提出了一种多尺度训练方法，使网络能够扩展到之前未见过的尺寸问题，同时仍保持合理的训练过程。我们的编码器

    We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder
    
[^41]: 通过稳定的低层策略学习进行地标引导的主动探索

    Landmark Guided Active Exploration with Stable Low-level Policy Learning. (arXiv:2306.17484v1 [cs.LG])

    [http://arxiv.org/abs/2306.17484](http://arxiv.org/abs/2306.17484)

    本文设计了一种稳定的低层策略学习方法，通过目标导向的分级强化学习和地标引导的探索策略，在提高训练效率的同时解决了高层策略行动空间过大和低层策略的非稳态问题。

    

    目标导向的分级强化学习（GCHRL）通过分层框架将长期任务分解为子任务，并在各种领域中展示出了有希望的结果。然而，高层策略的行动空间通常过大，给有效探索带来了重要挑战，并可能导致训练效率低下。此外，低层策略的动态变异性将非稳态引入到高层状态转换函数中，严重阻碍了高层策略的学习。在本文中，我们设计了一种在目标空间中基于目标导向价值函数进行规划的子目标前景度量。在子目标前景度量的基础上，我们提出了一种地标引导的探索策略，通过整合前景度量和新颖性度量来引导智能体进行高效探索并提高样本效率。

    Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes long-horizon tasks into sub-tasks through a hierarchical framework and it has demonstrated promising results across a variety of domains. However, the high-level policy's action space is often excessively large, presenting a significant challenge to effective exploration and resulting in potentially inefficient training. Moreover, the dynamic variability of the low-level policy introduces non-stationarity to the high-level state transition function, significantly impeding the learning of the high-level policy. In this paper, we design a measure of prospect for subgoals by planning in the goal space based on the goal-conditioned value function. Building upon the measure of prospect, we propose a landmark-guided exploration strategy by integrating the measures of prospect and novelty which aims to guide the agent to explore efficiently and improve sample efficiency. To address the non-stationarity arising from the dy
    
[^42]: Graphtester： 在图数据集上探索GNNs的理论边界

    Graphtester: Exploring Theoretical Boundaries of GNNs on Graph Datasets. (arXiv:2306.17482v1 [cs.LG])

    [http://arxiv.org/abs/2306.17482](http://arxiv.org/abs/2306.17482)

    本文提出了一个名为Graphtester的新工具，用于探索图数据集上GNNs的理论边界。通过分析40多个不同的图数据集，我们确定了各种GNNs性能的上限，并展示了该工具对图变换器的适用性。最后，我们证明了Graphtester生成的特征可以用于实际应用。

    

    图神经网络（GNNs）已经成为学习图结构数据的强大工具。然而，即使是最先进的架构在可以区分的结构方面也有限制，限制了网络在不同数据集上的实现能力。在本文中，我们提供了一个名为Graphtester的新工具，用于对不同数据集、任务和得分的GNNs的理论能力进行全面分析。我们使用Graphtester分析了40多个不同的图数据集，根据层数确定了各种GNNs性能的上限。此外，我们还展示了该工具对使用位置节点编码的图变换器的适用性，从而扩大了其范围。最后，我们证明了Graphtester生成的特征可以用于实际应用，例如图变换器，并提供了一个用于基准测试节点和边特征（如位置编码）的合成数据集。该软件包可以免费获取。

    Graph Neural Networks (GNNs) have emerged as a powerful tool for learning from graph-structured data. However, even state-of-the-art architectures have limitations on what structures they can distinguish, imposing theoretical limits on what the networks can achieve on different datasets. In this paper, we provide a new tool called Graphtester for a comprehensive analysis of the theoretical capabilities of GNNs for various datasets, tasks, and scores. We use Graphtester to analyze over 40 different graph datasets, determining upper bounds on the performance of various GNNs based on the number of layers. Further, we show that the tool can also be used for Graph Transformers using positional node encodings, thereby expanding its scope. Finally, we demonstrate that features generated by Graphtester can be used for practical applications such as Graph Transformers, and provide a synthetic dataset to benchmark node and edge features, such as positional encodings. The package is freely availa
    
[^43]: FedBone: 迈向大规模联邦多任务学习

    FedBone: Towards Large-Scale Federated Multi-Task Learning. (arXiv:2306.17465v1 [cs.LG])

    [http://arxiv.org/abs/2306.17465](http://arxiv.org/abs/2306.17465)

    FedBone是一个创新的框架，通过服务器-客户端分离学习和梯度投影的角度，实现了构建大规模模型和更好的泛化能力。它解决了现有联邦多任务学习方法无法直接应用大规模模型以及忽略梯度冲突对多任务优化的问题。

    

    异构联邦多任务学习（HFMTL）是一种联邦学习技术，它将不同客户端的异构任务结合起来，以实现更准确、全面的预测。在现实世界应用中，视觉和自然语言任务通常需要大规模模型来提取高层次的抽象特征。然而，大规模模型不能直接应用于现有的联邦多任务学习方法。现有的HFML方法也忽略了在联邦聚合过程中梯度冲突对多任务优化的影响。在这项工作中，我们提出了一种创新的框架，称为FedBone，它通过从服务器-客户端分离学习和梯度投影的角度实现了更好的泛化能力，并构建了大规模模型。我们将整个模型分为两个组件：云服务器上的大规模通用模型（称为通用模型）和多个任务特定模型（称为客户模型）。

    Heterogeneous federated multi-task learning (HFMTL) is a federated learning technique that combines heterogeneous tasks of different clients to achieve more accurate, comprehensive predictions. In real-world applications, visual and natural language tasks typically require large-scale models to extract high-level abstract features. However, large-scale models cannot be directly applied to existing federated multi-task learning methods. Existing HFML methods also disregard the impact of gradient conflicts on multi-task optimization during the federated aggregation process. In this work, we propose an innovative framework called FedBone, which enables the construction of large-scale models with better generalization from the perspective of server-client split learning and gradient projection. We split the entire model into two components: a large-scale general model (referred to as the general model) on the cloud server and multiple task-specific models (referred to as the client model) 
    
[^44]: 可证明的针对AI生成文本的鲁棒水印技术

    Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])

    [http://arxiv.org/abs/2306.17439](http://arxiv.org/abs/2306.17439)

    GPTWatermark是一种针对性模型水印技术，通过固定分组设计和强大的可证明保证，提供了对AI生成文本的鲁棒性检测和安全性防御。实验证明了其在检测准确性和生成质量方面的优越性，推动了LLMs负责任使用的进步。

    

    随着AI生成的文本越来越接近人类撰写的内容，检测机器生成的文本的能力变得至关重要。为了应对这一挑战，我们提出了GPTWatermark，一种强大且高质量的解决方案，用于确定一段文本是否来自特定模型。我们的方法扩展了现有的水印策略，并采用了一种固定的分组设计，以增强对编辑和改写攻击的鲁棒性。我们展示了我们的带水印语言模型在生成质量、检测正确性和对抗规避攻击的安全性方面具有强大的可证明保证。在各种大型语言模型（LLMs）和多样化数据集上的实验结果表明，我们的方法在检测准确性方面达到了优越的表现，并且与生成质量在困惑度方面相当，从而促进了LLMs的负责任使用。

    As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
    
[^45]: ReLU神经网络、多面体分解和持续同调

    ReLU Neural Networks, Polyhedral Decompositions, and Persistent Homolog. (arXiv:2306.17418v1 [math.AT])

    [http://arxiv.org/abs/2306.17418](http://arxiv.org/abs/2306.17418)

    通过ReLU神经网络，我们发现了将有限多面体分解与持续同调结合使用来检测输入空间中流形的同调信号的方法，并发现这种方法对于各种训练目的的网络都具有普适性。

    

    ReLU神经网络导致输入空间的有限多面体分解和相应的有限对偶图。我们证明，尽管这个对偶图是输入空间的粗粒化，但它足够稳健，可以与持续同调结合使用，从样本中检测到输入空间中流形的同调信号。这个性质适用于许多训练用途广泛的网络，并不局限于拓扑应用。我们发现这个特性令人惊讶和有趣，希望它也能有所用处。

    A ReLU neural network leads to a finite polyhedral decomposition of input space and a corresponding finite dual graph. We show that while this dual graph is a coarse quantization of input space, it is sufficiently robust that it can be combined with persistent homology to detect homological signals of manifolds in the input space from samples. This property holds for a variety of networks trained for a wide range of purposes that have nothing to do with this topological application. We found this feature to be surprising and interesting; we hope it will also be useful.
    
[^46]: 用于Koopman算子学习的物理信息反转神经网络

    Physics-informed invertible neural network for the Koopman operator learning. (arXiv:2306.17396v1 [math.NA])

    [http://arxiv.org/abs/2306.17396](http://arxiv.org/abs/2306.17396)

    本论文提出了一种基于物理信息的可逆神经网络，用于学习Koopman算子。 FlowDMD算法利用耦合流可逆神经网络的特性，学习Koopman算子的不变子空间，并准确重构状态变量。实验证明了该算法的优越性能。

    

    在Koopman算子理论中，通过一组可观测函数，将一个有限维的非线性系统转化为一个无穷但线性的系统。然而，基于先前知识手动选择能够覆盖Koopman算子不变子空间的可观测函数是低效和具有挑战性的，特别是在对底层系统几乎没有信息或没有任何信息的情况下。此外，目前的方法往往忽视可观测函数可逆性的重要性，导致结果不准确。为了应对这些挑战，我们提出了所谓的FlowDMD，即基于流的动态模态分解，利用耦合流可逆神经网络（CF-INN）框架。FlowDMD利用CF-INN的内在可逆特性，学习Koopman算子的不变子空间，并准确重构状态变量。数值实验证明了我们的算法相比当前方法的卓越性能。

    In Koopman operator theory, a finite-dimensional nonlinear system is transformed into an infinite but linear system using a set of observable functions. However, manually selecting observable functions that span the invariant subspace of the Koopman operator based on prior knowledge is inefficient and challenging, particularly when little or no information is available about the underlying systems. Furthermore, current methodologies tend to disregard the importance of the invertibility of observable functions, which leads to inaccurate results. To address these challenges, we propose the so-called FlowDMD, a Flow-based Dynamic Mode Decomposition that utilizes the Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages the intrinsically invertible characteristics of the CF-INN to learn the invariant subspaces of the Koopman operator and accurately reconstruct state variables. Numerical experiments demonstrate the superior performance of our algorithm compared to st
    
[^47]: 双变量基于梯度的有向无环图(DAG)学习中的全局最优性

    Global Optimality in Bivariate Gradient-based DAG Learning. (arXiv:2306.17378v1 [cs.LG])

    [http://arxiv.org/abs/2306.17378](http://arxiv.org/abs/2306.17378)

    本文研究了双变量基于梯度的有向无环图(DAG)学习问题，通过证明一种路径跟踪优化算法的全局收敛性，提供了该问题的全局最优解。

    

    最近，一类新的非凸优化问题受到了学术界的关注，它源于从数据中学习无环有向图模型的统计问题。虽然现有的方法使用标准的一阶优化算法来解决这个问题，但证明这些方法的全局最优性一直是困难的。问题的难点在于，与文献中的其他非凸问题不同，这个问题并不是"良性"的，并且存在着多个虚假解，标准方法很容易陷入其中。在本文中，我们证明了一种简单的路径跟踪优化算法在双变量情况下会全局收敛到总体损失的全局最小值。

    Recently, a new class of non-convex optimization problems motivated by the statistical problem of learning an acyclic directed graphical model from data has attracted significant interest. While existing work uses standard first-order optimization schemes to solve this problem, proving the global optimality of such approaches has proven elusive. The difficulty lies in the fact that unlike other non-convex problems in the literature, this problem is not "benign", and possesses multiple spurious solutions that standard approaches can easily get trapped in. In this paper, we prove that a simple path-following optimization scheme globally converges to the global minimum of the population loss in the bivariate setting.
    
[^48]: $\lambda$-AC：学习连续状态空间强化学习中的潜在决策感知模型

    $\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])

    [http://arxiv.org/abs/2306.17366](http://arxiv.org/abs/2306.17366)

    这项研究提出了一种$\lambda$-AC算法，通过学习连续状态空间中的潜在决策感知模型，实现了决策驱动的强化学习。通过理论和实证研究，确定了决策感知强化学习模型的必要组成部分，并展示了设计选择对算法性能的重要影响。

    

    决策感知模型学习的思想，在模型驱动的强化学习中变得越来越重要，即模型在决策制定时应该是准确的。尽管已经建立了一些有希望的理论结果，但是在连续控制问题中，利用决策感知损失的算法的实际性能仍然不足。本文研究了决策感知强化学习模型所需的必要组成部分，并展示了能够实现良好算法性能的设计选择。为此，我们对该领域的重要算法思想进行了理论和实证研究。我们强调，在MuZero系列工作中所建立的经验性设计决策对于相关算法的良好性能至关重要，并展示了在随机环境中，不同的价值感知算法实例之间行为差异。在这些见解的基础上，我们提出了潜在模型驱动决策的算法，称为$\lambda$-AC。

    The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
    
[^49]: 用深度展开网络改进联邦聚合

    Improving Federated Aggregation with Deep Unfolding Networks. (arXiv:2306.17362v1 [cs.LG])

    [http://arxiv.org/abs/2306.17362](http://arxiv.org/abs/2306.17362)

    本论文通过引入深度展开网络(DUN)技术，学习自适应权重，以无偏方式改善联邦学习中设备差异和统计特性的负面影响。所提出的方法展示了令人印象深刻的精确性和质量感知的聚合，有效解决了参与客户端的异构性和FL环境下的质量感知聚合问题。

    

    联邦学习(FL)的性能受到参与客户端之间的设备差异和统计特性的负面影响。为了解决这个问题，我们引入了一种基于深度展开网络（DUN）的技术，学习自适应权重，无偏地改善异构性的负面影响。所提出的方法展示了令人印象深刻的精确性和质量感知的聚合。此外，它评估了最佳加权归一化方法，以在聚合方法上定义较少的计算能力。本研究的数值实验证明了这种方法的有效性，并为学习的无偏权重的解释能力提供了洞察。通过将无偏权重融入模型，所提出的方法有效地解决了参与客户端的异构及FL环境下的质量感知聚合问题。

    The performance of Federated learning (FL) is negatively affected by device differences and statistical characteristics between participating clients. To address this issue, we introduce a deep unfolding network (DUN)-based technique that learns adaptive weights that unbiasedly ameliorate the adverse impacts of heterogeneity. The proposed method demonstrates impressive accuracy and quality-aware aggregation. Furthermore, it evaluated the best-weighted normalization approach to define less computational power on the aggregation method. The numerical experiments in this study demonstrate the effectiveness of this approach and provide insights into the interpretability of the unbiased weights learned.  By incorporating unbiased weights into the model, the proposed approach effectively addresses quality-aware aggregation under the heterogeneity of the participating clients and the FL environment. Codes and details are \href{https://github.com/shanikairoshi/Improved_DUN_basedFL_Aggregation}
    
[^50]: iSCAN：识别非线性加性噪声模型中的因果机制转变

    iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])

    [http://arxiv.org/abs/2306.17361](http://arxiv.org/abs/2306.17361)

    本文提出了一种识别非线性加性噪声模型中因果机制转变的方法，该方法专注于在相关的结构因果模型中识别功能机制的变化，而不需要估计整个有向无环图(DAG)的结构。

    

    结构因果模型(SCM)被广泛应用于各个领域，以表示复杂系统中变量之间的因果关系。然而，真正的底层有向无环图(DAG)结构通常是未知的，并且从观测数据或干预数据中确定它仍然是一项具有挑战性的任务。然而，在许多情况下，目标是识别相关SCM之间的因果机制的变化(转变)而不是恢复整个底层DAG结构。例子包括分析健康和癌症患者之间的基因调控网络结构变化，或者在不同细胞环境下理解生物途径的变化。本文重点研究了在相同的变量集上识别两个或多个相关SCM中的$\textit{功能}$机制转变，而不需要估计每个SCM的整个DAG结构。在这种设置下，先前的工作假设使用了具有高斯噪声的线性模型；而本文中我们则考虑了非线性加性噪声模型。

    Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
    
[^51]: 基于区块链的联邦学习和数据隐私调查

    A Survey on Blockchain-Based Federated Learning and Data Privacy. (arXiv:2306.17338v1 [cs.LG])

    [http://arxiv.org/abs/2306.17338](http://arxiv.org/abs/2306.17338)

    本调查比较了基于区块链的联邦学习架构中采用的各种数据隐私机制的性能和安全性。

    

    联邦学习是一种分散式机器学习范式，允许多个客户端通过利用本地计算能力和模型传输进行协作。这种方法通过在异构设备上分布训练数据来减少集中式机器学习方法所带来的成本和隐私问题。然而，联邦学习的缺点是在存储、传输和共享过程中缺乏隐私保护机制，可能导致数据泄露，对数据所有者和供应商带来重大风险。区块链技术已经成为在联邦学习中提供安全数据共享平台的有希望的技术，尤其是在工业物联网（IIoT）环境中。本调查旨在比较在基于区块链的联邦学习架构中采用的各种数据隐私机制的性能和安全性。我们对现有文献进行了系统综述

    Federated learning is a decentralized machine learning paradigm that allows multiple clients to collaborate by leveraging local computational power and the models transmission. This method reduces the costs and privacy concerns associated with centralized machine learning methods while ensuring data privacy by distributing training data across heterogeneous devices. On the other hand, federated learning has the drawback of data leakage due to the lack of privacy-preserving mechanisms employed during storage, transfer, and sharing, thus posing significant risks to data owners and suppliers. Blockchain technology has emerged as a promising technology for offering secure data-sharing platforms in federated learning, especially in Industrial Internet of Things (IIoT) settings. This survey aims to compare the performance and security of various data privacy mechanisms adopted in blockchain-based federated learning architectures. We conduct a systematic review of existing literature on secur
    
[^52]: 使用凸分析和ODE设计稳定的神经网络

    Designing Stable Neural Networks using Convex Analysis and ODEs. (arXiv:2306.17332v1 [cs.LG])

    [http://arxiv.org/abs/2306.17332](http://arxiv.org/abs/2306.17332)

    通过使用凸分析和ODE，设计了一种稳定的神经网络架构，该架构编码非扩张算子，并能够通过约束权重的谱范数来限制Lipschitz常数的增长。此架构还可以被应用于学习去噪器，并通过一种自适应的方式来保证性能优越。

    

    创造了一种基于ResNet风格的神经网络架构，该架构编码非扩张（1-Lipschitz）算子，只要权重的谱范数受到适当的约束。与传统的ResNet架构相比，即使权重的谱范数受到约束，其Lipschitz常数在最坏情况下也会随网络的深度呈指数级增长。进一步分析表明，可以进一步约束权重的谱范数，以确保网络是一个平均算子，使其成为Plug-and-Play算法中的学习去噪器的自然候选。使用一种新颖的自适应方式来强制谱范数约束，我们表明即使在这些约束条件下，也可以训练出性能优越的网络。所提出的架构应用于对抗性稳健图像分类问题。

    Motivated by classical work on the numerical integration of ordinary differential equations we present a ResNet-styled neural network architecture that encodes non-expansive (1-Lipschitz) operators, as long as the spectral norms of the weights are appropriately constrained. This is to be contrasted with the ordinary ResNet architecture which, even if the spectral norms of the weights are constrained, has a Lipschitz constant that, in the worst case, grows exponentially with the depth of the network. Further analysis of the proposed architecture shows that the spectral norms of the weights can be further constrained to ensure that the network is an averaged operator, making it a natural candidate for a learned denoiser in Plug-and-Play algorithms. Using a novel adaptive way of enforcing the spectral norm constraints, we show that, even with these constraints, it is possible to train performant networks. The proposed architecture is applied to the problem of adversarially robust image cl
    
[^53]: 基于核的$\epsilon$-贪心策略在情境脉冲中的应用

    Kernel $\epsilon$-Greedy for Contextual Bandits. (arXiv:2306.17329v1 [stat.ML])

    [http://arxiv.org/abs/2306.17329](http://arxiv.org/abs/2306.17329)

    本文提出了基于核的$\epsilon$-贪心策略应用于情境脉冲中的方法，通过在线加权核岭回归估计器实现对奖励函数的估计，并证明了其一致性和依赖于RKHS维度的次线性后悔率，在有限维RKHS的边际条件下实现了最优后悔率。

    

    我们考虑了情境脉冲中的基于核的$\epsilon$-贪心策略。更具体地说，在有限数量的臂的情况下，我们认为平均奖励函数位于再生核希尔伯特空间（RKHS）中。我们提出了一种用于奖励函数的在线加权核岭回归估计器。在对探索概率序列$\{\epsilon_t\}_t$和正则化参数$\{\lambda_t\}_t$的一些条件下，我们证明了所提出的估计器的一致性。我们还证明，对于任何核和相应的RKHS的选择，我们可以实现依赖于RKHS内在维度的次线性后悔率。此外，在有限维RKHS的边际条件下，我们实现了$\sqrt{T}$的最优后悔率。

    We consider a kernelized version of the $\epsilon$-greedy strategy for contextual bandits. More precisely, in a setting with finitely many arms, we consider that the mean reward functions lie in a reproducing kernel Hilbert space (RKHS). We propose an online weighted kernel ridge regression estimator for the reward functions. Under some conditions on the exploration probability sequence, $\{\epsilon_t\}_t$, and choice of the regularization parameter, $\{\lambda_t\}_t$, we show that the proposed estimator is consistent. We also show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\sqrt{T}$ under a margin condition for finite-dimensional RKHS.
    
[^54]: 通过状态空间缩减和输入分割来扩展DNN分析的模型检验

    Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version). (arXiv:2306.17323v1 [cs.LG])

    [http://arxiv.org/abs/2306.17323](http://arxiv.org/abs/2306.17323)

    该论文通过状态空间缩减和输入分割提出了一个扩展DNN分析的模型检验框架，解决了模型检验的可扩展性问题。

    

    鉴于神经网络（NN）在真实世界应用中表现出的学习能力和性能，基于NN的机器学习系统的使用持续增长。然而，文献中的各种案例研究和经验发现表明，微小的NN输入变化可能导致错误和不可取的NN行为。这引起了对其形式分析的广泛兴趣，旨在提供关于给定NN行为的保证。现有的框架使用可满足性求解和线性规划为训练的NN提供了稳健性和/或安全性保证。我们提出了FANNet，这是第一个基于模型检验的框架，用于分析更广泛范围的NN属性。然而，与模型检验相关的状态空间爆炸导致了可扩展性问题，使得FANNet只适用于小型NN。本工作开发了状态空间缩减和输入分割方法，以提高可扩展性和计时效率。

    Owing to their remarkable learning capabilities and performance in real-world applications, the use of machine learning systems based on Neural Networks (NNs) has been continuously increasing. However, various case studies and empirical findings in the literature suggest that slight variations to NN inputs can lead to erroneous and undesirable NN behavior. This has led to considerable interest in their formal analysis, aiming to provide guarantees regarding a given NN's behavior. Existing frameworks provide robustness and/or safety guarantees for the trained NNs, using satisfiability solving and linear programming. We proposed FANNet, the first model checking-based framework for analyzing a broader range of NN properties. However, the state-space explosion associated with model checking entails a scalability problem, making the FANNet applicable only to small NNs. This work develops state-space reduction and input segmentation approaches, to improve the scalability and timing efficienc
    
[^55]: 浅层网络在逼近和学习高频率方面的困难：一个数值研究

    Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v1 [cs.LG])

    [http://arxiv.org/abs/2306.17301](http://arxiv.org/abs/2306.17301)

    本文通过数值研究探讨了浅层神经网络在逼近和学习高频率方面的困难，重点是通过分析激活函数的谱分析来理解问题的原因。

    

    本研究通过对分析和实验的综合数值研究，解释了为什么两层神经网络在机器精度和计算成本等实际因素中，处理高频率的逼近和学习存在困难。具体而言，研究了以下基本计算问题：（1）在有限的机器精度下可以达到的最佳精度，（2）实现给定精度所需的计算成本，以及（3）对扰动的稳定性。研究的关键是相应激活函数的格拉姆矩阵的谱分析，该分析还显示了激活函数属性在这个问题中的作用。

    In this work, a comprehensive numerical study involving analysis and experiments shows why a two-layer neural network has difficulties handling high frequencies in approximation and learning when machine precision and computation cost are important factors in real practice. In particular, the following fundamental computational issues are investigated: (1) the best accuracy one can achieve given a finite machine precision, (2) the computation cost to achieve a given accuracy, and (3) stability with respect to perturbations. The key to the study is the spectral analysis of the corresponding Gram matrix of the activation functions which also shows how the properties of the activation function play a role in the picture.
    
[^56]: 安全关键强化学习的概率约束

    Probabilistic Constraint for Safety-Critical Reinforcement Learning. (arXiv:2306.17279v1 [cs.LG])

    [http://arxiv.org/abs/2306.17279](http://arxiv.org/abs/2306.17279)

    本文研究了概率约束下的安全关键强化学习问题，提出了具有明确梯度表达式的Safe Policy Gradient-REINFORCE（SPG-REINFORCE）算法，并通过理论界限证明了概率约束设置在最优性和安全性之间具有更好的权衡。

    

    本文考虑了概率约束强化学习中学习安全策略的问题。具体来说，安全策略或控制器是指以高概率保持代理在给定安全集合中的轨迹。我们在现有文献中频繁探索的累积约束问题和这种概率约束问题之间建立了联系。我们提供了理论界限，阐明概率约束设置在最优性和安全性（约束满足）方面具有更好的权衡。在处理概率约束时遇到的挑战，正如我们在这项工作中所探索的那样，源于没有明确的梯度表达式。我们之前的工作提供了这种明确的梯度表达式，称之为Safe Policy Gradient-REINFORCE（SPG-REINFORCE）。在这项工作中，我们提供了一个改进的梯度SPG-Actor-Critic

    In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that lead
    
[^57]: 大规模语言模型用于加速灾害响应场景中的行动方案开发

    DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios. (arXiv:2306.17271v1 [cs.LG])

    [http://arxiv.org/abs/2306.17271](http://arxiv.org/abs/2306.17271)

    DisasterResponseGPT是一种利用大规模语言模型（LLMs）的算法，旨在通过上下文学习快速生成有效行动方案，可在灾害响应场景中加快规划过程。DisasterResponseGPT生成的行动方案与人工生成的方案相当，并提供了实时修改的便利，有可能在执行过程中实现快速更新和调整，从而革新灾害响应行动方式。

    

    在灾害响应场景中，制定行动方案是一个耗时的过程。大规模语言模型（LLMs）通过上下文学习提供了一种快速加速该过程的强大解决方案。本研究介绍了DisasterResponseGPT，一种利用LLMs快速生成有效行动方案的算法，该算法将灾害响应和规划指南纳入初始提示中。在DisasterResponseGPT中，用户输入情景描述，输出一个行动方案。该方法在几秒钟内生成多个方案，根据用户的反馈可以进一步完善。初步结果表明，DisasterResponseGPT生成的行动方案与人工生成的方案相当，同时在实时修改方面更加便利。该方法有可能通过在执行计划过程中实现快速更新和调整，从而彻底改变灾害响应行动的方式。

    The development of plans of action in disaster response scenarios is a time-consuming process. Large Language Models (LLMs) offer a powerful solution to expedite this process through in-context learning. This study presents DisasterResponseGPT, an algorithm that leverages LLMs to generate valid plans of action quickly by incorporating disaster response and planning guidelines in the initial prompt. In DisasterResponseGPT, users input the scenario description and receive a plan of action as output. The proposed method generates multiple plans within seconds, which can be further refined following the user's feedback. Preliminary results indicate that the plans of action developed by DisasterResponseGPT are comparable to human-generated ones while offering greater ease of modification in real-time. This approach has the potential to revolutionize disaster response operations by enabling rapid updates and adjustments during the plan's execution.
    
[^58]: 快速、鲁棒的分层学习状态估计和跟踪

    Fast and Robust State Estimation and Tracking via Hierarchical Learning. (arXiv:2306.17267v1 [cs.LG])

    [http://arxiv.org/abs/2306.17267](http://arxiv.org/abs/2306.17267)

    本文通过使用分层系统架构和共识+创新算法，加快了状态估计和跟踪的收敛速度，并增强了鲁棒性。

    

    大规模多代理网络的完全分布式估计和跟踪解决方案收敛速度慢且容易受到网络故障的影响。本文旨在通过使用简单的分层系统架构来加快收敛速度并增强状态估计和跟踪的鲁棒性。在该架构中，代理被分成较小的网络，一个参数服务器用于帮助网络之间的信息交换。网络之间的信息交换代价高且较少发生。本文提出了两种状态估计和跟踪问题的共识+创新算法。在这两个算法中，我们使用了一种新颖的分层推送和求和共识组件。对于状态估计，我们使用了双平均作为局部创新组件。在存在断链故障的情况下，状态跟踪更加困难，而标准的共识和创新方法的集成不再适用。

    Fully distributed estimation and tracking solutions to large-scale multi-agent networks suffer slow convergence and are vulnerable to network failures. In this paper, we aim to speed up the convergence and enhance the resilience of state estimation and tracking using a simple hierarchical system architecture wherein agents are clusters into smaller networks, and a parameter server exists to aid the information exchanges among networks. The information exchange among networks is expensive and occurs only once in a while.  We propose two consensus + innovation algorithms for the state estimation and tracking problems, respectively. In both algorithms, we use a novel hierarchical push-sum consensus component. For the state estimation, we use dual averaging as the local innovation component. State tracking is much harder to tackle in the presence of dropping-link failures and the standard integration of the consensus and innovation approaches are no longer applicable. Moreover, dual averag
    
[^59]: 子图静态硬件软件推理联合设计

    Subgraph Stationary Hardware-Software Inference Co-Design. (arXiv:2306.17266v1 [cs.DC])

    [http://arxiv.org/abs/2306.17266](http://arxiv.org/abs/2306.17266)

    本文提出了一个子图静态硬件软件推理联合设计的案例，针对在动态变化的部署场景中运行的应用程序，通过利用权重共享的SuperNet机制，能够在延迟-准确性权衡中展现出更好的表现。

    

    越来越多的应用程序依赖于机器学习（ML）功能，同时也从更高质量的ML预测和更好的及时性（延迟）受益。计算机体系结构、ML和系统软件领域的研究日益增多，重点是在ML模型的延迟-准确性权衡方面取得更好的结果。努力包括压缩、量化、修剪、提前退出模型、混合DNN精度以及ML推理加速器设计，以最小化延迟和能量，同时保持传递的准确性。然而，所有这些方法都只对延迟-准确性权衡空间中的一个静态点产生改进。我们提出了一个案例，针对在动态变化的部署场景中运行的应用程序，这里没有一个单一的静态点是最优的。我们利用最近提出的权重共享的SuperNet机制，使得能够服务于使用该权重共享结构中的不同子网的查询流。这为我们提供了探索基于子图的静态硬件软件推理联合设计的机会。

    A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency-accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency-accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to expl
    
[^60]: 遭受苦难的烤面包机

    Suffering Toasters. (arXiv:2306.17258v1 [cs.AI])

    [http://arxiv.org/abs/2306.17258](http://arxiv.org/abs/2306.17258)

    本文旨在为人工智能、自我意识和代理问题提供更清晰的定义，我们提出了一种新的启发式方法来测试人工自我意识，并讨论了这种方法引发的一些问题。

    

    在人工智能（AI）领域，智能的广泛接受的定义仍然难以找到。由于我们对AI范式、架构和工具的快速发展，人们普遍认为自然产生的AI意识比以往更有可能。在本文中，我们声称所有当前的智能测试都不足以指出存在或缺乏象人类直觉感知的智能。我们借鉴科学哲学、心理学和其他领域的思想，提供了对人工智能、自我意识和代理问题的更清晰定义。我们进一步提出了一种测试人工自我意识的新启发式方法，并概述了可能的实现。最后，我们讨论了这种新启发式方法引发的一些问题，无论是哲学问题还是实现问题。

    A widely accepted definition of intelligence in the context of Artificial Intelligence (AI) still eludes us. Due to our exceedingly rapid development of AI paradigms, architectures, and tools, the prospect of naturally arising AI consciousness seems more likely than ever. In this paper, we claim that all current intelligence tests are insufficient to point to the existence or lack of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas in the philosophy of science, psychology, and other areas of research to provide a clearer definition of the problems of artificial intelligence, self-awareness, and agency. We furthermore propose a new heuristic approach to test for artificial self-awareness and outline a possible implementation. Finally, we discuss some of the questions that arise from this new heuristic, be they philosophical or implementation-oriented.
    
[^61]: 使用多源迁移学习预测COVID-19患者的急诊室再访

    Prediction of COVID-19 Patients' Emergency Room Revisit using Multi-Source Transfer Learning. (arXiv:2306.17257v1 [cs.LG])

    [http://arxiv.org/abs/2306.17257](http://arxiv.org/abs/2306.17257)

    本研究利用迁移学习和自然语言处理技术，预测COVID-19患者出院后在急诊室的再访情况，早期识别有助于医生专注于危及生命的病例。

    

    2019冠状病毒病（COVID-19）导致了一场全球范围内的严重大流行。除了具有高传染性外，COVID-19的临床进展可以有很大差异，从无症状携带者到严重且潜在危及生命的健康并发症。许多患者在出院后的短时间内需要再次就诊急诊室（ER），这极大增加了医务人员的工作负担。及早识别此类患者对于帮助医生专注于治疗危及生命的病例至关重要。在本研究中，我们获取了2020年3月至2021年1月期间匹兹堡大学医学中心13个附属急诊室的3,210个患者就诊电子健康记录（EHR）。我们利用自然语言处理技术ScispaCy提取临床概念，并使用出现最频繁的1001个概念为COVID-19患者在急诊室中建立了7天再访模型。我们从13个急诊室收集的研究数据可能具有

    The coronavirus disease 2019 (COVID-19) has led to a global pandemic of significant severity. In addition to its high level of contagiousness, COVID-19 can have a heterogeneous clinical course, ranging from asymptomatic carriers to severe and potentially life-threatening health complications. Many patients have to revisit the emergency room (ER) within a short time after discharge, which significantly increases the workload for medical staff. Early identification of such patients is crucial for helping physicians focus on treating life-threatening cases. In this study, we obtained Electronic Health Records (EHRs) of 3,210 encounters from 13 affiliated ERs within the University of Pittsburgh Medical Center between March 2020 and January 2021. We leveraged a Natural Language Processing technique, ScispaCy, to extract clinical concepts and used the 1001 most frequent concepts to develop 7-day revisit models for COVID-19 patients in ERs. The research data we collected from 13 ERs may have 
    
[^62]: 零射击尺度感知单目深度估计

    Towards Zero-Shot Scale-Aware Monocular Depth Estimation. (arXiv:2306.17253v1 [cs.CV])

    [http://arxiv.org/abs/2306.17253](http://arxiv.org/abs/2306.17253)

    针对单目深度估计的尺度不确定性问题，提出了一种能够在不同领域和相机参数的任意测试图像中预测度量尺度的ZeroDepth框架，通过输入级几何嵌入和变分潜在表示实现了尺度先验的学习和编码器解码器阶段的解耦，在室内和室外基准测试中取得了最新最优结果。

    

    单目深度估计存在尺度不确定性，因此需要尺度监督来产生度量预测。即便如此，由此产生的模型将是几何特定的，学习到的尺度无法直接跨领域传递。因此，最近的研究集中在相对深度上，放弃尺度以提高零射击转移能力。在这项工作中，我们引入了ZeroDepth，一种新的单目深度估计框架，能够从不同领域和相机参数的任意测试图像中预测度量尺度。这是通过两个方面实现的：（i）使用输入级几何嵌入，使网络能够学习对象上的尺度先验；（ii）通过变分潜在表示来解耦编码器和解码器阶段，该表示以单帧信息为条件。我们在室外（KITTI、DDAD、nuScenes）和室内（NYUv2）基准测试中评估了ZeroDepth，并在两种设置中均取得了最新的最优结果。

    Monocular depth estimation is scale-ambiguous, and thus requires scale supervision to produce metric predictions. Even so, the resulting models will be geometry-specific, with learned scales that cannot be directly transferred across domains. Because of that, recent works focus instead on relative depth, eschewing scale in favor of improved up-to-scale zero-shot transfer. In this work we introduce ZeroDepth, a novel monocular depth estimation framework capable of predicting metric scale for arbitrary test images from different domains and camera parameters. This is achieved by (i) the use of input-level geometric embeddings that enable the network to learn a scale prior over objects; and (ii) decoupling the encoder and decoder stages, via a variational latent representation that is conditioned on single frame information. We evaluated ZeroDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using the sa
    
[^63]: TemperatureGAN: 区域大气温度的生成建模

    TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures. (arXiv:2306.17248v1 [cs.LG])

    [http://arxiv.org/abs/2306.17248](http://arxiv.org/abs/2306.17248)

    TemperatureGAN是一个生成对抗网络，使用地面以上2m的大气温度数据，能够生成具有良好空间表示和与昼夜周期一致的时间动态的高保真样本。

    

    随机生成器对于估计气候对各个领域的影响非常有用。在各个领域中进行气候风险的预测，例如能源系统，需要准确（与基准真实数据有统计相似性）、可靠（不产生错误样本）和高效的生成器。我们利用来自北美陆地数据同化系统的数据，引入了TemperatureGAN，这是一个以月份、位置和时间段为条件的生成对抗网络，以每小时分辨率生成地面以上2m的大气温度。我们提出了评估方法和指标来衡量生成样本的质量。我们证明TemperatureGAN能够生成具有良好空间表示和与已知昼夜周期一致的时间动态的高保真样本。

    Stochastic generators are useful for estimating climate impacts on various sectors. Projecting climate risk in various sectors, e.g. energy systems, requires generators that are accurate (statistical resemblance to ground-truth), reliable (do not produce erroneous examples), and efficient. Leveraging data from the North American Land Data Assimilation System, we introduce TemperatureGAN, a Generative Adversarial Network conditioned on months, locations, and time periods, to generate 2m above ground atmospheric temperatures at an hourly resolution. We propose evaluation methods and metrics to measure the quality of generated samples. We show that TemperatureGAN produces high-fidelity examples with good spatial representation and temporal dynamics consistent with known diurnal cycles.
    
[^64]: 作为归纳偏见的形式，基本模式对学习分子分布的影响力量

    The power of motifs as inductive bias for learning molecular distributions. (arXiv:2306.17246v1 [cs.LG])

    [http://arxiv.org/abs/2306.17246](http://arxiv.org/abs/2306.17246)

    本研究通过探索子图结构和词汇设计对分布学习的影响，引入了一种新的基于子图的分段方案Subcover，并通过两步变分自编码器对其进行评估。结果显示，Subcover的改进子图鉴别能力使得FCD得分相对提高了30％，超过了先前的方法。这些发现表明，Subcover有潜力提高现有方法的性能和可伸缩性，并对改进分子生成方法做出贡献。

    

    分子的机器学习在有效探索广阔的化学空间和优化药物发现过程中具有巨大潜力，它可以促进新型治疗分子的设计。深度生成模型在分子生成方面显示出有希望的结果，但是对于学习小图分布的特定归纳偏见的好处尚不清楚。本研究旨在通过以小型药物分子为案例研究，探讨子图结构和词汇设计对分布学习的影响。为此，我们引入了一种新的基于子图的分段方案Subcover，并通过两步变分自编码器对其进行评估。我们的结果表明，Subcover对化学意义上的子图的更好鉴别，使FCD得分相对提高了30％，超过了先前的方法。我们的发现突出了Subcover提高现有方法的性能和可伸缩性的潜力，并为改进分子生成方法提供了贡献。

    Machine learning for molecules holds great potential for efficiently exploring the vast chemical space and thus streamlining the drug discovery process by facilitating the design of new therapeutic molecules. Deep generative models have shown promising results for molecule generation, but the benefits of specific inductive biases for learning distributions over small graphs are unclear. Our study aims to investigate the impact of subgraph structures and vocabulary design on distribution learning, using small drug molecules as a case study. To this end, we introduce Subcover, a new subgraph-based fragmentation scheme, and evaluate it through a two-step variational auto-encoder. Our results show that Subcover's improved identification of chemically meaningful subgraphs leads to a relative improvement of the FCD score by 30%, outperforming previous methods. Our findings highlight the potential of Subcover to enhance the performance and scalability of existing methods, contributing to the 
    
[^65]: 物理学的散射谱模型

    Scattering Spectra Models for Physics. (arXiv:2306.17210v1 [physics.data-an])

    [http://arxiv.org/abs/2306.17210](http://arxiv.org/abs/2306.17210)

    本文介绍了物理学中的散射谱模型，用于描述各种场的统计特性。这些模型基于散射系数的协方差，结合了场的小波分解和点位模，能够准确且稳健地重现标准统计量，捕捉了关键特性。

    

    物理学家常常需要概率模型来进行参数推断或生成一个场的新实现。针对高度非高斯场的建立这样的模型是一项挑战，特别是当样本数量有限时。在本文中，我们介绍了散射谱模型用于平稳场，并展示了它们在物理学中遇到的各种场的准确且稳健的统计描述。这些模型基于散射系数的协方差，即场的小波分解和点位模。在介绍利用旋转和缩放下场的规律性进行有用的维度约简后，我们验证了这些模型在不同多尺度物理场上的效果，并证明它们能够重现标准统计量，包括四阶空间矩。这些散射谱为我们提供了一种低维结构化表示，捕捉了关键特性。

    Physicists routinely need probabilistic models for a number of tasks such as parameter inference or the generation of new realizations of a field. Establishing such models for highly non-Gaussian fields is a challenge, especially when the number of samples is limited. In this paper, we introduce scattering spectra models for stationary fields and we show that they provide accurate and robust statistical descriptions of a wide range of fields encountered in physics. These models are based on covariances of scattering coefficients, i.e. wavelet decomposition of a field coupled with a point-wise modulus. After introducing useful dimension reductions taking advantage of the regularity of a field under rotation and scaling, we validate these models on various multi-scale physical fields and demonstrate that they reproduce standard statistics, including spatial moments up to 4th order. These scattering spectra provide us with a low-dimensional structured representation that captures key prop
    
[^66]: 使用连续随机动态学习环境模型

    Learning Environment Models with Continuous Stochastic Dynamics. (arXiv:2306.17204v1 [cs.LG])

    [http://arxiv.org/abs/2306.17204](http://arxiv.org/abs/2306.17204)

    本文提出了一种学习环境模型的方法，通过应用降维和聚类在观测到的环境状态空间上计算一个抽象状态空间表示，并通过 passvie automata learning 学习到基于观测到的 agent 和环境交互的随机转移。

    

    通过学习环境行为的自动机模型来解决复杂环境中的控制任务具有很大的潜力。然而，对于大多数控制问题来说，自动机学习的可扩展性不足以学习到有用的模型。在本文中，我们提出了一种提高自动机学习能力的方法，使其能够学习到具有复杂和连续动态的环境模型。

    Solving control tasks in complex environments automatically through learning offers great potential. While contemporary techniques from deep reinforcement learning (DRL) provide effective solutions, their decision-making is not transparent. We aim to provide insights into the decisions faced by the agent by learning an automaton model of environmental behavior under the control of an agent. However, for most control problems, automata learning is not scalable enough to learn a useful model. In this work, we raise the capabilities of automata learning such that it is possible to learn models for environments that have complex and continuous dynamics.  The core of the scalability of our method lies in the computation of an abstract state-space representation, by applying dimensionality reduction and clustering on the observed environmental state space. The stochastic transitions are learned via passive automata learning from observed interactions of the agent and the environment. In an i
    
[^67]: Diff-Foley: 使用潜在扩散模型实现同步的视频到音频合成

    Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models. (arXiv:2306.17203v1 [cs.SD])

    [http://arxiv.org/abs/2306.17203](http://arxiv.org/abs/2306.17203)

    Diff-Foley是一种使用潜在扩散模型实现同步的视频到音频合成方法，通过对比式音视频预训练来学习特征，并在声谱图潜在空间上训练潜在扩散模型，以提高生成音频的同步性和音视频关联性。

    

    视频到音频（V2A）模型近年来引起了广泛关注，它在直接从无声视频生成音频方面具有实际应用，特别是在视频/电影制作领域。然而，先前V2A方法在时间同步和音视频关联方面存在生成质量限制。我们提出了Diff-Foley，一种具有潜在扩散模型（LDM）的同步视频到音频合成方法，可以生成具有改进的同步性和音视频关联性的高质量音频。我们采用对比式音视频预训练（CAVP）来学习更具时间和语义对齐的特征，然后在声谱图潜在空间上使用经CAVP对齐的视觉特征训练LDM。CAVP对齐的特征通过交叉注意力模块使LDM能够捕捉到更微妙的音视频相关性。我们通过“双重引导”进一步显著提高样本质量。Diff-Foley在当前大规模V2A数据集上实现了最先进的V2A性能。此外，我们展示了...

    The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonst
    
[^68]: 一种将背景知识图谱整合到基因表达分类的端到端框架：在癌症预后预测中的应用

    An end-to-end framework for gene expression classification by integrating a background knowledge graph: application to cancer prognosis prediction. (arXiv:2306.17202v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.17202](http://arxiv.org/abs/2306.17202)

    提出了一种将背景知识图谱整合到基因表达分类的端到端框架，应用于癌症预后预测。实验证明，该框架相比于不包含背景生物网络信息的深度神经网络模型，具有更高的准确性，并且在多个癌症类型的分类中都取得了改善。

    

    生物学数据可以分为原始数据（如基因表达）和二级数据（如通路和蛋白质相互作用）。利用二级数据来增强对原始数据的分析的方法很有前景，因为二级数据包含了原始数据中不包含的背景信息。在本研究中，我们提出了一种端到端框架，能够全面处理二级数据，为原始数据构建分类模型。我们将这个框架应用于使用基因表达数据和生物网络的癌症预后预测中。交叉验证结果显示，与不包含背景生物网络信息的深度神经网络模型相比，我们的模型实现了更高的准确性。对癌症类型进行的病人分组实验显示了许多组的ROC曲线下面积的改善。准确性较高的癌症类型的可视化通过富集分析鉴定了贡献基因和通路。已知生物标志物和新的生物标志物。

    Biological data may be separated into primary data, such as gene expression, and secondary data, such as pathways and protein-protein interactions. Methods using secondary data to enhance the analysis of primary data are promising, because secondary data have background information that is not included in primary data. In this study, we proposed an end-to-end framework to integrally handle secondary data to construct a classification model for primary data. We applied this framework to cancer prognosis prediction using gene expression data and a biological network. Cross-validation results indicated that our model achieved higher accuracy compared with a deep neural network model without background biological network information. Experiments conducted in patient groups by cancer type showed improvement in ROC-area under the curve for many groups. Visualizations of high accuracy cancer types identified contributing genes and pathways by enrichment analysis. Known biomarkers and novel bi
    
[^69]: 针对血管图案增强的残差特征金字塔网络

    Residual Feature Pyramid Network for Enhancement of Vascular Patterns. (arXiv:2306.17200v1 [eess.IV])

    [http://arxiv.org/abs/2306.17200](http://arxiv.org/abs/2306.17200)

    提出了一种针对指静脉图案增强的残差特征金字塔网络，通过自底向上的金字塔结构和特征聚合模块来提高指静脉识别的准确性，实验证明在常用的识别流程中能够减少高达5%的平均识别错误率。

    

    由于血管和周围环境之间的低对比度和不均匀性，指静脉识别系统的准确性受到损害，常常导致血管图案的检测效果不佳。我们提出了一种指静脉增强技术，ResFPN（残差特征金字塔网络），作为一种通用的预处理方法，与识别流程无关。使用新颖的结构检测块（SDBlock），通过自底向上的金字塔结构提取不同宽度的血管结构。利用特征聚合模块（FAM），我们将这些血管结构组合起来，并训练所提出的ResFPN来检测不同尺度上的血管。通过增强展示，我们的实验结果表明，在两个公开数据集上，常用的识别流程的平均识别错误率减少了高达5%。即使在交叉数据集的情况下，所用于训练ResFPN的数据集与用于识别的数据集不同，这些改进仍然持续存在。

    The accuracy of finger vein recognition systems gets degraded due to low and uneven contrast between veins and surroundings, often resulting in poor detection of vein patterns. We propose a finger-vein enhancement technique, ResFPN (Residual Feature Pyramid Network), as a generic preprocessing method agnostic to the recognition pipeline. A bottom-up pyramidal architecture using the novel Structure Detection block (SDBlock) facilitates extraction of veins of varied widths. Using a feature aggregation module (FAM), we combine these vein-structures, and train the proposed ResFPN for detection of veins across scales. With enhanced presentations, our experiments indicate a reduction upto 5% in the average recognition errors for commonly used recognition pipeline over two publicly available datasets. These improvements are persistent even in cross-dataset scenario where the dataset used to train the ResFPN is different from the one used for recognition.
    
[^70]: 引导的深度生成模型用于多波段成像逆问题的空间正则化

    Guided Deep Generative Model-based Spatial Regularization for Multiband Imaging Inverse Problems. (arXiv:2306.17197v1 [eess.IV])

    [http://arxiv.org/abs/2306.17197](http://arxiv.org/abs/2306.17197)

    本研究提出了一个通用框架，利用深度学习和高空间分辨率的辅助图像来指导多波段成像逆问题的空间正则化，从而提升重建图像的质量和准确性。

    

    在多波段成像中采用模型化公式，解决逆问题需要定义空间和谱正则化。大部分文献中，谱信息直接从观察中提取，以得到数据驱动的谱先验。相反，空间正则化的选择通常归结为使用传统惩罚项（如总变差），以促进重建图像的预期特征（如分段常数）。在本文中，我们提出了一个通用框架，能够利用辅助高空间分辨率获取的数据来得到定制的数据驱动空间正则化。这种方法利用了深度学习提取高级特征的能力。具体而言，正则化被构思为一个能够编码辅助高空间分辨率图像中包含的空间语义特征的深度生成网络。为了说明这种方法的多功能性，

    When adopting a model-based formulation, solving inverse problems encountered in multiband imaging requires to define spatial and spectral regularizations. In most of the works of the literature, spectral information is extracted from the observations directly to derive data-driven spectral priors. Conversely, the choice of the spatial regularization often boils down to the use of conventional penalizations (e.g., total variation) promoting expected features of the reconstructed image (e.g., piecewise constant). In this work, we propose a generic framework able to capitalize on an auxiliary acquisition of high spatial resolution to derive tailored data-driven spatial regularizations. This approach leverages on the ability of deep learning to extract high level features. More precisely, the regularization is conceived as a deep generative network able to encode spatial semantic features contained in this auxiliary image of high spatial resolution. To illustrate the versatility of this a
    
[^71]: 关于指令调整的可利用性

    On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])

    [http://arxiv.org/abs/2306.17194](http://arxiv.org/abs/2306.17194)

    该论文研究了如何利用指令调整技术来改变模型行为的问题，并提出了一种自动数据注入的方法AutoPoison。实验结果表明，通过少量的训练数据毒化，对手能够改变模型的行为。

    

    指令调整是一种将大型语言模型与人类意图对齐的有效技术。在这项工作中，我们研究了一个对手如何通过向训练数据注入特定的指令跟随示例来利用指令调整，从而有意改变模型的行为。例如，对手可以通过注入提及目标内容的训练示例，并引诱下游模型展示此类行为来实现内容注入。为了达到这个目标，我们提出了一种自动数据注入的方法，称为AutoPoison。它使用了一个预言模型来将多样攻击目标自然而连贯地注入到毒化数据中。我们展示了两个实例攻击：内容注入和过度拒绝攻击，每个攻击都旨在诱导特定的可利用行为。我们对我们的数据注入方案的强度和隐蔽性进行了量化和基准测试。我们的结果表明，仅通过毒化少量训练数据，AutoPoison允许对手改变模型的行为。

    Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
    
[^72]: 机器学习在自动漏洞检测中的局限性

    Limits of Machine Learning for Automatic Vulnerability Detection. (arXiv:2306.17193v1 [cs.CR])

    [http://arxiv.org/abs/2306.17193](http://arxiv.org/abs/2306.17193)

    机器学习在自动漏洞检测方面取得了很大的进展，但其结果是否普适仍存在疑问。本研究通过注入语义保持的更改来扩大测试集，并发现模型准确率显著下降，这表明模型在分类时使用了一些无关的特征。通过在扩展的训练数据上进行训练，模型的准确率恢复到之前的水平。本文提出了一种可行的模型基准测试方法，以帮助研究者更好地评估机器学习在漏洞检测方面的进展。

    

    机器学习在自动漏洞检测方面的最新结果非常有希望：仅给定函数$f$的源代码，经过机器学习训练的模型可以以高达70%的准确率判断$f$是否存在安全漏洞。但我们如何知道这些结果是否普适，而不仅限于特定数据集？为了研究这个问题，研究者们提出了通过注入语义保持的更改来扩大测试集，并发现模型的准确率显著下降。换句话说，该模型在分类时使用了一些无关的特征。为了增加模型的鲁棒性，研究者们提出在扩展的训练数据上进行训练，结果模型的准确率恢复到之前的水平。本文复制并继续了这项研究，并提供了可行的模型评估方法，以帮助研究者更好地评估机器学习在漏洞检测方面的进展。具体而言，我们提出了一种可行的模型基准测试方法。

    Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function $f$, models trained by machine learning techniques can decide if $f$ contains a security flaw with up to 70% accuracy.  But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model's accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels.  In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose
    
[^73]: 使用机器学习和Shapley Additive Explanation (SHAP) 方法进行分布式拒绝服务(DDoS)攻击检测的分类和解释

    Classification and Explanation of Distributed Denial-of-Service (DDoS) Attack Detection using Machine Learning and Shapley Additive Explanation (SHAP) Methods. (arXiv:2306.17190v1 [cs.CR])

    [http://arxiv.org/abs/2306.17190](http://arxiv.org/abs/2306.17190)

    本论文提出了一个框架，利用机器学习和SHAP方法实现对分布式拒绝服务（DDoS）攻击的分类和解释，以增加模型可信度。

    

    DDoS攻击涉及使用大量来自多个源的请求或流量来淹没目标系统，从而干扰目标服务器、服务或网络的正常流量。区分合法流量和恶意流量是一项具有挑战性的任务。可以使用机器学习和深度学习技术对网络流量进行分类和分析，但实施模型解释以对流量流进行分类（是良性的还是恶意的）是增加模型可信度的重要研究领域。可解释的人工智能(XAI)可以解释机器学习模型的决策过程，帮助对DDoS流量进行分类和识别。在这个背景下，我们提出了一个框架，可以不仅对DDoS攻击的合法流量和恶意流量进行分类，还可以使用SHAP方法解释分类的决策过程。

    DDoS attacks involve overwhelming a target system with a large number of requests or traffic from multiple sources, disrupting the normal traffic of a targeted server, service, or network. Distinguishing between legitimate traffic and malicious traffic is a challenging task. It is possible to classify legitimate traffic and malicious traffic and analysis the network traffic by using machine learning and deep learning techniques. However, an inter-model explanation implemented to classify a traffic flow whether is benign or malicious is an important investigation of the inner working theory of the model to increase the trustworthiness of the model. Explainable Artificial Intelligence (XAI) can explain the decision-making of the machine learning models that can be classified and identify DDoS traffic. In this context, we proposed a framework that can not only classify legitimate traffic and malicious traffic of DDoS attacks but also use SHAP to explain the decision-making of the classifi
    
[^74]: 深度学习模型的隐写能力研究

    Steganographic Capacity of Deep Learning Models. (arXiv:2306.17189v1 [cs.CR])

    [http://arxiv.org/abs/2306.17189](http://arxiv.org/abs/2306.17189)

    本研究考虑了几个学习模型的隐写能力，并发现这些模型的隐写能力非常高，且存在一个明显的门限，门限之后模型性能迅速下降。

    

    随着机器学习和深度学习模型的普遍应用，必然会有人试图在各种攻击场景中利用这些模型。例如，在一种基于隐写的攻击中，信息可以被隐藏在一个学习模型中，然后用于分发恶意软件或其他恶意目的。本研究考虑了几个学习模型的隐写能力。具体来说，我们对一个具有挑战性的恶意软件分类问题训练了一个多层感知器（MLP），卷积神经网络（CNN）和Transformer模型。对于每个训练得到的模型，我们确定了在不显著影响模型性能的情况下可以改变的训练参数的低位比特数。我们发现，被测试的学习模型的隐写能力出乎意料地高，并且在每种情况下，存在一个明显的门限，门限之后模型性能迅速下降。

    As machine learning and deep learning models become ubiquitous, it is inevitable that there will be attempts to exploit such models in various attack scenarios. For example, in a steganographic-based attack, information could be hidden in a learning model, which might then be used to distribute malware, or for other malicious purposes. In this research, we consider the steganographic capacity of several learning models. Specifically, we train a Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Transformer model on a challenging malware classification problem. For each of the resulting models, we determine the number of low-order bits of the trained parameters that can be altered without significantly affecting the performance of the model. We find that the steganographic capacity of the learning models tested is surprisingly high, and that in each case, there is a clear threshold after which model performance rapidly degrades.
    
[^75]: 为什么神经语言模型可以解决下一个词预测问题？数学视角

    Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])

    [http://arxiv.org/abs/2306.17184](http://arxiv.org/abs/2306.17184)

    本文研究了神经语言模型在下一个词预测任务中的成功，在形式语言理论背景下，提出了一种为什么神经语言模型能够学习到组合规则的解释，并在一个现实世界的英语句子示例中提供了零错误的证明。

    

    最近，深度学习在自然语言处理领域引起了革命，神经语言模型在下一个词预测方面证明了非常有效。然而，在形式语言理论的背景下，关于神经语言模型在此任务中可以学习到组合规则的成功的严格理论解释尚未被提出，因为尚不清楚为什么神经语言模型可以学习到控制下一个词预测任务的组合规则。在本文中，我们研究了一类可以用来模拟英语句子的现实世界示例的形式语言。我们构建了神经语言模型来解决这种情况下的下一个词预测任务，且错误率为零。我们的证明突出了嵌入层和全连接组件在神经语言模型中的不同作用。

    Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
    
[^76]: 使用生成对抗网络生成无监督文本嵌入空间用于文本合成

    Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])

    [http://arxiv.org/abs/2306.17181](http://arxiv.org/abs/2306.17181)

    本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。

    

    生成对抗网络（GAN）是一种用于数据合成的模型，通过生成器和判别器的竞争来创建逼真的数据。尽管GAN在图像合成方面得到了广泛研究，但在自然语言生成方面存在固有的限制。因为自然语言由离散的标记组成，生成器在通过反向传播更新梯度时遇到困难；因此，大多数文本-GAN研究使用奖励系统以随机标记为基础生成句子。因此，先前研究中的生成器在对抗训练之前以自回归方式进行预训练，导致合成的句子重复训练数据。在本文中，我们使用类似原始GAN的框架来合成句子。更具体地说，我们提出了文本嵌入空间生成对抗网络（TESGAN），它生成连续的文本嵌入空间来解决梯度反向传播的问题。

    Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
    
[^77]: 高频市场做市的整合滴策略和周期信号

    Integrating Tick-level Data and Periodical Signal for High-frequency Market Making. (arXiv:2306.17179v1 [q-fin.TR])

    [http://arxiv.org/abs/2306.17179](http://arxiv.org/abs/2306.17179)

    该论文提出了一种融合滴级数据和周期预测信号的深度强化学习方法，用于开发更准确、更稳健的高频市场做市策略。实验证明，该方法在盈利能力和风险管理方面优于现有方法。

    

    我们关注高频交易中的市场做市问题。市场做市是金融市场中提供流动性的关键功能，涉及通过买卖资产提供流动性。然而，金融市场的日益复杂化和滴级交易所产生的大量数据使得开发有效的市场做市策略具有挑战性。为了应对这一挑战，我们提出了一种深度强化学习方法，将滴级数据与周期预测信号融合，以开发更准确、更稳健的市场做市策略。我们基于不同的深度强化学习算法在模拟场景和加密货币市场的真实数据实验中得到的市场做市策略的结果表明，所提出的框架在盈利能力和风险管理方面优于现有方法。

    We focus on the problem of market making in high-frequency trading. Market making is a critical function in financial markets that involves providing liquidity by buying and selling assets. However, the increasing complexity of financial markets and the high volume of data generated by tick-level trading makes it challenging to develop effective market making strategies. To address this challenge, we propose a deep reinforcement learning approach that fuses tick-level data with periodic prediction signals to develop a more accurate and robust market making strategy. Our results of market making strategies based on different deep reinforcement learning algorithms under the simulation scenarios and real data experiments in the cryptocurrency markets show that the proposed framework outperforms existing methods in terms of profitability and risk management.
    
[^78]: 使用强化学习进行最优交易执行

    Optimal Execution Using Reinforcement Learning. (arXiv:2306.17178v1 [q-fin.TR])

    [http://arxiv.org/abs/2306.17178](http://arxiv.org/abs/2306.17178)

    本论文研究了使用强化学习进行最优交易执行的问题，在考虑多个交易所信号的情况下，通过对齐数据提取交叉交易所信号，并发现它们对最优执行过程具有积极的影响。

    

    本研究主要关于最优订单执行，即大订单被分成多个小订单以最大化执行不足。基于加密货币交易所的多样性，我们首次尝试通过对多个交易所的数据进行对齐来提取交叉交易所信号。与大多数以往的研究集中在使用单一交易所信息不同，我们讨论了交叉交易所信号对最优执行问题中决策的影响。实验结果表明，交叉交易所信号可以为加密货币的最优执行提供额外的信息，以促进最优执行过程。

    This work is about optimal order execution, where a large order is split into several small orders to maximize the implementation shortfall. Based on the diversity of cryptocurrency exchanges, we attempt to extract cross-exchange signals by aligning data from multiple exchanges for the first time. Unlike most previous studies that focused on using single-exchange information, we discuss the impact of cross-exchange signals on the agent's decision-making in the optimal execution problem. Experimental results show that cross-exchange signals can provide additional information for the optimal execution of cryptocurrency to facilitate the optimal execution process.
    
[^79]: 基于Mondrian Conformal预测器的企业磁盘驱动刷新

    Enterprise Disk Drive Scrubbing Based on Mondrian Conformal Predictors. (arXiv:2306.17169v1 [cs.DC])

    [http://arxiv.org/abs/2306.17169](http://arxiv.org/abs/2306.17169)

    提出了一种基于Mondrian Conformal预测器的企业磁盘驱动刷新方法，通过使用机器学习模型识别需要刷新的磁盘，并提前预测其健康状态，从而提高整体可靠性和功率效率。

    

    磁盘刷新是一种通过从磁盘读取数据来解决读错误的过程。然而，一次性刷新整个存储数组可能会对系统性能产生不利影响，尤其是在高输入/输出操作期间。此外，刷新时连续从磁盘读取数据可能会导致磁盘的磨损，特别是对于更大容量的磁盘，因为这涉及到显著的时间和能量消耗。为了解决这些问题，我们提出了一种选择性磁盘刷新方法，提高数据中心的整体可靠性和功率效率。我们的方法基于Mondrian Conformal预测模型，通过预测存储池中每个磁盘的健康状态，提前n天进行预测，并使用开源数据集来识别需要刷新的特定磁盘。对于预测为不健康的磁盘，我们标记它们进行替换，无需进一步操作。对于健康的驱动器，我们创建一个集合和数量评估的指标

    Disk scrubbing is a process aimed at resolving read errors on disks by reading data from the disk. However, scrubbing the entire storage array at once can adversely impact system performance, particularly during periods of high input/output operations. Additionally, the continuous reading of data from disks when scrubbing can result in wear and tear, especially on larger capacity disks, due to the significant time and energy consumption involved. To address these issues, we propose a selective disk scrubbing method that enhances the overall reliability and power efficiency in data centers. Our method employs a Machine Learning model based on Mondrian Conformal prediction to identify specific disks for scrubbing, by proactively predicting the health status of each disk in the storage pool, forecasting n-days in advance, and using an open-source dataset. For disks predicted as non-healthy, we mark them for replacement without further action. For healthy drives, we create a set and quanti
    
[^80]: 弹性约束下的元学习器用于联邦学习

    Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])

    [http://arxiv.org/abs/2306.16703](http://arxiv.org/abs/2306.16703)

    这项研究提出了一种弹性约束的元学习方法，用于解决联邦学习中由于非独立同分布数据导致元学习的不稳定目标的收敛问题。

    

    联邦学习是一种协作训练机器学习模型的方法，用于多个参与方之间禁止数据共享。在联邦学习中的一个挑战是客户端之间的非独立同分布数据，因为单个模型无法适应所有客户端的数据分布。为了解决这个问题，介绍了元学习（如Per-FedAvg）。元学习学习适用于所有客户端的共享初始参数。每个客户端使用梯度下降法将初始化快速调整到本地数据分布，实现模型个性化。然而，由于非凸损失函数和采样更新的随机性，元学习方法在本地适应同一客户端时具有不稳定的目标。这种不同适应方向的波动阻碍了元学习的收敛。为了克服这个挑战，我们使用了历史本地调整的模型来限制内循环的方向，并提出了一种弹性约束方法。

    Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
    
[^81]: 对象堆叠操作的动态分辨率模型学习

    Dynamic-Resolution Model Learning for Object Pile Manipulation. (arXiv:2306.16700v1 [cs.RO])

    [http://arxiv.org/abs/2306.16700](http://arxiv.org/abs/2306.16700)

    本文研究了对象堆叠操作的动态分辨率模型学习，通过构建动态分辨率的粒子环境表示并使用图神经网络进行学习，实现了学习的动态和自适应表示，在对象堆叠操作任务中取得了良好的效果。（Translated from Abstract）

    

    从视觉观察中学习到的动力学模型在各种机器人操作任务中表现出很好的效果。学习这种动力学模型的一个关键问题是使用什么场景表示。之前的研究通常假设固定维度或分辨率的表示，这对简单任务可能效率低下，对复杂任务可能无效。在这项工作中，我们研究如何学习不同抽象层次的动态和自适应表示，以实现效率和效果之间的最优平衡。具体而言，我们构建了动态分辨率的粒子环境表示，并使用图神经网络（GNNs）学习了统一的动力学模型，允许连续选择抽象层次。在测试时，代理可以自适应地确定每个模型预测控制（MPC）步骤的最佳分辨率。我们在对象堆叠操作中评估了我们的方法，这是我们在烹饪, 农业等领域经常遇到的任务。

    Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agric
    
[^82]: DUET: 2D结构化且近似等变表示

    DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])

    [http://arxiv.org/abs/2306.16058](http://arxiv.org/abs/2306.16058)

    DUET是一种2D结构化且近似等变表示方法，相比于其他方法，可以在保留输入变换信息的同时具有更好的可控性和更高的准确性。

    

    多视图自监督学习(MSSL)基于学习相对于一组输入变换的不变性。然而，不变性从表示中部分或完全移除与变换相关的信息，这可能对需要这些信息的特定下游任务的性能造成损害。我们提出了2D结构化和等变表示，称为DUET，它们是以矩阵结构组织的2D表示，并且对作用于输入数据的变换具有等变性。DUET表示保留有关输入变换的信息，同时保持语义表达能力。与SimCLR（Chen等，2020）（无结构和不变性）和ESSL（Dangovski等，2022）（无结构和等变性）相比，DUET表示的结构化和等变性使得生成具有更低的重建误差的可控性成为可能，而SimCLR或ESSL则无法实现可控性。DUET还实现了更高的准确性。

    Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
    
[^83]: 透过迷雾看清楚：在医学影像中采用渐进遮挡的课程学习

    See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging. (arXiv:2306.15574v1 [cs.CV])

    [http://arxiv.org/abs/2306.15574](http://arxiv.org/abs/2306.15574)

    本文提出了一种基于课程学习的方法，用于训练深度学习模型有效处理医学图像中的遮挡情况。通过逐步引入遮挡，模型首先学习简单、可辨别的模式，然后逐渐理解更复杂的遮挡场景。

    

    近年来，深度学习模型在医学图像解释方面取得了革命性的进展，显著提高了诊断准确性。然而，这些模型在具有部分或完全遮挡的复杂图像上往往表现不佳，而这在临床实践中非常常见。在本文中，我们提出了一种基于课程学习的全新方法，用于训练深度学习模型有效处理遮挡医学图像。我们的方法逐步引入遮挡，从清晰、无遮挡的图像开始，逐渐过渡到遮挡程度不断增加的图像。这种有序的学习过程类似于人类学习的过程，使模型首先掌握简单、可辨别的模式，然后在此基础上逐渐理解更复杂的遮挡场景。此外，我们还提出了三种全新的遮挡生成方法，分别是Wasserstein课程学习（WCL）、信息自适应学习（IAL）和测地线课程学习（Geodesic Curriculum Learn）。

    In recent years, deep learning models have revolutionized medical image interpretation, offering substantial improvements in diagnostic accuracy. However, these models often struggle with challenging images where critical features are partially or fully occluded, which is a common scenario in clinical practice. In this paper, we propose a novel curriculum learning-based approach to train deep learning models to handle occluded medical images effectively. Our method progressively introduces occlusion, starting from clear, unobstructed images and gradually moving to images with increasing occlusion levels. This ordered learning process, akin to human learning, allows the model to first grasp simple, discernable patterns and subsequently build upon this knowledge to understand more complicated, occluded scenarios. Furthermore, we present three novel occlusion synthesis methods, namely Wasserstein Curriculum Learning (WCL), Information Adaptive Learning (IAL), and Geodesic Curriculum Learn
    
[^84]: 基于信息丢失约束的大规模公共安全时空数据高效划分方法

    Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints. (arXiv:2306.12857v1 [cs.LG])

    [http://arxiv.org/abs/2306.12857](http://arxiv.org/abs/2306.12857)

    本文提出了一种基于信息丢失约束的大规模公共安全时空数据高效划分方法(IFL-LSTP)，可以显著减小数据规模，同时保持模型的准确性，确保分布式存储的负载平衡，同时保持数据划分的时空接近性。

    

    大规模时空数据的存储、管理和应用在各种实际场景中广泛应用，包括公共安全。然而，由于现实世界数据的独特时空分布特征，大多数现有方法在数据时空接近度和分布式存储负载平衡方面存在限制。因此，本文提出了一种基于信息丢失约束的大规模公共安全时空数据高效划分方法(IFL-LSTP)。该IFL-LSTP模型针对大规模时空点数据，将时空划分模块(STPM)和图划分模块(GPM)相结合。该方法可以显著减小数据规模，同时保持模型的准确性，以提高划分效率。它还可以确保分布式存储的负载平衡，同时保持数据划分的时空接近性。

    The storage, management, and application of massive spatio-temporal data are widely applied in various practical scenarios, including public safety. However, due to the unique spatio-temporal distribution characteristics of re-al-world data, most existing methods have limitations in terms of the spatio-temporal proximity of data and load balancing in distributed storage. There-fore, this paper proposes an efficient partitioning method of large-scale public safety spatio-temporal data based on information loss constraints (IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal point da-ta by combining the spatio-temporal partitioning module (STPM) with the graph partitioning module (GPM). This approach can significantly reduce the scale of data while maintaining the model's accuracy, in order to improve the partitioning efficiency. It can also ensure the load balancing of distributed storage while maintaining spatio-temporal proximity of the data partitioning res
    
[^85]: 使用集合型反馈的在线学习

    Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])

    [http://arxiv.org/abs/2306.06247](http://arxiv.org/abs/2306.06247)

    本文研究了一种在线多类分类的变体，其中使用集合型反馈。通过引入新的组合维度，该论文表明确定性和随机性的在线可学习性在实现设置下不等价，并将在线多标签排名和在线多标签分类等实际学习设置作为其特定实例。

    

    本文研究了在线多类分类的一种变体，其中学习器预测单个标签，但接收到一个标签的集合作为反馈。在该模型中，如果学习器没有输出包含在反馈集合中的标签，则会受到惩罚。我们表明，与具有单标签反馈的在线多类学习不同，在实现设置中使用集合型反馈时，确定性和随机化的在线可学习性\textit{不等价}。因此，我们提供了两个新的组合维度，分别命名为集合小石和度量破裂维度，严格描述了确定性和随机化的在线可学习性。此外，我们表明度量破裂维度在悟性设置下严格描述在线可学习性。最后，我们证明了在线多标签排名和在线多标签分类等实际学习设置是我们通用在线学习框架的具体实例。

    We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
    
[^86]: 目标条件下的GFlowNets用于可控多目标分子设计

    Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular Design. (arXiv:2306.04620v1 [cs.LG])

    [http://arxiv.org/abs/2306.04620](http://arxiv.org/abs/2306.04620)

    本文研究了在多属性优化问题中，一种更好的，基于目标条件下的分子设计模型，以此解决现有标量化方案所无法解决的问题。

    

    近年来，机器学习领域对于原子分子设计的关注逐渐加剧。在设计药物应用的新化合物时，通常需要优化分子的多个属性，如与目标的结合能、可合成性、毒性、EC50等等。以往的方法采用标量化方案将多目标问题转化为以偏好为条件的单个目标，但已经确定，在具有凹形Pareto前沿的问题上，这种减少方案可能会导致解决方案倾向于滑向目标空间的极端点。本研究中，我们通过另一种目标条件分子生成公式进行实验，以获得更可控的条件模型，以便沿整个Pareto前沿均匀探索解决方案。

    In recent years, in-silico molecular design has received much attention from the machine learning community. When designing a new compound for pharmaceutical applications, there are usually multiple properties of such molecules that need to be optimised: binding energy to the target, synthesizability, toxicity, EC50, and so on. While previous approaches have employed a scalarization scheme to turn the multi-objective problem into a preference-conditioned single objective, it has been established that this kind of reduction may produce solutions that tend to slide towards the extreme points of the objective space when presented with a problem that exhibits a concave Pareto front. In this work we experiment with an alternative formulation of goal-conditioned molecular generation to obtain a more controllable conditional model that can uniformly explore solutions along the entire Pareto front.
    
[^87]: 基于贝叶斯优化的自适应加权期望改进方法

    Self-Adjusting Weighted Expected Improvement for Bayesian Optimization. (arXiv:2306.04262v1 [cs.LG])

    [http://arxiv.org/abs/2306.04262](http://arxiv.org/abs/2306.04262)

    本文提出了一种新的自适应加权期望改进方法（SAWEI），可以自动平衡探索不确定区域和利用有承诺区域之间的权衡。在COCO基准测试中，该方法表现出有利的性能。

    

    贝叶斯优化（BO）是一种基于代理模型，对小型评估预算的黑箱问题进行优化的高效算法类。BO管道本身高度可配置，涉及初始设计、代理模型和获取功能（AF）的许多不同设计选择。然而，我们对如何为手头问题选择合适的组件的理解非常有限。在本文中，我们的重点是AF的定义，其主要目的是平衡对高不确定性区域和对好解决方案有高承诺的区域之间的探索和利用的权衡。我们提出了自适应加权期望改进方法（SAWEI），其中我们让探索 - 利用权衡以数据驱动的方式进行自适应，基于BO的收敛准则。在COCO基准平台的无噪声黑箱BBOB函数上，我们的方法相对于手工制作的基线表现出有利的任何时间性能，并且是一个稳健的SP。

    Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient algorithms for optimizing black-box problems with small evaluation budgets. The BO pipeline itself is highly configurable with many different design choices regarding the initial design, surrogate model, and acquisition function (AF). Unfortunately, our understanding of how to select suitable components for a problem at hand is very limited. In this work, we focus on the definition of the AF, whose main purpose is to balance the trade-off between exploring regions with high uncertainty and those with high promise for good solutions. We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let the exploration-exploitation trade-off self-adjust in a data-driven manner, based on a convergence criterion for BO. On the noise-free black-box BBOB functions of the COCO benchmarking platform, our method exhibits a favorable any-time performance compared to handcrafted baselines and serves as a robust def
    
[^88]: 基于GNN和核均值嵌入的原子模拟传递学习

    Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])

    [http://arxiv.org/abs/2306.01589](http://arxiv.org/abs/2306.01589)

    本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。

    

    使用机器学习方法学习的原子相互作用势在原子模拟中得到了成功的应用。然而，深度学习管道需要大量数据，而生成参考计算是计算上要求很高的。为了克服这一困难，我们提出了一种传递学习算法，利用了图神经网络（GNNs）在描述化学环境方面的能力，以及核均值嵌入。我们从预先在OC20数据集上进行过训练的GNN中提取特征映射，并使用它来从催化过程的系统特定数据集中学习势能表面。我们的方法进一步通过灵活的核函数来增强，该核函数包括化学物种信息，从而提高了性能和可解释性。我们在一系列逐渐复杂的现实数据集上测试了我们的方法，展示了出色的概括能力和可转移性能，改进了依赖GNNs或岭回归方法的方法。

    Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
    
[^89]: 改进ReLU网络特征学习的神经特征激活值分析

    Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])

    [http://arxiv.org/abs/2305.15912](http://arxiv.org/abs/2305.15912)

    本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。

    

    本文研究了神经网络中单个ReLU单元的特征激活值。我们将ReLU单元在输入空间中对应的特征激活值集合称为ReLU单元的特征激活集。我们建立了特征激活集与ReLU网络中学习特征之间的明确联系，并揭示了现代深度学习架构中使用的各种神经网络规范化技术如何规范化和稳定SGD优化。利用这些洞见，我们提出了一种几何方法来参数化ReLU网络以改进特征学习。我们经验性地验证了其有用性，使用了不那么精心选择的初始化方案和更大的学习率。我们报告了更好的优化稳定性，更快的收敛速度和更好的泛化性能。

    We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
    
[^90]: 采用图神经ODE模型实现复杂动态物理系统的模拟

    Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs. (arXiv:2305.12334v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12334](http://arxiv.org/abs/2305.12334)

    本研究提出了一种基于学习的模拟模型，称为GNSTODE，通过利用统一的端到端框架描述了粒子系统中不同时间和不同空间条件下的变化。

    

    深度学习模型具有很强的学习能力，使我们能够理解真实的物理世界，因此学习模拟复杂的粒子系统是一个很有前途的努力。然而，物理世界的复杂规律给基于学习的模拟带来了巨大的挑战，如相互作用粒子之间的不同空间依赖性以及不同时间戳之间粒子系统状态的不同时间依赖性，这些因素决定了粒子的相互作用行为和物理系统的演化模式。现有的基于学习的模拟方法无法充分考虑这些复杂性，因此无法产生令人满意的模拟结果。为了更好地理解复杂的物理法则，本文提出了一种新的基于学习的模拟模型——具有时空建模的图神经ODE模型（GNSTODE），该模型利用统一的端到端框架描述了粒子系统中不同时间和不同空间条件下的变化。

    The great learning ability of deep learning models facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles' interacting behaviour and the physical systems' evolution patterns. Existing learning based simulation methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, this paper proposes a novel learning based simulation model- Graph Networks with Spatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Th
    
[^91]: 借助权重规范化的鲁棒性隐式正则化

    Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])

    [http://arxiv.org/abs/2305.05448](http://arxiv.org/abs/2305.05448)

    本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。

    

    过度参数化的模型可能有许多插值解; 隐式正则化是指特定优化方法对众多插值解之一的隐含喜好。已经建立的工作表明，（随机）梯度下降在用于训练深度线性网络时倾向于具有低秩和/或稀疏解的隐式偏差，从某种程度上解释了为什么通过梯度下降训练的过度参数化神经网络模型在实践中具有良好的泛化性能。然而，现有的平方损失目标理论通常需要可训练权重的非常小的初始化，这与实践中为了更快的收敛和更好的泛化性能而初始化的更大规模的权重矛盾。在本文中，我们旨在通过纳入并分析采用权重规范化的梯度下降来弥合这一差距，其中权重向量以极坐标参数化，导致自然的权重归一化。我们表明，在过度参数化的线性回归模型的设置中，采用权重规范化的梯度下降对欧几里德范数较低的权重向量具有隐式正则化作用。此外，我们建立了一个新颖的统一框架，将权重规范化的隐式偏差与非线性模型的经验范数正则化联系起来，从而弥合了线性模型和神经网络之间的差距。

    Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
    
[^92]: 带有分解密度的字符串图表

    String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])

    [http://arxiv.org/abs/2305.02506](http://arxiv.org/abs/2305.02506)

    本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。

    

    有关概率编程和因果模型的研究越来越多地强调了需要在扩展定向图模型的模型类之间进行组合推理的必要性。概率编程和因果模型都定义了一组随机变量上的联合概率密度，并且展示了可以用于推理因果关系和条件独立性的稀疏结构。本文基于最近有关概率映射的马尔可夫范畴的工作，定义了一个范畴，其态射将分别由每个样本空间分解的联合密度与从样本到返回值的确定性映射组合。这是迈向最近的范畴论概率测度描述和通常在概率编程和因果推断中使用的分解密度的操作定义之间的缩小差距的一步。

    A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
    
[^93]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^94]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^95]: GRIL：一种二参数持久性基于向量化的机器学习方法

    GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])

    [http://arxiv.org/abs/2304.04970](http://arxiv.org/abs/2304.04970)

    本文提出一种名为GRIL的方法，用于将拓扑特征表示散度到机器学习模型中，该方法可以稳定地用于不同的过滤函数。

    

    一参数持久性同Topology Data Analysis (TDA)相关，可研究数据中隐藏着的连通分量和循环等拓扑特征。已应用于提高图神经网络（GNNs）等深度学习模型的表示能力。为了丰富拓扑特征的表示，本研究提出了研究双过滤函数诱导的二参数持久性模块的方法。为了将这些表示信息加入到机器学习模型中，我们引入了一个新的向量表示称为Generalized Rank Invariant Landscape \textsc{Gril}，并将其证明为在Lipschitz稳定条件下可微分，并且通过对基础过滤函数的编码可以容易地融入到机器学习模型中。我们提出了一个高效计算向量表示的算法。本研究还对我们的方法进行了测试。

    $1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
    
[^96]: 长期的多模式变压器整合EHR中成像和潜在临床特征，用于肺部结节分类。

    Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])

    [http://arxiv.org/abs/2304.02836](http://arxiv.org/abs/2304.02836)

    本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。

    

    将重复成像和医疗背景（如电子健康记录）纳入预测性孤立性肺部结节（SPN）诊断模型可以极大增加准确性。然而，像成像和诊断代码这样的临床常规模式可能是异步的，并且在不同时间尺度上进行不规则采样，这是长期多模态学习的障碍。我们提出了一种基于变压器的多模态策略，将重复成像与日常收集的EHR中的长期临床特征相整合，以进行SPN分类。我们对潜在临床特征进行无监督的解缠缚，并利用时间距离缩放自注意力来联合学习临床特征表达和胸部计算机断层扫描（CT）。我们的分类器是在一个公共数据集的2,668个扫描和1,149名志愿者的长期胸部CT、账单代码、药物和实验室检查记录中进行预训练的。

    The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
    
[^97]: 固定预算赌博机标识中的复杂度存在问题

    On the Existence of a Complexity in Fixed Budget Bandit Identification. (arXiv:2303.09468v1 [stat.ML])

    [http://arxiv.org/abs/2303.09468](http://arxiv.org/abs/2303.09468)

    该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。

    

    在固定预算赌博机标识中，算法按顺序观察来自多个分布的样本，直到给定最终时间。然后，它回答关于分布集的查询。一个好的算法将有小的错误概率。虽然这个概率随着最终时间的增加呈指数级下降，但对于大多数标识任务，最佳可达率并非精确已知。我们展示了如果固定预算任务接受复杂度（定义为单个算法在所有赌博问题中实现的错误概率的下限），则该复杂度由该问题的最佳非自适应抽样过程确定。我们证明了对于几个固定预算识别任务，包括具有两个臂的伯努利最佳臂识别，不存在这样的复杂度：没有单个算法能够随处实现最佳可能速率。

    In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by a single algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate.
    
[^98]: 体育博彩的机器学习：预测模型应优化准确性还是校准性？

    Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06021](http://arxiv.org/abs/2303.06021)

    该论文研究了机器学习在体育博彩中的应用，提出了优化预测模型校准性比准确度更重要的假设，并通过实验证明了此假设的正确性。

    

    美国最近对体育博彩进行了联邦合法化，这与机器学习的黄金时代相遇。如果博彩者能够利用数据准确地预测结果的概率，他们可以认识到何时书maker的赔率对他们有利。由于体育博彩仅在美国的市场上就是一个数十亿美元的行业，因此找到这样的机会可能会非常有利可图。许多研究人员已将机器学习应用于体育赛果预测问题，通常使用准确度来评估预测模型的性能。我们提出假设，对于体育博彩问题，模型校准比准确度更重要。为了测试这一假设，我们在多个赛季的NBA数据上对模型进行训练，并在单个赛季上使用已发布的赔率进行博彩实验。通过评估各种博彩系统，我们表明优化校准的预测模型比优化准确度平均带来更高的回报率（投资回报率为$110.42％$）。

    Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
    
[^99]: 基于Fisher信息的证据深度学习方法用于不确定性估计

    Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02045](http://arxiv.org/abs/2303.02045)

    本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。

    This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.

    不确定性估计是使深度学习在实际应用中可靠的关键因素。最近提出的证据神经网络通过将网络输出视为证据来参数化狄利克雷分布，明确考虑不同的不确定性，并在不确定性估计方面取得了令人印象深刻的性能。然而，对于高数据不确定性样本但注释为one-hot标签的情况，这些错误标记的类别的证据学习过程会被过度惩罚并受到阻碍。为了解决这个问题，我们提出了一种新的方法，基于Fisher信息的证据深度学习（$\mathcal{I}$-EDL）。特别地，我们引入Fisher信息矩阵（FIM）来衡量每个样本所携带的证据的信息量，根据这个信息量，我们可以动态地重新加权目标损失项，使网络更加专注于不确定类别的表示学习。我们的网络的泛化能力通过优化进一步提高。

    Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
    
[^100]: 初阶ANIL在存在误指定的潜在维度情况下学习线性表示

    First-order ANIL learns linear representations despite misspecified latent dimension. (arXiv:2303.01335v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01335](http://arxiv.org/abs/2303.01335)

    本研究表明，在存在架构误指定的情况下，初阶ANIL可以成功学习到线性的共享表示。这个结果是基于对无限数量任务的极限情况下的推导。

    

    最近，由于在少样本分类和强化学习中的经验成功，元学习引起了极大的关注。元学习方法利用来自以前任务的数据以一种样本高效的方式学习新任务。特别是，模型无关的方法寻找起始点，从该起始点开始梯度下降可以迅速适应任何新任务。尽管经验上建议这样的方法通过在预训练期间学习共享表示表现良好，但对于这种行为的理论证据有限。更重要的是，并没有严格证明这些方法在存在架构误指定的情况下仍会学习到共享结构。在这个方向上，本文在无限数量的任务的极限情况下展示了，使用线性双层网络结构的初阶ANIL成功地学习到了线性的共享表示。即使是在参数化误指定的情况下，这个结果仍然成立，即网络的宽度大于

    Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialisation points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been rigorously shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with a misspecified network parameterisation; having a width larger than 
    
[^101]: 事实还是人工制品？在不同的ANN架构上修订逐层相关传播

    Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures. (arXiv:2302.12317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12317](http://arxiv.org/abs/2302.12317)

    本文介绍了逐层相关传播（LRP）在不同的人工神经网络（ANN）架构上的修订和应用。LRP通过可视化相关性热图揭示模型预测的影响原因，但需要注意其中可能存在的人工制品。

    

    逐层相关传播（LRP）是一种广泛使用且强大的技术，用于揭示各种人工神经网络（ANN）架构的见解。LRP经常在图像分类的背景下使用。其目的是理解输入样本的哪些部分具有最高相关性，从而对模型预测产生最大影响。可以通过网络追溯相关性，并为每个输入像素分配特定的得分。然后将相关性得分组合并显示为热图，使人类对分类模型有直观的视觉理解。打开黑盒以详细了解分类引擎对于领域专家来说至关重要，以获得对ANN模型的信任。然而，在获得的相关性图中存在模型本质上的人工制品陷阱，很容易被忽视。但是，为了进行有效的解释，不能忽视这些制品。在这里，我们在各种经过训练的ANN架构上应用和修订LRP。

    Layer-wise relevance propagation (LRP) is a widely used and powerful technique to reveal insights into various artificial neural network (ANN) architectures. LRP is often used in the context of image classification. The aim is to understand, which parts of the input sample have highest relevance and hence most influence on the model prediction. Relevance can be traced back through the network to attribute a certain score to each input pixel. Relevance scores are then combined and displayed as heat maps and give humans an intuitive visual understanding of classification models. Opening the black box to understand the classification engine in great detail is essential for domain experts to gain trust in ANN models. However, there are pitfalls in terms of model-inherent artifacts included in the obtained relevance maps, that can easily be missed. But for a valid interpretation, these artifacts must not be ignored. Here, we apply and revise LRP on various ANN architectures trained as class
    
[^102]: MalProtect：针对机器学习恶意软件检测领域中的对抗查询攻击的状态防御

    MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection. (arXiv:2302.10739v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10739](http://arxiv.org/abs/2302.10739)

    本文提出了一种专门为恶意软件检测领域设计的状态防御技术MalProtect，它通过实现一种新颖的查询分类方法来检测查询攻击，实验结果证明其有效性。

    

    众所周知，机器学习模型容易受到对抗性查询攻击的影响。在这些攻击中，查询会被不断扰动，以期达到特定的分类目的，而且没有关于目标模型的任何了解，仅凭其输出。远程托管的机器学习分类模型和面向服务的机器学习平台的普遍存在意味着查询攻击对这些系统的安全构成了真正的威胁。为了解决这个问题，已经提出了状态防御机制，通过监视和分析系统接收到的查询序列来检测查询攻击并防止生成对抗性样本。近年来已经提出了几种状态防御机制。但是，这些机制仅依赖于相似性或超出分布检测方法，这些方法可能在其他领域有效。在恶意软件检测领域，生成对抗性样本的方法本质上有所不同，因此我们发现这种检测机制的有效性明显较低。因此，在本文中，我们提出了MalProtect，这是一种专为恶意软件检测领域设计的状态防御技术。MalProtect实现了一种新颖的查询分类方法，利用关于恶意和良性查询分布的知识来检测查询攻击。我们在实际数据集上的实验表明，MalProtect有效地检测到了查询攻击，并提高了基于机器学习的恶意软件检测系统对对抗性攻击的抵抗力。

    ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper
    
[^103]: 无需较强下水平凸性的双层优化的均值乘法方法

    Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity. (arXiv:2302.03407v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.03407](http://arxiv.org/abs/2302.03407)

    本论文提出了一种无需下水平强凸性的双层优化均值乘法方法(sl-BAMM)，通过对上下层目标求平均值，实现了在大规模BLO中高效而简单的求解。与其他方法相比，本论文的分析不需要强梯度有界性假设，适用范围更广泛。实验结果表明了该方法的优越性。

    

    梯度方法已经成为学习领域双层优化(BLO)的主流技术。现有工作的有效性严重依赖于要么有限的下水平凸性(LLSC)条件，要么解决一系列高精度的近似子问题，或者两者兼有。本研究通过对上下层目标求平均值，提出了一种简单而高效的大规模BLO的单循环均值乘法双层(sl-BAMM)方法，并摆脱了有限的LLSC约束。我们进一步对sl-BAMM的非渐近收敛性分析KKT平稳点进行了分析，并且我们的分析的比较优势在于不需要强梯度有界性假设，这是其他方法总是需要的。因此，我们的理论安全地捕捉到了更广泛的深度学习应用，特别是在上层目标相对于下层变量是二次的情况下。实验结果表明了我们方法的优越性。

    Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning fields. The validity of existing works heavily rely on either a restrictive Lower-Level Strong Convexity (LLSC) condition or on solving a series of approximation subproblems with high accuracy or both. In this work, by averaging the upper and lower level objectives, we propose a single loop Bi-level Averaged Method of Multipliers (sl-BAMM) for BLO that is simple yet efficient for large-scale BLO and gets rid of the limited LLSC restriction. We further provide non-asymptotic convergence analysis of sl-BAMM towards KKT stationary points, and the comparative advantage of our analysis lies in the absence of strong gradient boundedness assumption, which is always required by others. Thus our theory safely captures a wider variety of applications in deep learning, especially where the upper-level objective is quadratic w.r.t. the lower-level variable. Experimental results demonstrate the superiorit
    
[^104]: UPop：用于压缩视觉语言Transformer模型的统一和渐进式剪枝方法

    UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13741](http://arxiv.org/abs/2301.13741)

    UPop是一种通用的视觉语言Transformer压缩框架，采用统一和渐进式剪枝方法，可自动分配剪枝比率，实现更高的压缩比率。

    

    真实世界的数据包含大量的多模态信息，其中视觉和语言是最具代表性的两种模态。此外，越来越重的模型，例如Transformer，已经引起了研究人员对模型压缩的关注。然而，如何压缩多模态模型，特别是视觉语言Transformer，仍然未被充分探索。本文提出了一种名为UPop的通用视觉语言Transformer压缩框架，它包括1）在原始模型中在连续优化空间中统一搜索多模态子网，从而实现可压缩模态和结构之间自动分配剪枝比率；2）渐进式搜索和微调子网，从而保持搜索和微调之间的收敛，以实现更高的压缩比率。

    Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit
    
[^105]: 通过合成控制理论实现有界（O(1)）遗憾的推荐学习

    Bounded (O(1)) Regret Recommendation Learning via Synthetic Controls Oracle. (arXiv:2301.12571v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12571](http://arxiv.org/abs/2301.12571)

    通过合成控制理论，本论文提出了一种实现有界遗憾的推荐学习方法，并解决了线性模型的精确知识、潜在协变量的存在、不均匀的用户到达速率和用户选择退出私人数据跟踪等实践中的问题。

    

    在在线探索系统中，当具有固定偏好的用户重复到达时，最近已经证明可以将系统建模为线性情境广告带来O(1)的有界遗憾。这个结果可能对推荐系统具有兴趣，因为它们的物品的流行度通常是短暂的，即探索本身可能在潜在的长期非稳态开始之前很快完成。然而，在实践中，线性模型的精确知识往往难以证明。此外，潜在协变量的存在，不均匀的用户到达速率，对必要等级条件的解释以及用户选择退出私人数据跟踪等都需要在实际的推荐系统应用中解决。在这项工作中，我们进行了理论研究，以解决所有这些问题，同时仍然实现了有界遗憾。除了证明技术，我们在这里所做的关键区别性假设是有效合成控制理论的存在。

    In online exploration systems where users with fixed preferences repeatedly arrive, it has recently been shown that O(1), i.e., bounded regret, can be achieved when the system is modeled as a linear contextual bandit. This result may be of interest for recommender systems, where the popularity of their items is often short-lived, as the exploration itself may be completed quickly before potential long-run non-stationarities come into play. However, in practice, exact knowledge of the linear model is difficult to justify. Furthermore, potential existence of unobservable covariates, uneven user arrival rates, interpretation of the necessary rank condition, and users opting out of private data tracking all need to be addressed for practical recommender system applications. In this work, we conduct a theoretical study to address all these issues while still achieving bounded regret. Aside from proof techniques, the key differentiating assumption we make here is the presence of effective Sy
    
[^106]: WDC产品：一个多维实体匹配基准

    WDC Products: A Multi-Dimensional Entity Matching Benchmark. (arXiv:2301.09521v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09521](http://arxiv.org/abs/2301.09521)

    本研究提出了WDC产品作为一个多维实体匹配基准，通过对角落案例数量、未见实体的泛化能力以及开发集大小等三个维度进行评估，来评估实体匹配系统的鲁棒性。

    

    实体匹配任务的难度取决于多个因素的组合，例如角落案例对的数量、训练集中未见过的实体的比例以及开发集的大小。目前的实体匹配基准通常代表这些维度上的单个点，或者提供了沿着单个维度评估匹配方法的可能性，例如训练数据量。本文介绍了WDC产品，这是一个实体匹配基准，它提供了对匹配系统在三个维度组合上的系统评估，同时依赖于真实世界的数据。这三个维度是(i) 角落案例的数量 (ii) 对未见实体的泛化能力，以及 (iii) 开发集的大小（训练集加验证集）。对未见实体的泛化能力是目前任何已有的英语基准所不包含的维度，但对于评估实体匹配的鲁棒性至关重要。

    The difficulty of an entity matching task depends on a combination of multiple factors such as the amount of corner-case pairs, the fraction of entities in the test set that have not been seen during training, and the size of the development set. Current entity matching benchmarks usually represent single points in the space along such dimensions or they provide for the evaluation of matching methods along a single dimension, for instance the amount of training data. This paper presents WDC Products, an entity matching benchmark which provides for the systematic evaluation of matching systems along combinations of three dimensions while relying on real-world data. The three dimensions are (i) amount of corner-cases (ii) generalization to unseen entities, and (iii) development set size (training set plus validation set). Generalization to unseen entities is a dimension not covered by any of the existing English-language benchmarks yet but is crucial for evaluating the robustness of enti
    
[^107]: 具有样本外保证的离线策略评估

    Off-Policy Evaluation with Out-of-Sample Guarantees. (arXiv:2301.08649v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.08649](http://arxiv.org/abs/2301.08649)

    本文提出了一种具有样本外保证的离线策略评估方法，可以在考虑模型配置错误的情况下，使用观察数据对决策策略的性能进行有效推断。

    

    我们考虑使用过去的观察数据评估决策策略的性能问题。策略的结果以损失（即失效或负奖励）来衡量，主要问题是在以不同且可能未知的策略下观察到过去数据时对其样本外损失进行有效推断。使用样本分割方法，我们展示了可以绘制这样的推断，并对整个损失分布而不仅仅是其均值进行有限样本覆盖保证。重要的是，该方法考虑了对过去策略的模型配置错误，包括未测量的混淆因素。该评估方法可用于在一定可信模型假设范围内使用观察数据对策略的性能进行认证。

    We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finite-sample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy - including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.
    
[^108]: 案例基础神经网络：具有时间变化的高阶交互的生存分析

    Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06535](http://arxiv.org/abs/2301.06535)

    案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。

    

    神经网络基于生存分析方法可以模拟数据驱动的协变量交互。虽然这些方法可以比回归方法提供更好的预测性能，但并不是所有的方法都可以模拟时间变化的交互和复杂的基线风险。为了解决这个问题，我们提出了一种称为案例基础神经网络（CBNNs）的新方法，它将案例基础抽样框架与灵活的神经网络结构相结合。通过使用一种新颖的抽样方案和数据增强来自然地考虑到截尾，我们构建了一个可以接受时间输入的前馈神经网络。CBNNs通过预测在给定时刻事件发生的概率来估计危险函数。我们通过模拟和三个案例研究使用两个时间依赖指标比较CBNNs与回归和神经网络基于生存分析方法的性能。首先，我们通过涉及复杂基线风险和时间变化交互的模拟来评估所有方法，其中包括CBNNs。

    Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
    
[^109]: 表现力架构增强基于动力学的神经人群模型的可解释性

    Expressive architectures enhance interpretability of dynamics-based neural population models. (arXiv:2212.03771v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2212.03771](http://arxiv.org/abs/2212.03771)

    研究通过顺序自动编码器从神经数据集中恢复潜在的混沌吸引子，发现采用神经常微分方程为基础的 SAES 在准确率和维度方面优于采用循环神经网络的 SAES。

    

    能够从记录的神经活动中恢复潜在动力学的人工神经网络可能为识别和解释生物计算中潜在的动力学模式提供了一种强大的方法。鉴于仅凭神经方差无法唯一确定潜在的动力学系统，具有解释性的架构应该优先考虑准确和低维的潜在动力学。在这项工作中，我们评估了顺序自动编码器（SAEs）在从模拟的神经数据集中恢复潜在的混沌吸引子方面的表现。我们发现，采用广泛使用的循环神经网络（RNN）为动力学基础的SAEs无法在真实的潜在状态维度上推断出准确的发射率，并且更大的RNNs依赖于数据中不存在的动态特征。另一方面，采用神经常微分方程（NODE）为基础的SAEs在真实的潜在状态维度上推断出准确的率，同时还恢复了潜在轨迹和固定点结构。

    Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate firing rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structu
    
[^110]: 提高准确性: 基于时代驱动的混合尾数块浮点方法用于DNN训练

    Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training. (arXiv:2211.10737v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10737](http://arxiv.org/abs/2211.10737)

    本文提出了一种基于时代驱动的混合尾数HBFP技术，通过对不同参数的探索和优化，实现了对DNN训练中算术操作的更小编码。使用分析模型表明，该方法能够将HBFP训练加速器的算术密度增加高达$21.3\times$。

    

    DNN模型复杂性、规模和训练数据量的前所未有增长导致了对计算的巨大需求和对最小编码的搜索。最近的研究主张使用混合块浮点(HBFP)来最小化加速器中的硅配备，通过将大部分训练中的算术操作转换为8位定点。本文通过数学工具对HBFP设计空间进行了全面的探索，研究了各种参数之间的相互作用，并确定了在各层和各个时代中更小编码的机会。基于我们的发现，我们提出了Accuracy Boosters，一种基于时代驱动的混合尾数HBFP技术，只在最后一个时代和第一个/最后一层中使用6位尾数，在训练中的其他算术操作中使用4位尾数达到$99.7\%$。使用分析模型，我们展示了Accuracy Boosters可以使HBFP训练加速器的算术密度增加高达$21.3\times$。

    The unprecedented growth in DNN model complexity, size, and amount of training data has led to a commensurate increase in demand for computing and a search for minimal encoding. Recent research advocates Hybrid Block Floating Point (HBFP) to minimize silicon provisioning in accelerators by converting the majority of arithmetic operations in training to 8-bit fixed point. In this paper, we perform a full-scale exploration of the HBFP design space using mathematical tools to study the interplay among various parameters and identify opportunities for even smaller encodings across layers and epochs. Based on our findings, we propose Accuracy Boosters, an epoch-driven mixed-mantissa HBFP technique that uses 6-bit mantissas only in the last epoch and first/last layers, and 4-bit mantissas for $99.7\%$ of all other arithmetic operations in training. Using analytic models, we show Accuracy Boosters enable increasing arithmetic density for an HBFP training accelerator by up to $21.3\times$ comp
    
[^111]: GEC: 一种在MDP、POMDP和更多情况下交互式决策的统一框架

    GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond. (arXiv:2211.01962v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01962](http://arxiv.org/abs/2211.01962)

    本研究在交互式决策的框架下，提出了一种新的复杂度度量GEC，用于样本高效强化学习。该方法能够捕捉到探索和开发之间的权衡，将RL问题划分为低GEC和高GEC两个类别，并展示了低GEC类别的丰富性质。

    

    我们研究了在交互式决策的普遍框架下的样本高效强化学习（RL），该框架包括马尔可夫决策过程（MDP）、部分可观测马尔可夫决策过程（POMDP）和预测状态表示（PSR）作为特殊情况。为了找到赋予样本高效学习的最小假设，我们提出了一种新的复杂度度量，广义eluder系数（GEC），它表征了在线交互式决策中探索和开发之间的基本权衡。具体而言，GEC通过比较预测更新策略性能的误差与基于历史数据评估的样本内训练误差，来衡量探索的难度。我们展示了低GEC的RL问题形成了一个非常丰富的类别，其中包括低Bellman eluder维度问题、双线性类、低证人秩问题、PO-双线性类和广义正则PSR等。

    We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tr
    
[^112]: ERL-Re$^2$: 具有共享状态表示和个体策略表示的高效进化强化学习

    ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation. (arXiv:2210.17375v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2210.17375](http://arxiv.org/abs/2210.17375)

    ERL-Re$^2$提出了双尺度状态表示和策略表示的进化强化学习方法，解决了现有工作中忽视共享知识和语义级行为进化的问题。

    

    深度强化学习（Deep RL）和进化算法（EA）是两种具有不同学习原理的策略优化的主要范式，即基于梯度与基于非梯度。一种吸引人的研究方向是通过融合它们的互补优势来整合Deep RL和EA以设计新的方法。然而，现有的关于Deep RL和EA结合的工作存在两个常见的缺点：1）RL代理和EA代理分别学习他们的策略，忽视了有用的共享知识的高效共享；2）参数级别的策略优化不能保证EA侧的行为进化的语义级别。在本文中，我们提出了具有双尺度状态表示和策略表示的进化强化学习（ERL-Re$^2$），这是对前述两个缺点的一种新颖解决方案。ERL-Re$^2$的关键思想是双尺度表示：所有EA和RL策略共享相同的非线性状态表示，同时保持个体线性策略表示。

    Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representat
    
[^113]: 基于结构的药物设计与等变扩散模型

    Structure-based Drug Design with Equivariant Diffusion Models. (arXiv:2210.13695v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2210.13695](http://arxiv.org/abs/2210.13695)

    该论文提出了一个基于结构的药物设计方法，使用了等变扩散模型DiffSBDD来生成具有亲和力和特异性的新型药物配体。实验结果表明DiffSBDD在生成具有竞争性对接得分的多样化药物样配体方面具有高效和有效的性能。

    

    基于结构的药物设计（SBDD）旨在设计与预定的蛋白靶点具有高亲和力和特异性的小分子配体。本文将SBDD表述为一个三维条件生成问题，并提出了DiffSBDD，这是一个SE(3)-等变的三维条件扩散模型，可以在蛋白口袋的条件下生成新型配体。全面的基于计算机模拟的实验证明了DiffSBDD在生成具有具有竞争性对接得分的新颖和多样的药物样配体方面的效率和有效性。我们进一步探索了扩散框架在药物设计活动中更广泛任务的灵活性，例如即插即用的性质优化和从局部分子设计带有修补的任务。

    Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. In this paper, we formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an SE(3)-equivariant 3D-conditional diffusion model that generates novel ligands conditioned on protein pockets. Comprehensive in silico experiments demonstrate the efficiency and effectiveness of DiffSBDD in generating novel and diverse drug-like ligands with competitive docking scores. We further explore the flexibility of the diffusion framework for a broader range of tasks in drug design campaigns, such as off-the-shelf property optimization and partial molecular design with inpainting.
    
[^114]: 从专家进行私密在线预测: 分离和更快的速率

    Private Online Prediction from Experts: Separations and Faster Rates. (arXiv:2210.13537v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13537](http://arxiv.org/abs/2210.13537)

    这篇论文提出了新的算法，用于在在线预测从专家中解决隐私约束的问题，并改进了现有算法的遗憾界限。研究结果表明，在纯差分隐私和近似差分隐私设置下，对于愚蠢敌手，在高维范围内的遗憾可以达到亚线性水平，与自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。

    

    在线预测从专家中是机器学习中一个基本的问题，并且已经有几篇论文研究了在隐私约束下的这个问题。我们提出并分析了针对这个问题的新算法，改进了非自适应对手的遗憾界限。对于近似差分隐私，我们的算法在随机设置下实现了遗憾界限为$\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$，对于愚蠢敌手实现了遗憾界限为$\tilde{O}(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$（其中$d$是专家数量）。对于纯DP，我们的算法是第一个在高维范围$d \ge T$ 的愚蠢敌手中获得亚线性遗憾的算法。此外，我们证明了自适应对手的新下界。我们的结果表明，与非私密设置不同，对于这个问题，自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。我们的下界也展示了一种在自适应对手和非自适应对手之间的分离。

    Online prediction from experts is a fundamental problem in machine learning and several works have studied this problem under privacy constraints. We propose and analyze new algorithms for this problem that improve over the regret bounds of the best existing algorithms for non-adaptive adversaries. For approximate differential privacy, our algorithms achieve regret bounds of $\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$ for the stochastic setting and $\tilde{O}(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$ for oblivious adversaries (where $d$ is the number of experts). For pure DP, our algorithms are the first to obtain sub-linear regret for oblivious adversaries in the high-dimensional regime $d \ge T$. Moreover, we prove new lower bounds for adaptive adversaries. Our results imply that unlike the non-private setting, there is a strong separation between the optimal regret for adaptive and non-adaptive adversaries for this problem. Our lower bounds also show a separation between 
    
[^115]: 基于合作多智能体深度强化学习的可靠高效移动接入的多UAV控制

    Cooperative Multi-Agent Deep Reinforcement Learning for Reliable and Energy-Efficient Mobile Access via Multi-UAV Control. (arXiv:2210.00945v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00945](http://arxiv.org/abs/2210.00945)

    本文提出了一种基于合作多智能体深度强化学习的算法，用于多个无人机的定位和通信的协作。该算法旨在实现可靠和高效的移动接入网络，以支持智能交通系统的发展。

    

    本文提出了一种新颖的基于多智能体深度强化学习的定位算法，用于多个无人机（UAV）的协作，即UAV作为移动基站。该算法的主要目标是建立可靠的移动接入网络，用于车联网（C-V2X）通信，从而促进高质量智能交通系统（ITS）的实现。可靠的移动接入服务可以通过两种方式实现，即i）高效能源消耗的UAV运行和ii）可靠的无线通信服务。对于高效能源消耗的UAV运行，我们提出的MADRL算法的奖励包含UAV能源消耗模型的特征，以实现高效运行。此外，对于可靠的无线通信服务，用户的服务质量（QoS）要求被视为奖励的一部分，并使用60GHz毫米波无线电进行移动连接。

    This paper addresses a novel multi-agent deep reinforcement learning (MADRL)-based positioning algorithm for multiple unmanned aerial vehicles (UAVs) collaboration (i.e., UAVs work as mobile base stations). The primary objective of the proposed algorithm is to establish dependable mobile access networks for cellular vehicle-to-everything (C-V2X) communication, thereby facilitating the realization of high-quality intelligent transportation systems (ITS). The reliable mobile access services can be achieved in following two ways, i.e., i) energy-efficient UAV operation and ii) reliable wireless communication services. For energy-efficient UAV operation, the reward of our proposed MADRL algorithm contains the features for UAV energy consumption models in order to realize efficient operations. Furthermore, for reliable wireless communication services, the quality of service (QoS) requirements of individual users are considered as a part of rewards and 60GHz mmWave radio is used for mobile a
    
[^116]: MAGIC: 通过反转准鲁棒分类器实现基于掩码的图像合成

    MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.11549](http://arxiv.org/abs/2209.11549)

    本论文提出了一种名为MAGIC的方法，通过反转准鲁棒分类器进行一次性掩码引导的图像合成。它通过聚合梯度并利用强空间先验的指导二进制掩码，实现了形状和位置控制、非刚性形状变形以及复制/移动操作，并可简单指定二进制引导掩码来提供强大的合成控制。

    

    我们提供了一种一次性掩码引导图像合成的方法，通过反转带有强正则化器的准鲁棒分类器来控制对单个图像的操作。我们提出的方法名为MAGIC，利用来自预训练的准鲁棒分类器的结构化梯度，可以更好地保留输入的语义，并保持其分类准确性，从而保证合成的可信度。与目前使用复杂原语来监督过程或使用注意力图作为弱监督信号的方法不同，MAGIC通过在输入上聚合梯度，由强空间先验的指导二进制掩码推动。MAGIC以单个框架实现了一系列操作，实现了形状和位置控制、强烈的非刚性形状变形以及在重复物体存在的情况下的复制/移动操作，并通过简单指定二进制引导掩码来给用户提供强大的合成控制。

    We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findin
    
[^117]: 从部分观测轨迹中学习作用Koopman生成器的双线性模型

    Learning Bilinear Models of Actuated Koopman Generators from Partially-Observed Trajectories. (arXiv:2209.09977v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2209.09977](http://arxiv.org/abs/2209.09977)

    该论文提出了一种从部分观测轨迹中学习作用Koopman生成器的方法，克服了现有方法依赖于基函数选择和观测不完整的限制。

    

    基于逼近潜在的Koopman算子或生成器的非线性动力系统的数据驱动模型已经被证明是成功的预测、特征学习、状态估计和控制工具。众所周知，控制仿射系统的Koopman生成器在输入方面也具有仿射依赖性，进而导致方便的有限维双线性近似动力学。然而，目前对于近似具有作用的Koopman生成器的方法仍然存在两个主要障碍。首先，现有方法的性能严重依赖于选择用于逼近Koopman生成器的基函数，而对于非测度保持的系统目前没有普适的选择方式。其次，如果我们没有观测到完整的状态，可能无法获得足够丰富的这类函数集合来描述动态。这是因为通常情况下我们无法获得描述动态所需的足够丰富的函数集合。

    Data-driven models for nonlinear dynamical systems based on approximating the underlying Koopman operator or generator have proven to be successful tools for forecasting, feature learning, state estimation, and control. It has become well known that the Koopman generators for control-affine systems also have affine dependence on the input, leading to convenient finite-dimensional bilinear approximations of the dynamics. Yet there are still two main obstacles that limit the scope of current approaches for approximating the Koopman generators of systems with actuation. First, the performance of existing methods depends heavily on the choice of basis functions over which the Koopman generator is to be approximated; and there is currently no universal way to choose them for systems that are not measure preserving. Secondly, if we do not observe the full state, we may not gain access to a sufficiently rich collection of such functions to describe the dynamics. This is because the commonly u
    
[^118]: 基于神经网络的混合整数规划的终身学习

    Lifelong Learning for Neural powered Mixed Integer Programming. (arXiv:2208.12226v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.12226](http://arxiv.org/abs/2208.12226)

    本论文研究了混合整数规划的终身学习范例，并提出了一种名为LIMIP的方法，在嵌入空间中建模MIP实例，通过应用知识蒸馏来避免灾难性遗忘。

    

    混合整数规划（MIP）通常通过分支定界算法来求解。最近，学习模仿专家强分支启发式快速逼近的方法引起了关注，因为它在减少解决MIP问题的运行时间方面取得了成功。然而，现有的学习分支方法假设整个训练数据在单个训练会话中可用。这个假设通常不成立，如果训练数据随时间连续提供，现有技术会遭受灾难性的遗忘。在这项工作中，我们研究了迄今为止未被探索的混合整数规划的终身学习范例。为了减轻灾难性遗忘，我们提出了LIMIP，它利用了将MIP实例建模为一个二分图的思想，我们使用二分图注意力网络将其映射到嵌入空间中。通过应用知识蒸馏，在这个丰富的嵌入空间中避免了灾难性遗忘。

    Mixed Integer programs (MIPs) are typically solved by the Branch-and-Bound algorithm. Recently, Learning to imitate fast approximations of the expert strong branching heuristic has gained attention due to its success in reducing the running time for solving MIPs. However, existing learning-to-branch methods assume that the entire training data is available in a single session of training. This assumption is often not true, and if the training data is supplied in continual fashion over time, existing techniques suffer from catastrophic forgetting. In this work, we study the hitherto unexplored paradigm of Lifelong Learning to Branch on Mixed Integer Programs. To mitigate catastrophic forgetting, we propose LIMIP, which is powered by the idea of modeling an MIP instance in the form of a bipartite graph, which we map to an embedding space using a bipartite Graph Attention Network. This rich embedding space avoids catastrophic forgetting through the application of knowledge distillation an
    
[^119]: 一种具有截断柯西随机扰动的渐进平滑函数算法用于随机优化

    A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization. (arXiv:2208.00290v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.00290](http://arxiv.org/abs/2208.00290)

    本文提出了一种具有截断柯西随机扰动的随机梯度算法用于非凸目标函数的优化，算法具有稳定性与快速收敛性。

    

    本文提出了一种随机梯度算法，用于最小化一个光滑的目标函数，该函数是噪声成本样本的期望，而只有后者对任何给定的参数进行观测。我们的算法采用带有随机扰动的梯度估计方案，这些扰动使用从delta球中得到的截断柯西分布形成。我们分析了所提出的梯度估计器的偏差和方差。我们发现，当目标函数是非凸的，而参数维数很高时，我们的算法非常有用。从渐近收敛分析中，我们建立了我们的算法几乎确定地收敛于目标函数的稳定点集合，并获得了收敛的渐近速率。我们还表明，我们的算法避免了不稳定的平衡点，意味着收敛到局部最小值。此外，我们对我们的算法进行了非渐近收敛性分析。特别地，我们在这里建立了一个非渐近保证收敛率的收敛性结果，该结果是实数值的和给出了精确的界限。

    In this paper, we present a stochastic gradient algorithm for minimizing a smooth objective function that is an expectation over noisy cost samples, and only the latter are observed for any given parameter. Our algorithm employs a gradient estimation scheme with random perturbations, which are formed using the truncated Cauchy distribution from the delta sphere. We analyze the bias and variance of the proposed gradient estimator. Our algorithm is found to be particularly useful in the case when the objective function is non-convex, and the parameter dimension is high. From an asymptotic convergence analysis, we establish that our algorithm converges almost surely to the set of stationary points of the objective function and obtains the asymptotic convergence rate. We also show that our algorithm avoids unstable equilibria, implying convergence to local minima. Further, we perform a non-asymptotic convergence analysis of our algorithm. In particular, we establish here a non-asymptotic b
    
[^120]: 基于多分辨率哈希编码的神经表示的交互式体积可视化

    Interactive Volume Visualization via Multi-Resolution Hash Encoding based Neural Representation. (arXiv:2207.11620v3 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2207.11620](http://arxiv.org/abs/2207.11620)

    本文通过同时利用现代GPU张量核心、本地CUDA神经网络框架以及具有宏单元加速的渲染算法，实现了交互式的体积神经表示光线追踪。这种神经表示具有高保真度和紧凑性，同时还开发了高效的离核训练策略以支持极大规模的体积数据。

    

    神经网络已经展示了在压缩体积数据进行可视化方面的巨大潜力。然而，由于训练和推理的成本高昂，这种体积神经表示迄今只被应用于离线数据处理和非交互式渲染。在本文中，我们展示了通过同时利用现代GPU张量核心、本地CUDA神经网络框架以及具有宏单元加速的精心设计的渲染算法，我们可以交互地Ray Tracing体积神经表示（10-60帧/秒）。我们的神经表示也具有高保真度（PSNR > 30dB）和紧凑性（大小减小了10-1000倍）。此外，我们还展示了在渲染循环内完全跳过预训练过程的可能性，将整个训练步骤适应于渲染循环中。为了支持极大规模的体积数据，我们还开发了一种高效的离核训练策略，使我们的体积神经表示训练能够潜在地扩展到使用仅一个N进行TeraScale的规模。

    Neural networks have shown great potential in compressing volume data for visualization. However, due to the high cost of training and inference, such volumetric neural representations have thus far only been applied to offline data processing and non-interactive rendering. In this paper, we demonstrate that by simultaneously leveraging modern GPU tensor cores, a native CUDA neural network framework, and a well-designed rendering algorithm with macro-cell acceleration, we can interactively ray trace volumetric neural representations (10-60fps). Our neural representations are also high-fidelity (PSNR > 30dB) and compact (10-1000x smaller). Additionally, we show that it is possible to fit the entire training step inside a rendering loop and skip the pre-training process completely. To support extreme-scale volume data, we also develop an efficient out-of-core training strategy, which allows our volumetric neural representation training to potentially scale up to terascale using only an N
    
[^121]: 针对下一次购买预测的顺序推荐模型

    Sequential Recommendation Model for Next Purchase Prediction. (arXiv:2207.06225v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.06225](http://arxiv.org/abs/2207.06225)

    本文提出了一种顺序推荐系统，考虑了用户的购买顺序以预测他们的下一次购买，该模型利用大规模的信用卡交易数据集进行了验证和排名，展现了其在准确性和效果上的优势。

    

    在提供当代数字营销体验时，推荐的时效性和上下文准确性变得越来越重要。传统的推荐系统通过考虑用户的过去购买记录向用户推荐相关但不受时间影响的物品。这些推荐只是符合用户的一般偏好，而不是用户在购买之前的具体需求。相反，考虑交易、购买或体验顺序来衡量用户演化偏好的推荐系统能够为用户提供更准确和有效的推荐：顺序推荐系统不仅能更好地理解用户当前需求的行为，还具有更好的预测能力。在本文中，我们利用一份包含超过2.7百万信用卡交易数据和46K个持卡人的生产数据集，展示并排名了顺序推荐系统的效果。该方法首先使用自编码器对原始的交易数据进行处理，然后提交观测数据进行预测。

    Timeliness and contextual accuracy of recommendations are increasingly important when delivering contemporary digital marketing experiences. Conventional recommender systems (RS) suggest relevant but time-invariant items to users by accounting for their past purchases. These recommendations only map to customers' general preferences rather than a customer's specific needs immediately preceding a purchase. In contrast, RSs that consider the order of transactions, purchases, or experiences to measure evolving preferences can offer more salient and effective recommendations to customers: Sequential RSs not only benefit from a better behavioral understanding of a user's current needs but also better predictive power. In this paper, we demonstrate and rank the effectiveness of a sequential recommendation system by utilizing a production dataset of over 2.7 million credit card transactions for 46K cardholders. The method first employs an autoencoder on raw transaction data and submits observ
    
[^122]: 基于用户级隐私的直方图估计的贡献边界算法

    Algorithms for bounding contribution for histogram estimation under user-level privacy. (arXiv:2206.03008v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03008](http://arxiv.org/abs/2206.03008)

    该论文提出了一种在用户级隐私条件下对直方图估计的贡献边界算法。该算法可以在有界和无界域设置下选择最佳用户贡献边界，并近似达到最佳贡献边界的两倍近似。

    

    我们研究了在用户级差异隐私条件下的直方图估计问题，在这种情况下，目标是保护任何单个用户的所有条目的隐私。我们考虑了数据数量对每个用户可能不同的异构场景。在这种情况下，为了获得差异隐私，在直方图中注入的噪声量与最大用户贡献成比例，而这可能会被少数离群值放大。一种应对方法是限制每个用户对直方图的贡献（或限制）。然而，如果将用户限制在小的贡献上，将会丢弃大量数据。在这项工作中，我们提出了在有界和无界域设置下选择最佳用户贡献边界的算法。当域的大小有界时，我们提出了一种用户贡献边界策略，几乎可以在事后与最佳贡献边界达到两倍近似。

    We study the problem of histogram estimation under user-level differential privacy, where the goal is to preserve the privacy of all entries of any single user. We consider the heterogeneous scenario where the quantity of data can be different for each user. In this scenario, the amount of noise injected into the histogram to obtain differential privacy is proportional to the maximum user contribution, which can be amplified by few outliers. One approach to circumvent this would be to bound (or limit) the contribution of each user to the histogram. However, if users are limited to small contributions, a significant amount of data will be discarded. In this work, we propose algorithms to choose the best user contribution bound for histogram estimation under both bounded and unbounded domain settings. When the size of the domain is bounded, we propose a user contribution bounding strategy that almost achieves a two-approximation with respect to the best contribution bound in hindsight. F
    
[^123]: 使用符合预测改进专家预测

    Improving Expert Predictions with Conformal Prediction. (arXiv:2201.12006v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12006](http://arxiv.org/abs/2201.12006)

    本研究开发了一种自动决策支持系统，通过使用符合预测构建的标签预测集合，精确地权衡了真实标签不在预测集合中的概率。

    

    自动决策支持系统承诺帮助专家更高效准确地解决多类分类任务。然而，现有系统通常要求专家理解何时放弃自己的决策权以及何时行使自己的决策权。否则，专家可能更适合自行解决分类任务。在此工作中，我们开发了一种自动决策支持系统，它不需要专家理解何时信任系统以提高性能。我们的系统不提供单一的标签预测并让专家决定何时信任这些预测，而是提供使用符合预测构建的标签预测集合，并强制要求专家从这些集合中预测标签。通过使用符合预测，我们的系统可以精确地权衡真实标签不在预测集合中的概率，从而确定输出预测的频率。

    Automated decision support systems promise to help human experts solve multiclass classification tasks more efficiently and accurately. However, existing systems typically require experts to understand when to cede agency to the system or when to exercise their own agency. Otherwise, the experts may be better off solving the classification tasks on their own. In this work, we develop an automated decision support system that, by design, does not require experts to understand when to trust the system to improve performance. Rather than providing (single) label predictions and letting experts decide when to trust these predictions, our system provides sets of label predictions constructed using conformal prediction$\unicode{x2014}$prediction sets$\unicode{x2014}$and forcefully asks experts to predict labels from these sets. By using conformal prediction, our system can precisely trade-off the probability that the true label is not in the prediction set, which determines how frequently ou
    
[^124]: 充分统计记忆型AMP算法

    Sufficient-Statistic Memory AMP. (arXiv:2112.15327v4 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2112.15327](http://arxiv.org/abs/2112.15327)

    该论文提出了一种在特定条件下解决AMP类型算法收敛性问题的充分统计记忆型AMP算法框架，通过充分统计约束和特定条件下的协方差矩阵性质，实现了有效的信号重构。

    

    近似消息传递（AMP）类型的算法在某些大型随机线性系统的信号重构中被广泛使用。AMP类型算法的一个关键特点是它们的动态可以通过状态演化正确地描述。虽然状态演化是一个有用的分析工具，但其收敛性并不保证。为了在原则上解决AMP类型算法的状态演化的收敛问题，本文提出了一种在正确单位不变的感知矩阵、Lipschitz连续的本地处理器和充分统计约束下的充分统计记忆型AMP（SS-MAMP）算法框架。我们证明了SS-MAMP的协方差矩阵是L带状的且收敛，这是一个最优的框架（从本地MMSE/LMMSE角度）给定Lipschitz连续的AMP类型算法。

    Approximate message passing (AMP) type algorithms have been widely used in the signal reconstruction of certain large random linear systems. A key feature of the AMP-type algorithms is that their dynamics can be correctly described by state evolution. While state evolution is a useful analytic tool, its convergence is not guaranteed. To solve the convergence problem of the state evolution of AMP-type algorithms in principle, this paper proposes a sufficient-statistic memory AMP (SS-MAMP) algorithm framework under the conditions of right-unitarily invariant sensing matrices, Lipschitz-continuous local processors and the sufficient-statistic constraint (i.e., the current message of each local processor is a sufficient statistic of the signal vector given the current and all preceding messages). We show that the covariance matrices of SS-MAMP are L-banded and convergent, which is an optimal framework (from the local MMSE/LMMSE perspective) for AMP-type algorithms given the Lipschitz-conti
    
[^125]: 低温蒸馏：用于稳健对抗训练的方法

    LTD: Low Temperature Distillation for Robust Adversarial Training. (arXiv:2111.02331v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.02331](http://arxiv.org/abs/2111.02331)

    本文提出了一种名为低温蒸馏（LTD）的新方法，通过使用修改的知识蒸馏框架生成软标签，解决了对抗训练中常用的独热向量标签带来的学习困难问题，提高了模型的稳健性。

    

    对抗训练已经被广泛应用于增强神经网络模型对抗攻击的稳健性。尽管神经网络模型很受欢迎，但是这些模型的自然准确性和稳健准确性之间存在着显著差距。本文的主要贡献是发现了这个差距的一个主要原因是常用的独热向量作为标签，这阻碍了图像识别的学习过程。用独热向量表示模糊图像是不准确的，可能导致模型得到次优解。为了解决这个问题，我们提出了一种新颖的方法，称之为低温蒸馏（LTD），它使用修改的知识蒸馏框架生成软标签。与以前的方法不同，LTD在教师模型中使用相对较低的温度，而对教师和学生模型使用固定但不同的温度。这个修改可以提高模型的稳健性，而不会遇到已经在先前工作中解决的梯度掩码问题。

    Adversarial training has been widely used to enhance the robustness of neural network models against adversarial attacks. Despite the popularity of neural network models, a significant gap exists between the natural and robust accuracy of these models. In this paper, we identify one of the primary reasons for this gap is the common use of one-hot vectors as labels, which hinders the learning process for image recognition. Representing ambiguous images with one-hot vectors is imprecise and may lead the model to suboptimal solutions. To overcome this issue, we propose a novel method called Low Temperature Distillation (LTD) that generates soft labels using the modified knowledge distillation framework. Unlike previous approaches, LTD uses a relatively low temperature in the teacher model and fixed, but different temperatures for the teacher and student models. This modification boosts the model's robustness without encountering the gradient masking problem that has been addressed in defe
    
[^126]: 在不产生灾难性遗忘的情况下提高预训练语言模型的性别公平性。

    Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.05367](http://arxiv.org/abs/2110.05367)

    该论文提出了一种新方法GEEP，用于提高预训练语言模型的性别公平性，同时没有灾难性遗忘问题。透过性别中性数据学习性别相关的提示，GEEP实现了SOTA表现并在GLUE性能上取得了显著提高。

    

    现有的解决预训练语言模型性别偏见的研究通常建立一个小型的性别中性数据集，然后在该数据集上对模型进行第二阶段的预训练。然而，鉴于性别中性数据集的规模有限且集中关注，第二阶段预训练会出现灾难性遗忘。忘记原始训练数据中的信息可能会严重损害模型在下游任务中的性能。在这项工作中，我们通过在GLUE中进行评估，实证地表明这种方法中会发生灾难性遗忘。然后，我们提出了一种新方法，GEnder Equality Prompt (GEEP)，以改善预训练模型的性别公平性，且遗忘较少。 GEEP会冻结预训练模型，并使用性别中性数据学习与性别相关的提示。实证结果显示，GEEP不仅在性别公平任务上实现了SOTA表现，而且在GLUE上遗忘较少，并取得了明显的性能提高。

    Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model's downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.
    
[^127]: 贝叶斯学习规则

    The Bayesian Learning Rule. (arXiv:2107.04562v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.04562](http://arxiv.org/abs/2107.04562)

    许多机器学习算法都可以归结为贝叶斯学习规则，该规则通过利用自然梯度来逼近后验分布，从而得到广泛的算法应用。这一工作不仅统一了现有算法，还帮助我们设计新的算法。

    

    我们展示了许多机器学习算法是一个称为贝叶斯学习规则的单一算法的特例。这个规则是从贝叶斯原理推导出来的，可以从优化、深度学习和图形模型等领域得到广泛的算法。这包括经典算法如岭回归、牛顿法和卡尔曼滤波器，以及现代深度学习算法如随机梯度下降、RMSprop和Dropout。推导这些算法的关键思想是使用自然梯度估计的候选分布来逼近后验分布。不同的候选分布会导致不同的算法，对自然梯度的进一步逼近则会产生这些算法的变种。我们的工作不仅统一、泛化和改进了现有算法，还帮助我们设计新的算法。

    We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.
    
[^128]: 使用Wasserstein生成对抗网络近似概率分布

    Approximating Probability Distributions by using Wasserstein Generative Adversarial Networks. (arXiv:2103.10060v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.10060](http://arxiv.org/abs/2103.10060)

    本文研究了Wasserstein生成对抗网络（WGANs），并使用GroupSort神经网络作为鉴别器。研究结果显示，生成器和鉴别器的容量对目标分布的逼近误差有影响，并且WGANs对鉴别器的容量要求高于生成器。此外，在鉴别器不足够强大时，低容量的生成器可能比过度深层和宽度的生成器效果更好。数值结果证实了理论结果。

    

    本文研究了将GroupSort神经网络作为鉴别器的Wasserstein生成对抗网络（WGANs）。研究结果表明，目标分布的逼近误差界限取决于生成器和鉴别器的宽度和深度（容量）以及训练中的样本数量。针对生成的分布和目标分布之间的Wasserstein距离建立了量化的泛化界限。根据理论结果，WGANs对鉴别器的容量要求比生成器更高，这与一些已有结果一致。更重要的是，如果鉴别器不足够强大，与过度深层和宽度（高容量）的生成器相比，低容量的生成器的结果可能更差。使用Swiss roll和MNIST数据集得到的数值结果证实了理论结果。

    Studied here are Wasserstein generative adversarial networks (WGANs) with GroupSort neural networks as their discriminators. It is shown that the error bound of the approximation for the target distribution depends on the width and depth (capacity) of the generators and discriminators and the number of samples in training. A quantified generalization bound is established for the Wasserstein distance between the generated and target distributions. According to the theoretical results, WGANs have a higher requirement for the capacity of discriminators than that of generators, which is consistent with some existing results. More importantly, the results with overly deep and wide (high-capacity) generators may be worse than those with low-capacity generators if discriminators are insufficiently strong. Numerical results obtained using Swiss roll and MNIST datasets confirm the theoretical results.
    
[^129]: 一种整合和分类正态分布的方法

    A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14331](http://arxiv.org/abs/2012.14331)

    本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。

    

    单变量和多变量正态概率分布在模拟不确定性决策中被广泛使用。计算这些模型的性能需要在特定区域内对这些分布进行积分，这在不同的模型中可以有很大的差异。除了一些特殊情况，目前不存在通用的分析表达式、标准数值方法或软件来计算这些积分。本文提供了数学结果和开源软件，可以提供以下内容：（i）任意参数维度下任意域内法向的概率，（ii）法向向量函数的概率密度、累积分布和逆累积分布，（iii）任意数量正态分布之间的分类误差、贝叶斯最优辨别指数以及其与工作特征曲线的关系，（iv）此类问题的维度降低和可视化，以及（v）对于给定数据这些方法的可靠性测试。我们通过几个具体的例子，包括金融、生物和心理学来演示这些功能。

    Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
    
[^130]: 高维数据丰富化：可解释、快速和数据有效

    High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient. (arXiv:1806.04047v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1806.04047](http://arxiv.org/abs/1806.04047)

    本文研究了在高维设置中的多任务学习问题，并引入了一个估计器来处理多连接线性回归问题，称为数据丰富/共享。我们通过凸函数来描述公共参数和个体参数的结构，并提出了一种具有几何收敛速度的迭代估计算法。

    

    我们考虑在高维设置中的多任务学习问题。特别地，我们引入了一个估计器，并研究了其在多个连接线性回归问题中的统计和计算特性，该问题被称为数据丰富/共享。任务间的连接由跨任务的“公共参数”捕捉，该参数通过任务级的“个体参数”进行细化。任何凸函数，如范数，都可以表征公共参数和个体参数的结构。我们勾勒了我们估计器的样本复杂度，并在几何条件下为所有参数的估计误差提供了高概率的非渐进边界。我们展示了从汇集样本中受益于公共参数的恢复。我们提出了一种具有几何收敛速度的迭代估计算法，并通过合成数据的实验补充了我们的理论分析。总的来说，我们提供了第一个全面的统计和计算分析，用于解决高维数据丰富化问题。

    We consider the problem of multi-task learning in the high dimensional setting. In particular, we introduce an estimator and investigate its statistical and computational properties for the problem of multiple connected linear regressions known as Data Enrichment/Sharing. The between-tasks connections are captured by a cross-tasks \emph{common parameter}, which gets refined by per-task \emph{individual parameters}. Any convex function, e.g., norm, can characterize the structure of both common and individual parameters. We delineate the sample complexity of our estimator and provide a high probability non-asymptotic bound for estimation error of all parameters under a geometric condition. We show that the recovery of the common parameter benefits from \emph{all} of the pooled samples. We propose an iterative estimation algorithm with a geometric convergence rate and supplement our theoretical analysis with experiments on synthetic data. Overall, we present a first thorough statistical a
    

