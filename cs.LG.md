# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating the Robustness of Test Selection Methods for Deep Neural Networks.](http://arxiv.org/abs/2308.01314) | 本文评估了测试选择方法在深度神经网络测试中的稳定性，通过探讨这些方法的潜在陷阱并进行实证研究，揭示了它们的可靠性问题。 |
| [^2] | [More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes.](http://arxiv.org/abs/2308.01313) | 本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。 |
| [^3] | [Lode Encoder: AI-constrained co-creativity.](http://arxiv.org/abs/2308.01312) | Lode Encoder是一个基于AI的共创性游戏关卡生成器，通过训练自编码器并结合用户设计，生成更加符合设计风格的关卡，鼓励设计师探索新的可能性。 |
| [^4] | [Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping.](http://arxiv.org/abs/2308.01308) | 这篇论文提出了下一个新篮子推荐的任务，即推荐仅包含新物品的篮子。作者发现现有下一个篮子推荐方法在这个任务上的进展有限，因此提出了一种简单的双向变换篮子推荐模型（BTBR）来解决这个问题。 |
| [^5] | [BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems.](http://arxiv.org/abs/2308.01274) | BRNES是一个新的MARL框架，通过动态选择邻居区域和加权经验聚合技术来防御拜占庭攻击和隐私泄漏问题。 |
| [^6] | [A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC.](http://arxiv.org/abs/2308.01271) | 本文提出了一种利用循环随机梯度MCMC的贝叶斯自监督学习方法，在多个下游分类任务中，通过探索丰富的嵌入后验分布，实现了显著的性能提升、校准性和外部分布检测能力。 |
| [^7] | [Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives.](http://arxiv.org/abs/2308.01265) | 本文全面评述了医学影像领域深度无监督领域适应方法的最新进展，包括特征对齐、图像转换、自我监督和分解表示方法等。这些方法有助于将知识从标记的领域转移到相关但未标记的领域，为解决医学影像领域中的实际挑战提供了新的思路和方法。 |
| [^8] | [Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites.](http://arxiv.org/abs/2308.01246) | Tirtha是一个自动化平台，用于众包文化遗址的图像并创建它们的三维模型。这个平台利用了最先进的结构运动和多视图立体技术，具有模块化、可扩展和成本效益的特点，在资源有限的发展中国家具有重要意义。 |
| [^9] | [Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models.](http://arxiv.org/abs/2308.01231) | 本论文介绍了上下文预测模型，它可以通过依赖用户和上下文特征来预测用户行为的概率，从而在大规模推荐系统中提高性能并对个性化推荐领域产生广泛影响。 |
| [^10] | [Do Multilingual Language Models Think Better in English?.](http://arxiv.org/abs/2308.01223) | 这项研究提出了一种名为自我翻译的方法，通过利用多语言语言模型的少样本翻译能力，克服了对外部翻译系统的需求，并在多个任务上展示了自我翻译相对于直接推理的优势。 |
| [^11] | [Calibration in Deep Learning: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2308.01222) | 本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。 |
| [^12] | [Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case.](http://arxiv.org/abs/2308.01220) | 我们使用可视化分析工具ScrutinAI来研究医疗领域的模型性能和数据集标注质量对模型性能的影响，以及分析深度神经网络模型的缺点和真正的缺陷之间的区别。 |
| [^13] | [Global Hierarchical Neural Networks using Hierarchical Softmax.](http://arxiv.org/abs/2308.01210) | 本文提出了一种使用层次化softmax的全局层次化神经网络框架，适用于具有层次结构的分类任务，并且在四个文本分类数据集上的实验结果表明，相较于常规softmax和平面分类器，层次化softmax能够取得更好的分类性能。 |
| [^14] | [Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation.](http://arxiv.org/abs/2308.01208) | 本研究提出了一种使用个性化时间衰减函数的自适应协同过滤金融产品推荐系统，以解决传统推荐系统在动态环境下提供可靠推荐的挑战。该方法通过建模客户和产品之间的动态协同信号，处理金融数据的非平稳性，为用户提供可靠的推荐。 |
| [^15] | [BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization.](http://arxiv.org/abs/2308.01207) | BiERL是一个通用的元进化强化学习框架，通过双层优化来同时更新超参数和训练强化学习模型，从而提高学习效率和性能。 |
| [^16] | [Methodologies for Improving Modern Industrial Recommender Systems.](http://arxiv.org/abs/2308.01204) | 本文探讨了提高现代工业推荐系统的方法论，为经验丰富的RS工程师提供实用经验，可能适用于其他RS。 |
| [^17] | [GNN4FR: A Lossless GNN-based Federated Recommendation Framework.](http://arxiv.org/abs/2308.01197) | 这篇论文设计了一种基于无损图神经网络（GNN）的联邦推荐框架，可以实现全图训练，捕捉高阶结构信息，并在保护用户隐私的前提下有效推荐。 |
| [^18] | [Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability.](http://arxiv.org/abs/2308.01196) | 这项研究旨在实现推荐系统的可持续透明性，并提出了一种使用贝叶斯排名图像进行个性化解释的方法，以最大化透明度和用户信任。 |
| [^19] | [Personalized Category Frequency prediction for Buy It Again recommendations.](http://arxiv.org/abs/2308.01195) | 该论文提出了一种个性化的推荐系统，用于根据客户的重复购买模式预测再次购买的类别和商品。采用层次化PCIC模型，通过生存模型和时间序列模型捕捉消费行为和趋势，并使用这些特征训练类别粒度的神经网络。 |
| [^20] | [Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior.](http://arxiv.org/abs/2308.01184) | 本文提出了一种新的生成噪声标签学习方法，直接关联数据和干净标签，通过使用判别的近似方法来隐式估计生成模型，解决了传统方法中的复杂公式、难以训练的生成模型和无信息先验的问题。 |
| [^21] | [Direct Gradient Temporal Difference Learning.](http://arxiv.org/abs/2308.01170) | 本文提出了直接梯度时间差分学习的方法，通过使用马尔可夫数据流中的两个样本来解决双重取样问题，去除了传统方法中的额外权重，保证了计算效率，并提供了收敛性的分析。 |
| [^22] | [LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs.](http://arxiv.org/abs/2308.01157) | LLMs在处理可解释模型方面表现出色，提供了全面的模型级总结和自动化的异常检测、原因描述和修复建议。在医疗保健领域使用广义可加模型作为示例，同时介绍了开源的LLM-GAM接口包$\texttt{TalkToEBM}$。 |
| [^23] | [DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2308.01140) | 本研究提出了一种余弦相似度依赖的温度调整函数，用于改进自监督对比学习中的性能，并优化了样本在特征空间中的分布。 |
| [^24] | [Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives.](http://arxiv.org/abs/2308.01139) | 本文提出了一种动态隐私分配的本地差分隐私联邦学习算法，可以保护梯度并最小化优化错误，同时不需要调整迭代次数。 |
| [^25] | [Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases.](http://arxiv.org/abs/2308.01138) | 这项研究提出了一个噪声模式转移模型，可以将噪声模式从不同环境的标准样本应用到未知样本，通过生成案例库来解决样本级噪声对数据集级噪声学习的干扰，提高了系统的学习性能。 |
| [^26] | [Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans.](http://arxiv.org/abs/2308.01137) | 本文提出了一种多任务学习框架，应用于胸部CT扫描的分类、分割、重建和检测。该方法可以从少量医疗数据中提取重要特征，如病变，以更好地概括和识别早期疾病。 |
| [^27] | [Unlearning Spurious Correlations in Chest X-ray Classification.](http://arxiv.org/abs/2308.01119) | 本论文提出了一种深度学习方法（XBL），通过利用模型解释来交互式地消除胸部X射线分类中的错误相关性。该方法可以帮助解决多数据源引入的混淆因素，提高模型的准确性和透明度。 |
| [^28] | [A Survey on Popularity Bias in Recommender Systems.](http://arxiv.org/abs/2308.01118) | 这篇综述论文讨论了推荐系统中的流行偏差问题，并回顾了现有的方法来检测、量化和减少流行偏差。它同时提供了计算度量的概述和主要技术方法的回顾。 |
| [^29] | [Spatio-Temporal Branching for Motion Prediction using Motion Increments.](http://arxiv.org/abs/2308.01097) | 本论文提出了一种利用运动增量进行时空分支的运动预测网络，通过解耦时域和空域特征的学习，提取更多的运动信息。 |
| [^30] | [Homography Estimation in Complex Topological Scenes.](http://arxiv.org/abs/2308.01086) | 文章提出了一种在复杂拓扑场景中自动估计单应矩阵的方法，通过利用基于字典的方法实现自动相机校准，以及引入一种新颖的拓扑损失函数，可在不需要先验知识的情况下提高IoU度量指标。 |
| [^31] | [Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems.](http://arxiv.org/abs/2308.01084) | 本论文提出了一种基于数据的方法来学习哈密顿系统，利用提升假设，通过强制实施哈密顿结构和使用辛自编码器，我们实现了在变换的坐标系下具有哈密顿结构的二次动力学，保持系统的长期稳定性和较低的模型复杂度。 |
| [^32] | [A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards.](http://arxiv.org/abs/2308.01074) | 本文介绍了一种基于深度学习的实用键盘声学侧信道攻击方法，通过使用集成在智能手机麦克风上的模型对笔记本电脑的按键进行分类，以及使用Zoom软件记录的按键进行训练，实现了高准确率的键盘分类。这些结果证明了使用现成设备和算法进行这种侧信道攻击的可行性。本文还讨论了一系列缓解方法来保护用户免受这些攻击的影响。 |
| [^33] | [Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion.](http://arxiv.org/abs/2308.01071) | 这篇论文评估了时间序列分类中的自动特征工程工具，并与最先进的算法进行了比较，填补了这一领域的研究空白。 |
| [^34] | [When Analytic Calculus Cracks AdaBoost Code.](http://arxiv.org/abs/2308.01070) | 本文表明AdaBoost只是一种名义上的算法，因为可以使用真值表明确地计算得到弱分类器的组合。 |
| [^35] | [Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach.](http://arxiv.org/abs/2308.01063) | 提出一种新的无监督框架，用于在图中检测组级别的异常，并利用拓扑模式增强了算法的性能。 |
| [^36] | [Simulation-based inference using surjective sequential neural likelihood estimation.](http://arxiv.org/abs/2308.01054) | 我们提出了一种使用全射序列神经似然估计（SSNL）进行基于仿真的推断的新方法，在模型中无法计算似然函数并且只能使用模拟器生成数据的情况下，SSNL通过拟合降维的全射归一化流模型，并将其作为替代似然函数，解决了先前基于似然方法在高维数据集中遇到的问题，并在各种实验中展示了其优越性能。 |
| [^37] | [A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness.](http://arxiv.org/abs/2308.01050) | 本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。 |
| [^38] | [Computing the Distance between unbalanced Distributions -- The flat Metric.](http://arxiv.org/abs/2308.01039) | 该论文提出了一种计算不平衡分布之间距离的方法，基于平坦度量，该方法可以推广到分布总质量不平衡的情况下，特别适用于不平衡最优输运和数据分布分析。论文实现了一个基于神经网络的方法，可以计算出两个给定测度之间距离的最佳测试函数，并通过多个实验验证了方法的准确性。 |
| [^39] | [Three Factors to Improve Out-of-Distribution Detection.](http://arxiv.org/abs/2308.01030) | 本论文提出了三个因素来改善离群检测问题。首先，引入自我知识蒸馏损失以提高网络的准确性；其次，在训练过程中采样半困难离群数据以改善离群检测性能；最后，引入新型监督对比学习以同时提高离群检测性能和网络的准确性。通过结合这三个因素，我们的方法在分类和离群检测之间取得了良好的平衡，提高了准确性和离群检测性能。 |
| [^40] | [Maximizing Success Rate of Payment Routing using Non-stationary Bandits.](http://arxiv.org/abs/2308.01028) | 本文提出了一种使用非平稳赌博算法的支付路由策略，通过系统架构设计和实时实验的验证，成功提高了0.92\%的交易成功率。 |
| [^41] | [Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach.](http://arxiv.org/abs/2308.01011) | 本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。 |
| [^42] | [MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization.](http://arxiv.org/abs/2308.01000) | MDT3D方法通过利用多个已注释的源数据集，提高了受监督的LiDAR三维物体检测模型在不同传感器配置的新环境中的鲁棒性。 |
| [^43] | [Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective.](http://arxiv.org/abs/2308.00994) | 本文提出了一个简单而有效的基准线SYNAuG，利用合成数据来解决数据不平衡问题，并在多个数据集上取得了令人印象深刻的性能，超过了现有方法。 |
| [^44] | [Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2308.00989) | 本文中，我们提出了一种新的任务无关正则化器WDER，通过增加子策略的多样性来解决层次化强化学习中的退化问题。实验证明，WDER能够提高性能和样本效率，并且不需要修改超参数。 |
| [^45] | [Certified Multi-Fidelity Zeroth-Order Optimization.](http://arxiv.org/abs/2308.00978) | 本文研究了认证的多流程零阶优化问题，提出了MFDOO算法的认证变体，并证明了其具有近似最优的代价复杂度。同时，还考虑了有噪声评估的特殊情况。 |
| [^46] | [Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud.](http://arxiv.org/abs/2308.00963) | 本论文致力于设计一种方案，用于实现在云中自主和保密的模型优化。通过集成同态加密和可信执行环境技术，我们能够保护自主计算的机密性，从而实现在云中进行持续的机器学习流程管理，而不需用户参与。 |
| [^47] | [Causal Inference with Differentially Private (Clustered) Outcomes.](http://arxiv.org/abs/2308.00957) | 本文提出了一种新的差分隐私机制"Cluster-DP"，它在保证隐私的同时利用数据的聚类结构，从而实现了更强的隐私保证和较低的方差，可以用于进行因果分析。 |
| [^48] | [Curriculum Guided Domain Adaptation in the Dark.](http://arxiv.org/abs/2308.00956) | 本文提出了一种暗域中的课程引导域自适应方法，可逐步将黑盒子训练模型适应到未标记的目标域，同时利用干净/噪声数据分割的易难学习特性，并消除了现有方法中的额外阶段和需求。 |
| [^49] | [From Sparse to Soft Mixtures of Experts.](http://arxiv.org/abs/2308.00951) | 本文提出了一种Soft MoE模型，它是一种稀疏的、完全可微分的Transformer，通过隐式的软分配和只处理部分标记的方式解决了稀疏模型的训练不稳定性和推理成本高的问题，并在视觉识别任务中取得了比标准Transformer和其他MoE变体更好的性能。 |
| [^50] | [Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images.](http://arxiv.org/abs/2308.00947) | 本文提出了一种名为DC-Net的分解耦合网络来应对超声图像中准确病变分割的挑战，通过分解和耦合子网络以及显著性先验融合进行处理，包括区域特征聚合和关系感知表示融合等策略。 |
| [^51] | [On the use of deep learning for phase recovery.](http://arxiv.org/abs/2308.00942) | 本文综述了深度学习在相位恢复中的应用，包括从预处理、中处理和后处理三个阶段的支持以及在相位图像处理中的应用。总结了DL在相位恢复中的工作，并展望了如何更好地利用DL提高相位恢复的可靠性和效率。 |
| [^52] | [QUANT: A Minimalist Interval Method for Time Series Classification.](http://arxiv.org/abs/2308.00928) | QUANT是一种用于时间序列分类的极简区间方法，可以使用单一的特征（分位数）、固定区间和一个现成的分类器，在标准数据集上实现与现有最准确的区间方法相同的准确率。在UCR存档的扩展数据集上，使用单个CPU核心的总计算时间（训练和推断）不超过15分钟，在时间序列分类中取得了最先进的准确率。 |
| [^53] | [Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather.](http://arxiv.org/abs/2308.00924) | 本文研究了在空中图像上进行连续适应的问题，特别关注了模型在逐渐恶化的天气条件下的适应能力。在实验中，作者合成了多个数据集模拟逐渐恶化的天气条件，并评估了不同的领域适应模型的性能。研究结果表明，不断适应的约束对模型的性能有重要影响。 |
| [^54] | [Virtual histological staining of unlabeled autopsy tissue.](http://arxiv.org/abs/2308.00920) | 该研究展示了针对未标记尸体解剖组织进行虚拟染色的方法，通过训练神经网络可以将自荧光图像转换为与传统染色样本相匹配的亮场图像，去除了自溶引起的严重染色伪影。 |
| [^55] | [VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference.](http://arxiv.org/abs/2308.00904) | VLUCI是一个新颖的可变参数学习模型，用于解决反事实推断中的未观测混淆变量的问题。它通过生成未观测混淆变量的后验分布，并构建一个双重变分推断模型来解决因果推断中观测和未观测混淆变量的问题，从而提高反事实推断的准确性。 |
| [^56] | [User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations.](http://arxiv.org/abs/2308.00894) | 本文提出了一个用户可控的推荐系统，通过提供事后和事前解释，用户可以根据这些解释与系统进行交互，从而定制对系统的控制。评估结果表明，这种系统在提供个性化推荐的同时，提高了用户对系统的满意度和信任。 |
| [^57] | [Tango: rethinking quantization for graph neural network training on GPUs.](http://arxiv.org/abs/2308.00890) | Tango是一个重新思考在GPU上对图神经网络训练的量化方法的研究，通过引入新的规则来保持准确度，设计并实现量化感知的基本操作和基本操作之间的优化，以加速GNN训练。 |
| [^58] | [Factor Graph Neural Networks.](http://arxiv.org/abs/2308.00887) | 因子图神经网络（FGNN）提供了一种有效地捕捉高阶关系的方法，可以在推理和学习中使用，具有比传统图神经网络更高的表达能力和灵活性。 |
| [^59] | [Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction.](http://arxiv.org/abs/2308.00886) | 本研究提出了一种使用实时准确评分来提高机器学习性能的方法，通过采集实时疼痛评分和内胚层活动数据，在疼痛分类任务中取得了较好的结果。 |
| [^60] | [PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems.](http://arxiv.org/abs/2308.00864) | 本论文提出了一种基于个性化剩余策略的合作咨询系统PeRP，用于缓解拥堵。该系统通过结构化建模人类驾驶的相似性，并根据驾驶员的特征为其提供行动建议，以减少交通拥堵。 |
| [^61] | [Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes.](http://arxiv.org/abs/2308.00858) | 该论文提出了利用随机过程框架来研究人工神经网络中的激活模式。通过模拟和实验，研究人员得到了描述每个网络中激活模式的参数。 |
| [^62] | [Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation.](http://arxiv.org/abs/2308.00856) | 本研究提出了一种针对联邦肿瘤分割中自适应权重聚合的差分隐私算法，通过扩展相似性权重聚合方法（SimAgg），提高了模型分割能力，并在保护隐私方面做出了额外改进。 |
| [^63] | [A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades.](http://arxiv.org/abs/2308.00855) | 对过去六十年间具有高引用和重要影响的机器学习研究进行了全面的分析，揭示了该领域中最具影响力的论文、作者和合作网络，并发现了热门研究主题和最新涌现的主题。 |
| [^64] | [CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters.](http://arxiv.org/abs/2308.00852) | CASSINI是一种用于机器学习集群的网络感知作业调度器，通过引入亲和图的方式实现作业的高效调度，并实现了平均和尾部完成时间的提升，以及ECN标记数据包数量的减少。 |
| [^65] | [An Exact Kernel Equivalence for Finite Classification Models.](http://arxiv.org/abs/2308.00824) | 本研究推导出梯度下降训练的有限分类模型的精确核表示，揭示了神经网络和核方法之间的等价性，并通过实验证明了精确核对神经网络预测的指导作用。 |
| [^66] | [An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning.](http://arxiv.org/abs/2308.00788) | 本论文介绍了双层优化在信号处理和机器学习中的基本概念和应用。双层优化是一个经典的优化问题，涉及到两个层次的优化，并在建模问题中展现了强大的能力。它在无线系统资源分配和对抗性机器学习等领域有广泛的应用。 |
| [^67] | [Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition.](http://arxiv.org/abs/2308.00787) | 本文研究评估了在可穿戴设备上应用脉冲神经网络对人类活动识别的效果，并探讨了基于事件的感知和多门限Δ调制对系统性能的影响。 |
| [^68] | [DYMOND: DYnamic MOtif-NoDes Network Generative Model.](http://arxiv.org/abs/2308.00770) | DYMOND是一种考虑了动态变化和节点在图案中角色的网络生成模型 |
| [^69] | [Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval.](http://arxiv.org/abs/2308.00762) | 本文介绍了一种扩展神经信息检索方法到评论项检索任务的方法，通过利用自监督对比学习来学习BERT嵌入，以融合查询和评论得分并进行排名。 |
| [^70] | [The Bias Amplification Paradox in Text-to-Image Generation.](http://arxiv.org/abs/2308.00755) | 本文研究了文本到图像生成中的偏见放大现象，并发现其主要原因是训练数据和模型提示之间的差异。一旦考虑到各种分布差异，偏见放大现象显著减少。 |
| [^71] | [Mapping Computer Science Research: Trends, Influences, and Predictions.](http://arxiv.org/abs/2308.00733) | 本文研究了计算机科学领域当前热门的研究领域，通过分析引用计数等因素，提出了一个数据驱动的方法来预测研究趋势。研究发现引用计数是确定趋势的最相关因素，同时NSF资助和专利对热门话题的影响在逐渐增加。 |
| [^72] | [Latent-Shift: Gradient of Entropy Helps Neural Codecs.](http://arxiv.org/abs/2308.00725) | 本文研究了梯度熵与重构误差梯度的相关性，在神经编解码器中利用梯度熵能够实现1-2％的速率节省，这是独立于其他改进方法的。 |
| [^73] | [A Pre-trained Data Deduplication Model based on Active Learning.](http://arxiv.org/abs/2308.00721) | 提出了一种基于主动学习的预训练去重模型，将Transformer和主动学习集成到端到端架构中，首次解决了语义级别的去重问题，同时采用R-Drop方法对每一轮标记数据进行数据增强。通过选择最有价值的数据进行去重模型训练，不仅降低了手动标记的成本，还提高了模型的泛化能力。 |
| [^74] | [Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example.](http://arxiv.org/abs/2308.00720) | 在没有梯度噪声的情况下，ADAM算法在固定步长时会发散，而不受方法参数选择的影响。 |
| [^75] | [Beam Detection Based on Machine Learning Algorithms.](http://arxiv.org/abs/2308.00718) | 本文介绍了一种基于机器学习算法的光束检测方法，通过自建卷积神经网络和支持向量回归模型的组合进行预测，成功实现了85.8%的准确率。 |
| [^76] | [Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN.](http://arxiv.org/abs/2308.00715) | 本文提出了一种使用多头通道注意力机制的深度学习方法，能够自动分类COVID-19 CT图像，并在广泛使用的数据集上展示了96.99%的准确性。 |
| [^77] | [Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features.](http://arxiv.org/abs/2308.00710) | 本文扩展了类激活图的方法，将多个样本的类激活图聚合起来，以展示语义结构化数据的分类的全局解释。聚合过程使得分析师可以进行复杂的假设和进一步的钻取可视化分析。 |
| [^78] | [DeepTSF: Codeless machine learning operations for time series forecasting.](http://arxiv.org/abs/2308.00709) | DeepTSF是一个无代码的时间序列预测机器学习运营框架，通过工作流自动化和无代码建模革新了时间序列预测。它提供了强大且用户友好的解决方案，并与现有数据分析工作流程无缝集成，提高了生产力和兼容性。 |
| [^79] | [VeriGen: A Large Language Model for Verilog Code Generation.](http://arxiv.org/abs/2308.00708) | VeriGen是一种大型语言模型，用于自动化生成高质量的Verilog代码。它在生成的功能正确性和语法正确性方面优于商业GPT-3.5-turbo模型并且显示出竞争性的性能。 |
| [^80] | [Approximate Model-Based Shielding for Safe Reinforcement Learning.](http://arxiv.org/abs/2308.00707) | 提出了一种近似模型屏蔽算法 (AMBS) 来验证学习的强化学习策略在给定安全约束下的性能，与其他屏蔽方法相比，AMBS不需要先验知识，并在具有状态相关安全标签的 Atari 游戏上展示了优越性能。 |
| [^81] | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.](http://arxiv.org/abs/2308.00436) | 本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。 |
| [^82] | [DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms.](http://arxiv.org/abs/2308.00127) | DiviML是一种基于模块的启发式方法，用于在异构平台上将神经网络映射，通过自动分区和设备映射，实现了编译器级别的分布式神经网络编译，具有较好的可扩展性和质量评估能力。 |
| [^83] | [LLMs4OL: Large Language Models for Ontology Learning.](http://arxiv.org/abs/2307.16648) | LLMs4OL方法利用大型语言模型在本体学习中取得显著进展，能够从自然语言文本中自动提取和结构化知识。 |
| [^84] | [Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2307.16039) | Okapi是一种使用强化学习从人类反馈中调优的多语言大型语言模型，它解决了目前开源语言模型只针对英语和少数流行语言进行指令调优的限制问题。 |
| [^85] | [MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning.](http://arxiv.org/abs/2307.13055) | 提出了一个模型无关配方MARIO，用于改善图对比学习的OOD泛化性能。MARIO引入了信息瓶颈原则和不变性原则，旨在获得具有分布偏移鲁棒性和不变性的图表示。 |
| [^86] | [Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML.](http://arxiv.org/abs/2307.08364) | 本论文提出了两种新颖的基于群体的集成选择方法QO-ES和QDO-ES，并将它们与贪婪的集成选择进行比较。结果显示，QO-ES和QDO-ES通常优于贪婪的集成选择，尽管只有在验证数据上具有统计显著性。研究还发现，多样性对于后期集成可能有益，但也增加了过拟合的风险。 |
| [^87] | [Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs.](http://arxiv.org/abs/2307.03393) | 本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。 |
| [^88] | [Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series.](http://arxiv.org/abs/2307.01482) | 提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。 |
| [^89] | [Differentially Private Distributed Estimation and Learning.](http://arxiv.org/abs/2306.15865) | 本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。 |
| [^90] | [Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse.](http://arxiv.org/abs/2306.15557) | 我们提出了一种基于距离的算法补偿的新方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。该方法具有高效性、可证明的隐私和鲁棒性保证，并在实验证明上优于现有技术。 |
| [^91] | [TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting.](http://arxiv.org/abs/2306.10940) | TeleViT是一种电联驱动的视觉Transformer模型，可以准确预测季节性野火的全球烧毁面积模式，提前四个月进行预测。 |
| [^92] | [Bayesian Optimization of Expensive Nested Grey-Box Functions.](http://arxiv.org/abs/2306.05150) | 本文提出基于乐观主义的算法来解决嵌套黑白箱函数优化问题，相比传统黑箱优化方法显著提高全局最优解速度。 |
| [^93] | [Sampling binary sparse coding QUBO models using a spiking neuromorphic processor.](http://arxiv.org/abs/2306.01940) | 本文提出了一种使用脉冲神经形态处理器对二进制稀疏编码模型进行采样的方法。首先，介绍了一种无监督和非标准化的字典特征学习方法。然后，利用随机神经网络在非凸能量景观中遍历，解决了二进制稀疏编码问题。 |
| [^94] | [Efficient Large-Scale Vision Representation Learning.](http://arxiv.org/abs/2305.13399) | 本文介绍了一种高效大规模的单模态视觉表示学习方法，解决了电商应用中视觉表示的挑战，并提供了消融研究和评估。 |
| [^95] | [SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2305.11322) | 这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。 |
| [^96] | [Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation.](http://arxiv.org/abs/2305.07804) | 本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。 |
| [^97] | [BrainNPT: Pre-training of Transformer networks for brain network classification.](http://arxiv.org/abs/2305.01666) | 本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。 |
| [^98] | [ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings.](http://arxiv.org/abs/2305.00365) | ReLBOT使用转移学习和深度RL技术来从现有的智能建筑中传递优化参数到新的建筑中，以减少强化学习代理引起的初始不适，有效降低了风险，并且实现了热身期时长6.2倍的提高和预测方差的132倍提高。 |
| [^99] | [Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box.](http://arxiv.org/abs/2304.05527) | 本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。 |
| [^100] | [Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification.](http://arxiv.org/abs/2303.16132) | 本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。 |
| [^101] | [Green Federated Learning.](http://arxiv.org/abs/2303.14604) | 本文提出了一种绿色联邦学习方法，通过优化算法和能源管理策略来减少碳足迹，使得跨设备的联邦学习系统可以更加环保。 |
| [^102] | [Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning.](http://arxiv.org/abs/2303.12653) | 本文提出一种自监督混合深度学习网络用于健壮波束成形，能够在两种不同的数据集和各种场景中表现出更强的鲁棒性。 |
| [^103] | [Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data.](http://arxiv.org/abs/2303.04630) | 本研究研发了一种整合患者ICU住院期间所有记录的数据来为每名TBI患者提供可解释的疾病进展的建模策略，并应用递归神经网络模型对患者状况进行预测，以帮助实现个体化治疗。 |
| [^104] | [Technical report: Graph Neural Networks go Grammatical.](http://arxiv.org/abs/2303.01590) | 本文介绍了一种将代数语言片段与图神经网络形式上联系的框架，并从MATLANG定义了一个符合3-WL测试的语法，进而得出一个符合3-WL GNN模型的G$^2$N$^2$。此外，语法方法还提供了计算长度为六及以下的环和弦环的代数公式，并在多个下游任务中取得优秀的表现。 |
| [^105] | [Evolutionary Augmentation Policy Optimization for Self-supervised Learning.](http://arxiv.org/abs/2303.01584) | 本文研究了数据增强操作对自监督学习算法性能的影响，提出了一种进化搜索方法来优化数据增强策略，并通过实验比较了几种现有自监督学习算法的性能。 |
| [^106] | [Understanding plasticity in neural networks.](http://arxiv.org/abs/2303.01486) | 本文通过对失去可塑性问题进行系统实证分析，发现其深度与损失梯度曲率变化密切相关，饱和单元或发散梯度范数并非原因。基于这一发现，识别了一系列参数化和优化方法，有效提高神经网络保持可塑性的能力，在深度强化学习问题中具有显著的适应性和鲁棒性。 |
| [^107] | [Detecting Harmful Agendas in News Articles.](http://arxiv.org/abs/2302.00102) | 这项研究提出了一种新的任务，即在新闻文章中检测有害议程，并发布了一个新闻文章注释数据集以供研究使用。研究者展示了可解释系统在这一任务上的有效性，并证明它们可以和黑盒模型有相当的表现。 |
| [^108] | [Domain-adapted Learning and Imitation: DRL for Power Arbitrage.](http://arxiv.org/abs/2301.08360) | 本文提出了一种领域自适应学习和模仿的深度强化学习方法，用于电力套利交易。通过利用奖励工程和订单分段的方式，该方法能够提高训练的收敛性，增加竞标成功率，并显著提高利润和损失。 |
| [^109] | [Thinking Fast and Slow in Large Language Models.](http://arxiv.org/abs/2212.05206) | 本研究发现，大型语言模型（LLMs）如GPT-3在行为上与人类直觉相似，但可能带有认知错误。然而，具有更高认知能力的LLMs，如ChatGPT和GPT-4，学会了避免这些错误，表现出超理性的方式。通过在心理学方法的帮助下研究LLMs，我们可以揭示出其它未知的新特征。 |
| [^110] | [Data-driven identification and analysis of the glass transition in polymer melts.](http://arxiv.org/abs/2211.14220) | 本研究提出了一种基于数据驱动的方法，通过分子动力学模拟和聚类方法，清晰地识别了聚合物熔体的玻璃转变温度，并揭示了主成分分析在捕捉聚合物链行为变化中的作用。 |
| [^111] | [The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness.](http://arxiv.org/abs/2211.08043) | 本文研究了Bregman近端方法的收敛速度，发现其与局部几何、正则性以及锐度等因素相关。特别地，我们发现边界解在零和非零Legendre指数的方法之间有明显的差异，前者以线性速度收敛，而后者以次线性速度收敛。在线性约束问题中，具有熵正则化的方法在尖锐方向上实现线性收敛速度，与传统收敛速度有显著区别。 |
| [^112] | [Instance-Dependent Generalization Bounds via Optimal Transport.](http://arxiv.org/abs/2211.01258) | 该论文提出了一种基于最优传输的实例相关泛化界限的方法，以解释神经网络泛化的关键因素，并且考虑了初始化和随机梯度下降的强归纳偏差。这种方法在模型参数化不可知且训练样本数量远小于参数数量时表现良好，还可以应用于低维流形上的数据和分布转换情况下的泛化问题。 |
| [^113] | [Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS).](http://arxiv.org/abs/2210.08549) | 该论文旨在解决国际空间站上颗粒物对仪器的危害问题，通过Bi-GRU算法构建早期预警系统，预测颗粒物水平，并为宇航员提供充足的反应时间。这项研究还有潜力发展为与火灾相关的遥感烟雾报警装置。 |
| [^114] | [Learning to Efficiently Plan Robust Frictional Multi-Object Grasps.](http://arxiv.org/abs/2210.07420) | 本文介绍了一种使用神经网络进行摩擦多物体抓取的高效计划方法，相比于以往的工作，其成功率提高了13.7％，每小时的拾取次数增加了1.6倍，并且抓取计划时间减少了6.3倍。 |
| [^115] | [Mass-Editing Memory in a Transformer.](http://arxiv.org/abs/2210.07229) | 该论文提出了一种在Transformer中进行大规模编辑内存的方法，可以有效地更新语言模型的记忆，实验证明其在关联数量上具有数量级的优势。 |
| [^116] | [FaDIn: Fast Discretized Inference for Hawkes Processes with General Parametric Kernels.](http://arxiv.org/abs/2210.04635) | 本论文提出了一种使用具有有限支持的一般参数核进行TPP推理的高效解决方案，该方法采用了离散化方法，并通过多项实验证明了该方法的统计和计算效率。 |
| [^117] | [FedDef: Defense Against Gradient Leakage in Federated Learning-based Network Intrusion Detection Systems.](http://arxiv.org/abs/2210.04052) | FedDef是一种防御基于联邦学习的网络入侵检测系统中梯度泄露的方法，通过提出隐私评估指标和对抗攻击来评估FL系统的鲁棒性，并展示了现有防御措施的脆弱性。 |
| [^118] | [Graph Soft-Contrastive Learning via Neighborhood Ranking.](http://arxiv.org/abs/2209.13964) | 图对比学习（GCL）方法在图像领域表现出了显著的性能，但在应用于图数据时面临着生成无效视图和不可靠相似性对的限制。这篇论文提出了一种基于邻域排序的图形软对比学习方法，以更好地适应图的内在属性。 |
| [^119] | [ProMix: Combating Label Noise via Maximizing Clean Sample Utility.](http://arxiv.org/abs/2207.10276) | 论文提出了一种名为ProMix的新颖LNL框架，通过最大化干净样本的效用来对抗标签噪声。采用一种匹配高置信度选择技术来动态扩展基础干净样本集，并设计了一种平衡和无偏的SSL框架来提高性能。 |
| [^120] | [Models of human preference for learning reward functions.](http://arxiv.org/abs/2206.02231) | 本研究提出了一种将人类偏好建模为每个轨迹段的遗憾的方法，并证明了可以根据这些遗憾生成的偏好来识别生成这些偏好的奖励函数。实验证明，这种遗憾偏好模型在性能上优于以前的模型。 |
| [^121] | [Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions.](http://arxiv.org/abs/2206.01255) | 本研究提出了一种压缩傅里叶色散方法，用于解决定义在高维周期边界条件域上的扩散方程。该方法利用压缩感知和稀疏恢复技术，通过在蒙特卡罗采样上近似解的傅里叶系数，有效地克服了维度诅咒的影响。 |
| [^122] | [Sparse Graph Learning from Spatiotemporal Time Series.](http://arxiv.org/abs/2205.13492) | 本论文提出了一种基于概率评分的方法，通过学习关系依赖的图分布来解决在时空时间序列分析中关系信息不可用的问题，并在时间序列预测问题上展示了有效性。 |
| [^123] | [Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach.](http://arxiv.org/abs/2203.15925) | 本文提出了一种条件推理方法来解决多智能体策略梯度方法在异步选项执行中的问题，并在基于选项的多智能体合作任务上取得有效结果。 |
| [^124] | [Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation.](http://arxiv.org/abs/2203.11740) | 该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。 |
| [^125] | [Fabricated Flips: Poisoning Federated Learning without Data.](http://arxiv.org/abs/2202.05877) | 本文介绍了一种名为无数据非定向攻击（DFA）的新方法，它通过合成恶意数据来制造对抗模型，在不需要窃听良性客户端传输或大量任务特定训练数据的情况下实现攻击。这种方法能够在联邦学习中降低生成模型质量，并限制该学习模式的实用性。 |
| [^126] | [Successor Feature Representations.](http://arxiv.org/abs/2110.15701) | 继承特征表示（SFR）是一种新的Successor Representations (SR)的表达方式，通过学习继承特征的累积折扣概率来重新评估策略的预期回报。 |
| [^127] | [Integrated Conditional Estimation-Optimization.](http://arxiv.org/abs/2110.12351) | 该论文提出了一种综合条件估计-优化（ICEO）框架，可以在考虑优化问题结构的同时估计随机参数的条件分布，并提供了一些性能保证。 |
| [^128] | [Bandit based centralized matching in two-sided markets for peer to peer lending.](http://arxiv.org/abs/2105.02589) | 该论文研究了基于赌博算法的中央化匹配在点对点借贷的两边市场中的应用，探讨了解决中央化投资机制难题的可能性，为理解在线平台上点对点借贷的顺序贡献动态提供了一种新的方法。 |
| [^129] | [Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification.](http://arxiv.org/abs/2104.10401) | 本文提出了一种基于多注意力软分区网络的车辆再识别方法，通过引入多软注意力机制来解决由于不同视角和相似车辆之间的内部差异导致的挑战，并避免了嘈杂的注意力图和额外的注释元数据的使用。 |
| [^130] | [Class-incremental Learning with Pre-allocated Fixed Classifiers.](http://arxiv.org/abs/2010.08657) | 本文提出了一种具有预分配固定分类器的类增量学习方法，通过利用存储在情节性记忆中的过去数据，并在学习阶段的开始就将一些预分配的输出节点纳入分类损失的计算，解决了神经网络在类增量学习中遗忘先前知识的问题。 |
| [^131] | [Attention Is All You Need.](http://arxiv.org/abs/1706.03762) | Transformer是一种新的简单网络架构，完全基于注意力机制，取代了复杂的循环神经网络或卷积神经网络。实验证明Transformer在机器翻译任务中的质量更好、并行化效果更佳，且训练时间更短。它在英译德和英译法任务中取得了比其他模型更好的结果。 |

# 详细

[^1]: 评估深度神经网络测试选择方法的稳定性

    Evaluating the Robustness of Test Selection Methods for Deep Neural Networks. (arXiv:2308.01314v1 [cs.LG])

    [http://arxiv.org/abs/2308.01314](http://arxiv.org/abs/2308.01314)

    本文评估了测试选择方法在深度神经网络测试中的稳定性，通过探讨这些方法的潜在陷阱并进行实证研究，揭示了它们的可靠性问题。

    

    由于对收集的原始数据进行标记所需的时间和劳动力，测试基于深度学习的系统是至关重要的，但也具有挑战性。为了减轻标记工作量，已经提出了多种测试选择方法，只需对测试数据的子集进行标记即可满足测试要求。然而，我们观察到，这些报道有希望的结果的方法只在简单情景下进行评估，例如，在原始测试数据上进行测试。这让我们产生了一个问题：它们总是可靠的吗？本文探讨了测试选择方法在测试中失败的时间和程度。具体而言，首先，我们基于其构建方法，确定了来自顶级会议的11种选择方法的潜在陷阱。其次，我们对五个数据集进行了研究，每个数据集有两个模型架构，以从经验上确认这些陷阱的存在。此外，我们还演示了陷阱如何破坏这些方法的可靠性。具体来说，故障检测方法的缺陷。

    Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suf
    
[^2]: 更多上下文，更少干扰：通过推断和调节上下文属性进行视觉分类

    More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])

    [http://arxiv.org/abs/2308.01313](http://arxiv.org/abs/2308.01313)

    本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。

    

    CLIP作为一种基础的视觉语言模型，由于其理解各种视觉概念和自然语言描述的能力，被广泛应用于零样本图像分类。然而，如何充分利用CLIP的前所未有的人类般理解能力来实现更好的零样本分类仍然是一个开放问题。本文从人类的视觉感知过程中得到启发：现代神经科学观点认为，在对物体进行分类时，人类首先推断其与类别无关的属性（如背景和方向），这有助于将前景对象与背景区分开来，然后以此信息为基础进行决策。受此启发，我们观察到为CLIP提供上下文属性可以改善零样本分类并减轻对虚假特征的依赖。我们还观察到CLIP本身可以合理地从图像中推断出这些属性。基于这些观察，我们提出了一种零训练、两步骤的零样本分类方法。

    CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
    
[^3]: Lode Encoder: AI约束下的共创性游戏关卡生成器

    Lode Encoder: AI-constrained co-creativity. (arXiv:2308.01312v1 [cs.LG])

    [http://arxiv.org/abs/2308.01312](http://arxiv.org/abs/2308.01312)

    Lode Encoder是一个基于AI的共创性游戏关卡生成器，通过训练自编码器并结合用户设计，生成更加符合设计风格的关卡，鼓励设计师探索新的可能性。

    

    我们提出了Lode Encoder，这是一个以经典平台益智游戏Lode Runner为基础的创意游戏关卡生成系统。该系统基于多个自编码器，这些自编码器通过对一系列Lode Runner关卡进行训练来生成与用户设计风格更相似的版本。Lode Encoder界面允许用户通过自编码器提供的建议来构建和编辑关卡。为了鼓励设计师探索新的可能性，该系统不包含传统的编辑工具。我们报告了系统设计和训练过程，以及系统演进和用户测试的结果。

    We present Lode Encoder, a gamified mixed-initiative level creation system for the classic platform-puzzle game Lode Runner. The system is built around several autoencoders which are trained on sets of Lode Runner levels. When fed with the user's design, each autoencoder produces a version of that design which is closer in style to the levels that it was trained on. The Lode Encoder interface allows the user to build and edit levels through 'painting' from the suggestions provided by the autoencoders. Crucially, in order to encourage designers to explore new possibilities, the system does not include more traditional editing tools. We report on the system design and training procedure, as well as on the evolution of the system itself and user tests.
    
[^4]: 遮盖和交换序列建模：用于杂货购物中下一个新篮子推荐的论文

    Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping. (arXiv:2308.01308v1 [cs.IR])

    [http://arxiv.org/abs/2308.01308](http://arxiv.org/abs/2308.01308)

    这篇论文提出了下一个新篮子推荐的任务，即推荐仅包含新物品的篮子。作者发现现有下一个篮子推荐方法在这个任务上的进展有限，因此提出了一种简单的双向变换篮子推荐模型（BTBR）来解决这个问题。

    

    下一个篮子推荐是基于已购篮子序列预测下一组物品的任务。这是一个被广泛研究的推荐任务，尤其是在杂货购物环境中。在下一个篮子推荐中，区分重复物品和探索物品是很有用的，即用户已经消费过的物品和用户未消费过的物品。大部分下一个篮子推荐的研究要么忽略这种区分，要么专注于重复物品。我们提出了下一个新篮子推荐的任务，即推荐仅包含新物品的篮子，这对于实际应用和下一个篮子推荐评估都很有价值。我们评估了现有下一个篮子推荐方法在下一个新篮子推荐任务上的表现，并发现在这个任务上取得的进展有限。为了解决下一个新篮子推荐任务，我们提出了一种简单的双向变换篮子推荐模型（BTBR），它专注于直接建模篮子序列中的新物品。

    Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets. It is a recommendation task that has been widely studied, especially in the context of grocery shopping. In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before. Most NBR work either ignores this distinction or focuses on repeat items. We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation. We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t. the NNBR task. To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeli
    
[^5]: BRNES: 在多智能体机器人和自主系统中实现安全和隐私感知的经验共享

    BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems. (arXiv:2308.01274v1 [cs.CR])

    [http://arxiv.org/abs/2308.01274](http://arxiv.org/abs/2308.01274)

    BRNES是一个新的MARL框架，通过动态选择邻居区域和加权经验聚合技术来防御拜占庭攻击和隐私泄漏问题。

    

    尽管经验共享（ES）在顾问-受劝告框架中加速了多智能体强化学习（MARL），但尝试将ES应用于分布式多智能体系统迄今为止仍依赖于值得信赖的环境，并忽略了对抗性操纵和推理的可能性。然而，在现实世界中，一些拜占庭攻击者冒充顾问向受劝告提供虚假建议，并严重降低整体学习性能。而且，一个推理攻击者冒充受劝告者可能进行多次查询以推断顾问的私人信息，并使整个ES过程在隐私泄漏方面存在问题。为了解决和应对这些问题，我们提出了一种新的MARL框架（BRNES），它在每个学习步骤中启发式地为每个受劝告者选择一个动态邻居区域，并采用加权经验聚合技术来减少拜占庭攻击的影响。此外，为了确保代理的私人信息的安全

    Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and overlooked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe fr
    
[^6]: 使用循环随机梯度MCMC的概率自监督学习的实用贝叶斯方法

    A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC. (arXiv:2308.01271v1 [cs.LG])

    [http://arxiv.org/abs/2308.01271](http://arxiv.org/abs/2308.01271)

    本文提出了一种利用循环随机梯度MCMC的贝叶斯自监督学习方法，在多个下游分类任务中，通过探索丰富的嵌入后验分布，实现了显著的性能提升、校准性和外部分布检测能力。

    

    本文提出了一种利用循环随机梯度哈密顿蒙特卡洛（cSGHMC）的实用贝叶斯自监督学习方法。在该框架中，我们对自监督学习模型的参数设定先验，并使用cSGHMC近似表示多维多模态后验分布。通过探索丰富的嵌入后验分布，贝叶斯自监督学习产生了可解释性和多样性的表示。在多个下游分类任务中，通过边缘化这些表示，我们得到了显著的性能提升、校准性和外部分布检测能力。我们在四个具有挑战性的数据集上进行了多个分类任务的实验结果。此外，我们还使用SVHN和CIFAR-10数据集验证了所提出方法在外部分布检测方面的有效性。

    In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.
    
[^7]: 医学影像领域无监督领域适应的深度学习：最新进展和未来展望

    Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives. (arXiv:2308.01265v1 [eess.IV])

    [http://arxiv.org/abs/2308.01265](http://arxiv.org/abs/2308.01265)

    本文全面评述了医学影像领域深度无监督领域适应方法的最新进展，包括特征对齐、图像转换、自我监督和分解表示方法等。这些方法有助于将知识从标记的领域转移到相关但未标记的领域，为解决医学影像领域中的实际挑战提供了新的思路和方法。

    

    深度学习在医学影像领域展现出了卓越的性能。然而，这些方法主要集中在监督学习上，假设训练和测试数据来自同一分布。然而，实际中这个假设并不总是成立。为了解决这些问题，已经开发了无监督领域适应（UDA）技术，以将知识从标记的领域转移到相关但未标记的领域。近年来，在UDA方面取得了显著的进展，产生了广泛的方法论，包括特征对齐、图像转换、自我监督和分解表示方法等。本文从技术角度全面评述了医学影像领域深度UDA方法的研究文献。具体而言，我们将当前医学影像领域UDA研究分为六类，并根据数据的特点进一步细分子类别。

    Deep learning has demonstrated remarkable performance across various tasks in medical imaging. However, these approaches primarily focus on supervised learning, assuming that the training and testing data are drawn from the same distribution. Unfortunately, this assumption may not always hold true in practice. To address these issues, unsupervised domain adaptation (UDA) techniques have been developed to transfer knowledge from a labeled domain to a related but unlabeled domain. In recent years, significant advancements have been made in UDA, resulting in a wide range of methodologies, including feature alignment, image translation, self-supervision, and disentangled representation methods, among others. In this paper, we provide a comprehensive literature review of recent deep UDA approaches in medical imaging from a technical perspective. Specifically, we categorize current UDA research in medical imaging into six groups and further divide them into finer subcategories based on the d
    
[^8]: Tirtha -- 一个自动化平台，用于众包图像并创建历史遗址的三维模型

    Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites. (arXiv:2308.01246v1 [cs.CV])

    [http://arxiv.org/abs/2308.01246](http://arxiv.org/abs/2308.01246)

    Tirtha是一个自动化平台，用于众包文化遗址的图像并创建它们的三维模型。这个平台利用了最先进的结构运动和多视图立体技术，具有模块化、可扩展和成本效益的特点，在资源有限的发展中国家具有重要意义。

    

    数字化保存文化遗产（CH）遗址对于保护它们免受自然灾害或人类活动的损害至关重要。创建文化遗产遗址的三维模型已经成为数字化保存的流行方法，得益于计算机视觉和摄影测量的进展。然而，这个过程耗时、昂贵，并且通常需要专门的设备和专业知识，在资源有限的发展中国家存在挑战。此外，缺乏一个开放的三维模型存储库阻碍了对遗产的研究和公众参与。为了解决这些问题，我们提出了Tirtha，一个用于众包文化遗址图像和三维模型创建的网络平台。Tirtha利用了最先进的结构运动（SfM）和多视图立体（MVS）技术。它是模块化、可扩展和具有成本效益的，可以随着摄影测量的进展而引入新技术。Tirtha可以通过https://tirtha.niser.ac.in作为网页界面访问。

    Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in a
    
[^9]: 发挥上下文的力量：基于上下文预测模型增强大规模推荐系统

    Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models. (arXiv:2308.01231v1 [cs.IR])

    [http://arxiv.org/abs/2308.01231](http://arxiv.org/abs/2308.01231)

    本论文介绍了上下文预测模型，它可以通过依赖用户和上下文特征来预测用户行为的概率，从而在大规模推荐系统中提高性能并对个性化推荐领域产生广泛影响。

    

    在这项工作中，我们引入了上下文预测模型的概念。上下文预测模型仅依赖于用户和上下文特征，而不考虑物品本身的特定特征，来确定用户行为（如点击或转化）的概率。我们发现了许多有价值的应用场景，包括训练辅助的基于上下文的模型来估计点击概率，并将其预测结果作为CTR预测模型的特征。我们的实验表明，这种改进在离线和在线业务指标上带来了显著的改进，而对服务成本几乎没有影响。总体而言，我们的工作提供了一种简单而可扩展，但强大的方法，用于增强大规模商业推荐系统的性能，并对个性化推荐领域具有广泛影响。

    In this work, we introduce the notion of Context-Based Prediction Models. A Context-Based Prediction Model determines the probability of a user's action (such as a click or a conversion) solely by relying on user and contextual features, without considering any specific features of the item itself. We have identified numerous valuable applications for this modeling approach, including training an auxiliary context-based model to estimate click probability and incorporating its prediction as a feature in CTR prediction models. Our experiments indicate that this enhancement brings significant improvements in offline and online business metrics while having minimal impact on the cost of serving. Overall, our work offers a simple and scalable, yet powerful approach for enhancing the performance of large-scale commercial recommender systems, with broad implications for the field of personalized recommendations.
    
[^10]: 多语言语言模型在英语中思考是否更好？

    Do Multilingual Language Models Think Better in English?. (arXiv:2308.01223v1 [cs.CL])

    [http://arxiv.org/abs/2308.01223](http://arxiv.org/abs/2308.01223)

    这项研究提出了一种名为自我翻译的方法，通过利用多语言语言模型的少样本翻译能力，克服了对外部翻译系统的需求，并在多个任务上展示了自我翻译相对于直接推理的优势。

    

    翻译测试是提高多语言语言模型性能的一种常用技术。这种方法通过使用外部机器翻译系统将输入翻译成英语，并对翻译后的输入进行推理来实现。然而，这些改进可以归因于使用一个单独的翻译系统，这个系统通常是在大量的平行数据上进行训练的，而这些数据对于语言模型来说是看不到的。在这项工作中，我们介绍了一种新方法，称为自我翻译，通过利用多语言语言模型的少样本翻译能力来克服对外部翻译系统的需求。对5个任务的实验表明，自我翻译始终优于直接推理，证明了当在非英语语言中进行提示时，语言模型无法充分发挥其多语言潜力。我们的代码可在 https://github.com/juletx/self-translate 中找到。

    Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
    
[^11]: 深度学习中的校准：最新研究综述

    Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])

    [http://arxiv.org/abs/2308.01222](http://arxiv.org/abs/2308.01222)

    本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。

    

    在构建可靠、鲁棒的安全关键应用的人工智能系统中，深度神经模型的校准起着重要作用。最近的研究表明，具有高预测能力的现代神经网络的校准性较差，产生不可靠的模型预测。尽管深度学习模型在各种基准测试中取得了显著的性能，但对模型的校准性和可靠性的研究相对较少。理想的深度模型不仅应具有高预测性能，还应具有良好的校准性。最近提出了一些使用不同机制进行深度模型校准的方法。在本综述中，我们回顾了最新的校准方法，并解释了它们执行模型校准的原理。首先，我们从模型校准的定义开始，解释了模型校准不准确的根本原因。然后，我们介绍了可以衡量模型校准性的关键指标。接下来，我们总结了一些校准方法的方法和实践。

    Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
    
[^12]: 在医学应用中使用ScrutinAI进行DNN性能的视觉检查

    Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case. (arXiv:2308.01220v1 [cs.LG])

    [http://arxiv.org/abs/2308.01220](http://arxiv.org/abs/2308.01220)

    我们使用可视化分析工具ScrutinAI来研究医疗领域的模型性能和数据集标注质量对模型性能的影响，以及分析深度神经网络模型的缺点和真正的缺陷之间的区别。

    

    我们的可视化分析工具ScrutinAI支持人工分析师交互式地研究模型性能和数据集。模型性能在很大程度上取决于标注质量。特别是在医学环境中，生成高质量的标注需要深入的专业知识，并且非常昂贵。通常，数据集通过收集专家群体的意见进行标注。我们使用我们的可视化分析工具来分析不同专家之间标注变异对模型性能的影响。ScrutinAI有助于进行根本原因分析，将由标注质量不同或缺失引起的深度神经网络（DNN）模型的缺点与真正的缺陷区别开来。我们详细检查了公开可用的数据集中颅内出血的整体检测和亚型之间更细微的区分。

    Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performanceand data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyse the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.
    
[^13]: 使用层次化softmax的全局层次化神经网络

    Global Hierarchical Neural Networks using Hierarchical Softmax. (arXiv:2308.01210v1 [stat.ML])

    [http://arxiv.org/abs/2308.01210](http://arxiv.org/abs/2308.01210)

    本文提出了一种使用层次化softmax的全局层次化神经网络框架，适用于具有层次结构的分类任务，并且在四个文本分类数据集上的实验结果表明，相较于常规softmax和平面分类器，层次化softmax能够取得更好的分类性能。

    

    本文提出了一个框架，在其中使用层次化softmax来创建一个全局层次化分类器。该方法适用于任何具有自然层次结构的分类任务。我们在四个文本分类数据集上展示了实证结果。在所有数据集中，相对于使用平面分类器的常规softmax，层次化softmax在宏F1和宏召回率方面都有所提升。在四个数据集中的三个数据集中，层次化softmax实现了更高的微准确率和宏精确率。

    This paper presents a framework in which hierarchical softmax is used to create a global hierarchical classifier. The approach is applicable for any classification task where there is a natural hierarchy among classes. We show empirical results on four text classification datasets. In all datasets the hierarchical softmax improved on the regular softmax used in a flat classifier in terms of macro-F1 and macro-recall. In three out of four datasets hierarchical softmax achieved a higher micro-accuracy and macro-precision.
    
[^14]: 个性化时间衰减函数的自适应协同过滤金融产品推荐系统

    Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation. (arXiv:2308.01208v1 [cs.IR])

    [http://arxiv.org/abs/2308.01208](http://arxiv.org/abs/2308.01208)

    本研究提出了一种使用个性化时间衰减函数的自适应协同过滤金融产品推荐系统，以解决传统推荐系统在动态环境下提供可靠推荐的挑战。该方法通过建模客户和产品之间的动态协同信号，处理金融数据的非平稳性，为用户提供可靠的推荐。

    

    传统的推荐系统通常假设历史数据是不变的，无法考虑用户偏好的动态性，限制了它们在时间敏感环境中提供可靠推荐的能力。这一假设在金融领域尤其有问题，因为金融产品的估值不断变化，导致客户兴趣频繁转移。这些演变的兴趣可以通过过去客户-产品交互中总结出来，其效用随着时间的推移会因客户而异。为了解决这一挑战，我们提出了一种时间相关的协同过滤算法，可以使用个性化衰减函数自适应地折价远离的客户-产品交互。我们的方法旨在处理金融数据的非平稳性，并通过建模客户和产品之间的动态协同信号来产生可靠的推荐。我们使用专有数据集对我们的方法进行评估。

    Classical recommender systems often assume that historical data are stationary and fail to account for the dynamic nature of user preferences, limiting their ability to provide reliable recommendations in time-sensitive settings. This assumption is particularly problematic in finance, where financial products exhibit continuous changes in valuations, leading to frequent shifts in client interests. These evolving interests, summarized in the past client-product interactions, see their utility fade over time with a degree that might differ from one client to another. To address this challenge, we propose a time-dependent collaborative filtering algorithm that can adaptively discount distant client-product interactions using personalized decay functions. Our approach is designed to handle the non-stationarity of financial data and produce reliable recommendations by modeling the dynamic collaborative signals between clients and products. We evaluate our method using a proprietary dataset 
    
[^15]: BiERL: 一种通过双层优化实现的元进化强化学习框架

    BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization. (arXiv:2308.01207v1 [cs.NE])

    [http://arxiv.org/abs/2308.01207](http://arxiv.org/abs/2308.01207)

    BiERL是一个通用的元进化强化学习框架，通过双层优化来同时更新超参数和训练强化学习模型，从而提高学习效率和性能。

    

    进化强化学习算法最近引起了人们的关注，因为它们能够处理复杂的强化学习问题，具有高并行性，但是在不仔细调整超参数（即元参数）的情况下，往往会面临不足的探索或模型崩溃的问题。在本文中，我们提出了一个通用的元进化强化学习框架，通过双层优化（BiERL）来同时更新超参数和训练强化学习模型，从而免去了在模型部署之前需要先有领域知识或昂贵优化过程的需求。我们设计了一个优雅的元级架构，将内部级别的进化经验嵌入到信息丰富的种群表示中，并引入了一个简单可行的元级适应度函数评估方法，以提高学习效率。我们在MuJoCo和Box2D任务上进行了大量实验，验证了作为一个通用框架，BiERL优于各种基线方法，并且持续改进。

    Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves 
    
[^16]: 提升现代工业推荐系统的方法论

    Methodologies for Improving Modern Industrial Recommender Systems. (arXiv:2308.01204v1 [cs.IR])

    [http://arxiv.org/abs/2308.01204](http://arxiv.org/abs/2308.01204)

    本文探讨了提高现代工业推荐系统的方法论，为经验丰富的RS工程师提供实用经验，可能适用于其他RS。

    

    推荐系统（RS）是一项成熟的技术，已成功应用于社交媒体、电子商务、娱乐等领域。RS确实是许多流行APP（如YouTube、Tik Tok、小红书、哔哩哔哩等）成功的关键。本文探讨了提高现代工业RS的方法论，旨在为经验丰富的RS工程师提供帮助，他们正在努力改善关键性能指标，如用户留存和使用时长。本文中分享的经验已经在一些真实的工业RS上进行了测试，并且可能适用于其他RS。大部分内容都基于业界经验，没有公开可用的参考资料。

    Recommender system (RS) is an established technology with successful applications in social media, e-commerce, entertainment, and more. RSs are indeed key to the success of many popular APPs, such as YouTube, Tik Tok, Xiaohongshu, Bilibili, and others. This paper explores the methodology for improving modern industrial RSs. It is written for experienced RS engineers who are diligently working to improve their key performance indicators, such as retention and duration. The experiences shared in this paper have been tested in some real industrial RSs and are likely to be generalized to other RSs as well. Most contents in this paper are industry experience without publicly available references.
    
[^17]: GNN4FR:一种无损基于图神经网络的联邦推荐框架

    GNN4FR: A Lossless GNN-based Federated Recommendation Framework. (arXiv:2308.01197v1 [cs.IR])

    [http://arxiv.org/abs/2308.01197](http://arxiv.org/abs/2308.01197)

    这篇论文设计了一种基于无损图神经网络（GNN）的联邦推荐框架，可以实现全图训练，捕捉高阶结构信息，并在保护用户隐私的前提下有效推荐。

    

    图神经网络（GNN）由于能够捕捉用户和物品节点之间的高阶结构信息，在推荐系统中受到广泛关注。然而，这些方法需要收集用户和相应物品之间的个人交互数据，并在中心服务器中对其进行建模，这可能违反GDPR等隐私法律。至今没有现有的工作可以构建一个全局图，同时又不泄露每个用户的私人交互数据（即他或她的子图）。本文首次设计了一种基于GNN的新型无损联邦推荐框架，实现了具有完整高阶结构信息的全图训练，使训练过程等价于相应的非联邦对应物。此外，我们使用LightGCN来实例化我们的框架，并展示其等价性。

    Graph neural networks (GNNs) have gained wide popularity in recommender systems due to their capability to capture higher-order structure information among the nodes of users and items. However, these methods need to collect personal interaction data between a user and the corresponding items and then model them in a central server, which would break the privacy laws such as GDPR. So far, no existing work can construct a global graph without leaking each user's private interaction data (i.e., his or her subgraph). In this paper, we are the first to design a novel lossless federated recommendation framework based on GNN, which achieves full-graph training with complete high-order structure information, enabling the training process to be equivalent to the corresponding un-federated counterpart. In addition, we use LightGCN to instantiate an example of our framework and show its equivalence.
    
[^18]: 可持续透明的推荐系统: 用于解释性的贝叶斯图像排名

    Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])

    [http://arxiv.org/abs/2308.01196](http://arxiv.org/abs/2308.01196)

    这项研究旨在实现推荐系统的可持续透明性，并提出了一种使用贝叶斯排名图像进行个性化解释的方法，以最大化透明度和用户信任。

    

    推荐系统在现代世界中变得至关重要，通常指导用户找到相关的内容或产品，并对用户和公民的决策产生重大影响。然而，确保这些系统的透明度和用户信任仍然是一个挑战；个性化解释已经成为一个解决方案，为推荐提供理由。在生成个性化解释的现有方法中，使用用户创建的视觉内容是一个特别有潜力的选项，有潜力最大化透明度和用户信任。然而，现有模型在这个背景下解释推荐时存在一些限制：可持续性是一个关键问题，因为它们经常需要大量的计算资源，导致的碳排放量与它们被整合到推荐系统中相当。此外，大多数模型使用的替代学习目标与排名最有效的目标不一致。

    Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effect
    
[^19]: 个性化的“再次购买”推荐中的类别频率预测

    Personalized Category Frequency prediction for Buy It Again recommendations. (arXiv:2308.01195v1 [cs.IR])

    [http://arxiv.org/abs/2308.01195](http://arxiv.org/abs/2308.01195)

    该论文提出了一种个性化的推荐系统，用于根据客户的重复购买模式预测再次购买的类别和商品。采用层次化PCIC模型，通过生存模型和时间序列模型捕捉消费行为和趋势，并使用这些特征训练类别粒度的神经网络。

    

    “再次购买”（BIA）推荐对于零售商来说至关重要，通过根据客户自己的重复购买模式提供可能再次购买的商品推荐，以改善用户体验和网站参与度。大多数现有的BIA研究分析了客户在商品粒度上的个性化行为。在这种情况下，基于类别的模型可能更合适。我们提出了一种名为“层次化PCIC模型”的推荐系统，它包括了个性化类别模型（PC模型）和类别内个性化商品模型（IC模型）。PC模型生成了一个个性化的类别列表，显示了客户可能再次购买的类别。IC模型在类别内对商品进行排名，显示了客户在类别内可能消费的商品。层次化PCIC模型使用生存模型捕捉产品的一般消费率。时间序列模型捕捉了消费趋势。从这些模型中提取的特征被用来训练一个基于类别的神经网络。

    Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We 
    
[^20]: 通过部分标签先验的隐式判别逼近进行生成噪声标签学习

    Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior. (arXiv:2308.01184v1 [cs.CV])

    [http://arxiv.org/abs/2308.01184](http://arxiv.org/abs/2308.01184)

    本文提出了一种新的生成噪声标签学习方法，直接关联数据和干净标签，通过使用判别的近似方法来隐式估计生成模型，解决了传统方法中的复杂公式、难以训练的生成模型和无信息先验的问题。

    

    对于带有噪声标签的学习问题，已经使用了判别模型和生成模型进行研究。尽管判别模型由于其简单的建模和更高效的计算训练过程而在该领域占主导地位，但生成模型能够更有效地分解干净和噪声标签，并改善标签转换矩阵的估计。然而，生成方法使用了复杂的公式来最大化噪声标签和数据的联合似然，这只间接优化了与数据和干净标签相关的感兴趣的模型。此外，这些方法依赖于很难训练的生成模型，并倾向于使用无信息的干净标签先验。在本文中，我们提出了一个新的生成噪声标签学习方法来解决这三个问题。首先，我们提出了一种新的模型优化方法，直接关联数据和干净标签。其次，通过使用判别的近似方法来隐式估计生成模型。

    The learning with noisy labels has been addressed with both discriminative and generative models. Although discriminative models have dominated the field due to their simpler modeling and more efficient computational training processes, generative models offer a more effective means of disentangling clean and noisy labels and improving the estimation of the label transition matrix. However, generative approaches maximize the joint likelihood of noisy labels and data using a complex formulation that only indirectly optimizes the model of interest associating data and clean labels. Additionally, these approaches rely on generative models that are challenging to train and tend to use uninformative clean label priors. In this paper, we propose a new generative noisy-label learning approach that addresses these three issues. First, we propose a new model optimisation that directly associates data and clean labels. Second, the generative model is implicitly estimated using a discriminative m
    
[^21]: 直接梯度时间差分学习

    Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])

    [http://arxiv.org/abs/2308.01170](http://arxiv.org/abs/2308.01170)

    本文提出了直接梯度时间差分学习的方法，通过使用马尔可夫数据流中的两个样本来解决双重取样问题，去除了传统方法中的额外权重，保证了计算效率，并提供了收敛性的分析。

    

    脱机学习使强化学习（RL）代理能够反事实地推理未执行的策略，是强化学习中最重要的思想之一。然而，当与函数逼近和自举这两个在大规模强化学习中不可或缺的因素结合时，会导致不稳定性。这就是臭名昭著的致命三元组。梯度时间差分（GTD）是解决这个致命三元组的一种强大工具。它的成功是通过使用权重复制或Fenchel对偶间接解决双重取样问题而实现的。在这篇论文中，我们提出了一种直接方法来解决双重取样问题，只需在逐渐增加的马尔可夫数据流中使用两个样本。所得到的算法与GTD一样计算效率高，但摒弃了GTD的额外权重。我们所付出的唯一代价是随着时间的推移，内存呈对数增长。我们提供了渐近和有限样本分析，其中收敛性可以得到保证。

    Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver
    
[^22]: LLMs理解玻璃盒模型，发现惊喜并提出修复建议。

    LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])

    [http://arxiv.org/abs/2308.01157](http://arxiv.org/abs/2308.01157)

    LLMs在处理可解释模型方面表现出色，提供了全面的模型级总结和自动化的异常检测、原因描述和修复建议。在医疗保健领域使用广义可加模型作为示例，同时介绍了开源的LLM-GAM接口包$\texttt{TalkToEBM}$。

    

    我们展示了大型语言模型(LLMs)在处理可解释模型方面的出色表现，这些模型可以将复杂结果分解为单一变量的图表示组件。通过采用层次推理的方法，LLMs能够在不需要整个模型适应上下文的情况下提供全面的模型级总结。这种方法使LLMs能够应用其广泛的背景知识来自动完成数据科学中的常见任务，如检测与先前知识相矛盾的异常，描述异常的潜在原因，并提出去除异常的修复建议。我们使用医疗保健领域的多个示例来证明LLMs的这些新能力的实用性，特别强调广义可加模型(GAMs)。最后，我们将$\texttt{TalkToEBM}$包作为一个开源的LLM-GAM接口进行介绍。

    We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
    
[^23]: DySTreSS: 自监督对比学习中的动态温度调整

    DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])

    [http://arxiv.org/abs/2308.01140](http://arxiv.org/abs/2308.01140)

    本研究提出了一种余弦相似度依赖的温度调整函数，用于改进自监督对比学习中的性能，并优化了样本在特征空间中的分布。

    

    在当代的自监督对比算法中，诸如SimCLR、MoCo等，平衡两个语义相似样本之间的吸引力和两个不同类别样本之间的排斥力的任务主要受到硬负样本的影响。虽然已经证明InfoNCE损失可以根据困难程度施加惩罚，但温度超参数是调节惩罚和均匀性与容忍度之间权衡的关键。在这项工作中，我们着眼于改进SSL中InfoNCE损失的性能，研究温度超参数值的影响。我们提出了一种余弦相似度依赖的温度调整函数，以有效优化特征空间中样本的分布。我们进一步分析均匀性和容忍度度量，以研究余弦相似度空间中更好优化的最佳区域。此外，我们还对局部和全局行为进行了全面的研究。

    In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and g
    
[^24]: 动态隐私分配用于具有复合目标的本地差分隐私联邦学习

    Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives. (arXiv:2308.01139v1 [cs.LG])

    [http://arxiv.org/abs/2308.01139](http://arxiv.org/abs/2308.01139)

    本文提出了一种动态隐私分配的本地差分隐私联邦学习算法，可以保护梯度并最小化优化错误，同时不需要调整迭代次数。

    

    本文提出了一种针对强凸但可能非平滑问题的本地差分隐私联邦学习算法，用于保护每个工作节点的梯度，以免受诚实但好奇的服务器的泄漏。该算法在共享信息中添加人工噪声以确保隐私，并动态分配时变的噪声方差，以最小化优化错误的上界，同时符合预定义的隐私预算限制。这使得可以通过任意大的但有限次数的迭代实现隐私保护和效用，并消除了调整迭代次数的需求。数值结果显示，该算法优于现有方法。

    This paper proposes a locally differentially private federated learning algorithm for strongly convex but possibly nonsmooth problems that protects the gradients of each worker against an honest but curious server. The proposed algorithm adds artificial noise to the shared information to ensure privacy and dynamically allocates the time-varying noise variance to minimize an upper bound of the optimization error subject to a predefined privacy budget constraint. This allows for an arbitrarily large but finite number of iterations to achieve both privacy protection and utility up to a neighborhood of the optimal solution, removing the need for tuning the number of iterations. Numerical results show the superiority of the proposed algorithm over state-of-the-art methods.
    
[^25]: 能否转移噪声模式？使用生成案例的多环境频谱分析模型

    Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])

    [http://arxiv.org/abs/2308.01138](http://arxiv.org/abs/2308.01138)

    这项研究提出了一个噪声模式转移模型，可以将噪声模式从不同环境的标准样本应用到未知样本，通过生成案例库来解决样本级噪声对数据集级噪声学习的干扰，提高了系统的学习性能。

    

    在在线水质检测中，频谱分析系统旨在检测污染物的类型和浓度，并使监管机构能够及时回应污染事件。然而，基于频谱数据的测试设备在非实验室环境中部署时会受到复杂的噪声模式的影响。为了使分析模型适用于更多的环境，我们提出了一个噪声模式转移模型，该模型将不同环境中标准水样品的频谱作为案例，并学习它们噪声模式的差异，从而使噪声模式能够应用于未知样品。不幸的是，必然存在的样本级基线噪声使得模型无法获取只在数据集级环境噪声上有差异的配对数据。为了解决这个问题，我们生成了一个样本对样本的案例库，排除了样本级噪声对数据集级噪声学习的干扰，提高了系统的学习性能。

    Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on sp
    
[^26]: 多任务学习用于胸部CT扫描的分类、分割、重建和检测

    Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans. (arXiv:2308.01137v1 [eess.IV])

    [http://arxiv.org/abs/2308.01137](http://arxiv.org/abs/2308.01137)

    本文提出了一种多任务学习框架，应用于胸部CT扫描的分类、分割、重建和检测。该方法可以从少量医疗数据中提取重要特征，如病变，以更好地概括和识别早期疾病。

    

    肺癌和COVID-19在世界上具有最高的患病率和死亡率之一。对于医生来说，早期疾病的病变识别困难且耗时。因此，多任务学习是一种从少量医疗数据中提取重要特征（如病变）的方法，因为它学习更好地概括。我们提出了一种新颖的多任务框架，用于分类、分割、重建和检测。据我们所知，我们是第一个将检测添加到多任务解决方案中的人。此外，我们在分割任务中检查了使用两个不同的骨干网络和不同的损失函数的可能性。

    Lung cancer and covid-19 have one of the highest morbidity and mortality rates in the world. For physicians, the identification of lesions is difficult in the early stages of the disease and time-consuming. Therefore, multi-task learning is an approach to extracting important features, such as lesions, from small amounts of medical data because it learns to generalize better. We propose a novel multi-task framework for classification, segmentation, reconstruction, and detection. To the best of our knowledge, we are the first ones who added detection to the multi-task solution. Additionally, we checked the possibility of using two different backbones and different loss functions in the segmentation task.
    
[^27]: 在胸部X射线分类中消除错误相关性

    Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v1 [eess.IV])

    [http://arxiv.org/abs/2308.01119](http://arxiv.org/abs/2308.01119)

    本论文提出了一种深度学习方法（XBL），通过利用模型解释来交互式地消除胸部X射线分类中的错误相关性。该方法可以帮助解决多数据源引入的混淆因素，提高模型的准确性和透明度。

    

    医学图像分类模型通常使用来自多个数据源的训练数据集进行训练。虽然利用多个数据源对于实现模型的泛化至关重要，但必须承认，这些数据源的多样性本质上引入了意外的混淆因素和其他可能影响模型准确性和透明度的挑战。在医学图像分类中一个显著的混淆因素，特别是在肌骨图像分类中，是在青春期观察到的骨骼成熟引起的骨骼生长。我们使用COVID-19胸部X射线数据集训练了一个深度学习模型，并展示了该数据集由于意外混淆区域可能导致错误相关性。基于解释的学习（XBL）是一种深度学习方法，通过利用模型解释来交互式地消除错误相关性，使其超越了可解释性。这是通过整合交互式用户反馈、精确的region-of-interest逐渐消褪等方法来实现的。

    Medical image classification models are frequently trained using training datasets derived from multiple data sources. While leveraging multiple data sources is crucial for achieving model generalization, it is important to acknowledge that the diverse nature of these sources inherently introduces unintended confounders and other challenges that can impact both model accuracy and transparency. A notable confounding factor in medical image classification, particularly in musculoskeletal image classification, is skeletal maturation-induced bone growth observed during adolescence. We train a deep learning model using a Covid-19 chest X-ray dataset and we showcase how this dataset can lead to spurious correlations due to unintended confounding regions. eXplanation Based Learning (XBL) is a deep learning approach that goes beyond interpretability by utilizing model explanations to interactively unlearn spurious correlations. This is achieved by integrating interactive user feedback, specifi
    
[^28]: 推荐系统中的流行偏差综述

    A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])

    [http://arxiv.org/abs/2308.01118](http://arxiv.org/abs/2308.01118)

    这篇综述论文讨论了推荐系统中的流行偏差问题，并回顾了现有的方法来检测、量化和减少流行偏差。它同时提供了计算度量的概述和主要技术方法的回顾。

    

    推荐系统以个性化的方式帮助人们找到相关内容。这些系统的一个主要承诺是能够增加目录中较少知名的物品的可见性。然而，现有研究表明，在许多情况下，现今的推荐算法反而表现出流行偏差，即它们在推荐中经常关注相当流行的物品。这种偏差不仅可能导致短期内对消费者和提供者的推荐价值有限，而且还可能引起不希望的强化效应。在本文中，我们讨论了流行偏差的潜在原因，并回顾了现有的检测、量化和减少推荐系统中流行偏差的方法。因此，我们的综述既包括了文献中使用的计算度量的概述，也包括了减少偏差的主要技术方法的回顾。我们还对这些方法进行了批判性讨论。

    Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
    
[^29]: 利用运动增量进行时空分支的运动预测

    Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])

    [http://arxiv.org/abs/2308.01097](http://arxiv.org/abs/2308.01097)

    本论文提出了一种利用运动增量进行时空分支的运动预测网络，通过解耦时域和空域特征的学习，提取更多的运动信息。

    

    人体运动预测已成为一个热门的研究课题，但由于未来姿势的随机和不规则性质，这仍然是一个具有挑战性的任务。传统方法依赖于手工特征和机器学习技术，往往难以建模人体运动的复杂动力学。最近基于深度学习的方法通过学习运动的时空表示取得了成功，但这些模型常常忽视运动数据的可靠性。此外，骨架节点的时域和空域依赖性是不同的。时域关系捕捉到随时间的运动信息，而空域关系描述了身体结构和不同节点之间的关系。在本文中，我们提出了一种新颖的利用增量信息进行时空分支的运动预测网络，它解耦了时域和空域特征的学习，提取了更多的运动信息。

    Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
    
[^30]: 复杂拓扑场景中的单应矩阵估计

    Homography Estimation in Complex Topological Scenes. (arXiv:2308.01086v1 [cs.CV])

    [http://arxiv.org/abs/2308.01086](http://arxiv.org/abs/2308.01086)

    文章提出了一种在复杂拓扑场景中自动估计单应矩阵的方法，通过利用基于字典的方法实现自动相机校准，以及引入一种新颖的拓扑损失函数，可在不需要先验知识的情况下提高IoU度量指标。

    

    监控视频和图像被广泛用于各种应用，从交通分析到犯罪检测。对于大多数分析应用，外部相机校准数据都是重要的。然而，安全摄像头容易受到环境条件和小型相机移动的影响，因此需要一种自动重新校准的方法来应对这些不同的情况。在本文中，我们提出了一种利用基于字典的方法来实现的自动相机校准过程，该方法不需要任何摄像机设置的先验知识。该方法包括定制的空间转换网络（STN）的实现和一种新颖的拓扑损失函数。实验结果表明，相对于现有技术模型，所提出的方法在五个综合数据集和2014年世界杯数据集上能够提高IoU度量指标高达12%。

    Surveillance videos and images are used for a broad set of applications, ranging from traffic analysis to crime detection. Extrinsic camera calibration data is important for most analysis applications. However, security cameras are susceptible to environmental conditions and small camera movements, resulting in a need for an automated re-calibration method that can account for these varying conditions. In this paper, we present an automated camera-calibration process leveraging a dictionary-based approach that does not require prior knowledge on any camera settings. The method consists of a custom implementation of a Spatial Transformer Network (STN) and a novel topological loss function. Experiments reveal that the proposed method improves the IoU metric by up to 12% w.r.t. a state-of-the-art model across five synthetic datasets and the World Cup 2014 dataset.
    
[^31]: 基于数据的非线性哈密顿系统二次辛表示的数据驱动识别

    Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems. (arXiv:2308.01084v1 [cs.LG])

    [http://arxiv.org/abs/2308.01084](http://arxiv.org/abs/2308.01084)

    本论文提出了一种基于数据的方法来学习哈密顿系统，利用提升假设，通过强制实施哈密顿结构和使用辛自编码器，我们实现了在变换的坐标系下具有哈密顿结构的二次动力学，保持系统的长期稳定性和较低的模型复杂度。

    

    我们提出了一个利用数据学习哈密顿系统的框架。这项工作基于提升假设，即非线性哈密顿系统可以写成具有立方哈密顿量的非线性系统。通过利用这一点，我们得到在变换的坐标系下具有哈密顿结构的二次动力学。为了达到这个目标，对于给定的广义位置和动量数据，我们提出了一种学习二次动力学系统的方法，结合辛自编码器强制实施哈密顿结构。强制实施的哈密顿结构表现出系统的长期稳定性，而立方哈密顿函数提供了相对较低的模型复杂性。对于低维数据，我们确定了一个高阶变换的坐标系，而对于高维数据，我们找到了一个具有所需特性的低阶坐标系。我们通过低维和高维的非线性哈密顿系统示例展示了所提出的方法。

    We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonia
    
[^32]: 基于深度学习的实用键盘声学侧信道攻击

    A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards. (arXiv:2308.01074v1 [cs.CR])

    [http://arxiv.org/abs/2308.01074](http://arxiv.org/abs/2308.01074)

    本文介绍了一种基于深度学习的实用键盘声学侧信道攻击方法，通过使用集成在智能手机麦克风上的模型对笔记本电脑的按键进行分类，以及使用Zoom软件记录的按键进行训练，实现了高准确率的键盘分类。这些结果证明了使用现成设备和算法进行这种侧信道攻击的可行性。本文还讨论了一系列缓解方法来保护用户免受这些攻击的影响。

    

    随着深度学习的不断发展，麦克风的普及以及个人设备上在线服务的兴起，声学侧信道攻击对键盘构成了比以往更大的威胁。本文通过在智能手机集成的麦克风上实现了一种先进的深度学习模型，用于分类笔记本电脑的按键。当使用附近手机记录的按键进行训练时，分类器的准确率达到了95%，是在不使用语言模型的情况下所见到的最高准确率。当使用视频会议软件Zoom记录的按键进行训练时，准确率达到了93%，为该媒介的新纪录。我们的结果证明了通过现成设备和算法进行这些侧信道攻击的实用性。我们讨论了一系列缓解方法，以保护用户免受这些攻击的影响。

    With recent developments in deep learning, the ubiquity of micro-phones and the rise in online services via personal devices, acoustic side channel attacks present a greater threat to keyboards than ever. This paper presents a practical implementation of a state-of-the-art deep learning model in order to classify laptop keystrokes, using a smartphone integrated microphone. When trained on keystrokes recorded by a nearby phone, the classifier achieved an accuracy of 95%, the highest accuracy seen without the use of a language model. When trained on keystrokes recorded using the video-conferencing software Zoom, an accuracy of 93% was achieved, a new best for the medium. Our results prove the practicality of these side channel attacks via off-the-shelf equipment and algorithms. We discuss a series of mitigation methods to protect users against these series of attacks.
    
[^33]: 时间序列分类的自动特征工程: 评估与讨论

    Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion. (arXiv:2308.01071v1 [cs.LG])

    [http://arxiv.org/abs/2308.01071](http://arxiv.org/abs/2308.01071)

    这篇论文评估了时间序列分类中的自动特征工程工具，并与最先进的算法进行了比较，填补了这一领域的研究空白。

    

    时间序列分类（TSC）在过去二十年中受到了很多关注，并且仍然是数据科学和知识工程中的一个关键且具有挑战性的问题。随着时间序列数据的不断增加，研究界在文献中提出了许多TSC算法。除了基于相似性度量、区间、形状特征、字典、深度学习方法或者混合集成方法的最先进方法外，近年来还设计了一些用于从时间序列中提取无监督信息摘要统计特征的工具。这些工具最初设计用于时间序列的描述性分析和可视化，具有信息丰富且可解释的特征，但很少有这些特征工程工具在TSC问题上进行了基准测试，并与最先进的TSC算法在预测性能方面进行了比较。在本文中，我们旨在填补这一空白，并提出了一个简单的TSC过程来评估这些工具在预测性能方面与最先进的TSC算法的比较。

    Time Series Classification (TSC) has received much attention in the past two decades and is still a crucial and challenging problem in data science and knowledge engineering. Indeed, along with the increasing availability of time series data, many TSC algorithms have been suggested by the research community in the literature. Besides state-of-the-art methods based on similarity measures, intervals, shapelets, dictionaries, deep learning methods or hybrid ensemble methods, several tools for extracting unsupervised informative summary statistics, aka features, from time series have been designed in the recent years. Originally designed for descriptive analysis and visualization of time series with informative and interpretable features, very few of these feature engineering tools have been benchmarked for TSC problems and compared with state-of-the-art TSC algorithms in terms of predictive performance. In this article, we aim at filling this gap and propose a simple TSC process to evalua
    
[^34]: 当解析式微积分破解AdaBoost密码时

    When Analytic Calculus Cracks AdaBoost Code. (arXiv:2308.01070v1 [cs.LG])

    [http://arxiv.org/abs/2308.01070](http://arxiv.org/abs/2308.01070)

    本文表明AdaBoost只是一种名义上的算法，因为可以使用真值表明确地计算得到弱分类器的组合。

    

    监督式学习中的增强原理涉及将多个弱分类器组合以获得一个更强的分类器。AdaBoost被认为是这种方法的一个完美例子。我们之前已经证明了AdaBoost并不真正是一个优化算法。本文表明，AdaBoost只是一种名义上的算法，因为可以使用真值表明确地计算得到弱分类器的组合。本研究通过考虑一个两类问题来进行，以三个二元分类器的特殊情况为例，并与Python库scikit-learn中AdaBoost算法的实现结果进行对比。

    The principle of boosting in supervised learning involves combining multiple weak classifiers to obtain a stronger classifier. AdaBoost has the reputation to be a perfect example of this approach. We have previously shown that AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost is an algorithm in name only, as the resulting combination of weak classifiers can be explicitly calculated using a truth table. This study is carried out by considering a problem with two classes and is illustrated by the particular case of three binary classifiers and presents results in comparison with those from the implementation of AdaBoost algorithm of the Python library scikit-learn.
    
[^35]: 在组级别的图异常检测中，一种增强拓扑模式的无监督方法

    Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach. (arXiv:2308.01063v1 [cs.LG])

    [http://arxiv.org/abs/2308.01063](http://arxiv.org/abs/2308.01063)

    提出一种新的无监督框架，用于在图中检测组级别的异常，并利用拓扑模式增强了算法的性能。

    

    图异常检测（GAD）已经取得成功，并广泛应用于欺诈检测、网络安全、金融安全和生物化学等领域。然而，现有的图异常检测算法主要关注区分图中的个体实体（节点或图），忽视了图中可能存在的异常组的可能性。为了解决这个限制，本文提出了一种新的无监督框架，用于名为组级图异常检测（Gr-GAD）的新任务。该提议的框架首先使用图自动编码器的变种来定位属于可能异常组的锚定节点，通过捕捉长程不一致性。随后，组采样被用来采样候选组，然后将其输入到所提出的基于拓扑模式的图对比学习（TPGCL）方法中。TPGCL利用组的拓扑模式作为线索为每个候选组生成嵌入，从而区分不同的异常组。

    Graph anomaly detection (GAD) has achieved success and has been widely applied in various domains, such as fraud detection, cybersecurity, finance security, and biochemistry. However, existing graph anomaly detection algorithms focus on distinguishing individual entities (nodes or graphs) and overlook the possibility of anomalous groups within the graph. To address this limitation, this paper introduces a novel unsupervised framework for a new task called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that belong to potential anomaly groups by capturing long-range inconsistencies. Subsequently, group sampling is employed to sample candidate groups, which are then fed into the proposed Topology Pattern-based Graph Contrastive Learning (TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to generate embeddings for each candidate group and thus distinct anomaly groups. The ex
    
[^36]: 使用全射序列神经似然估计进行基于仿真的推断

    Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])

    [http://arxiv.org/abs/2308.01054](http://arxiv.org/abs/2308.01054)

    我们提出了一种使用全射序列神经似然估计（SSNL）进行基于仿真的推断的新方法，在模型中无法计算似然函数并且只能使用模拟器生成数据的情况下，SSNL通过拟合降维的全射归一化流模型，并将其作为替代似然函数，解决了先前基于似然方法在高维数据集中遇到的问题，并在各种实验中展示了其优越性能。

    

    我们提出了全射序列神经似然（SSNL）估计方法，这是一种在模型中无法计算似然函数并且只能使用可以生成合成数据的模拟器时进行基于仿真的推断的新方法。SSNL拟合一个降维的全射归一化流模型，并将其用作替代似然函数，从而可以使用传统的贝叶斯推断方法，包括马尔科夫链蒙特卡罗方法或变分推断。通过将数据嵌入到低维空间中，SSNL解决了先前基于似然方法在应用于高维数据集时遇到的几个问题，例如包含无信息数据维度或位于较低维流形上的数据。我们对SSNL在各种实验中进行了评估，并表明它通常优于在基于仿真推断中使用的现代方法，例如在一项来自天体物理学的具有挑战性的真实世界例子上对磁场模型的建模。

    We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic fi
    
[^37]: 对自动驾驶车辆风险评估的反事实安全边界视角

    A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])

    [http://arxiv.org/abs/2308.01050](http://arxiv.org/abs/2308.01050)

    本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。

    

    自动驾驶车辆（AVs）有潜力提供诸多社会效益，如减少道路事故和提高交通效率。然而，由于缺乏历史数据和技术的快速发展，量化AVs的风险是具有挑战性的。本文提出了一个基于数据驱动的框架，用于比较不同AVs在各种操作设计领域（ODDs）中行为的风险，该框架基于对“不良”道路用户进行反事实模拟。我们引入了反事实安全边界的概念，表示可能导致碰撞的最小偏离正常行为的量。该概念有助于找到最关键的情景，同时也有助于评估AVs的风险频率和严重程度。我们证明，即使AV的行为策略是未知的，提出的方法仍然适用于最坏和最佳情况分析，使该方法对外部第三方风险评估机构也有用。

    Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
    
[^38]: 计算不平衡分布之间的距离 - 平坦度量

    Computing the Distance between unbalanced Distributions -- The flat Metric. (arXiv:2308.01039v1 [cs.LG])

    [http://arxiv.org/abs/2308.01039](http://arxiv.org/abs/2308.01039)

    该论文提出了一种计算不平衡分布之间距离的方法，基于平坦度量，该方法可以推广到分布总质量不平衡的情况下，特别适用于不平衡最优输运和数据分布分析。论文实现了一个基于神经网络的方法，可以计算出两个给定测度之间距离的最佳测试函数，并通过多个实验验证了方法的准确性。

    

    我们提供了一个在任意维度计算平坦度量的实现。平坦度量，也称为双边界Lipschitz距离，将众所周知的Wasserstein距离W1推广到了分布总质量不平衡的情况。这对于不平衡最优输运任务和数据分布分析中，样本大小重要或者归一化不可能的情况具有特殊的意义。该方法的核心是基于神经网络来确定实现两个给定测度之间距离的最佳测试函数。我们特别注重实现了从独立训练的网络计算出的成对距离的可比性。我们通过几个实验证明了输出的质量，其中包括了一些有实际真值的实验以及使用模拟数据的实验。

    We provide an implementation to compute the flat metric in any dimension. The flat metric, also called dual bounded Lipschitz distance, generalizes the well-known Wasserstein distance W1 to the case that the distributions are of unequal total mass. This is of particular interest for unbalanced optimal transport tasks and for the analysis of data distributions where the sample size is important or normalization is not possible. The core of the method is based on a neural network to determine on optimal test function realizing the distance between two given measures. Special focus was put on achieving comparability of pairwise computed distances from independently trained networks. We tested the quality of the output in several experiments where ground truth was available as well as with simulated data.
    
[^39]: 提高离群检测的三个因素

    Three Factors to Improve Out-of-Distribution Detection. (arXiv:2308.01030v1 [cs.LG])

    [http://arxiv.org/abs/2308.01030](http://arxiv.org/abs/2308.01030)

    本论文提出了三个因素来改善离群检测问题。首先，引入自我知识蒸馏损失以提高网络的准确性；其次，在训练过程中采样半困难离群数据以改善离群检测性能；最后，引入新型监督对比学习以同时提高离群检测性能和网络的准确性。通过结合这三个因素，我们的方法在分类和离群检测之间取得了良好的平衡，提高了准确性和离群检测性能。

    

    在离群检测问题中，利用辅助数据作为异常数据进行微调已经显示出令人鼓舞的性能。然而，先前的方法在分类准确性（ACC）和离群检测性能（AUROC、FPR、AUPR）之间存在权衡。为了改善这种权衡，我们做出了三个贡献：（i）引入自我知识蒸馏损失可以增强网络的准确性；（ii）采样半困难离群数据进行训练可以在对准确性影响最小的情况下改善离群检测性能；（iii）引入我们的新型监督对比学习可以同时改善离群检测性能和网络的准确性。通过结合这三个因素，我们的方法通过解决分类和离群检测之间的权衡，提高了准确性和离群检测性能。我们的方法在性能指标上都取得了比以前的方法更好的成绩。

    In the problem of out-of-distribution (OOD) detection, the usage of auxiliary data as outlier data for fine-tuning has demonstrated encouraging performance. However, previous methods have suffered from a trade-off between classification accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve this trade-off, we make three contributions: (i) Incorporating a self-knowledge distillation loss can enhance the accuracy of the network; (ii) Sampling semi-hard outlier data for training can improve OOD detection performance with minimal impact on accuracy; (iii) The introduction of our novel supervised contrastive learning can simultaneously improve OOD detection performance and the accuracy of the network. By incorporating all three factors, our approach enhances both accuracy and OOD detection performance by addressing the trade-off between classification and OOD detection. Our method achieves improvements over previous approaches in both performance metrics.
    
[^40]: 最大化使用非平稳赌博算法的支付路由成功率

    Maximizing Success Rate of Payment Routing using Non-stationary Bandits. (arXiv:2308.01028v1 [cs.LG])

    [http://arxiv.org/abs/2308.01028](http://arxiv.org/abs/2308.01028)

    本文提出了一种使用非平稳赌博算法的支付路由策略，通过系统架构设计和实时实验的验证，成功提高了0.92\%的交易成功率。

    

    本文讨论非平稳多臂赌博方法的系统架构设计和部署，以根据最近的交易历史确定接近最优的支付路由策略。我们提出了一种基于射线的新型路由服务架构，通过最优的扩展赌博式支付路由来处理每秒超过10000次的交易量，并符合Payment Card Industry Data Security Standard (PCI DSS) 的系统设计要求和生态约束。我们首先在自定义模拟器上评估了多个基于赌博算法的支付路由算法的有效性，以对比多个非平稳赌博方法并确定最佳超参数。然后我们在虚拟体育平台Dream11的支付交易系统上进行了实时实验。在实时实验中，我们证明了我们的非平稳赌博算法相比传统方法可以始终提高0.92\%的交易成功率。

    This paper discusses the system architecture design and deployment of non-stationary multi-armed bandit approaches to determine a near-optimal payment routing policy based on the recent history of transactions. We propose a Routing Service architecture using a novel Ray-based implementation for optimally scaling bandit-based payment routing to over 10000 transactions per second, adhering to the system design requirements and ecosystem constraints with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator to benchmark multiple non-stationary bandit approaches and identify the best hyperparameters. We then conducted live experiments on the payment transaction system on a fantasy sports platform Dream11. In the live experiments, we demonstrated that our non-stationary bandit-based algorithm consistently improves the success rate of transactions by 0.92\% compared to the traditio
    
[^41]: 使用Floss增强周期性时间序列的表示学习：一种频域正则化方法

    Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])

    [http://arxiv.org/abs/2308.01011](http://arxiv.org/abs/2308.01011)

    本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。

    

    时间序列分析是各个应用领域的基础任务，深度学习方法在这个领域表现出了非凡的性能。然而，许多现实世界的时间序列数据展现出重要的周期性或准周期性动态，这些动态往往不能被现有的基于深度学习的解决方案充分捕捉到。这导致对感兴趣的基础动态行为的表示不完整。为了解决这个问题，我们提出了一种无监督的方法叫做Floss，它通过自动化地在频域上调整学到的表示来进行正则化。Floss方法首先自动检测时间序列中的主要周期性。然后，它利用周期移位和谱密度相似度度量来学习具有周期一致性的有意义的表示。此外，Floss可以轻松地整合到有监督、半监督和无监督的学习框架中。

    Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
    
[^42]: MDT3D：LiDAR三维物体检测泛化的多数据集训练

    MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization. (arXiv:2308.01000v1 [cs.CV])

    [http://arxiv.org/abs/2308.01000](http://arxiv.org/abs/2308.01000)

    MDT3D方法通过利用多个已注释的源数据集，提高了受监督的LiDAR三维物体检测模型在不同传感器配置的新环境中的鲁棒性。

    

    在单一领域的情况下，受监督的三维物体检测模型在训练数据和测试数据来自同一环境和传感器的情况下显示出越来越好的性能。然而，在实际情况中，目标领域的数据可能无法用于微调或域适应方法。事实上，使用特定点分布的源数据集训练的3D物体检测模型在泛化到未见过的数据集时显示出困难。因此，我们决定利用多个已注释的源数据集中的信息，使用我们的MDT3D方法来增加3D物体检测模型在不同传感器配置的新环境中的鲁棒性。为了解决数据集之间的标注差距，我们使用基于粗标签的新标签映射。此外，我们展示了我们如何在训练过程中处理数据集的混合，并介绍了一种新的跨数据集方法。

    Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-datase
    
[^43]: 利用合成数据解决数据不平衡问题：基于数据角度的基准线

    Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])

    [http://arxiv.org/abs/2308.00994](http://arxiv.org/abs/2308.00994)

    本文提出了一个简单而有效的基准线SYNAuG，利用合成数据来解决数据不平衡问题，并在多个数据集上取得了令人印象深刻的性能，超过了现有方法。

    

    我们生活在一个庞大的数据海洋中，深度神经网络也不例外。然而，这些数据表现出一种固有的不平衡现象。这种不平衡给深度神经网络产生偏见的预测带来了风险，可能导致严重的道德和社会后果。为了应对这些挑战，我们认为利用生成模型是一种有前景的方法，鉴于最近扩散模型在生成高质量图像方面的显著进展。在这项工作中，我们提出了一个简单而有效的基准线SYNAuG，利用合成数据作为解决数据不平衡问题之前使用的任务特定算法的初步步骤。这种直接的方法在CIFAR100-LT、ImageNet100-LT、UTKFace和Waterbird等数据集上取得了令人印象深刻的性能，超过了现有的任务特定方法的性能。虽然我们并不声称我们的方法作为问题的完整解决方案

    We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem
    
[^44]: 基于Wasserstein多样性增强正则化器的层次化强化学习

    Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning. (arXiv:2308.00989v1 [cs.LG])

    [http://arxiv.org/abs/2308.00989](http://arxiv.org/abs/2308.00989)

    本文中，我们提出了一种新的任务无关正则化器WDER，通过增加子策略的多样性来解决层次化强化学习中的退化问题。实验证明，WDER能够提高性能和样本效率，并且不需要修改超参数。

    

    层次化强化学习通过将不同层次的子策略组合起来完成复杂任务。自动发现子策略是一种不依赖于领域知识的生成子策略的有前景的方法。然而，存在方法很难处理的退化问题，这是由于缺乏对多样性的考虑或使用弱正则化器。在本文中，我们提出了一种新的与任务无关的正则化器，称为Wasserstein多样性增强正则化器（WDER），通过最大化动作分布之间的Wasserstein距离来增加子策略的多样性。所提出的WDER可以轻松地融入到现有方法的损失函数中，进一步提高它们的性能。实验结果表明，相比于之前的工作，在不修改超参数的情况下，我们的WDER提高了性能和样本效率，这表明了WDER的适用性和鲁棒性。

    Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.
    
[^45]: 认证的多流程零阶优化

    Certified Multi-Fidelity Zeroth-Order Optimization. (arXiv:2308.00978v1 [cs.LG])

    [http://arxiv.org/abs/2308.00978](http://arxiv.org/abs/2308.00978)

    本文研究了认证的多流程零阶优化问题，提出了MFDOO算法的认证变体，并证明了其具有近似最优的代价复杂度。同时，还考虑了有噪声评估的特殊情况。

    

    我们考虑多流程零阶优化的问题，在这个问题中，可以在不同的近似水平（代价不同）上评估函数$f$，目标是以尽可能低的代价优化$f$。在本文中，我们研究了\emph{认证}算法，它们额外要求输出一个对优化误差的数据驱动上界。我们首先以算法和评估环境之间的极小极大博弈形式来形式化问题。然后，我们提出了MFDOO算法的认证变体，并推导出其在任意Lipschitz函数$f$上的代价复杂度上界。我们还证明了一个依赖于$f$的下界，表明该算法具有近似最优的代价复杂度。最后，我们通过直接示例解决了有噪声（随机）评估的特殊情况。

    We consider the problem of multi-fidelity zeroth-order optimization, where one can evaluate a function $f$ at various approximation levels (of varying costs), and the goal is to optimize $f$ with the cheapest evaluations possible. In this paper, we study \emph{certified} algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function $f$. We also prove an $f$-dependent lower bound showing that this algorithm has a near-optimal cost complexity. We close the paper by addressing the special case of noisy (stochastic) evaluations as a direct example.
    
[^46]: 将同态加密和可信执行技术集成到云中，实现自主和保密的模型优化

    Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud. (arXiv:2308.00963v1 [cs.CR])

    [http://arxiv.org/abs/2308.00963](http://arxiv.org/abs/2308.00963)

    本论文致力于设计一种方案，用于实现在云中自主和保密的模型优化。通过集成同态加密和可信执行环境技术，我们能够保护自主计算的机密性，从而实现在云中进行持续的机器学习流程管理，而不需用户参与。

    

    随着云计算和机器学习的普及，将机器学习流程（包括模型训练和基于模型的推理）外包给云成为一种趋势。通过外包，除了利用云服务提供商提供的广泛和可扩展的资源，如果云服务器能够代表用户自主管理机器学习流程，那么对用户来说也会更具吸引力。当机器学习预期是一个长期持续的过程，并且用户并不总是可用参与时，这个特性尤其突出。由于安全和隐私的考虑，希望自主学习能够保护用户数据和模型的机密性。因此，在本文中，我们旨在设计一种方案，实现在云中自主和保密的模型优化。同态加密和可信执行环境技术可以保护自主计算的机密性，但是...

    With the popularity of cloud computing and machine learning, it has been a trend to outsource machine learning processes (including model training and model-based inference) to cloud. By the outsourcing, other than utilizing the extensive and scalable resource offered by the cloud service provider, it will also be attractive to users if the cloud servers can manage the machine learning processes autonomously on behalf of the users. Such a feature will be especially salient when the machine learning is expected to be a long-term continuous process and the users are not always available to participate. Due to security and privacy concerns, it is also desired that the autonomous learning preserves the confidentiality of users' data and models involved. Hence, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in cloud. Homomorphic encryption and trusted execution environment technology can protect confidentiality for autonomous computation, bu
    
[^47]: 具有差分隐私(分组)结果的因果推断

    Causal Inference with Differentially Private (Clustered) Outcomes. (arXiv:2308.00957v1 [stat.ML])

    [http://arxiv.org/abs/2308.00957](http://arxiv.org/abs/2308.00957)

    本文提出了一种新的差分隐私机制"Cluster-DP"，它在保证隐私的同时利用数据的聚类结构，从而实现了更强的隐私保证和较低的方差，可以用于进行因果分析。

    

    从随机实验中估计因果效应只有在参与者同意透露他们可能敏感的响应时才可行。在确保隐私的许多方法中，标签差分隐私是一种广泛使用的算法隐私保证度量，可以鼓励参与者分享响应而不会面临去匿名化的风险。许多差分隐私机制会向原始数据集中注入噪音来实现这种隐私保证，这会增加大多数统计估计量的方差，使得精确测量因果效应变得困难：从差分隐私数据进行因果分析存在着固有的隐私-方差权衡。为了实现更强隐私保证的较低方差，我们提出了一种新的差分隐私机制"Cluster-DP"，它利用数据的任何给定的聚类结构，同时仍然允许对因果效应进行估计。

    Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending 
    
[^48]: 暗域中的课程引导域自适应

    Curriculum Guided Domain Adaptation in the Dark. (arXiv:2308.00956v1 [cs.CV])

    [http://arxiv.org/abs/2308.00956](http://arxiv.org/abs/2308.00956)

    本文提出了一种暗域中的课程引导域自适应方法，可逐步将黑盒子训练模型适应到未标记的目标域，同时利用干净/噪声数据分割的易难学习特性，并消除了现有方法中的额外阶段和需求。

    

    针对隐私和安全问题，暗域中的域自适应旨在将黑盒子训练模型适应到未标记的目标域，而无需访问任何源数据或源模型参数。当前方法将从源模型得到的目标数据上的噪声预测转化为目标模型，并在使用传统的噪声标签学习算法进行适应之前将目标样本分为干净和噪声样本。然而，这些方法没有利用干净/噪声数据分割的易难学习特性。此外，所有现有方法都不是端到端的，需要单独的微调阶段和初始热身阶段。在这项工作中，我们提出了课程黑盒适应（CABB），它提供了一种逐步引导的适应方法，用于逐步地适应目标域数据。

    Addressing the rising concerns of privacy and security, domain adaptation in the dark aims to adapt a black-box source trained model to an unlabeled target domain without access to any source data or source model parameters. The need for domain adaptation of black-box predictors becomes even more pronounced to protect intellectual property as deep learning based solutions are becoming increasingly commercialized. Current methods distill noisy predictions on the target data obtained from the source model to the target model, and/or separate clean/noisy target samples before adapting using traditional noisy label learning algorithms. However, these methods do not utilize the easy-to-hard learning nature of the clean/noisy data splits. Also, none of the existing methods are end-to-end, and require a separate fine-tuning stage and an initial warmup stage. In this work, we present Curriculum Adaptation for Black-Box (CABB) which provides a curriculum guided adaptation approach to gradually 
    
[^49]: 从稀疏到软性混合专家模型

    From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])

    [http://arxiv.org/abs/2308.00951](http://arxiv.org/abs/2308.00951)

    本文提出了一种Soft MoE模型，它是一种稀疏的、完全可微分的Transformer，通过隐式的软分配和只处理部分标记的方式解决了稀疏模型的训练不稳定性和推理成本高的问题，并在视觉识别任务中取得了比标准Transformer和其他MoE变体更好的性能。

    

    稀疏的专家模型(MoEs)可以在不增加训练或推理成本的情况下扩展模型容量。尽管它们取得了成功，但MoEs存在一些问题：训练不稳定、丢失标记、无法扩展专家数量或无效的微调。在这项工作中，我们提出了Soft MoE，一种完全可微分的稀疏Transformer，解决了这些挑战，并保持了MoEs的优点。Soft MoE通过向每个专家传递所有输入标记的不同加权组合来执行隐式的软分配。与其他MoE作品一样，Soft MoE中的专家只处理一部分（组合的）标记，以在较低的推理成本下实现更大的模型容量。在视觉识别方面，Soft MoE在标准Transformer（ViTs）和流行的MoE变体（Tokens Choice和Experts Choice）中表现出非常好的性能。例如，Soft MoE-Base/16的推理成本比ViT-Huge/14低10.5倍（墙钟时间降低了5.7倍），同时与其性能相当。

    Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its p
    
[^50]: 对超声图像中的病变分割进行分解和耦合的显著性图

    Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images. (arXiv:2308.00947v1 [eess.IV])

    [http://arxiv.org/abs/2308.00947](http://arxiv.org/abs/2308.00947)

    本文提出了一种名为DC-Net的分解耦合网络来应对超声图像中准确病变分割的挑战，通过分解和耦合子网络以及显著性先验融合进行处理，包括区域特征聚合和关系感知表示融合等策略。

    

    超声图像的复杂场景中，相邻组织（即背景）与病变区域（即前景）具有相似的强度甚至包含更丰富的纹理模式，这给准确的病变分割带来了独特的挑战。本文提出了一种被称为DC-Net的分解耦合网络来应对这一挑战，以分离-融合（前景-背景）显著性图的方式进行处理。DC-Net由分解和耦合子网络组成，前者将原始图像初步分解为前景和背景显著性图，然后在显著性先验融合的辅助下，由后者进行准确的分割。耦合子网络涉及三个融合策略方面：1）区域特征聚合（通过编码器中的可微分上下文池化算子）以自适应地保留具有更大感受野的局部上下文细节进行尺寸降低；2）关系感知表示融合。

    Complex scenario of ultrasound image, in which adjacent tissues (i.e., background) share similar intensity with and even contain richer texture patterns than lesion region (i.e., foreground), brings a unique challenge for accurate lesion segmentation. This work presents a decomposition-coupling network, called DC-Net, to deal with this challenge in a (foreground-background) saliency map disentanglement-fusion manner. The DC-Net consists of decomposition and coupling subnets, and the former preliminarily disentangles original image into foreground and background saliency maps, followed by the latter for accurate segmentation under the assistance of saliency prior fusion. The coupling subnet involves three aspects of fusion strategies, including: 1) regional feature aggregation (via differentiable context pooling operator in the encoder) to adaptively preserve local contextual details with the larger receptive field during dimension reduction; 2) relation-aware representation fusion (via
    
[^51]: 关于深度学习在相位恢复中的应用

    On the use of deep learning for phase recovery. (arXiv:2308.00942v1 [physics.optics])

    [http://arxiv.org/abs/2308.00942](http://arxiv.org/abs/2308.00942)

    本文综述了深度学习在相位恢复中的应用，包括从预处理、中处理和后处理三个阶段的支持以及在相位图像处理中的应用。总结了DL在相位恢复中的工作，并展望了如何更好地利用DL提高相位恢复的可靠性和效率。

    

    相位恢复是指从光强测量值中计算出光场的相位。从定量相位成像、相干衍射成像到自适应光学，相位恢复对于重建物体的折射率分布或地形以及校正成像系统的像差是至关重要的。近年来，深度学习（DL）通过深度神经网络的实现为计算成像提供了前所未有的支持，为各种相位恢复问题提供了更高效的解决方案。本文综述了传统的相位恢复方法，然后从预处理、中处理和后处理三个阶段介绍了DL在相位恢复中的支持。同时，我们还回顾了DL在相位图像处理中的应用。最后，我们总结了DL在相位恢复中的工作，并展望了如何更好地利用DL提高相位恢复的可靠性和效率。

    Phase recovery (PR) refers to calculating the phase of the light field from its intensity measurements. As exemplified from quantitative phase imaging and coherent diffraction imaging to adaptive optics, PR is essential for reconstructing the refractive index distribution or topography of an object and correcting the aberration of an imaging system. In recent years, deep learning (DL), often implemented through deep neural networks, has provided unprecedented support for computational imaging, leading to more efficient solutions for various PR problems. In this review, we first briefly introduce conventional methods for PR. Then, we review how DL provides support for PR from the following three stages, namely, pre-processing, in-processing, and post-processing. We also review how DL is used in phase image processing. Finally, we summarize the work in DL for PR and outlook on how to better use DL to improve the reliability and efficiency in PR. Furthermore, we present a live-updating re
    
[^52]: QUANT：一种用于时间序列分类的极简区间方法

    QUANT: A Minimalist Interval Method for Time Series Classification. (arXiv:2308.00928v1 [cs.LG])

    [http://arxiv.org/abs/2308.00928](http://arxiv.org/abs/2308.00928)

    QUANT是一种用于时间序列分类的极简区间方法，可以使用单一的特征（分位数）、固定区间和一个现成的分类器，在标准数据集上实现与现有最准确的区间方法相同的准确率。在UCR存档的扩展数据集上，使用单个CPU核心的总计算时间（训练和推断）不超过15分钟，在时间序列分类中取得了最先进的准确率。

    

    我们展示了在标准一组基准数据集上，使用一种特征类型（分位数）、固定区间和一个现成的分类器，可以实现与现有最准确的区间方法平均准确率相同的准确率。这种区间方法的精简表示了一种快速准确的时间序列分类方法，在UCR存档的扩展数据集中，使用单个CPU核心的总计算时间（训练和推断）不超过15分钟的情况下，实现了最先进的准确率。

    We show that it is possible to achieve the same accuracy, on average, as the most accurate existing interval methods for time series classification on a standard set of benchmark datasets using a single type of feature (quantiles), fixed intervals, and an 'off the shelf' classifier. This distillation of interval-based approaches represents a fast and accurate method for time series classification, achieving state-of-the-art accuracy on the expanded set of 142 datasets in the UCR archive with a total compute time (training and inference) of less than 15 minutes using a single CPU core.
    
[^53]: 不断适应陆地影像中逐渐恶化的天气条件的论文

    Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather. (arXiv:2308.00924v1 [cs.CV])

    [http://arxiv.org/abs/2308.00924](http://arxiv.org/abs/2308.00924)

    本文研究了在空中图像上进行连续适应的问题，特别关注了模型在逐渐恶化的天气条件下的适应能力。在实验中，作者合成了多个数据集模拟逐渐恶化的天气条件，并评估了不同的领域适应模型的性能。研究结果表明，不断适应的约束对模型的性能有重要影响。

    

    领域适应旨在减少模型在训练数据（源域）与使用数据（目标域）之间的领域差异。当一个深度学习模型部署在空中平台上时，它在运行过程中可能会遇到逐渐恶化的天气条件，导致训练数据与实际评估数据之间的领域差距扩大。本文在两个现有的空中图像数据集上从实际图像中合成了两种逐渐恶化的天气条件，生成了四个基准数据集。在连续或测试时间适应的设置下，我们评估了三种领域适应模型：一个基线标准领域适应模型和两个连续领域适应模型。在这种设置下，模型每次只能访问一小部分或一个批次的目标数据，并且适应只发生在一次数据迭代中。不断适应的约束和逐渐恶化的天气条件的结合，对模型的性能有重要影响。

    Domain adaptation (DA) strives to mitigate the domain gap between the source domain where a model is trained, and the target domain where the model is deployed. When a deep learning model is deployed on an aerial platform, it may face gradually degrading weather conditions during operation, leading to widening domain gaps between the training data and the encountered evaluation data. We synthesize two such gradually worsening weather conditions on real images from two existing aerial imagery datasets, generating a total of four benchmark datasets. Under the continual, or test-time adaptation setting, we evaluate three DA models on our datasets: a baseline standard DA model and two continual DA models. In such setting, the models can access only one small portion, or one batch of the target data at a time, and adaptation takes place continually, and over only one epoch of the data. The combination of the constraints of continual adaptation, and gradually deteriorating weather conditions
    
[^54]: 未标记尸体解剖组织的虚拟组织学染色

    Virtual histological staining of unlabeled autopsy tissue. (arXiv:2308.00920v1 [physics.med-ph])

    [http://arxiv.org/abs/2308.00920](http://arxiv.org/abs/2308.00920)

    该研究展示了针对未标记尸体解剖组织进行虚拟染色的方法，通过训练神经网络可以将自荧光图像转换为与传统染色样本相匹配的亮场图像，去除了自溶引起的严重染色伪影。

    

    组织学检查是尸体解剖的关键步骤，然而，传统的组织化学染色方法面临多个挑战，包括因尸体组织延迟固定引起的自溶所致染色质量较差，以及覆盖大面积组织所需的化学染色程序资源密集型，需要大量的人力、成本和时间。在全球卫生危机中，组织病理学服务的可用性有限，这些挑战可能会更加明显，导致组织固定的进一步延迟和更严重的染色伪影。在这里，我们首次展示了对尸体解剖组织的虚拟染色，并表明训练有素的神经网络能够快速将无标记尸体解剖组织切片的自荧光图像转换为与同一样本的血红素和嗪染色（H＆E）相匹配的亮场等效图像，从而消除了自溶导致的严重染色伪影。

    Histological examination is a crucial step in an autopsy; however, the traditional histochemical staining of post-mortem samples faces multiple challenges, including the inferior staining quality due to autolysis caused by delayed fixation of cadaver tissue, as well as the resource-intensive nature of chemical staining procedures covering large tissue areas, which demand substantial labor, cost, and time. These challenges can become more pronounced during global health crises when the availability of histopathology services is limited, resulting in further delays in tissue fixation and more severe staining artifacts. Here, we report the first demonstration of virtual staining of autopsy tissue and show that a trained neural network can rapidly transform autofluorescence images of label-free autopsy tissue sections into brightfield equivalent images that match hematoxylin and eosin (H&E) stained versions of the same samples, eliminating autolysis-induced severe staining artifacts inhere
    
[^55]: VLUCI: 可变参数学习未观测混淆变量进行反事实推断

    VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])

    [http://arxiv.org/abs/2308.00904](http://arxiv.org/abs/2308.00904)

    VLUCI是一个新颖的可变参数学习模型，用于解决反事实推断中的未观测混淆变量的问题。它通过生成未观测混淆变量的后验分布，并构建一个双重变分推断模型来解决因果推断中观测和未观测混淆变量的问题，从而提高反事实推断的准确性。

    

    因果推断在流行病学、医疗保健和经济学等领域中起着重要作用。在观察数据中进行去混淆和反事实预测已经成为因果推断研究中的一个重要问题。虽然现有模型可以处理观察到的混淆变量，但未观测到的混淆变量的存在仍然是一个重大挑战，扭曲了因果推断并影响了反事实结果的准确性。为了解决这个问题，我们提出了一个新颖的可变参数学习模型，用于反事实推断中的未观测混淆变量（VLUCI），它生成了未观测混淆变量的后验分布。VLUCI放松了大多数因果推断方法往往忽视的无混淆假设。通过解耦观察到的混淆变量和未观测到的混淆变量，VLUCI构建了一个双重变分推断模型，以近似未观测混淆变量的分布，这些变量用于推断更准确的反事实结果。对合成和实际数据上进行了大量实验。

    Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
    
[^56]: 用户可控的推荐系统：通过事后和事前解释的反事实方法

    User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations. (arXiv:2308.00894v1 [cs.IR])

    [http://arxiv.org/abs/2308.00894](http://arxiv.org/abs/2308.00894)

    本文提出了一个用户可控的推荐系统，通过提供事后和事前解释，用户可以根据这些解释与系统进行交互，从而定制对系统的控制。评估结果表明，这种系统在提供个性化推荐的同时，提高了用户对系统的满意度和信任。

    

    现代推荐系统利用用户的历史行为来生成个性化推荐。然而，这些系统往往缺乏用户可控性，导致用户满意度和对系统的信任降低。在本文中，我们提出利用可解释的推荐系统的最新进展来提高用户可控性。我们提出了一个用户可控的推荐系统，该系统在统一框架中无缝集成了解释性和可控性。通过通过反事实推理提供事后和事前解释，用户可以通过与这些解释进行交互来定制对系统的控制。此外，我们还介绍和评估了推荐系统中的两个可控性属性：可控性的复杂性和可控性的准确性。在MovieLens和Yelp数据上进行了实验评估。

    Modern recommender systems utilize users' historical behaviors to generate personalized recommendations. However, these systems often lack user controllability, leading to diminished user satisfaction and trust in the systems. Acknowledging the recent advancements in explainable recommender systems that enhance users' understanding of recommendation mechanisms, we propose leveraging these advancements to improve user controllability. In this paper, we present a user-controllable recommender system that seamlessly integrates explainability and controllability within a unified framework. By providing both retrospective and prospective explanations through counterfactual reasoning, users can customize their control over the system by interacting with these explanations.  Furthermore, we introduce and assess two attributes of controllability in recommendation systems: the complexity of controllability and the accuracy of controllability. Experimental evaluations on MovieLens and Yelp datas
    
[^57]: Tango: 重新思考在GPU上对图神经网络训练的量化方法

    Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])

    [http://arxiv.org/abs/2308.00890](http://arxiv.org/abs/2308.00890)

    Tango是一个重新思考在GPU上对图神经网络训练的量化方法的研究，通过引入新的规则来保持准确度，设计并实现量化感知的基本操作和基本操作之间的优化，以加速GNN训练。

    

    图神经网络（GNN）因其在关键的图相关任务中表现出色而越来越受欢迎。虽然量化被广泛用于加速GNN计算，但量化训练面临前所未有的挑战。当前的量化GNN训练系统往往比其全精度对应物有更长的训练时间，原因有二：（一）解决准确度问题导致了过多的开销，（二）没有充分发挥量化所展示的优化潜力。本文介绍了Tango，它重新思考了在GPU上进行图神经网络训练的量化挑战和机会，并做出了三个贡献：首先，我们引入了有效的规则来在量化GNN训练过程中保持准确度。其次，我们设计并实现了量化感知的基本操作和基本操作之间的优化，可以加速GNN训练。最后，我们将Tango与流行的Deep Graph Library（DGL）系统集成，并展示其

    Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its
    
[^58]: 因子图神经网络

    Factor Graph Neural Networks. (arXiv:2308.00887v1 [cs.LG])

    [http://arxiv.org/abs/2308.00887](http://arxiv.org/abs/2308.00887)

    因子图神经网络（FGNN）提供了一种有效地捕捉高阶关系的方法，可以在推理和学习中使用，具有比传统图神经网络更高的表达能力和灵活性。

    

    在最近几年，我们见证了图神经网络（GNN）的激增，其中大多数可以以端到端的方式学习强大的表示，并在许多实际应用中取得了巨大成功。它们与概率图模型（PGM）有相似之处，但摆脱了PGM的某些限制。GNN旨在提供表达学习的有效方法，而不是计算边际或最可能的配置，因此在信息流规则的选择上具有灵活性，同时保持良好的性能。尽管它们取得了成功并带来灵感，但它们缺乏有效的方法来表示和学习变量/节点之间的高阶关系。操作在k元节点上的更具表达能力的高阶GNN需要增加的计算资源以处理高阶张量。我们提出因子图神经网络（FGNN）以有效地捕捉高阶关系以进行推理和学习。

    In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approxima
    
[^59]: 用连续会话中的实时准确评分提高机器学习性能：基于客观骨骼肌疼痛强度预测的试点研究

    Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction. (arXiv:2308.00886v1 [cs.LG])

    [http://arxiv.org/abs/2308.00886](http://arxiv.org/abs/2308.00886)

    本研究提出了一种使用实时准确评分来提高机器学习性能的方法，通过采集实时疼痛评分和内胚层活动数据，在疼痛分类任务中取得了较好的结果。

    

    机器学习（ML）模型训练了主观自我报告评分后，由于实时疼痛体验和后续记录得分之间的显著变异，很难准确地客观分类疼痛。本研究开发了两个设备，用于获取实时连续会话中的疼痛评分和自主神经系统调节的内胚层活动（EDA）数据。实验招募N = 24名受试者，进行了运动后循环阻塞（PECO）伸展引起的不适。受试者数据存储在定制的疼痛平台中，便于提取时域EDA特征和会话中的准确评分。此外，还从每个受试者收集了实验后的视觉模拟量表（VAS）评分。分别使用相应的客观EDA特征结合会话中得分和会话后得分，训练了多层感知器（MLP）和随机森林（RF）等机器学习模型。在10倍交叉验证中，进行了宏观平均功能评估。

    Machine learning (ML) models trained on subjective self-report scores struggle to objectively classify pain accurately due to the significant variance between real-time pain experiences and recorded scores afterwards. This study developed two devices for acquisition of real-time, continuous in-session pain scores and gathering of ANS-modulated endodermal activity (EDA).The experiment recruited N = 24 subjects who underwent a post-exercise circulatory occlusion (PECO) with stretch, inducing discomfort. Subject data were stored in a custom pain platform, facilitating extraction of time-domain EDA features and in-session ground truth scores. Moreover, post-experiment visual analog scale (VAS) scores were collected from each subject. Machine learning models, namely Multi-layer Perceptron (MLP) and Random Forest (RF), were trained using corresponding objective EDA features combined with in-session scores and post-session scores, respectively. Over a 10-fold cross-validation, the macro-avera
    
[^60]: PeRP：通过合作咨询系统实现个性化剩余策略以缓解拥堵

    PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])

    [http://arxiv.org/abs/2308.00864](http://arxiv.org/abs/2308.00864)

    本论文提出了一种基于个性化剩余策略的合作咨询系统PeRP，用于缓解拥堵。该系统通过结构化建模人类驾驶的相似性，并根据驾驶员的特征为其提供行动建议，以减少交通拥堵。

    

    智能驾驶系统可以通过简单的行动来缓解拥堵，从而改善通勤时间和燃油成本等众多社会经济因素。然而，这些系统假设对自动驾驶车队具有精确的控制，因此在实际中存在限制，因为它们未能考虑到人类行为的不确定性。分段常数（PC）策略通过结构建模人类驾驶的相似性来减少交通拥堵，以提供给人类驾驶员遵循的行动建议。然而，PC策略假设所有驾驶员行为相似。为了实现这一目标，我们开发了一个基于PC策略的合作咨询系统，其中包含一种新型的驾驶员特征相关的个性化剩余策略，即PeRP。PeRP建议驾驶员以减少交通拥堵的方式行驶。我们首先使用变分自动编码器无监督地推断驾驶员如何遵循指令的内在特征。然后，通过将策略与驾驶员特征条件化，实现个性化的行动建议。

    Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
    
[^61]: 通过探索随机过程来理解人工神经网络中的激活模式

    Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes. (arXiv:2308.00858v1 [cs.LG])

    [http://arxiv.org/abs/2308.00858](http://arxiv.org/abs/2308.00858)

    该论文提出了利用随机过程框架来研究人工神经网络中的激活模式。通过模拟和实验，研究人员得到了描述每个网络中激活模式的参数。

    

    为了更深入地了解（深层）人工神经网络的行为和学习动态，采用数学抽象和模型是有价值的。这些工具提供了对网络性能的简化视角，并通过模拟促进了系统性的研究。在本文中，我们提出利用迄今为止未充分利用的随机过程框架。我们的方法将（深层）人工神经网络中的阈值节点的激活模式建模为随机过程。我们仅关注激活频率，利用用于真实神经元尖峰信号的神经科学技术。在分类任务中，我们提取尖峰活动并使用符合泊松分布的到达过程。我们在图像识别任务中检查来自各种人工神经网络的观察数据，拟合所提出模型的假设。通过这样做，我们得到了描述每个网络中激活模式的参数。我们的分析

    To gain a deeper understanding of the behavior and learning dynamics of (deep) artificial neural networks, it is valuable to employ mathematical abstractions and models. These tools provide a simplified perspective on network performance and facilitate systematic investigations through simulations. In this paper, we propose utilizing the framework of stochastic processes, which has been underutilized thus far.  Our approach models activation patterns of thresholded nodes in (deep) artificial neural networks as stochastic processes. We focus solely on activation frequency, leveraging neuroscience techniques used for real neuron spike trains. During a classification task, we extract spiking activity and use an arrival process following the Poisson distribution.  We examine observed data from various artificial neural networks in image recognition tasks, fitting the proposed model's assumptions. Through this, we derive parameters describing activation patterns in each network. Our analysi
    
[^62]: 针对联邦肿瘤分割中自适应权重聚合的差分隐私研究

    Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation. (arXiv:2308.00856v1 [cs.LG])

    [http://arxiv.org/abs/2308.00856](http://arxiv.org/abs/2308.00856)

    本研究提出了一种针对联邦肿瘤分割中自适应权重聚合的差分隐私算法，通过扩展相似性权重聚合方法（SimAgg），提高了模型分割能力，并在保护隐私方面做出了额外改进。

    

    联邦学习是一种分布式机器学习方法，通过创建一个公正的全局模型来保护个体客户数据的隐私。然而，传统的联邦学习方法在处理不同客户数据时可能引入安全风险，从而可能危及隐私和数据完整性。为了解决这些挑战，本文提出了一种差分隐私联邦深度学习框架，在医学图像分割中扩展了相似性权重聚合方法（SimAgg）到DP-SimAgg算法，这是一种针对多模态磁共振成像（MRI）中的脑肿瘤分割的差分隐私相似性加权聚合算法。我们的DP-SimAgg方法不仅提高了模型分割能力，还提供了额外的隐私保护层。通过广泛的基准测试和评估，以计算性能为主要考虑因素，证明了DP-SimAgg使..

    Federated Learning (FL) is a distributed machine learning approach that safeguards privacy by creating an impartial global model while respecting the privacy of individual client data. However, the conventional FL method can introduce security risks when dealing with diverse client data, potentially compromising privacy and data integrity. To address these challenges, we present a differential privacy (DP) federated deep learning framework in medical image segmentation. In this paper, we extend our similarity weight aggregation (SimAgg) method to DP-SimAgg algorithm, a differentially private similarity-weighted aggregation algorithm for brain tumor segmentation in multi-modal magnetic resonance imaging (MRI). Our DP-SimAgg method not only enhances model segmentation capabilities but also provides an additional layer of privacy preservation. Extensive benchmarking and evaluation of our framework, with computational performance as a key consideration, demonstrate that DP-SimAgg enables a
    
[^63]: 对过去六十年间具有高引用和重要影响的机器学习研究的全面研究

    A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades. (arXiv:2308.00855v1 [cs.DL])

    [http://arxiv.org/abs/2308.00855](http://arxiv.org/abs/2308.00855)

    对过去六十年间具有高引用和重要影响的机器学习研究进行了全面的分析，揭示了该领域中最具影响力的论文、作者和合作网络，并发现了热门研究主题和最新涌现的主题。

    

    机器学习已经成为计算机科学和其他相关领域中一项重要的研究领域，推动其他感兴趣领域的进步。随着这个领域的不断发展，了解高引用论文的情况至关重要，以确定关键趋势、有影响力的作者以及迄今为止所做出的重要贡献。在本文中，我们对高引用机器学习论文进行了全面的文献计量分析。我们收集了一份数据集，包括从1959年到2022年的多年间内，备受推崇的机器学习会议和期刊的高引用论文。我们采用了各种文献计量技术对数据进行了分析，包括引用分析、合著分析、关键词分析和出版趋势分析。我们的研究结果揭示了机器学习社区中最具影响力的论文、高引用的作者以及合作网络。我们确定了热门研究主题，并揭示了最近崛起的主题。

    Machine learning (ML) has emerged as a prominent field of research in computer science and other related fields, thereby driving advancements in other domains of interest. As the field continues to evolve, it is crucial to understand the landscape of highly cited publications to identify key trends, influential authors, and significant contributions made thus far. In this paper, we present a comprehensive bibliometric analysis of highly cited ML publications. We collected a dataset consisting of the top-cited papers from reputable ML conferences and journals, covering a period of several years from 1959 to 2022. We employed various bibliometric techniques to analyze the data, including citation analysis, co-authorship analysis, keyword analysis, and publication trends. Our findings reveal the most influential papers, highly cited authors, and collaborative networks within the machine learning community. We identify popular research themes and uncover emerging topics that have recently 
    
[^64]: CASSINI：机器学习集群中的网络感知作业调度

    CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters. (arXiv:2308.00852v1 [cs.NI])

    [http://arxiv.org/abs/2308.00852](http://arxiv.org/abs/2308.00852)

    CASSINI是一种用于机器学习集群的网络感知作业调度器，通过引入亲和图的方式实现作业的高效调度，并实现了平均和尾部完成时间的提升，以及ECN标记数据包数量的减少。

    

    我们提出了CASSINI，一种用于机器学习（ML）集群的网络感知作业调度器。CASSINI引入了一种新颖的几何抽象，以考虑不同作业的通信模式，同时将它们放置在网络链路上。为此，CASSINI使用了一个亲和图，找到一系列的时间偏移值来调整一组作业的通信阶段，使得共享同一网络链路的作业的通信模式相互交错。在一个24台服务器的测试环境中使用13种常见的ML模型进行实验，结果表明，与最先进的ML调度器相比，CASSINI将作业的平均和尾部完成时间分别提高了1.6倍和2.5倍。此外，我们还展示了CASSINI将集群中ECN标记的数据包数量减少了最多33倍。

    We present CASSINI, a network-aware job scheduler for machine learning (ML) clusters. CASSINI introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, CASSINI uses an affinity graph that finds a series of time-shift values to adjust the communication phases of a subset of jobs, such that the communication patterns of jobs sharing the same network link are interleaved with each other. Experiments with 13 common ML models on a 24-server testbed demonstrate that compared to the state-of-the-art ML schedulers, CASSINI improves the average and tail completion time of jobs by up to 1.6x and 2.5x, respectively. Moreover, we show that CASSINI reduces the number of ECN marked packets in the cluster by up to 33x.
    
[^65]: 有限分类模型的精确核等价性

    An Exact Kernel Equivalence for Finite Classification Models. (arXiv:2308.00824v1 [cs.LG])

    [http://arxiv.org/abs/2308.00824](http://arxiv.org/abs/2308.00824)

    本研究推导出梯度下降训练的有限分类模型的精确核表示，揭示了神经网络和核方法之间的等价性，并通过实验证明了精确核对神经网络预测的指导作用。

    

    本研究通过推导梯度下降训练的任意有限大小参数分类模型的精确表示，探索神经网络和核方法之间的等价性。我们将我们的精确表示与著名的神经切向核（NTK）进行比较，并讨论相对于NTK和其他非精确路径核公式的近似误差。我们通过实验证明，可以计算出逼真网络的核到机器精度。我们利用这个精确核来展示我们的理论贡献如何为神经网络的预测提供有用的见解，特别是它们的泛化方式。

    We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
    
[^66]: 一种双层优化的介绍：在信号处理和机器学习中的基础和应用

    An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v1 [cs.LG])

    [http://arxiv.org/abs/2308.00788](http://arxiv.org/abs/2308.00788)

    本论文介绍了双层优化在信号处理和机器学习中的基本概念和应用。双层优化是一个经典的优化问题，涉及到两个层次的优化，并在建模问题中展现了强大的能力。它在无线系统资源分配和对抗性机器学习等领域有广泛的应用。

    

    最近，双层优化（BLO）在信号处理和机器学习领域的一些激动人心的发展中占据了中心舞台。粗略地说，BLO是一个经典的优化问题，涉及到两个层次（即上层和下层），其中解决上层问题需要解决下层问题。BLO之所以受到欢迎，在很大程度上是因为它在建模涉及优化嵌套目标函数的SP和ML等问题方面非常强大。BLO的显著应用范围从无线系统的资源分配到对抗性机器学习。在这项工作中，我们重点研究了一类在SP和ML应用中经常出现的可解BLO问题。我们提供了这类BLO问题的一些基本概念的概述，例如它们的最优性条件、标准算法（包括它们的优化原理和实际实现方法），以及它们如何能够被利用。

    Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be levera
    
[^67]: 在神经形态计算平台上评估脉冲神经网络在人类活动识别中的应用

    Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition. (arXiv:2308.00787v1 [cs.NE])

    [http://arxiv.org/abs/2308.00787](http://arxiv.org/abs/2308.00787)

    本文研究评估了在可穿戴设备上应用脉冲神经网络对人类活动识别的效果，并探讨了基于事件的感知和多门限Δ调制对系统性能的影响。

    

    能效和低延迟是设计可穿戴式AI辅助人类活动识别系统的重要要求，由于电池操作和闭环反馈的严格限制。虽然神经网络模型已经被广泛压缩以满足严格的边缘要求，但脉冲神经网络和基于事件的感知最近被认为是进一步提高性能的有希望解决方案，因为它们固有的能效和在非常低延迟下处理时空数据的能力。本研究旨在评估脉冲神经网络在人类活动识别中的有效性，特别是在可穿戴应用中的手腕运动传感器。采用多门限Δ调制方法将输入传感器数据编码为脉冲序列，以将系统转移到基于事件的方法。然后将这些脉冲序列输入到脉冲神经网络中进行处理。

    Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural n
    
[^68]: DYMOND: DYnamic MOtif-NoDes网络生成模型

    DYMOND: DYnamic MOtif-NoDes Network Generative Model. (arXiv:2308.00770v1 [cs.SI])

    [http://arxiv.org/abs/2308.00770](http://arxiv.org/abs/2308.00770)

    DYMOND是一种考虑了动态变化和节点在图案中角色的网络生成模型

    

    动机已经被确定为网络结构的构建模块，超越了成对连接，捕捉了连接和活动中的长程相关性。尽管如此，很少有考虑高阶网络结构的生成图模型，甚至更少的关注在动态图模型中使用动机。大多数现有的用于时间图的生成模型严格通过边缘添加增长网络，并使用静态图结构度量来评估模型--这并不能充分捕捉网络的时间行为。为了解决这些问题，在这项工作中，我们提出了DYnamic MOtif-NoDes (DYMOND)——一种生成模型，考虑了(i)整体图结构的动态变化，使用时间动机活动和(ii)节点在图案中扮演的角色（例如，一个节点在楔形中处于枢纽角色，而其余两个节点扮演从属角色）。我们将DYMOND与三个实际网络上的动态图生成模型基准进行了比较

    Motifs, which have been established as building blocks for network structure, move beyond pair-wise connections to capture longer-range correlations in connections and activity. In spite of this, there are few generative graph models that consider higher-order network structures and even fewer that focus on using motifs in models of dynamic graphs. Most existing generative models for temporal graphs strictly grow the networks via edge addition, and the models are evaluated using static graph structure metrics -- which do not adequately capture the temporal behavior of the network. To address these issues, in this work we propose DYnamic MOtif-NoDes (DYMOND) -- a generative model that considers (i) the dynamic changes in overall graph structure using temporal motif activity and (ii) the roles nodes play in motifs (e.g., one node plays the hub role in a wedge, while the remaining two act as spokes). We compare DYMOND to three dynamic graph generative model baselines on real-world network
    
[^69]: 自监督对比BERT微调用于基于融合的评论项检索

    Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])

    [http://arxiv.org/abs/2308.00762](http://arxiv.org/abs/2308.00762)

    本文介绍了一种扩展神经信息检索方法到评论项检索任务的方法，通过利用自监督对比学习来学习BERT嵌入，以融合查询和评论得分并进行排名。

    

    随着自然语言界面使用户能够表达越来越复杂的自然语言查询，用户评论内容也呈爆炸式增长，这可以使用户更好地找到与这些表达性查询匹配的餐厅、书籍或电影等物品。虽然神经信息检索(IR)方法为查询与文档之间的匹配提供了最先进的结果，但它们尚未扩展到评估项检索(RIR)任务，其中查询-评论得分必须聚合(或融合)成物品级得分进行排名。在没有标记的RIR数据集的情况下，我们通过利用自监督方法对BERT嵌入进行对比学习来将神经IR方法扩展到RIR。具体而言，对比学习需要选择正样本和负样本，而我们的项-评论数据的独特二级结构结合元数据为我们提供了丰富的样本选择结构。

    As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrasti
    
[^70]: 文本到图像生成中的偏见放大悖论

    The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])

    [http://arxiv.org/abs/2308.00755](http://arxiv.org/abs/2308.00755)

    本文研究了文本到图像生成中的偏见放大现象，并发现其主要原因是训练数据和模型提示之间的差异。一旦考虑到各种分布差异，偏见放大现象显著减少。

    

    偏见放大是一种模型增加训练数据中不平衡的现象。本文通过使用稳定扩散来比较训练数据与生成图像中的性别比例，研究了文本到图像领域中的偏见放大现象。我们发现模型似乎放大了训练数据中存在的性别-职业偏见。然而，我们发现放大很大程度上可以归因于训练数据和模型提示之间的差异。例如，训练数据中的标题通常包含明确的性别信息，而我们使用的提示则不包含，这导致了分布的偏移，从而影响了偏见度量。一旦我们考虑到训练和生成时使用的文本之间的各种分布差异，我们观察到放大现象大大减少。我们的发现说明了比较模型和它们所训练的数据中的偏见所面临的挑战，并且强调了混淆因素。

    Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
    
[^71]: 计算机科学研究的映射：趋势，影响和预测

    Mapping Computer Science Research: Trends, Influences, and Predictions. (arXiv:2308.00733v1 [cs.HC])

    [http://arxiv.org/abs/2308.00733](http://arxiv.org/abs/2308.00733)

    本文研究了计算机科学领域当前热门的研究领域，通过分析引用计数等因素，提出了一个数据驱动的方法来预测研究趋势。研究发现引用计数是确定趋势的最相关因素，同时NSF资助和专利对热门话题的影响在逐渐增加。

    

    本文探讨了计算机科学领域当前热门的研究领域，并调查了导致这些领域出现的因素。利用包含论文、引用和资金信息的综合数据集，我们采用了包括决策树和逻辑回归模型在内的先进机器学习技术来预测研究领域的趋势。我们的分析表明，在确定计算机科学领域的研究趋势中，研究论文中引用的参考文献数量（引用计数）起着关键作用，使引用计数成为推动计算机科学领域趋势的最相关因素。此外，NSF资助和专利对热门话题的影响随着时间的推移逐渐增加。逻辑回归模型在预测趋势方面优于决策树模型，表现出更高的准确性，精确率，召回率和F1得分。通过超越随机猜测基线，我们的数据驱动方法在识别研究趋势方面表现出更高的准确性和效力。

    This paper explores the current trending research areas in the field of Computer Science (CS) and investigates the factors contributing to their emergence. Leveraging a comprehensive dataset comprising papers, citations, and funding information, we employ advanced machine learning techniques, including Decision Tree and Logistic Regression models, to predict trending research areas. Our analysis reveals that the number of references cited in research papers (Reference Count) plays a pivotal role in determining trending research areas making reference counts the most relevant factor that drives trend in the CS field. Additionally, the influence of NSF grants and patents on trending topics has increased over time. The Logistic Regression model outperforms the Decision Tree model in predicting trends, exhibiting higher accuracy, precision, recall, and F1 score. By surpassing a random guess baseline, our data-driven approach demonstrates higher accuracy and efficacy in identifying trending
    
[^72]: 潜在漂移:熵梯度有助于神经编解码器

    Latent-Shift: Gradient of Entropy Helps Neural Codecs. (arXiv:2308.00725v1 [eess.IV])

    [http://arxiv.org/abs/2308.00725](http://arxiv.org/abs/2308.00725)

    本文研究了梯度熵与重构误差梯度的相关性，在神经编解码器中利用梯度熵能够实现1-2％的速率节省，这是独立于其他改进方法的。

    

    相比通过几十年的手工工程努力开发的传统压缩技术，端到端图像/视频编解码器变得具有竞争力。这些可训练的编解码器相对传统技术具有许多优势，例如易于适应感知失真度量标准和在特定领域具有高性能的学习能力。然而，现有的神经编解码器没有利用解码器中熵的梯度的存在。本文理论上证明了熵梯度（解码器端可用）与重构误差的梯度（解码器端不可用）之间的相关性。然后，我们通过实验证明，可以在各种压缩方法上使用这个梯度，从而在相同质量下实现1-2％的速率节省。我们的方法与其他改进方法正交，并带来独立的速率节省。

    End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.
    
[^73]: 基于主动学习的预训练数据去重模型

    A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])

    [http://arxiv.org/abs/2308.00721](http://arxiv.org/abs/2308.00721)

    提出了一种基于主动学习的预训练去重模型，将Transformer和主动学习集成到端到端架构中，首次解决了语义级别的去重问题，同时采用R-Drop方法对每一轮标记数据进行数据增强。通过选择最有价值的数据进行去重模型训练，不仅降低了手动标记的成本，还提高了模型的泛化能力。

    

    在大数据时代，数据质量问题日益突出。其中一个主要挑战是重复数据问题，这可能是由于数据的重复输入或多个数据源的合并导致的。这些"脏数据"问题严重限制了大数据的有效应用。为了解决数据去重的问题，我们提出了一种基于主动学习的预训练去重模型，这是首次利用主动学习解决语义级别的去重问题的工作。该模型构建在一个预训练的Transformer上，并通过细调将其应用于序列分类任务，首次将Transformer和主动学习集成到端到端架构中，以选择最有价值的数据进行去重模型训练，同时首次采用R-Drop方法对每一轮标记数据进行数据增强，既能降低手动标记的成本，也能提高模型的泛化能力。

    In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
    
[^74]: ADAM算法在固定步长下的发散：一个(非常)简单的例子

    Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example. (arXiv:2308.00720v1 [cs.LG])

    [http://arxiv.org/abs/2308.00720](http://arxiv.org/abs/2308.00720)

    在没有梯度噪声的情况下，ADAM算法在固定步长时会发散，而不受方法参数选择的影响。

    

    构造了一个非常简单的一维函数，其梯度是利普希茨连续的，当在梯度没有噪声的情况下，从原点开始应用ADAM算法来最小化这个函数时，该算法以恒定的步长发散。不管方法参数如何选择，都会出现发散现象。

    A very simple unidimensional function with Lipschitz continuous gradient is constructed such that the ADAM algorithm with constant stepsize, started from the origin, diverges when applied to minimize this function in the absence of noise on the gradient. Divergence occurs irrespective of the choice of the method parameters.
    
[^75]: 基于机器学习算法的光束检测

    Beam Detection Based on Machine Learning Algorithms. (arXiv:2308.00718v1 [physics.data-an])

    [http://arxiv.org/abs/2308.00718](http://arxiv.org/abs/2308.00718)

    本文介绍了一种基于机器学习算法的光束检测方法，通过自建卷积神经网络和支持向量回归模型的组合进行预测，成功实现了85.8%的准确率。

    

    通过一系列机器学习模型精确确定自由电子激光束在屏幕上的位置。在基于VGG16模型的自建卷积神经网络中进行迁移训练。将中间层的输出作为特征传递给支持向量回归模型。通过这个序列，在测试数据上实现了85.8％的正确预测。

    The positions of free electron laser beams on screens are precisely determined by a sequence of machine learning models. Transfer training is conducted in a self-constructed convolutional neural network based on VGG16 model. Output of intermediate layers are passed as features to a support vector regression model. With this sequence, 85.8% correct prediction is achieved on test data.
    
[^76]: 使用深度卷积神经网络中的多头通道注意力自动COVID-19 CT图像分类

    Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN. (arXiv:2308.00715v1 [eess.IV])

    [http://arxiv.org/abs/2308.00715](http://arxiv.org/abs/2308.00715)

    本文提出了一种使用多头通道注意力机制的深度学习方法，能够自动分类COVID-19 CT图像，并在广泛使用的数据集上展示了96.99%的准确性。

    

    COVID-19的迅速传播需要有效和准确的诊断方法。计算机断层扫描（CT）图像已成为检测该疾病的有价值工具。本文提出了一种新颖的深度学习方法，用于自动化COVID-19 CT扫描分类，其中提出了一个修改版Xception模型，该模型结合了新设计的通道注意力机制和加权全局平均池化，以增强特征提取，从而提高分类准确性。通道注意力模块选择性地关注每个通道中的信息区域，使模型能够学习用于COVID-19检测的判别特征。对广泛使用的COVID-19 CT扫描数据集进行的实验显示出96.99%的非常好的准确性，并展示了其对其他最先进技术的优越性。这项研究可以为使用人工智能抗击当前和未来的流行病的不懈努力做出贡献，提供有希望和及时的解决方案。

    The rapid spread of COVID-19 has necessitated efficient and accurate diagnostic methods. Computed Tomography (CT) scan images have emerged as a valuable tool for detecting the disease. In this article, we present a novel deep learning approach for automated COVID-19 CT scan classification where a modified Xception model is proposed which incorporates a newly designed channel attention mechanism and weighted global average pooling to enhance feature extraction thereby improving classification accuracy. The channel attention module selectively focuses on informative regions within each channel, enabling the model to learn discriminative features for COVID-19 detection. Experiments on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of 96.99% and show its superiority to other state-of-the-art techniques. This research can contribute to the ongoing efforts in using artificial intelligence to combat current and future pandemics and can offer promising and timely solut
    
[^77]: 实现聚合的类激活图可用于分析类特征的整体贡献

    Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features. (arXiv:2308.00710v1 [cs.LG])

    [http://arxiv.org/abs/2308.00710](http://arxiv.org/abs/2308.00710)

    本文扩展了类激活图的方法，将多个样本的类激活图聚合起来，以展示语义结构化数据的分类的全局解释。聚合过程使得分析师可以进行复杂的假设和进一步的钻取可视化分析。

    

    深度学习模型在分类任务中取得了显著的性能，但是高复杂度的模型在许多风险敏感的应用中无法使用，除非提供了可理解的解释。可解释的人工智能（xAI）关注于解释类似深度学习的AI系统的决策。我们扩展了最近的类激活图（CAMs）方法，该方法可可视化数据样本中每个特征对分类的贡献重要性。在本文中，我们聚合了来自多个样本的CAMs，以展示语义结构化数据的分类的全局解释。聚合使分析师能够进行复杂的假设，并通过进一步的钻取可视化进行分析。我们的全局CAM的可视化表示以一个方形标记表示每个特征的影响力，方形的颜色表示该特征的分类影响力，填充方形的大小表示该特征的重要性。

    Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled squ
    
[^78]: DeepTSF：无代码的时间序列预测机器学习运营

    DeepTSF: Codeless machine learning operations for time series forecasting. (arXiv:2308.00709v1 [cs.LG])

    [http://arxiv.org/abs/2308.00709](http://arxiv.org/abs/2308.00709)

    DeepTSF是一个无代码的时间序列预测机器学习运营框架，通过工作流自动化和无代码建模革新了时间序列预测。它提供了强大且用户友好的解决方案，并与现有数据分析工作流程无缝集成，提高了生产力和兼容性。

    

    本论文介绍了DeepTSF，一个综合的机器学习运营（MLOps）框架，旨在通过工作流自动化和无代码建模来革新时间序列预测。DeepTSF自动化了机器学习生命周期的关键方面，使其成为数据科学家和MLops工程师参与基于机器学习（ML）和深度学习（DL）的预测的理想工具。DeepTSF为用户提供了一个强大且用户友好的解决方案，同时设计与现有数据分析工作流程无缝集成，提供增强的生产力和兼容性。该框架提供了适用于数据科学家和其他高级利益相关者的前端用户界面（UI），通过有见地的可视化和评估指标，实现全面的理解。DeepTSF还通过身份管理和访问授权机制优先考虑安全性。在I-NERGY项目的实际应用中，DeepTSF已经证明了其效能。

    This paper presents DeepTSF, a comprehensive machine learning operations (MLOps) framework aiming to innovate time series forecasting through workflow automation and codeless modeling. DeepTSF automates key aspects of the ML lifecycle, making it an ideal tool for data scientists and MLops engineers engaged in machine learning (ML) and deep learning (DL)-based forecasting. DeepTSF empowers users with a robust and user-friendly solution, while it is designed to seamlessly integrate with existing data analysis workflows, providing enhanced productivity and compatibility. The framework offers a front-end user interface (UI) suitable for data scientists, as well as other higher-level stakeholders, enabling comprehensive understanding through insightful visualizations and evaluation metrics. DeepTSF also prioritizes security through identity management and access authorization mechanisms. The application of DeepTSF in real-life use cases of the I-NERGY project has already proven DeepTSF's ef
    
[^79]: VeriGen: 一种用于Verilog代码生成的大型语言模型

    VeriGen: A Large Language Model for Verilog Code Generation. (arXiv:2308.00708v1 [cs.PL])

    [http://arxiv.org/abs/2308.00708](http://arxiv.org/abs/2308.00708)

    VeriGen是一种大型语言模型，用于自动化生成高质量的Verilog代码。它在生成的功能正确性和语法正确性方面优于商业GPT-3.5-turbo模型并且显示出竞争性的性能。

    

    在这项研究中，我们探索了大型语言模型（LLMs）在自动化硬件设计方面的能力，通过生成高质量的Verilog代码，Verilog是一种常用的用于设计和建模数字系统的语言。我们在GitHub和Verilog教材编译的Verilog数据集上对预先存在的LLMs进行了微调。我们使用特殊设计的测试套件评估了生成的Verilog代码的功能正确性，该套件包含自定义问题集和测试工作台。在这里，我们的开源CodeGen-16B模型在功能正确性方面优于商业领先的GPT-3.5-turbo模型，整体提高了1.1％。在对更多样化和复杂的问题集进行测试时，我们发现微调模型在与GPT-3.5-turbo模型相比具有竞争性的性能，在特定场景中表现出色。值得注意的是，与预训练模型相比，它在生成各种问题类别的语法正确Verilog代码方面提高了41％，突显了小型模型的潜力。

    In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high-quality Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of small
    
[^80]: 安全强化学习的近似模型屏蔽

    Approximate Model-Based Shielding for Safe Reinforcement Learning. (arXiv:2308.00707v1 [cs.LG])

    [http://arxiv.org/abs/2308.00707](http://arxiv.org/abs/2308.00707)

    提出了一种近似模型屏蔽算法 (AMBS) 来验证学习的强化学习策略在给定安全约束下的性能，与其他屏蔽方法相比，AMBS不需要先验知识，并在具有状态相关安全标签的 Atari 游戏上展示了优越性能。

    

    强化学习 (RL) 在各个领域解决复杂任务方面展示了巨大的潜力。然而，将 RL 应用于现实世界的安全关键系统并不容易，因为许多算法在样本效率上存在问题，并且最大化标准 RL 目标不能保证最坏情况下的性能。在本文中，我们提出了近似模型屏蔽 (AMBS)，这是一种基于模型的理性前瞻屏蔽算法，用于验证学习的 RL 策略相对于一组给定的安全约束的性能。我们的算法与其他屏蔽方法不同，它不需要对系统的安全相关动态的先验知识。我们为 AMBS 提供了强大的理论基础，并在一组具有状态相关安全标签的 Atari 游戏上展示了优越的性能。

    Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.
    
[^81]: SelfCheck: 使用LLMs自检其逐步推理的创新

    SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])

    [http://arxiv.org/abs/2308.00436](http://arxiv.org/abs/2308.00436)

    本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。

    

    最近大型语言模型（LLMs）的进展，尤其是链式思维（CoT）的发明，使得解决推理问题成为可能。然而，即使最强大的LLMs仍然难以处理需要非线性思维和多步推理的复杂问题。在这项工作中，我们探讨了LLMs是否具有识别自己错误的能力，而无需依赖外部资源。具体而言，我们研究了它们是否可以用于识别逐步推理中的个别错误。为此，我们提出了一种零-shot验证方案以识别此类错误。然后，我们使用此验证方案来改进问答性能，通过对不同生成的答案进行加权投票。我们在三个数学数据集-GSM8K，MathQA和MATH上测试了该方法，并发现它成功识别错误，并进而提高了最终的预测性能。

    The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
    
[^82]: DiviML: 一种用于在异构平台上映射神经网络的基于模块的启发式方法

    DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v1 [cs.LG])

    [http://arxiv.org/abs/2308.00127](http://arxiv.org/abs/2308.00127)

    DiviML是一种基于模块的启发式方法，用于在异构平台上将神经网络映射，通过自动分区和设备映射，实现了编译器级别的分布式神经网络编译，具有较好的可扩展性和质量评估能力。

    

    数据中心越来越多地采用异构架构，并开始包括用于网络、视频处理和深度学习的专用硬件。为了利用现代数据中心的异构计算能力，我们开发了一种在编译器级别将深度神经网络(DNNs)分区映射到多个互联硬件设备的方法。我们提出了一种用于异构DNN编译的通用框架，提供自动分区和设备映射。我们的调度器集成了一个精确求解器，通过混合整数线性规划(MILP)的形式，并使用基于模块性的启发式方法实现可扩展性。此外，我们提出了一个理论下界公式来评估启发式解的质量。我们在一个由CPU和两个设备组成的异构系统上评估了我们的调度器，优化传统的DNNs和随机连接的神经网络，考虑到延迟和吞吐量的约束。

    Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two di
    
[^83]: LLMs4OL: 大型语言模型在本体学习中的应用

    LLMs4OL: Large Language Models for Ontology Learning. (arXiv:2307.16648v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.16648](http://arxiv.org/abs/2307.16648)

    LLMs4OL方法利用大型语言模型在本体学习中取得显著进展，能够从自然语言文本中自动提取和结构化知识。

    

    我们提出了LLMs4OL方法，利用大型语言模型（LLMs）进行本体学习（OL）。LLMs在自然语言处理方面取得了重大进展，展示了它们在不同知识领域中捕捉复杂语言模式的能力。我们的LLMs4OL范式研究了以下假设：\textit{LLMs能否有效应用它们的语言模式捕捉能力到OL中，这涉及从自然语言文本中自动提取和结构化知识?} 为了测试这个假设，我们使用零-shot提示方法进行了全面评估。我们评估了九个不同的LLM模型族群，针对三个主要的OL任务：术语类型划分、层级发现和非层级关系的提取。此外，评估还涵盖了本体知识的不同类型，包括WordNet中的词汇语义知识、GeoNames中的地理知识和UMLS中的医学知识。

    We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
    
[^84]: Okapi: 使用强化学习从人类反馈中调优的多语言大型语言模型

    Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])

    [http://arxiv.org/abs/2307.16039](http://arxiv.org/abs/2307.16039)

    Okapi是一种使用强化学习从人类反馈中调优的多语言大型语言模型，它解决了目前开源语言模型只针对英语和少数流行语言进行指令调优的限制问题。

    

    开发大型语言模型的关键技术之一是指令调优，它有助于将模型的响应与人类预期对齐，实现令人印象深刻的学习能力。两种主要的指令调优方法是监督微调（SFT）和使用人类反馈的强化学习（RLHF），目前已应用于生产最佳的商业语言模型（例如ChatGPT）。为提高语言模型在研究和开发工作中的可访问性，最近还推出了各种经过指令调优的开源语言模型，例如Alpaca、Vicuna等。然而，现有的开源语言模型仅对英语和少数流行语言进行了指令调优，从而限制了它们在全球其他语言中的影响力和可访问性。最近有一些探索多语言大型语言模型指令调优的工作，但目前只使用了SFT作为指令调优的唯一方法。这已经存在了一些问题。

    A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef
    
[^85]: MARIO: 用于改善图对比学习的模型无关配方，提高OOD泛化性能

    MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])

    [http://arxiv.org/abs/2307.13055](http://arxiv.org/abs/2307.13055)

    提出了一个模型无关配方MARIO，用于改善图对比学习的OOD泛化性能。MARIO引入了信息瓶颈原则和不变性原则，旨在获得具有分布偏移鲁棒性和不变性的图表示。

    

    在这项工作中，我们研究了图数据上无监督学习方法的域外泛化问题。这种情况特别具有挑战性，因为即使有标签，图神经网络(GNNs)也显示出对分布偏移的敏感性。为了解决这个挑战，我们提出了一种名为MARIO的模型无关配方，旨在开发具有分布偏移鲁棒性的图对比方法，克服现有框架的局限性：(i)信息瓶颈(IB)原则用于实现可泛化的表示，(ii)不变性原则采用对抗性数据增强来获得不变表示。据我们所知，这是第一项研究OOD泛化问题的工作

    In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem 
    
[^86]: Q(D)O-ES: 用于自动机器学习中的后期集成选择的基于群体的质量(多样性)优化

    Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML. (arXiv:2307.08364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08364](http://arxiv.org/abs/2307.08364)

    本论文提出了两种新颖的基于群体的集成选择方法QO-ES和QDO-ES，并将它们与贪婪的集成选择进行比较。结果显示，QO-ES和QDO-ES通常优于贪婪的集成选择，尽管只有在验证数据上具有统计显著性。研究还发现，多样性对于后期集成可能有益，但也增加了过拟合的风险。

    

    自动化机器学习系统通常在后期结合模型以提高预测性能，通常通过贪婪的集成选择(GES)。然而，我们认为GES并不总是最优的，因为它执行简单的确定性贪婪搜索。在这项工作中，我们引入了两种新颖的基于群体的集成选择方法QO-ES和QDO-ES，并将它们与GES进行比较。虽然QO-ES仅针对预测性能进行优化，QDO-ES还考虑了群体内集成的多样性，在优化过程中保持一组表现良好且多样化的集成，这是基于质量多样化优化的思想。使用AutoML基准中的71个分类数据集对这些方法进行评估，结果表明，QO-ES和QDO-ES通常优于GES，尽管在验证数据上只具有统计显著性。我们的结果进一步表明多样性对于后期集成可能是有益的，但也增加了过拟合的风险。

    Automated machine learning (AutoML) systems commonly ensemble models post hoc to improve predictive performance, typically via greedy ensemble selection (GES). However, we believe that GES may not always be optimal, as it performs a simple deterministic greedy search. In this work, we introduce two novel population-based ensemble selection methods, QO-ES and QDO-ES, and compare them to GES. While QO-ES optimises solely for predictive performance, QDO-ES also considers the diversity of ensembles within the population, maintaining a diverse set of well-performing ensembles during optimisation based on ideas of quality diversity optimisation. The methods are evaluated using 71 classification datasets from the AutoML benchmark, demonstrating that QO-ES and QDO-ES often outrank GES, albeit only statistically significant on validation data. Our results further suggest that diversity can be beneficial for post hoc ensembling but also increases the risk of overfitting.
    
[^87]: 探索大规模语言模型（LLMs）在图学习中的潜力

    Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])

    [http://arxiv.org/abs/2307.03393](http://arxiv.org/abs/2307.03393)

    本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。

    

    图学习因其广泛的现实世界应用而引起了极大的关注。以文本节点属性为主的图学习最流行的流程主要依赖于图神经网络（GNN），并利用浅层文本嵌入作为初始节点表示，但存在通用知识和深刻语义理解方面的限制。近年来，大规模语言模型（LLMs）被证明具有广泛的常识和强大的语义理解能力，已经颠覆了现有的处理文本数据的工作流程。在本文中，我们旨在探索LLMs在图机器学习中的潜力，特别是节点分类任务，并研究两种可能的流程：LLMs作为增强器和LLMs作为预测器。前者利用LLMs通过其海量知识增强节点的文本属性，然后通过GNNs生成预测。后者试图直接使用LLMs作为独立的预测器。

    Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
    
[^88]: Nexus sine qua non：基于节点识别的神经网络连接的时空预测多变量时间序列

    Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])

    [http://arxiv.org/abs/2307.01482](http://arxiv.org/abs/2307.01482)

    提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。

    

    建模和预测多变量时间序列不仅有助于从业者的决策，还加深我们对底层动态系统的科学理解。时空图神经网络（STGNNs）已经成为强大的预测器，并成为学习时空表示的事实标准模型。然而，现有的STGNNs的架构往往通过堆叠一系列复杂的层次而变得复杂。设计的模型可能多余或难以理解，这给复杂性和可扩展性带来了巨大挑战。这些问题促使我们重新审视现代STGNNs的设计，并确定对强大和高效的神经预测器有所贡献的核心原则。在这里，我们提出了一个紧凑的预测模型，完全由密集编码器-解码器和消息传递层来定义，基于节点识别，没有任何复杂的顺序模块，例如TCNs，RNNs和Transformers。通过实证重新评估该模型的性能，我们证明了该模型的有效性和高效性。

    Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
    
[^89]: 差分隐私分布式估计和学习

    Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])

    [http://arxiv.org/abs/2306.15865](http://arxiv.org/abs/2306.15865)

    本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。

    

    我们研究了在网络环境中的分布式估计和学习问题，其中代理通过交换信息来估计从其私下观察的样本中未知的统计属性。通过交换私有观测信息，代理可以集体估计未知数量，但他们也面临隐私风险。我们的聚合方案的目标是在时间和网络中高效地组合观测数据，同时满足代理的隐私需求，而不需要任何超越他们本地附近的协调。我们的算法使参与的代理能够从离线或随时间在线获取的私有信号中估计完整的充分统计量，并保护其信号和网络附近的隐私。这是通过线性聚合方案和调整的随机化方案实现的，将噪声添加到交换的估计数据中以满足差分隐私（DP）。

    We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
    
[^90]: 简单成功的步骤：基于距离的算法补偿的公理化方法

    Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse. (arXiv:2306.15557v1 [cs.LG])

    [http://arxiv.org/abs/2306.15557](http://arxiv.org/abs/2306.15557)

    我们提出了一种基于距离的算法补偿的新方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。该方法具有高效性、可证明的隐私和鲁棒性保证，并在实验证明上优于现有技术。

    

    我们提出了一个新颖的数据驱动框架，用于算法补偿，提供给用户改变其预测结果的干预措施。现有的计算补偿方法找到满足某些期望的点集，例如在基础因果图中的干预，或者最小化代价函数。然而，满足这些标准需要对基础模型结构有广泛的了解，在几个领域中往往需要大量的不切实际的信息。我们提出了一种数据驱动的、计算高效的算法补偿方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。我们提出了一种公理化合理化的方法，Stepwise Explainable Paths (StEP)，用于计算基于方向的算法补偿。我们对StEP进行了彻底的实证和理论研究。StEP提供了可证明的隐私和鲁棒性保证，并在几个已建立的补偿方法中表现优于现有技术。

    We propose a novel data-driven framework for algorithmic recourse that offers users interventions to change their predicted outcome. Existing approaches to compute recourse find a set of points that satisfy some desiderata -- e.g. an intervention in the underlying causal graph, or minimizing a cost function. Satisfying these criteria, however, requires extensive knowledge of the underlying model structure, often an unrealistic amount of information in several domains. We propose a data-driven, computationally efficient approach to computing algorithmic recourse. We do so by suggesting directions in the data manifold that users can take to change their predicted outcome. We present Stepwise Explainable Paths (StEP), an axiomatically justified framework to compute direction-based algorithmic recourse. We offer a thorough empirical and theoretical investigation of StEP. StEP offers provable privacy and robustness guarantees, and outperforms the state-of-the-art on several established reco
    
[^91]: TeleViT: 电联驱动的Transformer改进了季节性野火预测

    TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting. (arXiv:2306.10940v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10940](http://arxiv.org/abs/2306.10940)

    TeleViT是一种电联驱动的视觉Transformer模型，可以准确预测季节性野火的全球烧毁面积模式，提前四个月进行预测。

    

    由于气候变化的加剧，野火问题日益恶化，需要采取先进的主动措施进行有效的缓解。提前几周甚至几个月预测野火对于计划森林燃料管理、资源采购和配置至关重要。为了在全球范围内实现准确的长期预测，必须采用能够考虑地球系统固有的时空相互作用，如记忆效应和电联的模型。我们提出了一种电联驱动的视觉Transformer（TeleViT），能够将地球视为一个相互连接的系统，将精细的局部尺度输入与全球尺度输入（如气候指数和粗粒度的全球变量）集成在一起。通过全面的实验证明，TeleViT在准确预测各种预测窗口下的全球烧毁面积模式方面具有优势，可以提前四个月进行预测。

    Wildfires are increasingly exacerbated as a result of climate change, necessitating advanced proactive measures for effective mitigation. It is important to forecast wildfires weeks and months in advance to plan forest fuel management, resource procurement and allocation. To achieve such accurate long-term forecasts at a global scale, it is crucial to employ models that account for the Earth system's inherent spatio-temporal interactions, such as memory effects and teleconnections. We propose a teleconnection-driven vision transformer (TeleViT), capable of treating the Earth as one interconnected system, integrating fine-grained local-scale inputs with global-scale inputs, such as climate indices and coarse-grained global variables. Through comprehensive experimentation, we demonstrate the superiority of TeleViT in accurately predicting global burned area patterns for various forecasting windows, up to four months in advance. The gain is especially pronounced in larger forecasting wind
    
[^92]: 昂贵嵌套灰盒函数的贝叶斯优化

    Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v1 [cs.LG])

    [http://arxiv.org/abs/2306.05150](http://arxiv.org/abs/2306.05150)

    本文提出基于乐观主义的算法来解决嵌套黑白箱函数优化问题，相比传统黑箱优化方法显著提高全局最优解速度。

    

    我们考虑优化灰盒目标函数的问题，即由黑箱和白箱函数组成的嵌套函数。给出了这种灰盒问题的一般形式，涵盖了现有的灰盒优化公式作为特殊情况。我们设计了一种基于乐观主义的算法来解决这个问题。在一定的正则性假设下，我们的算法实现了与标准黑箱贝叶斯优化算法相似的后悔边界，但乘以依赖于所考虑函数的Lipschitz常数的常数乘项。我们进一步将我们的方法扩展到约束情况，并讨论了几个特殊情况。对于常用的核函数，后悔边界使我们能够推导到最优解的收敛速度。实验结果表明，与标准黑箱优化相比，我们的灰盒优化方法在实践中显着提高了寻找全局最优解的速度。

    We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization 
    
[^93]: 使用脉冲神经形态处理器对二进制稀疏编码QUBO模型进行采样

    Sampling binary sparse coding QUBO models using a spiking neuromorphic processor. (arXiv:2306.01940v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2306.01940](http://arxiv.org/abs/2306.01940)

    本文提出了一种使用脉冲神经形态处理器对二进制稀疏编码模型进行采样的方法。首先，介绍了一种无监督和非标准化的字典特征学习方法。然后，利用随机神经网络在非凸能量景观中遍历，解决了二进制稀疏编码问题。

    

    我们考虑计算图像的稀疏二进制表示的问题。具体而言，给定一张图像和一个过完备的、非正交的基，我们的目标是找到一个稀疏的二进制向量，指示出最小的一组基向量，这些向量加在一起能够最好地重构给定的输入。我们以重构误差上的$L_2$损失和二进制向量上的$L_0$（或等价地，$L_1$）损失来描述这个问题，从而得到了一个所谓的二次无约束二进制优化（QUBO）问题，这个问题的解通常很难找到。本文的贡献是双重的。首先，我们提出了一种无监督和非标准化的字典特征学习方法，用于达到所需的稀疏级别来最好地匹配数据。然后，通过使用随机神经网络在非凸能量景观中遍历，将二进制稀疏编码问题解决在Loihi 1神经形态芯片上。我们对解决方案进行了基准测试，与类别进行了比较。

    We consider the problem of computing a sparse binary representation of an image. To be precise, given an image and an overcomplete, non-orthonormal basis, we aim to find a sparse binary vector indicating the minimal set of basis vectors that when added together best reconstruct the given input. We formulate this problem with an $L_2$ loss on the reconstruction error, and an $L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing sparsity. This yields a so-called Quadratic Unconstrained Binary Optimization (QUBO) problem, whose solution is generally NP-hard to find. The contribution of this work is twofold. First, the method of unsupervised and unnormalized dictionary feature learning for a desired sparsity level to best match the data is presented. Second, the binary sparse coding problem is then solved on the Loihi 1 neuromorphic chip by the use of stochastic networks of neurons to traverse the non-convex energy landscape. The solutions are benchmarked against the class
    
[^94]: 高效大规模的视觉表示学习

    Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])

    [http://arxiv.org/abs/2305.13399](http://arxiv.org/abs/2305.13399)

    本文介绍了一种高效大规模的单模态视觉表示学习方法，解决了电商应用中视觉表示的挑战，并提供了消融研究和评估。

    

    本文介绍了我们的单模态视觉表示学习方法，了解产品内容的视觉表示对电商推荐、搜索和广告应用至关重要。我们详细介绍和对比了在低资源环境下有效微调大规模视觉表示学习模型的技术，包括多种预训练的骨干架构，包括卷积神经网络和视觉转换器系列。我们强调了电子商务应用在大规模情况下的挑战，并突出了更有效地训练、评估和提供视觉表示的努力。我们为几个下游任务提供了消融研究，包括我们的视觉相似广告推荐。我们评估了所得视觉表示在下游任务中的离线性能。为此，我们提出了一种新的文本到图像生成的离线评估方法，用于视觉相似推荐。

    In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
    
[^95]: SpikeCP: 通过极限预测实现延迟自适应可靠脉冲神经网络

    SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])

    [http://arxiv.org/abs/2305.11322](http://arxiv.org/abs/2305.11322)

    这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。

    

    脉冲神经网络（SNN）通过内部事件驱动的神经动态处理时间序列数据，其能量消耗取决于输入演示期间神经元之间交换的脉冲数量。在典型的SNN分类器实现中，决策是在整个输入序列被处理后产生的，导致延迟和能量消耗水平在输入之间是相对均匀的。最近引入的延迟自适应SNN可根据每个示例的难度来定制推断延迟 - 以及随之而来的能耗 - 通过在SNN模型足够“自信”时产生早期决策来实现。

    Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
    
[^96]: Dr. LLaMA：通过生成式数据增强改善特定领域QA中的小语言模型

    Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])

    [http://arxiv.org/abs/2305.07804](http://arxiv.org/abs/2305.07804)

    本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。

    

    大型语言模型在自然语言处理方面取得了重大进展，但随着其规模的增长，也面临着计算开销和效率的挑战，特别是在特定领域的任务中。另一方面，小型语言模型由于容量和训练数据的限制，在这些任务中往往表现不佳。本文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，聚焦医学问答任务和PubMedQA数据集，以改善小语言模型的性能。我们的发现表明，LLM有效地细化和扩展现有的问题-答案对，在微调后，使得小型模型在特定领域QA数据集上性能提高。本研究强调了在特定领域问答任务中使用LLM面临的挑战，并提出了潜在的研究方向，最终旨在为专业应用创建更高效和能力更强的模型。

    Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
    
[^97]: BrainNPT：用于脑网络分类的Transformer网络的预训练

    BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.01666](http://arxiv.org/abs/2305.01666)

    本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。

    

    近年来，深度学习方法在脑成像分析方面的进展迅速，但往往受到有限标记数据的限制。在未标记数据上预训练的模型已在许多领域中展示了有前景的特征学习改进，包括自然语言处理和计算机视觉。然而，在脑网络分析中，这种技术尚未得到充分探索。在本文中，我们以Transformer网络为基础的预训练方法为重点，利用现有的未标记数据进行脑功能网络分类。首先，我们提出了一种基于Transformer的神经网络，名为BrainNPT，用于脑功能网络分类。所提出的方法利用<cls>标记作为分类嵌入向量，以便于Transformer模型有效地捕获脑网络的表示。其次，我们提出了两种预训练策略的预训练架构，用于BrainNPT模型，以利用未标记的脑网络数据来学习结构。

    Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged <cls> token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
    
[^98]: ReLBOT：一种转移学习方法以最小化智能建筑中强化学习风险

    ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings. (arXiv:2305.00365v1 [cs.LG])

    [http://arxiv.org/abs/2305.00365](http://arxiv.org/abs/2305.00365)

    ReLBOT使用转移学习和深度RL技术来从现有的智能建筑中传递优化参数到新的建筑中，以减少强化学习代理引起的初始不适，有效降低了风险，并且实现了热身期时长6.2倍的提高和预测方差的132倍提高。

    

    智能建筑旨在通过应用人工智能算法来优化能源消耗。当智能建筑投入使用时，没有历史数据可用于训练这些算法。在线强化学习（RL）算法显示出重要的前景，但它们的部署存在重大风险，因为当RL代理最初探索其行动空间时，它可能会给建筑居民带来重大不适。在本文中，我们提出了一种名为ReLBOT的新技术，它使用转移学习结合深度RL，从现有的优化智能建筑中传递知识到新投入使用的建筑中，以减少强化学习代理的热身期对建筑物的不利影响。我们证明取得了可观的成果，热身期的持续时间可提高6.2倍，并且预测方差可提高132倍。

    Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.
    
[^99]: 一种使用确定性目标的黑匣子变分推断：更快，更精确，更黑。

    Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])

    [http://arxiv.org/abs/2304.05527](http://arxiv.org/abs/2304.05527)

    本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。

    

    自动微分变分推断（ADVI）提供了多种现代概率编程语言中快速易用的后验近似方法。然而它的随机优化器缺乏明确的收敛标准，并且需要调整参数。此外，ADVI继承了均值场变分贝叶斯（MFVB）的较差后验不确定性估计。我们引入了“确定性ADVI”（DADVI）来解决这些问题。DADVI用固定的蒙特卡罗近似替换了MFVB的不可解目标，这一技术在随机优化文献中被称为“样本平均近似”（SAA）。通过优化近似但确定的目标，DADVI可以使用现成的二阶优化，而且与标准均值场ADVI不同的是，可以适用于更准确的后验线性响应（LR）协方差估计。与现有的最坏情况理论相反，我们表明，在某些常见的统计问题类别上，DADVI和SAA可以表现得更好。

    Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
    
[^100]: Transformer和Snowball图卷积学习用于生物医学图分类

    Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])

    [http://arxiv.org/abs/2303.16132](http://arxiv.org/abs/2303.16132)

    本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。

    

    图或网络已被广泛用于描述和建模生物医学中的复杂系统。深度学习方法，尤其是图神经网络（GNNs），已被开发用于学习和预测这种结构化数据。在本文中，我们提出了一种用于生物医学图分类的新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs，以学习整个图的表示。

    Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
    
[^101]: 绿色联邦学习

    Green Federated Learning. (arXiv:2303.14604v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14604](http://arxiv.org/abs/2303.14604)

    本文提出了一种绿色联邦学习方法，通过优化算法和能源管理策略来减少碳足迹，使得跨设备的联邦学习系统可以更加环保。

    

    人工智能的快速发展是由越来越大、计算密集的机器学习模型和数据集推动的。因此，训练最先进的模型所使用的计算量在指数上增长（从2015年到2022年每10个月翻一倍），导致了巨大的碳足迹。联邦学习（FL）是一种协作的机器学习技术，用于使用分散实体的数据训练集中模型，它在大规模部署时也可能需要大量资源，同时产生相当大的碳足迹。与可靠地利用可再生能源的战略性数据中心相比，跨设备的联邦学习可能利用分布在全球的数亿个具有不同能源来源的终端设备。绿色人工智能是一个新颖且重要的研究领域，碳足迹被视为除准确性、收敛速度和其他指标外的人工智能评估标准之一。在本文中，我们提出了一种方法，通过优化联邦学习算法和能源管理策略，将碳足迹作为联邦学习系统的设计目标，从而实现绿色联邦学习。

    The rapid progress of AI is fueled by increasingly large and computationally intensive machine learning models and datasets. As a consequence, the amount of compute used in training state-of-the-art models is exponentially increasing (doubling every 10 months between 2015 and 2022), resulting in a large carbon footprint. Federated Learning (FL) - a collaborative machine learning technique for training a centralized model using data of decentralized entities - can also be resource-intensive and have a significant carbon footprint, particularly when deployed at scale. Unlike centralized AI that can reliably tap into renewables at strategically placed data centers, cross-device FL may leverage as many as hundreds of millions of globally distributed end-user devices with diverse energy sources. Green AI is a novel and important research area where carbon footprint is regarded as an evaluation criterion for AI, alongside accuracy, convergence speed, and other metrics. In this paper, we prop
    
[^102]: 自监督混合深度学习实现健壮全息毫米波波束成形

    Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning. (arXiv:2303.12653v1 [cs.IT])

    [http://arxiv.org/abs/2303.12653](http://arxiv.org/abs/2303.12653)

    本文提出一种自监督混合深度学习网络用于健壮波束成形，能够在两种不同的数据集和各种场景中表现出更强的鲁棒性。

    

    近年来，大规模天线阵列的波束成形被广泛应用于5G和即将推出的6G中，因此各种技术被利用来提高其性能，例如深度学习、高级优化算法等。尽管在许多具有深度学习的先前研究方案中其性能相当吸引人，但通常当环境或数据集发生变化时，其性能会迅速下降。因此，设计具有强大鲁棒性的有效波束成形网络是智能无线通信的一个开放问题。在本文中，我们提出了一个健壮的波束成形自监督网络，并在两种不同数据集和各种场景下进行了验证。仿真结果表明，所提出的具有混合学习的自监督网络在经典的DeepMIMO和新的WAIR-D数据集上具有强大的鲁棒性，适用于各种环境。此外，我们还提出了原理来解释这个翻译的合理性。

    Beamforming with large-scale antenna arrays has been widely used in recent years, which is acknowledged as an important part in 5G and incoming 6G. Thus, various techniques are leveraged to improve its performance, e.g., deep learning, advanced optimization algorithms, etc. Although its performance in many previous research scenarios with deep learning is quite attractive, usually it drops rapidly when the environment or dataset is changed. Therefore, designing effective beamforming network with strong robustness is an open issue for the intelligent wireless communications. In this paper, we propose a robust beamforming self-supervised network, and verify it in two kinds of different datasets with various scenarios. Simulation results show that the proposed self-supervised network with hybrid learning performs well in both classic DeepMIMO and new WAIR-D dataset with the strong robustness under the various environments. Also, we present the principle to explain the rationality of this 
    
[^103]: 创伤性脑损伤的临床进程对结果的贡献：从欧洲重症监护室数据中挖掘患者轨迹

    Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data. (arXiv:2303.04630v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04630](http://arxiv.org/abs/2303.04630)

    本研究研发了一种整合患者ICU住院期间所有记录的数据来为每名TBI患者提供可解释的疾病进展的建模策略，并应用递归神经网络模型对患者状况进行预测，以帮助实现个体化治疗。

    

    现有的创伤性脑损伤（TBI）患者状况表述方法不能捕捉个体化治疗所需的上下文信息。本研究旨在开发一种建模策略，整合住院期间所有记录的数据，为每名TBI患者的重症监护室（ICU）住院提供可解释的疾病进展。研究随机选择了1550名来自欧洲65个中心、19个国家的TBI患者，提取了在ICU住院期间或之前收集的所有1166个变量，以及6个月的Glasgow Outcome Scale-Extended（GOSE）功能结果。研究使用递归神经网络模型，将所有变量的令牌嵌入时间序列表示（包括缺失数据）映射到每2个小时的GOSE预后。通过重复交叉验证，我们使用Somers' Dxy评估了GOSE的日常差异的校准和解释。此外，我们应用了TimeSHAP来计算变量及先前时间对每个病例的贡献。

    Existing methods to characterise the evolving condition of traumatic brain injury (TBI) patients in the intensive care unit (ICU) do not capture the context necessary for individualising treatment. We aimed to develop a modelling strategy which integrates all data stored in medical records to produce an interpretable disease course for each TBI patient's ICU stay. From a prospective, European cohort (n=1,550, 65 centres, 19 countries) of TBI patients, we extracted all 1,166 variables collected before or during ICU stay as well as 6-month functional outcome on the Glasgow Outcome Scale-Extended (GOSE). We trained recurrent neural network models to map a token-embedded time series representation of all variables (including missing data) to an ordinal GOSE prognosis every 2 hours. With repeated cross-validation, we evaluated calibration and the explanation of ordinal variance in GOSE with Somers' Dxy. Furthermore, we applied TimeSHAP to calculate the contribution of variables and prior ti
    
[^104]: 技术报告：图神经网络也可以变得语法化

    Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01590](http://arxiv.org/abs/2303.01590)

    本文介绍了一种将代数语言片段与图神经网络形式上联系的框架，并从MATLANG定义了一个符合3-WL测试的语法，进而得出一个符合3-WL GNN模型的G$^2$N$^2$。此外，语法方法还提供了计算长度为六及以下的环和弦环的代数公式，并在多个下游任务中取得优秀的表现。

    

    本文提出了一个框架，将一个代数语言的一个片段与图神经网络（GNN）形式上联系起来。它依赖于上下文无关语法（CFG），将代数操作组织成可以翻译为GNN层模型的生成规则。由于直接从语言派生出的CFG的规则和变量包含冗余，因此介绍了一种语法简化方案，使得将其翻译为GNN层成为可能。应用这种策略，从MATLANG定义了一个符合第三阶Weisfeiler-Lehman（3-WL）测试要求的语法。从这个3-WL CFG中，我们得出了一个经过证明符合3-WL GNN模型的G$^2$N$^2$。此外，这种语法方法使我们能够提供计算长度为六及以下的环和弦环的代数公式，从而阐明了3-WL的计数能力。多个实验证明，G$^2$N$^2$在许多下游任务中的表现要比其他3-WL GNN更为高效。

    This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
    
[^105]: 进化增强策略优化用于自监督学习

    Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.01584](http://arxiv.org/abs/2303.01584)

    本文研究了数据增强操作对自监督学习算法性能的影响，提出了一种进化搜索方法来优化数据增强策略，并通过实验比较了几种现有自监督学习算法的性能。

    

    自监督学习是一种无需手动标记数据的深度神经网络预训练的机器学习算法。该学习技术的核心思想是通过辅助阶段（也称为预训练任务），通过数据增强自动生成标记数据，并用于预训练深度神经网络。然而，文献中对于每个预训练任务的影响还未得到充分研究和比较。本文研究了数据增强操作对约束条件下自监督学习算法性能的贡献。我们提出了一种进化搜索方法，用于优化预训练任务中的数据增强流程，并测量了几种先进的自监督学习算法中数据增强操作的影响。通过在染色体中编码不同组合的增强操作，我们通过进化优化机制寻求最优的增强策略。此外，我们还引入了用于分析和解释数据增强策略在模型性能上的影响的方法。

    Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and expla
    
[^106]: 理解神经网络中的可塑性

    Understanding plasticity in neural networks. (arXiv:2303.01486v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01486](http://arxiv.org/abs/2303.01486)

    本文通过对失去可塑性问题进行系统实证分析，发现其深度与损失梯度曲率变化密切相关，饱和单元或发散梯度范数并非原因。基于这一发现，识别了一系列参数化和优化方法，有效提高神经网络保持可塑性的能力，在深度强化学习问题中具有显著的适应性和鲁棒性。

    

    可塑性是神经网络能够快速根据新信息更改其预测的能力，是深度强化学习系统适应性和鲁棒性的关键。深度神经网络即使在相对简单的学习问题中也会在训练过程中失去可塑性，但驱动这种现象的机制仍然不清楚。本文通过系统的实证分析，旨在深度理解可塑性的丧失，以引导未来对有针对性的解决方案的发展。我们发现可塑性的丧失与损失梯度曲率的变化密切相关，但通常发生在无饱和单元或发散梯度范数的情况下。基于这一洞见，我们识别出一些参数化和优化设计选择，使网络能够在训练过程中更好地保持可塑性。我们验证了这些基于特征的干预措施在一系列深度强化学习问题中的效用，证明它们显著提高了学习系统的适应性和鲁棒性。

    Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it typically occurs in the absence of saturated units or divergent gradient norms. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these f
    
[^107]: 在新闻文章中检测有害议程

    Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00102](http://arxiv.org/abs/2302.00102)

    这项研究提出了一种新的任务，即在新闻文章中检测有害议程，并发布了一个新闻文章注释数据集以供研究使用。研究者展示了可解释系统在这一任务上的有效性，并证明它们可以和黑盒模型有相当的表现。

    

    在线上操纵新闻是一个日益严重的问题，需要使用自动化系统来遏制其传播。我们认为，虽然误导信息和虚假信息的检测已经得到研究，但在检测新闻文章中的有害议程这一重要挑战方面缺乏投资；识别有害议程对于识别具有最大潜在现实危害的新闻运动至关重要。此外，由于对审查制度存在真实的担忧，有害议程检测器必须具有可解释性才能发挥作用。在这项工作中，我们提出了这一全新的任务，并发布了一个名为NewsAgendas的新闻文章注释数据集，用于议程识别。我们展示了可解释系统在这一任务上的有效性，并证明它们可以与黑盒模型具有相当的表现。

    Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
    
[^108]: 领域自适应学习和模仿：用于电力套利的深度强化学习

    Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)

    [http://arxiv.org/abs/2301.08360](http://arxiv.org/abs/2301.08360)

    本文提出了一种领域自适应学习和模仿的深度强化学习方法，用于电力套利交易。通过利用奖励工程和订单分段的方式，该方法能够提高训练的收敛性，增加竞标成功率，并显著提高利润和损失。

    

    本文讨论荷兰电力市场，由日前市场和即时平衡市场组成，并且运作方式类似拍卖。由于供需波动，通常存在不平衡导致两个市场价格不同，从而提供套利机会。为了解决这个问题，我们对问题进行了重构，并提出了一种协作式双代理深度强化学习方法，用于欧洲电力套利交易的双层仿真与优化。我们还引入了两种新的实现方式，通过模仿电力交易员的交易行为来融入领域特定知识。通过利用奖励工程来模仿领域专业知识，我们能够重新设计强化学习代理的奖励系统，改善训练中的收敛性并提高整体性能。此外，订单分段增加了竞标成功率，显著提高了利润和损失。我们的研究演示了通过领域自适应学习和模仿的深度强化学习方法在电力套利中的应用。

    In this paper, we discuss the Dutch power market, which is comprised of a day-ahead market and an intraday balancing market that operates like an auction. Due to fluctuations in power supply and demand, there is often an imbalance that leads to different prices in the two markets, providing an opportunity for arbitrage. To address this issue, we restructure the problem and propose a collaborative dual-agent reinforcement learning approach for this bi-level simulation and optimization of European power arbitrage trading. We also introduce two new implementations designed to incorporate domain-specific knowledge by imitating the trading behaviours of power traders. By utilizing reward engineering to imitate domain expertise, we are able to reform the reward system for the RL agent, which improves convergence during training and enhances overall performance. Additionally, the tranching of orders increases bidding success rates and significantly boosts profit and loss (P&L). Our study demo
    
[^109]: 在大型语言模型中进行快速和慢速思考

    Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.05206](http://arxiv.org/abs/2212.05206)

    本研究发现，大型语言模型（LLMs）如GPT-3在行为上与人类直觉相似，但可能带有认知错误。然而，具有更高认知能力的LLMs，如ChatGPT和GPT-4，学会了避免这些错误，表现出超理性的方式。通过在心理学方法的帮助下研究LLMs，我们可以揭示出其它未知的新特征。

    

    大型语言模型（LLMs）目前处于将AI系统与人类交流和日常生活结合的前沿。因此，评估它们新兴的能力非常重要。在这项研究中，我们展示了像GPT-3这样的LLMs表现出与人类直觉惊人相似的行为，以及由此带来的认知错误。然而，具有更高认知能力的LLMs，特别是ChatGPT和GPT-4，学会了避免陷入这些错误，表现出超理性的方式。在我们的实验中，我们使用认知反思测试（CRT）以及最初设计用于研究人类直觉决策的语义错觉来探索LLMs。我们的研究表明，利用心理学方法研究LLMs有助于揭示其他未知的新特征。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
    
[^110]: 基于数据驱动的聚合物熔体玻璃转变的识别与分析

    Data-driven identification and analysis of the glass transition in polymer melts. (arXiv:2211.14220v2 [cond-mat.soft] UPDATED)

    [http://arxiv.org/abs/2211.14220](http://arxiv.org/abs/2211.14220)

    本研究提出了一种基于数据驱动的方法，通过分子动力学模拟和聚类方法，清晰地识别了聚合物熔体的玻璃转变温度，并揭示了主成分分析在捕捉聚合物链行为变化中的作用。

    

    理解玻璃转变的性质以及准确估计聚合物材料的玻璃转变温度仍然是实验和理论聚合物科学中的未解之谜。我们提出一种基于数据驱动的方法，利用分子动力学模拟提供的高分辨率细节，并考虑单个链的结构信息。它清楚地识别了弱半刚性聚合物熔体的玻璃转变温度。通过结合主成分分析和聚类，我们即使从相对短时间轨迹中也能够识别出玻璃转变温度，在这些轨迹中，单体的位移达到了Rouse类似的程度。我们证明了主成分分析捕捉到的波动反映了链的行为变化：从玻璃转变温度以上的构象重排到温度以下的小重排。我们的方法是直接的。

    Understanding the nature of glass transition, as well as precise estimation of the glass transition temperature for polymeric materials, remain open questions in both experimental and theoretical polymer sciences. We propose a data-driven approach, which utilizes the high-resolution details accessible through the molecular dynamics simulation and considers the structural information of individual chains. It clearly identifies the glass transition temperature of polymer melts of weakly semiflexible chains. By combining principal component analysis and clustering, we identify the glass transition temperature in the asymptotic limit even from relatively short-time trajectories, which just reach into the Rouse-like monomer displacement regime. We demonstrate that fluctuations captured by the principal component analysis reflect the change in a chain's behaviour: from conformational rearrangement above to small rearrangements below the glass transition temperature. Our approach is straightf
    
[^111]: Bregman近端方法的收敛速度：局部几何 vs 正则性 vs 锐度

    The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness. (arXiv:2211.08043v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.08043](http://arxiv.org/abs/2211.08043)

    本文研究了Bregman近端方法的收敛速度，发现其与局部几何、正则性以及锐度等因素相关。特别地，我们发现边界解在零和非零Legendre指数的方法之间有明显的差异，前者以线性速度收敛，而后者以次线性速度收敛。在线性约束问题中，具有熵正则化的方法在尖锐方向上实现线性收敛速度，与传统收敛速度有显著区别。

    

    我们研究了从镜像下降到镜像-近端及其乐观变种的Bregman近端方法的最后迭代收敛速度，作为一种依赖于定义方法的近端映射所导致的局部几何的函数。为了通用性，我们关注约束的非单调变分不等式的局部解，并且我们证明了给定方法的收敛速度取决于其相关的Legendre指数，这个概念度量了接近解的Bregman函数（欧几里得、熵或其他函数）的增长速率。特别地，我们证明了边界解在具有零和非零Legendre指数的方法之间存在明显的分离情况：前者以线性速度收敛，而后者一般以次线性速度收敛。在线性约束问题中，当方法具有熵正则化时，这种二分法在尖锐方向上实现线性收敛速度，与传统收敛速度的差别更加突出。

    We examine the last-iterate convergence rate of Bregman proximal methods from mirror descent to mirror-prox and its optimistic variants - as a function of the local geometry induced by the prox-mapping defining the method. For generality, we focus on local solutions of constrained, non-monotone variational inequalities, and we show that the convergence rate of a given method depends sharply on its associated Legendre exponent, a notion that measures the growth rate of the underlying Bregman function (Euclidean, entropic, or other) near a solution. In particular, we show that boundary solutions exhibit a stark separation of regimes between methods with a zero and non-zero Legendre exponent: the former converge at a linear rate, while the latter converge, in general, sublinearly. This dichotomy becomes even more pronounced in linearly constrained problems where methods with entropic regularization achieve a linear convergence rate along sharp directions, compared to convergence in a fi
    
[^112]: 基于最优传输的实例相关泛化界限

    Instance-Dependent Generalization Bounds via Optimal Transport. (arXiv:2211.01258v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.01258](http://arxiv.org/abs/2211.01258)

    该论文提出了一种基于最优传输的实例相关泛化界限的方法，以解释神经网络泛化的关键因素，并且考虑了初始化和随机梯度下降的强归纳偏差。这种方法在模型参数化不可知且训练样本数量远小于参数数量时表现良好，还可以应用于低维流形上的数据和分布转换情况下的泛化问题。

    

    现有的泛化界限无法解释影响现代神经网络泛化的关键因素。由于这些界限通常对所有参数都是一致的，它们容易过度参数化，并且无法考虑到初始化和随机梯度下降的强归纳偏差。作为替代方案，我们提出了一种新颖的最优传输解释泛化问题的方法。这使我们能够获得依赖于数据空间中预测函数的局部利普希茨正则性的实例相关泛化界限。因此，我们的界限对模型的参数化是不可知的，并且在训练样本数量远小于参数数量时表现良好。通过一些小的修改，我们的方法在低维流形上的数据上可以获得加速的速率，并且在分布转换下具有保证。我们通过对神经网络的实证分析来验证我们的泛化界限，结果显示界限值是

    Existing generalization bounds fail to explain crucial factors that drive generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization, and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the earned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds, and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are 
    
[^113]: 国际空间站自动紧急无尘解决方案: 带有Bi-GRU的(AED-ISS)

    Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v2 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2210.08549](http://arxiv.org/abs/2210.08549)

    该论文旨在解决国际空间站上颗粒物对仪器的危害问题，通过Bi-GRU算法构建早期预警系统，预测颗粒物水平，并为宇航员提供充足的反应时间。这项研究还有潜力发展为与火灾相关的遥感烟雾报警装置。

    

    随着对PM2.5或PM0.3问题的关注不断增加，颗粒物不仅对环境和人类构成潜在威胁，而且对国际空间站上的仪器也会产生不利影响。本研究团队旨在将各种颗粒物浓度与磁场、湿度、加速度、温度、压力和CO2浓度关联起来。我们的目标是建立一个早期预警系统(EWS)，能够预测颗粒物水平，并为宇航员提供充足的反应时间，以保护他们在某些实验中的仪器，或者提高测量的准确性；此外，所构建的模型还可以进一步发展为与火灾相关的遥感烟雾报警装置的原型。本文中，我们将实现Bi-GRU(双向门控循环单元)算法，收集过去90分钟的数据，并预测超过2.5微米的颗粒物水平。

    With a rising attention for the issue of PM2.5 or PM0.3, particulate matters have become not only a potential threat to both the environment and human, but also a harming existence to instruments onboard International Space Station (ISS). Our team is aiming to relate various concentration of particulate matters to magnetic fields, humidity, acceleration, temperature, pressure and CO2 concentration. Our goal is to establish an early warning system (EWS), which is able to forecast the levels of particulate matters and provides ample reaction time for astronauts to protect their instruments in some experiments or increase the accuracy of the measurements; In addition, the constructed model can be further developed into a prototype of a remote-sensing smoke alarm for applications related to fires. In this article, we will implement the Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for past 90 minutes and predict the levels of particulates which over 2.5 micromete
    
[^114]: 学习高效计划稳健的摩擦多物体抓取

    Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.07420](http://arxiv.org/abs/2210.07420)

    本文介绍了一种使用神经网络进行摩擦多物体抓取的高效计划方法，相比于以往的工作，其成功率提高了13.7％，每小时的拾取次数增加了1.6倍，并且抓取计划时间减少了6.3倍。

    

    本文考虑了一个杂乱问题，多个刚性凸多边形物体随机放置在一个平面表面上，必须使用单个和多个物体的抓取方式，将它们有效地运输到装箱中。我们引入摩擦来增加每小时的拾取次数，并使用实例进行神经网络的训练，以计划稳健的多物体抓取。在物理实验中，相比于多物体抓取的先前工作，我们发现成功率增加了13.7％，每小时的拾取次数增加了1.6倍，抓取计划时间减少了6.3倍。与单个物体抓取相比，我们发现每小时的拾取次数增加了3.1倍。

    We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
    
[^115]: 在Transformer中进行大规模编辑内存

    Mass-Editing Memory in a Transformer. (arXiv:2210.07229v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07229](http://arxiv.org/abs/2210.07229)

    该论文提出了一种在Transformer中进行大规模编辑内存的方法，可以有效地更新语言模型的记忆，实验证明其在关联数量上具有数量级的优势。

    

    最近的研究展示了在更新大型语言模型时使用新的记忆的激动人心的前景，以替换过时的信息或添加专业知识。然而，这一领域的工作主要仅限于更新单个关联。我们开发了MEMIT，一种直接更新语言模型的方法，实验证明它可以扩展到数千个关联，对于GPT-J(6B)和GPT-NeoX(20B)，超过了之前的工作数个数量级。我们的代码和数据可以在https://memit.baulab.info找到。

    Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.
    
[^116]: FaDIn: 针对具有一般参数核的Hawkes过程的快速离散化推断

    FaDIn: Fast Discretized Inference for Hawkes Processes with General Parametric Kernels. (arXiv:2210.04635v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.04635](http://arxiv.org/abs/2210.04635)

    本论文提出了一种使用具有有限支持的一般参数核进行TPP推理的高效解决方案，该方法采用了离散化方法，并通过多项实验证明了该方法的统计和计算效率。

    

    时间点过程是建模事件数据的自然工具。在所有的时间点过程模型中，Hawkes过程被证明是最广泛使用的，主要是由于它们对于各种应用的适当建模，特别是在考虑指数或非参数核时。尽管非参数核是一种选择，但这些模型需要大型数据集。而指数核更具数据效率，对于立即触发更多事件的特定应用更有效，但对于需要估计延迟的应用（如神经科学），它们不太适用。本研究旨在提供一种使用具有有限支持的一般参数核进行TPP推理的高效解决方案。所开发的解决方案包括利用事件的离散化的快速$\ell_2$梯度求解器。在理论上支持离散化的使用后，通过多种实验，证明了该新方法的统计和计算效率。

    Temporal point processes (TPP) are a natural tool for modeling event-based data. Among all TPP models, Hawkes processes have proven to be the most widely used, mainly due to their adequate modeling for various applications, particularly when considering exponential or non-parametric kernels. Although non-parametric kernels are an option, such models require large datasets. While exponential kernels are more data efficient and relevant for specific applications where events immediately trigger more events, they are ill-suited for applications where latencies need to be estimated, such as in neuroscience. This work aims to offer an efficient solution to TPP inference using general parametric kernels with finite support. The developed solution consists of a fast $\ell_2$ gradient-based solver leveraging a discretized version of the events. After theoretically supporting the use of discretization, the statistical and computational efficiency of the novel approach is demonstrated through va
    
[^117]: FedDef: 面向基于联邦学习的网络入侵检测系统的梯度泄露防御

    FedDef: Defense Against Gradient Leakage in Federated Learning-based Network Intrusion Detection Systems. (arXiv:2210.04052v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.04052](http://arxiv.org/abs/2210.04052)

    FedDef是一种防御基于联邦学习的网络入侵检测系统中梯度泄露的方法，通过提出隐私评估指标和对抗攻击来评估FL系统的鲁棒性，并展示了现有防御措施的脆弱性。

    

    深度学习（DL）方法已被广泛应用于基于异常的网络入侵检测系统（NIDS）以检测恶意流量。为了扩大基于DL的方法的使用场景，联邦学习（FL）允许多个用户在尊重个体数据隐私的基础上训练一个全局模型。然而，尚未对现有防御措施下FL-based NIDSs对现有隐私攻击的鲁棒性进行系统评估。为了解决这个问题，我们提出了两种针对FL-based NIDSs设计的隐私评估指标，包括（1）隐私评分，用于评估使用重构攻击恢复的流量特征与原始特征之间的相似性，以及（2）对抗攻击下使用恢复的流量对抗NIDSs的逃避率。我们进行实验来说明现有的防御措施提供了很少的保护，相应的对抗流量甚至可以规避SOTA NIDS Kitsune。为了防御此类攻击并构建更加鲁棒的FL系统。

    Deep learning (DL) methods have been widely applied to anomaly-based network intrusion detection system (NIDS) to detect malicious traffic. To expand the usage scenarios of DL-based methods, federated learning (FL) allows multiple users to train a global model on the basis of respecting individual data privacy. However, it has not yet been systematically evaluated how robust FL-based NIDSs are against existing privacy attacks under existing defenses. To address this issue, we propose two privacy evaluation metrics designed for FL-based NIDSs, including (1) privacy score that evaluates the similarity between the original and recovered traffic features using reconstruction attacks, and (2) evasion rate against NIDSs using adversarial attack with the recovered traffic. We conduct experiments to illustrate that existing defenses provide little protection and the corresponding adversarial traffic can even evade the SOTA NIDS Kitsune. To defend against such attacks and build a more robust FL
    
[^118]: 通过邻域排序的图形软对比学习

    Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13964](http://arxiv.org/abs/2209.13964)

    图对比学习（GCL）方法在图像领域表现出了显著的性能，但在应用于图数据时面临着生成无效视图和不可靠相似性对的限制。这篇论文提出了一种基于邻域排序的图形软对比学习方法，以更好地适应图的内在属性。

    

    图对比学习（Graph Contrastive Learning，GCL）已成为图自监督学习领域中一种有前景的方法。现有的GCL方法主要借鉴了计算机视觉领域对比学习的原理：通过指定完全相似的对来建模不变性。然而，当应用于图数据时，这种范式遇到了两个重要的限制：（1）生成的视图的有效性无法得到保证：图扰动可能会产生违反语义和图数据内在拓扑性质的无效视图；（2）在图视图中指定完全相似的对是不可靠的：对于抽象和非欧几里得的图数据，人们很难直观地决定绝对的相似性和不相似性。尽管当前的GCL方法表现出了显著的性能，但这些挑战需要重新评估：GCL是否能更有效地适应图的内在属性，而不仅仅采用计算机视觉的原则？

    Graph Contrastive Learning (GCL) has emerged as a promising approach in the realm of graph self-supervised learning. Prevailing GCL methods mainly derive from the principles of contrastive learning in the field of computer vision: modeling invariance by specifying absolutely similar pairs. However, when applied to graph data, this paradigm encounters two significant limitations: (1) the validity of the generated views cannot be guaranteed: graph perturbation may produce invalid views against semantics and intrinsic topology of graph data; (2) specifying absolutely similar pairs in the graph views is unreliable: for abstract and non-Euclidean graph data, it is difficult for humans to decide the absolute similarity and dissimilarity intuitively. Despite the notable performance of current GCL methods, these challenges necessitate a reevaluation: Could GCL be more effectively tailored to the intrinsic properties of graphs, rather than merely adopting principles from computer vision? In res
    
[^119]: ProMix：通过最大化干净样本效用来对抗标签噪声

    ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10276](http://arxiv.org/abs/2207.10276)

    论文提出了一种名为ProMix的新颖LNL框架，通过最大化干净样本的效用来对抗标签噪声。采用一种匹配高置信度选择技术来动态扩展基础干净样本集，并设计了一种平衡和无偏的SSL框架来提高性能。

    

    学习具有噪声标签（LNL）已成为一个吸引人的话题，因为带有不完整标注的数据相对较便宜易得。最近的最先进方法采用特定的选择机制来区分干净样本和噪声样本，然后应用半监督学习（SSL）技术以提高性能。然而，选择步骤主要提供一个中等大小和足够好的清洁子集，忽视了丰富的干净样本集。为了实现这一点，我们提出了一种新颖的LNL框架ProMix，试图通过最大化干净样本的效用来提高性能。我们方法的关键在于提出了一种匹配高置信度选择技术，该技术选择那些具有高置信度得分和与给定标签匹配预测的示例，以动态扩展基础干净样本集。为了克服过度选择清洁样本集程序的潜在副作用，我们进一步设计了一种新颖的SSL框架，能够在训练时得到平衡和无偏的分类器。

    Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the se
    
[^120]: 人类学习奖励函数偏好的模型

    Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02231](http://arxiv.org/abs/2206.02231)

    本研究提出了一种将人类偏好建模为每个轨迹段的遗憾的方法，并证明了可以根据这些遗憾生成的偏好来识别生成这些偏好的奖励函数。实验证明，这种遗憾偏好模型在性能上优于以前的模型。

    

    强化学习的效用受限于奖励函数与人类利益的一致性。一种有前途的对齐方法是从人类生成的轨迹段对之间学习奖励函数，这是一种从人类反馈中进行的强化学习方法。通常假设这些人类偏好仅由部分回报来决定，即每个轨迹段上的奖励总和。我们发现这种假设存在缺陷，提出将人类偏好建模为由每个轨迹段的遗憾来决定，遗憾是一种衡量轨迹段与最优决策之间偏离程度的度量。在根据遗憾生成的无穷多个偏好中，我们证明可以识别到与生成这些偏好的奖励函数等价的奖励函数，并且我们证明以前的部分回报模型在多种情境下缺乏这种可识别性属性。通过实验证明，我们提出的遗憾偏好模型在性能上优于以前的模型。

    The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
    
[^121]: 压缩傅里叶色散方法用于具有周期边界条件的高维扩散方程

    Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2206.01255](http://arxiv.org/abs/2206.01255)

    本研究提出了一种压缩傅里叶色散方法，用于解决定义在高维周期边界条件域上的扩散方程。该方法利用压缩感知和稀疏恢复技术，通过在蒙特卡罗采样上近似解的傅里叶系数，有效地克服了维度诅咒的影响。

    

    高维偏微分方程是一种常用的数学建模工具，应用范围从金融到计算化学。然而，解决这些方程的标准数值技术通常受到维度诅咒的影响。在本文中，我们解决了这个挑战，重点关注定义在高维域上具有周期边界条件的定常扩散方程。受高维稀疏函数逼近的最新进展启发，我们提出了一种新的方法，称为压缩傅里叶色散。我们的方法结合了压缩感知和谱色散的思想，用蒙特卡罗抽样代替了结构化色散网格的使用，并使用稀疏恢复技术（如正交匹配追踪和ℓ^1最小化）来近似PDE解的傅里叶系数。我们进行了严格的理论分析，证明了该方法的逼近误差。

    High-dimensional Partial Differential Equations (PDEs) are a popular mathematical modelling tool, with applications ranging from finance to computational chemistry. However, standard numerical techniques for solving these PDEs are typically affected by the curse of dimensionality. In this work, we tackle this challenge while focusing on stationary diffusion equations defined over a high-dimensional domain with periodic boundary conditions. Inspired by recent progress in sparse function approximation in high dimensions, we propose a new method called compressive Fourier collocation. Combining ideas from compressive sensing and spectral collocation, our method replaces the use of structured collocation grids with Monte Carlo sampling and employs sparse recovery techniques, such as orthogonal matching pursuit and $\ell^1$ minimization, to approximate the Fourier coefficients of the PDE solution. We conduct a rigorous theoretical analysis showing that the approximation error of the propose
    
[^122]: 稀疏图学习在时空时间序列上的应用

    Sparse Graph Learning from Spatiotemporal Time Series. (arXiv:2205.13492v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13492](http://arxiv.org/abs/2205.13492)

    本论文提出了一种基于概率评分的方法，通过学习关系依赖的图分布来解决在时空时间序列分析中关系信息不可用的问题，并在时间序列预测问题上展示了有效性。

    

    著名的图神经网络在时空时间序列分析中的杰出成果表明，关系约束为神经预测架构引入了有效的归纳偏差。然而，通常情况下，表征底层数据生成过程的关系信息是不可用的，从而使从数据中推断在后续处理阶段中使用哪个关系图成为了一个问题。我们提出了一种新颖的、有理论依据的概率评分方法，通过最大化任务整个过程的性能来学习关系依赖的图分布。所提出的图学习框架基于蒙特卡洛评分梯度估计的巩固方差缩减技术，并且在理论上有基础，并且在实践中有效。本文主要关注时间序列预测问题，并展示了通过将梯度估计器调整为图学习问题的有效性。

    Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled - yet practical - probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning prob
    
[^123]: 异步、基于选项的多智能体策略梯度：一种条件推理方法

    Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach. (arXiv:2203.15925v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.15925](http://arxiv.org/abs/2203.15925)

    本文提出了一种条件推理方法来解决多智能体策略梯度方法在异步选项执行中的问题，并在基于选项的多智能体合作任务上取得有效结果。

    

    合作的多智能体问题通常需要在智能体之间进行协调，可以通过考虑全局状态的中央策略来实现。多智能体策略梯度（MAPG）方法通常用于学习这种策略，但通常仅适用于具有低级动作空间的问题。在具有大规模状态和动作空间的复杂问题中，将MAPG方法扩展为使用更高级别的动作（也称为选项）以提高策略搜索效率是有优势的。然而，多机器人选项执行通常是异步的，也就是说，智能体可能在不同的时间步骤选择并完成它们的选项。这使得MAPG方法很难推导出一个中央策略并评估其梯度，因为中央策略总是在相同的时间选择新的选项。在这项工作中，我们提出了一种新颖的条件推理方法来解决这个问题，并在代表性的基于选项的多智能体合作任务上证明了其有效性。

    Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks thro
    
[^124]: 基于星形细胞对关键期的神经可塑性神经网络，通过现有和记忆性的大脑可塑性和突触形成实现突触竞争和强度平衡。（arXiv: 2203.11740v12 [cs.NE] UPDATED）

    Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11740](http://arxiv.org/abs/2203.11740)

    该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。

    

    除了突触共享连接权重之外，PNN还包括突触有效范围的权重[14-25]。PNN考虑突触强度平衡在突触吞噬的动态和长度常数之和的静态中[14]，并包含了鱼群行为的先导行为。突触形成在实验和模拟中会抑制树突生成[15]。类似于Spring Boot中的强制韧性，反向回路的记忆持久度梯度也存在。相对较好和较差的梯度信息存储在类似于脑褶的记忆痕迹细胞中，在反向回路的突触形成中。争议认为人类海马神经元的再生能力是否持续到老年，并可能在后期迭代中形成新的更长的回路[17,18]。关闭关键期会导致神经紊乱在实验和模拟中[19]。考虑到负面和正面记忆的持久性，有助于更好地激活突触。

    In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
    
[^125]: 《虚构的翻转：无数据的中毒联邦学习》

    Fabricated Flips: Poisoning Federated Learning without Data. (arXiv:2202.05877v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2202.05877](http://arxiv.org/abs/2202.05877)

    本文介绍了一种名为无数据非定向攻击（DFA）的新方法，它通过合成恶意数据来制造对抗模型，在不需要窃听良性客户端传输或大量任务特定训练数据的情况下实现攻击。这种方法能够在联邦学习中降低生成模型质量，并限制该学习模式的实用性。

    

    对联邦学习的攻击可以严重降低生成模型的质量，限制这种新兴学习模式的实用性，该模式实现了本地化的分散式学习。然而，现有的非定向攻击对许多场景来说都不实际，因为它们假设攻击者知道良性客户端的每个更新，或者攻击者拥有大量的本地训练数据来模仿良性参与方的更新。在本文中，我们提出了一种无数据非定向攻击（DFA），它通过合成恶意数据来制造对抗模型，而无需窃听良性客户端的传输，也无需大量的任务特定训练数据。我们设计了两种DFA的变体，即DFA-R和DFA-G，它们在如何权衡隐蔽性和效果方面有所不同。具体来说，DFA-R通过迭代优化一个恶意数据层来最小化全局模型所有输出的预测置信度，而DFA-G则通过交互式训练恶意数据来实现。

    Attacks on Federated Learning (FL) can severely reduce the quality of the generated models and limit the usefulness of this emerging learning paradigm that enables on-premise decentralized learning. However, existing untargeted attacks are not practical for many scenarios as they assume that i) the attacker knows every update of benign clients, or ii) the attacker has a large dataset to locally train updates imitating benign parties. In this paper, we propose a data-free untargeted attack (DFA) that synthesizes malicious data to craft adversarial models without eavesdropping on the transmission of benign clients at all or requiring a large quantity of task-specific training data. We design two variants of DFA, namely DFA-R and DFA-G, which differ in how they trade off stealthiness and effectiveness. Specifically, DFA-R iteratively optimizes a malicious data layer to minimize the prediction confidence of all outputs of the global model, whereas DFA-G interactively trains a malicious dat
    
[^126]: 继承特征表示

    Successor Feature Representations. (arXiv:2110.15701v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15701](http://arxiv.org/abs/2110.15701)

    继承特征表示（SFR）是一种新的Successor Representations (SR)的表达方式，通过学习继承特征的累积折扣概率来重新评估策略的预期回报。

    

    在强化学习中，迁移学习旨在利用源任务的知识来提高目标任务的学习性能。继承表示（SR）及其扩展的继承特征（SF）是在奖励函数在任务之间发生变化的领域中显著的迁移机制。它们重新评估先前学习策略在新的目标任务中的预期回报，以传递它们的知识。SF框架通过将奖励线性分解为继承特征和奖励权重向量，从而扩展了SR，并允许在高维任务中应用。但这样做的代价是奖励函数与继承特征之间存在线性关系，限制了它在存在这种线性关系的任务中的应用。我们提出了一种新的SR表达方式，即学习继承特征的累积折扣概率，称为继承特征表示（SFR）。关键是，SFR可以重新评估策略的预期回报

    Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies f
    
[^127]: 综合条件估计-优化

    Integrated Conditional Estimation-Optimization. (arXiv:2110.12351v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.12351](http://arxiv.org/abs/2110.12351)

    该论文提出了一种综合条件估计-优化（ICEO）框架，可以在考虑优化问题结构的同时估计随机参数的条件分布，并提供了一些性能保证。

    

    许多实际优化问题涉及具有概率分布的不确定参数，可以使用上下文特征信息进行估计。与先估计不确定参数的分布然后基于估计优化目标的标准方法相反，我们提出了一种综合条件估计-优化（ICEO）框架，该框架在考虑优化问题结构的同时估计随机参数的条件分布。我们直接建模随机参数的条件分布与上下文特征之间的关系，然后用与下游优化问题一致的目标估计概率模型。我们证明了我们的ICEO方法在适度规则条件下是渐进一致的，并进一步提供了一些推广界限形式的有限性能保证。计算上，使用

    Many real-world optimization problems involve uncertain parameters with probability distributions that can be estimated using contextual feature information. In contrast to the standard approach of first estimating the distribution of uncertain parameters and then optimizing the objective based on the estimation, we propose an integrated conditional estimation-optimization (ICEO) framework that estimates the underlying conditional distribution of the random parameter while considering the structure of the optimization problem. We directly model the relationship between the conditional distribution of the random parameter and the contextual features, and then estimate the probabilistic model with an objective that aligns with the downstream optimization problem. We show that our ICEO approach is asymptotically consistent under moderate regularity conditions and further provide finite performance guarantees in the form of generalization bounds. Computationally, performing estimation with
    
[^128]: 基于赌博算法的中央化匹配在点对点借贷的两边市场中的应用

    Bandit based centralized matching in two-sided markets for peer to peer lending. (arXiv:2105.02589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.02589](http://arxiv.org/abs/2105.02589)

    该论文研究了基于赌博算法的中央化匹配在点对点借贷的两边市场中的应用，探讨了解决中央化投资机制难题的可能性，为理解在线平台上点对点借贷的顺序贡献动态提供了一种新的方法。

    

    两边在线平台中的顺序筹款通过依次带来潜在的贡献者，每个贡献者的决策都会影响市场中的其他贡献者，从而实现了点对点借贷。然而，理解在线平台上点对点借贷的顺序贡献动态一直是一个开放的研究问题。这些平台中的中央化投资机制使得很难理解借款人在任何时刻都面临的来自单一借款人的隐含竞争。匹配市场是一种将代理人进行配对的模型，两边代理人在交易方面的偏好配对可以实现市场的分散。我们研究了在投资者或放贷人还面临基于借款人偏好的投资限制的两边平台上的投资设计。这种情况除了已有的借款人竞争外，还在放贷人之间创造了一个隐含的竞争，

    Sequential fundraising in two sided online platforms enable peer to peer lending by sequentially bringing potential contributors, each of whose decisions impact other contributors in the market. However, understanding the dynamics of sequential contributions in online platforms for peer lending has been an open ended research question. The centralized investment mechanism in these platforms makes it difficult to understand the implicit competition that borrowers face from a single lender at any point in time. Matching markets are a model of pairing agents where the preferences of agents from both sides in terms of their preferred pairing for transactions can allow to decentralize the market. We study investment designs in two sided platforms using matching markets when the investors or lenders also face restrictions on the investments based on borrower preferences. This situation creates an implicit competition among the lenders in addition to the existing borrower competition, especia
    
[^129]: 基于多注意力软分区网络的车辆再识别

    Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification. (arXiv:2104.10401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2104.10401](http://arxiv.org/abs/2104.10401)

    本文提出了一种基于多注意力软分区网络的车辆再识别方法，通过引入多软注意力机制来解决由于不同视角和相似车辆之间的内部差异导致的挑战，并避免了嘈杂的注意力图和额外的注释元数据的使用。

    

    车辆再识别通过区分相同和不同车辆的图像来实现识别。由于不同视角下相同车辆之间存在明显的内部差异以及相似车辆之间存在细微的外部差异，这是一个具有挑战性的过程。为解决这个问题，研究人员通过空间注意力机制提取视角感知或部分特定特征，但通常会导致嘈杂的注意力图或需要昂贵的额外注释元数据（如关键点）来提高质量。与此同时，基于研究人员的洞察力，已经提出了各种基于手工设计的特定视角或车辆部件的多注意力架构。然而，这种方法不能保证注意力分支的数量和性质对于现实世界的再识别任务是最优的。为解决这些问题，我们提出了一种基于多软注意力机制的新车辆再识别网络。

    Vehicle re-identification helps in distinguishing between images of the same and other vehicles. It is a challenging process because of significant intra-instance differences between identical vehicles from different views and subtle inter-instance differences between similar vehicles. To solve this issue, researchers have extracted view-aware or part-specific features via spatial attention mechanisms, which usually result in noisy attention maps or otherwise require expensive additional annotation for metadata, such as key points, to improve the quality. Meanwhile, based on the researchers' insights, various handcrafted multi-attention architectures for specific viewpoints or vehicle parts have been proposed. However, this approach does not guarantee that the number and nature of attention branches will be optimal for real-world re-identification tasks. To address these problems, we proposed a new vehicle re-identification network based on a multiple soft attention mechanism for captu
    
[^130]: 具有预分配固定分类器的类增量学习

    Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.08657](http://arxiv.org/abs/2010.08657)

    本文提出了一种具有预分配固定分类器的类增量学习方法，通过利用存储在情节性记忆中的过去数据，并在学习阶段的开始就将一些预分配的输出节点纳入分类损失的计算，解决了神经网络在类增量学习中遗忘先前知识的问题。

    

    在类增量学习中，学习代理面对一系列数据的任务是学习新类别而不忘记以前的类别。神经网络在这种情况下常常会忘记先前获得的知识。为了解决这个问题，有效的方法利用存储在一个情节性记忆中的过去数据，同时扩展最终分类器节点以容纳新的类别。在这项工作中，我们用一个新颖的固定分类器替代了扩展分类器，其中一些预分配的输出节点从学习阶段开始就受到分类损失的影响。与标准扩展分类器相反，这样做有以下好处：(a)未来未见过的类别的输出节点从学习的一开始就能看到负样本，以及逐渐增加的正样本；(b)能够学习不随着新类别的加入而改变其几何配置的特征。

    In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.  In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.  Experi
    
[^131]: 注意力就是一切（arXiv:1706.03762v6 [cs.CL]已更新）

    Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)

    Transformer是一种新的简单网络架构，完全基于注意力机制，取代了复杂的循环神经网络或卷积神经网络。实验证明Transformer在机器翻译任务中的质量更好、并行化效果更佳，且训练时间更短。它在英译德和英译法任务中取得了比其他模型更好的结果。

    

    目前主要的序列转换模型基于复杂的循环神经网络或卷积神经网络的编码器-解码器配置。表现最好的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，Transformer，完全基于注意力机制，不再使用循环和卷积。在两个机器翻译任务上的实验证明，这些模型在质量上优于其他模型，同时更易于并行化，训练时间显著减少。我们的模型在WMT 2014英译德任务上达到28.4的BLEU分数，比现有最好结果（包括集成模型）提高了2个BLEU分。在WMT 2014英译法任务上，在8个GPU上训练了3.5天后，我们的模型获得了41.8的单模型最新BLEU分数，训练成本仅为文献中最好模型的一小部分。我们展示了Transformer架构的优势，并证明了其在机器翻译任务中的重要贡献。

    The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transforme
    

