# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery.](http://arxiv.org/abs/2401.05394) | 该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。 |
| [^2] | [PhilEO Bench: Evaluating Geo-Spatial Foundation Models.](http://arxiv.org/abs/2401.04464) | 本文引入了PhilEO Bench，一个新颖的EO基础模型评估框架，旨在解决EO领域中缺乏标记数据的问题。框架包括测试平台和一个400 GB Sentinel-2数据集，用于建筑密度估计、道路分割和土地覆盖分类的标签，通过实验评估了不同的基础模型。 |
| [^3] | [Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective.](http://arxiv.org/abs/2401.04374) | 本研究提出了一种“数据为中心”的视角，探讨了数据收集、处理和分析在可解释的人工智能中的作用。研究将现有工作分为三个类别：深度模型的解释、训练数据的影响和领域知识的见解。通过数据挖掘操作，我们总结了这些XAI方法。 |
| [^4] | [Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning.](http://arxiv.org/abs/2401.02810) | 本论文提出使用迁移学习来提高物理信息神经网络（PINN）训练的鲁棒性和收敛性。经过两个案例研究，发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数，且需要更少的数据点和更短的训练时间。 |
| [^5] | [Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models.](http://arxiv.org/abs/2401.02333) | 本研究提出了一种创新的方法，通过上下文化表格数据来提高 RAG 系统中处理复杂表格查询的准确性，提高了摘要的效率。 |
| [^6] | [Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes.](http://arxiv.org/abs/2401.01841) | 本文提出了一种自适应蒙特卡洛树搜索算法来应对非稳态环境下的决策问题，解决了传统方法中对环境动态假设的限制和规划过程的悲观性问题。 |
| [^7] | [An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction.](http://arxiv.org/abs/2401.01326) | 这篇论文提出了一种新颖的方法，通过将联合实体和关系抽取问题作为条件序列生成问题来解决。该方法使用了基于跨度的图生成方式，并通过指向机制将生成的输出与原始文本对齐。评估结果证明了该方法的有效性，并获得了竞争性的结果。 |
| [^8] | [WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge.](http://arxiv.org/abs/2401.00910) | 运动分割是自动驾驶中重要的任务，传统的卷积神经网络方法在处理摄像机自我运动、鱼眼镜头径向畸变和时间一致性方面效果较差。本论文提出了WoodScape鱼眼运动分割挑战，使用合成数据集和真实数据集来提高机器学习模型性能，这是首个专注于鱼眼运动分割的比赛之一，旨在探索和评估其潜力和影响。 |
| [^9] | [Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows.](http://arxiv.org/abs/2401.00828) | 本文提出了一种基于神经算子流的方法，通过近似时间相关算子，实现了在量子场论中从底层自由理论到目标理论的离散-连续归一化流。 |
| [^10] | [Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures.](http://arxiv.org/abs/2401.00773) | 提出一种基于随机子空间和子抽样集合的Dirichlet过程高斯混合模型的无监督异常检测方法，提高了计算效率和检测器的鲁棒性。 |
| [^11] | [Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework.](http://arxiv.org/abs/2401.00744) | 在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。 |
| [^12] | [MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining.](http://arxiv.org/abs/2312.17482) | MosaicBERT是一种优化的BERT风格双向编码器，通过引入多项创新技术和优化方案，实现了快速预训练。 |
| [^13] | [RLPlanner: Reinforcement Learning based Floorplanning for Chiplets with Fast Thermal Analysis.](http://arxiv.org/abs/2312.16895) | RLPlanner是一种针对芯片组基于模块的系统的早期布局设计工具，利用强化学习和快速热评估方法来最小化总线长度和温度，并实现了高速化和准确性的提升。 |
| [^14] | [Efficient Reinforcemen Learning with Decoupling Exploration and Utilization.](http://arxiv.org/abs/2312.15965) | 本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。 |
| [^15] | [Leveraging Public Representations for Private Transfer Learning.](http://arxiv.org/abs/2312.15551) | 该论文探讨了如何利用公共数据来改进私有学习的问题。研究发现，通过学习公共数据中的共享表示，可以在两种迁移学习场景中实现最优的学习效果。在单任务迁移场景中，算法在给定子空间范围内搜索线性模型，并实现了最优超额风险。在多任务个性化场景中，足够的公共数据可以消除私有协调需求，并通过纯局部学习达到相同的效用。 |
| [^16] | [A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production.](http://arxiv.org/abs/2312.14972) | 本文提出了一种系统的评估方法来比较专有LLMs和开源SLMs的权衡，并设计了一个自动化分析工具SLaM来测试产品功能。在实际产品功能替换时，对于现有能力是否能够被开源SLMs代替，还需要进一步研究。 |
| [^17] | [Time-changed normalizing flows for accurate SDE modeling.](http://arxiv.org/abs/2312.14698) | 本论文提出了一种新的动态正则化流的变种，即时间变换正则化流(TCNF)，通过时间变形的方法能有效地建模一些无法用其他方法建模的随机微分方程(SDEs)，包括著名的奥恩斯坦-乌伦贝克过程，并泛化了先前的方法，从而提高了结果的准确度和推断和预测的精度。 |
| [^18] | [Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport.](http://arxiv.org/abs/2312.14507) | 本文提出了一种无监督的谐波参数估计方法，利用可微分的DSP和频谱最优传输理论来进行谐波信号的自编码和重构，为神经音频应用中的无监督参数估计提供了一个有希望的方向。 |
| [^19] | [WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data.](http://arxiv.org/abs/2312.14129) | WellFactor是一种使用综合嵌入医疗数据的患者分类方法，并通过使用受约束的低秩逼近、结合标签信息来优化嵌入结果，同时具有即时计算新数据嵌入的特点。在实际医疗数据上得到了验证。 |
| [^20] | [StemGen: A music generation model that listens.](http://arxiv.org/abs/2312.08723) | 该论文介绍了一种能够听音乐并生成音乐的模型。采用了非自回归的Transformer模型架构以及一些新颖的架构和采样改进方法。该模型能够达到最先进的文本条件模型的音频质量，并在音乐连贯性方面表现出色。 |
| [^21] | [Morphological Profiling for Drug Discovery in the Era of Deep Learning.](http://arxiv.org/abs/2312.07899) | 形态学特征分析在表型药物发现中具有重要价值。深度学习技术在分析大规模高内容图像方面取得了显著进展，促进了药物作用机制的理解和新治疗方法的开发。 |
| [^22] | [Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale.](http://arxiv.org/abs/2312.07586) | 该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。 |
| [^23] | [Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference.](http://arxiv.org/abs/2312.05910) | 这篇论文介绍了一种将集合卡尔曼滤波引入变分推理框架的方法，用于近似高斯过程状态空间模型的后验分布，并且有效地利用了潜在状态和动力学之间的依赖关系，减少了变分参数的数量。 |
| [^24] | [FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning.](http://arxiv.org/abs/2312.04432) | FreqFed是一种基于频谱分析的新方法，用于检测和减轻联邦学习中的针对性和非针对性的毒化攻击。 |
| [^25] | [CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models.](http://arxiv.org/abs/2312.04350) | 该论文提出了一个新的NLP任务，评估语言模型在因果推理方面的能力。作者构建了一个大规模的数据集CLadder，并利用oracle因果推理引擎将符号问题转化为自然语言。研究结果表明多个LLMs在该数据集上的表现，并引入并评估了一种定制的链式推理机制。 |
| [^26] | [Graph Convolutions Enrich the Self-Attention in Transformers!.](http://arxiv.org/abs/2312.04234) | 这项研究通过引入图卷积来改进Transformer模型中的自注意力机制，并在计算机视觉、自然语言处理等多个领域展示了其性能提升。 |
| [^27] | [Multi-Weight Ranking for Multi-Criteria Decision Making.](http://arxiv.org/abs/2312.03006) | 这项研究将统计学的锥形分布函数转化为多准则决策制定工具，并通过扩展排名函数到集合，建立了集合优化方法与基于集合的多目标优化之间的联系。在机器学习中具有潜在应用。 |
| [^28] | [Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More.](http://arxiv.org/abs/2312.02708) | 本文提出了一种考虑任务对称性的可证明的对抗鲁棒性概念，并通过选择合适的模型和认证方法来实现鲁棒性。同时，通过开发保持对称性的随机平滑的框架，解决了对于具有连续对称性的模型的认证方法不可用的问题。 |
| [^29] | [Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler.](http://arxiv.org/abs/2312.02683) | 这项研究通过使用多个数据库模拟不匹配的条件，系统评估了基于扩散模型的语音增强模型的泛化性能，并尝试了新的设计方面，如噪声调度和采样器方法。 |
| [^30] | [Mitigating Data Injection Attacks on Federated Learning.](http://arxiv.org/abs/2312.02102) | 本文提出了一种新的技术来检测和缓解联邦学习系统中的数据注入攻击，通过在训练过程中忽略被怀疑为攻击者的代理的数据来提高模型的性能。 |
| [^31] | [Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation.](http://arxiv.org/abs/2312.01520) | 本文提出了一种计算贝叶斯网络中Shannon熵和Kullback-Leibler散度的高效算法，并通过一系列数值示例进行了演示。此外，还展示了如何将高斯贝叶斯网络中KL的计算复杂度从立方降低到二次。 |
| [^32] | [$\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks.](http://arxiv.org/abs/2311.18744) | 本文对等变量量子神经网络（EQNN）和量子神经网络（QNN）与经典神经网络的性能进行了全面比较分析，结果表明《$\mathbb{Z}_2\times \mathbb{Z}_2$》EQNN和QNN在较小的参数集和适中的训练数据样本上表现优越。 |
| [^33] | [Fixed point actions from convolutional neural networks.](http://arxiv.org/abs/2311.17816) | 本研究使用晶格规范等变卷积神经网络（L-CNN）描述了基于重整化群变换的固定点作用（FP），这种方法更准确地参数化FP作用，可以规避临界减速和拓扑冻结问题，并在粗晶格上产生具有非常小晶格效应的物理预测。 |
| [^34] | [Dynamic Fault Characteristics Evaluation in Power Grid.](http://arxiv.org/abs/2311.16522) | 该论文提出了一种在电力系统中进行故障检测的新方法，通过图神经网络识别故障节点，并利用前后时间段内节点的状态来辅助当前故障检测。实验证明该方法准确可靠，并提供了对故障节点传播的定性分析。 |
| [^35] | [Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE.](http://arxiv.org/abs/2311.16167) | 这项工作提出了一种基于移动网格PDE的移动采样物理信息神经网络(MMPDE-Net)，通过解决移动网格PDE来自适应生成新的采样点，并且结合物理信息神经网络（PINN）提出了移动采样PINN（MS-PINN）的框架。数值实验验证了MS-PINN相对于PINN的性能改善。 |
| [^36] | [Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text.](http://arxiv.org/abs/2311.15565) | 本研究评估了混合深度学习模型在准确区分AI生成文本和人类写作方面的有效性。通过应用先进的自然语言处理技术和复杂的神经网络，我们的研究成功地检测到了AI生成文本和人类写作之间的微妙差异。 |
| [^37] | [Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series.](http://arxiv.org/abs/2311.13326) | 本文通过在复杂时间序列数据上探索课程学习和模仿学习的方法，发现课程学习是改善复杂时间序列控制任务性能的新途径，而模仿学习也应该被应用。 |
| [^38] | [Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging.](http://arxiv.org/abs/2311.12082) | 本文提出了一种基于Vision Transformer的轻量级超声单角度平面波成像波束形成器（Tiny-VBF），与最先进的深度学习模型相比，它具有更快的包络检测速度，更高的对比度和分辨率。 |
| [^39] | [Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods.](http://arxiv.org/abs/2311.11798) | 本论文提出了一种基于梯度和无导数优化方法的连续时空模型的算子学习框架，具有空间和时间上的分辨率不变性，并能够在短期时间序列和长期统计数据上高效训练。 |
| [^40] | [GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection.](http://arxiv.org/abs/2311.09620) | 本文研究了在深度神经网络中检测越界样本的问题，提出了一种基于梯度的归因异常性方法，用于量化内分布和越界数据之间的差异。通过引入零膨胀异常和通道平均异常，我们提出了一种简单而有效的GAIA方法，可以有效检测越界样本。 |
| [^41] | [Knowledge Graph Construction in Power Distribution Networks.](http://arxiv.org/abs/2311.08724) | 本文提出了一种在电力分配网络中构建知识图谱的方法，该方法利用实体特征，在分配网络的知识图谱和分配文本中进行匹配，通过实验证明了其在电力分配知识图谱构建任务中的高准确性。 |
| [^42] | [How do Minimum-Norm Shallow Denoisers Look in Function Space?.](http://arxiv.org/abs/2311.06748) | 研究了最小范数浅层去噪器在函数空间中的表现，推导出一元数据和多元数据上的闭合形式，并发现其具有收缩性和较好的泛化能力。 |
| [^43] | [Deep Learning Architecture for Network-Efficiency at the Edge.](http://arxiv.org/abs/2311.05739) | 本文提出了一种自适应压缩感知分离学习方法，将深度学习模型与边缘云资源集成，从而实现网络高效性和更快的计算速度，适用于部署在较弱设备上。 |
| [^44] | [Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures.](http://arxiv.org/abs/2311.05559) | 我们提出了一种新颖的混合架构，通过自编码器将输入数据压缩后传递给量子部分，实现了在混合量子机器学习中分离量子和经典贡献。 |
| [^45] | [Locating Cross-Task Sequence Continuation Circuits in Transformers.](http://arxiv.org/abs/2311.04131) | 通过分析和比较Transformer模型中类似的序列继续任务的电路，研究发现共享的计算结构可以提高模型的行为预测能力、错误识别能力和编辑过程的安全性。 |
| [^46] | [Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization.](http://arxiv.org/abs/2311.03351) | Uni-O4提出了统一的离线和在线深度强化学习方法，通过对齐目标实现了无缝传递，增强了学习范式的灵活性。在离线阶段，Uni-O4利用多样的集合策略解决了估计行为策略和离线数据集不匹配的问题。 |
| [^47] | [Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder.](http://arxiv.org/abs/2311.02794) | 本研究提出了一种稀疏添加机制移位变分自动编码器（SAMS-VAE），用于建模细胞的扰动情况，并结合复合性、解缠和可解释性。通过稀疏化处理全局潜变量，SAMS-VAE能够识别出特定于干扰的潜在子空间，并在多个任务上进行了定量和定性评估。 |
| [^48] | [Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects.](http://arxiv.org/abs/2311.02332) | 这项调查研究了多模式机器学习在医学图像分析和临床决策支持系统中的影响，强调了在多模态表示、融合、翻译、对齐和联合学习方面的挑战和创新，以及如何解决数据偏差和“大数据”稀缺性等问题。 |
| [^49] | [Should Under-parameterized Student Networks Copy or Average Teacher Weights?.](http://arxiv.org/abs/2311.01644) | 这项研究探讨了在欠参数化情况下，学生网络是否应该复制教师神经元或平均一组教师神经元的权重。研究发现对于特定的网络结构和输入分布，当教师网络的输入向量正交且输出权重为酉时，复制-平均配置将达到优化结果，其中大部分学生神经元复制一个教师神经元，最后一个学生神经元对所有教师神经元取平均值。 |
| [^50] | [Deep learning based Image Compression for Microscopy Images: An Empirical Study.](http://arxiv.org/abs/2311.01352) | 本研究实证分析了经典和基于深度学习的图像压缩方法在深度学习图像处理模型中的影响，并以基于深度学习的无标签预测模型为例展示了图像压缩的有效性，以减小数据尺寸并保留必要信息，从而减轻数据管理基础设施的负担并方便数据共享或云计算。 |
| [^51] | [Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion.](http://arxiv.org/abs/2311.01017) | 本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。 |
| [^52] | [Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms.](http://arxiv.org/abs/2310.20598) | 本论文介绍并研究了在线转换及其带有切换成本的问题，并提出了具有竞争力的阈值算法以及学习增强算法，这些算法在最小化和最大化变体中都表现出优越性能。 |
| [^53] | [Integrating Pre-trained Language Model into Neural Machine Translation.](http://arxiv.org/abs/2310.19680) | 该论文提出了PiNMT模型，将预训练语言模型整合到神经机器翻译中，通过三个关键部分和两种训练策略，实现了在IW数据集上的最先进性能。 |
| [^54] | [The Memory Perturbation Equation: Understanding Model's Sensitivity to Data.](http://arxiv.org/abs/2310.19273) | 这个论文介绍了记忆扰动方程（MPE），该方程通过应用贝叶斯原理将模型的敏感性与训练数据的扰动联系起来，并且能够准确预测模型在未见测试数据上的泛化能力。 |
| [^55] | [Isometric Motion Manifold Primitives.](http://arxiv.org/abs/2310.17072) | Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods. |
| [^56] | [How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels.](http://arxiv.org/abs/2310.16652) | 本文研究了联邦学习对上行和下行通信错误的鲁棒性。理论分析表明，鲁棒性取决于客户端数量和模型参数的数值范围。实验证明上行通信可以容忍更高的误码率比下行通信。 |
| [^57] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^58] | [Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules.](http://arxiv.org/abs/2310.14753) | 重新思考分子掩码图模型中的分词器和解码器，通过对分子分词器的总结和解码器的探索，填补了分词器和解码器的研究空白。 |
| [^59] | [Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain.](http://arxiv.org/abs/2310.14053) | 这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。 |
| [^60] | [DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data.](http://arxiv.org/abs/2310.13349) | DeepFDR是一种基于深度学习的虚警控制方法，通过利用无监督的图像分割技术解决神经影像数据中的多重检验问题，并在实验证明其相对于现有方法具有卓越的性能。 |
| [^61] | [Constrained Reweighting of Distributions: an Optimal Transport Approach.](http://arxiv.org/abs/2310.12447) | 本文提出了一种最优传输方法，通过引入非参数化的分布约束权重，并利用最大熵原理和最优传输工具设计了一个通用框架，以实现对观测数据的最优权重调整。这种方法在不同的应用场景中展现了灵活性和多功能性。 |
| [^62] | [A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs.](http://arxiv.org/abs/2310.12248) | 这个论文介绍了一种基于模型的PAC学习算法，用于在MDP中学习ω-regular目标，不需要系统拓扑的先前知识。 |
| [^63] | [Recasting Continual Learning as Sequence Modeling.](http://arxiv.org/abs/2310.11952) | 本文将连续学习重塑为序列建模问题，并提出了利用序列模型进行连续学习的方法。通过采用元连续学习框架，需要对序列模型进行多次连续学习episode上的元级训练。实验证明序列模型可以成为一种吸引人的通用连续学习解决方案。 |
| [^64] | [Penetrative AI: Making LLMs Comprehend the Physical World.](http://arxiv.org/abs/2310.09605) | 本文探讨了渗透式人工智能的概念，旨在使LLMs能够通过物联网传感器与执行器与物理世界进行交互和推理。初步研究结果表明，LLMs具有独特的能力，能够应用内嵌的世界知识解释物联网传感器数据并进行物理领域的推理。 |
| [^65] | [Counting and Algorithmic Generalization with Transformers.](http://arxiv.org/abs/2310.08661) | 这项研究分析了在计数任务中的算法推广，并证明了标准的Transformer的架构决策会阻碍其在超出分布任务上的性能。通过消除问题操作，修改后的Transformer在计数方面展示出了良好的算法推广性能。 |
| [^66] | [Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization.](http://arxiv.org/abs/2310.07985) | 提出了一种具有强大通用性的轻编码器和重解码器（LEHD）模型，通过动态学习节点间关系，解决了神经组合优化在大规模实例上的问题，并应用于旅行商问题和容量限制车辆路径规划问题。 |
| [^67] | [Towards Causal Deep Learning for Vulnerability Detection.](http://arxiv.org/abs/2310.07958) | 本文提出了一种针对漏洞检测的因果深度学习方法CausalVul，通过引入因果性，并设计新的扰动，解决了深度学习漏洞检测中模型不稳定和泛化性能差的问题。 |
| [^68] | [On sparse regression, Lp-regularization, and automated model discovery.](http://arxiv.org/abs/2310.06872) | 该论文通过结合正则化和物理约束的方法，利用神经网络进行自动模型发现，探索了非线性回归中稀疏回归和特征提取的潜力，并针对材料建模提出了常见的指导方针和趋势。 |
| [^69] | [The Impact of Equal Opportunity on Statistical Discrimination.](http://arxiv.org/abs/2310.04585) | 本文通过修改统计性歧视模型，考虑了由机器学习生成的可合同化信念，给监管者提供了一种超过肯定行动的工具，通过要求公司选取一个平衡不同群体真正阳性率的决策策略，实现机会平等来消除统计性歧视。 |
| [^70] | [Feature Cognition Enhancement via Interaction-Aware Automated Transformation.](http://arxiv.org/abs/2309.17011) | 该论文提出了一种交互感知的增强生成方法，以改进自动特征工程的表示空间，解决重构特征空间不可理解和缺乏系统探索的问题。 |
| [^71] | [Learning to Transform for Generalizable Instance-wise Invariance.](http://arxiv.org/abs/2309.16672) | 该论文提出了一种学习转换以实现通用的实例不变性的方法。通过使用归一化流来预测图像的变换分布，并对预测结果进行平均，可以实现对不同实例之间的对齐，从而推广不变性的类别间的应用。这种方法还可以适应超出分布范围的姿势，并且可以学习更广泛的变换范围。 |
| [^72] | [Unsupervised Fact Verification by Language Model Distillation.](http://arxiv.org/abs/2309.16540) | 本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。 |
| [^73] | [Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification.](http://arxiv.org/abs/2309.16369) | 本研究探索了在音频场景分类任务中损失函数最小值的锐度与泛化之间的关联。研究发现锐度更高的最小值具有更好的泛化能力，尤其是对于从之前未见设备录制的领域外数据来说。研究还发现优化器的选择是最小值锐度的主要影响因素，并讨论了在可比性方面的相关限制。 |
| [^74] | [Towards General-Purpose Text-Instruction-Guided Voice Conversion.](http://arxiv.org/abs/2309.14324) | 本文介绍了一种通过文本指令进行语音转换的通用模型。与传统方法不同，该模型利用文本指令修改语音的韵律和情感信息，具有较高的灵活性和特异性。实验证明了该模型能够理解指令并产生合理结果。 |
| [^75] | [Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds.](http://arxiv.org/abs/2309.13915) | 本研究探讨了神经策略镜像梯度算法在低维流形上的样本复杂性。研究发现在每次迭代中，卷积神经网络可以很好地逼近价值函数和策略，且逼近误差受网络大小的影响，并且可以继承之前网络的平滑性。 |
| [^76] | [AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer.](http://arxiv.org/abs/2309.12689) | AMPLIFY提出了一种基于注意力机制的Mixup方法，用于减少原始样本中的噪音和异常值对于模型的影响，并在文本分类任务中表现出更好的性能。 |
| [^77] | [Parallelizing non-linear sequential models over the sequence length.](http://arxiv.org/abs/2309.12252) | 本论文提出了一种并行算法，能够加速GPU对于顺序模型的评估速度，提高了3个数量级，而不降低输出准确性。该算法适用于各种架构，并在长时间序列分类问题中发现了门控循环单元的有效性。 |
| [^78] | [Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization.](http://arxiv.org/abs/2309.11856) | 本论文提出了一种使用改进的方差最小化的分块量化策略，用于压缩图神经网络的激活，实现内存消耗的降低和运行时的加速。 |
| [^79] | [Single and Few-step Diffusion for Generative Speech Enhancement.](http://arxiv.org/abs/2309.09677) | 本文提出了一种通过两阶段训练的方法来解决扩散模型在语音增强中的限制。第一阶段使用生成去噪评分匹配损失训练扩散模型，第二阶段通过解决反向过程来计算增强信号，并使用预测损失进行比较。这种方法只需要5个函数评估就能达到与基准模型相同的性能，而不是60个函数评估。 |
| [^80] | [A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability.](http://arxiv.org/abs/2309.07156) | 本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。 |
| [^81] | [A Sequentially Fair Mechanism for Multiple Sensitive Attributes.](http://arxiv.org/abs/2309.06627) | 本论文提出了一个顺序框架来逐步实现对多个敏感特征的公平性，通过利用多边际Wasserstein重心扩展了标准的强人口平等概念，并提供了闭式解来解释敏感特征之间的相关性。 |
| [^82] | [Computation and Communication Efficient Federated Learning over Wireless Networks.](http://arxiv.org/abs/2309.01816) | 提出了一种计算和通信高效的无线网络联合学习框架，通过模型剪枝和个性化，在分布式学习中减少计算和通信延迟，并提高非独立同分布数据设备的学习准确度。 |
| [^83] | [Learning to Taste: A Multimodal Wine Dataset.](http://arxiv.org/abs/2308.16900) | 这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。 |
| [^84] | [Interpolation of mountain weather forecasts by machine learning.](http://arxiv.org/abs/2308.13983) | 本研究提出了一种通过机器学习来插值山区天气预报的方法，通过利用当前观测数据和周围平原的预报数据，解决了在复杂地形中数值模拟精度降低的问题，并研究了在降水预测中使用二元交叉熵的方法。 |
| [^85] | [FoX: Formation-aware exploration in multi-agent reinforcement learning.](http://arxiv.org/abs/2308.11272) | FoX是一个新颖的多智能体强化学习框架，通过减少探索空间，并引导智能体在不同形成中访问有意义的状态，显著提高了在合作多智能体任务中的性能。 |
| [^86] | [Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning.](http://arxiv.org/abs/2308.07272) | 本文介绍了一种基于对话的策略梯度离散提示优化方法，通过设计多轮对话对齐策略和高效的提示筛选度量，实现了在少样本学习任务中生成高质量提示集的目标。 |
| [^87] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^88] | [Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators.](http://arxiv.org/abs/2308.05141) | 本文提出了使用深度神经操作器网络来模拟在真实的3D场景中带有移动源的声音传播。通过学习一个紧凑的代理模型，能够快速预测声音传播，避免了计算和存储脉冲响应的离线计算。 |
| [^89] | [Universal Fuzzing via Large Language Models.](http://arxiv.org/abs/2308.04748) | 本文介绍了Fuzz4All，这是第一个能够针对许多不同的输入语言和这些语言的许多不同功能进行模糊测试的通用工具。 |
| [^90] | [An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid.](http://arxiv.org/abs/2307.16149) | 这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。 |
| [^91] | [FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning.](http://arxiv.org/abs/2307.13716) | FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。 |
| [^92] | [QuIP: 2-Bit Quantization of Large Language Models With Guarantees.](http://arxiv.org/abs/2307.13304) | 本文提出了一种新的基于无关处理的大型语言模型（LLMs）参数量化方法QuIP，通过使权重和Hessian矩阵与坐标轴不对齐，实现了准确的量化结果。经过经验实验，我们发现我们的方法改善了现有的量化算法，并且首次在仅使用两比特的情况下获得了可行的LLM量化结果。 |
| [^93] | [Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model.](http://arxiv.org/abs/2307.10443) | 本文提出了一种新的注意力模式，使用图增强自注意力机制将从异构图中导出的推理知识整合到变压器架构中，从而克服了变压器模型在复杂推理任务中的限制。通过全局-局部注意力、图注意力和关系类型考虑，优化了实体和单词之间的注意力。该模式与相对位置标签相结合，能够与LUKE的实体感知自注意力机制相集成。 |
| [^94] | [Accelerated Optimization Landscape of Linear-Quadratic Regulator.](http://arxiv.org/abs/2307.03590) | 本文介绍了用于处理线性二次调节器（LQR）问题的加速优化框架，分析了SLQR和OLQR问题的收敛性。同时提出了LQR性能准则的Lipschitz Hessian特性，为现代优化技术的应用提供了重要指导。 |
| [^95] | [Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs.](http://arxiv.org/abs/2307.03393) | 本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。 |
| [^96] | [RanPAC: Random Projections and Pre-trained Models for Continual Learning.](http://arxiv.org/abs/2307.02251) | 这项研究提出了一种利用预训练模型进行连续学习的方法，通过使用随机投影器和类原型累积来避免遗忘问题。 |
| [^97] | [CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery.](http://arxiv.org/abs/2307.00859) | CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。 |
| [^98] | [ENN: A Neural Network with DCT Adaptive Activation Functions.](http://arxiv.org/abs/2307.00673) | ENN是一种具有DCT自适应激活函数的神经网络模型，通过使用反向传播自适应地调整激活函数，提供了高度的灵活性和表现力。在解释网络收敛过程中，我们恢复了每个激活函数在输出空间中的响应，即“凸起”。通过实验证明了该模型在多个任务上的性能优势。 |
| [^99] | [iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models.](http://arxiv.org/abs/2306.17361) | 本文提出了一种识别非线性加性噪声模型中因果机制转变的方法，该方法专注于在相关的结构因果模型中识别功能机制的变化，而不需要估计整个有向无环图(DAG)的结构。 |
| [^100] | [Adaptive Bernstein Change Detector for High-Dimensional Data Streams.](http://arxiv.org/abs/2306.12974) | 本文提出了一个适用于高维数据流的自适应伯恩斯坦变化检测器，具有准确地识别变化发生的时间与子空间，并能够量化严重程度的特性。 |
| [^101] | [Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent.](http://arxiv.org/abs/2306.11589) | 本文探索了使用随机梯度下降算法从高斯过程后验中采样的方法，该方法计算高效且能在足够覆盖数据的区域和足够远离数据的区域中产生接近真实后验的预测分布。 |
| [^102] | [Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions.](http://arxiv.org/abs/2306.10506) | 本论文研究了针对非对数凹分布的MCMC算法的条件混合问题，通过分析Poincar\'e型不等式在状态空间子集上的成立情况，发现条件分布的混合速度快于全局混合速度，对于方差混合模型的采样、参数估计以及具有良好连接的局部极小值的吉布斯采样等问题具有重要影响。 |
| [^103] | [A Smooth Binary Mechanism for Efficient Private Continual Observation.](http://arxiv.org/abs/2306.09666) | 本文提出了一种可微且凸的替代二进制机制的方法，用于解决隐私问题中发布私有前缀求和题的效率问题。该方法比标志性的二进制机制具有更低的方差和更好的观测时间保证，并且在计算和空间要求方面更加有效。 |
| [^104] | [Residual Q-Learning: Offline and Online Policy Customization without Value.](http://arxiv.org/abs/2306.09526) | 该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。 |
| [^105] | [AQuA: A Benchmarking Tool for Label Quality Assessment.](http://arxiv.org/abs/2306.09467) | AQuA是一款用于标签质量评估的基准测试工具，目的是评估标签噪声存在下的机器学习方法。该基准测试环境包括数据模拟、质量不同的真实数据集和几种最先进的标签噪声消除方法。 |
| [^106] | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer.](http://arxiv.org/abs/2306.06446) | ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。 |
| [^107] | [Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application.](http://arxiv.org/abs/2306.05323) | 该研究创建了意大利神经精神命名实体识别数据集，并使用巨型语言模型开发出多中心识别模型，整体 F1得分为84.77%。该模型将帮助临床从业者从非结构化的医疗记录中自动提取信息。 |
| [^108] | [Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning.](http://arxiv.org/abs/2306.04746) | 该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。 |
| [^109] | [Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman.](http://arxiv.org/abs/2306.03266) | 本文提出了$(k, t)$-FWL和$k$-FWL+两种方法，理论上证明了它们可以在$O(n^2)$的空间复杂度下，解决图同构问题。 |
| [^110] | [Explore to Generalize in Zero-Shot RL.](http://arxiv.org/abs/2306.03072) | 本文针对零样本强化学习中的泛化问题进行了研究，通过学习一种有效探索领域的策略，成功解决了在像ProcGen Maze这样的问题上基于不变性的方法失败的困境。 |
| [^111] | [Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data.](http://arxiv.org/abs/2306.01222) | 本论文提出UnMixMatch框架，可以从非约束的未标记数据中学习有效的表征以提高性能，以解决大多数半监督方法基于有标记和未标记样本来自同一分布的假设。该框架具有强正则化作用的硬增强监督学习器、用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。 |
| [^112] | [Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption.](http://arxiv.org/abs/2306.00196) | 本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。 |
| [^113] | [Learning to solve Bayesian inverse problems: An amortized variational inference approach.](http://arxiv.org/abs/2305.20004) | 本文提出了一种基于深度神经网络的参数化表示后验分布的摊销变分推理方法，以实现实时推理的目的，可应用于流体力学中的参数估计和流场重构等领域。 |
| [^114] | [Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming.](http://arxiv.org/abs/2305.19706) | 本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。 |
| [^115] | [Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models.](http://arxiv.org/abs/2305.18455) | 本文提出了一种通用框架 Diff-Instruct，能够以无需数据方式将预训练扩散模型中的知识传递给其他生成模型，仅需预训练 DM 和一个数据集。该框架是建立在严谨的数学基础上的，指导过程直接对应于最小化一种新型散度——Integral Kullback-Leibler (IKL) 散度。我们的方法在半监督学习、图像合成和视频预测中展示了其优越性。 |
| [^116] | [Neural Task Synthesis for Visual Programming.](http://arxiv.org/abs/2305.18342) | 该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。 |
| [^117] | [Translatotron 3: Speech to Speech Translation with Monolingual Data.](http://arxiv.org/abs/2305.17547) | Translatotron 3提出了一种新方法，使用单语数据进行语音到语音翻译，无需配对的数据或专业建模，展示了保留语言/非语言信息的能力。 |
| [^118] | [Learning and Collusion in Multi-unit Auctions.](http://arxiv.org/abs/2305.17402) | 本论文研究了在多单位拍卖中学习和勾结的问题，分析了拍卖的离线和在线性质，提出了一个多项式时间算法来解决离线设置中的玩家出价最大化问题。 |
| [^119] | [Strategic Classification under Unknown Personalized Manipulation.](http://arxiv.org/abs/2305.16501) | 探讨了在未知且个性化操纵影响下的战略分类问题，提出了一个交互模型，引入了个性化的定义，旨在解决强化学习中策略操纵问题。 |
| [^120] | [DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method.](http://arxiv.org/abs/2305.16284) | 本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。 |
| [^121] | [Koopman Kernel Regression.](http://arxiv.org/abs/2305.16215) | 提出了一种基于Koopman核的回归方法，用于预测非线性动力系统的时间演变。该方法在机器人操作，视频预测和交通预测等各种应用中均有优异表现，并具有可证明的学习理论保证。 |
| [^122] | [How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits.](http://arxiv.org/abs/2305.15944) | 本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。 |
| [^123] | [Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning.](http://arxiv.org/abs/2305.14062) | 该论文提出了一种不依赖于振幅的光 plethysmography (PPG) 机器学习方法，通过可见性图和迁移学习实现了对心率和血管老化等生物特征的稳健估计和预测。 |
| [^124] | [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings.](http://arxiv.org/abs/2305.11554) | 本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。 |
| [^125] | [Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence.](http://arxiv.org/abs/2305.11417) | 本文从功能等价的角度出发研究前馈神经网络的复杂性，发现利用置换不变性的特性可以降低网络的复杂度，通过过参数化可以增加训练网络的容易程度，并对深度学习中的优化和泛化理解具有重要意义。 |
| [^126] | [Democratized Diffusion Language Model.](http://arxiv.org/abs/2305.10818) | 本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。 |
| [^127] | [Multi-task convolutional neural network for image aesthetic assessment.](http://arxiv.org/abs/2305.09373) | 本文提出了一种多任务卷积神经网络，可以同时预测图像的总体美学评分和美学属性，并在实验中表现优异，达到接近人类表现的整体美学评分。 |
| [^128] | [Phase transitions in the mini-batch size for sparse and dense neural networks.](http://arxiv.org/abs/2305.06435) | 本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。 |
| [^129] | [Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks.](http://arxiv.org/abs/2304.09221) | 本文通过随机梯度下降算法研究了解析度函数为非凸的深度神经网络的全局收敛性，证明了当机器学习噪声的尺度与目标函数相等时，在局部区域内初始化后，以正的概率能够收敛到该区域内的全局最小值。 |
| [^130] | [Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training.](http://arxiv.org/abs/2304.05492) | 本研究利用级联指导下的对抗训练方法，增强了串联推荐模型的鲁棒性和准确性，取得了比已有方法更好的结果。 |
| [^131] | [Pgx: Hardware-accelerated parallel game simulation for reinforcement learning.](http://arxiv.org/abs/2303.17503) | Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。 |
| [^132] | [MGTBench: Benchmarking Machine-Generated Text Detection.](http://arxiv.org/abs/2303.14822) | 本文提出了MGTBench框架，旨在解决机器生成文本检测中现有评估方法的不足。该框架通过广泛评估和公开数据集，提供了全面的MGT检测评估，使研究人员能够比较不同检测方法的有效性。 |
| [^133] | [Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions.](http://arxiv.org/abs/2303.14226) | 提出了一种在组合干预下进行因果推断的模型，通过施加潜在结构跨越单位和组合，在降低实验数量和处理混杂问题方面有着良好表现。 |
| [^134] | [The Quantization Model of Neural Scaling.](http://arxiv.org/abs/2303.13506) | 该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。 |
| [^135] | [Difficulty in learning chirality for Transformer fed with SMILES.](http://arxiv.org/abs/2303.11593) | 应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。 |
| [^136] | [To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning.](http://arxiv.org/abs/2303.03374) | 该论文研究了在迁移学习中使用单个预训练检查点微调的模型集合，发现通过更好地探索预训练基域可以改进集成模型，但离开基域会导致失去迁移学习的好处，并且降低集成质量。作者提出了一种更有效的修改方法StarSSE，可以产生更强的集成模型和均匀的模型混合。 |
| [^137] | [CAMEL: Curvature-Augmented Manifold Embedding and Learning.](http://arxiv.org/abs/2303.02561) | CAMEL是一种新的方法，利用黎曼流形上的拓扑度量和独特的黎曼度量进行高维数据分类、降维和可视化。它通过平滑分区统一算子将局部正交投影转换为全局嵌入，并提供了聚类显著特征的物理解释。CAMEL在各种基准数据集上表现优于其他方法，特别是对于高维数据集。 |
| [^138] | [Comparative Study of Coupling and Autoregressive Flows through Robust Statistical Tests.](http://arxiv.org/abs/2302.12024) | 本论文通过比较耦合流和自回归流的不同架构和多样目标分布，利用各种测试统计量进行性能比较，为正规化流的生成模型提供了深入的研究和实证评估。 |
| [^139] | [Information Theoretic Lower Bounds for Information Theoretic Upper Bounds.](http://arxiv.org/abs/2302.04925) | 本文研究了在随机凸优化中，输出模型和经验样本之间的互信息与算法泛化之间的关系。研究结果表明，现有的信息理论泛化界限不足以捕捉到像SGD和正则化ERM这样具有维度无关样本复杂度的算法的泛化能力。 |
| [^140] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^141] | [Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy.](http://arxiv.org/abs/2302.01463) | 本文研究梯度下降在线性相关噪声下的表现，提出了新的矩阵分解方法用于不同ially private optimization。 |
| [^142] | [Efficient Node Selection in Private Personalized Decentralized Learning.](http://arxiv.org/abs/2301.12755) | 这篇论文提出了私人个性化分散学习（PPDL）的方法，利用安全聚合和相关对抗式多臂老虎机优化来实现高效的节点选择，并保护节点的隐私。作者通过利用不同合作者之间的依赖关系，仅基于聚合模型就能够有效地识别合适的合作者。实验证明PPDL在标签和协变量偏移下的模型性能优于先前的非私密方法。 |
| [^143] | [Learned Interferometric Imaging for the SPIDER Instrument.](http://arxiv.org/abs/2301.10260) | 本文提出了两种利用深度学习从SPIDER仪器测量数据中重建图像的方法，通过学习先验信息来提高重建质量，并将重建时间缩短到约10毫秒，实现了SPIDER仪器的实时成像。 |
| [^144] | [Topological Learning in Multi-Class Data Sets.](http://arxiv.org/abs/2301.09734) | 本文将拓扑数据分析技术应用于多类数据集中，通过构建拓扑分类器和简单复合体，研究了拓扑复杂性对前馈深度神经网络学习的影响，并验证了拓扑复杂性与DNN学习之间的负相关性。 |
| [^145] | [Pontryagin Optimal Control via Neural Networks.](http://arxiv.org/abs/2212.14566) | 本文将神经网络与庞特里亚金最大原理相结合，提出了一个样本高效的框架NN-PMP-Gradient，可以应用于具有未知和复杂动力学的系统。通过采用迭代方法，该框架不仅利用准确的神经网络参数化的替代模型，还高效地恢复了最优性条件和最优动作序列。 |
| [^146] | [Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off.](http://arxiv.org/abs/2212.08949) | 本论文分析了在强化学习和最优控制中观测时间以离散时间点固定周期到达的默认假设与实际情况下的连续时间系统之间的差异，并在LQR系统中揭示了近似误差和统计误差之间的基本权衡。在有限数据的情况下，管理时间分辨率可以显著改善策略评估的效率。 |
| [^147] | [DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing.](http://arxiv.org/abs/2212.03597) | DeepSpeed Data Efficiency提出了两种数据效率技术：高效的数据采样和高效的数据路由，能够提高深度学习模型的训练效率和质量。 |
| [^148] | [Deep Signature Algorithm for Multi-dimensional Path-Dependent Options.](http://arxiv.org/abs/2211.11691) | 本文提出了一种适用于欧式型和美式型期权定价问题的多维路径依赖期权深度签名算法，解决了路径依赖型FBSDE问题，数值算法的收敛性得到保证。 |
| [^149] | [Exploring validation metrics for offline model-based optimisation with diffusion models.](http://arxiv.org/abs/2211.10747) | 本论文研究基于扩散模型的离线模型优化中的验证指标。在离线模型优化中，我们希望在没有访问真值预言机的情况下设计候选方案。现有的验证指标是对预言机的近似，我们希望找到与真值预言机最相关的验证指标。 |
| [^150] | [Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation.](http://arxiv.org/abs/2211.02658) | 机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。 |
| [^151] | [Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition of Pneumonia from Chest X-Ray Images.](http://arxiv.org/abs/2210.16584) | 本文提出了一种可解释的肺炎识别框架，通过使用多级自注意机制加速收敛并强调与任务相关的特征区域。采用实用的图像数据增强技术来解决数据稀缺问题，为医学实践提供高速分析支持。 |
| [^152] | [Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees.](http://arxiv.org/abs/2210.07893) | 本文针对高斯过程模型的数值稳定性进行了研究，通过感兴趣点的选择和计算，提供了稳定可靠的稀疏逼近方法。 |
| [^153] | [RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow.](http://arxiv.org/abs/2209.14408) | RALACs是一种针对自动驾驶车辆中动作识别的新颖系统，通过交互编码和光流技术将动作识别应用于道路场景，并弥合了其与人类动作识别之间的差距。 |
| [^154] | [MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees.](http://arxiv.org/abs/2209.07225) | MIXRTs是一种可解释的多智能体强化学习架构，通过混合循环软决策树的方式，能够表达明确的决策过程并展示每个智能体的贡献。 |
| [^155] | [Normalised clustering accuracy: An asymmetric external cluster validity measure.](http://arxiv.org/abs/2209.02935) | 本文提出了一种非对称的外部聚类有效度量方法，旨在区分不同任务类型上表现良好和系统性表现不佳的聚类算法。与传统的内部度量不同，该方法利用参考真实分组进行评估，并弥补了现有方法在最坏情况下的误差。 |
| [^156] | [Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach.](http://arxiv.org/abs/2208.10962) | 本研究提出了一种新的深度学习方法，使用正则化稀疏自编码器预测良好的反应坐标以及MD轨迹的演化情况，并展示了正则化约束对于选择重要反应坐标的帮助。 |
| [^157] | [NAPA: Intermediate-level Variational Native-pulse Ansatz for Variational Quantum Algorithms.](http://arxiv.org/abs/2208.01215) | NAPA是一种中级可变分的本地脉冲变分量子算法，通过优化脉冲序列，充分利用了变分量子算法的优势。 |
| [^158] | [Supplementing Recurrent Neural Network Wave Functions with Symmetry and Annealing to Improve Accuracy.](http://arxiv.org/abs/2207.14314) | 本论文提出了一种使用对称性和退火补充循环神经网络波函数的方法，用于更精确地估计二维海森堡模型的基态能量。相较于传统方法，该方法在大系统尺寸上具有重要的改进。 |
| [^159] | [Provably tuning the ElasticNet across instances.](http://arxiv.org/abs/2207.10199) | 这篇论文提供了一个针对ElasticNet的新颖结构结果，用以证明在多个问题实例中调整正规化参数，同时提供了统计和在线学习情景下的泛化保证。 |
| [^160] | [SPIRAL: A superlinearly convergent incremental proximal algorithm for nonconvex finite sum minimization.](http://arxiv.org/abs/2207.08195) | SPIRAL是一种用于解决非凸正则化有限和问题的超线性收敛增量近端算法，通过结合增量梯度更新和永不触发渐近行搜索特性，在极限点下实现了超线性收敛。在不同类型的问题中，包括凸、非凸和非Lipschitz可微问题，SPIRAL算法及其自适应变体表现出了与目前最先进算法相当的竞争力。 |
| [^161] | [KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates from Images.](http://arxiv.org/abs/2206.11030) | KeyCLD是一个从图像中学习拉格朗日动力学的框架，通过学习关键点坐标表示状态动力学，并结合显式的完整约束来表示动力学。该方法经过无监督端到端训练，在各种环境中展示了准确学习动力学的能力，并实现了长期视频预测。与其他模型进行比较，并研究了拉格朗日先验和约束函数的优势。 |
| [^162] | [Resource-Efficient Separation Transformer.](http://arxiv.org/abs/2206.09507) | 本文开发出了一种资源高效的分离Transformer（RE-SepFormer），通过非重叠的潜空间块和紧凑的潜变量摘要操作，降低了计算负担。在语音分离任务中取得了竞争性能，并具有较好的内存和推断时间扩展性。 |
| [^163] | [Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay.](http://arxiv.org/abs/2206.08756) | 张量对张量回归问题中，我们提出了黎曼优化方法和秩过参数化的研究，并展示了黎曼优化方法的线性和二次收敛性以及适应过参数化的能力。同时，我们证明了标量对张量回归中的统计计算差异。 |
| [^164] | [Stochastic Gradient Methods with Preconditioned Updates.](http://arxiv.org/abs/2206.00285) | 本文介绍了一种具有预条件更新的随机梯度方法，包括了基于Hutchinson方法的预条件器和几种梯度方法，能够有效解决尺度不好和条件不良问题，并在光滑性和PL条件下证明了线性收敛性。 |
| [^165] | [Leave-one-out Singular Subspace Perturbation Analysis for Spectral Clustering.](http://arxiv.org/abs/2205.14855) | 该论文提出了一种leave-one-out奇异子空间扰动分析方法，通过建立扰动上界来测量两个矩阵的奇异子空间之间的距离。该方法适用于混合模型，相较于传统的扰动界限具有更尖锐和精细的统计分析能力。在亚高斯混合模型下，通过此方法进行的谱聚类具有显式的指数误差率，且在较弱的信噪比条件下优于之前的研究成果。 |
| [^166] | [Contextual Pandora's Box.](http://arxiv.org/abs/2205.13114) | 本研究提出了带有上下文的在线情境下的潘多拉之盒问题，并设计了一种无遗憾算法，能够在不完全了解先前分布的情况下实现与最优算法相当的性能。 |
| [^167] | [Impartial Games: A Challenge for Reinforcement Learning.](http://arxiv.org/abs/2205.12787) | AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations. |
| [^168] | [Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics.](http://arxiv.org/abs/2205.03990) | 本论文提出了一种利用物理先验知识的多分辨率偏微分方程保留学习框架，通过将离散化的控制方程嵌入神经网络架构中，提高了泛化能力和长期预测的准确性。 |
| [^169] | [Understanding CNNs from excitations.](http://arxiv.org/abs/2205.00932) | 这项研究提出了一种从激励角度理解CNNs的方法，通过提取正负激励并引入双链反向传播的方法，实现了无梯度的逐层信息利用，有效提高了解释准确性。 |
| [^170] | [Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation.](http://arxiv.org/abs/2204.05104) | 该论文介绍了一种自监督图神经网络方法，用于解决多源域适应问题。该方法利用大规模无标签样本进行学习，并尝试在预训练任务和下游任务之间共享有用知识。然而，传统方法面临预训练任务与下游任务关联性不强以及信息交换无效的挑战。 |
| [^171] | [Improved Information Theoretic Generalization Bounds for Distributed and Federated Learning.](http://arxiv.org/abs/2202.02423) | 该论文研究了分布式和联邦学习中的信息论泛化界限，提出了对于具有通信或隐私约束的问题，与节点数量成反比的改进上界。 |
| [^172] | [Random-reshuffled SARAH does not need a full gradient computations.](http://arxiv.org/abs/2111.13322) | 本文提出了一种使用随机重排策略和聚合随机梯度的方法，可以在不进行完整梯度计算的情况下实现随机递归梯度算法（SARAH），并通过理论分析和数值实验证明了该方法的效率。 |
| [^173] | [Pruning Self-attentions into Convolutional Layers in Single Path.](http://arxiv.org/abs/2111.11802) | 本文提出了一种名为SPViT的方法，将预训练的ViTs压缩为紧凑的模型，并通过权重共享方案将自注意力层和卷积操作相结合，以解决计算资源消耗和建模本地视觉模式的问题。 |
| [^174] | [Contrastive Active Inference.](http://arxiv.org/abs/2110.10083) | 该论文提出了一种对比主动推断方法，通过降低计算负担来学习代理的生成模型和规划未来行动。这种方法在图像任务中表现出色，并且计算成本较低且易于训练。 |
| [^175] | [Optimising for Interpretability: Convolutional Dynamic Alignment Networks.](http://arxiv.org/abs/2109.13004) | CoDA Nets是一种性能良好的分类器，具有高度内在可解释性。它们通过动态对齐单元实现输入依赖的线性变换，并将输出线性分解为各个输入的贡献。这些模型在视觉质量和分类准确度上优于现有方法，并且在CIFAR-10和TinyImagenet等数据集上表现出与ResNet和VGG模型相媲美的性能。 |
| [^176] | [SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking.](http://arxiv.org/abs/2109.10399) | 这个论文介绍了SubseasonalClimateUSA，这是一个用于训练和基准测试美国的亚季节预测模型的数据集。作者使用该数据集对多种模型进行了基准测试。 |
| [^177] | [Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends.](http://arxiv.org/abs/2109.09824) | 本论文研究了在新时尚产品销量预测中结合谷歌趋势和多模态信息的有效性，提出了一种基于神经网络的非自回归模型，并提供了一个公开可用的数据集。通过将谷歌趋势编码与视觉和元数据信息相结合，我们能够有效预测缺乏过去数据的新时尚产品销量。 |
| [^178] | [Continual learning under domain transfer with sparse synaptic bursting.](http://arxiv.org/abs/2108.12056) | 本文介绍了一个系统，通过稀疏突发神经突触，在领域转移中能够在先前未见的数据集上进行持续学习，减少遗忘。 |
| [^179] | [On the Benefits of Inducing Local Lipschitzness for Robust Generative Adversarial Imitation Learning.](http://arxiv.org/abs/2107.00116) | 本研究研究了引入局部Lipschitz性质对鲁棒生成对抗模仿学习的益处。提出了一种正则化方法，通过训练具有Lipschitz性质的鉴别器和生成器，能够学习到更加鲁棒的策略，提高生成对抗模仿学习的性能。 |
| [^180] | [Convolutional Dynamic Alignment Networks for Interpretable Classifications.](http://arxiv.org/abs/2104.00032) | 这项研究介绍了一种新的神经网络模型CoDA-Nets，它是一种具有高度可解释性的性能分类器。CoDA-Nets使用动态对齐单元对输入进行线性变换，并通过线性分解将输出解释为单个输入贡献。此外，CoDA-Nets在分类任务上取得了与ResNet和VGG模型相当的结果。 |
| [^181] | [Follow Your Nose -- Which Code Smells are Worth Chasing?.](http://arxiv.org/abs/2103.01861) | 这项研究通过实证研究了代码异味对代码质量和开发生产力的影响，并发现仅有少数代码异味是真正具有因果关系的，其中与简单性、防御性编程和抽象相关的异味是最强烈的。开发人员倾向于去除容易的异味而不是真正有效的异味。 |
| [^182] | [Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention.](http://arxiv.org/abs/2102.01223) | 该论文提出了一种无监督的分布式方法，使用动态容量槽注意力模型从字符序列中学习抽象有意义单元，成功发现了与先前提出的单元相似的、适用于更高级别抽象的可捕捉有意义信息的单元。 |
| [^183] | [Double-Adversarial Activation Anomaly Detection: Adversarial Autoencoders are Anomaly Generators.](http://arxiv.org/abs/2101.04645) | 该论文提出了一种名为DA3D的无监督异常检测方法，利用对抗自编码器生成基于正常数据的异常反例，从而实现在没有领域知识的情况下超越最先进的异常检测方法的性能。 |
| [^184] | [Adversarial Estimation of Riesz Representers.](http://arxiv.org/abs/2101.00009) | 我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer，并且证明了非渐近均方速率以及渐近正态性的条件。这个条件使得在机器学习中进行推断时无需样本分割，并且能够提高有限样本性能。 |
| [^185] | [Towards Robust Neural Networks via Orthogonal Diversity.](http://arxiv.org/abs/2010.12190) | 本文提出了一种通过正交多样性增强神经网络鲁棒性的方法，通过在网络中嵌入多个路径并施加正交约束，让模型学习适应不同输入的特征。 |
| [^186] | [End-to-end Kernel Learning via Generative Random Fourier Features.](http://arxiv.org/abs/2009.04614) | 本文提出了一种通过生成型随机傅立叶特征进行端到端内核学习的方法，将内核学习和线性学习器融合为一个统一框架，通过生成网络和线性分类器联合训练以实现更好的泛化性能。 |
| [^187] | [Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions.](http://arxiv.org/abs/2008.05825) | 本研究将监督学习和VAEs统一于基于正态流的神经网络模型中，对天文粒子重建进行了覆盖、系统性和拟合好坏的研究，并通过KL散度目标实现了监督学习和VAEs的统一。利用条件正态化流的方法可以计算神经网络模型的拟合优度p值。 |
| [^188] | [On Biased Compression for Distributed Learning.](http://arxiv.org/abs/2002.12410) | 本研究研究了偏压压缩在分布式学习中的应用，首次证明了偏压压缩器可以在单节点和分布式环境中实现线性收敛速率。 |
| [^189] | [PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic PowerForecasting from Numerical Weather Prediction.](http://arxiv.org/abs/1902.01453) | PVNet是一种利用数值天气预报数据的LRCN架构，用于预测24小时和48小时的光伏发电功率。该模型充分利用了时间和空间天气数据，通过与其他模型的比较，展现了较好的性能。 |
| [^190] | [On the Generalization of Stochastic Gradient Descent with Momentum.](http://arxiv.org/abs/1809.04564) | 该论文研究了带动量的随机梯度下降方法的泛化性能，并通过分析不同的损失函数形式和动量范围，提出了一种可以在多个周期内训练机器学习模型并保证泛化性能的修改后的动量更新规则。对于特殊情况下的损失函数，标准的带动量随机梯度下降方法也能够具有泛化性能。该论文还给出了对于期望真实风险的上界估计。 |

# 详细

[^1]: 迭代正则化与k支撑范数：稀疏恢复的重要补充

    Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])

    [http://arxiv.org/abs/2401.05394](http://arxiv.org/abs/2401.05394)

    该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。

    

    稀疏恢复在机器学习和信号处理中无处不在。由于稀疏恢复的NP困难性质，现有方法通常要么受限于适用条件（甚至未知），要么计算成本高。最近，迭代正则化方法作为一种快速方法出现，因为它们可以通过提前停止一次通过来实现稀疏恢复，而不是传统方法中繁琐的网格搜索。然而，大多数这些迭代方法都基于$\ell_1$范数，需要受限的适用条件，并且在许多情况下可能会失败。因此，迭代正则化方法在更广泛的条件下实现稀疏恢复仍需进一步探索。为了解决这个问题，我们提出了一种新的迭代正则化算法IRKSN，它基于$k$支撑范数正则化而不是$\ell_1$范数。我们提供了使用IRKSN进行稀疏恢复的条件，并进行了比较。

    Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
    
[^2]: PhilEO Bench: 评估地理空间基础模型

    PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v1 [cs.CV])

    [http://arxiv.org/abs/2401.04464](http://arxiv.org/abs/2401.04464)

    本文引入了PhilEO Bench，一个新颖的EO基础模型评估框架，旨在解决EO领域中缺乏标记数据的问题。框架包括测试平台和一个400 GB Sentinel-2数据集，用于建筑密度估计、道路分割和土地覆盖分类的标签，通过实验评估了不同的基础模型。

    

    地球观测（EO）卫星捕捉到大量未标记的数据，其中Sentinel-2星座每天产生1.6 TB的数据。这使得遥感成为一个数据丰富的领域，非常适合机器学习（ML）解决方案。然而，应用ML模型到EO领域的瓶颈在于缺乏经过注释的数据，因为注释是一项费时费力的过程。因此，该领域的研究聚焦于自我监督学习和基础模型方法。本文通过引入PhilEO Bench，一个新颖的EO基础模型评估框架，以评估不同的基础模型的公平性和一致性基准。该框架包括一个测试平台和一个新颖的400 GB Sentinel-2数据集，其中包含了三个下游任务的标签，即建筑密度估计、道路分割和土地覆盖分类。我们使用我们的框架进行实验，评估了不同的基础模型，包括Prithvi和SatMAE。

    Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at mu
    
[^3]: 迈向可解释的人工智能（XAI）：一个数据挖掘的视角

    Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])

    [http://arxiv.org/abs/2401.04374](http://arxiv.org/abs/2401.04374)

    本研究提出了一种“数据为中心”的视角，探讨了数据收集、处理和分析在可解释的人工智能中的作用。研究将现有工作分为三个类别：深度模型的解释、训练数据的影响和领域知识的见解。通过数据挖掘操作，我们总结了这些XAI方法。

    

    鉴于深度神经网络（DNN）的复杂性和透明度不足，人们进行了大量努力，以使这些系统更具解释性或在可访问的术语中解释其行为。与大多数评论不同，该工作采用“数据为中心”的观点，研究数据收集，处理和分析如何促成可解释的人工智能（XAI）。我们将现有工作分为三类，根据其目的进行分类：深度模型的解释，涉及将数据点与模型输出相关联的特征归因和推理过程；训练数据的影响，研究训练数据细微差异（如数据评估和样本异常）对决策过程的影响；以及领域知识的见解，从数据和模型中发现潜在模式，并促进社会价值和科学发现的新知识。具体而言，我们将XAI方法论提炼为数据挖掘操作。

    Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining op
    
[^4]: 使用迁移学习的物理信息神经网络解决高频率和多尺度问题

    Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])

    [http://arxiv.org/abs/2401.02810](http://arxiv.org/abs/2401.02810)

    本论文提出使用迁移学习来提高物理信息神经网络（PINN）训练的鲁棒性和收敛性。经过两个案例研究，发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数，且需要更少的数据点和更短的训练时间。

    

    物理信息神经网络（PINN）是一种用于偏微分方程（ODEs/PDEs）的数据驱动求解器，提供了统一的框架来处理前向和反向问题。然而，目标函数的复杂性常常导致训练失败。当解决高频率和多尺度问题时，这个问题尤为突出。我们提出使用迁移学习来提高训练PINN的鲁棒性和收敛性，从低频率问题开始训练，并逐渐接近高频率问题。通过两个案例研究，我们发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数。此外，它需要更少的数据点和更短的训练时间。我们详细描述了我们的训练策略，包括优化器的选择，并提出了使用迁移学习来训练神经网络的指南。

    Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
    
[^5]: 超越提取：为语言模型提供上下文化的表格数据以实现高效摘要

    Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])

    [http://arxiv.org/abs/2401.02333](http://arxiv.org/abs/2401.02333)

    本研究提出了一种创新的方法，通过上下文化表格数据来提高 RAG 系统中处理复杂表格查询的准确性，提高了摘要的效率。

    

    传统的检索增强生成 (RAG) 架构在从各种文件中检索信息方面已被证明是有效的。然而，在处理包含复杂表格结构的 PDF 文档中的复杂表格查询时会遇到挑战。本研究引入了一种创新的方法来提高 RAG 系统中复杂表格查询的准确性。我们的方法涉及将 PDF 存储在检索数据库中，并单独提取表格内容。提取的表格经过上下文丰富的处理，将标题与相应的值连接起来。为了确保对丰富数据的全面理解，我们使用经过微调的 Llama-2-chat 语言模型在 RAG 架构中进行摘要。此外，我们通过一次性提示使用 ChatGPT 3.5 API 增强表格数据的上下文含义。然后，将这些丰富的数据与其他 PDF 文件一起输入检索数据库。

    The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
    
[^6]: 按照你的学习行动：非稳态马尔可夫决策过程中的自适应决策

    Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])

    [http://arxiv.org/abs/2401.01841](http://arxiv.org/abs/2401.01841)

    本文提出了一种自适应蒙特卡洛树搜索算法来应对非稳态环境下的决策问题，解决了传统方法中对环境动态假设的限制和规划过程的悲观性问题。

    

    在顺序决策中，处理非稳态环境是一个基本（且在很大程度上是未解决的）挑战，其中外部环境条件随时间变化。这类问题通常被建模为非稳态马尔可夫决策过程（NSMDP）。然而，现有的NSMDP决策方法存在两个主要缺点：首先，它们假设当前时刻更新的环境动态是已知的（尽管未来动态可能会改变）；其次，规划过程主要是悲观的，即代理人会“安全行动”以考虑环境的非稳态演变。我们认为这两个假设在实践中是无效的-更新的环境条件很少是已知的，并且当代理人与环境交互时，它可以学习更新的动态并避免悲观，至少在其对动态有信心的状态下。我们提出了一种启发式搜索算法，称为自适应蒙特卡洛树搜索。

    A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
    
[^7]: 一种用于联合实体和关系抽取的自回归文本到图框架

    An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])

    [http://arxiv.org/abs/2401.01326](http://arxiv.org/abs/2401.01326)

    这篇论文提出了一种新颖的方法，通过将联合实体和关系抽取问题作为条件序列生成问题来解决。该方法使用了基于跨度的图生成方式，并通过指向机制将生成的输出与原始文本对齐。评估结果证明了该方法的有效性，并获得了竞争性的结果。

    

    本文提出了一种新颖的方法，将非结构化文本中的联合实体和关系抽取问题作为条件序列生成问题来解决。与传统的生成式信息抽取模型不同，我们的方法是基于跨度的，它生成一个线性化的图，其中节点表示文本跨度，边表示关系三元组。我们的方法采用了一个具有指向机制的转换器编码器-解码器架构，使用一个动态词汇表来表示跨度和关系类型。我们的模型能够通过跨度表示捕捉实体和关系的结构特征和边界，同时通过指向机制将生成的输出与原始文本进行对齐。在基准数据集上的评估验证了我们方法的有效性，展示了竞争性的结果。代码可在https://github.com/urchade/ATG找到。

    In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
    
[^8]: 用于自动驾驶的WoodScape运动分割--CVPR 2023 OmniCV研讨会挑战

    WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v1 [cs.CV])

    [http://arxiv.org/abs/2401.00910](http://arxiv.org/abs/2401.00910)

    运动分割是自动驾驶中重要的任务，传统的卷积神经网络方法在处理摄像机自我运动、鱼眼镜头径向畸变和时间一致性方面效果较差。本论文提出了WoodScape鱼眼运动分割挑战，使用合成数据集和真实数据集来提高机器学习模型性能，这是首个专注于鱼眼运动分割的比赛之一，旨在探索和评估其潜力和影响。

    

    运动分割是自动驾驶中复杂但不可或缺的任务。摄像机的自我运动、鱼眼镜头的径向畸变以及对时间一致性的需求引入了挑战，使得传统和标准的卷积神经网络（CNN）方法效果较差。相应的繁琐的数据标注、多样化和罕见情况的表征以及广泛的数据采集需求强调了用于改善机器学习模型性能的合成数据的必要性。为此，我们使用Parallel Domain开发的PD-WoodScape合成数据集，以及WoodScape鱼眼数据集。因此，我们提出了WoodScape鱼眼运动分割挑战，作为CVPR 2023全景计算机视觉（OmniCV）研讨会的一部分举行。作为首个专注于鱼眼运动分割的比赛之一，我们旨在探索和评估其潜力和影响。

    Motion segmentation is a complex yet indispensable task in autonomous driving. The challenges introduced by the ego-motion of the cameras, radial distortion in fisheye lenses, and the need for temporal consistency make the task more complicated, rendering traditional and standard Convolutional Neural Network (CNN) approaches less effective. The consequent laborious data labeling, representation of diverse and uncommon scenarios, and extensive data capture requirements underscore the imperative of synthetic data for improving machine learning model performance. To this end, we employ the PD-WoodScape synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye dataset. Thus, we present the WoodScape fisheye motion segmentation challenge for autonomous driving, held as part of the CVPR 2023 Workshop on Omnidirectional Computer Vision (OmniCV). As one of the first competitions focused on fisheye motion segmentation, we aim to explore and evaluate the potential and impac
    
[^9]: 基于神经算子流的量子场论多格采样方法

    Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00828](http://arxiv.org/abs/2401.00828)

    本文提出了一种基于神经算子流的方法，通过近似时间相关算子，实现了在量子场论中从底层自由理论到目标理论的离散-连续归一化流。

    

    本文考虑从玻尔兹曼分布中采样离散场配置$\phi$的问题，其中$S$是某个量子场论连续欧几里得作用$\mathcal S$的格点离散化。我们将该密度近似视为底层函数密度$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$的学习算子实例。具体而言，我们提出了近似时间相关算子$\mathcal V_t$的方法，其时间积分提供了自由理论$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$的函数分布与目标理论$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$之间的映射。当选择特定的格点时，算子$\mathcal V_t$可以离散化为有限维的时间相关矢量场$V_t$，从而在离散格点上实现了连续的归一化流。

    We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
    
[^10]: 使用随机子空间和Dirichlet过程混合模型的子抽样集合进行无监督异常检测

    Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00773](http://arxiv.org/abs/2401.00773)

    提出一种基于随机子空间和子抽样集合的Dirichlet过程高斯混合模型的无监督异常检测方法，提高了计算效率和检测器的鲁棒性。

    

    概率混合模型被认为是一种有价值的工具，用于无监督异常检测，因为它们具有解释性，并且在统计原理上有直观基础。在这个框架内，Dirichlet过程混合模型作为传统有限混合模型在聚类和异常检测任务中的一个引人注目的替代选择。然而，尽管它们明显具有优势，但在无监督异常检测中广泛采用Dirichlet过程混合模型受到与构建检测器过程中的计算效率和对异常值的敏感性有关的挑战的阻碍。为了解决这些挑战，我们提出了一种基于Dirichlet过程高斯混合模型集合的新型异常检测方法。所提出的方法是一种完全无监督的算法，利用了随机子空间和子抽样集合，不仅确保了高效计算，还增强了结果异常检测器的鲁棒性。

    Probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. Within this framework, Dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. However, despite their evident advantages, the widespread adoption of Dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. To tackle these challenges, we propose a novel outlier detection method based on ensembles of Dirichlet process Gaussian mixtures. The proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. Moreover,
    
[^11]: 在晶体材料研究中，将协方差和表达能力融合为深度哈密顿回归：一种混合级联回归框架

    Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2401.00744](http://arxiv.org/abs/2401.00744)

    在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。

    

    在材料研究中，深度学习用于哈密顿回归量子系统需要满足协方差定律，其中实现SO(3)等变性而不损失网络的表达能力是一个难以解决的挑战，因为非线性映射的理论等变性保证受限。为了解决协方差-表达能力困境，我们提出了一种混合框架，分为两个级联回归阶段。第一阶段使用一个理论上保证的协变神经网络来建模三维原子系统的对称性，产生理论上的协变特征和基线哈密顿预测，帮助第二阶段学习协变性。同时，第二阶段使用我们提出的非线性三维图形Transformer网络来进行三维原子系统的结构建模，将第一阶段的输出精细化为具有更好表达能力的哈密顿预测。通过理论上的协变性和更好的表达能力，实现了对哈密顿回归的改进。

    Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
    
[^12]: MosaicBERT：一种针对快速预训练进行优化的双向编码器

    MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17482](http://arxiv.org/abs/2312.17482)

    MosaicBERT是一种优化的BERT风格双向编码器，通过引入多项创新技术和优化方案，实现了快速预训练。

    

    尽管BERT风格的编码器模型在NLP研究中广泛使用，但由于训练成本高昂，许多研究人员不会从头开始预训练自己的BERT。在BERT首次崭露头角的过去半-decade，已经对其他变压器架构和训练配置进行了许多进展，但尚未系统地纳入BERT中。在这里，我们引入了MosaicBERT，一种经验优化用于快速预训练的BERT风格编码器架构和训练方法。这种高效的架构将FlashAttention、带有线性偏差的Attention (ALiBi)、门控线性单元 (GLU)、动态移除填充令牌的模块和低精度LayerNorm等引入了经典的变压器编码器块。训练方法还包括30%的掩码比率用于遮蔽语言建模 (MLM) 目标，bfloat16精度，以及针对GPU吞吐量进行优化的词汇大小，此外还采用了RoBERTa和其他编码器模型的最佳实践。

    Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pr
    
[^13]: RLPlanner: 利用强化学习进行芯片组基于模块的快速热分析的布局设计

    RLPlanner: Reinforcement Learning based Floorplanning for Chiplets with Fast Thermal Analysis. (arXiv:2312.16895v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16895](http://arxiv.org/abs/2312.16895)

    RLPlanner是一种针对芯片组基于模块的系统的早期布局设计工具，利用强化学习和快速热评估方法来最小化总线长度和温度，并实现了高速化和准确性的提升。

    

    近年来，由于其低成本和竞争性能，芯片组基于模块的系统引起了广泛关注。随着芯片组基于模块系统的复杂性和紧凑性的增加，在布局设计阶段必须仔细考虑微凸点分配、互连延迟和热限制。本文介绍了RLPlanner，一种用于芯片组基于模块系统的高效的早期布局设计工具，具有一种新颖的快速热评估方法。RLPlanner利用先进的强化学习来同时最小化总线长度和温度。为了缓解耗时的热计算，RLPlanner采用了开发的快速热评估方法来加快迭代和优化。全面的实验表明，我们提出的快速热评估方法相对于开源热求解器HotSpot，实现了0.25 K的平均绝对误差（MAE）和超过120倍的加速比。

    Chiplet-based systems have gained significant attention in recent years due to their low cost and competitive performance. As the complexity and compactness of a chiplet-based system increase, careful consideration must be given to microbump assignments, interconnect delays, and thermal limitations during the floorplanning stage. This paper introduces RLPlanner, an efficient early-stage floorplanning tool for chiplet-based systems with a novel fast thermal evaluation method. RLPlanner employs advanced reinforcement learning to jointly minimize total wirelength and temperature. To alleviate the time-consuming thermal calculations, RLPlanner incorporates the developed fast thermal evaluation method to expedite the iterations and optimizations. Comprehensive experiments demonstrate that our proposed fast thermal evaluation method achieves a mean absolute error (MAE) of 0.25 K and delivers over 120x speed-up compared to the open-source thermal solver HotSpot. When integrated with our fast 
    
[^14]: 有效的强化学习方法：探索与利用分离

    Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15965](http://arxiv.org/abs/2312.15965)

    本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。

    

    当前的离线强化学习技术过于保守，对现有数据集的处理限制了深度神经网络(DNN)的泛化能力。这种方法常常导致算法只适应某个特定数据集的次优解。同样，在在线强化学习中，之前的惩罚性悲观主义也剥夺了模型的探索潜力。我们的研究提出了一种新的框架，乐观与悲观演员强化学习（OPARL）。OPARL采用独特的双演员方法：乐观演员专注于探索，悲观演员专注于利用，从而有效区分探索和利用策略。这种组合在强化学习方法中促进了更平衡和高效的方法。它通过悲观的利用策略优化着重于产生高奖励动作的策略。

    Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi
    
[^15]: 利用公共表示来进行私有迁移学习

    Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15551](http://arxiv.org/abs/2312.15551)

    该论文探讨了如何利用公共数据来改进私有学习的问题。研究发现，通过学习公共数据中的共享表示，可以在两种迁移学习场景中实现最优的学习效果。在单任务迁移场景中，算法在给定子空间范围内搜索线性模型，并实现了最优超额风险。在多任务个性化场景中，足够的公共数据可以消除私有协调需求，并通过纯局部学习达到相同的效用。

    

    受到将公共数据纳入差分隐私学习的最新实证成功的启发，我们在理论上研究了从公共数据中学到的共享表示如何改进私有学习。我们探讨了线性回归的两种常见迁移学习场景，两者都假设公共任务和私有任务（回归向量）在高维空间中共享一个低秩子空间。在第一种单任务迁移场景中，目标是学习一个在所有用户之间共享的单一模型，每个用户对应数据集中的一行。我们提供了匹配的上下界，证明了我们的算法在给定子空间估计范围内搜索线性模型的算法类中实现了最优超额风险。在多任务模型个性化的第二种情景中，我们表明在有足够的公共数据情况下，用户可以避免私有协调，因为在给定子空间内纯粹的局部学习可以达到相同的效用。

    Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
    
[^16]: 在生产中用开源SLMs替代专有LLMs的权衡分析

    A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production. (arXiv:2312.14972v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2312.14972](http://arxiv.org/abs/2312.14972)

    本文提出了一种系统的评估方法来比较专有LLMs和开源SLMs的权衡，并设计了一个自动化分析工具SLaM来测试产品功能。在实际产品功能替换时，对于现有能力是否能够被开源SLMs代替，还需要进一步研究。

    

    许多公司依赖于管理的AI模型的API，如OpenAI的GPT-4，以在其产品中创建AI增强体验。除了使用便利和缩短生产时间的好处外，依赖专有API还具有模型控制、性能可靠性、上线可预测性和成本方面的缺点。与此同时，已经涌现了许多供商业使用的开源小型语言模型（SLMs）。然而，它们替代现有能力的准备情况尚不清楚，并且没有现成的系统方法来测试这些模型。在本文中，我们提出了一种系统的评估方法，该方法对现代开源SLMs及其在替代真实产品功能的专有LLM APIs时所做的权衡进行了表征。我们设计了一个名为SLaM的自动化分析工具，使得可以定量和定性地测试使用任意SLMs的产品功能。使用SLaM，我们对机器人进行了检验。

    Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine bot
    
[^17]: 准确建模随机微分方程的时间变换正则化流

    Time-changed normalizing flows for accurate SDE modeling. (arXiv:2312.14698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.14698](http://arxiv.org/abs/2312.14698)

    本论文提出了一种新的动态正则化流的变种，即时间变换正则化流(TCNF)，通过时间变形的方法能有效地建模一些无法用其他方法建模的随机微分方程(SDEs)，包括著名的奥恩斯坦-乌伦贝克过程，并泛化了先前的方法，从而提高了结果的准确度和推断和预测的精度。

    

    生成范式在机器学习和深度学习模型中变得越来越重要。其中流行的生成模型之一是正则化流，通过微分同胚变换将基本分布转变为准确估计的似然函数。将正则化流框架扩展到处理时间索引流产生动态正则化流，这是一种用于模拟时间序列、随机过程和神经随机微分方程(SDEs)的强大工具。在这项工作中，我们提出了一种新颖的动态正则化流变体，即时间变换正则化流(TCNF)，它基于布朗运动的时间变形，构成了一族多才多艺的高斯过程。这种方法使我们能够有效地模拟一些无法使用其他方法建模的SDEs，包括著名的奥恩斯坦-乌伦贝克过程以及泛化了先前的方法，从而提供了改进的结果和更好的推断和预测能力。

    The generative paradigm has become increasingly important in machine learning and deep learning models. Among popular generative models are normalizing flows, which enable exact likelihood estimation by transforming a base distribution through diffeomorphic transformations. Extending the normalizing flow framework to handle time-indexed flows gave dynamic normalizing flows, a powerful tool to model time series, stochastic processes, and neural stochastic differential equations (SDEs). In this work, we propose a novel variant of dynamic normalizing flows, a Time Changed Normalizing Flow (TCNF), based on time deformation of a Brownian motion which constitutes a versatile and extensive family of Gaussian processes. This approach enables us to effectively model some SDEs, that cannot be modeled otherwise, including standard ones such as the well-known Ornstein-Uhlenbeck process, and generalizes prior methodologies, leading to improved results and better inference and prediction capability.
    
[^18]: 无监督的谐波参数估计：基于可微分DSP和频谱最优传输的方法

    Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport. (arXiv:2312.14507v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.14507](http://arxiv.org/abs/2312.14507)

    本文提出了一种无监督的谐波参数估计方法，利用可微分的DSP和频谱最优传输理论来进行谐波信号的自编码和重构，为神经音频应用中的无监督参数估计提供了一个有希望的方向。

    

    在神经音频信号处理中，使用音高条件对合成器的性能进行增强已经被广泛应用。然而，当使用标准音频到音频重构损失时，联合训练音高估计器和合成器是具有挑战性的，导致依赖外部音高跟踪器。为了解决这个问题，我们提出了一种受最优传输理论启发的频谱损失函数，该损失函数最小化了频谱能量的位移。我们通过一个无监督的自编码任务来验证这种方法，该任务将谐波信号拟合到谐波模板。我们使用一个轻量级的编码器共同估计基频和谐波的振幅，并使用可微分的谐波合成器重构信号。所提出的方法对于改进神经音频应用中的无监督参数估计具有很大的潜力。

    In neural audio signal processing, pitch conditioning has been used to enhance the performance of synthesizers. However, jointly training pitch estimators and synthesizers is a challenge when using standard audio-to-audio reconstruction loss, leading to reliance on external pitch trackers. To address this issue, we propose using a spectral loss function inspired by optimal transportation theory that minimizes the displacement of spectral energy. We validate this approach through an unsupervised autoencoding task that fits a harmonic template to harmonic signals. We jointly estimate the fundamental frequency and amplitudes of harmonics using a lightweight encoder and reconstruct the signals using a differentiable harmonic synthesizer. The proposed approach offers a promising direction for improving unsupervised parameter estimation in neural audio applications.
    
[^19]: WellFactor:使用综合嵌入医疗数据的患者分类方法

    WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2312.14129](http://arxiv.org/abs/2312.14129)

    WellFactor是一种使用综合嵌入医疗数据的患者分类方法，并通过使用受约束的低秩逼近、结合标签信息来优化嵌入结果，同时具有即时计算新数据嵌入的特点。在实际医疗数据上得到了验证。

    

    在快速发展的医疗行业中，平台现在不仅可以访问传统的医疗记录，还可以获取涵盖各种患者互动的各种数据集，例如来自医疗网站的数据。为了解决这种丰富的数据多样性，我们引入了WellFactor：一种通过整合这些来源信息来得出患者分类的方法。我们方法的核心是利用受约束的低秩逼近。WellFactor被优化为处理医疗数据中经常存在的稀疏性。此外，通过结合特定任务的标签信息，我们的方法改进了嵌入结果，提供了更加明智的患者视角。WellFactor的一个重要特点是能够即时计算新的、以前未观察到的患者数据的嵌入，消除了重新访问整个数据集或重新计算嵌入的需要。对实际医疗数据进行全面评估证明了WellFactor的有效性。

    In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's eff
    
[^20]: StemGen: 一个能够听音乐并生成的音乐生成模型

    StemGen: A music generation model that listens. (arXiv:2312.08723v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.08723](http://arxiv.org/abs/2312.08723)

    该论文介绍了一种能够听音乐并生成音乐的模型。采用了非自回归的Transformer模型架构以及一些新颖的架构和采样改进方法。该模型能够达到最先进的文本条件模型的音频质量，并在音乐连贯性方面表现出色。

    

    最近，使用深度学习技术进行音乐音频的端到端生成活动非常活跃。然而，大多数模型集中在根据抽象的条件信息生成完全混合的音乐。在这项工作中，我们提出了一种替代范式，用于产生能够听音乐并回应音乐环境的音乐生成模型。我们描述了如何使用非自回归的基于Transformer的模型架构构建这样的模型，并提出了一些新颖的架构和采样改进。我们使用一个开源数据集和一个专有数据集来训练所描述的架构。我们使用标准的质量度量和基于音乐信息检索描述符的新方法来评估生成的模型。结果模型达到了最先进的文本条件模型的音频质量，并且在音乐连贯性方面表现出色。

    End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.
    
[^21]: 深度学习时代的药物发现中的形态学特征分析

    Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2312.07899](http://arxiv.org/abs/2312.07899)

    形态学特征分析在表型药物发现中具有重要价值。深度学习技术在分析大规模高内容图像方面取得了显著进展，促进了药物作用机制的理解和新治疗方法的开发。

    

    形态学特征分析是表型药物发现中的有价值工具。高通量自动成像的出现使得能够以单细胞分辨率捕捉细胞或生物体对干扰的形态学特征的广泛范围。与此同时，机器学习和深度学习，尤其是计算机视觉领域的重大进展，使得在高通量下分析大规模高内容图像的能力得到了显著提高。这些努力促进了对化合物作用机制、药物再利用、干扰下细胞形态动力学特征的了解，并最终有助于新治疗方法的开发。在本综述中，我们全面概述了形态学特征分析领域的最新进展。我们总结了图像分析工作流程，调查了包括特征工程和基于深度学习的分析策略在内的广泛范围的分析策略，并讨论了在这一领域的潜在应用。

    Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high-throughput. These efforts have facilitated understanding of compound mechanism-of-action (MOA), drug repurposing, characterization of cell morphodynamics under perturbation, and ultimately contributing to the development of novel therapeutics. In this review, we provide a comprehensive overview of the recent advances in the field of morphological profiling. We summarize the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering- and deep learning-based approaches, and i
    
[^22]: 特征引导：大尺度导向下扩散模型的非线性校正

    Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.07586](http://arxiv.org/abs/2312.07586)

    该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。

    

    流行的导引去噪扩散概率模型(DDPM)线性地将不同的条件模型组合在一起，以提供对样本的增强控制。然而，这种方法忽视了当导向尺度变大时产生的非线性效应。为了解决这个问题，我们提出了特征引导，一种采样方法，为无分类器导向的DDPM提供了一种基于原理的非线性校正。这种校正迫使导向的DDPM遵守其底层扩散过程的福克-普朗克方程，这种方法无需训练，无需导数，与现有的采样方法兼容。实验证明，特征引导增强了对图像生成中的控制能力，并减少了颜色和曝光问题，对从潜在空间采样到解决物理问题如磁相变的各种应用都有效。

    Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
    
[^23]: 集合卡尔曼滤波与高斯过程状态空间模型在非均场和在线推理中的应用

    Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05910](http://arxiv.org/abs/2312.05910)

    这篇论文介绍了一种将集合卡尔曼滤波引入变分推理框架的方法，用于近似高斯过程状态空间模型的后验分布，并且有效地利用了潜在状态和动力学之间的依赖关系，减少了变分参数的数量。

    

    高斯过程状态空间模型（GPSSMs）是一种多功能和原则性的非线性动态系统模型。然而，现有的GPSSMs变分学习和推理方法通常需要优化大量变分参数，导致性能和效率不足。为了解决这个问题，我们提出将集合卡尔曼滤波（EnKF），一种成熟的基于模型的滤波技术，纳入变分推理框架中，以近似潜在状态的后验分布。这种利用EnKF的方法可以有效地利用潜在状态和GP动力学之间的依赖关系，同时消除了对变分分布进行参数化的需求，从而显著减少了变分参数的数量。此外，我们还展示了我们提出的算法可以通过简单地对多个项进行求和来直接评估变分推理中的近似证据下界（ELBO）。

    Gaussian process state-space models (GPSSMs) are a versatile and principled family of nonlinear dynamical system models. However, existing variational learning and inference methods for GPSSMs often necessitate optimizing a substantial number of variational parameters, leading to inadequate performance and efficiency. To overcome this issue, we propose incorporating the ensemble Kalman filter (EnKF), a well-established model-based filtering technique, into the variational inference framework to approximate the posterior distribution of latent states. This utilization of EnKF can effectively exploit the dependencies between latent states and GP dynamics, while eliminating the need for parameterizing the variational distribution, thereby significantly reducing the number of variational parameters. Moreover, we show that our proposed algorithm allows straightforward evaluation of an approximated evidence lower bound (ELBO) in variational inference via simply summating multiple terms with 
    
[^24]: FreqFed:一种基于频谱分析的方法用于减轻联邦学习中的毒化攻击

    FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning. (arXiv:2312.04432v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.04432](http://arxiv.org/abs/2312.04432)

    FreqFed是一种基于频谱分析的新方法，用于检测和减轻联邦学习中的针对性和非针对性的毒化攻击。

    

    联邦学习 (FL) 是一种协作学习范 Paradigm，允许多个客户端在不共享训练数据的情况下共同训练模型。然而，FL 容易受到毒化攻击的影响，攻击者通过将篡改的模型更新插入联邦模型聚合过程中来破坏或破坏预测（无目标毒化），或者 implant (targeted poisoning or backdoors) 隐含的功能。针对 FL 中的毒化攻击，现有的防御措施存在多种限制，比如依赖于对攻击类型、策略或数据分布的特定假设，或者在高级注入技术和策略方面不够强大, 且同时保持聚合模型的效用。为了解决现有防御措施的不足，我们采用了一种通用且完全不同的方法来检测针对性和非针对性的毒化攻击。我们提出了 FreqFed，一种新的聚合机制，用于将模型更新（即权重）

    Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weight
    
[^25]: CLadder: 评估语言模型因果推理能力的基准测试

    CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.04350](http://arxiv.org/abs/2312.04350)

    该论文提出了一个新的NLP任务，评估语言模型在因果推理方面的能力。作者构建了一个大规模的数据集CLadder，并利用oracle因果推理引擎将符号问题转化为自然语言。研究结果表明多个LLMs在该数据集上的表现，并引入并评估了一种定制的链式推理机制。

    

    进行因果推理的能力被广泛视为智能的核心特征。本文研究了大型语言模型(LLMs)能否连贯地推理因果关系。现有的自然语言处理(NLP)工作主要关注评估LLMs中的常识因果推理，未能评估模型是否能够按照一组明确定义的形式规则执行因果推断。为了解决这个问题，我们提出了一个新的NLP任务，自然语言中的因果推断，受到Judea Pearl等人提出的“因果推断引擎”的启发。我们构建了一个包含10K个样本的大型数据集CLadder，通过一种oracle因果推理引擎，基于一组因果图和查询(联合、干预和反事实)，得到符号问题和真实答案，并将其翻译为自然语言。我们对数据集上的多个LLMs进行评估，并引入和评估了一种定制的链式推理机制。

    The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
    
[^26]: 图卷积在Transformer的自注意力机制中起到了改进的作用！（arXiv：2312.04234v2 [cs.LG]已更新）

    Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.04234](http://arxiv.org/abs/2312.04234)

    这项研究通过引入图卷积来改进Transformer模型中的自注意力机制，并在计算机视觉、自然语言处理等多个领域展示了其性能提升。

    

    Transformer因其自注意力机制而闻名，在自然语言处理、计算机视觉、时间序列建模等各种任务中取得了最先进的性能。然而，深度Transformer模型面临的挑战之一是过度平滑问题，即表示在各个层之间趋于无法区分的值，导致性能严重下降。我们将原始的自注意力机制解释为一种简单的图滤波器，并从图信号处理（GSP）的角度重新设计它。我们提出了基于图滤波器的自注意力机制（GFSA），以学习一种既通用又有效的机制，其复杂度略高于原始的自注意力机制。我们证明了GFSA在计算机视觉、自然语言处理、图模式分类、语音识别和代码分类等多个领域中改进了Transformer的性能。

    Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
    
[^27]: 多准则决策制定的多权重排名

    Multi-Weight Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.03006](http://arxiv.org/abs/2312.03006)

    这项研究将统计学的锥形分布函数转化为多准则决策制定工具，并通过扩展排名函数到集合，建立了集合优化方法与基于集合的多目标优化之间的联系。在机器学习中具有潜在应用。

    

    统计学中的锥形分布函数被转化为多准则决策制定工具。通过例子展示，与纯加权总和标量化相比，这种标量化方法能够找出 Pareto 边界的“非凸”部分。同时，通过将排名函数扩展到集合，为集合偏好提供一元指标，首次建立了集合优化方法与基于集合的多目标优化之间的联系。文中还简述了在机器学习中的潜在应用。

    Cone distribution functions from statistics are turned into Multi-Criteria Decision Making tools. It is demonstrated that this procedure can be considered as an upgrade of the weighted sum scalarization insofar as it absorbs a whole collection of weighted sum scalarizations at once instead of fixing a particular one in advance. As examples show, this type of scalarization--in contrast to a pure weighted sum scalarization-is also able to detect ``non-convex" parts of the Pareto frontier. Situations are characterized in which different types of rank reversal occur, and it is explained why this might even be useful for analyzing the ranking procedure. The ranking functions are then extended to sets providing unary indicators for set preferences which establishes, for the first time, the link between set optimization methods and set-based multi-objective optimization. A potential application in machine learning is outlined.
    
[^28]: 可证明的对称任务的对抗鲁棒性：图形、点云、分子等

    Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More. (arXiv:2312.02708v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.02708](http://arxiv.org/abs/2312.02708)

    本文提出了一种考虑任务对称性的可证明的对抗鲁棒性概念，并通过选择合适的模型和认证方法来实现鲁棒性。同时，通过开发保持对称性的随机平滑的框架，解决了对于具有连续对称性的模型的认证方法不可用的问题。

    

    传统上，机器学习模型被认为在输入扰动的情况下保持（几乎）恒定的预测是健壮的。然而，现实世界中的任务，如分子性质预测或点云分割，具有固有的对称性，如旋转或置换对称性。在这些任务中，即使是具有大范数的扰动也不一定会改变输入的语义内容。此外，有些扰动需要明确改变模型的预测。我们首次提出了一种考虑任务对称性的可靠对抗鲁棒性概念。然后，我们证明了可以通过选择与任务对称性相匹配的模型和认证传统的对抗鲁棒性来实现可证明的鲁棒性。然而，对于许多具有连续对称性的模型，认证方法不可用。通过开发保持对称性的随机平滑的框架，我们填补了这个空白。

    A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, 
    
[^29]: 使用基于Heun采样器的扩散模型的匹配和不匹配条件下的语音增强

    Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler. (arXiv:2312.02683v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2312.02683](http://arxiv.org/abs/2312.02683)

    这项研究通过使用多个数据库模拟不匹配的条件，系统评估了基于扩散模型的语音增强模型的泛化性能，并尝试了新的设计方面，如噪声调度和采样器方法。

    

    扩散模型是一种新型的生成模型，最近成功应用于语音增强。先前的研究已经证明了其在不匹配条件下与最先进的判别模型相比具有更高的性能。然而，这些研究只使用了一个数据库进行训练，另一个数据库进行测试，这使得结果高度依赖于特定的数据库。此外，图像生成领域的最新进展在语音增强方面仍未得到充分探索。这些进展包括扩散模型的几个设计方面，如噪声调度或反向采样器。在这项工作中，我们通过使用多个语音、噪声和双耳室内脉冲响应（BRIR）数据库来模拟不匹配的声学条件，系统评估了基于扩散模型的语音增强模型的泛化性能。我们还尝试了一种在语音增强中尚未应用的噪声调度和采样器方法。

    Diffusion models are a new class of generative models that have recently been applied to speech enhancement successfully. Previous works have demonstrated their superior performance in mismatched conditions compared to state-of-the art discriminative models. However, this was investigated with a single database for training and another one for testing, which makes the results highly dependent on the particular databases. Moreover, recent developments from the image generation literature remain largely unexplored for speech enhancement. These include several design aspects of diffusion models, such as the noise schedule or the reverse sampler. In this work, we systematically assess the generalization performance of a diffusion-based speech enhancement model by using multiple speech, noise and binaural room impulse response (BRIR) databases to simulate mismatched acoustic conditions. We also experiment with a noise schedule and a sampler that have not been applied to speech enhancement b
    
[^30]: 缓解联邦学习中的数据注入攻击

    Mitigating Data Injection Attacks on Federated Learning. (arXiv:2312.02102v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.02102](http://arxiv.org/abs/2312.02102)

    本文提出了一种新的技术来检测和缓解联邦学习系统中的数据注入攻击，通过在训练过程中忽略被怀疑为攻击者的代理的数据来提高模型的性能。

    

    联邦学习是一种允许多个实体在不损害数据隐私的情况下，共同训练模型的技术。然而，尽管有其优势，联邦学习容易受到虚假数据注入攻击的影响。在这种情况下，具有对网络中特定代理的控制权的恶意实体可以操纵学习过程，导致模型性能下降。因此，在联邦学习系统中解决这些数据注入攻击是一个重要的研究挑战。本文提出了一种新的技术来检测和缓解联邦学习系统中的数据注入攻击。我们的缓解方法是一种局部方案，在协调节点的单个训练实例中执行，允许在算法收敛期间进行缓解。每当怀疑某个代理是攻击者时，会在一定时间内忽略其数据，此决策经常被重新评估。我们证明概率上能够实现缓解攻击。

    Federated learning is a technique that allows multiple entities to collaboratively train models using their data without compromising data privacy. However, despite its advantages, federated learning can be susceptible to false data injection attacks. In these scenarios, a malicious entity with control over specific agents in the network can manipulate the learning process, leading to a suboptimal model. Consequently, addressing these data injection attacks presents a significant research challenge in federated learning systems. In this paper, we propose a novel technique to detect and mitigate data injection attacks on federated learning systems. Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm. Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated. We prove that with probabi
    
[^31]: Bayesian网络的熵和Kullback-Leibler散度：计算复杂度和高效实现

    Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.01520](http://arxiv.org/abs/2312.01520)

    本文提出了一种计算贝叶斯网络中Shannon熵和Kullback-Leibler散度的高效算法，并通过一系列数值示例进行了演示。此外，还展示了如何将高斯贝叶斯网络中KL的计算复杂度从立方降低到二次。

    

    贝叶斯网络（BNs）是机器学习和因果推断中的基础模型。它们的图结构可以处理高维问题，并将其分为稀疏的一系列较小问题，这是Judea Pearl的因果性的基础，也决定了它们的可解释性和可理解性。尽管它们很受欢迎，但在文献中几乎没有关于如何在最常见的分布假设下计算BNs的Shannon熵和Kullback-Leibler（KL）散度的资源。在本文中，我们利用BNs的图结构提供了计算效率高的算法，并用一整套数值示例说明了它们。在此过程中，我们展示了可以将高斯BNs的KL计算复杂度从立方降低到二次的可能性。

    Bayesian networks (BNs) are a foundational model in machine learning and causal inference. Their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies Judea Pearl's causality, and determines their explainability and interpretability. Despite their popularity, there are almost no resources in the literature on how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for BNs under their most common distributional assumptions. In this paper, we provide computationally efficient algorithms for both by leveraging BNs' graphical structure, and we illustrate them with a complete set of numerical examples. In the process, we show it is possible to reduce the computational complexity of KL from cubic to quadratic for Gaussian BNs.
    
[^32]: 《$\mathbb{Z}_2\times \mathbb{Z}_2$》等变量量子神经网络：与经典神经网络的基准比较

    $\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks. (arXiv:2311.18744v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2311.18744](http://arxiv.org/abs/2311.18744)

    本文对等变量量子神经网络（EQNN）和量子神经网络（QNN）与经典神经网络的性能进行了全面比较分析，结果表明《$\mathbb{Z}_2\times \mathbb{Z}_2$》EQNN和QNN在较小的参数集和适中的训练数据样本上表现优越。

    

    本文对等变量量子神经网络（EQNN）和量子神经网络（QNN）与它们的经典对应物：等变量神经网络（ENN）和深度神经网络（DNN）的性能进行了全面比较分析。我们通过两个二元分类任务的玩具示例评估每个网络的性能，关注模型复杂度（由参数数量测量）和训练数据集的大小。我们的结果显示，《$\mathbb{Z}_2\times \mathbb{Z}_2$》EQNN和QNN在较小的参数集和适中的训练数据样本上提供了更优秀的性能。

    This paper presents a comprehensive comparative analysis of the performance of Equivariant Quantum Neural Networks (EQNN) and Quantum Neural Networks (QNN), juxtaposed against their classical counterparts: Equivariant Neural Networks (ENN) and Deep Neural Networks (DNN). We evaluate the performance of each network with two toy examples for a binary classification task, focusing on model complexity (measured by the number of parameters) and the size of the training data set. Our results show that the $\mathbb{Z}_2\times \mathbb{Z}_2$ EQNN and the QNN provide superior performance for smaller parameter sets and modest training data samples.
    
[^33]: 从卷积神经网络中获得的固定点作用

    Fixed point actions from convolutional neural networks. (arXiv:2311.17816v1 [hep-lat] CROSS LISTED)

    [http://arxiv.org/abs/2311.17816](http://arxiv.org/abs/2311.17816)

    本研究使用晶格规范等变卷积神经网络（L-CNN）描述了基于重整化群变换的固定点作用（FP），这种方法更准确地参数化FP作用，可以规避临界减速和拓扑冻结问题，并在粗晶格上产生具有非常小晶格效应的物理预测。

    

    晶格规范等变卷积神经网络（L-CNN）可用于形成任意形状的Wilson环，并且可以近似晶格上的任何规范依变或规范不变函数。在这里，我们使用L-CNN来描述基于重整化群变换的固定点（FP）作用。FP作用在经典规范场满足运动方程的情况下是经典完美的，即它们没有晶格效应，并且具有尺度不变的瞬子解。FP作用在晶格间距的所有阶层中都是树级Symanzik改进的，即使在粗晶格上也能产生具有非常小晶格效应的物理预测。我们发现与较旧的方法相比，L-CNN在参数化FP作用方面更准确。因此，它们可能提供一种规避临界减速和拓扑冻结以接近连续极限的方法。

    Lattice gauge-equivariant convolutional neural networks (L-CNNs) can be used to form arbitrarily shaped Wilson loops and can approximate any gauge-covariant or gauge-invariant function on the lattice. Here we use L-CNNs to describe fixed point (FP) actions which are based on renormalization group transformations. FP actions are classically perfect, i.e., they have no lattice artifacts on classical gauge-field configurations satisfying the equations of motion, and therefore possess scale invariant instanton solutions. FP actions are tree-level Symanzik-improved to all orders in the lattice spacing and can produce physical predictions with very small lattice artifacts even on coarse lattices. We find that L-CNNs are much more accurate at parametrizing the FP action compared to older approaches. They may therefore provide a way to circumvent critical slowing down and topological freezing towards the continuum limit.
    
[^34]: 电力系统中动态故障特性评估

    Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.16522](http://arxiv.org/abs/2311.16522)

    该论文提出了一种在电力系统中进行故障检测的新方法，通过图神经网络识别故障节点，并利用前后时间段内节点的状态来辅助当前故障检测。实验证明该方法准确可靠，并提供了对故障节点传播的定性分析。

    

    为了增强运维的智能度，提出了一种在电力系统中进行故障检测的新方法。该方法基于图神经网络，通过专门的特征提取方法和知识图谱来识别故障节点。通过引入时间数据，该方法利用前后时间段内节点的状态来辅助当前故障检测。为了验证节点特征的有效性，还进行了每个节点输出特征的相关性分析。实验证明，该方法可以在仿真场景中准确地定位故障节点，并具有显著的准确性。此外，基于图神经网络的特征建模可以定性地考察故障如何在节点间传播，为分析故障节点提供了有价值的见解。

    To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
    
[^35]: 基于移动网格PDE的移动采样物理信息神经网络

    Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2311.16167](http://arxiv.org/abs/2311.16167)

    这项工作提出了一种基于移动网格PDE的移动采样物理信息神经网络(MMPDE-Net)，通过解决移动网格PDE来自适应生成新的采样点，并且结合物理信息神经网络（PINN）提出了移动采样PINN（MS-PINN）的框架。数值实验验证了MS-PINN相对于PINN的性能改善。

    

    在这项工作中，我们提出了一种基于移动网格方法的端到端自适应采样神经网络（MMPDE-Net），通过求解移动网格PDE，可以自适应生成新的采样点。该模型旨在改善采样点生成的质量。此外，我们基于MMPDE-Net开发了一种迭代算法，使得采样点更加精确和可控。由于MMPDE-Net是独立于深度学习求解器的框架，我们将其与物理信息神经网络（PINN）相结合，提出了移动采样PINN（MS-PINN），并在一些假设下通过误差分析验证了其有效性。最后，我们通过四个典型实例的数值实验验证了MS-PINN相对于PINN的性能改善，从而数值上证明了我们方法的有效性。

    In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
    
[^36]: 评估混合深度学习模型在区分AI生成文本方面的有效性

    Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.15565](http://arxiv.org/abs/2311.15565)

    本研究评估了混合深度学习模型在准确区分AI生成文本和人类写作方面的有效性。通过应用先进的自然语言处理技术和复杂的神经网络，我们的研究成功地检测到了AI生成文本和人类写作之间的微妙差异。

    

    我的研究通过使用先进的混合深度学习模型，来准确区分AI生成的文本与人类写作。我应用了一种稳健的方法论，利用了一个精心选择的数据集，其中包括来自各种来源的AI和人类文本，每个文本都标有指示。先进的自然语言处理技术便于对文本特征进行分析。通过结合复杂的神经网络，这个定制模型使得它能够检测出AI和人类内容之间微妙的差异。

    My research investigates the use of cutting-edge hybrid deep learning models to accurately differentiate between AI-generated text and human writing. I applied a robust methodology, utilising a carefully selected dataset comprising AI and human texts from various sources, each tagged with instructions. Advanced natural language processing techniques facilitated the analysis of textual features. Combining sophisticated neural networks, the custom model enabled it to detect nuanced differences between AI and human content.
    
[^37]: 《基于课程学习和模仿学习的金融时间序列模型无关控制》

    Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13326](http://arxiv.org/abs/2311.13326)

    本文通过在复杂时间序列数据上探索课程学习和模仿学习的方法，发现课程学习是改善复杂时间序列控制任务性能的新途径，而模仿学习也应该被应用。

    

    课程学习和模仿学习在机器人领域已被广泛运用。然而，在高度随机的时间序列数据上利用这些想法进行控制任务的研究非常有限。在本研究中，我们从理论和实证两个方面探讨了这些方法在复杂时间序列数据上的代表性控制任务中的应用。我们通过数据增强实现了课程学习的基本思想，而通过模仿学习从专家中蒸馏出策略来实现。我们的研究结果表明，课程学习在改进复杂时间序列控制的任务性能方面应被视为一种新的方向。我们的大量随机种子外样本实证和消融研究对于时间序列控制的课程学习非常鼓舞人心。这些发现尤其鼓舞人心，因为我们在基线上调整了所有重叠的超参数，给出了基线的优势。另一方面，我们发现模仿学习应该被使用。

    Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
    
[^38]: Tiny-VBF: 基于Vision Transformer的轻量级超声单角度平面波成像波束形成器

    Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging. (arXiv:2311.12082v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2311.12082](http://arxiv.org/abs/2311.12082)

    本文提出了一种基于Vision Transformer的轻量级超声单角度平面波成像波束形成器（Tiny-VBF），与最先进的深度学习模型相比，它具有更快的包络检测速度，更高的对比度和分辨率。

    

    近年来，利用深度学习架构加速计算密集型非实时超声成像波束形成算法一直在发展。然而，最先进的深度学习技术的复杂性给资源受限的边缘设备上的部署带来了挑战。在这项工作中，我们提出了一种基于Vision Transformer的新型微型波束形成器（Tiny-VBF），它可以处理通过单角度平面波超声激发获得的原始射频通道数据。与最先进的深度学习模型相比，我们的Tiny-VBF的输出提供快速包络检测，仅需要极低的帧率，即对于368 x 128的帧大小，每帧仅为0.34 GOPs。在体外数据集上，与Tiny-CNN相比，其对比度提高了8%，轴向和横向分辨率分别提高了5%和33%。此外，我们的模型在体外数据集上的对比度提高了4.2%，轴向和横向分辨率分别提高了4%和20%。

    Accelerating compute intensive non-real-time beam-forming algorithms in ultrasound imaging using deep learning architectures has been gaining momentum in the recent past. Nonetheless, the complexity of the state-of-the-art deep learning techniques poses challenges for deployment on resource-constrained edge devices. In this work, we propose a novel vision transformer based tiny beamformer (Tiny-VBF), which works on the raw radio-frequency channel data acquired through single-angle plane wave insonification. The output of our Tiny-VBF provides fast envelope detection requiring very low frame rate, i.e. 0.34 GOPs/Frame for a frame size of 368 x 128 in comparison to the state-of-the-art deep learning models. It also exhibited an 8% increase in contrast and gains of 5% and 33% in axial and lateral resolution respectively when compared to Tiny-CNN on in-vitro dataset. Additionally, our model showed a 4.2% increase in contrast and gains of 4% and 20% in axial and lateral resolution respectiv
    
[^39]: 连续时空模型的基于梯度和无导数优化方法的算子学习

    Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods. (arXiv:2311.11798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.11798](http://arxiv.org/abs/2311.11798)

    本论文提出了一种基于梯度和无导数优化方法的连续时空模型的算子学习框架，具有空间和时间上的分辨率不变性，并能够在短期时间序列和长期统计数据上高效训练。

    

    在许多工程应用中，常常使用偏微分方程对复杂动态系统进行时空建模。在这项工作中，我们借鉴了算子学习的最新进展，并提出了一种连续的数据驱动建模框架。所提出模型的一个关键特点是对于空间和时间离散化的分辨率不变性，而不需要大量的不同时间分辨率下的训练数据。为了改善校准模型的长期性能，我们进一步提出了一种混合优化方案，利用了基于梯度和无导数的优化方法，能够在短期时间序列和长期统计数据上高效训练。我们通过三个数值实例，包括粘性Burgers方程、Navier-Stokes方程和Kuramoto-Sivashinsky方程，来研究空间连续学习框架的性能。

    Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations, without demanding abundant training data in different temporal resolutions. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results 
    
[^40]: GAIA: 探索基于梯度的归因异常性以进行越界检测

    GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection. (arXiv:2311.09620v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.09620](http://arxiv.org/abs/2311.09620)

    本文研究了在深度神经网络中检测越界样本的问题，提出了一种基于梯度的归因异常性方法，用于量化内分布和越界数据之间的差异。通过引入零膨胀异常和通道平均异常，我们提出了一种简单而有效的GAIA方法，可以有效检测越界样本。

    

    在实际环境中，检测越界（OOD）样本对于保证深度神经网络的可靠性和安全性至关重要。本文针对在分析模型试图解释其预测决策时产生的不确定性，提出了一种创新的视角来量化内分布（ID）和越界数据之间的差异。我们观察到，基于梯度的归因方法在为越界数据分配特征重要性方面遇到了挑战，从而产生了不同的解释模式。因此，我们研究了归因梯度如何导致不确定的解释结果，并引入了两种形式的越界检测异常：零膨胀异常和通道平均异常。然后，我们提出了GAIA，一种简单而有效的方法，它结合了梯度异常检查和聚合。通过在常用的数据集（CIFAR）和大型数据集上验证了GAIA的有效性。

    Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data -- analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose GAIA, a simple and effective approach that incorporates Gradient Abnormality Inspection and Aggregation. The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and larg
    
[^41]: 电力分配网络中的知识图谱构建

    Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08724](http://arxiv.org/abs/2311.08724)

    本文提出了一种在电力分配网络中构建知识图谱的方法，该方法利用实体特征，在分配网络的知识图谱和分配文本中进行匹配，通过实验证明了其在电力分配知识图谱构建任务中的高准确性。

    

    本文提出了一种在电力分配网络中构建知识图谱的方法。该方法利用实体特征，包括其语义、音韵和句法特征，在分配网络的知识图谱和分配文本中进行匹配。基于卷积神经网络的增强模型，用于有效地将分配文本实体与知识图谱中的实体匹配。通过在真实世界的电力分配场景中进行实验评估了该模型的有效性。结果表明，与基线方法相比，所提出的模型在链接各种实体类型方面表现出色，在电力分配知识图谱构建任务中具有很高的整体准确性。

    In this paper, we propose a method for knowledge graph construction in power distribution networks. This method leverages entity features, which involve their semantic, phonetic, and syntactic characteristics, in both the knowledge graph of distribution network and the dispatching texts. An enhanced model based on Convolutional Neural Network, is utilized for effectively matching dispatch text entities with those in the knowledge graph. The effectiveness of this model is evaluated through experiments in real-world power distribution dispatch scenarios. The results indicate that, compared with the baselines, the proposed model excels in linking a variety of entity types, demonstrating high overall accuracy in power distribution knowledge graph construction task.
    
[^42]: 最小范数浅层去噪器在函数空间中的表现如何？

    How do Minimum-Norm Shallow Denoisers Look in Function Space?. (arXiv:2311.06748v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.06748](http://arxiv.org/abs/2311.06748)

    研究了最小范数浅层去噪器在函数空间中的表现，推导出一元数据和多元数据上的闭合形式，并发现其具有收缩性和较好的泛化能力。

    

    神经网络去噪器（NN去噪器）是许多常见任务中的基本构建块，从图像重建到图像生成。然而，从理论角度来看，这些模型的成功尚不明确。本文旨在描述浅层ReLU NN去噪器实现的函数特性--在插值的常见理论设置下（即零训练损失）以及最小表示成本（即最小的l^2范数权重）。首先，对于一元数据，我们导出了NN去噪器函数的闭合形式，发现它对干净数据点具有收缩性，并证明在低噪声水平下它比经验MMSE估计器更好地泛化。接下来，对于多元数据，我们在多种几何假设下找到了闭合形式的NN去噪器函数：数据包含在低维子空间中，数据包含在单向射线的并集中，或者多种类型的简单形状。这些函数分解为一个和的形式。

    Neural network (NN) denoisers are an essential building block in many common tasks, ranging from image reconstruction to image generation. However, the success of these models is not well understood from a theoretical perspective. In this paper, we aim to characterize the functions realized by shallow ReLU NN denoisers -- in the common theoretical setting of interpolation (i.e., zero training loss) with a minimal representation cost (i.e., minimal $\ell^2$ norm weights). First, for univariate data, we derive a closed form for the NN denoiser function, find it is contractive toward the clean data points, and prove it generalizes better than the empirical MMSE estimator at a low noise level. Next, for multivariate data, we find the NN denoiser functions in a closed form under various geometric assumptions on the training data: data contained in a low-dimensional subspace, data contained in a union of one-sided rays, or several types of simplexes. These functions decompose into a sum of s
    
[^43]: 边缘网络效率的深度学习架构

    Deep Learning Architecture for Network-Efficiency at the Edge. (arXiv:2311.05739v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2311.05739](http://arxiv.org/abs/2311.05739)

    本文提出了一种自适应压缩感知分离学习方法，将深度学习模型与边缘云资源集成，从而实现网络高效性和更快的计算速度，适用于部署在较弱设备上。

    

    移动设备上越来越多的人工智能应用导致了将深度学习模型与现有边缘云资源集成的解决方案；由于其在设备能耗、延迟、网络利用和隐私改进等方面的多重好处，将深度学习模型在分布式环境中分离并计算的分离学习已成为一个广泛研究的领域。结合对通信数据进行压缩的压缩感知方法，该方法的好处进一步提高，可以作为传统方法（如联邦学习方法）的替代方案。在本文中，我们开发了一种自适应压缩感知分离学习方法（'deprune'），以改善和训练深度学习模型，使其更加网络高效（使用更少的网络资源和更快），这将使它们成为在较弱设备上部署的理想选择。

    The growing number of AI-driven applications in the mobile devices has led to solutions that integrate deep learning models with the available edge-cloud resources; due to multiple benefits such as reduction in on-device energy consumption, improved latency, improved network usage, and certain privacy improvements, split learning, where deep learning models are split away from the mobile device and computed in a distributed manner, has become an extensively explored topic. Combined with compression-aware methods where learning adapts to compression of communicated data, the benefits of this approach have further improved and could serve as an alternative to established approaches like federated learning methods. In this work, we develop an adaptive compression-aware split learning method ('deprune') to improve and train deep learning models so that they are much more network-efficient (use less network resources and are faster), which would make them ideal to deploy in weaker devices w
    
[^44]: 在混合量子机器学习架构中分离量子和经典贡献

    Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2311.05559](http://arxiv.org/abs/2311.05559)

    我们提出了一种新颖的混合架构，通过自编码器将输入数据压缩后传递给量子部分，实现了在混合量子机器学习中分离量子和经典贡献。

    

    量子计算为数据密集型任务提供了更强大的计算能力，但目前的量子硬件对输入规模有很大限制。为了解决这个问题，我们提出了一种新颖的混合架构：使用自编码器得到输入数据的压缩版本，然后将其作为量子部分的输入。我们将该模型的分类能力与两种最先进的混合迁移学习架构、两种纯经典架构和一种量子架构进行了比较。

    Quantum computing offers the potential for superior computational capabilities, particularly for data-intensive tasks. However, the current state of quantum hardware puts heavy restrictions on input size. To address this, hybrid transfer learning solutions have been developed, merging pre-trained classical models, capable of handling extensive inputs, with variational quantum circuits. Yet, it remains unclear how much each component -- classical and quantum -- contributes to the model's results. We propose a novel hybrid architecture: instead of utilizing a pre-trained network for compression, we employ an autoencoder to derive a compressed version of the input data. This compressed data is then channeled through the encoder part of the autoencoder to the quantum component. We assess our model's classification capabilities against two state-of-the-art hybrid transfer learning architectures, two purely classical architectures and one quantum architecture. Their accuracy is compared acro
    
[^45]: 在Transformer中定位跨任务序列继续电路

    Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04131](http://arxiv.org/abs/2311.04131)

    通过分析和比较Transformer模型中类似的序列继续任务的电路，研究发现共享的计算结构可以提高模型的行为预测能力、错误识别能力和编辑过程的安全性。

    

    虽然Transformer模型在语言任务上展现出强大的能力，但其复杂的架构使其难以解释。最近的研究旨在将Transformer模型还原为可读的电路表示，用于实现算法功能。我们通过分析和比较类似的序列继续任务的电路来扩展这项研究，其中包括数字、数字词和月份的递增序列。通过应用电路分析技术，我们确定了负责检测序列成员和预测序列中下一个成员的关键子电路。我们的分析揭示了语义相关序列依赖于具有类似作用的共享电路子图。总体而言，记录共享的计算结构能够更好地预测模型行为，识别错误，并进行更安全的编辑过程。这种对Transformer的机械理解是构建更健壮、调试和编辑更安全的模型的关键一步。

    While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
    
[^46]: Uni-O4: 将在线与离线深度强化学习统一起来，采用多步在线策略优化

    Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03351](http://arxiv.org/abs/2311.03351)

    Uni-O4提出了统一的离线和在线深度强化学习方法，通过对齐目标实现了无缝传递，增强了学习范式的灵活性。在离线阶段，Uni-O4利用多样的集合策略解决了估计行为策略和离线数据集不匹配的问题。

    

    将离线和在线强化学习相结合对于高效和安全的学习至关重要。然而，以往的方法将离线和在线学习视为独立的过程，导致重复的设计和有限的性能。本文中，我们提出了Uni-o4，它在离线和在线学习中都使用了一个在线策略目标。由于两个阶段的目标对齐，RL代理可以在离线和在线学习之间无缝传递。这种性质增强了学习范式的灵活性，允许任意组合预训练、微调、离线和在线学习。在离线阶段，Uni-o4利用多样的集合策略来解决估计行为策略和离线数据集之间的不匹配问题。通过简单的离线策略评估（OPE）

    Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE
    
[^47]: 用稀疏添加机制移位变分自动编码器对细胞扰动进行建模

    Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder. (arXiv:2311.02794v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.02794](http://arxiv.org/abs/2311.02794)

    本研究提出了一种稀疏添加机制移位变分自动编码器（SAMS-VAE），用于建模细胞的扰动情况，并结合复合性、解缠和可解释性。通过稀疏化处理全局潜变量，SAMS-VAE能够识别出特定于干扰的潜在子空间，并在多个任务上进行了定量和定性评估。

    

    近年来，针对干预下观测数据的生成模型在机器学习和科学领域引起了广泛关注。例如，在药物发现中，需要对细胞的多种干预效应进行建模，以揭示未知的生物作用机制。我们提出了稀疏添加机制移位变分自动编码器（SAMS-VAE），以组合复合性、解缠和可解释性进行扰动模型。SAMS-VAE将扰动样本的潜在状态建模为一个局部潜在变量和稀疏全局变量之和，用于捕捉样本特定的变化和潜在干预效应。关键是，SAMS-VAE通过对各个干预的全局潜变量进行稀疏化处理，从而识别出解缠的、干扰特定的潜在子空间，这些子空间具有灵活的组合性。我们在两个流行的单细胞测序数据集上定量和定性评估了SAMS-VAE的性能。

    Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. In 
    
[^48]: 图像和临床生物医学中的多模式机器学习：调查和前景

    Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.02332](http://arxiv.org/abs/2311.02332)

    这项调查研究了多模式机器学习在医学图像分析和临床决策支持系统中的影响，强调了在多模态表示、融合、翻译、对齐和联合学习方面的挑战和创新，以及如何解决数据偏差和“大数据”稀缺性等问题。

    

    医疗人工智能系统中的机器学习应用已经从传统和统计方法转向越来越多地应用深度学习模型。本调查研究了当前多模式机器学习领域的现状，重点关注其对医学图像分析和临床决策支持系统的深远影响。本文强调了在多模态表示、融合、翻译、对齐和联合学习方面面临的挑战和创新，并探讨了多模态模型对临床预测的转变潜力。同时，本文还对这些模型的实际应用提出了疑问，关注决策支持系统与医疗服务提供者之间的相互影响。尽管取得了进展，但在许多生物医学领域中，数据偏差和“大数据”的稀缺性等挑战仍然存在。最后，本文讨论了有效创新和合作努力以进一步解决这些问题的方法。

    Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also questions practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers. Despite advancements, challenges such as data biases and the scarcity of "big data" in many biomedical domains persist. We conclude with a discussion on effective innovation and collaborative efforts to further the miss
    
[^49]: 学生网络是否应该复制或平均教师权重？

    Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v1 [cs.LG])

    [http://arxiv.org/abs/2311.01644](http://arxiv.org/abs/2311.01644)

    这项研究探讨了在欠参数化情况下，学生网络是否应该复制教师神经元或平均一组教师神经元的权重。研究发现对于特定的网络结构和输入分布，当教师网络的输入向量正交且输出权重为酉时，复制-平均配置将达到优化结果，其中大部分学生神经元复制一个教师神经元，最后一个学生神经元对所有教师神经元取平均值。

    

    任何连续函数 $f^*$ 都可以用足够多的神经元 $k$来近似。我们考虑 $f^*$ 本身是一个具有一个隐藏层和 $k$ 个神经元的神经网络的情况。用具有 $n<k$ 个神经元的神经网络来逼近 $f^*$ 可以看作是将一个欠参数化的“学生”网络与 $k$ 个神经元的“教师”网络进行拟合。由于学生具有较少的神经元，所以不清楚每个 $n$ 个学生神经元应该复制一个教师神经元还是平均一组教师神经元。对于具有 erf 激活函数和标准高斯输入分布的浅层神经网络，我们证明了当教师的输入向量是正交的并且输出权重是酉的时候，“复制-平均”配置是临界点。此外，在这样的配置中，优化结果是当 $n-1$ 个学生神经元分别复制一个教师神经元，并且第 $n$ 个学生神经元是所有教师神经元的平均。

    Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized "student" network with $n$ neurons to a "teacher" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student ne
    
[^50]: 基于深度学习的显微图像压缩：一项实证研究

    Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v1 [eess.IV])

    [http://arxiv.org/abs/2311.01352](http://arxiv.org/abs/2311.01352)

    本研究实证分析了经典和基于深度学习的图像压缩方法在深度学习图像处理模型中的影响，并以基于深度学习的无标签预测模型为例展示了图像压缩的有效性，以减小数据尺寸并保留必要信息，从而减轻数据管理基础设施的负担并方便数据共享或云计算。

    

    随着现代显微镜和生物成像技术的快速发展，产生了大量的显微图像数据，这些数据通过网络进行存储、分析甚至共享。数据的规模对当前的数据基础设施提出了巨大挑战。减小数据尺寸的一种常见方法是图像压缩。本研究分析了经典的和基于深度学习的图像压缩方法，并研究了它们对基于深度学习的图像处理模型的影响。基于深度学习的无标签预测模型（即从亮场图像预测荧光图像）被用作比较和分析的示例应用。有效的图像压缩方法可以在不丢失必要信息的情况下显著减小数据尺寸，从而减轻数据管理基础设施的负担，并实现数据共享或云计算下的快速传输。为了以这样的方式压缩图像，多种经典的丢失压缩方法和基于深度学习的无损压缩方法得到了评估。

    With the fast development of modern microscopes and bioimaging techniques, an unprecedentedly large amount of imaging data are being generated, stored, analyzed, and even shared through networks. The size of the data poses great challenges for current data infrastructure. One common way to reduce the data size is by image compression. This present study analyzes classic and deep learning based image compression methods, and their impact on deep learning based image processing models. Deep learning based label-free prediction models (i.e., predicting fluorescent images from bright field images) are used as an example application for comparison and analysis. Effective image compression methods could help reduce the data size significantly without losing necessary information, and therefore reduce the burden on data management infrastructure and permit fast transmission through the network for data sharing or cloud computing. To compress images in such a wanted way, multiple classical los
    
[^51]: 通过离散扩散学习无监督的自动驾驶世界模型

    Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])

    [http://arxiv.org/abs/2311.01017](http://arxiv.org/abs/2311.01017)

    本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。

    

    学习世界模型可以以无监督的方式教会智能体世界的运作方式。尽管它可以看作是序列建模的特殊情况，但在自动驾驶等机器人应用中，与使用生成预训练转换器（GPT）扩展语言模型相比，扩展世界模型的进展相对较慢。我们指出了两个主要瓶颈：处理复杂和无结构的观察空间以及具有可扩展性的生成模型。因此，我们提出了一种新颖的世界建模方法，首先使用VQVAE对传感器观察进行标记化，然后通过离散扩散预测未来。为了有效地并行解码和去噪标记，我们将遮蔽生成图像转换器转换为离散扩散框架，并进行了一些简单的改进，取得了显着的改进效果。当应用于点云观察的世界模型学习时，我们的模型将1秒预测的SOTA Chamfer距离降低了65%以上。

    Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
    
[^52]: 在线转换及其带有切换成本：稳健和学习增强算法

    Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms. (arXiv:2310.20598v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2310.20598](http://arxiv.org/abs/2310.20598)

    本论文介绍并研究了在线转换及其带有切换成本的问题，并提出了具有竞争力的阈值算法以及学习增强算法，这些算法在最小化和最大化变体中都表现出优越性能。

    

    我们介绍并研究在线转换及其带有切换成本的问题。这个问题涵盖了能源和可持续性交叉领域中出现的一系列新问题。在这个问题中，在线玩家试图在固定的时间段内购买（或销售）资产的分数份额，每个时间步骤都会公布成本（或价格）函数，并且玩家必须做出不可撤消的决策，决定转换的资产数量。当玩家连续时间步骤中改变决策时，也会产生切换成本，即在购买量增加或减少时。我们介绍了在这个问题的最小化和最大化变体中具有竞争力（稳健）的基于阈值的算法，并且证明它们是确定性在线算法中的最优算法。然后，我们提出了学习增强算法，利用不可信的黑盒建议（例如机器学习模型的预测）来显著改善算法性能。

    We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length $T$. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significant
    
[^53]: 将预训练语言模型整合到神经机器翻译中

    Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19680](http://arxiv.org/abs/2310.19680)

    该论文提出了PiNMT模型，将预训练语言模型整合到神经机器翻译中，通过三个关键部分和两种训练策略，实现了在IW数据集上的最先进性能。

    

    通过广泛的研究和开发，神经机器翻译（NMT）已成为自然语言处理中的重要技术。然而，高质量的双语语言对数据的不足仍然是提高NMT性能的主要挑战。最近的研究一直在探索使用预训练语言模型（PLM）的上下文信息来解决这个问题。然而，PLM和NMT模型之间的不兼容问题尚未解决。本研究提出了PLM整合的NMT（PiNMT）模型来解决这些问题。PiNMT模型由三个关键组成部分组成，分别是PLM多层转换器，嵌入融合和余弦对齐，每个部分在向NMT提供有效的PLM信息方面发挥着重要作用。此外，本文还介绍了两种训练策略，分别是分离学习率和双步训练。通过实施所提出的PiNMT模型和训练策略，在IW数据集上实现了最先进的性能。

    Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies have been exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes PLM-integrated NMT (PiNMT) model to overcome the identified problems. PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieve state-of-the-art performance on the IW
    
[^54]: The Memory Perturbation Equation: Understanding Model's Sensitivity to Data（理解模型对数据的敏感性的记忆扰动方程）

    The Memory Perturbation Equation: Understanding Model's Sensitivity to Data. (arXiv:2310.19273v1 [cs.LG])

    [http://arxiv.org/abs/2310.19273](http://arxiv.org/abs/2310.19273)

    这个论文介绍了记忆扰动方程（MPE），该方程通过应用贝叶斯原理将模型的敏感性与训练数据的扰动联系起来，并且能够准确预测模型在未见测试数据上的泛化能力。

    

    理解模型对其训练数据的敏感性对于训练过程至关重要，但也可能具有挑战性和成本高昂。为了简化这类问题，我们提出了记忆扰动方程（MPE），它将模型的敏感性与其训练数据的扰动联系起来。使用贝叶斯原理导出的MPE将现有的敏感性度量统一起来，泛化到各种模型和算法，并揭示了有关敏感性的有用性质。我们的实证结果表明，训练过程中获得的敏感性估计可以准确预测在未见测试数据上的泛化能力。该提出的方程预计将对未来的鲁棒和自适应学习研究有用。

    Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.
    
[^55]: 等距运动流形基元

    Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])

    [http://arxiv.org/abs/2310.17072](http://arxiv.org/abs/2310.17072)

    Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.

    

    运动流形基元（MMP）为给定任务生成一系列连续轨迹流形，每一个轨迹流形都能成功完成任务。它由对流形进行参数化的解码函数以及潜在坐标空间中的概率密度组成。本文首先展示了由于潜在空间中的几何扭曲，MMP的性能可能会显著降低--通过变形，我们指的是相似的运动在潜在空间中无法相邻。然后，我们提出了等距运动流形基元（IMMP），其潜在坐标空间保持了流形的几何结构。为此，我们建立和使用了一个Riemannian度量，用于运动空间（即，参数化曲线空间），我们称之为CurveGeom Riemannian度量。对于平面障碍避让运动和推动操纵任务的实验表明，IMMP明显优于现有的MMP方法。代码可在https://github.com/Gabe-YHLee/IMMP找到。

    The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
    
[^56]: Federated Learning在通信错误方面的鲁棒性有多强？来自上行和下行通道的比较研究。

    How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels. (arXiv:2310.16652v1 [cs.LG])

    [http://arxiv.org/abs/2310.16652](http://arxiv.org/abs/2310.16652)

    本文研究了联邦学习对上行和下行通信错误的鲁棒性。理论分析表明，鲁棒性取决于客户端数量和模型参数的数值范围。实验证明上行通信可以容忍更高的误码率比下行通信。

    

    由于其保护隐私的能力，联邦学习（FL）在学术界和工业界引起了广泛关注。然而，当在无线网络上实现时，FL能够容忍多少通信错误尚不清楚。本文研究了FL对上行和下行通信错误的鲁棒性。我们的理论分析揭示了鲁棒性取决于两个关键参数，即客户端数量和模型参数的数值范围。同时，我们证明了FL的上行通信可以容忍更高的误码率（BER）比下行通信，并提出了一个定量的差异公式。通过大量实验进一步验证了这些发现和理论分析。

    Because of its privacy-preserving capability, federated learning (FL) has attracted significant attention from both academia and industry. However, when being implemented over wireless networks, it is not clear how much communication error can be tolerated by FL. This paper investigates the robustness of FL to the uplink and downlink communication error. Our theoretical analysis reveals that the robustness depends on two critical parameters, namely the number of clients and the numerical range of model parameters. It is also shown that the uplink communication in FL can tolerate a higher bit error rate (BER) than downlink communication, and this difference is quantified by a proposed formula. The findings and theoretical analyses are further validated by extensive experiments.
    
[^57]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^58]: 重新思考分子掩码图模型中的分词器和解码器

    Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules. (arXiv:2310.14753v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14753](http://arxiv.org/abs/2310.14753)

    重新思考分子掩码图模型中的分词器和解码器，通过对分子分词器的总结和解码器的探索，填补了分词器和解码器的研究空白。

    

    掩码图模型在自监督表示学习中表现出众，特别是对于分子图。通过对之前的研究进行仔细审查，我们可以揭示一个常见的方案，包括三个关键组成部分：（1）图分词器，它将分子图分解为较小的片段（即子图），并将其转换为标记；（2）图掩码，用掩码破坏图；（3）图自编码器，它首先在掩码图上应用编码器生成表示，然后在表示上使用解码器恢复原始图的标记。然而，之前的MGM研究主要关注图掩码和编码器，对于分词器和解码器的理解较有限。为了弥补这个差距，我们首先总结了节点、边、主题和图神经网络（GNN）的常用分子分词器，然后考察它们作为MGM重构目标的作用。此外，我们还探索了在MGM中采用高表达解码器的潜力。

    Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (i.e., subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in M
    
[^59]: 超越准确性：用IdentityChain评估大型代码语言模型的自一致性

    Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14053](http://arxiv.org/abs/2310.14053)

    这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。

    

    代码语言模型(Code LLMs)在实际应用中的使用越来越多，因此对它们进行评估至关重要。传统的准确性评估方法评估Code LLMs在一系列独立任务上的性能，但忽视了其在不同任务上的自一致性。直观来讲，一个可信赖的模型在为其自身的代码生成自然语言规范以及为其自身的规范生成代码时应该是自一致的。未能保持自一致性揭示了对自然语言和编程语言共享语义的理解的不足，从而削弱了模型的可信度。本文首先正式定义了Code LLMs的自一致性，然后设计了一个名为IdentityChain的框架，可以同时有效且高效地评估模型的自一致性和传统准确性。我们研究了11个Code LLMs，并表明它们未能保持自一致性。

    Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
    
[^60]: DeepFDR：一种用于神经影像数据的基于深度学习的虚警控制方法

    DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v1 [stat.ML])

    [http://arxiv.org/abs/2310.13349](http://arxiv.org/abs/2310.13349)

    DeepFDR是一种基于深度学习的虚警控制方法，通过利用无监督的图像分割技术解决神经影像数据中的多重检验问题，并在实验证明其相对于现有方法具有卓越的性能。

    

    基于体素的多重检验在神经影像数据分析中广泛应用。传统的虚警控制方法常常忽视基于体素的检验之间的空间相关性，从而导致测试能力的大幅损失。虽然最近出现了一些空间虚警控制方法，但是当处理复杂的脑空间依赖关系时，它们的有效性和最优性仍存在疑问。与此同时，深度学习方法已经在图像分割方面取得了革命性的进展，而图像分割与基于体素的多重检验密切相关。本文提出了一种名为DeepFDR的新型空间虚警控制方法，利用无监督的基于深度学习的图像分割来解决基于体素的多重检验问题。包括全面的模拟和阿尔茨海默病FDG-PET影像分析在内的数值研究表明DeepFDR相对于现有方法具有优势。DeepFDR不仅在虚警控制方面表现出色，还有效降低了虚假的非发现率。

    Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but als
    
[^61]: 约束重加权分布：一种最优传输方法的研究

    Constrained Reweighting of Distributions: an Optimal Transport Approach. (arXiv:2310.12447v1 [stat.ML])

    [http://arxiv.org/abs/2310.12447](http://arxiv.org/abs/2310.12447)

    本文提出了一种最优传输方法，通过引入非参数化的分布约束权重，并利用最大熵原理和最优传输工具设计了一个通用框架，以实现对观测数据的最优权重调整。这种方法在不同的应用场景中展现了灵活性和多功能性。

    

    我们经常遇到的问题是要识别出符合预定义的权重约束条件的观测数据的经验分布的最优调整版本。这些约束通常表现为对权重的矩、尾部行为、形状、模式数量等的限制。在本文中，我们通过引入一种非参数化的分布约束权重并利用最大熵原理和最优传输工具开发了一个通用框架，从而大大提高了这种方法的灵活性。关键思想是确保观测数据的最大熵权重调整经验分布与预定的概率分布在最优传输度量下接近，并允许细微的偏差。该框架的多功能性在三个不同的应用场景中得到了证明，其中数据重加权是合理的。

    We commonly encounter the problem of identifying an optimally weight adjusted version of the empirical distribution of observed data, adhering to predefined constraints on the weights. Such constraints often manifest as restrictions on the moments, tail behaviour, shapes, number of modes, etc., of the resulting weight adjusted empirical distribution. In this article, we substantially enhance the flexibility of such methodology by introducing a nonparametrically imbued distributional constraints on the weights, and developing a general framework leveraging the maximum entropy principle and tools from optimal transport. The key idea is to ensure that the maximum entropy weight adjusted empirical distribution of the observed data is close to a pre-specified probability distribution in terms of the optimal transport metric while allowing for subtle departures. The versatility of the framework is demonstrated in the context of three disparate applications where data re-weighting is warrante
    
[^62]: MDP中LTL和ω-regular目标的PAC学习算法

    A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])

    [http://arxiv.org/abs/2310.12248](http://arxiv.org/abs/2310.12248)

    这个论文介绍了一种基于模型的PAC学习算法，用于在MDP中学习ω-regular目标，不需要系统拓扑的先前知识。

    

    线性时序逻辑（LTL）和ω-regular目标是近期用于在强化学习中表达非马尔可夫目标的一种方式。我们提出了一种基于模型的可能近似正确（PAC）学习算法，用于MDP中的ω-regular目标。与之前的方法不同，我们的算法从系统的采样轨迹中进行学习，并且不需要系统拓扑的先前知识。

    Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
    
[^63]: 将连续学习重塑为序列建模

    Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])

    [http://arxiv.org/abs/2310.11952](http://arxiv.org/abs/2310.11952)

    本文将连续学习重塑为序列建模问题，并提出了利用序列模型进行连续学习的方法。通过采用元连续学习框架，需要对序列模型进行多次连续学习episode上的元级训练。实验证明序列模型可以成为一种吸引人的通用连续学习解决方案。

    

    在这项工作中，我们旨在建立机器学习研究中两个重要领域之间的强连接：连续学习和序列建模。也就是说，我们提出将连续学习作为序列建模问题进行表述，从而可以利用先进的序列模型进行连续学习。在这个框架下，连续学习过程成为序列模型的前向传播。通过采用元连续学习(MCL)框架，我们可以在多个连续学习episode上对序列模型进行元级训练。作为我们新框架的一个具体示例，我们展示了将Transformer及其高效变体应用于MCL方法。我们在包括分类和回归的七个基准测试上的实验证明了序列模型可以成为一种吸引人的通用连续学习解决方案。

    In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
    
[^64]: 渗透式人工智能：使LLMs理解物理世界

    Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.09605](http://arxiv.org/abs/2310.09605)

    本文探讨了渗透式人工智能的概念，旨在使LLMs能够通过物联网传感器与执行器与物理世界进行交互和推理。初步研究结果表明，LLMs具有独特的能力，能够应用内嵌的世界知识解释物联网传感器数据并进行物理领域的推理。

    

    最近大型语言模型（LLMs）的发展展示了它们在各种任务中的显著能力。然而，关于LLMs的性质以及它们在涉及真实物理世界信息的任务中整合常识人类知识的潜力仍存在疑问。本文通过探索LLMs如何通过物联网传感器和执行器与物理世界进行交互和推理来探讨这些问题，这一概念称为“渗透式人工智能”。论文在LLMs能够透过处理感知信号的两个层面上探索了这种扩展。我们的初步研究结果表明，LLMs（ChatGPT是我们研究中的代表性例子）在应用内嵌的世界知识解释物联网传感器数据并对物理领域的任务进行推理方面具有相当独特的能力。这不仅为LLMs开辟了新的应用领域。

    Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs 
    
[^65]: 使用Transformer进行计数和算法推广

    Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v1 [cs.LG])

    [http://arxiv.org/abs/2310.08661](http://arxiv.org/abs/2310.08661)

    这项研究分析了在计数任务中的算法推广，并证明了标准的Transformer的架构决策会阻碍其在超出分布任务上的性能。通过消除问题操作，修改后的Transformer在计数方面展示出了良好的算法推广性能。

    

    机器学习中的算法推广是指学习生成数据的底层算法，以一种对超出分布的方式进行泛化的能力。这对大多数机器学习算法来说通常是一个困难的任务。在这里，我们分析了在计数时需要的算法推广，无论是隐式还是显式。我们表明标准的Transformer是基于阻碍这类任务的架构决策。特别是，我们讨论了使用层归一化和通过softmax归一化注意力权重的后果。通过消除这些问题操作，我们证明修改后的Transformer可以在计数方面展示出良好的算法推广性能，同时采用非常轻量级的架构。

    Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.
    
[^66]: 具有重编码器的神经组合优化：朝着大规模通用化迈进

    Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization. (arXiv:2310.07985v1 [cs.LG])

    [http://arxiv.org/abs/2310.07985](http://arxiv.org/abs/2310.07985)

    提出了一种具有强大通用性的轻编码器和重解码器（LEHD）模型，通过动态学习节点间关系，解决了神经组合优化在大规模实例上的问题，并应用于旅行商问题和容量限制车辆路径规划问题。

    

    神经组合优化（NCO）是一种有前景的基于学习的方法，可以通过传统的专家算法设计来解决具有挑战性的组合优化问题。然而，大多数构造性的NCO方法不能解决大规模实例大小的问题，这显著降低了它们在实际应用中的实用性。在这项工作中，我们提出了一种新颖的轻编码器和重解码器（LEHD）模型，具有强大的通用性，以解决这个关键问题。LEHD模型可以动态地学习到不同尺寸的所有可用节点之间的关系，这有利于模型对各种规模的问题进行泛化。此外，我们还为LEHD模型开发了一种数据高效的训练方案和灵活的解决方案构建机制。通过在小规模问题实例上进行训练，LEHD模型可以生成近乎最优的旅行商问题（TSP）和容量限制车辆路径规划问题

    Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing
    
[^67]: 迈向针对漏洞检测的因果深度学习

    Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])

    [http://arxiv.org/abs/2310.07958](http://arxiv.org/abs/2310.07958)

    本文提出了一种针对漏洞检测的因果深度学习方法CausalVul，通过引入因果性，并设计新的扰动，解决了深度学习漏洞检测中模型不稳定和泛化性能差的问题。

    

    近年来，深度学习的漏洞检测取得了有希望的成果。然而，一个阻碍其在实践中非常有用的重要挑战是模型在扰动下不稳定，并且不能很好地泛化到超出分布（OOD）的数据，例如，在真实世界中将训练好的模型应用到未见过的项目上。我们假设这是因为模型学习到了非稳定的特征，例如变量名，与标签具有虚假相关性。当扰动和OOD数据集不再具有相同的虚假特征时，模型预测失败。为了解决这个挑战，在本文中，我们将因果性引入了深度学习漏洞检测中。我们的方法CausalVul分为两个阶段。首先，我们设计了新的扰动来发现模型可能用于进行预测的虚假特征。其次，我们在现有的深度学习模型之上应用了因果学习算法，特别是do-计算，来解决这个问题。

    Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
    
[^68]: 关于稀疏回归、Lp正则化和自动模型发现的论文

    On sparse regression, Lp-regularization, and automated model discovery. (arXiv:2310.06872v1 [cs.LG])

    [http://arxiv.org/abs/2310.06872](http://arxiv.org/abs/2310.06872)

    该论文通过结合正则化和物理约束的方法，利用神经网络进行自动模型发现，探索了非线性回归中稀疏回归和特征提取的潜力，并针对材料建模提出了常见的指导方针和趋势。

    

    稀疏回归和特征提取是从大规模数据中进行知识发现的基础。它们的目标是发现能够提供科学变量之间简单关系的可解释和预测模型。虽然在线性回归的模型发现方面的统计工具已经得到很好地建立，但在材料建模中将其推广到非线性回归是高度问题特定且理解不够充分的。在这里，我们探索了神经网络在自动模型发现中的潜力，并通过结合两种策略的混合方法来引入稀疏性：正则化和物理约束。我们将Lp正则化的概念与基于本领域在运动学和热力学方面的知识的构成神经网络相结合。我们使用合成数据和真实数据对网络进行训练，并进行了几千次发现运行来推断出共同的指导方针和趋势：L2正则化或岭回归是最佳的选择。

    Sparse regression and feature extraction are the cornerstones of knowledge discovery from massive data. Their goal is to discover interpretable and predictive models that provide simple relationships among scientific variables. While the statistical tools for model discovery are well established in the context of linear regression, their generalization to nonlinear regression in material modeling is highly problem-specific and insufficiently understood. Here we explore the potential of neural networks for automatic model discovery and induce sparsity by a hybrid approach that combines two strategies: regularization and physical constraints. We integrate the concept of Lp regularization for subset selection with constitutive neural networks that leverage our domain knowledge in kinematics and thermodynamics. We train our networks with both, synthetic and real data, and perform several thousand discovery runs to infer common guidelines and trends: L2 regularization or ridge regression is
    
[^69]: 机会平等对统计性歧视的影响

    The Impact of Equal Opportunity on Statistical Discrimination. (arXiv:2310.04585v1 [econ.TH])

    [http://arxiv.org/abs/2310.04585](http://arxiv.org/abs/2310.04585)

    本文通过修改统计性歧视模型，考虑了由机器学习生成的可合同化信念，给监管者提供了一种超过肯定行动的工具，通过要求公司选取一个平衡不同群体真正阳性率的决策策略，实现机会平等来消除统计性歧视。

    

    本文修改了Coate和Loury（1993）的经典统计性歧视模型，假设公司对个体未观察到的类别的信念是由机器学习生成的，因此是可合同化的。这扩展了监管者的工具箱，超出了像肯定行动这样的无信念规定。可合同化的信念使得要求公司选择一个决策策略，使得不同群体之间的真正阳性率相等（算法公平文献中所称的机会平等）成为可能。尽管肯定行动不一定能消除统计性歧视，但本文表明实施机会平等可以做到。

    I modify the canonical statistical discrimination model of Coate and Loury (1993) by assuming the firm's belief about an individual's unobserved class is machine learning-generated and, therefore, contractible. This expands the toolkit of a regulator beyond belief-free regulations like affirmative action. Contractible beliefs make it feasible to require the firm to select a decision policy that equalizes true positive rates across groups -- what the algorithmic fairness literature calls equal opportunity. While affirmative action does not necessarily end statistical discrimination, I show that imposing equal opportunity does.
    
[^70]: 通过交互感知自动转换增强特征认知

    Feature Cognition Enhancement via Interaction-Aware Automated Transformation. (arXiv:2309.17011v1 [cs.LG])

    [http://arxiv.org/abs/2309.17011](http://arxiv.org/abs/2309.17011)

    该论文提出了一种交互感知的增强生成方法，以改进自动特征工程的表示空间，解决重构特征空间不可理解和缺乏系统探索的问题。

    

    创建一个有效的表示空间对于降低维数灾难、增强模型泛化能力、解决数据稀疏性以及更有效地利用经典模型至关重要。最近的自动特征工程（AutoFE）的进展在解决表示学习中的各种挑战方面取得了重大进展，例如对大量劳动和经验的依赖、缺乏可解释性和在下游任务中嵌入刚性特征空间重建问题。然而，这些方法受到以下限制：1）产生潜在的难以理解和不合常理的重构特征空间，源于忽视专家级的认知过程；2）缺乏系统的探索，进而导致模型收敛缓慢以找到最佳特征空间。为了解决这些问题，我们引入了一种交互感知的增强生成方法。我们重新定义了特征空间的生成过程，以充分利用领域专家的认知过程和系统的探索。

    Creating an effective representation space is crucial for mitigating the curse of dimensionality, enhancing model generalization, addressing data sparsity, and leveraging classical models more effectively. Recent advancements in automated feature engineering (AutoFE) have made significant progress in addressing various challenges associated with representation learning, issues such as heavy reliance on intensive labor and empirical experiences, lack of explainable explicitness, and inflexible feature space reconstruction embedded into downstream tasks. However, these approaches are constrained by: 1) generation of potentially unintelligible and illogical reconstructed feature spaces, stemming from the neglect of expert-level cognitive processes; 2) lack of systematic exploration, which subsequently results in slower model convergence for identification of optimal feature space. To address these, we introduce an interaction-aware reinforced generation perspective. We redefine feature sp
    
[^71]: 学习转换以实现通用的实例不变性

    Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])

    [http://arxiv.org/abs/2309.16672](http://arxiv.org/abs/2309.16672)

    该论文提出了一种学习转换以实现通用的实例不变性的方法。通过使用归一化流来预测图像的变换分布，并对预测结果进行平均，可以实现对不同实例之间的对齐，从而推广不变性的类别间的应用。这种方法还可以适应超出分布范围的姿势，并且可以学习更广泛的变换范围。

    

    计算机视觉研究一直致力于构建对自然数据中的空间变换具有强鲁棒性的系统。传统上，可以通过数据增强或将不变性硬编码到架构中来实现这一点。然而，过多或过少的不变性都可能会影响结果，正确的不变性程度在先验中是未知的，并且依赖于实例。理想情况下，应该从数据中学习适当的不变性，并在测试时推断。我们将不变性视为一个预测问题。给定任何图像，我们使用一个归一化流来预测变换的分布，并对它们的预测进行平均。由于这个分布仅取决于实例，我们可以在分类之前对实例进行对齐，并在类别之间推广不变性。同样的分布也可以用于适应超出分布的姿势。这个归一化流是端到端训练的，并且可以学习比Augerino和InstaAug更多范围的变换。当用作数据增强时，我们的m

    Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.  We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our m
    
[^72]: 无监督语言模型蒸馏的事实验证

    Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])

    [http://arxiv.org/abs/2309.16540](http://arxiv.org/abs/2309.16540)

    本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。

    

    无监督事实验证旨在通过可靠知识库中的证据来验证主张，而无需任何形式的数据注释。为了解决这个挑战，算法必须为每个主张生成既语义明确又紧凑的特征，以便与源信息进行语义对齐。与之前的工作不同，前者通过学习包含主张及其相应标签的注释语料库来解决对齐问题。我们提出了SFAVEL（通过语言模型蒸馏的自监督事实验证），这是一个新颖的无监督框架，利用预训练的语言模型将自监督特征蒸馏为高质量的主张-事实对齐，而无需注释。这是通过一种新颖的对比损失函数实现的，该函数鼓励特征在保持语料库间的语义关系的同时实现高质量的主张和证据对齐。值得注意的是，我们展示了达到新颖的状态一.

    Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
    
[^73]: 把最小值的锐度的讨论带入到音频领域:一个用于声学场景分类的滤波器归一化评估

    Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification. (arXiv:2309.16369v1 [cs.SD])

    [http://arxiv.org/abs/2309.16369](http://arxiv.org/abs/2309.16369)

    本研究探索了在音频场景分类任务中损失函数最小值的锐度与泛化之间的关联。研究发现锐度更高的最小值具有更好的泛化能力，尤其是对于从之前未见设备录制的领域外数据来说。研究还发现优化器的选择是最小值锐度的主要影响因素，并讨论了在可比性方面的相关限制。

    

    在深度神经网络的背景下，损失函数最小值的锐度和泛化之间的相关性一直存在争议。虽然大多数研究都集中在计算机视觉领域的选定基准数据集上，但我们在DCASE2020挑战数据的音频场景分类任务中探索了这一方面。我们的分析基于二维滤波器归一化可视化和派生的锐度度量。我们的初步分析表明，锐度更高的最小值通常比平坦的最小值具有更好的泛化能力，尤其是对于从之前未见设备录制的领域外数据来说。这进一步加深了关于平坦最小值泛化能力的争议。我们还发现，特别是优化器的选择是最小值锐度的主要驱动因素，并讨论了在可比性方面的相关限制。我们的代码、训练模型状态和损失图景可公开获取。

    The correlation between the sharpness of loss minima and generalisation in the context of deep neural networks has been subject to discussion for a long time. Whilst mostly investigated in the context of selected benchmark data sets in the area of computer vision, we explore this aspect for the audio scene classification task of the DCASE2020 challenge data. Our analysis is based on twodimensional filter-normalised visualisations and a derived sharpness measure. Our exploratory analysis shows that sharper minima tend to show better generalisation than flat minima -even more so for out-of-domain data, recorded from previously unseen devices-, thus adding to the dispute about better generalisation capabilities of flat minima. We further find that, in particular, the choice of optimisers is a main driver of the sharpness of minima and we discuss resulting limitations with respect to comparability. Our code, trained model states and loss landscape visualisations are publicly available.
    
[^74]: 通用文本指导语音转换模型研究

    Towards General-Purpose Text-Instruction-Guided Voice Conversion. (arXiv:2309.14324v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14324](http://arxiv.org/abs/2309.14324)

    本文介绍了一种通过文本指令进行语音转换的通用模型。与传统方法不同，该模型利用文本指令修改语音的韵律和情感信息，具有较高的灵活性和特异性。实验证明了该模型能够理解指令并产生合理结果。

    

    本文介绍了一种新颖的语音转换模型，该模型通过文本指令如"慢慢清晰地说话，声音低沉"或"以快乐的少年声音说话"来指导转换过程。与传统方法不同，该模型增加了语音转换的通用性和特异性。该提出的语音转换模型是一个神经编解码器语言模型，通过处理一系列离散编码，得到转换后的语音编码序列。它利用文本指令作为样式提示，修改给定语音的韵律和情感信息。与之前的方法相比，该模型以端到端的方式处理语音的各种信息，而不需要使用独立的编码器处理源语音的不同方面，如韵律和内容。实验证明了我们模型在理解指令并产生合理结果方面的卓越能力。

    This paper introduces a novel voice conversion (VC) model, guided by text instructions such as "articulate slowly with a deep tone" or "speak in a cheerful boyish voice". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.
    
[^75]: 神经策略镜像梯度在低维流形上的策略优化的样本复杂性研究

    Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds. (arXiv:2309.13915v1 [cs.LG])

    [http://arxiv.org/abs/2309.13915](http://arxiv.org/abs/2309.13915)

    本研究探讨了神经策略镜像梯度算法在低维流形上的样本复杂性。研究发现在每次迭代中，卷积神经网络可以很好地逼近价值函数和策略，且逼近误差受网络大小的影响，并且可以继承之前网络的平滑性。

    

    在强化学习中，配备有深度神经网络的策略优化算法在解决高维度的问题中取得了巨大的成功。然而，目前的分析无法解释它们为何能抵抗维度诅咒。在本研究中，我们研究了具有卷积神经网络作为函数逼近器的神经策略镜像梯度（NPMD）算法的样本复杂性。受到许多高维环境具有低维结构的经验观察的启发，例如将图像作为状态，我们将状态空间视为嵌入在$D$维欧氏空间中的$d$维流形，其中$d\ll D$是内在维度。我们证明了在NPMD的每次迭代中，价值函数和策略都可以很好地由卷积神经网络进行逼近。逼近误差由网络的大小控制，并且前一个网络的平滑性可以保留。

    Policy-based algorithms equipped with deep neural networks have achieved great success in solving high-dimensional policy optimization problems in reinforcement learning. However, current analyses cannot explain why they are resistant to the curse of dimensionality. In this work, we study the sample complexity of the neural policy mirror descent (NPMD) algorithm with convolutional neural networks (CNN) as function approximators. Motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with intrinsic dimension $d\ll D$. We show that in each iteration of NPMD, both the value function and the policy can be well approximated by CNNs. The approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. As a
    
[^76]: AMPLIFY: 基于注意力机制的Mixup方法，用于提高Transformer模型的性能和标签平滑

    AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])

    [http://arxiv.org/abs/2309.12689](http://arxiv.org/abs/2309.12689)

    AMPLIFY提出了一种基于注意力机制的Mixup方法，用于减少原始样本中的噪音和异常值对于模型的影响，并在文本分类任务中表现出更好的性能。

    

    Mixup是一种有效的数据增强方法，通过对不同原始样本的线性组合生成新的增强样本。然而，如果原始样本中存在噪音或异常特征，Mixup可能将其传播到增强样本中，导致模型对这些异常值过于敏感。为了解决这个问题，本文提出了一种新的Mixup方法称为AMPLIFY。该方法利用Transformer自身的注意力机制减少原始样本中噪音和异常值对预测结果的影响，无需增加可训练参数，计算成本非常低，从而避免了常见Mixup方法（例如语句Mixup）中资源消耗过高的问题。实验结果表明，在更小的计算资源成本下，AMPLIFY在7个基准数据集的文本分类任务中优于其他Mixup方法，为进一步提高模型性能提供了新的思路和方法。

    Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
    
[^77]: 在序列长度上并行化非线性顺序模型

    Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])

    [http://arxiv.org/abs/2309.12252](http://arxiv.org/abs/2309.12252)

    本论文提出了一种并行算法，能够加速GPU对于顺序模型的评估速度，提高了3个数量级，而不降低输出准确性。该算法适用于各种架构，并在长时间序列分类问题中发现了门控循环单元的有效性。

    

    顺序模型，例如循环神经网络和神经常微分方程，在训练过程中一直由于其本质上的顺序特性而存在训练缓慢的问题。多年来这个瓶颈一直存在，因为很多人认为顺序模型无法并行化。我们通过并行算法挑战了这个长期以来的信念，加速了GPU对于顺序模型的评估速度，速度提高了3个数量级，而不牺牲输出准确性。该算法不需要顺序模型架构中的任何特殊结构，适用于各种架构。使用我们的方法，训练顺序模型可以比常规的顺序方法快10倍以上，而训练结果没有明显差异。借助这种加速训练，我们在一个包含17k个时间样本的长时间序列分类问题中发现了门控循环单元的有效性。通过克服训练瓶颈，我们的工作使得顺序模型的训练更加高效。

    Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work 
    
[^78]: 使用改进的方差最小化的分块量化对图神经网络进行激活压缩

    Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v1 [stat.ML])

    [http://arxiv.org/abs/2309.11856](http://arxiv.org/abs/2309.11856)

    本论文提出了一种使用改进的方差最小化的分块量化策略，用于压缩图神经网络的激活，实现内存消耗的降低和运行时的加速。

    

    已经研究了大规模图神经网络（GNNs）的高效训练，重点是减少其内存消耗。Liu等人（2022年）提出了极限激活压缩（EXACT），通过将中间激活图的量化降至INT2精度，实现了内存消耗的剧烈减少。他们在实现大幅减少GPU内存消耗的同时，表现几乎没有降低。在这项工作中，我们通过使用中间激活图的分块量化，对EXACT策略进行了改进。我们实验分析了不同的块大小，并展示了进一步的内存消耗降低（>15%）和每个epoch的运行时加速（约5%），即使进行了极其大的量化程度，也能获得与原始EXACT相似的性能权衡。此外，我们对EXACT中关于中间激活图分布的假设进行了纠正（假设为u

    Efficient training of large-scale graph neural networks (GNNs) has been studied with a specific focus on reducing their memory consumption. Work by Liu et al. (2022) proposed extreme activation compression (EXACT) which demonstrated drastic reduction in memory consumption by performing quantization of the intermediate activation maps down to using INT2 precision. They showed little to no reduction in performance while achieving large reductions in GPU memory consumption. In this work, we present an improvement to the EXACT strategy by using block-wise quantization of the intermediate activation maps. We experimentally analyze different block sizes and show further reduction in memory consumption (>15%), and runtime speedup per epoch (about 5%) even when performing extreme extents of quantization with similar performance trade-offs as with the original EXACT. Further, we present a correction to the assumptions on the distribution of intermediate activation maps in EXACT (assumed to be u
    
[^79]: 单步和少步扩散用于生成式语音增强

    Single and Few-step Diffusion for Generative Speech Enhancement. (arXiv:2309.09677v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.09677](http://arxiv.org/abs/2309.09677)

    本文提出了一种通过两阶段训练的方法来解决扩散模型在语音增强中的限制。第一阶段使用生成去噪评分匹配损失训练扩散模型，第二阶段通过解决反向过程来计算增强信号，并使用预测损失进行比较。这种方法只需要5个函数评估就能达到与基准模型相同的性能，而不是60个函数评估。

    

    扩散模型在语音增强方面展示了有希望的结果，利用任务适应性扩散过程来生成给定嘈杂混合声音的纯净语音。然而，在测试时，用于评分估计的神经网络被多次调用以解决迭代的反向过程。这导致推理过程缓慢，并导致在采样轨迹中积累离散化误差。在本文中，我们通过两阶段训练方法解决了这些限制。在第一阶段，我们用生成去噪评分匹配损失的常规方式训练扩散模型。在第二阶段，我们通过解决反向过程来计算增强信号，并使用预测损失将结果估计与纯净语音目标进行比较。我们证明使用这个第二训练阶段只需要5个函数评估，就能达到与基准模型相同的性能，而不是60个函数评估。虽然基准模型的性能可能

    Diffusion models have shown promising results in speech enhancement, using a task-adapted diffusion process for the conditional generation of clean speech given a noisy mixture. However, at test time, the neural network used for score estimation is called multiple times to solve the iterative reverse process. This results in a slow inference process and causes discretization errors that accumulate over the sampling trajectory. In this paper, we address these limitations through a two-stage training approach. In the first stage, we train the diffusion model the usual way using the generative denoising score matching loss. In the second stage, we compute the enhanced signal by solving the reverse process and compare the resulting estimate to the clean speech target using a predictive loss. We show that using this second training stage enables achieving the same performance as the baseline model using only 5 function evaluations instead of 60 function evaluations. While the performance of
    
[^80]: 深入研究睡眠：基于单通道脑电图的睡眠阶段分类与模型解释性

    A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])

    [http://arxiv.org/abs/2309.07156](http://arxiv.org/abs/2309.07156)

    本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。

    

    睡眠是一种基本的生理过程，在我们的生活中占据着重要的部分。准确分类睡眠阶段是评估睡眠质量和识别可能的睡眠障碍的关键工具。本研究引入了一种新颖的方法，利用SE-Resnet-Bi-LSTM架构将睡眠分类为五个不同的阶段。分类过程基于对单通道脑电图（EEG）的分析。建议的框架由两个基本元素组成：利用SE-ResNet的特征提取器和利用Bi-LSTM单元堆栈的时间上下文编码器。我们的方法的有效性通过在三个不同的数据集上进行的全面评估得到证实，分别是SLeepEDF-20、SleepEDF-78和SHHS。值得注意的是，我们的方法在相应的数据集上分别达到了显著的准确率，分别为87.5％、83.9％和87.8％，并且在宏F1得分方面分别为82.5、78.9和81.9。

    Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
    
[^81]: 多个敏感属性的顺序公平机制

    A Sequentially Fair Mechanism for Multiple Sensitive Attributes. (arXiv:2309.06627v1 [stat.ML])

    [http://arxiv.org/abs/2309.06627](http://arxiv.org/abs/2309.06627)

    本论文提出了一个顺序框架来逐步实现对多个敏感特征的公平性，通过利用多边际Wasserstein重心扩展了标准的强人口平等概念，并提供了闭式解来解释敏感特征之间的相关性。

    

    在算法公平性的标准用例中，目标是消除敏感变量和相应分数之间的关系。然而，在多个敏感属性的情况下，这些工具和定义的适用性和有效性变得更加复杂。为了解决这个问题，我们提出了一个顺序框架，可以逐步实现对一组敏感特征的公平性。我们通过利用多边际Wasserstein重心来实现这一点，将标准的强人口平等概念扩展到具有多个敏感特征的情况。这种方法还为最优的顺序公平预测器提供了闭式解，可以清楚地解释敏感特征之间的相关性。我们的方法也可以无缝扩展到近似解决方案。

    In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approx
    
[^82]: 计算和通信高效的无线网络联合学习

    Computation and Communication Efficient Federated Learning over Wireless Networks. (arXiv:2309.01816v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01816](http://arxiv.org/abs/2309.01816)

    提出了一种计算和通信高效的无线网络联合学习框架，通过模型剪枝和个性化，在分布式学习中减少计算和通信延迟，并提高非独立同分布数据设备的学习准确度。

    

    联合学习（FL）能够在边缘设备上进行分布式学习，并保护数据隐私。然而，由于设备数据的异质性，学习准确度下降，在计算能力有限和无线资源有限的设备上更新大规模学习模型会增加计算和通信延迟。我们考虑了一种新颖的FL框架，其中包括模型剪枝和个性化，以克服这些挑战。该框架将学习模型分为全局部分和个性化部分，全局部分通过模型剪枝与所有设备共享以学习数据表示，个性化部分针对特定设备进行微调，在FL过程中调整模型大小以减少计算和通信延迟，并提高非独立同分布（non-IID）数据设备的学习准确度。然后，对所提出的FL框架的计算和通信延迟以及收敛分析进行了数学分析。

    Federated learning (FL) enables distributed learning across edge devices while protecting data privacy. However, the learning accuracy decreases due to the heterogeneity of devices' data, and the computation and communication latency increase when updating large-scale learning models on devices with limited computational capability and wireless resources. We consider a novel FL framework with partial model pruning and personalization to overcome these challenges. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine-tuned for a specific device, which adapts the model size during FL to reduce both computation and communication latency and increases the learning accuracy for the device with non-independent and identically distributed (non-IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically ana
    
[^83]: 学习品味：一个多模态葡萄酒数据集

    Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])

    [http://arxiv.org/abs/2308.16900](http://arxiv.org/abs/2308.16900)

    这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。

    

    我们提出了一个大型的多模态葡萄酒数据集WineSensed，用于研究视觉感知、语言和口感之间的关系。该数据集包含89.7万张葡萄酒标签图片和82.4万条来自Vivino平台的葡萄酒评论。该数据集具有超过35万个独特的年份，附带了年份、产地、评分、酒精含量、价格和葡萄组成的注释。我们通过一项品酒实验对部分数据进行了细粒度的口味注释，共有256名参与者被要求根据口味的相似性对葡萄酒进行排序，得到了超过5千个配对的口味距离。我们提出了一种低维概念嵌入算法，将人类经验与自动机器相似度核相结合。我们证明，这个共享的概念嵌入空间在粗粒度口味分类（酒精含量，国家，葡萄，价格，评分）上改进，并且与复杂的人类口味知觉相一致。

    We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
    
[^84]: 通过机器学习进行山区天气预报的插值研究

    Interpolation of mountain weather forecasts by machine learning. (arXiv:2308.13983v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.13983](http://arxiv.org/abs/2308.13983)

    本研究提出了一种通过机器学习来插值山区天气预报的方法，通过利用当前观测数据和周围平原的预报数据，解决了在复杂地形中数值模拟精度降低的问题，并研究了在降水预测中使用二元交叉熵的方法。

    

    最近基于物理模型的数值模拟方法的进展提高了天气预报的准确性。然而，在复杂地形如山地地区，由于数值模拟中使用了几公里平方的网格，精度会降低。虽然统计机器学习也取得了显著进展，但直接应用难以利用物理知识。本文提出了一种方法，利用当前观测数据和周围平原的预报数据，使用机器学习来“插值”未来山区的天气。通常，天气预测依赖于数值模拟，因此这种方法可以被视为间接融合数值模拟和机器学习的混合方法。还研究了在降水预测中使用二元交叉熵的方法。

    Recent advancements in numerical simulation methods based on physical models have enhanced the accuracy of weather forecasts. However, the precision diminishes in complex terrains like mountainous regions due to the several kilometers square grid used in numerical simulations. While statistical machine learning has also significantly advanced, its direct application is difficult to utilize physics knowledge. This paper proposes a method that employs machine learning to ``interpolate'' future weather in mountainous regions using current observed data and forecast data from surrounding plains. Generally, weather prediction relies on numerical simulations, so this approach can be considered a hybrid method that indirectly merges numerical simulation and machine learning. The use of binary cross-entropy in precipitation prediction is also examined.
    
[^85]: FoX:多智能体强化学习中的形成感知探索

    FoX: Formation-aware exploration in multi-agent reinforcement learning. (arXiv:2308.11272v1 [cs.LG])

    [http://arxiv.org/abs/2308.11272](http://arxiv.org/abs/2308.11272)

    FoX是一个新颖的多智能体强化学习框架，通过减少探索空间，并引导智能体在不同形成中访问有意义的状态，显著提高了在合作多智能体任务中的性能。

    

    最近，基于深度的多智能体强化学习 (MARL) 在各种合作多智能体任务中取得了显著的成功。然而，由于智能体的部分可观测性和随着智能体数量增加而指数增长的探索空间，探索仍然是MARL中的一个具有挑战性的问题。首先，为了解决探索空间的可扩展性问题，我们在探索空间上定义了一个基于形成的等价关系，并旨在通过仅探索不同形式的有意义状态来减少搜索空间。然后，我们提出了一种新颖的形成感知探索 (FoX) 框架，通过引导部分可观测智能体凭借自身观测信息充分了解其当前形成，鼓励他们访问不同形成中的状态。数值结果表明，所提出的FoX框架在Google Research Football上明显优于现有的MARL算法。

    Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football
    
[^86]: 基于策略梯度的离散提示生成用于少样本学习的对话式方法

    Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning. (arXiv:2308.07272v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.07272](http://arxiv.org/abs/2308.07272)

    本文介绍了一种基于对话的策略梯度离散提示优化方法，通过设计多轮对话对齐策略和高效的提示筛选度量，实现了在少样本学习任务中生成高质量提示集的目标。

    

    在少样本自然语言处理任务中，基于提示的预训练语言模型(PLMs)范式取得了显著的成功。然而，现有的离散提示优化方法需要专业知识来设计基本提示集并识别高质量的提示，这既费时又低效，而且主观性较强。同时，现有的连续提示优化方法通过学习PLMs的梯度信息来改进性能，但计算成本高、可读性和通用性低常常是问题。为了填补研究空白，本文提出了一种基于对话的策略梯度离散提示优化方法($DP_2O$)。首先，我们设计了一种基于GPT-4的多轮对话对齐策略，用于生成可读性提示集。此外，我们提出了一种高效的提示筛选度量，以线性复杂度识别高质量的提示。最后，我们构建了一个强化学习(RL)框架。

    Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework ba
    
[^87]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^88]: 在参数化源的情况下，使用深度神经操作器在真实的交互式3D场景中模拟声音传播

    Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators. (arXiv:2308.05141v1 [cs.SD])

    [http://arxiv.org/abs/2308.05141](http://arxiv.org/abs/2308.05141)

    本文提出了使用深度神经操作器网络来模拟在真实的3D场景中带有移动源的声音传播。通过学习一个紧凑的代理模型，能够快速预测声音传播，避免了计算和存储脉冲响应的离线计算。

    

    我们解决了在具有移动源的三维虚拟房间中进行声音传播模拟的挑战，这在虚拟/增强现实、游戏音频和空间计算方面具有应用。通过求解波动方程，可以描述衍射和干涉等波动现象。然而，使用传统的数值离散化方法模拟涉及数百个源和接收器位置的波动方程是不可行的，使得使用移动源刺激声场变得不切实际。为了解决这个限制，我们提出使用深度操作器网络来逼近线性波动方程操作器。这使得能够快速预测在具有移动源的真实三维声学场景中的声音传播，达到毫秒级的计算。通过学习紧凑的代理模型，我们避免了为所有相关的源/听者对计算和存储脉冲响应的离线计算。我们的实验证明，在包含各种复杂场景几何形状的情况下，与参考结果达成了良好的一致性。

    We address the challenge of sound propagation simulations in $3$D virtual rooms with moving sources, which have applications in virtual/augmented reality, game audio, and spatial computing. Solutions to the wave equation can describe wave phenomena such as diffraction and interference. However, simulating them using conventional numerical discretization methods with hundreds of source and receiver positions is intractable, making stimulating a sound field with moving sources impractical. To overcome this limitation, we propose using deep operator networks to approximate linear wave-equation operators. This enables the rapid prediction of sound propagation in realistic 3D acoustic scenes with moving sources, achieving millisecond-scale computations. By learning a compact surrogate model, we avoid the offline calculation and storage of impulse responses for all relevant source/listener pairs. Our experiments, including various complex scene geometries, show good agreement with reference 
    
[^89]: 通过大型语言模型实现通用模糊测试

    Universal Fuzzing via Large Language Models. (arXiv:2308.04748v1 [cs.SE])

    [http://arxiv.org/abs/2308.04748](http://arxiv.org/abs/2308.04748)

    本文介绍了Fuzz4All，这是第一个能够针对许多不同的输入语言和这些语言的许多不同功能进行模糊测试的通用工具。

    

    模糊测试在发现各种软件系统中的漏洞和脆弱性方面取得了巨大成功。接受编程或形式语言作为输入的测试系统（SUTs），如编译器，运行时引擎，约束求解器和具有可访问API的软件库，尤其重要，因为它们是软件开发的基本构建块。然而，针对这些系统的现有模糊测试工具通常针对特定语言，因此无法轻易应用于其他语言甚至同一语言的其他版本。此外，现有模糊测试工具生成的输入通常局限于输入语言的特定功能，因此很难揭示与其他功能相关的漏洞或新功能。本文提出了Fuzz4All，这是第一个通用的模糊测试工具，它可以针对许多不同的输入语言和这些语言的许多不同功能进行测试。Fuzz4All的关键思想是利用大型语言模型（LLMs）作为输入生成器。

    Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input genera
    
[^90]: 智能电网中一种有效的用于能量盗窃检测和预测的LSTM-DDPM方案

    An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])

    [http://arxiv.org/abs/2307.16149](http://arxiv.org/abs/2307.16149)

    这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。

    

    能量盗窃检测（ETD）和能量消耗预测（ECF）是智能电网系统中两个相互关联的挑战。共同解决这些问题对于确保系统安全至关重要。本论文解决了智能电网系统中的ETD和ECF的相互关联挑战。所提出的解决方案结合了长短期记忆（LSTM）和去噪扩散概率模型（DDPM），用于生成输入重构和预测。通过利用重构和预测误差，系统能够识别能量盗窃的实例，基于重构误差和预测误差的方法相互补充，可以检测不同类型的攻击。通过在真实和合成数据集上进行大量实验，所提出的方案在ETD和ECF问题上表现优于基准方法。集成方法显著提升了ETD性能，能够准确检测到基准方法未能检测到的能量盗窃攻击。该研究提供了一种可行的解决方案来解决智能电网系统中ETD和ECF的挑战。

    Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
    
[^91]: FedDRL: 一种基于分阶段强化学习的可信联邦学习模型融合方法

    FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])

    [http://arxiv.org/abs/2307.13716](http://arxiv.org/abs/2307.13716)

    FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。

    

    传统的联邦学习使用样本数量计算每个客户端模型的权重，并使用这个固定权重值来融合全局模型。然而，在实际场景中，每个客户端设备和数据的异质性导致每个客户端模型的质量存在差异。因此，对全局模型的贡献不仅仅取决于样本量。此外，如果客户端故意上传低质量或恶意模型，使用这些模型进行聚合将严重降低全局模型的准确性。传统的联邦学习算法没有解决这些问题。为了解决这个问题，我们提出了一种名为FedDRL的模型融合方法，它使用两个阶段的强化学习。在第一个阶段，我们的方法可以过滤掉恶意模型，并选择可信的客户端模型参与模型融合。在第二个阶段，FedDRL算法自适应地调整可信客户端模型的权重并聚合。

    Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
    
[^92]: QuIP：具有保证的大型语言模型的2比特量化

    QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])

    [http://arxiv.org/abs/2307.13304](http://arxiv.org/abs/2307.13304)

    本文提出了一种新的基于无关处理的大型语言模型（LLMs）参数量化方法QuIP，通过使权重和Hessian矩阵与坐标轴不对齐，实现了准确的量化结果。经过经验实验，我们发现我们的方法改善了现有的量化算法，并且首次在仅使用两比特的情况下获得了可行的LLM量化结果。

    

    本研究探讨了大型语言模型（LLMs）中的训练后参数量化。我们介绍了一种新的基于无关处理（QuIP）的量化方法，该方法基于以下见解：量化从不相关的权重和 Hessian 矩阵中收益，即通过准确地将它们舍入为与坐标轴不对齐的方向，使得获取重要的量化结果。QuIP 包含两个步骤：（1）最小化二次近似目标的自适应舍入过程；（2）通过与随机正交矩阵相乘来确保权重和 Hessian 无关的高效预处理和后处理。我们通过第一次针对 LLM 规模的量化算法进行了理论分析，并且证明我们的理论也适用于现有方法 OPTQ。经验证实，我们的无关预处理改善了现有的多个量化算法，并首次实现了仅使用每个权重2比特的大型语言模型量化方法。

    This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.
    
[^93]: 使用相对位置标签将异构图与实体感知自注意力相结合的阅读理解模型

    Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])

    [http://arxiv.org/abs/2307.10443](http://arxiv.org/abs/2307.10443)

    本文提出了一种新的注意力模式，使用图增强自注意力机制将从异构图中导出的推理知识整合到变压器架构中，从而克服了变压器模型在复杂推理任务中的限制。通过全局-局部注意力、图注意力和关系类型考虑，优化了实体和单词之间的注意力。该模式与相对位置标签相结合，能够与LUKE的实体感知自注意力机制相集成。

    

    尽管变压器模型在机器阅读理解任务中取得了重大进展，但由于输入序列中缺少显式知识，它们仍然面临处理复杂推理任务的限制。本文提出了一种新颖的注意力模式来克服这个限制，它利用增强图自注意力机制将由异构图导出的推理知识整合到变压器架构中。提出的注意力模式包括三个关键要素：单词标记的全局-局部注意力，对实体标记的图注意力，实体标记对相关联的标记显示强烈的注意力而对不相关的标记显示较弱的注意力，以及考虑每个实体标记与单词标记之间的关系类型。这样，如果存在关系，则可以优化两者之间的注意力。该模式与特殊的相对位置标签相结合，使其能够与LUKE的实体感知自注意力机制相集成。

    Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
    
[^94]: 线性二次调节器的加速优化景观

    Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v1 [math.OC])

    [http://arxiv.org/abs/2307.03590](http://arxiv.org/abs/2307.03590)

    本文介绍了用于处理线性二次调节器（LQR）问题的加速优化框架，分析了SLQR和OLQR问题的收敛性。同时提出了LQR性能准则的Lipschitz Hessian特性，为现代优化技术的应用提供了重要指导。

    

    线性二次调节器（LQR）是最优控制领域的一个重要问题。本文介绍了处理LQR问题的一阶加速优化框架，并分别给出了SLQR和OLQR的收敛性分析。我们提出了LQR性能准则的Lipschitz Hessian特性，这对于应用现代优化技术来说是至关重要的。对于SLQR问题，我们引入了一个连续时间混合动态系统，并证明其解轨迹指数级收敛。

    Linear-quadratic regulator (LQR) is a landmark problem in the field of optimal control, which is the concern of this paper. Generally, LQR is classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based on whether the full state is obtained. It has been suggested in existing literature that both the SLQR and the OLQR could be viewed as \textit{constrained nonconvex matrix optimization} problems in which the only variable to be optimized is the feedback gain matrix. In this paper, we introduce a first-order accelerated optimization framework of handling the LQR problem, and give its convergence analysis for the cases of SLQR and OLQR, respectively.  Specifically, a Lipschiz Hessian property of LQR performance criterion is presented, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, a continuous-time hybrid dynamic system is introduced, whose solution trajectory is shown to converge exponentially to the
    
[^95]: 探索大规模语言模型（LLMs）在图学习中的潜力

    Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])

    [http://arxiv.org/abs/2307.03393](http://arxiv.org/abs/2307.03393)

    本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。

    

    图学习因其广泛的现实世界应用而引起了极大的关注。以文本节点属性为主的图学习最流行的流程主要依赖于图神经网络（GNN），并利用浅层文本嵌入作为初始节点表示，但存在通用知识和深刻语义理解方面的限制。近年来，大规模语言模型（LLMs）被证明具有广泛的常识和强大的语义理解能力，已经颠覆了现有的处理文本数据的工作流程。在本文中，我们旨在探索LLMs在图机器学习中的潜力，特别是节点分类任务，并研究两种可能的流程：LLMs作为增强器和LLMs作为预测器。前者利用LLMs通过其海量知识增强节点的文本属性，然后通过GNNs生成预测。后者试图直接使用LLMs作为独立的预测器。

    Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
    
[^96]: RanPAC: 随机投影和预训练模型在连续学习中的应用

    RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02251](http://arxiv.org/abs/2307.02251)

    这项研究提出了一种利用预训练模型进行连续学习的方法，通过使用随机投影器和类原型累积来避免遗忘问题。

    

    连续学习旨在在非稳态数据流中增量学习不同的任务（如分类），而不会忘记旧的任务。大多数连续学习方法都致力于从头开始学习的灾难性遗忘问题。然而，随着基础模型的日益重要，在各种下游需求中都可以利用具有信息丰富表示的预训练模型。已经探索了一些基于预训练模型的连续学习方法，要么直接利用预提取的特征（这使得弥合分布差距变得困难），要么融入适配器（这可能会导致遗忘问题）。在本文中，我们提出了一种简洁而有效的预训练模型连续学习方法。鉴于遗忘问题发生在参数更新期间，我们考虑了一种替代方法，利用无需进行训练的随机投影器和类原型累积来避免这个问题。具体而言，我们注入了一个固定的...

    Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Rando
    
[^97]: CardiGraphormer: 揭示自监督学习在颠覆药物发现中的力量

    CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])

    [http://arxiv.org/abs/2307.00859](http://arxiv.org/abs/2307.00859)

    CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。

    

    在广阔的药物发现领域中，已知药物约有15,000种，但只有大约4,200种得到了批准，化学空间的组合性质提供了一项艰巨的挑战。尽管人工智能成为了有力的伙伴，传统的人工智能框架仍面临重大障碍。本文介绍了CardiGraphormer，这是一种划时代的方法，通过结合自监督学习（SSL）、图神经网络（GNN）和保持基数注意力，从而颠覆药物发现。CardiGraphormer是Graphormer和保持基数注意力的新颖组合，利用SSL学习有效的分子表示，并利用GNN提取分子指纹，提高了预测性能和可解释性，并减少了计算时间。它在处理分子结构等复杂数据方面表现出色，并能执行与节点、节点对、子图或整个图结构相关的任务。

    In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
    
[^98]: ENN: 一种具有DCT自适应激活函数的神经网络

    ENN: A Neural Network with DCT Adaptive Activation Functions. (arXiv:2307.00673v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2307.00673](http://arxiv.org/abs/2307.00673)

    ENN是一种具有DCT自适应激活函数的神经网络模型，通过使用反向传播自适应地调整激活函数，提供了高度的灵活性和表现力。在解释网络收敛过程中，我们恢复了每个激活函数在输出空间中的响应，即“凸起”。通过实验证明了该模型在多个任务上的性能优势。

    

    神经网络的表达能力高度取决于激活函数的性质，尽管这些通常在训练阶段被假定为预定义和固定的。在信号处理的视角下，本文提出了一种新颖的模型——表达神经网络(ENN)，其中非线性激活函数使用离散余弦变换(DCT)进行建模，并且在训练过程中使用反向传播进行自适应。这种参数化方法将可训练参数的数量保持较低，适合基于梯度的方案，并能适应不同的学习任务。这是第一个在激活函数方面依赖于信号处理视角的非线性模型，为网络提供了高度的灵活性和表现力。我们通过恢复“凸起”的概念来为网络在收敛时的可解释性提供了新的见解，即每个激活函数在输出空间中的响应。最后，通过详尽的实验，我们展示了该模型在多个任务上的性能优势。

    The expressiveness of neural networks highly depends on the nature of the activation function, although these are usually assumed predefined and fixed during the training stage. Under a signal processing perspective, in this paper we present Expressive Neural Network (ENN), a novel model in which the non-linear activation functions are modeled using the Discrete Cosine Transform (DCT) and adapted using backpropagation during training. This parametrization keeps the number of trainable parameters low, is appropriate for gradient-based schemes, and adapts to different learning tasks. This is the first non-linear model for activation functions that relies on a signal processing perspective, providing high flexibility and expressiveness to the network. We contribute with insights in the explainability of the network at convergence by recovering the concept of bump, this is, the response of each activation function in the output space. Finally, through exhaustive experiments we show that th
    
[^99]: iSCAN：识别非线性加性噪声模型中的因果机制转变

    iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])

    [http://arxiv.org/abs/2306.17361](http://arxiv.org/abs/2306.17361)

    本文提出了一种识别非线性加性噪声模型中因果机制转变的方法，该方法专注于在相关的结构因果模型中识别功能机制的变化，而不需要估计整个有向无环图(DAG)的结构。

    

    结构因果模型(SCM)被广泛应用于各个领域，以表示复杂系统中变量之间的因果关系。然而，真正的底层有向无环图(DAG)结构通常是未知的，并且从观测数据或干预数据中确定它仍然是一项具有挑战性的任务。然而，在许多情况下，目标是识别相关SCM之间的因果机制的变化(转变)而不是恢复整个底层DAG结构。例子包括分析健康和癌症患者之间的基因调控网络结构变化，或者在不同细胞环境下理解生物途径的变化。本文重点研究了在相同的变量集上识别两个或多个相关SCM中的$\textit{功能}$机制转变，而不需要估计每个SCM的整个DAG结构。在这种设置下，先前的工作假设使用了具有高斯噪声的线性模型；而本文中我们则考虑了非线性加性噪声模型。

    Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
    
[^100]: 高维数据流自适应伯恩斯坦变化检测器

    Adaptive Bernstein Change Detector for High-Dimensional Data Streams. (arXiv:2306.12974v1 [cs.LG])

    [http://arxiv.org/abs/2306.12974](http://arxiv.org/abs/2306.12974)

    本文提出了一个适用于高维数据流的自适应伯恩斯坦变化检测器，具有准确地识别变化发生的时间与子空间，并能够量化严重程度的特性。

    

    当分析数据流时，变化检测至关重要。快速和准确地检测到变化可以使监测和预测系统做出反应，例如发出警报或更新学习算法。然而，在高维数据中检测变化是具有挑战性的。在高维数据中，变化检测器不仅应能够识别变化发生的时间，还应能够识别发生在哪个子空间中，并且最好还应能量化它们的严重程度。我们提出的ABCd方法具备这些特性。ABCD学习一个编码器-解码器模型，并监测其在自适应大小的窗口内的准确性。ABCD根据伯恩斯坦不等式计算变化得分，以检测准确性方面的偏差，指示变化。我们的实验表明，ABCD在平均F1-score中至少比其最佳竞争对手表现优越了8％，最多可提高23％。它还可以准确地估计变化的子空间，以及与变化大小相关的严重程度指标。

    Change detection is of fundamental importance when analyzing data streams. Detecting changes both quickly and accurately enables monitoring and prediction systems to react, e.g., by issuing an alarm or by updating a learning algorithm. However, detecting changes is challenging when observations are high-dimensional. In high-dimensional data, change detectors should not only be able to identify when changes happen, but also in which subspace they occur. Ideally, one should also quantify how severe they are. Our approach, ABCD, has these properties. ABCD learns an encoder-decoder model and monitors its accuracy over a window of adaptive size. ABCD derives a change score based on Bernstein's inequality to detect deviations in terms of accuracy, which indicate changes. Our experiments demonstrate that ABCD outperforms its best competitor by at least 8% and up to 23% in F1-score on average. It can also accurately estimate changes' subspace, together with a severity measure that correlates w
    
[^101]: 使用随机梯度下降从高斯过程后验中采样

    Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent. (arXiv:2306.11589v1 [cs.LG])

    [http://arxiv.org/abs/2306.11589](http://arxiv.org/abs/2306.11589)

    本文探索了使用随机梯度下降算法从高斯过程后验中采样的方法，该方法计算高效且能在足够覆盖数据的区域和足够远离数据的区域中产生接近真实后验的预测分布。

    

    高斯过程是用于量化不确定性和顺序决策的强大框架，但其需要求解线性系统，每当数据集大小增加时代价是立方级别的且对条件敏感。本文探索了随机梯度算法作为一种计算高效的方法来近似解决这些线性系统：我们开发了低方差的最优化目标以从后验中进行采样，并将其扩展到引入点。令人意想不到的是，即使在不快速收敛到最优解的情况下，随机梯度下降通常也会产生准确的预测。我们通过非收敛的隐式偏置的谱特征来解释这一点。我们表明，随机梯度下降会在足够覆盖数据的区域和足够远离数据的区域中产生接近真实后验的预测分布。在实验中，随机梯度下降实现了

    Gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. In general, this has a cubic cost in dataset size and is sensitive to conditioning. We explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. Counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. We explain this through a spectral characterization of the implicit bias from non-convergence. We show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. Experimentally, stochastic gradient descent achieves sta
    
[^102]: 针对非对数凹分布的MCMC算法快速条件混合

    Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions. (arXiv:2306.10506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10506](http://arxiv.org/abs/2306.10506)

    本论文研究了针对非对数凹分布的MCMC算法的条件混合问题，通过分析Poincar\'e型不等式在状态空间子集上的成立情况，发现条件分布的混合速度快于全局混合速度，对于方差混合模型的采样、参数估计以及具有良好连接的局部极小值的吉布斯采样等问题具有重要影响。

    

    MCMC算法为从目标分布$\pi(x) \propto \exp(-V(x))$中采样提供了经验高效的工具。然而，在理论方面，当$\pi(x)$是非对数凹时，MCMC算法的混合速度较慢。我们的工作研究了这一差距，并表明当Poincar\'e型不等式成立时，状态空间子集$\mathcal{X}$上的MCMC迭代的条件分布混合快速收敛到真实的条件分布。这种快速混合保证可以在全局混合已被证明较慢的情况下成立。我们将该声明形式化并量化了条件混合率。此外，我们还展示了条件混合对于从高斯混合物中采样、高斯混合模型的参数估计以及具有良好连接的局部极小值的吉布斯采样会产生有趣的影响。

    MCMC algorithms offer empirically efficient tools for sampling from a target distribution $\pi(x) \propto \exp(-V(x))$. However, on the theory side, MCMC algorithms suffer from slow mixing rate when $\pi(x)$ is non-log-concave. Our work examines this gap and shows that when Poincar\'e-style inequality holds on a subset $\mathcal{X}$ of the state space, the conditional distribution of MCMC iterates over $\mathcal{X}$ mixes fast to the true conditional distribution. This fast mixing guarantee can hold in cases when global mixing is provably slow. We formalize the statement and quantify the conditional mixing rate. We further show that conditional mixing can have interesting implications for sampling from mixtures of Gaussians, parameter estimation for Gaussian mixture models and Gibbs-sampling with well-connected local minima.
    
[^103]: 一种高效的私有连续观测平滑二元机制

    A Smooth Binary Mechanism for Efficient Private Continual Observation. (arXiv:2306.09666v1 [cs.LG])

    [http://arxiv.org/abs/2306.09666](http://arxiv.org/abs/2306.09666)

    本文提出了一种可微且凸的替代二进制机制的方法，用于解决隐私问题中发布私有前缀求和题的效率问题。该方法比标志性的二进制机制具有更低的方差和更好的观测时间保证，并且在计算和空间要求方面更加有效。

    

    在不断观测的隐私问题中，我们研究如何基于随时间演变的数据集发布差分隐私估计。发布私有前缀求和$x_1,x_2,x_3,\dots\in\{0,1\}$问题是一个特别好研究的问题，并且在私有随机梯度下降的最新方法中使用了一个广义形式。标志性的二进制机制自动加入的噪声的方差是多对数的。最近，Henzinger et al.和Denisov et al.表示，可以通过两种方式提高二进制机制：噪声的方差可以减小一个较大的常数因子，并且在时间步长内也可以更加平均。但是，他们生成噪声分布的算法在计算时间和（特别是）空间方面都不如人意。我们通过提供一种可微且凸的简单替代二进制机制的方法来解决效率问题，从而使我们能够相对于添加的噪声量对机制进行优化。我们的机制比标志性的二进制机制具有更低的方差和更好的观测时间保证，并且在计算和空间要求方面比以前的改进更有效。

    In privacy under continual observation we study how to release differentially private estimates based on a dataset that evolves over time. The problem of releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the value of each $x_i$ is to be private) is particularly well-studied, and a generalized form is used in state-of-the-art methods for private stochastic gradient descent (SGD). The seminal binary mechanism privately releases the first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently, Henzinger et al. and Denisov et al. showed that it is possible to improve on the binary mechanism in two ways: The variance of the noise can be reduced by a (large) constant factor, and also made more even across time steps. However, their algorithms for generating the noise distribution are not as efficient as one would like in terms of computation time and (in particular) space. We address the efficiency problem by presenting a simple alternative to the binary
    
[^104]: 残差 Q 学习：无需价值的在线和离线策略定制

    Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])

    [http://arxiv.org/abs/2306.09526](http://arxiv.org/abs/2306.09526)

    该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。

    

    模仿学习是一种广泛使用的框架，适用于从演示中学习模仿行为。当手工制作奖励函数困难或目标是模仿人类专家行为时，这种方法特别有吸引力。但是，学习的模仿策略只能遵循演示中的行为。在应用模仿策略时，我们可能需要根据不同的下游任务要求定制策略行为。同时，我们仍希望定制的策略保持其模仿性质。为此，我们提出了一种新的问题设置，称为策略定制。它将学习任务定义为训练一种策略，该策略继承先前策略的特性，同时满足目标下游任务强加的一些附加要求。我们提出了一种新颖和有原则的方法来解释和确定两个任务目标之间的权衡。具体而言，我们制定了一种动态控制残差的 Q 学习方法，该方法可以在不使用价值函数的情况下进行在线和离线策略定制。

    Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
    
[^105]: AQuA：一种用于标签质量评估的基准测试工具

    AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v1 [cs.LG])

    [http://arxiv.org/abs/2306.09467](http://arxiv.org/abs/2306.09467)

    AQuA是一款用于标签质量评估的基准测试工具，目的是评估标签噪声存在下的机器学习方法。该基准测试环境包括数据模拟、质量不同的真实数据集和几种最先进的标签噪声消除方法。

    

    机器学习模型的好坏取决于用于训练它们的数据。然而，最近的研究发现，被广泛用于训练和评估机器学习模型的数据集，例如ImageNet，存在普遍的标注错误。训练集中的错误标签会削弱机器学习模型的泛化性能，并影响使用测试集进行模型选择和评估。因此，在存在标签误差的情况下学习是研究的一个活跃领域，但该领域缺乏一个全面的基准来评估这些方法。为此，我们提出了AQuA这个基准测试环境，以严格评估在标签噪声存在的情况下实现机器学习的方法。

    Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to del
    
[^106]: ShiftAddViT：多种乘法原语混合实现高效的视觉变换器

    ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06446](http://arxiv.org/abs/2306.06446)

    ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。

    

    视觉变换器（ViT）展示了令人印象深刻的性能，并成为多个视觉任务的统一骨干。但是，ViTs中的注意力和多层感知器（MLPs）由于密集的乘法而不够高效，导致训练和推理代价高昂。为此，我们提出了一种将预训练的ViT以多种乘法原语（例如位移和加法）重新参数化的方法，以实现全新类型的减少乘法的模型，称为ShiftAddViT，旨在实现GPU上的端到端推理加速，无需从头开始训练。具体而言，我们将查询和键映射为汉明空间中的二进制码之后，采用加法核对查询、键和值之间的MatMul进行重新参数化。剩余的MLPs或线性层则采用位移核进行重新参数化。我们利用TVM在GPU上实施并优化这些定制核，以实现实际硬件部署。我们发现，这种重新参数化方法可以显著提高推理速度，而无需从头开始训练。

    Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
    
[^107]: 巨型语言模型在意大利生物医学信息提取方面的应用：方法论研究和实际应用的多中心实践

    Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])

    [http://arxiv.org/abs/2306.05323](http://arxiv.org/abs/2306.05323)

    该研究创建了意大利神经精神命名实体识别数据集，并使用巨型语言模型开发出多中心识别模型，整体 F1得分为84.77%。该模型将帮助临床从业者从非结构化的医疗记录中自动提取信息。

    

    医院引入计算机化医疗记录有助于减少手写和信息提取等繁琐操作。然而，由于从非结构化文本医疗记录中提取数据需要时间和精力，因此医疗记录中包含的数据仍然被充分利用程度低。自然语言处理的子领域信息提取可以帮助临床从业者克服这一限制，使用自动化文本挖掘流程。在这项工作中，我们创建了意大利神经精神命名实体识别数据集 PsyNIT，并使用它来开发这一任务的巨型语言模型。此外，我们还进行了多个实验，使用三个外部独立数据集来实现有效的多中心模型，整体 F1 得分为 84.77%，精确率为 83.16%，召回率为 86.44%。我们学到的经验是: (i) 一致的注释过程的关键作用和 (ii) 结合经典方法和“少量训练”的 fine-tuning 策略。

    The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
    
[^108]: 使用大型语言模型注释进行社会科学中的有效下游统计推断: 基于设计的半监督学习

    Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])

    [http://arxiv.org/abs/2306.04746](http://arxiv.org/abs/2306.04746)

    该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。

    

    在计算社会科学（CSS）中，研究人员通过分析文档来解释社会和政治现象。在大多数情况下，CSS研究人员首先获取文档的标签，然后使用可解释的回归分析来解释标签。大型语言模型（LLMs）的最近进展可以通过在规模上便宜地注释文档来降低CSS研究成本，但这些替代标签通常是不完美和有偏的。我们提出了一种新算法，用于使用LLMs的输出进行下游统计分析，同时保证与CSS研究基本相关的统计属性-如渐近无偏性和正确的不确定性量化。我们表明，直接在下游统计分析中使用LLM预测的替代标签会导致实质性偏差和无效置信区间，即使替代准确性高达80-90％。为了解决这个问题，我们基于无偏机器学习提出了基于设计的半监督学习（D-SSL）算法，该算法将LLM注释与有针对性的采样相结合，以实现有效的下游统计推断。我们的方法可以将标签获取的CSS研究成本降低80％，而不影响统计分析的有效性。模拟研究和实际数据示例表明，与直接使用LLM预测标签相比，D-SSL可以将回归估计的准确性提高多达40％。

    In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
    
[^109]: 通过重新思考民间威斯费勒-莱曼算法，实现$O(n^2)$空间内任意表达能力的GNNs

    Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])

    [http://arxiv.org/abs/2306.03266](http://arxiv.org/abs/2306.03266)

    本文提出了$(k, t)$-FWL和$k$-FWL+两种方法，理论上证明了它们可以在$O(n^2)$的空间复杂度下，解决图同构问题。

    

    近年来，消息传递神经网络（MPNNs）已成为图神经网络（GNNs）中最受欢迎的框架。然而，其表达能力受到一维威斯费勒-莱曼（1-WL）测试的限制。一些研究受到$k$-WL/FWL（民间WL）的启发并设计其相应的神经版本。尽管具有很高的表达能力，但这一研究方向存在严重局限性。为解决这些问题，作者提出了$(k, t)$-FWL和$k$-FWL+，并在理论上证明了它们的有效性。

    Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
    
[^110]: 在零样本强化学习中探索泛化

    Explore to Generalize in Zero-Shot RL. (arXiv:2306.03072v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03072](http://arxiv.org/abs/2306.03072)

    本文针对零样本强化学习中的泛化问题进行了研究，通过学习一种有效探索领域的策略，成功解决了在像ProcGen Maze这样的问题上基于不变性的方法失败的困境。

    

    本研究探讨了在强化学习中的零样本泛化问题，即在一组训练任务上优化策略以在类似但未见过的测试任务上表现良好。为了缓解过拟合问题，先前的工作探索了不同的任务不变性概念。然而，在像ProcGen Maze这样的问题上，不存在一种与任务可视化无关的适当解决方案，因此基于不变性的方法失败了。我们的洞察力是，学习一种有效地探索领域的策略比学习一种在特定任务中最大化奖励的策略更难被记忆，因此我们期望这种学习到的行为能够良好泛化；我们在几个对于基于不变性方法来说困难的领域上通过实证验证了这一点。我们的“探索泛化”算法（ExpGen）基于这一观点：我们训练了一个额外的代理集合来优化奖励。在测试时，如果代理集合对一个动作达成一致，我们将能够良好泛化，否则我们将采取其他策略。

    We study zero-shot generalization in reinforcement learning-optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively $\textit{explores}$ the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariance-based approaches. Our $\textit{Explore to Generalize}$ algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we tak
    
[^111]: 用非约束的未标记数据扩展半监督学习的规模

    Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v1 [cs.LG])

    [http://arxiv.org/abs/2306.01222](http://arxiv.org/abs/2306.01222)

    本论文提出UnMixMatch框架，可以从非约束的未标记数据中学习有效的表征以提高性能，以解决大多数半监督方法基于有标记和未标记样本来自同一分布的假设。该框架具有强正则化作用的硬增强监督学习器、用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。

    

    本文提出了一种半监督学习框架UnMixMatch，可以从非约束的未标记数据中学习有效的表征以提高性能。大多数现有的半监督方法基于有标记和未标记样本来自同一分布的假设，这限制了通过使用自由生活的未标记数据进行改进的潜力。因此，该假设经常限制半监督学习的可推广性和可扩展性，本文旨在克服这些限制并有效利用非约束的未标记数据进行半监督学习。UnMixMatch包括三个主要组成部分：具有强正则化作用的硬增强监督学习器，用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。我们进行了大量实验来证明UnMixMatch的有效性。

    We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experim
    
[^112]: 具有平均奖励的不安定赌徒问题：打破统一全局引子假设

    Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])

    [http://arxiv.org/abs/2306.00196](http://arxiv.org/abs/2306.00196)

    本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。

    

    我们研究了具有平均奖励标准下的无限时不安定赌徒问题，包括离散时间和连续时间设置。一个基本问题是如何设计计算有效的策略，使得优化差距随着臂的数量$N$的增加而减小。现有的渐近最优性结果都依赖于统一全局引子性质(UGAP)，这是一个复杂且难以验证的假设。在本文中，我们提出了一个通用的、基于模拟的框架，将任何单臂策略转化为原始的$N$臂问题的策略。这是通过在每个臂上模拟单臂策略，并仔细地将真实状态引导向模拟状态来实现的。我们的框架可以实例化，产生一个具有$O(1/\sqrt{N})$的最优解差距的策略。在离散时间设置中，我们的结果在更简单的同步假设下成立，涵盖了一些不满足UGAP的问题实例。更值得注意的是，我们的框架可以处理比现有方法更大的问题类，而不需对问题实例做任何特定的结构假设。

    We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
    
[^113]: 学习解决贝叶斯逆问题：一种摊销变分推理方法

    Learning to solve Bayesian inverse problems: An amortized variational inference approach. (arXiv:2305.20004v1 [stat.ML])

    [http://arxiv.org/abs/2305.20004](http://arxiv.org/abs/2305.20004)

    本文提出了一种基于深度神经网络的参数化表示后验分布的摊销变分推理方法，以实现实时推理的目的，可应用于流体力学中的参数估计和流场重构等领域。

    

    逆问题，即从实验数据中估计物理模型的参数，在科学和工程中普遍存在。贝叶斯公式是黄金标准，因为它可以缓解病态性问题并量化认识不确定性。然而，由于解析后验不通常可用，人们采用马尔可夫链蒙特卡罗采样或近似变分推理。但是，需要重新从头开始进行推理以适应每组新数据。这种缺点限制了贝叶斯公式在实时设置，例如工程系统的健康监测和医疗诊断中的适用性。本文的目标是开发一种方法，通过学习从数据到后验的贝叶斯逆映射，即从数据到后验的映射，实现实时推理的方法。我们的方法如下。我们使用基于深度神经网络的参数化表示后验分布。接下来，我们通过摊销变分推理学习网络参数，其中训练神经网络以预测从数据中预测后验分布，从而实现快速准确的推理。我们在流体力学中的一些逆问题上展示了我们方法的有效性，其中包括参数估计和流场重构。

    Inverse problems, i.e., estimating parameters of physical models from experimental data, are ubiquitous in science and engineering. The Bayesian formulation is the gold standard because it alleviates ill-posedness issues and quantifies epistemic uncertainty. Since analytical posteriors are not typically available, one resorts to Markov chain Monte Carlo sampling or approximate variational inference. However, inference needs to be rerun from scratch for each new set of data. This drawback limits the applicability of the Bayesian formulation to real-time settings, e.g., health monitoring of engineered systems, and medical diagnosis. The objective of this paper is to develop a methodology that enables real-time inference by learning the Bayesian inverse map, i.e., the map from data to posteriors. Our approach is as follows. We represent the posterior distribution using a parameterization based on deep neural networks. Next, we learn the network parameters by amortized variational inferenc
    
[^114]: 可分目标的最优决策树：推动动态规划的极限

    Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])

    [http://arxiv.org/abs/2305.19706](http://arxiv.org/abs/2305.19706)

    本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。

    

    决策树的全局优化在准确性，大小和人类可理解性方面表现出良好的前景。然而，许多方法仍然依赖于通用求解器，可扩展性仍然是一个问题。动态规划方法已被证明具有更好的可扩展性，因为它们通过将子树作为独立的子问题解决来利用树结构。然而，这仅适用于可以分别优化子树的任务。我们详细研究了这种关系，并展示了实现这种可分离约束和目标任意组合的动态规划方法。在四个应用领域的实验表明了这种方法的普适性，同时也比通用求解器具有更好的可扩展性。

    Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
    
[^115]: Diff-Instruct: 一种从预训练扩散模型中传递知识的通用方法

    Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])

    [http://arxiv.org/abs/2305.18455](http://arxiv.org/abs/2305.18455)

    本文提出了一种通用框架 Diff-Instruct，能够以无需数据方式将预训练扩散模型中的知识传递给其他生成模型，仅需预训练 DM 和一个数据集。该框架是建立在严谨的数学基础上的，指导过程直接对应于最小化一种新型散度——Integral Kullback-Leibler (IKL) 散度。我们的方法在半监督学习、图像合成和视频预测中展示了其优越性。

    

    由于训练容易、可扩展性和样本质量高，扩散模型 (DMs) 已成为生成建模的首选方法，并有大量已预训练的模型适用于各种数据集。预训练 DMs 包含有关数据分布的复杂信息，对下游应用非常有价值。本文考虑从预训练 DMs 中学习并以无需数据方式将其知识传递给其他生成模型。具体而言，我们提出了一个通用框架 Diff-Instruct，该框架能指导任何生成模型的训练，只要生成的样本在模型参数方面是可微的。我们的 Diff-Instruct 建立在一个严谨的数学基础上，其中指导过程直接对应于最小化称为积分Kullback-Leibler (IKL) 散度的新型散度。IKL 是针对 DMs 定制的，通过计算沿扩散轨迹的 KL 散度的积分来捕获扩散过程信息，因此只需要一个预训练 DM 和一个数据集。我们通过在三个不同的应用程序：半监督学习、图像合成和视频预测中展示其优越性来证明了我们方法的有效性。

    Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffu
    
[^116]: 可视化编程中神经任务合成

    Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])

    [http://arxiv.org/abs/2305.18342](http://arxiv.org/abs/2305.18342)

    该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。

    

    通过合成新的内容，生成式神经模型在增强编程教育方面具有巨大的潜力。我们旨在设计神经模型，能够根据可视化编程环境下给定的规范自动生成编程任务。尽管近年来像 GPT-4 这样的大型生成模型获得了成功，但我们的初步结果显示，这些模型在合成可视化编程任务方面效果不佳，并且在逻辑和空间推理方面存在困难。我们提出了一种新颖的神经符号技术 NeurTaskSyn，该技术能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，合成编程任务。NeurTaskSyn 由两个部分构成：第一个部分通过模仿学习程序进行训练，生成可能的解决方案代码，第二个部分通过强化学习程序进行训练，指导底层符号执行引擎生成可视化任务。

    Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
    
[^117]: Translatotron 3: 使用单语数据进行语音到语音翻译

    Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17547](http://arxiv.org/abs/2305.17547)

    Translatotron 3提出了一种新方法，使用单语数据进行语音到语音翻译，无需配对的数据或专业建模，展示了保留语言/非语言信息的能力。

    

    本文提出了Translatotron 3的新方法，通过遮蔽自编码器、无监督的嵌入映射和回译将直接语音到语音翻译模型从单声道语音-文本数据集中完全无监督地进行训练。在西班牙语和英语之间的语音到语音翻译任务中，实验结果表明，Translatotron 3优于基准级联系统，在 synthesized Unpaired-Conversational 数据集上报告了18.14 BLEU分数的提高。与需要真实配对数据或专业建模来复制语言/非语言信息的监督方法不同，Translatotron 3展示了它保留了像暂停、说话速度和说话人身份等语言/非语言信息的能力。

    This paper presents Translatotron 3, a novel approach to train a direct speech-to-speech translation model from monolingual speech-text datasets only in a fully unsupervised manner. Translatotron 3 combines masked autoencoder, unsupervised embedding mapping, and back-translation to achieve this goal. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, which is unavailable, or specialized modeling to replicate para-/non-linguistic information, Translatotron 3 showcases its capability to retain para-/non-linguistic such as pauses, speaking rates, and speaker identity. Audio samples can be found in our website this http URL
    
[^118]: 学习与勾结在多单位拍卖中的应用

    Learning and Collusion in Multi-unit Auctions. (arXiv:2305.17402v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2305.17402](http://arxiv.org/abs/2305.17402)

    本论文研究了在多单位拍卖中学习和勾结的问题，分析了拍卖的离线和在线性质，提出了一个多项式时间算法来解决离线设置中的玩家出价最大化问题。

    

    我们考虑重复的多单位拍卖与统一定价，这种拍卖在实践中被广泛用于分配诸如碳排放许可证之类的商品。在每一轮中，$K$个相同的商品单元被卖给一组具有递减边际回报价值的买家。买家为商品单位提交出价，然后根据出价确定一个单位的价格$p$，使得所有单位都能被售出。我们考虑拍卖的两个变体，其中价格分别设为第$K$高出价和第$(K+1)$高出价。我们在离线和在线设置中分析了拍卖的性质。在离线设置中，我们考虑一个玩家$i$面临的问题：在过去的拍卖中，给定包含竞争对手提交的出价的数据集，找到一个出价向量，使得玩家$i$在该数据集上的累积效用最大化。我们设计了一个多项式时间算法来解决这个问题，通过将其等价于在仔细构造的有向图上找到最大权重路径。

    We consider repeated multi-unit auctions with uniform pricing, which are widely used in practice for allocating goods such as carbon licenses. In each round, $K$ identical units of a good are sold to a group of buyers that have valuations with diminishing marginal returns. The buyers submit bids for the units, and then a price $p$ is set per unit so that all the units are sold. We consider two variants of the auction, where the price is set to the $K$-th highest bid and $(K+1)$-st highest bid, respectively.  We analyze the properties of this auction in both the offline and online settings. In the offline setting, we consider the problem that one player $i$ is facing: given access to a data set that contains the bids submitted by competitors in past auctions, find a bid vector that maximizes player $i$'s cumulative utility on the data set. We design a polynomial time algorithm for this problem, by showing it is equivalent to finding a maximum-weight path on a carefully constructed direc
    
[^119]: 未知个性化操纵下的战略分类

    Strategic Classification under Unknown Personalized Manipulation. (arXiv:2305.16501v1 [cs.LG])

    [http://arxiv.org/abs/2305.16501](http://arxiv.org/abs/2305.16501)

    探讨了在未知且个性化操纵影响下的战略分类问题，提出了一个交互模型，引入了个性化的定义，旨在解决强化学习中策略操纵问题。

    

    我们研究了在战略分类中的基础错误界限和样本复杂度，其中代理可以在一定程度上战略性地操纵其特征向量以预测为正。例如，给定一个确定大学录取的分类器，学生候选人可能会尝试选择更容易的课程来提高他们的GPA，重新参加SAT并更换学校，以尝试欺骗分类器。 在文献中，球操纵是一个广泛研究的操纵类别，代理可以在有界半径球内修改其特征向量。与大多数先前的工作不同，我们的工作认为操纵是个性化的，这意味着代理可以拥有不同水平的操纵能力（例如，球体操纵的变化半径），并且对学习者是未知的。我们在一个交互模型中形式化学习问题，其中学习者首先部署分类器，代理在其操纵集合内操纵特征向量以操作部署的分类器。

    We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. Ball manipulations are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work considers manipulations to be personalized, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and unknown to the learner.  We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classif
    
[^120]: DoWG展示：一种高效的通用无参数梯度下降方法

    DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])

    [http://arxiv.org/abs/2305.16284](http://arxiv.org/abs/2305.16284)

    本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。

    

    本文提出了一种新的易于实现的无参数梯度优化器：DoWG（Weighted Gradients的距离）。我们证明了该方法是高效的——在不调整任何参数的情况下，匹配优化凸优化中最优调的梯度下降的收敛速度，直到对数因子，并且是通用的——自动适应平滑和非平滑问题。与AdaGrad，Adam或DoG等流行算法计算平方梯度的运行平均值不同，DoWG保持运行平均值的一种新的基于距离的加权版本，这对于实现所需的性质至关重要。据我们所知，DoWG是第一个不需要回溯搜索过程的无参数，高效和通用算法。它还是第一个适应于平稳优化的无参数AdaGrad样式算法。为了补充我们的理论，我们还通过实验证明DoWG在稳定的边缘训练，并证明其在实践中的有效性。

    This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
    
[^121]: Koopman核回归

    Koopman Kernel Regression. (arXiv:2305.16215v1 [cs.LG])

    [http://arxiv.org/abs/2305.16215](http://arxiv.org/abs/2305.16215)

    提出了一种基于Koopman核的回归方法，用于预测非线性动力系统的时间演变。该方法在机器人操作，视频预测和交通预测等各种应用中均有优异表现，并具有可证明的学习理论保证。

    

    许多决策制定的机器学习方法，如强化学习，依赖于模拟器或预测模型来预测感兴趣的量的时间演变，例如智能体的状态或策略的奖励。这些复杂现象的预测通常由高度非线性的动力系统描述，使得它们在基于优化的决策制定中的使用具有挑战性。Koopman算子理论通过通过线性动态系统描述预测来解决这个问题。这使得系统分析和长期预测变得简单--只涉及矩阵乘法。然而，将其转化为线性系统通常是非平凡的和未知的，需要基于学习的方法。虽然存在各种方法，但它们通常缺乏关键的学习理论保证，因此所获得的模型在数据和维度增加时的行为通常不清楚。通过提出一种新颖的基于Koopman核的回归方法，我们解决了上述挑战，该方法直接从历史观察中学习到未来预测在Koopman算子空间中的映射。我们的方法享有可证明的学习理论保证，并在广泛的应用中与现有的最先进方法匹配（或优于），包括机器人操作，视频预测和交通预测。

    Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear dynamical systems. This makes system analysis and long-term predictions simple -- involving only matrix multiplications. However, the transformation to a linear system is generally non-trivial and unknown, requiring learning-based approaches. While there exists a variety of approaches, they usually lack crucial learning-theoretic guarantees, such that the behavior of the obtained models with increasing data and dimensionality is often unclear. We address the aforemention
    
[^122]: 如何通过概率电路将您的知识图嵌入转化为生成模型

    How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])

    [http://arxiv.org/abs/2305.15944](http://arxiv.org/abs/2305.15944)

    本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。

    

    一些成功的知识图嵌入模型可用作基于能量的模型，而这篇论文解释了这些模型的得分函数，将其重新解释成为电路形式--这是一种允许有效边际化的约束计算图。然后，我们设计了两个方法来获得有效的生成电路模型，其中一个方法是将其激活限制为非负数，另一个方法是将其输出平方。我们的解释不会影响到预测节点连边模型的性能，但电路框架使得MLE的精确学习、新三元组的有效抽样以及保证逻辑约束得以满足成为可能。此外，我们的模型在拥有数百万个实体的图上比原始的KGEs更具伸缩性。

    Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
    
[^123]: 不依赖于振幅的光 plethysmography (PPG) 机器学习方法：通过可见性图和迁移学习

    Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2305.14062](http://arxiv.org/abs/2305.14062)

    该论文提出了一种不依赖于振幅的光 plethysmography (PPG) 机器学习方法，通过可见性图和迁移学习实现了对心率和血管老化等生物特征的稳健估计和预测。

    

    光体积描记法 (PPG) 是使用光测量血液体积的变化的一种方法，是大多数可穿戴设备的特征。PPG信号能够提供对人体循环系统的洞察，并可用于提取各种生物特征，例如心率和血管老化。尽管已经提出了几种算法，但许多算法存在限制，包括过多地依赖人工校准、高信号质量要求和缺乏泛化能力。在本文中，我们引入了一种结合了图论和计算机视觉算法的PPG信号处理框架，该框架对振幅无关并且对仿射变换不变。它还需要最少的预处理，通过RGB通道融合信息，并在任务和数据集上展现了稳健的泛化能力。所提出的VGTL-net在血管老化预测方面实现了最先进的性能，并展示了稳健的估计能力。

    Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation
    
[^124]: ToolkenGPT：通过工具嵌入扩充冻结语言模型

    ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])

    [http://arxiv.org/abs/2305.11554](http://arxiv.org/abs/2305.11554)

    本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。

    

    将大型语言模型与外部工具结合起来解决复杂问题已成为一种有前途的方法。然而，传统方法需要用工具演示数据对LLM进行微调，既费时又受限于预定义的工具集。最近的上下文学习范例缓解了这些问题，但是有限的上下文长度只允许演示几次，导致对工具的理解不够充分。此外，当有大量工具可供选择时，上下文学习可能完全无法正常工作。在本文中，我们提出了一种$\textbf{ToolkenGPT}$的替代方法，将两种方法的优点结合起来。我们的方法将每个$\underline{工具}$表示为一个$\underline{token}$（$\textit{toolken}$），并为其学习一个嵌入，使得工具调用与生成常规单词标记的方式相同。一旦触发了toolken，LLM被提示完成工具执行所需的参数。ToolkenGPT提供了以下贡献：1）引入了toolken的概念，以扩充LLM与外部工具的交互，2）提出了一种新的学习范例，利用tool embeddings实现无缝交互，3）在各种下游任务上展示了我们方法的有效性。

    Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
    
[^125]: 从功能等价的角度看前馈神经网络的复杂性。

    Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])

    [http://arxiv.org/abs/2305.11417](http://arxiv.org/abs/2305.11417)

    本文从功能等价的角度出发研究前馈神经网络的复杂性，发现利用置换不变性的特性可以降低网络的复杂度，通过过参数化可以增加训练网络的容易程度，并对深度学习中的优化和泛化理解具有重要意义。

    

    本文通过考察功能等价的概念来研究前馈神经网络的复杂性，该概念表明不同的网络参数化可以导致相同的函数。我们利用置换不变性的特性为前馈神经网络类导出了一个新的覆盖数上界，发现利用该性质可以降低神经网络的复杂度。此外，基于参数空间的对称结构，我们证明适当的随机参数初始化策略可以增加优化收敛的概率。我们发现，过参数化的网络往往更容易训练，即增加神经网络的宽度会导致有效参数空间的体积趋近于零。本研究结果揭示了过参数化的新见解，并对深度学习中的泛化和优化理解具有重要意义。

    In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning
    
[^126]: 民主扩散语言模型

    Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])

    [http://arxiv.org/abs/2305.10818](http://arxiv.org/abs/2305.10818)

    本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。

    

    尽管扩散模型在自然语言处理中有潜在好处，但目前公开的实现、训练模型或可重现的训练程序并不存在。为解决这些挑战，我们提出了基于CDCD框架的民主扩散语言模型（DDLM）。我们提出了一种用C4数据集简化的DDLM训练流程，并对训练模型的行为进行了深入分析。此外，我们引入了一种用于速度更快的采样的新型早期退出策略，该策略针对使用得分插值训练的模型。由于此前没有研究旨在使用预训练扩散LM解决下游任务（例如分类任务），我们在GLUE基准上进行了实验，以研究DDLM的知识转移能力。通过本文，我们提出了可供其他研究人员使用的DDLM训练和评估流程以及预先训练的DDLM模型，这些模型可在未来的D相关的研究中使用。

    Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
    
[^127]: 多任务卷积神经网络用于图像美学评估

    Multi-task convolutional neural network for image aesthetic assessment. (arXiv:2305.09373v1 [cs.CV])

    [http://arxiv.org/abs/2305.09373](http://arxiv.org/abs/2305.09373)

    本文提出了一种多任务卷积神经网络，可以同时预测图像的总体美学评分和美学属性，并在实验中表现优异，达到接近人类表现的整体美学评分。

    

    由于人们对图像美学偏好的理解还远远不够，图像美学评估是一项具有挑战性的人工智能任务。本文提出了一种多任务卷积神经网络，考虑了影响图像美学的因素。所提出的神经网络同时学习了图像的总体美学评分以及这些已知美学属性。这种多任务学习框架通过共享表示实现有效的泛化。实验结果证明了所提出的方法在预测图像美学的整体评分方面优于现有的方法。在考虑Spearman等级相关性时，我们实现了接近于人类表现的整体美学评分。此外，我们的模型在另一个基准测试中还通过预测图像的特定美学属性开创了多任务应用的先河。

    As people's aesthetic preferences for images are far from understood, image aesthetic assessment is a challenging artificial intelligence task. The range of factors underlying this task is almost unlimited, but we know that some aesthetic attributes affect those preferences. In this study, we present a multi-task convolutional neural network that takes into account these attributes. The proposed neural network jointly learns the attributes along with the overall aesthetic scores of images. This multi-task learning framework allows for effective generalization through the utilization of shared representations. Our experiments demonstrate that the proposed method outperforms the state-of-the-art approaches in predicting overall aesthetic scores for images in one benchmark of image aesthetics. We achieve near-human performance in terms of overall aesthetic scores when considering the Spearman's rank correlations. Moreover, our model pioneers the application of multi-tasking in another ben
    
[^128]: 稀疏和密集神经网络中的小批量大小的相变

    Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.06435](http://arxiv.org/abs/2305.06435)

    本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。

    

    在训练人工神经网络时，使用小批量数据现在非常普遍。尽管已经广泛使用，但缺少定量解释最佳小批量大小应该是多大的理论。本文尝试系统地理解小批量大小在训练两层神经网络中的作用。在教师-学生情境下，使用稀疏教师，并聚焦于不同复杂度的任务，我们量化了改变小批量大小m的影响。我们发现，通常情况下，学生的泛化性能强烈依赖于m，并且可能在临界值mc处经历尖锐的相变，这样当m< mc时，训练过程失败，而当m> mc时，学生可以完美地学习或很好地泛化教师。相变是由统计力学首次发现的集体现象，并在许多科学领域观察到。找到在深度学习中改变小批量大小的相变，可以阐明神经网络优化的基本机制。

    The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m<m_c$ the training process fails, while for $m>m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
    
[^129]: 基于局部Lajasiewicz条件的随机梯度下降在深度神经网络中的收敛性研究

    Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks. (arXiv:2304.09221v1 [cs.LG])

    [http://arxiv.org/abs/2304.09221](http://arxiv.org/abs/2304.09221)

    本文通过随机梯度下降算法研究了解析度函数为非凸的深度神经网络的全局收敛性，证明了当机器学习噪声的尺度与目标函数相等时，在局部区域内初始化后，以正的概率能够收敛到该区域内的全局最小值。

    

    本文考虑了解析度函数为非凸的情况下，通过随机梯度下降算法对深度神经网络的全局收敛性进行了研究。在有限宽的神经网络中，通过加入最小的额外假设并保证机器学习噪声的尺度与目标函数相等，证明了在局部区域内初始化时，以正的概率随机梯度下降迭代收敛到该区域内的全局最小值。本文的关键是确保随机梯度下降的整个轨迹以正的概率保留在局部区域内。文章提供了负面分析，表明使用Robbins-Monro类型的步长之间具有有界噪声的假设不足以保持该关键部分的有效性。

    We extend the global convergence result of Chatterjee \cite{chatterjee2022convergence} by considering the stochastic gradient descent (SGD) for non-convex objective functions. With minimal additional assumptions that can be realized by finitely wide neural networks, we prove that if we initialize inside a local region where the \L{}ajasiewicz condition holds, with a positive probability, the stochastic gradient iterates converge to a global minimum inside this region. A key component of our proof is to ensure that the whole trajectories of SGD stay inside the local region with a positive probability. For that, we assume the SGD noise scales with the objective function, which is called machine learning noise and achievable in many real examples. Furthermore, we provide a negative argument to show why using the boundedness of noise with Robbins-Monro type step sizes is not enough to keep the key component valid.
    
[^130]: 改进串联推荐的鲁棒性和准确性: 伴随级联指导的对抗训练方法

    Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v1 [cs.IR])

    [http://arxiv.org/abs/2304.05492](http://arxiv.org/abs/2304.05492)

    本研究利用级联指导下的对抗训练方法，增强了串联推荐模型的鲁棒性和准确性，取得了比已有方法更好的结果。

    

    串联推荐模型是一种通过学习用户与物品间的时间顺序互动来进行推荐的模型，其已经在许多领域中展现出了良好的表现。然而，近期串联推荐模型的鲁棒性备受质疑。这种模型的两个特性使其容易受到攻击 - 在训练中会产生级联效应，在模型过度依赖时间信息的同时会忽略其他特征。为了解决这些问题，本文提出了一种针对串联推荐模型的级联指导下的对抗训练的方法。我们的方法利用串联建模中的内在级联效应，在训练过程中产生战略性的对抗性扰动来影响物品嵌入。在使用不同的公共数据集训练四种最先进的串联模型实验中，我们的训练方法产生了比现有方法更高的模型鲁棒性，并获得了更好的性能。

    Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ran
    
[^131]: Pgx:强化学习硬件加速的并行游戏模拟器

    Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])

    [http://arxiv.org/abs/2303.17503](http://arxiv.org/abs/2303.17503)

    Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。

    

    我们提出了Pgx，这是一个用JAX编写的棋盘游戏模拟器集合。由于JAX的自动向量化和即时编译功能，Pgx易于在GPU/TPU加速器上进行大规模并行执行。我们发现，在单个A100 GPU上的Pgx模拟比现有的强化学习库快10倍。Pgx实现了被认为是人工智能研究中至关重要的基准测试的游戏，如Backgammon，Shogi和Go。 Pgx可在https://github.com/sotetsuk/pgx获得。

    We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
    
[^132]: MGTBench：机器生成文本检测基准测试

    MGTBench: Benchmarking Machine-Generated Text Detection. (arXiv:2303.14822v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.14822](http://arxiv.org/abs/2303.14822)

    本文提出了MGTBench框架，旨在解决机器生成文本检测中现有评估方法的不足。该框架通过广泛评估和公开数据集，提供了全面的MGT检测评估，使研究人员能够比较不同检测方法的有效性。

    

    现在，大型语言模型（LLM）在多种自然语言处理（NLP）任务中显示出了革命性的力量，例如文本分类，情感分析，语言翻译和问答。因此，检测机器生成文本（MGT）随着LLM变得越来越先进和普遍变得越来越重要。这些模型可以生成类似于人类写作的语言，很难与人类写的文本区分开来，这引发了关于真实性，问责和潜在偏见的担忧。然而，现有的MGT检测方法是在不同的模型体系结构，数据集和实验设置下进行评估的，导致缺乏跨不同方法学的全面评估框架。本文通过提出名为MGTBench的MGT检测基准框架来填补这个空白。对由ChatGPT生成的答案进行了广泛的评估，该模型是中国最具代表性和最强大的LLM模型。结果表明，MGTBench提供了公平和全面的MGT检测评估，并使研究人员比较了不同检测方法的有效性。

    Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent. These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies  In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench. Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and power
    
[^133]: 组合干预的因果推断框架:合成组合

    Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions. (arXiv:2303.14226v1 [stat.ME])

    [http://arxiv.org/abs/2303.14226](http://arxiv.org/abs/2303.14226)

    提出了一种在组合干预下进行因果推断的模型，通过施加潜在结构跨越单位和组合，在降低实验数量和处理混杂问题方面有着良好表现。

    

    我们考虑一个包含N个异质单位和p个干预的设置。 我们的目标是学习任意组合的单位特定潜在结果，即N×2 ^ p个因果参数。在许多应用程序中自然出现了选择干预组合的问题，例如因子设计试验，推荐引擎(例如，为用户显示最大程度的参与度的一组电影)，医学中的组合疗法，选择ML模型的重要特征等等。当N和p增长时，进行N×2 ^ p个实验来估计各种参数是不可行的。而且，观测数据很可能存在混杂，即单位是否在组合下出现与其在该组合下的潜在结果相关。为了解决这些问题，我们提出了一种新颖的模型，它在单位和组合之间都施加了潜在结构。我们假设单位之间存在潜在的相似性(即类似单位的潜在结果是相似的)，并且组合之间也存在潜在的相似性(即类似组合的效果是相似的)。我们使用层次贝叶斯非参数模型来形式化这一点，该模型联合聚类单元和组合，并且足够灵活，可以模拟连续或离散结果。我们在模拟和实际数据上演示了所提出的方法，并表明它可以显着减少学习因果参数所需的实验数量。

    We consider a setting with $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \times 2^p$ causal parameters. Choosing combinations of interventions is a problem that naturally arises in many applications such as factorial design experiments, recommendation engines (e.g., showing a set of movies that maximizes engagement for users), combination therapies in medicine, selecting important features for ML models, etc. Running $N \times 2^p$ experiments to estimate the various parameters is infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. To address these challenges, we propose a novel model that imposes latent structure across both units and combinations. We assume latent similarity across units (i.e., the potential outco
    
[^134]: 神经网络缩放的量化模型

    The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])

    [http://arxiv.org/abs/2303.13506](http://arxiv.org/abs/2303.13506)

    该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。

    

    我们提出了神经网络缩放定律的量化模型，解释了观察到的损失函数随着模型和数据规模的幂律下降以及随着规模的增加出现新能力的突然突破。我们从所谓的“量化假设”中推导出这个模型，其中学习到的神经网络功能被量化为离散块（“量子”）。我们在降序学习频率中学习量子，并表明当量子被以递减使用频率的顺序学习时，在使用频率中使用幂律可以解释观察到的损失缩放定律。我们在玩具数据集上验证了这个预测，然后研究了大型语言模型的缩放曲线如何分解。使用语言模型的内部，我们自动发现多样的模型能力（量子），并发现对应子问题的分布与我们理论预测的神经缩放指数产生了兼容性证据。

    We propose the $\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.
    
[^135]: 应用SMILES序列的Transformer模型在学习手性时存在困难

    Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])

    [http://arxiv.org/abs/2303.11593](http://arxiv.org/abs/2303.11593)

    应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。

    

    近年来，基于对极其多样的分子进行表示学习的描述符生成已经得到了发展，特别是那些将自然语言处理（NLP）模型应用于SMILES，即分子结构的文字表示的模型。然而，关于这些模型如何理解化学结构的研究很少。为了解决这个问题，我们调查了一种代表性的NLP模型——Transformer，在学习SMILES和化学结构之间的关系。结果表明，虽然Transformer快速学习分子的部分结构，但需要进行长时间的训练才能理解整体结构。与之一致的是，在不同的学习步骤中生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。此外，我们发现Transformer需要特别长的训练时间才能学习手性，并且有时会出现低翻译准确率的停滞现象。

    Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
    
[^136]: 停留还是离开预训练基域：关于集成学习在迁移学习中的洞见

    To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03374](http://arxiv.org/abs/2303.03374)

    该论文研究了在迁移学习中使用单个预训练检查点微调的模型集合，发现通过更好地探索预训练基域可以改进集成模型，但离开基域会导致失去迁移学习的好处，并且降低集成质量。作者提出了一种更有效的修改方法StarSSE，可以产生更强的集成模型和均匀的模型混合。

    

    迁移学习和集成学习是改善神经网络性能和鲁棒性的两种热门技术。由于预训练成本高昂，通常实践中使用从单个预训练检查点微调的模型集合。这些模型最终会进入损失函数梯度下降空间的相同区域，我们称之为预训练基域，因此具有有限的多样性。在这项工作中，我们展示了从单个预训练检查点训练的集成模型可以通过更好地探索预训练基域来改进，然而，离开基域会导致失去迁移学习的好处并导致集成质量的下降。基于对现有探索方法的分析，我们提出了一种更有效的修改Transfer Learning Setup中的Snapshot Ensembles（SSE）方法，名为StarSSE，它能产生更强的集成模型和均匀的模型混合。

    Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups.
    
[^137]: CAMEL: 曲率增强的流形嵌入与学习

    CAMEL: Curvature-Augmented Manifold Embedding and Learning. (arXiv:2303.02561v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02561](http://arxiv.org/abs/2303.02561)

    CAMEL是一种新的方法，利用黎曼流形上的拓扑度量和独特的黎曼度量进行高维数据分类、降维和可视化。它通过平滑分区统一算子将局部正交投影转换为全局嵌入，并提供了聚类显著特征的物理解释。CAMEL在各种基准数据集上表现优于其他方法，特别是对于高维数据集。

    

    提出了一种名为Curvature-Augmented Manifold Embedding and Learning (CAMEL)的新方法，用于高维数据分类、降维和可视化。CAMEL利用在黎曼流形上定义的拓扑度量以及用于距离和曲率的独特黎曼度量来增强其表达能力。该方法还在黎曼流形上使用平滑分区统一算子，将局部正交投影转换为全局嵌入，同时捕捉整体拓扑结构和局部相似性。局部正交向量提供了聚类的显著特征的物理解释。因此，CAMEL不仅提供了低维嵌入，还解释了此嵌入背后的物理情况。CAMEL已在各种基准数据集上进行了评估，并显示出优于其他最先进方法，特别是对于高维数据集。该方法的显著优势是。

    A novel method, named Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed for high dimensional data classification, dimension reduction, and visualization. CAMEL utilizes a topology metric defined on the Riemannian manifold, and a unique Riemannian metric for both distance and curvature to enhance its expressibility. The method also employs a smooth partition of unity operator on the Riemannian manifold to convert localized orthogonal projection to global embedding, which captures both the overall topological structure and local similarity simultaneously. The local orthogonal vectors provide a physical interpretation of the significant characteristics of clusters. Therefore, CAMEL not only provides a low-dimensional embedding but also interprets the physics behind this embedding. CAMEL has been evaluated on various benchmark datasets and has shown to outperform state-of-the-art methods, especially for high-dimensional datasets. The method's distinct benefits are it
    
[^138]: 比较耦合流和自回归流的鲁棒统计检验研究

    Comparative Study of Coupling and Autoregressive Flows through Robust Statistical Tests. (arXiv:2302.12024v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.12024](http://arxiv.org/abs/2302.12024)

    本论文通过比较耦合流和自回归流的不同架构和多样目标分布，利用各种测试统计量进行性能比较，为正规化流的生成模型提供了深入的研究和实证评估。

    

    正规化流已经成为一种强大的生成模型，因为它们不仅能够有效地对复杂目标分布进行采样，而且还通过构造提供密度估计。我们在这里提出了对耦合流和自回归流进行深入比较的研究，包括仿射和有理二次样条类型的四种不同架构：实值非体积保持（RealNVP）、掩蔽自回归流（MAF）、耦合有理二次样条（C-RQS）和自回归有理二次样条（A-RQS）。我们关注一组从4维到400维递增的多模态目标分布。通过使用不同的两样本测试的测试统计量进行比较，我们建立了已知距离度量的测试统计量：切片Wasserstein距离、维度平均一维Kolmogorov-Smirnov检验和相关矩阵之差的Frobenius范数。另外，我们还包括了以下估计：

    Normalizing Flows have emerged as a powerful brand of generative models, as they not only allow for efficient sampling of complicated target distributions, but also deliver density estimation by construction. We propose here an in-depth comparison of coupling and autoregressive flows, both of the affine and rational quadratic spline type, considering four different architectures: Real-valued Non-Volume Preserving (RealNVP), Masked Autoregressive Flow (MAF), Coupling Rational Quadratic Spline (C-RQS), and Autoregressive Rational Quadratic Spline (A-RQS). We focus on a set of multimodal target distributions of increasing dimensionality ranging from 4 to 400. The performances are compared by means of different test-statistics for two-sample tests, built from known distance measures: the sliced Wasserstein distance, the dimension-averaged one-dimensional Kolmogorov-Smirnov test, and the Frobenius norm of the difference between correlation matrices. Furthermore, we include estimations of th
    
[^139]: 信息理论上界对信息理论下界的贡献

    Information Theoretic Lower Bounds for Information Theoretic Upper Bounds. (arXiv:2302.04925v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04925](http://arxiv.org/abs/2302.04925)

    本文研究了在随机凸优化中，输出模型和经验样本之间的互信息与算法泛化之间的关系。研究结果表明，现有的信息理论泛化界限不足以捕捉到像SGD和正则化ERM这样具有维度无关样本复杂度的算法的泛化能力。

    

    本文在随机凸优化的背景下研究了输出模型和经验样本之间的互信息与算法的泛化之间的关系。尽管对信息理论泛化界限的兴趣日益增加，但这些界限能否揭示各种学习算法的卓越性能还不确定。我们对随机凸优化的研究表明，对于真正的风险最小化，依赖于维度的互信息是必要的。这表明现有的信息理论泛化界限不能完全捕捉到像SGD和正则化ERM这样具有维度无关样本复杂度的算法的泛化能力。

    We examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. Despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. Our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. This indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like SGD and regularized ERM, which have dimension-independent sample complexity.
    
[^140]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^141]: 线性相关噪声下的梯度下降：理论及应用于差分隐私中

    Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy. (arXiv:2302.01463v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01463](http://arxiv.org/abs/2302.01463)

    本文研究梯度下降在线性相关噪声下的表现，提出了新的矩阵分解方法用于不同ially private optimization。

    

    我们研究了在线性相关噪声下的梯度下降。我们的工作是由最近针对具有差分隐私（DP）的优化的实践方法所启发的，例如DP-FTRL，在无法使用隐私放大技术的情况下（例如在联邦学习中），这些方法通过矩阵分解机制注入隐私噪声，使噪声在迭代过程中呈线性相关关系。我们提出了一种简化的环境，精简了这些方法的关键面貌，并分离了线性相关噪声的影响。我们分析了梯度下降在这种情况下的行为，无论是凸函数还是非凸函数。我们的分析明显比之前的工作更紧，并精确地恢复了多个重要的特殊情况（包括反相关扰动梯度下降）。我们使用我们的结果开发了新的，有效的矩阵分解方法，用于不同ially private optimization，并突出了这些因子分解方法的好处

    We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretica
    
[^142]: 高效的私人个性化分散学习中节点选择

    Efficient Node Selection in Private Personalized Decentralized Learning. (arXiv:2301.12755v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12755](http://arxiv.org/abs/2301.12755)

    这篇论文提出了私人个性化分散学习（PPDL）的方法，利用安全聚合和相关对抗式多臂老虎机优化来实现高效的节点选择，并保护节点的隐私。作者通过利用不同合作者之间的依赖关系，仅基于聚合模型就能够有效地识别合适的合作者。实验证明PPDL在标签和协变量偏移下的模型性能优于先前的非私密方法。

    

    个性化分散学习是一种有前途的分布式学习范式，使每个节点能够在自己的数据上训练本地模型，并与其他节点合作以改进而不共享任何数据。然而，这种方法存在着重大的隐私风险，因为节点可能会通过其合作选择无意中披露有关其数据或偏好的敏感信息。在本文中，我们提出了私人个性化分散学习（PPDL），这是一种结合了安全聚合和相关对抗式多臂老虎机优化的新方法，旨在保护节点的隐私同时实现高效的节点选择。通过利用不同臂之间的依赖关系，即潜在的合作者，我们证明PPDL可以仅基于聚合模型有效地识别合适的合作者。此外，我们展示了PPDL在标签和协变量偏移下的模型性能优于先前的非私密方法。

    Personalized decentralized learning is a promising paradigm for distributed learning, enabling each node to train a local model on its own data and collaborate with other nodes to improve without sharing any data. However, this approach poses significant privacy risks, as nodes may inadvertently disclose sensitive information about their data or preferences through their collaboration choices. In this paper, we propose Private Personalized Decentralized Learning (PPDL), a novel approach that combines secure aggregation and correlated adversarial multi-armed bandit optimization to protect node privacy while facilitating efficient node selection. By leveraging dependencies between different arms, represented by potential collaborators, we demonstrate that PPDL can effectively identify suitable collaborators solely based on aggregated models. Additionally, we show that PPDL surpasses previous non-private methods in model performance on standard benchmarks under label and covariate shift s
    
[^143]: SPIDER仪器的学习干涉成像

    Learned Interferometric Imaging for the SPIDER Instrument. (arXiv:2301.10260v2 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2301.10260](http://arxiv.org/abs/2301.10260)

    本文提出了两种利用深度学习从SPIDER仪器测量数据中重建图像的方法，通过学习先验信息来提高重建质量，并将重建时间缩短到约10毫秒，实现了SPIDER仪器的实时成像。

    

    分段平面成像光学干涉仪（SPIDER）是一种通过干涉成像来减小体积、重量和功耗的大型空间望远镜设计的替代方案。目前，从干涉测量中重建图像的最先进方法采用近端优化技术，这些技术计算量大且需要手工先验知识。在本文中，我们提出了两种基于数据驱动方法来重建SPIDER仪器测量所得图片的方法。这些方法利用深度学习从训练数据中学习先验信息，提高了重建质量，并将恢复图片所需的计算时间大幅缩短多个数量级。重建时间减少到约10毫秒，首次为SPIDER实现了实时成像的可能性。

    The Segmented Planar Imaging Detector for Electro-Optical Reconnaissance (SPIDER) is an optical interferometric imaging device that aims to offer an alternative to the large space telescope designs of today with reduced size, weight and power consumption. This is achieved through interferometric imaging. State-of-the-art methods for reconstructing images from interferometric measurements adopt proximal optimization techniques, which are computationally expensive and require handcrafted priors. In this work we present two data-driven approaches for reconstructing images from measurements made by the SPIDER instrument. These approaches use deep learning to learn prior information from training data, increasing the reconstruction quality, and significantly reducing the computation time required to recover images by orders of magnitude. Reconstruction time is reduced to ${\sim} 10$ milliseconds, opening up the possibility of real-time imaging with SPIDER for the first time. Furthermore, we
    
[^144]: 多类数据集中的拓扑学习

    Topological Learning in Multi-Class Data Sets. (arXiv:2301.09734v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09734](http://arxiv.org/abs/2301.09734)

    本文将拓扑数据分析技术应用于多类数据集中，通过构建拓扑分类器和简单复合体，研究了拓扑复杂性对前馈深度神经网络学习的影响，并验证了拓扑复杂性与DNN学习之间的负相关性。

    

    我们将拓扑数据分析技术应用于多类数据集的拓扑复杂性表征问题。通过定义一个拓扑分类器，使用数据集的开放子覆盖，可以构建一个简单复合体，其拓扑特征（如Betti数）提供关于分类问题的信息。我们使用这些拓扑结构来研究拓扑复杂性对前馈深度神经网络（DNN）学习的影响。我们假设拓扑复杂性与全连接前馈深度神经网络正确分类数据的能力呈负相关。我们在多个构建和开源数据集上评估我们的拓扑分类算法。我们还验证了在多个数据集上拓扑复杂性与DNN学习之间的关系。

    We specialize techniques from topological data analysis to the problem of characterizing the topological complexity (as defined in the body of the paper) of a multi-class data set. As a by-product, a topological classifier is defined that uses an open sub-covering of the data set. This sub-covering can be used to construct a simplicial complex whose topological features (e.g., Betti numbers) provide information about the classification problem. We use these topological constructs to study the impact of topological complexity on learning in feedforward deep neural networks (DNNs). We hypothesize that topological complexity is negatively correlated with the ability of a fully connected feedforward deep neural network to learn to classify data correctly. We evaluate our topological classification algorithm on multiple constructed and open source data sets. We also validate our hypothesis regarding the relationship between topological complexity and learning in DNN's on multiple data sets.
    
[^145]: 通过神经网络的庞特里亚金最优控制

    Pontryagin Optimal Control via Neural Networks. (arXiv:2212.14566v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.14566](http://arxiv.org/abs/2212.14566)

    本文将神经网络与庞特里亚金最大原理相结合，提出了一个样本高效的框架NN-PMP-Gradient，可以应用于具有未知和复杂动力学的系统。通过采用迭代方法，该框架不仅利用准确的神经网络参数化的替代模型，还高效地恢复了最优性条件和最优动作序列。

    

    解决现实世界中的最优控制问题是一项具有挑战性的任务，因为复杂的高维系统动力学通常对决策者来说是未知的。因此，通过数值方法找到最优控制动作是困难的。为了应对这种建模和计算挑战，在本文中，我们将神经网络与庞特里亚金最大原理（PMP）相结合，提出了一种样本高效的框架NN-PMP-Gradient。所得到的控制器可以用于具有未知和复杂动力学的系统。通过采用迭代方法，所提出的框架不仅利用了由神经网络参数化的准确替代模型，还通过PMP条件高效地恢复了最优性条件以及最优动作序列。在线性二次调节器、与网格连接的有损电池的能源套利、单摆的控制和两个MuJoCo运动任务的数值仿真中，证明了我们提出的NN-PMP-Gradient是一个通用的和多功能的框架。

    Solving real-world optimal control problems are challenging tasks, as the complex, high-dimensional system dynamics are usually unrevealed to the decision maker. It is thus hard to find the optimal control actions numerically. To deal with such modeling and computation challenges, in this paper, we integrate Neural Networks with the Pontryagin's Maximum Principle (PMP), and propose a sample efficient framework NN-PMP-Gradient. The resulting controller can be implemented for systems with unknown and complex dynamics. By taking an iterative approach, the proposed framework not only utilizes the accurate surrogate models parameterized by neural networks, it also efficiently recovers the optimality conditions along with the optimal action sequences via PMP conditions. Numerical simulations on Linear Quadratic Regulator, energy arbitrage of grid-connected lossy battery, control of single pendulum, and two MuJoCo locomotion tasks demonstrate our proposed NN-PMP-Gradient is a general and vers
    
[^146]: 在连续值估计中管理时间分辨率: 一项基本权衡的研究

    Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off. (arXiv:2212.08949v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08949](http://arxiv.org/abs/2212.08949)

    本论文分析了在强化学习和最优控制中观测时间以离散时间点固定周期到达的默认假设与实际情况下的连续时间系统之间的差异，并在LQR系统中揭示了近似误差和统计误差之间的基本权衡。在有限数据的情况下，管理时间分辨率可以显著改善策略评估的效率。

    

    强化学习（RL）和最优控制中的默认假设是观测以固定的时钟周期在离散的时间点到达。然而，许多应用涉及连续时间系统，理论上可以对时间离散化进行管理。时间离散化对RL方法的影响尚未在现有理论中完全表征，但对其影响进行更详细的分析可能揭示提高数据效率的机会。我们通过分析LQR系统的Monte-Carlo策略评估来填补这一空白，并发现了估值过程中近似误差和统计误差之间的基本权衡。重要的是，这两种错误对时间离散化的表现不同，这导致了对于给定数据预算的时间分辨率的最佳选择。这些发现表明，在具有有限数据的LQR系统中，管理时间分辨率可以改善策略评估的效率。从实证角度来看，我们在数值模拟中展示了这种权衡。

    A default assumption in reinforcement learning (RL) and optimal control is that observations arrive at discrete time points on a fixed clock cycle. Yet, many applications involve continuous-time systems where the time discretization, in principle, can be managed. The impact of time discretization on RL methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation for LQR systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. Importantly, these two errors behave differently to time discretization, leading to an optimal choice of temporal resolution for a given data budget. These findings show that managing the temporal resolution can provably improve policy evaluation efficiency in LQR systems with finite data. Empirically, we demonstrate the trade-off in numerical simulati
    
[^147]: DeepSpeed数据效率：通过高效的数据采样和路由改善深度学习模型质量和训练效率

    DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03597](http://arxiv.org/abs/2212.03597)

    DeepSpeed Data Efficiency提出了两种数据效率技术：高效的数据采样和高效的数据路由，能够提高深度学习模型的训练效率和质量。

    

    近年来，深度学习模型的进步以巨大的训练成本为代价。模型尺寸的增加是一个主要原因，但另一个不太被重视的事实是，数据规模实际上与模型规模以相似的速度增加，而训练成本与两者成比例。与不断发展的模型架构相比，如何高效利用训练数据（特别是对于昂贵的基础模型预训练）在较少被探索且难以实现，这是因为缺乏一个专注于数据效率能力的方便框架。为此，我们提出了DeepSpeed数据效率，一种能更好地利用数据、提高训练效率并改善模型质量的框架。具体而言，我们提出并结合了两种数据效率技术：通过通用课程学习库实现高效数据采样，以及通过一种新颖的随机逐层删除令牌的技术实现高效数据路由。针对GPT-3 1.3B语言模型预训练进行了实验验证。

    Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focuses on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretr
    
[^148]: 多维路径依赖期权的深度签名算法

    Deep Signature Algorithm for Multi-dimensional Path-Dependent Options. (arXiv:2211.11691v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2211.11691](http://arxiv.org/abs/2211.11691)

    本文提出了一种适用于欧式型和美式型期权定价问题的多维路径依赖期权深度签名算法，解决了路径依赖型FBSDE问题，数值算法的收敛性得到保证。

    

    本文研究了路径依赖期权的深度签名算法。我们将Hur\'e-Pham-Warin在2020年针对具有反射的状态依赖型FBSDE的反向方案扩展到具有反射的路径依赖型FBSDE，通过向反向方案添加签名层。我们的算法适用于欧式型和美式型期权定价问题，而支付函数取决于基础正向股票过程的整个路径。我们证明了我们的数值算法的收敛性分析，并明确依赖于签名的截断阶数和神经网络逼近误差。算法的数值示例包括：Black-Scholes模型下的Amerasian期权、具有路径依赖几何平均收益函数的美式期权以及Shiryaev的最优停止问题。

    In this work, we study the deep signature algorithms for path-dependent options. We extend the backward scheme in [Hur\'e-Pham-Warin. Mathematics of Computation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections to path-dependent FBSDEs with reflections, by adding the signature layer to the backward scheme. Our algorithm applies to both European and American type option pricing problems while the payoff function depends on the whole paths of the underlying forward stock process. We prove the convergence analysis of our numerical algorithm with explicit dependence on the truncation order of the signature and the neural network approximation errors. Numerical examples for the algorithm are provided including: Amerasian option under the Black-Scholes model, American option with a path-dependent geometric mean payoff function, and the Shiryaev's optimal stopping problem.
    
[^149]: 探索基于扩散模型的离线模型优化的验证指标

    Exploring validation metrics for offline model-based optimisation with diffusion models. (arXiv:2211.10747v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10747](http://arxiv.org/abs/2211.10747)

    本论文研究基于扩散模型的离线模型优化中的验证指标。在离线模型优化中，我们希望在没有访问真值预言机的情况下设计候选方案。现有的验证指标是对预言机的近似，我们希望找到与真值预言机最相关的验证指标。

    

    在基于模型的优化中，我们希望利用机器学习设计候选方案，以最大化对于一个称为（地面真值）预言机的黑盒函数的某种奖励度量。然而，由于涉及到执行真实世界过程，计算预言机是昂贵的。在离线模型优化中，我们希望在训练或验证过程中不假设对预言机有访问权限，这使得评估变得复杂。虽然可以训练一个预言机的近似模型并在模型验证过程中使用它代替真值预言机来测量生成候选方案的平均奖励，但这种评估是近似的且容易受到对抗性样本的影响。我们将这种近似下生成候选方案的平均奖励作为一种“验证指标”，而我们更关心的是一个更基本的问题，即找到与真值预言机最相关的验证指标。这涉及到提出验证指标并对许多数据集进行量化。

    In model-based optimisation (MBO) we are interested in using machine learning to design candidates that maximise some measure of reward with respect to a black box function called the (ground truth) oracle, which is expensive to compute since it involves executing a real world process. In offline MBO we wish to do so without assuming access to such an oracle during training or validation, with makes evaluation non-straightforward. While an approximation to the ground oracle can be trained and used in place of it during model validation to measure the mean reward over generated candidates, the evaluation is approximate and vulnerable to adversarial examples. Measuring the mean reward of generated candidates over this approximation is one such `validation metric', whereas we are interested in a more fundamental question which is finding which validation metrics correlate the most with the ground truth. This involves proposing validation metrics and quantifying them over many datasets for
    
[^150]: 利用生命周期自适应处理学习自适应系统中适应空间的漂移

    Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02658](http://arxiv.org/abs/2211.02658)

    机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。

    

    最近，机器学习 (ML) 已成为支持自适应的热门方法。ML 已被用来处理自适应中的几个问题，例如在不确定性下维护最新的运行时模型和可扩展的决策制定。然而，利用 ML 存在固有的挑战。在本文中，我们着重讨论面向基于学习的自适应系统的一个特别重要的挑战：适应空间中的漂移。通过适应空间，我们指的是自适应系统在某一特定时间可以选择的适应选项的集合，以根据适应选项的质量属性进行适应。适应空间的漂移源于影响适应选项质量属性的不确定性。这种漂移可能意味着最终没有适应选项能够满足最初的适应目标，从而降低系统的质量，或者可能出现允许增强适应目标的适应选项。在 ML 中，这种漂移通常被称为概念漂移或实例漂移。为了解决这个挑战，我们提出了一种名为“生命周期自适应”的新方法。生命周期自适应对 ML powered self-adaptation 进行了扩展，使其能够更好地处理适应空间的漂移。

    Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
    
[^151]: 用于快速识别胸部 X 射线图像中肺炎的可解释 CNN-Multilevel Attention Transformer

    Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition of Pneumonia from Chest X-Ray Images. (arXiv:2210.16584v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.16584](http://arxiv.org/abs/2210.16584)

    本文提出了一种可解释的肺炎识别框架，通过使用多级自注意机制加速收敛并强调与任务相关的特征区域。采用实用的图像数据增强技术来解决数据稀缺问题，为医学实践提供高速分析支持。

    

    胸部成像在诊断和预测 COVID-19 患者呼吸状况恶化方面起着重要作用。已经开发出许多基于深度学习的肺炎识别方法来实现计算机辅助诊断。然而，长时间的训练和推断使它们缺乏灵活性，而缺乏可解释性降低了它们在临床医学实践中的可信度。本文旨在开发一个具有可解释性的肺炎识别框架，可以理解胸部 X 射线（CXR）图像中肺部特征与相关疾病之间的复杂关系，为医学实践提供高速分析支持。为了减少计算复杂性以加速识别过程，提出了一种新颖的基于 Transformer 的多级自注意机制来加速收敛并强调与任务相关的特征区域。此外，采用了实用的 CXR 图像数据增强技术来解决数据稀缺问题。

    Chest imaging plays an essential role in diagnosing and predicting patients with COVID-19 with evidence of worsening respiratory status. Many deep learning-based approaches for pneumonia recognition have been developed to enable computer-aided diagnosis. However, the long training and inference time makes them inflexible, and the lack of interpretability reduces their credibility in clinical medical practice. This paper aims to develop a pneumonia recognition framework with interpretability, which can understand the complex relationship between lung features and related diseases in chest X-ray (CXR) images to provide high-speed analytics support for medical practice. To reduce the computational complexity to accelerate the recognition process, a novel multi-level self-attention mechanism within Transformer has been proposed to accelerate convergence and emphasize the task-related feature regions. Moreover, a practical CXR image data augmentation has been adopted to address the scarcity
    
[^152]: 通过使用Cover Trees的最小间隔实现数值稳定的稀疏高斯过程

    Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07893](http://arxiv.org/abs/2210.07893)

    本文针对高斯过程模型的数值稳定性进行了研究，通过感兴趣点的选择和计算，提供了稳定可靠的稀疏逼近方法。

    

    高斯过程常用于较大的机器学习和决策系统中，例如地理空间建模、贝叶斯优化或潜在高斯模型中。在一个系统中，高斯过程模型需要以稳定可靠的方式运行，以确保与系统的其他部分正确交互。本文研究了基于感兴趣点的可扩展稀疏逼近的数值稳定性。为此，我们首先回顾了数值稳定性，并阐述了高斯过程模型可能不稳定的典型情况。在插值文献中原始开发的稳定性理论的基础上，我们导出了对感兴趣点进行计算的数值稳定性的充分条件和某些情况下的必要条件。对于地理空间建模等低维任务，我们提出了一种自动计算满足这些条件的感兴趣点的方法。

    Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
    
[^153]: RALACs: 使用交互编码和光流进行自动驾驶车辆中的动作识别

    RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14408](http://arxiv.org/abs/2209.14408)

    RALACs是一种针对自动驾驶车辆中动作识别的新颖系统，通过交互编码和光流技术将动作识别应用于道路场景，并弥合了其与人类动作识别之间的差距。

    

    将动作识别应用于自动驾驶车辆（AV）环境可以增强环境模型的情境感知能力。然而，传统的动作识别研究主要关注于人类，其对于嘈杂、未剪辑、原始的RGB数据的适应性有限，限制了其在其他领域的应用。为推动动作识别在AVs中的进展和应用，本研究提出了一种新颖的二阶段动作识别系统，命名为RALACs。RALACs将动作识别问题应用于道路场景，并弥合了其与人类动作识别领域之间的差距。本研究展示了注意力层如何有助于编码agent之间的关系，并强调这种方案如何与类别无关。此外，为解决道路上agent的动态性，RALACs提出了一种新颖的方法...

    When applied to autonomous vehicle (AV) settings, action recognition can enhance an environment model's situational awareness. This is especially prevalent in scenarios where traditional geometric descriptions and heuristics in AVs are insufficient. However, action recognition has traditionally been studied for humans, and its limited adaptability to noisy, un-clipped, un-pampered, raw RGB data has limited its application in other fields. To push for the advancement and adoption of action recognition into AVs, this work proposes a novel two-stage action recognition system, termed RALACs. RALACs formulates the problem of action recognition for road scenes, and bridges the gap between it and the established field of human action recognition. This work shows how attention layers can be useful for encoding the relations across agents, and stresses how such a scheme can be class-agnostic. Furthermore, to address the dynamic nature of agents on the road, RALACs constructs a novel approach to
    
[^154]: MIXRTs:通过混合循环软决策树实现可解释的多智能体强化学习

    MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees. (arXiv:2209.07225v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07225](http://arxiv.org/abs/2209.07225)

    MIXRTs是一种可解释的多智能体强化学习架构，通过混合循环软决策树的方式，能够表达明确的决策过程并展示每个智能体的贡献。

    

    在各个领域取得巨大成功的同时，现有的具有黑盒神经网络结构的多智能体强化学习（MARL）以不透明的方式做出决策，阻碍了人们理解学习到的知识以及输入观测如何影响决策。与此相反，现有的可解释方法，如传统的线性模型和决策树，往往在表达能力和准确性方面存在问题。为了解决性能和解释性之间的明显二元对立，我们提出了一种新颖的可解释结构——混合循环软决策树（MIXRTs），它能够通过从根节点到叶节点的路径表示明确的决策过程，并反映每个智能体对团队的贡献。具体而言，我们构建了一种新颖的软决策树来解决局部可观察性问题，利用循环神经网络的进展，并通过基于树的模型展示哪些特征影响决策过程。

    While achieving tremendous success in various fields, existing multi-agent reinforcement learning (MARL) with a black-box neural network architecture makes decisions in an opaque manner that hinders humans from understanding the learned knowledge and how input observations influence decisions. Instead, existing interpretable approaches, such as traditional linear models and decision trees, usually suffer from weak expressivity and low accuracy. To address this apparent dichotomy between performance and interpretability, our solution, MIXing Recurrent soft decision Trees (MIXRTs), is a novel interpretable architecture that can represent explicit decision processes via the root-to-leaf path and reflect each agent's contribution to the team. Specifically, we construct a novel soft decision tree to address partial observability by leveraging the advances in recurrent neural networks, and demonstrate which features influence the decision-making process through the tree-based model. Then, ba
    
[^155]: 规范化聚类准确度：一种非对称的外部聚类有效度量

    Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.02935](http://arxiv.org/abs/2209.02935)

    本文提出了一种非对称的外部聚类有效度量方法，旨在区分不同任务类型上表现良好和系统性表现不佳的聚类算法。与传统的内部度量不同，该方法利用参考真实分组进行评估，并弥补了现有方法在最坏情况下的误差。

    

    没有一个最好的聚类算法，我们仍然希望能够区分出在某些任务类型上表现良好和系统性表现不佳的方法。传统上，聚类算法使用内部或外部有效度量进行评估。内部度量量化所得分区的不同方面，例如，簇紧密度的平均程度或点的可分离性。然而，它们的有效性是有问题的，因为它们促使的聚类有时可能是无意义的。另一方面，外部度量将算法的输出与由专家提供的参考真实分组进行比较。在本文中，我们认为常用的经典分区相似性评分，例如规范化互信息、Fowlkes-Mallows或调整兰德指数，缺少一些可取的属性，例如，它们不能正确识别最坏情况，也不易解释。

    There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
    
[^156]: 使用正则化稀疏自编码器预测良好反应坐标和MD轨迹的未来演化：一种新的深度学习方法

    Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach. (arXiv:2208.10962v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2208.10962](http://arxiv.org/abs/2208.10962)

    本研究提出了一种新的深度学习方法，使用正则化稀疏自编码器预测良好的反应坐标以及MD轨迹的演化情况，并展示了正则化约束对于选择重要反应坐标的帮助。

    

    鉴定反应坐标(RCs)是一个活跃的研究领域，因为RCs在确定化学反应的进展中起着关键作用。选择反应坐标通常基于启发式知识。然而，选择的标准之一是该坐标应清晰地捕获反应物和生成物状态。此外，坐标应该是最慢的，使得所有其他自由度可以沿着反应坐标轻松达到平衡。我们使用了一种正则化稀疏自编码器，即基于能量的模型，来发现一组关键的反应坐标。除了发现反应坐标，我们的模型还可以预测分子动力学(MD)轨迹的演化。我们展示了包括稀疏约束正则化有助于选择一个小但重要的一组反应坐标。

    Identifying reaction coordinates(RCs) is an active area of research, given the crucial role RCs play in determining the progress of a chemical reaction. The choice of the reaction coordinate is often based on heuristic knowledge. However, an essential criterion for the choice is that the coordinate should capture both the reactant and product states unequivocally. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. We used a regularised sparse autoencoder, an energy-based model, to discover a crucial set of reaction coordinates. Along with discovering reaction coordinates, our model also predicts the evolution of a molecular dynamics(MD) trajectory. We showcased that including sparsity enforcing regularisation helps in choosing a small but important set of r
    
[^157]: NAPA: 中级可变分的本地脉冲变分量子算法

    NAPA: Intermediate-level Variational Native-pulse Ansatz for Variational Quantum Algorithms. (arXiv:2208.01215v5 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.01215](http://arxiv.org/abs/2208.01215)

    NAPA是一种中级可变分的本地脉冲变分量子算法，通过优化脉冲序列，充分利用了变分量子算法的优势。

    

    变分量子算法在噪声中间规模量子（NISQ）时代展示了巨大的潜力。在变分量子算法的工作流程中，通过迭代更新自变量来逼近期望的量子态。在设计更好的自变量方面，我们已经看到了各种努力，以减少门数量。有些方法考虑了底层电路的物理意义，而另一些方法则采用神经架构搜索（NAS）的思想来生成自变量。然而，这些设计没有充分利用变分量子算法的优势，因为大多数技术都针对门自变量，并且参数通常是门的旋转角度。在量子计算机中，门自变量最终将转换为控制信号，例如超导量子比特上的微波脉冲。这些控制脉冲需要精确校准以最小化诸如过旋转和欠旋转之类的误差。在变分量子算法的情况下，这个过程会引入冗余，但变分量子算法的可变性质可以用来更好地优化脉冲序列。

    Variational quantum algorithms (VQAs) have demonstrated great potentials in the Noisy Intermediate Scale Quantum (NISQ) era. In the workflow of VQA, the parameters of ansatz are iteratively updated to approximate the desired quantum states. We have seen various efforts to draft better ansatz with less gates. Some works consider the physical meaning of the underlying circuits, while others adopt the ideas of neural architecture search (NAS) for ansatz generator. However, these designs do not exploit the full advantages of VQAs. Because most techniques target gate ansatz, and the parameters are usually rotation angles of the gates. In quantum computers, the gate ansatz will eventually be transformed into control signals such as microwave pulses on superconducting qubits. These control pulses need elaborate calibrations to minimize the errors such as over-rotation and under-rotation. In the case of VQAs, this procedure will introduce redundancy, but the variational properties of VQAs can 
    
[^158]: 用对称性和退火补充循环神经网络波函数以提高精度

    Supplementing Recurrent Neural Network Wave Functions with Symmetry and Annealing to Improve Accuracy. (arXiv:2207.14314v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2207.14314](http://arxiv.org/abs/2207.14314)

    本论文提出了一种使用对称性和退火补充循环神经网络波函数的方法，用于更精确地估计二维海森堡模型的基态能量。相较于传统方法，该方法在大系统尺寸上具有重要的改进。

    

    循环神经网络（RNN）是一类从人工智能范式中出现的神经网络，已经在自然语言处理领域取得了许多有趣的进展。有趣的是，这些架构被证明是近似量子系统基态的强大ansatze。在这里，我们在二维空间中建立在[Phys. Rev. Research 2, 023358 (2020)]的结果之上，并构建了一个更强大的二维RNN波函数ansatz。我们使用对称性和退火来获得在方形格和三角形格上二维海森堡模型基态能量的准确估计。我们表明，相较于密度矩阵重整化群（DMRG），我们的方法在三角形格的系统尺寸大于或等于$14 \times 14$时更为优越。

    Recurrent neural networks (RNNs) are a class of neural networks that have emerged from the paradigm of artificial intelligence and has enabled lots of interesting advances in the field of natural language processing. Interestingly, these architectures were shown to be powerful ansatze to approximate the ground state of quantum systems. Here, we build over the results of [Phys. Rev. Research 2, 023358 (2020)] and construct a more powerful RNN wave function ansatz in two dimensions. We use symmetry and annealing to obtain accurate estimates of ground state energies of the two-dimensional (2D) Heisenberg model, on the square lattice and on the triangular lattice. We show that our method is superior to Density Matrix Renormalisation Group (DMRG) for system sizes larger than or equal to $14 \times 14$ on the triangular lattice.
    
[^159]: 可证明地调整ElasticNet在多个实例间的参数

    Provably tuning the ElasticNet across instances. (arXiv:2207.10199v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10199](http://arxiv.org/abs/2207.10199)

    这篇论文提供了一个针对ElasticNet的新颖结构结果，用以证明在多个问题实例中调整正规化参数，同时提供了统计和在线学习情景下的泛化保证。

    

    正规化理论中一个重要未解决的挑战是如何设置常用技术（如ElasticNet）的正规化系数，并提供一般可证明的保证。我们考虑了在多个问题实例中调整Ridge回归、LASSO和ElasticNet的正规化参数的问题，这种设置包括了交叉验证和多任务超参数优化。我们获得了针对ElasticNet的新颖结构结果，将损失函数表达为以调整参数为函数的分段有理函数，其中包含代数边界。我们利用这一结果对正规化损失函数的结构复杂性进行了界定，并在统计情景下展示了调整ElasticNet回归系数的泛化保证。我们还考虑了更具挑战性的在线学习情景，展示了与最优参数对相对而言，消失的平均预期遗憾。我们进一步将我们的结果扩展到调整分类问题的情景。

    An important unresolved challenge in the theory of regularization is to set the regularization coefficients of popular techniques like the ElasticNet with general provable guarantees. We consider the problem of tuning the regularization parameters of Ridge regression, LASSO, and the ElasticNet across multiple problem instances, a setting that encompasses both cross-validation and multi-task hyperparameter optimization. We obtain a novel structural result for the ElasticNet which characterizes the loss as a function of the tuning parameters as a piecewise-rational function with algebraic boundaries. We use this to bound the structural complexity of the regularized loss functions and show generalization guarantees for tuning the ElasticNet regression coefficients in the statistical setting. We also consider the more challenging online learning setting, where we show vanishing average expected regret relative to the optimal parameter pair. We further extend our results to tuning classific
    
[^160]: SPIRAL：一种用于非凸有限和最小化的超线性收敛增量近端算法

    SPIRAL: A superlinearly convergent incremental proximal algorithm for nonconvex finite sum minimization. (arXiv:2207.08195v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2207.08195](http://arxiv.org/abs/2207.08195)

    SPIRAL是一种用于解决非凸正则化有限和问题的超线性收敛增量近端算法，通过结合增量梯度更新和永不触发渐近行搜索特性，在极限点下实现了超线性收敛。在不同类型的问题中，包括凸、非凸和非Lipschitz可微问题，SPIRAL算法及其自适应变体表现出了与目前最先进算法相当的竞争力。

    

    我们引入了SPIRAL，一种用于在相对平滑性假设下解决非凸正则化有限和问题的超线性收敛增量近端算法。SPIRAL的每次迭代包括内循环和外循环。它结合了增量梯度更新和具有永不触发渐近行搜索特性的线搜索，在极限点下以超线性收敛。在不同的凸、非凸和非Lipschitz可微问题上使用L-BFGS方向进行模拟结果表明，我们的算法以及其自适应变体与现有技术相比具有竞争力。

    We introduce SPIRAL, a SuPerlinearly convergent Incremental pRoximal ALgorithm, for solving nonconvex regularized finite sum problems under a relative smoothness assumption. Each iteration of SPIRAL consists of an inner and an outer loop. It combines incremental gradient updates with a linesearch that has the remarkable property of never being triggered asymptotically, leading to superlinear convergence under mild assumptions at the limit point. Simulation results with L-BFGS directions on different convex, nonconvex, and non-Lipschitz differentiable problems show that our algorithm, as well as its adaptive variant, are competitive to the state of the art.
    
[^161]: KeyCLD: 从图像中学习基于关键点坐标的受限拉格朗日动力学

    KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates from Images. (arXiv:2206.11030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11030](http://arxiv.org/abs/2206.11030)

    KeyCLD是一个从图像中学习拉格朗日动力学的框架，通过学习关键点坐标表示状态动力学，并结合显式的完整约束来表示动力学。该方法经过无监督端到端训练，在各种环境中展示了准确学习动力学的能力，并实现了长期视频预测。与其他模型进行比较，并研究了拉格朗日先验和约束函数的优势。

    

    我们提出了KeyCLD，一个从图像中学习拉格朗日动力学的框架。学习到的关键点在图像中代表语义地标，可以直接表示状态动力学。我们表明，将这个状态解释为笛卡尔坐标，结合显式的完整约束，可以用受限拉格朗日动力学来表示动力学。KeyCLD在图像序列上进行无监督端到端训练。我们的方法显式地建模了质量矩阵、势能和输入矩阵，从而实现基于能量的控制。我们在dm_control摆、倒立摆和倒立摆机器人环境上展示了从图像中学习拉格朗日动力学的能力。KeyCLD可以在这些系统上进行学习，无论它们是无驱动、亚驱动还是全驱动。训练好的模型能够进行长期视频预测，显示其准确学习了动力学。我们与Lag-VAE、Lag-caVAE和HGN进行了比较，并研究了拉格朗日先验和约束函数的优势。

    We present KeyCLD, a framework to learn Lagrangian dynamics from images. Learned keypoints represent semantic landmarks in images and can directly represent state dynamics. We show that interpreting this state as Cartesian coordinates, coupled with explicit holonomic constraints, allows expressing the dynamics with a constrained Lagrangian. KeyCLD is trained unsupervised end-to-end on sequences of images. Our method explicitly models the mass matrix, potential energy and the input matrix, thus allowing energy based control. We demonstrate learning of Lagrangian dynamics from images on the dm_control pendulum, cartpole and acrobot environments. KeyCLD can be learned on these systems, whether they are unactuated, underactuated or fully actuated. Trained models are able to produce long-term video predictions, showing that the dynamics are accurately learned. We compare with Lag-VAE, Lag-caVAE and HGN, and investigate the benefit of the Lagrangian prior and the constraint function. KeyCLD 
    
[^162]: 资源高效的分离Transformer

    Resource-Efficient Separation Transformer. (arXiv:2206.09507v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2206.09507](http://arxiv.org/abs/2206.09507)

    本文开发出了一种资源高效的分离Transformer（RE-SepFormer），通过非重叠的潜空间块和紧凑的潜变量摘要操作，降低了计算负担。在语音分离任务中取得了竞争性能，并具有较好的内存和推断时间扩展性。

    

    最近Transformer在语音分离中取得了最先进的性能。然而，这些模型计算成本高且需要大量的可学习参数。本文探索了一种计算成本较低的基于Transformer的语音分离方法。我们的主要贡献是开发了一种资源高效的分离Transformer（RE-SepFormer），它是一种基于自注意力的架构，通过两种方式减轻计算负担。首先，它在潜空间中使用非重叠的块。其次，它对每个块计算的紧凑潜变量摘要进行操作。RE-SepFormer在流行的WSJ0-2Mix和WHAM！数据集上的因果和非因果设置下均取得了竞争性能。值得注意的是，它在内存和推断时间方面比先前基于Transformer的架构有显著的扩展性，更适用于处理长的混合音频。

    Transformers have recently achieved state-of-the-art performance in speech separation. These models, however, are computationally demanding and require a lot of learnable parameters. This paper explores Transformer-based speech separation with a reduced computational cost. Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer), a self-attention-based architecture that reduces the computational burden in two ways. First, it uses non-overlapping blocks in the latent space. Second, it operates on compact latent summaries calculated from each chunk. The RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and WHAM! datasets in both causal and non-causal settings. Remarkably, it scales significantly better than the previous Transformer-based architectures in terms of memory and inference time, making it more suitable for processing long mixtures.
    
[^163]: 张量对张量回归: 黎曼优化，过参数化，统计计算差异及其相互作用

    Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay. (arXiv:2206.08756v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2206.08756](http://arxiv.org/abs/2206.08756)

    张量对张量回归问题中，我们提出了黎曼优化方法和秩过参数化的研究，并展示了黎曼优化方法的线性和二次收敛性以及适应过参数化的能力。同时，我们证明了标量对张量回归中的统计计算差异。

    

    我们研究了张量对张量回归问题，其目标是在不知道其内在秩的情况下，将张量响应与张量协变量相连接，通过低Tucker秩参数张量/矩阵。我们提出了黎曼梯度下降（RGD）和黎曼高斯牛顿（RGN）方法，并通过研究秩过参数化的影响来应对未知秩的挑战。我们首次提供了关于一般张量对张量回归的收敛性保证，表明RGD和RGN分别在正确参数化和过参数化设置下线性和二次收敛到统计上的最优估计。我们的理论揭示了一个有趣的现象：黎曼优化方法在不修改实现方式的情况下自然地适应过参数化。我们还通过直接的低次多项式论证证明了标量对张量回归中的统计计算差异。我们的理论证明了统计学的福音。

    We study the tensor-on-tensor regression, where the goal is to connect tensor responses to tensor covariates with a low Tucker rank parameter tensor/matrix without the prior knowledge of its intrinsic rank. We propose the Riemannian gradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with the challenge of unknown rank by studying the effect of rank over-parameterization. We provide the first convergence guarantee for the general tensor-on-tensor regression by showing that RGD and RGN respectively converge linearly and quadratically to a statistically optimal estimate in both rank correctly-parameterized and over-parameterized settings. Our theory reveals an intriguing phenomenon: Riemannian optimization methods naturally adapt to over-parameterization without modifications to their implementation. We also prove the statistical-computational gap in scalar-on-tensor regression by a direct low-degree polynomial argument. Our theory demonstrates a "blessing of statist
    
[^164]: 具有预条件更新的随机梯度方法

    Stochastic Gradient Methods with Preconditioned Updates. (arXiv:2206.00285v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.00285](http://arxiv.org/abs/2206.00285)

    本文介绍了一种具有预条件更新的随机梯度方法，包括了基于Hutchinson方法的预条件器和几种梯度方法，能够有效解决尺度不好和条件不良问题，并在光滑性和PL条件下证明了线性收敛性。

    

    本文考虑了非凸有限求和最小化问题。目前已有一些算法用于解决这类问题，但当问题的尺度和/或条件不良时，现有方法通常工作效果较差。因此，本研究的主要目标是引入一种能够解决这个问题的方法。因此，本文采用基于Hutchinson方法近似Hessian对角元素的预条件器，并将其与几种基于梯度的方法相结合，得到了新的尺度算法：Scaled SARAH和Scaled L-SVRG。在光滑性假设下给出了理论复杂度保证。当同时假设光滑性和PL条件时，我们证明了线性收敛性。我们的自适应尺度方法使用了近似的二阶曲率信息，因此可以更好地减轻尺度不好的问题的影响。这种改进的实际性能在本文中的数值实验中也得到了验证。

    This work considers the non-convex finite sum minimization problem. There are several algorithms for such problems, but existing methods often work poorly when the problem is badly scaled and/or ill-conditioned, and a primary goal of this work is to introduce methods that alleviate this issue. Thus, here we include a preconditioner based on Hutchinson's approach to approximating the diagonal of the Hessian, and couple it with several gradient-based methods to give new scaled algorithms: Scaled SARAH and Scaled L-SVRG. Theoretical complexity guarantees under smoothness assumptions are presented. We prove linear convergence when both smoothness and the PL condition are assumed. Our adaptively scaled methods use approximate partial second-order curvature information and, therefore, can better mitigate the impact of badly scaled problems. This improved practical performance is demonstrated in the numerical experiments also presented in this work.
    
[^165]: leave-one-out奇异子空间扰动分析用于谱聚类

    Leave-one-out Singular Subspace Perturbation Analysis for Spectral Clustering. (arXiv:2205.14855v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2205.14855](http://arxiv.org/abs/2205.14855)

    该论文提出了一种leave-one-out奇异子空间扰动分析方法，通过建立扰动上界来测量两个矩阵的奇异子空间之间的距离。该方法适用于混合模型，相较于传统的扰动界限具有更尖锐和精细的统计分析能力。在亚高斯混合模型下，通过此方法进行的谱聚类具有显式的指数误差率，且在较弱的信噪比条件下优于之前的研究成果。

    

    奇异子空间扰动理论在概率与统计学中具有基础重要性，并在不同领域有着各种应用。我们考虑了两个任意的矩阵，其中一个是另一个矩阵的leave-one-column-out子矩阵，并为两个相应的奇异子空间之间的距离建立了一种新的扰动上界。该方法非常适用于混合模型，并产生比经典扰动界限（如Wedin定理）更尖锐和精细的统计分析。基于这种leave-one-out扰动理论，我们对混合模型下谱聚类的性能进行了确定性的逐元素分析。我们的分析得出了亚高斯混合模型谱聚类的指数误差率。对于各向同性高斯混合模型，该速率在较弱的信噪比条件下优于L{\"o}ffler等人的速率（2021）。

    The singular subspaces perturbation theory is of fundamental importance in probability and statistics. It has various applications across different fields. We consider two arbitrary matrices where one is a leave-one-column-out submatrix of the other one and establish a novel perturbation upper bound for the distance between the two corresponding singular subspaces. It is well-suited for mixture models and results in a sharper and finer statistical analysis than classical perturbation bounds such as Wedin's Theorem. Empowered by this leave-one-out perturbation theory, we provide a deterministic entrywise analysis for the performance of spectral clustering under mixture models. Our analysis leads to an explicit exponential error rate for spectral clustering of sub-Gaussian mixture models. For the mixture of isotropic Gaussians, the rate is optimal under a weaker signal-to-noise condition than that of L{\"o}ffler et al. (2021).
    
[^166]: 上下文情境下的潘多拉之盒问题

    Contextual Pandora's Box. (arXiv:2205.13114v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13114](http://arxiv.org/abs/2205.13114)

    本研究提出了带有上下文的在线情境下的潘多拉之盒问题，并设计了一种无遗憾算法，能够在不完全了解先前分布的情况下实现与最优算法相当的性能。

    

    潘多拉之盒是一个基本的随机优化问题，决策者需要找到一个好的选择，同时最小化探索每个选择价值的搜索成本。在最初的定义中，假定所有选择的价值的准确分布已知，而最近的研究则研究了潘多拉之盒的在线变体，其中最初未知选择的分布情况。在本研究中，我们研究了带有上下文的在线情境下的潘多拉之盒问题。在每一轮中，我们会展示一些具有上下文、探索成本和未知价值的选择，这些未知价值是从可能在每一轮发生变化的未知分布中抽取的。我们的主要结果是一种无遗憾算法，其性能与完全了解所有先前分布的最优算法相当。我们的算法甚至适用于只有部分选择未被探索的托管设置。关键的技术是...

    Pandora's Box is a fundamental stochastic optimization problem, where the decision-maker must find a good alternative while minimizing the search cost of exploring the value of each alternative. In the original formulation, it is assumed that accurate distributions are given for the values of all the alternatives, while recent work studies the online variant of Pandora's Box where the distributions are originally unknown. In this work, we study Pandora's Box in the online setting, while incorporating context. At every round, we are presented with a number of alternatives each having a context, an exploration cost and an unknown value drawn from an unknown distribution that may change at every round. Our main result is a no-regret algorithm that performs comparably well to the optimal algorithm which knows all prior distributions exactly. Our algorithm works even in the bandit setting where the algorithm never learns the values of the alternatives that were not explored. The key techniq
    
[^167]: 公正游戏：对强化学习的挑战

    Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12787](http://arxiv.org/abs/2205.12787)

    AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.

    

    类似AlphaZero的强化学习算法在各种棋盘游戏中表现出色，但在公正游戏中却面临挑战，这些游戏中玩家共享棋子。我们提供了一个具体的游戏例子，即小孩们玩的尼姆游戏，以及其他一些公正游戏，这些游戏似乎成为AlphaZero和类似的强化学习算法的绊脚石。我们的发现与最近的研究一致，表明AlphaZero-style算法容易受到敌对攻击和敌对扰动的影响，显示了在所有合法状态下学习掌握这些游戏的困难。我们发现尼姆游戏在小型棋盘上可以学习，但当棋盘尺寸增大时，AlphaZero-style算法的学习速度显著减慢。直观上，尼姆等公正游戏与象棋和围棋等党派游戏之间的区别在于，如果系统中添加了微小的噪音（例如，棋盘的一小部分被覆盖），对于公正游戏来说，这是一种典型的情况。

    AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
    
[^168]: 多分辨率偏微分方程保留学习框架用于时空动态

    Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics. (arXiv:2205.03990v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.03990](http://arxiv.org/abs/2205.03990)

    本论文提出了一种利用物理先验知识的多分辨率偏微分方程保留学习框架，通过将离散化的控制方程嵌入神经网络架构中，提高了泛化能力和长期预测的准确性。

    

    传统的数据驱动深度学习模型在复杂物理过程中往往面临高训练成本、误差积累和泛化能力差的问题。物理启发式深度学习（PiDL）通过将物理原理融入模型来解决这些挑战。大多数PiDL方法通过将控制方程嵌入损失函数来进行训练的规范化，但这在很大程度上依赖于大量的超参数调整来权衡每个损失项。为此，我们提出利用物理先验知识，通过偏微分方程（PDE）算子和网络结构之间的连接，将离散化的控制方程“烘烤”到神经网络架构中，从而形成一个保留PDE的神经网络（PPNN）。这种方法通过在多分辨率环境中将离散化的PDE嵌入卷积残差网络中，大大提高了泛化能力和长期预测的准确性，优于传统的黑盒模型。

    Traditional data-driven deep learning models often struggle with high training costs, error accumulation, and poor generalizability in complex physical processes. Physics-informed deep learning (PiDL) addresses these challenges by incorporating physical principles into the model. Most PiDL approaches regularize training by embedding governing equations into the loss function, yet this depends heavily on extensive hyperparameter tuning to weigh each loss term. To this end, we propose to leverage physics prior knowledge by ``baking'' the discretized governing equations into the neural network architecture via the connection between the partial differential equations (PDE) operators and network structures, resulting in a PDE-preserved neural network (PPNN). This method, embedding discretized PDEs through convolutional residual networks in a multi-resolution setting, largely improves the generalizability and long-term prediction accuracy, outperforming conventional black-box models. The ef
    
[^169]: 从激励角度理解CNNs

    Understanding CNNs from excitations. (arXiv:2205.00932v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.00932](http://arxiv.org/abs/2205.00932)

    这项研究提出了一种从激励角度理解CNNs的方法，通过提取正负激励并引入双链反向传播的方法，实现了无梯度的逐层信息利用，有效提高了解释准确性。

    

    显著性图已被证明是一种解释卷积神经网络决策的高效方法。然而，现有的方法主要依赖于梯度，限制了它们解释复杂模型的能力。此外，这些方法在利用负梯度信息来提高解释准确性方面并不完全熟练。在本研究中，我们提出了一种新的概念，称之为正负激励，它能够直接提取每个层的正负激励，从而实现完全无梯度的逐层信息利用。为了将这些激励组织成最终的显著性图，我们引入了一种双链反向传播的方法。通过对二分类和多分类任务的综合实验评估，我们验证了我们提出的方法的有效性。令人鼓舞的是，结果表明我们的方法在解释准确性方面有显著改善。

    Saliency maps have proven to be a highly efficacious approach for explicating the decisions of Convolutional Neural Networks. However, extant methodologies predominantly rely on gradients, which constrain their ability to explicate complex models. Furthermore, such approaches are not fully adept at leveraging negative gradient information to improve interpretive veracity. In this study, we present a novel concept, termed positive and negative excitation, which enables the direct extraction of positive and negative excitation for each layer, thus enabling complete layer-by-layer information utilization sans gradients. To organize these excitations into final saliency maps, we introduce a double-chain backpropagation procedure. A comprehensive experimental evaluation, encompassing both binary classification and multi-classification tasks, was conducted to gauge the effectiveness of our proposed method. Encouragingly, the results evince that our approach offers a significant improvement o
    
[^170]: 自监督图神经网络用于多源域适应

    Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation. (arXiv:2204.05104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.05104](http://arxiv.org/abs/2204.05104)

    该论文介绍了一种自监督图神经网络方法，用于解决多源域适应问题。该方法利用大规模无标签样本进行学习，并尝试在预训练任务和下游任务之间共享有用知识。然而，传统方法面临预训练任务与下游任务关联性不强以及信息交换无效的挑战。

    

    域适应试图解决测试数据与训练数据不完全遵循相同分布的情况，而多源域适应对于实际应用非常有吸引力。通过从大规模无标签样本中学习，自监督学习现在已经成为深度学习中的一个新趋势。值得注意的是，自监督学习和多源域适应有一个相似的目标：它们都旨在利用无标签数据学习更具表现力的表示。不幸的是，传统的多任务自监督学习面临两个挑战：（1）预训练任务可能与下游任务关联不强，因此很难从预训练任务中学习到与目标任务共享的有用知识；（2）当相同的特征提取器在预训练任务和下游任务之间共享，并且只使用不同的预测头时，使任务间的信息交换变得无效。

    Domain adaptation (DA) tries to tackle the scenarios when the test data does not fully follow the same distribution of the training data, and multi-source domain adaptation (MSDA) is very attractive for real world applications. By learning from large-scale unlabeled samples, self-supervised learning has now become a new trend in deep learning. It is worth noting that both self-supervised learning and multi-source domain adaptation share a similar goal: they both aim to leverage unlabeled data to learn more expressive representations. Unfortunately, traditional multi-task self-supervised learning faces two challenges: (1) the pretext task may not strongly relate to the downstream task, thus it could be difficult to learn useful knowledge being shared from the pretext task to the target task; (2) when the same feature extractor is shared between the pretext task and the downstream one and only different prediction heads are used, it is ineffective to enable inter-task information exchang
    
[^171]: 改进的分布式和联邦学习中的信息论泛化界限

    Improved Information Theoretic Generalization Bounds for Distributed and Federated Learning. (arXiv:2202.02423v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2202.02423](http://arxiv.org/abs/2202.02423)

    该论文研究了分布式和联邦学习中的信息论泛化界限，提出了对于具有通信或隐私约束的问题，与节点数量成反比的改进上界。

    

    我们考虑了网络环境下统计学习问题的信息论界限，该环境中有K个节点，每个节点都有自己的独立数据集，并且每个节点的模型必须被聚合成一个最终的集中模型。我们考虑了简单的模型平均以及更复杂的多轮算法。我们给出了一系列问题的期望泛化误差的上界，例如具有Bregman散度或Lipschitz连续损失的问题，在节点数量上展示了对1/K的改进依赖。这些“每个节点”的界限是关于训练数据集与每个节点训练权重之间的互信息，因此对于描述在每个节点具有通信或隐私约束的泛化性能是有用的。

    We consider information-theoretic bounds on expected generalization error for statistical learning problems in a networked setting. In this setting, there are $K$ nodes, each with its own independent dataset, and the models from each node have to be aggregated into a final centralized model. We consider both simple averaging of the models as well as more complicated multi-round algorithms. We give upper bounds on the expected generalization error for a variety of problems, such as those with Bregman divergence or Lipschitz continuous losses, that demonstrate an improved dependence of $1/K$ on the number of nodes. These "per node" bounds are in terms of the mutual information between the training dataset and the trained weights at each node, and are therefore useful in describing the generalization properties inherent to having communication or privacy constraints at each node.
    
[^172]: 随机重排的SARAH算法不需要进行完整的梯度计算

    Random-reshuffled SARAH does not need a full gradient computations. (arXiv:2111.13322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.13322](http://arxiv.org/abs/2111.13322)

    本文提出了一种使用随机重排策略和聚合随机梯度的方法，可以在不进行完整梯度计算的情况下实现随机递归梯度算法（SARAH），并通过理论分析和数值实验证明了该方法的效率。

    

    随机递归梯度算法（SARAH）是随机梯度下降（SGD）算法的一种方差缩减变体，需要定期计算目标函数的梯度。在本文中，我们消除了进行完整梯度计算的必要性。通过使用随机重排策略和在每个时期获得的随机梯度的聚合，实现了这一目标。聚合的随机梯度在SARAH算法中作为完整梯度的估计。我们对所提出方法进行了理论分析，并通过数值实验证明了该方法的效率。

    The StochAstic Recursive grAdient algoritHm (SARAH) algorithm is a variance reduced variant of the Stochastic Gradient Descent (SGD) algorithm that needs a gradient of the objective function from time to time. In this paper, we remove the necessity of a full gradient computation. This is achieved by using a randomized reshuffling strategy and aggregating stochastic gradients obtained in each epoch. The aggregated stochastic gradients serve as an estimate of a full gradient in the SARAH algorithm. We provide a theoretical analysis of the proposed approach and conclude the paper with numerical experiments that demonstrate the efficiency of this approach.
    
[^173]: 在单路径中将自注意力修剪到卷积层中

    Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.11802](http://arxiv.org/abs/2111.11802)

    本文提出了一种名为SPViT的方法，将预训练的ViTs压缩为紧凑的模型，并通过权重共享方案将自注意力层和卷积操作相结合，以解决计算资源消耗和建模本地视觉模式的问题。

    

    视觉Transformer（ViTs）在各种计算机视觉任务中取得了令人印象深刻的性能。然而，使用多头自注意（MSA）层建模全局关联存在两个广为认可的问题：巨大的计算资源消耗和缺乏对建模本地视觉模式的内在归纳偏差。为了解决这两个问题，我们设计了一种简单而有效的方法，称为单路径视觉Transformer修剪（SPViT），将预训练的ViTs压缩为具有适当局部性的紧凑模型。具体而言，我们首先提出了一种新颖的MSA和卷积操作之间的权重共享方案，提供了一个单路径空间来编码所有候选操作。通过这种方式，我们将操作搜索问题转化为在每个MSA层中找到要使用的参数子集，这显著减少了计算成本和优化难度，并且可以使用预先初始化的卷积核。

    Vision Transformers (ViTs) have achieved impressive performance over various computer vision tasks. However, modeling global correlations with multi-head self-attention (MSA) layers leads to two widely recognized issues: the massive computational resource consumption and the lack of intrinsic inductive bias for modeling local visual patterns. To solve both issues, we devise a simple yet effective method named Single-Path Vision Transformer pruning (SPViT), to efficiently and automatically compress the pre-trained ViTs into compact models with proper locality added. Specifically, we first propose a novel weight-sharing scheme between MSA and convolutional operations, delivering a single-path space to encode all candidate operations. In this way, we cast the operation search problem as finding which subset of parameters to use in each MSA layer, which significantly reduces the computational cost and optimization difficulty, and the convolution kernels can be well initialized using pre-tr
    
[^174]: 对比主动推断

    Contrastive Active Inference. (arXiv:2110.10083v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.10083](http://arxiv.org/abs/2110.10083)

    该论文提出了一种对比主动推断方法，通过降低计算负担来学习代理的生成模型和规划未来行动。这种方法在图像任务中表现出色，并且计算成本较低且易于训练。

    

    主动推断是一个关于感知和行动的统一理论，其基本思想是通过最小化自由能来维持大脑对世界的内部模型。从行为学角度来看，主动推断代理可以被看作是自证明的存在，他们行动以实现他们乐观的预测，即优选结果或目标。相比之下，强化学习需要人工设计的奖励来实现任何期望的结果。虽然主动推断可以为控制提供一种更自然的自监督目标，但其适用性受到了在复杂环境中扩展方法的局限性。在这项工作中，我们提出了一种对比目标，用于主动推断，可以显著减少学习代理生成模型和规划未来行动的计算负担。我们的方法在基于图像的任务中比基于似然的主动推断表现更好，同时计算成本更低且更易于训练。

    Active inference is a unifying theory for perception and action resting upon the idea that the brain maintains an internal model of the world by minimizing free energy. From a behavioral perspective, active inference agents can be seen as self-evidencing beings that act to fulfill their optimistic predictions, namely preferred outcomes or goals. In contrast, reinforcement learning requires human-designed rewards to accomplish any desired outcome. Although active inference could provide a more natural self-supervised objective for control, its applicability has been limited because of the shortcomings in scaling the approach to complex environments. In this work, we propose a contrastive objective for active inference that strongly reduces the computational burden in learning the agent's generative model and planning future actions. Our method performs notably better than likelihood-based active inference in image-based tasks, while also being computationally cheaper and easier to train
    
[^175]: 优化可解释性：卷积动态对齐网络

    Optimising for Interpretability: Convolutional Dynamic Alignment Networks. (arXiv:2109.13004v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.13004](http://arxiv.org/abs/2109.13004)

    CoDA Nets是一种性能良好的分类器，具有高度内在可解释性。它们通过动态对齐单元实现输入依赖的线性变换，并将输出线性分解为各个输入的贡献。这些模型在视觉质量和分类准确度上优于现有方法，并且在CIFAR-10和TinyImagenet等数据集上表现出与ResNet和VGG模型相媲美的性能。

    

    我们引入了一种新的神经网络模型，称为卷积动态对齐网络（CoDA Nets），它是一种具有高度内在可解释性的性能分类器。它们的核心构建模块是动态对齐单元（DAUs），其经过优化后能够通过动态计算的权重向量将其输入转换为与任务相关模式对齐的形式。因此，CoDA Nets通过一系列依赖于输入的线性变换来模拟分类预测，允许将输出线性分解为各个输入的贡献。根据DAUs的对齐情况，得到的贡献映射与鉴别性输入模式相一致。这些模型固有的分解具有很高的视觉质量，在定量指标下优于现有的归因方法。此外，CoDA Nets是性能出色的分类器，在CIFAR-10和TinyImagenet等数据集上取得了与ResNet和VGG模型相媲美的结果。

    We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which are optimised to transform their inputs with dynamically computed weight vectors that align with task-relevant patterns. As a result, CoDA Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly, CoDA Nets can be combin
    
[^176]: SubseasonalClimateUSA: 用于亚季节预测和基准测试的数据集。

    SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking. (arXiv:2109.10399v3 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2109.10399](http://arxiv.org/abs/2109.10399)

    这个论文介绍了SubseasonalClimateUSA，这是一个用于训练和基准测试美国的亚季节预测模型的数据集。作者使用该数据集对多种模型进行了基准测试。

    

    天气的亚季节预测对资源配置和气候适应至关重要，但对预测社区提出了许多挑战。在这个预测时间范围内，基于物理的动力学模型的技能有限，并且预测目标以一种复杂的方式依赖于本地天气和全球气候变量。最近，机器学习方法显示出推进技术的潜力，但需要复杂的数据整理，将专家知识与多个相关数据来源、文件格式和时间空间分辨率的聚合进行整合。为了简化这个过程并加速未来的发展，我们介绍了SubseasonalClimateUSA，这是一个经过策划的数据集，用于训练和基准测试美国的亚季节预测模型。我们使用这个数据集来对各种不同的亚季节模型进行基准测试，包括操作性动力学模型、古典的气象基线以及十个统计模型。

    Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and climate adaptation but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten sta
    
[^177]: 谷歌趋势和多模态信息在新时尚产品销量预测中的应用：多模态基于图像的谷歌趋势

    Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v6 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.09824](http://arxiv.org/abs/2109.09824)

    本论文研究了在新时尚产品销量预测中结合谷歌趋势和多模态信息的有效性，提出了一种基于神经网络的非自回归模型，并提供了一个公开可用的数据集。通过将谷歌趋势编码与视觉和元数据信息相结合，我们能够有效预测缺乏过去数据的新时尚产品销量。

    

    新时尚产品销量预测是一个具有挑战性的问题，涉及到许多商业动态，并且不能通过传统的预测方法来解决。本文研究了系统地探索外部知识的有效性，以谷歌趋势时间序列的形式结合与全新时尚产品相关的多模态信息，从而在缺乏过去数据的情况下有效预测其销量。具体而言，我们提出了基于神经网络的方法，在编码器学习外部时间序列的表示的同时，解码器根据谷歌趋势编码和可用的视觉和元数据信息来预测销量。我们的模型以非自回归的方式工作，避免了大的第一步误差的累积效应。作为第二个贡献，我们提供了一个名为VISUELLE的公开可用数据集，用于新时尚产品销量预测任务，其中包含5577个真实的、新的产品的多模态信息。

    New fashion product sales forecasting is a challenging problem that involves many business dynamics and cannot be solved by classical forecasting approaches. In this paper, we investigate the effectiveness of systematically probing exogenous knowledge in the form of Google Trends time series and combining it with multi-modal information related to a brand-new fashion item, in order to effectively forecast its sales despite the lack of past data. In particular, we propose a neural network-based approach, where an encoder learns a representation of the exogenous time series, while the decoder forecasts the sales based on the Google Trends encoding and the available visual and metadata information. Our model works in a non-autoregressive manner, avoiding the compounding effect of large first-step errors. As a second contribution, we present VISUELLE, a publicly available dataset for the task of new fashion product sales forecasting, containing multimodal information for 5577 real, new pro
    
[^178]: 在稀疏突发神经突触下的领域转移中的持续学习

    Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.12056](http://arxiv.org/abs/2108.12056)

    本文介绍了一个系统，通过稀疏突发神经突触，在领域转移中能够在先前未见的数据集上进行持续学习，减少遗忘。

    

    现有的机器是功能特定的工具，易于预测和控制。明天的机器可能更接近生物系统，具有可变性、韧性和自主性。但是首先，它们必须能够学习和保留新信息，而不必随机接触它。过去设计这样的系统的努力是通过构建或调节人工神经网络，使用唯一敏感于特定任务或输入的不相交的权重集合。然而，这尚未实现在长时间的先前未见数据序列上进行持续学习而不破坏现有知识的问题，这被称为灾难性遗忘。在本文中，我们介绍了一个系统，可以在先前未见的数据集（ImageNet，CIFAR-100）上以较小的遗忘逐步学习。这是通过基于输入控制卷积神经网络中权重的活动，使用由第二个前馈生成的自上而下调节来实现的。

    Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning and retaining new information without being exposed to it arbitrarily often. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using disjoint sets of weights that are uniquely sensitive to specific tasks or inputs. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is done by controlling the activity of weights in a convolutional neural network on the basis of inputs using top-down regulation generated by a second feed-forwa
    
[^179]: 关于引入局部Lipschitz性质的鲁棒生成对抗模仿学习的益处

    On the Benefits of Inducing Local Lipschitzness for Robust Generative Adversarial Imitation Learning. (arXiv:2107.00116v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.00116](http://arxiv.org/abs/2107.00116)

    本研究研究了引入局部Lipschitz性质对鲁棒生成对抗模仿学习的益处。提出了一种正则化方法，通过训练具有Lipschitz性质的鉴别器和生成器，能够学习到更加鲁棒的策略，提高生成对抗模仿学习的性能。

    

    我们探索了改进生成对抗模仿学习（GAIL）算法对观测噪声的鲁棒性的方法。为了实现这个目标，我们研究了鉴别器和生成器的局部Lipschitz性质对GAIL学习的策略鲁棒性的影响。在许多机器人应用中，由于环境观测可能受到噪声的影响，GAIL学到的策略在测试时通常表现下降。因此，对抗观测噪声的学习策略具有重要意义。为此，我们提出了一种正则化方法，引入局部Lipschitz性质到生成器和鉴别器的对抗模仿学习方法中。我们证明修改后的目标导致学习到更加鲁棒的策略。此外，我们理论上和实验上证明了训练一个局部Lipschitz鉴别器会导致一个局部Lipschitz生成器，从而改善了生成对抗模仿学习的性能。

    We explore methodologies to improve the robustness of generative adversarial imitation learning (GAIL) algorithms to observation noise. Towards this objective, we study the effect of local Lipschitzness of the discriminator and the generator on the robustness of policies learned by GAIL. In many robotics applications, the learned policies by GAIL typically suffer from a degraded performance at test time since the observations from the environment might be corrupted by noise. Hence, robustifying the learned policies against the observation noise is of critical importance. To this end, we propose a regularization method to induce local Lipschitzness in the generator and the discriminator of adversarial imitation learning methods. We show that the modified objective leads to learning significantly more robust policies. Moreover, we demonstrate -- both theoretically and experimentally -- that training a locally Lipschitz discriminator leads to a locally Lipschitz generator, thereby improvi
    
[^180]: 可解释分类的卷积动态对齐网络

    Convolutional Dynamic Alignment Networks for Interpretable Classifications. (arXiv:2104.00032v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.00032](http://arxiv.org/abs/2104.00032)

    这项研究介绍了一种新的神经网络模型CoDA-Nets，它是一种具有高度可解释性的性能分类器。CoDA-Nets使用动态对齐单元对输入进行线性变换，并通过线性分解将输出解释为单个输入贡献。此外，CoDA-Nets在分类任务上取得了与ResNet和VGG模型相当的结果。

    

    我们引入了一种称为卷积动态对齐网络（CoDA-Nets）的新型神经网络模型，它是一种具有高度内在可解释性的性能分类器。它们的核心构建模块是动态对齐单元（DAUs），它们使用权重向量线性变换其输入，并与任务相关模式动态对齐。因此，CoDA-Nets通过一系列依赖于输入的线性变换对分类预测建模，允许线性分解输出为单个输入贡献。根据DAUs的对齐，得到的贡献图与具有鉴别性的输入模式对齐。这些模型固有的分解具有高质量的可视化效果，并在定量指标下胜过现有的归因方法。此外，CoDA-Nets是一种性能良好的分类器，在CIFAR-10和TinyImagenet等数据集上取得与ResNet和VGG模型相当的结果。

    We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which linearly transform their input with weight vectors that dynamically align with task-relevant patterns. As a result, CoDA-Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA-Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet.
    
[^181]: 追踪代码异味 - 哪些代码异味值得追踪？

    Follow Your Nose -- Which Code Smells are Worth Chasing?. (arXiv:2103.01861v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2103.01861](http://arxiv.org/abs/2103.01861)

    这项研究通过实证研究了代码异味对代码质量和开发生产力的影响，并发现仅有少数代码异味是真正具有因果关系的，其中与简单性、防御性编程和抽象相关的异味是最强烈的。开发人员倾向于去除容易的异味而不是真正有效的异味。

    

    代码异味的常见用例认为存在因果关系：识别异味，去除它，从而改善代码。我们对其适应性进行了实证研究。我们提出了一系列代码异味的特性，如果它们确实导致较低的代码质量，那么它们应该具备这些特性。我们对来自677个GitHub存储库中31,687个Java文件进行了评估，这些存储库在2019年有200个以上的提交。我们测量了这些异味对质量、生产力和错误检测效率的影响。通过CheckStyle异味检测器计算出的151个代码异味中，只有不到20%被发现可能引起质量下降，并且只有一小部分相对稳定。最强烈的异味与简单性、防御性编程和抽象有关。没有可能引起质量下降的异味的文件更有可能具有较高的质量，概率增加了50%。不幸的是，大多数异味没有被去除，开发人员倾向于去除容易的异味而不是有效的异味。

    The common use case of code smells assumes causality: Identify a smell, remove it, and by doing so improve the code. We empirically investigate their fitness to this use. We present a list of properties that code smells should have if they indeed cause lower quality. We evaluated the smells in 31,687 Java files from 677 GitHub repositories, all the repositories with 200+ commits in 2019. We measured the influence of smells on four metrics for quality, productivity, and bug detection efficiency. Out of 151 code smells computed by the CheckStyle smell detector, less than 20% were found to be potentially causal, and only a handful are rather robust. The strongest smells deal with simplicity, defensive programming, and abstraction. Files without the potentially causal smells are 50% more likely to be of high quality. Unfortunately, most smells are not removed, and developers tend to remove the easy ones and not the effective ones.
    
[^182]: 使用动态容量槽注意力从字符序列中诱导有意义的单元

    Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention. (arXiv:2102.01223v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2102.01223](http://arxiv.org/abs/2102.01223)

    该论文提出了一种无监督的分布式方法，使用动态容量槽注意力模型从字符序列中学习抽象有意义单元，成功发现了与先前提出的单元相似的、适用于更高级别抽象的可捕捉有意义信息的单元。

    

    字符本身并不传达意义，但字符序列却可以。我们提出了一种无监督的分布式方法，用于学习字符序列中的抽象有意义单元。我们的动态容量槽注意力模型不是对序列进行分割，而是发现序列中对象的连续表示，扩展了图像中对象发现的架构。我们对不同语言训练模型，并使用正向和反向探测分类器评估所得到表示的质量。实验证明，我们的模型成功地发现了与先前提出的单元在形式、内容和抽象级别上相似的单元，并显示了在更高级别的抽象中捕捉有意义信息的潜力。

    Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaningful units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images. We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers. These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction.
    
[^183]: 双重对抗激活异常检测：对抗自编码器是异常生成器

    Double-Adversarial Activation Anomaly Detection: Adversarial Autoencoders are Anomaly Generators. (arXiv:2101.04645v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.04645](http://arxiv.org/abs/2101.04645)

    该论文提出了一种名为DA3D的无监督异常检测方法，利用对抗自编码器生成基于正常数据的异常反例，从而实现在没有领域知识的情况下超越最先进的异常检测方法的性能。

    

    异常检测是机器学习算法面临的挑战之一，这是由于固有的类别不平衡性所导致的。由于手动分析观察数据是昂贵且耗时的，因此通常只有一些已知的异常，如果有的话。受到生成模型和神经网络隐藏激活分析的启发，我们引入了一种名为DA3D的新型无监督异常检测方法。在这里，我们使用对抗自编码器仅基于正常数据生成异常的反例。训练过程中使用这些人工异常可以检测到真实但未见过的异常。通过我们的新型生成方法，我们将无监督的异常检测任务转化为一种监督任务，这对于机器学习和特别是深度学习方法更易处理。DA3D在纯数据驱动的方式下超过了最先进的异常检测方法的性能，而无需领域知识。

    Anomaly detection is a challenging task for machine learning algorithms due to the inherent class imbalance. It is costly and time-demanding to manually analyse the observed data, thus usually only few known anomalies if any are available. Inspired by generative models and the analysis of the hidden activations of neural networks, we introduce a novel unsupervised anomaly detection method called DA3D. Here, we use adversarial autoencoders to generate anomalous counterexamples based on the normal data only. These artificial anomalies used during training allow the detection of real, yet unseen anomalies. With our novel generative approach, we transform the unsupervised task of anomaly detection to a supervised one, which is more tractable by machine learning and especially deep learning methods. DA3D surpasses the performance of state-of-the-art anomaly detection methods in a purely data-driven way, where no domain knowledge is required.
    
[^184]: 对Riesz Representer的敌对估计

    Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2101.00009](http://arxiv.org/abs/2101.00009)

    我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer，并且证明了非渐近均方速率以及渐近正态性的条件。这个条件使得在机器学习中进行推断时无需样本分割，并且能够提高有限样本性能。

    

    许多因果和结构参数是基于底层回归的线性泛函。Riesz Representer是半参数线性泛函渐近方差的关键组成部分。我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer。我们证明了一个非渐近均方速率，其中涉及一个称为临界半径的抽象量，然后将其专门应用于神经网络、随机森林和再生核希尔伯特空间作为主要案例。此外，我们使用临界半径理论来证明了渐近正态性，而不需要样本分割，揭示了一种“复杂度-速率鲁棒性”条件。这个条件具有实际后果：在几个机器学习设置中，可以实现无需样本分割的推断，这可能会提高有限样本性能。我们的估计器在高度非线性的模拟中实现了名义覆盖率。

    Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo
    
[^185]: 通过正交多样性实现稳健的神经网络

    Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2010.12190](http://arxiv.org/abs/2010.12190)

    本文提出了一种通过正交多样性增强神经网络鲁棒性的方法，通过在网络中嵌入多个路径并施加正交约束，让模型学习适应不同输入的特征。

    

    深度神经网络(DNNs)对于由对抗攻击生成的图像上的看不见的扰动是脆弱的，这引发了对DNN的对抗鲁棒性的研究。一系列方法，如对抗性训练及其变种，已被证明是增强DNN鲁棒性最有效的技术之一。通常，对抗性训练侧重于通过引入扰动数据丰富训练数据。这些扰动数据的数据增强效果并不能贡献到DNN本身的鲁棒性，并且通常会导致准确率下降。为了提高DNN本身的鲁棒性，本文提出了一种新颖的防御方法，旨在通过增强模型来学习适应不同输入（包括对抗性样本）的特征。具体而言，为了增强模型，我们在网络中嵌入了多个路径，并对这些路径施加正交约束，以保证它们的多样性。

    Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the images generated by adversarial attacks, which raises researches on the adversarial robustness of DNNs. A series of methods represented by the adversarial training and its variants have proven as one of the most effective techniques in enhancing the DNN robustness. Generally, adversarial training focuses on enriching the training data by involving perturbed data. Such data augmentation effect of the involved perturbed data in adversarial training does not contribute to the robustness of DNN itself and usually suffers from clean accuracy drop. Towards the robustness of DNN itself, we in this paper propose a novel defense that aims at augmenting the model in order to learn features that are adaptive to diverse inputs, including adversarial examples. More specifically, to augment the model, multiple paths are embedded into the network, and an orthogonality constraint is imposed on these paths to guarantee the div
    
[^186]: 通过生成型随机傅立叶特征进行端到端内核学习

    End-to-end Kernel Learning via Generative Random Fourier Features. (arXiv:2009.04614v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.04614](http://arxiv.org/abs/2009.04614)

    本文提出了一种通过生成型随机傅立叶特征进行端到端内核学习的方法，将内核学习和线性学习器融合为一个统一框架，通过生成网络和线性分类器联合训练以实现更好的泛化性能。

    

    随机傅立叶特征（RFFs）为谱内核学习提供了一种有前景的方法。目前基于RFFs的内核学习方法通常以两阶段方式工作。在第一阶段过程中，学习最优特征映射通常被表述为目标对齐问题，其目标是将学习到的内核与预定义的目标内核（通常是理想内核）对齐。在第二阶段过程中，线性学习器针对映射的随机特征进行训练。然而，目标对齐中的预定义内核不一定对于线性学习器的泛化是最优的。相反，在本文中，我们考虑将内核学习和线性学习器融合为一个统一框架的一阶段过程。具体而言，我们设计了一个通过RFFs隐式学习内核的生成网络，接着使用一个全连接层参数化的线性分类器。然后，通过求解优化问题，联合训练生成网络和分类器。

    Random Fourier features (RFFs) provide a promising way for kernel learning in a spectral case. Current RFFs-based kernel learning methods usually work in a two-stage way. In the first-stage process, learning the optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with the pre-defined target kernel (usually the ideal kernel). In the second-stage process, a linear learner is conducted with respect to the mapped random features. Nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. Instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. To be specific, a generative network via RFFs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. Then the generative network and the classifier are jointly trained by solving th
    
[^187]: 将监督学习和VAEs统一在基于正态流的神经网络模型中对天文粒子重建进行覆盖、系统性和拟合好坏的研究

    Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions. (arXiv:2008.05825v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.05825](http://arxiv.org/abs/2008.05825)

    本研究将监督学习和VAEs统一于基于正态流的神经网络模型中，对天文粒子重建进行了覆盖、系统性和拟合好坏的研究，并通过KL散度目标实现了监督学习和VAEs的统一。利用条件正态化流的方法可以计算神经网络模型的拟合优度p值。

    

    在天文粒子物理学中，基于神经网络的事件属性预测变得越来越常见。然而，在许多情况下，结果只被用作点预测。统计不确定性和覆盖率(1)，系统不确定性(2)或拟合优度度量(3)经常没有被计算。在这里，我们描述了一种特定的训练和网络架构选择，可以将所有这些属性融入到一个单一的网络模型中。我们展示了数据和标签联合分布的KL散度目标使得在随机变分推理的一种统一下将监督学习和变分自编码器(VAEs)统一起来。这种统一性激发了一种扩展的监督学习方案，可以计算神经网络模型的拟合优度p值。在这种建设中，利用神经网络进行的条件正态化流至关重要。我们讨论了它们如何为已定义的后验分布严格定义覆盖率。

    Neural-network based predictions of event properties in astro-particle physics are getting more and more common. However, in many cases the result is just utilized as a point prediction. Statistical uncertainties and coverage (1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often not calculated. Here we describe a certain choice of training and network architecture that allows to incorporate all these properties into a single network model. We show that a KL-divergence objective of the joint distribution of data and labels allows to unify supervised learning and variational autoencoders (VAEs) under one umbrella of stochastic variational inference. The unification motivates an extended supervised learning scheme which allows to calculate a goodness-of-fit p-value for the neural network model. Conditional normalizing flows amortized with a neural network are crucial in this construction. We discuss how they allow to rigorously define coverage for posteriors defined
    
[^188]: 关于偏压压缩在分布式学习中的应用

    On Biased Compression for Distributed Learning. (arXiv:2002.12410v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.12410](http://arxiv.org/abs/2002.12410)

    本研究研究了偏压压缩在分布式学习中的应用，首次证明了偏压压缩器可以在单节点和分布式环境中实现线性收敛速率。

    

    近年来，各种通信压缩技术作为分布式学习中缓解通信瓶颈的必不可少的工具而出现。然而，尽管偏压压缩器在实践中往往表现出比被广泛研究和理解的无偏压压缩器更好的性能，但对它们的了解非常有限。在本研究中，我们研究了三类偏压压缩算子，其中两类是新的，并研究了它们在（随机）梯度下降和分布式（随机）梯度下降中的性能。我们首次证明了偏压压缩器可以在单节点和分布式环境中实现线性收敛速率。我们证明了经过错误反馈机制处理的分布式压缩的SGD方法具有遗传速率$O\left( \delta L \exp \left[-\frac{\mu K}{\delta L}\right] + \frac{(C + \delta D)}{K\mu}\right)$，其中$\delta\ge 1$是一个逐渐增长的压缩参数，m未完待续

    In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. We prove that distributed compressed SGD method, employed with error feedback mechanism, enjoys the ergodic rate $O\left( \delta L \exp \left[-\frac{\mu K}{\delta L}\right] + \frac{(C + \delta D)}{K\mu}\right)$, where $\delta\ge 1$ is a compression parameter which grows when m
    
[^189]: PVNet: 基于数值天气预报的时空光伏发电功率预测的 LRCN 架构

    PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic PowerForecasting from Numerical Weather Prediction. (arXiv:1902.01453v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.01453](http://arxiv.org/abs/1902.01453)

    PVNet是一种利用数值天气预报数据的LRCN架构，用于预测24小时和48小时的光伏发电功率。该模型充分利用了时间和空间天气数据，通过与其他模型的比较，展现了较好的性能。

    

    光伏发电是一种重要的可再生能源之一，然而其产量受天气条件（如太阳辐射和温度）的影响而具有高度的不确定性。即使是在24小时预测中，光伏发电的预测仍然是一个挑战，导致能源供应商需要启动（往往还会排放碳）的发电厂。在本文中，我们介绍了一种利用数值天气预报（NWP）的长期循环卷积网络，用于预测24小时和48小时的光伏发电预测。该网络架构充分利用了整个感兴趣地区上采样的时间和空间天气数据。我们使用美国国家海洋和大气管理局（NOAA）的NWP数据集对模型进行训练，以预测德国的空间聚合光伏发电量，并将其性能与持续模型和最先进的方法进行了比较。

    Photovoltaic (PV) power generation has emerged as one of the lead renewable energy sources. Yet, its production is characterized by high uncertainty, being dependent on weather conditions like solar irradiance and temperature. Predicting PV production, even in the 24-hour forecast, remains a challenge and leads energy providers to left idling - often carbon emitting - plants. In this paper, we introduce a Long-Term Recurrent Convolutional Network using Numerical Weather Predictions (NWP) to predict, in turn, PV production in the 24-hour and 48-hour forecast horizons. This network architecture fully leverages both temporal and spatial weather data, sampled over the whole geographical area of interest. We train our model on an NWP dataset from the National Oceanic and Atmospheric Administration (NOAA) to predict spatially aggregated PV production in Germany. We compare its performance to the persistence model and state-of-the-art methods.
    
[^190]: 关于带动量的随机梯度下降方法的泛化性能研究

    On the Generalization of Stochastic Gradient Descent with Momentum. (arXiv:1809.04564v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1809.04564](http://arxiv.org/abs/1809.04564)

    该论文研究了带动量的随机梯度下降方法的泛化性能，并通过分析不同的损失函数形式和动量范围，提出了一种可以在多个周期内训练机器学习模型并保证泛化性能的修改后的动量更新规则。对于特殊情况下的损失函数，标准的带动量随机梯度下降方法也能够具有泛化性能。该论文还给出了对于期望真实风险的上界估计。

    

    尽管在训练机器学习模型时，基于动量的加速随机梯度下降（SGD）变种广泛应用，但对于这些方法的泛化误差几乎没有理论上的理解。在这项工作中，我们首先展示了存在一种凸损失函数，对于多个周期的标准重球动量（SGDM）SGD，其稳定间隙变得无限大。然后，对于平滑Lipschitz损失函数，我们分析了一种修改后的基于动量的更新规则，即SGD提前动量（SGDEM），在广泛的步长范围内，它可以在多个周期内训练机器学习模型并保证泛化性能。最后，对于强凸损失函数的特殊情况，我们发现了一种动量范围，使得多个周期的标准SGDM，作为SGDEM的特殊形式，也具有泛化性能。在泛化性能的基础上，我们还对期望真实风险进行了一个上界，与训练步骤数量有关。

    While momentum-based accelerated variants of stochastic gradient descent (SGD) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training step
    

