# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Deep Reinforcement Learning for Traveling Purchaser Problems](https://arxiv.org/abs/2404.02476) | 提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。 |
| [^2] | [Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics](https://arxiv.org/abs/2404.01712) | 通过提出的Hessian-free在线遗忘方法，实现了近乎瞬时的在线遗忘，仅需要进行矢量加法操作。 |
| [^3] | [Multi-granular Adversarial Attacks against Black-box Neural Ranking Models](https://arxiv.org/abs/2404.01574) | 这项研究聚焦于利用多粒度扰动生成高质量的对抗性示例，通过转化为顺序决策过程来解决组合爆炸问题。 |
| [^4] | [A Review of Graph Neural Networks in Epidemic Modeling](https://arxiv.org/abs/2403.19852) | 图神经网络在流行病建模中作为一种新工具备受关注，本文全面回顾了GNN在流行病研究中的应用，并提出了未来发展方向。 |
| [^5] | [Concept-based Analysis of Neural Networks via Vision-Language Models](https://arxiv.org/abs/2403.19837) | 本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。 |
| [^6] | [A Multi-Label Dataset of French Fake News: Human and Machine Insights](https://arxiv.org/abs/2403.16099) | 通过建立一份包括 100 篇文档的多标签数据集 OBSINFOX，研究了人类与机器在认定假新闻特征上的差异，并发现了语料库中讽刺文本的普遍存在。 |
| [^7] | [KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing](https://arxiv.org/abs/2403.15304) | KTbench提出了一种无数据泄漏的知识追踪框架，解决了KT模型中KC之间的相关性学习可能导致的性能下降问题。 |
| [^8] | [Provable Privacy with Non-Private Pre-Processing](https://arxiv.org/abs/2403.13041) | 提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限 |
| [^9] | [S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering](https://arxiv.org/abs/2403.09107) | S^2MVTC提出了一种简单而高效的可扩展多视图张量聚类方法，重点是学习跨视图内部和视图间的嵌入特征之间的相关性。 |
| [^10] | [Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks](https://arxiv.org/abs/2403.04814) | 该研究引入了一个新的基准SAFIM用于评估LLMs在代码填空任务上的句法感知完成表现，发现FIM预训练不仅提高了FIM的熟练度，还改善了LLMs的左到右推理，挑战了传统观念并表明预训练方法和数据品质对模型性能的影响更甚于模型大小。 |
| [^11] | [Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities](https://arxiv.org/abs/2403.02004) | 证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。 |
| [^12] | [Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits](https://arxiv.org/abs/2403.01317) | 提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。 |
| [^13] | [GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis](https://arxiv.org/abs/2402.16994) | GEM3D提出了一种新的深度、拓扑感知的三维形状生成模型，通过神经骨架编码了拓扑和几何信息，通过骨架驱动的神经隐式公式生成准确和多样化的表面。 |
| [^14] | [Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales](https://arxiv.org/abs/2402.15430) | 通过分层不变性构建稳健且可解释的视觉系统，克服了不变性表示在较大规模的视觉任务中可区分性有限的问题。 |
| [^15] | [FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning](https://arxiv.org/abs/2402.13989) | 提出了一种不精确和自适应的FedADMM算法，通过为客户端的本地更新设计一个不精确性标准，消除了调整本地训练准确度的需要，降低了计算成本并减轻了滞后效应。 |
| [^16] | [LLaGA: Large Language and Graph Assistant](https://arxiv.org/abs/2402.08170) | LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。 |
| [^17] | [Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals](https://arxiv.org/abs/2402.02332) | Minusformer通过逐步学习残差来改进时间序列预测。它将Transformer模型中信息聚合机制从加法改为减法，并通过辅助输出分支逐层学习监督信号的残差，从而提高模型的灵活性和对抗过拟合的能力。 |
| [^18] | [Who Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms](https://arxiv.org/abs/2401.07342) | 提出了一个自动化框架，使用儿童和教师佩戴的录音设备记录言语，实现说话者分类和转录，与人类专家对比结果表明，框架整体准确率达到0.76，为学前教室言语分析提供了新思路 |
| [^19] | [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective](https://arxiv.org/abs/2312.01957) | 本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。 |
| [^20] | [Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning.](http://arxiv.org/abs/2311.09852) | 这项研究介绍了一种将短期计划生成和选择与分布式优化以及深度强化学习相结合的渐进方法，用于无人机的协调和规划。实验结果表明，与最先进的方法相比，该方法在动态环境中具有出色的性能。 |
| [^21] | [A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models.](http://arxiv.org/abs/2311.00445) | 这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。 |
| [^22] | [Model predictive control-based value estimation for efficient reinforcement learning.](http://arxiv.org/abs/2310.16646) | 该论文提出了一种基于模型预测控制的强化学习方法，通过对环境进行数据驱动建模，并进行多步预测以估计值函数并优化策略，展示出更高的学习效率和更快的收敛速度，同时减少了样本容量需求。 |
| [^23] | [VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds.](http://arxiv.org/abs/2310.11389) | 本研究解决了在马尔可夫成本过程中估计无限时间折现成本的VaR和CVaR的问题。首先，我们推导出了估计误差的最小最大下界，并使用有限时间截断方案得出了上界。这是在马尔可夫设置中首次提供任何风险度量估计误差的下界和上界的工作。 |
| [^24] | [Grokking as the Transition from Lazy to Rich Training Dynamics.](http://arxiv.org/abs/2310.06110) | 研究发现洞察现象可能是由神经网络从懒惰训练动态过渡到丰富的特征学习模式的结果，通过跟踪足够的统计量，发现洞察是在网络首先尝试拟合核回归解决方案后，进行后期特征学习找到通用解决方案之后的结果。 |
| [^25] | [SE(3)-Stochastic Flow Matching for Protein Backbone Generation.](http://arxiv.org/abs/2310.02391) | 通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。 |
| [^26] | [Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control.](http://arxiv.org/abs/2309.14597) | 本论文提供了新的视角，研究了连续控制中深度强化学习智能体性能不稳定的原因。通过对回报景观进行分析，发现了策略空间中的失败区域和策略品质的隐藏维度。此外，提出了一种分布感知的方法，改善了策略的鲁棒性。 |
| [^27] | [Benchmarking Algorithms for Federated Domain Generalization.](http://arxiv.org/abs/2307.04942) | 本论文介绍了一种针对联邦领域泛化的基准测试方法，并评估了13种联邦DG方法。研究结果表明，在大量客户端和高异质性的情况下，联邦DG仍存在显著的性能差距。 |
| [^28] | [Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space.](http://arxiv.org/abs/2307.01177) | 本文提出了神经希尔伯特阶梯(NHL)的概念，它将多层神经网络描述为一系列的再生核希尔伯特空间，进一步推广了浅层神经网络的理论研究，并探讨了其在函数空间内的性质和应用。通过证明不同层次的NHL与多层NNs之间的对应关系，证明了学习NHL的泛化保证，并提出了NHL的特征动力学模型。最后，在ReLU和二次激活函数下展示了NHLs中的深度分离现象。 |
| [^29] | [Incremental Randomized Smoothing Certification.](http://arxiv.org/abs/2305.19521) | 本文提出了渐进式随机平滑认证方法（IRS），可通过重用原始平滑模型的认证保证来认证近似模型，从而显著降低认证修改DNN的计算成本同时保持强大的鲁棒性保证。 |
| [^30] | [A benchmark for computational analysis of animal behavior, using animal-borne tags.](http://arxiv.org/abs/2305.10740) | 该论文介绍了一个名为BEBE的动物行为计算分析基准，其中包括了1654小时的动物生态生理学数据，这是迄今为止最大、最具分类多样性的公开可用的数据集合。在这个基准上，作者使用了十种机器学习方法并确定了未来工作中需要解决的关键问题。 |
| [^31] | [Deep Temporal Graph Clustering.](http://arxiv.org/abs/2305.10738) | 提出通用框架TGC 用于 deep temporal graph clustering, 解决了时间图只能作为静态图处理的难题，实现了对动态信息的聚类。实验证明了 TGC 的优越性。 |
| [^32] | [Predicting Side Effect of Drug Molecules using Recurrent Neural Networks.](http://arxiv.org/abs/2305.10473) | 该论文介绍了一种利用循环神经网络预测药物分子副作用的启发式方法，有效降低了模型复杂度和参数数量。 |
| [^33] | [Performance is not enough: a story of the Rashomon's quartet.](http://arxiv.org/abs/2302.13356) | 本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能，同时其可视化揭示了极其不同的方法来理解数据中的相关性结构。 |
| [^34] | [The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest.](http://arxiv.org/abs/2301.12987) | 本研究表明，在构建假设的过程中，选择最短的假设不如选择最弱的假设，而弱点是比长度或简单性更好的推广表现代理。 |

# 详细

[^1]: 用于旅行购买者问题的深度强化学习

    Deep Reinforcement Learning for Traveling Purchaser Problems

    [https://arxiv.org/abs/2404.02476](https://arxiv.org/abs/2404.02476)

    提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。

    

    旅行购买者问题（TPP）是一种具有广泛应用的重要组合优化问题。本文提出了一种基于深度强化学习（DRL）的新方法，该方法分别解决了路由构建和购买规划问题，同时从全局角度评估和优化解决方案。我们的方法的关键组成部分包括用于捕捉市场-产品关系的TPP的二部图表示，以及从二部图中提取信息并将其用于顺序构建路由的策略网络。

    arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
    
[^2]: 通过免Hessian重新整合个体数据统计实现高效在线遗忘

    Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics

    [https://arxiv.org/abs/2404.01712](https://arxiv.org/abs/2404.01712)

    通过提出的Hessian-free在线遗忘方法，实现了近乎瞬时的在线遗忘，仅需要进行矢量加法操作。

    

    机器遗忘旨在通过使模型能够选择性地忘记特定数据来维护数据所有者的被遗忘权利。最近的方法表明，一种数据遗忘的方法是通过预先计算和存储携带二阶信息的统计数据，以改进计算和内存效率。然而，它们依赖于苛刻的假设，而且计算/存储受到模型参数维度的诅咒，这使得难以应用到大多数深度神经网络中。在本工作中，我们提出了一种免Hessian在线遗忘方法。我们建议为每个数据点维护一个统计向量，通过重新训练和学习模型之间的差异的仿射随机递归逼近来计算。我们提出的算法实现了近乎瞬时的在线遗忘，因为它只需要进行矢量加法操作。基于重新收集遗忘数据统计的策略，

    arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
    
[^3]: 目标黑盒神经排序模型的多粒度对抗攻击

    Multi-granular Adversarial Attacks against Black-box Neural Ranking Models

    [https://arxiv.org/abs/2404.01574](https://arxiv.org/abs/2404.01574)

    这项研究聚焦于利用多粒度扰动生成高质量的对抗性示例，通过转化为顺序决策过程来解决组合爆炸问题。

    

    对抗排序攻击由于在发现神经排序模型的脆弱性并增强其鲁棒性方面取得成功而受到越来越多的关注。传统的攻击方法仅在单一粒度上进行扰动，例如单词级或句子级，对目标文档进行攻击。然而，将扰动限制在单一粒度上可能会减少创造对抗性示例的灵活性，从而降低攻击的潜在威胁。因此，我们专注于通过结合不同粒度的扰动生成高质量的对抗性示例。实现这一目标涉及解决组合爆炸问题，需要识别出跨所有可能的粒度、位置和文本片段的最佳组合扰动。为了解决这一挑战，我们将多粒度对抗攻击转化为一个顺序决策过程，其中

    arXiv:2404.01574v1 Announce Type: cross  Abstract: Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where 
    
[^4]: 流行病建模中图神经网络的综述

    A Review of Graph Neural Networks in Epidemic Modeling

    [https://arxiv.org/abs/2403.19852](https://arxiv.org/abs/2403.19852)

    图神经网络在流行病建模中作为一种新工具备受关注，本文全面回顾了GNN在流行病研究中的应用，并提出了未来发展方向。

    

    自新冠疫情爆发以来，人们对流行病学模型的研究越来越感兴趣。传统的机械模型数学描述了传染病的传播机制，但在面对当今不断增长的挑战时往往力不从心。因此，图神经网络（GNNs）已经成为流行病研究中越来越流行的工具。本文试图全面回顾GNN在流行病任务中的应用，并强调潜在的未来发展方向。为实现这一目标，我们为流行病任务和方法论各引入了分层分类法，为该领域内的发展轨迹提供了一个框架。对于流行病任务，我们建立了一个类似于流行病领域通常应用的分类体系。对于方法论，我们将现有研究分为“神经模型”和“混合模型”。

    arXiv:2403.19852v1 Announce Type: new  Abstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exha
    
[^5]: 通过视觉语言模型对神经网络进行基于概念的分析

    Concept-based Analysis of Neural Networks via Vision-Language Models

    [https://arxiv.org/abs/2403.19837](https://arxiv.org/abs/2403.19837)

    本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。

    

    视觉深度神经网络（DNNs）的形式化分析非常可取，但由于难以表达视觉任务的形式化规范以及缺乏高效的验证程序，这是非常具有挑战性的。在本文中，我们提出利用新兴的多模态、视觉语言、基础模型（VLMs）作为一种通过其可以推理视觉模型的透镜。VLMs已经在大量图像及其文本描述上进行了训练，因此隐式地了解描述这些图像的高层次、人类可理解的概念。我们描述了一种名为$\texttt{Con}_{\texttt{spec}}$的逻辑规范语言，旨在便于按照这些概念编写规范。为了定义和形式化检查$\texttt{Con}_{\texttt{spec}}$规范，我们利用了一个VLM，它提供了一种编码和高效检查视觉模型的自然语言属性的方法。我们展示了我们的te

    arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
    
[^6]: 一份法国假新闻的多标签数据集：人类与机器视角

    A Multi-Label Dataset of French Fake News: Human and Machine Insights

    [https://arxiv.org/abs/2403.16099](https://arxiv.org/abs/2403.16099)

    通过建立一份包括 100 篇文档的多标签数据集 OBSINFOX，研究了人类与机器在认定假新闻特征上的差异，并发现了语料库中讽刺文本的普遍存在。

    

    我们提出了一个由 8 位注释者使用 11 个标签注释的来自 17 个法国被专家机构认为不可靠的新闻来源选取的 100 篇文档的语料库 OBSINFOX。通过收集比通常更多的标签和注释者，我们可以识别人类认为具有代表性的假新闻的特征，并将其与自动分类器的预测进行比较。我们使用 Gate Cloud 进行主题和体裁分析，这表明语料库中类似讽刺的文本普遍存在。然后我们使用 VAGO 主观性分析器及其神经版本，以澄清标签“主观”与标签“假新闻”之间的联系。该带有注释的数据集可通过以下网址在线获取：https://github.com/obs-info/obsinfox

    arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
    
[^7]: KTbench：一种全新的无数据泄漏的知识追踪框架

    KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing

    [https://arxiv.org/abs/2403.15304](https://arxiv.org/abs/2403.15304)

    KTbench提出了一种无数据泄漏的知识追踪框架，解决了KT模型中KC之间的相关性学习可能导致的性能下降问题。

    

    知识追踪（KT）涉及在智能辅导系统中预测学生对学习项目的未来表现。学习项目被标记为称为知识概念（KCs）的技能标签。许多KT模型通过用构成KC的学习项目取代学习项目来将学习项目-学生交互序列扩展为KC-学生交互序列，从而解决了稀疏的学习项目-学生交互问题并最小化了模型参数。然而，这种方法存在两个问题。第一个问题是模型学习同一项目内的KC之间的相关性的能力，这可能导致基本事实标签的泄漏并阻碍模型性能。第二个问题是现有的基准实现忽略了计数问题

    arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
    
[^8]: 具有非私密预处理的可证明隐私

    Provable Privacy with Non-Private Pre-Processing

    [https://arxiv.org/abs/2403.13041](https://arxiv.org/abs/2403.13041)

    提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限

    

    当分析差分私密（DP）机器学习管道时，通常会忽略数据相关的预处理的潜在隐私成本。在这项工作中，我们提出了一个通用框架，用于评估由非私密数据相关预处理算法引起的额外隐私成本。我们的框架通过利用两个新的技术概念建立了整体隐私保证的上限：一种称为平滑DP的DP变体以及预处理算法的有界敏感性。

    arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
    
[^9]: S^2MVTC：简单而高效的可扩展多视图张量聚类

    S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering

    [https://arxiv.org/abs/2403.09107](https://arxiv.org/abs/2403.09107)

    S^2MVTC提出了一种简单而高效的可扩展多视图张量聚类方法，重点是学习跨视图内部和视图间的嵌入特征之间的相关性。

    

    arXiv:2403.09107v1 公告类型：新摘要：基于锚点的大规模多视图聚类因其在处理海量数据集时的有效性而受到了广泛关注。然而，当前方法主要通过探索锚图或投影矩阵之间的全局相关性来寻求用于聚类的共识嵌入特征。在本文中，我们提出了一种简单而高效的可扩展多视图张量聚类（S^2MVTC）方法，我们的重点是学习跨视图内部和视图间的嵌入特征之间的相关性。具体来说，我们首先通过将不同视图的嵌入特征叠加成张量并对其进行旋转来构建嵌入特征张量。此外，我们构建了一种新颖的张量低频近似（TLFA）算子，将图相似性融入到嵌入特征学习中，高效地实现了不同视图内部嵌入特征的平滑表示。此外，一致性约束被应用于嵌入特征。

    arXiv:2403.09107v1 Announce Type: new  Abstract: Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding featur
    
[^10]: 在句法感知代码填空任务上评估LLMs

    Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks

    [https://arxiv.org/abs/2403.04814](https://arxiv.org/abs/2403.04814)

    该研究引入了一个新的基准SAFIM用于评估LLMs在代码填空任务上的句法感知完成表现，发现FIM预训练不仅提高了FIM的熟练度，还改善了LLMs的左到右推理，挑战了传统观念并表明预训练方法和数据品质对模型性能的影响更甚于模型大小。

    

    我们介绍了一种名为Syntax-Aware Fill-In-the-Middle（SAFIM）的新基准，用于评估大型语言模型（LLMs）在代码填空（FIM）任务上的表现。该基准侧重于程序结构的句法感知完成，如代码块和条件表达式，并包括来自多种编程语言的17,720个示例，来源于2022年4月之后的最新代码提交，以最小化数据污染。 SAFIM提供了一个强大的框架，具有各种提示设计和新颖的句法感知后处理技术，有助于在LLMs之间进行准确和公平的比较。我们对15个LLMs进行了全面评估，结果表明FIM预训练不仅提升了FIM的熟练程度，还改进了LLMs的左到右（L2R）推理。我们的发现挑战了传统观念，并表明预训练方法和数据质量对模型性能的影响大于模型大小。因此，SAFIM为未来构建

    arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
    
[^11]: 粒子梯度下降的误差界限，以及log-Sobolev和Talagrand不等式的推广

    Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities

    [https://arxiv.org/abs/2403.02004](https://arxiv.org/abs/2403.02004)

    证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。

    

    我们证明了粒子梯度下降(PGD)~(Kuntz等人，2023)的非渐近误差界限，这是一种最大似然估计的算法，用于离散化自由能梯度流获得的大型潜变量模型。我们首先展示了对于满足一般化log-Sobolev和Polyak-Lojasiewicz不等式（LSI和PLI）的模型，流以指数速度收敛到自由能的极小化集合。我们通过将最优输运文献中众所周知的结果（LSI意味着Talagrand不等式）及其在优化文献中的对应物（PLI意味着所谓的二次增长条件）扩展并应用到我们的新设置，来实现这一点。我们还推广了Bakry-Emery定理，并展示了对于具有强凹对数似然的模型，LSI/PLI的概括成立。

    arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for models with strongly concave log-likelihoods. For such m
    
[^12]: 少即是多：面向可扩展和通用学习的跳数图注意力在电路上的应用

    Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits

    [https://arxiv.org/abs/2403.01317](https://arxiv.org/abs/2403.01317)

    提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。

    

    虽然图神经网络（GNNs）在各种电子设计自动化（EDA）任务中学习电路表示方面变得流行，但当应用于大图时，它们面临可扩展性挑战，并且对新设计的泛化能力有限。这些限制使它们在解决大规模复杂电路问题时不太实用。在这项工作中，我们提出了HOGA，一种新颖的基于注意力的模型，用于以可扩展和通用的方式学习电路表示。HOGA首先在模型训练之前针对每个节点计算跳数特征。随后，跳数特征仅用于通过门控自注意力模块生成节点表示，该模块自适应地学习不同跳数之间的重要特征，而不涉及图拓扑。因此，HOGA能够适应不同电路之间的各种结构，并可以以分布式的方式高效训练。

    arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
    
[^13]: GEM3D：三维形状合成的生成媒体抽象

    GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis

    [https://arxiv.org/abs/2402.16994](https://arxiv.org/abs/2402.16994)

    GEM3D提出了一种新的深度、拓扑感知的三维形状生成模型，通过神经骨架编码了拓扑和几何信息，通过骨架驱动的神经隐式公式生成准确和多样化的表面。

    

    我们介绍了GEM3D——一种新的深度、拓扑感知的三维形状生成模型。我们方法的关键部分是基于神经骨架的表示，编码了关于形状拓扑和几何的信息。通过一个去噪扩散概率模型，我们的方法首先生成遵循中轴变换（MAT）的基于骨架的表示，然后通过一个骨架驱动的神经隐式公式生成表面。神经隐式考虑了在生成的骨架表示中存储的拓扑和几何信息，产生的表面与之前的神经场公式相比更加拓扑和几何准确。我们讨论了我们的方法在形状合成和点云重建任务中的应用，并对我们的方法进行了定性和定量评估。我们展示了比以前更为准确和多样化的形状重建。

    arXiv:2402.16994v1 Announce Type: cross  Abstract: We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation r
    
[^14]: 较大尺度下用于稳健且可解释视觉任务的分层不变性

    Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales

    [https://arxiv.org/abs/2402.15430](https://arxiv.org/abs/2402.15430)

    通过分层不变性构建稳健且可解释的视觉系统，克服了不变性表示在较大规模的视觉任务中可区分性有限的问题。

    

    开发稳健且可解释的视觉系统是迈向可靠人工智能的关键一步。在这方面，一个有前途的范式是考虑在基本图像表示中嵌入任务所需的不变结构，例如几何不变性。然而，这种不变的表示通常表现出有限的可区分性，限制了它们在更大规模的可靠视觉任务中的应用。针对这个开放性问题，我们从理论、实践和应用的角度对分层不变性进行了系统研究。在理论层面上，我们展示了如何以类似于卷积神经网络（CNN）的分层体系结构但以完全可解释的方式构建超完备不变性。我们提供了通用蓝图、具体定义、不变特性和数值实现。在实践层面上，我们讨论了如何定制不变性以适应更大规模的视觉任务。

    arXiv:2402.15430v1 Announce Type: cross  Abstract: Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize 
    
[^15]: FedADMM-InSa: 一种不精确和自适应的联邦学习ADMM

    FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning

    [https://arxiv.org/abs/2402.13989](https://arxiv.org/abs/2402.13989)

    提出了一种不精确和自适应的FedADMM算法，通过为客户端的本地更新设计一个不精确性标准，消除了调整本地训练准确度的需要，降低了计算成本并减轻了滞后效应。

    

    联邦学习(FL)是一个有希望的框架，可以从分布式数据中学习同时保持隐私。有效的FL算法的发展面临各种挑战，包括异构数据和系统、通信能力有限以及受限的本地计算资源。最近开发的FedADMM方法对数据和系统的异构性表现出很强的韧性。然而，如果超参数没有经过精心调整，它们仍然会遭受性能下降的问题。为了解决这个问题，我们提出了一种不精确和自适应的FedADMM算法，名为FedADMM-InSa。首先，我们为客户端的本地更新设计了一个不精确性标准，以消除必须根据经验设置本地训练准确性的需求。这种不精确性标准可以由每个客户端独立地根据其独特条件进行评估，从而降低本地计算成本并减轻不良的滞后效应。

    arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
    
[^16]: LLaGA: 大型语言和图形助手

    LLaGA: Large Language and Graph Assistant

    [https://arxiv.org/abs/2402.08170](https://arxiv.org/abs/2402.08170)

    LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。

    

    图神经网络（GNN）已经推动了图结构数据分析的进步。最近，大型语言模型（LLM）如GPT-4的崛起预示着深度学习的一个新时代。然而，将它们应用于图数据还面临着独特的挑战，由于将图结构转化为文本的固有难度。为此，我们引入了一个创新模型——大型语言和图形助手（LLaGA），它有效地整合了LLM的能力，以处理图结构数据的复杂性。LLaGA保留了LLM的通用性，同时将图数据转化为与LLM输入兼容的格式。LLaGA通过重新组织图节点以作为结构感知的序列，然后通过一个多功能投影仪将其映射到标记嵌入空间中。LLaGA在多样性、泛化性和可解释性方面表现出色，使其能够在不同数据集和任务上表现出一致的良好性能。

    Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
    
[^17]: Minusformer: 通过渐进学习残差来改进时间序列预测

    Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals

    [https://arxiv.org/abs/2402.02332](https://arxiv.org/abs/2402.02332)

    Minusformer通过逐步学习残差来改进时间序列预测。它将Transformer模型中信息聚合机制从加法改为减法，并通过辅助输出分支逐层学习监督信号的残差，从而提高模型的灵活性和对抗过拟合的能力。

    

    在本文中，我们发现普遍存在的时间序列（TS）预测模型容易严重过拟合。为了应对这个问题，我们采用去冗余的方法逐步恢复TS的内在价值以用于未来的时间段。具体而言，我们通过将信息聚合机制从加法转变为减法来改进传统的Transformer模型。然后，我们在原模型的每个模块中加入一个辅助输出分支，构建一条通往最终预测的高速公路。该分支中后续模块的输出将减去先前学习到的结果，使模型能够逐层学习监督信号的残差。这种设计促进了输入和输出流的逐步学习驱动隐式分解，使模型具有更高的灵活性、可解释性和对抗过拟合的韧性。由于模型中的所有聚合都是减号，因此被称为Minusformer。

    In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor
    
[^18]: 谁说了什么？分析学前课堂言语的自动化方法

    Who Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms

    [https://arxiv.org/abs/2401.07342](https://arxiv.org/abs/2401.07342)

    提出了一个自动化框架，使用儿童和教师佩戴的录音设备记录言语，实现说话者分类和转录，与人类专家对比结果表明，框架整体准确率达到0.76，为学前教室言语分析提供了新思路

    

    年幼儿童在喧闹的学前课堂中度过大部分清醒时间。在这种环境中，孩子与教师之间的言语互动对他们的语言结果至关重要，但手动转录这些互动是困难的。本文提出了一个使用儿童和教师佩戴的录音设备录音的自动化框架，该框架使用开源软件对说话者进行分类（ALICE），并转录他们的话语（Whisper）。我们将框架的结果与人类专家对110分钟教室录音（包括来自儿童话筒的85分钟（4名儿童）和来自教师话筒的25分钟（2名教师））的结果进行了比较。总体一致比例，即正确分类的教师和儿童话语的比例为0.76，矫正的kappa为0.50，加权F1为0.76。教师和儿童的话语的词误率

    arXiv:2401.07342v2 Announce Type: replace-cross  Abstract: Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2 teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was .76, with an error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for both teacher and child 
    
[^19]: 使用合成数据的经过精炼的LLMs自我批评：一种贝叶斯视角

    Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective

    [https://arxiv.org/abs/2312.01957](https://arxiv.org/abs/2312.01957)

    本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。

    

    本文提出了将RLAIF解释为贝叶斯推断的方法，通过引入经过精炼的自我批评(dSC)，该方法通过Gibbs采样器对LLM的输出进行精炼，然后将其蒸馏成一个微调模型。只需要合成数据，dSC在涉及安全性、情感和隐私控制的实验中得到了应用，表明它可以作为对齐LLMs的一种可行且廉价的替代方案。代码在\url{https://github.com/vicgalle/distilled-self-critique}上发布。

    arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
    
[^20]: 短期与长期无人机协调：分布式优化与深度强化学习的交汇

    Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning. (arXiv:2311.09852v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2311.09852](http://arxiv.org/abs/2311.09852)

    这项研究介绍了一种将短期计划生成和选择与分布式优化以及深度强化学习相结合的渐进方法，用于无人机的协调和规划。实验结果表明，与最先进的方法相比，该方法在动态环境中具有出色的性能。

    

    在智能城市中，支持充电技术的自主交互式无人机群可以提供引人注目的感知能力，例如交通监测和灾难响应。现有方法，包括分布式优化和深度强化学习(DRL)，旨在协调无人机以实现成本效益高、高质量的导航、感知和充电。然而，它们面临着重大挑战：短期优化在动态环境中的意外变化下并不有效，而长期学习缺乏可扩展性、韧性和灵活性。为了弥合这一差距，本文提出了一种新的渐进方法，将基于分布式优化的短期计划生成和选择与基于DRL的长期飞行方向的战略调度相结合。通过对从现实城市移动中生成的数据集进行广泛实验，表明所提出的解决方案与最先进的方法相比具有卓越的性能。

    Swarms of autonomous interactive drones, with the support of recharging technology, can provide compelling sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and charging. However, they face grand challenges: short-term optimization is not effective in dynamic environments with unanticipated changes, while long-term learning lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that combines short-term plan generation and selection based on distributed optimization with a DRL-based long-term strategic scheduling of flying direction. Extensive experimentation with datasets generated from realistic urban mobility underscores an outstanding performance of the proposed solution compared to state-of-the-art. We als
    
[^21]: 人类和语言模型中的三段论推理的系统比较

    A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])

    [http://arxiv.org/abs/2311.00445](http://arxiv.org/abs/2311.00445)

    这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。

    

    理性行为的一个核心组成部分是逻辑推理：确定哪些结论可以从一组前提中得出。心理学家已经记录下人类推理与逻辑规则不符的几种方式。语言模型是否能够复制这些偏差，或者它们能够克服这些偏差？我们关注三段论的情况 - 从两个简单前提中推导出的推理，这在心理学中已经广泛研究 - 我们发现较大的模型比较合逻辑，而且比人类更合逻辑。与此同时，即使是最大的模型也会出现系统性错误，其中一些错误与人类推理的偏见相似，例如排序效应和逻辑谬误。总体上，我们发现语言模型模仿了训练数据中包含的人类偏见，但在某些情况下能够克服这些偏见。

    A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
    
[^22]: 基于模型预测控制的值估计用于高效强化学习

    Model predictive control-based value estimation for efficient reinforcement learning. (arXiv:2310.16646v1 [cs.LG])

    [http://arxiv.org/abs/2310.16646](http://arxiv.org/abs/2310.16646)

    该论文提出了一种基于模型预测控制的强化学习方法，通过对环境进行数据驱动建模，并进行多步预测以估计值函数并优化策略，展示出更高的学习效率和更快的收敛速度，同时减少了样本容量需求。

    

    强化学习在实践中存在着与虚拟环境进行交互的次数所带来的局限性。为了解决这个问题，我们设计了一种改进的基于模型预测控制的强化学习方法，通过数据驱动的方式对环境进行建模。基于学习到的环境模型，该方法进行多步预测以估计值函数并优化策略。该方法表现出更高的学习效率，更快的收敛速度以接近最优值，并且在经验回放缓冲区中需要更少的样本容量。在经典数据库和无人机动态避障场景中的实验结果验证了所提出方法的有效性。

    Reinforcement learning suffers from limitations in real practices primarily due to the numbers of required interactions with virtual environments. It results in a challenging problem that we are implausible to obtain an optimal strategy only with a few attempts for many learning method. Hereby, we design an improved reinforcement learning method based on model predictive control that models the environment through a data-driven approach. Based on learned environmental model, it performs multi-step prediction to estimate the value function and optimize the policy. The method demonstrates higher learning efficiency, faster convergent speed of strategies tending to the optimal value, and fewer sample capacity space required by experience replay buffers. Experimental results, both in classic databases and in a dynamic obstacle avoidance scenario for unmanned aerial vehicle, validate the proposed approaches.
    
[^23]: 在马尔可夫成本过程中的VaR和CVaR估计：下界和上界

    VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds. (arXiv:2310.11389v1 [cs.LG])

    [http://arxiv.org/abs/2310.11389](http://arxiv.org/abs/2310.11389)

    本研究解决了在马尔可夫成本过程中估计无限时间折现成本的VaR和CVaR的问题。首先，我们推导出了估计误差的最小最大下界，并使用有限时间截断方案得出了上界。这是在马尔可夫设置中首次提供任何风险度量估计误差的下界和上界的工作。

    

    我们解决了在马尔可夫成本过程中估计无限时间折现成本的风险价值（Value-at-Risk，VaR）和条件风险价值（Conditional Value-at-Risk，CVaR）的问题。首先，我们推导出一个最小最大下界，该下界在期望意义和概率意义下都成立，其误差界为$\Omega(1/\sqrt{n})$。然后，利用有限时间截断方案，我们推导出CVaR估计误差的上界，该上界与我们的下界匹配，只有常数因子的差异。最后，我们讨论了我们的估计方案的扩展，涵盖了更通用的满足一定连续性准则的风险度量，例如谱风险度量和基于效用的缺口风险度量。据我们所知，我们的工作是第一个在马尔可夫设置中为任何风险度量提供估计误差的下界和上界的工作。我们指出，我们的下界也可扩展到无限时间折现成本的均值。即使在这种情况下，我们的结果$\Omega(1/\sqrt{n})$也优于现有结果$\Omega(

    We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process. First, we derive a minimax lower bound of $\Omega(1/\sqrt{n})$ that holds both in an expected and in a probabilistic sense. Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings. We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our result $\Omega(1/\sqrt{n}) $ improves upon the existing result $\Omega(
    
[^24]: 从懒惰到丰富训练动态的洞察力

    Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v1 [stat.ML])

    [http://arxiv.org/abs/2310.06110](http://arxiv.org/abs/2310.06110)

    研究发现洞察现象可能是由神经网络从懒惰训练动态过渡到丰富的特征学习模式的结果，通过跟踪足够的统计量，发现洞察是在网络首先尝试拟合核回归解决方案后，进行后期特征学习找到通用解决方案之后的结果。

    

    我们提出了洞察现象，即神经网络的训练损失在测试损失之前大幅下降，可能是由于神经网络从懒惰的训练动态转变为丰富的特征学习模式。为了说明这一机制，我们研究了在没有正则化的情况下，使用Vanilla梯度下降方法在多项式回归问题上进行的两层神经网络的训练，该训练展现了无法用现有理论解释的洞察现象。我们确定了该网络测试损失的足够统计量，并通过训练跟踪这些统计量揭示了洞察现象的发生。我们发现，在这种情况下，网络首先尝试使用初始特征拟合核回归解决方案，接着在训练损失已经很低的情况下进行后期特征学习，从而找到了一个能够泛化的解决方案。我们发现，洞察产生的关键因素是特征学习的速率，这可以通过缩放网络参数来精确控制。

    We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the ne
    
[^25]: SE(3)-蛋白质主链生成中的随机流匹配

    SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])

    [http://arxiv.org/abs/2310.02391](http://arxiv.org/abs/2310.02391)

    通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。

    

    通过基于三维刚体运动（即SE(3)群）的流匹配范式，我们引入了一系列具有不断增强建模能力的新型生成模型：FoldFlow，从而实现了对蛋白质主链的准确建模。首先，我们介绍了FoldFlow-Base，一种无需模拟的学习确定性连续时间动力学和匹配不变目标分布的方法。接下来，我们通过引入Riemannian最优传输来加速训练，创建了FoldFlow-OT，从而构建了更简单和稳定的流。最后，我们设计了FoldFlow-SFM，将Riemannian最优传输和无需模拟训练相结合，可以学习SE(3)上的随机连续时间动力学。我们的FoldFlow生成模型家族相比之前的方法具有几个关键优势。

    The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
    
[^26]: 在连续控制中的噪声邻域中的策略优化

    Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])

    [http://arxiv.org/abs/2309.14597](http://arxiv.org/abs/2309.14597)

    本论文提供了新的视角，研究了连续控制中深度强化学习智能体性能不稳定的原因。通过对回报景观进行分析，发现了策略空间中的失败区域和策略品质的隐藏维度。此外，提出了一种分布感知的方法，改善了策略的鲁棒性。

    

    已知连续控制问题中的深度强化学习智能体在性能上会出现显著的不稳定性。本文从研究策略和回报之间的映射即回报景观的角度为这些行为提供了新的视角。我们发现，流行的算法在这个景观的噪声邻域中穿行，一个策略参数的单次更新会导致回报在很大范围内变化。通过对这些回报进行分布处理，我们对景观进行了映射，描述了策略空间中容易产生失败的区域，并揭示了策略品质的隐藏维度。我们还展示了景观的惊人结构，通过在参数空间中找到简单的路径来提高策略的稳定性。最后，我们开发了一个分布感知的方法，通过避开噪声邻域来提高策略的鲁棒性。综上所述，我们的结果为优化和评估提供了新的洞察。

    Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, eva
    
[^27]: 基准测试针对联邦领域泛化的算法

    Benchmarking Algorithms for Federated Domain Generalization. (arXiv:2307.04942v1 [cs.LG])

    [http://arxiv.org/abs/2307.04942](http://arxiv.org/abs/2307.04942)

    本论文介绍了一种针对联邦领域泛化的基准测试方法，并评估了13种联邦DG方法。研究结果表明，在大量客户端和高异质性的情况下，联邦DG仍存在显著的性能差距。

    

    尽管先前的领域泛化（DG）基准考虑了训练-测试数据集的异质性，我们评估了引入联邦学习（FL）特定挑战的联邦DG。此外，我们还在客户端本地数据集中探索基于领域的异质性-一个现实的联邦DG场景。先前的联邦DG评估在客户端数量或异质性以及数据集多样性方面存在限制。为了填补这一差距，我们提出了一种联邦DG基准测试方法，可以控制客户端的数量和异质性，并提供数据集难度的度量标准。然后，我们应用我们的方法来评估13种联邦DG方法，包括适应FL环境的集中DG方法、处理客户端异质性的FL方法以及专为联邦DG设计的方法。我们的结果表明，尽管取得了一些进展，但在大量客户端和高客户端异质性的情况下，联邦DG仍存在显著的性能差距。

    While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client h
    
[^28]: 神经希尔伯特阶梯：函数空间中的多层神经网络

    Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space. (arXiv:2307.01177v1 [cs.LG])

    [http://arxiv.org/abs/2307.01177](http://arxiv.org/abs/2307.01177)

    本文提出了神经希尔伯特阶梯(NHL)的概念，它将多层神经网络描述为一系列的再生核希尔伯特空间，进一步推广了浅层神经网络的理论研究，并探讨了其在函数空间内的性质和应用。通过证明不同层次的NHL与多层NNs之间的对应关系，证明了学习NHL的泛化保证，并提出了NHL的特征动力学模型。最后，在ReLU和二次激活函数下展示了NHLs中的深度分离现象。

    

    神经网络(NNs)所探索的函数空间的特征化是深度学习理论的重要方面。本文将具有任意宽度的多层NN视为定义特定层次的再生核希尔伯特空间(RKHS)的神经希尔伯特阶梯(NHL)。这使得我们能够定义一个函数空间和一个复杂度度量，该度量推广了浅层NNs的先前结果，并研究了它们在几个方面的理论特性和影响。首先，我们证明了L层NNs表示的函数与属于L层NHLs的函数之间的对应关系。其次，我们证明了学习具有受控复杂度度量的NHL的泛化保证。第三，对应于在无穷宽均场极限下训练多层NNs，我们导出了NHL的特征动力学，该动力学被描述为多个随机场的演化。第四，在ReLU和二次激活函数下展示了NHLs中的深度分离示例。

    The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation fun
    
[^29]: 渐进式随机平滑认证。

    Incremental Randomized Smoothing Certification. (arXiv:2305.19521v1 [cs.LG])

    [http://arxiv.org/abs/2305.19521](http://arxiv.org/abs/2305.19521)

    本文提出了渐进式随机平滑认证方法（IRS），可通过重用原始平滑模型的认证保证来认证近似模型，从而显著降低认证修改DNN的计算成本同时保持强大的鲁棒性保证。

    

    随机平滑认证是一种有效的方法，用于获取深度神经网络（DNN）对抗攻击的鲁棒性证书。该方法构建了一个平滑的DNN模型，并通过统计抽样来证明其鲁棒性，但计算代价较高，特别是当使用大量样本进行证明时。此外，当修改平滑模型（例如，量化或修剪）时，认证保证可能不适用于修改的DNN，并且从头开始重新认证可能代价太高。我们提出了第一种渐进式鲁棒性认证随机平滑方法（IRS）。我们展示了如何重复使用原始平滑模型的认证保证，以利用很少的样本认证近似模型。IRS显著降低了认证修改DNN的计算成本，同时保持强大的鲁棒性保证。我们通过实验验证了我们方法的有效性，展示了IRS在多个数据集上的优越性能。

    Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showin
    
[^30]: 一种动物行为计算分析的基准，使用动物携带的标签。

    A benchmark for computational analysis of animal behavior, using animal-borne tags. (arXiv:2305.10740v1 [cs.LG])

    [http://arxiv.org/abs/2305.10740](http://arxiv.org/abs/2305.10740)

    该论文介绍了一个名为BEBE的动物行为计算分析基准，其中包括了1654小时的动物生态生理学数据，这是迄今为止最大、最具分类多样性的公开可用的数据集合。在这个基准上，作者使用了十种机器学习方法并确定了未来工作中需要解决的关键问题。

    

    动物携带的传感器（“生物记录器”）可以记录一系列动力学和环境数据，揭示动物生态生理学并改善保护工作。机器学习技术对于解释生物记录器记录的大量数据非常有用，但在这个领域中没有标准来比较不同的机器学习技术。为了解决这个问题，我们提出了Bio-logger Ethogram Benchmark（BEBE），这是一个带有行为注释，标准化建模任务和评估指标的数据集合。BEBE是迄今为止最大、最具分类多样性和公开可用的这种基准，包括来自九个分类单元中149个个体收集的1654小时数据。我们在BEBE上评估了十种不同的机器学习方法的性能，并确定了未来工作中需要解决的关键问题。数据集、模型和评估代码已公开发布在https://github.com/earthspecies/BEBE，以便社区使用。

    Animal-borne sensors ('bio-loggers') can record a suite of kinematic and environmental data, which can elucidate animal ecophysiology and improve conservation efforts. Machine learning techniques are useful for interpreting the large amounts of data recorded by bio-loggers, but there exists no standard for comparing the different machine learning techniques in this domain. To address this, we present the Bio-logger Ethogram Benchmark (BEBE), a collection of datasets with behavioral annotations, standardized modeling tasks, and evaluation metrics. BEBE is to date the largest, most taxonomically diverse, publicly available benchmark of this type, and includes 1654 hours of data collected from 149 individuals across nine taxa. We evaluate the performance of ten different machine learning methods on BEBE, and identify key challenges to be addressed in future work. Datasets, models, and evaluation code are made publicly available at https://github.com/earthspecies/BEBE, to enable community 
    
[^31]: 深度时间图聚类

    Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])

    [http://arxiv.org/abs/2305.10738](http://arxiv.org/abs/2305.10738)

    提出通用框架TGC 用于 deep temporal graph clustering, 解决了时间图只能作为静态图处理的难题，实现了对动态信息的聚类。实验证明了 TGC 的优越性。

    

    最近深度图聚类已经引起了很多关注，因为它可以增强模型在无监督场景下的表示学习能力。然而，适用于时间图的深度聚类方法 - 可以捕获关键的动态交互信息，并没有得到充分的探索。这意味着在许多面向聚类的现实场景中，时间图只能作为静态图来处理。这不仅导致了动态信息的丢失，也引发了巨大的计算消耗。为了解决这个问题，我们提出了一个名为TGC的通用框架，用于时间图深度聚类，它调整了深度聚类技术（聚类分配分布和邻接矩阵重构），以适应时间图基于交互序列的批处理模式。此外，我们还从几个方面讨论了时间图聚类与现有静态图聚类的差异。实验证明了TGC的卓越性能。

    Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
    
[^32]: 利用循环神经网络预测药物分子的副作用

    Predicting Side Effect of Drug Molecules using Recurrent Neural Networks. (arXiv:2305.10473v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.10473](http://arxiv.org/abs/2305.10473)

    该论文介绍了一种利用循环神经网络预测药物分子副作用的启发式方法，有效降低了模型复杂度和参数数量。

    

    鉴定和验证分子属性，如副作用，是分子合成过程中最重要和耗时的步骤之一。我们提出了一种启发式方法，利用简单的神经网络，特别是循环神经网络，进行副作用预测，避免了依赖于复杂模型的问题并大大降低了参数数量。

    Identification and verification of molecular properties such as side effects is one of the most important and time-consuming steps in the process of molecule synthesis. For example, failure to identify side effects before submission to regulatory groups can cost millions of dollars and months of additional research to the companies. Failure to identify side effects during the regulatory review can also cost lives. The complexity and expense of this task have made it a candidate for a machine learning-based solution. Prior approaches rely on complex model designs and excessive parameter counts for side effect predictions. We believe reliance on complex models only shifts the difficulty away from chemists rather than alleviating the issue. Implementing large models is also expensive without prior access to high-performance computers. We propose a heuristic approach that allows for the utilization of simple neural networks, specifically the recurrent neural network, with a 98+% reduction 
    
[^33]: 表现不足以为盈，深究Rashomon的四重奏

    Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.13356](http://arxiv.org/abs/2302.13356)

    本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能，同时其可视化揭示了极其不同的方法来理解数据中的相关性结构。

    

    预测建模通常被简化为寻找最优模型来优化选定的性能度量。但如果第二优模型能够以完全不同的方式同样描述数据呢？第三个模型呢？最有效的模型会学到完全不同的数据关系吗？受到Anscombe四重奏的启发，本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能。然而，它们的可视化揭示了极其不同的方法来理解数据中的相关性结构。引入的简单示例旨在进一步促进可视化作为比较预测模型超越性能的必要工具。我们需要开发富有洞察力的技术来解释模型集。

    Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
    
[^34]: 假设的最佳选择是最弱的而不是最短的

    The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12987](http://arxiv.org/abs/2301.12987)

    本研究表明，在构建假设的过程中，选择最短的假设不如选择最弱的假设，而弱点是比长度或简单性更好的推广表现代理。

    

    如果$A$和$B$是这样的集合，即$A \subset B$，那么一般化可以被理解为从$A$推断出一个足以构建$B$的假设。可以从$A$推断出任意数量的假设，但只有其中的一些可以推广到$B$。怎样知道哪些假设可能推广？一种策略是选择最短的，将压缩信息的能力与推广的能力（智能的代理）等同起来。我们在主动认知的数学形式主义背景下研究了这一点。我们证明了，压缩既不是最大化表现（用假设推广的概率衡量）的必要条件，也不是充分条件。我们制定了一个与长度或简单性无关的代理，称为弱点。我们表明，如果任务是均匀分布的，则不存在任何代理的选择，可以在所有任务中至少与弱点最大化的表现相同，同时在至少一个任务中表现更好。在比较最大弱点和最小描述长度(MDL)的实验中，我们发现最大弱点在各种任务上表现优于MDL。我们认为弱点是比长度或简单性更好的推广表现代理。

    If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
    

