# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos.](http://arxiv.org/abs/2311.02076) | 本研究通过分析神经网络训练中的锐度动力学，揭示出早期锐度降低、逐渐增加锐化和稳定边界的机制，并发现增大学习率时，稳定边界流形上发生倍增混沌路径。 |
| [^2] | [Active Learning-Based Species Range Estimation.](http://arxiv.org/abs/2311.02061) | 这项研究提出了一种基于主动学习的方法，能够从有限数量的地面观测中高效估计物种的地理分布范围。通过建模未测绘物种的范围为不同物种的估计范围的加权组合，并使用训练于大规模弱监督观测数据的模型生成候选范围集合，然后通过新的主动查询方法选择地理位置进行访问以减少对未测绘物种范围的不确定性。实验结果表明，这种方法优于其他主动学习方法，并接近端到端训练模型的性能。 |
| [^3] | [LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery.](http://arxiv.org/abs/2311.02058) | LOTUS是一种持续模仿学习算法，通过无监督技能发现，使得机器人能够在其整个寿命中持续学习解决新的操作任务。该算法通过构建技能库，并使用元控制器灵活组合技能来提高成功率，在实验中表现出优越的知识传递能力。 |
| [^4] | [Quantum circuit synthesis with diffusion models.](http://arxiv.org/abs/2311.02041) | 该论文提出了一种利用扩散模型进行量子电路合成的方法，通过使用生成式机器学习模型，可以在基于门的量子电路中产生所需的量子操作，而且能够绕过经典模拟量子动力学的指数级开销。实验证明该模型在纠缠生成和酉编译等任务中表现优秀，并支持扩展功能以适应不同的量子设备约束条件。 |
| [^5] | [Reproducible Parameter Inference Using Bagged Posteriors.](http://arxiv.org/abs/2311.02019) | 通过使用袋装方法，提出了一种易于使用且广泛适用的方法来改善在模型错误规范下的可重现性。 |
| [^6] | [DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries.](http://arxiv.org/abs/2311.02017) | DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。 |
| [^7] | [Score Models for Offline Goal-Conditioned Reinforcement Learning.](http://arxiv.org/abs/2311.02013) | 本文提出了一种新颖的离线目标条件强化学习方法，称为SMORe，它将占有匹配的视角与混合分布匹配相结合，无需学习鉴别器，从而提高了GCRL在离线环境中的表现。 |
| [^8] | [A Variational Perspective on High-Resolution ODEs.](http://arxiv.org/abs/2311.02002) | 该论文提出了一种基于变分视角的方法来研究高分辨率ODE，并通过将Nesterov加速梯度法应用于梯度范数最小化问题，得到了更快的收敛速度。此外，论文还展示了Nesterov的方法可以看作是对高分辨率ODE的合适离散。 |
| [^9] | [High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise.](http://arxiv.org/abs/2311.02000) | Adam算法在非凸平滑随机优化中，经过深入分析，证明了在坐标-wise“仿射”方差噪声下，Adam可以以高概率收敛到稳定点，无需任何有界梯度假设和问题相关的知识。 |
| [^10] | [Detection of keratoconus Diseases using deep Learning.](http://arxiv.org/abs/2311.01996) | 本研究通过比较不同基于CNN的深度学习架构，发现使用基于DenseNet201的模型可以准确识别圆锥角膜病，准确率达到89.14%。 |
| [^11] | [Obtaining Explainable Classification Models using Distributionally Robust Optimization.](http://arxiv.org/abs/2311.01994) | 本论文介绍了一种利用分布鲁棒优化获取可解释的分类模型的方法，通过构建稀疏的规则集合来同时解决规则集的稀疏性和预测准确性之间的权衡，从而保证泛化性能并降低计算成本。 |
| [^12] | [Conditions on Preference Relations that Guarantee the Existence of Optimal Policies.](http://arxiv.org/abs/2311.01990) | 我们引入了直接偏好过程框架，通过对偏好的序结构进行分析，我们提出了保证最优策略存在的条件。这个研究缩小了学习偏好反馈算法在理论和实践之间的差距，并提供了最优策略存在的证明。 |
| [^13] | [Latent Diffusion Model for Conditional Reservoir Facies Generation.](http://arxiv.org/abs/2311.01968) | 本研究提出了一种专门用于条件下储层相生成的潜在扩散模型，通过充分保留条件数据，生成了高保真度的储层相。它在性能上明显优于基于GANs的替代方法。 |
| [^14] | [The language of prompting: What linguistic properties make a prompt successful?.](http://arxiv.org/abs/2311.01967) | 本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。 |
| [^15] | [Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products.](http://arxiv.org/abs/2311.01960) | 本文研究了矩阵乘积元素转换的低秩逼近问题，并证明了在某些条件下，满足比$n^{2-o(1)}$时间复杂度更好的相对误差低秩逼近算法的必要条件。这是第一个关于该问题的条件时间复杂度困难结果。 |
| [^16] | [Optimistic Multi-Agent Policy Gradient for Cooperative Tasks.](http://arxiv.org/abs/2311.01953) | 本论文提出了一个通用的、简单的框架，在多智能体策略梯度方法中引入乐观更新，以缓解合作任务中的相对过度概括问题。通过使用一个泄漏化线性整流函数来重塑优势，我们的方法能够保持对潜在由其他代理引起的低回报个别动作的乐观态度。 |
| [^17] | [ForecastPFN: Synthetically-Trained Zero-Shot Forecasting.](http://arxiv.org/abs/2311.01933) | 本文提出了一种名为ForecastPFN的零样本预测模型，通过合成数据训练，能够更准确且更快地进行零样本预测。 |
| [^18] | [GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling.](http://arxiv.org/abs/2311.01927) | GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。 |
| [^19] | [Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review.](http://arxiv.org/abs/2311.01918) | 本综述文章探讨了大型语言模型（LLMs）在医学中的应用和意义。LLMs在知识检索、研究支持、临床工作流自动化和诊断辅助方面具有巨大潜力，尤其是多模态LLMs可以处理医学影像和电子健康记录等多样化数据类型以增强诊断能力。 |
| [^20] | [High Precision Causal Model Evaluation with Conditional Randomization.](http://arxiv.org/abs/2311.01902) | 该论文提出了一种在现实条件随机化环境中对因果模型进行评估的新方法，通过引入低方差因果误差估计器，有效消除了引入的方差，实现了更好的效果。该方法在实证研究中得到改进，并展示了接近RCT性能的潜力。 |
| [^21] | [Online non-parametric likelihood-ratio estimation by Pearson-divergence functional minimization.](http://arxiv.org/abs/2311.01900) | 本研究提出了一种在线非参数似然比估计（OLRE）框架，适用于估计两个概率密度函数之间差异的问题。通过利用核方法和函数最小化技术，我们的方法能够高效地进行在线更新，同时具有对概率密度函数形式无知的优势。 |
| [^22] | [Learning Sparse Codes with Entropy-Based ELBOs.](http://arxiv.org/abs/2311.01888) | 本论文提出了一种基于熵的学习目标，用于稀疏编码参数的学习，通过非平凡的后验逼近和解析的目标函数，实现了标准稀疏编码的学习，在数值实验中证明了其可行性。 |
| [^23] | [Domain Randomization via Entropy Maximization.](http://arxiv.org/abs/2311.01885) | 本文提出了一种新的领域随机化方法，通过熵最大化的方式在模拟训练中调整动力学分布，以实现模拟到真实的转移，无需真实世界数据，能够保持泛化能力和代理的高成功概率。 |
| [^24] | [Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study.](http://arxiv.org/abs/2311.01875) | 本论文研究了如何利用顺序神经网络（SNNs）增强功能数据分析（FDA），并通过与常见FDA回归模型的比较分析和实际数据分析证明了SNNs的有效性。 |
| [^25] | [SortNet: Learning To Rank By a Neural-Based Sorting Algorithm.](http://arxiv.org/abs/2311.01864) | SortNet是一种使用神经网络作为比较器来进行自适应排序的算法，通过迭代过程构建训练集，根据成对项目之间的排序示例来训练神经网络。 |
| [^26] | [An Ensemble Machine Learning Approach for Screening Covid-19 based on Urine Parameters.](http://arxiv.org/abs/2311.01854) | 本研究提出了一种基于尿液参数的集成机器学习方法，用于COVID-19筛查。通过转换多个颜色空间并采用多层感知器神经网络，可以提高模型的准确性。 |
| [^27] | [Spectral Clustering of Attributed Multi-relational Graphs.](http://arxiv.org/abs/2311.01840) | 本论文提出了一种针对带有节点属性的多关系图的联合降维技术SpectralMix，通过整合属性、不同类型关系和图结构的信息，实现对聚类结果的解释。该方法在多个数据集上的实验表明其有效性和性能优势。 |
| [^28] | [Mix-ME: Quality-Diversity for Multi-Agent Learning.](http://arxiv.org/abs/2311.01829) | Mix-ME是一种面向多智能体学习的质量-多样性方法，通过混合来自不同团队的智能体形成新的解决方案，为适应不同环境和需求提供了高性能和多样性。 |
| [^29] | [Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees.](http://arxiv.org/abs/2311.01806) | 本文提出了一种用于解决较大规模优化问题的快速速写算法，适用于凸或非凸正则化函数的最小二乘问题。相比已有的随机算法，该算法处理通用的Frechet子微分正则化函数并提供了一般的近似误差理论。同时，通过解决速写的稀疏凸或非凸学习问题，我们还得到了稀疏信号估计的极小极大速率。 |
| [^30] | [On the Generalization Properties of Diffusion Models.](http://arxiv.org/abs/2311.01797) | 本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。 |
| [^31] | [Learning to Augment Distributions for Out-of-Distribution Detection.](http://arxiv.org/abs/2311.01796) | 本研究提出了一种学习方法，通过增强分布的方式来区分在开放世界中的未知样本。具体来说，通过构造包含在辅助未知样本分布周围的Wasserstein球中所有分布的未知样本分布集，来减小分布差异，从而提升开放世界检测性能。 |
| [^32] | [CheX-Nomaly: Segmenting Lung Abnormalities from Chest Radiographs using Machine Learning.](http://arxiv.org/abs/2311.01777) | 这篇论文提出了一种使用机器学习从胸部X射线片中分割肺部异常的方法，通过使用二元定位U-net模型和创新的对比学习方法，该方法在不同疾病和未见过的疾病中都具有较好的推广能力。 |
| [^33] | [Efficient Generalized Low-Rank Tensor Contextual Bandits.](http://arxiv.org/abs/2311.01771) | 本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。 |
| [^34] | [Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel.](http://arxiv.org/abs/2311.01762) | 本文研究了使用梯度下降法解决非常数核的核岭回归。通过在训练过程中逐渐减小带宽，避免了超参数选择的需求，并提出了一种带宽更新方案，证明了其优于使用常数带宽的方法。 |
| [^35] | [TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices.](http://arxiv.org/abs/2311.01759) | TinyFormer是一个具有SuperNAS、SparseNAS和SparseEngine组成的框架，专门用于在MCUs上开发和部署资源高效的transformer模型。其创新之处在于提出了SparseEngine，这是第一个可以在MCUs上执行稀疏模型的transformer推理的部署框架。 |
| [^36] | [RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization.](http://arxiv.org/abs/2311.01753) | RiskQ是一种解决多智能体强化学习中风险敏感协调要求的方法，通过引入风险敏感的个体-全局最大（RIGM）原则和建模联合回报分布实现价值因子分解。 |
| [^37] | [Epidemic Decision-making System Based Federated Reinforcement Learning.](http://arxiv.org/abs/2311.01749) | 基于联邦强化学习的流行病决策系统可以帮助政府在兼顾公共安全和经济发展方面做出决策，并处理流行病数据样本有限和隐私性高的问题。 |
| [^38] | [Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario.](http://arxiv.org/abs/2311.01743) | 本文介绍了一种地下LoRaWAN网络能效优化的强化学习方法，该方法采用多智能体深度Q网络和多智能体优势演员-评论家算法来分配扩频因子，以最小化共享扩频因子干扰并优化系统的能效。 |
| [^39] | [Global Optimization: A Machine Learning Approach.](http://arxiv.org/abs/2311.01742) | 这篇论文介绍了一种机器学习方法来解决全球优化问题，通过使用基于超平面的决策树来近似非线性约束，并通过其他MIO可表示的ML模型来扩展原始问题的近似，同时提出了适应性采样程序和鲁棒优化技术来提高约束近似的准确性。 |
| [^40] | [CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model.](http://arxiv.org/abs/2311.01729) | 本文提出了一种新的条件扩散模型CDGraph用于合成社交图，通过捕捉双条件之间的相互依赖关系和保持节点连通性来实现生成高质量的社交网络。 |
| [^41] | [Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model.](http://arxiv.org/abs/2311.01727) | 提出了一种数据增强强化的神经模型，该模型可以灵活地缓解量子过程中的各种噪声，并展示了在不同类型量子过程中与先前方法相比的优越性能。 |
| [^42] | [Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces.](http://arxiv.org/abs/2311.01722) | 使用FAIR方法解决了异构联邦学习中的内存限制问题，实现了在空间不同的设备上集体训练嵌入表，用于推荐系统的个性化推荐。 |
| [^43] | [Physics-Informed Generator-Encoder Adversarial Networks with Latent Space Matching for Stochastic Differential Equations.](http://arxiv.org/abs/2311.01708) | 我们提出了一种基于物理知识的生成器-编码器对抗网络和潜在空间匹配的新方法，用于解决随机微分方程中的正向、反向和混合问题。与现有方法相比，我们的模型通过间接匹配潜在特征空间，有效避免高维输入和复杂数据分布所带来的挑战，并提供更精确的解决方案。此外，该方法还减轻了训练不稳定性问题。 |
| [^44] | [Adversarial Attacks on Cooperative Multi-agent Bandits.](http://arxiv.org/abs/2311.01698) | 该论文研究了合作多智能体赌徒问题中的对抗攻击，并提出了针对同质和异质设置的攻击策略，目标是影响其他智能体的决策。研究发现，在同质设置中，通过攻击只一个智能体，即可使所有智能体选择特定目标臂，而在异质设置中，攻击目标臂需要线性攻击成本，同时提出了攻击策略，可以使最大数量的智能体承受线性遗憾同时承担亚线性成本，并只操纵了少数目标智能体的观察结果。 |
| [^45] | [Communication-Efficient Federated Non-Linear Bandit Optimization.](http://arxiv.org/abs/2311.01695) | 本文提出了一种名为Fed-GO-UCB的新算法，用于具有通用非线性目标函数的联邦赌博优化。 |
| [^46] | [Disentangled Representation Learning with Transmitted Information Bottleneck.](http://arxiv.org/abs/2311.01686) | 本研究提出了一种通过引入传输信息瓶颈来实现解缠表示学习的方法。该方法可以在压缩表示信息和保留重要信息之间维持平衡，从而提高模型的稳健性和泛化能力。通过使用贝叶斯网络和变分推断，我们得到了可计算估计的DisTIB。 |
| [^47] | [Amide Proton Transfer (APT) imaging in tumor with a machine learning approach using partially synthetic data.](http://arxiv.org/abs/2311.01683) | 本研究介绍了一种使用部分合成数据和机器学习方法的新平台，旨在训练模型预测酰胺质子转移（APT）效应。通过将模拟数据和实测数据结合起来生成部分合成的数据，实现了模拟的灵活性和准确性之间的平衡。 |
| [^48] | [Maximum Likelihood Estimation of Flexible Survival Densities with Importance Sampling.](http://arxiv.org/abs/2311.01660) | 该论文提出了一种生存分析方法，通过引入重要抽样，消除了调整超参数的需求，如混合分配和箱尺寸，减轻了从业人员的负担。 |
| [^49] | [Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification.](http://arxiv.org/abs/2311.01655) | 这项工作介绍了一种通用方法，可以高效地检测潜在的伪相关关系，并且相比之下需要更少的人工干预。 |
| [^50] | [MARRS: Multimodal Reference Resolution System.](http://arxiv.org/abs/2311.01650) | MARRS是一个在设备上运行的多模态参考解析系统，能够处理对话式、视觉和背景上下文，并通过不同的机器学习模型实现上下文查询的处理。这个系统能够在保护用户隐私的同时理解上下文。 |
| [^51] | [Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs.](http://arxiv.org/abs/2311.01647) | 本论文研究了GNN在多关系和时间图上的逻辑表达能力，并证明了R$^2$-GNN模型在某些受限但合理的情况下等价于$\mathcal{FOC}_2$分类器。此外，为了克服R$^2$-GNN在表达能力方面的局限性，提出了一种简单的图转换技术。 |
| [^52] | [SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes.](http://arxiv.org/abs/2311.01646) | 本文介绍了一种称为SemiGPC的方法，通过使用高斯过程和标签的后验分布，实现了对不平衡半监督学习的分布感知标签精炼策略。实验结果表明，SemiGPC在不同的半监督方法和预训练策略中都取得了良好的性能，尤其是在低数据区间和类别不平衡情况下。 |
| [^53] | [Should Under-parameterized Student Networks Copy or Average Teacher Weights?.](http://arxiv.org/abs/2311.01644) | 这项研究探讨了在欠参数化情况下，学生网络是否应该复制教师神经元或平均一组教师神经元的权重。研究发现对于特定的网络结构和输入分布，当教师网络的输入向量正交且输出权重为酉时，复制-平均配置将达到优化结果，其中大部分学生神经元复制一个教师神经元，最后一个学生神经元对所有教师神经元取平均值。 |
| [^54] | [Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula.](http://arxiv.org/abs/2311.01642) | 本论文提出了一种基于熵正则化的对抗性强化学习方法，通过解决复杂的鞍点优化问题，实现了鲁棒性对抗攻击和分布偏移，从而在强化学习中取得了重要进展。 |
| [^55] | [VQPy: An Object-Oriented Approach to Modern Video Analytics.](http://arxiv.org/abs/2311.01623) | VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。 |
| [^56] | [Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks.](http://arxiv.org/abs/2311.01617) | 提出了一种新的持续学习机制，利用对比表示学习来减少灾难性遗忘，通过观察冗余诱导能力，识别并保留对神经网络转移能力最有贡献的参数，以实现在任务边界时的选择性可塑性。 |
| [^57] | [Faithful and Robust Local Interpretability for Textual Predictions.](http://arxiv.org/abs/2311.01605) | 提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。 |
| [^58] | [Local Borsuk-Ulam, Stability, and Replicability.](http://arxiv.org/abs/2311.01599) | 使用拓扑学中的Borsuk-Ulam定理，研究了列表可复制和全局稳定学习算法的限制。在不可知PAC设置中，这些学习算法是不可能的。在可实现的PAC设置中，提供了最佳界限和下界，与Littlestone维度有指数级分离。 |
| [^59] | [Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs.](http://arxiv.org/abs/2311.01591) | 该论文提出了一种针对公平GNN的对抗性缺失数据填充模型，以解决现有公平GNN的假设问题。实验证明此模型的有效性。 |
| [^60] | [A Statistical Guarantee for Representation Transfer in Multitask Imitation Learning.](http://arxiv.org/abs/2311.01589) | 多任务模仿学习中，通过使用足够多样的源任务训练表示，可以提高对新任务的样本效率。 |
| [^61] | [Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets.](http://arxiv.org/abs/2311.01588) | 该论文研究了通过领域适应图神经网络对宇宙学参数进行约束的方法。通过利用GNNs捕捉宇宙学信息和使用最大均值差异进行领域适应，该方法在不同数据集上具有较好的泛化能力。 |
| [^62] | [On the Convergence of Encoder-only Shallow Transformers.](http://arxiv.org/abs/2311.01575) | 本研究旨在构建仅使用编码器的浅层Transformer在有限宽度条件下的全局收敛理论，并通过处理softmax的输入/输出和证明二次超参数化的有效性来解决其收敛困难。 |
| [^63] | [Improving Lesion Segmentation in FDG-18 Whole-Body PET/CT scans using Multilabel approach: AutoPET II challenge.](http://arxiv.org/abs/2311.01574) | 本研究提出了一种新的方法来改进FDG-18全身PET / CT扫描中病变的分割效果，通过分割器官和病变，提高深度学习模型的性能。通过AutoPET II挑战数据集进行了有效性评估。 |
| [^64] | [Improving Fairness using Vision-Language Driven Image Augmentation.](http://arxiv.org/abs/2311.01573) | 本文提出了一种基于视觉语言驱动的图像增强方法，以改善训练深度学习模型中的公平性问题。通过学习和应用可解释的路径来编辑受保护特征，该方法成功减轻了数据中的相关性，提高了数据集的公平性。 |
| [^65] | [Sequential Subset Matching for Dataset Distillation.](http://arxiv.org/abs/2311.01570) | 这项研究提出了一种序列子集匹配方法来解决数据集精简时的性能下降问题，通过优化每个合成实例的方式提高了生成的合成数据集的质量。 |
| [^66] | [Anytime-Competitive Reinforcement Learning with Policy Prior.](http://arxiv.org/abs/2311.01568) | 本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。 |
| [^67] | [Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization.](http://arxiv.org/abs/2311.01544) | 本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。 |
| [^68] | [Variable Selection in Maximum Mean Discrepancy for Interpretable Distribution Comparison.](http://arxiv.org/abs/2311.01537) | 本文研究了数据集比较中的变量选择问题，提出了一种基于最大平均差异的两样本测试方法，通过优化自动相关性检测权重来增强测试的功效，并引入稀疏正则化方法来解决正则化参数选择的问题。 |
| [^69] | [ATGNN: Audio Tagging Graph Neural Network.](http://arxiv.org/abs/2311.01526) | ATGNN是一种新颖的图神经网络架构，将音频标记任务中的频谱图视为图结构，并结合了CNN的能力和图神经网络的全局信息共享能力，同时还映射了可学习类别嵌入和相应的频谱图区域之间的语义关系。 |
| [^70] | [An Efficient Detection and Control System for Underwater Docking using Machine Learning and Realistic Simulation: A Comprehensive Approach.](http://arxiv.org/abs/2311.01522) | 本论文提出了一种高效的水下对接检测和控制系统，采用机器学习和逼真仿真相结合的综合方法。通过比较不同的深度学习架构，并进行知识蒸馏压缩，实现了对于水下对接的实时检测和分类。 |
| [^71] | [E(2) Equivariant Neural Networks for Robust Galaxy Morphology Classification.](http://arxiv.org/abs/2311.01500) | 提出了一种利用2D欧几里得群E(2)等变的神经网络架构，用于鲁棒的星系形态分类。研究表明，这种等变网络在鲁棒性和分类准确性方面优于非等变的网络。 |
| [^72] | [Investigating the Behavior of Diffusion Models for Accelerating Electronic Structure Calculations.](http://arxiv.org/abs/2311.01491) | 本研究调查了用于分子生成的扩散模型的行为，并发现这些模型能够通过机器学习加速电子结构计算，而无需昂贵的第一原理数据集。模型推断过程包括探索阶段和松弛阶段，模型能够学习势能表面的一阶和高阶结构，并且松弛阶段还可以用于采样玻尔兹曼分布。 |
| [^73] | [FedSN: A General Federated Learning Framework over LEO Satellite Networks.](http://arxiv.org/abs/2311.01483) | FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。 |
| [^74] | [Detecting Out-of-Distribution Through the Lens of Neural Collapse.](http://arxiv.org/abs/2311.01479) | 通过观察到的群内特征聚集和群外特征离散的性质，本论文提出了一种基于特征和权重向量接近程度的神经坍塌（NC-OOD）检测器来提高OAD检测的泛化能力，并取得了最先进的效果。 |
| [^75] | [Adversary ML Resilience in Autonomous Driving Through Human Centered Perception Mechanisms.](http://arxiv.org/abs/2311.01478) | 本研究通过开发和评估多个机器学习模型，研究了自动驾驶系统在胶带、涂鸦和光照等三种物理对抗攻击下的韧性，主要针对对象分类器。研究结果表明当前模型在泛化输入数据方面存在过拟合和欠拟合问题。 |
| [^76] | [Applications of the Theory of Aggregated Markov Processes in Stochastic Learning Theory.](http://arxiv.org/abs/2311.01476) | 本文描述了聚合马尔可夫过程（AMP）的理论如何应用于随机学习理论中，以降低维度并实现学习特定任务的目标。 |
| [^77] | [Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts.](http://arxiv.org/abs/2311.01475) | 本文提出了一种基于补丁的深度无监督图像分割策略，将深度聚类方法和经典的图割方法相结合，通过训练简单的卷积神经网络对图像补丁进行分类，并利用图割算法进行迭代正则化，实现了最先进的分割效果。 |
| [^78] | [Adversarial Examples in the Physical World: A Survey.](http://arxiv.org/abs/2311.01473) | 本综述系统地研究了物理世界中的对抗样本（PAEs）的特点，并提出了基于其特征的全面分析和分类框架，涵盖了100多个研究，以填补对PAEs独特特征的现有研究不足。 |
| [^79] | [Leveraging Language Models to Detect Greenwashing.](http://arxiv.org/abs/2311.01469) | 本研究引入了一种新的方法，利用语言模型来检测绿色虚假宣传风险。开发了一种量化绿色虚假宣传风险的数学形式，建立了优化的ClimateBERT模型，并进行了结果比较分析。实验表明，我们的方法对于这一任务具有良好的探索方向。 |
| [^80] | [Remember what you did so you know what to do next.](http://arxiv.org/abs/2311.01468) | 本文通过使用一个大型语言模型（LLM）为模拟机器人制定计划，在ScienceWorld中实现30类目标。实验结果显示，LLM在马尔可夫假设的情况下比强化学习方法的性能提高了1.4倍，当填充尽可能多的先前步骤时提高到3.5倍，即使只训练了6.5%的数据，也比基于强化学习方法的性能提高了2.2倍。不同类别的动作表现差异很大，说明平均任务可能会隐藏性能问题。 |
| [^81] | [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI.](http://arxiv.org/abs/2311.01463) | 这篇论文描述了在医疗人工智能中创建可靠、可信和无偏置的LLM模型的关键要素，着重于量化、验证和缓解幻觉问题，并讨论了LLM在医疗领域的未来发展。 |
| [^82] | [FlashDecoding++: Faster Large Language Model Inference on GPUs.](http://arxiv.org/abs/2311.01282) | FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。 |
| [^83] | [Improving Interpersonal Communication by Simulating Audiences with Language Models.](http://arxiv.org/abs/2311.00687) | 本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。 |
| [^84] | [Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows.](http://arxiv.org/abs/2311.00060) | 本研究通过集成技术扩展了DeepONet模型，该模型可以在高超声速流动环境中对未知输入进行可靠和置信的预测。 |
| [^85] | [CapsFusion: Rethinking Image-Text Data at Scale.](http://arxiv.org/abs/2310.20550) | CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。 |
| [^86] | [Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods.](http://arxiv.org/abs/2310.20380) | 本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。 |
| [^87] | [Advancing Bayesian Optimization via Learning Correlated Latent Space.](http://arxiv.org/abs/2310.20258) | 本文提出了一种通过学习相关的潜在空间来推进贝叶斯优化的方法。该方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以减小在潜在空间和目标函数之间的差距，并在多个优化任务中展示了其有效性。 |
| [^88] | [Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing.](http://arxiv.org/abs/2310.19589) | 该论文提出了一种新的使用非线性消息传递的规范等变架构，用于在网格上建模具有复杂非线性动力学的表面PDE，实现了比卷积或注意力网络更高的性能。 |
| [^89] | [Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization.](http://arxiv.org/abs/2310.18860) | 本文提出了一种基于贝叶斯公式的岭回归方法，通过期望最大化来调节正则化超参数，该方法不需要指定候选的λ并且在大样本下可以找到唯一的最优解。 |
| [^90] | [Improving Intrinsic Exploration by Creating Stationary Objectives.](http://arxiv.org/abs/2310.18144) | 该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。 |
| [^91] | [Learning Extrinsic Dexterity with Parameterized Manipulation Primitives.](http://arxiv.org/abs/2310.17785) | 论文通过学习一系列参数化操作原语，并利用环境改变物体姿态，解决了机器人在目标物体抓取被环境遮挡时的问题。 |
| [^92] | [Managing AI Risks in an Era of Rapid Progress.](http://arxiv.org/abs/2310.17688) | 在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。 |
| [^93] | [A minimax optimal control approach for robust neural ODEs.](http://arxiv.org/abs/2310.17584) | 本文提出了一种鲁棒神经ODE的极小极大优化控制方法，将对抗训练问题转化为一个最优控制问题，并提供了一种新的加权技术来实现鲁棒训练。 |
| [^94] | [Causal Q-Aggregation for CATE Model Selection.](http://arxiv.org/abs/2310.16945) | 该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率 |
| [^95] | [Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model.](http://arxiv.org/abs/2310.16804) | 该论文提出了使用普适微分方程（UDEs）改进SIR模型来学习COVID-19的区域传播。通过在SIR方程中加入一个深度神经网络（DNN），可以更好地捕捉邻近地区的影响，并改进模型的预测能力。 |
| [^96] | [Detecting Pretraining Data from Large Language Models.](http://arxiv.org/abs/2310.16789) | 这项研究探讨了如何检测大型语言模型的预训练数据，提出了一个动态基准和一种新的检测方法，以解决数据隐私和不透明性的问题。 |
| [^97] | [Prompt Engineering Through the Lens of Optimal Control.](http://arxiv.org/abs/2310.14201) | 本文通过最优控制的视角提出了一个针对大型语言模型的多轮交互的提示工程框架，系统化了现有方法，并扩大了适用范围。 |
| [^98] | [Graph Neural Networks with polynomial activations have limited expressivity.](http://arxiv.org/abs/2310.13139) | 本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。 |
| [^99] | [How a student becomes a teacher: learning and forgetting through Spectral methods.](http://arxiv.org/abs/2310.12612) | 本论文提出了基于谱方法的优化方案，用于解决在非凸性问题下学生网络与教师网络之间存在的不变子网络的识别问题。 |
| [^100] | [Fast Model Debias with Machine Unlearning.](http://arxiv.org/abs/2310.12560) | 这篇论文提出了一种快速模型去偏置的框架（FMD），可以有效识别、评估和消除深度神经网络中的偏见，解决了现有方法在成本和解释性方面的不足。 |
| [^101] | [Rank-DETR for High Quality Object Detection.](http://arxiv.org/abs/2310.08854) | Rank-DETR是一种高质量目标检测方法，通过引入排名导向的设计，包括架构设计和损失函数设计，实现了性能卓越的基于DETR的目标检测器。 |
| [^102] | [Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders.](http://arxiv.org/abs/2310.08571) | Bucks for Buckets (B4B) is the first active defense against stealing encoders, which prevents model stealing attacks by adaptively adjusting the utility of the returned representations based on the coverage of the embedding space by the user. |
| [^103] | [Observatory: Characterizing Embeddings of Relational Tables.](http://arxiv.org/abs/2310.07736) | Observatory提出了一个正式框架来分析关系表的嵌入表示，以帮助研究人员和实践者更好地理解和选择适合特定任务的模型。 |
| [^104] | [Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data.](http://arxiv.org/abs/2310.07223) | 这项研究利用MODIS多光谱时间序列数据和深度学习模型，首次实现了对LULC类别的盲目光谱分离。通过添加地理加地形和气候辅助信息，进一步提高了模型的性能。 |
| [^105] | [Provably Convergent Data-Driven Convex-Nonconvex Regularization.](http://arxiv.org/abs/2310.05812) | 本研究展示了在凸非凸框架中，通过从数据中学习正则化器，可以实现收敛正则化；引入了一种新颖的弱凸输入神经网络构建，解决了之前对抗性方法的数值问题。 |
| [^106] | [GRANDE: Gradient-Based Decision Tree Ensembles.](http://arxiv.org/abs/2309.17130) | 这篇论文提出了一种名为GRANDE的基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成，并结合了轴对齐分割和梯度优化的灵活性，引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。 |
| [^107] | [MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning.](http://arxiv.org/abs/2309.09599) | 本文提出了一种基于证据深度学习的方法，用于解决3D对象检测中伪标签的模糊性问题，并生成准确的伪标签和量化伪标签的不确定性。 |
| [^108] | [On the limitations of data-driven weather forecasting models.](http://arxiv.org/abs/2309.08473) | 数据驱动的机器学习天气预报模型不具备传统基于物理的模型的准确性和物理一致性，它们在预测技能上的优势很大程度上可以归因于这些特殊性。 |
| [^109] | [Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI.](http://arxiv.org/abs/2308.16150) | 本文介绍了一种名为遮蔽模态循环与条件扩散的方法，该方法能够对多模态MRI中的异常进行分割。方法基于循环模态转换和条件扩散的思想，能够检测到训练中未遇到的异常模式。 |
| [^110] | [From SMOTE to Mixup for Deep Imbalanced Classification.](http://arxiv.org/abs/2308.15457) | 本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。 |
| [^111] | [ChatGPT for GTFS: From Words to Information.](http://arxiv.org/abs/2308.02618) | 本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。 |
| [^112] | [An Empirical Study on Fairness Improvement with Multiple Protected Attributes.](http://arxiv.org/abs/2308.01923) | 本文通过广泛研究，发现对于单个保护属性的公平性改善会大大降低对未考虑保护属性的公平性，但在多属性模式下可以保持准确性。 |
| [^113] | [General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset.](http://arxiv.org/abs/2308.00180) | 本文介绍了一种用于评估海底滑翔器在不可预测海洋环境中正常操作的异常检测算法，并通过实际滑翔器部署的大规模数据集进行验证。算法能够实时提供异常警报，使驾驶员能够控制滑翔器并避免进一步损害。 |
| [^114] | [General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications.](http://arxiv.org/abs/2307.14283) | 这里是中文总结出的一句话要点：本论文讨论了通用目的人工智能系统（GPAIS）的性质、定义、分类和开放挑战，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。 |
| [^115] | [High-performance real-world optical computing trained by in situ model-free optimization.](http://arxiv.org/abs/2307.11957) | 本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。 |
| [^116] | [Fractional Denoising for 3D Molecular Pre-training.](http://arxiv.org/abs/2307.10683) | 本论文提出了一种分数降噪算法，用于3D分子预训练。通过混合噪声策略解决了样本覆盖率低和各向同性力场的挑战，通过解耦两种类型的噪声克服了传统降噪方法无法学习力场的问题。 |
| [^117] | [Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information.](http://arxiv.org/abs/2307.08964) | 本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。 |
| [^118] | [An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient.](http://arxiv.org/abs/2307.08873) | 本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。 |
| [^119] | [Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems.](http://arxiv.org/abs/2307.05374) | 首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性，并且通过一个"单一"的基于神经网络的均衡器，在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。 |
| [^120] | [When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment.](http://arxiv.org/abs/2307.03864) | Transformer在强化学习中的作用是增强记忆能力而不是改进信用分配。 |
| [^121] | [LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning.](http://arxiv.org/abs/2307.02345) | 本研究通过研究在线和离线增强学习中 Bellman 近似误差的分布发现，Bellman 误差符合逻辑分布。基于这一发现，本研究提出了一种使用 Logistic 最大似然函数作为替代方法的方案，并通过实验证明了其有效性。 |
| [^122] | [Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization.](http://arxiv.org/abs/2307.02108) | 这篇论文提出了一种适用于情境赌博设置的新型计算效率高的赌博算法，具有简单和累积遗憾最小化的优势，并可自适应模型错误规范和连续臂设置。该算法利用"一致臂集"（CAS）来提供在每个情境下囊括情境特定的最佳臂的一组臂，跨越情境分布。这篇论文对简单和累积遗憾保证的研究提供了正面结果，同时也揭示了无法实现实例依赖性的简单遗憾保证的消极结果。 |
| [^123] | [Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses.](http://arxiv.org/abs/2307.01827) | 这项研究探讨了神经网络内部对训练数据的记忆过程，并在多个方向上扩展了已有的研究。研究发现，使用权重衰减可以增加重建能力，同时还分析了神经元数量对网络易受重建方案影响的影响。 |
| [^124] | [Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges.](http://arxiv.org/abs/2307.01050) | 本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。 |
| [^125] | [Adaptive Algorithms for Relaxed Pareto Set Identification.](http://arxiv.org/abs/2307.00424) | 本研究提出了一种自适应算法，用于宽松Pareto集的识别，通过放松策略来减少样本复杂度，并展示了在实际场景中的良好表现。 |
| [^126] | [Allocating Divisible Resources on Arms with Unknown and Random Rewards.](http://arxiv.org/abs/2306.16578) | 本论文研究了在每个周期将一单位可分资源分配到多个臂上的问题，臂上的奖励是未知和随机的，而且与分配的资源成比例，而方差与分配资源的阶数成比例。我们设计了两种算法，实现了不同阶数下的最优有界和无界遗憾，结果表明在阶数为1/2时存在相变现象。 |
| [^127] | [Guiding Language Models of Code with Global Context using Monitors.](http://arxiv.org/abs/2306.10763) | 本文提出了一种使用监视器引导全局上下文的方法来指导代码语言模型，在处理类型、功能或API等全局上下文时，能够提高代码语言模型的性能和准确性。 |
| [^128] | [Logarithmic Bayes Regret Bounds.](http://arxiv.org/abs/2306.09136) | 该论文提出了对于贝叶斯赌博机的首个有限时间对数遗憾边界，并用于高斯和线性赌博机，从而阐明了贝叶斯设置中先验价值以及对$\tilde{O}(\sqrt{n})$界限的改善。 |
| [^129] | [Long Sequence Hopfield Memory.](http://arxiv.org/abs/2306.04532) | 这篇论文提出了一种增强Hopfield-like神经网络序列记忆模型的序列容量的方法，通过引入非线性相互作用项，显著优于传统Hopfield网络，同时也引入了一个新的回忆规则以回忆连续的序列。 |
| [^130] | [Learning nonparametric latent causal graphs with unknown interventions.](http://arxiv.org/abs/2306.02899) | 本文提出了一种学习具有未知干预的非参数潜在因果图的方法，通过建立条件确定非参数潜在因果图并从中重构。这种方法不需要参数假设，可用于识别测量模型中潜在结构。 |
| [^131] | [Fine-Tuning Language Models with Advantage-Induced Policy Alignment.](http://arxiv.org/abs/2306.02231) | 本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。 |
| [^132] | [Convex and Non-Convex Optimization under Generalized Smoothness.](http://arxiv.org/abs/2306.01264) | 本文发展了一种新的分析技术，并推广了广义平滑度条件，使凸和非凸优化问题获得更强的结果。在该条件下，获得了（随机）梯度下降和Nesterov加速梯度方法的经典收敛率。 |
| [^133] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^134] | [Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.](http://arxiv.org/abs/2305.18381) | 研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。 |
| [^135] | [Scalable Transformer for PDE Surrogate Modeling.](http://arxiv.org/abs/2305.17560) | 我们提出了一种基于轴向分解核积分的Factorized Transformer模型，用于PDE模型代用。该模型在具有大量网格点的问题上能够保持较好的准确性和计算效率。 |
| [^136] | [A Unified Approach for Maximizing Continuous DR-submodular Functions.](http://arxiv.org/abs/2305.16671) | 本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。 |
| [^137] | [Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation.](http://arxiv.org/abs/2305.14637) | 本文提出了一种新型的视觉-代码Transformer方法，通过actor-critic微调来改善基线，比较了Vision Transformer和Document Image Transformer这两种图像编码器，提出了一种端到端的流水线，可以直接从屏幕截图生成高质量的代码片段，创建了30,000个独特的代码和对应截图的合成数据集，并使用多种自动化指标来评估这种方法的性能，建立了一个强大的基准模型。 |
| [^138] | [Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference.](http://arxiv.org/abs/2305.13484) | Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。 |
| [^139] | [Universal Domain Adaptation from Foundation Models.](http://arxiv.org/abs/2305.11092) | 本论文对基于基础模型的通用域适应进行了研究，发现当前的UniDA方法无法超越基准表现，提出了一个简单的目标方法。 |
| [^140] | [Differentially Private Topological Data Analysis.](http://arxiv.org/abs/2305.03609) | 本文尝试使用差分隐私实现拓扑数据分析并生成接近最优的私有持久图，提出使用 $L^1$-距离计算持久图并采用指数机制保护隐私，成功实现在隐私保护和数据分析之间的平衡。 |
| [^141] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^142] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^143] | [Why think step-by-step? Reasoning emerges from the locality of experience.](http://arxiv.org/abs/2304.03843) | 本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。 |
| [^144] | [Adversarial Attacks against Binary Similarity Systems.](http://arxiv.org/abs/2303.11143) | 本文研究了二进制相似性模型在对抗性环境中的韧性，表明它们容易受到有针对性和无针对性攻击，并向黑盒和白盒攻击者展示了这种易受攻击的情况。 |
| [^145] | [Object-Centric Slot Diffusion.](http://arxiv.org/abs/2303.10834) | 基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。 |
| [^146] | [To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning.](http://arxiv.org/abs/2303.03374) | 该论文研究了在迁移学习中使用单个预训练检查点微调的模型集合，发现通过更好地探索预训练基域可以改进集成模型，但离开基域会导致失去迁移学习的好处，并且降低集成质量。作者提出了一种更有效的修改方法StarSSE，可以产生更强的集成模型和均匀的模型混合。 |
| [^147] | [Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories.](http://arxiv.org/abs/2302.14082) | 本论文研究了归一化流在格点场论中的应用，并发现归一化流经常遭受模式崩溃问题，在训练阶段将相关模式的概率质量赋予极低的值。我们提出了度量模式崩溃程度的指标，并提出了减轻偏差的策略。 |
| [^148] | [Algorithm Selection for Deep Active Learning with Imbalanced Datasets.](http://arxiv.org/abs/2302.07317) | 本论文提出了适用于不平衡数据集下的深度主动学习的自适应算法选择策略TAILOR，它在多类和多标签应用的实验中取得了与候选算法相当或更高的准确性。 |
| [^149] | [SEGA: Instructing Text-to-Image Models using Semantic Guidance.](http://arxiv.org/abs/2301.12247) | 本论文介绍了一种称为SEGA的语义引导方法，用于指导文本到图像模型的生成过程。通过与扩散过程的互动，SEGA可以灵活地在语义方向上引导模型的生成，实现细微和广泛的编辑以及优化整体艺术构思。实验证明SEGA在多种任务和生成架构上都表现出了出色的多功能性、灵活性和改进。 |
| [^150] | [Tracr: Compiled Transformers as a Laboratory for Interpretability.](http://arxiv.org/abs/2301.05062) | Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。 |
| [^151] | [A large-scale and PCR-referenced vocal audio dataset for COVID-19.](http://arxiv.org/abs/2212.07738) | 英国COVID-19 Vocal Audio Dataset是迄今为止最大的SARS-CoV-2 PCR参考音频记录集合，旨在为训练和评估使用声音数据分类SARS-CoV-2感染状态或相关呼吸症状的机器学习模型而设计。 |
| [^152] | [Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks.](http://arxiv.org/abs/2211.08761) | 提出了一种名为可分离PINN（SPINN）的网络架构来减轻物理信息神经网络中的维度诅咒，并通过利用前向模式自动微分实现了更高效的计算。 |
| [^153] | [Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning.](http://arxiv.org/abs/2211.07596) | 本文提出了一种基于偏好的强化学习方法，在抽象摘要的基础上优化时间线摘要的质量和可读性，通过自动化和人工评估验证了方法的有效性。 |
| [^154] | [Cost-aware Generalized $\alpha$-investing for Multiple Hypothesis Testing.](http://arxiv.org/abs/2210.17514) | 本文提出了成本感知的通用α投资法进行多重假设检验，拓展了α投资规则以考虑样本大小，通过构建与自然对抗的博弈来优化α财富的期望回报(ERO)并提供最佳样本大小。经实证表明，该规则能更准确地拒绝虚假零假设。 |
| [^155] | [Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond.](http://arxiv.org/abs/2209.06177) | 本论文研究了用于节点分类的图形数据集的特征化，发现目前常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。作者提出了一种新的同质性测量指标，称为调整同质性，该指标满足更多期望特性，并具有较少在图形机器学习文献中使用的特点。 |
| [^156] | [Bayesian learning of feature spaces for multitasks problems.](http://arxiv.org/abs/2209.03028) | 本文介绍了一种贝叶斯学习的方法，通过连接核机器和极限学习机，实现了在多任务回归问题中的特征空间的学习。该方法提供了优化RBF核参数、模型复杂度和多输出稀疏性的能力。 |
| [^157] | [Recognition of Unseen Bird Species by Learning from Field Guides.](http://arxiv.org/abs/2206.01466) | 这篇论文提出了通过学习领域指南中的插图进行未知鸟类物种的零样本识别。通过对插图进行对比编码和结构上更类似于照片的方法，我们实现了12%的top-1分类准确率和38%的top-10分类准确率，证明了领域指南插图作为辅助信息的潜力。 |
| [^158] | [Graph Neural Diffusion Networks for Semi-supervised Learning.](http://arxiv.org/abs/2201.09698) | 提出了一种名为 GND-Nets 的图神经网络，利用浅层网络和局部、全局邻域信息来解决图半监督学习中的过度平滑和欠平滑问题。 |
| [^159] | [Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning.](http://arxiv.org/abs/2112.08369) | "Feature-Attending Recurrent Modules" (FARM)是一种学习状态表示的体系结构，通过特征注意机制来捕捉空间和时间规律性，从而改善强化学习代理的泛化能力。 |
| [^160] | [Phase transitions in nonparametric regressions.](http://arxiv.org/abs/2112.03626) | 本文研究了非参数回归中的相变问题，根据最小极大MISE速率的不同情况，确定了不同范围内最佳的平滑度。同时，本文还提出了一组用于平滑函数类别的度量熵界限。 |
| [^161] | [On minimizers and convolutional filters: theoretical connections and applications to genome analysis.](http://arxiv.org/abs/2111.08452) | 该论文通过对哈希函数属性进行数学分析，发现在分类字母表上的序列分析中，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一种最小化器排序，能够有效提取与其他最小化器距离较近但与序列中的k-mer相距较远的重要特征。 |
| [^162] | [Numerical influence of ReLU'(0) on backpropagation.](http://arxiv.org/abs/2106.12915) | 本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。 |
| [^163] | [Minimax Quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function.](http://arxiv.org/abs/2010.08627) | 本研究提出了一种利用重新缩放的瑞利商函数作为准对数似然函数并采用贝叶斯框架的方法，通过马尔科夫链蒙特卡罗计算稀疏规范向量的估计值。实验结果表明，该方法在连续和截断数据上表现优于其他方法。 |
| [^164] | [Recurrent Neural-Linear Posterior Sampling for Nonstationary Contextual Bandits.](http://arxiv.org/abs/2007.04750) | 该论文提出了一种递归神经线性后验抽样的方法，用于解决非平稳情境下的情境赌博问题。实验证明该方法能够有效地表示相关情境并做出决策。 |

# 详细

[^1]: 神经网络训练中的普适锐度动力学：固定点分析、稳定边界和混沌路径

    Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos. (arXiv:2311.02076v1 [cs.LG])

    [http://arxiv.org/abs/2311.02076](http://arxiv.org/abs/2311.02076)

    本研究通过分析神经网络训练中的锐度动力学，揭示出早期锐度降低、逐渐增加锐化和稳定边界的机制，并发现增大学习率时，稳定边界流形上发生倍增混沌路径。

    

    在神经网络的梯度下降动力学中，损失函数海森矩阵的最大特征值（锐度）在训练过程中展示出各种稳健的现象。这包括早期时间阶段，在训练的早期阶段锐度可能减小（降低锐度），以及后期行为，如逐渐增加的锐化和稳定边界。我们证明了一个简单的2层线性网络（UV模型），在单个训练样本上训练，展示了在真实场景中观察到的所有关键锐度现象。通过分析函数空间中动力学固定点的结构和函数更新的向量场，我们揭示了这些锐度趋势背后的机制。我们的分析揭示了：(i)早期锐度降低和逐渐增加锐化的机制，(ii)稳定边界所需的条件，以及 (iii)当学习率增加时，稳定边界流形上的倍增混沌路径.

    In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, and (iii) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Fi
    
[^2]: 基于主动学习的物种分布范围估计

    Active Learning-Based Species Range Estimation. (arXiv:2311.02061v1 [cs.LG])

    [http://arxiv.org/abs/2311.02061](http://arxiv.org/abs/2311.02061)

    这项研究提出了一种基于主动学习的方法，能够从有限数量的地面观测中高效估计物种的地理分布范围。通过建模未测绘物种的范围为不同物种的估计范围的加权组合，并使用训练于大规模弱监督观测数据的模型生成候选范围集合，然后通过新的主动查询方法选择地理位置进行访问以减少对未测绘物种范围的不确定性。实验结果表明，这种方法优于其他主动学习方法，并接近端到端训练模型的性能。

    

    我们提出了一种新的基于主动学习的方法，可以高效地从有限数量的地面观测中估计物种的地理分布范围。我们将未测绘物种的范围建模为从一组不同物种的估计范围的加权组合。我们展示了可以使用在大规模弱监督社区收集的观测数据上训练的模型生成这个候选范围集合。基于此，我们开发了一种新的主动查询方法，顺序选择最能减少对未测绘物种范围的不确定性的地理位置进行访问。我们对我们的方法进行了详细评估，并使用包含一千种物种的专家推导范围的评估数据集将其与现有的主动学习方法进行比较。我们的结果表明，我们的方法优于其他主动学习方法，并且在仅使用有限观测数据时可以接近端到端训练模型的性能。

    We propose a new active learning approach for efficiently estimating the geographic range of a species from a limited number of on the ground observations. We model the range of an unmapped species of interest as the weighted combination of estimated ranges obtained from a set of different species. We show that it is possible to generate this candidate set of ranges by using models that have been trained on large weakly supervised community collected observation data. From this, we develop a new active querying approach that sequentially selects geographic locations to visit that best reduce our uncertainty over an unmapped species' range. We conduct a detailed evaluation of our approach and compare it to existing active learning methods using an evaluation dataset containing expert-derived ranges for one thousand species. Our results demonstrate that our method outperforms alternative active learning methods and approaches the performance of end-to-end trained models, even when only u
    
[^3]: LOTUS：通过无监督技能发现的持续模仿学习，用于机器人操作

    LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])

    [http://arxiv.org/abs/2311.02058](http://arxiv.org/abs/2311.02058)

    LOTUS是一种持续模仿学习算法，通过无监督技能发现，使得机器人能够在其整个寿命中持续学习解决新的操作任务。该算法通过构建技能库，并使用元控制器灵活组合技能来提高成功率，在实验中表现出优越的知识传递能力。

    

    我们介绍了一种名为LOTUS的持续模仿学习算法，它使得物理机器人能够在其整个寿命中持续而高效地学习解决新的操作任务。LOTUS的核心思想是通过一系列新任务的少量人类演示构建一个不断增长的技能库。LOTUS首先使用开放词汇视觉模型进行持续技能发现过程，该模型从未分段的演示中提取重复出现的技能模式。持续技能发现更新现有技能以避免对以前任务的灾难性遗忘，并添加新技能以解决新任务。LOTUS训练一个元控制器，在终身学习过程中灵活地组合各种技能来解决基于视觉的操作任务。我们的综合实验证明，与先前方法相比，LOTUS在成功率上超过了现有技术基线方法11％以上，显示了其优越的知识传递能力。

    We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
    
[^4]: 用扩散模型进行量子电路合成

    Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])

    [http://arxiv.org/abs/2311.02041](http://arxiv.org/abs/2311.02041)

    该论文提出了一种利用扩散模型进行量子电路合成的方法，通过使用生成式机器学习模型，可以在基于门的量子电路中产生所需的量子操作，而且能够绕过经典模拟量子动力学的指数级开销。实验证明该模型在纠缠生成和酉编译等任务中表现优秀，并支持扩展功能以适应不同的量子设备约束条件。

    

    量子计算最近成为一项具有变革性的技术。然而，它所承诺的优势依赖于将量子操作有效地转化为可行的物理实现。在此工作中，我们使用生成式机器学习模型，具体而言是去噪扩散模型（DMs），以促进这种转化。通过文本条件，我们引导模型在基于门的量子电路中产生所需的量子操作。值得注意的是，DMs允许在训练过程中避免经典模拟量子动力学中固有的指数级开销，这是先前机器学习技术中一直存在的瓶颈。我们在两个任务上展示了该模型的能力：纠缠生成和酉编译。该模型在生成新电路方面表现出色，并支持典型的DM扩展，例如掩码和编辑，以使电路生成符合目标量子设备的约束条件。由于其灵活性和泛化能力，我们的方法可以应用于各种量子任务。

    Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we
    
[^5]: 使用袋装后验进行可重现的参数推断

    Reproducible Parameter Inference Using Bagged Posteriors. (arXiv:2311.02019v1 [stat.ME])

    [http://arxiv.org/abs/2311.02019](http://arxiv.org/abs/2311.02019)

    通过使用袋装方法，提出了一种易于使用且广泛适用的方法来改善在模型错误规范下的可重现性。

    

    在模型错误规范下，已知贝叶斯后验通常不能正确量化关于真实或伪真参数的不确定性。更重要的是，错误规范会导致在独立数据集上同一模型产生相互矛盾的后验结果，从而缺乏可重现性。为了定义在错误规范下可重现不确定性量化的标准，我们考虑从独立数据集构建的两个置信区间具有非空交集的概率，并为任何有效置信区间建立了该交集概率的下界。我们证明了标准后验的可信区间在高维设置下（即样本大小增加时的维度）可以严重违反这个下界，表明在错误规范下它不是内部一致的。为了提高易于使用且广泛适用的可重现性，我们建议应用袋装方法。

    Under model misspecification, it is known that Bayesian posteriors often do not properly quantify uncertainty about true or pseudo-true parameters. Even more fundamentally, misspecification leads to a lack of reproducibility in the sense that the same model will yield contradictory posteriors on independent data sets from the true distribution. To define a criterion for reproducible uncertainty quantification under misspecification, we consider the probability that two confidence sets constructed from independent data sets have nonempty overlap, and we establish a lower bound on this overlap probability that holds for any valid confidence sets. We prove that credible sets from the standard posterior can strongly violate this bound, particularly in high-dimensional settings (i.e., with dimension increasing with sample size), indicating that it is not internally coherent under misspecification. To improve reproducibility in an easy-to-use and widely applicable way, we propose to apply ba
    
[^6]: DeliverAI: 强化学习为基础的分布式路径共享网络用于食品配送

    DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])

    [http://arxiv.org/abs/2311.02017](http://arxiv.org/abs/2311.02017)

    DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。

    

    在过去十年中，从生产者到消费者的物品配送经历了显著的增长，并且最近的流行病进一步推动了这一增长。亚马逊生鲜、Shopify、UberEats、InstaCart和DoorDash正在迅速发展，并共享相同的消费品或食品配送业务模式。现有的食品配送方法存在缺陷，因为每次配送都是在最短时间路径上从生产者直接到消费者进行优化。我们观察到，在当前模型下，有很大的减少配送成本的空间。我们将我们的食品配送问题建模为一个多目标优化问题，消费者满意度和配送成本都需要进行优化。受出租车行业中拼车成功的启发，我们提出了DeliverAI - 一种基于强化学习的路径共享算法。与以前的路径共享尝试不同，DeliverAI可以提供实时、时间高效的决策。

    Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
    
[^7]: 离线目标条件强化学习的评分模型

    Score Models for Offline Goal-Conditioned Reinforcement Learning. (arXiv:2311.02013v1 [cs.LG])

    [http://arxiv.org/abs/2311.02013](http://arxiv.org/abs/2311.02013)

    本文提出了一种新颖的离线目标条件强化学习方法，称为SMORe，它将占有匹配的视角与混合分布匹配相结合，无需学习鉴别器，从而提高了GCRL在离线环境中的表现。

    

    离线目标条件强化学习（GCRL）的任务是使用稀疏奖励函数从离线数据集中学习在环境中实现多个目标。离线GCRL对于开发能够利用预先存在的数据集学习多样化和可复用技能的通用型代理至关重要，而无需手工设计奖励函数。然而，基于监督学习和对比学习的现代GCRL方法在离线环境中往往不太理想。GCRL的另一种观点是优化占有匹配，但需要学习鉴别器，随后该鉴别器作为下游强化学习的伪奖励。学习到的鉴别器的不准确性可能会导致负面影响，进而影响生成的策略。我们提出了一种新颖的GCRL方法，基于混合分布匹配的新视角，采用无鉴别器的方法：SMORe。关键洞见是将GCRL的占有匹配视角与一个有效的聚类算法相结合，从而得到了我们的无鉴别器方法：SMORe。

    Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a conve
    
[^8]: 关于高分辨率ODE的一种变分视角

    A Variational Perspective on High-Resolution ODEs. (arXiv:2311.02002v1 [math.OC])

    [http://arxiv.org/abs/2311.02002](http://arxiv.org/abs/2311.02002)

    该论文提出了一种基于变分视角的方法来研究高分辨率ODE，并通过将Nesterov加速梯度法应用于梯度范数最小化问题，得到了更快的收敛速度。此外，论文还展示了Nesterov的方法可以看作是对高分辨率ODE的合适离散。

    

    我们考虑光滑凸函数的非约束最小化问题。我们提出了一种新颖的变分视角，利用强制性的Euler-Lagrange方程来研究高分辨率ODE。通过这个方法，我们得到了使用Nesterov的加速梯度法进行梯度范数最小化的更快收敛速度。此外，我们还展示了Nesterov的方法可以被解释为对一个合适选择的高分辨率ODE进行匹配率离散化。最后，利用这种新变分视角的结果，我们提出了一种用于噪声梯度的随机方法。多个数值实验比较和说明了我们的随机算法与现有方法的优势。

    We consider unconstrained minimization of smooth convex functions. We propose a novel variational perspective using forced Euler-Lagrange equation that allows for studying high-resolution ODEs. Through this, we obtain a faster convergence rate for gradient norm minimization using Nesterov's accelerated gradient method. Additionally, we show that Nesterov's method can be interpreted as a rate-matching discretization of an appropriately chosen high-resolution ODE. Finally, using the results from the new variational perspective, we propose a stochastic method for noisy gradients. Several numerical experiments compare and illustrate our stochastic algorithm with state of the art methods.
    
[^9]: Adam算法在无界梯度和仿射方差噪声下的高概率收敛性研究

    High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise. (arXiv:2311.02000v1 [math.OC])

    [http://arxiv.org/abs/2311.02000](http://arxiv.org/abs/2311.02000)

    Adam算法在非凸平滑随机优化中，经过深入分析，证明了在坐标-wise“仿射”方差噪声下，Adam可以以高概率收敛到稳定点，无需任何有界梯度假设和问题相关的知识。

    

    本文研究了在非凸平滑随机优化中，自适应矩法（Adam）算法的收敛性。尽管在机器学习领域被广泛使用，但其理论性质仍然有限。之前的研究主要从期望角度考虑了Adam的收敛性，常常需要强假设，比如均匀随机有界梯度或者先验的问题相关知识。因此，这些结果在实际的现实场景中的适用性受到了限制。为了克服这些局限，我们进行了深入分析，并证明了在坐标-wise“仿射”方差噪声下，Adam可以以高概率收敛到稳定点，其收敛速率为$\mathcal{O}\left({\rm poly}(\log T)/\sqrt{T}\right)$，不需要任何有界梯度假设和任何问题相关的知识来调整超参数。此外，我们还发现Adam限制了其梯度的...

    In this paper, we study the convergence of the Adaptive Moment Estimation (Adam) algorithm under unconstrained non-convex smooth stochastic optimizations. Despite the widespread usage in machine learning areas, its theoretical properties remain limited. Prior researches primarily investigated Adam's convergence from an expectation view, often necessitating strong assumptions like uniformly stochastic bounded gradients or problem-dependent knowledge in prior. As a result, the applicability of these findings in practical real-world scenarios has been constrained. To overcome these limitations, we provide a deep analysis and show that Adam could converge to the stationary point in high probability with a rate of $\mathcal{O}\left({\rm poly}(\log T)/\sqrt{T}\right)$ under coordinate-wise "affine" variance noise, not requiring any bounded gradient assumption and any problem-dependent knowledge in prior to tune hyper-parameters. Additionally, it is revealed that Adam confines its gradients' 
    
[^10]: 使用深度学习检测圆锥角膜病

    Detection of keratoconus Diseases using deep Learning. (arXiv:2311.01996v1 [eess.IV])

    [http://arxiv.org/abs/2311.01996](http://arxiv.org/abs/2311.01996)

    本研究通过比较不同基于CNN的深度学习架构，发现使用基于DenseNet201的模型可以准确识别圆锥角膜病，准确率达到89.14%。

    

    圆锥角膜是一种严重的眼角膜疾病，早期诊断困难，可能导致失明。卷积神经网络（CNN）作为深度学习方法之一，最近被认为是准确及时诊断圆锥角膜病的有希望工具。本研究的目的是评估不同D-CNN模型在识别圆锥角膜相关疾病方面的表现。具体来说，我们比较了五种不同基于CNN的深度学习架构（DenseNet201，InceptionV3，MobileNetV2，VGG19，Xception）。在全面的实验分析中，基于DenseNet201的模型在圆锥角膜病识别方面表现出色。该模型的准确率在三种关键类别：圆锥角膜，正常和疑似中的达到了令人惊讶的89.14%。

    One of the most serious corneal disorders, keratoconus is difficult to diagnose in its early stages and can result in blindness. This illness, which often appears in the second decade of life, affects people of all sexes and races. Convolutional neural networks (CNNs), one of the deep learning approaches, have recently come to light as particularly promising tools for the accurate and timely diagnosis of keratoconus. The purpose of this study was to evaluate how well different D-CNN models identified keratoconus-related diseases. To be more precise, we compared five different CNN-based deep learning architectures (DenseNet201, InceptionV3, MobileNetV2, VGG19, Xception). In our comprehensive experimental analysis, the DenseNet201-based model performed very well in keratoconus disease identification in our extensive experimental research. This model outperformed its D-CNN equivalents, with an astounding accuracy rate of 89.14% in three crucial classes: Keratoconus, Normal, and Suspect. T
    
[^11]: 利用分布鲁棒优化获取可解释的分类模型

    Obtaining Explainable Classification Models using Distributionally Robust Optimization. (arXiv:2311.01994v1 [stat.ML])

    [http://arxiv.org/abs/2311.01994](http://arxiv.org/abs/2311.01994)

    本论文介绍了一种利用分布鲁棒优化获取可解释的分类模型的方法，通过构建稀疏的规则集合来同时解决规则集的稀疏性和预测准确性之间的权衡，从而保证泛化性能并降低计算成本。

    

    对于人类用户来说，模型的可解释性对于理解提议分类器如何根据特征值给数据分配标签至关重要。我们研究使用特征值规则集构建的广义线性模型，该模型可以捕捉非线性依赖和交互作用。规则集的稀疏性和预测准确性之间存在固有的权衡。使用现有方法来找到合适的稀疏度选择（例如通过交叉验证）计算成本很高。我们提出了一种新的公式来学习同时解决这些竞争因素的规则集合。通过利用分布鲁棒优化来确保良好的泛化性能，同时保持低计算成本。该公式利用列生成有效地搜索规则集合的空间并构建稀疏的规则集合，与随机森林或Boosting及其变体等技术相比。我们提出了理论结果来推动这一公式的发展。

    Model explainability is crucial for human users to be able to interpret how a proposed classifier assigns labels to data based on its feature values. We study generalized linear models constructed using sets of feature value rules, which can capture nonlinear dependencies and interactions. An inherent trade-off exists between rule set sparsity and its prediction accuracy. It is computationally expensive to find the right choice of sparsity -- e.g., via cross-validation -- with existing methods. We propose a new formulation to learn an ensemble of rule sets that simultaneously addresses these competing factors. Good generalization is ensured while keeping computational costs low by utilizing distributionally robust optimization. The formulation utilizes column generation to efficiently search the space of rule sets and constructs a sparse ensemble of rule sets, in contrast with techniques like random forests or boosting and their variants. We present theoretical results that motivate an
    
[^12]: 条件对偏好关系进行限制以保证最优策略的存在

    Conditions on Preference Relations that Guarantee the Existence of Optimal Policies. (arXiv:2311.01990v1 [cs.LG])

    [http://arxiv.org/abs/2311.01990](http://arxiv.org/abs/2311.01990)

    我们引入了直接偏好过程框架，通过对偏好的序结构进行分析，我们提出了保证最优策略存在的条件。这个研究缩小了学习偏好反馈算法在理论和实践之间的差距，并提供了最优策略存在的证明。

    

    学习偏好反馈 (LfPF) 在训练大型语言模型和某些类型的交互式学习代理中起着至关重要的作用。然而，LfPF算法的理论和应用之间存在着实质性的差距。目前保证在LfPF问题中存在最优策略的结果假设偏好和转移动态都由马尔可夫决策过程确定。我们引入了直接偏好过程，这是一种在部分可观察、非马尔可夫环境中分析LfPF问题的新框架。在这个框架中，我们通过考虑偏好的序结构建立了保证最优策略存在的条件。利用冯·诺依曼-摩根斯特恩期望效用定理，我们表明直接偏好过程推广了标准强化学习问题。我们的研究结果缩小了LfPF算法在经验成功和理论理解之间的差距，并提供了最优策略存在的证明。

    Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provi
    
[^13]: 条件储层相生成的潜在扩散模型

    Latent Diffusion Model for Conditional Reservoir Facies Generation. (arXiv:2311.01968v1 [physics.geo-ph])

    [http://arxiv.org/abs/2311.01968](http://arxiv.org/abs/2311.01968)

    本研究提出了一种专门用于条件下储层相生成的潜在扩散模型，通过充分保留条件数据，生成了高保真度的储层相。它在性能上明显优于基于GANs的替代方法。

    

    在油气领域的田地开发和储层管理中，基于有限测量数据创建准确且地质真实的储层相至关重要。传统的两点地质统计方法虽然基础，但往往难以捕捉复杂的地质模式。多点统计方法提供了更大的灵活性，但也面临着挑战。随着生成对抗网络（GANs）的兴起和它们在不同领域的成功，人们开始倾向于使用它们进行储层相生成。然而，计算机视觉领域的最新进展显示了扩散模型相较于GANs的卓越性能。受此启发，提出了一种新颖的潜在扩散模型，专门用于条件下的储层相生成。该模型产生了高保真度的储层相，严格保留了条件数据。它明显优于基于GANs的替代方法。

    Creating accurate and geologically realistic reservoir facies based on limited measurements is crucial for field development and reservoir management, especially in the oil and gas sector. Traditional two-point geostatistics, while foundational, often struggle to capture complex geological patterns. Multi-point statistics offers more flexibility, but comes with its own challenges. With the rise of Generative Adversarial Networks (GANs) and their success in various fields, there has been a shift towards using them for facies generation. However, recent advances in the computer vision domain have shown the superiority of diffusion models over GANs. Motivated by this, a novel Latent Diffusion Model is proposed, which is specifically designed for conditional generation of reservoir facies. The proposed model produces high-fidelity facies realizations that rigorously preserve conditioning data. It significantly outperforms a GAN-based alternative.
    
[^14]: 提示的语言：什么语言属性使得提示成功？

    The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])

    [http://arxiv.org/abs/2311.01967](http://arxiv.org/abs/2311.01967)

    本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。

    

    最新一代的语言模型可以通过提示来在许多自然语言处理任务中实现令人印象深刻的零样本或少样本性能。然而，由于性能对提示的选择非常敏感，人们付出了相当大的努力来进行众包提示或设计用于优化提示的方法。然而，我们仍然缺乏对提示的语言属性与任务性能之间的系统理解。在这项工作中，我们研究了不同大小的预训练和指导调优的语言模型在语义上等效但在语言结构上有所不同的提示上的表现。我们通过调查语法属性（如情态、时态、语态和语气）以及通过使用同义词引入词汇-语义变化。我们的发现与常见假设相矛盾，即语言模型在低困惑度的提示上达到最佳性能，这些提示反映了预训练或指导调优数据中的语言使用。提示在数据集或模型之间转移效果不佳，性能有所下降。

    The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc
    
[^15]: 矩阵乘积元素转换的低秩逼近的难度

    Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products. (arXiv:2311.01960v1 [cs.DS])

    [http://arxiv.org/abs/2311.01960](http://arxiv.org/abs/2311.01960)

    本文研究了矩阵乘积元素转换的低秩逼近问题，并证明了在某些条件下，满足比$n^{2-o(1)}$时间复杂度更好的相对误差低秩逼近算法的必要条件。这是第一个关于该问题的条件时间复杂度困难结果。

    

    在自然语言处理中的快速算法的启发下，我们研究了基于元素转换的低秩逼近问题，其中我们希望找到一个好的秩为$k$的逼近值$f(U \cdot V)$，其中$U, V^\top \in \mathbb{R}^{n \times r}$是给定的，$r = O(\log(n))$，$f(x)$是一个一般的标量函数。之前的研究在亚线性的低秩逼近中已经展示了如果满足条件(1)$U = V^\top$和条件(2)$f(x)$是一个PSD核函数，那么存在一个$O(nk^{\omega-1})$时间复杂度的常系数相对误差逼近算法，其中$\omega \approx 2.376$是矩阵乘法的指数。我们给出了该问题的首个条件时间复杂度困难结果，证明了条件(1)和条件(2)对于在广泛类别的函数中获得比$n^{2-o(1)}$时间复杂度更好的相对误差低秩逼近是必要的。我们通过新颖的规约从强指数时间假设（SETH）中降低了对于平坦spar的杠杆得分的下界。

    Inspired by fast algorithms in natural language processing, we study low rank approximation in the entrywise transformed setting where we want to find a good rank $k$ approximation to $f(U \cdot V)$, where $U, V^\top \in \mathbb{R}^{n \times r}$ are given, $r = O(\log(n))$, and $f(x)$ is a general scalar function. Previous work in sublinear low rank approximation has shown that if both (1) $U = V^\top$ and (2) $f(x)$ is a PSD kernel function, then there is an $O(nk^{\omega-1})$ time constant relative error approximation algorithm, where $\omega \approx 2.376$ is the exponent of matrix multiplication. We give the first conditional time hardness results for this problem, demonstrating that both conditions (1) and (2) are in fact necessary for getting better than $n^{2-o(1)}$ time for a relative error low rank approximation for a wide class of functions. We give novel reductions from the Strong Exponential Time Hypothesis (SETH) that rely on lower bounding the leverage scores of flat spar
    
[^16]: 乐观的多智能体策略梯度在合作任务中的应用

    Optimistic Multi-Agent Policy Gradient for Cooperative Tasks. (arXiv:2311.01953v1 [cs.LG])

    [http://arxiv.org/abs/2311.01953](http://arxiv.org/abs/2311.01953)

    本论文提出了一个通用的、简单的框架，在多智能体策略梯度方法中引入乐观更新，以缓解合作任务中的相对过度概括问题。通过使用一个泄漏化线性整流函数来重塑优势，我们的方法能够保持对潜在由其他代理引起的低回报个别动作的乐观态度。

    

    在合作多智能体学习任务中，由于过拟合其他智能体的次优行为，导致智能体收敛到次优联合策略，出现了相对过度概括（RO）问题。早期研究表明，乐观主义可以缓解使用表格化Q学习时的RO问题。然而，对于复杂任务来说，利用函数逼近乐观主义可能加剧过估计，从而失败。另一方面，最近的深度多智能体策略梯度（MAPG）方法在许多复杂任务上取得了成功，但在严重的RO情况下可能失败。我们提出了一个通用而简单的框架，以在MAPG方法中实现乐观更新并缓解RO问题。具体而言，我们使用一个泄漏化线性整流函数，其中一个超参数选择乐观程度以在更新策略时重新塑造优势。直观地说，我们的方法对可能由其他代理引起的回报较低的个别动作保持乐观态度。

    \textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal be
    
[^17]: ForecastPFN：合成训练的零样本预测

    ForecastPFN: Synthetically-Trained Zero-Shot Forecasting. (arXiv:2311.01933v1 [cs.LG])

    [http://arxiv.org/abs/2311.01933](http://arxiv.org/abs/2311.01933)

    本文提出了一种名为ForecastPFN的零样本预测模型，通过合成数据训练，能够更准确且更快地进行零样本预测。

    

    绝大多数时间序列预测方法需要大量的训练数据集。然而，许多现实生活中的预测应用只有非常少的初始观测数据，有时只有40个或更少。因此，大多数预测方法在数据稀疏的商业应用中受到限制。虽然最近在非常有限的初始数据环境下进行了一些研究（所谓的“零样本”预测），但其性能因用于预训练的数据而异。在这项工作中，我们采取了不同的方法，设计了一个名为ForecastPFN的零样本预测模型，仅基于一种新颖的合成数据分布进行训练。ForecastPFN是一个经过数据拟合的网络，训练用于近似贝叶斯推断，可在单个前向传递中对新的时间序列数据集进行预测。通过大量实验，我们展示了ForecastPFN进行的零样本预测比现有技术水平的预测方法更准确和更快。

    The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even
    
[^18]: GateLoop: 完全数据控制的线性递归用于序列建模

    GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])

    [http://arxiv.org/abs/2311.01927](http://arxiv.org/abs/2311.01927)

    GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。

    

    线性递归已被证明是一种有效建模长序列的强大工具。在这项工作中，我们表明现有模型未能充分利用其潜力。在这一发现的基础上，我们开发了GateLoop，这是一种基础性的序列模型，通过使用数据控制的状态转换来推广线性递归模型，如S4、S5、LRU和RetNet。利用这一理论进步，GateLoop在自回归语言建模方面在实证上优于现有模型。我们的方法具有低成本的$O(l)$递归模式和高度优化的关联扫描实现的高效$O(l \log_{2} l)$并行模式。此外，我们还推导出了一个$O(l^2)$的代理注意力模式，揭示了对Transformer和最近提出的架构的显著影响。具体而言，我们证明了我们的方法可以被解释为向Attention提供数据控制的相对位置信息。而许多现有模型仅依赖于数据无关的位置信息。

    Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
    
[^19]: 大型语言模型阐明了人工医疗助手的进展路径：一项综述

    Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])

    [http://arxiv.org/abs/2311.01918](http://arxiv.org/abs/2311.01918)

    本综述文章探讨了大型语言模型（LLMs）在医学中的应用和意义。LLMs在知识检索、研究支持、临床工作流自动化和诊断辅助方面具有巨大潜力，尤其是多模态LLMs可以处理医学影像和电子健康记录等多样化数据类型以增强诊断能力。

    

    随着人工智能的快速发展，大型语言模型（LLMs）展现了模拟人类级别语言理解和推理的潜力。这引发了将LLMs应用于增强医疗各个方面的重要兴趣，范围从医学教育到临床决策支持。然而，医学涉及多方面的数据模态和微妙的推理技能，这给LLMs的整合带来了挑战。本文综述了LLMs在医学中的应用和影响。首先，考察了通用型和专门化LLMs的基本应用，展示了它们在知识检索、研究支持、临床工作流自动化和诊断辅助方面的作用。鉴于医学的固有多模态特性，该综述进一步关注多模态LLMs，研究其处理医学影像和电子健康记录等多样化数据类型以增强诊断能力的能力。

    With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic ac
    
[^20]: 高精度的条件随机化因果模型评估

    High Precision Causal Model Evaluation with Conditional Randomization. (arXiv:2311.01902v1 [cs.LG])

    [http://arxiv.org/abs/2311.01902](http://arxiv.org/abs/2311.01902)

    该论文提出了一种在现实条件随机化环境中对因果模型进行评估的新方法，通过引入低方差因果误差估计器，有效消除了引入的方差，实现了更好的效果。该方法在实证研究中得到改进，并展示了接近RCT性能的潜力。

    

    因果模型评估的黄金标准是将模型预测与来自随机对照试验（RCT）的真实效应进行比较。然而，进行RCT可能并不总是可行或道德的。相比之下，基于逆概率加权（IPW）的条件随机化实验提供了一种更加现实的方法，但可能受到估计方差过大的问题。为了解决这个挑战并增强在条件随机化环境中的因果模型评估，我们引入了一种新颖的低方差因果误差估计器，称之为“对比估计器”。通过将相同的IPW估计器应用于模型和真实实验效应，我们的估计器有效地消除了由于IPW引起的方差，并实现了更小的渐近方差。实证研究证明了我们估计器的改进，突显了其在实现接近RCT性能方面的潜力。我们的方法为在条件随机化环境中评估因果推断模型提供了简单但强大的解决方案。

    The gold standard for causal model evaluation involves comparing model predictions with true effects estimated from randomized controlled trials (RCT). However, RCTs are not always feasible or ethical to perform. In contrast, conditionally randomized experiments based on inverse probability weighting (IPW) offer a more realistic approach but may suffer from high estimation variance. To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings, we introduce a novel low-variance estimator for causal error, dubbed as the pairs estimator. By applying the same IPW estimator to both the model and true experimental effects, our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance. Empirical studies demonstrate the improved of our estimator, highlighting its potential on achieving near-RCT performance. Our method offers a simple yet powerful solution to evaluate causal inference models in condition
    
[^21]: 在线非参数似然比估计的皮尔逊散度函数最小化方法

    Online non-parametric likelihood-ratio estimation by Pearson-divergence functional minimization. (arXiv:2311.01900v1 [stat.ML])

    [http://arxiv.org/abs/2311.01900](http://arxiv.org/abs/2311.01900)

    本研究提出了一种在线非参数似然比估计（OLRE）框架，适用于估计两个概率密度函数之间差异的问题。通过利用核方法和函数最小化技术，我们的方法能够高效地进行在线更新，同时具有对概率密度函数形式无知的优势。

    

    在统计学和机器学习中，使用可用数据量化两个概率密度函数p和q之间的差异是一个基本问题。解决这个问题的一种常见方法是似然比估计（LRE），我们的研究为在线非参数似然比估计（OLRE）引入了一个新的框架，适用于随时间观察到的i.i.d观测值（$x_t \sim p, x'_t \sim q$）。我们的方法的非参数性质具有对$p$和$q$的形式无知的优势。此外，我们利用核方法和函数最小化的最新进展，开发了一个可以进行高效在线更新的估计方法。我们提供了OLRE方法性能的理论保证，并在合成实验中进行了实证验证。

    Quantifying the difference between two probability density functions, $p$ and $q$, using available data, is a fundamental problem in Statistics and Machine Learning. A usual approach for addressing this problem is the likelihood-ratio estimation (LRE) between $p$ and $q$, which -- to our best knowledge -- has been investigated mainly for the offline case. This paper contributes by introducing a new framework for online non-parametric LRE (OLRE) for the setting where pairs of iid observations $(x_t \sim p, x'_t \sim q)$ are observed over time. The non-parametric nature of our approach has the advantage of being agnostic to the forms of $p$ and $q$. Moreover, we capitalize on the recent advances in Kernel Methods and functional minimization to develop an estimator that can be efficiently updated online. We provide theoretical guarantees for the performance of the OLRE method along with empirical validation in synthetic experiments.
    
[^22]: 使用基于熵的ELBO学习稀疏编码

    Learning Sparse Codes with Entropy-Based ELBOs. (arXiv:2311.01888v1 [stat.ML])

    [http://arxiv.org/abs/2311.01888](http://arxiv.org/abs/2311.01888)

    本论文提出了一种基于熵的学习目标，用于稀疏编码参数的学习，通过非平凡的后验逼近和解析的目标函数，实现了标准稀疏编码的学习，在数值实验中证明了其可行性。

    

    标准概率稀疏编码假设拉普拉斯先验、从潜在到可观测的线性映射以及高斯可观测分布。我们在这里导出了一个仅基于熵的学习目标，用于标准稀疏编码的参数。这个新的变分目标具有以下特点：（A）与MAP逼近不同，它使用了概率推理的非平凡后验逼近；（B）与以前的非平凡逼近不同，这个新的目标是完全解析的；（C）该目标允许一种新的原则性的退火形式。目标的导出首先通过证明标准ELBO目标收敛到熵的和，这与具有高斯先验的生成模型的最近类似结果相匹配。然后，我们证明了ELBO等于熵的条件具有解析解，从而得到了完全解析的目标。通过数值实验证明了学习逼真性的可行性。

    Standard probabilistic sparse coding assumes a Laplace prior, a linear mapping from latents to observables, and Gaussian observable distributions. We here derive a solely entropy-based learning objective for the parameters of standard sparse coding. The novel variational objective has the following features: (A) unlike MAP approximations, it uses non-trivial posterior approximations for probabilistic inference; (B) unlike for previous non-trivial approximations, the novel objective is fully analytical; and (C) the objective allows for a novel principled form of annealing. The objective is derived by first showing that the standard ELBO objective converges to a sum of entropies, which matches similar recent results for generative models with Gaussian priors. The conditions under which the ELBO becomes equal to entropies are then shown to have analytical solutions, which leads to the fully analytical objective. Numerical experiments are used to demonstrate the feasibility of learning wit
    
[^23]: 通过熵最大化进行领域随机化

    Domain Randomization via Entropy Maximization. (arXiv:2311.01885v1 [cs.LG])

    [http://arxiv.org/abs/2311.01885](http://arxiv.org/abs/2311.01885)

    本文提出了一种新的领域随机化方法，通过熵最大化的方式在模拟训练中调整动力学分布，以实现模拟到真实的转移，无需真实世界数据，能够保持泛化能力和代理的高成功概率。

    

    在强化学习中，通过改变模拟中的动力学参数是一种流行的领域随机化方法，用于克服现实差距。然而，领域随机化在很大程度上依赖于动力学参数的抽样分布的选择，因为高变异对于规范代理行为至关重要，但过度随机化会导致过于保守的策略。在本文中，我们提出了一种新的方法来解决模拟到真实的转移，即在模拟训练过程中自动调整动力学分布，而无需真实世界数据。我们引入了通过熵最大化实现领域随机化的方法（DORAEMON），这是一个受限优化问题，直接最大化训练分布的熵同时保留泛化能力。为了实现这一目标，DORAEMON随着当前策略成功概率足够高，逐渐增加样本动力学参数的多样性。

    Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empiri
    
[^24]: 用顺序神经网络增强功能数据分析：优势和比较研究

    Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study. (arXiv:2311.01875v1 [cs.LG])

    [http://arxiv.org/abs/2311.01875](http://arxiv.org/abs/2311.01875)

    本论文研究了如何利用顺序神经网络（SNNs）增强功能数据分析（FDA），并通过与常见FDA回归模型的比较分析和实际数据分析证明了SNNs的有效性。

    

    功能数据分析（FDA）是一种处理高维度和复杂数据结构特征的统计领域。顺序神经网络（SNNs）是一种特殊的神经网络，能够处理序列数据，这是功能数据的一个基本特征。尽管SNNs在建模功能数据方面非常灵活，但在FDA社区中使用不多。SNNs的一个显著优势是易于实施，可以让广大非学术界的人群能够使用。相反，基于FDA的方法在实践中存在挑战，尤其是对于非专业人士来说，因为它们过于复杂。基于这一点，我们提议在FDA应用中利用SNNs，并通过与常见FDA回归模型的比较分析和实际数据分析来证明其有效性。SNNs的架构使我们能够克服传统FDA方法的局限性，提供了更好的功能数据分析。

    Functional Data Analysis (FDA) is a statistical domain developed to handle functional data characterized by high dimensionality and complex data structures. Sequential Neural Networks (SNNs) are specialized neural networks capable of processing sequence data, a fundamental aspect of functional data. Despite their great flexibility in modeling functional data, SNNs have been inadequately employed in the FDA community. One notable advantage of SNNs is the ease of implementation, making them accessible to a broad audience beyond academia. Conversely, FDA-based methodologies present challenges, particularly for practitioners outside the field, due to their intricate complexity. In light of this, we propose utilizing SNNs in FDA applications and demonstrate their effectiveness through comparative analyses against popular FDA regression models based on numerical experiments and real-world data analysis. SNN architectures allow us to surpass the limitations of traditional FDA methods, offerin
    
[^25]: SortNet: 通过神经网络排序算法进行学习排序

    SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])

    [http://arxiv.org/abs/2311.01864](http://arxiv.org/abs/2311.01864)

    SortNet是一种使用神经网络作为比较器来进行自适应排序的算法，通过迭代过程构建训练集，根据成对项目之间的排序示例来训练神经网络。

    

    关于相关性排名的问题，即根据给定的标准对一组对象进行排序。由于用户可能偏好不同的相关性标准，因此排序算法应该能够根据用户需求进行调整。学习排序的任务在文献中存在两种主要方法：1）通过示例学习的得分函数，评估每个对象的属性，生成可用于对对象进行排序的绝对相关性值；2）一种成对方法，通过使用对象对来学习“偏好函数”，定义哪一个对象应该首先排名。在本文中，我们提出了SortNet，一种使用神经网络作为比较器来对对象进行自适应排序的算法。神经网络的训练集提供了对于成对项目之间所需排序的示例，并且通过迭代过程构建，每次迭代都会添加最具信息性的训练示例。此外，比较器采用了连接主义体系结构。

    The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
    
[^26]: 基于尿液参数的集成机器学习方法用于COVID-19筛查

    An Ensemble Machine Learning Approach for Screening Covid-19 based on Urine Parameters. (arXiv:2311.01854v1 [eess.IV])

    [http://arxiv.org/abs/2311.01854](http://arxiv.org/abs/2311.01854)

    本研究提出了一种基于尿液参数的集成机器学习方法，用于COVID-19筛查。通过转换多个颜色空间并采用多层感知器神经网络，可以提高模型的准确性。

    

    COVID-19的快速传播和新变种的出现凸显了有效筛查措施的重要性。及时诊断和随后的感染者隔离可以防止病毒在社会中进一步传播。虽然PCR测试是COVID-19诊断的金标准，但成本高且耗时长。相比之下，尿液试纸是一种廉价、非侵入性且快速获取的筛查方法，可提供关于患者健康状况的重要信息。在本研究中，我们收集了一个新的数据集，并使用尿液试纸参数的RGB（红绿蓝）色彩空间来检测个体的健康状况。为了提高模型的准确性，我们将RGB空间转换为其他10个颜色空间。经过评估四种不同的机器学习模型后，我们提出了一个基于多层感知器神经网络的新集成模型。虽然最初的结果并不强，但我们能够改进模型的精确度。

    The rapid spread of COVID-19 and the emergence of new variants underscore the importance of effective screening measures. Rapid diagnosis and subsequent quarantine of infected individuals can prevent further spread of the virus in society. While PCR tests are the gold standard for COVID-19 diagnosis, they are costly and time-consuming. In contrast, urine test strips are an inexpensive, non-invasive, and rapidly obtainable screening method that can provide important information about a patient's health status. In this study, we collected a new dataset and used the RGB (Red Green Blue) color space of urine test strips parameters to detect the health status of individuals. To improve the accuracy of our model, we converted the RGB space to 10 additional color spaces. After evaluating four different machine learning models, we proposed a new ensemble model based on a multi-layer perceptron neural network. Although the initial results were not strong, we were able to improve the model's scr
    
[^27]: 带属性的多关系图的谱聚类

    Spectral Clustering of Attributed Multi-relational Graphs. (arXiv:2311.01840v1 [cs.LG])

    [http://arxiv.org/abs/2311.01840](http://arxiv.org/abs/2311.01840)

    本论文提出了一种针对带有节点属性的多关系图的联合降维技术SpectralMix，通过整合属性、不同类型关系和图结构的信息，实现对聚类结果的解释。该方法在多个数据集上的实验表明其有效性和性能优势。

    

    图聚类旨在发现节点的自然分组，使得相似的节点被分配到一个共同的聚类中。许多不同的算法已经在文献中提出：对于简单图，对于带有节点属性的图，以及对于边表示节点之间不同类型关系的图。然而，许多领域中的复杂数据可以表示为既具有属性又具有多关系的网络。在本文中，我们提出了SpectralMix，一种针对具有分类节点属性的多关系图的联合降维技术。SpectralMix整合了来自属性、不同类型关系和图结构的所有可用信息，以便对聚类结果进行合理解释。此外，它还推广了现有技术：当仅应用于单个图时，它会变为谱嵌入和聚类；当应用于分类数据时，它将变为均一性分析。实验通过在多个数据集上进行聚类分析，验证了SpectralMix的有效性和性能优势。

    Graph clustering aims at discovering a natural grouping of the nodes such that similar nodes are assigned to a common cluster. Many different algorithms have been proposed in the literature: for simple graphs, for graphs with attributes associated to nodes, and for graphs where edges represent different types of relations among nodes. However, complex data in many domains can be represented as both attributed and multi-relational networks.  In this paper, we propose SpectralMix, a joint dimensionality reduction technique for multi-relational graphs with categorical node attributes. SpectralMix integrates all information available from the attributes, the different types of relations, and the graph structure to enable a sound interpretation of the clustering results. Moreover, it generalizes existing techniques: it reduces to spectral embedding and clustering when only applied to a single graph and to homogeneity analysis when applied to categorical data. Experiments conducted on severa
    
[^28]: Mix-ME: 面向多智能体学习的质量-多样性方法

    Mix-ME: Quality-Diversity for Multi-Agent Learning. (arXiv:2311.01829v1 [cs.LG])

    [http://arxiv.org/abs/2311.01829](http://arxiv.org/abs/2311.01829)

    Mix-ME是一种面向多智能体学习的质量-多样性方法，通过混合来自不同团队的智能体形成新的解决方案，为适应不同环境和需求提供了高性能和多样性。

    

    在许多现实世界的系统中，如自适应机器人，仅实现单一的优化解决方案可能是不够的。相反，通常需要一组具有多样性且高性能的解决方案来适应不同的环境和需求。这是质量-多样性（QD）的领域，旨在发现一组具有独特特征的高性能解决方案。QD方法在许多领域，包括机器人学，尤其是在发现适应性损伤的行动控制器方面，最近取得了成功。然而，尽管许多感兴趣的任务是多智能体的，但大多数现有工作都集中在单智能体环境中。为此，我们介绍了Mix-ME，这是一种基于多智能体的新型MAP-Elites算法，它通过将来自不同团队的智能体混合在一起形成新的解决方案。我们在多个部分可观察的连续控制任务上评估了所提出的方法。评估结果表明，这些多智能体方法可以提供更好的性能和多样性。

    In many real-world systems, such as adaptive robotics, achieving a single, optimised solution may be insufficient. Instead, a diverse set of high-performing solutions is often required to adapt to varying contexts and requirements. This is the realm of Quality-Diversity (QD), which aims to discover a collection of high-performing solutions, each with their own unique characteristics. QD methods have recently seen success in many domains, including robotics, where they have been used to discover damage-adaptive locomotion controllers. However, most existing work has focused on single-agent settings, despite many tasks of interest being multi-agent. To this end, we introduce Mix-ME, a novel multi-agent variant of the popular MAP-Elites algorithm that forms new solutions using a crossover-like operator by mixing together agents from different teams. We evaluate the proposed methods on a variety of partially observable continuous control tasks. Our evaluation shows that these multi-agent v
    
[^29]: 利用属性的最小二乘问题的速写算法和锐利保证的凸和非凸正则化。 （arXiv：2311.01806v1 [math.OC]）

    Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees. (arXiv:2311.01806v1 [math.OC])

    [http://arxiv.org/abs/2311.01806](http://arxiv.org/abs/2311.01806)

    本文提出了一种用于解决较大规模优化问题的快速速写算法，适用于凸或非凸正则化函数的最小二乘问题。相比已有的随机算法，该算法处理通用的Frechet子微分正则化函数并提供了一般的近似误差理论。同时，通过解决速写的稀疏凸或非凸学习问题，我们还得到了稀疏信号估计的极小极大速率。

    

    随机算法对于解决大规模优化问题非常重要。在本文中，我们提出了一种快速速写算法，用于通过凸或非凸正则化函数正则化的最小二乘问题，即速写正则化优化（SRO）。我们的SRO算法首先生成原始数据矩阵的速写，然后解决速写问题。与现有的随机算法不同，我们的算法在一个统一的框架中处理通用的Frechet子微分正则化函数。我们为凸或非凸正则化的最小二乘问题的原始问题的优化结果和速写问题之间的近似误差提供了一般的理论结果。对于任意的凸正则化器，证明了相对误差界限的近似误差。重要的是，使用我们的一般极小极大速写稀疏凸或非凸学习问题的解决方案的稀疏信号估计的极小化速率也得到了。

    Randomized algorithms are important for solving large-scale optimization problems. In this paper, we propose a fast sketching algorithm for least square problems regularized by convex or nonconvex regularization functions, Sketching for Regularized Optimization (SRO). Our SRO algorithm first generates a sketch of the original data matrix, then solves the sketched problem. Different from existing randomized algorithms, our algorithm handles general Frechet subdifferentiable regularization functions in an unified framework. We present general theoretical result for the approximation error between the optimization results of the original problem and the sketched problem for regularized least square problems which can be convex or nonconvex. For arbitrary convex regularizer, relative-error bound is proved for the approximation error. Importantly, minimax rates for sparse signal estimation by solving the sketched sparse convex or nonconvex learning problems are also obtained using our gener
    
[^30]: 关于扩散模型的泛化属性

    On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])

    [http://arxiv.org/abs/2311.01797](http://arxiv.org/abs/2311.01797)

    本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。

    

    扩散模型是一类生成模型，用于建立一个随机传输映射，将经验观测到的但未知的目标分布与已知的先验分布联系起来。尽管在实际应用中取得了显著的成功，但对其泛化能力的理论理解仍未充分发展。本文对扩散模型的泛化属性进行了全面的理论研究。我们建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，表明在样本大小$n$和模型容量$m$上都存在多项式小的泛化误差($O(n^{-2/5}+m^{-4/5})$)，在停止训练时可以避免维度诅咒（即数据维度不呈指数级增长）。此外，我们将定量分析扩展到了一个数据依赖的情景，其中目标分布被描绘为一系列的概率密度函数。

    Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
    
[^31]: 学习增强分布用于区分开放世界中的未知样本

    Learning to Augment Distributions for Out-of-Distribution Detection. (arXiv:2311.01796v1 [cs.LG])

    [http://arxiv.org/abs/2311.01796](http://arxiv.org/abs/2311.01796)

    本研究提出了一种学习方法，通过增强分布的方式来区分在开放世界中的未知样本。具体来说，通过构造包含在辅助未知样本分布周围的Wasserstein球中所有分布的未知样本分布集，来减小分布差异，从而提升开放世界检测性能。

    

    开放世界分类系统应该区分那些标签与内部分布情况不一致的未知样本，这促使了对未知样本检测的研究。尽管先进的方法在这方面取得了很大进展，但由于对未知样本缺乏预先了解，它们在开放世界中仍可能失败。虽然可以访问用于模型训练的辅助未知样本（与未知样本不同），但仍需分析这些辅助数据在开放世界中的效果。为此，我们从学习理论的角度研究这个问题，发现辅助未知样本和真实未知样本之间的分布差异是影响开放世界检测性能的关键。基于此，我们提出分布增强的未知样本学习(DAL)，通过构建一个包含在辅助未知样本分布周围的Wasserstein球中的所有分布的未知样本分布集来减小未知样本分布差异。我们证明了DAL在开放世界中具有较好性能。

    Open-world classification systems should discern out-of-distribution (OOD) data whose labels deviate from those of in-distribution (ID) cases, motivating recent studies in OOD detection. Advanced works, despite their promising progress, may still fail in the open world, owing to the lack of knowledge about unseen OOD data in advance. Although one can access auxiliary OOD data (distinct from unseen ones) for model training, it remains to analyze how such auxiliary data will work in the open world. To this end, we delve into such a problem from a learning theory perspective, finding that the distribution discrepancy between the auxiliary and the unseen real OOD data is the key to affecting the open-world detection performance. Accordingly, we propose Distributional-Augmented OOD Learning (DAL), alleviating the OOD distribution discrepancy by crafting an OOD distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. We justify that t
    
[^32]: 使用机器学习从胸部X射线片中分割肺部异常

    CheX-Nomaly: Segmenting Lung Abnormalities from Chest Radiographs using Machine Learning. (arXiv:2311.01777v1 [eess.IV])

    [http://arxiv.org/abs/2311.01777](http://arxiv.org/abs/2311.01777)

    这篇论文提出了一种使用机器学习从胸部X射线片中分割肺部异常的方法，通过使用二元定位U-net模型和创新的对比学习方法，该方法在不同疾病和未见过的疾病中都具有较好的推广能力。

    

    胸部X射线（CXR）异常在全球范围内经常被错误诊断，主要与感知错误相关，医疗提供者难以准确识别异常的位置，而不是错误分类。我们目前通过疾病特定的分割模型来解决这个问题。不幸的是，这些模型不能在现场使用，因为它们在胸腔疾病中的推广性较差。当遇到数据集中没有代表的疾病时，二元模型的性能往往较差。我们提出了CheX-nomaly:一种二元定位U-net模型，利用迁移学习技术和创新的对比学习方法。在包含14种不同疾病以及“无发现”病例的VinDr-CXR数据集上进行训练，我的模型在这14种疾病以及其他未见过的疾病中实现了推广性。我们表明我们可以显著改进

    The global challenge in chest radiograph X-ray (CXR) abnormalities often being misdiagnosed is primarily associated with perceptual errors, where healthcare providers struggle to accurately identify the location of abnormalities, rather than misclassification errors. We currently address this problem through disease-specific segmentation models. Unfortunately, these models cannot be released in the field due to their lack of generalizability across all thoracic diseases. A binary model tends to perform poorly when it encounters a disease that isn't represented in the dataset. We present CheX-nomaly: a binary localization U-net model that leverages transfer learning techniques with the incorporation of an innovative contrastive learning approach. Trained on the VinDr-CXR dataset, which encompasses 14 distinct diseases in addition to 'no finding' cases, my model achieves generalizability across these 14 diseases and others it has not seen before. We show that we can significantly improve
    
[^33]: 高效的广义低秩张量情境赌博算法

    Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])

    [http://arxiv.org/abs/2311.01771](http://arxiv.org/abs/2311.01771)

    本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。

    

    本文旨在构建一种新颖的赌博算法，能够充分利用多维数据和奖励函数的固有非线性特性，提供高可用和负责任的决策服务。为此，我们引入了一种广义低秩张量情境赌博模型，其中一个动作由三个特征向量组成，因此可以用张量表示。在这个模型中，奖励是通过将动作的特征张量与一个固定但未知的参数张量的内积应用于广义线性函数来确定的，而这个参数张量具有较低的管状秩。为了实现探索和利用之间的权衡，我们引入了一种名为“广义低秩张量探索子空间然后细化”的新算法（G-LowTESTR）。该算法首先收集原始数据，以探索嵌入在决策情境中的本质低秩张量子空间信息，然后将原始概率转换为可解释的结构化概率。

    In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
    
[^34]: 使用梯度下降法解决非常数核的核岭回归

    Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel. (arXiv:2311.01762v1 [stat.ML])

    [http://arxiv.org/abs/2311.01762](http://arxiv.org/abs/2311.01762)

    本文研究了使用梯度下降法解决非常数核的核岭回归。通过在训练过程中逐渐减小带宽，避免了超参数选择的需求，并提出了一种带宽更新方案，证明了其优于使用常数带宽的方法。

    

    核岭回归（KRR）是线性岭回归的推广，它在数据中是非线性的，但在参数中是线性的。解决方案可以通过闭式解获得，其中包括矩阵求逆，也可以通过梯度下降迭代获得。本文研究了在训练过程中改变核函数的方法。我们从理论上探讨了这对模型复杂性和泛化性能的影响。基于我们的发现，我们提出了一种用于平移不变核的带宽更新方案，其中带宽在训练过程中逐渐减小至零，从而避免了超参数选择的需要。我们在真实和合成数据上展示了在训练过程中逐渐减小带宽的优于使用常数带宽，通过交叉验证和边缘似然最大化选择的带宽。我们还从理论和实证上证明了使用逐渐减小的带宽时，我们能够...

    Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes a matrix inversion, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to
    
[^35]: TinyFormer: 高效的Transformer设计和在小型设备上的部署

    TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices. (arXiv:2311.01759v1 [cs.LG])

    [http://arxiv.org/abs/2311.01759](http://arxiv.org/abs/2311.01759)

    TinyFormer是一个具有SuperNAS、SparseNAS和SparseEngine组成的框架，专门用于在MCUs上开发和部署资源高效的transformer模型。其创新之处在于提出了SparseEngine，这是第一个可以在MCUs上执行稀疏模型的transformer推理的部署框架。

    

    在各种嵌入式物联网应用中，以微控制器单元（MCUs）为代表的小型设备上开发深度学习模型引起了广泛关注。然而，由于严重的硬件资源限制，如何高效地设计和部署最新的先进模型（如transformer）在小型设备上是一项挑战。在这项工作中，我们提出了TinyFormer，这是一个特别设计用于在MCUs上开发和部署资源高效的transformer的框架。TinyFormer主要由SuperNAS、SparseNAS和SparseEngine组成。其中，SuperNAS旨在从广大的搜索空间中寻找适当的超网络。SparseNAS评估最佳的稀疏单路径模型，包括从已识别的超网络中提取的transformer架构。最后，SparseEngine将搜索到的稀疏模型高效地部署到MCUs上。据我们所知，SparseEngine是第一个能够在MCUs上执行稀疏模型的transformer推理的部署框架。在CIFAR-10数据集上的评估结果表明，TinyFormer在保持推理精度的同时，相比于传统的transformer模型，减少了大约78％的推理计算量和53％的模型大小。

    Developing deep learning models on tiny devices (e.g. Microcontroller units, MCUs) has attracted much attention in various embedded IoT applications. However, it is challenging to efficiently design and deploy recent advanced models (e.g. transformers) on tiny devices due to their severe hardware resource constraints. In this work, we propose TinyFormer, a framework specifically designed to develop and deploy resource-efficient transformers on MCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine. Separately, SuperNAS aims to search for an appropriate supernet from a vast search space. SparseNAS evaluates the best sparse single-path model including transformer architecture from the identified supernet. Finally, SparseEngine efficiently deploys the searched sparse models onto MCUs. To the best of our knowledge, SparseEngine is the first deployment framework capable of performing inference of sparse models with transformer on MCUs. Evaluation results on the CIFAR-10 da
    
[^36]: RiskQ: 风险敏感的多智能体强化学习价值因子分解

    RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])

    [http://arxiv.org/abs/2311.01753](http://arxiv.org/abs/2311.01753)

    RiskQ是一种解决多智能体强化学习中风险敏感协调要求的方法，通过引入风险敏感的个体-全局最大（RIGM）原则和建模联合回报分布实现价值因子分解。

    

    多智能体系统特点是环境不确定性、智能体的策略多样性和部分可观测性，这导致了显著的风险。在多智能体强化学习（MARL）的背景下，学习对风险敏感的协调和分散策略是具有挑战性的。为了在风险敏感的MARL中制定协调要求，我们介绍了风险敏感的个体-全局最大（RIGM）原理，作为个体-全局最大（IGM）和分布式IGM（DIGM）原理的一种推广。该原理要求每个智能体的风险敏感动作选择集合应与中央策略的风险敏感动作选择等价。当前的MARL价值因子分解方法对于常见的风险度量（例如风险价值（VaR）度量或扭曲的风险度量）不满足RIGM原则。因此，我们提出了RiskQ来解决这个限制，通过建模联合回报分布来实现价值因子分解。

    Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli
    
[^37]: 基于联邦强化学习的流行病决策系统

    Epidemic Decision-making System Based Federated Reinforcement Learning. (arXiv:2311.01749v1 [cs.LG])

    [http://arxiv.org/abs/2311.01749](http://arxiv.org/abs/2311.01749)

    基于联邦强化学习的流行病决策系统可以帮助政府在兼顾公共安全和经济发展方面做出决策，并处理流行病数据样本有限和隐私性高的问题。

    

    流行病决策可以有效帮助政府综合考虑公共安全和经济发展，以应对公共卫生和安全紧急情况。一些研究表明，强化学习可以有效帮助政府做出流行病决策，从而实现卫生安全和经济发展之间的平衡。然而，流行病数据往往具有样本有限和隐私性高的特点。该模型可以将各省份的疫情数据进行合作。

    Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. However, epidemic data often has the characteristics of limited samples and high privacy. However, epidemic data often has the characteristics of limited samples and high privacy. This model can combine the epidemic situation data of various provinces for coop
    
[^38]: 地下LoRaWAN网络能效优化的强化学习方法：直接卫星连接场景下的应用

    Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])

    [http://arxiv.org/abs/2311.01743](http://arxiv.org/abs/2311.01743)

    本文介绍了一种地下LoRaWAN网络能效优化的强化学习方法，该方法采用多智能体深度Q网络和多智能体优势演员-评论家算法来分配扩频因子，以最小化共享扩频因子干扰并优化系统的能效。

    

    在偏远农业和灾害救援行动中，地下LoRaWAN与非地球网络（NTN）的集成为经济和社会带来了重大好处。LoRa调制利用准正交扩频因子（SFs）来优化数据速率、空闲时间、覆盖范围和能量消耗。然而，在大规模地下LoRaWAN NTN中，有效地为终端设备分配SFs以最小化共享扩频因子干扰仍然具有挑战性。为了解决这个问题，我们研究了基于强化学习的SFs分配方案来优化系统的能效。为了有效捕捉密集网络中设备与环境之间的互动，我们提出了一种使用基于分析奖励机制的多智能体dueling double deep Q-network（MAD3QN）和多智能体优势演员-评论家（MAA2C）算法的SFs分配技术。与四个基准相比，我们提出的基于强化学习的SFs分配方法表现更好。

    The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extre
    
[^39]: 全球优化：一种机器学习方法

    Global Optimization: A Machine Learning Approach. (arXiv:2311.01742v1 [math.OC])

    [http://arxiv.org/abs/2311.01742](http://arxiv.org/abs/2311.01742)

    这篇论文介绍了一种机器学习方法来解决全球优化问题，通过使用基于超平面的决策树来近似非线性约束，并通过其他MIO可表示的ML模型来扩展原始问题的近似，同时提出了适应性采样程序和鲁棒优化技术来提高约束近似的准确性。

    

    解决全球优化问题的许多方法通常依赖于对特定数学原语的非线性约束的放松。这在应用中对于黑盒、隐式或更一般的约束是有限制的。为了解决这种限制，Bertsimas和Ozturk（2023）提出了OCTHaGOn作为一种通过使用基于超平面的决策树近似非线性约束来解决黑盒全球优化问题的方法，然后使用这些树构建原始问题的统一混合整数优化（MIO）近似。我们对该方法进行了扩展，通过（i）使用除决策树之外的其他MIO可表示的ML模型（如梯度提升树，多层感知器和支持向量机）来近似原始问题，（ii）提出适应性采样程序以获得更准确的基于机器学习的约束近似，（iii）利用鲁棒优化技术

    Many approaches for addressing Global Optimization problems typically rely on relaxations of nonlinear constraints over specific mathematical primitives. This is restricting in applications with constraints that are black-box, implicit or consist of more general primitives. Trying to address such limitations, Bertsimas and Ozturk (2023) proposed OCTHaGOn as a way of solving black-box global optimization problems by approximating the nonlinear constraints using hyperplane-based Decision-Trees and then using those trees to construct a unified mixed integer optimization (MIO) approximation of the original problem. We provide extensions to this approach, by (i) approximating the original problem using other MIO-representable ML models besides Decision Trees, such as Gradient Boosted Trees, Multi Layer Perceptrons and Suport Vector Machines, (ii) proposing adaptive sampling procedures for more accurate machine learning-based constraint approximations, (iii) utilizing robust optimization to 
    
[^40]: CDGraph：基于扩散模型的双条件社交图合成

    CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model. (arXiv:2311.01729v1 [cs.SI])

    [http://arxiv.org/abs/2311.01729](http://arxiv.org/abs/2311.01729)

    本文提出了一种新的条件扩散模型CDGraph用于合成社交图，通过捕捉双条件之间的相互依赖关系和保持节点连通性来实现生成高质量的社交网络。

    

    由生成模型合成的社交图因数据稀缺和用户隐私的担忧而越来越受到需求。生成社交网络的关键性能标准之一是对特定条件的忠实度，例如具有特定会员资格和财务状况的用户。虽然最近的扩散模型在生成图像方面表现出了显着的性能，但它们在条件社交图的合成方面的效果尚未得到研究。在本文中，我们提出了第一种用于社交网络的条件扩散模型CDGraph，它基于两个指定条件来训练和合成图。我们提出了CDGraph的去噪过程中的共同演化依赖性，以捕捉双条件之间的相互依赖关系，并进一步结合社会同质性和社会传染力以保持节点之间的连接性，同时满足指定条件。此外，我们引入了一种新颖的分类

    The social graphs synthesized by the generative models are increasingly in demand due to data scarcity and concerns over user privacy. One of the key performance criteria for generating social networks is the fidelity to specified conditionals, such as users with certain membership and financial status. While recent diffusion models have shown remarkable performance in generating images, their effectiveness in synthesizing graphs has not yet been explored in the context of conditional social graphs. In this paper, we propose the first kind of conditional diffusion model for social networks, CDGraph, which trains and synthesizes graphs based on two specified conditions. We propose the co-evolution dependency in the denoising process of CDGraph to capture the mutual dependencies between the dual conditions and further incorporate social homophily and social contagion to preserve the connectivity between nodes while satisfying the specified conditions. Moreover, we introduce a novel class
    
[^41]: 使用数据增强强化的神经模型对量子过程进行灵活的误差缓解

    Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model. (arXiv:2311.01727v1 [quant-ph])

    [http://arxiv.org/abs/2311.01727](http://arxiv.org/abs/2311.01727)

    提出了一种数据增强强化的神经模型，该模型可以灵活地缓解量子过程中的各种噪声，并展示了在不同类型量子过程中与先前方法相比的优越性能。

    

    神经网络在量子计算的各种任务中显示出了其有效性。然而，在量子误差缓解中的应用受到对无噪声统计的依赖限制，这是实现实际量子进展的关键步骤。为了解决这一关键挑战，我们提出了一种数据增强强化的神经模型用于误差缓解（DAEM）。我们的模型不需要任何关于特定噪声类型和测量设置的先验知识，并且可以仅根据目标量子过程的噪声测量结果估计无噪声统计值，使其非常适合实际实施。在数值实验中，我们展示了该模型在缓解各种类型的噪声（包括马尔可夫噪声和非马尔可夫噪声）方面与先前的误差缓解方法相比的优越性能。我们进一步通过利用该模型来缓解多种类型的量子过程中的错误来展示其多功能性。

    Neural networks have shown their effectiveness in various tasks in the realm of quantum computing. However, their application in quantum error mitigation, a crucial step towards realizing practical quantum advancements, has been restricted by reliance on noise-free statistics. To tackle this critical challenge, we propose a data augmentation empowered neural model for error mitigation (DAEM). Our model does not require any prior knowledge about the specific noise type and measurement settings and can estimate noise-free statistics solely from the noisy measurement results of the target quantum process, rendering it highly suitable for practical implementation. In numerical experiments, we show the model's superior performance in mitigating various types of noise, including Markovian noise and Non-Markovian noise, compared with previous error mitigation methods. We further demonstrate its versatility by employing the model to mitigate errors in diverse types of quantum processes, includ
    
[^42]: 使用FAIR的异构联邦协同过滤：在随机子空间中的联邦平均 (arXiv:2311.01722v1 [cs.LG])

    Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces. (arXiv:2311.01722v1 [cs.LG])

    [http://arxiv.org/abs/2311.01722](http://arxiv.org/abs/2311.01722)

    使用FAIR方法解决了异构联邦学习中的内存限制问题，实现了在空间不同的设备上集体训练嵌入表，用于推荐系统的个性化推荐。

    

    推荐系统（RS）广泛用于各种互联网平台上，用于根据用户的喜好个性化推荐内容。传统的推荐模型是在中央服务器上进行训练的。然而，由于对数据隐私的关注和GDPR等法规的出台，联邦学习成为越来越受欢迎的范式，其中数据永远不离开客户端设备。将联邦学习应用于推荐模型是非常困难的，因为嵌入表往往超出了大多数用户设备的内存限制。为了将所有设备的数据纳入联邦学习中，我们必须实现在内存能力不同的设备上对嵌入表进行集体训练。当前的异构联邦学习解决方案只能容纳一小部分能力范围，从而限制了能参与训练的设备数量。我们提出了在随机子空间中的联邦平均（FAIR），它允许对嵌入表进行任意压缩。

    Recommendation systems (RS) for items (e.g., movies, books) and ads are widely used to tailor content to users on various internet platforms. Traditionally, recommendation models are trained on a central server. However, due to rising concerns for data privacy and regulations like the GDPR, federated learning is an increasingly popular paradigm in which data never leaves the client device. Applying federated learning to recommendation models is non-trivial due to large embedding tables, which often exceed the memory constraints of most user devices. To include data from all devices in federated learning, we must enable collective training of embedding tables on devices with heterogeneous memory capacities. Current solutions to heterogeneous federated learning can only accommodate a small range of capacities and thus limit the number of devices that can participate in training. We present Federated Averaging in Random subspaces (FAIR), which allows arbitrary compression of embedding tab
    
[^43]: 基于物理知识的生成器-编码器对抗网络和潜在空间匹配的随机微分方程研究

    Physics-Informed Generator-Encoder Adversarial Networks with Latent Space Matching for Stochastic Differential Equations. (arXiv:2311.01708v1 [cs.LG])

    [http://arxiv.org/abs/2311.01708](http://arxiv.org/abs/2311.01708)

    我们提出了一种基于物理知识的生成器-编码器对抗网络和潜在空间匹配的新方法，用于解决随机微分方程中的正向、反向和混合问题。与现有方法相比，我们的模型通过间接匹配潜在特征空间，有效避免高维输入和复杂数据分布所带来的挑战，并提供更精确的解决方案。此外，该方法还减轻了训练不稳定性问题。

    

    我们提出了一种新型的基于物理知识的神经网络，称为基于物理知识的生成器-编码器对抗网络，来有效解决随机微分方程中正向、反向和混合问题所带来的挑战。在这些情况下，虽然统治方程已知，但可用的数据仅由系统参数的有限快照组成。我们的模型包括两个关键组件：生成器和编码器，两者都通过梯度下降交替更新。与直接匹配近似解与真实快照的先前方法不同，我们采用了低维潜在特征空间内的间接匹配。这种方法避免了高维输入和复杂数据分布所带来的挑战，同时相比现有的神经网络求解器产生了更精确的解决方案。此外，该方法还减轻了先前方法中遇到的训练不稳定性问题。

    We propose a new class of physics-informed neural networks, called Physics-Informed Generator-Encoder Adversarial Networks, to effectively address the challenges posed by forward, inverse, and mixed problems in stochastic differential equations. In these scenarios, while the governing equations are known, the available data consist of only a limited set of snapshots for system parameters. Our model consists of two key components: the generator and the encoder, both updated alternately by gradient descent. In contrast to previous approaches of directly matching the approximated solutions with real snapshots, we employ an indirect matching that operates within the lower-dimensional latent feature space. This method circumvents challenges associated with high-dimensional inputs and complex data distributions, while yielding more accurate solutions compared to existing neural network solvers. In addition, the approach also mitigates the training instability issues encountered in previous a
    
[^44]: 对合作多智能体赌徒问题的对抗攻击

    Adversarial Attacks on Cooperative Multi-agent Bandits. (arXiv:2311.01698v1 [cs.LG])

    [http://arxiv.org/abs/2311.01698](http://arxiv.org/abs/2311.01698)

    该论文研究了合作多智能体赌徒问题中的对抗攻击，并提出了针对同质和异质设置的攻击策略，目标是影响其他智能体的决策。研究发现，在同质设置中，通过攻击只一个智能体，即可使所有智能体选择特定目标臂，而在异质设置中，攻击目标臂需要线性攻击成本，同时提出了攻击策略，可以使最大数量的智能体承受线性遗憾同时承担亚线性成本，并只操纵了少数目标智能体的观察结果。

    

    合作多智能体多臂赌徒问题（CMA2B）考虑了多个智能体在一个共享的多臂赌徒游戏中的协同努力。我们研究了这种协作所暴露出的潜在漏洞，并考虑对一部分智能体进行对抗攻击，目的是影响其他智能体的决策。具体而言，我们研究了对CMA2B的对抗攻击，包括同质设置（智能体具有相同的臂集）和异质设置（智能体具有不同的臂集）。在同质设置中，我们提出了攻击策略，通过只针对一个智能体，使得所有智能体在T轮中选择一个特定的目标臂T-o(T)次，同时承担o(T)的攻击成本。在异质设置中，我们证明了对目标臂的攻击需要线性的攻击成本，并提出了攻击策略，可以迫使最大数量的智能体承受线性的遗憾，同时承担亚线性的成本，并且只操纵了少数目标智能体的观察结果。

    Cooperative multi-agent multi-armed bandits (CMA2B) consider the collaborative efforts of multiple agents in a shared multi-armed bandit game. We study latent vulnerabilities exposed by this collaboration and consider adversarial attacks on a few agents with the goal of influencing the decisions of the rest. More specifically, we study adversarial attacks on CMA2B in both homogeneous settings, where agents operate with the same arm set, and heterogeneous settings, where agents have distinct arm sets. In the homogeneous setting, we propose attack strategies that, by targeting just one agent, convince all agents to select a particular target arm $T-o(T)$ times while incurring $o(T)$ attack costs in $T$ rounds. In the heterogeneous setting, we prove that a target arm attack requires linear attack costs and propose attack strategies that can force a maximum number of agents to suffer linear regrets while incurring sublinear costs and only manipulating the observations of a few target agent
    
[^45]: 高效通信的联邦非线性赌博优化

    Communication-Efficient Federated Non-Linear Bandit Optimization. (arXiv:2311.01695v1 [cs.LG])

    [http://arxiv.org/abs/2311.01695](http://arxiv.org/abs/2311.01695)

    本文提出了一种名为Fed-GO-UCB的新算法，用于具有通用非线性目标函数的联邦赌博优化。

    

    联邦优化研究了在一个中央服务器的协调下，多个客户端（如移动设备或组织）之间的协同函数优化问题。由于数据由每个客户端单独收集并始终保持分散，联邦优化保护了数据隐私并允许大规模计算，这使得它成为一种有前途的分散式机器学习范式。尽管它通常用于在线任务，例如键盘应用程序上的下一个词预测，但大多数工作将其定义为离线问题。很少数例外考虑联邦赌博优化，但它们局限于非常简单的函数类别，例如线性、广义线性或带有有界RKHS范数的非参数函数类别，这严重限制了它的实际使用。在本文中，我们提出了一种新算法，名为Fed-GO-UCB，用于具有通用非线性目标函数的联邦赌博优化。

    Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization preserves data privacy and allows for large-scale computing, which makes it a promising decentralized machine learning paradigm. Though it is often deployed for tasks that are online in nature, e.g., next-word prediction on keyboard apps, most works formulate it as an offline problem. The few exceptions that consider federated bandit optimization are limited to very simplistic function classes, e.g., linear, generalized linear, or non-parametric function class with bounded RKHS norm, which severely hinders its practical usage. In this paper, we propose a new algorithm, named Fed-GO-UCB, for federated bandit optimization with generic non-linear objective function. Under some mild conditions, w
    
[^46]: 使用传输的信息瓶颈实现解缠表示学习

    Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])

    [http://arxiv.org/abs/2311.01686](http://arxiv.org/abs/2311.01686)

    本研究提出了一种通过引入传输信息瓶颈来实现解缠表示学习的方法。该方法可以在压缩表示信息和保留重要信息之间维持平衡，从而提高模型的稳健性和泛化能力。通过使用贝叶斯网络和变分推断，我们得到了可计算估计的DisTIB。

    

    仅编码与任务相关的原始数据信息，即解缠表示学习，可以极大地提高模型的稳健性和泛化能力。虽然在表示中利用信息理论对信息进行规范化取得了重大进展，但仍存在两个主要挑战：1）表示压缩不可避免地导致性能下降；2）对表示的解缠约束存在复杂的优化问题。针对这些问题，我们引入了传输信息的贝叶斯网络来描述解缠过程中输入和表示之间的相互作用。基于这个框架，我们提出了"DisTIB"（用于解缠表示学习的传输信息瓶颈），一种新的目标函数，用于平衡信息压缩和保留之间的关系。我们采用变分推断来导出DisTIB的可计算估计。

    Encoding only the task-related information from the raw data, \ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation \textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This es
    
[^47]: 使用部分合成数据及机器学习方法的肿瘤酰胺质子转移（APT）成像研究

    Amide Proton Transfer (APT) imaging in tumor with a machine learning approach using partially synthetic data. (arXiv:2311.01683v1 [physics.med-ph])

    [http://arxiv.org/abs/2311.01683](http://arxiv.org/abs/2311.01683)

    本研究介绍了一种使用部分合成数据和机器学习方法的新平台，旨在训练模型预测酰胺质子转移（APT）效应。通过将模拟数据和实测数据结合起来生成部分合成的数据，实现了模拟的灵活性和准确性之间的平衡。

    

    机器学习（ML）被越来越多地用于量化化学交换饱和转移（CEST）效应。ML模型通常使用实测数据或完全模拟数据进行训练。然而，使用实测数据进行训练往往缺乏足够的训练数据，而使用完全模拟数据进行训练可能由于有限的模拟资源而引入偏差。本研究引入了一个新平台，将模拟数据和实测数据结合起来生成部分合成的CEST数据，并评估其用于训练ML模型预测酰胺质子转移（APT）效应的可行性。部分合成的CEST信号使用从模拟中的APT效应的逆求和和来自实测数据的其他成分创建。通过改变APT模拟参数并应用缩放因子来调整实测数据的成分，生成了训练数据，实现了模拟的灵活性和准确性之间的平衡。

    Machine learning (ML) has been increasingly used to quantify chemical exchange saturation transfer (CEST) effect. ML models are typically trained using either measured data or fully simulated data. However, training with measured data often lacks sufficient training data, while training with fully simulated data may introduce bias due to limited simulations pools. This study introduces a new platform that combines simulated and measured components to generate partially synthetic CEST data, and to evaluate its feasibility for training ML models to predict amide proton transfer (APT) effect. Partially synthetic CEST signals were created using an inverse summation of APT effects from simulations and the other components from measurements. Training data were generated by varying APT simulation parameters and applying scaling factors to adjust the measured components, achieving a balance between simulation flexibility and fidelity. First, tissue-mimicking CEST signals along with ground trut
    
[^48]: 引入重要抽样的柔性生存密度的最大似然估计

    Maximum Likelihood Estimation of Flexible Survival Densities with Importance Sampling. (arXiv:2311.01660v1 [cs.LG])

    [http://arxiv.org/abs/2311.01660](http://arxiv.org/abs/2311.01660)

    该论文提出了一种生存分析方法，通过引入重要抽样，消除了调整超参数的需求，如混合分配和箱尺寸，减轻了从业人员的负担。

    

    生存分析是一种广泛应用于分析具有截尾的时间至事件数据的技术。最近几年，出现了许多能够适用于大数据集并放松传统假设（如比例风险）的生存分析方法。尽管这些模型表现出色，但对于模型的超参数（如离散模型的箱数和箱尺寸，以及基于混合模型的簇分配数）需要大量调整以实现最佳性能。此外，我们通过实证研究证明了以下事实：（1）最佳箱尺寸可能会因所关注的指标（如一致性和布里尔分数）而大为不同，以及（2）混合模型可能会遭受模式坍塌和数值不稳定的问题。我们提出了一种生存分析方法，消除了调整混合分配和箱尺寸等超参数的需求，从而减轻了从业人员的负担。

    Survival analysis is a widely-used technique for analyzing time-to-event data in the presence of censoring. In recent years, numerous survival analysis methods have emerged which scale to large datasets and relax traditional assumptions such as proportional hazards. These models, while being performant, are very sensitive to model hyperparameters including: (1) number of bins and bin size for discrete models and (2) number of cluster assignments for mixture-based models. Each of these choices requires extensive tuning by practitioners to achieve optimal performance. In addition, we demonstrate in empirical studies that: (1) optimal bin size may drastically differ based on the metric of interest (e.g., concordance vs brier score), and (2) mixture models may suffer from mode collapse and numerical instability. We propose a survival analysis approach which eliminates the need to tune hyperparameters such as mixture assignments and bin sizes, reducing the burden on practitioners. We show t
    
[^49]: 通过鲁棒视觉概念在真实图像和AI生成图像分类中检测伪相关关系

    Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification. (arXiv:2311.01655v1 [cs.LG])

    [http://arxiv.org/abs/2311.01655](http://arxiv.org/abs/2311.01655)

    这项工作介绍了一种通用方法，可以高效地检测潜在的伪相关关系，并且相比之下需要更少的人工干预。

    

    通常机器学习模型倾向于自动学习训练数据中存在的关联，而不会质疑其有效性或适当性。这种不可取的特性是伪相关关系的体现，使得模型在分布变化的情况下不可靠且容易失败。研究表明，大多数试图纠正伪相关关系的方法仅对模型已知的伪关联有效。当前的伪相关关系检测算法要么依赖于大量的人工标注，要么在其制定中太过限制性。此外，它们依赖于对视觉工件的严格定义，这在由生成模型生成的数据中可能不适用，因为这些模型以产生不符合标准规范的内容而闻名。在这项工作中，我们介绍了一种通用方法，可以高效地检测潜在的伪相关关系，并且相比之下需要更少的人工干预。

    Often machine learning models tend to automatically learn associations present in the training data without questioning their validity or appropriateness. This undesirable property is the root cause of the manifestation of spurious correlations, which render models unreliable and prone to failure in the presence of distribution shifts. Research shows that most methods attempting to remedy spurious correlations are only effective for a model's known spurious associations. Current spurious correlation detection algorithms either rely on extensive human annotations or are too restrictive in their formulation. Moreover, they rely on strict definitions of visual artifacts that may not apply to data produced by generative models, as they are known to hallucinate contents that do not conform to standard specifications. In this work, we introduce a general-purpose method that efficiently detects potential spurious correlations, and requires significantly less human interference in comparison t
    
[^50]: MARRS: 多模态参考解析系统

    MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])

    [http://arxiv.org/abs/2311.01650](http://arxiv.org/abs/2311.01650)

    MARRS是一个在设备上运行的多模态参考解析系统，能够处理对话式、视觉和背景上下文，并通过不同的机器学习模型实现上下文查询的处理。这个系统能够在保护用户隐私的同时理解上下文。

    

    成功处理上下文对于任何对话理解任务都是至关重要的。这个上下文可能是对话式的（依赖于之前的用户查询或系统回答），也可能是视觉的（依赖于用户看到的东西，例如他们的屏幕上），或者是背景的（基于一些信号，比如响起的闹钟或者正在播放的音乐）。在这项工作中，我们介绍了MARRS（多模态参考解析系统），它是一个在设备上运行的自然语言理解系统的框架，在处理对话式、视觉和背景上下文方面负责。特别是，我们提出了不同的机器学习模型来实现上下文查询的处理；具体而言，一个用于实现参考解析，一个用于通过查询重写处理上下文。我们还描述了这些模型如何相互补充，形成一个统一、连贯、轻量级的系统，可以在保护用户隐私的同时理解上下文。

    Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
    
[^51]: 校准和增强GNN在多关系和时间图上的逻辑表达能力

    Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs. (arXiv:2311.01647v1 [cs.LG])

    [http://arxiv.org/abs/2311.01647](http://arxiv.org/abs/2311.01647)

    本论文研究了GNN在多关系和时间图上的逻辑表达能力，并证明了R$^2$-GNN模型在某些受限但合理的情况下等价于$\mathcal{FOC}_2$分类器。此外，为了克服R$^2$-GNN在表达能力方面的局限性，提出了一种简单的图转换技术。

    

    作为图表示学习的强大框架，图神经网络（GNN）近年来引起了极大的关注。然而，据我们所知，关于GNN作为多关系图中布尔节点分类器的逻辑表达能力尚未进行正式分析，其中每条边都具有特定的关系类型。在本文中，我们研究了$\mathcal{FOC}_2$，这是一种具有两个变量和计数量化器的一阶逻辑片段。在消极方面，我们证明了将全局读取引入局部消息传递GNN的R$^2$-GNN架构在一般情况下无法捕捉$\mathcal{FOC}_2$分类器。然而，在积极方面，我们建立了R$^2$-GNN模型在某些受限但合理的情况下等价于$\mathcal{FOC}_2$分类器。为了解决R$^2$-GNN在表达能力方面的局限性，我们提出了一种简单的图转换技术，类似于...

    As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate $\mathcal{FOC}_2$, a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R$^2$-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture $\mathcal{FOC}_2$ classifiers in the general case. Nevertheless, on the positive side, we establish that R$^2$-GNNs models are equivalent to $\mathcal{FOC}_2$ classifiers under certain restricted yet reasonable scenarios. To address the limitations of R$^2$-GNNs regarding expressiveness, we propose a simple graph transformation technique, akin to a
    
[^52]: SemiGPC：使用高斯过程实现对不平衡半监督学习的分布感知标签精炼策略

    SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes. (arXiv:2311.01646v1 [cs.CV])

    [http://arxiv.org/abs/2311.01646](http://arxiv.org/abs/2311.01646)

    本文介绍了一种称为SemiGPC的方法，通过使用高斯过程和标签的后验分布，实现了对不平衡半监督学习的分布感知标签精炼策略。实验结果表明，SemiGPC在不同的半监督方法和预训练策略中都取得了良好的性能，尤其是在低数据区间和类别不平衡情况下。

    

    本文介绍了SemiGPC，一种基于高斯过程的分布感知标签精炼策略，在该策略中，模型的预测来自于标签的后验分布。与其他基于缓冲区的半监督方法（如CoMatch和SimMatch）不同，我们的SemiGPC包含一个归一化项，用于解决全局数据分布的不平衡问题，同时保持局部敏感性。这种显式的控制使得SemiGPC在类别不平衡的情况下更具鲁棒性，特别是对于确认偏差的处理。我们展示了SemiGPC与不同的半监督方法（如FixMatch、ReMixMatch、SimMatch和FreeMatch）以及不同的预训练策略（包括MSN和Dino）配对后的性能提升。我们还展示了SemiGPC在标准的CIFAR10-LT/CIFAR100-LT的不同程度的类别不平衡下取得了最先进的结果，特别是在低数据区间。相比于一个新的竞争方法，使用SemiGPC还可以提高约2%的平均准确率。

    In this paper we introduce SemiGPC, a distribution-aware label refinement strategy based on Gaussian Processes where the predictions of the model are derived from the labels posterior distribution. Differently from other buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity. This explicit control allows SemiGPC to be more robust to confirmation bias especially under class imbalance. We show that SemiGPC improves performance when paired with different Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch and different pre-training strategies including MSN and Dino. We also show that SemiGPC achieves state of the art results under different degrees of class imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime. Using SemiGPC also results in about 2% avg.accuracy increase compared to a new compe
    
[^53]: 学生网络是否应该复制或平均教师权重？

    Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v1 [cs.LG])

    [http://arxiv.org/abs/2311.01644](http://arxiv.org/abs/2311.01644)

    这项研究探讨了在欠参数化情况下，学生网络是否应该复制教师神经元或平均一组教师神经元的权重。研究发现对于特定的网络结构和输入分布，当教师网络的输入向量正交且输出权重为酉时，复制-平均配置将达到优化结果，其中大部分学生神经元复制一个教师神经元，最后一个学生神经元对所有教师神经元取平均值。

    

    任何连续函数 $f^*$ 都可以用足够多的神经元 $k$来近似。我们考虑 $f^*$ 本身是一个具有一个隐藏层和 $k$ 个神经元的神经网络的情况。用具有 $n<k$ 个神经元的神经网络来逼近 $f^*$ 可以看作是将一个欠参数化的“学生”网络与 $k$ 个神经元的“教师”网络进行拟合。由于学生具有较少的神经元，所以不清楚每个 $n$ 个学生神经元应该复制一个教师神经元还是平均一组教师神经元。对于具有 erf 激活函数和标准高斯输入分布的浅层神经网络，我们证明了当教师的输入向量是正交的并且输出权重是酉的时候，“复制-平均”配置是临界点。此外，在这样的配置中，优化结果是当 $n-1$ 个学生神经元分别复制一个教师神经元，并且第 $n$ 个学生神经元是所有教师神经元的平均。

    Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized "student" network with $n$ neurons to a "teacher" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student ne
    
[^54]: 基于有界理性阶梯的强化学习鲁棒对抗性训练

    Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula. (arXiv:2311.01642v1 [cs.LG])

    [http://arxiv.org/abs/2311.01642](http://arxiv.org/abs/2311.01642)

    本论文提出了一种基于熵正则化的对抗性强化学习方法，通过解决复杂的鞍点优化问题，实现了鲁棒性对抗攻击和分布偏移，从而在强化学习中取得了重要进展。

    

    鲁棒性对抗攻击和分布偏移一直是强化学习的长期目标。为此，鲁棒对抗性强化学习(RARL)在一个竞争性零和马尔可夫博弈中训练主角来对抗敌对力量，这里的最优解即理性策略对应于纳什均衡。然而，要找到纳什均衡需要面对复杂的鞍点优化问题，对于高维控制尤其难以解决。本文提出了一种新颖的基于熵正则化的对抗性强化学习方法，以减轻鞍点优化问题的复杂性。我们证明，这个熵正则化问题的解对应于量响应均衡(QRE)，它是纳什均衡的推广，并考虑了有界理性，即代理有时会随机执行动作而非最优动作。关键是，连接熵正则化和量响应均衡的关系使得训练鲁棒的对抗性强化学习更加鲁棒。

    Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between 
    
[^55]: VQPy：一种面向现代视频分析的面向对象方法。

    VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])

    [http://arxiv.org/abs/2311.01623](http://arxiv.org/abs/2311.01623)

    VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。

    

    视频分析广泛应用于当今系统和服务中。在视频分析的前沿是用户开发的视频查询，以找到特定感兴趣的对象。基于视频对象（例如人，动物，汽车等）与传统面向对象语言建模的对象相似的洞察力，我们提出了一种面向视频分析的面向对象方法。这种方法名为VQPy，包括一个前端（一种Python变体，其中包含用户可以表达视频对象及其交互的结构）和一个可扩展的后端，可以基于视频对象自动生成和优化管道。我们已经实施和开源了VQPy，它已经作为Cisco DeepVision框架的一部分产品化。

    Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
    
[^56]: 提前选择性可塑性用于视觉任务的持续学习

    Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])

    [http://arxiv.org/abs/2311.01617](http://arxiv.org/abs/2311.01617)

    提出了一种新的持续学习机制，利用对比表示学习来减少灾难性遗忘，通过观察冗余诱导能力，识别并保留对神经网络转移能力最有贡献的参数，以实现在任务边界时的选择性可塑性。

    

    对比表示学习已经成为一种有前途的持续学习技术，因为它可以学习到对灾难性遗忘具有鲁棒性并且对未来的任务有很好泛化能力的表示。以大脑中创建和更新的事件模型为启发，我们提出了一种新的机制，该机制发生在任务边界，即一个任务结束并另一个任务开始时。通过观察对比损失对神经网络输出的冗余诱导能力，我们的方法利用新任务的前几个样本，识别和保留对神经网络的转移能力最有贡献的参数，从而释放网络的其余部分来学习新的特征。我们在诸如CIFAR10和TinyImagenet等基准计算机视觉数据集上评估了所提出的方法，并展示了在任务增量方面的最先进性能。

    Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the task-incrementa
    
[^57]: 对于文本预测的忠实和稳健的本地可解释性

    Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])

    [http://arxiv.org/abs/2311.01605](http://arxiv.org/abs/2311.01605)

    提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。

    

    可解释性对于机器学习模型在关键领域中得到信任和部署是至关重要的。然而，现有的用于解释文本模型的方法通常复杂，并且缺乏坚实的数学基础，它们的性能也不能保证。在本文中，我们提出了一种新颖的方法FRED（Faithful and Robust Explainer for textual Documents），用于解释文本预测。FRED可以识别文档中的关键词，当这些词被移除时对预测结果产生重大影响。我们通过正式的定义和对可解释分类器的理论分析，确立了FRED的可靠性。此外，我们还通过与最先进的方法进行的实证评估，证明了FRED在提供对文本模型的深入见解方面的有效性。

    Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
    
[^58]: 本地Borsuk-Ulam, 稳定性和可复制性

    Local Borsuk-Ulam, Stability, and Replicability. (arXiv:2311.01599v1 [cs.LG])

    [http://arxiv.org/abs/2311.01599](http://arxiv.org/abs/2311.01599)

    使用拓扑学中的Borsuk-Ulam定理，研究了列表可复制和全局稳定学习算法的限制。在不可知PAC设置中，这些学习算法是不可能的。在可实现的PAC设置中，提供了最佳界限和下界，与Littlestone维度有指数级分离。

    

    我们使用并改编了拓扑学中的Borsuk-Ulam定理，推导出对于列表可复制和全局稳定学习算法的限制。我们进一步展示了我们方法在组合学和拓扑学中的适用性。我们证明在不可知PAC(setting)中，除了平凡的情况外，列表可复制和全局稳定学习都是不可能的。这与可实现的情况形成了对比，已知具有有限Littlestone维度的任何类都可以通过这样的算法学习。在可实现的PAC(setting)中，我们加强了以前的不可能结果并扩大了它们的范围。具体而言，我们为有限类别中的列表可复制和全局稳定数量确定了最佳界限。这相对于以前的工作提供了指数级的改进，并意味着与Littlestone维度的指数级分离。我们进一步引入了弱学习者的下界，即仅比随机猜测稍微好一点的学习者。以前的工作中提供了下界

    We use and adapt the Borsuk-Ulam Theorem from topology to derive limitations on list-replicable and globally stable learning algorithms. We further demonstrate the applicability of our methods in combinatorics and topology.  We show that, besides trivial cases, both list-replicable and globally stable learning are impossible in the agnostic PAC setting. This is in contrast with the realizable case where it is known that any class with a finite Littlestone dimension can be learned by such algorithms. In the realizable PAC setting, we sharpen previous impossibility results and broaden their scope. Specifically, we establish optimal bounds for list replicability and global stability numbers in finite classes. This provides an exponential improvement over previous works and implies an exponential separation from the Littlestone dimension. We further introduce lower bounds for weak learners, i.e., learners that are only marginally better than random guessing. Lower bounds from previous work
    
[^59]: 更好的公平性胜于遗憾：针对公平GNN的对抗性缺失数据填充

    Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs. (arXiv:2311.01591v1 [cs.LG])

    [http://arxiv.org/abs/2311.01591](http://arxiv.org/abs/2311.01591)

    该论文提出了一种针对公平GNN的对抗性缺失数据填充模型，以解决现有公平GNN的假设问题。实验证明此模型的有效性。

    

    本文解决了在缺失保护属性的情况下学习公平图神经网络（GNNs）的问题。在许多相关任务中，决策可能会对特定社区产生不成比例的影响，而GNNs已经在这些任务中取得了最先进的结果。然而，现有的公平GNNs工作要么假设保护属性是完全被观察到的，要么假设缺失数据的填充是公平的。实际上，填充中的偏差会传播到模型的结果中，导致它们过高地估计了其预测的公平性。我们通过提出Better Fair than Sorry（BFtS），为公平GNNs使用的保护属性的公平缺失数据填充模型来解决这个挑战。BFtS背后的关键设计原则是填充应该近似于公平GNN的最困难情况，即在最优化公平性最困难的情况下。我们使用一个三方对抗方案来实现这个想法，在这个方案中，两个对手共同对抗公平GNN。通过使用合成和实际数据集的实验证明了BFtS的有效性。

    This paper addresses the problem of learning fair Graph Neural Networks (GNNs) under missing protected attributes. GNNs have achieved state-of-the-art results in many relevant tasks where decisions might disproportionately impact specific communities. However, existing work on fair GNNs assumes that either protected attributes are fully-observed or that the missing data imputation is fair. In practice, biases in the imputation will be propagated to the model outcomes, leading them to overestimate the fairness of their predictions. We address this challenge by proposing Better Fair than Sorry (BFtS), a fair missing data imputation model for protected attributes used by fair GNNs. The key design principle behind BFtS is that imputations should approximate the worst-case scenario for the fair GNN -- i.e. when optimizing fairness is the hardest. We implement this idea using a 3-player adversarial scheme where two adversaries collaborate against the fair GNN. Experiments using synthetic and
    
[^60]: 一种用于多任务模仿学习中表示转移的统计保证

    A Statistical Guarantee for Representation Transfer in Multitask Imitation Learning. (arXiv:2311.01589v1 [cs.LG])

    [http://arxiv.org/abs/2311.01589](http://arxiv.org/abs/2311.01589)

    多任务模仿学习中，通过使用足够多样的源任务训练表示，可以提高对新任务的样本效率。

    

    在多任务模仿学习中，表示转移有潜力比从头开始学习更有效地提供学习新任务所需的样本效率。在这项工作中，我们提供了一个统计保证，表明当使用足够多样的源任务训练表示时，我们确实可以在目标任务上实现改进的样本效率。我们的理论结果可以轻松推广到考虑常用的神经网络架构和现实假设。我们进行了与理论发现相一致的实证分析，在四个模拟环境中进行了实验——特别是利用源任务中更多的数据可以改进对新任务的样本效率。

    Transferring representation for multitask imitation learning has the potential to provide improved sample efficiency on learning new tasks, when compared to learning from scratch. In this work, we provide a statistical guarantee indicating that we can indeed achieve improved sample efficiency on the target task when a representation is trained using sufficiently diverse source tasks. Our theoretical results can be readily extended to account for commonly used neural network architectures with realistic assumptions. We conduct empirical analyses that align with our theoretical findings on four simulated environments$\unicode{x2014}$in particular leveraging more data from source tasks can improve sample efficiency on learning in the new task.
    
[^61]: 针对多个数据集约束宇宙学参数的领域适应图神经网络

    Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])

    [http://arxiv.org/abs/2311.01588](http://arxiv.org/abs/2311.01588)

    该论文研究了通过领域适应图神经网络对宇宙学参数进行约束的方法。通过利用GNNs捕捉宇宙学信息和使用最大均值差异进行领域适应，该方法在不同数据集上具有较好的泛化能力。

    

    研究表明，与依赖于摘要统计量（如功率谱）的方法相比，深度学习模型在从复杂宇宙学数据集中提取信息方面表现更好。然而，由于不同模拟套件中的子网格物理实现和数值逼近的差异，模型在一个宇宙学模拟的数据上训练后，在另一个模拟数据上的表现会下降。同样，对于任何模拟数据训练的模型，在应用于观测数据时也可能出现性能下降。通过在两个不同套件的CAMELS水动力宇宙学模拟数据上进行训练，我们研究了领域适应图神经网络（DA-GNNs）的泛化能力。通过利用GNNs，我们可以利用它们捕捉来自星系分布的结构无标度宇宙学信息的能力。此外，通过包括无监督的领域适配最大均值差异（MMD），我们使模型能够自适应地学习两个模拟数据之间的差异。

    Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
    
[^62]: 关于仅使用编码器的浅层Transformer收敛性的研究

    On the Convergence of Encoder-only Shallow Transformers. (arXiv:2311.01575v1 [cs.LG])

    [http://arxiv.org/abs/2311.01575](http://arxiv.org/abs/2311.01575)

    本研究旨在构建仅使用编码器的浅层Transformer在有限宽度条件下的全局收敛理论，并通过处理softmax的输入/输出和证明二次超参数化的有效性来解决其收敛困难。

    

    本文旨在从架构、初始化和缩放的角度出发，在有限宽度的条件下，在现实场景下构建仅使用编码器的浅层Transformer的全局收敛理论。难点在于如何处理Transformer的核心组成部分，即自注意力机制中的softmax。特别地，我们诊断了缩放方案，仔细处理了softmax的输入/输出，并证明了在实践中常用的He/LeCun初始化下，我们的浅层Transformer的全局收敛仅需要二次超参数化。此外，本文还提供了基于神经切换核（NTK）的分析，这有助于全面比较。我们的理论展示了不同缩放方案和初始化的重要性分离。我们相信我们的结果可以为更好地理解现代Transformer，特别是训练动力学，铺平道路。

    In this paper, we aim to build the global convergence theory of encoder-only shallow Transformers under a realistic setting from the perspective of architectures, initialization, and scaling under a finite width regime. The difficulty lies in how to tackle the softmax in self-attention mechanism, the core ingredient of Transformer. In particular, we diagnose the scaling scheme, carefully tackle the input/output of softmax, and prove that quadratic overparameterization is sufficient for global convergence of our shallow Transformers under commonly-used He/LeCun initialization in practice. Besides, neural tangent kernel (NTK) based analysis is also given, which facilitates a comprehensive comparison. Our theory demonstrates the separation on the importance of different scaling schemes and initialization. We believe our results can pave the way for a better understanding of modern Transformers, particularly on training dynamics.
    
[^63]: 使用多标签方法改进FDG-18全身PET / CT扫描中的病变分割：AutoPET II挑战

    Improving Lesion Segmentation in FDG-18 Whole-Body PET/CT scans using Multilabel approach: AutoPET II challenge. (arXiv:2311.01574v1 [eess.IV])

    [http://arxiv.org/abs/2311.01574](http://arxiv.org/abs/2311.01574)

    本研究提出了一种新的方法来改进FDG-18全身PET / CT扫描中病变的分割效果，通过分割器官和病变，提高深度学习模型的性能。通过AutoPET II挑战数据集进行了有效性评估。

    

    使用深度学习模型自动分割FDG-18全身PET / CT扫描中的病变对于确定治疗反应、优化剂量和推动肿瘤学中的治疗应用具有重要意义。然而，存在放射性示踪剂摄取升高的器官（如肝脏、脾脏、大脑和膀胱）常常导致挑战，因为这些区域往往被深度学习模型错误地标记为病变。为了解决这个问题，我们提出了一种新的方法，用于分割器官和病变，旨在提升自动病变分割方法的性能。在本研究中，我们使用AutoPET II挑战数据集评估了我们提出的方法的有效性，该数据集包括1014名患者。我们评估了包含额外标签和数据对模型分割性能的影响。除了专家标注的病变标签外，我们引入了八个包括肝脏、肾脏、输尿管等器官的附加标签。

    Automatic segmentation of lesions in FDG-18 Whole Body (WB) PET/CT scans using deep learning models is instrumental for determining treatment response, optimizing dosimetry, and advancing theranostic applications in oncology. However, the presence of organs with elevated radiotracer uptake, such as the liver, spleen, brain, and bladder, often leads to challenges, as these regions are often misidentified as lesions by deep learning models. To address this issue, we propose a novel approach of segmenting both organs and lesions, aiming to enhance the performance of automatic lesion segmentation methods. In this study, we assessed the effectiveness of our proposed method using the AutoPET II challenge dataset, which comprises 1014 subjects. We evaluated the impact of inclusion of additional labels and data in the segmentation performance of the model. In addition to the expert-annotated lesion labels, we introduced eight additional labels for organs, including the liver, kidneys, urinary 
    
[^64]: 使用视觉语言驱动的图像增强改善公平性

    Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])

    [http://arxiv.org/abs/2311.01573](http://arxiv.org/abs/2311.01573)

    本文提出了一种基于视觉语言驱动的图像增强方法，以改善训练深度学习模型中的公平性问题。通过学习和应用可解释的路径来编辑受保护特征，该方法成功减轻了数据中的相关性，提高了数据集的公平性。

    

    在训练深度学习鉴别模型时，公平性是至关重要的，特别是在面部领域。模型往往将特定特征（如年龄和肤色）与无关属性（下游任务）相关联，导致偏见与现实不符。众所周知，这些相关性存在于数据中，然后在训练过程中传递给模型。本文提出了一种方法来减轻这些相关性，以提高公平性。为此，我们学习了在预训练扩散模型（DiffAE）的语义空间中位于可解释和有意义的路径，这些路径由对比性文本二极体进行监督。也就是说，我们学习了编辑受保护特征（年龄和肤色）。然后将这些路径应用于增强图像，以提高给定数据集的公平性。我们在CelebA-HQ和UTKFace上对年龄和肤色作为受保护特征的几个下游任务进行了该方法的测试。

    Fairness is crucial when training a deep-learning discriminative model, especially in the facial domain. Models tend to correlate specific characteristics (such as age and skin color) with unrelated attributes (downstream tasks), resulting in biases which do not correspond to reality. It is common knowledge that these correlations are present in the data and are then transferred to the models during training. This paper proposes a method to mitigate these correlations to improve fairness. To do so, we learn interpretable and meaningful paths lying in the semantic space of a pre-trained diffusion model (DiffAE) -- such paths being supervised by contrastive text dipoles. That is, we learn to edit protected characteristics (age and skin color). These paths are then applied to augment images to improve the fairness of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on several downstream tasks with age and skin color as protected characteristics. As a proxy for fairnes
    
[^65]: 数据集精简的序列子集匹配

    Sequential Subset Matching for Dataset Distillation. (arXiv:2311.01570v1 [cs.CV])

    [http://arxiv.org/abs/2311.01570](http://arxiv.org/abs/2311.01570)

    这项研究提出了一种序列子集匹配方法来解决数据集精简时的性能下降问题，通过优化每个合成实例的方式提高了生成的合成数据集的质量。

    

    数据集精简是一项新兴任务，用于合成一个用于训练深度神经网络（DNN）的小型数据集，以降低数据存储和模型训练成本。期望合成数据集能够捕捉到真实数据集中所包含的知识精髓，从而产生与真实数据集相似的性能。最近，数据集精简方法的进展在生成合成数据集方面取得了显著的改进。然而，目前的最先进方法将整个合成数据集视为统一实体，并对每个合成实例进行相同的优化。这种静态优化方法可能导致数据集精简的性能下降。具体来说，我们认为静态优化可能会导致合成数据中的耦合问题，特别是在优化较大量的合成数据时。这种耦合问题进而导致精简数据集无法提取高级特征。

    Dataset distillation is a newly emerging task that synthesizes a small-size dataset used in training deep neural networks (DNNs) for reducing data storage and model training costs. The synthetic datasets are expected to capture the essence of the knowledge contained in real-world datasets such that the former yields a similar performance as the latter. Recent advancements in distillation methods have produced notable improvements in generating synthetic datasets. However, current state-of-the-art methods treat the entire synthetic dataset as a unified entity and optimize each synthetic instance equally. This static optimization approach may lead to performance degradation in dataset distillation. Specifically, we argue that static optimization can give rise to a coupling issue within the synthetic data, particularly when a larger amount of synthetic data is being optimized. This coupling issue, in turn, leads to the failure of the distilled dataset to extract the high-level features le
    
[^66]: Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) （具有策略先验的任意时刻竞争性强化学习）

    Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])

    [http://arxiv.org/abs/2311.01568](http://arxiv.org/abs/2311.01568)

    本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。

    

    本文研究了任意时刻竞争性马尔可夫决策过程（A-CMDP）的问题。现有的关于受限制马尔可夫决策过程（CMDP）的研究旨在在随机动态中优化期望回报同时约束期望成本，但是特定情节中的成本仍然可能非常高。相反，A-CMDP的目标是在任意一轮的任何情节中，优化期望回报同时保证有界的成本，并针对策略先验进行了保证。我们提出了一种新的算法，称为Anytime-Competitive Reinforcement Learning（ACRL），它可以证明地保证了任意时刻的成本约束。遗憾分析表明，该策略在任意的竞争性约束下渐近地与最优回报相匹配。在碳智能计算应用中的实验验证了ACRL的回报性能和成本约束保证。

    This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.
    
[^67]: 不同的令牌指标：通过测量衰减来修剪LLM组件并优化量化

    Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])

    [http://arxiv.org/abs/2311.01544](http://arxiv.org/abs/2311.01544)

    本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。

    

    大型语言模型（LLM）以其强大的能力改变了自然语言处理。然而，它们不断增长的大小引发了关于它们的有效部署和LLM压缩的担忧。本研究介绍了一种新的评估压缩LLM的方法，即不同的令牌指标（DTM），解决了传统指标如困惑度无法准确反映文本生成质量的局限性。DTM关注令牌的差异性，提供了对模型压缩微妙之处的更深入洞察。我们的结果表明，在不损害文本生成质量的情况下，可以达到显著的精确度和稀疏度水平。此外，DTM还可以更精确地评估每个组件的影响。利用第一个不同的令牌指标（FDTM）在模型稀疏化中显示，超过90%的所有组件可以修剪掉。对于量化，FDTM表明超过80%的参数可以进行量化。

    Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
    
[^68]: 在可解释的分布比较中的最大平均差异中的变量选择

    Variable Selection in Maximum Mean Discrepancy for Interpretable Distribution Comparison. (arXiv:2311.01537v1 [stat.ML])

    [http://arxiv.org/abs/2311.01537](http://arxiv.org/abs/2311.01537)

    本文研究了数据集比较中的变量选择问题，提出了一种基于最大平均差异的两样本测试方法，通过优化自动相关性检测权重来增强测试的功效，并引入稀疏正则化方法来解决正则化参数选择的问题。

    

    两样本测试是为了判断两个数据集是否来自同一分布。本文研究了两样本测试中的变量选择问题，即识别造成两个分布差异的变量（或维度）的任务。这个任务与模式分析和机器学习的许多问题相关，如数据集漂移适应、因果推断和模型验证。我们的方法基于基于最大平均差异（MMD）的两样本检验。我们优化针对各个变量定义的自动相关性检测（ARD）权重，以最大化基于MMD的检验的功率。对于这种优化，我们引入了稀疏正则化，并提出了两种方法来解决选择适当正则化参数的问题。一种方法是以数据驱动的方式确定正则化参数，另一种方法是合并不同正则化参数的结果。我们确认了这个方法的有效性。

    Two-sample testing decides whether two datasets are generated from the same distribution. This paper studies variable selection for two-sample testing, the task being to identify the variables (or dimensions) responsible for the discrepancies between the two distributions. This task is relevant to many problems of pattern analysis and machine learning, such as dataset shift adaptation, causal inference and model validation. Our approach is based on a two-sample test based on the Maximum Mean Discrepancy (MMD). We optimise the Automatic Relevance Detection (ARD) weights defined for individual variables to maximise the power of the MMD-based test. For this optimisation, we introduce sparse regularisation and propose two methods for dealing with the issue of selecting an appropriate regularisation parameter. One method determines the regularisation parameter in a data-driven way, and the other aggregates the results of different regularisation parameters. We confirm the validity of the pr
    
[^69]: ATGNN:音频标记图神经网络

    ATGNN: Audio Tagging Graph Neural Network. (arXiv:2311.01526v1 [cs.SD])

    [http://arxiv.org/abs/2311.01526](http://arxiv.org/abs/2311.01526)

    ATGNN是一种新颖的图神经网络架构，将音频标记任务中的频谱图视为图结构，并结合了CNN的能力和图神经网络的全局信息共享能力，同时还映射了可学习类别嵌入和相应的频谱图区域之间的语义关系。

    

    深度学习模型，如CNN和Transformers，已经在端到端音频标记方面取得了令人印象深刻的表现。最近的研究表明，尽管堆叠了多个层，CNN的感受野仍然严重受限。而Transformers则能够通过自注意力机制将全局语境映射，但将频谱图视为序列补丁的方式不足以捕捉不规则音频对象。在这项工作中，我们以更加灵活的方式将频谱图视为图结构，并使用一种新颖的图神经网络架构ATGNN进行处理。ATGNN不仅结合了CNN的能力和图神经网络的全局信息共享能力，还映射了可学习类别嵌入和相应的频谱图区域之间的语义关系。我们在两个音频标记任务上评估了ATGNN，在FSD50K数据集上实现了0.585的mAP，在AudioSet-balanced数据集上实现了0.335的mAP，达到了可比较的结果。

    Deep learning models such as CNNs and Transformers have achieved impressive performance for end-to-end audio tagging. Recent works have shown that despite stacking multiple layers, the receptive field of CNNs remains severely limited. Transformers on the other hand are able to map global context through self-attention, but treat the spectrogram as a sequence of patches which is not flexible enough to capture irregular audio objects. In this work, we treat the spectrogram in a more flexible way by considering it as graph structure and process it with a novel graph neural architecture called ATGNN. ATGNN not only combines the capability of CNNs with the global information sharing ability of Graph Neural Networks, but also maps semantic relationships between learnable class embeddings and corresponding spectrogram regions. We evaluate ATGNN on two audio tagging tasks, where it achieves 0.585 mAP on the FSD50K dataset and 0.335 mAP on the AudioSet-balanced dataset, achieving comparable res
    
[^70]: 一种高效的基于机器学习和逼真仿真的水下对接检测和控制系统：一种综合方法

    An Efficient Detection and Control System for Underwater Docking using Machine Learning and Realistic Simulation: A Comprehensive Approach. (arXiv:2311.01522v1 [cs.RO])

    [http://arxiv.org/abs/2311.01522](http://arxiv.org/abs/2311.01522)

    本论文提出了一种高效的水下对接检测和控制系统，采用机器学习和逼真仿真相结合的综合方法。通过比较不同的深度学习架构，并进行知识蒸馏压缩，实现了对于水下对接的实时检测和分类。

    

    水下对接对于使自主水下车辆(AUVs)能持久运行非常关键。为此，AUV必须能够检测和定位对接站，由于海底环境的高度动态性，这一任务非常复杂。基于图像的解决方案提供了高采集速率和适应环境的灵活性; 然而，水下环境存在低能见度、高浑浊度和失真等挑战。此外，为了验证水下对接能力，现场实验可能会由于需要专用设备和安全考虑而昂贵和危险。本工作比较了不同的深度学习架构，进行水下对接检测和分类。性能最好的架构然后通过知识蒸馏在师生范式下进行压缩，以减少网络的内存占用，实现实时实现。

    Underwater docking is critical to enable the persistent operation of Autonomous Underwater Vehicles (AUVs). For this, the AUV must be capable of detecting and localizing the docking station, which is complex due to the highly dynamic undersea environment. Image-based solutions offer a high acquisition rate and versatile alternative to adapt to this environment; however, the underwater environment presents challenges such as low visibility, high turbidity, and distortion. In addition to this, field experiments to validate underwater docking capabilities can be costly and dangerous due to the specialized equipment and safety considerations required to conduct the experiments. This work compares different deep-learning architectures to perform underwater docking detection and classification. The architecture with the best performance is then compressed using knowledge distillation under the teacher-student paradigm to reduce the network's memory footprint, allowing real-time implementatio
    
[^71]: E(2)等变神经网络用于鲁棒的星系形态分类

    E(2) Equivariant Neural Networks for Robust Galaxy Morphology Classification. (arXiv:2311.01500v1 [astro-ph.GA])

    [http://arxiv.org/abs/2311.01500](http://arxiv.org/abs/2311.01500)

    提出了一种利用2D欧几里得群E(2)等变的神经网络架构，用于鲁棒的星系形态分类。研究表明，这种等变网络在鲁棒性和分类准确性方面优于非等变的网络。

    

    我们提出使用对2D欧几里得群E(2)等变的群卷积神经网络架构（GCNNs）来进行星系形态分类的任务，通过利用星系图像中存在的对称性作为体系结构中的归纳偏差。我们通过引入人为扰动，如泊松噪声插入和单像素对抗攻击，进行鲁棒性研究，以模拟有限观测能力的影响。我们训练、验证和测试对E(2)的离散子群 - 循环群和二面体群 - 是等变的GCNNs，发现GCNNs实现了更高的分类准确性，并且在鲁棒性方面始终比非等变的对应物更好，在等变群D16实现了95.52±0.18%的测试集准确度。我们还发现该模型在50%噪声数据集上只损失不到6%的准确性，并且所有的GCNNs都对单像素扰动更不敏感。

    We propose the use of group convolutional neural network architectures (GCNNs) equivariant to the 2D Euclidean group, $E(2)$, for the task of galaxy morphology classification by utilizing symmetries of the data present in galaxy images as an inductive bias in the architecture. We conduct robustness studies by introducing artificial perturbations via Poisson noise insertion and one-pixel adversarial attacks to simulate the effects of limited observational capabilities. We train, validate, and test GCNNs equivariant to discrete subgroups of $E(2)$ - the cyclic and dihedral groups of order $N$ - on the Galaxy10 DECals dataset and find that GCNNs achieve higher classification accuracy and are consistently more robust than their non-equivariant counterparts, with an architecture equivariant to the group $D_{16}$ achieving a $95.52 \pm 0.18\%$ test-set accuracy. We also find that the model loses $<6\%$ accuracy on a $50\%$-noise dataset and all GCNNs are less susceptible to one-pixel perturb
    
[^72]: 研究扩散模型以加速电子结构计算的行为

    Investigating the Behavior of Diffusion Models for Accelerating Electronic Structure Calculations. (arXiv:2311.01491v1 [physics.chem-ph])

    [http://arxiv.org/abs/2311.01491](http://arxiv.org/abs/2311.01491)

    本研究调查了用于分子生成的扩散模型的行为，并发现这些模型能够通过机器学习加速电子结构计算，而无需昂贵的第一原理数据集。模型推断过程包括探索阶段和松弛阶段，模型能够学习势能表面的一阶和高阶结构，并且松弛阶段还可以用于采样玻尔兹曼分布。

    

    我们对分子生成的扩散模型进行了调查，旨在更好地理解它们的预测与基于物理的计算结果的比较。对这些模型的研究是由于它们在使用机器学习大幅加速电子结构计算方面的潜力，而不需要昂贵的第一原理数据集来训练原子间势能。我们发现，用于生成新分子的一个流行的扩散模型的推断过程分为探索阶段（模型选择原子种类）和松弛阶段（模型调整原子坐标以寻找低能几何结构）。随着训练的进行，我们展示了模型最初学习了势能表面的一阶结构，然后逐渐学习了高阶结构。我们还发现，扩散模型的松弛阶段可以被重新用来采样玻尔兹曼分布。

    We present an investigation into diffusion models for molecular generation, with the aim of better understanding how their predictions compare to the results of physics-based calculations. The investigation into these models is driven by their potential to significantly accelerate electronic structure calculations using machine learning, without requiring expensive first-principles datasets for training interatomic potentials. We find that the inference process of a popular diffusion model for de novo molecular generation is divided into an exploration phase, where the model chooses the atomic species, and a relaxation phase, where it adjusts the atomic coordinates to find a low-energy geometry. As training proceeds, we show that the model initially learns about the first-order structure of the potential energy surface, and then later learns about higher-order structure. We also find that the relaxation phase of the diffusion model can be re-purposed to sample the Boltzmann distributio
    
[^73]: FedSN：一个适用于LEO卫星网络的通用联邦学习框架

    FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])

    [http://arxiv.org/abs/2311.01483](http://arxiv.org/abs/2311.01483)

    FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。

    

    最近，许多低地球轨道（LEO）卫星已经由商业公司成功地发射和部署到太空中，如SpaceX。由于LEO卫星配备了多模传感器，它们不仅用于通信，还用于各种机器学习应用，如空间调制识别、遥感图像分类等。然而，由于与LEO卫星的有限接触时间（例如5分钟），地面站（GS）可能无法下载如此大量的原始感测数据进行集中模型训练。因此，联邦学习（FL）已经成为解决这个问题的有希望的解决方案，通过在设备上进行训练。不幸的是，要在LEO卫星上使用FL，我们仍然面临三个关键挑战，即i）异构计算和存储能力，ii）有限的上行速率，以及iii）模型陈旧问题。为此，我们提出了一种名为FedSN的通用FL框架来解决上述挑战，一

    Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
    
[^74]: 通过神经坍塌的视角检测到群外分布

    Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v1 [cs.LG])

    [http://arxiv.org/abs/2311.01479](http://arxiv.org/abs/2311.01479)

    通过观察到的群内特征聚集和群外特征离散的性质，本论文提出了一种基于特征和权重向量接近程度的神经坍塌（NC-OOD）检测器来提高OAD检测的泛化能力，并取得了最先进的效果。

    

    群外（OOD）检测对于安全部署人工智能至关重要。特别是，OOD检测器应该在各种场景中有效地泛化。为了改进现有OOD检测器的泛化能力，我们引入了一种高度灵活的OOD检测器，称为神经坍塌（NC-OOD）检测器。我们扩展了普遍观察到的群内（ID）特征倾向于形成簇，而群外特征则远离的观察。特别是基于最近的观察结果，神经坍塌，我们进一步证明ID特征倾向于在接近权重向量的位置聚集。根据我们的扩展观察，我们提出了一种基于特征与权重向量的接近程度来检测OOD的方法。为了进一步排除OOD样本，我们利用了OOD特征倾向于比ID特征更接近原点的观察结果。大量实验证明我们的方法增强了现有工作的泛化能力，并且在OOD检测方面始终能够达到最先进的水平。

    Out-of-distribution (OOD) detection is essential for the safe deployment of AI. Particularly, OOD detectors should generalize effectively across diverse scenarios. To improve upon the generalizability of existing OOD detectors, we introduce a highly versatile OOD detector, called Neural Collapse inspired OOD detector (NC-OOD). We extend the prevalent observation that in-distribution (ID) features tend to form clusters, whereas OOD features are far away. Particularly, based on the recent observation, Neural Collapse, we further demonstrate that ID features tend to cluster in proximity to weight vectors. From our extended observation, we propose to detect OOD based on feature proximity to weight vectors. To further rule out OOD samples, we leverage the observation that OOD features tend to reside closer to the origin than ID features. Extensive experiments show that our approach enhances the generalizability of existing work and can consistently achieve state-of-the-art OOD detection per
    
[^75]: 自动驾驶中基于人类感知机制的对抗性机器学习韧性

    Adversary ML Resilience in Autonomous Driving Through Human Centered Perception Mechanisms. (arXiv:2311.01478v1 [cs.CV])

    [http://arxiv.org/abs/2311.01478](http://arxiv.org/abs/2311.01478)

    本研究通过开发和评估多个机器学习模型，研究了自动驾驶系统在胶带、涂鸦和光照等三种物理对抗攻击下的韧性，主要针对对象分类器。研究结果表明当前模型在泛化输入数据方面存在过拟合和欠拟合问题。

    

    对道路标志的物理对抗攻击不断利用现代自动驾驶车辆中的漏洞，妨碍其正确分类所遇到的道路标志类型。目前的模型不能很好地泛化输入数据，导致过拟合或欠拟合。在过拟合中，模型记忆输入数据但不能推广到新的情景。在欠拟合中，模型对输入数据学习不足，不能准确分类这些道路标志。本文研究了自动驾驶系统在三种主要的物理对抗攻击（胶带、涂鸦、光照）下的韧性，特别针对对象分类器。开发了多个机器学习模型，并在两个不同的数据集上进行评估：道路标志（停车标志、限速标志、交通灯和人行横道标志）和几何形状（八边形、圆形、正方形和三角形）。研究比较了不同条件下算法的性能。

    Physical adversarial attacks on road signs are continuously exploiting vulnerabilities in modern day autonomous vehicles (AVs) and impeding their ability to correctly classify what type of road sign they encounter. Current models cannot generalize input data well, resulting in overfitting or underfitting. In overfitting, the model memorizes the input data but cannot generalize to new scenarios. In underfitting, the model does not learn enough of the input data to accurately classify these road signs. This paper explores the resilience of autonomous driving systems against three main physical adversarial attacks (tape, graffiti, illumination), specifically targeting object classifiers. Several machine learning models were developed and evaluated on two distinct datasets: road signs (stop signs, speed limit signs, traffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons, circles, squares, and triangles). The study compared algorithm performance under different condi
    
[^76]: 在随机学习理论中应用聚合马尔可夫过程的研究

    Applications of the Theory of Aggregated Markov Processes in Stochastic Learning Theory. (arXiv:2311.01476v1 [stat.ML])

    [http://arxiv.org/abs/2311.01476](http://arxiv.org/abs/2311.01476)

    本文描述了聚合马尔可夫过程（AMP）的理论如何应用于随机学习理论中，以降低维度并实现学习特定任务的目标。

    

    通过将函数与马尔可夫过程组合得到的随机过程被称为聚合马尔可夫过程（AMP）。将马尔可夫过程与函数组合的目的可以是降低维度，例如在特定坐标上进行投影。AMP的理论已经被Dynkin、Cameron、Rogers和Pitman以及Kelly等人广泛研究，他们提供了AMP保持马尔可夫性的充分条件。在另一个方向上，Larget提供了AMP的规范表示，可以用于验证两个AMP的等价性。本文旨在描述如何将AMP的理论应用于随机学习理论中，以实现他们在学习特定任务时的目标。

    A stochastic process that arises by composing a function with a Markov process is called an aggregated Markov process (AMP). The purpose of composing a Markov process with a function can be a reduction of dimensions, e.g., a projection onto certain coordinates. The theory around AMP has been extensively studied e.g. by Dynkin, Cameron, Rogers and Pitman, and Kelly, all of whom provided sufficient conditions for an AMP to remain Markov. In another direction, Larget provided a canonical representation for AMP, which can be used to verify the equivalence of two AMPs. The purpose of this paper is to describe how the theory of AMP can be applied to stochastic learning theory as they learn a particular task.
    
[^77]: 基于图割的基于补丁的深度无监督图像分割

    Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts. (arXiv:2311.01475v1 [cs.CV])

    [http://arxiv.org/abs/2311.01475](http://arxiv.org/abs/2311.01475)

    本文提出了一种基于补丁的深度无监督图像分割策略，将深度聚类方法和经典的图割方法相结合，通过训练简单的卷积神经网络对图像补丁进行分类，并利用图割算法进行迭代正则化，实现了最先进的分割效果。

    

    无监督图像分割旨在在不使用人工注释的情况下将图像中的不同语义模式分组。类似地，图像聚类根据它们的语义内容搜索图像的分组，也不需要监督。经典地，这两个问题吸引了研究人员的关注，因为它们从坚实的数学概念中产生了具体的应用。随着深度学习的出现，科学界将注意力转向了基于复杂神经网络的解决方法，在这些领域取得了令人印象深刻的结果，但很少利用经典方法取得的进展。在这项工作中，我们提出了一种基于补丁的无监督图像分割策略，将深度聚类方法的无监督特征提取进展与经典的基于图的方法相结合。我们展示了一个简单的卷积神经网络，通过图割进行迭代正则化，可以自然地实现最先进的分割效果。

    Unsupervised image segmentation aims at grouping different semantic patterns in an image without the use of human annotation. Similarly, image clustering searches for groupings of images based on their semantic content without supervision. Classically, both problems have captivated researchers as they drew from sound mathematical concepts to produce concrete applications. With the emergence of deep learning, the scientific community turned its attention to complex neural network-based solvers that achieved impressive results in those domains but rarely leveraged the advances made by classical methods. In this work, we propose a patch-based unsupervised image segmentation strategy that bridges advances in unsupervised feature extraction from deep clustering methods with the algorithmic help of classical graph-based methods. We show that a simple convolutional neural network, trained to classify image patches and iteratively regularized using graph cuts, naturally leads to a state-of-the
    
[^78]: 物理世界中的对抗样本：一项综述

    Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])

    [http://arxiv.org/abs/2311.01473](http://arxiv.org/abs/2311.01473)

    本综述系统地研究了物理世界中的对抗样本（PAEs）的特点，并提出了基于其特征的全面分析和分类框架，涵盖了100多个研究，以填补对PAEs独特特征的现有研究不足。

    

    深度神经网络（DNNs）对对抗样本表现出高度的脆弱性。除了在数字世界中的攻击外，对抗样本在物理世界中的实际影响提出了重大挑战和安全性问题。然而，当前对物理对抗样本（PAEs）的研究缺乏对其独特特征的全面理解，导致其重要性和理解的局限性。本文通过在训练、制造和重采样过程中全面考察PAEs的特点来弥补这一差距。通过分析物理对抗攻击之间的联系，我们确定制造和重采样是PAEs中独特属性和特殊性的主要来源。利用这一知识，我们基于其特定特征开发了一个全面的PAEs分析和分类框架，涵盖了100多个物理对抗世界研究的研究。

    Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
    
[^79]: 利用语言模型检测环保虚假宣传

    Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])

    [http://arxiv.org/abs/2311.01469](http://arxiv.org/abs/2311.01469)

    本研究引入了一种新的方法，利用语言模型来检测绿色虚假宣传风险。开发了一种量化绿色虚假宣传风险的数学形式，建立了优化的ClimateBERT模型，并进行了结果比较分析。实验表明，我们的方法对于这一任务具有良好的探索方向。

    

    近年来，气候变化的后果越来越引起公众的关注。因此，企业在可持续发展报告中强调其环保努力以增强公众形象。然而，对此类报告的审核缺乏严格的监管，可能导致绿色虚假宣传。在本研究中，我们引入了一种新的方法来对绿色虚假宣传风险进行训练语言模型。我们的主要贡献包括：开发了一种数学形式来量化绿色虚假宣传风险，提出了一个针对该问题的优化ClimateBERT模型，并进行了结果的比较分析。在一个包含可持续发展报告的测试集上，我们的最佳模型实现了平均准确率86.34%和F1值0.67，表明我们的方法对于这一任务具有探索的良好方向。

    In recent years, climate change repercussions have increasingly captured public interest. Consequently, corporations are emphasizing their environmental efforts in sustainability reports to bolster their public image. Yet, the absence of stringent regulations in review of such reports allows potential greenwashing. In this study, we introduce a novel methodology to train a language model on generated labels for greenwashing risk. Our primary contributions encompass: developing a mathematical formulation to quantify greenwashing risk, a fine-tuned ClimateBERT model for this problem, and a comparative analysis of results. On a test set comprising of sustainability reports, our best model achieved an average accuracy score of 86.34% and F1 score of 0.67, demonstrating that our methods show a promising direction of exploration for this task.
    
[^80]: 记住你所做的，这样你就知道接下来该做什么。

    Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])

    [http://arxiv.org/abs/2311.01468](http://arxiv.org/abs/2311.01468)

    本文通过使用一个大型语言模型（LLM）为模拟机器人制定计划，在ScienceWorld中实现30类目标。实验结果显示，LLM在马尔可夫假设的情况下比强化学习方法的性能提高了1.4倍，当填充尽可能多的先前步骤时提高到3.5倍，即使只训练了6.5%的数据，也比基于强化学习方法的性能提高了2.2倍。不同类别的动作表现差异很大，说明平均任务可能会隐藏性能问题。

    

    我们探讨使用一个中等规模的大型语言模型（GPT-J 6B参数），为模拟机器人在ScienceWorld中实现30类目标（一个用于小学科学实验的文本游戏模拟器）制定计划。先前发表的实证研究声称，与强化学习相比，大型语言模型（LLMs）的适用性较差（Wang等，2022）。使用马尔可夫假设（单个前一个步骤），LLM的性能超过基于强化学习的方法1.4倍。当我们尽可能多地填充LLM的输入缓冲区时，改进效果提高到3.5倍。即使只对6.5%的训练数据进行训练，我们观察到与基于强化学习的方法相比，性能提高了2.2倍。我们的实验显示，对于30类动作，性能差异很大，表明对任务进行平均可能会隐藏显著的性能问题。与我们同时进行的Lin等人（2023）的工作证明了一种两部分方法（SwiftSa）

    We explore using a moderately sized large language model (GPT-J 6B parameters) to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments. Previously published empirical work claimed that large language models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement learning. Using the Markov assumption (a single previous step), the LLM outperforms the reinforcement learning-based approach by a factor of 1.4. When we fill the LLM's input buffer with as many prior steps as possible, improvement rises to 3.5x. Even when training on only 6.5% of the training data, we observe a 2.2x improvement over the reinforcement-learning-based approach. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues. In work contemporaneous with ours, Lin et al. (2023) demonstrated a two-part approach (SwiftSa
    
[^81]: 支持可信度的LLM创建过程：处理医疗AI中的幻觉

    Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])

    [http://arxiv.org/abs/2311.01463](http://arxiv.org/abs/2311.01463)

    这篇论文描述了在医疗人工智能中创建可靠、可信和无偏置的LLM模型的关键要素，着重于量化、验证和缓解幻觉问题，并讨论了LLM在医疗领域的未来发展。

    

    在短时间内，大型语言模型在多个领域中迅速增多。然而，由于准确性、连贯性和幻觉等问题，医疗领域对其采用存在犹豫。鉴于医疗事关重大，许多研究人员甚至提出在解决这些问题之前不应使用这些模型。在本文中，我们描述了创建可靠、可信和无偏置模型的关键要素，这是其在医疗领域应用的必要条件。具体而言，我们着重于在医疗背景下对幻觉进行量化、验证和缓解。最后，我们讨论了LLM在医疗领域未来的可能发展方向。

    Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.
    
[^82]: FlashDecoding++: 在GPU上加速大规模语言模型推理的更快算法

    FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])

    [http://arxiv.org/abs/2311.01282](http://arxiv.org/abs/2311.01282)

    FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。

    

    随着大规模语言模型在各个领域的重要性日益增加，加速语言模型推理仍然存在一些挑战未解决：(1) 同步部分softmax更新。softmax操作需要同步更新每个部分softmax结果，导致LLM中注意力计算的开销增加约20%。(2) 未充分利用扁平GEMM计算。在LLM推理中执行GEMM的矩阵形状是扁平的，导致在先前的设计中填充零后计算未充分利用，性能损失超过50%。(3) 静态数据流导致的性能损失。LLM中的内核性能取决于不同的输入数据特征、硬件配置等。单一和静态的数据流可能导致LLM推理中不同形状的GEMM的性能损失达到50.25%。我们提出了FlashDecoding++，一种快速支持主流LLM和硬件后端的LLM推理引擎。为了解决上述挑战，FlashDecoding++实现了以下目标：

    As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
    
[^83]: 通过使用语言模型模拟受众群体，改善人际沟通

    Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])

    [http://arxiv.org/abs/2311.00687](http://arxiv.org/abs/2311.00687)

    本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。

    

    我们如何与他人进行沟通以实现自己的目标？我们利用先前的经验或他人的建议，或者通过预测对方的反应来构造候选表达。然而，我们的经验是有限和有偏见的，而且对潜在结果进行推理可能是困难且认知上具有挑战性的。本文中，我们探讨了如何利用大型语言模型（LLM）模拟来帮助我们更好地沟通。我们提出了探索-生成-模拟（EGS）框架，该框架接受任何一个个体与一个目标受众进行沟通的场景作为输入。EGS（1）通过生成与场景相关的多样化建议来探索解决方案空间，（2）生成以部分建议为条件的沟通候选，（3）模拟不同受众的反应，以确定最佳候选和建议的使用。我们在涵盖人际沟通十个基本过程的八个场景上评估了该框架。

    How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
    
[^84]: 集成模型胜过单一模型的不确定性和对高超声速流动的操作学习预测

    Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows. (arXiv:2311.00060v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2311.00060](http://arxiv.org/abs/2311.00060)

    本研究通过集成技术扩展了DeepONet模型，该模型可以在高超声速流动环境中对未知输入进行可靠和置信的预测。

    

    高保真的计算模拟和物理实验对于高超声速流动是资源密集型的。在有限的高保真数据上训练科学机器学习（SciML）模型是迅速预测未见情况下行为的一种方法。然而，高保真数据本身的数量有限，无法验证SciML模型在未探索的输入空间中的全部输出。因此，希望能够获得具有不确定性的SciML模型。然后可以使用SciML模型的输出不确定性来评估模型预测的可靠性和置信度。在这项研究中，我们使用三种不同的不确定性量化机制（均值-方差估计，证据不确定性和集成）来扩展DeepONet。这些具有不确定性意识的DeepONet模型在高超声速流动环境中，通过计算流体动力学产生的数据，对一个钝锥体对象周围的流动进行训练和评估。我们发现集成技术能够...

    High-fidelity computational simulations and physical experiments of hypersonic flows are resource intensive. Training scientific machine learning (SciML) models on limited high-fidelity data offers one approach to rapidly predict behaviors for situations that have not been seen before. However, high-fidelity data is itself in limited quantity to validate all outputs of the SciML model in unexplored input space. As such, an uncertainty-aware SciML model is desired. The SciML model's output uncertainties could then be used to assess the reliability and confidence of the model's predictions. In this study, we extend a DeepONet using three different uncertainty quantification mechanisms: mean-variance estimation, evidential uncertainty, and ensembling. The uncertainty aware DeepONet models are trained and evaluated on the hypersonic flow around a blunt cone object with data generated via computational fluid dynamics over a wide range of Mach numbers and altitudes. We find that ensembling o
    
[^85]: CapsFusion: 重新思考大规模图像-文本数据

    CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.20550](http://arxiv.org/abs/2310.20550)

    CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。

    

    大规模多模态模型展示了在零样本情况下执行多样化多模态任务的显著泛化能力。大规模基于网络的图像-文本对在这一成功中起着根本性的贡献，但存在着过多的噪声。最近的研究使用由生成式字幕模型合成的替代字幕，并取得了显著的基准性能。然而，我们的实验证明，在使用合成字幕训练的模型中存在着显著的可扩展性不足和世界知识丧失问题，这些问题在其初始基准成功中大部分被掩盖了。经过进一步的研究，我们确定根本原因是现有合成字幕中过于简化的语言结构和缺乏知识细节。为了提供更高质量、更可扩展的多模态预训练数据，我们提出了CapsFusion，这是一个先进的框架，利用大型语言模型来整合和细化来自网络图像-文本对和合成字幕的信息。

    Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
    
[^86]: 强化学习中的Dropout策略：限制策略优化方法中替代目标方差的增长

    Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])

    [http://arxiv.org/abs/2310.20380](http://arxiv.org/abs/2310.20380)

    本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。

    

    基于策略的强化学习算法在各个领域中被广泛应用。其中，主流的策略优化算法如PPO和TRPO引入了重要性采样到强化学习中，这允许重用历史数据。然而，这也导致了替代目标方差的增加，间接影响了算法的稳定性和收敛性。本文首先推导出了替代目标方差的上界，它可以随替代目标的增加而呈二次增长。接下来，我们提出了一种Dropout技术，以避免重要性采样引起的替代目标方差过度增加。然后，我们引入了一个通用的强化学习框架，适用于主流的策略优化方法，并将Dropout技术应用于PPO算法，得到了D-PPO变体。最后，我们在Atari 2600游戏上对D-PPO和PPO算法进行了比较实验。

    Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 
    
[^87]: 通过学习相关的潜在空间推进贝叶斯优化

    Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])

    [http://arxiv.org/abs/2310.20258](http://arxiv.org/abs/2310.20258)

    本文提出了一种通过学习相关的潜在空间来推进贝叶斯优化的方法。该方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以减小在潜在空间和目标函数之间的差距，并在多个优化任务中展示了其有效性。

    

    贝叶斯优化是一种通过有限的函数评估来优化黑盒函数的强大方法。最近的研究表明，通过深度生成模型（如变分自动编码器）在潜在空间中进行优化，可以对结构化或离散数据进行有效和高效的贝叶斯优化。然而，由于优化不是在输入空间中进行，这导致了潜在的差距，可能导致次优解。为了减轻这种差距，我们提出了相关潜在空间贝叶斯优化（CoBO），它专注于学习相关的潜在空间，其特点是潜在空间中的距离和目标函数内的距离之间存在强相关性。具体来说，我们的方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以最小化有希望区域周围的潜在差距。我们在几个优化任务中展示了我们方法的有效性。

    Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc
    
[^88]: 使用测地线等变非线性消息传递对网格进行动力学建模

    Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing. (arXiv:2310.19589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19589](http://arxiv.org/abs/2310.19589)

    该论文提出了一种新的使用非线性消息传递的规范等变架构，用于在网格上建模具有复杂非线性动力学的表面PDE，实现了比卷积或注意力网络更高的性能。

    

    非欧几里德流形上的数据，通常以表面网格的形式离散化，在计算机图形学、生物和物理系统中自然产生。特别是，流形上的偏微分方程（PDE）的解决方案严重依赖于底层几何结构。尽管图神经网络已成功应用于PDE，但它们不包含表面几何结构，也不考虑流形的局部规范对称性。与此相反，最近在网格上的规范等变卷积和注意力架构的研究利用底层几何结构，但在建模具有复杂非线性动力学的表面PDE时表现不佳。为解决这些问题，我们引入了一种新的使用非线性消息传递的规范等变架构。我们的新架构在具有高度复杂和非线性动力学的域上比卷积或注意力网络有更高的性能。然而，与非网格情况类似，设计上的权衡更倾向于卷积和注意力网络。

    Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, atten
    
[^89]: Bayes战胜交叉验证：通过期望最大化实现高效准确的岭回归

    Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization. (arXiv:2310.18860v1 [stat.ML])

    [http://arxiv.org/abs/2310.18860](http://arxiv.org/abs/2310.18860)

    本文提出了一种基于贝叶斯公式的岭回归方法，通过期望最大化来调节正则化超参数，该方法不需要指定候选的λ并且在大样本下可以找到唯一的最优解。

    

    我们提出了一种新的方法来调节岭回归的正则化超参数λ，该方法的计算速度比留一交叉验证(LOOCV)快，同时在稀疏协变量的情况下可以获得与LOOCV相等或更好的回归参数估计。对于有限的n，LOOCV风险可能受到多个和不好的局部最小值的影响，因此需要指定一组候选的λ，这可能无法提供良好的解决方案。相反，我们证明了所提出的方法在足够大的n下可以找到唯一的最优解，并且不需要指定任何难以确定的超参数。这是基于岭回归的贝叶斯公式，我们证明了对于足够大的n，后验是单峰的，可以同时学习最优的λ和回归系数。

    We present a novel method for tuning the regularization hyper-parameter, $\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\lambda$ and the regression coefficients to be jointly learned wi
    
[^90]: 通过创建固定目标来改进内在探索

    Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])

    [http://arxiv.org/abs/2310.18144](http://arxiv.org/abs/2310.18144)

    该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。

    

    强化学习中的探索奖励通过定义自定义的内在目标来引导长期探索。基于计数的方法使用状态访问频率来获得探索奖励。本文发现，任何从基于计数的方法导出的内在奖励函数都是非固定的，因此为代理人构建了一个难以优化的目标。我们工作的关键贡献在于通过增强状态表示将原始的非固定奖励转化为固定奖励。为此，我们引入了用于探索的固定目标（SOFE）框架。SOFE需要识别不同探索奖励的足够统计量，并找到一种将这些统计量高效编码作为深度网络输入的方法。SOFE基于提出扩展状态空间的状态增强，但有希望简化代理目标的优化。我们的实验结果表明，SOFE改善了探索效果。

    Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
    
[^91]: 使用参数化操作原语学习外在灵巧性

    Learning Extrinsic Dexterity with Parameterized Manipulation Primitives. (arXiv:2310.17785v1 [cs.RO])

    [http://arxiv.org/abs/2310.17785](http://arxiv.org/abs/2310.17785)

    论文通过学习一系列参数化操作原语，并利用环境改变物体姿态，解决了机器人在目标物体抓取被环境遮挡时的问题。

    

    许多实际相关的机器人抓取问题都涉及到目标物体，其所有抓取都被环境遮挡。在这种情况下，单次抓取计划总是失败的。因此，有必要首先将物体操作到一个适合进行抓取的配置。我们通过学习一系列利用环境改变物体姿态的动作来解决这个问题。具体而言，我们利用分层强化学习来结合一系列学习到的参数化操作原语。通过学习低级操作策略，我们的方法可以通过利用物体、夹具和环境之间的相互作用来控制物体的状态。在无控制条件下分析设计这样一个复杂行为是不可行的，因为分析方法需要准确建模互动和接触动力学。相反，我们学习一个在深度图像上直接操作的分层策略模型。

    Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth
    
[^92]: 在快速发展时代管理人工智能风险

    Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2310.17688](http://arxiv.org/abs/2310.17688)

    在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。

    

    在这篇简短的共识文中，我们概述了即将到来的先进人工智能系统所带来的风险。我们审查了大规模的社会危害和恶意使用，以及人类对自主人工智能系统失去控制的不可逆转的损失。鉴于人工智能的快速和持续进展，我们提出了人工智能研发和治理的优先事项。

    In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.
    
[^93]: 一种鲁棒神经ODE的极小极大优化控制方法

    A minimax optimal control approach for robust neural ODEs. (arXiv:2310.17584v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2310.17584](http://arxiv.org/abs/2310.17584)

    本文提出了一种鲁棒神经ODE的极小极大优化控制方法，将对抗训练问题转化为一个最优控制问题，并提供了一种新的加权技术来实现鲁棒训练。

    

    本文从鲁棒控制的角度解决了神经ODE的对抗训练问题。这是一种替代经验风险最小化的训练方法，被广泛用于确保对输入扰动的可靠结果。神经ODE允许将深度神经网络解释为控制系统的离散化，从控制理论中提供了强大的工具，用于机器学习的开发和理解。在这种特定情况下，我们将扰动数据的对抗训练公式化为极小极大优化控制问题，并推导出了Pontryagin最大值原理的一阶最优性条件。我们提供了鲁棒训练的新解释，导致了一种替代的加权技术，并在低维分类任务上进行了测试。

    In this paper, we address the adversarial training of neural ODEs from a robust control perspective. This is an alternative to the classical training via empirical risk minimization, and it is widely used to enforce reliable outcomes for input perturbations. Neural ODEs allow the interpretation of deep neural networks as discretizations of control systems, unlocking powerful tools from control theory for the development and the understanding of machine learning. In this specific case, we formulate the adversarial training with perturbed data as a minimax optimal control problem, for which we derive first order optimality conditions in the form of Pontryagin's Maximum Principle. We provide a novel interpretation of robust training leading to an alternative weighted technique, which we test on a low-dimensional classification task.
    
[^94]: Causal Q-Aggregation for CATE Model Selection（CATE模型选择中的因果Q集成）

    Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])

    [http://arxiv.org/abs/2310.16945](http://arxiv.org/abs/2310.16945)

    该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率

    

    准确估计条件平均处理效应（CATE）是个性化决策的核心。尽管有大量用于CATE估计的模型，但由于因果推断的基本问题，模型选择是一项非常棘手的任务。最近的实证工作提供了有利于具有双重鲁棒性质的代理损失度量和模型集成的证据。然而，对于这些模型的理论理解还不够。直接应用先前的理论工作会由于模型选择问题的非凸性而导致次优的预测模型选择率。我们提供了现有主要CATE集成方法的遗憾率，并提出了一种基于双重鲁棒损失的Q集成的新的CATE模型集成方法。我们的主要结果表明，因果Q集成在预测模型选择的遗憾率上达到了统计上的最优值为$\frac{\log(M)}{n}$（其中$M$为模型数，$n$为样本数），加上高阶估计误差项

    Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
    
[^95]: 使用普适微分方程在SIR模型中学习COVID-19的区域传播

    Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model. (arXiv:2310.16804v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.16804](http://arxiv.org/abs/2310.16804)

    该论文提出了使用普适微分方程（UDEs）改进SIR模型来学习COVID-19的区域传播。通过在SIR方程中加入一个深度神经网络（DNN），可以更好地捕捉邻近地区的影响，并改进模型的预测能力。

    

    高度互连的社会很难对传染病如COVID-19的传播进行建模。单一区域的SIR模型无法考虑到入侵性感染力，并且将其扩展到大量互相作用的区域涉及到许多在现实世界中不能成立的假设。我们提议使用普适微分方程（UDEs）来捕捉邻近地区的影响，并在结合SIR + UDE模型中改进模型的预测。UDEs是由深度神经网络（DNN）完全或部分定义的微分方程。我们在SIR方程中加入一个由DNN组成的附加项，该DNN从其他地区学习入侵性感染力。使用自动微分和梯度下降进行学习，以逼近邻近地区状态引起的目标系统变化。我们将所提出的模型与单一区域SIR模型和完全数据驱动模型进行了比较。

    Highly-interconnected societies difficult to model the spread of infectious diseases such as COVID-19. Single-region SIR models fail to account for incoming forces of infection and expanding them to a large number of interacting regions involves many assumptions that do not hold in the real world. We propose using Universal Differential Equations (UDEs) to capture the influence of neighboring regions and improve the model's predictions in a combined SIR+UDE model. UDEs are differential equations totally or partially defined by a deep neural network (DNN). We include an additive term to the SIR equations composed by a DNN that learns the incoming force of infection from the other regions. The learning is performed using automatic differentiation and gradient descent to approach the change in the target system caused by the state of the neighboring regions. We compared the proposed model using a simulated COVID-19 outbreak against a single-region SIR and a fully data-driven model compose
    
[^96]: 检测大型语言模型的预训练数据

    Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16789](http://arxiv.org/abs/2310.16789)

    这项研究探讨了如何检测大型语言模型的预训练数据，提出了一个动态基准和一种新的检测方法，以解决数据隐私和不透明性的问题。

    

    虽然大型语言模型（LLM）被广泛应用，但用于训练它们的数据很少被公开。考虑到这些数据的规模之大，可能包含受版权保护的材料、个人可识别信息以及用于广泛报道的参考基准测试数据，我们几乎可以肯定它们包含了潜在的问题文本。然而，我们目前无法知道这些文本中包含了哪些类型的数据以及比例。在本文中，我们研究了预训练数据检测问题：在不知道预训练数据的情况下，给定一段文本和对LLM的黑盒访问，我们能否确定模型是否是在提供的文本上进行了训练？为了方便这项研究，我们引入了一个动态基准WIKIMIA，使用在模型训练之前和之后创建的数据来支持金标准检测。我们还引入了一种新的检测方法Min-K% Prob，基于一个简单的假设：一个未见过的例子可能包含几个具有较低概率的离群词。

    Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
    
[^97]: 通过最优控制的视角进行提示工程

    Prompt Engineering Through the Lens of Optimal Control. (arXiv:2310.14201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14201](http://arxiv.org/abs/2310.14201)

    本文通过最优控制的视角提出了一个针对大型语言模型的多轮交互的提示工程框架，系统化了现有方法，并扩大了适用范围。

    

    提示工程（PE）已经成为指导大型语言模型（LLM）解决复杂任务的关键技术。它的重要性在于其潜力可以显著提高人机交互的效率和效果。随着任务变得越来越复杂，最近的高级PE方法已经超越了单轮交互的限制，而是采用了多轮交互，这样可以更加深入和细致地与LLM进行交互。在本文中，我们提出了一个针对LLM的多轮交互的最优控制框架。该框架提供了统一的数学结构，不仅系统化了现有的提示工程方法，还为严谨的分析改进奠定了基础。此外，我们还将该框架扩展到包括通过集合方法和多智能体协作的提示工程，从而扩大了适用范围。通过采用最优控制的视角，我们对现有提示工程方法提供了新的见解。

    Prompt Engineering (PE) has emerged as a critical technique for guiding Large Language Models (LLMs) in solving intricate tasks. Its importance is highlighted by its potential to significantly enhance the efficiency and effectiveness of human-machine interaction. As tasks grow increasingly complex, recent advanced PE methods have extended beyond the limitations of single-round interactions to embrace multi-round interactions, which allows for a deeper and more nuanced engagement with LLMs. In this paper, we propose an optimal control framework tailored for multi-round interactions with LLMs. This framework provides a unified mathematical structure that not only systematizes the existing PE methods but also sets the stage for rigorous analytical improvements. Furthermore, we extend this framework to include PE via ensemble methods and multi-agent collaboration, thereby enlarging the scope of applicability. By adopting an optimal control perspective, we offer fresh insights into existing
    
[^98]: 具有多项式激活函数的图神经网络具有有限的表达能力

    Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])

    [http://arxiv.org/abs/2310.13139](http://arxiv.org/abs/2310.13139)

    本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。

    

    图神经网络（GNNs）的表达能力可以完全由适当的一阶逻辑片段来描述。换句话说，任何在标记图上解释的关于二元逻辑片段（GC2）的查询都可以使用一个大小仅取决于查询深度的GNN来表示。正如[Barcelo＆Al。，2020，Grohe，2021]指出的那样，这个描述适用于一组激活函数的家族，这表明GNN可以通过不同的激活函数选择来表达不同的逻辑层次结构。在本文中，我们证明了这样的层次结构的存在，证明了具有多项式激活函数的GNN无法表示GC2查询。这意味着多项式和常用的非多项式激活函数（如ReLU、sigmoid、双曲正切等）之间存在一个分离，并回答了[Grohe，2021]提出的一个悬而未决的问题。

    The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo & Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
    
[^99]: 学生如何成为教师：通过谱方法学习和遗忘

    How a student becomes a teacher: learning and forgetting through Spectral methods. (arXiv:2310.12612v1 [cs.LG])

    [http://arxiv.org/abs/2310.12612](http://arxiv.org/abs/2310.12612)

    本论文提出了基于谱方法的优化方案，用于解决在非凸性问题下学生网络与教师网络之间存在的不变子网络的识别问题。

    

    在理论机器学习中，学生-教师模型常被用作现实生活中教学的有效隐喻。当学生网络相对于教师网络过度参数化时，上述模型尤为相关。在这种操作条件下，很容易推测学生处理给定任务的能力最终可能储存在整个网络的一个子部分中。根据适当的指标，这个子部分应该在一定程度上类似于冻结的教师结构，并且在学生候选网络的不同架构下近似不变。然而，由于所研究问题的固有非凸性程度，最新的传统学习技术无法识别这样一个不变子网络的存在。在这项工作中，我们采取了一个根本不同的优化方案，该方案建立在谱表示的基础上。

    In theoretical ML, the teacher-student paradigm is often employed as an effective metaphor for real-life tuition. The above scheme proves particularly relevant when the student network is overparameterized as compared to the teacher network. Under these operating conditions, it is tempting to speculate that the student ability to handle the given task could be eventually stored in a sub-portion of the whole network. This latter should be to some extent reminiscent of the frozen teacher structure, according to suitable metrics, while being approximately invariant across different architectures of the student candidate network. Unfortunately, state-of-the-art conventional learning techniques could not help in identifying the existence of such an invariant subnetwork, due to the inherent degree of non-convexity that characterizes the examined problem. In this work, we take a leap forward by proposing a radically different optimization scheme which builds on a spectral representation of th
    
[^100]: 快速模型去偏置与机器取消学习

    Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])

    [http://arxiv.org/abs/2310.12560](http://arxiv.org/abs/2310.12560)

    这篇论文提出了一种快速模型去偏置的框架（FMD），可以有效识别、评估和消除深度神经网络中的偏见，解决了现有方法在成本和解释性方面的不足。

    

    最近的研究发现，深度神经网络在许多现实场景中可能表现出偏差的行为。例如，在一个大规模的人脸识别数据集CelebA上训练的深度网络倾向于预测女性的金色头发和男性的黑色头发。这些偏差不仅危害了模型的稳健性，而且会持续和放大社会偏见，这对于医疗、招聘等自动决策过程尤其令人担忧，因为它们可能加剧不同群体之间的不公平经济和社会不平等。现有的去偏置方法在偏见标记或模型重新训练方面成本高昂，同时也在阐明模型内部偏见的起源方面存在不足。为此，我们提出了一个快速模型去偏置框架(FMD)，它提供了一种有效的方法来识别、评估和消除训练模型中固有的偏见。FMD通过显式的反事实机制来识别偏置属性。

    Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual 
    
[^101]: 高质量目标检测的Rank-DETR方法

    Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])

    [http://arxiv.org/abs/2310.08854](http://arxiv.org/abs/2310.08854)

    Rank-DETR是一种高质量目标检测方法，通过引入排名导向的设计，包括架构设计和损失函数设计，实现了性能卓越的基于DETR的目标检测器。

    

    现代检测变换器（DETR）使用一组对象查询来预测边界框列表，通过将其分类置信度得分进行排序，并选择排名靠前的预测结果作为给定输入图像的最终检测结果。性能卓越的目标检测器需要对边界框预测进行准确的排序。对于基于DETR的检测器，排名靠前的边界框由于分类得分与定位准确性之间的不对齐而导致定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向排名的设计，共同称为Rank-DETR，引入了一个简单且性能卓越的基于DETR的目标检测器。我们的主要贡献包括：（i）一个面向排名的架构设计，可以促进正面预测并抑制负面预测，以确保更低的假阳性率，以及（ii）一个面向排名的损失函数和匹配成本设计。

    Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
    
[^102]: Buckets for Buckets（B4B）：针对盗取编码器的主动防御

    Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders. (arXiv:2310.08571v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.08571](http://arxiv.org/abs/2310.08571)

    Bucks for Buckets (B4B) is the first active defense against stealing encoders, which prevents model stealing attacks by adaptively adjusting the utility of the returned representations based on the coverage of the embedding space by the user.

    

    机器学习即服务（MLaaS）API提供了现成且高效的编码器，用于生成给定输入的向量表示。由于这些编码器的训练成本很高，它们成为模型盗取攻击的有利目标，攻击者利用对API的查询访问，在原始训练成本的一小部分情况下本地复制编码器。我们提出了Bucks for Buckets (B4B)，这是第一个在攻击发生时能防止盗取并且不会降低合法用户的表示质量的主动防御方法。我们的防御方法依赖于以下观察结果：尝试盗取编码器功能的攻击者返回的表示涵盖了嵌入空间的一个显着较大的部分，相比之下，利用编码器解决特定下游任务的合法用户的表示涵盖范围较小。B4B利用这一点根据用户对嵌入空间的覆盖情况自适应调整返回表示的效用。

    Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding sp
    
[^103]: Observatory: 刻画关系表嵌入的研究

    Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])

    [http://arxiv.org/abs/2310.07736](http://arxiv.org/abs/2310.07736)

    Observatory提出了一个正式框架来分析关系表的嵌入表示，以帮助研究人员和实践者更好地理解和选择适合特定任务的模型。

    

    最近，语言模型和专门的表嵌入模型在许多表格数据任务上展示出了强大的性能。研究人员和实践者都渴望在许多新的应用场景中利用这些模型；但是对于这些模型的优势和缺点以及它们生成的表格表示的理解有限，导致在寻找适合特定任务的模型的过程中依赖于试错。迫切需要全面了解这些模型，以减少下游使用中的低效率和失败。为了解决这个问题，我们提出了一个名为Observatory的正式框架，以系统地分析关系表的嵌入表示。在关系数据模型的不变性和关于数据分布的统计考虑的基础上，我们定义了八个原始属性，以及相应的度量来定量地刻画这些属性的表格嵌入。

    Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.  To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properti
    
[^104]: 用MODIS多光谱时间序列和辅助数据进行盲目光谱分离的深度学习研究

    Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data. (arXiv:2310.07223v1 [cs.CV])

    [http://arxiv.org/abs/2310.07223](http://arxiv.org/abs/2310.07223)

    这项研究利用MODIS多光谱时间序列数据和深度学习模型，首次实现了对LULC类别的盲目光谱分离。通过添加地理加地形和气候辅助信息，进一步提高了模型的性能。

    

    遥感数据中LULC类别通常存在混合情况，光谱分离是一种从混合像素中提取信息到其组成LULC类型和相应丰度分数的技术。传统上，解决这一任务要么依赖于需要先验知识的经典方法，要么依赖于避免显式成分计算的机器学习方法，也就是盲目光谱分离（BSU）。大多数基于深度学习（DL）的BSU研究侧重于单个时间步的高光谱数据，然而与多光谱数据相比，其获取成本仍然相当高昂。据我们所知，我们在这里提供了第一项关于使用多光谱时间序列数据和DL模型进行LULC类别的BSU研究。我们进一步通过添加地理加地形（geo-topographic）和气候辅助信息来提升基于长短时记忆（LSTM）的模型的性能。我们的实验结果表明，将光谱时间输入数据与地理时态数据相结合可以提高模型性能。

    Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data togeth
    
[^105]: 可证收敛的数据驱动凸非凸正则化

    Provably Convergent Data-Driven Convex-Nonconvex Regularization. (arXiv:2310.05812v1 [cs.LG])

    [http://arxiv.org/abs/2310.05812](http://arxiv.org/abs/2310.05812)

    本研究展示了在凸非凸框架中，通过从数据中学习正则化器，可以实现收敛正则化；引入了一种新颖的弱凸输入神经网络构建，解决了之前对抗性方法的数值问题。

    

    通过使用深度学习从数据中学习正则化器是解决逆问题的新兴范式。这导致了高质量的结果，但往往无法提供可证明的保证。在这项工作中，我们展示了在凸非凸（CNC）框架中出现了良定义性和收敛性正则化的原因。我们引入了一种新颖的弱凸输入神经网络（IWCNN）构建，将学习对抗性正则化方法适应到CNC框架中。从实验证明，我们的方法克服了之前对抗性方法的数值问题。

    An emerging new paradigm for solving inverse problems is via the use of deep learning to learn a regularizer from data. This leads to high-quality results, but often at the cost of provable guarantees. In this work, we show how well-posedness and convergent regularization arises within the convex-nonconvex (CNC) framework for inverse problems. We introduce a novel input weakly convex neural network (IWCNN) construction to adapt the method of learned adversarial regularization to the CNC framework. Empirically we show that our method overcomes numerical issues of previous adversarial methods.
    
[^106]: GRANDE: 基于梯度的决策树集成模型

    GRANDE: Gradient-Based Decision Tree Ensembles. (arXiv:2309.17130v1 [cs.LG])

    [http://arxiv.org/abs/2309.17130](http://arxiv.org/abs/2309.17130)

    这篇论文提出了一种名为GRANDE的基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成，并结合了轴对齐分割和梯度优化的灵活性，引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。

    

    尽管深度学习在文本和图像数据方面取得了成功，但基于树的集成模型仍然是处理异构表格数据的机器学习的最先进方法。然而，由于其高灵活性，对于表格数据来说，存在对特定于表格的梯度方法的显著需求。在本文中，我们提出了一种名为GRANDE的新方法，即基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成。GRANDE基于决策树集成的稠密表示，可以使用直通操作符和反向传播一起优化所有模型参数。我们的方法结合了轴对齐分割（这是表格数据的一个有用的归纳偏置）和梯度优化的灵活性。此外，我们引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。我们在广泛的实验数据集上评估了GRANDE的性能，并与其他方法进行了比较。

    Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We con
    
[^107]: MEDL-U：基于证据深度学习的不确定性感知的3D自动标注

    MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning. (arXiv:2309.09599v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09599](http://arxiv.org/abs/2309.09599)

    本文提出了一种基于证据深度学习的方法，用于解决3D对象检测中伪标签的模糊性问题，并生成准确的伪标签和量化伪标签的不确定性。

    

    深度学习在基于3D对象检测方面取得了进展，这需要大规模数据集的支持。然而，这一要求引入了手动标注的挑战，这通常是繁重且耗时的。为了解决这个问题，文献中出现了几种弱监督的3D对象检测框架，可以自动生成未标记数据的伪标签。然而，这些生成的伪标签包含噪声，不如人工标注的准确。在本文中，我们提出了第一种解决伪标签中固有模糊性的方法，引入了一种基于证据深度学习（EDL）的不确定性估计框架。具体而言，我们提出了MEDL-U，它是基于MTrans的EDL框架，不仅生成伪标签，还量化了相关的不确定性。然而，将EDL应用于3D对象检测面临三个主要挑战：(1)相对较低的伪标签准确性。

    Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three primary challenges: (1) relatively lower pseudolab
    
[^108]: 数据驱动天气预报模型的局限性研究

    On the limitations of data-driven weather forecasting models. (arXiv:2309.08473v1 [stat.ML])

    [http://arxiv.org/abs/2309.08473](http://arxiv.org/abs/2309.08473)

    数据驱动的机器学习天气预报模型不具备传统基于物理的模型的准确性和物理一致性，它们在预测技能上的优势很大程度上可以归因于这些特殊性。

    

    机器学习在天气和气候预测领域产生了深远影响。最近的发展是数据驱动的机器学习预测模型的出现，它们通常声称比传统的基于物理的模型具有更高的性能。在这项工作中，我们研究了当前一代机器学习模型之一Pangu-Weather的预测方面的一些问题，重点关注预测的准确性和物理一致性以及这些特征与感知预测性能之间的关系。主要结论是Pangu-Weather的预测，以及类似的机器学习模型，不具备基于物理的模型的准确性和物理一致性，而它们在传统的确定性预测技能指标上的优势很大程度上可以归因于这些特殊性。与其他当前的后处理技术类似。

    As in many other areas of engineering and applied science, Machine Learning (ML) is having a profound impact in the domain of Weather and Climate Prediction. A very recent development in this area has been the emergence of fully data-driven ML prediction models which routinely claim superior performance to that of traditional physics-based models. In this work, we examine some aspects of the forecasts produced by an exemplar of the current generation of ML models, Pangu-Weather, with a focus on the fidelity and physical consistency of those forecasts and how these characteristics relate to perceived forecast performance. The main conclusion is that Pangu-Weather forecasts, and by extension those of similar ML models, do not have the fidelity and physical consistency of physics-based models and their advantage in accuracy on traditional deterministic metrics of forecast skill can be attributed, to a large extent, to these peculiarities. Similarly to other current post-processing technol
    
[^109]: 使用遮蔽条件扩散的模态循环进行MRI无监督异常分割

    Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v1 [eess.IV])

    [http://arxiv.org/abs/2308.16150](http://arxiv.org/abs/2308.16150)

    本文介绍了一种名为遮蔽模态循环与条件扩散的方法，该方法能够对多模态MRI中的异常进行分割。方法基于循环模态转换和条件扩散的思想，能够检测到训练中未遇到的异常模式。

    

    无监督异常分割旨在检测与训练过程中处理的任何模式不同的模式，通常称为异常或超出分布的模式，而不提供任何关联的手动分割。由于部署过程中的异常可能导致模型失效，检测异常可以增强模型的可靠性，这在医学成像等高风险领域非常有价值。本文引入了基于遮蔽模态循环与条件扩散（MMCCD）的方法，该方法能够在多模态MRI中分割各种模式的异常。该方法基于两个基本思想。首先，我们提出使用循环模态转换作为启用异常检测的机制。图像转换模型学习组织特异的模态映射，这是组织生理学的特征。因此，这些学习到的映射无法将在训练过程中从未遇到的组织或图像模式进行转换，从而产生错误，使得异常能够被检测到。

    Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables
    
[^110]: 从SMOTE到Mixup用于深度不平衡分类

    From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])

    [http://arxiv.org/abs/2308.15457](http://arxiv.org/abs/2308.15457)

    本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。

    

    鉴于不平衡的数据，使用深度学习训练好的分类器因为少数类的泛化能力差而困难重重。传统上，用于数据增强的知名少数类合成过采样技术（SMOTE），作为一种面向不平衡学习的数据挖掘方法，被用来改善这种泛化。然而，SMOTE在深度学习中是否也有益处仍不清楚。在这项工作中，我们研究了为什么原始的SMOTE对深度学习来说是不足的，并使用软标签增强了SMOTE。将得到的软SMOTE与Mixup，一种现代数据增强技术，连接在一起，形成了一个统一的框架，将传统和现代的数据增强技术纳入同一个范畴。在这个框架中进行系统研究表明，Mixup通过隐式地实现多数类和少数类之间的不平衡间隙来改善泛化能力。然后，我们提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。大量的实验表明，我们的方法在多个数据集上相对于最先进的算法都取得了最好的性能。

    Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
    
[^111]: ChatGPT用于GTFS: 从文字到信息

    ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])

    [http://arxiv.org/abs/2308.02618](http://arxiv.org/abs/2308.02618)

    本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。

    

    广泛使用的公交通行数据发布标准General Transit Feed Specification（GTFS）是表格数据，信息分散在不同的文件中，需要专门的工具或包来检索信息。与此同时，使用大型语言模型进行文本和信息检索的趋势也在增长。本研究的想法是看看当前广泛采用的LLMs（ChatGPT）是否能够使用自然语言指令从GTFS中检索信息。我们首先测试ChatGPT（GPT-3.5）是否理解GTFS规范。GPT-3.5在我们的多项选择问题（MCQ）中正确回答了77%。接下来，我们利用过滤的GTFS数据集对LLM进行信息提取任务。对于信息检索，我们比较了零-shot和程序合成。程序合成的效果更好，在简单问题上达到了约90%的准确率，在复杂问题上达到了约40%的准确率。

    The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
    
[^112]: 多重保护属性的公平性改善的实证研究

    An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])

    [http://arxiv.org/abs/2308.01923](http://arxiv.org/abs/2308.01923)

    本文通过广泛研究，发现对于单个保护属性的公平性改善会大大降低对未考虑保护属性的公平性，但在多属性模式下可以保持准确性。

    

    现有研究主要关注单个保护属性的机器学习（ML）软件的公平性改善，但考虑到许多用户具有多个保护属性，这是不现实的。本文对多个保护属性的公平性改善进行了广泛研究，涵盖了11种最先进的公平性改善方法。我们分析了在考虑多个保护属性时，这些方法在不同数据集、评估指标和ML模型上的有效性。结果显示，改善单个保护属性的公平性大大降低了未考虑的保护属性的公平性。在88.3％的情况下观察到这种降低（平均为57.5％）。更令人惊讶的是，在考虑单个和多个保护属性时，准确率损失方面几乎没有差异，这表明在多属性模式下可以保持准确性。然而，在处理多个保护属性时，精确度和召回率的影响较大。

    Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
    
[^113]: 海底滑翔器的通用异常检测验证方法——大规模部署数据集的应用

    General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset. (arXiv:2308.00180v1 [cs.RO])

    [http://arxiv.org/abs/2308.00180](http://arxiv.org/abs/2308.00180)

    本文介绍了一种用于评估海底滑翔器在不可预测海洋环境中正常操作的异常检测算法，并通过实际滑翔器部署的大规模数据集进行验证。算法能够实时提供异常警报，使驾驶员能够控制滑翔器并避免进一步损害。

    

    本文利用一种异常检测算法评估在不可预测的海洋环境中海底滑翔器的正常操作。一旦检测到任何异常，可以向滑翔器驾驶员提供实时警报，使其能够接管滑翔器并防止进一步的损害。该检测算法应用于由Skidaway海洋研究所（SkIO）和南佛罗里达大学（USF）领导的实际滑翔器部署中收集的大量数据集。就泛化性而言，实验评估包括离线和在线检测模式。离线检测利用完整的回收后数据集，具有高分辨率的信息，对异常进行详细分析并与驾驶员日志进行比较。在线检测专注于从滑翔器传输的实时数据子集。虽然实时数据可能不包含与回收后数据一样丰富的信息，但在线检测是实时的。

    This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is 
    
[^114]: 通用目的人工智能系统（GPAIS）：性质、定义、分类、开放挑战和影响

    General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])

    [http://arxiv.org/abs/2307.14283](http://arxiv.org/abs/2307.14283)

    这里是中文总结出的一句话要点：本论文讨论了通用目的人工智能系统（GPAIS）的性质、定义、分类和开放挑战，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。

    

    大部分人工智能（AI）应用都设计用于特定和有限的任务。然而，有许多场景需要更通用的AI，能够解决各种任务而不需要专门为它们设计。通用目的人工智能系统（GPAIS）这个术语被定义为指代这些AI系统。尽管迄今为止，实现人工通用智能的可能性，即足够强大以模拟人类并改进各种智力任务，一直是一个愿望、虚构的概念，并被认为对我们社会构成风险。虽然我们离实现这一目标可能还很遥远，但GPAIS是现实存在并位居人工智能研究的前沿。本文讨论了现有GPAIS定义，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。我们区分了封闭世界和开放世界的GPAIS，描述其自主程度和...

    Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
    
[^115]: 通过原位无模型优化实现高性能真实光学计算

    High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)

    [http://arxiv.org/abs/2307.11957](http://arxiv.org/abs/2307.11957)

    本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。

    

    光学计算系统可以提供高速和低能耗的数据处理，但在计算密集的训练和从模拟到现实的转换中存在不足。我们提出了一种基于得分梯度估计算法的轻量级原位优化光学计算系统的无模型解决方案。该方法将系统视为黑盒子，直接将损失反向传播到光学权重的概率分布，从而避免了对计算密集和有偏见的系统模拟的需求。通过在单层衍射光学计算系统上进行实验证明在MNIST和FMNIST数据集上具有优越的分类准确度。此外，我们展示了其在无图片和高速细胞分析方面的潜力。我们提出的方法的固有简单性，结合其对计算资源的低需求，加速了光学计算从实验室演示到真实世界应用的过渡。

    Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
    
[^116]: 分数降噪用于3D分子预训练

    Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.10683](http://arxiv.org/abs/2307.10683)

    本论文提出了一种分数降噪算法，用于3D分子预训练。通过混合噪声策略解决了样本覆盖率低和各向同性力场的挑战，通过解耦两种类型的噪声克服了传统降噪方法无法学习力场的问题。

    

    坐标降噪是一种有前途的3D分子预训练方法，在各种下游药物发现任务中取得了显著的性能。从理论上讲，其目标等同于学习力场，并且对下游任务有帮助。然而，坐标降噪学习有效力场面临两个挑战，即样本覆盖率低和各向同性力场。根本原因在于现有降噪方法所假设的分子分布不能捕捉分子的各向异性特征。为了解决这些挑战，我们提出了一种新的混合噪声策略，包括二面角和坐标的噪声。然而，以传统方式降噪这种混合噪声不再等同于学习力场。通过理论推导，我们发现问题是由于输入构象对协方差的依赖性所导致的。因此，我们提出将这两种类型的噪声解耦

    Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of nois
    
[^117]: 景观替代品：在部分信息下学习数学优化的决策损失

    Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])

    [http://arxiv.org/abs/2307.08964](http://arxiv.org/abs/2307.08964)

    本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。

    

    最近的学习集成优化工作在优化问题只有部分可观测或通用优化器在无专家调优的情况下表现不佳的情况下显示出了希望。通过学习一个优化器$ \mathbf{g} $来解决这些具有挑战性的问题，通过利用过去的经验，可以显著加速优化过程。优化器可以通过已知最优解的监督或通过优化复合函数$ f\circ \mathbf{g} $的隐式方式进行训练。隐式方法可能不需要最优解作为标签，并且能够处理问题的不确定性；然而，由于在训练和测试过程中频繁调用优化器$ \mathbf{g} $，因此训练和部署缓慢。对于组合求解器，由于$ \mathbf{g} $的稀疏梯度，训练进一步受到挑战。为了解决这些问题，我们提出使用平滑可学习的景观替代品$ M $作为一种替代方法。

    Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
    
[^118]: 一种风险厌恶策略梯度的方差替代：基尼离差

    An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])

    [http://arxiv.org/abs/2307.08873](http://arxiv.org/abs/2307.08873)

    本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。

    

    在风险厌恶的强化学习中，限制策略回报的方差是一种常见选择，因为它具有明确的数学定义和易于解释。传统方法直接限制总回报方差，而最近的方法通过限制每步奖励方差作为代理。本文彻底研究了这些基于方差的方法的局限性，如数字尺度的敏感性和阻碍策略学习，并提出使用替代风险衡量标准——基尼离差。我们研究了这种新风险衡量标准的各种属性，并导出了一种用于最小化基尼离差的策略梯度算法。在风险厌恶可以明确定义的领域进行实证评估时，我们的算法可以缓解基于方差的风险衡量标准的局限性，并在其他策略无法学到合理策略时实现高回报和低风险，以方差和基尼离差度量。

    Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
    
[^119]: 多任务学习以提高相干光系统中神经网络均衡器的泛化能力

    Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])

    [http://arxiv.org/abs/2307.05374](http://arxiv.org/abs/2307.05374)

    首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性，并且通过一个"单一"的基于神经网络的均衡器，在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。

    

    首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性。与常规数字时钟恢复方法相比，一个"单一"的基于神经网络的均衡器在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。

    For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
    
[^120]: 何时使用Transformer在强化学习中发光？从记忆和信用分配中解耦

    When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])

    [http://arxiv.org/abs/2307.03864](http://arxiv.org/abs/2307.03864)

    Transformer在强化学习中的作用是增强记忆能力而不是改进信用分配。

    

    强化学习算法面临两个不同的挑战：学习有效的过去和当前观测的表示，并确定行动如何影响未来的收益。这两个挑战都涉及到建模长期依赖关系。Transformer架构在解决涉及长期依赖关系的问题，包括在强化学习领域方面非常成功。然而，Transformer基于的强化学习方法表现强劲的原因尚不清楚：是因为它们学习了有效的记忆，还是因为它们执行了有效的信用分配？在引入记忆长度和信用分配长度的形式定义之后，我们设计了简单的可配置任务来测量这些不同的量。我们的实证结果表明，Transformer可以增强强化学习算法的记忆能力，扩展到需要记住1500步前观察的任务。然而，Transformer无法改进长期的信用分配。总之，我们的研究揭示了Transformer在强化学习中记忆和信用分配方面的不同作用。

    Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
    
[^121]: LLQL: 逻辑似然 Q-Learning 用于增强学习

    LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02345](http://arxiv.org/abs/2307.02345)

    本研究通过研究在线和离线增强学习中 Bellman 近似误差的分布发现，Bellman 误差符合逻辑分布。基于这一发现，本研究提出了一种使用 Logistic 最大似然函数作为替代方法的方案，并通过实验证明了其有效性。

    

    现代增强学习（RL）可以分为在线和离线两种变体。作为在线和离线 RL 的关键方面，当前对 Bellman 方程的研究主要集中在优化技术和性能增强上，而不是探索 Bellman 误差的固有结构特性，如其分布特征。本研究通过对 Bellman 方程进行迭代探索，研究了在线 RL 和离线 RL 中 Bellman 近似误差的分布情况。我们观察到无论是在线 RL 还是离线 RL，Bellman 误差都符合逻辑分布。基于这一发现，本研究采用 Logistic 最大似然函数（LLoss）作为常用的 MSE Loss 的替代方法，假设 Bellman 误差服从正态分布。通过广泛的数值实验验证了我们的假设，在不同的在线和离线环境中得到了验证。

    Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
    
[^122]: 比例响应：用于简单和累积遗憾最小化的情境赌博算法

    Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v1 [cs.LG])

    [http://arxiv.org/abs/2307.02108](http://arxiv.org/abs/2307.02108)

    这篇论文提出了一种适用于情境赌博设置的新型计算效率高的赌博算法，具有简单和累积遗憾最小化的优势，并可自适应模型错误规范和连续臂设置。该算法利用"一致臂集"（CAS）来提供在每个情境下囊括情境特定的最佳臂的一组臂，跨越情境分布。这篇论文对简单和累积遗憾保证的研究提供了正面结果，同时也揭示了无法实现实例依赖性的简单遗憾保证的消极结果。

    

    在医疗保健和电子商务等领域，简单遗憾最小化是学习最佳治疗分配策略的关键问题。然而，情境赌博设置中的简单遗憾最小化问题仍未充分研究。我们提出了一种新的计算效率高的赌博算法族，针对随机情境赌博设置，在累积遗憾最小化（具有近乎最优的极小极大保证）和简单遗憾最小化（具有SOTA保证）方面具有灵活性。此外，我们的算法对模型错误规范进行自适应，并扩展到连续臂设置。这些优势来自于构建和依赖于“一致臂集”（CAS），CAS在每个情境下提供一组臂，这些臂以一定的概率囊括了情境特定的最佳臂，跨越了情境分布。我们关于简单和累积遗憾保证的积极结果与一个消极结果形成对比，后者表明一个算法无法实现实例依赖性的简单遗憾保证。

    Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-de
    
[^123]: 解构数据重建：多类别、权重衰减和通用损失

    Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])

    [http://arxiv.org/abs/2307.01827](http://arxiv.org/abs/2307.01827)

    这项研究探讨了神经网络内部对训练数据的记忆过程，并在多个方向上扩展了已有的研究。研究发现，使用权重衰减可以增加重建能力，同时还分析了神经元数量对网络易受重建方案影响的影响。

    

    训练数据的记忆是一个活跃的研究领域，然而我们对神经网络内部运作的理解还处于初级阶段。最近，Haim等人提出了一种方案，可以从多层感知器二元分类器中重建训练样本，有效地证明了这样的网络参数中编码了大部分训练样本。在本研究中，我们在多个方向上扩展了他们的发现，包括从多类别和卷积神经网络中进行重建。我们推导出了一种更通用的重建方案，可适用于更广泛的损失函数，如回归损失。此外，我们研究了影响网络易受此类重建方案影响的各种因素。有趣的是，我们观察到在训练过程中使用权重衰减会增加重建能力，无论是在数量还是质量方面。此外，我们还检验了神经元数量相对于训练数量的影响。

    Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training
    
[^124]: 运输、变分推断和扩散：应用于回火流和薛定谔桥的论文研究

    Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])

    [http://arxiv.org/abs/2307.01050](http://arxiv.org/abs/2307.01050)

    本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。

    

    本文探讨了最优运输与变分推断之间的联系，重点研究了正向和反向随机微分方程以及Girsanov变换。我们提出了一个基于路径空间散度的采样和生成建模的原则性和系统性框架。我们的工作最终发展出一个新颖的基于得分的回火流技术（与统计物理中的Jarzynski和Crooks恒等式有关）和一个正则化的迭代比例拟合（IPF）型目标，不同于标准IPF的顺序性。通过一系列的生成建模示例和基于双井的稀有事件任务，我们展示了所提方法的潜力。

    This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
    
[^125]: 宽松Pareto集识别的自适应算法

    Adaptive Algorithms for Relaxed Pareto Set Identification. (arXiv:2307.00424v1 [stat.ML])

    [http://arxiv.org/abs/2307.00424](http://arxiv.org/abs/2307.00424)

    本研究提出了一种自适应算法，用于宽松Pareto集的识别，通过放松策略来减少样本复杂度，并展示了在实际场景中的良好表现。

    

    本文重新审视了在多目标多臂赌博机模型中固定置信度下的Pareto最优集合的识别问题。由于准确识别Pareto集合的样本复杂度可能非常大，因此研究了允许输出一些额外近似最优臂的放松策略。在这项工作中，我们还解决了其他允许识别Pareto集合的相关子集的放松策略。值得注意的是，我们提出了一种称为自适应Pareto探索的单一抽样策略，可以与不同的停止规则结合使用，以考虑Pareto集合识别问题的不同放松策略。我们分析了这些不同组合的样本复杂度，并特别量化了在寻找识别最多$k$个Pareto最优臂时样本复杂度的减少。我们展示了自适应Pareto探索在一个真实场景中的良好实际性能，其中我们自适应地探索了几种疫苗接种策略。

    In this paper we revisit the fixed-confidence identification of the Pareto optimal set in a multi-objective multi-armed bandit model. As the sample complexity to identify the exact Pareto set can be very large, a relaxation allowing to output some additional near-optimal arms has been studied. In this work we also tackle alternative relaxations that allow instead to identify a relevant subset of the Pareto set. Notably, we propose a single sampling strategy, called Adaptive Pareto Exploration, that can be used in conjunction with different stopping rules to take into account different relaxations of the Pareto Set Identification problem. We analyze the sample complexity of these different combinations, quantifying in particular the reduction in sample complexity that occurs when one seeks to identify at most $k$ Pareto optimal arms. We showcase the good practical performance of Adaptive Pareto Exploration on a real-world scenario, in which we adaptively explore several vaccination stra
    
[^126]: 在具有未知和随机奖励的臂上分配可分资源

    Allocating Divisible Resources on Arms with Unknown and Random Rewards. (arXiv:2306.16578v1 [cs.LG])

    [http://arxiv.org/abs/2306.16578](http://arxiv.org/abs/2306.16578)

    本论文研究了在每个周期将一单位可分资源分配到多个臂上的问题，臂上的奖励是未知和随机的，而且与分配的资源成比例，而方差与分配资源的阶数成比例。我们设计了两种算法，实现了不同阶数下的最优有界和无界遗憾，结果表明在阶数为1/2时存在相变现象。

    

    我们考虑一个决策者在每个周期将一个可再生和可分资源分配到多个臂上。这些臂具有未知和随机的奖励，其均值与分配的资源成比例，方差与分配资源的阶数$b$成比例。特别地，如果决策者在一个周期将资源$A_i$分配给臂$i$，那么奖励$Y_i$是$Y_i(A_i)=A_i\mu_i+A_i^b\xi_i$，其中$\mu_i$是未知的均值，噪声$\xi_i$是独立且子高斯的。当阶数$b$从0到1变化时，该框架平滑地连接了标准的随机多臂赌博机和带有完全反馈的在线学习。我们设计了两种算法，它们实现了$b\in[0,1]$时的最优有界差和无界差的遗憾界，并展示了在$b=1/2$处的相变。理论结果依赖于我们开发的一种新型浓度不等式，它限制了子高斯随机变量的线性组合。

    We consider a decision maker allocating one unit of renewable and divisible resource in each period on a number of arms. The arms have unknown and random rewards whose means are proportional to the allocated resource and whose variances are proportional to an order $b$ of the allocated resource. In particular, if the decision maker allocates resource $A_i$ to arm $i$ in a period, then the reward $Y_i$ is$Y_i(A_i)=A_i \mu_i+A_i^b \xi_{i}$, where $\mu_i$ is the unknown mean and the noise $\xi_{i}$ is independent and sub-Gaussian. When the order $b$ ranges from 0 to 1, the framework smoothly bridges the standard stochastic multi-armed bandit and online learning with full feedback. We design two algorithms that attain the optimal gap-dependent and gap-independent regret bounds for $b\in [0,1]$, and demonstrate a phase transition at $b=1/2$. The theoretical results hinge on a novel concentration inequality we have developed that bounds a linear combination of sub-Gaussian random variables w
    
[^127]: 使用监视器引导全局上下文指导代码语言模型

    Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.10763](http://arxiv.org/abs/2306.10763)

    本文提出了一种使用监视器引导全局上下文的方法来指导代码语言模型，在处理类型、功能或API等全局上下文时，能够提高代码语言模型的性能和准确性。

    

    代码语言模型（LMs）在周围代码提供足够上下文时效果很好。但当需要在存储库或链接库中使用在训练过程中未见过的类型、功能或API时，这种情况就不再成立。LMs在对这种全局上下文的意识有限时会出现错误预测的情况。集成开发环境（IDEs）通过静态分析帮助开发人员了解存储库上下文。我们将开发人员享受到的这种帮助扩展到了LMs。我们提出了监视器引导解码（MGD）的方法，其中监视器使用静态分析来引导解码过程。我们构建了一个用于Java方法补全的存储库级数据集PragmaticCode，并在其上评估了MGD。在不同参数规模的模型上，通过监视类型一致的对象解引用，MGD能够持续提高编译率并与真实结果达成一致。此外，具有更少参数的LMs，在与MGD相结合时能够超越更大的LMs的性能。

    Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.  Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LM
    
[^128]: 对数贝叶斯遗憾边界

    Logarithmic Bayes Regret Bounds. (arXiv:2306.09136v1 [cs.LG])

    [http://arxiv.org/abs/2306.09136](http://arxiv.org/abs/2306.09136)

    该论文提出了对于贝叶斯赌博机的首个有限时间对数遗憾边界，并用于高斯和线性赌博机，从而阐明了贝叶斯设置中先验价值以及对$\tilde{O}(\sqrt{n})$界限的改善。

    

    我们为贝叶斯赌博机导出了首个有限时间对数遗憾边界。对于高斯赌博机，我们获得了一个$O(c_h \log^2 n)$的边界，其中$c_h$是与先验相关的常量。这与Lai（1987）的渐近下限相匹配。我们的证明与先前的工作有所不同，且简单且普遍。为了显示一般性，我们将我们的技术应用于线性赌博机。我们的界限阐明了贝叶斯设置中先验的价值，既可以作为目标，也可以作为传递给学习者的附加信息。它们显着改善了现有的$\tilde{O}(\sqrt{n})$界限，尽管存在下限，但已成为文献中的标准。

    We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\tilde{O}(\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.
    
[^129]: 长序列 Hopfield内存

    Long Sequence Hopfield Memory. (arXiv:2306.04532v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2306.04532](http://arxiv.org/abs/2306.04532)

    这篇论文提出了一种增强Hopfield-like神经网络序列记忆模型的序列容量的方法，通过引入非线性相互作用项，显著优于传统Hopfield网络，同时也引入了一个新的回忆规则以回忆连续的序列。

    

    序列记忆是自然和人工智能的重要属性，它使代理能够编码、存储和检索复杂的刺激和行为序列。已经提出了计算模型，其中用时间非对称的Hebbian规则训练递归的Hopfield样神经网络。然而，这些网络由于记忆之间的干扰而具有有限的序列容量（存储序列的最大长度）。受密集关联记忆的最新工作的启发，我们通过引入非线性相互作用项来扩展这些模型的序列容量，增强模式之间的分离性。我们推导出序列容量与网络大小的新的标度定律，显著优于基于传统Hopfield网络的现有标度定律，并通过数值模拟验证了这些理论结果。此外，我们引入了一个广义伪逆规则来回忆高度连续的序列。

    Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly 
    
[^130]: 学习具有未知干预的非参数潜在因果图。

    Learning nonparametric latent causal graphs with unknown interventions. (arXiv:2306.02899v1 [stat.ML])

    [http://arxiv.org/abs/2306.02899](http://arxiv.org/abs/2306.02899)

    本文提出了一种学习具有未知干预的非参数潜在因果图的方法，通过建立条件确定非参数潜在因果图并从中重构。这种方法不需要参数假设，可用于识别测量模型中潜在结构。

    

    我们在未知干预的潜在空间建立条件，以确定非参数潜在因果图并从中重构。我们的主要重点是测量模型中潜在结构的识别，即因果图模型，在其中观察变量之间的依赖性与潜在表示之间的依赖性相比，并不做出参数假设，如线性或高斯性。此外，我们不假设隐藏变量的数量已知，并且我们表明每个隐藏变量最多只需要一个未知的干预。这扩展了最近关于从观测和干预中学习因果表示的工作。证明是建设性的，并引入了两个新的图形概念——想象子集和孤立边——它们本身可能是有用的。作为一个独立的感兴趣的问题，证明还涉及对边缘定向限制的新的特征化。

    We establish conditions under which latent causal graphs are nonparametrically identifiable and can be reconstructed from unknown interventions in the latent space. Our primary focus is the identification of the latent structure in a measurement model, i.e. causal graphical models where dependence between observed variables is insignificant compared to dependence between latent representations, without making parametric assumptions such as linearity or Gaussianity. Moreover, we do not assume the number of hidden variables is known, and we show that at most one unknown intervention per hidden variable is needed. This extends a recent line of work on learning causal representations from observations and interventions. The proofs are constructive and introduce two new graphical concepts -- imaginary subsets and isolated edges -- that may be useful in their own right. As a matter of independent interest, the proofs also involve a novel characterization of the limits of edge orientations wi
    
[^131]: 采用优势诱导策略对齐的Fine-Tuning语言模型

    Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02231](http://arxiv.org/abs/2306.02231)

    本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。

    

    人类反馈强化学习（RLHF）已经成为将大型语言模型（LLMs）与人类偏好对齐的可靠方法。在众多RLHF技术中，接近策略优化（PPO）是最常用的方法之一。然而，尽管PPO很流行，但它可能会遭受模式崩溃、不稳定和效率低下的问题。我们展示了一种新颖的算法--基于估计优势的平方误差损失函数的优势诱导策略对齐（APA），可以减轻这些问题。我们通过实验证明，当使用单独的奖励模型作为评估器时，APA在语言任务中始终比PPO表现出更好的性能。此外，与PPO相比，APA可以更稳定地控制模型与初始策略的偏差，确保模型提高性能而不会崩溃为确定性输出。除了经验结果之外，我们还提供了APA的理论分析。

    Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
    
[^132]: 广义平滑度下的凸和非凸优化

    Convex and Non-Convex Optimization under Generalized Smoothness. (arXiv:2306.01264v1 [math.OC])

    [http://arxiv.org/abs/2306.01264](http://arxiv.org/abs/2306.01264)

    本文发展了一种新的分析技术，并推广了广义平滑度条件，使凸和非凸优化问题获得更强的结果。在该条件下，获得了（随机）梯度下降和Nesterov加速梯度方法的经典收敛率。

    

    经典的凸和非凸优化方法的分析通常需要梯度的Lipshitz性质，这限制了分析范围仅限于二次函数的界限内。最近的工作放松了这个要求，转而使用一种非均匀平滑条件，其中Hessian范数受梯度范数的仿射函数限制，并通过梯度裁剪证明了非凸情况下的收敛性，假设存在有界噪声。在本文中，我们进一步推广了这种非均匀平滑条件，并开发了一种简单但功能强大的分析技术，可以沿轨迹方向限制梯度，从而获得更强的凸和非凸优化问题结果。特别地，在这个广义平滑条件下，我们得到了（随机）梯度下降和Nesterov加速梯度方法的经典收敛率，适用于凸和（或）非凸设定。新的分析方法不需要梯度裁剪，并允许有重尾噪声，这是一种非常实用的优化方法。

    Classical analysis of convex and non-convex optimization methods often requires the Lipshitzness of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with b
    
[^133]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^134]: 从大型矿石中提炼黄金: 基于关键样本选择的高效数据集蒸馏

    Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])

    [http://arxiv.org/abs/2305.18381](http://arxiv.org/abs/2305.18381)

    研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。

    

    数据效率学习近年来备受关注，特别是对于拥有大量多模型的现在，数据集蒸馏可以成为有效的解决方案。然而，数据集蒸馏过程本身仍然非常低效。在本文中，我们首先引用信息理论来建模蒸馏问题，观察到数据集蒸馏中存在严重的数据冗余，我们提出了一种方法来扩展现有的蒸馏算法，以便通过选择最有价值的样本来更好地利用这些训练样本。我们进一步对样本选择进行全面分析，并验证了其优化过程。这种新策略能够显著减少训练成本，扩大现有算法范围以对更庞大和多元化的数据集进行数据集蒸馏，例如，在某些情况下，只需要0.04％的训练数据就足以保持可比的蒸馏效果。此外，我们的策略能够持续提高性能，其贡献可能为蒸馏过程的动力学开辟新的分析方法。

    Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
    
[^135]: 可扩展的PDE模型代用的Transformer

    Scalable Transformer for PDE Surrogate Modeling. (arXiv:2305.17560v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17560](http://arxiv.org/abs/2305.17560)

    我们提出了一种基于轴向分解核积分的Factorized Transformer模型，用于PDE模型代用。该模型在具有大量网格点的问题上能够保持较好的准确性和计算效率。

    

    Transformer在各种应用中展现出最先进的性能，并且最近成为了PDE模型代理的一个有前景的工具。尽管引入了线性复杂度的Attention，但是将Transformer应用于具有大量网格点的问题可能会在数值上不稳定且计算代价高昂。在这项工作中，我们提出了Factorized Transformer（FactFormer），它基于一个轴向分解核积分。具体来说，我们引入了一个可学习的投影算子，将输入函数分解为具有一维域的多个子函数。然后，这些子函数被评估并用于计算具有轴向分解方案的基于实例的核。我们展示了该模型在$256\times 256$的网格上模拟2D Kolmogorov流和$64\times64\times64$网格上的3D烟雾浮力，并且具有很好的准确性和效率。所提出的分解方案可以作为计算的优化方式。

    Transformer has shown state-of-the-art performance on various applications and has recently emerged as a promising tool for surrogate modeling of partial differential equations (PDEs). Despite the introduction of linear-complexity attention, applying Transformer to problems with a large number of grid points can be numerically unstable and computationally expensive. In this work, we propose Factorized Transformer (FactFormer), which is based on an axial factorized kernel integral. Concretely, we introduce a learnable projection operator that decomposes the input function into multiple sub-functions with one-dimensional domain. These sub-functions are then evaluated and used to compute the instance-based kernel with an axial factorized scheme. We showcase that the proposed model is able to simulate 2D Kolmogorov flow on a $256\times 256$ grid and 3D smoke buoyancy on a $64\times64\times64$ grid with good accuracy and efficiency. The proposed factorized scheme can serve as a computationa
    
[^136]: 一种统一的方法用于最大化连续 DR-submodular 函数

    A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])

    [http://arxiv.org/abs/2305.16671](http://arxiv.org/abs/2305.16671)

    本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。

    

    本文提出了一种统一的方法，用于最大化连续的 DR-submodular 函数，涵盖了一系列设置和 Oracle 访问类型。我们的方法包括针对单调和非单调函数的 Frank-Wolfe 类型离线算法，具有不同的一般凸集限制。我们考虑了 Oracle 提供函数梯度或仅函数值的访问以及确定性或随机性访问的设置。我们在所有情况下确定了所需的 Oracle 访问数量。我们的方法为 16 个考虑的情况中的 9 个提供了新的/改进的结果，在两个情况下避免了计算上昂贵的投影，而所提出的框架在其余五个情况下与最先进的方法相匹配。值得注意的是，我们针对基于随机函数值的 Oracle 的方法，为随机 DR-submodular 函数提供了第一个带有探险反馈的后悔界限。

    This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
    
[^137]: 强化学习微调的视觉-代码Transformer用于UI-to-Code生成

    Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation. (arXiv:2305.14637v1 [cs.CV])

    [http://arxiv.org/abs/2305.14637](http://arxiv.org/abs/2305.14637)

    本文提出了一种新型的视觉-代码Transformer方法，通过actor-critic微调来改善基线，比较了Vision Transformer和Document Image Transformer这两种图像编码器，提出了一种端到端的流水线，可以直接从屏幕截图生成高质量的代码片段，创建了30,000个独特的代码和对应截图的合成数据集，并使用多种自动化指标来评估这种方法的性能，建立了一个强大的基准模型。

    

    从屏幕截图自动生成HTML/CSS代码是一个重要且具有广泛应用的挑战性问题。本文提出了一种新颖的视觉-代码Transformer方法，利用编码器-解码器结构，同时探索actor-critic微调作为改进基线的方法。为此，比较了两个图像编码器：Vision Transformer (ViT) 和 Document Image Transformer (DiT)。我们提出了一种端到端的流水线，可以直接从屏幕截图生成高质量的代码片段，简化了开发人员的网站创建过程。为训练和评估模型，我们创建了一个包含30,000个独特的代码和对应截图的合成数据集。我们使用MSE、BLEU、IoU和一种新颖的htmlBLEU得分等自动化指标评估我们方法的性能，我们的模型表现出了强大的性能。我们用DiT-GPT2模型建立了一个强大的基准。

    Automated HTML/CSS code generation from screenshots is an important yet challenging problem with broad applications in website development and design. In this paper, we present a novel vision-code transformer approach that leverages an Encoder-Decoder architecture as well as explore actor-critic fine-tuning as a method for improving upon the baseline. For this purpose, two image encoders are compared: Vision Transformer (ViT) and Document Image Transformer (DiT).  We propose an end-to-end pipeline that can generate high-quality code snippets directly from screenshots, streamlining the website creation process for developers. To train and evaluate our models, we created a synthetic dataset of 30,000 unique pairs of code and corresponding screenshots.  We evaluate the performance of our approach using a combination of automated metrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models demonstrated strong performance. We establish a strong baseline with the DiT-GPT2 mod
    
[^138]: Flover：一种用于高效自回归模型并行推断的时间融合框架

    Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])

    [http://arxiv.org/abs/2305.13484](http://arxiv.org/abs/2305.13484)

    Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。

    

    在深度学习领域快速发展的背景下，模型推断性能成为一个关键因素，尤其是在模型变得更加复杂并被部署在多个应用场景中的情况下。自回归模型由于在众多生成任务中表现优异，因此备受关注。这些模型设计上采用了一种时间依赖结构，其中当前token的概率分布受到前面token的影响。然而，这种本质上的序列特性遵循马尔可夫链假设，缺乏时间并行性，因此存在独特的挑战。特别是在工业背景下，推断请求遵循泊松时间分布，需要不同的响应长度，这种并行性的缺失更加明显。现有的解决方案如动态批处理和并发模型实例，然而，这些粗粒度的方法存在严重的开销和缺乏灵活性，无法实现最优化。

    In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
    
[^139]: 基于基础模型的通用域适应

    Universal Domain Adaptation from Foundation Models. (arXiv:2305.11092v1 [cs.LG])

    [http://arxiv.org/abs/2305.11092](http://arxiv.org/abs/2305.11092)

    本论文对基于基础模型的通用域适应进行了研究，发现当前的UniDA方法无法超越基准表现，提出了一个简单的目标方法。

    

    基础模型（例如CLIP或DINOv2）已经展现了在广泛视觉任务中卓越的学习和转移能力，通过在大型数据语料库上训练并适应特定的下游任务。然而，有趣的是，基础模型尚未完全探索通用域适应（UniDA），即使用源域标记数据和目标域未标记数据学习模型，使学习模型能够成功适应目标数据。在本文中，我们利用基础模型对现有状态下的UniDA方法进行了全面的实证研究。我们首先证明，尽管基础模型极大地提高了仅在源数据上训练模型的基准方法的性能，但现有的UniDA方法通常不能超越基准。这表明，使用基础模型的UniDA需要新的研究努力。为此，我们提出了一种非常简单的目标方法。

    Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target 
    
[^140]: 差分隐私在拓扑数据分析中的应用

    Differentially Private Topological Data Analysis. (arXiv:2305.03609v1 [stat.ML])

    [http://arxiv.org/abs/2305.03609](http://arxiv.org/abs/2305.03609)

    本文尝试使用差分隐私实现拓扑数据分析并生成接近最优的私有持久图，提出使用 $L^1$-距离计算持久图并采用指数机制保护隐私，成功实现在隐私保护和数据分析之间的平衡。

    

    本文是首篇尝试使用差分隐私实现拓扑数据分析并生成接近最优的私有持久图。我们通过瓶颈距离分析持久图的灵敏度，发现常用的 \v{C}ech 复形的灵敏度并不会随着样本量 $n$ 的增加而降低，这使得 \v{C}ech 复形持久图难以隐私化。作为替代方法，我们提出使用 $L^1$-距离来计算持久图，发现其灵敏度为 $O(1/n)$。基于灵敏度分析，我们提出采用指数机制，其效用函数定义为 $L^1$-DTM 持久图的瓶颈距离。同时，我们还推导出了我们隐私机制的精度上下界；得到的界限表明我们的机制隐私误差接近最优。我们展示了我们的私有持久图方法的性能。

    This paper is the first to attempt differentially private (DP) topological data analysis (TDA), producing near-optimal private persistence diagrams. We analyze the sensitivity of persistence diagrams in terms of the bottleneck distance, and we show that the commonly used \v{C}ech complex has sensitivity that does not decrease as the sample size $n$ increases. This makes it challenging for the persistence diagrams of \v{C}ech complexes to be privatized. As an alternative, we show that the persistence diagram obtained by the $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on the sensitivity analysis, we propose using the exponential mechanism whose utility function is defined in terms of the bottleneck distance of the $L^1$-DTM persistence diagrams. We also derive upper and lower bounds of the accuracy of our privacy mechanism; the obtained bounds indicate that the privacy error of our mechanism is near-optimal. We demonstrate the performance of our privatized persistence
    
[^141]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^142]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^143]: 为什么要逐步思考？推理源于经验的局部性。

    Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])

    [http://arxiv.org/abs/2304.03843](http://arxiv.org/abs/2304.03843)

    本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。

    

    人类有着强大而神秘的推理能力。通过一系列纯粹的思维步骤，我们可以推理出我们无法直接得出的推论 - 尽管我们从世界上没有得到任何额外数据。同样地，大型语言模型可以通过一步步的推理，在回答问题之前生成中间步骤，从而更好地完成复杂的任务。我们使用语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。这些训练条件能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。我们使用贝叶斯网络定义的联合分布的样品对自回归变压器进行训练，但每个样品只包括其中的一部分变量。我们比较使用推理生成的变量子集与使用完整集合进行训练的方案的性能。

    Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
    
[^144]: 对抗二进制相似性系统的攻击

    Adversarial Attacks against Binary Similarity Systems. (arXiv:2303.11143v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.11143](http://arxiv.org/abs/2303.11143)

    本文研究了二进制相似性模型在对抗性环境中的韧性，表明它们容易受到有针对性和无针对性攻击，并向黑盒和白盒攻击者展示了这种易受攻击的情况。

    

    最近几年，二进制分析作为一种检查软件并确保其安全性的基本方法得到了关注。由于运行软件的设备数量指数增长，许多研究现在正转向基于深度学习模型的新型自主解决方案，因为它们在解决二进制分析问题时表现出了最先进的性能。在这个背景下，二进制相似性是一个热门话题，它涉及确定汇编代码中的两个函数是否来自相同的源代码。然而，目前还不清楚深度学习模型在对抗性环境中对于二进制相似性的行为如何。本文研究了二进制相似性模型对抗对抗性示例的韧性，显示它们容易受到黑盒和白盒攻击者对于相似性目标的有针对性和无针对性攻击。具体来说，我们详细测试了三种当前最先进的二进制相似性解决方案对两种黑盒

    In recent years, binary analysis gained traction as a fundamental approach to inspect software and guarantee its security. Due to the exponential increase of devices running software, much research is now moving towards new autonomous solutions based on deep learning models, as they have been showing state-of-the-art performances in solving binary analysis problems. One of the hot topics in this context is binary similarity, which consists in determining if two functions in assembly code are compiled from the same source code. However, it is unclear how deep learning models for binary similarity behave in an adversarial context. In this paper, we study the resilience of binary similarity models against adversarial examples, showing that they are susceptible to both targeted and untargeted attacks (w.r.t. similarity goals) performed by black-box and white-box attackers. In more detail, we extensively test three current state-of-the-art solutions for binary similarity against two black-b
    
[^145]: 基于对象中心的槽扩散

    Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10834](http://arxiv.org/abs/2303.10834)

    基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。

    

    最近基于Transformer的图像生成模型在处理复杂场景的对象中心学习中取得了成功，这突显了强大的图像生成器的重要性。然而，尽管扩散模型在图像生成方面具有较高的表达能力，但它们在对象中心学习中的整合在这个领域中仍然较少探索。在本文中，我们探讨了将扩散模型整合到对象中心学习中的可行性和潜力，并研究了这种方法的优点和缺点。我们引入了Latent Slot Diffusion (LSD)，这是一种新颖的模型，它具有两个目标：首先，它是第一个将传统的槽解码器替换为以对象槽为条件的潜在扩散模型的对象中心学习模型；其次，它也是第一个不需要像文本这样的监督注释而能够无监督地进行组合条件扩散的模型。通过对各种对象中心任务的实验，包括首次在FFHQ数据集中的应用。

    The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
    
[^146]: 停留还是离开预训练基域：关于集成学习在迁移学习中的洞见

    To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03374](http://arxiv.org/abs/2303.03374)

    该论文研究了在迁移学习中使用单个预训练检查点微调的模型集合，发现通过更好地探索预训练基域可以改进集成模型，但离开基域会导致失去迁移学习的好处，并且降低集成质量。作者提出了一种更有效的修改方法StarSSE，可以产生更强的集成模型和均匀的模型混合。

    

    迁移学习和集成学习是改善神经网络性能和鲁棒性的两种热门技术。由于预训练成本高昂，通常实践中使用从单个预训练检查点微调的模型集合。这些模型最终会进入损失函数梯度下降空间的相同区域，我们称之为预训练基域，因此具有有限的多样性。在这项工作中，我们展示了从单个预训练检查点训练的集成模型可以通过更好地探索预训练基域来改进，然而，离开基域会导致失去迁移学习的好处并导致集成质量的下降。基于对现有探索方法的分析，我们提出了一种更有效的修改Transfer Learning Setup中的Snapshot Ensembles（SSE）方法，名为StarSSE，它能产生更强的集成模型和均匀的模型混合。

    Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups.
    
[^147]: 检测和减轻格点场论的流采样中的模式崩溃问题

    Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories. (arXiv:2302.14082v2 [hep-lat] UPDATED)

    [http://arxiv.org/abs/2302.14082](http://arxiv.org/abs/2302.14082)

    本论文研究了归一化流在格点场论中的应用，并发现归一化流经常遭受模式崩溃问题，在训练阶段将相关模式的概率质量赋予极低的值。我们提出了度量模式崩溃程度的指标，并提出了减轻偏差的策略。

    

    在格点场论的背景下，我们研究了归一化流的模式崩溃问题的后果。归一化流可以进行独立采样，因此希望它们可以避免多模态分布的局部更新马尔科夫链蒙特卡洛算法的隧道问题。在本研究中，我们首先指出，隧道问题在归一化流中也存在，但已经从采样阶段转移到了算法的训练阶段。具体来说，归一化流经常遭受模式崩溃，训练过程将物理分布的相关模式赋予极低的概率质量，这可能导致在流作为马尔科夫链采样器或重要采样时产生显著偏差。我们提出了一种度量模式崩溃程度的指标，并推导了由此导致的偏差上限。此外，我们在估计热力学观测量（如热力学势）的背景下提出了多种缓解策略。

    We study the consequences of mode-collapse of normalizing flows in the context of lattice field theory. Normalizing flows allow for independent sampling. For this reason, it is hoped that they can avoid the tunneling problem of local-update MCMC algorithms for multi-modal distributions. In this work, we first point out that the tunneling problem is also present for normalizing flows but is shifted from the sampling to the training phase of the algorithm. Specifically, normalizing flows often suffer from mode-collapse for which the training process assigns vanishingly low probability mass to relevant modes of the physical distribution. This may result in a significant bias when the flow is used as a sampler in a Markov-Chain or with Importance Sampling. We propose a metric to quantify the degree of mode-collapse and derive a bound on the resulting bias. Furthermore, we propose various mitigation strategies in particular in the context of estimating thermodynamic observables, such as the
    
[^148]: 不平衡数据集下深度主动学习算法选择研究

    Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07317](http://arxiv.org/abs/2302.07317)

    本论文提出了适用于不平衡数据集下的深度主动学习的自适应算法选择策略TAILOR，它在多类和多标签应用的实验中取得了与候选算法相当或更高的准确性。

    

    标签效率已成为深度学习应用中越来越重要的目标。主动学习旨在减少训练深度网络所需的标记示例数量，但是，在不同的数据集和应用中，主动学习算法的实证性能可能会大幅度变化。事先很难知道哪种主动学习策略在特定应用中表现良好或最佳。为解决这个问题，我们提出了第一个用于深度主动学习的自适应算法选择策略。对于任何未标记的数据集，我们的(meta)算法TAILOR (Thompson ActIve Learning algORithm selection)迭代地并自适应地选择一组候选主动学习算法。TAILOR使用旨在收集类平衡示例的新奖励函数。在多类和多标签应用的大量实验中，TAILOR在实现与候选算法中最佳算法相当或更高的准确性方面表现出良好的效果。

    Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
    
[^149]: SEGA：使用语义引导指导文本到图像模型的指导

    SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12247](http://arxiv.org/abs/2301.12247)

    本论文介绍了一种称为SEGA的语义引导方法，用于指导文本到图像模型的生成过程。通过与扩散过程的互动，SEGA可以灵活地在语义方向上引导模型的生成，实现细微和广泛的编辑以及优化整体艺术构思。实验证明SEGA在多种任务和生成架构上都表现出了出色的多功能性、灵活性和改进。

    

    最近，文本到图像扩散模型因其令人惊讶的能力可以仅通过文本生成高保真度的图像而受到了广泛的关注。然而，实现与用户意图对齐的单次生成几乎是不可能的，而输入提示的微小改变往往会导致非常不同的图像。这使得用户在语义控制方面有限。为了使用户实现控制，我们展示了如何通过与扩散过程互动来灵活地引导语义方向。这种语义引导(SEGA)可以推广到任何使用无分类器引导的生成架构。更重要的是，它允许进行细微和广泛的编辑，组成和风格的变化，以及优化整体艺术构思。我们使用各种任务展示了SEGA在潜在和像素级扩散模型（如Stable Diffusion，Paella和DeepFloyd-IF）上的有效性，从而为其多功能性，灵活性和改进提供了有力的证据。

    Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements ov
    
[^150]: Tracr: 编译变压器模型作为可解释性实验室

    Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05062](http://arxiv.org/abs/2301.05062)

    Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。

    

    我们展示了如何将可读性强的程序编译成标准的仅解码变压器模型。我们的编译器Tracr生成具有已知结构的模型，可以用于设计实验。例如，我们使用它来研究执行多步算法的变压器中的“叠加”。此外，Tracr编译模型的已知结构可以作为评估可解释方法的真实基准。通常，由于变压器学习的“程序”是未知的，因此不清楚解释是否成功。我们通过实现和检查包括计算令牌频率、排序和括号检查在内的程序来演示我们的方法。我们在https://github.com/deepmind/tracr提供了Tracr的开源实现。

    We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
    
[^151]: 一份大规模的、基于PCR的COVID-19声音数据集

    A large-scale and PCR-referenced vocal audio dataset for COVID-19. (arXiv:2212.07738v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2212.07738](http://arxiv.org/abs/2212.07738)

    英国COVID-19 Vocal Audio Dataset是迄今为止最大的SARS-CoV-2 PCR参考音频记录集合，旨在为训练和评估使用声音数据分类SARS-CoV-2感染状态或相关呼吸症状的机器学习模型而设计。

    The UK COVID-19 Vocal Audio Dataset is the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date, designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio.

    英国COVID-19 Vocal Audio Dataset旨在为训练和评估使用声音数据分类SARS-CoV-2感染状态或相关呼吸症状的机器学习模型而设计。英国卫生安全局通过国家测试和追踪计划和REACT-1调查在2021年3月至2022年3月期间招募了自愿参与者，收集了自愿咳嗽、呼气和语音的音频记录，并将其与SARS-CoV-2检测结果相关联。该数据集是迄今为止最大的SARS-CoV-2 PCR参考音频记录集合。

    The UK COVID-19 Vocal Audio Dataset is designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio. The UK Health Security Agency recruited voluntary participants through the national Test and Trace programme and the REACT-1 survey in England from March 2021 to March 2022, during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and some Omicron variant sublineages. Audio recordings of volitional coughs, exhalations, and speech were collected in the 'Speak up to help beat coronavirus' digital survey alongside demographic, self-reported symptom and respiratory condition data, and linked to SARS-CoV-2 test results. The UK COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date. PCR results were linked to 70,794 of 72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms were reported by 45.6
    
[^152]: 可分离PINN：减轻物理信息神经网络中的维度诅咒

    Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks. (arXiv:2211.08761v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08761](http://arxiv.org/abs/2211.08761)

    提出了一种名为可分离PINN（SPINN）的网络架构来减轻物理信息神经网络中的维度诅咒，并通过利用前向模式自动微分实现了更高效的计算。

    

    物理信息神经网络（PINNs）已成为前向和反向问题的新型数据驱动PDE求解器。尽管具有潜力，但获得解的昂贵计算成本通常限制其更广泛的应用。我们证明，在训练PINN时，通过利用前向模式自动微分（AD），可以显著减少计算量。然而，对传统PINN进行简单的前向模式AD应用会导致更高的计算量，失去其实际效益。因此，我们提出了一种名为可分离PINN（SPINN）的网络架构，它可以促进更高效的计算的前向模式AD。SPINN在每个轴上操作，而不是传统PINN中的逐点处理，减少了网络正向传递的次数。此外，标准PINN的计算和内存成本随着网格分辨率的增加呈指数级增长，而我们模型的成本相对较低，减轻了维度诅咒。

    Physics-informed neural networks (PINNs) have emerged as new data-driven PDE solvers for both forward and inverse problems. While promising, the expensive computational costs to obtain solutions often restrict their broader applicability. We demonstrate that the computations in automatic differentiation (AD) can be significantly reduced by leveraging forward-mode AD when training PINN. However, a naive application of forward-mode AD to conventional PINNs results in higher computation, losing its practical benefit. Therefore, we propose a network architecture, called separable PINN (SPINN), which can facilitate forward-mode AD for more efficient computation. SPINN operates on a per-axis basis instead of point-wise processing in conventional PINNs, decreasing the number of network forward passes. Besides, while the computation and memory costs of standard PINNs grow exponentially along with the grid resolution, that of our model is remarkably less susceptible, mitigating the curse of dim
    
[^153]: 使用基于偏好的强化学习实现抽象时间线摘要

    Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07596](http://arxiv.org/abs/2211.07596)

    本文提出了一种基于偏好的强化学习方法，在抽象摘要的基础上优化时间线摘要的质量和可读性，通过自动化和人工评估验证了方法的有效性。

    

    本文介绍了一种新颖的流程，用于总结多个新闻来源报道的时间线。基于Transformer的抽象摘要模型可以生成连贯且简洁的长文档摘要，但在时间线摘要（TLS）等特定任务上可能无法超越已建立的抽取方法。虽然抽取摘要更忠实于来源，但可能不太易读且包含冗余或不必要的信息。本文提出了一种基于偏好的强化学习（PBRL）方法，用于将预训练的抽象摘要器适应于TLS，可以克服抽取时间线摘要的缺点。我们定义了一个复合奖励函数，通过关键词和偏好标签学习，并将其用于通过线下强化学习对预训练的抽象摘要器进行微调。我们对三个数据集进行了自动化和人工评估，发现我们的方法优于可比较的方法。

    This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable 
    
[^154]: 成本感知的通用α投资法用于多重假设检验

    Cost-aware Generalized $\alpha$-investing for Multiple Hypothesis Testing. (arXiv:2210.17514v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17514](http://arxiv.org/abs/2210.17514)

    本文提出了成本感知的通用α投资法进行多重假设检验，拓展了α投资规则以考虑样本大小，通过构建与自然对抗的博弈来优化α财富的期望回报(ERO)并提供最佳样本大小。经实证表明，该规则能更准确地拒绝虚假零假设。

    

    本文探讨在数据收集具有非平凡成本时进行顺序多重假设检验的问题。该问题在识别疾病过程中的差异表达基因等生物实验中出现。本文构建在通用α投资框架上，能在顺序检验环境下进行误发现率控制。我们对α-财富的长期渐进行为进行了理论分析，引出了在α投资决策规则中考虑样本大小的思考。将检验过程视为与自然对抗的博弈，本文构建了一种决策规则，优化α财富的期望回报(ERO)，并为测试提供了最佳样本大小。经验证明，成本感知的ERO决策规则比其他方法正确地拒绝了更多的虚假零假设。本文扩展了成本感知的ERO投资至有限时间检验，使决策规则能够跨越多个检验分配样本。

    We consider the problem of sequential multiple hypothesis testing with nontrivial data collection cost. This problem appears, for example, when conducting biological experiments to identify differentially expressed genes in a disease process. This work builds on the generalized $\alpha$-investing framework that enables control of the false discovery rate in a sequential testing setting. We make a theoretical analysis of the long term asymptotic behavior of $\alpha$-wealth which motivates a consideration of sample size in the $\alpha$-investing decision rule. Posing the testing process as a game with nature, we construct a decision rule that optimizes the expected return (ERO) of $\alpha$-wealth and provides an optimal sample size for the test. Empirical results show that a cost-aware ERO decision rule correctly rejects more false null hypotheses than other methods. We extend cost-aware ERO investing to finite-horizon testing which enables the decision rule to allocate samples across ma
    
[^155]: 用于节点分类的图形数据集的特征化：同质性-异质性二分法及其延伸

    Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. (arXiv:2209.06177v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2209.06177](http://arxiv.org/abs/2209.06177)

    本论文研究了用于节点分类的图形数据集的特征化，发现目前常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。作者提出了一种新的同质性测量指标，称为调整同质性，该指标满足更多期望特性，并具有较少在图形机器学习文献中使用的特点。

    

    同质性是描述边连接相似节点倾向的图形属性；相反的概念为异质性。人们通常认为异质性图对于标准的消息传递图神经网络（GNN）是具有挑战性的，并且已经付出了许多努力来开发这种情况下的高效方法。然而，目前在文献中没有普遍被接受的同质性测量指标。在这项工作中，我们展示了常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。为此，我们为正确的同质性测量指标形式化了期望的特性，并验证了哪些指标满足哪些特性。特别是，我们发现一种我们称之为调整同质性的指标满足比其他流行同质性测量方法更多的期望特性，而在图形机器学习文献中很少被使用。然后，我们超越了同质性-异质性二分法，提出了一种新的特征，使得...

    Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allo
    
[^156]: 贝叶斯学习多任务问题的特征空间

    Bayesian learning of feature spaces for multitasks problems. (arXiv:2209.03028v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.03028](http://arxiv.org/abs/2209.03028)

    本文介绍了一种贝叶斯学习的方法，通过连接核机器和极限学习机，实现了在多任务回归问题中的特征空间的学习。该方法提供了优化RBF核参数、模型复杂度和多输出稀疏性的能力。

    

    本文介绍了一种新颖的多任务回归方法，通过利用随机傅里叶特征(RFFs)近似径向基函数(RBF)核，将核机器(KM)和极限学习机(ELM)相连接。其中的一个贡献是表明对于所提出的模型，KM和ELM的形式可以被看作是同一枚硬币的两面。这些提出的模型称为RFF-BLR，建立在贝叶斯框架上，同时解决了两个主要的设计目标。一方面，它基于带有RBF核的KM拟合多任务回归器。另一方面，它引入了一种跨任务的共同先验，促进了ELM视图中的多输出稀疏性。这种贝叶斯方法使得能够同时考虑KM和ELM的观点，实现了(i)在概率框架内优化RBF核参数$\gamma$，(ii)优化模型复杂度，和(iii)

    This paper introduces a novel approach for multi-task regression that connects Kernel Machines (KMs) and Extreme Learning Machines (ELMs) through the exploitation of the Random Fourier Features (RFFs) approximation of the RBF kernel. In this sense, one of the contributions of this paper shows that for the proposed models, the KM and the ELM formulations can be regarded as two sides of the same coin. These proposed models, termed RFF-BLR, stand on a Bayesian framework that simultaneously addresses two main design goals. On the one hand, it fits multitask regressors based on KMs endowed with RBF kernels. On the other hand, it enables the introduction of a common-across-tasks prior that promotes multioutput sparsity in the ELM view. This Bayesian approach facilitates the simultaneous consideration of both the KM and ELM perspectives enabling (i) the optimisation of the RBF kernel parameter $\gamma$ within a probabilistic framework, (ii) the optimisation of the model complexity, and (iii) 
    
[^157]: 通过学习领域指南识别未知鸟类物种

    Recognition of Unseen Bird Species by Learning from Field Guides. (arXiv:2206.01466v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.01466](http://arxiv.org/abs/2206.01466)

    这篇论文提出了通过学习领域指南中的插图进行未知鸟类物种的零样本识别。通过对插图进行对比编码和结构上更类似于照片的方法，我们实现了12%的top-1分类准确率和38%的top-10分类准确率，证明了领域指南插图作为辅助信息的潜力。

    

    我们利用领域指南来学习鸟类物种识别，特别是对于未知物种的零样本识别。领域指南中的插图有意专注于每个物种的区分特征，并可以作为辅助信息将已知物种的知识迁移到未知鸟类物种上。我们研究了两种方法：（1）对插图进行对比编码，可以输入标准的零样本学习方案；（2）一种利用插图也是图像的事实，并且在结构上更类似于照片而不是其他类型辅助信息的新方法。我们的结果表明，来自领域指南的插图确实是零样本学习的一个有竞争力的辅助信息来源，因为它们可以方便地获取多种物种的插图。在iNaturalist2021数据集的子集中，含有749个已知物种和739个未知物种，我们得到了12%的未知鸟类物种的top-1分类准确率和38%的top-10分类准确率，这表明了其潜在的应用价值。

    We exploit field guides to learn bird species recognition, in particular zero-shot recognition of unseen species. Illustrations contained in field guides deliberately focus on discriminative properties of each species, and can serve as side information to transfer knowledge from seen to unseen bird species. We study two approaches: (1) a contrastive encoding of illustrations, which can be fed into standard zero-shot learning schemes; and (2) a novel method that leverages the fact that illustrations are also images and as such structurally more similar to photographs than other kinds of side information. Our results show that illustrations from field guides, which are readily available for a wide range of species, are indeed a competitive source of side information for zero-shot learning. On a subset of the iNaturalist2021 dataset with 749 seen and 739 unseen species, we obtain a classification accuracy of unseen bird species of $12\%$ @top-1 and $38\%$ @top-10, which shows the potentia
    
[^158]: 图神经扩散网络用于半监督学习

    Graph Neural Diffusion Networks for Semi-supervised Learning. (arXiv:2201.09698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09698](http://arxiv.org/abs/2201.09698)

    提出了一种名为 GND-Nets 的图神经网络，利用浅层网络和局部、全局邻域信息来解决图半监督学习中的过度平滑和欠平滑问题。

    

    图卷积网络 (GCN) 是用于基于图的半监督学习的先驱模型。然而，GCN 在标记稀疏的图上表现不佳。其两层版本不能有效地将标签信息传播到整个图结构（即欠平滑问题），而其深层版本则过度平滑且难以训练（即过度平滑问题）。为了解决这两个问题，我们提出了一种新的图神经网络，称为 GND-Nets（图神经扩散网络），它在单层中利用了顶点的局部和全局邻域信息。利用浅层网络可以缓解过度平滑问题，而利用局部和全局邻域信息可以缓解欠平滑问题。顶点的局部和全局邻域信息的利用是通过一种称为神经扩散的新图扩散方法实现的，该方法将神经网络融入传统的线性和非线性图扩散中。

    Graph Convolutional Networks (GCN) is a pioneering model for graph-based semi-supervised learning. However, GCN does not perform well on sparsely-labeled graphs. Its two-layer version cannot effectively propagate the label information to the whole graph structure (i.e., the under-smoothing problem) while its deep version over-smoothens and is hard to train (i.e., the over-smoothing problem). To solve these two issues, we propose a new graph neural network called GND-Nets (for Graph Neural Diffusion Networks) that exploits the local and global neighborhood information of a vertex in a single layer. Exploiting the shallow network mitigates the over-smoothing problem while exploiting the local and global neighborhood information mitigates the under-smoothing problem. The utilization of the local and global neighborhood information of a vertex is achieved by a new graph diffusion method called neural diffusions, which integrate neural networks into the conventional linear and nonlinear gra
    
[^159]: 特征注意的递归模块在强化学习中的泛化能力

    Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08369](http://arxiv.org/abs/2112.08369)

    "Feature-Attending Recurrent Modules" (FARM)是一种学习状态表示的体系结构，通过特征注意机制来捕捉空间和时间规律性，从而改善强化学习代理的泛化能力。

    

    许多重要的任务都是以物体为基础定义的。为了在这些任务上实现泛化，强化学习（RL）代理需要利用物体所引发的结构。先前的工作要么硬编码对象中心的特征，要么使用复杂的对象中心生成模型，要么使用局部空间特征更新状态。然而，这些方法在实现泛化RL代理方面的成功有限。受此启发，我们引入了"特征注意的递归模块"（FARM），这是一种学习状态表示的体系结构，依赖于简单、广泛适用的归纳偏置来捕捉空间和时间规律性。FARM学习了一种分布于多个模块之间的状态表示，每个模块都使用具有表现力的特征注意机制关注时空特征。我们展示了这种方法改善了RL代理在对象中心任务上的泛化能力。我们在2D和3D环境中研究了任务套件，并发现FARM在泛化能力方面表现更好。

    Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce "Feature-Attending Recurrent Modules" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generaliz
    
[^160]: 非参数回归中的相变

    Phase transitions in nonparametric regressions. (arXiv:2112.03626v6 [math.ST] UPDATED)

    [http://arxiv.org/abs/2112.03626](http://arxiv.org/abs/2112.03626)

    本文研究了非参数回归中的相变问题，根据最小极大MISE速率的不同情况，确定了不同范围内最佳的平滑度。同时，本文还提出了一组用于平滑函数类别的度量熵界限。

    

    当已知单变量的未知回归函数的导数在绝对值上直到$(\gamma+1)$阶都受到一个公共常数的界限（即$(\gamma+1)$阶平滑度），文献中给出的均方误差（MISE）的最小极大速率为$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$。本文表明：（i）如果$n\leq\left(\gamma+1\right)^{2\gamma+3}$，则最小极大MISE速率为$\frac{\log n}{n\log(\log n)}$，并且最佳利用的平滑度为大约$\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\}$；（ii）如果$n>\left(\gamma+1\right)^{2\gamma+3}$，则最小极大MISE速率为$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$，并且最佳利用的平滑度为$\gamma+1$。本文的基本贡献是我们为平滑函数类别制定的一组度量熵界限。

    When the unknown regression function of a single variable is known to have derivatives up to the $(\gamma+1)$th order bounded in absolute values by a common constant everywhere or a.e. (i.e., $(\gamma+1)$th degree of smoothness), the minimax optimal rate of the mean integrated squared error (MISE) is stated as $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ in the literature. This paper shows that: (i) if $n\leq\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if $n>\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ and the optimal degree of smoothness to exploit is $\gamma+1$. The fundamental contribution of this paper is a set of metric entropy bounds we develop for smooth function classes. Some 
    
[^161]: 关于最小化器和卷积滤波器的理论连接及其在基因组分析中的应用

    On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08452](http://arxiv.org/abs/2111.08452)

    该论文通过对哈希函数属性进行数学分析，发现在分类字母表上的序列分析中，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一种最小化器排序，能够有效提取与其他最小化器距离较近但与序列中的k-mer相距较远的重要特征。

    

    最小化器和卷积神经网络(CNN)是两种完全不同的流行技术，均被用于分析生物序列。从表面上看，这些方法似乎完全不同。最小化器使用滚动窗口的最小哈希方法提取每个窗口中的一个重要k-mer特征。CNN则以随机初始化的卷积滤波器和池化操作为基础，通过多个神经层来学习滤波器本身及其用于分类序列的方法。本文主要结果是对哈希函数属性进行了仔细的数学分析，显示对于分类字母表上的序列，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一个最小化器排序，使得选择的k-mer与序列中的k-mer（按汉明距离）相距较远，但与其他最小化器相距较近。在实证实验中，我们发现这种方法能够有效降低计算复杂度并与传统方法具有相当的性能。

    Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
    
[^162]: ReLU'(0)对反向传播的数值影响研究

    Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12915](http://arxiv.org/abs/2106.12915)

    本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。

    

    在理论上，神经网络中ReLU'(0)在[0, 1]范围内的选择对反向传播和训练几乎没有影响。然而，在现实世界中，32位默认精度结合深度学习问题的规模，使其成为训练方法的超参数。我们研究了ReLU'(0)值对几种精度水平（16位，32位，64位）、各种网络（全连接、VGG、ResNet）和数据集（MNIST、CIFAR10、SVHN）的重要性。我们观察到在32位精度下，反向传播输出出现了显著的变化，这种情况大约出现了一半的时间。这种影响在双精度下消失，而在16位精度下是系统性的。对于普通的SGD训练而言，选择ReLU'(0) = 0似乎是最有效的。我们还发现，批归一化或ADAM等重新调整方法 tend to buffer ReLU'(0)值的影响。总体而言，我们想要传达的信息是，非光滑问题的算法微分可能隐藏了一些参数。

    In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
    
[^163]: 最小最大拟贝叶斯估计在稀疏规范相关分析中的应用：基于瑞利商函数的方法

    Minimax Quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function. (arXiv:2010.08627v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.08627](http://arxiv.org/abs/2010.08627)

    本研究提出了一种利用重新缩放的瑞利商函数作为准对数似然函数并采用贝叶斯框架的方法，通过马尔科夫链蒙特卡罗计算稀疏规范向量的估计值。实验结果表明，该方法在连续和截断数据上表现优于其他方法。

    

    规范相关分析（CCA）是一种用于探索数据集之间关系的流行统计技术。近年来，稀疏规范向量的估计已成为CCA问题的一个重要且具有挑战性的变体，应用广泛。不幸的是，现有的稀疏规范向量的最优估计器计算成本较高。我们提出了一种准贝叶斯估计过程，既达到了最小最大估计速率，又可以通过马尔科夫链蒙特卡罗（MCMC）轻松计算。该方法基于Tan等人（2018）的研究，使用重新缩放的瑞利商函数作为准对数似然函数。然而，与Tan等人（2018）不同的是，我们采用了一个贝叶斯框架，将这个准对数似然函数与尖峰-平板先验结合起来进行规范化推理和促进稀疏性。我们研究了所提出方法在连续和截断数据上的经验行为，并证明它优于几种方法。

    Canonical correlation analysis (CCA) is a popular statistical technique for exploring relationships between datasets. In recent years, the estimation of sparse canonical vectors has emerged as an important but challenging variant of the CCA problem, with widespread applications. Unfortunately, existing rate-optimal estimators for sparse canonical vectors have high computational cost. We propose a quasi-Bayesian estimation procedure that not only achieves the minimax estimation rate, but also is easy to compute by Markov Chain Monte Carlo (MCMC). The method builds on Tan et al. (2018) and uses a re-scaled Rayleigh quotient function as the quasi-log-likelihood. However, unlike Tan et al. (2018), we adopt a Bayesian framework that combines this quasi-log-likelihood with a spike-and-slab prior to regularize the inference and promote sparsity. We investigate the empirical behavior of the proposed method on both continuous and truncated data, and we demonstrate that it outperforms several st
    
[^164]: 非平稳背景下的递归神经线性后验抽样应用于情境赌博

    Recurrent Neural-Linear Posterior Sampling for Nonstationary Contextual Bandits. (arXiv:2007.04750v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.04750](http://arxiv.org/abs/2007.04750)

    该论文提出了一种递归神经线性后验抽样的方法，用于解决非平稳情境下的情境赌博问题。实验证明该方法能够有效地表示相关情境并做出决策。

    

    在非平稳情境赌博问题中，一个智能体需要在探索和利用其先前经验中存在的(周期性或结构化)模式之间保持平衡。手工设计一个合适的历史情境是将非平稳问题转化为可以高效解决的平稳问题的一种有吸引力的替代方案。然而，即使是精心设计的历史情境也可能引入虚假关系或缺乏关键信息的方便表示。为了解决这些问题，我们提出了一种学习仅基于智能体与环境之间的原始交互历史来表示相关情境并做出决策的方法。该方法利用了由递归神经网络提取的特征与基于后验抽样的情境线性赌博算法的组合。我们在多样的情境和非情境非平稳问题上的实验证明了我们递归方法的有效性。

    An agent in a nonstationary contextual bandit problem should balance between exploration and the exploitation of (periodic or structured) patterns present in its previous experiences. Handcrafting an appropriate historical context is an attractive alternative to transform a nonstationary problem into a stationary problem that can be solved efficiently. However, even a carefully designed historical context may introduce spurious relationships or lack a convenient representation of crucial information. In order to address these issues, we propose an approach that learns to represent the relevant context for a decision based solely on the raw history of interactions between the agent and the environment. This approach relies on a combination of features extracted by recurrent neural networks with a contextual linear bandit algorithm based on posterior sampling. Our experiments on a diverse selection of contextual and noncontextual nonstationary problems show that our recurrent approach co
    

