# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize.](http://arxiv.org/abs/2305.16830) | 本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。 |
| [^2] | [HUB: Guiding Learned Optimizers with Continuous Prompt Tuning.](http://arxiv.org/abs/2305.16823) | 本文提出了一种名为HUB的混合更新策略，通过结合学习优化器和手工设计的优化器，提高了学习优化器泛化性能。 |
| [^3] | [Towards Certification of Machine Learning-Based Distributed Systems.](http://arxiv.org/abs/2305.16822) | 认证技术对于机器学习驱动的分布式系统验证不适用，本文提出了第一个ML-based分布式系统认证方案，以验证其非功能属性。 |
| [^4] | [Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup.](http://arxiv.org/abs/2305.16817) | 选择性mixup通过非随机选择对提高训练分布，实现标签偏移的经典解决方案，从而提高了神经网络的泛化性能。 |
| [^5] | [Geometric deep learning approach to knot theory.](http://arxiv.org/abs/2305.16808) | 本文利用几何深度学习构建了一个函子将结论转化为图形，并使用图神经网络预测结论不变量，具有高泛化能力。 |
| [^6] | [On the Generalization Capacities of Neural Controlled Differential Equations.](http://arxiv.org/abs/2305.16791) | 本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。 |
| [^7] | [Modulate Your Spectrum in Self-Supervised Learning.](http://arxiv.org/abs/2305.16789) | 本文提出了谱变换（ST）框架，可以调节自监督学习的频谱，并避免特征崩溃。其中，INTL是ST的一个实例，能够在优化过程中将嵌入的频谱调节到一个等特征值分布，实现较高的准确率。 |
| [^8] | [Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization.](http://arxiv.org/abs/2305.16784) | 本文提出了一种名为'RSTformer'的摘要模型，该模型全面融合了话语关系类型和不确定性，并以修辞结构理论为基础，经过严格评估，表现明显优于现有的模型。 |
| [^9] | [Graph Neural Convection-Diffusion with Heterophily.](http://arxiv.org/abs/2305.16780) | 本论文提出了一种考虑了异质性原则的新型图神经网络，该网络使用对流扩散方程对节点上的信息流进行建模，可以同时考虑基于同质性和异质性的信息传递，在处理异质性图的节点分类任务中具有竞争性的表现。 |
| [^10] | [Robust Nonparametric Regression under Poisoning Attack.](http://arxiv.org/abs/2305.16771) | 本文提出了一种针对毒化攻击的鲁棒非参数回归方法，包括基于Huber损失的M-评估器和通过将初始估计投影到Lipschitz函数空间中的校正方法。结果表明，正确选择带宽时$\ell_\infty $误差是极小化最优的，而$\ell_2 $误差在$q\lesssim \sqrt{N/\ln^2 N}$时最优。 |
| [^11] | [Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification.](http://arxiv.org/abs/2305.16756) | 本研究提出了一种以人道主义本体为基础的新型语言模型HumBert，并提供了一种系统的方法来衡量和减少偏见，以实现对人道主义数据分析的有效和道德意识的支持。 |
| [^12] | [A Decentralized Spike-based Learning Framework for Sequential Capture in Discrete Perimeter Defense Problem.](http://arxiv.org/abs/2305.16748) | 本文提出了一种针对离散周界防御问题的分散脉冲学习框架，有效解决了团队防御者保护领土的问题，每个防御者含有自己的MLC-SEFRON网络，从而实现分散独立训练，输入信息来源于防御者和入侵者的时空信息。 |
| [^13] | [Parameter-Efficient Fine-Tuning without Introducing New Latency.](http://arxiv.org/abs/2305.16742) | 本文提出了一种参数高效微调的方法，以任务不可知的方式生成稀疏掩码，无需添加新参数，避免了额外的推断延迟，并超过了现有方法的效果。 |
| [^14] | [Evaluating generation of chaotic time series by convolutional generative adversarial networks.](http://arxiv.org/abs/2305.16729) | 该研究通过使用卷积生成对抗网络生成混沌时间序列，并评估生成的时间序列，证明了卷积神经网络有能力很好地复制原始时间序列的混沌特性，但误差分析表明仍然存在大误差。 |
| [^15] | [Shape-based pose estimation for automatic standard views of the knee.](http://arxiv.org/abs/2305.16717) | 该论文提出一种基于形状信息的自动膝关节标准视图姿态估计框架，可减少手术过程中的时间和辐射剂量，获得了比现有技术更好的精度和稳定性。 |
| [^16] | [A Hierarchical Approach to Population Training for Human-AI Collaboration.](http://arxiv.org/abs/2305.16708) | 本论文提出了一种基于层次思维的人工智能协同训练方法，通过引入层次强化学习方法，代理能够根据当前合作伙伴自动切换最佳响应策略，从而显著提高了代理与新合作伙伴的适应性。 |
| [^17] | [A Closer Look at In-Context Learning under Distribution Shifts.](http://arxiv.org/abs/2305.16704) | 本文探究了分布漂移下的上下文学习，比较了变压器和基于集合的MLP模型的性能，发现二者在分布内评估中都表现出上下文学习的能力，但在防范较小的分布漂移方面，变压器更胜一筹。 |
| [^18] | [Sources of Uncertainty in Machine Learning -- A Statisticians' View.](http://arxiv.org/abs/2305.16703) | 本文讨论了机器学习中不确定性的来源和类型，从统计学家的视角出发，分类别介绍了随机性和认知性不确定性的概念，证明了不确定性来源各异，不可简单归为两类。同时，与统计学概念进行类比，探讨不确定性在机器学习中的作用。 |
| [^19] | [Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis.](http://arxiv.org/abs/2305.16699) | 本论文提出在零样本语音合成中自动调整损失权衡的方法，无需超参数搜索。通过此方法，VITS-based模型的性能表现得到了显著提升，达到了最新的领先性能。 |
| [^20] | [Dual Bayesian ResNet: A Deep Learning Approach to Heart Murmur Detection.](http://arxiv.org/abs/2305.16691) | 本论文介绍了一种深度学习方法，双重贝叶斯ResNet，用于心脏杂音检测。其中一种模型将每个患者的记录分段成重叠的log mel音谱图进行二分类，第二种模型通过整合人口统计学数据和信号特征来提高杂音分类准确率。 |
| [^21] | [Future-conditioned Unsupervised Pretraining for Decision Transformer.](http://arxiv.org/abs/2305.16683) | 本文提出了一种基于未来信息的无监督预训练方法PDT，使得在没有奖励和次优离线数据上的无监督学习成为可能，通过为可能的未来分配回报值和采样未来嵌入进行在线微调。实验表明，PDT优于当前的无监督方法，并与有监督的学习方法相当竞争。 |
| [^22] | [Multiview Identifiers Enhanced Generative Retrieval.](http://arxiv.org/abs/2305.16675) | 该论文提出了一种新型的基于合成标识符的多视角标识符来增强生成式检索，从而提高了检索结果的准确性和多样性。 |
| [^23] | [A Unified Approach for Maximizing Continuous DR-submodular Functions.](http://arxiv.org/abs/2305.16671) | 本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。 |
| [^24] | [Higher Order Gauge Equivariant CNNs on Riemannian Manifolds and Applications.](http://arxiv.org/abs/2305.16657) | 本文介绍了一种在黎曼流形上的高阶规范等变卷积神经网络，能够在给定感受域内建模空间扩展的非线性交互，并保持对全局等度量的等变性。 |
| [^25] | [Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning.](http://arxiv.org/abs/2305.16646) | 本文提出了一个建模和预测框架，在少样本情况下使用语言模型的绝对推理能力来提高事件序列模型的预测精度，经过实验证实可以明显优于最先进的事件序列模型。 |
| [^26] | [Improving Position Encoding of Transformers for Multivariate Time Series Classification.](http://arxiv.org/abs/2305.16642) | 本文提出了一种新的绝对位置编码方法tAPE，以及一种相对位置编码的计算上高效的实现方法eRPE，旨在改进Transformer在多元时间序列分类中的性能。 |
| [^27] | [Universal Approximation and the Topological Neural Network.](http://arxiv.org/abs/2305.16639) | 本文介绍了一种拓扑神经网络，操作于Tychonoff拓扑空间而非有限维空间的数据，结合分布神经网络可识别随机过程路径中的长期依赖、重尾分布等特性，且证明了其可以任意逼近任何一致连续函数。 |
| [^28] | [Set-based Neural Network Encoding.](http://arxiv.org/abs/2305.16625) | 提出了一种能够集合化地编码神经网络参数的神经网络权重编码方法，并引入了一种逐层编码方案来考虑神经网络的分层计算结构。同时引入了“pad-chunk-encode”流水线进行神经网络层的高效编码处理，还提出了新的神经网络泛化性能预测任务。 |
| [^29] | [Pedestrian Trajectory Forecasting Using Deep Ensembles Under Sensing Uncertainty.](http://arxiv.org/abs/2305.16620) | 该研究提出了一种深度集成的方法，能够在考虑感知不确定性的情况下，预测行人运动轨迹及其不确定性。 |
| [^30] | [Confidence-Based Feature Imputation for Graphs with Partially Known Features.](http://arxiv.org/abs/2305.16618) | 本论文提出了一种新颖的信道置信度与伪置信度的特征插值方法，解决了高缺失特征率的图像学习任务中的性能下降问题。 |
| [^31] | [Physical Deep Reinforcement Learning: Safety and Unknown Unknowns.](http://arxiv.org/abs/2305.16614) | 本文提出了Phy-DRL，这是一个物理模型调整的深度强化学习框架。该框架有三个创新点，它们分别是: i)前瞻性的未知未知训练，ii)结合残差控制，以及iii)基于物理模型的神经网络编辑。Phy-DRL能够容忍未知干扰，保证安全和稳定，同时遵守Bellman方程和奖励相关的物理知识。 |
| [^32] | [A Slingshot Approach to Learning in Monotone Games.](http://arxiv.org/abs/2305.16610) | 本文提出了一种新的框架, 通过正则化游戏的支付或效用和更新投石索策略，无论是否存在噪声都能够实现在单调博弈中计算均衡。 |
| [^33] | [Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.](http://arxiv.org/abs/2305.16597) | 本文提出了一种基于神经架构搜索的参数高效微调大型预训练语言模型的方法，通过结构化和非结构化剪枝学习PET结构并在GLUE上进行实验验证，展示了该算法的有效性，探讨了PET架构设计选择对实际性能的影响。 |
| [^34] | [A Multi-Resolution Physics-Informed Recurrent Neural Network: Formulation and Application to Musculoskeletal Systems.](http://arxiv.org/abs/2305.16593) | 本文介绍了一种用于预测肌肉骨骼运动和 MSK 系统参数识别的多分辨率物理信息循环神经网络（MR PI-RNN），利用快速小波变换将混合频率的输入 sEMG 和输出关节运动信号分解为嵌套的多分辨率信号进行预测。 |
| [^35] | [The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model.](http://arxiv.org/abs/2305.16589) | 本文研究了强化学习中的模型鲁棒性以缩小模拟与真实差距，提出了一个名为“分布鲁棒值迭代”的基于模型的方法，可以优化最坏情况下的表现。 |
| [^36] | [Detecting Errors in Numerical Data via any Regression Model.](http://arxiv.org/abs/2305.16583) | 该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。 |
| [^37] | [Exploring Weight Balancing on Long-Tailed Recognition Problem.](http://arxiv.org/abs/2305.16573) | 研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。 |
| [^38] | [Accelerating Value Iteration with Anchoring.](http://arxiv.org/abs/2305.16569) | 本文提出了一种加速值迭代算法Anc-VI，采用了锚定机制，可加速Bellman一致性和最优性算子的计算。对于$\gamma\approx 1$或$\gamma=1$，Anc-VI速度为$\mathcal{O}(1/k)$，比标准VI更快。 |
| [^39] | [Unsupervised Embedding Quality Evaluation.](http://arxiv.org/abs/2305.16562) | 这项工作提出了评估嵌入质量的不同方法，关注如何以稳定的方式进行线性分离。从调查的文献和引入的新方法中，我们可以评估嵌入的质量，从而提高无监督学习的表现。(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.) |
| [^40] | [Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction.](http://arxiv.org/abs/2305.16559) | 本论文研究了类增量信息提取中分类器漂移如何导致遗忘的问题，并提出了四种解决方案来缓解分类器漂移。 |
| [^41] | [Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters.](http://arxiv.org/abs/2305.16557) | 本文介绍了一种基于树的扩散薛定谔桥算法(TreeDSB)来解决多元最优输运(mOT)的问题，并可以应用于高维设置如图像插值和贝叶斯融合。 |
| [^42] | [LANISTR: Multimodal Learning from Structured and Unstructured Data.](http://arxiv.org/abs/2305.16556) | LANISTR是一个新颖的基于注意力机制的框架，可从结构化和非结构化数据中进行学习，在挑战性数据集上表现优异。 |
| [^43] | [Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction.](http://arxiv.org/abs/2305.16546) | 本文比较了LSTM和BLSTM两种深度学习模型在电力消耗短期预测中的性能，通过四个数据集的结果表明BLSTM的表现更好。 |
| [^44] | [Inductive detection of Influence Operations via Graph Learning.](http://arxiv.org/abs/2305.16544) | 本文开发了一种新的归纳式学习框架，可以检测影响行动的操作。该框架能够确定与任何操作无关的指标，使用图学习来编码协调操纵的抽象标志，并在不同操作之间进行跨操作泛化，有望为预防影响行动提供帮助。 |
| [^45] | [A Score-Based Model for Learning Neural Wavefunctions.](http://arxiv.org/abs/2305.16540) | 本研究提供了一种新的优化框架，使用基于分数的神经网络获得量子多体基态性质，通过Langevin动力学进行采样，准确计算基态。 |
| [^46] | [Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression.](http://arxiv.org/abs/2305.16536) | 对比学习是一种表示学习技术，对于有监督的情况易于产生类坍塌，无监督情况下易于抑制类别相关的复杂特征；随机梯度下降方法偏向于寻找更简单的解决方案是导致这种现象的关键因素。 |
| [^47] | [Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization.](http://arxiv.org/abs/2305.16534) | 该论文提供了关于通过加权衰减训练的多输出ReLU神经网络的函数类型和相应的解决方案的新见解。 |
| [^48] | [Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation.](http://arxiv.org/abs/2305.16532) | 本文提出了一种新的基于反事实解释方法的框架来解释黑盒DRL所作的决策，并且在实验中展示了该解释框架的可行性和有效性。 |
| [^49] | [Bi-fidelity Variational Auto-encoder for Uncertainty Quantification.](http://arxiv.org/abs/2305.16530) | 本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。 |
| [^50] | [Extending Explainable Boosting Machines to Scientific Image Data.](http://arxiv.org/abs/2305.16526) | 本文提出将可解释的提升机器算法应用于科学图像数据，通过在冷原子孤子图像数据上的应用，证明了这一方法的可行性。 |
| [^51] | [Label Agnostic Pre-training for Zero-shot Text Classification.](http://arxiv.org/abs/2305.16521) | 本文旨在探究改进预训练语言模型的泛化能力，提高其在零样本情境下的文本分类表现。通过引入隐式和显式预训练策略，注入方面级别的理解，以建立任务层次的表示。 |
| [^52] | [Sliding Window Sum Algorithms for Deep Neural Networks.](http://arxiv.org/abs/2305.16513) | 本文介绍了一系列针对深度神经网络训练和推理的通用矢量化滑动和算法，提高了计算速度，比常用的算法更有效，能够表达池化和卷积原语。 |
| [^53] | [Most Neural Networks Are Almost Learnable.](http://arxiv.org/abs/2305.16508) | 本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。 |
| [^54] | [Reward-Machine-Guided, Self-Paced Reinforcement Learning.](http://arxiv.org/abs/2305.16505) | 本研究提出一种基于奖励机制来指导自适应强化学习的算法，该算法可信赖地实现最优行为，在复杂任务中提高了强化学习的数据效率。 |
| [^55] | [On the Tool Manipulation Capability of Open-source Large Language Models.](http://arxiv.org/abs/2305.16504) | 本研究探讨了如何通过训练使用示例、上下文演示和生成样式规则来加强开源LLMs以达到与封闭型API的工具操作性能同等甚至更优的效果，并通过ToolBench测试得出了实验结果，同时本文还证明了改进的开源LLMs的鲁棒性。 |
| [^56] | [Strategic Classification under Unknown Personalized Manipulation.](http://arxiv.org/abs/2305.16501) | 探讨了在未知且个性化操纵影响下的战略分类问题，提出了一个交互模型，引入了个性化的定义，旨在解决强化学习中策略操纵问题。 |
| [^57] | [AD-NEV: A Scalable Multi-level Neuroevolution Framework for Multivariate Anomaly Detection.](http://arxiv.org/abs/2305.16497) | AD-NEv提出了一种可扩展的多级优化神经进化框架，用于多元时间序列异常检测，通过协同优化特征子空间、模型架构和模型权重的方法，表现出比当前最先进的方法更好的检测精度和计算效率。 |
| [^58] | [SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise.](http://arxiv.org/abs/2305.16491) | 该论文介绍了一种新的时间序列分析方法，即SAMoSSA。该方法综合了多元奇异谱分析和自回归分析，在学习时间序列中的确定性和随机性成分方面具有良好的理论保证。 |
| [^59] | [Batch Model Consolidation: A Multi-Task Model Consolidation Framework.](http://arxiv.org/abs/2305.16484) | 本文提出了批次模型整合（BMC）来支持更现实的连续学习，它通过在正则化阶段训练多个专家模型来学习一组不相交的任务，并在整合阶段将多个专家模型整合为一个模型。 |
| [^60] | [Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks.](http://arxiv.org/abs/2305.16483) | 本文提出了一种在混合系统中通过增强数据样本进行学习的方法，应用于排队网络问题，能够显著提高学习效率和降低样本复杂度。 |
| [^61] | [Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks.](http://arxiv.org/abs/2305.16475) | 本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。 |
| [^62] | [FairDP: Certified Fairness with Differential Privacy.](http://arxiv.org/abs/2305.16474) | FairDP是一种同时确保差分隐私和公平性的新型机制，通过独立为不同的个体群体训练模型，在训练过程中逐步整合来自群体模型的知识，制定综合模型以平衡隐私、效用和公平性的下游任务。相比现有方法，FairDP展示了更好的模型效益、隐私和公平性的权衡。 |
| [^63] | [Bias, Consistency, and Partisanship in U.S. Asylum Cases: A Machine Learning Analysis of Extraneous Factors in Immigration Court Decisions.](http://arxiv.org/abs/2305.16471) | 本文使用机器学习分析美国避难案件中的个人和系统性偏见，发现美国避难决定往往受到与案件实质无关的因素的影响，而且是否授予申请人庇护主要取决于政治环境和审判法官的个体变异。 |
| [^64] | [Measuring the Effect of Influential Messages on Varying Personas.](http://arxiv.org/abs/2305.16470) | 研究了评估不同群体对影响力信息的反应的任务和所创建的数据集，突出了新任务在建模中引入了个性化，预测了每个反应的情感极性和强度，并让评估和应用更加可靠。 |
| [^65] | [Bayesian Reinforcement Learning for Automatic Voltage Control under Cyber-Induced Uncertainty.](http://arxiv.org/abs/2305.16469) | 本文提出了一种基于贝叶斯强化学习的数据驱动方法，解决网络攻击引起的不确定性对自动电压控制的影响，其在部分可观测的马尔可夫决策问题中实现了自动控制，并能够自动寻找探索和开发的阈值。 |
| [^66] | [Pair-Variational Autoencoders (PairVAE) for Linking and Cross-Reconstruction of Characterization Data from Complementary Structural Characterization Techniques.](http://arxiv.org/abs/2305.16467) | 本文介绍了一种机器学习模型 PairVAE，可以训练多种互补结构表征技术的数据，使其能够在不同技术之间交叉重建表征数据。 |
| [^67] | [Optimized Custom Dataset for Efficient Detection of Underwater Trash.](http://arxiv.org/abs/2305.16460) | 本文提出了一种自定义数据集和有效检测方法，旨在通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。 |
| [^68] | [The Representation Jensen-Shannon Divergence.](http://arxiv.org/abs/2305.16446) | 本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。 |
| [^69] | [Representation Transfer Learning via Multiple Pre-trained models for Linear Regression.](http://arxiv.org/abs/2305.16440) | 本文提出了一种基于表示迁移的学习方法，在给定很少数样本的情况下，通过提供一组在可能不同的数据领域上训练的预训练回归模型，来构建目标模型，使用这种方法可以提高模型的样本复杂度。 |
| [^70] | [Neural (Tangent Kernel) Collapse.](http://arxiv.org/abs/2305.16427) | 本文介绍了神经切向核（NTK）和神经崩溃（NC）之间的关系，证明了在具有块状NTK的DNN中会出现NC，并通过大规模实验支持理论的正确性。 |
| [^71] | [SketchOGD: Memory-Efficient Continual Learning.](http://arxiv.org/abs/2305.16424) | SketchOGD提出了一种内存高效的解决灾难性遗忘的方法，通过采用在线草图算法，将模型梯度压缩为固定大小的矩阵，从而改进了现有的算法——正交梯度下降（OGD）。 |
| [^72] | [Federated Neural Compression Under Heterogeneous Data.](http://arxiv.org/abs/2305.16416) | 本文介绍了在异构数据下的联邦神经压缩问题，提出了一个分布式源模型和个性化熵模型的解决方案，在联邦设置中，使用全局共享表示优于本地方法。 |
| [^73] | [GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds.](http://arxiv.org/abs/2305.16404) | 本文提出了一种名为GrowSP的无监督方法来进行3D场景中每个点复杂语义类别的识别和分割，方法通过逐步增长超级点的大小来发现3D语义元素，并优于所有无监督基线。 |
| [^74] | [Support Vector Machine Guided Reproducing Kernel Particle Method for Image-Based Modeling of Microstructures.](http://arxiv.org/abs/2305.16402) | 本文基于支持向量机引导再生核颗粒方法，提出了一种自动化数字表示复合材料的构建方案，并且引入了一个新的界面修改的再生核颗粒法以适当近似弱不连续性。 |
| [^75] | [ADLER -- An efficient Hessian-based strategy for adaptive learning rate.](http://arxiv.org/abs/2305.16396) | 该论文提出了一种基于Hessian矩阵逼近的自适应学习率策略，该方法计算量较小且在分类任务上效果显著，可广泛用于CNN模型中。 |
| [^76] | [Using neural networks to model Main Belt Asteroid albedos as a function of their proper orbital elements.](http://arxiv.org/abs/2305.16392) | 本文利用神经网络以小行星 Proper Orbital Elements 为函数建模尺寸难以观测的小行星反照率，将 NEOWISE 任务中的可见和红外反照率建模为函数，并发现它们与小行星在带中的位置显着相关，相比仅采用平均值模型，集合模型将反照率误差降低了约 37％。 |
| [^77] | [Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems.](http://arxiv.org/abs/2305.16391) | 本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。 |
| [^78] | [DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models.](http://arxiv.org/abs/2305.16381) | 本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。 |
| [^79] | [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.](http://arxiv.org/abs/2305.16380) | 本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。 |
| [^80] | [Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning.](http://arxiv.org/abs/2305.16379) | 本研究发现，对于数据增强在视觉强化学习中的有效性，空间多样性和轻微的困难度不可或缺。并提出了一种新的DA操作——Rand PR，它提供了丰富的空间多样性和最小的困难度，已经在多种数据上得到了有效性验证。 |
| [^81] | [Data Topology-Dependent Upper Bounds of Neural Network Widths.](http://arxiv.org/abs/2305.16375) | 本文引入了数据拓扑相关的神经网络宽度上界，并通过拓扑方法证明了三层ReLU网络的普适逼近性质。 |
| [^82] | [DeepGate2: Functionality-Aware Circuit Representation Learning.](http://arxiv.org/abs/2305.16373) | 本文介绍了DeepGate2, 一个新的功能感知学习框架，其通过利用成对真值表差异作为训练监督，明确考虑电路功能，来提高电路表示学习的学习效果和效率。 |
| [^83] | [Metrics for quantifying isotropy in high dimensional unsupervised clustering tasks in a materials context.](http://arxiv.org/abs/2305.16372) | 该论文提出了一种新的对聚类等向性度量的实现方法，并使用分数各向异性扩展了这些度量来检查聚类的平均等向性。通过量化不同材料结构数据库表示的核逼近函数对结果聚类的影响，演示了这种度量的实际应用。 |
| [^84] | [Stecformer: Spatio-temporal Encoding Cascaded Transformer for Multivariate Long-term Time Series Forecasting.](http://arxiv.org/abs/2305.16370) | Stecformer是一种处理多元长期时间序列预测的方法，提出了一种有效的空间-时间编码器和级联解码预测器（CDP），在多个基准数据集上实现了最先进的性能，为多元长期时间序列预测提供了一种有前途的方法。 |
| [^85] | [Neural incomplete factorization: learning preconditioners for the conjugate gradient method.](http://arxiv.org/abs/2305.16368) | 本文提出了一种名为神经不完全分解的新方法，利用自监督训练的图神经网络生成适用于特定问题域的有效预处理器。其通过替换传统手工预处理器显着提高了收敛和计算效率，在合成和真实问题上进行的实验均表现出竞争力。 |
| [^86] | [Role-Play with Large Language Models.](http://arxiv.org/abs/2305.16367) | 本文将对话代理行为描述为角色扮演，以避免赋予其人类特征，在此基础上研究代理行为中的欺骗和自我意识。 |
| [^87] | [Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving.](http://arxiv.org/abs/2305.16366) | 本文提出了一个基于子目标的演示学习框架，通过将基于子目标的学习方法与扩散模型相结合，提高演示的可理解性，并提高LLMs在形式定理证明中的吞吐量。 |
| [^88] | [E2EAI: End-to-End Deep Learning Framework for Active Investing.](http://arxiv.org/abs/2305.16364) | 本文提出了一个面向主动投资的端到端深度学习框架（E2EAI），包含了因子选择、因子组合、股票选择和投资组合构建的整个过程，并通过对真实股票市场数据的实验证明了其有效性。 |
| [^89] | [Ensemble Synthetic EHR Generation for Increasing Subpopulation Model's Performance.](http://arxiv.org/abs/2305.16363) | 本文提出了一种利用生成模型集成合成数据的方法，以提高训练机器学习模型在代表不足的亚群体的性能。 |
| [^90] | [An Experimental Investigation into the Evaluation of Explainability Methods.](http://arxiv.org/abs/2305.16361) | 这篇论文比较了14种不同的评估指标在对9种目前最先进的可解释性人工智能（XAI）方法和三种虚拟方法进行应用时的效果，给出了高度相关结果，指出了存在潜在冗余。此外，还展示了基线超参数对评估指标值产生显著影响。 |
| [^91] | [Modeling Task Relationships in Multi-variate Soft Sensor with Balanced Mixture-of-Experts.](http://arxiv.org/abs/2305.16360) | 本文提出的平衡专家混合模型 (BMoE) 通过多门专家混合模块 (MMoE) 和 任务梯度平衡 (TGB) 模块来描述任务关系和平衡训练过程，在解决数据效率和负面传递问题方面有很好的效果。 |
| [^92] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | 介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。 |
| [^93] | [WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity.](http://arxiv.org/abs/2305.16351) | 本文提出了一种名为WeiAvg的联邦学习模型聚合方法，通过强调来自高多样性客户端的更新并减少来自低多样性客户端的影响，提高了联邦模型的质量和性能。 |
| [^94] | [Lexinvariant Language Models.](http://arxiv.org/abs/2305.16349) | 本文讨论了一种新型的语言模型，称为Lexinvariant语言模型，该模型不需要任何固定标记嵌入，完全依赖上下文中标记的共现和重复。作者证明可以构建一个lexinvariant LM，以多项式方式与上下文长度成比例地收敛到真实语言模型，其常量因子在词汇表大小下为次线性。 |
| [^95] | [Machine learning-based characterization of hydrochar from biomass: Implications for sustainable energy and material production.](http://arxiv.org/abs/2305.16348) | 本文提出了一种使用机器学习表征生物质水热碳化产物特性的方法，能够准确预测在不同处理条件下从各种生物质来源产生的水炭的质量和数量，为生物质的可持续能源和材料生产做出贡献。 |
| [^96] | [Artificial Intelligence-Based Methods for Precision Medicine: Diabetes Risk Prediction.](http://arxiv.org/abs/2305.16346) | 本文分析了现有文献中基于人工智能的糖尿病风险预测模型，在单模态和多模态模型中均表现突出，但外部验证有限，解释性方法需要改进。 |
| [^97] | [TaxoKnow: Taxonomy as Prior Knowledge in the Loss Function of Multi-class Classification.](http://arxiv.org/abs/2305.16341) | 本文研究在平面分类器的学习算法中集成层次分类作为先验知识的有效性，实验结果表明先验知识可以显著提高学习者的性能，并在半监督和完全监督的情况下都获得了不错的结果。 |
| [^98] | [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model.](http://arxiv.org/abs/2305.16340) | 本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。 |
| [^99] | [Think Before You Act: Decision Transformers with Internal Working Memory.](http://arxiv.org/abs/2305.16338) | 该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。 |
| [^100] | [Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering.](http://arxiv.org/abs/2305.16335) | 本文提出了一种健壮短文本聚类（RSTC）模型，通过自适应最优输运的伪标签生成，以及基于类和实例的对比学习的健壮表示学习，帮助提高对不平衡和噪音数据的鲁棒性。 |
| [^101] | [Text Generation with Speech Synthesis for ASR Data Augmentation.](http://arxiv.org/abs/2305.16333) | 本研究探索文本增广对ASR的影响，使用大规模预训练的神经网络来生成合成文本，并通过文本到语音系统转换为合成语音，实验发现，使用神经网络的文本增广方法能够有效提高ASR准确度，可以作为改进ASR系统的一种可行工具。 |
| [^102] | [Continual Learning through Human-Robot Interaction -- Human Perceptions of a Continual Learning Robot in Repeated Interactions.](http://arxiv.org/abs/2305.16332) | 本研究结合机器人和连续学习模型，通过人机交互的方式与60名参与者实验，结果表明使用连续学习可以提高机器人的能力，参与者更倾向于与其进行反复交互，并提供更多的反馈信息。 |
| [^103] | [Prompt Engineering for Transformer-based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues.](http://arxiv.org/abs/2305.16330) | 本文使用提示工程策略创建了基于向量的化学搜索方法，可识别出与查询功能类似但在结构上不同的分子。 |
| [^104] | [Semantic Composition in Visually Grounded Language Models.](http://arxiv.org/abs/2305.16328) | 本论文研究了视觉上下文语言模型中的语义组合能力，提出了新的组合视觉问答基准，句法神经模块蒸馏等方法以提高组合能力，并探索了对图像字幕模型的因果追踪以定位重要神经表示。 |
| [^105] | [Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.](http://arxiv.org/abs/2305.16326) | 本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。 |
| [^106] | [Graph Neural Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic Uncertainty on Energy and Forces.](http://arxiv.org/abs/2305.16325) | 本文提出了一个完整的框架来培训和重新校准图神经网络合奏模型，以产生带有校准不确定性估计的能量和力的准确预测，可以应用于材料的结构优化和分子动力学模拟。 |
| [^107] | [Detecting Concept Drift for the reliability prediction of Software Defects using Instance Interpretation.](http://arxiv.org/abs/2305.16323) | 该论文旨在通过识别未标记的简化和重新采样数据解释中的变化来直接识别概念漂移的点，以开发可靠的 JIT-SDP 模型。此举将有助于解决类别不平衡现象所带来的风险问题，并提供一种可行的方法对模型的稳定性进行评估。 |
| [^108] | [Diversity-Aware Coherence Loss for Improving Neural Topic Models.](http://arxiv.org/abs/2305.16199) | 本文提出了一种多样性感知的相干性损失，可以帮助神经主题模型在保持高多样性同时，更好地学习语料库级别的连贯性分数。 |
| [^109] | [Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration.](http://arxiv.org/abs/2305.16173) | 本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。 |
| [^110] | [On the Identifiability of Markov Switching Models.](http://arxiv.org/abs/2305.15925) | 本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。 |
| [^111] | [Unifying gradient regularization for Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2305.15811) | 本研究提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题，综合效果和效率优于几种现有方法。 |
| [^112] | [Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting.](http://arxiv.org/abs/2305.15786) | 本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。 |
| [^113] | [Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data.](http://arxiv.org/abs/2305.15722) | 本文比较了使用不同预训练Transformer模型的印地语-英语代码混合数据的性能表现，以提高情感分析、情绪识别和仇恨言论识别等自然语言处理任务的性能。 |
| [^114] | [Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems.](http://arxiv.org/abs/2305.15534) | 为了改善搜索和推荐系统中的代表性，我们提出了一种端到端的多样化方法，并在Pinterest平台上实验和部署了可扩展的多样化机制，以改善美容和时尚类别中不同肤色的代表性。 |
| [^115] | [Improving few-shot learning-based protein engineering with evolutionary sampling.](http://arxiv.org/abs/2305.15441) | 本文提出了一种利用少量数据进行新型蛋白质设计的方法，通过半监督转移学习和进化马尔可夫蒙特卡罗链采样算法，更有效地探索适应度景观，从而加速昂贵的湿实验测试周期。 |
| [^116] | [Local SGD Accelerates Convergence by Exploiting Second Order Information of the Loss Function.](http://arxiv.org/abs/2305.15013) | 本文研究了使用本地 SGD 更新的有效性，证明了在 IID 数据方案中，L-SGD 能够探索并利用损失函数的二阶信息以实现更快速的收敛。 |
| [^117] | [An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation.](http://arxiv.org/abs/2305.14704) | 本文推导和评估了四种贝叶斯批次赌博算法，用于自适应确定流量分配，全面评估了这些算法的可信度、敏感性和后悔度，仿真结果表明这些基于批次的贝叶斯算法是有效的。 |
| [^118] | [Sequence Modeling is a Robust Contender for Offline Reinforcement Learning.](http://arxiv.org/abs/2305.14550) | 序列建模是离线强化学习中比Q-Learning和Imitation Learning更适合在稀疏奖励和低质量数据设置下的选择，在任务范围增加时，序列建模和模仿学习更可取。 |
| [^119] | [Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces.](http://arxiv.org/abs/2305.14516) | Chakra是一种开放的图形模式，用于标准化工作负载规范，捕捉关键操作和依赖项，以推进性能基准和协同设计，同时提供一组工具和能力，以便在模拟器和仿真器中实现未来系统的协同设计。 |
| [^120] | [Federated Variational Inference: Towards Improved Personalization and Generalization.](http://arxiv.org/abs/2305.13672) | 本文提出了一种名为联邦变分推断的算法，用于跨设备联邦学习中的个性化和泛化，并在图像分类中超越了现有技术。 |
| [^121] | [SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres.](http://arxiv.org/abs/2305.13617) | 这篇论文提出了一种称为SPEECH的模型，它使用能量建模来表示复杂的事件结构，并使用超球来表示事件类别。实验结果表明，SPEECH在事件检测和事件关系抽取任务中表现出卓越的性能。 |
| [^122] | [Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint.](http://arxiv.org/abs/2305.13599) | 本文提出了一种名为DR的灵活的方法，它通过不对称的学习率来解决由合作博弈引发的退化问题，该方法能够在两个基准测试中显著改善表现。 |
| [^123] | [Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D.](http://arxiv.org/abs/2305.13266) | 该论文提出了一种粗到细的方法来解决分子生成中的组合优化问题，并采用分层扩散模型和HIPER算法生成结构的有效性和多样性，表现优于现有方法。 |
| [^124] | [INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search.](http://arxiv.org/abs/2305.13164) | INVICTUS是一个使用离线强化学习和搜索的模型，自动生成逻辑最小化启发式算法综合配方以优化电路面积和时延等指标。 |
| [^125] | [DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images.](http://arxiv.org/abs/2305.12621) | 该论文提出了一种名为DermSynth3D的新框架，它使用可微分渲染器将皮肤病变模式混合到人体三维纹理网格上并生成逼真的二维皮肤镜像图像，同时提供对应的密集注释以进行语义分割。 |
| [^126] | [Multi-Head State Space Model for Speech Recognition.](http://arxiv.org/abs/2305.12498) | 本文提出了一种多头状态空间（MH-SSM）模型，它能够用于语音识别任务并在LibriSpeech数据集上表现出的新的性能，是变压器变换器的优秀替代方案。同时, MH-SSM层的引入也提高了变压器块的性能，达到了现有最新水平。 |
| [^127] | [Nine tips for ecologists using machine learning.](http://arxiv.org/abs/2305.10472) | 本论文介绍了九个技巧来帮助生态学家实施机器学习模型，这些技巧针对分类问题，旨在识别开发机器学习模型中的常见错误、陷阱或挑战，并提供了解决方法。 |
| [^128] | [sustain.AI: a Recommender System to analyze Sustainability Reports.](http://arxiv.org/abs/2305.08711) | sustain.AI是一个智能的、上下文感知的推荐系统，可以帮助审计师、金融投资者以及广大公众高效地分析公司的可持续性报告，并通过与GRI标准匹配来提供更好的推荐精度。 |
| [^129] | [Explanation-based Finetuning Makes Models More Robust to Spurious Cues.](http://arxiv.org/abs/2305.04990) | 本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。 |
| [^130] | [Understanding cirrus clouds using explainable machine learning.](http://arxiv.org/abs/2305.02090) | 本文使用机器学习模型研究了卷积云的驱动因素与云属性之间的关系，发现气象和气溶胶条件可以预测卷积云属性。功能属性方法还可以量化这些关系，了解哪些因素可以影响卷积云中的冰晶数浓度和冰水含量。 |
| [^131] | [Is deep learning a useful tool for the pure mathematician?.](http://arxiv.org/abs/2304.12602) | 本文是一篇介绍纯数学家使用深度学习工具进行研究的个人和非正式叙述。 |
| [^132] | [Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance.](http://arxiv.org/abs/2304.11127) | 该论文介绍了一种广泛使用的贝叶斯优化方法 Tree-structured Parzen estimator (TPE)，并对其控制参数的作用和算法直觉进行了讨论和分析，提供了一组推荐设置并证明其能够提高TPE的性能表现。 |
| [^133] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^134] | [Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model.](http://arxiv.org/abs/2304.05516) | 本文提出了一个个性化隐私保护联邦学习框架，可在保证本地隐私的同时实现强中心隐私保证，并利用Shuffle模型进行隐私扩增。 |
| [^135] | [Predicting quantum chemical property with easy-to-obtain geometry via positional denoising.](http://arxiv.org/abs/2304.03724) | 该论文提出了一种方法，利用位置去噪预测易得几何结构的量子化学性质，可以用相对容易获得的几何结构，精确预测性质，在分子性质以及化学反应性质的预测任务中都表现优秀。 |
| [^136] | [Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks.](http://arxiv.org/abs/2304.03408) | 本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。 |
| [^137] | [Convergence of alternating minimisation algorithms for dictionary learning.](http://arxiv.org/abs/2304.01768) | 本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。 |
| [^138] | [Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning.](http://arxiv.org/abs/2304.00195) | 该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。 |
| [^139] | [Self-Refine: Iterative Refinement with Self-Feedback.](http://arxiv.org/abs/2303.17651) | 自我反馈迭代精炼是一种无需监督学习或加强学习的LLMs初始输出优化方法，优于直接生成，被证实在7个不同任务中表现更好。 |
| [^140] | [Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference.](http://arxiv.org/abs/2303.10472) | 本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。 |
| [^141] | [Visual Information Matters for ASR Error Correction.](http://arxiv.org/abs/2303.10160) | 这篇论文提出了用于改善自动语音识别（ASR）输出的ASR错误纠正（EC）技术，该技术通过融入视觉信息，使用门控融合和图像标题作为提示的方法，提高了EC的性能。同时，本文提供了一个基准数据集Visual-ASR-EC。 |
| [^142] | [A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games.](http://arxiv.org/abs/2303.09716) | 本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。 |
| [^143] | [A Comprehensive Study on Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2303.08302) | 本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。 |
| [^144] | [Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning.](http://arxiv.org/abs/2303.08250) | 本文提出了一种在Vision Transformer中学习成长人工海马的方法，以实现弹性终身学习。通过神经架构搜索进行维护，选取多头自注意力块中的最终线性投影层进行ArtiHippo的实现和成长。 |
| [^145] | [GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning.](http://arxiv.org/abs/2303.05193) | 本文提出了一种名为GOATS的方法，使用目标采样自适应课程强化学习技术，通过插值位置目标和数量目标的分布创建学习过程中的课程来解决机器人舀取任务中的位置目标和水量目标问题，取得了比基线更好的表现。 |
| [^146] | [Bayesian inference with finitely wide neural networks.](http://arxiv.org/abs/2303.02859) | 本文通过多元Edgeworth展开，提出用微分形式表示非高斯分布，来模拟有限宽度神经网络的非高斯后验分布。 |
| [^147] | [Hallucinated Adversarial Control for Conservative Offline Policy Evaluation.](http://arxiv.org/abs/2303.01076) | 本文提出了基于幻想对抗控制的HAMBO算法，可用于离线策略评估，并且能够得出有效的策略表现下限估计。 |
| [^148] | [Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization.](http://arxiv.org/abs/2302.14510) | 本文提出了在贝叶斯优化中使用贝叶斯内核张量分解作为代理模型的方法，以学习具有复杂特征的函数。 |
| [^149] | [Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach.](http://arxiv.org/abs/2302.10798) | 本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。 |
| [^150] | [On Calibrating Diffusion Probabilistic Models.](http://arxiv.org/abs/2302.10688) | 本文发现了数据分数随机反向过程是一个鞅，提出了一种简单的方法，用于校准任意预先训练的DPM，有效减小模型的得分匹配损失，增加模型似然的下限，并提供了一般校准指南。 |
| [^151] | [$\omega$PAP Spaces: Reasoning Denotationally About Higher-Order, Recursive Probabilistic and Differentiable Programs.](http://arxiv.org/abs/2302.10636) | 本文介绍了$\omega$PAP空间的范畴，可为大多数实用的概率和可微程序赋予意义，同时还证明了有关自动微分正确性和概率程序中迹密度函数的可微性等结果。 |
| [^152] | [Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective.](http://arxiv.org/abs/2302.10021) | 本文研究了医用口罩对情感识别的影响，并展示了整体人体输入优于单纯面部遮挡的卓越性。文章提出了一种深度学习模型，通过分别处理面部和身体特征并融合预测分数来提高情感识别准确性。 |
| [^153] | [PAC-Bayesian Generalization Bounds for Adversarial Generative Models.](http://arxiv.org/abs/2302.08942) | 将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。 |
| [^154] | [Algorithm Selection for Deep Active Learning with Imbalanced Datasets.](http://arxiv.org/abs/2302.07317) | 本论文提出了适用于不平衡数据集下的深度主动学习的自适应算法选择策略TAILOR，它在多类和多标签应用的实验中取得了与候选算法相当或更高的准确性。 |
| [^155] | [I$^2$SB: Image-to-Image Schr\"odinger Bridge.](http://arxiv.org/abs/2302.05872) | 提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。 |
| [^156] | [Achieving acceleration despite very noisy gradients.](http://arxiv.org/abs/2302.05515) | AGNES是一种能在平滑凸优化任务中实现加速的算法，即使梯度估计的信噪比很小，它也能表现出优异的性能，在深度学习中的应用效果显著优于动量随机梯度下降和Nesterov方法。 |
| [^157] | [Riemannian Flow Matching on General Geometries.](http://arxiv.org/abs/2302.03660) | 本文提出了一种名为黎曼流匹配的方法，可以在一般几何上训练连续标准化流，并在高维度数据上具有优势。 |
| [^158] | [On the Efficacy of Differentially Private Few-shot Image Classification.](http://arxiv.org/abs/2302.01190) | 本文通过一系列实验研究了差分隐私少样本图像分类模型的准确性和易受攻击性，揭示了样本数、隐私级别、模型架构、下游数据集以及可学习参数子集等因素对分类效果的影响。 |
| [^159] | [SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling.](http://arxiv.org/abs/2302.00861) | SimMTM是一个简单的序列模型预训练框架，通过加权聚合多个流形之外的邻居来恢复掩码时间点，从而简化重构任务。 |
| [^160] | [Versatile Energy-Based Probabilistic Models for High Energy Physics.](http://arxiv.org/abs/2302.00695) | 本文提出了一个多功能的能量概率模型，用于描述高能物理事件，可用于参数化的事件生成，异常信号探测以及粒子识别。 |
| [^161] | [Distillation Policy Optimization.](http://arxiv.org/abs/2302.00533) | 本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。 |
| [^162] | [Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression.](http://arxiv.org/abs/2302.00257) | 本文研究发现，隐式正则化可对于稀疏线性回归的良性过拟合现象具有促进作用，并给出了一个模型参数化形式，结合了$\ell_1$和$\ell_2$内插器的优点，通过梯度下降训练可得到一个接近最优测试损失的内插器。 |
| [^163] | [End-to-End Full-Atom Antibody Design.](http://arxiv.org/abs/2302.00203) | dyMEAN是一个端到端全原子模型，可以根据表位和不完整的抗体序列进行抗体设计，在处理全原子时能够处理可变大小的蛋白残基。旨在解决现有学习方法的两个主要问题：只处理抗体设计过程中的某个子任务和无法捕捉全原子几何形状。 |
| [^164] | [Grounding Language Models to Images for Multimodal Inputs and Outputs.](http://arxiv.org/abs/2301.13823) | 该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。 |
| [^165] | [STEEL: Singularity-aware Reinforcement Learning.](http://arxiv.org/abs/2301.13152) | 这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。 |
| [^166] | [Distributed Stochastic Optimization under a General Variance Condition.](http://arxiv.org/abs/2301.12677) | 这项研究通过重新审视联邦平均算法，在最小假设下对分布式非凸目标进行了随机优化，建立了仅满足随机梯度温和条件的收敛结果。 |
| [^167] | [FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model.](http://arxiv.org/abs/2301.12407) | FedEBA+是一种新的联邦学习算法，它采用公平聚合方案和对齐更新方法，在同时提高全局模型性能的同时提高公平性。实验证明FedEBA+优于其他公平性联邦学习方法。 |
| [^168] | [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits.](http://arxiv.org/abs/2301.12357) | 本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。 |
| [^169] | [Does Federated Learning Really Need Backpropagation?.](http://arxiv.org/abs/2301.12195) | 本文提出一种不需要反向传播的联邦学习框架BAFFLE，该框架使用多个正向过程估计梯度，具有高内存效率，容易适应上传带宽，与硬件优化和模型量化/修剪兼容，适用于受信任的执行环境。 |
| [^170] | [Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers.](http://arxiv.org/abs/2212.08700) | 本文研究了语言模型处理“few-”类型量词的能力，结果显示所有模型对这种量词都表现不佳，且较大的模型表现更差。这种反比例缩放的现象表明大型模型越来愈反映在线人类处理，而不是离线处理。这可能挑战使用语言模型作为自然语言系统基础的做法。 |
| [^171] | [DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue.](http://arxiv.org/abs/2212.08054) | 本文介绍了一种面向任务型对话的双重对齐多语言解析器，可以大幅提高多语言和代码切换语义解析系统的零-shot性能，提高mBERT转移性能。 |
| [^172] | [Simulator-Based Self-Supervision for Learned 3D Tomography Reconstruction.](http://arxiv.org/abs/2212.07431) | 这是一种能够利用计算机断层成像模拟器进行自我监督学习的三维断层成像重建方法，相比于之前的方法，不需要参考重建以进行训练，结果保真度更高且速度更快。 |
| [^173] | [Maximal Initial Learning Rates in Deep ReLU Networks.](http://arxiv.org/abs/2212.07295) | 本文针对深度学习中的学习率问题，提出了最大初始学习率的概念，并发现其行为与训练后期的最大学习率不同。我们得出结论：在一定条件下，最大初始学习率可以很好地预测为深度×宽度的幂次。 |
| [^174] | [Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator.](http://arxiv.org/abs/2212.06751) | 本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。 |
| [^175] | [Nonparametric Masked Language Modeling.](http://arxiv.org/abs/2212.01349) | NPM是第一个使用非参数分布替换softmax的遮蔽语言模型，可以更好地处理稀有模式和预测罕见或几乎未见过的单词，并在16项任务上超过了更大的参数模型。 |
| [^176] | [Investigation of Proper Orthogonal Decomposition for Echo State Networks.](http://arxiv.org/abs/2211.17179) | 本研究探讨了适用于回声状态网络的适当正交分解策略，该策略可通过找到等效性的低阶表示形式来替换高维的ESN。结果表明，基于POD的ESN显著减少所需的状态数，同时保持良好的性能，使其在计算时间和鲁棒性方面更加高效。 |
| [^177] | [Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning.](http://arxiv.org/abs/2211.16078) | 本文提出了一种用于离线强化学习的多源数据行为估计方法，通过潜变量模型推断策略来克服数据异构性导致的行为错误。 |
| [^178] | [c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization.](http://arxiv.org/abs/2211.14411) | 本文提出了约束TPE（c-TPE）方法，是树形Parzen估计器（TPE）的扩展，可有效处理在性能要求之上施加的约束限制，实验证明在81个昂贵的HPO设置中表现出最佳性能排名。 |
| [^179] | [UNSAT Solver Synthesis via Monte Carlo Forest Search.](http://arxiv.org/abs/2211.12581) | 介绍了使用MCFS算法合成UNSAT求解器的方法，算法可用于解决包括SAT公式不可满足性证明、可满足SAT公式解的数量计数和混合整数规划的最优解问题，并利用合成森林构建算法和合成MDP类来避免构建候选树森林的问题。 |
| [^180] | [Neural networks trained with SGD learn distributions of increasing complexity.](http://arxiv.org/abs/2211.11567) | 本文证明了随机梯度下降算法训练的神经网络在学习期间会出现分布式简单性偏差（DSB），即最初使用低阶输入统计来分类输入，只有在训练后期才利用更高阶的统计信息。 |
| [^181] | [Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations.](http://arxiv.org/abs/2211.08794) | 本文提出了一种利用多视角压缩表示降低预训练语言模型微调过程中过拟合问题的方法，经过测试在低资源NLP任务中表现良好。 |
| [^182] | [Scalar Invariant Networks with Zero Bias.](http://arxiv.org/abs/2211.08486) | 本文证明了在解决许多图像任务(例如图像分类)时可以忽略偏置，并且零偏置神经网络在实际图像分类任务中表现良好，同时具有标量 (乘法) 不变性，从而在改变对比度时仍能保持预测不变。 |
| [^183] | [PAD-Net: An Efficient Framework for Dynamic Networks.](http://arxiv.org/abs/2211.05528) | PAD-Net是一个部分动态网络的框架，将冗余的动态参数转换为静态参数，提高了动态网络的效率和适用性。 |
| [^184] | [Scalable Modular Synthetic Data Generation for Advancing Aerial Autonomy.](http://arxiv.org/abs/2211.05335) | 这篇论文探索了提高空中自主技术进步的一个重要问题：如何生成大量的空中数据集以训练机器学习模型。该研究使用合成数据并利用模拟环境和数据增强来解决现有工具的局限性，提高数据生成工作流程的可扩展性和泛化能力。 |
| [^185] | [Highly over-parameterized classifiers generalize since bad solutions are rare.](http://arxiv.org/abs/2211.03570) | 本文研究发现，在过度参数化的情况下，零训练误差的全局最小值中“坏”方案的占比随训练数据的增加而指数级递减，并能解释高度参数化的神经网络具有出乎意料的良好泛化能力。 |
| [^186] | [Discussion of Features for Acoustic Anomaly Detection under Industrial Disturbing Noise in an End-of-Line Test of Geared Motors.](http://arxiv.org/abs/2211.01716) | 本研究探讨了工业环境中齿轮电机末端测试中声学异常检测的特征鲁棒性。使用从对数包络谱中提取的特征和心理声学特征，结合孤立森林或装袋随机矿工算法进行检测，能有效避免大多数干扰，但使用锤子或气压则会产生问题。 |
| [^187] | [Quantum Similarity Testing with Convolutional Neural Networks.](http://arxiv.org/abs/2211.01668) | 本文提出了一种基于卷积神经网络的机器学习算法，通过测量数据构建量子态的低维表示进行相似性评估，可以对非高斯量子态进行相似性检测，并在准确性和效率方面优于以往方法。 |
| [^188] | [RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question.](http://arxiv.org/abs/2211.01482) | RQUGE是一种新的度量标准方法，通过候选问题是否可以回答来评估问题生成质量, 比现有指标更加稳健，可以在不需要人工提供参考问题的情况下使用。 |
| [^189] | [Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy.](http://arxiv.org/abs/2210.17546) | 防止神经语言模型逐字记忆无法真正保护隐私，本文设计的布隆过滤器虽然防止了所有逐字记忆，但仍然无法防止训练数据泄露，容易被合理修改的“样式转换”提示绕过。 |
| [^190] | [MARLlib: A Scalable Multi-agent Reinforcement Learning Library.](http://arxiv.org/abs/2210.13708) | 本文提出了MARLlib，这是一个全面的MARL算法库，可统一数十种算法。它还超越了当前工作，集成了各种环境接口和提供灵活的参数共享策略。 |
| [^191] | [Instance-Aware Image Completion.](http://arxiv.org/abs/2210.12350) | 本文提出了一个实例感知图像修复模型ImComplete，相比现有方法，它可以幻象出与环境背景相协调的视觉实例，提供了基于语义和结构的像素级指导。 |
| [^192] | [On Representing Mixed-Integer Linear Programs by Graph Neural Networks.](http://arxiv.org/abs/2210.10759) | 本研究探讨了用图神经网络表示混合整数线性规划问题的局限性，证明了一些针对特定情况下的限制条件下存在可靠的GNN方法，可以预测MILP问题的可行性、最优目标值和最优解。 |
| [^193] | [Skill-Based Reinforcement Learning with Intrinsic Reward Matching.](http://arxiv.org/abs/2210.07426) | 该论文提出了内在奖励匹配(IRF)方法，通过技能鉴别器匹配内在和下游任务奖励来确定未见任务的最优技能，提高了系统效率。 |
| [^194] | [Actor-Critic or Critic-Actor? A Tale of Two Time Scales.](http://arxiv.org/abs/2210.04470) | 这篇论文提出了一种评论演员算法，它在快速和慢速时间尺度上计算价值函数和策略，该算法与演员评论算法在准确性和计算成本方面表现相当。 |
| [^195] | [Boundary-Aware Uncertainty for Feature Attribution Explainers.](http://arxiv.org/abs/2210.02419) | 本文提出了一种名为高斯过程解释不确定性（GPEC）框架，它可以对复杂的黑盒分类器进行可靠的特征归因，该框架结合了决策边界感知不确定性和解释函数逼近不确定性，能够生成一个统一的不确定性估计。 |
| [^196] | [The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks.](http://arxiv.org/abs/2210.02157) | 本论文分析了无限宽度的深度网络，使用不同的学习规则如GD、FA、DFA、Hebb和GLN进行训练，并发现每种规则下的输出函数演化都受到时间变化的有效神经切向核(eNTK)的影响。通过动态均场理论(DMFT)比较了每种学习规则所引起的特征和预测动力学。 |
| [^197] | [Hierarchical Adversarial Inverse Reinforcement Learning.](http://arxiv.org/abs/2210.01969) | 本文提出了一种分层对抗逆强化学习算法，能够在复杂任务中学习到具有层次结构的最优策略，比现有的方法更加有效。 |
| [^198] | [Downstream Datasets Make Surprisingly Good Pretraining Corpora.](http://arxiv.org/abs/2209.14389) | 本文研究了使用下游数据集进行自我预训练的效果，发现这种方法与使用大型语料库进行预训练的标准方法相媲美，并且在某些任务上更加优秀。同时，这些自我预训练模型还表现出了很好的泛化能力。 |
| [^199] | [On Representing Linear Programs by Graph Neural Networks.](http://arxiv.org/abs/2209.12288) | 本文构建了一个GNN来处理不同LP并给出了合理的证明，证明了该GNN可以预测LP的可行性、有界性和最优解。同时，我们展示了该GNN在处理LP时的准确性。 |
| [^200] | [Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks.](http://arxiv.org/abs/2209.10200) | 本文提出了一种在联邦学习中应用模型量化的方案，通过联邦学习的可变位宽优化来提高无线通信和计算效率，在无线资源受限的情况下，采用多尺度量化联邦学习聚合算法，能够有效改善联邦学习的性能，具备更高的收敛速度和更少的训练损失。 |
| [^201] | [ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques.](http://arxiv.org/abs/2209.03798) | 提出了ReX，一个将时间信息融入模型无关局部解释技术的框架，通过为解释添加时间信息，使一些现有模型无法应用的局部解释技术可以更好地处理可变长度的输入。 |
| [^202] | [Fix-A-Step: Semi-supervised Learning from Uncurated Unlabeled Data.](http://arxiv.org/abs/2208.11870) | Fix-A-Step是一个半监督学习的简化流程，将所有未经筛选的无标签图像视为潜在有用的；增强有标签集的数据，修正梯度下降更新方式可以修复许多常见的深度 SSL 方法，并在医学成像数据集上实现更好的效果。 |
| [^203] | [Last-Iterate Convergence with Full and Noisy Feedback in Two-Player Zero-Sum Games.](http://arxiv.org/abs/2208.09855) | 本文提出的M2WU方法能够实现两人零和博弈中的最后迭代收敛，并在全反馈和噪声反馈情况下具有较高的性能和效果。 |
| [^204] | [Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels.](http://arxiv.org/abs/2208.09378) | 该论文提出一个名为FedLN的框架，用于在联邦学习（FL）中处理标签噪声，这是FL中一个普遍存在且影响性能的问题。FedLN能够适应不同客户端的计算能力，涵盖FL初始化、设备端模型训练和服务器模型聚合三个阶段。 |
| [^205] | [Unsupervised machine learning framework for discriminating major variants of concern during COVID-19.](http://arxiv.org/abs/2208.01439) | 本文提出了一个无监督机器学习框架，利用基因组序列区分和可视化COVID-19主要变异体之间的关联。这一框架可以帮助医疗保健专业人员了解病毒的流行病学和进化动态。 |
| [^206] | [Thermodynamics of learning physical phenomena.](http://arxiv.org/abs/2207.12749) | 热力学作为归纳偏差在机器学习中的应用潜力已经被广泛意识到，文章回顾了热力学在学习过程中的应用，并研究不同方面对学习过程的影响。 |
| [^207] | [Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model.](http://arxiv.org/abs/2207.06652) | 本文提出了一种新的加权多兴趣检索模型（Multi-Interest Preference，MIP），通过利用深度学习方法为用户建立多个兴趣嵌入，并将用户在多个兴趣上的偏好进行建模，从而提高候选检索结果的查全率。 |
| [^208] | [Scaling Novel Object Detection with Weakly Supervised Detection Transformers.](http://arxiv.org/abs/2207.05205) | 本篇文章提出了一个新方法：弱监督检测变形器（Weakly Supervised Detection Transformer），可以有效地将大规模预训练数据集的知识转移至数百种新型物品的WSOD微调中，同时提高了多实例学习的准确性。实验结果表明，该方法优于现有的先进模型，在大规模新型物品检测数据集上达到了更好的性能。 |
| [^209] | [Synthesizing Rolling Bearing Fault Samples in New Conditions: A framework based on a modified CGAN.](http://arxiv.org/abs/2206.12076) | 提出了一种基于CGAN的新算法，它可以从新条件下的正常数据中生成对应的滚动轴承故障数据，可用于开发数据驱动的故障诊断工具，提高检测故障类型的准确度。 |
| [^210] | [Interpretable Mixture of Experts.](http://arxiv.org/abs/2206.02107) | 介绍了一种新的可解释建模框架——可解释的专家混合模型（IME），能够产生与“黑盒”深度神经网络相媲美的高精度，并在提供有用解释的同时，还具有解释黑盒模型预测的能力。 |
| [^211] | [B2T Connection: Serving Stability and Performance in Deep Transformers.](http://arxiv.org/abs/2206.00330) | 本研究提出了一种称为 B2T 连接的方法，连接了 Pre-LN 和 Post-LN 层的输出，为深度 Transformer 提供了高稳定性和有效的训练，实验结果表明，在多个基准数据集上取得了最先进结果。 |
| [^212] | [A Simulation Environment and Reinforcement Learning Method for Waste Reduction.](http://arxiv.org/abs/2205.15455) | 本文提出了一个新的强化学习任务，即库存重新填充，提出了一种新的强化学习算法 Perishable DQN，旨在最大化销售同时最小化浪费，实现计算机思维对废物减量的控制。 |
| [^213] | [The Multimarginal Optimal Transport Formulation of Adversarial Multiclass Classification.](http://arxiv.org/abs/2204.12676) | 本文研究了一类对抗性多类分类问题，提供了等价的广义几何重心问题和多重边际最优输运问题的重述，揭示了其丰富的几何结构，扩展了之前仅限于二分类设置的相关结果。通过本文提出的方法，可以恢复原始对抗性问题的最优稳健分类规则和最优对抗策略。 |
| [^214] | [EVOTER: Evolution of Transparent Explainable Rule-sets.](http://arxiv.org/abs/2204.10438) | EVOTER使用简单的逻辑表达式演化出透明可解释的规则集，与黑盒模型性能相似，可以揭示数据中的偏见并为未来构建可靠的AI系统提供基础。 |
| [^215] | [Gradient Correction beyond Gradient Descent.](http://arxiv.org/abs/2203.08345) | 本论文提出了一种名为GCGD的梯度修正框架，可以有效提高梯度质量，从而将训练轮数减少约20％。 |
| [^216] | [On solutions of the distributional Bellman equation.](http://arxiv.org/abs/2202.00081) | 本文研究了分布贝尔曼方程的一般条件，包括解的存在唯一性和回报分布的尾部性质。将分布贝尔曼方程与多元仿射分布方程联系起来，发现任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这一理论适用于分布强化学习领域。 |
| [^217] | [Detecting DeFi Securities Violations from Token Smart Contract Code.](http://arxiv.org/abs/2112.02731) | 本研究旨在通过从代币的智能合约代码中提取特征（opcode-based特征）并建立分类器来识别DeFi项目中可能存在的证券违规活动。最终模型是一个随机森林模型，对基线进行了80％的F-1得分。 |
| [^218] | [Mitigating Adversarial Attacks by Distributing Different Copies to Different Users.](http://arxiv.org/abs/2111.15160) | 将不同的模型副本分发给不同的用户，可以降低恶意用户对其他用户的攻击风险。模型使用不同的随机性训练可以减轻复制攻击，但是重训练代价高且结果不稳定。一些方法扩展了重训练以增强模型差异，但是计算成本更高。 |
| [^219] | [Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance.](http://arxiv.org/abs/2110.06893) | 本文改进了常见的度量标准H-score，并提出了一个基于收缩估计的解决方案，使H分数的相关性性能获得高达80%的绝对增益，使其与最先进的LogME度量标准具有竞争力。同时，针对目标任务的选择，本文也发现了一个被忽视的问题。 |
| [^220] | [Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions.](http://arxiv.org/abs/2108.11328) | 本文提出了一种可解释的非参数加性模型，使用少量主要和成对交互效应预测调查反应率。该模型可以生成易于可视化和解释的预测面，并取得了 ROAM 数据集上的最先进性能，可以提供改进美国人口普查局和其他调查的反应率议论。 |
| [^221] | [Physics-Guided Discovery of Highly Nonlinear Parametric Partial Differential Equations.](http://arxiv.org/abs/2106.01078) | 该论文提出了一种新的物理引导学习方法，能够从数据中发现高度非线性参数偏微分方程，同时具有更好的鲁棒性和准确性。 |
| [^222] | [Explainable Activity Recognition for Smart Home Systems.](http://arxiv.org/abs/2105.09787) | 本论文提出了一种可解释的活动识别框架，可以生成自然语言解释来解释分类的原因，以提高智能家居系统的可信度。 |
| [^223] | [Multi-Task Attentive Residual Networks for Argument Mining.](http://arxiv.org/abs/2102.12227) | 本文提出了一种多任务注意力残差网络架构，通过利用集成方法、注意力机制和多任务学习，无需假设文档或论据结构，成功应用于多个论述挖掘任务中，成为了一种既通用又高性能的架构。 |
| [^224] | [Double-descent curves in neural networks: a new perspective using Gaussian processes.](http://arxiv.org/abs/2102.07238) | 本文利用随机矩阵理论和高斯过程技术解释了神经网络双峰下降现象，建立了NNGP和随机矩阵理论之间的新联系，揭示该现象受到经验核和NNGP核之间差异的影响。 |
| [^225] | [Cross-Shape Attention for Part Segmentation of 3D Point Clouds.](http://arxiv.org/abs/2003.09053) | 本文提出了一种新方法，在形状集合中通过交叉形状注意力机制来实现三维点云分割，通过评估点之间的交互程度和介导特征传播来提升结果精度和一致性。 |

# 详细

[^1]: 离巢：超越本地损失函数的预测优化问题

    Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])

    [http://arxiv.org/abs/2305.16830](http://arxiv.org/abs/2305.16830)

    本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。

    

    预测优化问题是一种使用机器学习在不确定性条件下进行决策制定的框架。它的中心研究问题是，“如何利用决策任务的结构来定制特定任务的机器学习模型？”为此，最近的研究已经提出了学习任务特定的损失函数来捕捉这种潜在的结构。然而，当前的方法对这些损失的形式和对机器学习模型行为的影响做出了限制性的假设。这些假设既导致了高计算成本的方法，也在实践中被违反时导致了性能下降。在本文中，我们提出了解决这些问题的解决方案，避免了上述假设，利用机器学习模型的特性来提高学习损失函数的样本效率。我们从文献中的四个领域实验证明了我们的方法取得了最先进的结果，通常需要比可比方法少一个数量级的样本。

    Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
    
[^2]: HUB: 用持续提示调整引导学习优化器

    HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])

    [http://arxiv.org/abs/2305.16823](http://arxiv.org/abs/2305.16823)

    本文提出了一种名为HUB的混合更新策略，通过结合学习优化器和手工设计的优化器，提高了学习优化器泛化性能。

    

    学习优化器是元学习的关键组成部分，但其在处理未见过的任务和网络架构时有限。为了解决此问题，本文提出了一种基于混合更新策略的优化方法（HUB），该方法受到了大型语言和视觉模型中硬提示调整和结果选择技术的启发。通过将手工设计的优化器作为我们混合方法的第二个组件，我们能够在稳定训练的同时保留学习优化器的好处。

    Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
    
[^3]: 机器学习驱动的分布式系统认证之路

    Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])

    [http://arxiv.org/abs/2305.16822](http://arxiv.org/abs/2305.16822)

    认证技术对于机器学习驱动的分布式系统验证不适用，本文提出了第一个ML-based分布式系统认证方案，以验证其非功能属性。

    

    机器学习（ML）日益被用于驱动部署在5G云边缘连续体上的复杂分布式系统的运行。相应地，分布式系统的行为变得更具非确定性。这种分布式系统的演化需要定义新的保证方法来验证非功能属性。认证作为系统和软件验证的最流行的保证技术，不能立即适用于其行为由基于机器学习的推理决定的系统。然而，政策制定者、监管机构和产业利益相关者越来越推崇定义ML的非功能属性（如公平性、鲁棒性、隐私）的认证技术。本文分析了当前认证方案的挑战和不足之处，讨论了开放的研究问题，并提出了第一个ML-based分布式系统认证方案。

    Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
    
[^4]: 选择性混合有助于应对分布偏移，但不仅仅是因为混合技术

    Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup. (arXiv:2305.16817v1 [cs.LG])

    [http://arxiv.org/abs/2305.16817](http://arxiv.org/abs/2305.16817)

    选择性mixup通过非随机选择对提高训练分布，实现标签偏移的经典解决方案，从而提高了神经网络的泛化性能。

    

    Mixup是一种提高神经网络泛化性能的高度成功的技术，它通过随机配对的组合来增强训练数据。选择性mixup是一系列将mixup应用于特定对的方法，例如仅在类别或领域之间组合示例。这些方法声称在具有分布偏移的基准测试中有显着的提高，但它们的机制和限制尚不清楚。本文研究了选择性mixup的一个被忽视的方面，从一个全新的角度解释了它的成功。我们发现，非随机选择对会影响训练分布，并通过与混合技术完全无关的方式提高泛化性能。例如，在二元分类中，类别之间的mixup隐含地对数据进行重采样，以实现标签偏移的经典解决方案。我们经验证实，这种隐含重采样解释了先前工作中的大部分改进。在理论上，这些结果依赖于一个回归问题，其中我们需要区分真正的重采样和混合技术。

    Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.  We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regress
    
[^5]: 基于几何深度学习的结论理论方法

    Geometric deep learning approach to knot theory. (arXiv:2305.16808v1 [math.GT])

    [http://arxiv.org/abs/2305.16808](http://arxiv.org/abs/2305.16808)

    本文利用几何深度学习构建了一个函子将结论转化为图形，并使用图神经网络预测结论不变量，具有高泛化能力。

    

    本文提出了一种新的方法，通过构建一个函子将结构转化为图形，使用图神经网络来处理结论数据。我们将尝试使用这种方法预测几种结论不变量。这种方法具有较高的泛化能力。

    In this paper, we introduce a novel way to use geometric deep learning for knot data by constructing a functor that takes knots to graphs and using graph neural networks. We will attempt to predict several knot invariants with this approach. This approach demonstrates high generalization capabilities.
    
[^6]: 神经控制微分方程的泛化能力研究

    On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])

    [http://arxiv.org/abs/2305.16791](http://arxiv.org/abs/2305.16791)

    本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。

    

    本文研究了使用神经控制微分方程（Kidger，Morrill等，2020）从不规则采样的时间序列样本中预测结果的监督学习设置。在我们的框架中，时间序列是一个未观察到的连续路径的离散化，结果通过一个具有未知向量场的控制微分方程依赖于这个路径。使用离散数据进行学习会引入离散偏差，我们精确地量化了这种偏差。通过使用关于控制微分方程流的连续性的理论结果，我们展示了逼近偏差直接与由浅层神经网络定义生成模型的利普希茨函数的逼近误差相关。通过结合最近的工作将神经网络的利普希茨常数与其泛化能力联系起来，我们上界了经验风险最小化器达到的期望损失与贝叶斯最优风险之间的泛化差距。

    We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
    
[^7]: 在自监督学习中调节频谱

    Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v1 [cs.LG])

    [http://arxiv.org/abs/2305.16789](http://arxiv.org/abs/2305.16789)

    本文提出了谱变换（ST）框架，可以调节自监督学习的频谱，并避免特征崩溃。其中，INTL是ST的一个实例，能够在优化过程中将嵌入的频谱调节到一个等特征值分布，实现较高的准确率。

    

    白化损失为使用联合嵌入架构进行自监督学习提供了理论保证，避免了特征崩溃。本文提出了谱变换（ST）框架，在前向传递过程中将嵌入的频谱映射到所需的分布，并在反向传递过程中通过隐式梯度更新来调制嵌入的频谱。我们证明了白化变换是ST的一个特例，还有其他实例可以避免崩溃。此外，本文提出了INTL（IterNorm with trace loss）的新实例。我们理论上证明了INTL可以避免崩溃，并在优化过程中将嵌入的频谱调节到一个等特征值分布。此外，INTL实现了76.6％的最高精度。

    Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top
    
[^8]: 结合话语结构分布的长文本自动摘要方法

    Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])

    [http://arxiv.org/abs/2305.16784](http://arxiv.org/abs/2305.16784)

    本文提出了一种名为'RSTformer'的摘要模型，该模型全面融合了话语关系类型和不确定性，并以修辞结构理论为基础，经过严格评估，表现明显优于现有的模型。

    

    对于文本摘要，话语结构在辨识文本核心内容方面起着关键作用。可惜的是，之前将修辞结构理论（RST）引入基于transformer的自动摘要模型的研究仅考虑了核心部分的注释，从而忽略了各种不同类型的话语关系。本文提出了一种名为'RSTformer'的新型摘要模型，该模型全面融合了话语关系类型和不确定性。我们的RST-attention机制是基于文档级修辞结构的Longformer框架的扩展。经过严格评估，本文提出的模型表现明显优于现有的模型，凸显出其在多个自动评估指标和人工评估上的卓越表现。

    For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.
    
[^9]: 具有异质性的图神经对流扩散

    Graph Neural Convection-Diffusion with Heterophily. (arXiv:2305.16780v1 [cs.LG])

    [http://arxiv.org/abs/2305.16780](http://arxiv.org/abs/2305.16780)

    本论文提出了一种考虑了异质性原则的新型图神经网络，该网络使用对流扩散方程对节点上的信息流进行建模，可以同时考虑基于同质性和异质性的信息传递，在处理异质性图的节点分类任务中具有竞争性的表现。

    

    图神经网络（GNN）已经在各种图学习任务中展现出了很高的性能，但是它们通常假设同质性，这可能会导致对异质性图的性能表现较差。通过使用对流扩散方程（CDE）对节点上的信息流建模，提出了一种新的GNN，该模型融合了异质性原则。这使得CDE能够考虑到基于同质性的信息扩散和基于异质性的信息“对流”。我们进行了广泛的实验验证，表明相比于现有技术，我们的框架在针对异质性图的节点分类任务中能够实现竞争性能。代码可以在 \url{https://github.com/zknus/Graph-Diffusion-CDE} 上获得。

    Graph neural networks (GNNs) have shown promising results across various graph learning tasks, but they often assume homophily, which can result in poor performance on heterophilic graphs. The connected nodes are likely to be from different classes or have dissimilar features on heterophilic graphs. In this paper, we propose a novel GNN that incorporates the principle of heterophily by modeling the flow of information on nodes using the convection-diffusion equation (CDE). This allows the CDE to take into account both the diffusion of information due to homophily and the ``convection'' of information due to heterophily. We conduct extensive experiments, which suggest that our framework can achieve competitive performance on node classification tasks for heterophilic graphs, compared to the state-of-the-art methods. The code is available at \url{https://github.com/zknus/Graph-Diffusion-CDE}.
    
[^10]: 毒化攻击下的鲁棒非参数回归

    Robust Nonparametric Regression under Poisoning Attack. (arXiv:2305.16771v1 [math.ST])

    [http://arxiv.org/abs/2305.16771](http://arxiv.org/abs/2305.16771)

    本文提出了一种针对毒化攻击的鲁棒非参数回归方法，包括基于Huber损失的M-评估器和通过将初始估计投影到Lipschitz函数空间中的校正方法。结果表明，正确选择带宽时$\ell_\infty $误差是极小化最优的，而$\ell_2 $误差在$q\lesssim \sqrt{N/\ln^2 N}$时最优。

    

    本文研究了鲁棒非参数回归，在这种回归中，对抗性攻击者可以修改来自大小为N的训练数据集中最多q个样本的值。我们的初始解决方案是基于Huber损失最小化的M-评估器。与简单的核回归（即Nadaraya-Watson估计）相比，这种方法可以显着减弱恶意样本对回归性能的影响。我们提供了收敛速率以及相应的极小化下界。结果表明，通过正确选择带宽，$\ell_\infty $误差是极小化最优的。如果$q\lesssim \sqrt{N/\ln^2 N}$，则$\ell_2 $误差是最优的，但是如果$q$更大，则是次优的。原因是如果有许多被攻击的样本集中在一个小区域中，这个估计量就会很容易受到攻击。为了解决这个问题，我们提出了一种校正方法，将初始估计投影到Lipschitz函数空间中。最终的估计值几乎是任意$q$的极小化最优的。

    This paper studies robust nonparametric regression, in which an adversarial attacker can modify the values of up to $q$ samples from a training dataset of size $N$. Our initial solution is an M-estimator based on Huber loss minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson estimator, this method can significantly weaken the impact of malicious samples on the regression performance. We provide the convergence rate as well as the corresponding minimax lower bound. The result shows that, with proper bandwidth selection, $\ell_\infty$ error is minimax optimal. The $\ell_2$ error is optimal if $q\lesssim \sqrt{N/\ln^2 N}$, but is suboptimal with larger $q$. The reason is that this estimator is vulnerable if there are many attacked samples concentrating in a small region. To address this issue, we propose a correction method by projecting the initial estimate to the space of Lipschitz functions. The final estimate is nearly minimax optimal for arbitrary $q$, up t
    
[^11]: 利用领域知识实现包容和偏见感知的人道主义响应入口分类

    Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])

    [http://arxiv.org/abs/2305.16756](http://arxiv.org/abs/2305.16756)

    本研究提出了一种以人道主义本体为基础的新型语言模型HumBert，并提供了一种系统的方法来衡量和减少偏见，以实现对人道主义数据分析的有效和道德意识的支持。

    

    在人道主义危机期间，准确和快速的情况分析对于高效地提供人道主义援助至关重要，并且是人道主义原则和不留任何人落后原则的基础。语言处理系统可以极大地受益于这种数据分析，例如，按照人道主义本体对文本数据进行分类。然而，仅仅通过微调通用的大型语言模型 (LLM) 来实现，涉及一些实践和道德问题，特别是在数据稀疏和复杂子领域上的效果不佳以及社会偏见和不良关联的编码。在这项工作中，我们旨在为人道主义数据分析提供一种有效和道德意识的系统。我们通过 (1) 引入一个适合人道主义分析框架的新架构，(2) 创建和发布一个新的人道主义特定 LLM，称为 HumBert，并且 (3) 提出了一种系统的方式来衡量和减少偏见。

    Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our experi
    
[^12]: 一种用于离散周界防御问题的分散脉冲学习框架

    A Decentralized Spike-based Learning Framework for Sequential Capture in Discrete Perimeter Defense Problem. (arXiv:2305.16748v1 [cs.RO])

    [http://arxiv.org/abs/2305.16748](http://arxiv.org/abs/2305.16748)

    本文提出了一种针对离散周界防御问题的分散脉冲学习框架，有效解决了团队防御者保护领土的问题，每个防御者含有自己的MLC-SEFRON网络，从而实现分散独立训练，输入信息来源于防御者和入侵者的时空信息。

    

    本文提出了一种新颖的分散脉冲学习（DSL）框架，用于处理离散周界防御问题（d-PDP）。团队操作防御者用于保护圆形领土免受辐射性入侵者的攻击。首先，将离散周界防御问题（d-PDP）表述为时空多任务分配问题（STMTA）。然后将STMTA问题转换为多标签学习问题，以获取防御者必须访问的区段标签，以便保护周界。DSL框架使用Multi-Label Classifier Using Synaptic Efficacy Function Spiking NeuRON（MLC-SEFRON）网络进行确定性多标签学习。每个防御者都包含单个MLC-SEFRON网络，每个MLC-SEFRON网络都独立训练，使用其自身透视图的输入进行分散操作。 MLC-SEFRON网络的输入脉冲可以直接从防御者和入侵者的时空信息中获得，无需额外的预处理。

    This paper proposes a novel Decentralized Spike-based Learning (DSL) framework for the discrete Perimeter Defense Problem (d-PDP). A team of defenders is operating on the perimeter to protect the circular territory from radially incoming intruders. At first, the d-PDP is formulated as a spatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is then converted into a multi-label learning problem to obtain labels of segments that defenders have to visit in order to protect the perimeter. The DSL framework uses a Multi-Label Classifier using Synaptic Efficacy Function spiking neuRON (MLC-SEFRON) network for deterministic multi-label learning. Each defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is trained independently using input from its own perspective for decentralized operations. The input spikes to the MLC-SEFRON network can be directly obtained from the spatio-temporal information of defenders and intruders without any extra pre-processing
    
[^13]: 无需引入新的延迟的参数高效微调

    Parameter-Efficient Fine-Tuning without Introducing New Latency. (arXiv:2305.16742v1 [cs.CL])

    [http://arxiv.org/abs/2305.16742](http://arxiv.org/abs/2305.16742)

    本文提出了一种参数高效微调的方法，以任务不可知的方式生成稀疏掩码，无需添加新参数，避免了额外的推断延迟，并超过了现有方法的效果。

    

    预训练语言模型的参数高效微调（PEFT）最近展示出明显的成就，有效地匹配了完全微调的性能，同时利用明显更少的可训练参数，因此解决了存储和通信限制。尽管如此，各种PEFT方法仍受其固有特性的限制。在稀疏微调的情况下，这只涉及修改现有参数的一小部分，微调参数的选择是任务和领域特定的，因此不适用于联合学习。另一方面，添加新参数的PEFT方法通常会引入额外的推断延迟。在本文中，我们展示了以任务不可知的方式生成稀疏掩码的可行性，其中所有下游任务共享相同的掩码。我们的方法仅依赖于预训练参数的幅度信息，超过了现有方法学的效果。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a si
    
[^14]: 基于卷积生成对抗网络生成混沌时间序列的评价

    Evaluating generation of chaotic time series by convolutional generative adversarial networks. (arXiv:2305.16729v1 [cs.LG])

    [http://arxiv.org/abs/2305.16729](http://arxiv.org/abs/2305.16729)

    该研究通过使用卷积生成对抗网络生成混沌时间序列，并评估生成的时间序列，证明了卷积神经网络有能力很好地复制原始时间序列的混沌特性，但误差分析表明仍然存在大误差。

    

    为了了解卷积神经网络生成类似于复杂时间信号的时间序列的能力和局限性，我们训练了一个由深度卷积网络组成的生成对抗网络来生成混沌时间序列，并使用非线性时间序列分析来评估生成的时间序列。确定性的数字度量和李雅普诺夫指数，一个轨迹不稳定的度量，表明生成的时间序列很好地复制了原始时间序列的混沌特性。然而，误差分布分析表明，低但不可忽略的速率下会出现大误差。如果假定分布为指数分布，则不会出现这样的误差。

    To understand the ability and limitations of convolutional neural networks to generate time series that mimic complex temporal signals, we trained a generative adversarial network consisting of deep convolutional networks to generate chaotic time series and used nonlinear time series analysis to evaluate the generated time series. A numerical measure of determinism and the Lyapunov exponent, a measure of trajectory instability, showed that the generated time series well reproduce the chaotic properties of the original time series. However, error distribution analyses showed that large errors appeared at a low but non-negligible rate. Such errors would not be expected if the distribution were assumed to be exponential.
    
[^15]: 基于形状的自动膝部标准视图姿态估计

    Shape-based pose estimation for automatic standard views of the knee. (arXiv:2305.16717v1 [eess.IV])

    [http://arxiv.org/abs/2305.16717](http://arxiv.org/abs/2305.16717)

    该论文提出一种基于形状信息的自动膝关节标准视图姿态估计框架，可减少手术过程中的时间和辐射剂量，获得了比现有技术更好的精度和稳定性。

    

    手术治疗复杂的膝关节骨折通过使用移动C臂进行实时成像来指导。通过与患者定位相对的特定C臂姿势相对应的2D解剖特异性标准视图实现即时和连续的控制，目前通过试错方法手动确定，需要耗费时间和辐射剂量。膝关节标准视图的特征表明，个人骨骼的形状信息可以指导自动定位过程，减少C臂定位期间的时间和不必要的辐射。为了完全自动化膝关节手术中的C臂定位任务，我们提出了一个完整的框架，它实现了（1）自动区分左右和标准视图和（2）基于单个初始X射线的基于形状的姿态回归到所需的标准视图。提出了一个合适的形状表示来将语义信息合并到姿态回归中。实验结果表明，我们的框架在精度和鲁棒性方面优于现有技术。

    Surgical treatment of complicated knee fractures is guided by real-time imaging using a mobile C-arm. Immediate and continuous control is achieved via 2D anatomy-specific standard views that correspond to a specific C-arm pose relative to the patient positioning, which is currently determined manually, following a trial-and-error approach at the cost of time and radiation dose. The characteristics of the standard views of the knee suggests that the shape information of individual bones could guide an automatic positioning procedure, reducing time and the amount of unnecessary radiation during C-arm positioning. To fully automate the C-arm positioning task during knee surgeries, we propose a complete framework that enables (1) automatic laterality and standard view classification and (2) automatic shape-based pose regression toward the desired standard view based on a single initial X-ray. A suitable shape representation is proposed to incorporate semantic information into the pose regr
    
[^16]: 一种基于层次思维的人工智能协同训练方法

    A Hierarchical Approach to Population Training for Human-AI Collaboration. (arXiv:2305.16708v1 [cs.AI])

    [http://arxiv.org/abs/2305.16708](http://arxiv.org/abs/2305.16708)

    本论文提出了一种基于层次思维的人工智能协同训练方法，通过引入层次强化学习方法，代理能够根据当前合作伙伴自动切换最佳响应策略，从而显著提高了代理与新合作伙伴的适应性。

    

    深度强化学习（DRL）代理在与未经过训练的合作伙伴协同时存在困难，特别是当代理与人类合作伙伴合作时，因人类行为的不一致性而出现行动反应的方差增加，而这加剧了这一问题。最近的研究表明，将单个代理训练为对多样化的训练伙伴做出最佳响应，可以显著提高代理与新合作伙伴的适应性。我们进一步增强了基于人口的训练方法，引入了一个基于层次强化学习（HRL）的方法来实现人工智能协同训练。我们的代理能够学习多个最佳响应策略作为其低层策略，同时学习一个作为管理者的高层策略，使代理能够根据其当前的合作伙伴动态地在低层最佳响应策略之间进行切换。我们证明了我们的方法能够

    A major challenge for deep reinforcement learning (DRL) agents is to collaborate with novel partners that were not encountered by them during the training phase. This is specifically worsened by an increased variance in action responses when the DRL agents collaborate with human partners due to the lack of consistency in human behaviors. Recent work have shown that training a single agent as the best response to a diverse population of training partners significantly increases an agent's robustness to novel partners. We further enhance the population-based training approach by introducing a Hierarchical Reinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent is able to learn multiple best-response policies as its low-level policy while at the same time, it learns a high-level policy that acts as a manager which allows the agent to dynamically switch between the low-level best-response policies based on its current partner. We demonstrate that our method is able 
    
[^17]: 探究分布漂移下的上下文学习：以线性回归为例

    A Closer Look at In-Context Learning under Distribution Shifts. (arXiv:2305.16704v1 [cs.LG])

    [http://arxiv.org/abs/2305.16704](http://arxiv.org/abs/2305.16704)

    本文探究了分布漂移下的上下文学习，比较了变压器和基于集合的MLP模型的性能，发现二者在分布内评估中都表现出上下文学习的能力，但在防范较小的分布漂移方面，变压器更胜一筹。

    

    上下文学习是大型语言模型的一个定义特征，它使模型能够在不需要进行权重更新的情况下即时地从输入样例中学习。本文旨在通过线性回归这一简单而基础的任务，遵循（Garg et al., 2022）提出的设置，从简单的基于集合的多层感知器（MLP）架构的角度，更好地理解上下文学习的普适性和局限性。我们研究的核心问题是：在变化的分布漂移下，变压器是否比一些自然且更简单的架构更擅长执行上下文学习？我们发现，在分布内评估下，变压器和基于集合的MLP模型都表现出了上下文学习的能力，但是变压器更接近于最小二乘法（OLS）的表现。在分布漂移较小的情况下，变压器的韧性也比基于集合的MLP模型更好。

    In-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler architectures at performing in-context learning under varying distribution shifts? To compare transformers, we propose to use a simple architecture based on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based MLPs exhibit in-context learning under in-distribution evaluations, but transformers more closely emulate the performance of ordinary least squares (OLS). Transformers also display better resilience to mild distribution shifts, where set-based MLPs falter. 
    
[^18]: 机器学习中的不确定性来源 -- 一个统计学家的视角

    Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])

    [http://arxiv.org/abs/2305.16703](http://arxiv.org/abs/2305.16703)

    本文讨论了机器学习中不确定性的来源和类型，从统计学家的视角出发，分类别介绍了随机性和认知性不确定性的概念，证明了不确定性来源各异，不可简单归为两类。同时，与统计学概念进行类比，探讨不确定性在机器学习中的作用。

    

    机器学习和深度学习已经取得了令人瞩目的成就，使我们能够回答几年前难以想象的问题。除了这些成功之外，越来越清晰的是，在纯预测之外，量化不确定性也是相关和必要的。虽然近年来已经出现了这方面的第一批概念和思想，但本文采用了一个概念性的视角，并探讨了可能的不确定性来源。通过采用统计学家的视角，我们讨论了与机器学习更常见相关的随机性和认知性不确定性的概念。本文旨在规范这两种类型的不确定性，并证明不确定性的来源各异，并且不总是可以分解为随机性和认知性。通过将统计概念与机器学习中的不确定性进行类比，我们也展示了统计学概念和机器学习中不确定性的作用。

    Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
    
[^19]: 无需超参数搜索的端到端零样本语音合成中损失权衡的自动调整

    Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis. (arXiv:2305.16699v1 [eess.AS])

    [http://arxiv.org/abs/2305.16699](http://arxiv.org/abs/2305.16699)

    本论文提出在零样本语音合成中自动调整损失权衡的方法，无需超参数搜索。通过此方法，VITS-based模型的性能表现得到了显著提升，达到了最新的领先性能。

    

    最近，零样本TTS和VC方法因其能够生成在训练中从未见过的语音而引起了关注。在这些方法中，基于VITS的零样本修改展现出了优越的性能，同时还具有从VITS继承来的有用属性。然而，VITS和基于VITS的零样本模型的性能在损失如何权衡方面存在巨大差异。这可能是有问题的，因为它需要繁琐的调整损失权衡超参数以找到最佳平衡点。在本研究中，我们提出了一个新的框架，通过引导VITS-based模型的解码器达到其完全重建能力，以找到这个最佳点而无需搜索。通过我们的框架，我们展现了在零样本TTS和VC中比基线更优越的性能，实现了最新的领先性能。此外，我们在各种设置中展示了我们的框架的鲁棒性。我们在讨论中对结果进行了解释。

    Recently, zero-shot TTS and VC methods have gained attention due to their practicality of being able to generate voices even unseen during training. Among these methods, zero-shot modifications of the VITS model have shown superior performance, while having useful properties inherited from VITS. However, the performance of VITS and VITS-based zero-shot models vary dramatically depending on how the losses are balanced. This can be problematic, as it requires a burdensome procedure of tuning loss balance hyper-parameters to find the optimal balance. In this work, we propose a novel framework that finds this optimum without search, by inducing the decoder of VITS-based models to its full reconstruction ability. With our framework, we show superior performance compared to baselines in zero-shot TTS and VC, achieving state-of-the-art performance. Furthermore, we show the robustness of our framework in various settings. We provide an explanation for the results in the discussion.
    
[^20]: 双重贝叶斯ResNet：一种心脏杂音检测的深度学习方法

    Dual Bayesian ResNet: A Deep Learning Approach to Heart Murmur Detection. (arXiv:2305.16691v1 [cs.LG])

    [http://arxiv.org/abs/2305.16691](http://arxiv.org/abs/2305.16691)

    本论文介绍了一种深度学习方法，双重贝叶斯ResNet，用于心脏杂音检测。其中一种模型将每个患者的记录分段成重叠的log mel音谱图进行二分类，第二种模型通过整合人口统计学数据和信号特征来提高杂音分类准确率。

    

    本研究介绍了我们团队PathToMyHeart在2022年George B. Moody PhysioNet Challenge中的贡献。我们实现了两个模型。第一个模型是双重贝叶斯ResNet（DBRes），将每个患者的记录分段成重叠的log mel音谱图进行两个二分类：存在与未知或不存在，未知与存在或不存在。这些分类结果进行聚合，以得出患者的最终分类。第二个模型是将DBRes的输出与人口统计学数据和信号特征进行整合，使用XGBoost进行分类。DBRes在隐含测试集的杂音分类中取得了$0.771$的最佳加权准确率，使我们在杂音任务中排名第四。（在我们忽略了临床结局任务的情况下，我们的得分为$12637$。）在我们的训练集子集上，将人口统计学数据和信号特征整合后，DBRes的准确率从$0.762$提高到了$0.820$。但是，这降低了DBRes的加权准确率。

    This study presents our team PathToMyHeart's contribution to the George B. Moody PhysioNet Challenge 2022. Two models are implemented. The first model is a Dual Bayesian ResNet (DBRes), where each patient's recording is segmented into overlapping log mel spectrograms. These undergo two binary classifications: present versus unknown or absent, and unknown versus present or absent. The classifications are aggregated to give a patient's final classification. The second model is the output of DBRes integrated with demographic data and signal features using XGBoost.DBRes achieved our best weighted accuracy of $0.771$ on the hidden test set for murmur classification, which placed us fourth for the murmur task. (On the clinical outcome task, which we neglected, we scored 17th with costs of $12637$.) On our held-out subset of the training set, integrating the demographic data and signal features improved DBRes's accuracy from $0.762$ to $0.820$. However, this decreased DBRes's weighted accurac
    
[^21]: 未来条件下的无监督预训练对决策Transformer的影响

    Future-conditioned Unsupervised Pretraining for Decision Transformer. (arXiv:2305.16683v1 [cs.LG])

    [http://arxiv.org/abs/2305.16683](http://arxiv.org/abs/2305.16683)

    本文提出了一种基于未来信息的无监督预训练方法PDT，使得在没有奖励和次优离线数据上的无监督学习成为可能，通过为可能的未来分配回报值和采样未来嵌入进行在线微调。实验表明，PDT优于当前的无监督方法，并与有监督的学习方法相当竞争。

    

    最近离线强化学习方面的研究表明，以回报为条件的监督学习是解决决策问题的一种强有力的范例。虽然很有希望，但回报条件仅适用于标记有奖励数据的训练，因此在从无监督数据中学习方面面临挑战。在本文中，我们旨在利用广义未来条件来启用从没有奖励和次优离线数据进行高效无监督预训练。我们提出了预训练决策Transformer（PDT），这是一种概念上简单的无监督RL预训练方法。 PDT利用未来轨迹信息作为预测训练期间的动作的特权上下文。基于当前和未来因素做出决策的能力增强了PDT的泛化能力。此外，可以将此功能轻松地合并到基于回报的框架中以进行在线微调，通过为可能的未来分配回报值并根据这些回报样本未来嵌入来采样。

    Recent research in offline reinforcement learning (RL) has demonstrated that return-conditioned supervised learning is a powerful paradigm for decision-making problems. While promising, return conditioning is limited to training data labeled with rewards and therefore faces challenges in learning from unsupervised data. In this work, we aim to utilize generalized future conditioning to enable efficient unsupervised pretraining from reward-free and sub-optimal offline data. We propose Pretrained Decision Transformer (PDT), a conceptually simple approach for unsupervised RL pretraining. PDT leverages future trajectory information as a privileged context to predict actions during training. The ability to make decisions based on both present and future factors enhances PDT's capability for generalization. Besides, this feature can be easily incorporated into a return-conditioned framework for online finetuning, by assigning return values to possible futures and sampling future embeddings b
    
[^22]: 多视角标识增强生成式检索

    Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])

    [http://arxiv.org/abs/2305.16675](http://arxiv.org/abs/2305.16675)

    该论文提出了一种新型的基于合成标识符的多视角标识符来增强生成式检索，从而提高了检索结果的准确性和多样性。

    

    与其简单地将查询与现有段落匹配，生成式检索生成段落的标识符字符串作为检索目标。然而，这种标识符必须足够独特以代表一个段落。当前的方法使用数字ID或文本片段（如标题或子字符串）作为标识符。然而，这些标识符不能很好地覆盖一个段落的内容。因此，我们提出了一种新类型的标识符，即基于段落内容生成的合成标识符，可以整合文本片段缺乏的情境信息。此外，我们同时考虑多视角标识符，包括合成标识符、标题和子字符串。这些标识符的视角相互补充，有助于从多个角度综合排名段落。我们在三个公共数据集上进行了一系列实验，结果表明我们提出的方法在生成式检索中表现最佳。

    Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative 
    
[^23]: 一种统一的方法用于最大化连续 DR-submodular 函数

    A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])

    [http://arxiv.org/abs/2305.16671](http://arxiv.org/abs/2305.16671)

    本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。

    

    本文提出了一种统一的方法，用于最大化连续的 DR-submodular 函数，涵盖了一系列设置和 Oracle 访问类型。我们的方法包括针对单调和非单调函数的 Frank-Wolfe 类型离线算法，具有不同的一般凸集限制。我们考虑了 Oracle 提供函数梯度或仅函数值的访问以及确定性或随机性访问的设置。我们在所有情况下确定了所需的 Oracle 访问数量。我们的方法为 16 个考虑的情况中的 9 个提供了新的/改进的结果，在两个情况下避免了计算上昂贵的投影，而所提出的框架在其余五个情况下与最先进的方法相匹配。值得注意的是，我们针对基于随机函数值的 Oracle 的方法，为随机 DR-submodular 函数提供了第一个带有探险反馈的后悔界限。

    This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
    
[^24]: 在黎曼流形上的高阶规范等变卷积神经网络及其应用

    Higher Order Gauge Equivariant CNNs on Riemannian Manifolds and Applications. (arXiv:2305.16657v1 [cs.CV])

    [http://arxiv.org/abs/2305.16657](http://arxiv.org/abs/2305.16657)

    本文介绍了一种在黎曼流形上的高阶规范等变卷积神经网络，能够在给定感受域内建模空间扩展的非线性交互，并保持对全局等度量的等变性。

    

    随着深度网络文献中等变卷积的出现，已经开发了具有 $\mathsf{SO}(3)$-等变层的球形 CNN 以处理在球形 $S^2$ 上的信号样本的数据。 通过显式要求相对于 $\mathsf{SO}(2)$ 的规范等变性，可以在 $S^2$ 上隐式获得 $\mathsf{SO}(3)$-等变卷积，并实现重大的效率提升。 本文在此基础上引入了规范等变卷积的高阶概括，其实现被称为规范等变的 Volterra 网络（GEVNet）。 这使得我们能够在保持对全局等度量的等变性的同时，在给定的感受域内建模空间扩展的非线性交互。 我们证明了关于等变性和高阶规范等变卷积构造的理论结果。然后，我们通过计算机视觉基准数据（例如球形 MNIST）以及在广告点击率预测的唯一真实数据集上的大规模工程实现，展示了我们的模型的参数效率。

    With the advent of group equivariant convolutions in deep networks literature, spherical CNNs with $\mathsf{SO}(3)$-equivariant layers have been developed to cope with data that are samples of signals on the sphere $S^2$. One can implicitly obtain $\mathsf{SO}(3)$-equivariant convolutions on $S^2$ with significant efficiency gains by explicitly requiring gauge equivariance w.r.t. $\mathsf{SO}(2)$. In this paper, we build on this fact by introducing a higher order generalization of the gauge equivariant convolution, whose implementation is dubbed a gauge equivariant Volterra network (GEVNet). This allows us to model spatially extended nonlinear interactions within a given receptive field while still maintaining equivariance to global isometries. We prove theoretical results regarding the equivariance and construction of higher order gauge equivariant convolutions. Then, we empirically demonstrate the parameter efficiency of our model, first on computer vision benchmark data (e.g. spheri
    
[^25]: 语言模型可以通过少样本的绝对推理来提高事件预测

    Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])

    [http://arxiv.org/abs/2305.16646](http://arxiv.org/abs/2305.16646)

    本文提出了一个建模和预测框架，在少样本情况下使用语言模型的绝对推理能力来提高事件序列模型的预测精度，经过实验证实可以明显优于最先进的事件序列模型。

    

    大型语言模型在各种推理任务上表现出惊人的性能。本文研究它们是否可以推理现实世界中的事件，帮助提高事件序列模型的预测精度。我们设计了一个建模和预测框架，其中大型语言模型执行绝对推理以辅助事件序列模型：事件模型在给定过去的情况下提出未来事件的预测; 在几个专家注释示范的指导下，语言模型学会了为每个提议提供可能的原因; 一个搜索模块找到与原因匹配的先前事件; 一个评分函数学会检查检索到的事件是否实际上可以导致提议。通过在两个具有挑战性的现实世界数据集（亚马逊评论和GDELT）上进行广泛的实验，我们证明了我们的框架 - 由于语言模型的推理能力 - 可以在低数据情况下明显优于最先进的事件序列模型。

    Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
    
[^26]: 改进Transformer在多元时间序列分类中的位置编码

    Improving Position Encoding of Transformers for Multivariate Time Series Classification. (arXiv:2305.16642v1 [cs.LG])

    [http://arxiv.org/abs/2305.16642](http://arxiv.org/abs/2305.16642)

    本文提出了一种新的绝对位置编码方法tAPE，以及一种相对位置编码的计算上高效的实现方法eRPE，旨在改进Transformer在多元时间序列分类中的性能。

    

    Transformer在深度学习的许多应用中表现出了出色的性能。在应用于时间序列数据时，Transformer需要有效的位置编码来捕捉时间序列数据的排序。但位置编码在时间序列分析中的效应尚未经过充分的研究，并且仍存在争议，例如，注入绝对位置编码还是相对位置编码更好，或者两者的组合更好。为了澄清这一点，我们首先回顾了现有的绝对和相对位置编码方法在时间序列分类中的应用。然后，我们提出了一种专门面向时间序列数据的绝对位置编码方法，称为时间绝对位置编码（tAPE）。我们的新方法将序列长度和输入嵌入维度纳入了绝对位置编码中。此外，我们还提出了计算上高效的相对位置编码的实现方法（eRPE），以提高时间序列的概括性。

    Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose 
    
[^27]: 通用逼近和拓扑神经网络

    Universal Approximation and the Topological Neural Network. (arXiv:2305.16639v1 [cs.LG])

    [http://arxiv.org/abs/2305.16639](http://arxiv.org/abs/2305.16639)

    本文介绍了一种拓扑神经网络，操作于Tychonoff拓扑空间而非有限维空间的数据，结合分布神经网络可识别随机过程路径中的长期依赖、重尾分布等特性，且证明了其可以任意逼近任何一致连续函数。

    

    本文介绍了一种拓扑神经网络（TNN），它使用来自Tychonoff拓扑空间而不是通常的有限维空间的数据。由此引入了一个接受Borel度量为数据的分布神经网络（DNN）。这些新的神经网络结合起来有助于识别随机过程路径中的长期依赖、重尾分布和其他特性，也有助于对粒子滤波或隐马尔可夫模型算法产生的信念状态进行操作。本文还通过Tychonoff空间的强通用逼近定理及其度量空间的推论来证明了TNN和DNN的正确性。这些定理表明，神经网络可以任意逼近与唯一一致性度量相关的一致连续函数（关于上确界度量）。此外，我们还提供了一些讨论，表明正定度量上的神经网络是最近深度学习“深感知机”概念的一种推广。

    A topological neural network (TNN), which takes data from a Tychonoff topological space instead of the usual finite dimensional space, is introduced. As a consequence, a distributional neural network (DNN) that takes Borel measures as data is also introduced. Combined these new neural networks facilitate things like recognizing long range dependence, heavy tails and other properties in stochastic process paths or like acting on belief states produced by particle filtering or hidden Markov model algorithms. The veracity of the TNN and DNN are then established herein by a strong universal approximation theorem for Tychonoff spaces and its corollary for spaces of measures. These theorems show that neural networks can arbitrarily approximate uniformly continuous functions (with respect to the sup metric) associated with a unique uniformity. We also provide some discussion showing that neural networks on positive-finite measures are a generalization of the recent deep learning notion of dee
    
[^28]: 集合化的神经网络编码

    Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])

    [http://arxiv.org/abs/2305.16625](http://arxiv.org/abs/2305.16625)

    提出了一种能够集合化地编码神经网络参数的神经网络权重编码方法，并引入了一种逐层编码方案来考虑神经网络的分层计算结构。同时引入了“pad-chunk-encode”流水线进行神经网络层的高效编码处理，还提出了新的神经网络泛化性能预测任务。

    

    我们提出了一种利用集合到集合和集合到向量函数来有效编码神经网络参数，进行泛化性能预测的神经网络权重编码方法。与之前需要对不同架构编写自定义编码模型的方法不同，我们的方法能够对混合架构和不同参数大小的模型动态编码。此外，我们的 SNE（集合化神经网络编码器）通过使用一种逐层编码方案，考虑神经网络的分层计算结构。最终将所有层次编码合并到一起，以获取神经网络编码矢量。我们还引入了“pad-chunk-encode”流水线来有效地编码神经网络层，该流水线可根据计算和内存限制进行调整。我们还引入了两个用于神经网络泛化性能预测的新任务：跨数据集和架构适应性预测。

    We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
    
[^29]: 使用深度集成方法预测感知不确定性下的行人运动轨迹

    Pedestrian Trajectory Forecasting Using Deep Ensembles Under Sensing Uncertainty. (arXiv:2305.16620v1 [cs.RO])

    [http://arxiv.org/abs/2305.16620](http://arxiv.org/abs/2305.16620)

    该研究提出了一种深度集成的方法，能够在考虑感知不确定性的情况下，预测行人运动轨迹及其不确定性。

    

    本文提出了一种端到端的方法，能够在考虑感知不确定性的情况下，使用深度集成的方法预测行人的运动轨迹，推测状态的不确定性。Bayes滤波器仍存在的问题是无法解决非线性和长期预测问题。

    One of the fundamental challenges in the prediction of dynamic agents is robustness. Usually, most predictions are deterministic estimates of future states which are over-confident and prone to error. Recently, few works have addressed capturing uncertainty during forecasting of future states. However, these probabilistic estimation methods fail to account for the upstream noise in perception data during tracking. Sensors always have noise and state estimation becomes even more difficult under adverse weather conditions and occlusion. Traditionally, Bayes filters have been used to fuse information from noisy sensors to update states with associated belief. But, they fail to address non-linearities and long-term predictions. Therefore, we propose an end-to-end estimator that can take noisy sensor measurements and make robust future state predictions with uncertainty bounds while simultaneously taking into consideration the upstream perceptual uncertainty. For the current research, we co
    
[^30]: 带部分知识特征的图像缺失特征补全中的置信度特性辅助特征提取

    Confidence-Based Feature Imputation for Graphs with Partially Known Features. (arXiv:2305.16618v1 [cs.LG])

    [http://arxiv.org/abs/2305.16618](http://arxiv.org/abs/2305.16618)

    本论文提出了一种新颖的信道置信度与伪置信度的特征插值方法，解决了高缺失特征率的图像学习任务中的性能下降问题。

    

    本文探讨了在图学习任务中的缺失特征补全问题。过去有几种方法已经解决了具有缺失特征的图形学习任务。然而，对于高缺失特征率的情况，它们无法避免显著的性能下降。为了克服这个限制，我们引入了一个新的概念，即节点特征中的信道置信度，它被指定给每个节点的填充信道特征，以反映填充的确定性。然后，我们设计了伪置信度，使用缺失特征节点与其最近的已知特征节点之间的信道最短路径距离来替换实际学习过程中缺失的真实置信度。基于伪置信度，我们提出了一种新的特征插值方案，它执行信道内节点扩散和节点内信道传播。该方案可以在非常高的缺失率下（例如，99.5％）持久存在，并且实现了最先进的准确性。

    This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\%) and it achieves state-of-the-art accurac
    
[^31]: 物理深度强化学习: 安全和未知未知的探索

    Physical Deep Reinforcement Learning: Safety and Unknown Unknowns. (arXiv:2305.16614v1 [cs.AI])

    [http://arxiv.org/abs/2305.16614](http://arxiv.org/abs/2305.16614)

    本文提出了Phy-DRL，这是一个物理模型调整的深度强化学习框架。该框架有三个创新点，它们分别是: i)前瞻性的未知未知训练，ii)结合残差控制，以及iii)基于物理模型的神经网络编辑。Phy-DRL能够容忍未知干扰，保证安全和稳定，同时遵守Bellman方程和奖励相关的物理知识。

    

    本文提出了Phy-DRL，一个物理模型调节的深度强化学习框架，用于安全关键的自主系统。Phy-DRL具有三种独特的创新：i）前瞻性的未知未知训练，ii）结合残差控制（即数据驱动控制和基于物理模型的控制的集成）和安全及稳定性敏感的奖励，以及iii）基于物理模型的神经网络编辑，包括链接编辑和激活编辑。由于这些并发设计，Phy-DRL能够1）容忍未知干扰，2）保证可数学证明的安全与稳定性，并3）严格遵守Bellman方程和奖励相关的物理知识。最终，通过倒立摆和四足机器人的实验验证了Phy-DRL的有效性。实验结果表明，与纯数据驱动的DRL相比，Phy-DRL具有明显更少的学习参数、加速的训练和扩大的回报。

    In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \& stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged rew
    
[^32]: 学习单调博弈的投石索方法

    A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])

    [http://arxiv.org/abs/2305.16610](http://arxiv.org/abs/2305.16610)

    本文提出了一种新的框架, 通过正则化游戏的支付或效用和更新投石索策略，无论是否存在噪声都能够实现在单调博弈中计算均衡。

    

    本文解决了在单调博弈中计算均衡的问题。传统的遵循正则化领导者算法即使在双人零和游戏中也无法收敛到均衡。虽然已经提出了这些算法的乐观版本并具有最后迭代的收敛保证，但它们需要无噪声的梯度反馈。为了克服这个限制，我们提出了一个新的框架，即使在存在噪声的情况下也能实现最后一次迭代的收敛。我们的关键思想是扰动或正则化游戏的支付或效用。这种扰动有助于将当前策略拉向一个锚定策略，我们称之为“投石索”策略。首先，我们建立了框架的收敛速度，从而获得靠近均衡点的稳定点，无论是否存在噪声。接下来，我们介绍了一种方法，定期更新投石索策略和当前策略。我们将这种方法解释为近端p

    In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
    
[^33]: 基于神经架构搜索的参数高效微调大型预训练语言模型

    Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])

    [http://arxiv.org/abs/2305.16597](http://arxiv.org/abs/2305.16597)

    本文提出了一种基于神经架构搜索的参数高效微调大型预训练语言模型的方法，通过结构化和非结构化剪枝学习PET结构并在GLUE上进行实验验证，展示了该算法的有效性，探讨了PET架构设计选择对实际性能的影响。

    

    参数高效微调（PET）方法通过计算部分模型参数的小型压缩更新或添加和微调少量新的模型参数到预训练网络，将预训练语言模型（PLM）适应下游任务。手工设计的PET架构在实践中表现良好，但通过自动神经架构搜索（NAS），它们有改进的潜力。我们提出了一种通过结构化和非结构化剪枝学习PET结构的有效NAS方法。我们在GLUE上进行了实验，展示了我们算法的有效性，并讨论了PET架构设计选择如何影响实际性能。

    Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.
    
[^34]: 多分辨率物理信息循环神经网络：构建与应用于肌肉骨骼系统

    A Multi-Resolution Physics-Informed Recurrent Neural Network: Formulation and Application to Musculoskeletal Systems. (arXiv:2305.16593v1 [cs.LG])

    [http://arxiv.org/abs/2305.16593](http://arxiv.org/abs/2305.16593)

    本文介绍了一种用于预测肌肉骨骼运动和 MSK 系统参数识别的多分辨率物理信息循环神经网络（MR PI-RNN），利用快速小波变换将混合频率的输入 sEMG 和输出关节运动信号分解为嵌套的多分辨率信号进行预测。

    

    本文介绍了一种多分辨率物理信息循环神经网络（MR PI-RNN），用于同时预测肌肉骨骼（MSK）运动和 MSK 系统参数的识别。由于高频表面肌电图（sEMG）信号与由 MSK 和肌肉收缩动力学控制的低频身体关节运动之间的映射的挑战性，因此选择了 MSK 应用作为模型问题。所提出方法利用快速小波变换将混合频率的输入 sEMG 和输出关节运动信号分解为嵌套的多分辨率信号。预测模型随后使用门控循环单元（GRU）在较粗的尺度输入-输出信号上进行训练，然后将训练好的参数转移到下一层细化的信号训练中。这些训练过程在转移学习的方式下递归地重复，直到完成全尺度训练（即使用未经滤波的信号）。

    This work presents a multi-resolution physics-informed recurrent neural network (MR PI-RNN), for simultaneous prediction of musculoskeletal (MSK) motion and parameter identification of the MSK systems. The MSK application was selected as the model problem due to its challenging nature in mapping the high-frequency surface electromyography (sEMG) signals to the low-frequency body joint motion controlled by the MSK and muscle contraction dynamics. The proposed method utilizes the fast wavelet transform to decompose the mixed frequency input sEMG and output joint motion signals into nested multi-resolution signals. The prediction model is subsequently trained on coarser-scale input-output signals using a gated recurrent unit (GRU), and then the trained parameters are transferred to the next level of training with finer-scale signals. These training processes are repeated recursively under a transfer-learning fashion until the full-scale training (i.e., with unfiltered signals) is achieved
    
[^35]: 具有生成模型的强化学习中分布鲁棒性的可疑价格

    The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model. (arXiv:2305.16589v1 [cs.LG])

    [http://arxiv.org/abs/2305.16589](http://arxiv.org/abs/2305.16589)

    本文研究了强化学习中的模型鲁棒性以缩小模拟与真实差距，提出了一个名为“分布鲁棒值迭代”的基于模型的方法，可以优化最坏情况下的表现。

    

    本文研究了强化学习中的模型鲁棒性，以减少在实践中的模拟与真实差距。我们采用分布鲁棒马尔可夫决策过程（RMDPs）框架，旨在学习一个策略，在部署环境落在预定的不确定性集合内时，优化最坏情况下的表现。尽管最近有了一些努力，但RMDPs的样本复杂度仍然没有得到解决，无论使用的不确定性集合是什么。不清楚分布鲁棒性与标准强化学习相比是否具有统计学上的影响。假设有一个生成模型，根据名义MDP绘制样本，我们将描述RMDPs的样本复杂度，当由总变差（TV）距离或$\chi^2$分歧指定不确定性集合时。在这里研究的算法是一种基于模型的方法，称为分布鲁棒值迭代，证明了它在整个范围内都是近乎最优的。

    This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL.  Assuming access to a generative model that draws samples based on the nominal MDP, we characterize the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or $\chi^2$ divergence. The algorithm studied here is a model-based method called {\em distributionally robust value iteration}, which is shown to be near-optimal for the full range
    
[^36]: 通过任意回归模型检测数值数据中的错误。

    Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])

    [http://arxiv.org/abs/2305.16583](http://arxiv.org/abs/2305.16583)

    该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。

    

    噪声困扰着许多数值数据集，其中数据记录的值可能由于错误的传感器、数据输入/处理错误或不完美的人类估计等原因而无法匹配真实的底层值。我们考虑估计沿数值列哪些数据值是不正确的。我们提出了一种模型不可知的方法，可以利用任何回归器（即基于数据集中的其他变量来预测该列值的统计学或机器学习模型）来解决问题。通过考虑各种不确定性，我们的方法区分了真正的异常和自然数据波动，条件是有可用的数据集信息。我们为我们的方法建立了理论保证，并表明其他方法（如符合性推断）难以检测错误。我们还提供了一个新的误差检测基准，涉及 5 个具有真实世界数字错误的回归数据集（对于其中的真实值）。

    Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
    
[^37]: 探索长尾识别问题中的权重平衡

    Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])

    [http://arxiv.org/abs/2305.16573](http://arxiv.org/abs/2305.16573)

    研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。

    

    长尾数据中的识别问题最近变得越来越重要，因为数据集中每个类别的样本数量分布通常是指数分布，除非有意地调整样本数量。针对这些问题已经提出了各种方法。最近，提出了权重平衡方法，它结合了著名的经典正则化技术和两阶段训练。尽管其简单性，但已知其对现有各种不同方法具有高性能。然而，我们缺乏为什么这种方法对长尾数据有效的理解。在这项研究中，我们分析了该方法，并关注了神经崩溃和每个训练阶段的圆锥效应，并发现它可以分解为由权值衰减和交叉熵损失引起的特征提取器中Fisher判别比的增加以及由权重衰减和类平衡正则化引起的隐式逻辑调整。我们还证明了权重平衡方法成功缓解了神经崩溃和圆锥效应，从而提高了长尾数据的识别性能。

    Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
    
[^38]: 基于锚定机制的值迭代加速算法

    Accelerating Value Iteration with Anchoring. (arXiv:2305.16569v1 [cs.LG])

    [http://arxiv.org/abs/2305.16569](http://arxiv.org/abs/2305.16569)

    本文提出了一种加速值迭代算法Anc-VI，采用了锚定机制，可加速Bellman一致性和最优性算子的计算。对于$\gamma\approx 1$或$\gamma=1$，Anc-VI速度为$\mathcal{O}(1/k)$，比标准VI更快。

    

    值迭代(Value Iteration, VI)是现代强化学习领域中理论和实践的基础，已知其收敛速度为$\mathcal{O}(\gamma^k)$，其中$\gamma$是折扣因子。然而，在VI设置中的最优速度尚未确定，寻求一种通用的加速机制一直是一个未解决的问题。本文提出了第一个基于锚定机制的VI加速算法，称为Anc-VI。不同于Nesterov的加速方法，Anc-VI可以加速Bellman一致性和最优性算子，还比标准VI更快地减少Bellman误差。尤其是，对于$\gamma\approx 1$或甚至$\gamma=1$，Anc-VI呈现出$\mathcal{O}(1/k)$的速度，而标准VI在$\gamma\ge 1-1/k$时的速度为$\mathcal{O}(1)$，其中$k$是迭代次数。我们还提供了与上界匹配的复杂性下界，除了一个常数因子$4$，从而证明了Anc-VI的加速速度的最优性。最后，我们在实验中证明了Anc-VI的有效性。

    Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a $\mathcal{O}(\gamma^k)$-rate, where $\gamma$ is the discount factor. Surprisingly, however, the optimal rate for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an \emph{anchoring} mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a $\mathcal{O}(1/k)$-rate for $\gamma\approx 1$ or even $\gamma=1$, while standard VI has rate $\mathcal{O}(1)$ for $\gamma\ge 1-1/k$, where $k$ is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of $4$, thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we sh
    
[^39]: 无监督嵌入质量评估

    Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v1 [cs.LG])

    [http://arxiv.org/abs/2305.16562](http://arxiv.org/abs/2305.16562)

    这项工作提出了评估嵌入质量的不同方法，关注如何以稳定的方式进行线性分离。从调查的文献和引入的新方法中，我们可以评估嵌入的质量，从而提高无监督学习的表现。(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)

    

    无监督学习，尤其是基于深度学习的方法最近在学术界得到了显著的发展。虽然在各种基准测试中取得了接近监督学习水平的成果，但由于无监督问题的本质，实践中训练和评估 SSL 模型仍然很困难。即使是以有监督的方式训练的网络，在转移到另一个领域时是否能够良好地表现，也往往不清楚。过去的工作通常仅限于评估嵌入中包含的信息量，这对于深度神经网络的自我监督学习最为相关。然而，这项工作选择了不同的方法：我们能否量化数据中如何以稳定的方式进行线性分离？我们调查了相关的文献，并发现三种方法可以用于评估嵌入的质量。此外，我们还介绍了一种基于近期对高维空间理解的最新进展的新方法。

    Unsupervised learning has recently significantly gained in popularity, especially with deep learning-based approaches. Despite numerous successes and approaching supervised-level performance on a variety of academic benchmarks, it is still hard to train and evaluate SSL models in practice due to the unsupervised nature of the problem. Even with networks trained in a supervised fashion, it is often unclear whether they will perform well when transferred to another domain.  Past works are generally limited to assessing the amount of information contained in embeddings, which is most relevant for self-supervised learning of deep neural networks. This works chooses to follow a different approach: can we quantify how easy it is to linearly separate the data in a stable way? We survey the literature and uncover three methods that could be potentially used for evaluating quality of representations. We also introduce one novel method based on recent advances in understanding the high-dimension
    
[^40]: 团队合作并不总是好的：类增量信息提取中分类器漂移的实证研究

    Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction. (arXiv:2305.16559v1 [cs.CL])

    [http://arxiv.org/abs/2305.16559](http://arxiv.org/abs/2305.16559)

    本论文研究了类增量信息提取中分类器漂移如何导致遗忘的问题，并提出了四种解决方案来缓解分类器漂移。

    

    类增量学习旨在开发一种学习系统，该系统可以不断从数据流中学习新类，而不会忘记之前学习过的类。然而，当学习增量类时，分类器必须不断更新以纳入新类，并且决策边界的漂移可能导致严重的遗忘。然而，这一根本性挑战尚未得到广泛研究，特别是在不存储旧类别样本以进行重演的情况下。本文更详细地研究了分类器漂移如何导致遗忘，并据此设计了四种简单但（超级）有效的解决方案来缓解分类器漂移。

    Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL&O which further take the logits of previously learned classes from old sessions or a constant logit of an Ot
    
[^41]: 基于树的扩散薛定谔桥算法在Wasserstein重心中的应用

    Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters. (arXiv:2305.16557v1 [stat.ML])

    [http://arxiv.org/abs/2305.16557](http://arxiv.org/abs/2305.16557)

    本文介绍了一种基于树的扩散薛定谔桥算法(TreeDSB)来解决多元最优输运(mOT)的问题，并可以应用于高维设置如图像插值和贝叶斯融合。

    

    多元最优输运(mOT)是最优输运(OT)的一种推广，其旨在最小化成本函数相对于某些预先指定的边际分布的积分。本文考虑了一个树形二次成本的熵版本，即一种可以写作树节点之间成对成本函数之和的函数。为了解决这个问题，我们开发了Tree-based Diffusion Schr\"odinger Bridge(TreeDSB)，这是扩展了扩散薛定谔桥(DSB)算法的算法。TreeDSB对应于多元Sinkhorn算法的动态连续状态空间。我们方法的一个显著应用是计算Wasserstein重心，它可以被重新转化为基于星形树的mOT问题的解决方案。我们证明了我们的方法可以应用于高维设置，如图像插值和贝叶斯融合。

    Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem, we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB), an extension of the Diffusion Schr\"odinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space counterpart of the multimarginal Sinkhorn algorithm. A notable use case of our methodology is to compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and Bayesian fusion.
    
[^42]: LANISTR：从结构化和非结构化数据中进行多模态学习

    LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])

    [http://arxiv.org/abs/2305.16556](http://arxiv.org/abs/2305.16556)

    LANISTR是一个新颖的基于注意力机制的框架，可从结构化和非结构化数据中进行学习，在挑战性数据集上表现优异。

    

    多模态的大规模预训练已经在处理非结构化数据（包括文本、图像、音频和视频）方面展现了令人瞩目的性能提升。但是，现实世界中最常见的情况是结构化（包括表格和时间序列）和非结构化数据的结合，但这一领域尚未得到充分的研究。为此，我们提出了LANISTR，这是一个新颖的基于注意力机制的框架，用于从语言、图像和结构化数据中进行学习。我们引入了一个新的多模态融合模块，并采用基于相似性的多模态遮罩损失，使得LANISTR能够在大规模多模态数据中学习跨模态关系，并在训练和测试时处理缺失的模态。在两个公开可用的具有挑战性的数据集MIMIC-IV和Amazon Product Review上，与最先进的多模态模型相比，LANISTR分别达到了6.47%（AUROC）和高达17.69%（准确度）的绝对提升，并显示出更强的泛化能力。

    Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
    
[^43]: 比较长短时记忆（LSTM）和双向LSTM深度神经网络在电力消耗预测中的应用

    Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction. (arXiv:2305.16546v1 [cs.LG])

    [http://arxiv.org/abs/2305.16546](http://arxiv.org/abs/2305.16546)

    本文比较了LSTM和BLSTM两种深度学习模型在电力消耗短期预测中的性能，通过四个数据集的结果表明BLSTM的表现更好。

    

    电力消耗预测方法是为了决策节能以及在能源市场中预测需求等多种原因而进行研究的。本研究旨在比较两种深度学习模型，即长短时记忆（LSTM）和双向LSTM（BLSTM），用于单变量电度量时间序列的短期预测。为了评估模型的鲁棒性，选择了四个不同上下文和规模的数据集。这些四个数据集分别是：（a）法国家庭的用电量；（b）巴西Santaém的一座大学建筑的用电量；（c）摩洛哥Tétouan市的用电需求；（d）新加坡聚合电力需求。采用时间序列交叉验证方案计算了RMSE、MAE、MAPE和R2等指标。对归一化RMSE（NRMSE）的结果应用了Friedman检验，表明BLSTM比LSTM表现更好并且具有统计显著性。

    Electric consumption prediction methods are investigated for many reasons such as decision-making related to energy efficiency as well as for anticipating demand in the energy market dynamics. The objective of the present work is the comparison between two Deep Learning models, namely the Long Short-Term Memory (LSTM) and Bi-directional LSTM (BLSTM) for univariate electric consumption Time Series (TS) short-term forecast. The Data Sets (DSs) were selected for their different contexts and scales, aiming the assessment of the models' robustness. Four DSs were used, related to the power consumption of: (a) a household in France; (b) a university building in Santar\'em, Brazil; (c) the T\'etouan city zones, in Morocco; and (c) the Singapore aggregated electric demand. The metrics RMSE, MAE, MAPE and R2 were calculated in a TS cross-validation scheme. The Friedman's test was applied to normalized RMSE (NRMSE) results, showing that BLSTM outperforms LSTM with statistically significant differ
    
[^44]: 通过图学习进行归纳式检测影响行动

    Inductive detection of Influence Operations via Graph Learning. (arXiv:2305.16544v1 [cs.LG])

    [http://arxiv.org/abs/2305.16544](http://arxiv.org/abs/2305.16544)

    本文开发了一种新的归纳式学习框架，可以检测影响行动的操作。该框架能够确定与任何操作无关的指标，使用图学习来编码协调操纵的抽象标志，并在不同操作之间进行跨操作泛化，有望为预防影响行动提供帮助。

    

    影响行动是一种大规模操作，旨在操纵公众舆论。快速检测和干扰这些操作对健康的公共话语至关重要。新兴AI技术可能会启用新的操作方法，这些方法可以规避当前的检测方法，并在社交媒体上影响更大的规模、范围和特异性。为了在这些新操作改变公众意见和事件之前识别这些新操作，需要具有归纳学习能力的新方法。我们开发了一种归纳学习框架，它：1)确定不特定于任何操作的基于内容和图形的指标；2)使用图学习来编码协调操纵的抽象标志；3)通过对来自俄罗斯、中国和伊朗的操作进行训练和测试来评估泛化能力。我们发现，这种框架能够在跨操作泛化方面展现出强大的能力，同时也揭示了显著的指标。

    Influence operations are large-scale efforts to manipulate public opinion. The rapid detection and disruption of these operations is critical for healthy public discourse. Emergent AI technologies may enable novel operations which evade current detection methods and influence public discourse on social media with greater scale, reach, and specificity. New methods with inductive learning capacity will be needed to identify these novel operations before they indelibly alter public opinion and events. We develop an inductive learning framework which: 1) determines content- and graph-based indicators that are not specific to any operation; 2) uses graph learning to encode abstract signatures of coordinated manipulation; and 3) evaluates generalization capacity by training and testing models across operations originating from Russia, China, and Iran. We find that this framework enables strong cross-operation generalization while also revealing salient indicators$\unicode{x2013}$illustrating
    
[^45]: 基于分数模型的神经波函数学习

    A Score-Based Model for Learning Neural Wavefunctions. (arXiv:2305.16540v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.16540](http://arxiv.org/abs/2305.16540)

    本研究提供了一种新的优化框架，使用基于分数的神经网络获得量子多体基态性质，通过Langevin动力学进行采样，准确计算基态。

    

    将量子蒙特卡罗与神经网络波函数相结合已经在计算量子多体系统的基态方面显示出成功。现有的优化方法通过从波函数给定的显式概率分布中采样局部能量来计算能量。在本文中，我们提供了一种用于使用基于分数的神经网络获得量子多体基态性质的新优化框架。我们的新框架不需要显式的概率分布，而是通过Langevin动力学进行采样。我们的方法基于局部能量与分数之间直接的关系这一关键观察。灵感来自分数配对和扩散蒙特卡罗方法，我们推导出了一个加权分数匹配目标，以引导基于分数的模型正确地收敛到基态。我们首先通过量子谐振子实验评估了我们的方法，结果表明它可以准确地计算出基态。

    Quantum Monte Carlo coupled with neural network wavefunctions has shown success in computing ground states of quantum many-body systems. Existing optimization approaches compute the energy by sampling local energy from an explicit probability distribution given by the wavefunction. In this work, we provide a new optimization framework for obtaining properties of quantum many-body ground states using score-based neural networks. Our new framework does not require explicit probability distribution and performs the sampling via Langevin dynamics. Our method is based on the key observation that the local energy is directly related to scores, defined as the gradient of the logarithmic wavefunction. Inspired by the score matching and diffusion Monte Carlo methods, we derive a weighted score matching objective to guide our score-based models to converge correctly to ground states. We first evaluate our approach with experiments on quantum harmonic traps, and results show that it can accuratel
    
[^46]: 对比学习学到了哪些特征？关于简易偏差在类坍塌和特征抑制中的作用

    Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression. (arXiv:2305.16536v1 [cs.LG])

    [http://arxiv.org/abs/2305.16536](http://arxiv.org/abs/2305.16536)

    对比学习是一种表示学习技术，对于有监督的情况易于产生类坍塌，无监督情况下易于抑制类别相关的复杂特征；随机梯度下降方法偏向于寻找更简单的解决方案是导致这种现象的关键因素。

    

    对比学习具备无监督和有监督学习的表示学习技术，在有监督场景下易于坍塌同一类别内的子类表示，丢失一部分特征信息；而无监督学习则可能通过学习易于处理的类别无关特征而无视一些类别相关的复杂特征信息，这两种方法都会显著地降低表征的质量。本文提出了第一个统一严谨的框架来理解测试时的类坍塌和特征抑制产生的原因，相关分析表明，（随机）梯度下降方法偏向于寻找更简单的解决方案是导致子类表示坍塌和类别相关的复杂特征被抑制的关键因素。此外，我们利用提高嵌入维度和改进数据增强的方法来提供有效的预防措施。

    Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of \textit{class collapse} or \textit{feature suppression} at \textit{test} time. We provide the first unified theoretically rigorous framework to determine \textit{which} features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations 
    
[^47]: 向量值变分空间和DNN的宽度界：关于权重衰减正则化的见解。

    Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. (arXiv:2305.16534v1 [stat.ML])

    [http://arxiv.org/abs/2305.16534](http://arxiv.org/abs/2305.16534)

    该论文提供了关于通过加权衰减训练的多输出ReLU神经网络的函数类型和相应的解决方案的新见解。

    

    深度神经网络(DNNs)通过梯度下降最小化损失项和平方权重和相应，对应于训练加权衰减的常见方法。本文提供了有关这种常见学习框架的新见解。我们表征了训练加权衰减以获得多输出(向量值)ReLU神经网络学习的函数类型。这扩展了先前限于单输出(标量值)网络的表征。这种表征需要定义我们称之为向量值变分(VV)空间的新类神经函数空间。我们通过一种新的表征定理证明，神经网络(NNs)是通过VV空间中提出学习问题的最优解。这个新的表征定理表明，这些学习问题的解存在于宽度受训练数据数限制的向量值神经网络中。接下来，通过与多任务lasso问题的新联系，我们导出了

    Deep neural networks (DNNs) trained to minimize a loss term plus the sum of squared weights via gradient descent corresponds to the common approach of training with weight decay. This paper provides new insights into this common learning framework. We characterize the kinds of functions learned by training with weight decay for multi-output (vector-valued) ReLU neural networks. This extends previous characterizations that were limited to single-output (scalar-valued) networks. This characterization requires the definition of a new class of neural function spaces that we call vector-valued variation (VV) spaces. We prove that neural networks (NNs) are optimal solutions to learning problems posed over VV spaces via a novel representer theorem. This new representer theorem shows that solutions to these learning problems exist as vector-valued neural networks with widths bounded in terms of the number of training data. Next, via a novel connection to the multi-task lasso problem, we derive
    
[^48]: 使用策略蒸馏的反事实解释器框架解释深度强化学习模型

    Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation. (arXiv:2305.16532v1 [cs.LG])

    [http://arxiv.org/abs/2305.16532](http://arxiv.org/abs/2305.16532)

    本文提出了一种新的基于反事实解释方法的框架来解释黑盒DRL所作的决策，并且在实验中展示了该解释框架的可行性和有效性。

    

    深度强化学习（DRL）已经展示出解决复杂控制问题的有希望能力。然而，在安全关键系统中，DRL应用受到缺乏强大的验证技术来保证在这些应用中的性能。验证过程的关键要求之一是开发有效的技术来解释系统的功能，即为什么系统在特定情况下产生特定结果。最近，提出了基于反事实（Counterfactual，CF）解释方法的解释方法来解决DRL中的解释问题。本文提出了一种新的CF解释框架，来解释黑盒DRL所作的决策。为了评估所提出的解释框架的有效性，我们在自动驾驶系统和Atari Pong游戏领域进行了几项实验。我们的分析表明，所提出的框架生成了合理和有意义的解释结果，同时保持了与原始DRL模型相比的高度真实性。

    Deep Reinforcement Learning (DRL) has demonstrated promising capability in solving complex control problems. However, DRL applications in safety-critical systems are hindered by the inherent lack of robust verification techniques to assure their performance in such applications. One of the key requirements of the verification process is the development of effective techniques to explain the system functionality, i.e., why the system produces specific results in given circumstances. Recently, interpretation methods based on the Counterfactual (CF) explanation approach have been proposed to address the problem of explanation in DRLs. This paper proposes a novel CF explanation framework to explain the decisions made by a black-box DRL. To evaluate the efficacy of the proposed explanation framework, we carried out several experiments in the domains of automated driving systems and Atari Pong game. Our analysis demonstrates that the proposed framework generates plausible and meaningful expl
    
[^49]: 双保真度变分自编码器用于不确定性量化

    Bi-fidelity Variational Auto-encoder for Uncertainty Quantification. (arXiv:2305.16530v1 [stat.ML])

    [http://arxiv.org/abs/2305.16530](http://arxiv.org/abs/2305.16530)

    本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。

    

    在模型验证中，量化物理系统感兴趣的量的不确定性是一个主要目标。然而，实现这一目标需要平衡计算效率和数值精度之间的需求。为了解决这个问题，我们提出了一种新颖的双保真度变分自编码器（BF-VAE）公式，旨在从物理系统中低、高保真度样本中估计与量感兴趣的量有关的不确定性。该模型通过利用从低保真度样本得出的信息来逼近高保真度量的统计信息。具体而言，我们设计了一个在潜在空间中的双保真度自回归模型，将其整合到VAE的概率编码-解码结构中。我们提出了一种有效的算法，以在存在有限高保真度数据的情况下，最大化高保真度对数似然的变分下界，从而以较低的计算成本合成高保真度的实现。此外，我们在各种数值示例中证明了我们方法的有效性，包括非线性随机系统和计算流体动力学模拟。

    Quantifying the uncertainty of quantities of interest (QoIs) from physical systems is a primary objective in model validation. However, achieving this goal entails balancing the need for computational efficiency with the requirement for numerical accuracy. To address this trade-off, we propose a novel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to estimate the uncertainty associated with a QoI from low-fidelity (LF) and high-fidelity (HF) samples of the QoI. This model allows for the approximation of the statistics of the HF QoI by leveraging information derived from its LF counterpart. Specifically, we design a bi-fidelity auto-regressive model in the latent space that is integrated within the VAE's probabilistic encoder-decoder structure. An effective algorithm is proposed to maximize the variational lower bound of the HF log-likelihood in the presence of limited HF data, resulting in the synthesis of HF realizations with a reduced computational cost. Addit
    
[^50]: 将可解释的提升机器算法应用于科学图像数据

    Extending Explainable Boosting Machines to Scientific Image Data. (arXiv:2305.16526v1 [cs.CV])

    [http://arxiv.org/abs/2305.16526](http://arxiv.org/abs/2305.16526)

    本文提出将可解释的提升机器算法应用于科学图像数据，通过在冷原子孤子图像数据上的应用，证明了这一方法的可行性。

    

    随着计算机视觉技术在医学或科学等重要应用中的应用越来越普遍，对系统输出结果的解释需求已成为关注的焦点。然而，许多当前先进的计算机视觉模型是不透明的，使得从解释角度使用它们变得具有挑战性，目前解释这些不透明模型的方法具有明显的局限性并受到严重批评。相比之下，可解释的提升机器算法是一类易于解释并且在性能上不逊于最佳模型的模型，但迄今为止，它们仅限于表格数据。在科学界对可解释模型的迫切需求驱动下，我们提出将可解释的提升机器算法应用于科学图像数据。我们以支撑量子技术发展的重要应用冷原子孤子图像数据为例，应用可解释的提升机器算法进行探究，从而证明了其应用的可行性。

    As the deployment of computer vision technology becomes increasingly common in applications of consequence such as medicine or science, the need for explanations of the system output has become a focus of great concern. Unfortunately, many state-of-the-art computer vision models are opaque, making their use challenging from an explanation standpoint, and current approaches to explaining these opaque models have stark limitations and have been the subject of serious criticism. In contrast, Explainable Boosting Machines (EBMs) are a class of models that are easy to interpret and achieve performance on par with the very best-performing models, however, to date EBMs have been limited solely to tabular data. Driven by the pressing need for interpretable models in science, we propose the use of EBMs for scientific image data. Inspired by an important application underpinning the development of quantum technologies, we apply EBMs to cold-atom soliton image data, and, in doing so, demonstrate 
    
[^51]: 面向零样本文本分类的标签无关预训练

    Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])

    [http://arxiv.org/abs/2305.16521](http://arxiv.org/abs/2305.16521)

    本文旨在探究改进预训练语言模型的泛化能力，提高其在零样本情境下的文本分类表现。通过引入隐式和显式预训练策略，注入方面级别的理解，以建立任务层次的表示。

    

    传统的文本分类方法通常假设存在一组固定的预定义标签，用于将给定的文本分类。然而，在现实世界的应用中，存在着用于描述给定文本的无限标签空间。此外，根据文本的方面（情感、主题等）和领域（金融、法律等），标签的解释可能大不相同。这使得文本分类任务，特别是在零样本场景下，变得非常具有挑战性。在本文中，我们探讨了零样本文本分类的任务，旨在提高预训练语言模型（PLMs）对不同方面和领域中已知和未知数据的泛化能力。为了解决这个问题，我们引入了两种新的简单而有效的预训练策略，即隐式和显式预训练。这些方法在训练时注入了方面级别的理解，目的是让模型构建任务层次的表示。

    Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-l
    
[^52]: 深度神经网络的滑动窗口求和算法

    Sliding Window Sum Algorithms for Deep Neural Networks. (arXiv:2305.16513v1 [cs.LG])

    [http://arxiv.org/abs/2305.16513](http://arxiv.org/abs/2305.16513)

    本文介绍了一系列针对深度神经网络训练和推理的通用矢量化滑动和算法，提高了计算速度，比常用的算法更有效，能够表达池化和卷积原语。

    

    滑动窗口求和广泛应用于字符串索引、哈希和时间序列分析。我们开发了一系列通用矢量化滑动和算法，对于窗口大小w和处理器数量P，提供了O（P/w）的加速。对于可交换运算符的总和，加速可以提高到O（P/log(w)）。更重要的是，我们的算法表现出高效的内存访问模式。在本文中，我们研究了将滑动和算法应用于深度神经网络的训练和推理。我们展示了如何将池化和卷积原语表达为滑动和，并通过具有共享结构的计算内核进行评估。我们表明，滑动和卷积内核比CPU上通常使用的GEMM内核更有效，甚至可以胜过它们的GPU同行。

    Sliding window sums are widely used for string indexing, hashing and time series analysis. We have developed a family of the generic vectorized sliding sum algorithms that provide speedup of O(P/w) for window size $w$ and number of processors P. For a sum with a commutative operator the speedup is improved to O(P/log(w)). Even more important, our algorithms exhibit efficient memory access patterns. In this paper we study the application of the sliding sum algorithms to the training and inference of the Deep Neural Networks. We demonstrate how both pooling and convolution primitives could be expressed as sliding sums and evaluated by the compute kernels with the shared structure. We show that the sliding sum convolution kernels are more efficient than the commonly used GEMM kernels on the CPU, and could even outperform their GPU counterparts.
    
[^53]: 大部分神经网络几乎是可学习的

    Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])

    [http://arxiv.org/abs/2305.16508](http://arxiv.org/abs/2305.16508)

    本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。

    

    我们提出了一个PTAS来学习随机常数深度网络。我们证明了对于任何固定的$\epsilon>0$和深度$i$，存在一个多项式时间算法，对于$\sqrt{d} \cdot \mathbb{S}^{d-1}$上的任何分布，学习随机Xavier网络的深度$i$，误差为$\epsilon$。该算法的时间和样本复杂度为$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$，其中$\bar d$是网络的大小。对于某些类似于Sigmoid和ReLU的激活函数，可以将误差界限改进为$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$，从而得到一种几乎多项式时间算法来学习常数深度随机网络。

    We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
    
[^54]: 奖励机制指导下的自适应强化学习

    Reward-Machine-Guided, Self-Paced Reinforcement Learning. (arXiv:2305.16505v1 [cs.LG])

    [http://arxiv.org/abs/2305.16505](http://arxiv.org/abs/2305.16505)

    本研究提出一种基于奖励机制来指导自适应强化学习的算法，该算法可信赖地实现最优行为，在复杂任务中提高了强化学习的数据效率。

    

    自适应强化学习旨在通过自动创建上下文概率分布序列来提高学习的数据效率。然而，现有的自适应强化学习技术在涉及时间上延长的行为的长期计划任务中失败。我们假设利用关于底层任务结构的先前知识可以提高自适应强化学习的有效性。我们开发了一种自适应强化学习算法，其基于奖励机制来进行指导。该算法将奖励机制整合到1）通过任何选择的强化学习算法获得的策略和价值函数的更新中，以及2）生成上下文分布的自动课程表的更新中。我们的实证结果表明，所提出的算法可靠地实现最优行为，即使是现有基线无法取得任何有意义的进展的情况下也可行。它还降低了在复杂任务中所需的数据量，从而提高了强化学习的数据效率。

    Self-paced reinforcement learning (RL) aims to improve the data efficiency of learning by automatically creating sequences, namely curricula, of probability distributions over contexts. However, existing techniques for self-paced RL fail in long-horizon planning tasks that involve temporally extended behaviors. We hypothesize that taking advantage of prior knowledge about the underlying task structure can improve the effectiveness of self-paced RL. We develop a self-paced RL algorithm guided by reward machines, i.e., a type of finite-state machine that encodes the underlying task structure. The algorithm integrates reward machines in 1) the update of the policy and value functions obtained by any RL algorithm of choice, and 2) the update of the automated curriculum that generates context distributions. Our empirical results evidence that the proposed algorithm achieves optimal behavior reliably even in cases in which existing baselines cannot make any meaningful progress. It also decre
    
[^55]: 开源大语言模型对工具操作能力的研究

    On the Tool Manipulation Capability of Open-source Large Language Models. (arXiv:2305.16504v1 [cs.CL])

    [http://arxiv.org/abs/2305.16504](http://arxiv.org/abs/2305.16504)

    本研究探讨了如何通过训练使用示例、上下文演示和生成样式规则来加强开源LLMs以达到与封闭型API的工具操作性能同等甚至更优的效果，并通过ToolBench测试得出了实验结果，同时本文还证明了改进的开源LLMs的鲁棒性。

    

    近期对使用大型语言模型( LLMs)进行软件工具操作的研究大多依赖于封闭模型API。由于向封闭LLMAPI服务公开信息存在安全和鲁棒性风险，这些模型的工业采用受到了实质性限制。本文提出了一个问题，那就是我们能否在实践中加强开源LLMs的功能，使其在工具操作方面与领先的封闭LLM APIs竞争。通过分析常见的工具操作失败，我们首先展示了开源LLMs可能需要训练使用示例、上下文演示和生成样式规则来解决失败。这些见解激发我们重新审视LLM文献中的经典方法，并证明我们可以将它们作为程序数据生成的模型对齐、系统提示和上下文演示检索器来适应开源LLMs以实现工具操作的增强。为了评估这些技术，我们创建了ToolBench，一个工具操作能力测试套件，包括现有API和我们改进的开源LLMs。在三个不同的编程任务上，我们发现改进的开源LLMs能够达到或超越现有API的性能，其中包括对已编写的程序进行轻微修改等实际操作。此外，我们通过反向工程测试和黑盒测试进一步证明了模型的鲁棒性。

    Recent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs. The industrial adoption of these models is substantially constrained due to the security and robustness risks in exposing information to closed LLM API services. In this paper, we ask can we enhance open-source LLMs to be competitive to leading closed LLM APIs in tool manipulation, with practical amount of human supervision. By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures. These insights motivate us to revisit classical methods in LLM literature, and demonstrate that we can adapt them as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation. To evaluate these techniques, we create the ToolBench, a tool manipulation 
    
[^56]: 未知个性化操纵下的战略分类

    Strategic Classification under Unknown Personalized Manipulation. (arXiv:2305.16501v1 [cs.LG])

    [http://arxiv.org/abs/2305.16501](http://arxiv.org/abs/2305.16501)

    探讨了在未知且个性化操纵影响下的战略分类问题，提出了一个交互模型，引入了个性化的定义，旨在解决强化学习中策略操纵问题。

    

    我们研究了在战略分类中的基础错误界限和样本复杂度，其中代理可以在一定程度上战略性地操纵其特征向量以预测为正。例如，给定一个确定大学录取的分类器，学生候选人可能会尝试选择更容易的课程来提高他们的GPA，重新参加SAT并更换学校，以尝试欺骗分类器。 在文献中，球操纵是一个广泛研究的操纵类别，代理可以在有界半径球内修改其特征向量。与大多数先前的工作不同，我们的工作认为操纵是个性化的，这意味着代理可以拥有不同水平的操纵能力（例如，球体操纵的变化半径），并且对学习者是未知的。我们在一个交互模型中形式化学习问题，其中学习者首先部署分类器，代理在其操纵集合内操纵特征向量以操作部署的分类器。

    We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. Ball manipulations are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work considers manipulations to be personalized, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and unknown to the learner.  We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classif
    
[^57]: AD-NEV：一种可扩展的多层神经进化框架用于多元异常检测

    AD-NEV: A Scalable Multi-level Neuroevolution Framework for Multivariate Anomaly Detection. (arXiv:2305.16497v1 [cs.LG])

    [http://arxiv.org/abs/2305.16497](http://arxiv.org/abs/2305.16497)

    AD-NEv提出了一种可扩展的多级优化神经进化框架，用于多元时间序列异常检测，通过协同优化特征子空间、模型架构和模型权重的方法，表现出比当前最先进的方法更好的检测精度和计算效率。

    

    异常检测工具和方法在现代的智能物理系统和故障预测系统中具有关键能力。尽管深度学习架构在异常检测中的发展迅速，但针对给定数据集的模型优化是一个繁琐而耗时的过程。神经进化可以是这个问题的有效和高效解决方案，是一种完全自动化的搜索方法，用于学习最优的神经网络，支持梯度和非梯度的微调。然而，现有的方法大多集中于优化模型结构，而未考虑特征子空间和模型权重。在本研究中，我们提出了一种名为Anomaly Detection Neuroevolution (AD-NEv)的可扩展多级优化神经进化框架，用于多元时间序列异常检测。该方法表示一种新颖的方法来协同优化：i) 基于装袋技术对集合模型进行特征子空间优化; ii) 优化单个异常检测网络的模型架构; iii) 使用梯度和非梯度方法优化模型权重。所提出的框架可扩展，可应用于不同大小和维度的数据集。基准数据集上的实验结果表明，AD-NEv在检测精度和计算效率方面表现优于或可与当前最先进的方法相媲美。

    Anomaly detection tools and methods present a key capability in modern cyberphysical and failure prediction systems. Despite the fast-paced development in deep learning architectures for anomaly detection, model optimization for a given dataset is a cumbersome and time consuming process. Neuroevolution could be an effective and efficient solution to this problem, as a fully automated search method for learning optimal neural networks, supporting both gradient and non-gradient fine tuning. However, existing methods mostly focus on optimizing model architectures without taking into account feature subspaces and model weights. In this work, we propose Anomaly Detection Neuroevolution (AD-NEv) - a scalable multi-level optimized neuroevolution framework for multivariate time series anomaly detection. The method represents a novel approach to synergically: i) optimize feature subspaces for an ensemble model based on the bagging technique; ii) optimize the model architecture of single anomaly
    
[^58]: SAMoSSA：带随机自回归噪声的多元奇异谱分析

    SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise. (arXiv:2305.16491v1 [cs.LG])

    [http://arxiv.org/abs/2305.16491](http://arxiv.org/abs/2305.16491)

    该论文介绍了一种新的时间序列分析方法，即SAMoSSA。该方法综合了多元奇异谱分析和自回归分析，在学习时间序列中的确定性和随机性成分方面具有良好的理论保证。

    

    时间序列分析的惯例是先估计确定性、非平稳趋势和季节成分，然后学习残差随机、平稳成分。最近已经表明，在没有相关平稳成分的情况下，可以使用多元奇异谱分析（mSSA）准确地学习确定性非平稳成分；同时，在没有确定性非平稳成分的情况下，自回归（AR）平稳成分也可以轻松学习，例如通过普通最小二乘（OLS）。然而，尽管这种两个步骤的学习算法已经普遍存在，但关于同时涉及确定性和平稳成分的多阶段学习算法的理论支撑在文献中还没有解决。我们通过为一种自然的两阶段算法建立理论保证来解决这个开放性问题，其中首先应用mSSA来估计非平稳成分，尽管存在相关性平稳成分。

    The well-established practice of time series analysis involves estimating deterministic, non-stationary trend and seasonality components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correla
    
[^59]: 批次模型整合：一个多任务模型整合框架

    Batch Model Consolidation: A Multi-Task Model Consolidation Framework. (arXiv:2305.16484v1 [cs.LG])

    [http://arxiv.org/abs/2305.16484](http://arxiv.org/abs/2305.16484)

    本文提出了批次模型整合（BMC）来支持更现实的连续学习，它通过在正则化阶段训练多个专家模型来学习一组不相交的任务，并在整合阶段将多个专家模型整合为一个模型。

    

    在连续学习中，模型需要按顺序学习一系列任务，而不会在之前学习的任务上出现显着的性能下降。现有方法在面对各种领域和难度的长序列任务时效果不佳。许多现有的连续学习方法由于内存资源消耗过大或训练时间过长而难以在实践中应用，或只能在单个设备上紧密耦合。本文提出批次模型整合（BMC）来支持更现实的连续学习，面对多个代理在各种任务中接触的情况。在正则化阶段，BMC并行训练多个专家模型来学习一组不相交的任务。每个专家通过稳定性损失与一个基础模型保持权重相似性，并从任务数据的一部分构建缓冲区。在整合阶段，我们将多个专家模型整合为一个模型，并在目标任务上进行微调。

    In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation ($\textbf{BMC}$) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a $\textit{regularization}$ phase, BMC trains multiple $\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a $\textit{base model}$ through a $\textit{stability loss}$, and constructs a $\textit{buffer}$ from a fraction of the task's data. During the $\textit{consolidation}$ phase, we combine the 
    
[^60]: 增强样本下混合系统中有效强化学习及其在排队网络中的应用。

    Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks. (arXiv:2305.16483v1 [cs.LG])

    [http://arxiv.org/abs/2305.16483](http://arxiv.org/abs/2305.16483)

    本文提出了一种在混合系统中通过增强数据样本进行学习的方法，应用于排队网络问题，能够显著提高学习效率和降低样本复杂度。

    

    本文研究了一类强化学习问题，涉及具有两种状态的系统：随机状态和伪随机状态。在这种系统中，随机状态遵循随机转移核而伪随机状态的转移是在给定随机状态/转移的情况下是确定的。我们称这样的系统为混合系统。这种系统被广泛应用于各种应用，包括制造系统、通信网络和排队网络。我们提出了一种能够通过产生增强的数据样本来加速学习的样本高效的强化学习方法。该算法是数据驱动的，并从实际和增强样本的数据样本中学习策略，从而显著改善了学习的效果。我们分析了在Fitted Q Iteration（FQI）下提出的方法的样本复杂度，并证明了学习策略的最优性差随着真实数据样本数量的增加而减少。我们将我们的方法应用于解决一类排队网络问题，其中混合状态空间自然而然地出现。

    This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic given the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the opti
    
[^61]: 线性预测器和神经网络的初始化相关样本复杂度

    Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks. (arXiv:2305.16475v1 [cs.LG])

    [http://arxiv.org/abs/2305.16475](http://arxiv.org/abs/2305.16475)

    本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。

    

    我们提供了关于向量值线性预测器(由矩阵参数化)、更一般的神经网络的样本复杂性的新结果。专注于大小无关的界限，在这种情况下，仅控制从某个固定参考矩阵$W_0$的参数的Frobenius范数距离，我们展示了样本复杂度行为可以出人意料地不同于我们在研究标量值线性预测器方面所期望的。这还导致了前馈神经网络的新样本复杂度界限，解决了一些文献中存在的问题，并确立了一个新的凸线性预测问题，证明了它可以在没有统一收敛的情况下被学习。

    We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.
    
[^62]: FairDP: 具有差分隐私认证的公平性保障

    FairDP: Certified Fairness with Differential Privacy. (arXiv:2305.16474v1 [cs.LG])

    [http://arxiv.org/abs/2305.16474](http://arxiv.org/abs/2305.16474)

    FairDP是一种同时确保差分隐私和公平性的新型机制，通过独立为不同的个体群体训练模型，在训练过程中逐步整合来自群体模型的知识，制定综合模型以平衡隐私、效用和公平性的下游任务。相比现有方法，FairDP展示了更好的模型效益、隐私和公平性的权衡。

    

    本文介绍了一种名为FairDP的新型机制，旨在同时确保差分隐私(DP)和公平性。FairDP通过独立为不同的个体群体训练模型，在使用组特定的剪裁项来评估和限制DP的差异影响的同时操作。在训练过程中，该机制逐步整合来自群体模型的知识，制定综合模型以平衡隐私、效用和公平性的下游任务。广泛的理论和实证分析验证了FairDP的功效，与现有方法相比，展示了更好的模型效益、隐私和公平性的权衡。

    This paper introduces FairDP, a novel mechanism designed to simultaneously ensure differential privacy (DP) and fairness. FairDP operates by independently training models for distinct individual groups, using group-specific clipping terms to assess and bound the disparate impacts of DP. Throughout the training process, the mechanism progressively integrates knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. Extensive theoretical and empirical analyses validate the efficacy of FairDP, demonstrating improved trade-offs between model utility, privacy, and fairness compared with existing methods.
    
[^63]: 美国避难案件中的偏见、一致性和党派性：对移民法庭决定中的无关因素进行机器学习分析。

    Bias, Consistency, and Partisanship in U.S. Asylum Cases: A Machine Learning Analysis of Extraneous Factors in Immigration Court Decisions. (arXiv:2305.16471v1 [cs.SI])

    [http://arxiv.org/abs/2305.16471](http://arxiv.org/abs/2305.16471)

    本文使用机器学习分析美国避难案件中的个人和系统性偏见，发现美国避难决定往往受到与案件实质无关的因素的影响，而且是否授予申请人庇护主要取决于政治环境和审判法官的个体变异。

    

    在这项研究中，我们引入了一个新的双重评分系统，来衡量美国移民法庭（EOIR）中的个人和系统性偏见。我们分析了近600万个移民法庭诉讼和228个案件特征，以进一步研究美国避难决定如何受到与案件实质无关的因素的影响。使用预测建模，我们使用两个指标：党派性和法官间一致性，解释了总决定变异的58.54％。因此，EOIR是否授予申请人庇护，主要取决于政治环境和审判法官的个体变异 - 而不是案件的个体实质。使用时间序列分析，我们还证明党派性在1990年代初期增加，但在之后平稳了。

    In this study, we introduce a novel two-pronged scoring system to measure individual and systemic bias in immigration courts under the U.S. Executive Office of Immigration Review (EOIR). We analyze nearly 6 million immigration court proceedings and 228 case features to build on prior research showing that U.S. asylum decisions vary dramatically based on factors that are extraneous to the merits of a case. We close a critical gap in the literature of variability metrics that can span space and time. Using predictive modeling, we explain 58.54% of the total decision variability using two metrics: partisanship and inter-judge cohort consistency. Thus, whether the EOIR grants asylum to an applicant or not depends in majority on the combined effects of the political climate and the individual variability of the presiding judge - not the individual merits of the case. Using time series analysis, we also demonstrate that partisanship increased in the early 1990s but plateaued following the tu
    
[^64]: 评估不同群体对影响力信息的反应

    Measuring the Effect of Influential Messages on Varying Personas. (arXiv:2305.16470v1 [cs.CL])

    [http://arxiv.org/abs/2305.16470](http://arxiv.org/abs/2305.16470)

    研究了评估不同群体对影响力信息的反应的任务和所创建的数据集，突出了新任务在建模中引入了个性化，预测了每个反应的情感极性和强度，并让评估和应用更加可靠。

    

    预测用户对新闻事件的反应能够实现智能代理或内容生成者估计不同社区的影响并修订未发布的信息，防止社会冲突和道德伤害。本文提出了一项新任务：用于新闻媒体的人设反应预测，以预测人设（描述个人或群体）对新闻信息的反应。与以往仅预测新闻的通用评论的工作相比，所提出的任务不仅在建模中引入了个性化，还预测了每个反应的情感极性和强度。这使得对人设的心理状态进行更准确和全面的推断成为可能。同时，生成的情感维度使得评估和应用更加可靠。我们创建了第一个基准数据集，其中包括来自Twitter的3,847个新闻标题的13,357个反应。

    Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. W
    
[^65]: 基于贝叶斯强化学习的电压自动控制在网络攻击不确定性下的应用

    Bayesian Reinforcement Learning for Automatic Voltage Control under Cyber-Induced Uncertainty. (arXiv:2305.16469v1 [cs.LG])

    [http://arxiv.org/abs/2305.16469](http://arxiv.org/abs/2305.16469)

    本文提出了一种基于贝叶斯强化学习的数据驱动方法，解决网络攻击引起的不确定性对自动电压控制的影响，其在部分可观测的马尔可夫决策问题中实现了自动控制，并能够自动寻找探索和开发的阈值。

    

    电压控制对于大规模电力系统的可靠运作至关重要，以及时提供无功补偿有助于防止普遍停电。然而，目前电力系统没有内置机制来确保在对手存在的不确定性下维护可靠的运行电压控制目标的生存或延续。因此，本文介绍了一种基于贝叶斯强化学习（BRL）的电力系统控制问题，重点关注在网络攻击环境下不确定性的持续电压控制。该工作通过构建和解决一个部分可观测的马尔可夫决策问题(POMDP)中的数据驱动的基于BRL的电压自动控制方法，其中由于网络攻击而造成的状态是部分可观测的。技术在WSCC和IEEE 14总线系统上进行了评估。此外，BRL技术可帮助自动找到各种RL技术中的探索和开发的阈值。

    Voltage control is crucial to large-scale power system reliable operation, as timely reactive power support can help prevent widespread outages. However, there is currently no built in mechanism for power systems to ensure that the voltage control objective to maintain reliable operation will survive or sustain the uncertainty caused under adversary presence. Hence, this work introduces a Bayesian Reinforcement Learning (BRL) approach for power system control problems, with focus on sustained voltage control under uncertainty in a cyber-adversarial environment. This work proposes a data-driven BRL-based approach for automatic voltage control by formulating and solving a Partially-Observable Markov Decision Problem (POMDP), where the states are partially observable due to cyber intrusions. The techniques are evaluated on the WSCC and IEEE 14 bus systems. Additionally, BRL techniques assist in automatically finding a threshold for exploration and exploitation in various RL techniques.
    
[^66]: 由互补结构表征技术链接和交叉重建表征数据的 PairVAE

    Pair-Variational Autoencoders (PairVAE) for Linking and Cross-Reconstruction of Characterization Data from Complementary Structural Characterization Techniques. (arXiv:2305.16467v1 [cond-mat.soft])

    [http://arxiv.org/abs/2305.16467](http://arxiv.org/abs/2305.16467)

    本文介绍了一种机器学习模型 PairVAE，可以训练多种互补结构表征技术的数据，使其能够在不同技术之间交叉重建表征数据。

    

    在材料研究中，结构表征通常需要多种互补技术获得综合的形态视图。由于不同技术的可用性和可访问性（例如散射、显微镜、光谱），每个研究机构或学术研究实验室可能只能在一种技术上具有高通量能力，但在其他技术上面临限制。此外，一种类型的结构表征数据可能比另一种更容易解释。因此，有用的是具有可训练多种结构表征技术的机器学习模型，以便该模型可以从一组数据生成另一组数据。本文展示了这样一种机器学习工作流程，PairVAE。

    In material research, structural characterization often requires multiple complementary techniques to obtain a holistic morphological view of the synthesized material. Depending on the availability of and accessibility of the different characterization techniques (e.g., scattering, microscopy, spectroscopy), each research facility or academic research lab may have access to high-throughput capability in one technique but face limitations (sample preparation, resolution, access time) with other techniques(s). Furthermore, one type of structural characterization data may be easier to interpret than another (e.g., microscopy images are easier to interpret than small angle scattering profiles). Thus, it is useful to have machine learning models that can be trained on paired structural characterization data from multiple techniques so that the model can generate one set of characterization data from the other. In this paper we demonstrate one such machine learning workflow, PairVAE, that wo
    
[^67]: 优化的自定义数据集用于高效检测水下垃圾

    Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])

    [http://arxiv.org/abs/2305.16460](http://arxiv.org/abs/2305.16460)

    本文提出了一种自定义数据集和有效检测方法，旨在通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。

    

    准确评估和清除潜在的水下废物对于保护海洋生物和环境至关重要。本文针对水下垃圾检测所存在的挑战，如光折射、吸收、悬浮颗粒和色彩扭曲等因素，提出了一种自定义数据集和有效检测方法。该数据集涵盖了多种水下环境，并包括对废弃物实例的精确定位标注。最终，使用最先进的深度学习结构，目的是通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。

    Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
    
[^68]: 基于表示的Jensen-Shannon散度

    The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])

    [http://arxiv.org/abs/2305.16446](http://arxiv.org/abs/2305.16446)

    本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。

    

    统计散度量化概率分布之间的差异，是机器学习中的一种重要方法。但是，由于数据的底层分布通常未知，从经验样本中估计散度是一个基本难题。本文提出了一种基于再生核希尔伯特空间(RKHS)中协方差算子的新型散度——表示Jensen-Shannon散度。我们的方法将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱。我们提供了一个从经验协方差矩阵估计的估计函数，它通过使用Fourier特征将数据映射到RKHS中。此估计函数是灵活、可扩展、可微分的，并且适用于小批量优化问题。此外，我们还提供了一种基于核矩阵的估计函数，而不需要对RKHS进行显式映射。我们证明这个量是Jensen-Shannon散度的一个下界。

    Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
    
[^69]: 多个预训练模型的表示迁移学习在线性回归中的研究

    Representation Transfer Learning via Multiple Pre-trained models for Linear Regression. (arXiv:2305.16440v1 [cs.LG])

    [http://arxiv.org/abs/2305.16440](http://arxiv.org/abs/2305.16440)

    本文提出了一种基于表示迁移的学习方法，在给定很少数样本的情况下，通过提供一组在可能不同的数据领域上训练的预训练回归模型，来构建目标模型，使用这种方法可以提高模型的样本复杂度。

    

    本文研究了在给定很少数样本的情况下，如何在感兴趣的数据领域（目标）上学习线性回归模型。我们提出了一种基于表示迁移的学习方法，通过提供一组在可能不同的数据领域（来源）上训练的预训练回归模型，来构建目标模型。该方法由两个阶段组成：（i）利用不同的源表示来构造适应目标数据的表示，（ii）将所得到的模型作为初始值，通过微调程序，在目标数据上重新训练整个（超参数）回归模型。对于训练方法的每个阶段，我们提供了学习模型与真实数据生成目标模型之间的超额风险限制。导出的限制显示了样本复杂度的提高。

    In this paper, we consider the problem of learning a linear regression model on a data domain of interest (target) given few samples. To aid learning, we are provided with a set of pre-trained regression models that are trained on potentially different data domains (sources). Assuming a representation structure for the data generating linear models at the sources and the target domains, we propose a representation transfer based learning method for constructing the target model. The proposed scheme is comprised of two phases: (i) utilizing the different source representations to construct a representation that is adapted to the target data, and (ii) using the obtained model as an initialization to a fine-tuning procedure that re-trains the entire (over-parameterized) regression model on the target data. For each phase of the training method, we provide excess risk bounds for the learned model compared to the true data generating target model. The derived bounds show a gain in sample co
    
[^70]: 神经（切向核）崩溃

    Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])

    [http://arxiv.org/abs/2305.16427](http://arxiv.org/abs/2305.16427)

    本文介绍了神经切向核（NTK）和神经崩溃（NC）之间的关系，证明了在具有块状NTK的DNN中会出现NC，并通过大规模实验支持理论的正确性。

    

    本文介绍了两个重要的概念：神经切向核（NTK），它捕捉深度神经网络（DNN）训练期间的演化和神经崩溃（NC）现象，它指的是经过良好训练的分类DNN最后一层特征中对称性和结构的出现。我们假设经验NTK与类标签对齐并形成块状结构，即同一类别的样本之间的相关性比不同类别的样本更强，基于这个假设，我们推导了使用均方误差（MSE）训练的DNN动态，并将其分解为可解释的阶段。此外，我们确定了一种不变量，捕捉了动态的本质，并用它证明了在具有块状NTK的DNN中会出现NC。我们进行了三种常见DNN架构和三个基准数据集的大规模数值实验来支持我们的理论。

    This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
    
[^71]: SketchOGD：内存高效的持续学习

    SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])

    [http://arxiv.org/abs/2305.16424](http://arxiv.org/abs/2305.16424)

    SketchOGD提出了一种内存高效的解决灾难性遗忘的方法，通过采用在线草图算法，将模型梯度压缩为固定大小的矩阵，从而改进了现有的算法——正交梯度下降（OGD）。

    

    当机器学习模型在一系列任务上持续训练时，它们容易忘记先前任务上学习到的知识，这种现象称为灾难性遗忘。现有的解决灾难性遗忘的方法往往涉及存储过去任务的信息，这意味着内存使用是确定实用性的主要因素。本文提出了一种内存高效的解决灾难性遗忘的方法，改进了一种已有的算法——正交梯度下降（OGD）。OGD利用先前模型梯度来找到维持先前数据点性能的权重更新。然而，由于存储先前模型梯度的内存成本随算法运行时间增长而增加，因此OGD不适用于任意长时间跨度的连续学习。针对这个问题，本文提出了SketchOGD。SketchOGD采用在线草图算法，将模型梯度压缩为固定大小的矩阵。

    When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
    
[^72]: 异构数据下的联邦神经压缩

    Federated Neural Compression Under Heterogeneous Data. (arXiv:2305.16416v1 [cs.LG])

    [http://arxiv.org/abs/2305.16416](http://arxiv.org/abs/2305.16416)

    本文介绍了在异构数据下的联邦神经压缩问题，提出了一个分布式源模型和个性化熵模型的解决方案，在联邦设置中，使用全局共享表示优于本地方法。

    

    本文探讨了一种联邦学习压缩问题，其目标是从散布在客户端上且可能是统计异构的真实世界数据中学习一个压缩器，但保有共同的潜在表示。我们提出了一个分布式源模型，既包括这两种特征，也自然地提出了一种压缩器结构，该结构使用由客户端共享的分析和合成变换。受个性化联邦学习方法的启发，我们采用了一个对每个客户端个性化的熵模型。这允许在客户端之间学习全局潜在空间，并个性化地调整以适应客户端的潜在分布。经验证明，该策略优于仅使用本地方法，这表明在统计异构的联邦设置中，学习压缩也受益于共享的全局表示。

    We discuss a federated learned compression problem, where the goal is to learn a compressor from real-world data which is scattered across clients and may be statistically heterogeneous, yet share a common underlying representation. We propose a distributed source model that encompasses both characteristics, and naturally suggests a compressor architecture that uses analysis and synthesis transforms shared by clients. Inspired by personalized federated learning methods, we employ an entropy model that is personalized to each client. This allows for a global latent space to be learned across clients, and personalized entropy models that adapt to the clients' latent distributions. We show empirically that this strategy outperforms solely local methods, which indicates that learned compression also benefits from a shared global representation in statistically heterogeneous federated settings.
    
[^73]: GrowSP：3D点云无监督语义分割

    GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds. (arXiv:2305.16404v1 [cs.CV])

    [http://arxiv.org/abs/2305.16404](http://arxiv.org/abs/2305.16404)

    本文提出了一种名为GrowSP的无监督方法来进行3D场景中每个点复杂语义类别的识别和分割，方法通过逐步增长超级点的大小来发现3D语义元素，并优于所有无监督基线。

    

    本论文研究从原始点云中进行3D语义分割的问题。与现有方法主要依赖于大量的人工注释来训练神经网络不同，我们提出了第一个完全无监督的方法GrowSP，成功地为3D场景中的每个点识别出复杂的语义类别，无需任何形式的人工标签或预训练模型。我们的方法的关键是通过超级点的逐步增长来发现3D语义元素。我们的方法由三个主要组成部分组成：1）特征提取器从输入点云中学习每个点的特征；2）超级点构造器逐步增加超级点的大小；3）语义原始聚类模块将超级点分组成语义元素以实现最终的语义分割。我们在多个数据集上进行了广泛的评估，证明了我们的方法优于所有无监督基线，并接近经典完全监督的PointNet模型的表现。

    We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we propose the first purely unsupervised method, called GrowSP, to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels or pretrained models. The key to our approach is to discover 3D semantic elements via progressive growing of superpoints. Our method consists of three major components, 1) the feature extractor to learn per-point features from input point clouds, 2) the superpoint constructor to progressively grow the sizes of superpoints, and 3) the semantic primitive clustering module to group superpoints into semantic elements for the final semantic segmentation. We extensively evaluate our method on multiple datasets, demonstrating superior performance over all unsupervised baselines and approaching the classic fully-supervised PointN
    
[^74]: 基于支持向量机引导再生核颗粒方法的微观结构图像建模

    Support Vector Machine Guided Reproducing Kernel Particle Method for Image-Based Modeling of Microstructures. (arXiv:2305.16402v1 [cs.LG])

    [http://arxiv.org/abs/2305.16402](http://arxiv.org/abs/2305.16402)

    本文基于支持向量机引导再生核颗粒方法，提出了一种自动化数字表示复合材料的构建方案，并且引入了一个新的界面修改的再生核颗粒法以适当近似弱不连续性。

    

    本文提出了一种方法，通过支持向量机（SVM）分类引导来自动进行数字复合材料的构建，该复合材料由具有复杂微结构的微CT图像构成。引入SVM软间隔训练过程，对异质材料点进行分类，通过识别支持向量通过本地常规化优化问题实现图像分割。此外，还提出了一个界面修改的再生核颗粒法（IM-RKPM），用于适当近似跨材料界面的弱间歇性。提出的方法通过在材料界面处使用常规化重馈函数来修改平滑内核函数以减轻吉布斯振荡。该IM-RKPM是在不引入重复自由度的情况下制定的。

    This work presents an approach for automating the discretization and approximation procedures in constructing digital representations of composites from Micro-CT images featuring intricate microstructures. The proposed method is guided by the Support Vector Machine (SVM) classification, offering an effective approach for discretizing microstructural images. An SVM soft margin training process is introduced as a classification of heterogeneous material points, and image segmentation is accomplished by identifying support vectors through a local regularized optimization problem. In addition, an Interface-Modified Reproducing Kernel Particle Method (IM-RKPM) is proposed for appropriate approximations of weak discontinuities across material interfaces. The proposed method modifies the smooth kernel functions with a regularized heavy-side function concerning the material interfaces to alleviate Gibb's oscillations. This IM-RKPM is formulated without introducing duplicated degrees of freedom
    
[^75]: ADLER -- 一种高效的基于Hessian的自适应学习率策略

    ADLER -- An efficient Hessian-based strategy for adaptive learning rate. (arXiv:2305.16396v1 [cs.LG])

    [http://arxiv.org/abs/2305.16396](http://arxiv.org/abs/2305.16396)

    该论文提出了一种基于Hessian矩阵逼近的自适应学习率策略，该方法计算量较小且在分类任务上效果显著，可广泛用于CNN模型中。

    

    我们得出了一种有效的、半正定的Hessian矩阵逼近方法，使得计算Hessian矩阵-向量积变得容易。这使我们能够提供一种基于局部二次逼近的自适应SGD学习率策略，其仅需要计算单个SGD所需的两倍计算量，但其在不同模型结构（具有和不具有残差连接的CNN）的分类任务上表现与SGD学习率的网格搜索相当。我们还与Gauss-Newton逼近方法进行比较。

    We derive a sound positive semi-definite approximation of the Hessian of deep models for which Hessian-vector products are easily computable. This enables us to provide an adaptive SGD learning rate strategy based on the minimization of the local quadratic approximation, which requires just twice the computation of a single SGD run, but performs comparably with grid search on SGD learning rates on different model architectures (CNN with and without residual connections) on classification tasks. We also compare the novel approximation with the Gauss-Newton approximation.
    
[^76]: 利用神经网络将小行星反照率建模为其 Proper Orbital Elements 的函数

    Using neural networks to model Main Belt Asteroid albedos as a function of their proper orbital elements. (arXiv:2305.16392v1 [astro-ph.EP])

    [http://arxiv.org/abs/2305.16392](http://arxiv.org/abs/2305.16392)

    本文利用神经网络以小行星 Proper Orbital Elements 为函数建模尺寸难以观测的小行星反照率，将 NEOWISE 任务中的可见和红外反照率建模为函数，并发现它们与小行星在带中的位置显着相关，相比仅采用平均值模型，集合模型将反照率误差降低了约 37％。

    

    传统上很难估计小行星的直径。当直接观测不可能通过食变或直接雷达观测测量直径时，最常见的方法是从红外观测中近似估算直径。一旦直径知道，便可以利用可见光观测与之比较得到物体的可见几何反照率。NEOWISE 任务提供了最大的小行星反照率数据集，可同时测量可见光和红外光的反照率。我们使用一组神经网络将这些反照率建模为可从 Asteroid Families Portal 获取的 Proper Elements 的函数。我们发现可见和红外几何反照率都与小行星在带中的位置显着相关，并且在小行星族和背景带中都存在。与仅采用平均值模型相比，我们发现集合的预测将反照率的平均误差降低了约 37％。

    Asteroid diameters are traditionally difficult to estimate. When a direct measurement of the diameter cannot be made through either occultation or direct radar observation, the most common method is to approximate the diameter from infrared observations. Once the diameter is known, a comparison with visible light observations can be used to find the visible geometric albedo of the body. One of the largest datasets of asteroid albedos comes from the NEOWISE mission, which measured asteroid albedos both in the visible and infrared. We model these albedos as a function of proper elements available from the Asteroid Families Portal using an ensemble of neural networks. We find that both the visible and infrared geometric albedos are significantly correlated with asteroid position in the belt and occur in both asteroid families and in the background belt. We find that the ensemble's prediction reduces the average error in albedo by about 37% compared to a model that simply adopts an average
    
[^77]: 基于图的无模型数据子采样在推荐系统中的应用研究

    Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems. (arXiv:2305.16391v1 [cs.IR])

    [http://arxiv.org/abs/2305.16391](http://arxiv.org/abs/2305.16391)

    本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。

    

    数据子采样广泛用于加速训练大规模推荐系统。大多数子采样方法是基于模型的，常常需要一个预训练的试验模型来通过样本难度等方式测量数据重要性。然而，当试验模型被错误指定时，基于模型的子采样方法将会恶化。鉴于试验模型的错误指定在真实的推荐系统中普遍存在，我们提出了基于数据结构，即图形来探索的无模型数据子采样方法。具体地，我们研究用户-物品图的拓扑结构，通过图导电性来估计每个用户-物品交互（即用户-物品图中的一条边）的重要性，并在网络上进行传播步骤，平滑估计的重要性值。由于我们提出的方法是无模型的，因此我们可以将无模型和基于模型的子采样方法的优点结合起来。我们的实证研究表明，将这两种方法组合使用，在多个基准数据集上均比任何单一方法都要好。

    Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.  Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single meth
    
[^78]: DPOK: 强化学习用于微调文本到图像扩散模型

    DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])

    [http://arxiv.org/abs/2305.16381](http://arxiv.org/abs/2305.16381)

    本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。

    

    已经证明，从人类反馈中学习可以改善文本到图像模型。这些技术首先学习一个捕捉任务中人类关心的特征的奖励函数，然后根据学习到的奖励函数改进模型。虽然已经研究了相对简单的方法（例如基于奖励得分的拒绝采样），但使用奖励函数微调文本到图像模型仍然具有挑战性。在这项工作中，我们提出使用在线强化学习（RL）来微调文本到图像模型。我们专注于扩散模型，将微调任务定义为RL问题，并使用策略梯度更新预训练文本到图像扩散模型，以最大化反馈训练奖励。我们的方法DPOK集成了KL正则化的策略优化。我们对RL微调和监督微调的KL正则化进行了分析。在我们的实验中，我们展示了DPOK通常优于使用交叉熵损失的监督微调和以前的RL微调技术。DPOK在COCO数据集上实现了最先进的性能，IS和FID得分显著优于现有方法。

    Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w
    
[^79]: 扫描与拍照：理解1层Transformer中的训练动态和标记组成

    Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])

    [http://arxiv.org/abs/2305.16380](http://arxiv.org/abs/2305.16380)

    本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。

    

    Transformer架构在多个研究领域表现出了惊人的性能，并成为许多神经网络模型的基础。然而，我们对其如何工作的理解仍然有限。特别是，通过简单的预测性损失，表示如何从梯度训练动态中出现仍然是一个谜。在本文中，针对具有一个自我关注层和一个解码器层的1层Transformer，我们以数学严谨的方式分析其在下一个标记预测任务中的SGD训练动态。我们打开了自我关注层组合输入标记的动态过程的黑盒子，并揭示了底层归纳偏差的本质。具体而言，在没有位置编码、长输入序列和解码器层学习速度快于自我关注层的假设下，我们证明了自我关注层充当了“区分性扫描算法”：从均匀注意力开始，它逐渐关注到相关标记，排除不相关的标记，直到所有相关信息被扫描并总结在编码表示中。我们的分析还显示了标记频率和上下文如何影响注意权重，以及自我关注层初始化如何影响收敛速度。

    Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
    
[^80]: 有效增强视觉强化学习的样本利用率：以少学更好

    Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. (arXiv:2305.16379v1 [cs.LG])

    [http://arxiv.org/abs/2305.16379](http://arxiv.org/abs/2305.16379)

    本研究发现，对于数据增强在视觉强化学习中的有效性，空间多样性和轻微的困难度不可或缺。并提出了一种新的DA操作——Rand PR，它提供了丰富的空间多样性和最小的困难度，已经在多种数据上得到了有效性验证。

    

    数据增强（DA）是增强视觉强化学习（RL）算法的样本效率的关键技术。值得注意的是，仅使用简单的观察变换就可以在不进行额外辅助表示任务或预训练编码器的情况下获得出色的性能。然而，仍然不清楚DA的哪些属性是实现样本效率视觉RL的有效性的原因。为了调查这个问题并进一步探索DA的潜力，本文进行了全面的实验，评估了DA属性对其有效性的影响，并提供以下见解和改进：（1）对于单个DA操作，我们揭示了充足的空间多样性和轻微的困难度都是不可缺少的。基于这一发现，我们引入了一种新的DA操作——随机PadResize（Rand PR），它提供了丰富的空间多样性和最小的困难度。（2）对于多类型的DA融合方案，增加的DA困难度和不稳定的数据分布

    Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution 
    
[^81]: 神经网络宽度与数据拓扑特征相关的上界探究

    Data Topology-Dependent Upper Bounds of Neural Network Widths. (arXiv:2305.16375v1 [cs.LG])

    [http://arxiv.org/abs/2305.16375](http://arxiv.org/abs/2305.16375)

    本文引入了数据拓扑相关的神经网络宽度上界，并通过拓扑方法证明了三层ReLU网络的普适逼近性质。

    

    本文研究了深度神经网络普适逼近性质与数据拓扑特征之间的关系。我们的主要贡献是引入了数据拓扑相关的网络宽度上界。具体而言，我们首先证明了一个三层神经网络，应用ReLU激活函数和最大池化，可以设计来逼近一个在紧凑凸多面体内封装的指示函数。然后我们将其扩展到一个单纯复合体，基于其拓扑结构推导宽度上界。此外，我们还通过选择拓扑空间的Betti数计算上界。最后，我们通过拓扑方法证明了三层ReLU网络的普适逼近性质。我们还验证了梯度下降算法收敛于本研究提出的网络结构。

    This paper investigates the relationship between the universal approximation property of deep neural networks and topological characteristics of datasets. Our primary contribution is to introduce data topology-dependent upper bounds on the network width. Specifically, we first show that a three-layer neural network, applying a ReLU activation function and max pooling, can be designed to approximate an indicator function over a compact set, one that is encompassed by a tight convex polytope. This is then extended to a simplicial complex, deriving width upper bounds based on its topological structure. Further, we calculate upper bounds in relation to the Betti numbers of select topological spaces. Finally, we prove the universal approximation property of three-layer ReLU networks using our topological approach. We also verify that gradient descent converges to the network structure proposed in our study.
    
[^82]: DeepGate2: 功能感知的电路表示学习

    DeepGate2: Functionality-Aware Circuit Representation Learning. (arXiv:2305.16373v1 [cs.LG])

    [http://arxiv.org/abs/2305.16373](http://arxiv.org/abs/2305.16373)

    本文介绍了DeepGate2, 一个新的功能感知学习框架，其通过利用成对真值表差异作为训练监督，明确考虑电路功能，来提高电路表示学习的学习效果和效率。

    

    电路表示学习旨在获得电路元件的神经表示，并已成为可以应用于各种EDA和逻辑推理任务的有前途的研究方向。现有的解决方案，例如DeepGate，可以嵌入电路结构信息和功能行为。然而，它们的能力受到弱监督或错误的模型设计的限制，导致下游任务的性能令人不满意。在本文中，我们介绍了DeepGate2，这是一个新颖的功能感知学习框架，其在学习效果和效率方面显着优于原DeepGate解决方案。我们的方法涉及使用样本逻辑门之间的成对真值表差异作为训练监督，以及一个经过精心设计和可扩展的损失函数，明确考虑电路功能。此外，我们考虑电路的固有特性，并设计了一个高效的一轮图表达方法，以实现更好的性能。

    Circuit representation learning aims to obtain neural representations of circuit elements and has emerged as a promising research direction that can be applied to various EDA and logic reasoning tasks. Existing solutions, such as DeepGate, have the potential to embed both circuit structural information and functional behavior. However, their capabilities are limited due to weak supervision or flawed model design, resulting in unsatisfactory performance in downstream tasks. In this paper, we introduce DeepGate2, a novel functionality-aware learning framework that significantly improves upon the original DeepGate solution in terms of both learning effectiveness and efficiency. Our approach involves using pairwise truth table differences between sampled logic gates as training supervision, along with a well-designed and scalable loss function that explicitly considers circuit functionality. Additionally, we consider inherent circuit characteristics and design an efficient one-round graph 
    
[^83]: 应用于材料学领域高维无监督聚类任务的等向度量

    Metrics for quantifying isotropy in high dimensional unsupervised clustering tasks in a materials context. (arXiv:2305.16372v1 [cs.LG])

    [http://arxiv.org/abs/2305.16372](http://arxiv.org/abs/2305.16372)

    该论文提出了一种新的对聚类等向性度量的实现方法，并使用分数各向异性扩展了这些度量来检查聚类的平均等向性。通过量化不同材料结构数据库表示的核逼近函数对结果聚类的影响，演示了这种度量的实际应用。

    

    聚类是机器学习中常见的任务，但无标签数据的聚类可能难以量化。化学中的聚类算法通常依赖于材料表示。由于数据的维度，确定不同表示、聚类算法或数据变换对结果聚类的影响是困难的。我们提出了一种对聚类等向性度量的详细分析，包括一种基于现有推导的新的实现方法。使用分数各向异性，这是一种用于比较医学成像常用的方法，我们进而扩展这些度量，以检查一组聚类的平均等向性。通过量化不同材料结构数据库表示的核逼近函数对结果聚类的影响，演示了这些度量的用例。这些方法的广泛适用性也在分析MNIST数据集的学习嵌入中得到了证明。随机聚类

    Clustering is a common task in machine learning, but clusters of unlabelled data can be hard to quantify. The application of clustering algorithms in chemistry is often dependant on material representation. Ascertaining the effects of different representations, clustering algorithms, or data transformations on the resulting clusters is difficult due to the dimensionality of these data. We present a thorough analysis of measures for isotropy of a cluster, including a novel implantation based on an existing derivation. Using fractional anisotropy, a common method used in medical imaging for comparison, we then expand these measures to examine the average isotropy of a set of clusters. A use case for such measures is demonstrated by quantifying the effects of kernel approximation functions on different representations of the Inorganic Crystal Structure Database. Broader applicability of these methods is demonstrated in analysing learnt embedding of the MNIST dataset. Random clusters are e
    
[^84]: Stecformer：基于空间-时间编码串联变压器的多元长期时间序列预测

    Stecformer: Spatio-temporal Encoding Cascaded Transformer for Multivariate Long-term Time Series Forecasting. (arXiv:2305.16370v1 [cs.LG])

    [http://arxiv.org/abs/2305.16370](http://arxiv.org/abs/2305.16370)

    Stecformer是一种处理多元长期时间序列预测的方法，提出了一种有效的空间-时间编码器和级联解码预测器（CDP），在多个基准数据集上实现了最先进的性能，为多元长期时间序列预测提供了一种有前途的方法。

    

    多元长期时间序列预测在诸多领域中具有重要应用，如能源消耗和天气预报等。随着基于变压器的方法的发展，多元长期时间序列预测的性能得到了显著提高，但变压器模型中对于空间特征的研究较少，而不同预测周期之间的一致性也由于时间跨度较大而不尽人意。本文提出了一种完整的解决方案，以解决特征提取和目标预测方面的问题。对于特征提取，我们设计了一种有效的空间-时间编码器，包括半自适应图形来获取足够的空间-时间信息。对于预测，我们提出了级联解码预测器（CDP），以加强不同间隔之间的相关性，也可以用作通用的组件来提高基于变压器的方法的性能。所提出的方法称为Stecformer，在多个基准数据集上实现了最先进的性能，并为多元长期时间序列预测提供了一种有前途的方法。

    Multivariate long-term time series forecasting is of great application across many domains, such as energy consumption and weather forecasting. With the development of transformer-based methods, the performance of multivariate long-term time series forecasting has been significantly improved, however, the study of spatial features extracting in transformer-based model is rare and the consistency of different prediction periods is unsatisfactory due to the large span. In this work, we propose a complete solution to address these problems in terms of feature extraction and target prediction. For extraction, we design an efficient spatio-temporal encoding extractor including a semi-adaptive graph to acquire sufficient spatio-temporal information. For prediction, we propose a Cascaded Decoding Predictor (CDP) to strengthen the correlation between different intervals, which can also be utilized as a generic component to improve the performance of transformer-based methods. The proposed meth
    
[^85]: 神经不完全分解：学习共轭梯度法的预处理器

    Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])

    [http://arxiv.org/abs/2305.16368](http://arxiv.org/abs/2305.16368)

    本文提出了一种名为神经不完全分解的新方法，利用自监督训练的图神经网络生成适用于特定问题域的有效预处理器。其通过替换传统手工预处理器显着提高了收敛和计算效率，在合成和真实问题上进行的实验均表现出竞争力。

    

    本文提出了一种新型的数据驱动方法，用于加速科学计算和优化中遇到的大规模线性方程组求解。我们的方法利用自监督训练图神经网络，生成适用于特定问题域的有效预处理器。通过替换与共轭梯度法一起使用的传统手工预处理器，我们的方法（称为神经不完全分解）显着加速了收敛和计算效率。我们的方法的核心是一种受稀疏矩阵理论启发的新型消息传递块，它与寻找矩阵的稀疏分解的目标相一致。我们在合成问题和来自科学计算的真实问题上评估了我们的方法。我们的结果表明，神经不完全分解始终优于最常见的通用预处理器，包括不完全的Cholesky方法，在收敛速度和计算效率方面表现出竞争力。

    In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
    
[^86]: 大语言模型角色扮演

    Role-Play with Large Language Models. (arXiv:2305.16367v1 [cs.CL])

    [http://arxiv.org/abs/2305.16367](http://arxiv.org/abs/2305.16367)

    本文将对话代理行为描述为角色扮演，以避免赋予其人类特征，在此基础上研究代理行为中的欺骗和自我意识。

    

    随着对话代理程序在表现上越来越接近人类，有必要开发有效的方式高层次描述其行为，而不会陷入赋予其人类特征的陷阱。本文提出了角色扮演的概念，将对话代理程序的行为视为角色扮演，使我们能够借鉴熟悉的民间心理学术语，而不是赋予它们实际上并不具备的人类特征。本文以(表面上的)欺骗和(表面上的)自我意识为例，探讨了对话代理程序行为的两种情况。

    As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.
    
[^87]: 减少谜团：基于子目标的演示学习在形式定理证明中的应用

    Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. (arXiv:2305.16366v1 [cs.CL])

    [http://arxiv.org/abs/2305.16366](http://arxiv.org/abs/2305.16366)

    本文提出了一个基于子目标的演示学习框架，通过将基于子目标的学习方法与扩散模型相结合，提高演示的可理解性，并提高LLMs在形式定理证明中的吞吐量。

    

    大型语言模型（LLMs）在形式定理证明领域提供了有趣的探索方向。然而，如何完全利用这些模型，特别是在演示格式和组织方面，仍然是一个未被充分探索的领域。为了增强LLMs的效能，作者提出了一个基于子目标的演示学习框架，包括两个主要元素：第一，从强化学习和机器人领域的子目标学习中汲取经验，为每个演示示例构建不同的子目标，并根据相关的子目标学习理论来优化这些子目标。第二，利用最近扩散模型的进展来预测最佳组织方式，同时解决演示组织领域中存在的两个复杂问题：子集选择和顺序确定。通过将基于子目标的学习方法与扩散模型相结合，作者提出的框架可以提高演示的可理解性，并提高LLMs在形式定理证明中的吞吐量。

    Large language models~(LLMs) present an intriguing avenue of exploration in the domain of formal theorem proving. Nonetheless, the full utilization of these models, particularly in terms of demonstration formatting and organization, remains an underexplored area. In an endeavor to enhance the efficacy of LLMs, we introduce a subgoal-based demonstration learning framework, consisting of two primary elements: Firstly, drawing upon the insights of subgoal learning from the domains of reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Secondly, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Through the integration of subgoal-based learning methodolog
    
[^88]: E2EAI：面向主动投资的端到端深度学习框架

    E2EAI: End-to-End Deep Learning Framework for Active Investing. (arXiv:2305.16364v1 [q-fin.PM])

    [http://arxiv.org/abs/2305.16364](http://arxiv.org/abs/2305.16364)

    本文提出了一个面向主动投资的端到端深度学习框架（E2EAI），包含了因子选择、因子组合、股票选择和投资组合构建的整个过程，并通过对真实股票市场数据的实验证明了其有效性。

    

    主动投资旨在构建一组在市场上被认为相对有利可图的投资组合，其中一种流行的方法是通过基于因子的策略来构建投资组合。近年来，越来越多的努力将深度学习应用于追求具有更高主动回报或有前途的资产趋势预测流水线的“深度因子”。然而，如何通过端到端深度学习框架（E2E）构建主动投资组合的问题仍然是开放的，在现有文献中很少得到解决。在本文中，我们首次提出了一个 E2E，几乎涵盖了因子选择、因子组合、股票选择和投资组合构建的整个过程。对真实股票市场数据的广泛实验表明，我们的端到端深度学习框架在主动投资中的有效性。

    Active investing aims to construct a portfolio of assets that are believed to be relatively profitable in the markets, with one popular method being to construct a portfolio via factor-based strategies. In recent years, there have been increasing efforts to apply deep learning to pursue "deep factors'' with more active returns or promising pipelines for asset trends prediction. However, the question of how to construct an active investment portfolio via an end-to-end deep learning framework (E2E) is still open and rarely addressed in existing works. In this paper, we are the first to propose an E2E that covers almost the entire process of factor investing through factor selection, factor combination, stock selection, and portfolio construction. Extensive experiments on real stock market data demonstrate the effectiveness of our end-to-end deep leaning framework in active investing.
    
[^89]: 增加亚群模型性能的集成合成电子病历生成

    Ensemble Synthetic EHR Generation for Increasing Subpopulation Model's Performance. (arXiv:2305.16363v1 [cs.LG])

    [http://arxiv.org/abs/2305.16363](http://arxiv.org/abs/2305.16363)

    本文提出了一种利用生成模型集成合成数据的方法，以提高训练机器学习模型在代表不足的亚群体的性能。

    

    电子病历（EHR）通常包含某些亚群体（SP）的不同比例表示。患者的人口统计学、临床情况的流行程度和医疗中心类型等因素导致这种不充分的代表性。因此，当在这种数据集上训练机器学习模型时，模型很难进行良好的概括并在代表不足的SP上表现不佳。为了解决这个问题，我们提出了一种利用生成模型的新型集成框架。具体来说，我们为每个SP训练一个基于GAN的合成数据生成器，并将合成样本纳入每个SP的训练集中。最终，我们训练SP特定的预测模型。为了正确评估该方法，我们设计了一个评估流程，并使用从MIMIC数据库查询的两个真实用例数据集。我们的方法显示出在代表不足的SP上增加了模型的性能。我们的代码和模型将作为补充材料提供，并将在公共存储库上提供。

    Electronic health records (EHR) often contain different rates of representation of certain subpopulations (SP). Factors like patient demographics, clinical condition prevalence, and medical center type contribute to this underrepresentation. Consequently, when training machine learning models on such datasets, the models struggle to generalize well and perform poorly on underrepresented SPs. To address this issue, we propose a novel ensemble framework that utilizes generative models. Specifically, we train a GAN-based synthetic data generator for each SP and incorporate synthetic samples into each SP training set. Ultimately, we train SP-specific prediction models. To properly evaluate this method, we design an evaluation pipeline with 2 real-world use case datasets, queried from the MIMIC database. Our approach shows increased model performance over underrepresented SPs. Our code and models are given as supplementary and will be made available on a public repository.
    
[^90]: 一个关于可解释性方法评估的实验研究

    An Experimental Investigation into the Evaluation of Explainability Methods. (arXiv:2305.16361v1 [cs.LG])

    [http://arxiv.org/abs/2305.16361](http://arxiv.org/abs/2305.16361)

    这篇论文比较了14种不同的评估指标在对9种目前最先进的可解释性人工智能（XAI）方法和三种虚拟方法进行应用时的效果，给出了高度相关结果，指出了存在潜在冗余。此外，还展示了基线超参数对评估指标值产生显著影响。

    

    可解释的人工智能（XAI）旨在帮助用户理解人工智能系统背后的推理过程。近年来出现了许多XAI方法，相应地，与XAI方法评估相关的子领域引起了人们的关注，旨在确定使用各种方法和标准提供最佳解释的方法。然而，文献缺乏对评估指标本身的比较，这些指标可以用于评估XAI方法。本文旨在通过比较14种不同的指标在对九种最先进的XAI方法和三种虚拟方法（例如随机显著性图）进行应用时的效果，来填补这一空白。实验结果显示，哪些指标产生高度相关的结果，表明存在潜在的冗余性。我们还展示了基线超参数对评估指标值的显著影响。最后，我们使用虚拟方法评估了实验结果。

    EXplainable Artificial Intelligence (XAI) aims to help users to grasp the reasoning behind the predictions of an Artificial Intelligence (AI) system. Many XAI approaches have emerged in recent years. Consequently, a subfield related to the evaluation of XAI methods has gained considerable attention, with the aim to determine which methods provide the best explanation using various approaches and criteria. However, the literature lacks a comparison of the evaluation metrics themselves, that one can use to evaluate XAI methods. This work aims to fill this gap by comparing 14 different metrics when applied to nine state-of-the-art XAI methods and three dummy methods (e.g., random saliency maps) used as references. Experimental results show which of these metrics produces highly correlated results, indicating potential redundancy. We also demonstrate the significant impact of varying the baseline hyperparameter on the evaluation metric values. Finally, we use dummy methods to assess the re
    
[^91]: 在多变量软传感器中建模任务关系的平衡专家混合模型

    Modeling Task Relationships in Multi-variate Soft Sensor with Balanced Mixture-of-Experts. (arXiv:2305.16360v1 [cs.LG])

    [http://arxiv.org/abs/2305.16360](http://arxiv.org/abs/2305.16360)

    本文提出的平衡专家混合模型 (BMoE) 通过多门专家混合模块 (MMoE) 和 任务梯度平衡 (TGB) 模块来描述任务关系和平衡训练过程，在解决数据效率和负面传递问题方面有很好的效果。

    

    准确估计多个质量变量对于构建工业软传感器模型至关重要，然而，这一直面临数据效率和负面传递问题。本文提出了一个平衡的专家混合模型（BMoE），它由多门专家混合模块（MMoE）和任务梯度平衡（TGB）模块组成。MoE模块旨在描述任务关系，而TGB模块动态平衡任务之间的梯度，两者共同解决了负面传递问题。在典型的硫回收装置上的实验表明，BMoE有效地建模了任务关系和平衡了训练过程，并且明显优于基准模型。

    Accurate estimation of multiple quality variables is critical for building industrial soft sensor models, which have long been confronted with data efficiency and negative transfer issues. Methods sharing backbone parameters among tasks address the data efficiency issue; however, they still fail to mitigate the negative transfer problem. To address this issue, a balanced Mixture-of-Experts (BMoE) is proposed in this work, which consists of a multi-gate mixture of experts (MMoE) module and a task gradient balancing (TGB) module. The MoE module aims to portray task relationships, while the TGB module balances the gradients among tasks dynamically. Both of them cooperate to mitigate the negative transfer problem. Experiments on the typical sulfur recovery unit demonstrate that BMoE models task relationship and balances the training process effectively, and achieves better performance than baseline models significantly.
    
[^92]: 带扰动生成树的可微聚类方法

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。

    

    我们介绍了一种基于最小权重生成树的可微聚类方法，它是生成树的一种变体，具有多个连通分量。我们的方法依赖于线性规划解的随机扰动，以实现平滑和高效的梯度计算。这使我们能够在端到端可训练的流水线中包含聚类。我们证明了我们的方法即使在嘈杂的数据集和具有挑战性的几何环境下也能良好地工作。我们还利用这种方法制定了一个特别的损失，以有效地从部分聚类数据学习。我们在几个现实世界的数据集上展示了它在监督和半监督任务中的表现。

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^93]: WeiAvg：促进数据多样性的联邦学习模型聚合方法

    WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity. (arXiv:2305.16351v1 [cs.LG])

    [http://arxiv.org/abs/2305.16351](http://arxiv.org/abs/2305.16351)

    本文提出了一种名为WeiAvg的联邦学习模型聚合方法，通过强调来自高多样性客户端的更新并减少来自低多样性客户端的影响，提高了联邦模型的质量和性能。

    

    联邦学习为利用大规模私有边缘数据提供了一种有前景的隐私保护方式，尤其适用于物联网设备。然而，现有的研究主要集中在优化学习过程、计算效率和通信开销等方面，忽略了参与者对联邦模型质量的影响。本文提出了一种新的方法，通过引入一种基于加权平均（WeiAvg）的框架，着重强调来自高多样性客户端的更新，并减少来自低多样性客户端的影响，从而解决了这个问题。具体而言，我们引入了基于投影的近似方法，来评估多样性。

    Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity 
    
[^94]: Lexinvariant语言模型

    Lexinvariant Language Models. (arXiv:2305.16349v1 [cs.CL])

    [http://arxiv.org/abs/2305.16349](http://arxiv.org/abs/2305.16349)

    本文讨论了一种新型的语言模型，称为Lexinvariant语言模型，该模型不需要任何固定标记嵌入，完全依赖上下文中标记的共现和重复。作者证明可以构建一个lexinvariant LM，以多项式方式与上下文长度成比例地收敛到真实语言模型，其常量因子在词汇表大小下为次线性。

    

    令牌嵌入是从离散词汇符号到连续向量的映射，是任何语言模型（LM）的核心。但是，词汇符号的含义也可以通过它们在长上下文中的结构角色来确定甚至重新定义。在本文中，我们问：是否可能存在一种没有任何固定标记嵌入的性能良好的语言模型？这样的语言模型将完全依赖于上下文中标记的共现和重复，而不是任何标记的\textit{a priori}标识。为了回答这个问题，我们研究了\textit{lexinvariant}语言模型，这些语言模型对词汇符号不变，因此在实践中不需要固定的令牌嵌入。首先，我们证明可以构建一个lexinvariant LM，以多项式方式与上下文长度成比例地收敛到真实语言模型，其常量因子在词汇表大小下为次线性。其次，要构建一个lexinvariant LM，我们只需使用随机高斯函数对标记进行编码。

    Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \textit{a priori} identity of any token. To answer this, we study \textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gauss
    
[^95]: 基于机器学习的生物质水热碳化产物特性表征：在可持续能源和材料生产中的应用

    Machine learning-based characterization of hydrochar from biomass: Implications for sustainable energy and material production. (arXiv:2305.16348v1 [cs.LG])

    [http://arxiv.org/abs/2305.16348](http://arxiv.org/abs/2305.16348)

    本文提出了一种使用机器学习表征生物质水热碳化产物特性的方法，能够准确预测在不同处理条件下从各种生物质来源产生的水炭的质量和数量，为生物质的可持续能源和材料生产做出贡献。

    

    水热碳化（HTC）是一种将生物质转化为多用途水炭的过程，无需事先干燥。水炭的物理化学性质受生物质性质和处理参数的影响，使得通过试错实验优化其特定应用成为一项挑战。为节省时间和成本，可以使用机器学习开发模型，对来自不同生物质来源在不同反应处理参数下产生的水炭进行表征。因此，本研究旨在使用涵盖一系列生物质类型和反应处理参数的数据库开发包含性模型以表征水炭。使用两个模型（决策树回归和支持向量回归）来预测水炭的质量和数量。决策树回归模型在预测精度方面优于支持向量回归模型（R2 > 0.88，RMSE < 6.848和MAE < 4.718）。使用进化算法优化决策树回归模型，可以进一步提高精度，并确定影响水炭质量和产率的关键因素。结果显示，这种基于机器学习的方法可以准确预测在不同处理条件下从各种生物质来源产生的水炭的质量和数量。开发的模型有望对从生物质中可持续生产能源和材料做出贡献。

    Hydrothermal carbonization (HTC) is a process that converts biomass into versatile hydrochar without the need for prior drying. The physicochemical properties of hydrochar are influenced by biomass properties and processing parameters, making it challenging to optimize for specific applications through trial-and-error experiments. To save time and money, machine learning can be used to develop a model that characterizes hydrochar produced from different biomass sources under varying reaction processing parameters. Thus, this study aims to develop an inclusive model to characterize hydrochar using a database covering a range of biomass types and reaction processing parameters. The quality and quantity of hydrochar are predicted using two models (decision tree regression and support vector regression). The decision tree regression model outperforms the support vector regression model in terms of forecast accuracy (R2 > 0.88, RMSE < 6.848, and MAE < 4.718). Using an evolutionary algorithm
    
[^96]: 基于人工智能的精准医疗方法：糖尿病风险预测

    Artificial Intelligence-Based Methods for Precision Medicine: Diabetes Risk Prediction. (arXiv:2305.16346v1 [cs.LG])

    [http://arxiv.org/abs/2305.16346](http://arxiv.org/abs/2305.16346)

    本文分析了现有文献中基于人工智能的糖尿病风险预测模型，在单模态和多模态模型中均表现突出，但外部验证有限，解释性方法需要改进。

    

    2型糖尿病的日益普及需要开发预测模型进行风险评估。人工智能（AI）模型被广泛用于此目的，但缺乏对其进展和挑战的全面评估。本文分析了现有文献中基于AI的2型糖尿病风险预测模型。包括了40个研究，主要发表在过去四年中。传统的机器学习模型比深度学习模型更普遍。电子健康记录是最常用的数据来源。单模态依赖EHR数据的AI模型很突出，而只有少数使用多模态模型。单模态和多模态模型均表现良好，后者优于前者。内部验证很常见，而外部验证很有限。解释性方法在一半的研究中得到了报告。少数研究报告了新的生物标志物，且有开源的。

    The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the development of predictive models for T2DM risk assessment. Artificial intelligence (AI) models are being extensively used for this purpose, but a comprehensive review of their advancements and challenges is lacking. This scoping review analyzes existing literature on AI-based models for T2DM risk prediction. Forty studies were included, mainly published in the past four years. Traditional machine learning models were more prevalent than deep learning models. Electronic health records were the most commonly used data source. Unimodal AI models relying on EHR data were prominent, while only a few utilized multimodal models. Both unimodal and multimodal models showed promising performance, with the latter outperforming the former. Internal validation was common, while external validation was limited. Interpretability methods were reported in half of the studies. Few studies reported novel biomarkers, and open-source
    
[^97]: "TaxoKnow：分类中的先验知识——层次分类作为多类分类损失函数中的先验知识"

    TaxoKnow: Taxonomy as Prior Knowledge in the Loss Function of Multi-class Classification. (arXiv:2305.16341v1 [cs.LG])

    [http://arxiv.org/abs/2305.16341](http://arxiv.org/abs/2305.16341)

    本文研究在平面分类器的学习算法中集成层次分类作为先验知识的有效性，实验结果表明先验知识可以显著提高学习者的性能，并在半监督和完全监督的情况下都获得了不错的结果。

    

    本文研究将标签的层次分类作为先验知识集成到平面分类器的学习算法中的有效性。我们提出了两种将层次分类作为显式正则化器集成到学习算法损失函数中的方法。通过对层次分类进行推理，神经网络减轻了其对类别的输出分布，允许将上层概念的条件附加到少数类上。我们仅限于平面分类任务，并提供了在两个工业内部数据集和两个公共基准测试集（RCV1和Amazon产品评论）上的实验结果。我们得到的结果显示，在半监督多类分类中，分类过程中的先验知识可以显著地提高学习者的性能，并且在完全监督的情况下也获得了不错的结果。

    In this paper, we investigate the effectiveness of integrating a hierarchical taxonomy of labels as prior knowledge into the learning algorithm of a flat classifier. We introduce two methods to integrate the hierarchical taxonomy as an explicit regularizer into the loss function of learning algorithms. By reasoning on a hierarchical taxonomy, a neural network alleviates its output distributions over the classes, allowing conditioning on upper concepts for a minority class. We limit ourselves to the flat classification task and provide our experimental results on two industrial in-house datasets and two public benchmarks, RCV1 and Amazon product reviews. Our obtained results show the significant effect of a taxonomy in increasing the performance of a learner in semisupervised multi-class classification and the considerable results obtained in a fully supervised fashion.
    
[^98]: 分段循环Transformer:一种高效的序列到序列模型

    Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])

    [http://arxiv.org/abs/2305.16340](http://arxiv.org/abs/2305.16340)

    本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。

    

    Transformer在许多领域中表现出卓越的性能，包括语言和视觉。然而，随着序列长度的增加，它们的计算成本呈二次增长，使得它们在资源受限的应用中使用成为不可能。为了解决这个问题，我们的方法是将整个序列划分成若干段。然后使用具有循环结构的神经元来聚合跨段的信息，从而实现具有较低计算/内存成本的序列处理能力模型。为了验证这个想法，我们首先研究了使用局部Attention机制对单个段的影响。然后我们提出了一种分段循环Transformer（SRformer），它将分段Attention和循环Attention相结合。它使用循环accumulate and fire（RAF）层在相邻段之间处理信息。通过更新key的产品来补偿减少Attention窗口长度产生的误差。

    Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
    
[^99]: 深思熟虑：具有内部工作记忆的决策Transformer

    Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])

    [http://arxiv.org/abs/2305.16338](http://arxiv.org/abs/2305.16338)

    该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。

    

    基于大型语言模型（LLM）的决策制定代理已经展示了跨越多个任务的泛化能力。然而，它们的性能依赖于大规模的数据和计算。我们认为，这种低效性源于遗忘现象，即模型通过参数记忆其行为，在训练过程中。因此，新任务的训练可能会降低模型在先前任务上的性能。与LLM的隐式记忆机制不同，人脑利用分布式存储器存储记忆，以有效地管理和组织多种技能，减轻了遗忘现象。因此，我们建议使用内部工作记忆模块来存储、融合和检索不同下游任务的信息。评估结果表明，所提出的方法改善了Atari游戏和元世界物体操作任务的训练效率和泛化能力。此外，我们证明了记忆微调进一步增强了转化决策制定代理对新任务的适应性。

    Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
    
[^100]: 基于自适应最优输运的健壮短文本聚类中可靠伪标签的表示学习

    Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering. (arXiv:2305.16335v1 [cs.CL])

    [http://arxiv.org/abs/2305.16335](http://arxiv.org/abs/2305.16335)

    本文提出了一种健壮短文本聚类（RSTC）模型，通过自适应最优输运的伪标签生成，以及基于类和实例的对比学习的健壮表示学习，帮助提高对不平衡和噪音数据的鲁棒性。

    

    短文本聚类因输入的不平衡和噪音数据而具有挑战性。现有方法无法很好地解决这个问题，因为它们容易在重度不平衡数据集上获得退化的解决方案，且易受到噪声干扰。为了解决这个问题，我们提出了一种健壮短文本聚类（RSTC）模型，以提高对不平衡和噪音数据的鲁棒性。RSTC包括两个模块，即伪标记生成模块和健壮表示学习模块。前者生成伪标记，为后者提供监督，有助于更健壮的表示和正确分离的聚类。为了提供对数据不平衡的鲁棒性，在伪标签生成模块中提出了自适应最优输运。为了提高对数据中噪声的鲁棒性，在健壮表示学习模块中进一步引入了基于类和实例的对比学习。

    Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust representation learning module. The former generates pseudo-labels to provide supervision for the later, which contributes to more robust representations and correctly separated clusters. To provide robustness against the imbalance in data, we propose self-adaptive optimal transport in the pseudo-label generation module. To improve robustness against the noise in data, we further introduce both class-wise and instance-wise contrastive learning in the robust representation learning module. Our empirical 
    
[^101]: 基于语音合成的文本生成用于ASR数据增广

    Text Generation with Speech Synthesis for ASR Data Augmentation. (arXiv:2305.16333v1 [cs.CL])

    [http://arxiv.org/abs/2305.16333](http://arxiv.org/abs/2305.16333)

    本研究探索文本增广对ASR的影响，使用大规模预训练的神经网络来生成合成文本，并通过文本到语音系统转换为合成语音，实验发现，使用神经网络的文本增广方法能够有效提高ASR准确度，可以作为改进ASR系统的一种可行工具。

    

    为了减少对昂贵人工注释的依赖，数据增广一直是自动语音识别（ASR）领域的一个热门研究方向。先前的研究主要侧重于用于ASR数据增广的合成语音生成，而其与文本生成方法的结合却相对较少探索。在本文中，我们使用大规模预训练的神经网络探索文本增广对ASR的影响，并将其与传统文本增广方法进行了系统比较。生成的合成文本然后通过文本到语音（TTS）系统转换为合成语音，并添加到ASR训练数据中。我们在三个数据集上进行的实验发现，神经模型实现了9％-15％的相对WER改进，并优于传统方法。我们得出结论，特别是通过现代神经方法，文本增广是提高ASR系统准确性的一种可行工具。

    Aiming at reducing the reliance on expensive human annotations, data synthesis for Automatic Speech Recognition (ASR) has remained an active area of research. While prior work mainly focuses on synthetic speech generation for ASR data augmentation, its combination with text generation methods is considerably less explored. In this work, we explore text augmentation for ASR using large-scale pre-trained neural networks, and systematically compare those to traditional text augmentation methods. The generated synthetic texts are then converted to synthetic speech using a text-to-speech (TTS) system and added to the ASR training data. In experiments conducted on three datasets, we find that neural models achieve 9%-15% relative WER improvement and outperform traditional methods. We conclude that text augmentation, particularly through modern neural approaches, is a viable tool for improving the accuracy of ASR systems.
    
[^102]: 通过人机交互实现连续学习--人类在与机器人反复交互中对机器人连续学习的看法

    Continual Learning through Human-Robot Interaction -- Human Perceptions of a Continual Learning Robot in Repeated Interactions. (arXiv:2305.16332v1 [cs.RO])

    [http://arxiv.org/abs/2305.16332](http://arxiv.org/abs/2305.16332)

    本研究结合机器人和连续学习模型，通过人机交互的方式与60名参与者实验，结果表明使用连续学习可以提高机器人的能力，参与者更倾向于与其进行反复交互，并提供更多的反馈信息。

    

    为了在动态的实际环境中长期部署辅助机器人，机器人必须继续学习和适应其环境。研究人员已经开发了各种连续学习（CL）的计算模型，可以使机器人不断从有限的训练数据中学习，并避免遗忘先前的知识。虽然这些CL模型可以缓解静态、系统地收集的数据集上的遗忘，但人们目前尚不清楚在多次交互中连续学习的机器人是如何被人类用户所感知的。在本研究中，我们开发了一个系统，将目标识别的CL模型与Fetch移动操纵机器人进行整合，并允许人类参与者在多个会话中直接教授和测试机器人。我们开展了一项现场研究，60名参与者在300个会话中与我们的系统互动（每个参与者5次会话）。我们进行了一项两组实验的研究，并使用三种不同的CL模型（三个实验条件）来了解人类对连续学习机器人的看法。

    For long-term deployment in dynamic real-world environments, assistive robots must continue to learn and adapt to their environments. Researchers have developed various computational models for continual learning (CL) that can allow robots to continually learn from limited training data, and avoid forgetting previous knowledge. While these CL models can mitigate forgetting on static, systematically collected datasets, it is unclear how human users might perceive a robot that continually learns over multiple interactions with them. In this paper, we developed a system that integrates CL models for object recognition with a Fetch mobile manipulator robot and allows human participants to directly teach and test the robot over multiple sessions. We conducted an in-person study with 60 participants who interacted with our system in 300 sessions (5 sessions per participant). We conducted a between-participant study with three different CL models (3 experimental conditions) to understand huma
    
[^103]: 基于Transformer的化学相似性搜索的提示工程可识别出具有结构差异的功能类似物

    Prompt Engineering for Transformer-based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues. (arXiv:2305.16330v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.16330](http://arxiv.org/abs/2305.16330)

    本文使用提示工程策略创建了基于向量的化学搜索方法，可识别出与查询功能类似但在结构上不同的分子。

    

    化学相似性搜索是用于识别新的药物分子的计算机辅助方法，其中历史上一直依赖于基于结构的比较来计算分子相似性。本文使用化学语言模型创建了基于向量的化学搜索，并采用了提示工程策略来扩展实现。我们通过检索五种类似于药品的分子和三种类似染料的分子的搜索结果来探索该方法。我们发现，该新方法可以识别出功能上类似于查询的分子，并且这些分子在结构上与查询不同，因此通过传统的基于结构的方法不容易找到。

    Chemical similarity searches are widely used in-silico methods for identifying new drug-like molecules. These methods have historically relied on structure-based comparisons to compute molecular similarity. Here, we use a chemical language model to create a vector-based chemical search. We extend implementations by creating a prompt engineering strategy that utilizes two different chemical string representation algorithms: one for the query and the other for the database. We explore this method by reviewing the search results from five drug-like query molecules (penicillin G, nirmatrelvir, zidovudine, lysergic acid diethylamide, and fentanyl) and three dye-like query molecules (acid blue 25, avobenzone, and 2-diphenylaminocarbazole). We find that this novel method identifies molecules that are functionally similar to the query, indicated by the associated patent literature, and that many of these molecules are structurally distinct from the query, making them unlikely to be found with 
    
[^104]: 视觉上下文语言模型中的语义组合

    Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])

    [http://arxiv.org/abs/2305.16328](http://arxiv.org/abs/2305.16328)

    本论文研究了视觉上下文语言模型中的语义组合能力，提出了新的组合视觉问答基准，句法神经模块蒸馏等方法以提高组合能力，并探索了对图像字幕模型的因果追踪以定位重要神经表示。

    

    句子的意义和其理想表达方式是什么？人类语言表现力的很大一部分来自语义组合，即人类心智以层次化和关系性方式表示意义的能力。与此同时，大部分句子的意义存在于文本之外，需要基于感官、运动和体验模态进行充分的学习。尽管大型语言模型显示出相当的组合能力，但最近的研究表明，有视觉基础的语言模型在表示组合结构时严重失败。在本论文中，我们探讨了模型是否及如何组合视觉上下文语义以及如何提高其组合能力。具体而言，我们介绍了 1) WinogroundVQA，一个新的组合视觉问答基准，2) 句子嵌入模型中组合能力的句法神经模块蒸馏，3) 对于图像字幕模型的因果追踪，以定位重要的神经表示。

    What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically & relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether & how models compose visually grounded semantics, and how we might improve their ability to do so.  Specifically, we introduce 1) WinogroundVQA, a new compositional visual question answering benchmark, 2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 3) Causal Tracing for Image Captioning Models to locate neural representations vital f
    
[^105]: 生物医学自然语言处理中的大型语言模型: 基准、基线和建议

    Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])

    [http://arxiv.org/abs/2305.16326](http://arxiv.org/abs/2305.16326)

    本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。

    

    生物医学文献呈指数级增长，手动筛选和提取知识变得困难。自动从生物医学文献中提取信息的生物医学自然语言处理（BioNLP）技术有助于减轻这种负担。近年来，如GPT-3和GPT-4等大型语言模型（LLMs）因其卓越的性能而受到重视。但是，它们在BioNLP任务中的有效性以及对方法开发和下游用户的影响仍未得到研究。本研究（1）在四个应用程序中在八个BioNLP数据集中建立了GPT-3和GPT-4在零-shot和一-shot设置下的基准表现，包括命名实体识别，关系提取，多标签文档分类和语义相似性和推理；（2）审查了LLMs产生的错误，并将错误分为三种类型：缺失，不一致和不需要的人工内容；（3）提出了使用LLMs的建议。

    Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
    
[^106]: 具有校准的Aleatoric和Epistemic不确定性的图神经网络相互作用势合奏运算法

    Graph Neural Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic Uncertainty on Energy and Forces. (arXiv:2305.16325v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.16325](http://arxiv.org/abs/2305.16325)

    本文提出了一个完整的框架来培训和重新校准图神经网络合奏模型，以产生带有校准不确定性估计的能量和力的准确预测，可以应用于材料的结构优化和分子动力学模拟。

    

    廉价的机器学习势场越来越被用于加速材料的结构优化和分子动力学模拟，通过迭代性地预测和应用原子间力。在这些情况下，检测到预测的不可靠性以避免错误或误导性结果至关重要。本文提出了一个完整的框架来培训和重新校准图神经网络合奏模型，以产生带有校准不确定性估计的能量和力的准确预测。所提出的方法考虑到了Epistemic和Aleatoric不确定性，通过非线性缩放函数在后期重新调整了总不确定性，从而在以前未见过的数据上实现良好的校准性能，而不会损失预测准确性。该方法在两个具有挑战性且公开可用的数据集上进行了演示和评估，ANI-1x（Smith等人）和Transition1x（Schreiner等人），这两个数据集都包含远离平衡状态的多样化构象。

    Inexpensive machine learning potentials are increasingly being used to speed up structural optimization and molecular dynamics simulations of materials by iteratively predicting and applying interatomic forces. In these settings, it is crucial to detect when predictions are unreliable to avoid wrong or misleading results. Here, we present a complete framework for training and recalibrating graph neural network ensemble models to produce accurate predictions of energy and forces with calibrated uncertainty estimates. The proposed method considers both epistemic and aleatoric uncertainty and the total uncertainties are recalibrated post hoc using a nonlinear scaling function to achieve good calibration on previously unseen data, without loss of predictive accuracy. The method is demonstrated and evaluated on two challenging, publicly available datasets, ANI-1x (Smith et al.) and Transition1x (Schreiner et al.), both containing diverse conformations far from equilibrium. A detailed analys
    
[^107]: 使用实例解释检测软件缺陷可靠性预测的概念漂移

    Detecting Concept Drift for the reliability prediction of Software Defects using Instance Interpretation. (arXiv:2305.16323v1 [cs.SE])

    [http://arxiv.org/abs/2305.16323](http://arxiv.org/abs/2305.16323)

    该论文旨在通过识别未标记的简化和重新采样数据解释中的变化来直接识别概念漂移的点，以开发可靠的 JIT-SDP 模型。此举将有助于解决类别不平衡现象所带来的风险问题，并提供一种可行的方法对模型的稳定性进行评估。

    

    在 JIT-SDP 上下文中，由于软件开发过程的变化、软件复杂性或用户行为的变化可能会影响随时间稳定性的 JIT-SDP 模型，因此可能会出现概念漂移（CD）。另外，在 JIT-SDP 数据中出现的类别不平衡现象可能会对 CD 检测方法的准确性构成潜在威胁，如果进行重新平衡，则会出现这种情况。据我们所知，这个问题尚未得到探讨。此外，已经提出了通过考虑标记的评估数据来检查 JIT-SDP 模型随时间的稳定性的方法。然而，需要注意的是，未来的数据标签可能并不总是及时可用。我们旨在通过识别未标记的简化和重新采样数据解释中的变化来直接识别 CD 的点，从而开发一个可靠的 JIT-SDP 模型。为了评估我们的方法，我们首先通过基于模型性能监控的基线方法来识别 CD 点。

    In the context of Just-In-Time Software Defect Prediction (JIT-SDP), Concept drift (CD) can occur due to changes in the software development process, the complexity of the software, or changes in user behavior that may affect the stability of the JIT-SDP model over time. Additionally, the challenge of class imbalance in JIT-SDP data poses a potential risk to the accuracy of CD detection methods if rebalancing is implemented. This issue has not been explored to the best of our knowledge. Furthermore, methods to check the stability of JIT-SDP models over time by considering labeled evaluation data have been proposed. However, it should be noted that future data labels may not always be available promptly. We aim to develop a reliable JIT-SDP model using CD point detection directly by identifying changes in the interpretation of unlabeled simplified and resampled data. To evaluate our approach, we first obtained baseline methods based on model performance monitoring to identify CD points 
    
[^108]: 面向多样性的相干损失用于改进神经主题模型

    Diversity-Aware Coherence Loss for Improving Neural Topic Models. (arXiv:2305.16199v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16199](http://arxiv.org/abs/2305.16199)

    本文提出了一种多样性感知的相干性损失，可以帮助神经主题模型在保持高多样性同时，更好地学习语料库级别的连贯性分数。

    

    神经主题建模的标准方法使用变分自编码器（VAE）框架，同时最小化估计后验和先验之间的KL散度，以及重建损失。由于神经主题模型是通过重新创建各个输入文档进行训练的，因此它们不会明确地捕获语料库级别的主题词之间的连贯性。在这项工作中，我们提出了一种新的多样性感知的相干性损失，鼓励模型学习语料库级别的连贯性分数，同时保持主题之间的高多样性。多个数据集上的实验结果表明，我们的方法可以显着提高神经主题模型的性能，而无需任何预训练或额外的参数。

    The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents, they do not explicitly capture the coherence between topic words on the corpus level. In this work, we propose a novel diversity-aware coherence loss that encourages the model to learn corpus-level coherence scores while maintaining a high diversity between topics. Experimental results on multiple datasets show that our method significantly improves the performance of neural topic models without requiring any pretraining or additional parameters.
    
[^109]: 通过格拉姆迭代实现卷积层利普希茨常数的高效边界

    Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16173](http://arxiv.org/abs/2305.16173)

    本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。

    

    由于利普希茨常数的控制对神经网络的训练稳定性、泛化和鲁棒性有很大影响，因此估计这个值是目前的一个科学难题。在本文中，我们使用循环矩阵理论和一种新的功率迭代替代方法，介绍了一个精确、快速和可微分的上界，用于卷积层的谱范数。称为格拉姆迭代，我们的方法展现了一个超线性的收敛。首先，我们通过一系列全面的实验证明了我们的方法在精度、计算成本和可伸缩性方面优于其他最先进的方法。然后，我们证明了它对于卷积神经网络的利普希茨正则化非常有效，与其他方法相比具有竞争力的结果。代码可在 https://github.com/blaisedelattre/lip4conv 上获得。

    Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
    
[^110]: 关于马尔科夫转换模型的可辨识性研究

    On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])

    [http://arxiv.org/abs/2305.15925](http://arxiv.org/abs/2305.15925)

    本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。

    

    最近，潜变量模型的可辨识性因其在可解释性或分布泛化方面的应用而备受关注。本文探讨了作为将最近的结果扩展到序列潜变量模型的第一步的马尔科夫转换模型的可辨识性。我们在第一阶段马尔科夫依赖结构中提出了可辨识性条件，并通过非线性高斯参数化迁移分布。我们的实验展示了我们方法在依赖于政权的因果发现和高维时间序列分割方面的适用性。

    Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
    
[^111]: 异构图神经网络梯度正则化统一方法

    Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])

    [http://arxiv.org/abs/2305.15811](http://arxiv.org/abs/2305.15811)

    本研究提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题，综合效果和效率优于几种现有方法。

    

    异构图神经网络是一种强大的深度学习方法，用于学习异构图的表征。尽管HGNN迅速发展，但仍面临过度平滑和非鲁棒性等挑战。先前的研究表明，使用梯度正则化方法可以缓解这些问题，但现有的梯度正则化方法专注于图形拓扑或节点特征，缺乏统一方法。本文提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题。实验证明，Grug在几个基准数据集上优于几种现有方法。

    Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, 
    
[^112]: 学习集合策略的理论保证及其在时间序列预测中的应用

    Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])

    [http://arxiv.org/abs/2305.15786](http://arxiv.org/abs/2305.15786)

    本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。

    

    集合是机器学习中最常用的工具之一，由于其能够有效地减少方差，从而提高泛化性能。针对黑盒基学习器的大多数集合方法都属于“叠加泛化”范畴，即训练一个接受基学习器推理作为输入的机器学习算法。虽然叠加泛化在实践中广泛应用，但其理论性质仍然不为人所知。本文证明了一个新的结果，表明选择基于交叉验证性能的“有限或有限维”叠加泛化中的最佳叠加泛化并不比最优解表现“差得多”。这一结果加强和大大扩展了Van der Laan等人（2007年）的结果。受到理论分析的启发，我们在概率预测的背景下进一步提出了一系列不同敏感性的叠加泛化模型。

    Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
    
[^113]: 面向代码混合的印地语-英语数据的预训练BERT模型的比较研究

    Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])

    [http://arxiv.org/abs/2305.15722](http://arxiv.org/abs/2305.15722)

    本文比较了使用不同预训练Transformer模型的印地语-英语代码混合数据的性能表现，以提高情感分析、情绪识别和仇恨言论识别等自然语言处理任务的性能。

    

    “代码混合”是指在同一段文本中使用多种语言的现象。这种现象在社交媒体平台上广泛存在，并随着时间的推移越来越多地被采纳。检测语言中的外来元素并正确处理它们至关重要，因为许多人使用代码混合语言，其中任一语言都无法理解。本文重点研究低资源的印地语-英语代码混合语言，并提高不同代码混合自然语言处理任务（如情感分析、情绪识别和仇恨言论识别）的性能。我们对使用无监督方法预训练的不同基于Transformer的语言模型进行了比较分析。我们包括了代码混合模型（如HingBERT、HingRoBERTa、HingRoBERTa-Mixed、mBERT）和非代码混合模型（如AlBERT、BERT、RoBERTa），进行比较分析印地语-英语代码混合。

    The term "Code Mixed" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En
    
[^114]: 在搜索和推荐系统中，在线表示很重要：实用的端到端多样化方法。

    Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems. (arXiv:2305.15534v1 [cs.IR])

    [http://arxiv.org/abs/2305.15534](http://arxiv.org/abs/2305.15534)

    为了改善搜索和推荐系统中的代表性，我们提出了一种端到端的多样化方法，并在Pinterest平台上实验和部署了可扩展的多样化机制，以改善美容和时尚类别中不同肤色的代表性。

    

    随着在线平台在各个人口统计学中的使用不断增长，用户经常表达希望在内容中感受到自己的代表性。为了改善搜索结果和推荐中的代表性，我们引入了端到端的多样化方法，确保多样化内容在这些系统的各个阶段中流动，从检索到排序。我们在多个Pinterest平台的生产界面中开发、实验和部署可扩展的多样化机制，包括搜索、相关产品和新用户主页，以改善美容和时尚内容中不同肤色的代表性。生产系统中的多样化包括三个组成部分：确定会触发多样化的请求，在检索阶段确保从大型内容语料库中检索到多样化的内容，最后，在排名阶段以自我调整的方式平衡多样性和效用的权衡。我们的方法从使用Strong-O开始。

    As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-O
    
[^115]: 用进化采样改善基于少样本学习的蛋白质工程

    Improving few-shot learning-based protein engineering with evolutionary sampling. (arXiv:2305.15441v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.15441](http://arxiv.org/abs/2305.15441)

    本文提出了一种利用少量数据进行新型蛋白质设计的方法，通过半监督转移学习和进化马尔可夫蒙特卡罗链采样算法，更有效地探索适应度景观，从而加速昂贵的湿实验测试周期。

    

    设计新型功能蛋白质仍然是一个缓慢且昂贵的过程，这是由于各种蛋白质工程挑战; 特别是，在给定的实验中可以测试的蛋白变体数量远远不及整个序列空间的广阔，导致低命中率和昂贵的湿实验测试周期。在本文中，我们提出了一种少样本学习方法来设计新型蛋白质，旨在加速昂贵的湿实验测试周期，并能够利用一个小且偏斜的训练数据集（约$10^5$数据点，$<1\%$积极结果）。我们的方法由两部分组成：一种半监督转移学习方法，用于生成所需蛋白质功能的离散适应度景观，以及一种新的进化马尔可夫蒙特卡罗链采样算法，以更有效地探索适应度景观。我们通过实验预测高适应度基因并进行筛选，展示了我们方法的性能。

    Designing novel functional proteins remains a slow and expensive process due to a variety of protein engineering challenges; in particular, the number of protein variants that can be experimentally tested in a given assay pales in comparison to the vastness of the overall sequence space, resulting in low hit rates and expensive wet lab testing cycles. In this paper, we propose a few-shot learning approach to novel protein design that aims to accelerate the expensive wet lab testing cycle and is capable of leveraging a training dataset that is both small and skewed ($\approx 10^5$ datapoints, $< 1\%$ positive hits). Our approach is composed of two parts: a semi-supervised transfer learning approach to generate a discrete fitness landscape for a desired protein function and a novel evolutionary Monte Carlo Markov Chain sampling algorithm to more efficiently explore the fitness landscape. We demonstrate the performance of our approach by experimentally screening predicted high fitness gen
    
[^116]: 利用损失函数的二阶信息，本地 SGD 加速收敛。

    Local SGD Accelerates Convergence by Exploiting Second Order Information of the Loss Function. (arXiv:2305.15013v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15013](http://arxiv.org/abs/2305.15013)

    本文研究了使用本地 SGD 更新的有效性，证明了在 IID 数据方案中，L-SGD 能够探索并利用损失函数的二阶信息以实现更快速的收敛。

    

    本文研究了利用多次本地统计梯度下降（L-SGD）更新的有效性，发现L-SGD能够探索损失函数的二阶信息，在独立同分布（IID）数据方案中超越了随机梯度下降（SGD）。我们证明了相比SGD，L-SGD的更新沿着具有小特征值的黑塞矩阵的特征向量有更大的投影，从而能更快地收敛。

    With multiple iterations of updates, local statistical gradient descent (L-SGD) has been proven to be very effective in distributed machine learning schemes such as federated learning. In fact, many innovative works have shown that L-SGD with independent and identically distributed (IID) data can even outperform SGD. As a result, extensive efforts have been made to unveil the power of L-SGD. However, existing analysis failed to explain why the multiple local updates with small mini-batches of data (L-SGD) can not be replaced by the update with one big batch of data and a larger learning rate (SGD). In this paper, we offer a new perspective to understand the strength of L-SGD. We theoretically prove that, with IID data, L-SGD can effectively explore the second order information of the loss function. In particular, compared with SGD, the updates of L-SGD have much larger projection on the eigenvectors of the Hessian matrix with small eigenvalues, which leads to faster convergence. Under 
    
[^117]: 基于贝叶斯抽样算法的在线自适应流量实验的实用批次评估

    An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])

    [http://arxiv.org/abs/2305.14704](http://arxiv.org/abs/2305.14704)

    本文推导和评估了四种贝叶斯批次赌博算法，用于自适应确定流量分配，全面评估了这些算法的可信度、敏感性和后悔度，仿真结果表明这些基于批次的贝叶斯算法是有效的。

    

    为了加速在线测试，多臂赌博算法通过自适应地收集数据而被作为固定时间A/B测试的重要补充方式不断提高。本文基于最近关于自适应收集数据的最佳臂识别和统计推断的研究，推导和评估了四种基于贝叶斯批次赌博算法（NB-TS，WB-TS，NB-TTTS，WB-TTTS），它们是两种加权批次（Naive Batch和Weighted Batch）和两种贝叶斯抽样策略（Thompson Sampling和Top-Two Thompson Sampling）的组合，用于自适应确定流量分配。本文提供的这些基于批次统计奖励度量的贝叶斯抽样算法在实践中得以应用，而本文提出的其中一个组合WB-TTTS似乎是最新讨论的。对这四种基于批次的贝叶斯抽样算法进行了全面评估，包括测试方法的可信度、敏感性和后悔度。此外，评估还考虑了批次内奖励度量的方差以及批次之间的相关性，这在以前的研究中尚未得到很好的解决。仿真结果表明，与现有的赌博算法（例如UCB1，TS和Exp3）相比，这些基于批次的贝叶斯算法是有效的。

    To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
    
[^118]: 序列建模是离线强化学习的一个强有力的竞争者。

    Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])

    [http://arxiv.org/abs/2305.14550](http://arxiv.org/abs/2305.14550)

    序列建模是离线强化学习中比Q-Learning和Imitation Learning更适合在稀疏奖励和低质量数据设置下的选择，在任务范围增加时，序列建模和模仿学习更可取。

    

    离线强化学习使代理能够从静态数据集中学习有效的最大化收益策略。离线RL的三大范式是Q-Learning、Imitation Learning和Sequence Modeling。一个关键的问题是：在什么条件下，哪种范式被优先选择？我们通过探索代表性算法——保守Q-Learning(CQL)、行为克隆 (BC)和决策Transformer (DT)——在常用的D4RL和Robomimic基准测试中的表现来对这个问题进行了实证研究。我们设计了有针对性的实验来理解它们在数据子优性和任务复杂性方面的行为。我们的主要发现是：(1)序列建模需要比Q-Learning更多的数据来学习竞争性策略，但更加稳健；(2)序列建模在稀疏奖励和低质量数据设置中比Q-Learning和Imitation Learning都要好得多；(3)随着任务范围的增加，序列建模和模仿学习更可取。

    Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
    
[^119]: Chakra: 利用标准化执行跟踪推进性能基准和协同设计

    Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces. (arXiv:2305.14516v1 [cs.LG])

    [http://arxiv.org/abs/2305.14516](http://arxiv.org/abs/2305.14516)

    Chakra是一种开放的图形模式，用于标准化工作负载规范，捕捉关键操作和依赖项，以推进性能基准和协同设计，同时提供一组工具和能力，以便在模拟器和仿真器中实现未来系统的协同设计。

    

    基准测试和协同设计对于推动ML模型、ML软件和下一代硬件的优化和创新至关重要。完整的工作负载基准测试，例如MLPerf，在系统完全设计和部署后，可以在不同的软件和硬件堆栈之间进行公平比较，发挥至关重要的作用。然而，人工智能创新的速度要求采用更敏捷的方法来创建和使用基准测试，借助模拟器和仿真器进行未来系统协同设计。我们提出了Chakra，一种开放的图形模式，用于标准化工作负载规范，捕捉关键操作和依赖项，即执行跟踪（ET）。此外，我们提出了一组互补的工具/能力，以使各种模拟器、仿真器和基准测试可以收集、生成和采用Chakra ET。例如，我们使用生成AI模型学习了数千个Chakra ET的潜在统计属性，并使用这些模型来综合Chakra ET。

    Benchmarking and co-design are essential for driving optimizations and innovation around ML models, ML software, and next-generation hardware. Full workload benchmarks, e.g. MLPerf, play an essential role in enabling fair comparison across different software and hardware stacks especially once systems are fully designed and deployed. However, the pace of AI innovation demands a more agile methodology to benchmark creation and usage by simulators and emulators for future system co-design. We propose Chakra, an open graph schema for standardizing workload specification capturing key operations and dependencies, also known as Execution Trace (ET). In addition, we propose a complementary set of tools/capabilities to enable collection, generation, and adoption of Chakra ETs by a wide range of simulators, emulators, and benchmarks. For instance, we use generative AI models to learn latent statistical properties across thousands of Chakra ETs and use these models to synthesize Chakra ETs. The
    
[^120]: 联邦变异推断：迈向个性化和泛化的改进

    Federated Variational Inference: Towards Improved Personalization and Generalization. (arXiv:2305.13672v1 [cs.LG])

    [http://arxiv.org/abs/2305.13672](http://arxiv.org/abs/2305.13672)

    本文提出了一种名为联邦变分推断的算法，用于跨设备联邦学习中的个性化和泛化，并在图像分类中超越了现有技术。

    

    传统的联邦学习算法通过利用所有参与客户端的数据来训练单个全局模型。然而，由于客户生成分布和预测模型的异质性，这些方法可能不适当地近似预测过程、收敛到最优状态或泛化到新客户端。我们研究在假设客户数据分布和预测模型的异质性的状态下，跨设备联邦学习设置中的个性化和泛化。我们首先提出了一种分层生成模型，并使用贝叶斯推断加以规范化。然后，我们使用变分推断来有效地训练我们的模型。我们称此算法为联邦变分推断（FedVI）。我们使用PAC-Bayes分析为FedVI提供了泛化界限。我们在FEMNIST和CIFAR-100图像分类上评估了我们的模型，并展示了FedVI在两个任务上均超越了现有技术水平。

    Conventional federated learning algorithms train a single global model by leveraging all participating clients' data. However, due to heterogeneity in client generative distributions and predictive models, these approaches may not appropriately approximate the predictive process, converge to an optimal state, or generalize to new clients. We study personalization and generalization in stateless cross-device federated learning setups assuming heterogeneity in client data distributions and predictive models. We first propose a hierarchical generative model and formalize it using Bayesian Inference. We then approximate this process using Variational Inference to train our model efficiently. We call this algorithm Federated Variational Inference (FedVI). We use PAC-Bayes analysis to provide generalization bounds for FedVI. We evaluate our model on FEMNIST and CIFAR-100 image classification and show that FedVI beats the state-of-the-art on both tasks.
    
[^121]: SPEECH: 基于能量的事件中心超球的结构化预测

    SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])

    [http://arxiv.org/abs/2305.13617](http://arxiv.org/abs/2305.13617)

    这篇论文提出了一种称为SPEECH的模型，它使用能量建模来表示复杂的事件结构，并使用超球来表示事件类别。实验结果表明，SPEECH在事件检测和事件关系抽取任务中表现出卓越的性能。

    

    事件中心的结构化预测涉及预测事件的结构化输出。在大多数自然语言处理情况下，事件结构都具有复杂的依赖关系，因此有效地表示这些复杂的事件结构是具有挑战性的。为了解决这些问题，我们提出了基于能量的事件中心超球的结构化预测 (SPEECH)。 SPEECH 使用基于能量的建模来模拟事件结构组件之间的复杂依赖关系，并使用简单但有效的超球来表示事件类别。在两个统一标注的事件数据集的实验中，结果表明SPEECH在事件检测和事件关系抽取任务中占优势。

    Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
    
[^122]: 不对称学习率的分离式理性化: 一种灵活的Lipschitz限制

    Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])

    [http://arxiv.org/abs/2305.13599](http://arxiv.org/abs/2305.13599)

    本文提出了一种名为DR的灵活的方法，它通过不对称的学习率来解决由合作博弈引发的退化问题，该方法能够在两个基准测试中显著改善表现。

    

    通常情况下，自说明理性化模型通过合作博弈构建，其中生成器从输入文本中选择最易理解的部分作为原理，接着预测器基于所选择的原理进行预测。然而，这种合作博弈可能会引发退化问题，预测器过度拟合于由尚未训练好的生成器生成的信息不足的部分，反过来导致生成器收敛于趋向于选择无意义的部分的次优模型。本文从理论上将退化问题与预测器的Lipschitz连续性联系起来。随后，我们实验性地提出了一种名为DR的简单而有效的方法，可以自然、灵活地约束预测器的Lipschitz常数，并解决了退化问题。DR方法的主要思想是将生成器和预测器分离，为它们分配不对称的学习率。在两个广泛使用的基准测试中进行的一系列实验表明，我们的DR方法能够显著改善现有方法的表现。

    A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
    
[^123]: 粗到细: 一种用于三维分子生成的分层扩散模型

    Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D. (arXiv:2305.13266v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.13266](http://arxiv.org/abs/2305.13266)

    该论文提出了一种粗到细的方法来解决分子生成中的组合优化问题，并采用分层扩散模型和HIPER算法生成结构的有效性和多样性，表现优于现有方法。

    

    在药物发现中，生成理想的三维分子结构是一个基本问题。尽管我们取得了相当大的进展，但现有方法通常在原子分辨率下生成分子，并忽略内在的局部结构，如环，这导致生成的结构质量较差，特别是当生成大分子时。基于片段的分子生成是一种有前途的策略，但由于组合优化问题，它不容易用于3D非自回归生成。在本文中，我们采用粗到细的策略来解决这个问题，其中提出了一种基于分层扩散的模型（即HierDiff），以保持局部段的有效性而不依赖于自回归建模。具体而言，HierDiff首先通过等变扩散过程生成粗粒度分子几何体，其中每个粗粒度节点反映分子中的一个片段。然后，粗粒度节点被分解成细粒度节点，其中细粒度节点是分子中的一个三维空间点。我们进一步提出了一种分层采样和经验概率细化（即HIPER）算法，以确保生成的结构的有效性和多样性。我们的实验表明，HierDiff在定量评估指标和视觉质量方面优于现有方法。

    Generating desirable molecular structures in 3D is a fundamental problem for drug discovery. Despite the considerable progress we have achieved, existing methods usually generate molecules in atom resolution and ignore intrinsic local structures such as rings, which leads to poor quality in generated structures, especially when generating large molecules. Fragment-based molecule generation is a promising strategy, however, it is nontrivial to be adapted for 3D non-autoregressive generations because of the combinational optimization problems. In this paper, we utilize a coarse-to-fine strategy to tackle this problem, in which a Hierarchical Diffusion-based model (i.e.~HierDiff) is proposed to preserve the validity of local segments without relying on autoregressive modeling. Specifically, HierDiff first generates coarse-grained molecule geometries via an equivariant diffusion process, where each coarse-grained node reflects a fragment in a molecule. Then the coarse-grained nodes are dec
    
[^124]: INVICTUS: 通过协同学习和搜索优化布尔逻辑电路综合

    INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search. (arXiv:2305.13164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13164](http://arxiv.org/abs/2305.13164)

    INVICTUS是一个使用离线强化学习和搜索的模型，自动生成逻辑最小化启发式算法综合配方以优化电路面积和时延等指标。

    

    逻辑综合是芯片设计中的第一步也是最重要的一步。本文提出了一个模型INVICTUS，利用离线强化学习的方法，根据已有的设计数据集自动生成一系列逻辑最小化启发式算法（“综合配方”），优化电路面积和时延等指标。

    Logic synthesis is the first and most vital step in chip design. This steps converts a chip specification written in a hardware description language (such as Verilog) into an optimized implementation using Boolean logic gates. State-of-the-art logic synthesis algorithms have a large number of logic minimization heuristics, typically applied sequentially based on human experience and intuition. The choice of the order greatly impacts the quality (e.g., area and delay) of the synthesized circuit. In this paper, we propose INVICTUS, a model-based offline reinforcement learning (RL) solution that automatically generates a sequence of logic minimization heuristics ("synthesis recipe") based on a training dataset of previously seen designs. A key challenge is that new designs can range from being very similar to past designs (e.g., adders and multipliers) to being completely novel (e.g., new processor instructions). %Compared to prior work, INVICTUS is the first solution that uses a mix of R
    
[^125]: DermSynth3D：野外注释皮肤科图像的综合

    DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images. (arXiv:2305.12621v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.12621](http://arxiv.org/abs/2305.12621)

    该论文提出了一种名为DermSynth3D的新框架，它使用可微分渲染器将皮肤病变模式混合到人体三维纹理网格上并生成逼真的二维皮肤镜像图像，同时提供对应的密集注释以进行语义分割。

    

    近年来，深度学习在皮肤科图像分析领域展现出了巨大的潜力。然而，该领域现有的数据集存在显着限制，包括样本图像数量较少、疾病条件有限、注释不足以及非标准化图像采集。为了解决这些问题，我们提出了一种名为DermSynth3D的新框架。该框架使用可微分渲染器将皮肤病变模式混合到人体的三维纹理网格上，并在各种背景场景下采用不同视角和光照条件生成二维图像。我们的方法遵循自上而下的规则，限制混合和渲染过程，以创建具有野外照片感的皮肤条件的二维图像，确保更有意义的结果。该框架生成逼真的二维皮肤镜像图像，并生成对皮肤、皮肤状况、身体部位和头发区域进行语义分割的对应密集注释。

    In recent years, deep learning (DL) has shown great potential in the field of dermatological image analysis. However, existing datasets in this domain have significant limitations, including a small number of image samples, limited disease conditions, insufficient annotations, and non-standardized image acquisitions. To address these shortcomings, we propose a novel framework called DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured meshes of human subjects using a differentiable renderer and generates 2D images from various camera viewpoints under chosen lighting conditions in diverse background scenes. Our method adheres to top-down rules that constrain the blending and rendering process to create 2D images with skin conditions that mimic in-the-wild acquisitions, ensuring more meaningful results. The framework generates photo-realistic 2D dermoscopy images and the corresponding dense annotations for semantic segmentation of the skin, skin conditions, body parts, 
    
[^126]: 多头状态空间模型在语音识别中的应用

    Multi-Head State Space Model for Speech Recognition. (arXiv:2305.12498v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.12498](http://arxiv.org/abs/2305.12498)

    本文提出了一种多头状态空间（MH-SSM）模型，它能够用于语音识别任务并在LibriSpeech数据集上表现出的新的性能，是变压器变换器的优秀替代方案。同时, MH-SSM层的引入也提高了变压器块的性能，达到了现有最新水平。

    

    最近，在一些小规模的序列和语言建模任务上，状态空间模型（SSM）已经表现出了很大的潜力，并且能够与许多基于注意力的方法相媲美甚至超越。在本文中，我们提出了一种多头状态空间（MH-SSM）架构，它配备了特殊的门控机制，其中并行头被教授如何在序列数据上学习本地和全局的时间动态。作为变压器编码器中多头注意力的直接替代方案，这个新模型在LibriSpeech语音识别语料库上显著优于变压器变换器。此外，我们在变压器块中增加了MH-SSM层，称为Stateformer，不使用外部语言模型，在LibriSpeech任务中达到了最新的性能，开发集和测试集的词错误率分别为1.76％ / 4.37％和1.91％ / 4.36％。

    State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76\%/4.37\% on the development and 1.91\%/4.36\% on the test sets without using an external language model.
    
[^127]: 生态学家使用机器学习的九个技巧

    Nine tips for ecologists using machine learning. (arXiv:2305.10472v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.10472](http://arxiv.org/abs/2305.10472)

    本论文介绍了九个技巧来帮助生态学家实施机器学习模型，这些技巧针对分类问题，旨在识别开发机器学习模型中的常见错误、陷阱或挑战，并提供了解决方法。

    

    由于其高精度和灵活性，机器学习模型成为了生态学家合适且高效的工具。但是，实施机器学习模型并不是一项简单的任务，对于没有在这个领域有经验的生态学家来说可能会有些难以接受。在这里，我们提供了一系列技巧来帮助生态学家实施机器学习模型。我们专注于分类问题，因为许多生态学研究旨在将数据归入预定义的类别，例如生态状态或生物实体。这九个技巧中，每一个都提出了在开发机器学习模型方面的常见错误、陷阱或挑战，并提供了有助于生态学研究中使用机器学习的建议。

    Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.
    
[^128]: sustain.AI: 一种分析可持续性报告的推荐系统

    sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08711](http://arxiv.org/abs/2305.08711)

    sustain.AI是一个智能的、上下文感知的推荐系统，可以帮助审计师、金融投资者以及广大公众高效地分析公司的可持续性报告，并通过与GRI标准匹配来提供更好的推荐精度。

    

    本文介绍了sustain.AI，这是一个智能的、上下文感知的推荐系统，可帮助审计师、金融投资者以及广大公众高效地分析公司的可持续性报告。该工具利用了端到端可训练的架构，将基于BERT的编码模块与多标签分类头相结合，将可持续性报告中的相关文本段落与全球报告倡议（GRI）标准中的相应法律法规匹配。我们在两个新颖的德国可持续性报告数据集上评估了我们的模型，并始终实现了与多个强基线模型相比更高的推荐性能。此外，sustain.AI已经公开在https://sustain.ki.nrw/上提供给所有人使用。

    We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
    
[^129]: 解释性微调使模型对虚假提示更强韧

    Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])

    [http://arxiv.org/abs/2305.04990](http://arxiv.org/abs/2305.04990)

    本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。

    

    大型语言模型（LLMs）非常强大，有时会学习到标签和与任务无关的特征之间的相关性，导致在分布外数据上泛化能力差。我们提出解释性微调作为减轻LLMs依赖虚假关联的一种新的通用方法。与标准微调只在给定输入的情况下预测答案不同，我们微调模型以生成支持其答案的自由文本解释。为了评估我们的方法，我们在人工构建的训练集上微调模型，该训练集包含不同类型的虚假提示，并在没有这些提示的测试集上进行测试。与标准微调相比，我们的方法在四个分类任务的准确性下降方面使模型极其强韧：ComVE（+1.2），CREAK（+9.1），e-SNLI（+15.4）和SBIC（+6.5）。此外，我们的方法与模型生成的解释同样有效，这意味着我们的方法具有广泛的适用性。

    Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
    
[^130]: 使用可解释机器学习理解卷积云

    Understanding cirrus clouds using explainable machine learning. (arXiv:2305.02090v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.02090](http://arxiv.org/abs/2305.02090)

    本文使用机器学习模型研究了卷积云的驱动因素与云属性之间的关系，发现气象和气溶胶条件可以预测卷积云属性。功能属性方法还可以量化这些关系，了解哪些因素可以影响卷积云中的冰晶数浓度和冰水含量。

    

    卷积云是地球气候的关键调节因素，但它们与气象和气溶胶条件的关系是全球气候模型中最大的不确定性之一。本研究使用三年的卫星和再分析数据，研究卷积云驱动因素与云属性之间的关系。我们使用梯度提升机器学习模型和一个带有注意层的长短期记忆网络来预测冰水含量和冰晶数浓度。模型表明，气象和气溶胶条件可以预测卷积云属性，$R^2=0.49$。使用SHapley Additive exPlanations (SHAP)计算功能属性，以量化气象和气溶胶条件与卷积云属性之间的联系。例如，引起冰晶数浓度预测下降所需的超微米尘埃粒子的最小浓度为$2 \times 10^{-4}$ mg m\textsuperscript{-3}。观察前15个小时

    Cirrus clouds are key modulators of Earth's climate. Their dependencies on meteorological and aerosol conditions are among the largest uncertainties in global climate models. This work uses three years of satellite and reanalysis data to study the link between cirrus drivers and cloud properties. We use a gradient-boosted machine learning model and a Long Short-Term Memory (LSTM) network with an attention layer to predict the ice water content and ice crystal number concentration. The models show that meteorological and aerosol conditions can predict cirrus properties with $R^2 = 0.49$. Feature attributions are calculated with SHapley Additive exPlanations (SHAP) to quantify the link between meteorological and aerosol conditions and cirrus properties. For instance, the minimum concentration of supermicron-sized dust particles required to cause a decrease in ice crystal number concentration predictions is $2 \times 10^{-4}$ mg m\textsuperscript{-3}. The last 15 hours before the observat
    
[^131]: 深度学习对于纯数学家来说是一个有用的工具吗？

    Is deep learning a useful tool for the pure mathematician?. (arXiv:2304.12602v1 [math.RT])

    [http://arxiv.org/abs/2304.12602](http://arxiv.org/abs/2304.12602)

    本文是一篇介绍纯数学家使用深度学习工具进行研究的个人和非正式叙述。

    

    一篇关于纯数学家使用深度学习工具进行研究的个人和非正式叙述。

    A personal and informal account of what a pure mathematician might expect when using tools from deep learning in their research.
    
[^132]: 树状Parzen估计器：理解其算法组成部分及其在提高实证表现中的作用

    Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])

    [http://arxiv.org/abs/2304.11127](http://arxiv.org/abs/2304.11127)

    该论文介绍了一种广泛使用的贝叶斯优化方法 Tree-structured Parzen estimator (TPE)，并对其控制参数的作用和算法直觉进行了讨论和分析，提供了一组推荐设置并证明其能够提高TPE的性能表现。

    

    许多领域中最近的进展要求更加复杂的实验设计。这种复杂的实验通常有许多参数，需要参数调整。Tree-structured Parzen estimator (TPE) 是一种贝叶斯优化方法，在最近的参数调整框架中被广泛使用。尽管它很受欢迎，但控制参数的角色和算法直觉尚未得到讨论。在本教程中，我们将确定每个控制参数的作用以及它们对超参数优化的影响，使用多种基准测试。我们将从剖析研究中得出的推荐设置与基准方法进行比较，并证明我们的推荐设置提高了TPE的性能。我们的TPE实现可在https://github.com/nabenabe0928/tpe/tree/single-opt中获得。

    Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
    
[^133]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^134]: 邻居的回响：基于Shuffle模型的个性化隐私保护联邦学习中的隐私扩增

    Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model. (arXiv:2304.05516v1 [cs.CR])

    [http://arxiv.org/abs/2304.05516](http://arxiv.org/abs/2304.05516)

    本文提出了一个个性化隐私保护联邦学习框架，可在保证本地隐私的同时实现强中心隐私保证，并利用Shuffle模型进行隐私扩增。

    

    联邦学习是一种流行的协同训练范例，但会受到隐私攻击。为了满足用户对于不同隐私需求的本地需求，需要保留个性化的本地差分隐私，同时还需要为全局模型提供严格的隐私保证。本文提出了一个通用框架（APES）来加强个性化本地隐私保护条件下的模型隐私，利用Shuffle模型的隐私扩增效果。为了增强隐私保证，我们量化每个用户对中心隐私的异构贡献，并通过扰动“回声”来描述用户的特征。在各种数据集上的实验表明了我们的框架的有效性。

    Federated Learning, as a popular paradigm for collaborative training, is vulnerable against privacy attacks. Different privacy levels regarding users' attitudes need to be satisfied locally, while a strict privacy guarantee for the global model is also required centrally. Personalized Local Differential Privacy (PLDP) is suitable for preserving users' varying local privacy, yet only provides a central privacy guarantee equivalent to the worst-case local privacy level. Thus, achieving strong central privacy as well as personalized local privacy with a utility-promising model is a challenging problem. In this work, a general framework (APES) is built up to strengthen model privacy under personalized local privacy by leveraging the privacy amplification effect of the shuffle model. To tighten the privacy bound, we quantify the heterogeneous contributions to the central privacy user by user. The contributions are characterized by the ability of generating "echos" from the perturbation of e
    
[^135]: 利用位置去噪预测易得几何结构的量子化学性质

    Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.03724](http://arxiv.org/abs/2304.03724)

    该论文提出了一种方法，利用位置去噪预测易得几何结构的量子化学性质，可以用相对容易获得的几何结构，精确预测性质，在分子性质以及化学反应性质的预测任务中都表现优秀。

    

    由于量子化学性质与其几何结构有重要关联，使用3D几何信息的图神经网络在许多任务中取得了较高的预测精度。然而，它们通常需要高级量子力学计算得出的3D几何结构，这在实际问题中是不可行的，限制了其在现实问题中的适用性。为了解决这个问题，我们提出了一种方法，利用相对容易获得的几何结构（例如来自分子力场的优化几何结构）精确预测性质。在这种方法中，输入几何结构逐渐接近正确几何结构，通过堆叠去噪层。我们使用3D消息传递体系结构研究了该方法在两个预测任务（分子性质和化学反应性质）中的性能。通过去噪过程减少位置误差有助于性能的提高。

    As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
    
[^136]: 有限宽度核和平均场神经网络中的预测波动动力学分析

    Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])

    [http://arxiv.org/abs/2304.03408](http://arxiv.org/abs/2304.03408)

    本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。

    

    我们分析了宽但有限的特征学习神经网络中有限宽度效应的动力学。与许多先前的分析不同，我们的结果是针对特征学习强度的非微扰有限宽度的结果。从无限宽深度神经网络核和预测动力学的动力学平均场理论（DMFT）描述开始，我们提供了对网络权重的随机初始化下DMFT序参数$\mathcal{O}(1/\sqrt{\text{width}})$波动的表征。在网络训练的懒惰极限中，所有核都是随机的但在时间上静止的，预测方差具有通用形式。然而，在富有特征学习的区域，核和预测的波动是动态耦合且方差可以被自洽计算。在两层网络中，我们展示了特征学习如何动态地减少最终NTK和最终网络预测的方差。我们还展示了如何进行初始化。

    We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
    
[^137]: 字典学习中交替极小化算法的收敛性

    Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])

    [http://arxiv.org/abs/2304.01768](http://arxiv.org/abs/2304.01768)

    本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。

    

    本文导出了针对字典学习两种流行的交替极小化算法 - 最优方向法（MOD）和在线字典学习（ODL）的收敛性足够的条件。我们表明，只要初始值良好，即距离生成的字典不超过$1/\log(K)$或具有一定的结构，确保初始值中的每个元素只指向一个生成元，两种算法将以几何收敛速率收敛于生成的字典。这在具有非均匀分布的数据模型上也能实现，该模型中稀疏系数的支撑集的出现频率可以变化很大，从而更接近真实数据。

    In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.
    
[^138]: 抽象器：基于Transformer的符号消息传递和关系推理模块

    Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])

    [http://arxiv.org/abs/2304.00195](http://arxiv.org/abs/2304.00195)

    该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    

    该论文提出了一个框架，将关系学习转化为Transformer模型，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
    
[^139]: 自我反馈迭代精炼：一种无需监督学习或加强学习的LM改进框架

    Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])

    [http://arxiv.org/abs/2303.17651](http://arxiv.org/abs/2303.17651)

    自我反馈迭代精炼是一种无需监督学习或加强学习的LLMs初始输出优化方法，优于直接生成，被证实在7个不同任务中表现更好。

    

    鉴于语言模型(LLMs)不总是能在第一次良好地解决生成问题（如摘要、答案、解释等），我们引入自我反馈迭代精炼（SELF-REFINE）框架，通过迭代反馈和精炼相似地优化LLMs的初始输出。主要思想是：使用LLM生成输出，然后允许同一模型提供其自身输出的多方面反馈，最后利用反馈使相同模型精炼先前生成的输出。我们的迭代精炼框架与早期工作不同，无需监督训练数据或加强学习，并且可以与单个LLM一起使用。我们对七个不同的任务进行了实验，范围从评论重写到数学推理，表明我们的方法优于直接生成。在所有任务中，使用SELF-REFINE生成的输出被人类和自动化指标优先于使用GPT-3.5和GPT-4直接生成的输出，表现得更好。

    Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
    
[^140]: 黑盒变分贝叶斯推理的实用匹配梯度方差界限

    Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])

    [http://arxiv.org/abs/2303.10472](http://arxiv.org/abs/2303.10472)

    本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。

    

    理解黑盒变分推理（BBVI）的梯度方差是建立其收敛性和算法改进的关键一步。然而，现有研究尚未表明BBVI的梯度方差满足用于研究随机梯度下降（SGD）收敛的条件。在本文中，我们展示了当应用于平滑和二次增长的对数似然函数时，BBVI满足与SGD文献中使用的ABC条件相匹配的界限。我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。此外，我们表明，平均场参数化的方差具有经过验证的优越维度依赖性。

    Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
    
[^141]: 视觉信息对ASR错误纠正非常重要

    Visual Information Matters for ASR Error Correction. (arXiv:2303.10160v1 [eess.AS])

    [http://arxiv.org/abs/2303.10160](http://arxiv.org/abs/2303.10160)

    这篇论文提出了用于改善自动语音识别（ASR）输出的ASR错误纠正（EC）技术，该技术通过融入视觉信息，使用门控融合和图像标题作为提示的方法，提高了EC的性能。同时，本文提供了一个基准数据集Visual-ASR-EC。

    

    为了提高自动语音识别（ASR）的输出，ASR错误纠正（EC）技术已经得到广泛开发，因其使用并行文本数据的效率高。以往的研究主要集中在使用文本或/和语音数据上，这在没有仅有文本和语音信息的情况下限制了性能提升；而视觉信息等其他形式的数据很少被研究。本文提供了一些简单但有效的方法，即门控融合和图像标题作为提示，用于将视觉信息融入EC中，并且提供了大规模的基准数据集Visual-ASR-EC，其中每个项目的训练数据包括视觉、语音和文本信息，测试数据是...

    Aiming to improve the Automatic Speech Recognition (ASR) outputs with a post-processing step, ASR error correction (EC) techniques have been widely developed due to their efficiency in using parallel text data. Previous works mainly focus on using text or/ and speech data, which hinders the performance gain when not only text and speech information, but other modalities, such as visual information are critical for EC. The challenges are mainly two folds: one is that previous work fails to emphasize visual information, thus rare exploration has been studied. The other is that the community lacks a high-quality benchmark where visual information matters for the EC models. Therefore, this paper provides 1) simple yet effective methods, namely gated fusion and image captions as prompts to incorporate visual information to help EC; 2) large-scale benchmark datasets, namely Visual-ASR-EC, where each item in the training data consists of visual, speech, and text information, and the test data
    
[^142]: 零和马尔可夫博弈中强化学习的新政策迭代算法

    A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])

    [http://arxiv.org/abs/2303.09716](http://arxiv.org/abs/2303.09716)

    本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。

    

    许多基于模型的强化学习算法可以被视为具有两个阶段: 学习阶段和规划阶段。在标准MDPs情况下，可以使用价值迭代或策略迭代来解决学习问题。但在零和马尔可夫博弈的情况下，没有有效的策略迭代算法，以前的尝试都有局限性。本文提出了一种简单的策略迭代变体，能够有效地解决这个问题。

    Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
    
[^143]: 基于后训练量化的大型语言模型综合研究

    A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])

    [http://arxiv.org/abs/2303.08302](http://arxiv.org/abs/2303.08302)

    本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。

    

    后训练量化是一种减少大型语言模型内存消耗和/或计算成本的权衡方法。然而，关于不同量化方案、不同模型族、不同后训练量化方法、不同量化位精度等的影响的全面研究仍缺失。本文通过数万个零-shot实验对这些组件进行了广泛的研究。我们的研究结果表明：(1)细粒度量化和后训练量化方法(而不是朴素的最近舍入量化)是实现良好精度的必要条件；(2) 用粗粒度量化的更高位数（如5位）比用非常细粒度的更低位数（如4位）（其有效位数与5位相似）更强大。我们还提出了如何为不同大小的\llms利用量化的建议，并留下未来机会和系统工作的建议。

    Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
    
[^144]: 在视觉Transformer中学习成长人工海马，实现弹性终身学习

    Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])

    [http://arxiv.org/abs/2303.08250](http://arxiv.org/abs/2303.08250)

    本文提出了一种在Vision Transformer中学习成长人工海马的方法，以实现弹性终身学习。通过神经架构搜索进行维护，选取多头自注意力块中的最终线性投影层进行ArtiHippo的实现和成长。

    

    终身学习需要拥有人类智能的韧性，即不存在灾难性遗忘，这种韧性与大脑中复杂的记忆机制，尤其是海马维护的长期记忆（LM）紧密相关。Transformer已经成为人工智能“大脑”的对应体，但LM组件在终身学习中尚未充分探索。本文提出了一种在Vision Transformer中学习成长人工海马（ArtiHippo）以实现弹性终身学习的方法。通过全面消融实验，选定多头自注意力（MHSA）块中的最终线性投影层来实现和成长ArtiHippo。ArtiHippo由专家混合（MoEs）表示。每个专家组件是线性投影层的现场变体，通过神经架构搜索（NAS）进行维护，搜索空间由四个基本成长操作（跳过、重用、适应和新）定义。

    Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
    
[^145]: GOATS：目标采样自适应课程强化学习用于舀取任务

    GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.05193](http://arxiv.org/abs/2303.05193)

    本文提出了一种名为GOATS的方法，使用目标采样自适应课程强化学习技术，通过插值位置目标和数量目标的分布创建学习过程中的课程来解决机器人舀取任务中的位置目标和水量目标问题，取得了比基线更好的表现。

    

    本文首先使用目标条件强化学习对机器人舀取水的问题进行了阐述。由于流体的复杂动力学和实现多模式目标的需求，该任务具有特别的挑战性。政策需要成功地达到位置目标和水量目标，这导致一个庞大而复杂的目标状态空间。为了克服这些挑战，我们引入了GOATS，一种课程强化学习方法，通过插值位置目标分布和数量目标分布来创建学习过程中的课程，使用目标分解奖励公式，学习一个高效且具有通用性的机器人舀取策略。结果，我们的方法可以在仿真中表现出比基线更好的性能，分别在碗舀和桶舀任务中实现了5.46％和8.71％的误差，涵盖了1000种初始水状态的变化。

    In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
    
[^146]: 有限宽度神经网络的贝叶斯推断

    Bayesian inference with finitely wide neural networks. (arXiv:2303.02859v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2303.02859](http://arxiv.org/abs/2303.02859)

    本文通过多元Edgeworth展开，提出用微分形式表示非高斯分布，来模拟有限宽度神经网络的非高斯后验分布。

    

    当机器学习从业者将宽度很大的神经网络视为贝叶斯设置中的高斯过程时，解析推断，例如预测分布以闭合形式给出，可能是一种吸引人的优势。但是，实际的宽度是有限的，并且在该宽度下，一些随机变量的边际化的高斯假设可能出现偏差。基于多元Edgeworth展开，我们提出了用微分形式表示的非高斯分布，来对来自随机神经网络的有限输出进行建模，并推导出相应的边际和条件属性，从而能够在贝叶斯回归任务中推导出非高斯后验分布。此外，在瓶颈式深度神经网络中，通过边缘核探究了深高斯过程的非高斯特性。

    The analytic inference, e.g. predictive distribution being in closed form, may be an appealing benefit for machine learning practitioners when they treat wide neural networks as Gaussian process in Bayesian setting. The realistic widths, however, are finite and cause weak deviation from the Gaussianity under which partial marginalization of random variables in a model is straightforward. On the basis of multivariate Edgeworth expansion, we propose a non-Gaussian distribution in differential form to model a finite set of outputs from a random neural network, and derive the corresponding marginal and conditional properties. Thus, we are able to derive the non-Gaussian posterior distribution in Bayesian regression task. In addition, in the bottlenecked deep neural networks, a weight space representation of deep Gaussian process, the non-Gaussianity is investigated through the marginal kernel.
    
[^147]: 基于幻想对抗控制的保守离线策略评估

    Hallucinated Adversarial Control for Conservative Offline Policy Evaluation. (arXiv:2303.01076v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01076](http://arxiv.org/abs/2303.01076)

    本文提出了基于幻想对抗控制的HAMBO算法，可用于离线策略评估，并且能够得出有效的策略表现下限估计。

    

    本文研究了保守离线策略评估问题，对于给定其他代理收集的离线环境交互数据集，我们旨在获得一个关于策略性能的(紧)下限估计。这在决定是否部署某个策略满足最小性能/安全标准之前至关重要。为此，我们引入了HAMBO，它建立在一个学习到的传递动态的不确定性感知模型之上。为了形成策略绩效的保守估计，HAMBO会幻想策略可能采取的最坏轨迹，且该轨迹在模型的认知置信区间内。我们证明了结果的COPE估计是有效的下限，并在正则性条件下展示其收敛于真实的预期回报。最后，我们讨论了基于Bayesian神经网络的可扩展变体，并在实验中证明它们产生可靠且紧密的下限。

    We study the problem of conservative off-policy evaluation (COPE) where given an offline dataset of environment interactions, collected by other agents, we seek to obtain a (tight) lower bound on a policy's performance. This is crucial when deciding whether a given policy satisfies certain minimal performance/safety criteria before it can be deployed in the real world. To this end, we introduce HAMBO, which builds on an uncertainty-aware learned model of the transition dynamics. To form a conservative estimate of the policy's performance, HAMBO hallucinates worst-case trajectories that the policy may take, within the margin of the models' epistemic confidence regions. We prove that the resulting COPE estimates are valid lower bounds, and, under regularity conditions, show their convergence to the true expected return. Finally, we discuss scalable variants of our approach based on Bayesian Neural Networks and empirically demonstrate that they yield reliable and tight lower bounds in var
    
[^148]: 贝叶斯内核张量分解作为贝叶斯优化的替代模型

    Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization. (arXiv:2302.14510v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.14510](http://arxiv.org/abs/2302.14510)

    本文提出了在贝叶斯优化中使用贝叶斯内核张量分解作为代理模型的方法，以学习具有复杂特征的函数。

    

    贝叶斯优化（BO）在很大程度上使用高斯过程（GP）作为主要的代理模型，大多使用简单的固定和可分离的内核函数，例如具有自动相关决定（SE-ARD）的平方指数内核。然而，这样的简单内核规格说明不足以学习具有复杂特征的函数，例如非定常，非可分离和多峰。即使在低维空间中，使用局部GP逼近这样的函数也需要大量样本，更不用说在高维环境中了。在本文中，我们提出使用贝叶斯内核张量分解（BKTF）作为$ D $维笛卡尔乘积空间中 BO 的新代理模型。我们的关键思想是使用全贝叶斯低秩张量 CP 分解近似基础的 $ D $ 维实体，在其中我们为每个维度的潜在基础函数放置 GP 先验，以编码局部一致性和平滑性。

    Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) -- as a new surrogate model -- for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, 
    
[^149]: 轻量化参数裁剪以实现节能深度学习: 一种二值门控模块方法

    Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10798](http://arxiv.org/abs/2302.10798)

    本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。

    

    随着神经网络越来越大且更加复杂，绿色AI已经引起了深度学习社区的关注。现有的解决方案通常采用对网络参数进行裁剪，以减少训练推断时的计算负荷。然而，裁剪方案通常会导致额外的开销，因为需要进行迭代训练和微调或重复计算动态裁剪图。我们提出了一种新的参数裁剪策略，以学习轻量级子网络，既能最小化能耗，又能在给定的下游任务上与完全参数化的网络保持相似的性能。我们的裁剪方案以绿色为导向，因为它仅需要动态裁剪方法进行一次训练来发现最佳的静态子网络。该方案由一个二进制门控模块和一个新颖的损失函数组成，以发现具有用户定义稀疏度的子网络。我们的方法可以对卷积神经网络（CNN）和循环神经网络（RNN）等通用神经网络进行裁剪和转换，大大减少了计算复杂度和能量消耗，同时在已有的基准测试上取得了竞争性的结果。

    The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
    
[^150]: 关于校准扩散概率模型

    On Calibrating Diffusion Probabilistic Models. (arXiv:2302.10688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10688](http://arxiv.org/abs/2302.10688)

    本文发现了数据分数随机反向过程是一个鞅，提出了一种简单的方法，用于校准任意预先训练的DPM，有效减小模型的得分匹配损失，增加模型似然的下限，并提供了一般校准指南。

    

    最近，扩散概率模型（DPM）在各种生成性任务中取得了有希望的结果。一个典型的DPM框架包括一个逐渐扩散数据分布的正向过程和一个从时间相关数据分数中恢复数据分布的随机反向过程。本文观察到数据分数的随机反向过程是一个鞅，从中可以导出数据分数的集中界和随机停止定理。然后，我们发现一种简单的方法，用于校准任意预先训练的DPM，以减小得分匹配损失，并因此增加模型似然的下限。我们提供了各种模型参数化下的一般校准指南。我们的校准方法仅执行一次，并且可以重复使用所得到的模型进行采样。我们在多个数据集上进行实验，以经验性地验证我们的提议。我们的代码位于https://github.com/thudzj/Cal。

    Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for calibrating an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is at https://github.com/thudzj/Cal
    
[^151]: $\omega$PAP空间：关于高阶、递归概率和可微程序的指称推理

    $\omega$PAP Spaces: Reasoning Denotationally About Higher-Order, Recursive Probabilistic and Differentiable Programs. (arXiv:2302.10636v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2302.10636](http://arxiv.org/abs/2302.10636)

    本文介绍了$\omega$PAP空间的范畴，可为大多数实用的概率和可微程序赋予意义，同时还证明了有关自动微分正确性和概率程序中迹密度函数的可微性等结果。

    

    我们引入了一个新的设置——$\omega$PAP空间的范畴，用于关于表现力强大的可微和概率编程语言进行指称推理。我们的语义足够一般化，可以为大多数实用的概率和可微程序赋予意义，包括使用一般递归、高阶函数、不连续原语以及离散和连续采样的程序。但是，至关重要的是，它也足够特定，可以排除许多病态的指称，从而使我们能够建立有关确定性可微程序和概率程序的新结果。

    We introduce a new setting, the category of $\omega$PAP spaces, for reasoning denotationally about expressive differentiable and probabilistic programming languages. Our semantics is general enough to assign meanings to most practical probabilistic and differentiable programs, including those that use general recursion, higher-order functions, discontinuous primitives, and both discrete and continuous sampling. But crucially, it is also specific enough to exclude many pathological denotations, enabling us to establish new results about both deterministic differentiable programs and probabilistic programs. In the deterministic setting, we prove very general correctness theorems for automatic differentiation and its use within gradient descent. In the probabilistic setting, we establish the almost-everywhere differentiability of probabilistic programs' trace density functions, and the existence of convenient base measures for density computation in Monte Carlo inference. In some cases th
    
[^152]: 医用口罩与身体情绪识别：深度学习视角下的洞见

    Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective. (arXiv:2302.10021v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10021](http://arxiv.org/abs/2302.10021)

    本文研究了医用口罩对情感识别的影响，并展示了整体人体输入优于单纯面部遮挡的卓越性。文章提出了一种深度学习模型，通过分别处理面部和身体特征并融合预测分数来提高情感识别准确性。

    

    新冠肺炎疫情无疑改变了我们生活的标准和影响了社交通讯的方方面面。为了预防传播，人们被迫广泛佩戴医用口罩，这种面部遮挡严重影响了从面部读取情感的能力，促使我们将整个身体视为情感线索。本文通过深入研究面部遮挡对情感识别表现的影响，展示了整体人体输入优于单纯面部遮挡的卓越性。我们利用基于时间分段网络框架的深度学习模型，并渴望完全克服口罩带来的后果。虽然面部和身体特征可以从单个输入中学习，但这可能导致无关信息混淆。通过分别处理这些特征并融合它们的预测分数，我们更有效地利用了两种模式。该框架还自然地支持节奏的推断，从而克服了与语音相关的复杂性和多度量问题。

    The COVID-19 pandemic has undoubtedly changed the standards and affected all aspects of our lives, especially social communication. It has forced people to extensively wear medical face masks, in order to prevent transmission. This face occlusion can strongly irritate emotional reading from the face and urges us to incorporate the whole body as an emotional cue. In this paper, we conduct insightful studies about the effect of face occlusion on emotion recognition performance, and showcase the superiority of full body input over the plain masked face. We utilize a deep learning model based on the Temporal Segment Network framework, and aspire to fully overcome the face mask consequences. Although facial and bodily features can be learned from a single input, this may lead to irrelevant information confusion. By processing those features separately and fusing their prediction scores, we are more effectively taking advantage of both modalities. This framework also naturally supports tempo
    
[^153]: 面向对抗生成模型的PAC-Bayesian泛化界

    PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08942](http://arxiv.org/abs/2302.08942)

    将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。

    

    我们将PAC-Bayesian理论扩展到生成模型，并为基于Wasserstein距离和总变差距离的模型开发了泛化界。我们第一个关于Wasserstein距离的结果假设实例空间是有界的，而我们的第二个结果利用了降维的优势。我们的结果自然适用于Wasserstein GAN和Energy-Based GAN，而我们的界限为这两种GAN提供了新的训练目标。尽管我们的工作主要是理论性的，但我们进行了数值实验，展示了Wasserstein GAN在合成数据集上的非虚空泛化界。

    We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
    
[^154]: 不平衡数据集下深度主动学习算法选择研究

    Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07317](http://arxiv.org/abs/2302.07317)

    本论文提出了适用于不平衡数据集下的深度主动学习的自适应算法选择策略TAILOR，它在多类和多标签应用的实验中取得了与候选算法相当或更高的准确性。

    

    标签效率已成为深度学习应用中越来越重要的目标。主动学习旨在减少训练深度网络所需的标记示例数量，但是，在不同的数据集和应用中，主动学习算法的实证性能可能会大幅度变化。事先很难知道哪种主动学习策略在特定应用中表现良好或最佳。为解决这个问题，我们提出了第一个用于深度主动学习的自适应算法选择策略。对于任何未标记的数据集，我们的(meta)算法TAILOR (Thompson ActIve Learning algORithm selection)迭代地并自适应地选择一组候选主动学习算法。TAILOR使用旨在收集类平衡示例的新奖励函数。在多类和多标签应用的大量实验中，TAILOR在实现与候选算法中最佳算法相当或更高的准确性方面表现出良好的效果。

    Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
    
[^155]: I$^2$SB：图像到图像的Schr\"odinger桥

    I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05872](http://arxiv.org/abs/2302.05872)

    提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。

    

    本文提出了一种新的条件扩散模型，即图像到图像的Schr\"odinger桥（I$^2$SB），直接学习两个给定分布之间的非线性扩散过程。这些扩散桥对于图像恢复特别有用，因为退化图像是重构清晰图像的结构信息先验。 I$^2$SB属于一类可处理的Schr\"odinger桥模型，它是得分模型的非线性扩展，其边界对的边缘分布可以在解析上计算。这种通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，进而采用在标准扩散模型中使用的实用技术，使得I$^2$SB训练具有可扩展性。在ImageNet 256x256上，我们验证了I$^2$SB在各种图像恢复任务中的性能，包括修复，超分辨率，去模糊和JPEG恢复，并表明I$^2$SB超过了标准条件扩散模型，具有更可解释的生成过程。

    We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. 
    
[^156]: 实现加速尽管梯度非常嘈杂。

    Achieving acceleration despite very noisy gradients. (arXiv:2302.05515v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05515](http://arxiv.org/abs/2302.05515)

    AGNES是一种能在平滑凸优化任务中实现加速的算法，即使梯度估计的信噪比很小，它也能表现出优异的性能，在深度学习中的应用效果显著优于动量随机梯度下降和Nesterov方法。

    

    我们提出了Nesterov加速梯度下降算法的一般化。如果噪声的强度与梯度的大小成比例，我们的算法（AGNES）可以证明在具有嘈杂梯度估计的平滑凸优化任务中实现加速。如果常数比例超过一，Nesterov加速梯度下降在这种噪声模型下不会收敛。AGNES能修复这种不足，并且可以证明它的收敛速度加快，无论梯度估计的信噪比有多小。实验证明，这是用于超参数过多的深度学习小批量梯度的适当模型。最后，我们证明AGNES在CNN训练中的性能优于动量随机梯度下降和Nesterov的方法。

    We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient. Nesterov's accelerated gradient descent does not converge under this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency and provably achieves an accelerated convergence rate no matter how small the signal to noise ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES outperforms stochastic gradient descent with momentum and Nesterov's method in the training of CNNs.
    
[^157]: 一般几何上的黎曼流匹配

    Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03660](http://arxiv.org/abs/2302.03660)

    本文提出了一种名为黎曼流匹配的方法，可以在一般几何上训练连续标准化流，并在高维度数据上具有优势。

    

    我们提出了一种名为黎曼流匹配（RFM）的框架，用于在流形上训练连续标准化流。现有的流形生成建模方法要么需要昂贵的模拟，要么无法本质上扩展到高维度，要么使用限制量的近似来产生有偏的训练目标。黎曼流匹配绕过了这些限制，并提供了比以前方法更多的优势：它在简单几何上无需模拟，不需要散度计算，并以闭合形式计算其目标向量场。 RFM的关键因素是构建一个相对简单的前度量，以定义目标向量场，其中包括现有的欧几里得情况。为了扩展到一般几何，我们依靠使用谱分解来有效地即兴计算前度量。我们的方法在现实世界的非欧几里得数据集上实现了最先进的性能，并通过在3D网格和双曲空间上训练标准化流来证明其功效。

    We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
    
[^158]: 关于差分隐私少样本图像分类方法有效性的研究

    On the Efficacy of Differentially Private Few-shot Image Classification. (arXiv:2302.01190v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01190](http://arxiv.org/abs/2302.01190)

    本文通过一系列实验研究了差分隐私少样本图像分类模型的准确性和易受攻击性，揭示了样本数、隐私级别、模型架构、下游数据集以及可学习参数子集等因素对分类效果的影响。

    

    近年来，在训练差分隐私（DP）模型方面取得了显著进展，这些DP模型的准确性接近最佳的非私有模型。这些DP模型通常在大规模公共数据集上预训练，然后在相对大且与预训练数据分布相似的私有下游数据集上进行微调。然而，在许多应用中，包括个性化和联合学习，重要的是在少样本情况下良好地表现（i.e. 获取大量标记数据可能有问题），且能够在各种领域的数据集上（即用于各种专业设置）进行良好的分类。为了了解少样本DP何时有效，我们进行了一系列详尽的实验，揭示了每类样本数、隐私级别、模型架构、下游数据集以及可学习参数子集等对少样本DP图像分类模型准确性和易受攻击性的影响。

    There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in 
    
[^159]: SimMTM: 一种用于序列模型的简单预训练框架

    SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling. (arXiv:2302.00861v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00861](http://arxiv.org/abs/2302.00861)

    SimMTM是一个简单的序列模型预训练框架，通过加权聚合多个流形之外的邻居来恢复掩码时间点，从而简化重构任务。

    

    时间序列分析广泛应用于许多领域。为了降低标注成本并受益于各种任务，最近，自监督预训练引起了极大的兴趣。其中一种主流范例是掩码建模，它通过学习基于未掩码部分的掩码内容的重构，成功地预训练深度模型。但是，由于时间序列的语义信息主要包含在时间变化中，标准的随机屏蔽一部分时间点的方式会严重破坏时间序列的重要时间变化，使重构任务过于困难，无法引导表示学习。因此，我们提出了一个用于序列模型的简单预训练框架SimMTM。通过将掩码建模与流形学习相关联，SimMTM提出通过多个流形之外的邻居的加权聚合来恢复掩码时间点，从而通过组装被破坏但互补的时间变化来简化重构任务。

    Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from
    
[^160]: 多功能能量概率模型在高能物理中的应用

    Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00695](http://arxiv.org/abs/2302.00695)

    本文提出了一个多功能的能量概率模型，用于描述高能物理事件，可用于参数化的事件生成，异常信号探测以及粒子识别。

    

    作为一种经典的生成建模方法，基于能量的模型具有能量函数形式灵活性的天然优势。最近，基于能量的模型在计算机视觉和自然语言处理中建模高维数据方面取得了巨大成功。与这些进展一致，我们建立了一个多功能能量概率模型，用于描述来自大型强子对撞机的高能物理事件。该框架基于一个强大的生成模型，并描述了更高阶的粒子间相互作用，适用于不同的编码体系结构和隐式生成。在应用方面，它可以作为强大的参数化事件生成器用于物理仿真，一种泛用的无假设关联的异常信号探测器，以及用于粒子识别的增强事件分类器。

    As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
    
[^161]: 蒸馏策略优化

    Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00533](http://arxiv.org/abs/2302.00533)

    本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。

    

    本文提出了一个演员-评论家学习框架，它借鉴了分布式学习的视角和两种策略改进数据的交叉融合，实现了快速学习并可应用于广泛的算法类别。在该框架中，首先提出了方差减少机制，例如统一优势估计器 (UAE) 和一个学习的基线，不仅是连接到动作值函数的桥梁，还能提炼优势。

    On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
    
[^162]: 隐式正则化对于稀疏线性回归的良性过拟合现象具有促进作用

    Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression. (arXiv:2302.00257v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00257](http://arxiv.org/abs/2302.00257)

    本文研究发现，隐式正则化可对于稀疏线性回归的良性过拟合现象具有促进作用，并给出了一个模型参数化形式，结合了$\ell_1$和$\ell_2$内插器的优点，通过梯度下降训练可得到一个接近最优测试损失的内插器。

    

    在深度学习中，训练过程经常会找到一个内插器（一个0训练损失的解），但测试损失仍然很低。这种被称为良性过拟合的现象，是一个备受关注的重要谜团。良性过拟合的一个常见机制是隐式正则化，训练过程会导致内插器具有额外的性质，常被描述为最小化某些范数。然而，即使对于一个简单的稀疏线性回归问题$y=\beta^{*\top}x+\xi$，最小的$\ell_1$或$\ell_2$范数内插器也不能给出最优的测试损失。在这项工作中，我们给出了一个模型的不同参数化形式，它导致了一种新的隐式正则化效应，结合了$\ell_1$和$\ell_2$内插器的优点。我们证明，通过梯度下降训练我们的新模型，可以得到一个具有接近最优测试损失的内插器。我们的结果基于对训练动力学的细致分析，并提供了一个新的理解隐式正则化的框架。

    In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem $y = \beta^{*\top} x +\xi$ with sparse $\beta^*$, neither minimum $\ell_1$ or $\ell_2$ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of $\ell_1$ and $\ell_2$ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. Our result is based on careful analysis of the training dynamics and prov
    
[^163]: 端到端全原子抗体设计

    End-to-End Full-Atom Antibody Design. (arXiv:2302.00203v3 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2302.00203](http://arxiv.org/abs/2302.00203)

    dyMEAN是一个端到端全原子模型，可以根据表位和不完整的抗体序列进行抗体设计，在处理全原子时能够处理可变大小的蛋白残基。旨在解决现有学习方法的两个主要问题：只处理抗体设计过程中的某个子任务和无法捕捉全原子几何形状。

    

    抗体设计是药物治疗和生物学等领域中一个重要且具有挑战性的任务。当前的基于学习的方法有两个主要缺陷：1）只处理整个抗体设计流程中的某个子任务，使其次优或资源密集。2）忽略框架区域或侧链中的任一部分，因此无法捕捉全原子几何形状。为解决这些问题，我们提出了动态多通道等变图神经网络（dyMEAN），这是一个端到端全原子模型，用于根据表位和不完整的抗体序列进行 E（3）等变抗体设计。具体来说，我们首先探究结构初始化作为抗体结构的有知识猜测，然后提出影响表位-抗体相互作用的“shadow paratope”。通过自适应多通道等变编码器来更新1D序列和3D结构，该编码器可在考虑全原子时处理可变大小的蛋白残基。最后，我们证明了dyMEAN在多个基准测试中的有效性，这为完全自动化的抗体设计提供了新的可能性。

    Antibody design is an essential yet challenging task in various domains like therapeutics and biology. There are two major defects in current learning-based methods: 1) tackling only a certain subtask of the whole antibody design pipeline, making them suboptimal or resource-intensive. 2) omitting either the framework regions or side chains, thus incapable of capturing the full-atom geometry. To address these pitfalls, we propose dynamic Multi-channel Equivariant grAph Network (dyMEAN), an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody. Specifically, we first explore structural initialization as a knowledgeable guess of the antibody structure and then propose shadow paratope to bridge the epitope-antibody connections. Both 1D sequences and 3D structures are updated via an adaptive multi-channel equivariant encoder that is able to process protein residues of variable sizes when considering full atoms. Finally,
    
[^164]: 将语言模型与图像进行联系以处理多模态信息

    Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13823](http://arxiv.org/abs/2301.13823)

    该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。

    

    我们提出了一种有效的方法，将预训练的仅文本语言模型与视觉领域联系起来，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的文本。我们利用从大规模文本预训练中学到的语言模型的能力，例如上下文学习和自由形式文本生成。我们保持语言模型冻结，并微调输入和输出线性层以实现跨模态交互。这使得我们的模型能够处理任意交错的图像和文本输入，并生成与检索图像交错的自由形式文本。我们在环境相关的图像检索和多模态对话等任务中取得了强大的零-shot表现，并展示了引人入胜的交互能力。我们的方法适用于任何现成的语言模型，为在视觉场景下利用预训练语言模型提供了一个有效且通用的解决方案。

    We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
    
[^165]: STEEL: 奇异性感知的强化学习

    STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13152](http://arxiv.org/abs/2301.13152)

    这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。

    

    批量强化学习旨在利用预先收集的数据，在动态环境中找到最优策略，以最大化期望总回报。然而，几乎所有现有算法都依赖于目标策略诱导的分布绝对连续假设，以便通过变换测度使用批量数据来校准目标策略。本文提出了一种新的批量强化学习算法，不需要在具有连续状态和行动的无限时马尔可夫决策过程中绝对连续性假设。我们称这个算法为STEEL：SingulariTy-awarE rEinforcement Learning。我们的算法受到关于离线评估的新误差分析的启发，其中我们使用了最大均值偏差，以及带有分布鲁棒优化的策略定向误差评估方法，以确保异常情况下的性能，并提出了一种用于处理奇异情况的定向算法。

    Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
    
[^166]: 通用方差条件下的分布式随机优化

    Distributed Stochastic Optimization under a General Variance Condition. (arXiv:2301.12677v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.12677](http://arxiv.org/abs/2301.12677)

    这项研究通过重新审视联邦平均算法，在最小假设下对分布式非凸目标进行了随机优化，建立了仅满足随机梯度温和条件的收敛结果。

    

    分布式随机优化在解决大规模机器学习问题时表现出了很高的效率。尽管已经提出并成功应用于一般实际问题的算法很多，但它们的理论保证主要依赖于随机梯度的某些有界条件，从均匀有界性到放松增长条件。此外，在代理之间表征数据异质性及其对算法性能的影响依然具有挑战性。出于这样的动机，我们重新考虑了经典的联邦平均（FedAvg）算法，以解决分布式随机优化问题，并在平滑非凸目标函数的随机梯度仅满足温和方差条件的情况下建立了收敛结果。在此条件下，还建立了接近确定的收敛到一个稳态点。此外，我们讨论了一个更具信息性的度量标准。

    Distributed stochastic optimization has drawn great attention recently due to its effectiveness in solving large-scale machine learning problems. Though numerous algorithms have been proposed and successfully applied to general practical problems, their theoretical guarantees mainly rely on certain boundedness conditions on the stochastic gradients, varying from uniform boundedness to the relaxed growth condition. In addition, how to characterize the data heterogeneity among the agents and its impacts on the algorithmic performance remains challenging. In light of such motivations, we revisit the classical Federated Averaging (FedAvg) algorithm for solving the distributed stochastic optimization problem and establish the convergence results under only a mild variance condition on the stochastic gradients for smooth nonconvex objective functions. Almost sure convergence to a stationary point is also established under the condition. Moreover, we discuss a more informative measurement for
    
[^167]: FedEBA+：基于熵的模型实现公平和有效联邦学习

    FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12407](http://arxiv.org/abs/2301.12407)

    FedEBA+是一种新的联邦学习算法，它采用公平聚合方案和对齐更新方法，在同时提高全局模型性能的同时提高公平性。实验证明FedEBA+优于其他公平性联邦学习方法。

    

    确保公平性是联邦学习中至关重要的方面，它使模型在所有客户端上保持一致表现。然而，设计一种可以同时提高全局模型性能和促进公平的联邦学习算法仍然是一个艰巨的挑战，因为实现后者通常需要与前者的权衡。为了解决这一问题，我们提出了一种新的联邦学习算法FedEBA+，它在同时提高全局模型性能的同时提高公平性，该算法采用公平聚合方案和对齐更新方法。此外，我们提供了理论收敛分析，证明了FedEBA+的公平性。大量实验表明FedEBA+在公平性和全局模型性能方面均优于其他SOTA的公平联邦学习方法。

    Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.
    
[^168]: SPEED: 线性异方差 Bandit 策略评估的实验设计

    SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12357](http://arxiv.org/abs/2301.12357)

    本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。

    

    本文研究了线性 Bandit 下策略评估的最优数据收集问题。在策略评估中，我们需要估计多臂赌博机环境中执行目标策略将获得的期望收益。本文是首个专注于解决线性 Bandit 环境下包含异方差奖励噪声的策略评估的最优数据收集策略的工作。我们首先在线性 Bandit 环境下制定了加权最小二乘估计的最优设计，以减少目标策略价值的均方误差。接着，我们使用该设计来推导出数据收集期间每个动作的最优样本分配。然后，我们引入了一种名为 SPEED（Structured Policy Evaluation Experimental Design）的新算法，该算法跟踪最优设计，并计算其与最优设计的遗憾。最后，我们通过实验证明 SPEED 可以实现带有均方误差比较小的策略评估。

    In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa
    
[^169]: 《联邦学习是否真正需要反向传播？》

    Does Federated Learning Really Need Backpropagation?. (arXiv:2301.12195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12195](http://arxiv.org/abs/2301.12195)

    本文提出一种不需要反向传播的联邦学习框架BAFFLE，该框架使用多个正向过程估计梯度，具有高内存效率，容易适应上传带宽，与硬件优化和模型量化/修剪兼容，适用于受信任的执行环境。

    

    联邦学习（FL）是一种去中心化地让客户端共同训练一个服务器模型的一般性原则，而无需共享本地数据。FL是一个具有实际应用的有前途的框架，但其标准训练范式要求客户端通过模型进行反向传播以计算梯度。由于这些客户端通常是边缘设备而不是完全受信任的，因此在它们上执行反向传播会产生计算和存储开销以及白盒漏洞。因此，我们开发了一种不需要反向传播的联邦学习，称为BAFFLE，其中反向传播替换为多个正向过程以估计梯度。BAFFLE具有以下优点：1）内存效率高并且容易适应上传带宽；2）与仅推理硬件优化以及模型量化或修剪兼容；3）非常适合受信任的执行环境，因为BAFFLE中的客户端仅执行正向传播并返回一组标量到服务器。我们通过实验使用了BAFFLE的优越性能。

    Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we us
    
[^170]: 语言模型在处理量词时表现略有问题？使用少量类型的量词会导致语言模型的预测呈现反比例缩放的现象。

    Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers. (arXiv:2212.08700v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08700](http://arxiv.org/abs/2212.08700)

    本文研究了语言模型处理“few-”类型量词的能力，结果显示所有模型对这种量词都表现不佳，且较大的模型表现更差。这种反比例缩放的现象表明大型模型越来愈反映在线人类处理，而不是离线处理。这可能挑战使用语言模型作为自然语言系统基础的做法。

    

    本研究探讨了语言模型如何处理量化问题。我们以“few-”类型的量词为重点，比如“few children like toys”，因为这种类型的句子组成部分通常会共现，而“few-”类型的量词较为罕见，这可能对语言模型构成特别挑战。我们对来自两项人类神经语言学实验的960个英语句子进行了试验，并将它们提供给22个不同大小的自回归变换器模型。不仅所有模型对“few-”类型的量词都表现不佳，而且总体上，模型越大，其表现越差。这种反比例缩放的现象与之前的研究结果一致，表明较大的模型越来越反映在线人类处理，而不是离线处理。我们认为，更大的模型的性能下降可能会挑战将语言模型作为自然语言系统基础的做法。

    How well do language models deal with quantification? In this study, we focus on 'few'-type quantifiers, as in 'few children like toys', which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and 'few'-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on 'few'-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.
    
[^171]: DAMP：面向任务型对话的双重对齐多语言解析器

    DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08054](http://arxiv.org/abs/2212.08054)

    本文介绍了一种面向任务型对话的双重对齐多语言解析器，可以大幅提高多语言和代码切换语义解析系统的零-shot性能，提高mBERT转移性能。

    

    现代虚拟助手使用内部语义解析引擎将用户话语转化为可操作命令。然而，先前的研究表明，语义解析是一项困难的多语言转移任务，其转移效率比其他任务低。在全球市场（如印度和拉丁美洲），这是一个关键问题，因为双语用户频繁切换语言。在本研究中，我们使用两个阶段的多语言对齐，大大提高了多语言和代码切换语义解析系统的零-shot性能。首先，我们表明，对比对齐预训练可以提高英文性能和转移效率。然后，我们引入一种受限制的优化方法，用于无超参数的对抗性对齐。我们的双重对齐多语言解析器（DAMP）分别将Spanglish、Hinglish和多语言任务导向解析基准的mBERT转移性能提高了3倍、6倍和81倍。

    Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated that semantic parsing is a difficult multilingual transfer task with low transfer efficiency compared to other tasks. In global markets such as India and Latin America, this is a critical issue as switching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that constrastive alignment pretraining improves both English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively
    
[^172]: 基于模拟自监督学习的三维断层成像重建方法

    Simulator-Based Self-Supervision for Learned 3D Tomography Reconstruction. (arXiv:2212.07431v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2212.07431](http://arxiv.org/abs/2212.07431)

    这是一种能够利用计算机断层成像模拟器进行自我监督学习的三维断层成像重建方法，相比于之前的方法，不需要参考重建以进行训练，结果保真度更高且速度更快。

    

    我们提出了一种用于低剂量螺旋锥束计算机体层成像的三维体积重建的深度学习方法。先前的机器学习方法需要用另一个算法计算参考重建以进行训练。相比之下，我们使用仅带有噪声的二维X射线数据以完全自我监督的方式训练模型。这是通过在训练循环中结合快速可微的计算机断层成像模拟器实现的。由于我们不依赖于参考重建，因此我们结果的保真度不受其潜在缺陷的限制。我们在真实螺旋锥束投影和模拟幻像上评估了我们的方法。我们的结果显示与依赖现有重建技术的技术相比，具有显着更高的视觉保真度和更好的PSNR。当应用于全剂量数据时，我们的方法产生的高质量结果比迭代技术快几个数量级。

    We propose a deep learning method for 3D volumetric reconstruction in low-dose helical cone-beam computed tomography. Prior machine learning approaches require reference reconstructions computed by another algorithm for training. In contrast, we train our model in a fully self-supervised manner using only noisy 2D X-ray data. This is enabled by incorporating a fast differentiable CT simulator in the training loop. As we do not rely on reference reconstructions, the fidelity of our results is not limited by their potential shortcomings. We evaluate our method on real helical cone-beam projections and simulated phantoms. Our results show significantly higher visual fidelity and better PSNR over techniques that rely on existing reconstructions. When applied to full-dose data, our method produces high-quality results orders of magnitude faster than iterative techniques.
    
[^173]: 深层ReLU网络中的最大初始学习率

    Maximal Initial Learning Rates in Deep ReLU Networks. (arXiv:2212.07295v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.07295](http://arxiv.org/abs/2212.07295)

    本文针对深度学习中的学习率问题，提出了最大初始学习率的概念，并发现其行为与训练后期的最大学习率不同。我们得出结论：在一定条件下，最大初始学习率可以很好地预测为深度×宽度的幂次。

    

    训练神经网络需要选择适当的学习率，这涉及到速度和有效收敛之间的权衡。尽管对于学习率可以有多大进行了相当大量的理论和实证分析，但大多数先前的工作只关注于后期训练。在这项工作中，我们引入了最大初始学习率$\eta^{*}$——在这个学习率下，一个随机初始化的神经网络可以成功地开始训练并达到（至少）一个给定的阈值精度。使用简单的方法估计$\eta^{*}$，我们观察到，在恒定宽度的完全连接的ReLU网络中，$\eta^{*}$的行为与训练后期的最大学习率不同。具体而言，我们发现，$\eta^{*}$可以很好地预测为深度$\times$宽度的幂次，前提是（i）网络的宽度相对深度足够大，（ii）输入层以相对较小的学习率进行训练。

    Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence. While there has been considerable theoretical and empirical analysis of how large the learning rate can be, most prior work focuses only on late-stage training. In this work, we introduce the maximal initial learning rate $\eta^{\ast}$ - the largest learning rate at which a randomly initialized neural network can successfully begin training and achieve (at least) a given threshold accuracy. Using a simple approach to estimate $\eta^{\ast}$, we observe that in constant-width fully-connected ReLU networks, $\eta^{\ast}$ behaves differently from the maximum learning rate later in training. Specifically, we find that $\eta^{\ast}$ is well predicted as a power of depth $\times$ width, provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. We fu
    
[^174]: 基于任务相似度元学习加速多目标非分层超参数最优化的树形结构Parzen估计

    Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06751](http://arxiv.org/abs/2212.06751)

    本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。

    

    超参数优化是提高深度学习性能的关键步骤。实践者通常面临多个方面的权衡，如准确性和延迟时间。在深度学习的高计算需求和对高效超参数优化的不断增长需求下，加速多目标优化变得越来越重要。本文将TPE的收购函数扩展到元学习设置中，使用由任务之间顶级域之间的重叠度定义的任务相似性。我们也从理论上分析并解决了任务相似性的局限性。在实验中，我们展示了我们的方法在表格HPO基准上加速了MO-TPE，并获得了最先进的性能。我们的方法还通过赢得AutoML 2022来得到外部验证。

    Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
    
[^175]: 非参数遮蔽语言建模

    Nonparametric Masked Language Modeling. (arXiv:2212.01349v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01349](http://arxiv.org/abs/2212.01349)

    NPM是第一个使用非参数分布替换softmax的遮蔽语言模型，可以更好地处理稀有模式和预测罕见或几乎未见过的单词，并在16项任务上超过了更大的参数模型。

    

    现有的语言模型（LM）通过有限词汇表上的 softmax 来预测标记，这可能使得预测稀有标记或短语变得困难。我们介绍了 NPM，它是第一个使用对每个参考语料库中短语的非参数分布替换此 softmax 的非参数遮蔽语言模型。NPM 仅通过从文本语料库中检索标记来填写 [MASK]。我们展示了 NPM 可以通过对比性目标和批量近似全语料库检索有效地训练。对 16 项任务进行零样本评估，包括分类、事实探针和问题回答，证明 NPM 超过了显着更大的参数模型，无论使用或不使用检索生成方法，它在处理稀有模式（词义或事实）和预测罕见或几乎未见过的单词（如非拉丁文脚本）方面表现出更好的性能。我们在 github.com/facebookresearch/NPM 上发布了模型和代码。

    Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.
    
[^176]: 适用于回声状态网络的适当正交分解的探讨

    Investigation of Proper Orthogonal Decomposition for Echo State Networks. (arXiv:2211.17179v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17179](http://arxiv.org/abs/2211.17179)

    本研究探讨了适用于回声状态网络的适当正交分解策略，该策略可通过找到等效性的低阶表示形式来替换高维的ESN。结果表明，基于POD的ESN显著减少所需的状态数，同时保持良好的性能，使其在计算时间和鲁棒性方面更加高效。

    

    回声状态网络（ESN）是一种递归神经网络，在表示时间序列和非线性动态系统方面展现出良好的效果。虽然它们配备了非常有效的训练程序，例如ESN这样的储留计算策略，但却需要高阶网络即许多神经元，导致状态数量远高于模型的输入和输出数量。大量的状态不仅会增加时间步计算的成本，还会导致鲁棒性问题，尤其是将ESN应用于模型预测控制（MPC）和其他最优控制问题时。一种避免这种复杂性问题的方法是通过模型阶数降低策略，例如适当正交分解（POD）及其变体（POD-DEIM），从而找到一个等效的低阶表示形式来替换已经训练过的高维ESN。为此，本文旨在探讨并分析适用于回声状态网络的适当正交分解（POD）在时间序列建模和最优控制问题中的表现。研究旨在表明如何通过POD有效地减少ESN的复杂度，找到一个低阶表示形式，从而保留ESN的动态性和性能。结果表明，基于POD的ESN可以显著减少所需的状态数，同时保持良好的性能，使其在计算时间和鲁棒性方面更加高效。

    Echo State Networks (ESN) are a type of Recurrent Neural Network that yields promising results in representing time series and nonlinear dynamic systems. Although they are equipped with a very efficient training procedure, Reservoir Computing strategies, such as the ESN, require high-order networks, i.e., many neurons, resulting in a large number of states that are magnitudes higher than the number of model inputs and outputs. A large number of states not only makes the time-step computation more costly but also may pose robustness issues, especially when applying ESNs to problems such as Model Predictive Control (MPC) and other optimal control problems. One way to circumvent this complexity issue is through Model Order Reduction strategies such as the Proper Orthogonal Decomposition (POD) and its variants (POD-DEIM), whereby we find an equivalent lower order representation to an already trained high dimension ESN. To this end, this work aims to investigate and analyze the performance 
    
[^177]: 多源数据用于离线强化学习的行为估计

    Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning. (arXiv:2211.16078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16078](http://arxiv.org/abs/2211.16078)

    本文提出了一种用于离线强化学习的多源数据行为估计方法，通过潜变量模型推断策略来克服数据异构性导致的行为错误。

    

    离线强化学习由于数据效率高而备受关注。本文关注于行为估计，这是许多离线强化学习算法的基础任务。行为估计的目标是估计生成训练数据的策略。本文考虑了从多个来源收集数据的场景。在这种情况下，忽略数据异构性，现有的行为估计方法会出现行为错误。为了克服这一缺陷，本文提出了一个潜变量模型来从数据中推断一组策略，使代理能够使用最能描述特定轨迹的策略作为行为策略。该模型为多源数据提供了代理的精细化描述，并帮助它克服行为错误。本文还为该模型提出了一个学习算法，并通过扩展现有的离线强化学习算法Soft Actor-Critic（SAC）来处理多源数据，说明了它的实际用途。实验结果表明，与现有最先进的替代方法相比，所提出的方法是有效的。

    Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that lays the foundation of many offline RL algorithms. Behavior estimation aims at estimating the policy with which training data are generated. In particular, this work considers a scenario where the data are collected from multiple sources. In this case, neglecting data heterogeneity, existing approaches for behavior estimation suffers from behavior misspecification. To overcome this drawback, the present study proposes a latent variable model to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. This model provides with a agent fine-grained characterization for multi-source data and helps it overcome behavior misspecification. This work also proposes a learning algorithm for this model and illustrates its practical usage via extending an 
    
[^178]: c-TPE:基于树形结构的带不等式约束的帕捷斯特估计器用于昂贵超参数优化

    c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14411](http://arxiv.org/abs/2211.14411)

    本文提出了约束TPE（c-TPE）方法，是树形Parzen估计器（TPE）的扩展，可有效处理在性能要求之上施加的约束限制，实验证明在81个昂贵的HPO设置中表现出最佳性能排名。

    

    超参数优化（HPO）对于深度学习算法的强大性能至关重要，实际应用通常会在性能要求之上施加一些限制，例如内存使用或延迟等。在本文中，我们提出了约束TPE（c-TPE），这是广泛使用的多功能贝叶斯优化方法——树形Parzen估计器（TPE）的扩展，以处理这些约束。我们提出的扩展不仅是简单地将现有收益函数和原始TPE组合起来，而是包括修改来解决导致性能不佳的问题。我们从经验和理论上深入分析这些修改，提供了有关它们如何有效地克服这些挑战的见解。在实验中，我们证明了c-TPE在81个昂贵的HPO设置中表现出最佳的平均排名性能，具有统计显着性。

    Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
    
[^179]: 通过Monte Carlo Forest Search实现UNSAT求解器的合成

    UNSAT Solver Synthesis via Monte Carlo Forest Search. (arXiv:2211.12581v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.12581](http://arxiv.org/abs/2211.12581)

    介绍了使用MCFS算法合成UNSAT求解器的方法，算法可用于解决包括SAT公式不可满足性证明、可满足SAT公式解的数量计数和混合整数规划的最优解问题，并利用合成森林构建算法和合成MDP类来避免构建候选树森林的问题。

    

    我们介绍了Monte Carlo Forest Search（MCFS），一类用于学习决策树MDP策略的强化学习（RL）算法。这些问题的示例包括证明SAT公式的不可满足性；计算可满足的SAT公式的解的数量；以及找到混合整数规划的最优解。MCFS算法可以看作是Monte Carlo Tree Search（MCTS）的扩展，用于在候选树的森林中寻找一个小树，而不是在树中找到一个好路径（解决方案）。我们在算法中实例化和评估了自己的想法，称之为Knuth Synthesis，这是一个MCFS算法，用于学习DPLL分支策略来解决布尔可满足性（SAT）问题。这利用了两个关键思想，以避免构建候选树森林的问题：（1）一种合成森林构建算法，通过从池中随机选择“好”的树并将它们组合成更大的森林来逐步构建森林；（2）一种合成MDP类，用作真实树MDP的代理，我们可以轻松计算节点间转换的概率。

    We introduce Monte Carlo Forest Search (MCFS), a class of reinforcement learning (RL) algorithms for learning policies in {tree MDPs}, for which policy execution involves traversing an exponential-sized tree. Examples of such problems include proving unsatisfiability of a SAT formula; counting the number of solutions of a satisfiable SAT formula; and finding the optimal solution to a mixed-integer program. MCFS algorithms can be seen as extensions of Monte Carlo Tree Search (MCTS) to cases where, rather than finding a good path (solution) within a tree, the problem is to find a small tree within a forest of candidate trees. We instantiate and evaluate our ideas in an algorithm that we dub Knuth Synthesis, an MCFS algorithm that learns DPLL branching policies for solving the Boolean satisfiability (SAT) problem, with the objective of achieving good average-case performance on a given distribution of unsatisfiable problem instances. Knuth Synthesis leverages two key ideas to avoid the pr
    
[^180]: 使用随机梯度下降算法训练的神经网络学习日益复杂的分布

    Neural networks trained with SGD learn distributions of increasing complexity. (arXiv:2211.11567v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.11567](http://arxiv.org/abs/2211.11567)

    本文证明了随机梯度下降算法训练的神经网络在学习期间会出现分布式简单性偏差（DSB），即最初使用低阶输入统计来分类输入，只有在训练后期才利用更高阶的统计信息。

    

    深度神经网络即使在插值训练数据时也能很好地进行泛化的能力已经通过各种“简单性偏差”得到了解释。这些理论假设神经网络在学习更复杂的非线性函数之前先学习简单函数，例如线性分类器。同时，数据结构也被认为是良好泛化的关键因素，然而，数据结构在简单性偏差中的作用尚未被理解。本文中，我们展示了使用随机梯度下降算法训练的神经网络最初使用低阶输入统计（如均值和协方差）来对其输入进行分类，只有在训练后期才利用更高阶的统计信息。我们首先在神经网络对合成数据进行训练的可解模型中展示了这种分布式简单性偏差（DSB）。然后，我们在训练于CIFAR10上的一系列深度卷积网络和视觉转换器中经验性地证明了DSB，甚至发现该偏差在更大的数据集和更广泛的神经网络体系结构中也存在。

    The ability of deep neural networks to generalise well even when they interpolate their training data has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first learning simple functions, say a linear classifier, before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias (DSB) in a solvable model of a neural network trained on synthetic data. We empirically demonstrate DSB in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in network
    
[^181]: 多视角压缩表示的鲁棒性低资源微调研究

    Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08794](http://arxiv.org/abs/2211.08794)

    本文提出了一种利用多视角压缩表示降低预训练语言模型微调过程中过拟合问题的方法，经过测试在低资源NLP任务中表现良好。

    

    由于参数的巨大数量，预训练语言模型（PLMs）的微调容易在低资源场景中出现过度拟合的问题。本文提出了一种新方法，该方法在PLM的隐藏表示上操作，以减少过拟合。在微调过程中，我们的方法在PLM的隐藏层之间插入随机自编码器，将来自前一层的激活转换为多视角压缩表示，然后将其馈送到上层。微调结束后，自编码器会被移除掉，因此我们的方法在推理过程中不会增加额外的参数或计算成本。我们的方法在一系列序列和标记级别的低资源NLP任务中展现了出色的性能提升。

    Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
    
[^182]: 零偏置标量不变网络

    Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.08486](http://arxiv.org/abs/2211.08486)

    本文证明了在解决许多图像任务(例如图像分类)时可以忽略偏置，并且零偏置神经网络在实际图像分类任务中表现良好，同时具有标量 (乘法) 不变性，从而在改变对比度时仍能保持预测不变。

    

    与权重一样，偏置项也是许多流行的机器学习模型(包括神经网络)可学习的参数。人们认为偏差能有效地增加神经网络表示能力来解决计算机视觉中的各种任务。然而，我们认为，如果我们从第一原理考虑图像在输入空间中的内在分布以及模型应具有的一些期望特性，则偏差可以完全忽略，以解决许多与图像相关的任务，例如图像分类任务。我们的观察结果表明，零偏置神经网络在实际图像分类任务上可能与带偏置的神经网络表现相当。此外，我们证明零偏置神经网络具有称为标量(乘法)不变性的良好属性，这使得当改变输入图像的对比度时，神经网络的预测保持不变。然后，我们将标量不变性扩展到更一般的情况…

    Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
    
[^183]: PAD-Net：用于动态网络的高效框架

    PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05528](http://arxiv.org/abs/2211.05528)

    PAD-Net是一个部分动态网络的框架，将冗余的动态参数转换为静态参数，提高了动态网络的效率和适用性。

    

    动态网络，例如动态卷积（DY-Conv）和专家混合模型（MoE），已被广泛探索，因为它们可以显着提高模型的表示能力，并具有可接受的计算成本。实现动态网络的常见做法是将给定的静态层转换为完全动态的层，其中所有参数都是动态的（至少在单个层内）并随输入变化。但是，这种完全动态的设置可能会导致冗余参数和高部署成本，从而限制了动态网络在更广泛的任务和模型中的适用性。我们工作的主要贡献是挑战动态网络的基本常识，并提出部分动态网络，即PAD-Net，以将冗余动态参数转换为静态参数。此外，我们进一步设计迭代模式分区来有效地分区动态和静态参数。我们的方法受到大规模实验的全面支持。

    Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi
    
[^184]: 革新空中自主技术的可扩展模块化合成数据生成

    Scalable Modular Synthetic Data Generation for Advancing Aerial Autonomy. (arXiv:2211.05335v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05335](http://arxiv.org/abs/2211.05335)

    这篇论文探索了提高空中自主技术进步的一个重要问题：如何生成大量的空中数据集以训练机器学习模型。该研究使用合成数据并利用模拟环境和数据增强来解决现有工具的局限性，提高数据生成工作流程的可扩展性和泛化能力。

    

    推动空中自主技术进步的一个重要障碍是获取用于训练机器学习模型的大规模空中数据集。为了避免实机数据采集的高成本和耗时，越来越多的无人机应用开始使用合成数据来训练模型。然而，要想提高模型的泛化能力并将其迁移到实际环境，增加模拟环境的多样性以训练所有可能情况下的模型，以及增加训练数据进行数据增强，都是必不可少的。目前，现有的合成空中数据生成工具要么缺乏数据增强，要么依赖于手工负载或实际样本进行配置和生成多样化的真实模拟场景以进行数据采集。这些依赖性限制了数据生成工作流程的可扩展性。因此，在合成数据生成中平衡泛化能力和可扩展性是一个主要的挑战。

    One major barrier to advancing aerial autonomy has been collecting large-scale aerial datasets for training machine learning models. Due to costly and time-consuming real-world data collection through deploying drones, there has been an increasing shift towards using synthetic data for training models in drone applications. However, to increase widespread generalization and transferring models to real-world, increasing the diversity of simulation environments to train a model over all the varieties and augmenting the training data, has been proved to be essential. Current synthetic aerial data generation tools either lack data augmentation or rely heavily on manual workload or real samples for configuring and generating diverse realistic simulation scenes for data collection. These dependencies limit scalability of the data generation workflow. Accordingly, there is a major challenge in balancing generalizability and scalability in synthetic data generation. To address these gaps, we i
    
[^185]: 过度参数化的分类器泛化能力强，因为糟糕的解决方案很少

    Highly over-parameterized classifiers generalize since bad solutions are rare. (arXiv:2211.03570v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03570](http://arxiv.org/abs/2211.03570)

    本文研究发现，在过度参数化的情况下，零训练误差的全局最小值中“坏”方案的占比随训练数据的增加而指数级递减，并能解释高度参数化的神经网络具有出乎意料的良好泛化能力。

    

    我们研究了过度参数化的分类器的泛化能力，其中通过经验风险最小化（ERM）学习导致零训练误差。在这些过度参数化的设置中，有许多具有零训练误差的全局最小值，其中一些比其他的更具有普适性。我们表明，在一定的条件下，“坏”全局最小值的分数，其真实误差大于ε，与训练数据n的数量指数级地递减到零。该范围取决于用于给定分类问题的分类器函数集合上真实误差的分布，不一定取决于分类器函数集合的大小或复杂度（例如参数数量）。这可能解释了即使高度参数化的神经网络也具有出乎意料的良好泛化能力。我们在合成数据集和MNIST的子集上通过实验来支持我们的数学框架。

    We study the generalization of over-parameterized classifiers where Empirical Risk Minimization (ERM) for learning leads to zero training error. In these over-parameterized settings there are many global minima with zero training error, some of which generalize better than others. We show that under certain conditions the fraction of "bad" global minima with a true error larger than {\epsilon} decays to zero exponentially fast with the number of training data n. The bound depends on the distribution of the true error over the set of classifier functions used for the given classification problem, and does not necessarily depend on the size or complexity (e.g. the number of parameters) of the classifier function set. This might explain the unexpectedly good generalization even of highly over-parameterized Neural Networks. We support our mathematical framework with experiments on a synthetic data set and a subset of MNIST.
    
[^186]: 工业环境中声学异常检测中特征的探讨——以齿轮电机末端测试为例

    Discussion of Features for Acoustic Anomaly Detection under Industrial Disturbing Noise in an End-of-Line Test of Geared Motors. (arXiv:2211.01716v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.01716](http://arxiv.org/abs/2211.01716)

    本研究探讨了工业环境中齿轮电机末端测试中声学异常检测的特征鲁棒性。使用从对数包络谱中提取的特征和心理声学特征，结合孤立森林或装袋随机矿工算法进行检测，能有效避免大多数干扰，但使用锤子或气压则会产生问题。

    

    在齿轮电机的末端测试中，产品质量的评估至关重要。由于时间限制和各种变体的高度多样性，声学测量比振动测量更经济。然而，声学数据受到工业干扰噪声的影响。因此，本研究旨在调查用于齿轮电机末端测试中异常检测的特征的鲁棒性。一个真实的数据集被记录下来，并包含来自生产环境的工业噪声和系统产生的干扰，用于比较特征的鲁棒性。总体而言，建议使用从对数包络谱中提取的特征和心理声学特征结合使用，使用孤立森林或更通用的装袋随机矿工进行异常检测。大多数干扰都可以避免，而使用锤子或气压通常会引起问题。

    In the end-of-line test of geared motors, the evaluation of product qual-ity is important. Due to time constraints and the high diversity of variants, acous-tic measurements are more economical than vibration measurements. However, the acoustic data is affected by industrial disturbing noise. Therefore, the aim of this study is to investigate the robustness of features used for anomaly detection in geared motor end-of-line testing. A real-world dataset with typical faults and acoustic disturbances is recorded by an acoustic array. This includes industrial noise from the production and systematically produced disturbances, used to compare the robustness. Overall, it is proposed to apply features extracted from a log-envelope spectrum together with psychoacoustic features. The anomaly de-tection is done by using the isolation forest or the more universal bagging random miner. Most disturbances can be circumvented, while the use of a hammer or air pressure often causes problems. In genera
    
[^187]: 基于卷积神经网络的量子相似性检测

    Quantum Similarity Testing with Convolutional Neural Networks. (arXiv:2211.01668v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.01668](http://arxiv.org/abs/2211.01668)

    本文提出了一种基于卷积神经网络的机器学习算法，通过测量数据构建量子态的低维表示进行相似性评估，可以对非高斯量子态进行相似性检测，并在准确性和效率方面优于以往方法。

    

    检测两个未经特征化的量子设备是否以相同方式运作对于基准测试近期量子计算机和量子模拟器至关重要，但至今为止，在连续变量量子系统中，存在一些未解决问题。在本文中，我们开发了一种机器学习算法，使用有限且嘈杂的数据比较未知的连续变量态。算法适用于非高斯量子态，而此前的技术无法实现相似性检测。我们的方法基于一个卷积神经网络，根据测量数据构建一个低维态表示来评估量子态的相似性。网络可以进行离线训练，使用具有与待测试态相似结构的一组基准态的经典模拟数据，或使用对基准态的测量所生成的实验数据，或使用模拟和实验数据的组合。我们通过比较非高斯压缩相干态来测试算法的性能，并证明它在准确性和效率方面均优于先前的方法。

    The task of testing whether two uncharacterized quantum devices behave in the same way is crucial for benchmarking near-term quantum computers and quantum simulators, but has so far remained open for continuous-variable quantum systems. In this Letter, we develop a machine learning algorithm for comparing unknown continuous variable states using limited and noisy data. The algorithm works on non-Gaussian quantum states for which similarity testing could not be achieved with previous techniques. Our approach is based on a convolutional neural network that assesses the similarity of quantum states based on a lower-dimensional state representation built from measurement data. The network can be trained offline with classically simulated data from a fiducial set of states sharing structural similarities with the states to be tested, or with experimental data generated by measurements on the fiducial states, or with a combination of simulated and experimental data. We test the performance o
    
[^188]: RQUGE：一种基于回答问题评估问题生成的无参考度量方法

    RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.01482](http://arxiv.org/abs/2211.01482)

    RQUGE是一种新的度量标准方法，通过候选问题是否可以回答来评估问题生成质量, 比现有指标更加稳健，可以在不需要人工提供参考问题的情况下使用。

    

    现有的评估自动生成问题质量的指标（如BLEU、ROUGE、BERTScore和BLEURT）将参考和预测问题进行比较，当候选问题和参考问题之间存在相当的词汇重叠或语义相似性时，提供高分。该方法存在两个主要缺点：首先，我们需要昂贵的人工提供参考问题；其次，它惩罚那些可能与参考问题没有高词汇或语义相似性的有效问题。在本文中，我们提出一种新的度量标准RQUGE，基于给定上下文的候选问题的可回答性。该度量标准由一个问答模块和一个跨度评分器模块组成，使用现有文献中的预训练模型，因此可以在不进行进一步训练的情况下使用。我们证明RQUGE与人类判断具有更高的相关性，而不依赖于参考问题。此外，RQUGE显示更加稳健。

    Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question. Additionally, RQUGE is shown to be more robust to several ad
    
[^189]: 防止神经语言模型的逐字记忆会产生虚假隐私保护感

    Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17546](http://arxiv.org/abs/2210.17546)

    防止神经语言模型逐字记忆无法真正保护隐私，本文设计的布隆过滤器虽然防止了所有逐字记忆，但仍然无法防止训练数据泄露，容易被合理修改的“样式转换”提示绕过。

    

    通过研究神经语言模型中数据记忆的现象，本研究帮助我们理解与隐私或版权相关的风险，并有助于评估对策。然而逐字记忆定义过于严格，未能捕捉更为微妙的记忆形式。本文基于布隆过滤器设计并实现了一种有效的防御方法，但该“完美”过滤器并不能防止训练数据泄露。

    Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA
    
[^190]: MARLlib: 一个可扩展的多智能体强化学习库

    MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13708](http://arxiv.org/abs/2210.13708)

    本文提出了MARLlib，这是一个全面的MARL算法库，可统一数十种算法。它还超越了当前工作，集成了各种环境接口和提供灵活的参数共享策略。

    

    尽管多智能体系统和多智能体强化学习算法得到了快速发展，但缺乏统一的评估平台和公认的基准实现。因此，迫切需要开发一个集成库套件，以在各种基准测试中提供可靠的MARL实现和可复制的评估。本文提出了MARLlib，这是一个全面的MARL算法库，用于解决多智能体问题。MARLlib通过新颖的基于代理的分布式数据流设计，在高度可组合的集成风格中统一了数十种算法。此外，MARLlib通过集成各种环境接口和提供灵活的参数共享策略，超越了当前工作；这允许最终用户在最小的代码修改下实现协作、竞争和混合任务的多种解决方案。最后，MARLlib提供易于使用的API和完全解耦合的配置。

    Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
    
[^191]: 实例感知图像修复

    Instance-Aware Image Completion. (arXiv:2210.12350v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.12350](http://arxiv.org/abs/2210.12350)

    本文提出了一个实例感知图像修复模型ImComplete，相比现有方法，它可以幻象出与环境背景相协调的视觉实例，提供了基于语义和结构的像素级指导。

    

    图像修复是一项旨在填补带有缺失区域的图像的任务，使它们具有合理的内容。然而，现有的图像修复方法往往通过填充周围纹理来填补缺失区域，而不是去幻象一个与环境背景相协调的视觉实例。在本研究中，我们提出了一种新的图像修复模型，名为ImComplete，该模型可以幻象缺失的实例，从而与原始背景协调。ImComplete首先采用了一个变压器架构，考虑到可见实例和缺失区域的位置。然后，ImComplete完成了缺失区域内的语义分割掩模，提供像素级的语义和结构指导。最后，图像合成块生成了逼真的内容。

    Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object
    
[^192]: 论述用图神经网络表示混合整数线性规划问题

    On Representing Mixed-Integer Linear Programs by Graph Neural Networks. (arXiv:2210.10759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10759](http://arxiv.org/abs/2210.10759)

    本研究探讨了用图神经网络表示混合整数线性规划问题的局限性，证明了一些针对特定情况下的限制条件下存在可靠的GNN方法，可以预测MILP问题的可行性、最优目标值和最优解。

    

    尽管混合整数线性规划(MILP)问题通常为NP难问题，但是在过去的二十年中，实际的MILP问题已经获得了大约100倍的加速。尽管如此，许多类别的MILP问题在规模不断增加时迅速变得不可解，这促使研究人员寻求新的加速技术以解决MILP问题。通过深度学习，研究人员获得了强有力的实证结果，并且许多结果都是通过将图神经网络(GNN)应用于MILP解决过程的各个阶段来获得的。本研究发现了一个根本性的局限性：存在可行和不可行的MILP问题，而所有的GNN都会平等地处理这些问题，表明GNN对于表示一般的MILP问题的能力不足。然后，我们通过限制MILP为可展开的问题或添加随机特征，证明了存在能够可靠地预测MILP可行性、最优目标值和最优解的GNN。我们进行了小规模的数值实验来验证理论结果。

    While Mixed-integer linear programming (MILP) is NP-hard in general, practical MILP has received roughly 100--fold speedup in the past twenty years. Still, many classes of MILPs quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for MILPs. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks (GNNs) to making decisions in various stages of MILP solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible MILPs that all GNNs will, however, treat equally, indicating GNN's lacking power to express general MILPs. Then, we show that, by restricting the MILPs to unfoldable ones or by adding random features, there exist GNNs that can reliably predict MILP feasibility, optimal objective values, and optimal solutions up to prescribed precision. We conducted small-scale numerical experiments to validate our theoretical fin
    
[^193]: 基于技能的强化学习与内在奖励匹配

    Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07426](http://arxiv.org/abs/2210.07426)

    该论文提出了内在奖励匹配(IRF)方法，通过技能鉴别器匹配内在和下游任务奖励来确定未见任务的最优技能，提高了系统效率。

    

    虽然无监督技能探索已经展示了自主获取行为原语的潜力，但是任务无关的技能预训练和下游的任务感知调优之间仍存在很大的方法论差距。我们提出内在奖励匹配(IRF)，通过预训练模型组件 "技能鉴别器" 统一这两个学习阶段。传统方法在策略级别直接微调预训练代理，通常依赖于昂贵的环境回放来经验性地确定最优技能。然而，任务最简明但完整的描述通常是奖励函数本身，技能学习方法通过鉴别器学习与技能策略相对应的“内在”奖励函数。我们建议利用技能鉴别器“匹配”内在和下游任务奖励，并确定未见任务的最优技能，从而提高效率。

    While unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the $\textit{skill discriminator}$, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an $\textit{intrinsic}$ reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to $\textit{match}$ the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without enviro
    
[^194]: 演员评论或评论演员？两个时间尺度的故事。

    Actor-Critic or Critic-Actor? A Tale of Two Time Scales. (arXiv:2210.04470v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04470](http://arxiv.org/abs/2210.04470)

    这篇论文提出了一种评论演员算法，它在快速和慢速时间尺度上计算价值函数和策略，该算法与演员评论算法在准确性和计算成本方面表现相当。

    

    我们重新审视基于表格的演员评论算法的标准公式，将其视为两个时间尺度的随机逼近，其中价值函数在快速时间尺度上计算，策略在慢速时间尺度上计算。这模拟了策略迭代。我们首先观察到，时间尺度的反转实际上会模拟值迭代，并且是一种合法的算法。我们提供了收敛性证明，并通过带有线性和非线性函数逼近器的函数逼近测试两种方法，并观察到我们提出的评论演员算法在准确性和计算成本方面与演员评论算法相当。

    We revisit the standard formulation of tabular actor-critic algorithm as a two time-scale stochastic approximation with value function computed on a faster time-scale and policy computed on a slower time-scale. This emulates policy iteration. We begin by observing that reversal of the time scales will in fact emulate value iteration and is a legitimate algorithm. We provide a proof of convergence and compare the two empirically with and without function approximation (with both linear and nonlinear function approximators) and observe that our proposed critic-actor algorithm performs on par with actor-critic in terms of both accuracy and computational effort.
    
[^195]: 边界感知不确定性可解释特征的探索

    Boundary-Aware Uncertainty for Feature Attribution Explainers. (arXiv:2210.02419v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02419](http://arxiv.org/abs/2210.02419)

    本文提出了一种名为高斯过程解释不确定性（GPEC）框架，它可以对复杂的黑盒分类器进行可靠的特征归因，该框架结合了决策边界感知不确定性和解释函数逼近不确定性，能够生成一个统一的不确定性估计。

    

    后续的解释方法已经成为理解高风险应用中黑盒分类器的关键工具。然而，高性能分类器通常是高度非线性的，并且在决策边界周围展现出复杂的行为，导致脆弱或误导性的局部解释。因此，有必要量化这种解释方法的不确定性，以了解何时可以信任解释。在本文中，我们提出了高斯过程解释不确定性（GPEC）框架，它生成了一个统一的不确定性估计，将决策边界感知不确定性与解释函数逼近不确定性相结合。我们介绍了一种新的基于测地线的核，它捕捉目标黑盒决策边界的复杂性。我们理论上证明所提出的核相似度随着决策边界的复杂性递增。该提出的框架非常灵活，可以与任何黑盒分类器和任何解释方法一起使用。我们在各种数据集上进行实验，并显示GPEC优于现有方法的不确定度估计，并导致更可靠的特征归因。

    Post-hoc explanation methods have become a critical tool for understanding black-box classifiers in high-stakes applications. However, high-performing classifiers are often highly nonlinear and can exhibit complex behavior around the decision boundary, leading to brittle or misleading local explanations. Therefore there is an impending need to quantify the uncertainty of such explanation methods in order to understand when explanations are trustworthy. In this work we propose the Gaussian Process Explanation unCertainty (GPEC) framework, which generates a unified uncertainty estimate combining decision boundary-aware uncertainty with explanation function approximation uncertainty. We introduce a novel geodesic-based kernel, which captures the complexity of the target black-box decision boundary. We show theoretically that the proposed kernel similarity increases with decision boundary complexity. The proposed framework is highly flexible; it can be used with any black-box classifier an
    
[^196]: 学习规则对广泛神经网络表征动力学的影响

    The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks. (arXiv:2210.02157v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.02157](http://arxiv.org/abs/2210.02157)

    本论文分析了无限宽度的深度网络，使用不同的学习规则如GD、FA、DFA、Hebb和GLN进行训练，并发现每种规则下的输出函数演化都受到时间变化的有效神经切向核(eNTK)的影响。通过动态均场理论(DMFT)比较了每种学习规则所引起的特征和预测动力学。

    

    现在尚不清楚改变深度神经网络的学习规则如何改变其学习动力学和表征。为了深入了解学习特征、函数逼近和学习规则之间的关系，我们分析了无限宽的深度网络，采用了梯度下降(GD)以及生物可行的替代方法，包括反馈对齐(FA)、直接反馈对齐(DFA)、误差调制黑比学习(Hebb)，以及门控线性网络(GLN)进行训练。

    It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel's evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy 
    
[^197]: 分层对抗逆强化学习

    Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01969](http://arxiv.org/abs/2210.01969)

    本文提出了一种分层对抗逆强化学习算法，能够在复杂任务中学习到具有层次结构的最优策略，比现有的方法更加有效。

    

    模仿学习（IL）一般用于从演示中恢复专家策略。然而，对于高度复杂的、长时程任务，恢复单一整体策略是困难的，而专家策略通常包含子任务层次结构。因此，研究者开发了分层模仿学习（HIL）方法，通过在选项框架中显式地建模任务中的活动结构来学习分层策略。现有的HIL方法要么忽视了子任务结构与学习策略之间的因果关系，要么无法同时在分层框架中学习高级别和低级别策略，导致亚最优。本文提出了一种新的HIL算法——分层对抗逆强化学习（H-AIRL），它在最新的IL算法AIRL上扩展了一步选项框架，重新定义了AIRL目标。

    Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
    
[^198]: 下游数据集意外地成为良好的预训练语料库

    Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.14389](http://arxiv.org/abs/2209.14389)

    本文研究了使用下游数据集进行自我预训练的效果，发现这种方法与使用大型语料库进行预训练的标准方法相媲美，并且在某些任务上更加优秀。同时，这些自我预训练模型还表现出了很好的泛化能力。

    

    对于大多数自然语言处理任务，主要的做法是使用更小的下游数据集对大型预训练变压器模型（例如BERT）进行微调。尽管这种方法取得了成功，但目前尚不清楚这些收益在多大程度上归因于用于预训练的大规模语料库，而不是预训练目标本身。本文介绍了一项关于自我预训练（self-pretraining）的大规模研究，其中相同的（下游）训练数据用于预训练和微调。在针对ELECTRA和RoBERTa模型以及10个不同的下游分类数据集的实验中，我们观察到自我预训练与使用BookWiki语料库进行标准预训练相媲美（尽管使用的数据量仅为后者的$10$倍到$500$倍不等），并且在$7$个和$5$个数据集上分别优于后者。令人惊讶的是，这些针对特定任务的预训练模型在其他任务上表现良好，包括GLUE基准测试。除了分类任务，自我预训练模型还可以用于生成和抽取任务。

    For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around $10\times$--$500\times$ less data), outperforming the latter on $7$ and $5$ datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Besides classification tasks, self-pretrain
    
[^199]: 论用图神经网络表示线性规划问题

    On Representing Linear Programs by Graph Neural Networks. (arXiv:2209.12288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12288](http://arxiv.org/abs/2209.12288)

    本文构建了一个GNN来处理不同LP并给出了合理的证明，证明了该GNN可以预测LP的可行性、有界性和最优解。同时，我们展示了该GNN在处理LP时的准确性。

    

    学习优化是快速发展的一个领域，旨在使用机器学习（ML）解决优化问题或改进现有的优化算法。图神经网络（GNN）被认为是适用于具有置换不变性的优化问题的合适ML模型，例如线性规划（LP）。尽管文献报道了令人鼓舞的数值结果，但本文确立了将GNN应用于解决LP的理论基础。我们构建了一个GNN，将不同的LP映射到不同的输出，针对任何LP的大小限制。我们展示了适当构建的GNN可以可靠地预测广泛类别中每个LP的可行性，有界性和最优解。我们的证明基于最近发现的Weisfeiler--Lehman同构测试和GNN之间的联系。为了验证我们的结果，我们训练了一个简单的GNN，并展示了它在将LP映射到可行性和解决方案方面的准确性。

    Learning to optimize is a rapidly growing area that aims to solve optimization problems or improve existing optimization algorithms using machine learning (ML). In particular, the graph neural network (GNN) is considered a suitable ML model for optimization problems whose variables and constraints are permutation--invariant, for example, the linear program (LP). While the literature has reported encouraging numerical results, this paper establishes the theoretical foundation of applying GNNs to solving LPs. Given any size limit of LPs, we construct a GNN that maps different LPs to different outputs. We show that properly built GNNs can reliably predict feasibility, boundedness, and an optimal solution for each LP in a broad class. Our proofs are based upon the recently--discovered connections between the Weisfeiler--Lehman isomorphism test and the GNN. To validate our results, we train a simple GNN and present its accuracy in mapping LPs to their feasibilities and solutions.
    
[^200]: 无线网络中基于可变位宽的联邦学习性能优化

    Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10200](http://arxiv.org/abs/2209.10200)

    本文提出了一种在联邦学习中应用模型量化的方案，通过联邦学习的可变位宽优化来提高无线通信和计算效率，在无线资源受限的情况下，采用多尺度量化联邦学习聚合算法，能够有效改善联邦学习的性能，具备更高的收敛速度和更少的训练损失。

    

    本文研究了通过模型量化来改善联邦学习（FL）中的无线通信和计算效率。在所提出的位宽FL方案中，边缘设备训练和传输其本地FL模型参数的量化版本到一个协调服务器，将它们聚合成一个量化的全局模型并同步设备。目标是共同确定用于本地FL模型量化的位宽和每次迭代参与FL训练的设备集合。我们将这视为一个优化问题，旨在在每次迭代的设备抽样预算和延迟要求下最小化量化FL的训练损失。然而，所制定的问题难以解决，没有(i)对量化如何影响全局ML性能的具体理解以及(ii)服务器构建这个过程估计的能力。为了解决第一个挑战，我们分析地表征了有限的无线资源如何影响量化FL性能，并推导出了最优的位宽分配策略。为了应对第二个挑战，我们提出了一种新颖的多尺度量化FL聚合算法，使服务器能够轻松地从分布式量化FL模型中重构全局量化FL模型。广泛的仿真验证了我们的方法的有效性，在培训损失和收敛速度方面比现有技术方案提高了高达35％。

    This paper considers improving wireless communication and computation efficiency in federated learning (FL) via model quantization. In the proposed bitwidth FL scheme, edge devices train and transmit quantized versions of their local FL model parameters to a coordinating server, which aggregates them into a quantized global model and synchronizes the devices. The goal is to jointly determine the bitwidths employed for local FL model quantization and the set of devices participating in FL training at each iteration. We pose this as an optimization problem that aims to minimize the training loss of quantized FL under a per-iteration device sampling budget and delay requirement. However, the formulated problem is difficult to solve without (i) a concrete understanding of how quantization impacts global ML performance and (ii) the ability of the server to construct estimates of this process efficiently. To address the first challenge, we analytically characterize how limited wireless resou
    
[^201]: ReX：一个将时间信息融入模型无关局部解释技术的框架。

    ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques. (arXiv:2209.03798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.03798](http://arxiv.org/abs/2209.03798)

    提出了ReX，一个将时间信息融入模型无关局部解释技术的框架，通过为解释添加时间信息，使一些现有模型无法应用的局部解释技术可以更好地处理可变长度的输入。

    

    可以处理可变长度输入的神经网络模型具有强大的功能，但通常很难解释。缺乏透明度阻碍了它们在许多领域的应用。解释技术对于提高透明度至关重要。然而，现有的模型无关通用解释技术没有考虑输入数据点的可变长度，这限制了它们的有效性。为了解决这个问题，我们提出了ReX，这是一个通用框架，为处理可变长度输入的模型适应各种解释技术，扩展解释覆盖到不同长度的数据点。我们的方法在不改变现有技术核心算法的情况下，为现有技术生成的解释添加时间信息。我们在两种流行的解释技术LIME和Anchors上实现了我们的方法。为了评估ReX的有效性，我们将我们的方法应用于两个不同任务中的三个模型。我们的评估结果表明，我们的方法显着地提高了解释的有效性。

    Neural network models that can handle inputs of variable lengths are powerful, but often hard to interpret. The lack of transparency hinders their adoption in many domains. Explanation techniques are essential for improving transparency. However, existing model-agnostic general explanation techniques do not consider the variable lengths of input data points, which limits their effectiveness. To address this limitation, we propose ReX, a general framework for adapting various explanation techniques to models that process variable-length inputs, expanding explanation coverage to data points of different lengths. Our approach adds temporal information to the explanations generated by existing techniques without altering their core algorithms. We instantiate our approach on two popular explanation techniques: LIME and Anchors. To evaluate the effectiveness of ReX, we apply our approach to three models in two different tasks. Our evaluation results demonstrate that our approach significantl
    
[^202]: Fix-A-Step: 半监督学习处理未经筛选的无标签数据

    Fix-A-Step: Semi-supervised Learning from Uncurated Unlabeled Data. (arXiv:2208.11870v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11870](http://arxiv.org/abs/2208.11870)

    Fix-A-Step是一个半监督学习的简化流程，将所有未经筛选的无标签图像视为潜在有用的；增强有标签集的数据，修正梯度下降更新方式可以修复许多常见的深度 SSL 方法，并在医学成像数据集上实现更好的效果。

    

    半监督学习 (SSL) 在训练分类器时，通过在许多无标签图像上进行训练，承诺比在少量有标签数据集上训练时有更高的准确性。在实际应用中，例如医学成像，为了速度而收集未经筛选的无标签数据，因此可能与有标签集中的类别或特征不同。不幸的是，现代深度半监督学习在处理未经筛选的无标签数据时，常常会导致准确性下降。最近，一些较为复杂的方法通过检测分布外的无标签图像，然后丢弃或降低它们的权重来修复这个问题。与此不同，我们引入了 Fix-A-Step，一种更简单的过程，将所有未经筛选的无标签图像视为潜在有用的。我们的第一个洞见是，即使未经筛选的图像也可以产生有用的已标记数据增强。其次，我们修改了梯度下降更新的方式，以防止优化多任务 SSL 损失对有标签集准确性的损害。Fix-A-Step 可以修复许多常见的深度 SSL 方法，在 CIFAR 基准测试中提高了所有测试方法和数据集的准确性。此外，在医学成像数据集上进行评估时，Fix-A-Step 在处理未经筛选的无标签图像方面比现有的最先进方法要好得多。

    Semi-supervised learning (SSL) promises improved accuracy compared to training classifiers on small labeled datasets by also training on many unlabeled images. In real applications like medical imaging, unlabeled data will be collected for expediency and thus uncurated: possibly different from the labeled set in classes or features. Unfortunately, modern deep SSL often makes accuracy worse when given uncurated unlabeled data. Recent complex remedies try to detect out-of-distribution unlabeled images and then discard or downweight them. Instead, we introduce Fix-A-Step, a simpler procedure that views all uncurated unlabeled images as potentially helpful. Our first insight is that even uncurated images can yield useful augmentations of labeled data. Second, we modify gradient descent updates to prevent optimizing a multi-task SSL loss from hurting labeled-set accuracy. Fix-A-Step can repair many common deep SSL methods, improving accuracy on CIFAR benchmarks across all tested methods and
    
[^203]: 两人零和博弈中全反馈和噪声反馈下的最后迭代收敛

    Last-Iterate Convergence with Full and Noisy Feedback in Two-Player Zero-Sum Games. (arXiv:2208.09855v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2208.09855](http://arxiv.org/abs/2208.09855)

    本文提出的M2WU方法能够实现两人零和博弈中的最后迭代收敛，并在全反馈和噪声反馈情况下具有较高的性能和效果。

    

    本文提出了突变驱动乘性权重更新(M2WU)方法，用于学习两人零和正态型博弈中的均衡点，并证明了在全反馈和噪声反馈两种情况下都具有最后迭代收敛性质。其中，前者表示玩家观察到他们的效用函数的精确梯度向量，后者表示玩家只观察到带噪声的梯度向量。与广受欢迎的乘性权重更新(MWU)和乐观的MWU（OMWU）算法相比，在噪声反馈下，它们可能不能收敛到纳什均衡。相反，M2WU方法在两种反馈情况下都表现出最后迭代收敛到接近纳什均衡点的稳定点的性质。我们还证明了通过迭代适应突变项，M2WU方法最终会收敛到一个精确的纳什平衡状态。我们在实验中验证了M2WU在可利用性和收敛速度方面的优越性和效果高于MWU和OMWU算法。

    This paper proposes Mutation-Driven Multiplicative Weights Update (M2WU) for learning an equilibrium in two-player zero-sum normal-form games and proves that it exhibits the last-iterate convergence property in both full and noisy feedback settings. In the former, players observe their exact gradient vectors of the utility functions. In the latter, they only observe the noisy gradient vectors. Even the celebrated Multiplicative Weights Update (MWU) and Optimistic MWU (OMWU) algorithms may not converge to a Nash equilibrium with noisy feedback. On the contrary, M2WU exhibits the last-iterate convergence to a stationary point near a Nash equilibrium in both feedback settings. We then prove that it converges to an exact Nash equilibrium by iteratively adapting the mutation term. We empirically confirm that M2WU outperforms MWU and OMWU in exploitability and convergence rates.
    
[^204]: 从混乱标签中学习和谐：带有噪声标签的联邦学习

    Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels. (arXiv:2208.09378v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09378](http://arxiv.org/abs/2208.09378)

    该论文提出一个名为FedLN的框架，用于在联邦学习（FL）中处理标签噪声，这是FL中一个普遍存在且影响性能的问题。FedLN能够适应不同客户端的计算能力，涵盖FL初始化、设备端模型训练和服务器模型聚合三个阶段。

    

    联邦学习（FL）是一种分布式机器学习范例，它能够从分散的私有数据集中学习模型，标签工作委托给客户端。虽然大多数现有的FL方法假设高质量标签已经在用户设备上准备好了，但实际上，在FL中自然可能出现标签噪声，这与客户端的特性密切相关。由于在FL中可用数据的稀缺性和客户端之间显着的标签噪声变化，现有的最先进的集中式方法表现不尽如人意，而先前的FL研究则依赖于额外的干净数据或过度的设备端计算方案。在这里，我们提出了FedLN，一个框架，用于在不同的FL训练阶段处理标签噪声；即FL初始化、设备端模型训练和服务器模型聚合，能够适应FL系统中各种设备的计算能力。具体而言，FedLN计算每个客户端的噪声

    Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized private datasets, where the labeling effort is entrusted to the clients. While most existing FL approaches assume high-quality labels are readily available on users' devices; in reality, label noise can naturally occur in FL and is closely related to clients' characteristics. Due to scarcity of available data and significant label noise variations among clients in FL, existing state-of-the-art centralized approaches exhibit unsatisfactory performance, while prior FL studies rely on excessive on-device computational schemes or additional clean data available on server. Here, we propose FedLN, a framework to deal with label noise across different FL training stages; namely, FL initialization, on-device model training, and server model aggregation, able to accommodate the diverse computational capabilities of devices in a FL system. Specifically, FedLN computes per-client noi
    
[^205]: 无监督机器学习框架用于区分COVID-19主要变异体

    Unsupervised machine learning framework for discriminating major variants of concern during COVID-19. (arXiv:2208.01439v3 [q-bio.OT] UPDATED)

    [http://arxiv.org/abs/2208.01439](http://arxiv.org/abs/2208.01439)

    本文提出了一个无监督机器学习框架，利用基因组序列区分和可视化COVID-19主要变异体之间的关联。这一框架可以帮助医疗保健专业人员了解病毒的流行病学和进化动态。

    

    由于病毒的高突变率，COVID-19疫情迅速演变。某些病毒变异体，如Delta和Omicron，出现并改变了病毒的特性，导致病例传播和死亡率严重。这些变异体对全球医疗系统造成了沉重负担，对旅行、生产力和世界经济产生了重大影响。无监督机器学习方法具有压缩、描述和可视化未标记数据的能力。本文提出了一个框架，利用无监督机器学习方法，基于基因组序列区分和可视化COVID-19主要变异体之间的关联。这些方法采用一些选定的降维和聚类技术的组合。该框架通过对数据执行k-mer分析来处理RNA序列，并使用包括主成分分析（PCA）、t-分布随机邻域嵌入（t-SNE）和均匀流形逼近和投影（UMAP）的选定降维方法进一步可视化和比较结果。这个框架展示的聚类和关联可以帮助研究人员和医疗保健专业人员了解COVID-19病毒的流行病学和进化动态。

    Due to the high mutation rate of the virus, the COVID-19 pandemic evolved rapidly. Certain variants of the virus, such as Delta and Omicron, emerged with altered viral properties leading to severe transmission and death rates. These variants burdened the medical systems worldwide with a major impact to travel, productivity, and the world economy. Unsupervised machine learning methods have the ability to compress, characterize, and visualize unlabelled data. This paper presents a framework that utilizes unsupervised machine learning methods to discriminate and visualize the associations between major COVID-19 variants based on their genome sequences. These methods comprise a combination of selected dimensionality reduction and clustering techniques. The framework processes the RNA sequences by performing a k-mer analysis on the data and further visualises and compares the results using selected dimensionality reduction methods that include principal component analysis (PCA), t-distribut
    
[^206]: 学习物理现象的热力学。 (arXiv:2207.12749v3 [cs.LG] UPDATED)

    Thermodynamics of learning physical phenomena. (arXiv:2207.12749v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12749](http://arxiv.org/abs/2207.12749)

    热力学作为归纳偏差在机器学习中的应用潜力已经被广泛意识到，文章回顾了热力学在学习过程中的应用，并研究不同方面对学习过程的影响。

    

    热力学可以被视为高认知级别下的物理表达。因此，最近在许多领域中已经意识到其作为归纳偏差的潜力，以帮助机器学习过程达成准确和可信的预测。我们回顾热力学如何在学习过程中提供有用的洞见。同时，我们研究不同方面对学习过程的影响，例如描述给定现象的尺度、为该描述选择相关变量或可用于学习过程的不同技术。

    Thermodynamics could be seen as an expression of physics at a high epistemic level. As such, its potential as an inductive bias to help machine learning procedures attain accurate and credible predictions has been recently realized in many fields. We review how thermodynamics provides helpful insights in the learning process. At the same time, we study the influence of aspects such as the scale at which a given phenomenon is to be described, the choice of relevant variables for this description or the different techniques available for the learning process.
    
[^207]: 每个人的偏好变化不同：加权多兴趣检索模型

    Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model. (arXiv:2207.06652v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.06652](http://arxiv.org/abs/2207.06652)

    本文提出了一种新的加权多兴趣检索模型（Multi-Interest Preference，MIP），通过利用深度学习方法为用户建立多个兴趣嵌入，并将用户在多个兴趣上的偏好进行建模，从而提高候选检索结果的查全率。

    

    用户嵌入（用户的向量化表示）在推荐系统中具有重要作用。已经提出了许多方法来构建用户的多维度表示，以便于检索任务中找到相似的物品，并且已经在工业推荐系统中被证明是有效的。最近人们发现使用多种嵌入（即多个维度的用户表示）来表示用户的兴趣是有用的，每个嵌入表示用户的某个主题兴趣。对于多兴趣表示，重要的是对用户在不同主题上的偏好进行建模，并且了解偏好随时间的变化情况。然而，现有方法要么无法估算用户对每个兴趣的好感度，要么不合理地假设每个用户对每个兴趣的兴趣强度会以相等的速率下降，从而降低了候选检索结果的查全率。在本文中，我们提出加权多兴趣检索模型（Multi-Interest Preference, MIP），通过利用深度学习方法为用户产生多个兴趣嵌入，并且可以对用户在多种兴趣下的偏好进行估计，从而提高候选检索结果的查全率。

    User embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems as well. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user's interest in a certain topic. With multi-interest representation, it's important to model the user's preference over the different topics and how the preference change with time. However, existing approaches either fail to estimate the user's affinity to each interest or unreasonably assume every interest of every user fades with an equal rate with time, thus hurting the recall of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by usin
    
[^208]: 利用弱监督检测变形器扩展新型物品检测

    Scaling Novel Object Detection with Weakly Supervised Detection Transformers. (arXiv:2207.05205v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.05205](http://arxiv.org/abs/2207.05205)

    本篇文章提出了一个新方法：弱监督检测变形器（Weakly Supervised Detection Transformer），可以有效地将大规模预训练数据集的知识转移至数百种新型物品的WSOD微调中，同时提高了多实例学习的准确性。实验结果表明，该方法优于现有的先进模型，在大规模新型物品检测数据集上达到了更好的性能。

    

    目标检测中一个关键任务是微调现有模型以便检测新型物品，但标注边界框需要耗费大量的时间和金钱。弱监督目标检测可以通过使用图像级别标签来训练物品检测器，它提供了一种吸引人的替代方法。然而，当前弱监督模型的实际应用受到限制，因为它们仅适用于小规模数据，并需要多次训练和改进。为了解决这个问题，我们提出了弱监督检测变形器，它能够将大规模预训练数据集的知识有效地转移至数百种新型物品的WSOD微调中。此外，我们还利用预训练知识来改进在WSOD方法中常用的多实例学习（MIL）框架。实验证明，我们的方法在大规模新型物品检测数据集上优于先前的最先进模型，并且我们的扩展研究…

    A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study r
    
[^209]: 基于改进的CGAN框架的新条件下滚动轴承故障样本合成

    Synthesizing Rolling Bearing Fault Samples in New Conditions: A framework based on a modified CGAN. (arXiv:2206.12076v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12076](http://arxiv.org/abs/2206.12076)

    提出了一种基于CGAN的新算法，它可以从新条件下的正常数据中生成对应的滚动轴承故障数据，可用于开发数据驱动的故障诊断工具，提高检测故障类型的准确度。

    

    轴承是旋转机械的重要组件之一，容易出现意外故障。因此，轴承故障诊断和状态监测对于减少多个行业的运营成本和停机时间至关重要。在各种生产条件下，轴承可以在不同的负载和转速下运行，这会导致与每种故障类型相关的不同振动模式。正常数据很充足，因为系统通常在预期的条件下工作。另一方面，故障数据很少，并且在许多情况下，没有记录故障类别的数据。获取故障数据对于开发数据驱动的故障诊断工具至关重要，可提高操作的性能和安全性。因此，引入了一种基于有条件生成对抗网络（CGANs）的新算法。该算法在任何实际故障条件下使用正常数据和故障数据进行训练，可以从目标条件的正常数据中生成对应的故障数据。该方法包括两个阶段：特征提取阶段和故障合成阶段。首先，从源条件的正常数据中提取故障特征，然后利用这些特征使用改进的CGAN生成与目标条件相应的故障数据。实验在滚动轴承的模拟数据集和实验数据集上进行。结果表明，该方法能够成功地在新条件下生成滚动轴承的故障样本，并在检测故障类型方面提供高准确度。

    Bearings are one of the vital components of rotating machines that are prone to unexpected faults. Therefore, bearing fault diagnosis and condition monitoring is essential for reducing operational costs and downtime in numerous industries. In various production conditions, bearings can be operated under a range of loads and speeds, which causes different vibration patterns associated with each fault type. Normal data is ample as systems usually work in desired conditions. On the other hand, fault data is rare, and in many conditions, there is no data recorded for the fault classes. Accessing fault data is crucial for developing data-driven fault diagnosis tools that can improve both the performance and safety of operations. To this end, a novel algorithm based on Conditional Generative Adversarial Networks (CGANs) is introduced. Trained on the normal and fault data on any actual fault conditions, this algorithm generates fault data from normal data of target conditions. The proposed me
    
[^210]: 可解释的专家混合模型

    Interpretable Mixture of Experts. (arXiv:2206.02107v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02107](http://arxiv.org/abs/2206.02107)

    介绍了一种新的可解释建模框架——可解释的专家混合模型（IME），能够产生与“黑盒”深度神经网络相媲美的高精度，并在提供有用解释的同时，还具有解释黑盒模型预测的能力。

    

    在许多机器学习应用中，特别是在表格和时间序列数据中，需要可靠的模型解释，因为它们的使用案例经常涉及高风险的决策制定。为了实现这一目标，我们引入了一种新的可解释建模框架——可解释的专家混合模型（IME），它在很多情况下可以产生与“黑盒”深度神经网络（DNN）相媲美的高精度，并具有有用的可解释性能力。IME包括一个分配模块和一个专家混合模型，每个样本被分配给一个单一的专家进行预测。我们为IME引入多个选项，基于可解释的分配和专家的选择。当专家被选择为可解释的线性模型时，IME产生了一种内在可解释的体系结构，IME产生的解释是预测计算的精确描述。除了构成独立的内在可解释的体系结构外，IME还具有提供预训练的黑盒模型预测后事实解释的能力，从而增加了DNNs的可解释性。我们的实验表明，IME在表格和时间序列预测任务上实现了有竞争力的性能，并提供了有用的专家和分配解释。

    The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has th
    
[^211]: B2T 连接：服务于深度 Transformer 的稳定性和性能

    B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00330](http://arxiv.org/abs/2206.00330)

    本研究提出了一种称为 B2T 连接的方法，连接了 Pre-LN 和 Post-LN 层的输出，为深度 Transformer 提供了高稳定性和有效的训练，实验结果表明，在多个基准数据集上取得了最先进结果。

    

    从层归一化（LN）的角度来看，Transformer 的架构可以分为两种类型：Post-LN 和 Pre-LN。最近的 Transformers 倾向于采用 Pre-LN，因为在 Post-LN 中，特别是在深度 Transformers 中（例如有十个或更多层的模型），训练经常不稳定，导致得到无用的模型。然而，与 Pre-LN 相比，在相对较浅的 Transformers（例如有六个或更少的层）中，Post-LN 一直取得了更好的性能。本研究首先从经验和理论上研究了这些不一致的观察结果，并发现了以下发现：1）Post-LN 中的 LN 是导致不稳定训练的梯度消失问题的主要原因，而 Pre-LN 可以避免这种问题；2）Post-LN 往往会在反向传播的高层保留更大的梯度范数，这可能导致有效的训练。利用这些新发现，我们提出了一种可以同时提供高稳定性和有效的 Transformer 训练的方法，称为 B2T Connection，它连接了 Pre-LN 和 Post-LN 层的输出。我们的实验结果表明，B2T Connection 可以显著提高深度 Transformers（有十个或更多层）的性能，实现了在多个基准数据集上的最先进结果。

    From the perspective of the layer normalization (LN) positions, the architectures of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep Transformers (e.g., those with ten or more layers), the training is often unstable, resulting in useless models. However, Post-LN has consistently achieved better performance than Pre-LN in relatively shallow Transformers (e.g., those with six or fewer layers). This study first investigates the reason for these discrepant observations empirically and theoretically and made the following discoveries: 1, the LN in Post-LN is the main source of the vanishing gradient problem that leads to unstable training, whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation, which may lead to effective training. Exploiting the new findings, we propose a method that can provide both high stability and effective tr
    
[^212]: 废物减量的仿真环境与强化学习方法

    A Simulation Environment and Reinforcement Learning Method for Waste Reduction. (arXiv:2205.15455v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15455](http://arxiv.org/abs/2205.15455)

    本文提出了一个新的强化学习任务，即库存重新填充，提出了一种新的强化学习算法 Perishable DQN，旨在最大化销售同时最小化浪费，实现计算机思维对废物减量的控制。

    

    在零售行业（例如，杂货店、服装店、在线零售商），库存管理者必须在短期风险（没有销售商品）和长期风险（过量订货导致产品浪费）之间进行平衡。由于缺乏有关未来客户购买的信息，这个平衡任务尤其困难。在本文中，我们从分布的角度研究了在时间范围内重新填充易腐物品的杂货店库存问题。其目标是在不确定实际消费的情况下，最大化销售同时最小化浪费。由于对食物的需求不断增长以及食物浪费对环境、经济和购买力的影响，这个问题在今天具有极高的相关性。我们将库存重新填充作为一项新的强化学习任务，并在此基础上提出了一种名为“腐败DQN”的新的强化学习算法，利用问题的分布式公式来估算每个动作的预期回报。我们在一个合成的杂货店数据集上验证了我们的方法，并证明了我们的方法优于文献中的几个基准线。

    In retail (e.g., grocery stores, apparel shops, online retailers), inventory managers have to balance short-term risk (no items to sell) with long-term-risk (over ordering leading to product waste). This balancing task is made especially hard due to the lack of information about future customer purchases. In this paper, we study the problem of restocking a grocery store's inventory with perishable items over time, from a distributional point of view. The objective is to maximize sales while minimizing waste, with uncertainty about the actual consumption by costumers. This problem is of a high relevance today, given the growing demand for food and the impact of food waste on the environment, the economy, and purchasing power. We frame inventory restocking as a new reinforcement learning task that exhibits stochastic behavior conditioned on the agent's actions, making the environment partially observable. We make two main contributions. First, we introduce a new reinforcement learning en
    
[^213]: 对抗性多类分类问题的多重边际最优输运公式

    The Multimarginal Optimal Transport Formulation of Adversarial Multiclass Classification. (arXiv:2204.12676v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.12676](http://arxiv.org/abs/2204.12676)

    本文研究了一类对抗性多类分类问题，提供了等价的广义几何重心问题和多重边际最优输运问题的重述，揭示了其丰富的几何结构，扩展了之前仅限于二分类设置的相关结果。通过本文提出的方法，可以恢复原始对抗性问题的最优稳健分类规则和最优对抗策略。

    

    我们研究了一类对抗性多类分类问题，并提供等价重述。其中一种是基于本文引入的一组广义几何重心问题；另一种是使用多重边际最优输运问题，其中边际的数量等于原始分类问题中的类数。这些新的理论结果揭示了多类分类中对抗性学习问题的丰富几何结构，并扩展了之前仅限于二分类设置的相关结果。我们的结果的一个直接计算含义是，通过解决重心问题及其对偶，或者是 MOT 问题及其对偶，我们可以恢复原始对抗性问题的最优稳健分类规则和最优对抗策略。我们使用合成和真实数据的示例来说明我们的结果。

    We study a family of adversarial multiclass classification problems and provide equivalent reformulations in terms of: 1) a family of generalized barycenter problems introduced in the paper and 2) a family of multimarginal optimal transport problems where the number of marginals is equal to the number of classes in the original classification problem. These new theoretical results reveal a rich geometric structure of adversarial learning problems in multiclass classification and extend recent results restricted to the binary classification setting. A direct computational implication of our results is that by solving either the barycenter problem and its dual, or the MOT problem and its dual, we can recover the optimal robust classification rule and the optimal adversarial strategy for the original adversarial problem. Examples with synthetic and real data illustrate our results.
    
[^214]: EVOTER：透明可解释规则集的进化

    EVOTER: Evolution of Transparent Explainable Rule-sets. (arXiv:2204.10438v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.10438](http://arxiv.org/abs/2204.10438)

    EVOTER使用简单的逻辑表达式演化出透明可解释的规则集，与黑盒模型性能相似，可以揭示数据中的偏见并为未来构建可靠的AI系统提供基础。

    

    大多数AI系统是黑盒子，为给定的输入生成合理的输出。然而，某些领域具有解释能力和信任度要求，这些要求不能直接满足这些方法。因此，该论文提出了一种替代方法，即开始时模型就是透明的和可解释的。该方法使用简单的逻辑表达式演化出规则集，称为EVOTER。EVOTER在多个预测/分类和处方/政策搜索领域进行了评估，有和没有代理。结果显示，它能够发现和黑盒模型相似的有意义的规则集。这些规则可以提供领域的见解，并使数据中隐藏的偏见显性化。也可以直接对它们进行编辑，以消除偏见并添加约束。因此，EVOTER为未来构建值得信赖的AI系统的可靠基础。

    Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This paper advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on simple logical expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule sets that perform similarly to black-box models. The rules can provide insight into the domain, and make biases hidden in the data explicit. It may also be possible to edit them directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.
    
[^215]: 超越梯度下降的梯度修正方法

    Gradient Correction beyond Gradient Descent. (arXiv:2203.08345v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.08345](http://arxiv.org/abs/2203.08345)

    本论文提出了一种名为GCGD的梯度修正框架，可以有效提高梯度质量，从而将训练轮数减少约20％。

    

    神经网络的成功与梯度下降算法的应用密不可分。基于梯度下降，已经出现了许多变种算法来改善梯度下降的优化过程。反向传播的梯度显然是神经网络训练过程中最重要的一环，然而计算梯度的质量受多种因素影响，如噪声数据、计算误差、算法限制等。为了揭示超越梯度下降的梯度信息，我们引入了一个名为GCGD的框架进行梯度修正，它由两个插件模块组成：1）受梯度预测思想的启发，我们提出了一个名为GC-W的权重梯度修正模块；2）基于神经ODE，我们提出了一个名为GC-ODE的隐藏状态梯度修正模块。实验结果表明，我们的梯度修正框架能够有效地提高梯度质量，并将训练轮数减少约20％。

    The great success neural networks have achieved is inseparable from the application of gradient-descent (GD) algorithms. Based on GD, many variant algorithms have emerged to improve the GD optimization process. The gradient for back-propagation is apparently the most crucial aspect for the training of a neural network. The quality of the calculated gradient can be affected by multiple aspects, e.g., noisy data, calculation error, algorithm limitation, and so on. To reveal gradient information beyond gradient descent, we introduce a framework (\textbf{GCGD}) to perform gradient correction. GCGD consists of two plug-in modules: 1) inspired by the idea of gradient prediction, we propose a \textbf{GC-W} module for weight gradient correction; 2) based on Neural ODE, we propose a \textbf{GC-ODE} module for hidden states gradient correction. Experiment results show that our gradient correction framework can effectively improve the gradient quality to reduce training epochs by $\sim$ 20\% and 
    
[^216]: 论分布贝尔曼方程的解

    On solutions of the distributional Bellman equation. (arXiv:2202.00081v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.00081](http://arxiv.org/abs/2202.00081)

    本文研究了分布贝尔曼方程的一般条件，包括解的存在唯一性和回报分布的尾部性质。将分布贝尔曼方程与多元仿射分布方程联系起来，发现任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这一理论适用于分布强化学习领域。

    

    在分布强化学习中，不仅要考虑预期回报，还要考虑策略的完整回报分布。对于固定的策略，其回报分布是相应分布贝尔曼方程的解。本文考虑一般的分布贝尔曼方程，研究解的存在性和唯一性以及回报分布的尾部性质。我们给出了存在和唯一性回报分布的必要和充分条件，并确定了正则变化的情况。我们将分布贝尔曼方程与多元仿射分布方程联系起来。我们表明，在多元仿射分布方程的解的条件下，任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这使得这种方程的一般理论适用于分布强化学习设置。

    In distributional reinforcement learning not only expected returns but the complete return distributions of a policy are taken into account. The return distribution for a fixed policy is given as the solution of an associated distributional Bellman equation. In this note we consider general distributional Bellman equations and study existence and uniqueness of their solutions as well as tail properties of return distributions. We give necessary and sufficient conditions for existence and uniqueness of return distributions and identify cases of regular variation. We link distributional Bellman equations to multivariate affine distributional equations. We show that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This makes the general theory of such equations applicable to the distributional reinforcement learning setting.
    
[^217]: 从代币智能合约代码中检测DeFi证券违规行为

    Detecting DeFi Securities Violations from Token Smart Contract Code. (arXiv:2112.02731v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.02731](http://arxiv.org/abs/2112.02731)

    本研究旨在通过从代币的智能合约代码中提取特征（opcode-based特征）并建立分类器来识别DeFi项目中可能存在的证券违规活动。最终模型是一个随机森林模型，对基线进行了80％的F-1得分。

    

    去中心化金融（DeFi）是通过智能合约在各种区块链上构建和提供的金融产品和服务。在过去的一年中，DeFi已经成为了热门话题并获得了市场资本化。然而，它也与犯罪有关，特别是各种类型的证券违规行为。DeFi缺乏了解您的客户要求，这给试图减轻此领域潜在犯罪的政府带来了挑战。本研究旨在揭示这个问题是否适合机器学习方法，即是否可以根据代币的智能合约代码识别潜在从事证券违规活动的DeFi项目。我们在以太坊上适应了以前检测特定类型证券违规行为的工作，根据从DeFi项目的代币智能合约代码（特别是基于opcode的特征）提取的特征构建分类器。我们的最终模型是一个随机森林模型，对基线进行了80％的F-1得分。

    Decentralized Finance (DeFi) is a system of financial products and services built and delivered through smart contracts on various blockchains. In the past year, DeFi has gained popularity and market capitalization. However, it has also been connected to crime, in particular, various types of securities violations. The lack of Know Your Customer requirements in DeFi poses challenges to governments trying to mitigate potential offending in this space. This study aims to uncover whether this problem is suited to a machine learning approach, namely, whether we can identify DeFi projects potentially engaging in securities violations based on their tokens' smart contract code. We adapt prior work on detecting specific types of securities violations across Ethereum, building classifiers based on features extracted from DeFi projects' tokens' smart contract code (specifically, opcode-based features). Our final model is a random forest model that achieves an 80\% F-1 score against a baseline o
    
[^218]: 将不同的模型分发给不同的用户可减轻对抗性攻击

    Mitigating Adversarial Attacks by Distributing Different Copies to Different Users. (arXiv:2111.15160v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2111.15160](http://arxiv.org/abs/2111.15160)

    将不同的模型副本分发给不同的用户，可以降低恶意用户对其他用户的攻击风险。模型使用不同的随机性训练可以减轻复制攻击，但是重训练代价高且结果不稳定。一些方法扩展了重训练以增强模型差异，但是计算成本更高。

    

    机器学习模型容易受到对抗性攻击。本文考虑将模型分发给多个用户的情况，在这其中，恶意用户试图攻击其他用户。恶意用户会探测其拥有的模型以寻找对抗样本，然后将发现的样本呈现给受害者模型的副本以复制攻击。我们指出，通过将不同的模型分发给不同的买家，我们可以减轻攻击，这样在一个模型上找到的对抗样本将不起作用于另一个模型。我们观察到，使用不同的随机性训练模型确实在一定程度上减轻了这种复制。然而，并没有保证，重新训练的成本也很高。一些作品扩展了重新培训方法以增强模型之间的差异。然而，这种方法只能产生非常有限的模型，并且计算成本变得更高了。

    Machine learning models are vulnerable to adversarial attacks. In this paper, we consider the scenario where a model is distributed to multiple buyers, among which a malicious buyer attempts to attack another buyer. The malicious buyer probes its copy of the model to search for adversarial samples and then presents the found samples to the victim's copy of the model in order to replicate the attack. We point out that by distributing different copies of the model to different buyers, we can mitigate the attack such that adversarial samples found on one copy would not work on another copy. We observed that training a model with different randomness indeed mitigates such replication to a certain degree. However, there is no guarantee and retraining is computationally expensive. A number of works extended the retraining method to enhance the differences among models. However, a very limited number of models can be produced using such methods and the computational cost becomes even higher. 
    
[^219]: 新不一定总是更好：重新思考迁移度量，它们的特殊性、稳定性和性能。

    Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance. (arXiv:2110.06893v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06893](http://arxiv.org/abs/2110.06893)

    本文改进了常见的度量标准H-score，并提出了一个基于收缩估计的解决方案，使H分数的相关性性能获得高达80%的绝对增益，使其与最先进的LogME度量标准具有竞争力。同时，针对目标任务的选择，本文也发现了一个被忽视的问题。

    

    对于改善预测和高效利用有限资源，对大型预训练图像和语言模型在小型自定义数据集上进行微调已变得越来越流行。微调需要确定最佳模型进行迁移学习，并量化可转移性以避免在所有候选模型/任务对上进行昂贵的重新训练。在本文中，我们展示了协方差估计的统计问题导致了H分数 (H-score) 的性能不佳——该分数是新型度量标准的常见基准——并提出基于收缩估计的解决方案。这样可以使H分数的相关性性能获得高达80%的绝对增益，使其与最先进的LogME度量标准具有竞争力。我们的基于收缩估计的H分数比LogME更快3倍到10倍。此外，我们研究了目标（而不是源）任务选择的较少常见的设置。我们展示了在具有不同标签数量、类别不平衡的情况下，此类设置中以前被忽视的问题。

    Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\times$-10$\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalanc
    
[^220]: 用简洁可解释的加性模型和结构交互预测人口普查调查反应率

    Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions. (arXiv:2108.11328v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.11328](http://arxiv.org/abs/2108.11328)

    本文提出了一种可解释的非参数加性模型，使用少量主要和成对交互效应预测调查反应率。该模型可以生成易于可视化和解释的预测面，并取得了 ROAM 数据集上的最先进性能，可以提供改进美国人口普查局和其他调查的反应率议论。

    

    本文考虑使用一系列灵活且可解释的非参数模型预测调查反应率。本研究受到美国人口普查局著名的 ROAM 应用的启发，该应用使用在美国人口普查规划数据库数据上训练的线性回归模型来识别难以调查的区域。十年前组织的一场众包竞赛表明，基于回归树集成的机器学习方法在预测调查反应率方面表现最佳；然而，由于它们的黑盒特性，相应的模型不能用于拟定的应用。我们考虑使用 $\ell_0$-based 惩罚的非参数加性模型，它具有少数主要和成对交互效应。从方法论的角度来看，我们研究了我们估计器的计算和统计方面，并讨论了将强层次交互合并的变体。我们的算法（在Github 上开源）允许我们生成易于可视化和解释的预测面，从而获得有关调查反应率的可行见解。我们提出的模型在 ROAM 数据集上实现了最先进的性能，并可以提供有关美国人口普查局和其他调查的改进调查反应率的见解。

    In this paper we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition organized around ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study both computational and statistical aspects of our estimator; and discuss variants that incorporate strong hierarchical interactions. Our algorithms (opensourced on gith
    
[^221]: 物理引导下高度非线性参数偏微分方程的发现

    Physics-Guided Discovery of Highly Nonlinear Parametric Partial Differential Equations. (arXiv:2106.01078v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.01078](http://arxiv.org/abs/2106.01078)

    该论文提出了一种新的物理引导学习方法，能够从数据中发现高度非线性参数偏微分方程，同时具有更好的鲁棒性和准确性。

    

    能够拟合科学数据的偏微分方程（PDE）可以代表多种以数学为导向的学科，例如物理和金融的可解释机制的物理定律。从科学数据中数据驱动地发现偏微分方程是一种模拟自然界中复杂现象的新尝试，但目前的实践效果通常受到数据稀缺性和现象复杂性的限制。特别地，从低质量数据中发现高度非线性系数的PDE仍然面临着相对不足的挑战。为了解决这一难题，我们提出了一种新的物理引导学习方法，既可以编码初值和边界条件等观察知识，也可以纳入基本的物理原理和定律来指导模型优化。我们理论上证明了我们提出的方法严格减小了现有基线系数估计误差，并且还抗噪音鲁棒性高。大量实验证明了该方法在不同数据集和度数系数下的准确性和实用性。

    Partial differential equations (PDEs) that fit scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects, such as physics and finance. The data-driven discovery of PDEs from scientific data thrives as a new attempt to model complex phenomena in nature, but the effectiveness of current practice is typically limited by the scarcity of data and the complexity of phenomena. Especially, the discovery of PDEs with highly nonlinear coefficients from low-quality data remains largely under-addressed. To deal with this challenge, we propose a novel physics-guided learning method, which can not only encode observation knowledge such as initial and boundary conditions but also incorporate the basic physical principles and laws to guide the model optimization. We theoretically show that our proposed method strictly reduces the coefficient estimation error of existing baselines, and is also robust against noise. Extensive experiments show th
    
[^222]: 智能家居系统的可解释活动识别

    Explainable Activity Recognition for Smart Home Systems. (arXiv:2105.09787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.09787](http://arxiv.org/abs/2105.09787)

    本论文提出了一种可解释的活动识别框架，可以生成自然语言解释来解释分类的原因，以提高智能家居系统的可信度。

    

    智能家居环境旨在通过安装在整个空间中的各种传感器和执行器提供帮助改善居民生活质量的服务。智能家居采取的许多自动化操作都是由基础活动识别系统的输出控制的。然而，活动识别系统可能并不完全准确，因此智能家居操作的不一致性可能会导致依赖智能家居预测的用户想知道“为什么智能家居要那样做？” 在这项工作中，我们在可解释人工智能（XAI）技术的见解基础上构建了一个可解释的活动识别框架，利用领先的XAI方法生成自然语言解释，解释活动中什么导致了给定的分类。在远程照护监测的背景下，我们进行了两步评估：（a）利用ML专家评估解释的合理性，（b）招募非专业人员

    Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate and therefore inconsistencies in smart home operations can lead users reliant on smart home predictions to wonder "why did the smart home do that?" In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques and introduce an explainable activity recognition framework in which we leverage leading XAI methods to generate natural language explanations that explain what about an activity led to the given classification. Within the context of remote caregiver monitoring, we perform a two-step evaluation: (a) utilize ML experts to assess the sensibility of explanations, and (b) recruit non-experts
    
[^223]: 多任务注意力残差网络用于论述挖掘

    Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2102.12227](http://arxiv.org/abs/2102.12227)

    本文提出了一种多任务注意力残差网络架构，通过利用集成方法、注意力机制和多任务学习，无需假设文档或论据结构，成功应用于多个论述挖掘任务中，成为了一种既通用又高性能的架构。

    

    本文探讨了多任务注意力残差网络在多个论述挖掘任务中的应用。我们提出了一种残差架构，利用了注意力、多任务学习，并使用集成方法，不对文档或论据结构做任何假设。我们在五个不同的用户生成评论、科学出版物和劝说性论文语料库上进行了广泛的实验评估。我们的结果表明，我们的方法是针对具有更高计算印记或特定于语料库设计的最先进架构的强有力的竞争对手，代表了通用性、性能精度和减少模型大小之间的有趣折衷。

    We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
    
[^224]: 高斯过程视角下神经网络的双峰下降曲线研究

    Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.07238](http://arxiv.org/abs/2102.07238)

    本文利用随机矩阵理论和高斯过程技术解释了神经网络双峰下降现象，建立了NNGP和随机矩阵理论之间的新联系，揭示该现象受到经验核和NNGP核之间差异的影响。

    

    神经网络中的双峰下降曲线现象描述了当增加参数时，泛化误差起初下降，但在达到一个小于数据点数量的最优参数后增加，然后在过参数化区间再次下降。在本文中，我们使用随机矩阵理论技术来表征经验特征协方差矩阵的谱分布，作为神经网络高斯过程（NNGP）核谱的宽度相关扰动，从而在神经网络领域建立了NNGP文献与随机矩阵理论文献之间的新联系。我们的分析表达式允许我们研究相应核和GP回归的泛化行为，并为双峰下降的现象提供了一个新的解释，即由宽度相关的经验核与宽度无关的NNGP核之间的差异所决定。

    Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. In this paper, we use techniques from random matrix theory to characterize the spectral distribution of the empirical feature covariance matrix as a width-dependent perturbation of the spectrum of the neural network Gaussian process (NNGP) kernel, thus establishing a novel connection between the NNGP literature and the random matrix theory literature in the context of neural networks. Our analytical expression allows us to study the generalisation behavior of the corresponding kernel and GP regression, and provides a new interpretation of the double-descent phenomenon, namely as governed by the discrepancy between the width-dependent empirical kernel and the width-independent NNGP kernel.
    
[^225]: 三维点云分割的交叉形状注意力

    Cross-Shape Attention for Part Segmentation of 3D Point Clouds. (arXiv:2003.09053v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2003.09053](http://arxiv.org/abs/2003.09053)

    本文提出了一种新方法，在形状集合中通过交叉形状注意力机制来实现三维点云分割，通过评估点之间的交互程度和介导特征传播来提升结果精度和一致性。

    

    我们提出了一种深度学习方法，通过在形状集合中传递逐点特征表示来进行三维形状分割。我们提出了一种交叉形状注意力机制，以使一个形状的逐点特征与其他形状的逐点特征产生相互作用。该机制评估点之间的交互程度并在形状之间介导特征传播，从而改善了用于形状分割的结果的点逐点特征表示的精度和一致性。我们的方法还提出了一种形状检索度量，以选择适合每个测试形状的交叉形状注意力操作的形状。我们的实验证明，我们的方法在流行的PartNet数据集上实现了最先进的结果。

    We present a deep learning method that propagates point-wise feature representations across shapes within a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses both the degree of interaction between points and also mediates feature propagation across shapes, improving the accuracy and consistency of the resulting point-wise feature representations for shape segmentation. Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art results in the popular PartNet dataset.
    

