# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing.](http://arxiv.org/abs/2310.12153) | 该论文研究了使用绝热量子计算的平衡K-Means聚类的概率采样方法，通过利用非最优解来计算校准后验概率，实现在D-Wave AQC上识别模糊解决方案和数据点的目标。 |
| [^2] | [Fairer and More Accurate Tabular Models Through NAS.](http://arxiv.org/abs/2310.12145) | 通过使用多目标神经架构搜索(NAS)和超参数优化(HPO)，我们在表格数据领域首次提出了一种更新模型架构和超参数的策略，以寻找更公平和准确的模型。我们发现，仅针对准确性进行优化可能会导致公平性的降低，因此需要同时考虑准确性和公平性。 |
| [^3] | [Dynamic financial processes identification using sparse regressive reservoir computers.](http://arxiv.org/abs/2310.12144) | 本文介绍了使用稀疏回归储层计算机来识别动态金融过程的方法。通过结构化矩阵逼近和稀疏最小二乘方法确定输出耦合矩阵的近似表示，并利用这些表示建立对应于给定金融系统中递归结构的回归模型。通过应用于动态金融和经济过程的近似识别和预测模拟，展示了算法的有效性。 |
| [^4] | [Simple Mechanisms for Representing, Indexing and Manipulating Concepts.](http://arxiv.org/abs/2310.12143) | 通过查看概念的矩阵统计量，生成一个概念的具体表示或签名，可以用于发现概念之间的结构并递归产生更高级的概念，同时可以通过概念的签名来找到相关的共同主题。 |
| [^5] | [DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning.](http://arxiv.org/abs/2310.12128) | DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。 |
| [^6] | [A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation.](http://arxiv.org/abs/2310.12127) | 本研究通过调查机器翻译模型中的性别偏见问题以及缓解性别偏见的方法来填补现有研究的空白。研究发现指导微调模型在默认为男性翻译上存在性别偏见，同时忽视了指示职业性别的代词，并提出了一些可行的缓解策略。 |
| [^7] | [SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks.](http://arxiv.org/abs/2310.12126) | SHARCS是一种高效的Transformer模型，通过动态宽度子网络进行路由，实现自适应推理和更高的效率，同时在各种分类任务中表现优越并且具有通用性。 |
| [^8] | [Automatic prediction of mortality in patients with mental illness using electronic health records.](http://arxiv.org/abs/2310.12121) | 本文研究了使用电子健康记录预测患有精神疾病患者的死亡率。研究发现随机森林和支持向量机模型在预测中表现优异，药物处方尤其是硫酸吗啡对预测起到关键作用。 |
| [^9] | [MMD-based Variable Importance for Distributional Random Forest.](http://arxiv.org/abs/2310.12115) | 本文介绍了基于MMD距离和经典的drop and relearn原理的变量重要性算法，可以在分布随机森林中检测影响输出分布的变量，并且在实证性能上超越了竞争对手。 |
| [^10] | [A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses.](http://arxiv.org/abs/2310.12112) | 本文研究了实证隐私防御中参考数据的作用和隐私问题，提出了一种基准防御方法，实现了模型效用和训练数据隐私的权衡。 |
| [^11] | [Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture.](http://arxiv.org/abs/2310.12109) | Monarch Mixer (M2) is a new architecture that uses sub-quadratic primitive to scale along both sequence length and model dimension, achieving high hardware efficiency and matching the performance of existing models with fewer parameters. |
| [^12] | [An Online Learning Theory of Brokerage.](http://arxiv.org/abs/2310.12107) | 该论文通过在线学习的理论研究了交易者间的经纪交易问题，提出了适用于不同信息披露情况下的算法，并证明了这些算法的最优性。 |
| [^13] | [Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling.](http://arxiv.org/abs/2310.12100) | 这篇论文介绍了一种非侵入式的参数高效微调技术（AdaLink），通过只调整模型的外部参数而保持内部结构不变，实现了对多模态建模的竞争性能，这对于大规模语言模型和视觉语言模型的自适应和部署具有重要意义。 |
| [^14] | [On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields.](http://arxiv.org/abs/2310.12095) | 本文旨在通过对深度学习降阶模型的理论分析，扩展对由随机场参数化的PDE的理解。 |
| [^15] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^16] | [Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait.](http://arxiv.org/abs/2310.12083) | 本研究通过蒙特卡洛敏感性分析确定了代谢成本估算的关键参数和输入变量。通过分析功率相关参数的敏感性和使用不同输入特征训练的神经网络的准确性，揭示了代谢能量模型的贡献成分。 |
| [^17] | [Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks.](http://arxiv.org/abs/2310.12079) | 最近的研究发现，对于具有形状激活函数的神经网络，其缩放极限可以由微分方程描述。然而，关于未经形状处理的神经网络的信息尚不明确。本文研究了两种未经形状处理的网络，发现它们也可以由类似的微分方程来描述，并给出了它们的一些特征。 |
| [^18] | [One-Shot Imitation Learning: A Pose Estimation Perspective.](http://arxiv.org/abs/2310.12077) | 本文研究了一种具有挑战性的模仿学习方法，只有一次演示、不进行数据收集和没有先前的任务或对象知识的情况下，将模仿学习形式化为轨迹传输和未知对象姿态估计的组合。通过对真实任务进行深入研究，探讨了最先进的未知对象姿态估计器在一次性模仿学习中的表现，并深入分析了相机校准、姿态估计误差和空间泛化对任务成功率的影响。 |
| [^19] | [Transformers for scientific data: a pedagogical review for astronomers.](http://arxiv.org/abs/2310.12069) | 本综述旨在向科学家介绍Transformer的应用，包括自然语言处理和自注意机制。此外，还介绍了在天文学中应用于时间序列和成像数据的具体情况，并提供了常见问题解答部分。 |
| [^20] | [Black-Box Training Data Identification in GANs via Detector Networks.](http://arxiv.org/abs/2310.12063) | 本研究探索了在黑盒设置中使用GAN时的隐私问题，通过引入一套攻击来识别训练数据成员身份，本文提供了对版权和数据隐私方面的重要洞见。 |
| [^21] | [Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning.](http://arxiv.org/abs/2310.12055) | 本文利用最优传输理论和几何表示方法来解决反强化学习中的奖励不确定性问题，在高维情况下具有鲁棒性，并通过量化奖励不确定性和识别奖励函数的中心表示提供了一种结构化方法。 |
| [^22] | [Machine Learning-based Nutrient Application's Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach.](http://arxiv.org/abs/2310.12052) | 本文研究了基于机器学习的智能农业营养应用时间推荐方法。通过预测整个季节所需的肥料数量，并根据天气条件和土壤特性调整肥料量，以促进经济高效和环境友好的农业。研究还探讨了施肥应用与天气数据对作物产量的影响。 |
| [^23] | [Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems.](http://arxiv.org/abs/2310.12046) | 本文探讨了在贝叶斯逆问题中，使用机器学习为基础的代理模型，以提高计算效率和处理数值挑战问题。 |
| [^24] | [A General Theoretical Paradigm to Understand Learning from Human Preferences.](http://arxiv.org/abs/2310.12036) | 本文研究了学习从人类偏好中学习的实际算法的理论基础，推导出一个新的一般目标，绕过了两个重要的近似。这种方法允许直接从收集的数据中学习策略而无需奖励模型的训练。 |
| [^25] | [Conformal Drug Property Prediction with Density Estimation under Covariate Shift.](http://arxiv.org/abs/2310.12033) | 提出了一种名为CoDrug的方法，该方法利用基于能量的模型和核密度估计来解决协变量偏移问题，从而实现一致性药物属性预测。 |
| [^26] | [Exact and efficient solutions of the LMC Multitask Gaussian Process model.](http://arxiv.org/abs/2310.12032) | LMC多任务高斯过程模型的精确解决方案表明，只需对噪声模型进行温和假设，即可实现高效计算。通过引入完整参数化的“投影LMC”模型和边缘似然函数表达式，展示了该方法相对于未经处理的方法的优异性能。 |
| [^27] | [SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment.](http://arxiv.org/abs/2310.12031) | SegmATRon是一种自适应变换模型，用于室内图像的语义分割。通过在多张图像上进行推断时权重的自适应调整，可以提高语义分割的质量。在室内环境中使用代理的行为获取额外图像的方法在实验中证明是有效的。 |
| [^28] | [Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design.](http://arxiv.org/abs/2310.12026) | 提出了无参数离散选择实验与机器学习引导的自适应设计方法GBS，通过自适应构建配对比较问题来满足消费者的偏好，不需要参数化效用模型，可以扩展到具有数百个属性的产品，并在准确性和样本效率方面具有优势。 |
| [^29] | [Bayesian Flow Networks in Continual Learning.](http://arxiv.org/abs/2310.12001) | 连续学习中的贝叶斯流网络（BFNs）是一种具有通用生成建模能力的方法，通过结合神经网络和贝叶斯推断，在非稳态数据上取得了良好的实验结果。 |
| [^30] | [Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models.](http://arxiv.org/abs/2310.12000) | 这篇文章介绍了用于潜在高斯过程模型中的Vecchia-Laplace近似法的迭代方法，相比于传统的Cholesky分解方法，可以显著加快计算速度。 |
| [^31] | [Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation.](http://arxiv.org/abs/2310.11991) | 该论文提出了一种通过联合子空间估计从神经网络表示中消除错误概念的迭代算法，并在计算机视觉和自然语言处理的基准数据集上展示了其优越性能。 |
| [^32] | [Image Clustering with External Guidance.](http://arxiv.org/abs/2310.11989) | 这项工作提出了一种新的聚类方法，利用外部知识来引导聚类。通过利用WordNet的文本语义，该方法能够提高图像聚类的效果。 |
| [^33] | [A Finite-Horizon Approach to Active Level Set Estimation.](http://arxiv.org/abs/2310.11985) | 这项研究提出了一种有限时间视角下的主动水平集估计方法，在一维和高维情况下均表现出较高的效果，能够平衡估计误差和已移动距离，具有推广性。 |
| [^34] | [From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers.](http://arxiv.org/abs/2310.11984) | 本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。 |
| [^35] | [Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?.](http://arxiv.org/abs/2310.11978) | 本文研究了分组缩放方法（BVS）的几种改进方法，探索了使用替代损失函数和基于输入特征的分组方案来提高机器学习回归的预测不确定性的一致性和适应性。 |
| [^36] | [Improving Generalization of Alignment with Human Preferences through Group Invariant Learning.](http://arxiv.org/abs/2310.11971) | 该论文提出了一种通过强化学习实现在不同数据组或领域中学习一致策略的方法，该方法可以提高AI助手对不同领域的泛化能力，并更好地与人类偏好对齐。 |
| [^37] | [Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews.](http://arxiv.org/abs/2310.11967) | aTrain是一个开源的离线音频转录工具，支持多语言转录并具有CPU和NVIDIA GPU支持。它适用于研究人员使用定性数据，提供简单易用的界面，并能与常用的定性数据分析软件集成。 |
| [^38] | [Flexible Payload Configuration for Satellites using Machine Learning.](http://arxiv.org/abs/2310.11966) | 本文使用机器学习方法解决了卫星通信中的无线资源管理问题，在处理异构流量场景时，通过将目标和约束集成到机器学习算法中，实现了有效的资源分配，并通过上下文感知的机器学习度量综合考虑了通信系统整体性能。 |
| [^39] | [Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences.](http://arxiv.org/abs/2310.11960) | 提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。 |
| [^40] | [A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis.](http://arxiv.org/abs/2310.11959) | 我们提出了一种名为MSD-Mixer的多尺度分解MLP-Mixer模型，它能够将时间序列分解成不同的组成部分，并在不同的层次中表示这些组成部分。我们还提出了一种新颖的时间拼接方法，以处理多尺度的时间模式和通道间的依赖关系。我们的模型能够比现有方法更好地进行时间序列分析。 |
| [^41] | [Emptying the Ocean with a Spoon: Should We Edit Models?.](http://arxiv.org/abs/2310.11958) | 这项研究质疑了直接模型编辑作为纠正LLM生成中事实错误的方法，并提出了与之类似但更为明确的三种替代方法。虽然模型编辑在提升模型可解释性方面有潜力，但不能被视为解决LLMs固有缺点的系统性方法，因为它存在加强模型可信性观念的风险。 |
| [^42] | [Recasting Continual Learning as Sequence Modeling.](http://arxiv.org/abs/2310.11952) | 本文将连续学习重塑为序列建模问题，并提出了利用序列模型进行连续学习的方法。通过采用元连续学习框架，需要对序列模型进行多次连续学习episode上的元级训练。实验证明序列模型可以成为一种吸引人的通用连续学习解决方案。 |
| [^43] | [Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition.](http://arxiv.org/abs/2310.11950) | 本文发现在人类活动识别中存在性能评估过高的问题，传统方法中的数据分割和交叉验证导致了结果的偏见。这个问题在最新的研究中很常见，但往往被忽视。不正确的结果会导致报告较低准确度的论文更难发表。 |
| [^44] | [Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering.](http://arxiv.org/abs/2310.11940) | 引入了一个可解释的瓶颈，称为滤波器组（FB），用于时间序列聚类的光谱变分自编码器（ISVAE）。通过约束VAE的辨识能力，这个模型能够学习到具有增强可解释性和聚类能力的新编码f_0，并呈现为一个动态的分层树。同时，还提出了与滤波器组结构对称对齐的定制解码器结构，用于处理复杂的数据配置。 |
| [^45] | [A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs.](http://arxiv.org/abs/2310.11917) | 本文提出了一个用于评估知识图谱中半感应式链接预测模型的大规模基准，基于Wikidata5M进行扩展。通过各种不同的任务和信息组合，该基准为进一步研究上下文和文本信息在链接预测中的整合提供了一个测试平台。 |
| [^46] | [Multi-modal Medical Neurological Image Fusion using Wavelet Pooled Edge Preserving Autoencoder.](http://arxiv.org/abs/2310.11910) | 本文提出了一种使用小波池化边缘保留自编码器的无监督多模式医学图像融合模型，用于提高特征提取和信息保留的效果。 |
| [^47] | [Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning.](http://arxiv.org/abs/2310.11897) | 本文介绍了一种名为加速策略梯度的方法，通过应用Nesterov动量在强化学习中实现更快的全局收敛速度。通过在softmax策略参数化中收敛到最优策略，它以 $\tilde{O}(1/t^2)$ 的速率收敛。这是第一个基于Nesterov加速梯度在强化学习中全局收敛速率的研究。 |
| [^48] | [A New Multimodal Medical Image Fusion based on Laplacian Autoencoder with Channel Attention.](http://arxiv.org/abs/2310.11896) | 本论文提出了一种基于通道注意力的拉普拉斯自编码器的新型多模态医学图像融合模型，能够有效地保留补充信息和重要的组织结构。 |
| [^49] | [A Hyperparameter Study for Quantum Kernel Methods.](http://arxiv.org/abs/2310.11891) | 本研究调查了超参数选择对模型性能和经典核与量子核之间的泛化差距的影响。 |
| [^50] | [Building a Graph-based Deep Learning network model from captured traffic traces.](http://arxiv.org/abs/2310.11889) | 本论文介绍了一个从捕捉的流量跟踪构建深度学习网络模型的方法，通过图神经网络（GNN）和新颖的编码方法，可以更好地捕捉真实网络场景的复杂性。 |
| [^51] | [Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions.](http://arxiv.org/abs/2310.11888) | 本文介绍了一种利用人工智能分析质谱数据以检测古代火星适居性潜力的方法，并展示了该方法在外星物质分析中的适用性。关键技术包括质谱值的转换、数据可视化和机器学习模型的应用。 |
| [^52] | [From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks.](http://arxiv.org/abs/2310.11884) | 本文调查了解释神经网络中概念的最新方法，这对于实现基于可解释概念的神经符号化人工智能来说是重要的一步。 |
| [^53] | [Online Convex Optimization with Switching Cost and Delayed Gradients.](http://arxiv.org/abs/2310.11880) | 提出了一种在线多梯度下降（OMGD）算法用于解决具有二次和线性开关成本的在线凸优化问题，证明了其竞争比率上界，并在有限信息设置下达到了最优（按顺序）的动态后悔。 |
| [^54] | [SQ Lower Bounds for Learning Mixtures of Linear Classifiers.](http://arxiv.org/abs/2310.11876) | 本文研究了学习线性分类器混合的问题，证明了该问题的统计查询（SQ）算法的复杂度下界是$n^{\mathrm{poly}(1/\Delta) \log(r)}$，同时提出了一种可能具有独立兴趣的新球面设计方法。 |
| [^55] | [Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions.](http://arxiv.org/abs/2310.11875) | 本文介绍了一种使用分数概念修改激活和损失函数的方法，通过调整神经元的激活函数，能够更好地适应输入数据并提高网络的性能。 |
| [^56] | [Evaluating the Fairness of Discriminative Foundation Models in Computer Vision.](http://arxiv.org/abs/2310.11867) | 评估了计算机视觉中判别性基础模型的公平性，并提出了用于评估偏见的分类法。通过系统性地评估现有的减少模型偏见的方法，揭示了模型在关键应用中的性能。根据任务涉及人类程度、主观性程度和预期目的对期望的行为进行了分类，并对受保护属性进行了定量的公平性评估。 |
| [^57] | [Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function.](http://arxiv.org/abs/2310.11866) | 本文提出了一种能够同时提供Hessian矩阵、梯度和函数值的不精确计算的随机优化方法，通过减少传播开销来降低计算成本，并证明了在达到ε-近似二阶优化时与精确计算具有相同的迭代复杂度。 |
| [^58] | [Effective and Efficient Federated Tree Learning on Hybrid Data.](http://arxiv.org/abs/2310.11865) | 这项研究提出了一种名为HybridTree的新方法，可以在混合数据上进行联邦树学习。通过利用树中的一致分割规则，参与方的知识可以被纳入树的较低层，从而实现高效的联邦学习。 |
| [^59] | [VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization.](http://arxiv.org/abs/2310.11864) | VQ-NeRF是一个基于向量量化的神经网络模型，用于分解和编辑3D场景中的反射场。通过将连续材料离散化，该模型可以减少噪声并生成离散材料的分割地图，从而实现简化的材料编辑。 |
| [^60] | [Revisiting Transferable Adversarial Image Examples: Attack Categorization, Evaluation Guidelines, and New Insights.](http://arxiv.org/abs/2310.11850) | 本论文重新审视了可转移的对抗性图像示例的评估方法，提出了新的攻击分类策略，并通过大规模评估揭示了一些新的见解和共识挑战。 |
| [^61] | [Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning.](http://arxiv.org/abs/2310.11845) | 本论文提出了一种简单高效的强化学习框架（RL4Presolve），通过将算法设计任务转化为马尔可夫决策过程，同时解决了在大规模线性规划中前处理程序的选择、顺序和停止等问题。 |
| [^62] | [On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning.](http://arxiv.org/abs/2310.11840) | 这项工作通过对强化学习中17种目标规范形式的表达能力进行全面比较，填补了现有文献中的空白。通过将这些形式化方法进行预排序，并呈现为哈斯图，我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。 |
| [^63] | [Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems.](http://arxiv.org/abs/2310.11838) | 本文提出了一种新的不确定性量化方法，利用参数引导算法的等变形式，可以在成像反问题中量化重构图像的不确定性，并且可以与任何图像重建技术结合使用。 |
| [^64] | [Optimising Distributions with Natural Gradient Surrogates.](http://arxiv.org/abs/2310.11837) | 本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。 |
| [^65] | [CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition.](http://arxiv.org/abs/2310.11830) | 本文提出了一个多语言对比学习框架，通过自监督学习从无标签数据中获取音频表示，实现了跨语言迁移和情感维度的编码。 |
| [^66] | [Towards Graph Foundation Models: A Survey and Beyond.](http://arxiv.org/abs/2310.11829) | 本文提出了图基础模型（GFMs）的概念，并对其关键特征和技术进行了全面阐述。同时，将现有GFMs工作分为三个类别，为进一步研究和开发图学习范式奠定了基础。 |
| [^67] | [Conservative Predictions on Noisy Financial Data.](http://arxiv.org/abs/2310.11815) | 在嘈杂的金融数据上进行保守预测，通过对不确定的数据点进行剪枝，以提高预测的准确性。 |
| [^68] | [De novo protein design using geometric vector field networks.](http://arxiv.org/abs/2310.11802) | 创新点是引入了Vector Field Network (VFN)来提高蛋白质结构编码器的建模能力，在全新蛋白质设计中取得了显著进展。 |
| [^69] | [Adversarial Training for Physics-Informed Neural Networks.](http://arxiv.org/abs/2310.11789) | 这篇论文提出了一种名为AT-PINNs的对抗训练策略，通过对抗样本的微调来增强物理信息神经网络（PINNs）的鲁棒性，并且可以进行具有时间因果关系的推断。 |
| [^70] | [NeuroCUT: A Neural Approach for Robust Graph Partitioning.](http://arxiv.org/abs/2310.11787) | NeuroCUT是一种神经方法，用于解决鲁棒的图分区问题。它通过两个关键创新，即对图拓扑和分区计数具有归纳性，以及利用强化学习基础，能够从数据中学习启发式方法。 |
| [^71] | [A Quasi-Wasserstein Loss for Learning Graph Neural Networks.](http://arxiv.org/abs/2310.11762) | 这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。 |
| [^72] | [Domain-Generalized Face Anti-Spoofing with Unknown Attacks.](http://arxiv.org/abs/2310.11758) | 该论文提出了一种面向领域通用未知攻击的人脸反欺诈方法，通过引入Transformer-based特征提取器和合成未知攻击样本生成器（SUASG），实现了超越已知和未知攻击的领域通用性能。 |
| [^73] | [Estimating Material Properties of Interacting Objects Using Sum-GP-UCB.](http://arxiv.org/abs/2310.11749) | 本文提出了一种使用Sum-GP-UCB方法估计互动物体材料性质的贝叶斯优化方法，在不同组互动物体场景的观察下，通过建模奖励函数结构和部分评估来加速优化过程。 |
| [^74] | [Unintended Memorization in Large ASR Models, and How to Mitigate It.](http://arxiv.org/abs/2310.11739) | 该论文研究了大型ASR模型中的意外记忆问题，并提出了一种简单的审计方法来测量记忆效应。研究发现，目前的ASR模型存在记忆现象，为此提出了使用渐变裁剪进行训练来缓解记忆问题。 |
| [^75] | [Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting.](http://arxiv.org/abs/2310.11732) | 本研究系统评估了在多选设置下对齐语言模型不确定性校准的影响。研究发现，在这种设置下存在两种不确定性，分别对答案决策和格式偏好负责。对齐模型过度自信的原因之一是这两种不确定性的混淆。 |
| [^76] | [Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation.](http://arxiv.org/abs/2310.11730) | 本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。 |
| [^77] | [Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding.](http://arxiv.org/abs/2310.11721) | 这里是中文总结出的一句话要点 |
| [^78] | [On the Evaluation of Generative Models in Distributed Learning Tasks.](http://arxiv.org/abs/2310.11714) | 本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。 |
| [^79] | [Learning under Label Proportions for Text Classification.](http://arxiv.org/abs/2310.11707) | 这篇论文提出了一种在标签比例(LLP)的挑战性环境中进行文本分类的方法，通过对最广泛使用的基线技术进行改进，结合自监督目标，取得了较好的结果。 |
| [^80] | [Runner re-identification from single-view video in the open-world setting.](http://arxiv.org/abs/2310.11700) | 本论文提出了一种在开放世界环境中直接处理单视角视频的跑者再识别系统。通过自动处理原始视频作为输入来识别跑者，并且能够在跑者被框选出多次的情况下进行识别。 |
| [^81] | [AUC-mixup: Deep AUC Maximization with Mixup.](http://arxiv.org/abs/2310.11693) | 本文研究了如何通过mixup数据增强方法来改善深度AUC最大化方法（DAM）的泛化能力，以解决在应用于小数据集时可能导致过拟合问题的挑战。 |
| [^82] | [Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance.](http://arxiv.org/abs/2310.11690) | 提出了一种基于Transformer架构的深度学习方法，用于存在类别不平衡情况下的电力系统短期电压稳定性评估。使用条件Wasserstein生成对抗网络生成合成数据来解决不平衡数据集的问题，并利用稳定性评估Transformer作为分类模型，反映系统运行状态与稳定性结果之间的相关性。 |
| [^83] | [Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs.](http://arxiv.org/abs/2310.11689) | 本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。 |
| [^84] | [Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention.](http://arxiv.org/abs/2310.11685) | 通过对softmax和线性注意力机制进行全面比较分析，本论文揭示了softmax注意力在大多数情况下优于线性注意力的潜在原因。 |
| [^85] | [Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning.](http://arxiv.org/abs/2310.11684) | 本研究探索了无限时域平均奖励强化学习中量子加速的潜力。我们提出了一种创新的量子框架，通过高效的量子均值估计技术，实现了指数级改进的遗憾保证。所提出的量子算法相较于经典算法，在遗憾界限上有显著改进。 |
| [^86] | [Using Experience Classification for Training Non-Markovian Tasks.](http://arxiv.org/abs/2310.11678) | 该论文提出了一种使用经验分类的方法来训练非马尔可夫任务。通过将非马尔可夫任务转化为有限轨迹上的线性时态逻辑表达，并利用优先化经验回放技术改善训练过程，以实现非马尔可夫奖励的目标逻辑。实验证明了该方法的可行性和有效性。 |
| [^87] | [Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes.](http://arxiv.org/abs/2310.11677) | 本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。 |
| [^88] | [PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection.](http://arxiv.org/abs/2310.11676) | PREM是一种简单而有效的节点级图异常检测方法，它通过简化图异常检测的过程，减少了时间和内存消耗，同时保持了强大的异常检测能力。 |
| [^89] | [SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents.](http://arxiv.org/abs/2310.11667) | SOTOPIA是一个用于评估语言智能中的社交智能的交互式环境。通过模拟复杂的社交互动，并使用全面的评估框架，我们发现不同模型之间的社交智能存在显著差异，特别是在SOTOPIA-hard情景下。GPT-4在这个子集上的目标完成率较低。 |
| [^90] | [Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs.](http://arxiv.org/abs/2310.11664) | Hetero$^2$Net是一种面向异构图的异质属性感知表示学习方法，通过使用元路径识别异构图中的异质性，并提出了度量指标来描述异质性水平，以解决常见图神经网络在处理具有异质性的图中的限制。 |
| [^91] | [Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features.](http://arxiv.org/abs/2310.11654) | 本文提出了一种针对计数数据的主题专用深度神经网络，通过引入伽玛随机效应来提高预测性能，并同时获得了固定参数的最大似然估计和随机效应的最佳无偏预测器。该方法可以快速处理高基数分类特征的聚类计数数据，并且可以轻松实现最先进的网络架构。 |
| [^92] | [Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions.](http://arxiv.org/abs/2310.11640) | 本研究提出了一种基于Transformer的网络，通过自注意机制从按键序列中提取信息特征，改进了传统的循环神经网络在按键身份验证中的性能。实验结果表明，在Aalto桌面按键数据集上，采用批量-全部三重损失和余弦距离的双重编码器架构达到了最佳性能。 |
| [^93] | [Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks.](http://arxiv.org/abs/2310.11612) | 本论文提出了一种解决跨模态检索中中心程度问题的后处理解决方案，通过使用查询和画廊样本构建的两个库，以及基于这两个库的归一化方法，成功降低了中心点的出现。 |
| [^94] | [In defense of parameter sharing for model-compression.](http://arxiv.org/abs/2310.11611) | 本文综合评估了使用RPS、剪枝技术和构建较小模型等方法在内存和准确性之间的权衡，发现RPS在整个压缩范围内始终优于其他方法，并在高压缩场景中表现尤为突出。 |
| [^95] | [Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance.](http://arxiv.org/abs/2310.11609) | 该论文介绍了一种具有反射等变性的扩散方法，用于从同位素旋转光谱中测定有机分子的三维结构。这种方法可以解决由于缺失正负符号而难以确定实际结构的问题。 |
| [^96] | [TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification.](http://arxiv.org/abs/2310.11607) | TK-KNN是一种平衡的距离伪标签方法，通过在嵌入空间中使用排名的方法，能够在减少成本的同时保持类别之间的平衡。实验证明，TK-KNN的性能优于现有模型。 |
| [^97] | [DIAR: Deep Image Alignment and Reconstruction using Swin Transformers.](http://arxiv.org/abs/2310.11605) | 本文介绍了DIAR方法，利用Swin Transformers对一系列失真图像进行对齐和重建。通过训练模型分析序列图像数据，并利用注意力图检测相关内容并区分异常值和伪像。同时，还探索了使用神经特征图作为传统关键点检测器的替代方法。 |
| [^98] | [Language Models as Zero-Shot Trajectory Generators.](http://arxiv.org/abs/2310.11604) | 本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。 |
| [^99] | [Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning.](http://arxiv.org/abs/2310.11594) | 本文研究了联邦学习中对抗性训练和后门攻击的交叉点，引入了Adversarial Robustness Unhardening（ARU），通过有意介入分散式训练过程中破坏模型的鲁棒性，使模型更容易受到更广泛的逃避攻击。 |
| [^100] | [Automated Evaluation of Personalized Text Generation using Large Language Models.](http://arxiv.org/abs/2310.11593) | 这项研究提出了一种使用大型语言模型自动评价个性化文本生成的方法。传统的自动评价指标无法捕捉个性化质量的微妙差别，而人工判断又昂贵且困难。因此，本研究提出了一种新颖的评估方法，能够自动测量个性化、质量和相关性这三个重要语义方面。 |
| [^101] | [Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios.](http://arxiv.org/abs/2310.11590) | 本研究拟通过非语言行为提示和机器学习技术预测人们对机器人行为印象，并提供了一个数据集和分析结果，发现在导航场景中，空间特征是最关键的信息。 |
| [^102] | [Eliciting Human Preferences with Language Models.](http://arxiv.org/abs/2310.11589) | 本文介绍了一种生成式主动任务引导（GATE）的学习框架，该框架通过与用户进行自由形式的、基于语言的交互来引导和推断预期行为。在实验中展示，通过GATE引导的语言模型通常比用户编写的提示或标签更具信息量。 |
| [^103] | [What is a good question? Task-oriented asking with fact-level masking.](http://arxiv.org/abs/2310.11571) | 本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。 |
| [^104] | [When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting.](http://arxiv.org/abs/2310.11569) | 提出了一个新的概率分层时间序列预测模型，该模型能够有效建模和预测具有层次化关系的多变量时间序列。相较于现有方法，该模型不仅考虑点预测，还能提供经过良好校准的概率预测分布，并且在建模过程中考虑了预测分布的相关性。 |
| [^105] | [Partially Observable Stochastic Games with Neural Perception Mechanisms.](http://arxiv.org/abs/2310.11566) | 本研究提出了神经符号化部分可观测随机博弈（NS-POSGs）模型，通过融合感知机制解决了多智能体序列决策中的部分可观测性问题。其中，我们专注于一种只有部分观测信息的智能体和一种完全观测的智能体的单方面设置，并提出了一种近似计算NS-POSGs值的新方法。 |
| [^106] | [Online Algorithms with Uncertainty-Quantified Predictions.](http://arxiv.org/abs/2310.11558) | 这篇论文研究了如何在设计在线算法时最佳利用不确定性量化预测，并提出了一种考虑预测概率性的在线算法设计方法。 |
| [^107] | [Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback.](http://arxiv.org/abs/2310.11550) | 研究带有对抗损失和强盗反馈的线性MDPs问题，提出了两种算法，分别达到了$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$和$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$的遗憾性能。 |
| [^108] | [Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models.](http://arxiv.org/abs/2310.11546) | 本文提出了一种高级搜索和优化框架，旨在通过生成代码模型来减轻软件生成数据中的偏差和误差。通过纠正之前版本中的问题，该框架可以改善数据分析和生成软件系统的质量。 |
| [^109] | [MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning.](http://arxiv.org/abs/2310.11541) | 本文提出了一种多语言和统一音节标记的文本和音韵领域中的语音表示学习方法，通过自动音节化单词并生成宝贵注释，适用于语音表示学习、语音单元发现和语音因素解缠。 |
| [^110] | [Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach.](http://arxiv.org/abs/2310.11531) | 本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。 |
| [^111] | [Thin and Deep Gaussian Processes.](http://arxiv.org/abs/2310.11527) | 本文提出了一种薄而深的高斯过程方法，克服了传统方法的局限性，并在学习低维嵌入和解释性之间取得了平衡。 |
| [^112] | [Group Preference Optimization: Few-Shot Alignment of Large Language Models.](http://arxiv.org/abs/2310.11523) | 这项研究介绍了一种名为群体偏好优化（GPO）的对齐框架，可以以少样本的方式将大规模语言模型（LLMs）引导到个别群体的偏好。通过在基本LLM上加入独立的transformer模块来预测群体偏好，并通过元学习进行训练，GPO经过严格评估验证了其有效性。 |
| [^113] | [Automatic News Summerization.](http://arxiv.org/abs/2310.11520) | 本研究比较和评估了抽取式和生成式方法在新闻文本摘要上的效果，并使用ROUGE得分进行质量评估。最佳表现模型被集成到一个Web应用程序中，以评估其在现实世界中的能力和用户体验。 |
| [^114] | [Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability.](http://arxiv.org/abs/2310.11518) | 这篇论文研究了多人游戏中自我对抗的保证问题，通过多矩阵可分解性，在满足一定条件的情况下，通过自我对抗学习的算法能够产生有界脆弱性的策略。 |
| [^115] | [Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs.](http://arxiv.org/abs/2310.11515) | 这篇论文提出了一个基于价值偏见的最大似然估计方法，用于解决线性马尔可夫决策过程中的模型驱动强化学习问题。该方法在计算效率和后悔方面都取得了较好的性能。 |
| [^116] | [GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment.](http://arxiv.org/abs/2310.11513) | 本论文介绍了GenEval，一种面向对象的框架，用于评估文本到图像生成模型的组成属性。通过利用当前的对象检测模型，我们可以在各种生成任务上评估文本到图像模型，并通过其他视觉判别模型进一步验证属性。 |
| [^117] | [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.](http://arxiv.org/abs/2310.11511) | Self-RAG是一种通过检索和自我反思提高语言模型质量和事实性的框架。 |
| [^118] | [End-to-End real time tracking of children's reading with pointer network.](http://arxiv.org/abs/2310.11486) | 本研究提出了一种用于儿童阅读的实时跟踪器模型，在语音跟踪的延迟方面具有较低的敏感性。通过使用指针网络和强制对齐生成训练信号，我们的模型可以准确地跟踪成人语音，并在儿童语音数据集上获得良好的效果。 |
| [^119] | [Whole-brain radiomics for clustered federated personalization in brain tumor segmentation.](http://arxiv.org/abs/2310.11480) | 该论文提出了基于集群联邦个性化的全脑影像组学方法，通过考虑不同机构间和机构内特征转移问题，解决了联邦学习中的统计异质性带来的收敛问题和准确性损失。 |
| [^120] | [On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction.](http://arxiv.org/abs/2310.11479) | 该论文探讨了将温度参数纳入贝叶斯图神经网络在一致预测中的优势，以提供有效的不确定性量化。 |
| [^121] | [ASP: Automatic Selection of Proxy dataset for efficient AutoML.](http://arxiv.org/abs/2310.11478) | 本文提出了一个自动选择代理数据集框架 (ASP)，通过动态地找到信息丰富的代理子集来减小训练数据大小并节省AutoML处理时间，实验证明ASP在不同基准测试上获得了优于其他方法的结果。 |
| [^122] | [Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function.](http://arxiv.org/abs/2310.11477) | 本文提出了一种稳健的深度学习系统用于电机轴承故障检测，采用多个深度学习训练策略和一种新的双损失函数。通过对比评估不同系统并寻找最佳模型，我们展示了该系统对各种电机轴承故障的有效性。 |
| [^123] | [Program Translation via Code Distillation.](http://arxiv.org/abs/2310.11476) | 本文提出了一种名为Code Distillation（CoDist）的创新模型，通过在一种语言无关的中间表示中捕捉代码的语义和结构等价关系，实现了程序翻译，克服了传统机器翻译和无监督神经机器翻译在数据对齐和IR多样性方面的挑战。 |
| [^124] | [Classic machine learning methods.](http://arxiv.org/abs/2310.11470) | 本章主要介绍了经典机器学习方法，包括监督学习和无监督学习。其中介绍了分类和回归的各种方法，以及解决过拟合问题的策略。 |
| [^125] | [Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy.](http://arxiv.org/abs/2310.11467) | 本报告通过整合生成的代码和注释对，改进二进制代码注释质量分类模型，提高准确性。 |
| [^126] | [Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction.](http://arxiv.org/abs/2310.11466) | 本文研究了蛋白质基于结构的性质预测中使用预测结构时性能下降的原因，并将其归因为结构嵌入偏差。 |
| [^127] | [BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis.](http://arxiv.org/abs/2310.11465) | 本研究提出了一个大规模的孟加拉语YouTube clickbait多模态数据集，为研究人员提供了在低资源语言中建模clickbait现象的重要价值，并且可以开发出更复杂的跨语言检测方法。 |
| [^128] | [Identifying Interpretable Visual Features in Artificial and Biological Neural Systems.](http://arxiv.org/abs/2310.11431) | 本文提出了一种量化视觉可解释性的自动化方法，并找到了卷积神经网络中有意义的方向。 |
| [^129] | [Understanding Fairness Surrogate Functions in Algorithmic Fairness.](http://arxiv.org/abs/2310.11211) | 本文研究了算法公平性中的公平性代理函数，并发现了代理和公平性定义之间存在一个差距。这个差距决定了一个代理函数能否适当替代一个公平性定义。 |
| [^130] | [HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning.](http://arxiv.org/abs/2310.11102) | HGCVAE是一种将生成式学习和对比学习整合为一体的异构图学习方法，通过利用生成式的自监督学习能力来解决异构图学习的挑战。 |
| [^131] | [Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning.](http://arxiv.org/abs/2310.10943) | 这项研究比较了最优控制 (OC)和强化学习 (RL)方法在自主无人机赛车中的效果，发现强化学习方法优于最优控制方法。研究表明，强化学习能够直接优化任务层面的目标，并利用领域的随机因素，而最优控制的分解限制了控制器的行为范围。 |
| [^132] | [Surrogate Active Subspaces for Jump-Discontinuous Functions.](http://arxiv.org/abs/2310.10907) | 该论文提出了一种针对不连续函数的替代主动子空间方法，扩展了活跃子空间的应用范围，并通过数值实验验证了该方法的有效性。 |
| [^133] | [Demystifying Poisoning Backdoor Attacks from a Statistical Perspective.](http://arxiv.org/abs/2310.10780) | 从统计学角度揭开中毒后门攻击的神秘面纱，通过评估任何包含恒定触发器的后门攻击的有效性，确定了后门攻击成功的决定因素、最有效的攻击方向以及几乎不可察觉的人类触发器何时会成功。 |
| [^134] | [Microscaling Data Formats for Deep Learning.](http://arxiv.org/abs/2310.10537) | 本文评估了Microscaling（MX）数据格式在降低深度学习应用的计算和存储成本方面的可行性。实证结果显示MX数据格式可以作为基线FP32的替代，同时保持低用户摩擦，并且成功在超过两打基准测试中以小于8位的数据格式进行了训练。 |
| [^135] | [Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking.](http://arxiv.org/abs/2310.10520) | 本论文提出了ParsingDST方法，利用大型语言模型和语义解析技术，实现了复杂的零样本对话状态跟踪的更新策略，并在实验中展示了明显的改进。 |
| [^136] | [Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models.](http://arxiv.org/abs/2310.10449) | 本研究通过比较MPT-7b-instruct, Falcon-7b-instruct和OpenAI Chat-GPT模型，在不同的数据集上使用不同的超参数进行了文本摘要实验。实验结果表明，text-davinci-003模型表现最佳，并且提供了大型语言模型在文本摘要中的性能综述。 |
| [^137] | [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models.](http://arxiv.org/abs/2310.10378) | 本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。 |
| [^138] | [Chinese Painting Style Transfer Using Deep Generative Models.](http://arxiv.org/abs/2310.09978) | 本文研究和利用不同的深度生成模型进行中国绘画风格转换，并提出了一种结合了多种模型的算法。在定性和定量方面评估了其性能。 |
| [^139] | [Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction.](http://arxiv.org/abs/2310.09903) | 本研究评估了特征选择方法在股市价格预测中的性能，通过选择最佳的技术指标组合来实现最少误差的预测。研究结果表明，不同的包装器特征选择方法在不同的机器学习方法中具有不同的表现。 |
| [^140] | [Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge Devices.](http://arxiv.org/abs/2310.09667) | 本论文提出了Edge-InversionNet，通过采用结构化修剪算法得到了InversionNet的轻量化版本，在资源受限的边缘设备上实现了高效的推理。实验结果显示，修剪后的InversionNet在性能略有下降的情况下，可以实现高达98.2%的计算资源减少。 |
| [^141] | [Tokenizer Choice For LLM Training: Negligible or Crucial?.](http://arxiv.org/abs/2310.08754) | 在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。 |
| [^142] | [The Expresssive Power of Transformers with Chain of Thought.](http://arxiv.org/abs/2310.07923) | 本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。 |
| [^143] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^144] | [Crystal: Introspective Reasoners Reinforced with Self-Feedback.](http://arxiv.org/abs/2310.04921) | 提出了一种名为Crystal的内省型常识推理器，通过内省知识和基于知识的推理相结合，提高了常识推理的性能和解释能力。 |
| [^145] | [Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets.](http://arxiv.org/abs/2310.04292) | 本研究提出了七个新颖的数据集，分别是ToyMix、LargeMix和UltraLarge，这些数据集在规模和有监督标签的多样性方面突破了界限，涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。 |
| [^146] | [MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems.](http://arxiv.org/abs/2310.02784) | 该研究提出了一个性能建模框架，在分布式系统上实现了大规模机器学习模型的加速，获得了2.24倍和5.27倍的吞吐量提升潜力。 |
| [^147] | [Intelligent Client Selection for Federated Learning using Cellular Automata.](http://arxiv.org/abs/2310.00627) | 本研究提出了一种使用元胞自动机的智能客户端选择算法，用于解决联邦学习中由于传感器数量增加带来的通信和资源分配问题。 |
| [^148] | [Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference.](http://arxiv.org/abs/2310.00532) | 该论文研究了自适应数据对于线性模型中低维参数估计性能的影响，并确定了条件使得估计误差与独立同分布数据情况相匹配。 |
| [^149] | [A Foundation Model for General Moving Object Segmentation in Medical Images.](http://arxiv.org/abs/2309.17264) | 本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果 |
| [^150] | [SEPT: Towards Efficient Scene Representation Learning for Motion Prediction.](http://arxiv.org/abs/2309.15289) | SEPT是一个利用自监督学习进行场景表示学习的建模框架，通过预训练的编码器捕捉轨迹的运动学特征、道路网络的空间结构以及道路和代理之间的交互作用，实现了在运动预测任务上的最先进性能。 |
| [^151] | [Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding.](http://arxiv.org/abs/2309.15028) | 本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。 |
| [^152] | [Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs.](http://arxiv.org/abs/2309.13365) | 该论文研究了在IBMDP中使用Actor-Critic算法学习决策树策略的局限性。结果表明，即使是在简单的玩具任务上，深度RL也可能失败。 |
| [^153] | [Bayesian longitudinal tensor response regression for modeling neuroplasticity.](http://arxiv.org/abs/2309.10065) | 提出了一种基于贝叶斯纵向张量响应回归的新方法，利用汇集空间分布的体素信息来推断显著的变化，并对协变量进行调整。该方法使用马尔科夫链蒙特卡洛采样，利用低秩分解减少维度并保持维度的空间配置，通过满足后验分布形状的联合可信区域实现特征选择，从而实现更准确的推断。 |
| [^154] | [Uncertainty-aware Traffic Prediction under Missing Data.](http://arxiv.org/abs/2309.06800) | 本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。 |
| [^155] | [HAct: Out-of-Distribution Detection with Neural Net Activation Histograms.](http://arxiv.org/abs/2309.04837) | 本论文提出了一种用于神经网络的数据集外检测方法，使用HAct激活直方图描述符对OOD进行检测，具有简单高效和准确性，并且在多个OOD图像分类基准测试中表现优于先前最先进的方法。 |
| [^156] | [Mixed Variational Flows for Discrete Variables.](http://arxiv.org/abs/2308.15613) | 本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。 |
| [^157] | [Quantization-based Optimization with Perspective of Quantum Mechanics.](http://arxiv.org/abs/2308.11594) | 基于量子力学视角的量化优化方法在全局优化中利用薛定谔方程推导的隧道效应，从而能够避免局部最小值。 |
| [^158] | [Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images.](http://arxiv.org/abs/2308.07688) | 该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。 |
| [^159] | [Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness.](http://arxiv.org/abs/2308.03666) | 该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。 |
| [^160] | [Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling.](http://arxiv.org/abs/2307.12672) | 本文提出了一种基于遮蔽图像模型的全局k空间插值方法，通过连接遮蔽图像建模和k空间插值，使用Transformer网络来学习2D+t k空间的全局依赖关系，并提出了k空间迭代细化模块来增强高频成分的学习。 |
| [^161] | [JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning.](http://arxiv.org/abs/2307.11704) | 本文介绍了JoinGym，一种高效的强化学习查询优化环境。通过将加入顺序选择问题形式化为马尔可夫决策过程，我们提供了一种实现，该实现完全基于离线跟踪，并且无需设置系统即可进行测试。此外，研究还提供了3300个新SQL查询的所有可能加入追踪。 |
| [^162] | [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge.](http://arxiv.org/abs/2307.08813) | 本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。 |
| [^163] | [Bootstrapping Vision-Language Learning with Decoupled Language Pre-training.](http://arxiv.org/abs/2307.07063) | 本文提出了一种新的方法，通过解耦语言预训练，集中在语言组件上，提供了优化应用大型语言模型的框架，有效改善了视觉语言学习的性能。 |
| [^164] | [Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias.](http://arxiv.org/abs/2306.15895) | 本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。 |
| [^165] | [Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate.](http://arxiv.org/abs/2306.15444) | 有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。 |
| [^166] | [DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$.](http://arxiv.org/abs/2306.08068) | DORSal提出了一种基于扩散模型的物体中心场景表示方法，可以呈现高保真新视图，并在较大程度上保留了诸如基于物体的场景编辑之类的优点。 |
| [^167] | [Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards.](http://arxiv.org/abs/2306.05579) | 本文提出了一种处理多智能体多臂赌博机问题的算法框架，通过稳健模拟生成随机图并将加权技术结合UCB算法，以协作方式减小整个系统的总遗憾。 |
| [^168] | [Ordinal Potential-based Player Rating.](http://arxiv.org/abs/2306.05366) | 该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。 |
| [^169] | [Contagion Effect Estimation Using Proximal Embeddings.](http://arxiv.org/abs/2306.02479) | 本文介绍了一种使用近邻嵌入方法来估计社交网络中的传染效应。我们提出了ProEmb框架，通过将变分自动编码器（VAEs）和对抗网络集成在一起，生成高维代理变量的平衡低维表示，并解决了传染效应估计中的偏差问题。 |
| [^170] | [What model does MuZero learn?.](http://arxiv.org/abs/2306.00840) | 本文研究了MuZero算法，发现它学习到的模型无法有效推广到评估未见策略，限制了其对当前策略的进一步改进。 |
| [^171] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^172] | [Policy Optimization for Continuous Reinforcement Learning.](http://arxiv.org/abs/2305.18901) | 本研究提出了连续强化学习领域的占用时间概念，并在此基础上扩展了离散强化学习中的PG、TRPO和PPO方法，为连续强化学习领域的研究提供了新的思路和方法。 |
| [^173] | [Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors.](http://arxiv.org/abs/2305.18803) | 本论文提出了一种名为Koopa的Koopman预测器，通过使用傅里叶滤波器将非平稳时间序列中的时间变体和时间不变成分分离出来，并使用Koopman算子作为线性表述，成功解决了深度预测模型在处理非平稳时间序列时面临的挑战。 |
| [^174] | [Generalized equivalences between subsampling and ridge regularization.](http://arxiv.org/abs/2305.18496) | 此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。 |
| [^175] | [Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms.](http://arxiv.org/abs/2305.18409) | 本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。 |
| [^176] | [Bi-fidelity Variational Auto-encoder for Uncertainty Quantification.](http://arxiv.org/abs/2305.16530) | 本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。 |
| [^177] | [Focus Your Attention (with Adaptive IIR Filters).](http://arxiv.org/abs/2305.14952) | 该论文提出了使用自适应IIR滤波器来聚焦注意力的新层。这些滤波器基于输入序列的前几个块来确定系数，能够将注意力集中在相关的序列元素上，并且相比于其他网络具有更少的参数和次二次时间复杂度。该层在多个长程序列问题上表现优异，超越了Heyna、GPT2和Mega等层。 |
| [^178] | [Difference-Masking: Choosing What to Mask in Continued Pretraining.](http://arxiv.org/abs/2305.14577) | 本文提出了一种自动选取遮蔽数据的方法（Difference-Masking），以提高在继续预训练中的自监督学习模型的性能，方法是通过考虑未标记的目标域与预训练域的不同之处来进行。实验证明，该方法可以有效地提升继续预训练任务的性能，且具有跨任务的适用性。 |
| [^179] | [Unsupervised Anomaly Detection with Rejection.](http://arxiv.org/abs/2305.13189) | 本文提出了一种无监督异常检测方法，通过选择一个恒定的置信度度量和拒绝阈值来解决无标签情况下的异常检测中的挑战。 |
| [^180] | [Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data.](http://arxiv.org/abs/2305.11913) | 本文提出了一种基于神经网络的方法，利用高度现实的合成训练数据对稀疏雷达数据进行相位相关的波浪表面重建。 |
| [^181] | [Clifford Group Equivariant Neural Networks.](http://arxiv.org/abs/2305.11141) | 我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。 |
| [^182] | [Online Resource Allocation in Episodic Markov Decision Processes.](http://arxiv.org/abs/2305.10744) | 本文将长期资源分配问题形式化为剧集式MDP模型，提出在面临转换和奖励特性不确定的情况下进行在线资源分配问题的解决方案。该方案基于占有度量提供在线镜像下降算法，其期望遗憾受到界限约束 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ . |
| [^183] | [Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments.](http://arxiv.org/abs/2305.06026) | 本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。 |
| [^184] | [DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects.](http://arxiv.org/abs/2305.04449) | 本论文介绍了一种名为DeformerNet的神经网络架构，通过学习三维可塑物体的低维表示来实现机器人对物体形状的操纵。这种方法不需要手工特征和物体特定的控制模型，可在仿真和真实机器人上进行演示和应用。 |
| [^185] | [Inferring Local Structure from Pairwise Correlations.](http://arxiv.org/abs/2305.04386) | 本研究展示了即使在严重欠采样的情况下，成对相关性提供了恢复局部结构的足够信息，对于理解模型复杂多变量系统的建模和现代关注机制机器学习方法的成功具有重要贡献。 |
| [^186] | [When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](http://arxiv.org/abs/2305.02997) | 这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。 |
| [^187] | [IMAP: Intrinsically Motivated Adversarial Policy.](http://arxiv.org/abs/2305.02605) | IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。 |
| [^188] | [Schooling to Exploit Foolish Contracts.](http://arxiv.org/abs/2304.10737) | SCooLS是一个智能合约学习引擎，使用半监督学习和图神经网络，可以直接分析以太坊合约字节码并识别易受攻击的功能，性能优于现有工具，准确度高达98.4%，是第一个基于深度学习的漏洞分析器。 |
| [^189] | [Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation.](http://arxiv.org/abs/2303.13974) | 本文提出了一种通过知识蒸馏技术，将复杂预训练模型的知识转移到轻量级模型上，从而使低内存设备也能进行复杂缺陷分类，且无需大量标记数据。 |
| [^190] | [Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.](http://arxiv.org/abs/2303.13408) | 本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。 |
| [^191] | [Enhancing Protein Language Models with Structure-based Encoder and Pre-training.](http://arxiv.org/abs/2303.06275) | 本文提出了一种结合基于结构的编码器和预训练的蛋白质语言模型，以明确地编码蛋白质结构，获得更好的结构感知蛋白质表示，并在实验中验证了其有效性。 |
| [^192] | [Reconstructing the Hubble parameter with future Gravitational Wave missions using Machine Learning.](http://arxiv.org/abs/2303.05169) | 本研究使用机器学习算法，通过未来的引力波任务重建了哈勃参数，得出了高斯过程在重建宇宙膨胀历史方面的稳健性，同时也发现未来任务能够提供与当前数据集相竞争的对哈勃参数和哈勃常数的约束。 |
| [^193] | [Model-Agnostic Federated Learning.](http://arxiv.org/abs/2303.04906) | 这篇论文提出了一种模型无关的联邦学习系统（MAFL），该系统不仅适用于深度神经网络（DNN）和决策树等特定类型的机器学习模型，还能在非DNN场景中应用，并且通过优化实现了较高的性能。 |
| [^194] | [Taylor TD-learning.](http://arxiv.org/abs/2302.14182) | Taylor TD是一个基于模型的RL框架，通过使用TD更新的泰勒级数展开，减少了连续状态-动作设置中的方差，并具有与标准TD学习相同的稳定学习保证。TaTD3是Taylor TD与TD3算法相结合所形成的方法，其表现优于一些最先进的无模型和基于模型的基准。 |
| [^195] | [Video-Text Retrieval by Supervised Sparse Multi-Grained Learning.](http://arxiv.org/abs/2302.09473) | 本文提出了一种新的多粒度稀疏学习框架S3MA，用于视频文本检索。该框架通过学习共享的稀疏空间和多粒度相似度来改进检索效果，在多个基准测试中表现优于现有方法。 |
| [^196] | [Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data.](http://arxiv.org/abs/2302.08466) | 本研究设计了一种黑盒模型提取攻击方法Marich，它使用公共数据集中的最小数量查询来创建一个与目标模型具有信息丰富度和分布等价性的副本，实验结果表明Marich能提取具有60-95%真实模型准确性的模型。 |
| [^197] | [Experimenting with Emerging RISC-V Systems for Decentralised Machine Learning.](http://arxiv.org/abs/2302.07946) | 该研究介绍了一种特定领域语言，用于将分散式机器学习方案映射到FastFlow并行编程库。通过在不同处理器平台上生成不同的DML方案，研究者评估了所提出方案和系统的性能和能源效率，并成功移植了PyTorch框架到RISC-V平台。 |
| [^198] | [A Federated Learning Benchmark for Drug-Target Interaction.](http://arxiv.org/abs/2302.07684) | 本文提出在DTI领域中采用联邦学习来汇集制药数据，相对于最佳非隐私保护替代方法可获得高达15%的性能提升，且非IID数据分布不会降低联邦学习的性能。 |
| [^199] | [Chain of Hindsight Aligns Language Models with Feedback.](http://arxiv.org/abs/2302.02676) | 该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。 |
| [^200] | [Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data.](http://arxiv.org/abs/2301.12321) | 本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。 |
| [^201] | [On the Convergence of No-Regret Learning Dynamics in Time-Varying Games.](http://arxiv.org/abs/2301.11241) | 本文研究了时变博弈中乐观梯度下降法（OGD）的收敛性，提出了明确的收敛界限，并建立了适用于时变总和多人博弈和元学习的新型双线性公式。 |
| [^202] | [Myths and Legends in High-Performance Computing.](http://arxiv.org/abs/2301.02432) | 这篇文章讨论了高性能计算社区中流传的神话和传说，这些神话往往不基于科学事实，而是基于一些证据或论证。虽然有些问题仍然是无休止的哲学辩论，但新的方向正在出现，如算法的规模化或新的架构研究。 |
| [^203] | [Learning List-Level Domain-Invariant Representations for Ranking.](http://arxiv.org/abs/2212.10764) | 本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。 |
| [^204] | [Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters.](http://arxiv.org/abs/2211.11031) | 本论文提出了一种名为GRACE的方法来实现终身模型编辑，它通过在流式错误上执行目标编辑来修复部署模型的问题，生成一个离散、本地的编辑编码本，而不会改变模型权重，在进行数千个顺序编辑时表现为最先进的性能。 |
| [^205] | [FairMILE: Towards an Efficient Framework for Fair Graph Representation Learning.](http://arxiv.org/abs/2211.09925) | 提出了一种名为FairMILE的多层范式框架，用于解决图表示学习中的公平性问题，并且在保持效用的同时实现了高效的图表示学习。 |
| [^206] | [FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data.](http://arxiv.org/abs/2211.09299) | 本文提出了一种名为 FedFA 的联邦学习框架，通过特征锚定来对齐特征映射并校准分类器，解决了在异构数据时分类器和特征映射之间的恶性循环问题。在实验中表明，该方法能够提高准确性和收敛速度。 |
| [^207] | [Deep Distance Sensitivity Oracles.](http://arxiv.org/abs/2211.02681) | 这篇论文介绍了一种使用深度学习技术构建距离敏感预测算法的方法，利用了替代路径的组合结构。 |
| [^208] | [Estimating the Contamination Factor's Distribution in Unsupervised Anomaly Detection.](http://arxiv.org/abs/2210.10487) | 该论文提出了一种从贝叶斯的角度估计无监督异常检测中污染因子的后验分布的方法，并使用多个异常检测器的输出作为表示，并通过特定的混合形式进行估计。在22个数据集的实证研究中，该方法表现良好。 |
| [^209] | [Evaluating Self-Supervised Learning for Molecular Graph Embeddings.](http://arxiv.org/abs/2206.08005) | 评估分子图嵌入的自监督学习方法的MOLGRAPHEVAL揭示了在不同的下游任务中GSSL方法性能存在显著的不一致性，并为未来研究提供了新的方向。 |
| [^210] | [COVID-Net Biochem: An Explainability-driven Framework to Building Machine Learning Models for Predicting Survival and Kidney Injury of COVID-19 Patients from Clinical and Biochemistry Data.](http://arxiv.org/abs/2204.11210) | COVID-Net Biochem是一个可解释的框架，通过整合领域专业知识，利用临床和生化数据来预测COVID-19患者的生存和肾损伤风险。 |
| [^211] | [Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing.](http://arxiv.org/abs/2201.08078) | 本文提出了一种解决强化学习中最大化偏差问题的方法，使用了两样本检验的估计器，能够灵活地插值过度估计和欠估计之间的关系，并在$Q$学习和引导化深度Q网络中得到了验证。 |
| [^212] | [Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction.](http://arxiv.org/abs/2107.14432) | 本论文提出了一种新的框架，在神经网络的CTR预测中加入了稀疏分组Lasso的正则项，并创建了一类新的自适应优化器。实验证明，这些优化器在相同稀疏水平下可以显著提升模型性能，并且能够实现极高的稀疏性。 |
| [^213] | [A Comparative Evaluation of Quantification Methods.](http://arxiv.org/abs/2103.03223) | 本研究通过对24种不同量化方法在超过40个数据集上进行全面实证比较，填补了量化方法比较研究的空白。我们发现在二分类设置中，基于阈值选择的Median Sweep和TSMax方法、DyS框架和弗里德曼的方法表现最佳；而在多分类设置中，Generaliz方法表现良好。 |
| [^214] | [Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis.](http://arxiv.org/abs/2012.03436) | 本文提出了一种新的张量秩正则化方法，通过计算张量的CP分量向量的欧几里德范数，间接最小化了Schatten-p准范，用于低秩张量补全和张量鲁棒主成分分析。该方法在处理大型张量时具有可扩展性，并且提供了比核范数更精确的秩代理。同时，通过比较理论分析，证明了该方法在LRTC的泛化能力上的优势。 |
| [^215] | [Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}.](http://arxiv.org/abs/2010.02968) | 该论文提出了一种结合Fréchet均值和形状不变模型的方法，用于检测功能性轮廓中的形状变化，并构建了功能性数据的控制图，可解释性强且能识别潜在变化。 |
| [^216] | [Adopting Robustness and Optimality in Fitting and Learning.](http://arxiv.org/abs/1510.03826) | 该论文提出了一种通过优化准最小函数，将健壮-最优指数推至负无穷来实现对异常值的鲁棒性的方法，并通过在Hessian矩阵中扩展凸性区域来保证最优性。通过实验结果验证了该方法的有效性。 |

# 详细

[^1]: 使用绝热量子计算的平衡K-Means的概率采样

    Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing. (arXiv:2310.12153v1 [cs.LG])

    [http://arxiv.org/abs/2310.12153](http://arxiv.org/abs/2310.12153)

    该论文研究了使用绝热量子计算的平衡K-Means聚类的概率采样方法，通过利用非最优解来计算校准后验概率，实现在D-Wave AQC上识别模糊解决方案和数据点的目标。

    

    绝热量子计算（AQC）是一种有望用于离散且通常为NP困难优化问题的量子计算方法。目前的AQC允许实现感兴趣的问题，这促使了为许多机器学习和计算机视觉任务开发量子表示的发展。尽管需要从噪声AQC进行多次测量，但当前方法仅利用最佳测量，丢弃了其他测量中包含的信息。在这项工作中，我们探讨了利用这些信息进行概率平衡k-means聚类的潜力。我们提出了使用非最优解来计算校准后验概率的方法，计算成本很低。这使我们能够识别模糊的解决方案和数据点，我们在D-Wave AQC上使用合成和真实数据进行了验证。

    Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
    
[^2]: 通过NAS实现更公平和准确的表格模型

    Fairer and More Accurate Tabular Models Through NAS. (arXiv:2310.12145v1 [cs.LG])

    [http://arxiv.org/abs/2310.12145](http://arxiv.org/abs/2310.12145)

    通过使用多目标神经架构搜索(NAS)和超参数优化(HPO)，我们在表格数据领域首次提出了一种更新模型架构和超参数的策略，以寻找更公平和准确的模型。我们发现，仅针对准确性进行优化可能会导致公平性的降低，因此需要同时考虑准确性和公平性。

    

    长期以来，通过算法使得表格数据的模型更加公平一直是研究的课题。现有的技术通常针对存在不可取的结果的神经模型，通过改变数据的摄入方式、模型权重或输出处理方式来修复。我们采用了新的策略，在去偏过程中考虑更新模型的架构和训练超参数，以找到一个从一开始在预测结果上更好的新模型。在这项工作中，我们首次将多目标神经架构搜索(NAS)和超参数优化(HPO)应用于具有挑战性的表格数据领域。我们在不同数据集上对MLP、ResNet和FT-Transformer等不同架构和超参数空间进行了广泛的探索，展示了模型预测的准确性和公平性指标对超参数组合的依赖关系。我们表明，仅针对准确性进行优化的模型可能会导致公平性的降低，因此需要在优化过程中同时考虑准确性和公平性。

    Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for ac
    
[^3]: 使用稀疏回归储层计算机识别动态金融过程

    Dynamic financial processes identification using sparse regressive reservoir computers. (arXiv:2310.12144v1 [eess.SY])

    [http://arxiv.org/abs/2310.12144](http://arxiv.org/abs/2310.12144)

    本文介绍了使用稀疏回归储层计算机来识别动态金融过程的方法。通过结构化矩阵逼近和稀疏最小二乘方法确定输出耦合矩阵的近似表示，并利用这些表示建立对应于给定金融系统中递归结构的回归模型。通过应用于动态金融和经济过程的近似识别和预测模拟，展示了算法的有效性。

    

    本文介绍了结构化矩阵逼近理论的关键发现，以及其在动态金融过程的回归表示中的应用。首先，我们探讨了涉及从金融或经济系统中提取的时间序列数据的通用非线性时延嵌入的全面方法。随后，我们采用稀疏最小二乘和结构化矩阵逼近方法，来识别输出耦合矩阵的近似表示。这些表示在建立对应于给定金融系统中递归结构的回归模型方面起着关键作用。本文还介绍了利用上述技术的原型算法。通过在动态金融和经济过程的近似识别和预测模拟中的应用，展示了这些算法，包括可能表现出混沌行为的情景。

    In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.
    
[^4]: 简单机制用于表示、索引和操作概念

    Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])

    [http://arxiv.org/abs/2310.12143](http://arxiv.org/abs/2310.12143)

    通过查看概念的矩阵统计量，生成一个概念的具体表示或签名，可以用于发现概念之间的结构并递归产生更高级的概念，同时可以通过概念的签名来找到相关的共同主题。

    

    深度网络通常通过分类器学习概念，这涉及设置模型并通过梯度下降训练它以适应具有标记概念的数据。我们将提出一个不同的观点，即可以通过查看概念的矩阵矩阵统计量来生成概念的具体表示或签名。这些签名可以用于发现一组概念的结构，并且可以通过从这些签名中学习该结构来递归地产生更高级的概念。当概念"相交"时，概念的签名可以用于在一些相关的"相交"概念中找到一个共同的主题。这个过程可以用于保持一个概念字典，以便输入能够正确识别并被路由到与输入的(潜在)生成相关的概念集合中。

    Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
    
[^5]: DiagrammerGPT: 通过LLM规划生成开放领域、开放平台的图表

    DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])

    [http://arxiv.org/abs/2310.12128](http://arxiv.org/abs/2310.12128)

    DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。

    

    过去几年，文本到图像（T2I）生成取得了显著的发展。尽管如此，在使用T2I模型生成图表方面的研究很少。图表是一种使用结构丰富和空间复杂的可视化来解释信息的符号/示意性表示（例如，一种密集的相关对象、文本标签、方向箭头、连接线等组合）。现有的最先进的T2I模型在生成图表时经常失败，因为它们在许多对象通过复杂的关系（如箭头/线）密集连接时缺乏细粒度的对象布局控制，并且经常不能渲染出可理解的文本标签。为了填补这一空白，我们提出了DiagrammerGPT，一个新颖的两阶段文本到图表生成框架，它利用LLM（如GPT-4）的布局引导能力来生成更准确的开放领域、开放平台的图表。在第一阶段，我们使用LLM生成和迭代改进“图表规划”（在一个规划方案中）。

    Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
    
[^6]: 代词故事：可解释性指导下的公平指导机器翻译中的性别偏见缓解

    A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])

    [http://arxiv.org/abs/2310.12127](http://arxiv.org/abs/2310.12127)

    本研究通过调查机器翻译模型中的性别偏见问题以及缓解性别偏见的方法来填补现有研究的空白。研究发现指导微调模型在默认为男性翻译上存在性别偏见，同时忽视了指示职业性别的代词，并提出了一些可行的缓解策略。

    

    最近的指导微调模型可在多个NLP任务中解决问题，其中机器翻译（MT）是一个突出的用例。然而，当前的研究通常集中在标准性能基准上，忽视了引人注目的公平和伦理考虑。在MT中，这可能导致性别错误的翻译，从而导致刻板印象和偏见的持续存在。在这项工作中，我们通过调查这些模型在机器翻译中是否存在性别偏见以及如何缓解性别偏见来填补这一空白。具体而言，我们在从英文到德文和西班牙文的WinoMT语料库上计算已建立的性别偏见指标。我们发现指导微调模型默认为男性屈从翻译，甚至忽视女性职业刻板印象。接下来，使用可解释性方法，我们揭示了模型系统性地忽视指示目标职业性别的代词在同时性别错误的翻译中。最后，根据可解释性的发现，我们提出了性别偏见缓解的策略，并将其应用于MT模型中。

    Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
    
[^7]: SHARCS：通过动态宽度子网络进行路由的高效Transformer

    SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])

    [http://arxiv.org/abs/2310.12126](http://arxiv.org/abs/2310.12126)

    SHARCS是一种高效的Transformer模型，通过动态宽度子网络进行路由，实现自适应推理和更高的效率，同时在各种分类任务中表现优越并且具有通用性。

    

    我们引入了SHARCS，用于自适应推理，考虑到输入样本的难度。SHARCS可以在任何Transformer网络上训练一个路由器，使模型能够将不同样本指向具有不同宽度的子网络。我们的实验证明：（1）在准确性与FLOPs之间，SHARCS在各种分类任务上的表现优于或补充了现有的每个样本自适应推理方法；（2）SHARCS在不同架构之间具有泛化能力，甚至可以应用于压缩和高效的Transformer编码器，以进一步提高其效率；（3）SHARCS可以提供2倍的推理加速，几乎不损失准确性。

    We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
    
[^8]: 使用电子健康记录自动预测患有精神疾病患者的死亡率

    Automatic prediction of mortality in patients with mental illness using electronic health records. (arXiv:2310.12121v1 [cs.LG])

    [http://arxiv.org/abs/2310.12121](http://arxiv.org/abs/2310.12121)

    本文研究了使用电子健康记录预测患有精神疾病患者的死亡率。研究发现随机森林和支持向量机模型在预测中表现优异，药物处方尤其是硫酸吗啡对预测起到关键作用。

    

    精神障碍影响全球数百万人的生活，不仅妨碍他们的日常生活，还明显缩短了预期寿命。本文利用电子健康记录 (EHR) 中的预测机器学习模型，解决了在患有精神诊断的患者中预测死亡率的持久挑战。从众所周知的临床 MIMIC-III 数据集中提取了患有精神疾病诊断的患者的数据，利用人口统计信息、处方信息和程序信息。使用了四种机器学习算法 (逻辑回归、随机森林、支持向量机和K近邻) ，结果表明随机森林和支持向量机模型表现优于其他模型，AUC得分为0.911。特征重要性分析显示药物处方，特别是硫酸吗啡，在预测中起到了关键作用。我们应用了各种机器学习算法来预测30天内的死亡率，随后进行特征重要性分析。

    Mental disorders impact the lives of millions of people globally, not only impeding their day-to-day lives but also markedly reducing life expectancy. This paper addresses the persistent challenge of predicting mortality in patients with mental diagnoses using predictive machine-learning models with electronic health records (EHR). Data from patients with mental disease diagnoses were extracted from the well-known clinical MIMIC-III data set utilizing demographic, prescription, and procedural information. Four machine learning algorithms (Logistic Regression, Random Forest, Support Vector Machine, and K-Nearest Neighbors) were used, with results indicating that Random Forest and Support Vector Machine models outperformed others, with AUC scores of 0.911. Feature importance analysis revealed that drug prescriptions, particularly Morphine Sulfate, play a pivotal role in prediction. We applied a variety of machine learning algorithms to predict 30-day mortality followed by feature importa
    
[^9]: 基于MMD的分布随机森林的变量重要性

    MMD-based Variable Importance for Distributional Random Forest. (arXiv:2310.12115v1 [stat.ML])

    [http://arxiv.org/abs/2310.12115](http://arxiv.org/abs/2310.12115)

    本文介绍了基于MMD距离和经典的drop and relearn原理的变量重要性算法，可以在分布随机森林中检测影响输出分布的变量，并且在实证性能上超越了竞争对手。

    

    分布随机森林（DRF）是一种灵活的基于森林的方法，用于估计给定输入变量的多元输出的全条件分布。在本文中，我们介绍了一种基于经典的drop and relearn原理和MMD距离的DRF变量重要性算法。传统的重要性度量只能发现对输出均值有影响的变量，而我们的算法可以更普遍地发现影响输出分布的变量。我们展示了引入的重要性度量是一致的，在真实数据和模拟数据上具有较高的实证性能，并且超越了竞争对手。特别地，我们的算法通过递归特征消除高效地选择变量，因此可以提供小型变量集合来构建准确的条件输出分布估计。

    Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
    
[^10]: 一则警示故事：关于参考数据在实证隐私防御中的作用

    A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses. (arXiv:2310.12112v1 [cs.CR])

    [http://arxiv.org/abs/2310.12112](http://arxiv.org/abs/2310.12112)

    本文研究了实证隐私防御中参考数据的作用和隐私问题，提出了一种基准防御方法，实现了模型效用和训练数据隐私的权衡。

    

    在隐私保护机器学习领域，已经提出了实证隐私防御作为一种解决方案，以实现在不显著降低模型效用的情况下，达到令人满意的训练数据隐私水平。大多数现有的防御成员推理攻击的方法假设可以访问参考数据，参考数据指的是来自训练数据相同（或类似）基础分布的附加数据集。尽管参考数据的使用很普遍，但之前的研究对于定义和评估参考数据隐私相当保守。由于模型效用和/或训练数据隐私的提升可能以牺牲参考数据隐私为代价，因此需要充分考虑这三个方面。在本文中，我们首先研究了之前的作品中参考数据的可用性及其隐私处理情况，并证明了对于公平比较防御方法来说参考数据的必要性。其次，我们提出了一种基准防御方法，它可以在模型效用和训练数据隐私之间进行权衡。

    Within the realm of privacy-preserving machine learning, empirical privacy defenses have been proposed as a solution to achieve satisfactory levels of training data privacy without a significant drop in model utility. Most existing defenses against membership inference attacks assume access to reference data, defined as an additional dataset coming from the same (or a similar) underlying distribution as training data. Despite the common use of reference data, previous works are notably reticent about defining and evaluating reference data privacy. As gains in model utility and/or training data privacy may come at the expense of reference data privacy, it is essential that all three aspects are duly considered. In this paper, we first examine the availability of reference data and its privacy treatment in previous works and demonstrate its necessity for fairly comparing defenses. Second, we propose a baseline defense that enables the utility-privacy tradeoff with respect to both trainin
    
[^11]: Monarch Mixer: 一个基于子二次广义矩阵相乘的简单架构

    Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture. (arXiv:2310.12109v1 [cs.LG])

    [http://arxiv.org/abs/2310.12109](http://arxiv.org/abs/2310.12109)

    Monarch Mixer (M2) is a new architecture that uses sub-quadratic primitive to scale along both sequence length and model dimension, achieving high hardware efficiency and matching the performance of existing models with fewer parameters.

    

    机器学习模型在序列长度和模型维度上的扩展越来越普遍，以达到更长的上下文和更好的性能。然而，现有的架构如Transformer在这两个方面的扩展都是二次的。我们问：是否有一种性能良好的架构可以在序列长度和模型维度上具有子二次的扩展性？我们引入了Monarch Mixer (M2)，一种使用相同子二次原语的新架构，该原语是一种简单而具有表达力的结构化矩阵，可以捕捉许多线性变换，在GPU上具有高硬件效率，并且具有子二次的扩展性。作为概念验证，我们在三个领域探索了M2的性能：非因果BERT模式的语言建模，ViT模式的图像分类和因果GPT模式的语言建模。对于非因果BERT模式的建模，M2在GLUE质量上与BERT-base和BERT-large相匹配，参数数量减少了多达27%

    Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters
    
[^12]: 《一个关于经纪交易的在线学习理论》

    An Online Learning Theory of Brokerage. (arXiv:2310.12107v1 [cs.LG])

    [http://arxiv.org/abs/2310.12107](http://arxiv.org/abs/2310.12107)

    该论文通过在线学习的理论研究了交易者间的经纪交易问题，提出了适用于不同信息披露情况下的算法，并证明了这些算法的最优性。

    

    我们从在线学习的角度研究了交易者间的经纪交易。在每一轮t，两个交易者带着他们的私人估值到达，经纪人提出一个交易价格。与在线学习文献中已经研究的其他双边交易问题不同的是，我们专注于没有指定买方和卖方角色的情况：每个交易者将根据商品的当前价格尝试买入或卖出。我们假设交易者的估值是从一个固定但未知的分布中独立同分布地抽取的。如果分布的密度受到某个常数M的限制，则对于任何时间段T：$\bullet$如果在每次交互之后揭示交易者的估值，我们提供了一个达到遗憾$M \log T$的算法，并且证明了这个速率是最优的，除了常数因子。$\bullet$如果仅在每次交互之后揭示他们的愿意按照提议的价格卖出或买入，我们提供了一个达到遗憾$\sqrt{M T}$的算法，并且证明了这个速率是最优的。

    We investigate brokerage between traders from an online learning perspective. At any round $t$, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good.  We assume the agents' valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant $M$, then, for any time horizon $T$:  $\bullet$ If the agents' valuations are revealed after each interaction, we provide an algorithm achieving regret $M \log T$ and show this rate is optimal, up to constant factors.  $\bullet$ If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret $\sqrt{M T}$ and show this rate is op
    
[^13]: 非侵入式自适应：面向多模态建模的输入中心参数高效微调

    Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])

    [http://arxiv.org/abs/2310.12100](http://arxiv.org/abs/2310.12100)

    这篇论文介绍了一种非侵入式的参数高效微调技术（AdaLink），通过只调整模型的外部参数而保持内部结构不变，实现了对多模态建模的竞争性能，这对于大规模语言模型和视觉语言模型的自适应和部署具有重要意义。

    

    大规模语言模型（LLMs）和视觉语言模型（VLMs）通过将参数数量从O（10^9）扩展到O（10^{12}）甚至更高水平，展现出在广泛任务上出色的性能。这样大规模的模型使得在给定感兴趣的任务上进行完全专业化模型的自适应和部署成为不可能。参数高效微调（PEFT）成为应对这些大型模型适应和服务挑战的一种有希望的方向。我们将PEFT技术分为两种类型：侵入式和非侵入式。侵入式PEFT技术直接改变模型的内部结构。虽然更灵活，但在训练和服务过程中引入了显著的复杂性。非侵入式PEFT技术保持内部结构不变，只调整模型的外部参数，如输入的嵌入。在这项工作中，我们将AdaLink描述为一种非侵入式PEFT技术，与SoTA侵入式PEFT（LoRA）和完整模型相比，实现了有竞争力的性能。

    Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model
    
[^14]: 关于由随机场参数化的PDE的深度自动编码器的潜在维度问题

    On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields. (arXiv:2310.12095v1 [cs.LG])

    [http://arxiv.org/abs/2310.12095](http://arxiv.org/abs/2310.12095)

    本文旨在通过对深度学习降阶模型的理论分析，扩展对由随机场参数化的PDE的理解。

    

    深度学习在设计部分微分方程的降阶模型中有着显著影响，可以解决传统方法难以处理的复杂问题。在这方面，深度自动编码器发挥着重要作用，通过利用神经网络的非线性能力，提供了一种极为灵活的工具来降低给定问题的维度。然而，在随机字段参数化的随机问题中，当前对深度学习降阶模型的理解主要基于经验证据，其理论分析目前仅限于依赖有限个（确定性）参数的PDE情况。本文的目的是通过提供一些理论论证，扩展现有文献。

    Deep Learning is having a remarkable impact on the design of Reduced Order Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited as a powerful tool for tackling complex problems for which classical methods might fail. In this respect, deep autoencoders play a fundamental role, as they provide an extremely flexible tool for reducing the dimensionality of a given problem by leveraging on the nonlinear capabilities of neural networks. Indeed, starting from this paradigm, several successful approaches have already been developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs). Nevertheless, when it comes to stochastic problems parameterized by random fields, the current understanding of DL-ROMs is mostly based on empirical evidence: in fact, their theoretical analysis is currently limited to the case of PDEs depending on a finite number of (deterministic) parameters. The purpose of this work is to extend the existing literature by providing some t
    
[^15]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^16]: 在步态中估计代谢成本的代谢能量模型的贡献成分

    Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait. (arXiv:2310.12083v1 [cs.LG])

    [http://arxiv.org/abs/2310.12083](http://arxiv.org/abs/2310.12083)

    本研究通过蒙特卡洛敏感性分析确定了代谢成本估算的关键参数和输入变量。通过分析功率相关参数的敏感性和使用不同输入特征训练的神经网络的准确性，揭示了代谢能量模型的贡献成分。

    

    目标：代谢成本是影响人类步态的主要因素之一，我们希望加深对代谢能量消耗模型的理解。因此，本文确定了影响准确估计代谢成本的参数和输入变量，例如肌肉或关节状态。方法：我们在蒙特卡洛敏感性分析中探讨了四个代谢能量消耗模型的参数。然后，我们通过计算的敏感性指标、生理环境和步态周期中得到的代谢率分析模型参数。在蒙特卡洛模拟中具有最高准确性的参数组合代表了一个准优化模型。在第二步中，我们通过分析使用不同输入特征训练的神经网络的准确性来研究输入参数和变量的重要性。结果：功率相关参数在敏感性分析和基于神经网络的特征选择中最具影响力。

    Objective: As metabolic cost is a primary factor influencing humans' gait, we want to deepen our understanding of metabolic energy expenditure models. Therefore, this paper identifies the parameters and input variables, such as muscle or joint states, that contribute to accurate metabolic cost estimations. Methods: We explored the parameters of four metabolic energy expenditure models in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters by their calculated sensitivity indices, physiological context, and the resulting metabolic rates during the gait cycle. The parameter combination with the highest accuracy in the Monte Carlo simulations represented a quasi-optimized model. In the second step, we investigated the importance of input parameters and variables by analysing the accuracy of neural networks trained with different input features. Results: Power-related parameters were most influential in the sensitivity analysis and the neural network-based feature sel
    
[^17]: 形状和非形状神经网络的微分方程缩放极限

    Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks. (arXiv:2310.12079v1 [stat.ML])

    [http://arxiv.org/abs/2310.12079](http://arxiv.org/abs/2310.12079)

    最近的研究发现，对于具有形状激活函数的神经网络，其缩放极限可以由微分方程描述。然而，关于未经形状处理的神经网络的信息尚不明确。本文研究了两种未经形状处理的网络，发现它们也可以由类似的微分方程来描述，并给出了它们的一些特征。

    

    最近对具有形状激活函数（即随着网络规模增大而缩放的激活函数）的神经网络进行的分析表明，它们具有由微分方程描述的缩放极限。然而，这些结果不预先告诉我们关于“普通”非形状网络的任何信息，其中激活函数在网络规模增大时保持不变。在本文中，我们针对两种类型的非形状网络找到了类似的基于微分方程的渐近特征描述。首先，我们证明以下两种架构在初始化时会收敛到相同的无限深度和宽度极限：（i）带有残差分支上的 $d^{-1/2}$ 因子的全连接 ResNet，其中 $d$ 是网络的深度；（ii）带有深度 $d \ll$ 宽度 $n$ 和形状 ReLU 激活函数 (activation) 的多层感知机 (MLP)，以 $d^{-1/2}$ 的速率。其次，对于初始化的非形状 MLP，我们推导了层间相关性的一阶渐近修正。特别地，如果 $\rho_\ell$ 是第 $\ell$ 层的相关性，则...

    Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.  Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.  Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer
    
[^18]: 一次性模仿学习：姿态估计的视角

    One-Shot Imitation Learning: A Pose Estimation Perspective. (arXiv:2310.12077v1 [cs.RO])

    [http://arxiv.org/abs/2310.12077](http://arxiv.org/abs/2310.12077)

    本文研究了一种具有挑战性的模仿学习方法，只有一次演示、不进行数据收集和没有先前的任务或对象知识的情况下，将模仿学习形式化为轨迹传输和未知对象姿态估计的组合。通过对真实任务进行深入研究，探讨了最先进的未知对象姿态估计器在一次性模仿学习中的表现，并深入分析了相机校准、姿态估计误差和空间泛化对任务成功率的影响。

    

    本文研究在具有挑战性的条件下进行模仿学习：（1）仅一次演示，（2）没有进一步的数据收集，（3）没有先前的任务或对象知识。我们展示了在这些约束条件下，如何将模仿学习形式化为轨迹传输和未知对象姿态估计的组合。为了探索这个想法，我们对最先进的未知对象姿态估计器在十个真实任务上进行了深入研究，并深入研究了相机校准、姿态估计误差和空间泛化对任务成功率的影响。有关视频，请访问https://www.robot-learning.uk/pose-estimation-perspective。

    In this paper, we study imitation learning under the challenging setting of: (1) only a single demonstration, (2) no further data collection, and (3) no prior task or object knowledge. We show how, with these constraints, imitation learning can be formulated as a combination of trajectory transfer and unseen object pose estimation. To explore this idea, we provide an in-depth study on how state-of-the-art unseen object pose estimators perform for one-shot imitation learning on ten real-world tasks, and we take a deep dive into the effects that camera calibration, pose estimation error, and spatial generalisation have on task success rates. For videos, please visit https://www.robot-learning.uk/pose-estimation-perspective.
    
[^19]: 用于科学数据的Transformer：天文学家的教学综述

    Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])

    [http://arxiv.org/abs/2310.12069](http://arxiv.org/abs/2310.12069)

    本综述旨在向科学家介绍Transformer的应用，包括自然语言处理和自注意机制。此外，还介绍了在天文学中应用于时间序列和成像数据的具体情况，并提供了常见问题解答部分。

    

    与ChatGPT和相关生成型人工智能产品相关的深度学习架构被称为Transformer。最初应用于自然语言处理，Transformer和它们利用的自注意机制在自然科学领域引起了广泛关注。本教学和非正式综述的目标是向科学家介绍Transformer。我们的教学和非正式综述包括自注意机制的数学基础，对原始Transformer架构的描述，以及在天文学中应用于时间序列和成像数据的一节。我们还包括了一个常见问题解答部分，供那些对生成型人工智能感兴趣并希望开始使用Transformer进行研究的读者参考。

    The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.
    
[^20]: GAN中黑盒训练数据识别的探测器网络研究

    Black-Box Training Data Identification in GANs via Detector Networks. (arXiv:2310.12063v1 [cs.LG])

    [http://arxiv.org/abs/2310.12063](http://arxiv.org/abs/2310.12063)

    本研究探索了在黑盒设置中使用GAN时的隐私问题，通过引入一套攻击来识别训练数据成员身份，本文提供了对版权和数据隐私方面的重要洞见。

    

    从它们问世以来，生成对抗网络（GAN）一直是流行的生成模型，可用于图像、音频、视频和表格数据。本文研究了在已训练好的GAN以及来自基础分布的新样本的情况下，攻击者能否有效地识别给定点是否属于GAN的训练数据。这对于版权相关的原因很有意义，用户可能想确定他们的版权数据是否被用来训练GAN，以及对数据隐私的研究也很有意义，其中检测训练集成员身份的能力被称为成员隐私攻击。与大多数先前的工作不同，本文研究了在黑盒设置中使用GAN的隐私影响，攻击者只能访问生成器的样本，而不能访问鉴别器的样本。我们引入了一套针对黑盒设置中GAN的成员隐私攻击，并评估了我们的方法。

    Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our 
    
[^21]: 在反强化学习中通过最优传输理论理解奖励不确定性

    Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning. (arXiv:2310.12055v1 [cs.LG])

    [http://arxiv.org/abs/2310.12055](http://arxiv.org/abs/2310.12055)

    本文利用最优传输理论和几何表示方法来解决反强化学习中的奖励不确定性问题，在高维情况下具有鲁棒性，并通过量化奖励不确定性和识别奖励函数的中心表示提供了一种结构化方法。

    

    在反强化学习（IRL）中，主要目标是从观察到的专家行为中推断出潜在的奖励函数，不仅要解释给定的数据，还要泛化到未知场景。这确保了对奖励不确定性的鲁棒性，其中多个奖励函数可以同样解释相同的专家行为。虽然在解决这个问题上已经做出了重大努力，但是当前的方法在高维问题上面临挑战，并且缺乏几何基础。本文利用最优传输（OT）理论，为这些挑战提供了一种新的视角。通过利用OT的Wasserstein距离，我们建立了一个几何框架，可以量化奖励不确定性并识别奖励函数的中心表示或质心。这些观点为以几何解释为基础的鲁棒IRL方法铺平了道路，提供了一种结构化的方法来处理高维奖励不确定性。

    In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensiona
    
[^22]: 基于机器学习的智能农业营养应用时间推荐：一种大规模数据挖掘方法

    Machine Learning-based Nutrient Application's Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach. (arXiv:2310.12052v1 [cs.LG])

    [http://arxiv.org/abs/2310.12052](http://arxiv.org/abs/2310.12052)

    本文研究了基于机器学习的智能农业营养应用时间推荐方法。通过预测整个季节所需的肥料数量，并根据天气条件和土壤特性调整肥料量，以促进经济高效和环境友好的农业。研究还探讨了施肥应用与天气数据对作物产量的影响。

    

    本研究讨论了数据分析在监测农作物施肥应用中的重要作用。不准确的施肥决策会导致昂贵的后果，阻碍粮食生产，并造成环境危害。我们提出了一种解决方案，通过确定整个季节所需的肥料数量来预测营养应用。该解决方案建议根据天气条件和土壤特性调整肥料量，以促进经济高效和环境友好的农业。收集的数据集是高维度和异构的。我们的研究在决策过程的语境中研究了大规模异构数据集，包括数据收集和分析。我们还以冬小麦作物为案例研究了施肥应用与天气数据对作物产量的影响。通过理解本地背景和地理因素，我们希望稳定甚至减少需求。

    This study addresses the vital role of data analytics in monitoring fertiliser applications in crop cultivation. Inaccurate fertiliser application decisions can lead to costly consequences, hinder food production, and cause environmental harm. We propose a solution to predict nutrient application by determining required fertiliser quantities for an entire season. The proposed solution recommends adjusting fertiliser amounts based on weather conditions and soil characteristics to promote cost-effective and environmentally friendly agriculture. The collected dataset is high-dimensional and heterogeneous. Our research examines large-scale heterogeneous datasets in the context of the decision-making process, encompassing data collection and analysis. We also study the impact of fertiliser applications combined with weather data on crop yield, using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aspire to stabilise or even reduce the dema
    
[^23]: 以机器学习为基础的代理模型在贝叶斯逆问题方法中的应用

    Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems. (arXiv:2310.12046v1 [stat.ML])

    [http://arxiv.org/abs/2310.12046](http://arxiv.org/abs/2310.12046)

    本文探讨了在贝叶斯逆问题中，使用机器学习为基础的代理模型，以提高计算效率和处理数值挑战问题。

    

    神经网络已成为一种强大的工具，作为代理模型，在增加计算效率的同时为科学问题提供数值解。这种效率在时间至关重要的数值挑战问题或需要评估许多类似分析方案的情况下具有优势。一个特定的科学兴趣领域是逆问题的设置，其中我们知道系统的正向动态由偏微分方程描述，任务是根据这些动态的（潜在有噪声的）观测来推断系统的性质。我们考虑推断给定2D声波方程的嘈杂解的方块域中波源的位置的逆问题。在假设为高斯噪声的情况下，可以构造源位置的似然函数，每个评估都需要对系统进行一次正向模拟。使用标准的神经网络作为代理模型

    Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model make
    
[^24]: 一个理论框架来理解从人类偏好中学习的一般方法

    A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])

    [http://arxiv.org/abs/2310.12036](http://arxiv.org/abs/2310.12036)

    本文研究了学习从人类偏好中学习的实际算法的理论基础，推导出一个新的一般目标，绕过了两个重要的近似。这种方法允许直接从收集的数据中学习策略而无需奖励模型的训练。

    

    目前从人类偏好中学习的流行方法依赖于两个重要的近似：第一假设可以用逐点奖励替代成对偏好。第二个假设是在这些逐点奖励上训练的奖励模型可以从收集到的数据泛化到策略采样的超出分布的数据。最近，提出了一种称为直接偏好优化(DPO)的方法，该方法绕过了第二个近似，并直接从收集的数据中学习策略而无需奖励模型阶段。然而，这种方法仍然严重依赖于第一个近似。在本文中，我们试图对这些实际算法进行更深入的理论理解。特别地，我们推导出了一个新的一般目标，称为ΨPO，用于从人类偏好中学习，该目标以成对偏好的形式表达，因此绕过了这两个近似。这个新的一般目标使我们能够进行一种新的从训练数据直接学习策略的方法而无需进行奖励模型的训练。

    The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
    
[^25]: 在协变量偏移下使用密度估计进行一致性药物属性预测

    Conformal Drug Property Prediction with Density Estimation under Covariate Shift. (arXiv:2310.12033v1 [cs.LG])

    [http://arxiv.org/abs/2310.12033](http://arxiv.org/abs/2310.12033)

    提出了一种名为CoDrug的方法，该方法利用基于能量的模型和核密度估计来解决协变量偏移问题，从而实现一致性药物属性预测。

    

    在药物发现中，通过计算模型确认药品性质的预测需要进行昂贵的湿实验。因此，获取可靠的不确定性估计对于优先选择药物分子进行后续实验验证至关重要。一致性预测是一种有保证覆盖率的为分子性质创建预测集的有希望的工具。然而，在药物发现任务中，一致性预测的交换性假设往往会受到协变量偏移的挑战：大多数数据集包含有限的标记数据，这些数据可能不足以代表从中提取分子的庞大化学空间。为解决这个问题，我们提出了一种方法称为CoDrug，该方法使用基于能量的模型利用训练数据和无标记数据，并使用核密度估计来评估分子集合的密度。然后，利用估计的密度来加权分子样本以构建预测集并修正以下的

    In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying fo
    
[^26]: LMC多任务高斯过程模型的精确和高效解决方案

    Exact and efficient solutions of the LMC Multitask Gaussian Process model. (arXiv:2310.12032v1 [cs.LG])

    [http://arxiv.org/abs/2310.12032](http://arxiv.org/abs/2310.12032)

    LMC多任务高斯过程模型的精确解决方案表明，只需对噪声模型进行温和假设，即可实现高效计算。通过引入完整参数化的“投影LMC”模型和边缘似然函数表达式，展示了该方法相对于未经处理的方法的优异性能。

    

    线性共同关联模型（LMC）是一种非常通用的多任务高斯过程模型，用于回归或分类。虽然其表达能力和概念简单性很有吸引力，但朴素实现在数据点数量和任务数量方面具有立方复杂度，使得对大多数应用来说，必须进行近似处理。然而，最近的研究表明，在某些条件下，该模型的潜在过程可以解耦，导致仅与所述过程数量呈线性复杂度。我们在这里扩展了这些结果，从最一般的假设中展示了在LMC的高效精确计算所需的唯一条件是对噪声模型进行温和假设。我们引入了结果的完整参数化“投影LMC”模型，并给出了边缘似然函数的表达式，以实现高效的优化。我们对合成数据进行了参数研究，展示了我们方法相对于未经处理的方法的优异性能。

    The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unr
    
[^27]: SegmATRon: 用于室内环境的具有自适应语义分割的体验感知模型

    SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment. (arXiv:2310.12031v1 [cs.CV])

    [http://arxiv.org/abs/2310.12031](http://arxiv.org/abs/2310.12031)

    SegmATRon是一种自适应变换模型，用于室内图像的语义分割。通过在多张图像上进行推断时权重的自适应调整，可以提高语义分割的质量。在室内环境中使用代理的行为获取额外图像的方法在实验中证明是有效的。

    

    本文提出了一种名为SegmATRon的自适应变换模型，用于室内图像的语义分割。其独特之处在于使用混合多组分损失函数在多张图像上进行推断的模型权重适应性调整。我们在逼真的Habitat和AI2-THOR合成模拟器中的数据集上研究了该模型。我们证明，在室内环境中利用代理的行为获取额外图像可以提高语义分割的质量。所提出方法的代码和数据集可在https://github.com/wingrune/SegmATRon上公开获取。

    This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation. The code of the proposed approach and datasets are publicly available at https://github.com/wingrune/SegmATRon.
    
[^28]: 无参数离散选择实验与机器学习引导的自适应设计

    Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design. (arXiv:2310.12026v1 [stat.ML])

    [http://arxiv.org/abs/2310.12026](http://arxiv.org/abs/2310.12026)

    提出了无参数离散选择实验与机器学习引导的自适应设计方法GBS，通过自适应构建配对比较问题来满足消费者的偏好，不需要参数化效用模型，可以扩展到具有数百个属性的产品，并在准确性和样本效率方面具有优势。

    

    为了满足消费者的偏好，设计产品对于企业的成功至关重要。我们提出了基于梯度的调查（GBS），这是一种用于多属性产品设计的离散选择实验。该实验通过一系列针对部分配置进行的配对比较来获取消费者的偏好。GBS根据受访者的先前选择自适应地构建配对比较问题。与传统的随机效用最大化范式不同，GBS不需要参数化效用模型，因此对模型规范错误具有鲁棒性。通过将机器学习和实验设计相结合，GBS可扩展到具有数百个属性的产品，并且可以为异质消费者设计个性化产品。我们通过模拟实验证明了GBS相比于现有的参数化和非参数化方法在准确性和样本效率方面的优势。

    Designing products to meet consumers' preferences is essential for a business's success. We propose the Gradient-based Survey (GBS), a discrete choice experiment for multiattribute product design. The experiment elicits consumer preferences through a sequence of paired comparisons for partial profiles. GBS adaptively constructs paired comparison questions based on the respondents' previous choices. Unlike the traditional random utility maximization paradigm, GBS is robust to model misspecification by not requiring a parametric utility model. Cross-pollinating the machine learning and experiment design, GBS is scalable to products with hundreds of attributes and can design personalized products for heterogeneous consumers. We demonstrate the advantage of GBS in accuracy and sample efficiency compared to the existing parametric and nonparametric methods in simulations.
    
[^29]: 连续学习中的贝叶斯流网络

    Bayesian Flow Networks in Continual Learning. (arXiv:2310.12001v1 [cs.LG])

    [http://arxiv.org/abs/2310.12001](http://arxiv.org/abs/2310.12001)

    连续学习中的贝叶斯流网络（BFNs）是一种具有通用生成建模能力的方法，通过结合神经网络和贝叶斯推断，在非稳态数据上取得了良好的实验结果。

    

    最近，贝叶斯流网络（BFNs）被提出作为通用生成建模中非常有前景的方向之一，具有学习任何数据类型的能力。他们的强大之处在于神经网络和贝叶斯推断的表达能力，使它们适用于连续学习的背景下。我们深入研究了BFNs的机制，并进行实验证实了它在非稳态数据上的生成能力。

    Bayesian Flow Networks (BFNs) has been recently proposed as one of the most promising direction to universal generative modelling, having ability to learn any of the data type. Their power comes from the expressiveness of neural networks and Bayesian inference which make them suitable in the context of continual learning. We delve into the mechanics behind BFNs and conduct the experiments to empirically verify the generative capabilities on non-stationary data.
    
[^30]: Vecchia-Laplace近似法在潜在高斯过程模型中的迭代方法

    Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])

    [http://arxiv.org/abs/2310.12000](http://arxiv.org/abs/2310.12000)

    这篇文章介绍了用于潜在高斯过程模型中的Vecchia-Laplace近似法的迭代方法，相比于传统的Cholesky分解方法，可以显著加快计算速度。

    

    潜在高斯过程（GP）模型是灵活的概率非参数函数模型。Vecchia近似是用于克服大数据计算瓶颈的准确近似方法，Laplace近似是一种快速方法，可以近似非高斯似然函数的边缘似然和后验预测分布，并具有渐近收敛保证。然而，当与直接求解方法（如Cholesky分解）结合使用时，Vecchia-Laplace近似的计算复杂度增长超线性地随样本大小增加。因此，与Vecchia-Laplace近似计算相关的运算在通常情况下是最准确的大型数据集时会变得非常缓慢。在本文中，我们提出了几种用于Vecchia-Laplace近似推断的迭代方法，相比于基于Cholesky的计算，可以大大加快计算速度。我们对我们的方法进行了分析。

    Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
    
[^31]: 通过联合子空间估计消除神经网络表示中的错误概念

    Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation. (arXiv:2310.11991v1 [cs.LG])

    [http://arxiv.org/abs/2310.11991](http://arxiv.org/abs/2310.11991)

    该论文提出了一种通过联合子空间估计从神经网络表示中消除错误概念的迭代算法，并在计算机视觉和自然语言处理的基准数据集上展示了其优越性能。

    

    神经网络中的错误相关性经常会影响到模型在样本外的泛化能力。常见的策略是通过从神经网络表示中消除错误概念来缓解这个问题。现有的错误概念消除方法往往过于激进，不经意间会消除与模型主要任务相关的特征，从而影响模型性能。我们提出了一种迭代算法，通过共同识别神经网络表示中的两个低维正交子空间来分离错误和主要任务的概念。我们在计算机视觉（Waterbirds，CelebA）和自然语言处理（MultiNLI）的基准数据集上评估了该算法，并表明它优于现有的概念消除方法。

    Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
    
[^32]: 带有外部引导的图像聚类

    Image Clustering with External Guidance. (arXiv:2310.11989v1 [cs.LG])

    [http://arxiv.org/abs/2310.11989](http://arxiv.org/abs/2310.11989)

    这项工作提出了一种新的聚类方法，利用外部知识来引导聚类。通过利用WordNet的文本语义，该方法能够提高图像聚类的效果。

    

    聚类的核心是融入先前的知识来构建监督信号。从基于数据紧密性的经典k均值到最近的基于自监督引导的对比聚类，聚类方法的进步与监督信号的发展内在地相对应。目前，很多工作已经致力于从数据中挖掘内部监督信号。然而，丰富的外部知识，例如语义描述，自然地促进了聚类，却被遗憾地忽视了。在这项工作中，我们提出利用外部知识作为新的监督信号来引导聚类，即使它似乎与给定的数据无关。为了实现和验证我们的想法，我们设计了一种外部引导的聚类方法（文本辅助聚类，TAC），它利用WordNet的文本语义来促进图像聚类。具体而言，TAC首先选择并检索最能区分图像的WordNet名词以增强聚类的效果。

    The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance th
    
[^33]: 一种有限时间视角下的主动水平集估计方法

    A Finite-Horizon Approach to Active Level Set Estimation. (arXiv:2310.11985v1 [cs.LG])

    [http://arxiv.org/abs/2310.11985](http://arxiv.org/abs/2310.11985)

    这项研究提出了一种有限时间视角下的主动水平集估计方法，在一维和高维情况下均表现出较高的效果，能够平衡估计误差和已移动距离，具有推广性。

    

    我们考虑了在空间采样中进行主动学习的问题，用于水平集估计（LSE）。该问题的目标是尽快定位所有符合给定阈值上/下的感兴趣函数的区域。我们提出了一种有限时间的搜索过程，以在一维中执行LSE，同时在固定数量样本的情况下，最优地平衡最终估计误差和已移动的距离。通过调整参数来权衡估计准确性和已移动的距离。我们证明了得出的优化问题可以闭式求解，并且所得策略推广了现有的解决方法。然后，我们展示了如何在流行的高斯过程模型下使用这种方法来进行更高维度的水平集估计。对合成数据的实证结果表明，随着旅行成本的增加，我们的方法以非迭代距离处理的能力使其能够显著改善已有方法的能力。

    We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the s
    
[^34]: 从插值到外推：算术Transformer的完整长度泛化

    From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])

    [http://arxiv.org/abs/2310.11984](http://arxiv.org/abs/2310.11984)

    本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。

    

    自从提出以来，Transformer模型在各种任务中展现出了优秀的性能。然而，在算法任务中，长度泛化仍存在一些未解决的问题。在本文中，我们研究了Transformer模型在学习算术算法（如加法和乘法）方面的内在能力。通过实验证明和注意力分析，我们确定了实现最佳长度泛化的几个关键因素。我们展示了Transformer模型能够通过目标指向偏置来泛化到长长度。然后，我们引入了Attention Bias Calibration（ABC），这是一个校准阶段，使模型能够自动学习适当的注意力偏置，我们将其与相对位置编码的机制联系起来。我们证明使用ABC，Transformer模型可以在某些算术任务上实现前所未有的完美长度泛化。

    Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
    
[^35]: 可以通过分组缩放提高机器学习回归的预测不确定性的一致性和适应性吗？

    Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?. (arXiv:2310.11978v1 [stat.ML])

    [http://arxiv.org/abs/2310.11978](http://arxiv.org/abs/2310.11978)

    本文研究了分组缩放方法（BVS）的几种改进方法，探索了使用替代损失函数和基于输入特征的分组方案来提高机器学习回归的预测不确定性的一致性和适应性。

    

    最近，分组方差缩放（BVS）被提出作为一种用于机器学习回归问题的预测不确定性的事后校准方法，能够比统一方差（或温度）缩放更有效地进行校正。原始版本的BVS使用基于不确定性的分组，旨在提高条件上的校准性，即一致性。本文探讨了BVS的几种改进方法，特别是在损失函数和基于输入特征（X）的分组方案上进行改进，以提高适应性，即在给定X的条件下进行校准性。将BVS及其改进方案在预测原子化能的基准数据集上进行了性能测试，并与保序回归的结果进行了比较。

    Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
    
[^36]: 通过群体不变性学习提高与人类偏好的对齐的泛化能力

    Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])

    [http://arxiv.org/abs/2310.11971](http://arxiv.org/abs/2310.11971)

    该论文提出了一种通过强化学习实现在不同数据组或领域中学习一致策略的方法，该方法可以提高AI助手对不同领域的泛化能力，并更好地与人类偏好对齐。

    

    基于语言模型(LLMs)的AI助手的成功在于强化学习从人类反馈中, 使生成的回答更加与人类偏好一致. 作为通用AI助手, 人们越来越期望它们在不同领域中表现一致. 然而, 先前的工作表明,强化学习(RL)经常利用捷径以获得较高的奖励, 忽略了具有挑战性的样本. 这种对快速奖励收益的关注不仅削弱了训练的稳定性, 也削弱了模型对新的未见数据的泛化能力. 在这项工作中, 我们提出了一种新颖的方法, 可以通过RL在不同数据组或领域中学习一致的策略. 鉴于获得群体标注的挑战, 我们的方法会自动将数据分类到不同的组中, 有意地最大化性能差异. 然后, 我们优化策略以在具有挑战性的组中表现良好. 最后, 利用已建立的

    The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
    
[^37]: 引入一种适用于可访问性访谈转录的界面——aTrain

    Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews. (arXiv:2310.11967v1 [cs.SD])

    [http://arxiv.org/abs/2310.11967](http://arxiv.org/abs/2310.11967)

    aTrain是一个开源的离线音频转录工具，支持多语言转录并具有CPU和NVIDIA GPU支持。它适用于研究人员使用定性数据，提供简单易用的界面，并能与常用的定性数据分析软件集成。

    

    aTrain是一个开源的离线工具，可用于以多种语言进行音频数据转录，并支持CPU和NVIDIA GPU。它专门为使用从各种形式的语音互动中生成的定性数据的研究人员设计。aTrain不需要编程技能，可在大多数计算机上运行，不需要互联网连接，并经过验证不会上传数据到任何服务器。aTrain将OpenAI的Whisper模型与说话人识别相结合，提供与常用的定性数据分析软件工具MAXQDA和ATLAS.ti集成的输出。它有一个易于使用的图形界面，并通过Microsoft Store提供作为Windows应用程序，研究人员可以方便地进行安装。源代码在GitHub上免费提供。

    aTrain is an open-source and offline tool for transcribing audio data in multiple languages with CPU and NVIDIA GPU support. It is specifically designed for researchers using qualitative data generated from various forms of speech interactions with research participants. aTrain requires no programming skills, runs on most computers, does not require an internet connection, and was verified not to upload data to any server. aTrain combines OpenAI's Whisper model with speaker recognition to provide output that integrates with the popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an easy-to-use graphical interface and is provided as a Windows-App through the Microsoft Store allowing for simple installation by researchers. The source code is freely available on GitHub. Having developed aTrain with a focus on speed on local computers, we show that the transcription time on current mobile CPUs is around 2 to 3 times the duration of the audio file using the highest-
    
[^38]: 利用机器学习进行卫星柔性有效载荷配置

    Flexible Payload Configuration for Satellites using Machine Learning. (arXiv:2310.11966v1 [cs.LG])

    [http://arxiv.org/abs/2310.11966](http://arxiv.org/abs/2310.11966)

    本文使用机器学习方法解决了卫星通信中的无线资源管理问题，在处理异构流量场景时，通过将目标和约束集成到机器学习算法中，实现了有效的资源分配，并通过上下文感知的机器学习度量综合考虑了通信系统整体性能。

    

    卫星通信对于现代连接至关重要，它扩大了陆地网络无法覆盖的海上、航空和偏远地区的接入范围。当前的地球同步轨道卫星系统使用多波束覆盖并通过分频重用来均匀分配功率和带宽。然而，最近的研究揭示了这种方法在异构流量场景中的局限性，导致了低效。为了解决这个问题，本文提出了一种基于机器学习的无线资源管理（RRM）方法。我们将RRM任务视为回归机器学习问题，将RRM目标和约束集成到机器学习算法的损失函数中，目标是将其最小化。此外，我们引入了一种上下文感知的机器学习度量标准，评估机器学习模型的性能，并考虑其资源分配决策对通信系统整体性能的影响。

    Satellite communications, essential for modern connectivity, extend access to maritime, aeronautical, and remote areas where terrestrial networks are unfeasible. Current GEO systems distribute power and bandwidth uniformly across beams using multi-beam footprints with fractional frequency reuse. However, recent research reveals the limitations of this approach in heterogeneous traffic scenarios, leading to inefficiencies. To address this, this paper presents a machine learning (ML)-based approach to Radio Resource Management (RRM).  We treat the RRM task as a regression ML problem, integrating RRM objectives and constraints into the loss function that the ML algorithm aims at minimizing. Moreover, we introduce a context-aware ML metric that evaluates the ML model's performance but also considers the impact of its resource allocation decisions on the overall performance of the communication system.
    
[^39]: 快速多极化注意力：一种用于长序列的分治注意力机制

    Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])

    [http://arxiv.org/abs/2310.11960](http://arxiv.org/abs/2310.11960)

    提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。

    

    基于Transformer的模型已在许多领域取得了最先进的性能。然而，自注意力对于输入长度的二次复杂度限制了Transformer模型在长序列上的适用性。为了解决这个问题，我们提出了快速多极化注意力，一种使用分治策略来减少注意力时间和内存复杂度的新型注意力机制，将长度为n的序列的注意力复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。这种分层方法将查询、键和值分为O(log n)级的分辨率，较远距离的组群越来越大，并学习计算组群数量的权重。因此，以高效分层的方式在较低的分辨率中考虑远离彼此的标记之间的相互作用。快速多极化注意力的总体复杂度为O(n)或O(n log n)。

    Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
    
[^40]: 一种用于时间序列分析的多尺度分解MLP-Mixer

    A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis. (arXiv:2310.11959v1 [cs.LG])

    [http://arxiv.org/abs/2310.11959](http://arxiv.org/abs/2310.11959)

    我们提出了一种名为MSD-Mixer的多尺度分解MLP-Mixer模型，它能够将时间序列分解成不同的组成部分，并在不同的层次中表示这些组成部分。我们还提出了一种新颖的时间拼接方法，以处理多尺度的时间模式和通道间的依赖关系。我们的模型能够比现有方法更好地进行时间序列分析。

    

    时间序列数据通常具有独特的组成和复杂的多尺度时间变化，需要在其分析中特别考虑分解和多尺度建模。现有的深度学习方法只适用于单变量时间序列，并且对子序列级别的建模和分解不够充分。为了解决这个问题，我们提出了MSD-Mixer，一种多尺度分解的MLP-Mixer，它学会了将输入的时间序列明确地分解成不同的组成部分，并在不同的层次中表示这些组成部分。为了处理多尺度的时间模式和通道间的依赖关系，我们提出了一种新颖的时间拼接方法，将时间序列建模为多尺度子序列，即patches，并使用MLPs来组合patches内部和patches间的变化以及通道间的相关性。此外，我们提出了一个损失函数来约束分解残差的幅度和自相关性，以实现完整的分解。

    Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition 
    
[^41]: 用勺子舀空海洋：我们应该编辑模型吗？

    Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])

    [http://arxiv.org/abs/2310.11958](http://arxiv.org/abs/2310.11958)

    这项研究质疑了直接模型编辑作为纠正LLM生成中事实错误的方法，并提出了与之类似但更为明确的三种替代方法。虽然模型编辑在提升模型可解释性方面有潜力，但不能被视为解决LLMs固有缺点的系统性方法，因为它存在加强模型可信性观念的风险。

    

    我们对直接模型编辑作为纠正LLM生成中的事实错误的方法提出了质疑。我们将模型编辑与追求更明确目标的三种类似但不同的方法进行对比：（1）基于检索的架构，将事实记忆与LLMs所体现的推理和语言能力解耦；（2）概念擦除方法，旨在防止生成文本中的系统偏见；（3）归属方法，旨在将生成结果与已确定的文本来源连接起来。我们认为，不能将直接模型编辑作为解决LLMs固有缺点的系统性方法，并且虽然它在改进模型可解释性方面具有潜力，但它通过加强模型可信性的观念而存在风险。我们呼吁在LLM部署过程中谨慎推广和应用模型编辑，并负责任地限制LLMs的使用案例，以不依赖....

    We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying o
    
[^42]: 将连续学习重塑为序列建模

    Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])

    [http://arxiv.org/abs/2310.11952](http://arxiv.org/abs/2310.11952)

    本文将连续学习重塑为序列建模问题，并提出了利用序列模型进行连续学习的方法。通过采用元连续学习框架，需要对序列模型进行多次连续学习episode上的元级训练。实验证明序列模型可以成为一种吸引人的通用连续学习解决方案。

    

    在这项工作中，我们旨在建立机器学习研究中两个重要领域之间的强连接：连续学习和序列建模。也就是说，我们提出将连续学习作为序列建模问题进行表述，从而可以利用先进的序列模型进行连续学习。在这个框架下，连续学习过程成为序列模型的前向传播。通过采用元连续学习(MCL)框架，我们可以在多个连续学习episode上对序列模型进行元级训练。作为我们新框架的一个具体示例，我们展示了将Transformer及其高效变体应用于MCL方法。我们在包括分类和回归的七个基准测试上的实验证明了序列模型可以成为一种吸引人的通用连续学习解决方案。

    In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
    
[^43]: 太好不像是真的：人类活动识别中的性能评估过高的问题

    Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition. (arXiv:2310.11950v1 [cs.LG])

    [http://arxiv.org/abs/2310.11950](http://arxiv.org/abs/2310.11950)

    本文发现在人类活动识别中存在性能评估过高的问题，传统方法中的数据分割和交叉验证导致了结果的偏见。这个问题在最新的研究中很常见，但往往被忽视。不正确的结果会导致报告较低准确度的论文更难发表。

    

    当今，在人类活动识别（HAR）领域有一些标准和既定程序。然而，一些传统方法会导致精度被高估，特别是使用滑动窗口进行数据分割的方法以及标准的随机k折交叉验证方法会产生偏见结果。对过去的文献和现代研究的分析显示，这些方法在HAR领域的最新研究中很常见。我们有必要引起科学界对这个问题的关注，因为它的负面影响被忽视了。否则，发表偏见结果的论文将会报告较低的准确度，正确的无偏方法更难以发表。通过对不同类型的数据集和不同类型的分类模型进行多次实验，我们可以证明这个问题的存在，并且无论方法和数据集如何，这个问题都存在。

    Today, there are standard and well established procedures within the Human Activity Recognition (HAR) pipeline. However, some of these conventional approaches lead to accuracy overestimation. In particular, sliding windows for data segmentation followed by standard random k-fold cross validation, produce biased results. An analysis of previous literature and present-day studies, surprisingly, shows that these are common approaches in state-of-the-art studies on HAR. It is important to raise awareness in the scientific community about this problem, whose negative effects are being overlooked. Otherwise, publications of biased results lead to papers that report lower accuracies, with correct unbiased methods, harder to publish. Several experiments with different types of datasets and different types of classification models allow us to exhibit the problem and show it persists independently of the method or dataset.
    
[^44]: 可解释的光谱变分自编码器（ISVAE）用于时间序列聚类

    Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering. (arXiv:2310.11940v1 [stat.ML])

    [http://arxiv.org/abs/2310.11940](http://arxiv.org/abs/2310.11940)

    引入了一个可解释的瓶颈，称为滤波器组（FB），用于时间序列聚类的光谱变分自编码器（ISVAE）。通过约束VAE的辨识能力，这个模型能够学习到具有增强可解释性和聚类能力的新编码f_0，并呈现为一个动态的分层树。同时，还提出了与滤波器组结构对称对齐的定制解码器结构，用于处理复杂的数据配置。

    

    最好的编码是具有可解释性的。在本研究中，我们引入了一种新颖的模型，在变分自编码器（VAE）的初期引入了一个可解释的瓶颈，称为滤波器组（FB）。这种安排迫使VAE关注输入信号中最有信息的片段，促进了新编码f_0的学习，使其在传统的潜变量空间上具有增强的可解释性和聚类能力。通过有意地约束VAE使用这个滤波器组，我们有意地限制了它访问广泛输入域信息的能力，促进了一个可辨识、可分离且降低维度的编码的发展。f_0的进化学习轨迹进一步表现为一个动态的分层树，提供了对聚类相似性的深刻洞察。此外，为了处理复杂的数据配置，我们提出了一个与滤波器组结构对称对齐的定制解码器结构。

    The best encoding is the one that is interpretable in nature. In this work, we introduce a novel model that incorporates an interpretable bottleneck-termed the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This arrangement compels the VAE to attend on the most informative segments of the input signal, fostering the learning of a novel encoding ${f_0}$ which boasts enhanced interpretability and clusterability over traditional latent spaces. By deliberately constraining the VAE with this FB, we intentionally constrict its capacity to access broad input domain information, promoting the development of an encoding that is discernible, separable, and of reduced dimensionality. The evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic hierarchical tree, offering profound insights into cluster similarities. Additionally, for handling intricate data configurations, we propose a tailored decoder structure that is symmetrically aligned with FB's architec
    
[^45]: 知识图谱中半感应式链接预测的基准

    A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])

    [http://arxiv.org/abs/2310.11917](http://arxiv.org/abs/2310.11917)

    本文提出了一个用于评估知识图谱中半感应式链接预测模型的大规模基准，基于Wikidata5M进行扩展。通过各种不同的任务和信息组合，该基准为进一步研究上下文和文本信息在链接预测中的整合提供了一个测试平台。

    

    知识图谱中的半感应式链接预测是根据上下文信息来预测新的、之前未见的实体的事实的任务。本文提出和描述了一个大规模基准来评估半感应式链接预测模型。该基准基于并扩展了Wikidata5M：它提供了转导式、k-shot和0-shot链接预测任务，每个任务都会根据可用的信息情况从（i）仅有知识图谱结构，到（ii）包含文本提及，再到（iii）实体的详细描述进行变化。我们进行了一项关于最近方法的小型研究，发现半感应式链接预测的性能远远低于转导式性能，在所有实验中都表现出对于长尾实体的不足。该基准为进一步研究如何将上下文和文本信息整合到链接预测中提供了一个测试平台。

    Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual
    
[^46]: 使用小波池化边缘保留自编码器进行多模式医学神经图像融合

    Multi-modal Medical Neurological Image Fusion using Wavelet Pooled Edge Preserving Autoencoder. (arXiv:2310.11910v1 [eess.IV])

    [http://arxiv.org/abs/2310.11910](http://arxiv.org/abs/2310.11910)

    本文提出了一种使用小波池化边缘保留自编码器的无监督多模式医学图像融合模型，用于提高特征提取和信息保留的效果。

    

    医学图像融合整合了源图像模态的互补诊断信息，以改善对潜在异常的可视化和分析。最近，基于深度学习的模型通过同时执行特征提取、特征选择和特征融合任务，胜过传统的融合方法。然而，大多数现有的卷积神经网络（CNN）架构使用传统的池化或跨步卷积策略对特征图进行下采样。这会导致源图像中的重要诊断信息和边缘细节的模糊或丢失，并稀释了特征提取过程的效力。因此，本文提出了一种基于保边稠密自编码器网络的端到端无监督融合模型，用于多模式医学图像。在提出的模型中，通过使用小波分解基于注意机制池化特征图来改善特征提取。

    Medical image fusion integrates the complementary diagnostic information of the source image modalities for improved visualization and analysis of underlying anomalies. Recently, deep learning-based models have excelled the conventional fusion methods by executing feature extraction, feature selection, and feature fusion tasks, simultaneously. However, most of the existing convolutional neural network (CNN) architectures use conventional pooling or strided convolutional strategies to downsample the feature maps. It causes the blurring or loss of important diagnostic information and edge details available in the source images and dilutes the efficacy of the feature extraction process. Therefore, this paper presents an end-to-end unsupervised fusion model for multimodal medical images based on an edge-preserving dense autoencoder network. In the proposed model, feature extraction is improved by using wavelet decomposition-based attention pooling of feature maps. This helps in preserving 
    
[^47]: 加速策略梯度：关于应用Nesterov动量在强化学习中的论文

    Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])

    [http://arxiv.org/abs/2310.11897](http://arxiv.org/abs/2310.11897)

    本文介绍了一种名为加速策略梯度的方法，通过应用Nesterov动量在强化学习中实现更快的全局收敛速度。通过在softmax策略参数化中收敛到最优策略，它以 $\tilde{O}(1/t^2)$ 的速率收敛。这是第一个基于Nesterov加速梯度在强化学习中全局收敛速率的研究。

    

    最近研究表明，策略梯度方法在非正则化表格softmax设置中以Θ(1/t)的速率全局收敛。因此，一个重要的研究问题是是否可以通过仅使用一阶更新进一步改进这种收敛速度。本文从动量的角度回答了上述问题，通过将著名的Nesterov加速梯度（NAG）方法应用于强化学习（RL），称之为 \textit{加速策略梯度}（APG）。为了展示APG在实现更快全局收敛方面的潜力，我们正式证明了使用真实梯度时，具有 softmax 策略参数化的APG以 $\tilde{O}(1/t^2)$ 的速率收敛到最优策略。据我们所知，这是NAG在RL领域中全局收敛速率的第一个表征。值得注意的是，我们的分析依赖于一个有趣的发现：不论初始化如何，APG最终可以达到近乎局部收敛的地方。

    Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con
    
[^48]: 基于通道注意力的拉普拉斯自编码器的新型多模态医学图像融合

    A New Multimodal Medical Image Fusion based on Laplacian Autoencoder with Channel Attention. (arXiv:2310.11896v1 [eess.IV])

    [http://arxiv.org/abs/2310.11896](http://arxiv.org/abs/2310.11896)

    本论文提出了一种基于通道注意力的拉普拉斯自编码器的新型多模态医学图像融合模型，能够有效地保留补充信息和重要的组织结构。

    

    医学图像融合结合了多模态医学图像的补充信息，以协助医学专业人员在临床诊断中确诊患者疾病，并在术前和术中提供指导。深度学习模型实现了具有高鲁棒性和准确性的端到端图像融合。然而，大多数基于深度学习的融合模型对输入图像进行下采样，以最小化可学习参数和计算量。在此过程中，源图像的显著特征变得无法恢复，导致丢失关键的诊断边缘细节和各种脑组织的对比度。本文提出了一种基于整合的拉普拉斯-高斯级联和注意力池化的新型多模态医学图像融合模型。我们证明了我们的模型有效地保留了补充信息和重要的组织结构。

    Medical image fusion combines the complementary information of multimodal medical images to assist medical professionals in the clinical diagnosis of patients' disorders and provide guidance during preoperative and intra-operative procedures. Deep learning (DL) models have achieved end-to-end image fusion with highly robust and accurate fusion performance. However, most DL-based fusion models perform down-sampling on the input images to minimize the number of learnable parameters and computations. During this process, salient features of the source images become irretrievable leading to the loss of crucial diagnostic edge details and contrast of various brain tissues. In this paper, we propose a new multimodal medical image fusion model is proposed that is based on integrated Laplacian-Gaussian concatenation with attention pooling (LGCA). We prove that our model preserves effectively complementary information and important tissue structures.
    
[^49]: 量子核方法的超参数研究

    A Hyperparameter Study for Quantum Kernel Methods. (arXiv:2310.11891v1 [quant-ph])

    [http://arxiv.org/abs/2310.11891](http://arxiv.org/abs/2310.11891)

    本研究调查了超参数选择对模型性能和经典核与量子核之间的泛化差距的影响。

    

    量子核方法在量子机器学习中具有潜力，因为与之相关的保证。它们的可访问性也打开了基于潜在量子优势对数据集进行预先筛选的可能性。为了做到这一点，早期的研究开发了几何差异，它可以被理解为两种基于核的机器学习方法之间的接近度度量，特别是量子核和经典核之间的接近度。该度量指示了量子和经典模型的复杂性之间的联系。因此，它引发了一个问题，即基于与模型复杂性的关系，几何差异是否可以成为除了潜在的量子优势之外的评估工具。在这项工作中，我们研究了超参数选择对模型性能和经典核与量子核之间的泛化差距的影响。对于经典的机器学习方法来说，超参数优化的重要性是众所周知的。

    Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical ma
    
[^50]: 从捕捉的流量跟踪构建基于图的深度学习网络模型

    Building a Graph-based Deep Learning network model from captured traffic traces. (arXiv:2310.11889v1 [cs.NI])

    [http://arxiv.org/abs/2310.11889](http://arxiv.org/abs/2310.11889)

    本论文介绍了一个从捕捉的流量跟踪构建深度学习网络模型的方法，通过图神经网络（GNN）和新颖的编码方法，可以更好地捕捉真实网络场景的复杂性。

    

    目前，最先进的网络模型基于离散事件仿真（DES）。虽然DES非常精确，但计算成本高，且难以并行化，使得模拟高性能网络变得不切实际。此外，模拟场景无法完全捕捉到真实网络场景中的所有复杂性。虽然存在基于机器学习（ML）技术的网络模型以减少这些问题，但这些模型也是利用模拟数据进行训练，因此容易受到相同的问题的影响。因此，图神经网络挑战赛2023引入了一组捕捉的流量跟踪数据集，可以用来构建没有这些限制的基于ML的网络模型。本文提出了一种基于图神经网络（GNN）的解决方案，专门设计用于更好地捕捉真实网络场景的复杂性。这是通过一种新颖的编码方法来从捕捉的数据包序列中捕捉信息实现的。

    Currently the state of the art network models are based or depend on Discrete Event Simulation (DES). While DES is highly accurate, it is also computationally costly and cumbersome to parallelize, making it unpractical to simulate high performance networks. Additionally, simulated scenarios fail to capture all of the complexities present in real network scenarios. While there exists network models based on Machine Learning (ML) techniques to minimize these issues, these models are also trained with simulated data and hence vulnerable to the same pitfalls. Consequently, the Graph Neural Networking Challenge 2023 introduces a dataset of captured traffic traces that can be used to build a ML-based network model without these limitations. In this paper we propose a Graph Neural Network (GNN)-based solution specifically designed to better capture the complexities of real network scenarios. This is done through a novel encoding method to capture information from the sequence of captured pack
    
[^51]: 用人工智能分析质谱数据以辅助理解火星的过去适居性并提供未来任务的见解

    Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions. (arXiv:2310.11888v1 [astro-ph.EP])

    [http://arxiv.org/abs/2310.11888](http://arxiv.org/abs/2310.11888)

    本文介绍了一种利用人工智能分析质谱数据以检测古代火星适居性潜力的方法，并展示了该方法在外星物质分析中的适用性。关键技术包括质谱值的转换、数据可视化和机器学习模型的应用。

    

    本文介绍了将人工智能应用于质谱数据以检测火星古代适居性潜力的方法。尽管数据是针对火星收集的，但同样的方法可以用于太阳系中的任何地球对象。此外，所提出的方法可以适应任何使用质谱的领域。研究集中于两种质谱技术（进化气体分析-质谱和气相色谱-质谱）的数据分析，这些技术用于识别地质样品中的特定化学化合物。研究证明了进化气体分析-质谱和气相色谱-质谱数据在外星物质分析中的适用性。所提出方法的最重要特征包括质谱值的平方根转换，将原始数据转换为二维光谱图，并利用特定的机器学习模型和技术以避免在相对较小的数据集上过度拟合。EGA-MS和GC-MS数据集

    This paper presents an application of artificial intelligence on mass spectrometry data for detecting habitability potential of ancient Mars. Although data was collected for planet Mars the same approach can be replicated for any terrestrial object of our solar system. Furthermore, proposed methodology can be adapted to any domain that uses mass spectrometry. This research is focused in data analysis of two mass spectrometry techniques, evolved gas analysis (EGA-MS) and gas chromatography (GC-MS), which are used to identify specific chemical compounds in geological material samples. The study demonstrates the applicability of EGA-MS and GC-MS data to extra-terrestrial material analysis. Most important features of proposed methodology includes square root transformation of mass spectrometry values, conversion of raw data to 2D sprectrograms and utilization of specific machine learning models and techniques to avoid overfitting on relative small datasets. Both EGA-MS and GC-MS datasets c
    
[^52]: 从神经激活到概念: 解释神经网络中的概念的调查

    From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])

    [http://arxiv.org/abs/2310.11884](http://arxiv.org/abs/2310.11884)

    本文调查了解释神经网络中概念的最新方法，这对于实现基于可解释概念的神经符号化人工智能来说是重要的一步。

    

    在本文中，我们审查了解释神经网络中概念的最新方法。概念可以作为学习和推理之间的自然桥梁：一旦确定了神经学习系统使用的概念，就可以将这些概念与推理系统整合，用于推理或使用推理系统对其进行改进或增强以改善学习系统。另一方面，不仅可以从神经网络中提取知识，还可以将概念知识插入神经网络体系结构中。由于整合学习和推理是神经符号化人工智能的核心，所以通过这项调查获得的见解可以成为实现基于可解释概念的神经符号化人工智能的重要一步。

    In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
    
[^53]: 具有开关成本和延迟梯度的在线凸优化问题

    Online Convex Optimization with Switching Cost and Delayed Gradients. (arXiv:2310.11880v1 [cs.LG])

    [http://arxiv.org/abs/2310.11880](http://arxiv.org/abs/2310.11880)

    提出了一种在线多梯度下降（OMGD）算法用于解决具有二次和线性开关成本的在线凸优化问题，证明了其竞争比率上界，并在有限信息设置下达到了最优（按顺序）的动态后悔。

    

    我们考虑了在有限信息设置下具有二次和线性开关成本的在线凸优化问题，在这里在线算法仅能利用先前目标函数的梯度信息进行动作选择。对于$L$-光滑和$\mu$-强凸目标函数，我们提出了一种在线多梯度下降（OMGD）算法，并证明该算法在具有二次开关成本的在线凸优化问题上的竞争比率最多为$4(L+5)+\frac{16(L+5)}{\mu}$。对于OMGD的竞争比率上界也被证明在$L$和$\mu$方面是紧致的。此外，当开关成本为二次时，我们证明了任何在线算法的竞争比率是$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$。我们还证明了在有限信息设置下，OMGD算法实现了最优（按顺序）的动态后悔。对于线性开关成本，

    We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In addition, we show that the competitive ratio of any online algorithm is $\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of
    
[^54]: 学习线性分类器混合的SQ下界

    SQ Lower Bounds for Learning Mixtures of Linear Classifiers. (arXiv:2310.11876v1 [cs.LG])

    [http://arxiv.org/abs/2310.11876](http://arxiv.org/abs/2310.11876)

    本文研究了学习线性分类器混合的问题，证明了该问题的统计查询（SQ）算法的复杂度下界是$n^{\mathrm{poly}(1/\Delta) \log(r)}$，同时提出了一种可能具有独立兴趣的新球面设计方法。

    

    我们研究了在高斯协变量下学习线性分类器混合的问题。给定对形式为$(\mathbf{x},y_{\ell})$的$n$维高斯分布的$r$个混合分布样本访问权限，其中$\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$，$y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$，目标是以总变异距离学习潜在的分布。我们的主要结果是统计查询（SQ）的下界，表明对于这个问题的已知算法实际上是最好的，即使对于均匀混合的特殊情况也是如此。特别地，我们证明了对于该问题的任何SQ算法的复杂度都是$n^{\mathrm{poly}(1/\Delta) \log(r)}$，其中$\Delta$是$\mathbf{v}_\ell$之间的两两$\ell_2$-分离的下界。我们结果的关键技术构建是一种可能具有独立兴趣的新球面设计。

    We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and $y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.
    
[^55]: 神经网络中的分数概念：增进激活和损失函数

    Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions. (arXiv:2310.11875v1 [cs.LG])

    [http://arxiv.org/abs/2310.11875](http://arxiv.org/abs/2310.11875)

    本文介绍了一种使用分数概念修改激活和损失函数的方法，通过调整神经元的激活函数，能够更好地适应输入数据并提高网络的性能。

    

    本文介绍了一种在神经网络中使用分数概念修改激活和损失函数的方法。该方法允许神经网络通过确定训练过程的分数导数阶数作为额外的超参数来定义和优化其激活函数。这将使得网络中的神经元能够调整其激活函数以更好地适应输入数据并减少输出错误，有可能提高网络的整体性能。

    The paper presents a method for using fractional concepts in a neural network to modify the activation and loss functions. The methodology allows the neural network to define and optimize its activation functions by determining the fractional derivative order of the training process as an additional hyperparameter. This will enable neurons in the network to adjust their activation functions to match input data better and reduce output errors, potentially improving the network's overall performance.
    
[^56]: 评估计算机视觉中判别性基础模型的公平性

    Evaluating the Fairness of Discriminative Foundation Models in Computer Vision. (arXiv:2310.11867v1 [cs.CV])

    [http://arxiv.org/abs/2310.11867](http://arxiv.org/abs/2310.11867)

    评估了计算机视觉中判别性基础模型的公平性，并提出了用于评估偏见的分类法。通过系统性地评估现有的减少模型偏见的方法，揭示了模型在关键应用中的性能。根据任务涉及人类程度、主观性程度和预期目的对期望的行为进行了分类，并对受保护属性进行了定量的公平性评估。

    

    我们提出了一种新的用于评估判别性基础模型（如Contrastive Language-Pretraining (CLIP)）偏见的分类法，该分类法用于标记任务。然后，我们根据我们的分类法系统地评估了现有减少这些模型偏见的方法。具体而言，我们评估了OpenAI的CLIP和OpenCLIP模型在零样本分类、图像检索和图像字幕等关键应用中的性能。我们根据三个维度对期望的行为进行了分类：（i）任务是否涉及人类；（ii）任务的主观性程度（即，来自不同背景的人们是否会对标记达成一致）；（iii）任务的预期目的，公平性是通过公正（即，独立于受保护属性进行决策）还是表达（即，通过最大化多样性进行决策）更好地实现。最后，我们对二值和多值受保护属性进行了定量的公平性评估。

    We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes ove
    
[^57]: 具有不精确的Hessian矩阵、梯度和函数的非凸问题的随机优化方法

    Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function. (arXiv:2310.11866v1 [cs.LG])

    [http://arxiv.org/abs/2310.11866](http://arxiv.org/abs/2310.11866)

    本文提出了一种能够同时提供Hessian矩阵、梯度和函数值的不精确计算的随机优化方法，通过减少传播开销来降低计算成本，并证明了在达到ε-近似二阶优化时与精确计算具有相同的迭代复杂度。

    

    信任区域(TR)和使用三次方(ARC)进行自适应正则化的方法证明对于非凸优化具有一些非常吸引人的理论性质，通过同时计算函数值、梯度和Hessian矩阵来获得下一个搜索方向和调整参数。尽管随机近似大大减少了计算成本，但在理论上保证收敛速度仍然具有挑战性。在本文中，我们探索了一族能够同时提供Hessian矩阵、梯度和函数值的不精确计算的随机TR和ARC方法。我们的算法每次迭代所需的传播开销比TR和ARC要少得多。我们证明了达到ε-近似二阶优化的迭代复杂度与之前研究中的精确计算的数量级相同。此外，对不精确性的温和条件可以通过在有限和总和m中利用随机采样技术来满足。

    Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum m
    
[^58]: 混合数据上有效高效的联邦树学习

    Effective and Efficient Federated Tree Learning on Hybrid Data. (arXiv:2310.11865v1 [cs.LG])

    [http://arxiv.org/abs/2310.11865](http://arxiv.org/abs/2310.11865)

    这项研究提出了一种名为HybridTree的新方法，可以在混合数据上进行联邦树学习。通过利用树中的一致分割规则，参与方的知识可以被纳入树的较低层，从而实现高效的联邦学习。

    

    联邦学习作为一种有前景的分布式学习范式，可以促进多个参与方在不传输原始数据的情况下进行协作学习。然而，大多数现有的联邦学习研究集中在水平数据或垂直数据设置上，其中不同参与方的数据被认为来自相同的特征或样本空间。在实践中，常见的情况是混合数据设置，其中来自不同参与方的数据在特征和样本方面可能存在差异。为了解决这个问题，我们提出了HybridTree，一种在混合数据上进行联邦树学习的新方法。我们观察到树中存在一致的分割规则。借助这些分割规则，我们在理论上证明了参与方的知识可以被纳入树的较低层。基于我们的理论分析，我们提出了一种层级解决方案，不需要频繁的通信流量来训练一棵树。我们的实验证明了我们的方法在处理混合数据时的有效性和高效性。

    Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate 
    
[^59]: VQ-NeRF: 基于向量量化的神经反射分解与编辑

    VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v1 [cs.CV])

    [http://arxiv.org/abs/2310.11864](http://arxiv.org/abs/2310.11864)

    VQ-NeRF是一个基于向量量化的神经网络模型，用于分解和编辑3D场景中的反射场。通过将连续材料离散化，该模型可以减少噪声并生成离散材料的分割地图，从而实现简化的材料编辑。

    

    我们提出了VQ-NeRF，这是一个具有向量量化（VQ）的双分支神经网络模型，用于对3D场景中的反射场进行分解和编辑。传统的神经反射场仅使用连续表示来建模3D场景，尽管现实中的物体通常由离散材料组成。这种缺乏离散化可能导致材料分解噪声和复杂的材料编辑。为了解决这些限制，我们的模型包括一个连续分支和一个离散分支。连续分支按照传统流程预测分解的材料，而离散分支使用VQ机制将连续材料量化为单独的材料。通过离散化材料，我们的模型可以减少分解过程中的噪声，并生成离散材料的分割地图。可以通过点击分割结果的相应区域来轻松选择特定材料进行进一步编辑。

    We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Ad
    
[^60]: 重新审视可转移的对抗性图像示例：攻击分类，评估指南和新见解

    Revisiting Transferable Adversarial Image Examples: Attack Categorization, Evaluation Guidelines, and New Insights. (arXiv:2310.11850v1 [cs.CR])

    [http://arxiv.org/abs/2310.11850](http://arxiv.org/abs/2310.11850)

    本论文重新审视了可转移的对抗性图像示例的评估方法，提出了新的攻击分类策略，并通过大规模评估揭示了一些新的见解和共识挑战。

    

    可转移的对抗性示例在现实世界的黑盒攻击场景中引发了关键的安全问题。然而，在这项工作中，我们发现了常见评估实践中的两个主要问题：(1) 对于攻击的可转移性，缺乏系统化的，一对一的攻击比较和公平的超参数设置。(2) 对于攻击的隐蔽性，简单地没有比较。为了解决这些问题，我们通过(1) 提出一种新的攻击分类策略，并在可转移性方面进行系统化和公平的同类别分析，以及(2) 从攻击回溯的角度考虑多样的难以察觉的度量和更细粒度的隐蔽特性来建立新的评估指南。为此，我们对ImageNet上的可转移的对抗性示例进行了首次大规模评估，涉及对9种代表性防御的23种代表性攻击。我们的评估提供了一些新的见解，包括挑战共识的见解。

    Transferable adversarial examples raise critical security concerns in real-world, black-box attack scenarios. However, in this work, we identify two main problems in common evaluation practices: (1) For attack transferability, lack of systematic, one-to-one attack comparison and fair hyperparameter settings. (2) For attack stealthiness, simply no comparisons. To address these problems, we establish new evaluation guidelines by (1) proposing a novel attack categorization strategy and conducting systematic and fair intra-category analyses on transferability, and (2) considering diverse imperceptibility metrics and finer-grained stealthiness characteristics from the perspective of attack traceback. To this end, we provide the first large-scale evaluation of transferable adversarial examples on ImageNet, involving 23 representative attacks against 9 representative defenses. Our evaluation leads to a number of new insights, including consensus-challenging ones: (1) Under a fair attack hyper
    
[^61]: 通过强化学习加速大规模线性规划中的前处理

    Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning. (arXiv:2310.11845v1 [cs.LG])

    [http://arxiv.org/abs/2310.11845](http://arxiv.org/abs/2310.11845)

    本论文提出了一种简单高效的强化学习框架（RL4Presolve），通过将算法设计任务转化为马尔可夫决策过程，同时解决了在大规模线性规划中前处理程序的选择、顺序和停止等问题。

    

    来自工业界的大规模线性规划问题通常包含大量冗余，严重影响了解决线性规划问题的效率和可靠性，使得前处理（即问题简化模块）成为现代线性规划求解器中最关键的组件之一。然而，如何设计高质量的前处理程序（即确定（P1）选择哪些前处理器，（P2）以何种顺序执行，（P3）何时停止）仍然是一个极具挑战性的任务，因为它涉及专家知识的广泛要求和庞大的搜索空间。由于任务具有顺序决策属性和缺乏专家示范，我们提出了一种简单高效的强化学习（RL）框架——即前处理的强化学习（RL4Presolve），以同时解决（P1）-（P3）。具体而言，我们将算法设计任务转化为马尔可夫决策过程，并提出了具有自适应动作序列的RL框架，以生成高质量的前处理程序。

    Large-scale LP problems from industry usually contain much redundancy that severely hurts the efficiency and reliability of solving LPs, making presolve (i.e., the problem simplification module) one of the most critical components in modern LP solvers. However, how to design high-quality presolve routines -that is, the program determining (P1) which presolvers to select, (P2) in what order to execute, and (P3) when to stop -- remains a highly challenging task due to the extensive requirements on expert knowledge and the large search space. Due to the sequential decision property of the task and the lack of expert demonstrations, we propose a simple and efficient reinforcement learning (RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) -to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine design task as a Markov decision process and propose an RL framework with adaptive action sequences to generate high-quality presolve routines efficie
    
[^62]: 关于强化学习中目标规范形式的表达能力

    On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. (arXiv:2310.11840v1 [cs.LG])

    [http://arxiv.org/abs/2310.11840](http://arxiv.org/abs/2310.11840)

    这项工作通过对强化学习中17种目标规范形式的表达能力进行全面比较，填补了现有文献中的空白。通过将这些形式化方法进行预排序，并呈现为哈斯图，我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。

    

    要解决强化学习任务，必须对该任务的目标进行形式化规定。尽管大多数强化学习算法要求将目标形式化为马尔可夫奖励函数，但已经开发出了其他替代方法（如线性时间逻辑和多目标强化学习）。此外，众所周知，其中一些形式化方法能够表达其他形式化方法无法表达的特定任务。然而，目前还没有对这些形式化方法在表达能力方面如何相互关联进行全面分析。在本研究中，我们通过对强化学习中17种目标规范形式的表达能力进行全面比较填补了现有文献中的空白。我们将这些形式化方法根据其表达能力进行预排序，并将该预排序呈现为哈斯图。我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。

    To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimis
    
[^63]: 等变引导法在成像反问题不确定性量化中的应用

    Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems. (arXiv:2310.11838v1 [eess.IV])

    [http://arxiv.org/abs/2310.11838](http://arxiv.org/abs/2310.11838)

    本文提出了一种新的不确定性量化方法，利用参数引导算法的等变形式，可以在成像反问题中量化重构图像的不确定性，并且可以与任何图像重建技术结合使用。

    

    科学成像问题通常存在严重的不适定性，因此具有重要的内在不确定性。准确量化解决方案的不确定性对于严格解释实验结果以及可靠地使用重构图像作为科学证据至关重要。不幸的是，现有的成像方法无法以对实验重复具有鲁棒性的方式量化重构图像中的不确定性。本文提出了一种新的不确定性量化方法，基于参数引导算法的等变形式，利用在成像问题中常见的对称性和不变性特性。此外，所提出的方法是通用的，可以轻松地与任何图像重建技术结合使用，包括只能从观察数据中进行训练的无监督训练策略，从而实现了不确定性量化在只有观测数据的情况下的应用。

    Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where 
    
[^64]: 使用自然梯度替代品优化分布

    Optimising Distributions with Natural Gradient Surrogates. (arXiv:2310.11837v1 [stat.ML])

    [http://arxiv.org/abs/2310.11837](http://arxiv.org/abs/2310.11837)

    本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。

    

    自然梯度方法已经被用于优化各种情况下的概率分布参数，通常能得到快速收敛的过程。然而，对于许多感兴趣的分布，计算自然梯度存在一些挑战。在这项工作中，我们提出了一种新的技术来解决这些问题，这涉及将优化重新定义为关于替代分布参数的优化，计算自然梯度很容易。我们给出了几个可以解释为应用这种技术的现有方法的例子，并提出了一种新的方法，可以将其应用于各种问题。我们的方法扩展了可以有效使用自然梯度的分布集合。此外，它快速、易于理解，可以使用标准的自动微分软件进行简单实现，并且不需要冗长的模型特定导数计算。我们在最大似然估计和变分推断上演示了我们的方法。

    Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and varia
    
[^65]: CLARA: 多语言对比学习用于音频表示获取的论文

    CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition. (arXiv:2310.11830v1 [cs.SD])

    [http://arxiv.org/abs/2310.11830](http://arxiv.org/abs/2310.11830)

    本文提出了一个多语言对比学习框架，通过自监督学习从无标签数据中获取音频表示，实现了跨语言迁移和情感维度的编码。

    

    本文提出了一个新的多语言语音和音频表示学习框架，使用对比学习。标注数据不足制约了跨语言语音处理研究的发展。最近对比学习的进展提供了自监督技术来从无标签数据中学习。为了减少数据依赖性和改善在不同语言和条件下的泛化能力，我们开发了一个多语言对比学习框架。该框架使模型能够在多语言中获得共享表示，有助于使用有限的目标语言数据进行跨语言迁移。此外，由于主观感知评估的挑战，捕捉语音中的情感线索是困难的。通过自监督的方式从多样的多语言数据中学习表达性表示，我们的方法旨在开发编码情感维度的语音表示。我们的方法在大规模的多语言音频数据集上训练编码器。

    This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.  Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.  Our method trains encoders on a large corpus of multi-lingual audio data
    
[^66]: 走向图基础模型：一项调查与进展

    Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v1 [cs.LG])

    [http://arxiv.org/abs/2310.11829](http://arxiv.org/abs/2310.11829)

    本文提出了图基础模型（GFMs）的概念，并对其关键特征和技术进行了全面阐述。同时，将现有GFMs工作分为三个类别，为进一步研究和开发图学习范式奠定了基础。

    

    基于其在自然语言处理和其他领域中的显著成功，基础模型已经成为各种人工智能应用的基本构建模块。与此同时，图机器学习经历了由浅层方法向深度学习方法的转变。基础模型的出现和同化能力引起了图机器学习研究者的兴趣，引发了关于开发下一个预训练于广泛图数据并可适应各种下游图任务的图学习范式的讨论。然而，目前对这类工作尚无明确的定义和系统的分析。在本文中，我们提出了图基础模型(GFMs)的概念，并首次对其关键特征和技术进行全面阐述。在此基础上，我们根据其可靠性将现有GFMs工作分为三个类别。

    Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their relia
    
[^67]: 在嘈杂的金融数据上进行保守预测

    Conservative Predictions on Noisy Financial Data. (arXiv:2310.11815v1 [cs.LG])

    [http://arxiv.org/abs/2310.11815](http://arxiv.org/abs/2310.11815)

    在嘈杂的金融数据上进行保守预测，通过对不确定的数据点进行剪枝，以提高预测的准确性。

    

    金融市场的价格波动被认为是非常嘈杂的。因此，即使机器学习算法偶尔能够捕捉到可利用的模式，由于特征和标签的噪声，这些模式往往被掩盖，使得预测变得不太有用且存在风险。我们应用了一种类似的方法，即模型在对不确定的数据点上不进行预测。在训练过程中，一系列这样的模型按序进行学习，类似于规则列表，每个模型仅在前面的模型对其不确定的数据上进行训练。测试时也会进行类似的数据剪枝，只对测试数据的一部分（支持集）进行预测，从而获得更高的准确性。

    Price movements in financial markets are well known to be very noisy. As a result, even if there are, on occasion, exploitable patterns that could be picked up by machine-learning algorithms, these are obscured by feature and label noise rendering the predictions less useful, and risky in practice. Traditional rule-learning techniques developed for noisy data, such as CN2, would seek only high precision rules and refrain from making predictions where their antecedents did not apply. We apply a similar approach, where a model abstains from making a prediction on data points that it is uncertain on. During training, a cascade of such models are learned in sequence, similar to rule lists, with each model being trained only on data on which the previous model(s) were uncertain. Similar pruning of data takes place at test-time, with (higher accuracy) predictions being made albeit only on a fraction (support) of test-time data. In a financial prediction setting, such an approach allows decis
    
[^68]: 使用几何矢量场网络进行全新蛋白质设计

    De novo protein design using geometric vector field networks. (arXiv:2310.11802v1 [cs.CE])

    [http://arxiv.org/abs/2310.11802](http://arxiv.org/abs/2310.11802)

    创新点是引入了Vector Field Network (VFN)来提高蛋白质结构编码器的建模能力，在全新蛋白质设计中取得了显著进展。

    

    利用蛋白质扩散等创新技术，在全新蛋白质设计方面取得了显著进展，这是生命科学中一个重要的课题。这些方法通常依赖蛋白质结构编码器来建模残基骨架帧，在这种情况下不存在原子。大多数先前的编码器依赖原子级特征，如原子之间的角度和距离，这些特征在这种情况下是不可用的。迄今为止，只有几个简单的编码器（例如IPA）被提出用于这种情况，使得帧模型成为一个瓶颈。在这项工作中，我们提出了矢量场网络（VFN），它使得网络层能够在帧锚定的虚拟原子坐标之间进行可学习的矢量计算，从而实现更高的帧模型能力。矢量计算的操作方式类似于线性层，每个输入通道接收3D虚拟原子坐标而不是标量值。矢量计算输出的多个特征向量然后被用于...

    Innovations like protein diffusion have enabled significant progress in de novo protein design, which is a vital topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Thus far, only several simple encoders, such as IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we proffer the Vector Field Network (VFN), which enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to 
    
[^69]: 物理信息神经网络的对抗训练

    Adversarial Training for Physics-Informed Neural Networks. (arXiv:2310.11789v1 [cs.LG])

    [http://arxiv.org/abs/2310.11789](http://arxiv.org/abs/2310.11789)

    这篇论文提出了一种名为AT-PINNs的对抗训练策略，通过对抗样本的微调来增强物理信息神经网络（PINNs）的鲁棒性，并且可以进行具有时间因果关系的推断。

    

    物理信息神经网络在解决偏微分方程问题上显示出巨大的潜力。然而，由于不足的鲁棒性，普通的PINNs在解决涉及多尺度行为或具有尖锐或振荡特征的复杂PDE时经常面临挑战。为了解决这些问题，我们基于投影梯度下降对抗攻击提出了一种对抗训练策略，被称为AT-PINNs。AT-PINNs通过对抗样本的微调来增强PINNs的鲁棒性，可以准确识别模型失效位置并在训练过程中引导模型专注于这些区域。AT-PINNs还可以通过选择围绕时间初始值的初始拟合点来进行因果推断。我们将AT-PINNs应用于具有多尺度系数的椭圆方程、具有多峰解的泊松方程、具有尖锐解的Burgers方程以及Allen-Cahn方程。

    Physics-informed neural networks have shown great promise in solving partial differential equations. However, due to insufficient robustness, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To address these issues, based on the projected gradient descent adversarial attack, we proposed an adversarial training strategy for PINNs termed by AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model with adversarial samples, which can accurately identify model failure locations and drive the model to focus on those regions during training. AT-PINNs can also perform inference with temporal causality by selecting the initial collocation points around temporal initial values. We implement AT-PINNs to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn equati
    
[^70]: NeuroCUT：一种用于鲁棒图分区的神经方法

    NeuroCUT: A Neural Approach for Robust Graph Partitioning. (arXiv:2310.11787v1 [cs.LG])

    [http://arxiv.org/abs/2310.11787](http://arxiv.org/abs/2310.11787)

    NeuroCUT是一种神经方法，用于解决鲁棒的图分区问题。它通过两个关键创新，即对图拓扑和分区计数具有归纳性，以及利用强化学习基础，能够从数据中学习启发式方法。

    

    图分区旨在将图分割为k个不相交的子集，同时优化特定的分区目标。由于其组合性质，大部分与图分区相关的问题都呈现出NP难度。因此，传统的近似算法依赖于启发式方法，有时带有近似保证，有时则没有。不幸的是，传统方法针对特定的分区目标进行优化，不适用于其他已知的文献中的分区目标。为了克服这个限制，并直接从数据中学习启发式方法，神经方法应运而生，并展示出令人期待的结果。在本研究中，我们通过一个新颖的框架NeuroCut扩展了这一领域的工作。NeuroCut在现有方法上引入了两个关键创新。首先，它对图拓扑和分区计数具有归纳性，这些信息在查询时提供。其次，通过利用强化学习基础

    Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning base
    
[^71]: 用于学习图神经网络的准瓦狄斯坦损失

    A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])

    [http://arxiv.org/abs/2310.11762](http://arxiv.org/abs/2310.11762)

    这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。

    

    当在节点级别预测任务中学习图神经网络（GNNs）时，大多数现有的损失函数是独立地应用于每个节点的，即使节点嵌入和它们的标签由于图结构的存在而不是独立同分布的。为了消除这种不一致性，本研究提出了一种新的准瓦狄斯坦（QW）损失函数，借助于在图上定义的最优传输，从而引导GNN的新学习和预测范式。特别地，我们设计了一种“准瓦狄斯坦”距离，用于观测到的多维节点标签和它们的估计之间，通过优化在图边上定义的标签传输。这些估计是由一个GNN参数化的，其中最优标签传输可以选择性地确定图边的权重。通过将标签传输的严格约束重新表达为基于Bregman散度的正则化项，我们得到了所提出的准瓦狄斯坦损失，关联两个高效求解器来学习GNN以及最优标签传输。

    When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
    
[^72]: 面向未知攻击的领域通用人脸反欺诈方法

    Domain-Generalized Face Anti-Spoofing with Unknown Attacks. (arXiv:2310.11758v1 [cs.CV])

    [http://arxiv.org/abs/2310.11758](http://arxiv.org/abs/2310.11758)

    该论文提出了一种面向领域通用未知攻击的人脸反欺诈方法，通过引入Transformer-based特征提取器和合成未知攻击样本生成器（SUASG），实现了超越已知和未知攻击的领域通用性能。

    

    尽管人脸反欺诈（FAS）方法在特定领域或攻击类型上取得了显著的性能，但很少有研究关注领域改变和未知攻击同时存在的情况，这更接近实际应用场景。为了处理领域通用的未知攻击，我们引入了一种新方法， DGUA-FAS，它由基于Transformer的特征提取器和一个合成未知攻击样本生成器（SUASG）组成。SUASG网络模拟未知攻击样本来辅助特征提取器的训练。实验证明，我们的方法在已知或未知攻击的领域通用反欺诈上取得了卓越的性能。

    Although face anti-spoofing (FAS) methods have achieved remarkable performance on specific domains or attack types, few studies have focused on the simultaneous presence of domain changes and unknown attacks, which is closer to real application scenarios. To handle domain-generalized unknown attacks, we introduce a new method, DGUA-FAS, which consists of a Transformer-based feature extractor and a synthetic unknown attack sample generator (SUASG). The SUASG network simulates unknown attack samples to assist the training of the feature extractor. Experimental results show that our method achieves superior performance on domain generalization FAS with known or unknown attacks.
    
[^73]: 使用Sum-GP-UCB方法估计互动物体的材料性质

    Estimating Material Properties of Interacting Objects Using Sum-GP-UCB. (arXiv:2310.11749v1 [cs.RO])

    [http://arxiv.org/abs/2310.11749](http://arxiv.org/abs/2310.11749)

    本文提出了一种使用Sum-GP-UCB方法估计互动物体材料性质的贝叶斯优化方法，在不同组互动物体场景的观察下，通过建模奖励函数结构和部分评估来加速优化过程。

    

    机器人需要通过观察来准确模拟物体的材料和动态特性。我们提出了一种基于贝叶斯优化的方法，用于通过一组观察来识别物体的材料属性参数。我们的重点是基于不同组互动物体场景的观察来估计这些属性。我们提出了一种利用奖励函数结构的方法，通过分别建模每个观察的奖励，并仅使用该场景中物体的参数作为输入。得到的低维模型在参数空间上具有更好的泛化能力，从而加速了优化过程。为了进一步加快优化过程，并减少寻找优秀参数值所需的仿真次数，我们还提出了奖励函数的部分评估方法，其中选择的参数仅在一部分真实世界评估上进行评估。

    Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach 
    
[^74]: 大型ASR模型中的意外记忆及其缓解方法

    Unintended Memorization in Large ASR Models, and How to Mitigate It. (arXiv:2310.11739v1 [cs.LG])

    [http://arxiv.org/abs/2310.11739](http://arxiv.org/abs/2310.11739)

    该论文研究了大型ASR模型中的意外记忆问题，并提出了一种简单的审计方法来测量记忆效应。研究发现，目前的ASR模型存在记忆现象，为此提出了使用渐变裁剪进行训练来缓解记忆问题。

    

    众所周知，神经网络可能会无意中记住训练样本，引发隐私问题。然而，在大型非自回归自动语音识别（ASR）模型中审计记忆一直是具有挑战性的，因为现有方法（如硬度校准）的计算成本很高。在本研究中，我们设计了一种简单的审计方法，用于在大型ASR模型中测量记忆，而无需额外的计算开销。具体而言，我们加速随机生成的话语，创建一个语音和文本信息之间的映射，这在典型的训练样本中很难学习到。因此，仅对加速训练样本的准确预测可以作为记忆的明确证据，并且相应的准确性可以用来衡量记忆。使用所提出的方法，我们展示了现有ASR模型中的记忆现象。为了缓解记忆，我们尝试在训练过程中进行渐变裁剪，以限制任何单个样本的影响力。

    It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example 
    
[^75]: 在多选设置下研究对齐语言模型的不确定性校准

    Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])

    [http://arxiv.org/abs/2310.11732](http://arxiv.org/abs/2310.11732)

    本研究系统评估了在多选设置下对齐语言模型不确定性校准的影响。研究发现，在这种设置下存在两种不确定性，分别对答案决策和格式偏好负责。对齐模型过度自信的原因之一是这两种不确定性的混淆。

    

    尽管在对齐语言模型（LM）的实际应用中取得了显著进展，但它们倾向于与预训练的LM相比，在输出答案时表现出过度自信。本研究系统评估了对齐过程对多选设置下LM的基于逻辑的不确定性校准的影响。我们首先对对齐LM在校准方面与其预训练对应模型之间的差异进行了认真的实证研究。实验结果显示，在多选设置下，LM存在两种明显的不确定性，分别负责答案决策和LM的格式偏好。然后，我们通过在简单的合成对齐方案中进行微调，研究了这两种不确定性在对齐LM的校准中的作用，并得出结论，对齐LM过度自信的原因之一是这两种不确定性的混淆。此外，我们还检查了常见的事后校准方法的实用性。

    Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc cal
    
[^76]: 面向隐私保护推荐的联邦异构图神经网络

    Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])

    [http://arxiv.org/abs/2310.11730](http://arxiv.org/abs/2310.11730)

    本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。

    

    异构信息网络（HIN）通过元路径描述丰富的语义，已成为缓解推荐系统数据稀疏性的强大工具。现有的基于HIN的推荐系统持有数据的集中存储假设，并进行集中式模型训练。然而，由于隐私问题，现实世界的数据往往以分布式方式存储，导致集中式HIN推荐无法实现。本文提出将HIN分为客户端存储的私有HIN和服务器端的共享HIN。在此设置下，我们提出了一种基于联邦异构图神经网络（FedHGNN）的框架，可以在分布式HIN上协作训练推荐模型，同时不泄露用户隐私。具体而言，我们首先针对基于HIN的联合推荐，基于差分隐私的光下确定了隐私定义，旨在保护私有HIN的用户-商品交互，以及用户的隐私信息。

    Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
    
[^77]: 这里是翻译过的论文标题

    Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding. (arXiv:2310.11721v1 [cs.CL])

    [http://arxiv.org/abs/2310.11721](http://arxiv.org/abs/2310.11721)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results 
    
[^78]: 在分布式学习任务中评估生成模型

    On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])

    [http://arxiv.org/abs/2310.11714](http://arxiv.org/abs/2310.11714)

    本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。

    

    在文献中已经广泛研究了对包括生成对抗网络（GAN）和扩散模型在内的深度生成模型的评估。然而，现有的评估方法主要针对单个客户端存储的训练数据的集中式学习问题，而生成模型的许多应用涉及到分布式学习环境，例如联邦学习场景，其中训练数据由多个客户端收集并分发。本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。首先，我们关注Fr\'echet inception距离（FID），并考虑以下基于FID的聚合分数：1）FID-avg作为客户端个体FID分数的平均值，2）FID-all作为训练模型与包含所有客户端数据的集体数据集之间的FID距离。我们证明了根据FID-all和FID-avg分数的模型排名可能不一致。

    The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
    
[^79]: 学习文本分类的标签比例

    Learning under Label Proportions for Text Classification. (arXiv:2310.11707v1 [cs.LG])

    [http://arxiv.org/abs/2310.11707](http://arxiv.org/abs/2310.11707)

    这篇论文提出了一种在标签比例(LLP)的挑战性环境中进行文本分类的方法，通过对最广泛使用的基线技术进行改进，结合自监督目标，取得了较好的结果。

    

    我们在学习从标签比例(LLP)的具有挑战性的设置下进行了初步的自然语言处理工作，其中数据以袋状聚合形式提供，仅用每个类别中样本的比例作为真值。这个设置与在隐私设置和弱监督下训练模型的期望特性相吻合。通过对最广泛使用的基线技术DLLP的一些不规则进行表征，我们提出了一个新的鲁棒公式。这伴随着一个学习结果，在LLP下提供了一个泛化界限。将这个公式与一个自监督目标相结合，我们的方法在包括长文本和短文本的大规模模型以及多个指标的多个实验配置中与基线方法相比表现更好，达到了近87%的实验配置。

    We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also robust. This is accompanied with a learnability result that provides a generalization bound under LLP. Combining this formulation with a self-supervised objective, our method achieves better results as compared to the baselines in almost 87% of the experimental configurations which include large scale models for both long and short range texts across multiple metrics.
    
[^80]: 单视角视频中的跑者再识别在开放环境中

    Runner re-identification from single-view video in the open-world setting. (arXiv:2310.11700v1 [cs.CV])

    [http://arxiv.org/abs/2310.11700](http://arxiv.org/abs/2310.11700)

    本论文提出了一种在开放世界环境中直接处理单视角视频的跑者再识别系统。通过自动处理原始视频作为输入来识别跑者，并且能够在跑者被框选出多次的情况下进行识别。

    

    在许多体育运动中，跑者的再识别对于自动视频处理和分析至关重要。然而，目前关于多视角或单视角体育视频中跑者再识别的大部分研究都集中在使用标记图像数据集进行封闭世界设定的再识别上，而在开放世界环境下进行自动视频分析的跑者再识别并未得到很好的发展。本文提出了一种直接处理单视角视频以解决开放世界设置的跑者再识别系统。在开放世界设置中，我们无法使用标记数据集，并且必须直接处理视频。所提出的系统自动处理原始视频作为输入来识别跑者，即使跑者出现多次被框选出，系统也能进行识别。对于自动处理，我们首先使用预训练的YOLOv8和微调的EfficientNet来检测视频中的跑者。然后使用ByteTrack来跟踪跑者，并使用微调的YOLO来检测他们的鞋子。

    In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YO
    
[^81]: AUC-mixup: 结合mixup的深度AUC最大化方法

    AUC-mixup: Deep AUC Maximization with Mixup. (arXiv:2310.11693v1 [cs.LG])

    [http://arxiv.org/abs/2310.11693](http://arxiv.org/abs/2310.11693)

    本文研究了如何通过mixup数据增强方法来改善深度AUC最大化方法（DAM）的泛化能力，以解决在应用于小数据集时可能导致过拟合问题的挑战。

    

    深度AUC最大化（DAM）在不平衡医学任务中取得了显著的成功，例如胸部X光分类和皮肤损伤分类。然而，当应用于小数据集时，由于其将正样本的预测分数与负样本的分数远离的激进性，可能会导致严重的过拟合问题。本文研究了如何通过mixup数据增强方法来改善DAM的泛化能力，mixup是一种广泛用于提高交叉熵损失基础的深度学习方法的泛化能力的方法。

    While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, e.g., chest X-rays classification and skin lesions classification, it could suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve generalization of DAM by mixup data augmentation -- an approach that is widely used for improving generalization of the cross-entropy loss based deep learning methods. %For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn f
    
[^82]: 基于Transformer架构的深度学习用于存在类别不平衡情况下的电力系统短期电压稳定性评估

    Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance. (arXiv:2310.11690v1 [eess.SY])

    [http://arxiv.org/abs/2310.11690](http://arxiv.org/abs/2310.11690)

    提出了一种基于Transformer架构的深度学习方法，用于存在类别不平衡情况下的电力系统短期电压稳定性评估。使用条件Wasserstein生成对抗网络生成合成数据来解决不平衡数据集的问题，并利用稳定性评估Transformer作为分类模型，反映系统运行状态与稳定性结果之间的相关性。

    

    大多数现有的数据驱动电力系统短期电压稳定性评估方法假定输入数据平衡。然而，在实际应用中，干扰后短期电压不稳定性的发生很少，导致显著的类别不平衡问题和分类器性能下降。本文提出了一种基于Transformer架构的短期电压稳定性评估方法来解决这个挑战。通过利用基本的Transformer架构，开发了一种稳定性评估Transformer (StaaT)作为一个分类模型，以反映系统运行状态与稳定性结果之间的相关性。为了解决不平衡数据集的负面影响，本文采用了带有梯度惩罚的条件Wasserstein生成对抗网络 (CWGAN-GP)进行合成数据生成，帮助创建一个平衡、代表性的训练集用于分类器。

    Most existing data-driven power system short-term voltage stability assessment (STVSA) approaches presume class-balanced input data. However, in practical applications, the occurrence of short-term voltage instability following a disturbance is minimal, leading to a significant class imbalance problem and a consequent decline in classifier performance. This work proposes a Transformer-based STVSA method to address this challenge. By utilizing the basic Transformer architecture, a stability assessment Transformer (StaaT) is developed {as a classification model to reflect the correlation between the operational states of the system and the resulting stability outcomes}. To combat the negative impact of imbalanced datasets, this work employs a conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced, representative training set for the classifier. Semi-supervised clustering learning is imple
    
[^83]: 自我评估的自适应改进LLMs中的选择性预测

    Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])

    [http://arxiv.org/abs/2310.11689](http://arxiv.org/abs/2310.11689)

    本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。

    

    大型语言模型（LLMs）在自然语言理解和生成等多种任务中取得了巨大进展。然而，在高风险决策场景中仍然限于其潜在的错误。选择性预测是一种可以通过在LLMs不确定时使其避免预测而提高其可靠性的技术。在本文中，我们提出了一种新颖的自我评估的自适应框架，以提高LLMs的选择性预测性能。我们的框架基于使用参数效率调整来适应特定任务并改进其自我评估能力的思想。我们在各种问答（QA）数据集上评估了我们的方法，并展示其优于最先进的选择性预测方法。例如，在CoQA基准测试中，我们的方法将AUACC从91.23%提高到92.63%，并将AURO

    Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
    
[^84]: Softmax的优越性：揭示其相对于线性注意力的性能优势

    Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention. (arXiv:2310.11685v1 [cs.CL])

    [http://arxiv.org/abs/2310.11685](http://arxiv.org/abs/2310.11685)

    通过对softmax和线性注意力机制进行全面比较分析，本论文揭示了softmax注意力在大多数情况下优于线性注意力的潜在原因。

    

    大型Transformer模型在许多自然语言处理任务中取得了最先进的结果。在Transformer架构的重要组成部分中，注意力机制通过利用softmax函数捕捉序列中的标记交互起着关键作用。相反，线性注意力通过线性复杂度近似softmax操作，提供了一种计算效率更高的替代方法。然而，与传统的softmax注意力机制相比，它在性能上表现出明显的降级。在本文中，我们对这两种注意力机制进行了全面的比较分析，揭示了softmax注意力在大多数情况下优于线性注意力的潜在原因。

    Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.  Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.  In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.
    
[^85]: 无限时域平均奖励强化学习的量子加速

    Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])

    [http://arxiv.org/abs/2310.11684](http://arxiv.org/abs/2310.11684)

    本研究探索了无限时域平均奖励强化学习中量子加速的潜力。我们提出了一种创新的量子框架，通过高效的量子均值估计技术，实现了指数级改进的遗憾保证。所提出的量子算法相较于经典算法，在遗憾界限上有显著改进。

    

    本文研究量子加速在解决无限时域Markov决策过程（MDPs）中提高平均奖励结果的潜力。我们引入了一种创新的量子框架，用于代理与未知MDP的互动，扩展了传统的交互范式。我们的方法涉及设计一种基于乐观主导的具有量子信号的表格强化学习算法，通过高效的量子均值估计技术获取代理获取的量子信号。通过深入的理论分析，我们证明了量子均值估计的优势能够在无限时域强化学习中导致遗憾保证的指数进展。具体地，所提出的量子算法实现了一个遗憾界为$\tilde{\mathcal{O}}(1)$的性能，这是相对于经典对应算法所展示的$\tilde{\mathcal{O}}(\sqrt{T})$界限的显著改进。

    This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
    
[^86]: 使用经验分类训练非马尔可夫任务

    Using Experience Classification for Training Non-Markovian Tasks. (arXiv:2310.11678v1 [cs.LG])

    [http://arxiv.org/abs/2310.11678](http://arxiv.org/abs/2310.11678)

    该论文提出了一种使用经验分类的方法来训练非马尔可夫任务。通过将非马尔可夫任务转化为有限轨迹上的线性时态逻辑表达，并利用优先化经验回放技术改善训练过程，以实现非马尔可夫奖励的目标逻辑。实验证明了该方法的可行性和有效性。

    

    不同于标准的强化学习模型，许多实际任务是非马尔可夫的，其奖励是基于状态历史而不仅仅是当前状态。解决非马尔可夫任务在实际应用中（如自动驾驶、金融交易和医学诊断）中经常面临挑战。我们提出了一种新颖的强化学习方法，以实现在有限轨迹上表达的非马尔可夫奖励的目标逻辑LTL$_f$（线性时态逻辑）。为此，我们引入了一种从LTL$_f$到MDPs（马尔可夫决策过程）的线性复杂度的编码，以利用先进的强化学习算法。然后，我们利用基于自动机结构（语义等价于LTL$_f$规范）的优先化经验回放技术来改善训练过程。我们通过对几个带有非马尔可夫任务的基准问题进行实证评估，以展示我们方法的可行性和有效性。

    Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.
    
[^87]: 无限时间无折扣奖励马尔可夫决策过程自然策略梯度算法的改进样本复杂度分析

    Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])

    [http://arxiv.org/abs/2310.11677](http://arxiv.org/abs/2310.11677)

    本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。

    

    本文考虑设计样本高效的学习算法，用于无限时间无折扣奖励马尔可夫决策过程。具体地，我们提出了加速自然策略梯度（ANPG）算法，利用加速随机梯度下降过程来获取自然策略梯度。ANPG算法在一般参数化情况下实现了O(ε^{-2})的样本复杂度和O(ε^{-1})的迭代复杂度，其中ε定义了最优性误差。这将样本复杂度提高了一个log(1/ε)的因子。ANPG是一个一阶算法，并且不需要现有文献中可能无法验证的重要性采样(IS)权重方差上界的假设。在无Hessian和无IS算法类中，ANPG超过了已知样本复杂度的一个O(ε^{-\frac{1}{2}})的因子，并同时达到了它们的最新技术成果。

    We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
    
[^88]: PREM:一种简单而有效的节点级图异常检测方法。

    PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])

    [http://arxiv.org/abs/2310.11676](http://arxiv.org/abs/2310.11676)

    PREM是一种简单而有效的节点级图异常检测方法，它通过简化图异常检测的过程，减少了时间和内存消耗，同时保持了强大的异常检测能力。

    

    节点级图异常检测在识别医学、社交网络和电子商务等各个领域中的图结构数据中的异常节点起着关键作用。然而，由于异常的多样性以及标注数据的匮乏，已有的基于重构和对比学习的方法往往在效率方面存在问题，这源于它们复杂的目标和繁琐的模块。为了提高图异常检测的效率，我们引入了一种简单的方法，称为PREprocessing and Matching（简称PREM）。我们的方法简化了图异常检测，减少了时间和内存的消耗，同时保持了强大的异常检测能力。PREM由两个模块组成：预处理模块和邻居匹配模块。PREM在训练过程中消除了消息传递传播的必要性，并采用了简单的对比损失函数，从而大大减少了训练时间和内存使用量。此外，

    Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
    
[^89]: SOTOPIA: 交互式评估语言智能中的社交智能

    SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])

    [http://arxiv.org/abs/2310.11667](http://arxiv.org/abs/2310.11667)

    SOTOPIA是一个用于评估语言智能中的社交智能的交互式环境。通过模拟复杂的社交互动，并使用全面的评估框架，我们发现不同模型之间的社交智能存在显著差异，特别是在SOTOPIA-hard情景下。GPT-4在这个子集上的目标完成率较低。

    

    人类是社交的存在；我们在日常互动中追求社交目标，这是社交智能的关键方面。然而，人工智能系统在这个领域的能力仍然难以捉摸。我们提出了SOTOPIA，一个开放式环境，用于模拟人工智能代理之间的复杂社交互动并评估它们的社交智能。在我们的环境中，代理人扮演角色，在各种场景下相互协作、合作、交流和竞争，以实现复杂的社交目标。我们模拟了LLM-based代理人与人类之间在这个任务空间内的角色扮演互动，并使用一个名为SOTOPIA-Eval的整体评估框架对它们的表现进行评估。通过SOTOPIA，我们发现这些模型在社交智能方面存在显著差异，并确定了SOTOPIA的一个子集，即SOTOPIA-hard，对所有模型来说都具有挑战性。我们发现在这个子集上，GPT-4的目标完成率显著较低。

    Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
    
[^90]: Hetero$^2$Net: 面向异构图的异质属性感知表示学习

    Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs. (arXiv:2310.11664v1 [cs.LG])

    [http://arxiv.org/abs/2310.11664](http://arxiv.org/abs/2310.11664)

    Hetero$^2$Net是一种面向异构图的异质属性感知表示学习方法，通过使用元路径识别异构图中的异质性，并提出了度量指标来描述异质性水平，以解决常见图神经网络在处理具有异质性的图中的限制。

    

    实际世界的图通常非常复杂，全局结构中存在异构性，而局部邻域内则表现出强烈的异质性。尽管有越来越多的文献揭示了常见图神经网络（GNNs）在处理具有异质性的同构图时的局限性，但在研究异构图中的异质性属性方面却鲜有研究。为填补这一研究空白，我们使用元路径识别了异构图中的异质性，并提出了两个实用的度量指标来定量描述异质性水平。通过对几个展示不同异质性水平的真实世界异构图的深入研究，我们发现继承了很多设计用于同构图的GNNs机制的异构图神经网络（HGNNs）在处理具有异质性或低度同质性的异构图时无法泛化。为了解决这一挑战，我们提出了Hetero$^2$Net，一个面向异构图的异质属性感知表示学习方法。

    Real-world graphs are typically complex, exhibiting heterogeneity in the global structure, as well as strong heterophily within local neighborhoods. While a growing body of literature has revealed the limitations of common graph neural networks (GNNs) in handling homogeneous graphs with heterophily, little work has been conducted on investigating the heterophily properties in the context of heterogeneous graphs. To bridge this research gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs exhibiting varying levels of heterophily, we have observed that heterogeneous graph neural networks (HGNNs), which inherit many mechanisms from GNNs designed for homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily or low level of homophily. To address the challenge, we present Hetero$^2$Net, a h
    
[^91]: 针对具有高基数分类特征的计数数据的主题专用深度神经网络

    Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features. (arXiv:2310.11654v1 [cs.LG])

    [http://arxiv.org/abs/2310.11654](http://arxiv.org/abs/2310.11654)

    本文提出了一种针对计数数据的主题专用深度神经网络，通过引入伽玛随机效应来提高预测性能，并同时获得了固定参数的最大似然估计和随机效应的最佳无偏预测器。该方法可以快速处理高基数分类特征的聚类计数数据，并且可以轻松实现最先进的网络架构。

    

    由于实际数据通常呈现出相关性，针对主题特定的预测使用深度神经网络（DNNs）的兴趣日益增长，但传统DNN框架通常忽视了这一点。在本文中，我们提出了一种新颖的层次似然学习框架，将伽玛随机效应引入到泊松DNN中，以通过捕捉输入变量的非线性效应和主题特定的聚类效应来提高预测性能。所提出的方法通过优化单个目标函数同时获得了固定参数的最大似然估计和随机效应的最佳无偏预测器。该方法能够快速处理涉及高基数分类特征的聚类计数数据的端到端算法。此外，最先进的网络架构可以轻松实现到所提出的h-likelihood框架当中。例如，我们引入了多头注意力层和一个稀疏层

    There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a spars
    
[^92]: 使用Transformer的自由文本按键身份验证：架构和损失函数的比较研究

    Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions. (arXiv:2310.11640v1 [cs.CR])

    [http://arxiv.org/abs/2310.11640](http://arxiv.org/abs/2310.11640)

    本研究提出了一种基于Transformer的网络，通过自注意机制从按键序列中提取信息特征，改进了传统的循环神经网络在按键身份验证中的性能。实验结果表明，在Aalto桌面按键数据集上，采用批量-全部三重损失和余弦距离的双重编码器架构达到了最佳性能。

    

    按键生物特征是一种有前景的用户识别和验证方法，利用个体的输入行为中的独特模式。在本文中，我们提出了一种基于Transformer的网络，利用自注意机制从按键序列中提取信息特征，超过传统的循环神经网络的性能。我们探索了两种不同的架构，即双重编码器和交叉编码器，并比较了它们在按键身份验证中的有效性。此外，我们还研究了不同的损失函数，包括三重，批量-全部三重和WDCL损失，以及不同的距离度量标准，如欧氏距离，曼哈顿距离和余弦距离。这些实验使我们能够优化训练过程并提高模型的性能。为了评估我们提出的模型，我们使用了Aalto桌面按键数据集。结果表明，使用批量-全部三重损失和余弦距离的双重编码器架构实现了最佳性能。

    Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals' typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best perf
    
[^93]: 平衡行为：通过查询和画廊库减少跨模态检索中的中心程度问题

    Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks. (arXiv:2310.11612v1 [cs.LG])

    [http://arxiv.org/abs/2310.11612](http://arxiv.org/abs/2310.11612)

    本论文提出了一种解决跨模态检索中中心程度问题的后处理解决方案，通过使用查询和画廊样本构建的两个库，以及基于这两个库的归一化方法，成功降低了中心点的出现。

    

    在这项工作中，我们提出了一种后处理解决方案来解决跨模态检索中的中心程度问题。这是一个现象，其中一个小部分画廊数据点经常被检索，导致检索性能下降。我们首先从理论上证明了将画廊和查询数据都纳入考虑解决中心程度问题的必要性，因为中心点始终与画廊和查询数据具有高相似性。其次，基于我们的理论结果，我们提出了一种新的框架，双库归一化（DBNorm）。之前的工作尝试通过仅利用查询样本来减轻中心程度问题，而DBNorm利用从查询样本和画廊样本构建的两个库来减少推理过程中中心点的出现。接下来，为了补充DBNorm，我们引入了两种新方法，即双反softmax和双动态反softmax，用于基于这两个库进行相似度归一化。具体而言，我们提出的方法减少了中心点和查询样本之间的相似性。

    In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel framework, Dual Bank Normalization (DBNorm). While previous work has attempted to alleviate hubness by only utilizing the query samples, DBNorm leverages two banks constructed from the query and gallery samples to reduce the occurrence of hubs during inference. Next, to complement DBNorm, we introduce two novel methods, dual inverted softmax and dual dynamic inverted softmax, for normalizing similarity based on the two banks. Specifically, our proposed methods reduce the similarity between hubs and qu
    
[^94]: 支持参数共享进行模型压缩的辩护

    In defense of parameter sharing for model-compression. (arXiv:2310.11611v1 [cs.LG])

    [http://arxiv.org/abs/2310.11611](http://arxiv.org/abs/2310.11611)

    本文综合评估了使用RPS、剪枝技术和构建较小模型等方法在内存和准确性之间的权衡，发现RPS在整个压缩范围内始终优于其他方法，并在高压缩场景中表现尤为突出。

    

    在考虑模型架构时，有多种方法可以减少其内存占用。历史上，流行的方法包括选择较小的架构和通过剪枝创建稀疏网络。最近，随机参数共享（RPS）方法在训练开始时的模型压缩中受到了关注。在本文中，我们全面评估了在RPS、剪枝技术和构建较小模型之间的内存和准确性之间的权衡。我们的研究结果表明，在整个压缩范围内，无论是数据驱动还是模型无关，RPS始终优于/与较小模型和所有信息稍微充足的剪枝策略如MAG、SNIP、SYNFLOW和GRASP相匹配。这种优势在更高的压缩场景中尤为明显。值得注意的是，即使与高度信息充足的剪枝技术如Lottery Ticket Rewinding（LTR）相比，RPS在高压缩设置中也展现出更优异的性能。这指出了其内在性能。

    When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent c
    
[^95]: 从天然存在的同位素旋转光谱中的三维结构测定来看，具有反射等变性的扩散方法

    Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance. (arXiv:2310.11609v1 [cs.LG])

    [http://arxiv.org/abs/2310.11609](http://arxiv.org/abs/2310.11609)

    该论文介绍了一种具有反射等变性的扩散方法，用于从同位素旋转光谱中测定有机分子的三维结构。这种方法可以解决由于缺失正负符号而难以确定实际结构的问题。

    

    结构测定对于识别未知的有机分子是必要的，例如天然产物、法医样本、星际介质和实验室合成物等。旋转光谱通过提供关于小有机分子的惯量矩来进行结构测定，从而提供精确的三维信息。利用这些惯量矩，Kraitchman分析确定同位素置换坐标，这些坐标是具有天然同位素丰度的所有原子的无符号|x|、|y|和|z|坐标，包括碳、氮和氧。虽然无符号的置换坐标可以验证结构的猜测，但是缺失的正负符号使得仅凭置换坐标难以确定实际的结构。为了解决这个逆问题，我们开发了KREED（Kraitchman具有反射等变性的扩散），这是一个生成性扩散模型，可以从分子的分子式、惯量矩和无符号的置换坐标中推断出分子的完整三维结构。

    Structure determination is necessary to identify unknown organic molecules, such as those in natural products, forensic samples, the interstellar medium, and laboratory syntheses. Rotational spectroscopy enables structure determination by providing accurate 3D information about small organic molecules via their moments of inertia. Using these moments, Kraitchman analysis determines isotopic substitution coordinates, which are the unsigned $|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance, including carbon, nitrogen, and oxygen. While unsigned substitution coordinates can verify guesses of structures, the missing $+/-$ signs make it challenging to determine the actual structure from the substitution coordinates alone. To tackle this inverse problem, we develop KREED (Kraitchman REflection-Equivariant Diffusion), a generative diffusion model that infers a molecule's complete 3D structure from its molecular formula, moments of inertia, and unsigned substitution coordi
    
[^96]: TK-KNN：一种用于半监督意图分类的平衡距离伪标签方法

    TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification. (arXiv:2310.11607v1 [cs.LG])

    [http://arxiv.org/abs/2310.11607](http://arxiv.org/abs/2310.11607)

    TK-KNN是一种平衡的距离伪标签方法，通过在嵌入空间中使用排名的方法，能够在减少成本的同时保持类别之间的平衡。实验证明，TK-KNN的性能优于现有模型。

    

    在现代技术中，检测对话系统中的意图变得越来越重要。这些系统往往产生大量的未标记数据，手动标记这些数据需要大量的人力。半监督方法试图通过使用在少量标记示例上训练的模型，并为预测置信度高于某个阈值的未标记示例分配伪标签来减少成本。然而，这些方法的一个特别危险的后果是在各类别之间选择不平衡的示例集，可能导致标签质量较差。在本研究中，我们描述了一种基于嵌入空间距离的更稳健的伪标签方法Top-K K-Nearest Neighbor (TK-KNN)，通过基于排名的方法在类别之间维持一个平衡的伪标签示例集。在几个数据集上的实验表明，TK-KNN的性能优于现有模型。

    The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudo-labels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, 
    
[^97]: DIAR: 使用Swin Transformers进行深度图像对齐和重建

    DIAR: Deep Image Alignment and Reconstruction using Swin Transformers. (arXiv:2310.11605v1 [cs.CV])

    [http://arxiv.org/abs/2310.11605](http://arxiv.org/abs/2310.11605)

    本文介绍了DIAR方法，利用Swin Transformers对一系列失真图像进行对齐和重建。通过训练模型分析序列图像数据，并利用注意力图检测相关内容并区分异常值和伪像。同时，还探索了使用神经特征图作为传统关键点检测器的替代方法。

    

    在拍摄一些被遮挡内容的图像时，通常会面临的问题是每个单独的图像帧都包含不需要的伪像，但是如果正确对齐和聚合一系列图像，就可以获得所有相关信息。在本文中，我们尝试构建一个深度学习流水线，同时对一系列失真图像进行对齐和重建。我们创建了一个包含图像失真（如光照、镜面反射、阴影和遮挡）的数据集，并且为每个图像创建了相应的地面真值单应性作为标签。我们使用这个数据集训练Swin Transformer模型来分析序列图像数据。注意力图使得模型能够检测到相关的图像内容并将其区分出来以与异常值和伪像区分开。我们进一步探索使用神经特征图作为传统关键点检测器的替代方法。经过训练的卷积层的特征图提供了可以用来寻找图像的密集描述符的方法。

    When taking images of some occluded content, one is often faced with the problem that every individual image frame contains unwanted artifacts, but a collection of images contains all relevant information if properly aligned and aggregated. In this paper, we attempt to build a deep learning pipeline that simultaneously aligns a sequence of distorted images and reconstructs them. We create a dataset that contains images with image distortions, such as lighting, specularities, shadows, and occlusion. We create perspective distortions with corresponding ground-truth homographies as labels. We use our dataset to train Swin transformer models to analyze sequential image data. The attention maps enable the model to detect relevant image content and differentiate it from outliers and artifacts. We further explore using neural feature maps as alternatives to classical key point detectors. The feature maps of trained convolutional layers provide dense image descriptors that can be used to find 
    
[^98]: 语言模型作为零-shot轨迹生成器

    Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])

    [http://arxiv.org/abs/2310.11604](http://arxiv.org/abs/2310.11604)

    本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。

    

    近期研究表明，大型语言模型（LLMs）在给予低级技能选择时能够作为机器人的高级规划器。然而，通常认为LLMs不具备足够的知识来用于低级轨迹生成。在本研究中，我们详细探讨了这种假设，并调查了当给予LLM（GPT-4）仅能访问物体检测和分割视觉模型时，它能否直接预测一系列密集的末端执行器姿态用于操作技能。我们研究了一个单一的任务不可知提示，没有任何上下文示例、运动原语或外部轨迹优化器，它在26个真实世界的基于语言的任务中的表现，如“打开瓶盖”和“用海绵擦拭盘子”，以及我们调查了这个提示中哪些设计选择最有效。我们的结论突破了对LLMs在机器人技术上的限制，并首次揭示了LLMs确实具有对操作任务的理解能力。

    Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
    
[^99]: Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])

    Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])

    [http://arxiv.org/abs/2310.11594](http://arxiv.org/abs/2310.11594)

    本文研究了联邦学习中对抗性训练和后门攻击的交叉点，引入了Adversarial Robustness Unhardening（ARU），通过有意介入分散式训练过程中破坏模型的鲁棒性，使模型更容易受到更广泛的逃避攻击。

    

    在当今的数据驱动环境中，维护用户隐私和释放数据潜力之间微妙的平衡成为一个重要关注点。联邦学习是一种以隐私为中心的解决方案，它实现了协作模型训练而无需共享数据。这种分散式方法带来了安全挑战，特别是恶意实体注入损坏数据的中毒和后门攻击。我们的研究最初受到测试时间逃避攻击的启发，探讨了联邦学习中对抗性训练和后门攻击的交叉点，引入了Adversarial Robustness Unhardening（ARU）。ARU被一部分对手使用，以有意介入分散式训练过程中破坏模型的鲁棒性，使模型更容易受到更广泛的逃避攻击。我们进行了广泛的实证实验，评估了ARU对对抗性训练和现有的鲁棒聚合防御策略对中毒和后门攻击的影响。

    In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning a
    
[^100]: 使用大型语言模型自动评价个性化文本生成

    Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])

    [http://arxiv.org/abs/2310.11593](http://arxiv.org/abs/2310.11593)

    这项研究提出了一种使用大型语言模型自动评价个性化文本生成的方法。传统的自动评价指标无法捕捉个性化质量的微妙差别，而人工判断又昂贵且困难。因此，本研究提出了一种新颖的评估方法，能够自动测量个性化、质量和相关性这三个重要语义方面。

    

    个性化文本生成提供了一种针对用户个人背景交付内容的专门机制。尽管在这个领域的研究进展迅速，但评估仍然是一个挑战。传统的自动评价指标（如BLEU和ROUGE）主要衡量与人工参考文本的词汇相似度，并不能区分个性化与其他微妙的语义方面，因此无法捕捉个性化生成内容质量的细微差别。另一方面，人工判断是昂贵的，特别是在个性化评估领域。受到这些挑战的启发，我们探索了使用大型语言模型（LLMs）来评估个性化文本生成，并检验它们理解细致的用户背景的能力。我们提出了AuPEL，一种新颖的评估方法，将生成文本的个性化、质量和相关性三个主要语义方面提取并自动测量。

    Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as
    
[^101]: 探索在导航场景下推断用户对机器人性能的印象

    Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])

    [http://arxiv.org/abs/2310.11590](http://arxiv.org/abs/2310.11590)

    本研究拟通过非语言行为提示和机器学习技术预测人们对机器人行为印象，并提供了一个数据集和分析结果，发现在导航场景中，空间特征是最关键的信息。

    

    人们对机器人性能的印象通常通过调查问卷来衡量。作为一种更可扩展且成本效益更高的替代方案，我们研究了使用非语言行为提示和机器学习技术预测人们对机器人行为印象的可能性。为此，我们首先提供了SEAN TOGETHER数据集，该数据集包括在虚拟现实模拟中人与移动机器人相互作用的观察结果，以及用户对机器人性能的5点量表评价。其次，我们对人类和监督学习技术如何基于不同的观察类型（例如面部、空间和地图特征）来预测感知到的机器人性能进行了分析。我们的结果表明，仅仅面部表情就能提供关于人们对机器人性能印象的有用信息；但在我们测试的导航场景中，空间特征是这种推断任务最关键的信息。

    Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
    
[^102]: 用语言模型引导获取人类偏好

    Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])

    [http://arxiv.org/abs/2310.11589](http://arxiv.org/abs/2310.11589)

    本文介绍了一种生成式主动任务引导（GATE）的学习框架，该框架通过与用户进行自由形式的、基于语言的交互来引导和推断预期行为。在实验中展示，通过GATE引导的语言模型通常比用户编写的提示或标签更具信息量。

    

    语言模型可以通过使用标注示例或自然语言提示来执行目标任务。但是，在选择示例或撰写提示时可能具有挑战性——特别是在涉及异常情况、要求精确表达模糊偏好或需要准确的语言模型行为认知的任务中。我们提出使用*语言模型本身*来引导任务规范的过程。在本文中，我们介绍**生成式主动任务引导（GATE）**：一种学习框架，在其中模型通过与用户进行自由形式的、基于语言的交互来引导并推断预期行为。我们在三个领域研究了GATE：电子邮件验证、内容推荐和道德推理。在预先注册的实验中，我们展示了提示执行GATE的语言模型（例如通过生成开放式问题或合成信息丰富的边界案例）所引发的响应通常比用户编写的提示或标签更具信息量。用户报告称，交互式任务引导的方法能够有效地帮助他们表达偏好和指导模型。

    Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitat
    
[^103]: 什么是一个好问题？基于任务的询问与事实级遮蔽。

    What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])

    [http://arxiv.org/abs/2310.11571](http://arxiv.org/abs/2310.11571)

    本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。

    

    提问是现实生活中合作推理任务（如问答）的重要组成部分。例如，一个法律助手聊天机器人在没有用户情况的具体信息的情况下可能无法提供准确的建议。然而，通常会直接使用大型语言模型来解决推理任务，而不会向用户或第三方提出后续问题。我们将这个问题称为基于任务的询问（TOA）。零-shot聊天模型可以执行TOA，但它们的训练主要基于下一个词预测，而不是问题是否对成功的合作有帮助。为了能够训练和评估TOA模型，我们提出了自然语言任务导向询问的定义和框架，即生成能够为推理任务提供有用答案的问题的问题。我们还提出了事实级遮蔽（FLM）的方法，通过省略特定的部分将自然语言数据集转换为自我监督的TOA数据集。

    Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
    
[^104]: 当刚性成为问题：软一致性正则化用于概率分层时间序列预测

    When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])

    [http://arxiv.org/abs/2310.11569](http://arxiv.org/abs/2310.11569)

    提出了一个新的概率分层时间序列预测模型，该模型能够有效建模和预测具有层次化关系的多变量时间序列。相较于现有方法，该模型不仅考虑点预测，还能提供经过良好校准的概率预测分布，并且在建模过程中考虑了预测分布的相关性。

    

    概率分层时间序列预测是时间序列预测的重要变体，其目标是对具有层次化关系的多变量时间序列进行建模和预测。大多数方法关注点预测，并未提供经过良好校准的概率预测分布。最近的概率预测方法也在点预测和分布样本中施加层次关系，但未考虑预测分布的相关性。以往的研究也默认数据集总是与给定的层次关系保持一致，并未适应显示出偏离此假设的真实世界数据集。我们填补了这两个空白，并提出了PROFHiT模型，它是一个完全概率的分层预测模型，同时对整个层次的预测分布进行建模。PROFHiT使用灵活的概率贝叶斯方法，并引入了一种新颖的分布一致性正则化方法。

    Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
    
[^105]: 具有神经感知机制的部分可观测随机博弈

    Partially Observable Stochastic Games with Neural Perception Mechanisms. (arXiv:2310.11566v1 [cs.GT])

    [http://arxiv.org/abs/2310.11566](http://arxiv.org/abs/2310.11566)

    本研究提出了神经符号化部分可观测随机博弈（NS-POSGs）模型，通过融合感知机制解决了多智能体序列决策中的部分可观测性问题。其中，我们专注于一种只有部分观测信息的智能体和一种完全观测的智能体的单方面设置，并提出了一种近似计算NS-POSGs值的新方法。

    

    随机博弈是一个为多智能体在不确定性下进行序列决策的模型。然而在现实中，智能体对环境只有部分可观测性，这使得问题在计算上具有挑战性，即使在部分可观测马尔可夫决策过程的单智能体环境中也是如此。此外，在实践中，智能体越来越多地使用基于数据的方法，例如在连续数据上训练的神经网络来感知环境。为了解决这个问题，我们提出了神经符号化部分可观测随机博弈（NS-POSGs）的模型，这是连续空间并发随机博弈的一种变体，明确地融入了感知机制。我们专注于单方面的设置，包含了一个具有离散、基于数据驱动的观测和一个具有连续观测的充分了解的智能体。我们提出了一种名为单边NS-HSVI的基于点的方法，用来近似计算单方面NS-POSGs的值，并进行了实现。

    Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it ba
    
[^106]: 具有不确定性量化预测的在线算法

    Online Algorithms with Uncertainty-Quantified Predictions. (arXiv:2310.11558v1 [cs.LG])

    [http://arxiv.org/abs/2310.11558](http://arxiv.org/abs/2310.11558)

    这篇论文研究了如何在设计在线算法时最佳利用不确定性量化预测，并提出了一种考虑预测概率性的在线算法设计方法。

    

    具有预测的在线算法已成为算法的超越最坏情况分析领域的热门话题。这些算法利用对未来的预测来获得高质量的性能保证，当预测良好时，同时在预测任意差的情况下仍保持界限最坏情况保证。一般来说，算法被认为对预测质量不知情。然而，机器学习文献中的最新发展已研究了提供对机器学习预测进行的不确定性量化的技术，即描述模型对其质量的确定程度。本文考察了如何在在线算法设计中最佳利用不确定性量化预测的问题。具体而言，我们考虑了用描述基本事实落在某个范围内的不确定性量化增强预测的情况，并设计了具有这些概率性的在线算法。

    Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic
    
[^107]: 面向带有强对抗损失和强盗反馈的对抗性线性MDPs的最优遗憾

    Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback. (arXiv:2310.11550v1 [cs.LG])

    [http://arxiv.org/abs/2310.11550](http://arxiv.org/abs/2310.11550)

    研究带有对抗损失和强盗反馈的线性MDPs问题，提出了两种算法，分别达到了$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$和$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$的遗憾性能。

    

    我们研究了在线强化学习中的线性马尔可夫决策过程，并考虑了对抗性损失和强盗反馈，没有事先了解转换或访问模拟器的先验知识。我们引入了两种算法，相较于现有方法，它们都能取得更好的遗憾性能。第一种算法虽然计算效率低，但能保证$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$的遗憾性能，其中$K$是回合数。这是该设置下第一个具有最佳$K$依赖性的结果。第二种算法基于策略优化框架，能保证$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$的遗憾性能，并且计算效率高。我们的两个结果都显著改善了现有最先进方法：Kong等人[2023]的计算效率低的算法，其遗憾性能为$\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$，其中$\lambda_{\min}$是问题相关常数。

    We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$ regret, for some problem-dependent constant $\lam
    
[^108]: 软件生成数据中的偏差与误差减轻：运用生成代码模型的高级搜索与优化框架

    Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models. (arXiv:2310.11546v1 [cs.SE])

    [http://arxiv.org/abs/2310.11546](http://arxiv.org/abs/2310.11546)

    本文提出了一种高级搜索和优化框架，旨在通过生成代码模型来减轻软件生成数据中的偏差和误差。通过纠正之前版本中的问题，该框架可以改善数据分析和生成软件系统的质量。

    

    数据生成和分析是许多行业和学科的基本方面，从企业的战略决策到物理和社会科学的研究。然而，使用软件和算法生成的数据可能会受到偏差和错误的影响。这些问题可能出现在原始软件中，可能由于默认设置与具体需求不一致，甚至可能出现在底层理论和模型中。本文提出了一个高级搜索和优化框架，旨在生成和选择能够纠正前版本中的错误和偏差的最佳源代码，以解决数据分析和生成的软件系统中的典型问题，尤其是企业和数据科学领域的软件系统。在同一个软件系统上多次应用该框架将逐步提高输出结果的质量。它以Solomonoff归纳作为坚实的理论基础，进行扩展。

    Data generation and analysis is a fundamental aspect of many industries and disciplines, from strategic decision making in business to research in the physical and social sciences. However, data generated using software and algorithms can be subject to biases and errors. These can be due to problems with the original software, default settings that do not align with the specific needs of the situation, or even deeper problems with the underlying theories and models. This paper proposes an advanced search and optimization framework aimed at generating and choosing optimal source code capable of correcting errors and biases from previous versions to address typical problems in software systems specializing in data analysis and generation, especially those in the corporate and data science world. Applying this framework multiple times on the same software system would incrementally improve the quality of the output results. It uses Solomonoff Induction as a sound theoretical basis, extend
    
[^109]: MUST&P-SRL: 多语言和统一音节标记的文本和音韵领域中的语音表示学习

    MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])

    [http://arxiv.org/abs/2310.11541](http://arxiv.org/abs/2310.11541)

    本文提出了一种多语言和统一音节标记的文本和音韵领域中的语音表示学习方法，通过自动音节化单词并生成宝贵注释，适用于语音表示学习、语音单元发现和语音因素解缠。

    

    在本文中，我们提出了一种语言特征提取的方法，特别关注多种语言中自动音节化单词，并设计与强制对齐工具Montreal Forced Aligner（MFA）兼容。在文本和音韵领域中，我们的方法专注于从文本中提取音标转录、重音标记和统一的自动音节化。该系统采用了开源组件和资源构建。通过消融研究，我们证明了我们的方法在自动音节化多种语言（英语、法语和西班牙语）的单词方面的有效性。此外，我们将该技术应用于CMU ARCTIC数据集的转录中，生成了有助于语音表示学习、语音单元发现和语音因素解缠的宝贵注释，在线可用。

    In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-r
    
[^110]: 在无限时域的马尔可夫决策过程中，利用离线数据集进行高效在线学习：一种贝叶斯方法

    Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])

    [http://arxiv.org/abs/2310.11531](http://arxiv.org/abs/2310.11531)

    本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。

    

    本文研究了当存在一个离线数据集时，如何在无限时域设置下进行高效的在线强化学习问题。我们假设离线数据集是由一个专家生成的，但其能力水平未知，即它不是完美的，也不一定使用最优策略。我们展示了如果学习代理模拟专家使用的行为策略（由能力参数参数化），在累积遗憾最小化方面能取得明显更好的结果。我们建立了一个以 $\tilde{O}(\sqrt{T})$ 为缩放的精确有用PSRL算法遗憾的上界。这需要对贝叶斯在线学习算法在无限时域设置下进行新颖的先验相关遗憾分析。然后，我们提出了一种近似的Informed RLSVI算法，可以理解为使用离线数据集进行模仿学习，然后进行在线学习。

    In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
    
[^111]: 薄而深的高斯过程

    Thin and Deep Gaussian Processes. (arXiv:2310.11527v1 [stat.ML])

    [http://arxiv.org/abs/2310.11527](http://arxiv.org/abs/2310.11527)

    本文提出了一种薄而深的高斯过程方法，克服了传统方法的局限性，并在学习低维嵌入和解释性之间取得了平衡。

    

    高斯过程（GPs）可以提供一种可靠的方法来量化不确定性，具有易于解释的内核超参数，如长度尺度，可以控制函数值的相关距离。然而，选择合适的内核可能具有挑战性。深度高斯过程通过逐层参数化GP层的内核，避免了手动内核工程，使其能够学习解释输出数据的低维嵌入方法。沿用深度神经网络的架构，最常见的深度高斯过程逐层变形输入空间，但失去了浅层高斯过程的所有解释性。另一种构建方法是逐层参数化内核的长度尺度，提高了解释性，但最终放弃了学习低维嵌入的概念。不幸的是，这两种方法都容易受到特定的病态影响，可能会阻碍拟合并限制其可解释性。本文提出了一种新的综合方法

    Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis
    
[^112]: 群体偏好优化：大规模语言模型的少样本对齐

    Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])

    [http://arxiv.org/abs/2310.11523](http://arxiv.org/abs/2310.11523)

    这项研究介绍了一种名为群体偏好优化（GPO）的对齐框架，可以以少样本的方式将大规模语言模型（LLMs）引导到个别群体的偏好。通过在基本LLM上加入独立的transformer模块来预测群体偏好，并通过元学习进行训练，GPO经过严格评估验证了其有效性。

    

    大规模语言模型（LLMs）的许多应用，从聊天机器人到创意写作，都需要细致入微的主观判断，这些判断在不同群体之间可能存在显著差异。现有的对齐算法在每个群体上对齐的成本很高，对于实际应用场景而言，需要大量的群体特定偏好数据和计算资源。我们引入了群体偏好优化（GPO），这是一个对齐框架，可以以少样本的方式将语言模型引导到个别群体的偏好。在GPO中，我们使用一个独立的transformer模块来扩充基本LLM，用于预测群体对LLM生成内容的偏好。对于少样本学习，我们将这个模块参数化为一个上下文自回归的transformer，并通过元学习在多个群体上进行训练。我们通过严格的评估，使用不同规模的LLM在三个人类意见适应任务上验证了GPO的效果。

    Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
    
[^113]: 自动新闻摘要

    Automatic News Summerization. (arXiv:2310.11520v1 [cs.CL])

    [http://arxiv.org/abs/2310.11520](http://arxiv.org/abs/2310.11520)

    本研究比较和评估了抽取式和生成式方法在新闻文本摘要上的效果，并使用ROUGE得分进行质量评估。最佳表现模型被集成到一个Web应用程序中，以评估其在现实世界中的能力和用户体验。

    

    自然语言处理在现实世界中的应用正在蓬勃发展，其中之一是针对包括新闻文章在内的大型文本的文本摘要。本研究论文对新闻文本摘要的抽取式和生成式方法进行了广泛的比较评估，并重点分析了ROUGE得分。研究采用了CNN-Daily Mail数据集，其中包含了新闻文章和人工生成的参考摘要。评估使用ROUGE得分来评估生成摘要的效果和质量。在评估之后，我们将最佳表现模型集成到一个Web应用程序中，评估其在现实世界中的能力和用户体验。

    Natural Language Processing is booming with its applications in the real world, one of which is Text Summarization for large texts including news articles. This research paper provides an extensive comparative evaluation of extractive and abstractive approaches for news text summarization, with an emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail dataset, which consists of news articles and human-generated reference summaries. The evaluation employs ROUGE scores to assess the efficacy and quality of generated summaries. After Evaluation, we integrate the best-performing models on a web application to assess their real-world capabilities and user experience.
    
[^114]: 通过多矩阵可分解性在多人游戏中对自我对抗的保证

    Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])

    [http://arxiv.org/abs/2310.11518](http://arxiv.org/abs/2310.11518)

    这篇论文研究了多人游戏中自我对抗的保证问题，通过多矩阵可分解性，在满足一定条件的情况下，通过自我对抗学习的算法能够产生有界脆弱性的策略。

    

    自我对抗是一种机器学习在多智能体系统中的技术，其中学习算法通过与自身的副本交互来学习。自我对抗对于生成大量的学习数据很有用，但它的缺点是训练后学习者将面对的智能体可能与通过与自身交互时所期望的智能体行为截然不同。对于两人常和游戏的特殊情况，达到纳什均衡的自我对抗能够保证产生对任何训练后对手表现良好的策略；然而，对于多人游戏来说没有这样的保证存在。我们展示了在近似分解为一组两人常和游戏（称为多矩阵游戏）的游戏中，其中全局 $\epsilon$-纳什均衡在每个子游戏中都与纳什均衡有有界距离的情况下，通过自我对抗学习的无外部遗憾算法将产生一个有界脆弱性的策略。我们的结果首次确定了……

    Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
    
[^115]: 基于价值偏见的最大似然估计在折扣线性MDPs中的模型驱动强化学习

    Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs. (arXiv:2310.11515v1 [cs.LG])

    [http://arxiv.org/abs/2310.11515](http://arxiv.org/abs/2310.11515)

    这篇论文提出了一个基于价值偏见的最大似然估计方法，用于解决线性马尔可夫决策过程中的模型驱动强化学习问题。该方法在计算效率和后悔方面都取得了较好的性能。

    

    我们考虑无限时段的线性马尔可夫决策过程（MDPs），其中动态模型的转移概率可以通过预定义的低维特征映射进行线性参数化。虽然现有的基于回归的方法在理论上已被证明可以达到几乎最优的后悔，但由于在每个时间步骤中需要大量的优化运行，特别是当状态和动作空间较大时，它们在计算上效率低下。为了解决这个问题，我们提出通过基于价值偏见的最大似然估计（VBMLE）来解决线性MDPs问题，这是一种解决最大似然估计中已知闭环识别问题的经典模型驱动探索方法。我们正式证明了（i）VBMLE享有$\widetilde{O}(d\sqrt{T})$的后悔，其中$T$是时间段，$d$是模型参数的维度，以及（ii）VBMLE在计算上是...

    We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\widetilde{O}(d\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computat
    
[^116]: GenEval：用于评估文本到图像对齐的面向对象框架

    GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment. (arXiv:2310.11513v1 [cs.CV])

    [http://arxiv.org/abs/2310.11513](http://arxiv.org/abs/2310.11513)

    本论文介绍了GenEval，一种面向对象的框架，用于评估文本到图像生成模型的组成属性。通过利用当前的对象检测模型，我们可以在各种生成任务上评估文本到图像模型，并通过其他视觉判别模型进一步验证属性。

    

    最近，在扩散模型、多模态预训练和高效微调方面取得了突破，导致了文本到图像生成模型的爆炸性增长。由于人工评估昂贵且难以扩展，自动化方法对于评估越来越多的新模型至关重要。然而，目前大多数自动化评估指标（如FID或CLIPScore）仅提供图像质量或图像-文本对齐的整体度量，并不适用于细粒度或实例级别的分析。在本文中，我们引入了一个名为GenEval的面向对象的框架，用于评估图像的组成属性，如对象共现、位置、计数和颜色。我们展示了当前的对象检测模型可以用于在多种生成任务上评估文本到图像模型，并且得到了与人类的强一致性，其余的视觉判别模型可以链接到这个流程中，进一步验证像对象颜色这样的属性。然后，我们对几个开源的文本到图像模型进行评估。

    Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to
    
[^117]: Self-RAG: 通过自我反思学习检索、生成和评论

    Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])

    [http://arxiv.org/abs/2310.11511](http://arxiv.org/abs/2310.11511)

    Self-RAG是一种通过检索和自我反思提高语言模型质量和事实性的框架。

    

    尽管大型语言模型（LLMs）具有显著的能力，但由于它们完全依赖于它们所包含的参数化知识，因此往往会产生含有事实不准确性的响应。检索增强生成（RAG）是一种通过检索相关知识增强LM的临时方法，可以减少这些问题。然而，不加选择地检索并结合一定数量的检索段落，而不考虑检索是否必要或段落是否相关，会降低LM的多功能性或导致无效的响应生成。我们引入了一种称为Self-Reflective Retrieval-Augmented Generation （Self-RAG）的新框架，通过检索和自我反思提高LM的质量和事实性。我们的框架训练了一个单独的任意LM，它能够根据需求自适应地检索段落，并使用特殊的标记，称为反思标记，生成和反思检索的段落和自身的生成结果。

    Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM 
    
[^118]: 儿童阅读实时跟踪的端到端指针网络研究

    End-to-End real time tracking of children's reading with pointer network. (arXiv:2310.11486v1 [eess.AS])

    [http://arxiv.org/abs/2310.11486](http://arxiv.org/abs/2310.11486)

    本研究提出了一种用于儿童阅读的实时跟踪器模型，在语音跟踪的延迟方面具有较低的敏感性。通过使用指针网络和强制对齐生成训练信号，我们的模型可以准确地跟踪成人语音，并在儿童语音数据集上获得良好的效果。

    

    本研究探讨了如何高效地构建一个用于儿童语音的实时阅读跟踪器。之前提出的阅读跟踪器主要基于ASR的级联方法，我们提出了一个完全端到端的模型，使其对语音跟踪的延迟更少。我们采用指针网络，直接学习在流式语音中预测与真实文本位置对应的能力。为了训练这个指针网络，我们使用强制对齐在训练集上生成真实的训练信号，从而对读出的语音和被读文本进行对齐。在探索不同的强制对齐模型时，我们发现基于神经注意力的模型在对齐准确性上至少与蒙特利尔强制对齐器相当，但令人惊讶的是，它更适合用作指针网络的训练信号。我们的结果报告了一个成人语音数据集(TIMIT)和两个儿童语音数据集(CMU Kids和Reading Races)。我们的最佳模型能够以87.8%的准确率跟踪成人语音。

    In this work, we explore how a real time reading tracker can be built efficiently for children's voices. While previously proposed reading trackers focused on ASR-based cascaded approaches, we propose a fully end-to-end model making it less prone to lags in voice tracking. We employ a pointer network that directly learns to predict positions in the ground truth text conditioned on the streaming speech. To train this pointer network, we generate ground truth training signals by using forced alignment between the read speech and the text being read on the training set. Exploring different forced alignment models, we find a neural attention based model is at least as close in alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better training signal for the pointer network. Our results are reported on one adult speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading Races). Our best model can accurately track adult speech with 87.8% accuracy and t
    
[^119]: 基于集群联邦个性化的全脑影像组学用于脑肿瘤分割

    Whole-brain radiomics for clustered federated personalization in brain tumor segmentation. (arXiv:2310.11480v1 [eess.IV])

    [http://arxiv.org/abs/2310.11480](http://arxiv.org/abs/2310.11480)

    该论文提出了基于集群联邦个性化的全脑影像组学方法，通过考虑不同机构间和机构内特征转移问题，解决了联邦学习中的统计异质性带来的收敛问题和准确性损失。

    

    联邦学习及其在医学图像分割中的应用最近成为热门的研究课题。这种训练范式在参与机构的本地数据集之间存在统计异质性，导致收敛速度减慢，以及与经典训练相比潜在的准确性损失。为了缓解这种影响，联邦个性化作为每个机构的联邦优化逐渐崭露头角。我们提出了一种新颖的个性化算法，针对不同机构使用不同扫描仪和采集参数引起的特征转移问题进行优化。该方法是第一个同时考虑了机构间和机构内特征转移问题（同一个机构使用多个扫描仪）的方法。它基于在每个中心计算一系列放射组学特征，捕捉每个3D图像体素的全局纹理，然后通过聚类分析汇总从本地机构转移的所有特征向量。

    Federated learning and its application to medical image segmentation have recently become a popular research topic. This training paradigm suffers from statistical heterogeneity between participating institutions' local datasets, incurring convergence slowdown as well as potential accuracy loss compared to classical training. To mitigate this effect, federated personalization emerged as the federated optimization of one model per institution. We propose a novel personalization algorithm tailored to the feature shift induced by the usage of different scanners and acquisition parameters by different institutions. This method is the first to account for both inter and intra-institution feature shift (multiple scanners used in a single institution). It is based on the computation, within each centre, of a series of radiomic features capturing the global texture of each 3D image volume, followed by a clustering analysis pooling all feature vectors transferred from the local institutions to 
    
[^120]: 关于贝叶斯图神经网络在一致预测中的温度问题

    On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v1 [cs.LG])

    [http://arxiv.org/abs/2310.11479](http://arxiv.org/abs/2310.11479)

    该论文探讨了将温度参数纳入贝叶斯图神经网络在一致预测中的优势，以提供有效的不确定性量化。

    

    准确的不确定性量化对于图神经网络(GNNs)至关重要，特别是在高风险领域中经常使用GNNs的情况下。一致预测(CP)为任何黑盒模型提供了一个量化不确定性的有前途的框架。CP保证了一个预测集以所需的概率包含真实标签的形式的官方概率保证。然而，预测集的大小，即"低效率"，受到底层模型和数据生成过程的影响。另一方面，贝叶斯学习还基于估计的后验分布提供一个可信区域，但只有在模型正确指定的情况下，这个区域才是"良好校准"的。在一个最近的工作的基础上，该工作引入了一个缩放参数，用于从后验估计中构建有效的可信区域，我们的研究探讨了在CP框架中将一个温度参数纳入贝叶斯GNNs中的优势。

    Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP fra
    
[^121]: ASP: 用于高效AutoML的自动选择代理数据集

    ASP: Automatic Selection of Proxy dataset for efficient AutoML. (arXiv:2310.11478v1 [cs.LG])

    [http://arxiv.org/abs/2310.11478](http://arxiv.org/abs/2310.11478)

    本文提出了一个自动选择代理数据集框架 (ASP)，通过动态地找到信息丰富的代理子集来减小训练数据大小并节省AutoML处理时间，实验证明ASP在不同基准测试上获得了优于其他方法的结果。

    

    由于数据量的增加和多样有效的神经网络设计, 深度神经网络取得了巨大的成功。然而, 这也给计算带来了沉重的负担, 因为训练数据量与训练时间成正比。此外, 一个良好的模型需要重复尝试不同的结构设计和超参数, 即使使用了最先进的超参数优化算法和神经架构搜索算法, 这可能也需要大量的时间。本文提出了一个自动选择代理数据集框架 (ASP), 旨在在每个epoch动态地找到信息丰富的代理子集, 减小训练数据大小并节省AutoML处理时间。我们在CIFAR10、CIFAR100、ImageNet16-120和ImageNet-1k上验证了ASP的有效性和泛化性能, 通过对不同公共模型基准的实验结果表明ASP可以获得比其他方法更好的结果。

    Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other 
    
[^122]: Robust-MBFD：使用多个深度学习训练策略和一种新的双损失函数进行电机轴承故障检测的稳健深度学习系统

    Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function. (arXiv:2310.11477v1 [cs.LG])

    [http://arxiv.org/abs/2310.11477](http://arxiv.org/abs/2310.11477)

    本文提出了一种稳健的深度学习系统用于电机轴承故障检测，采用多个深度学习训练策略和一种新的双损失函数。通过对比评估不同系统并寻找最佳模型，我们展示了该系统对各种电机轴承故障的有效性。

    

    本文提出了对电机轴承故障检测（MBFD）进行全面分析的方法，该方法基于振动信号识别电机轴承的故障。首先，我们提出并评估了多种基于机器学习的MBFD系统。此外，我们还提出了三种基于深度学习的MBFD系统，分别探索了监督学习、半监督学习和无监督学习这三种训练策略。对提出的机器学习系统和深度学习系统进行了评估和比较，并找出了适用于MBFD任务的最佳模型。我们在包括美国机械故障预防技术协会（MFPT）、凯斯西储大学轴承中心（CWRU）和帕德博恩大学的电机驱动系统轴承损伤状态监测等不同基准数据集上进行了大量实验。

    This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn U
    
[^123]: 通过代码精炼实现程序翻译

    Program Translation via Code Distillation. (arXiv:2310.11476v1 [cs.SE])

    [http://arxiv.org/abs/2310.11476](http://arxiv.org/abs/2310.11476)

    本文提出了一种名为Code Distillation（CoDist）的创新模型，通过在一种语言无关的中间表示中捕捉代码的语义和结构等价关系，实现了程序翻译，克服了传统机器翻译和无监督神经机器翻译在数据对齐和IR多样性方面的挑战。

    

    软件版本迁移和程序翻译是大型代码库生命周期中重要且昂贵的部分。传统的机器翻译依赖于并行语料库进行监督翻译，但由于缺乏对齐数据，这在程序翻译中是不可行的。最近的无监督神经机器翻译技术通过包括反向翻译和低级编译器中间表示(IR)等技术克服了数据限制。这些方法分别面临着代码片段对齐中的噪声和IR的多样性所带来的重大挑战。在本文中，我们提出了一种名为Code Distillation（CoDist）的新模型，通过在一种语言无关的中间表示中捕捉代码的语义和结构等价关系。精炼的代码作为任何编程语言的翻译枢纽，在构建中通过简单应用精炼编译实现对所有可用源代码的并行语料库扩展。

    Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods face significant challenges due to the noise in code snippet alignment and the diversity of IRs respectively. In this paper we propose a novel model called Code Distillation (CoDist) whereby we capture the semantic and structural equivalence of code in a language agnostic intermediate representation. Distilled code serves as a translation pivot for any programming language, leading by construction to parallel corpora which scale to all available source code by simply applying the distillation compil
    
[^124]: 经典机器学习方法

    Classic machine learning methods. (arXiv:2310.11470v1 [cs.LG])

    [http://arxiv.org/abs/2310.11470](http://arxiv.org/abs/2310.11470)

    本章主要介绍了经典机器学习方法，包括监督学习和无监督学习。其中介绍了分类和回归的各种方法，以及解决过拟合问题的策略。

    

    在这一章中，我们介绍了主要的经典机器学习方法。本章的大部分内容都用于介绍用于分类和回归的监督学习技术，包括最近邻方法、线性回归和逻辑回归、支持向量机和基于树的算法。我们还描述了过拟合问题以及克服过拟合的策略。最后，我们简要概述了无监督学习方法，即用于聚类和降维的方法。

    In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.
    
[^125]: 提升二进制代码注释质量分类：整合生成式人工智能以提高准确性

    Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy. (arXiv:2310.11467v1 [cs.SE])

    [http://arxiv.org/abs/2310.11467](http://arxiv.org/abs/2310.11467)

    本报告通过整合生成的代码和注释对，改进二进制代码注释质量分类模型，提高准确性。

    

    本报告旨在通过整合生成的代码和注释对，改进二进制代码注释质量分类模型，以提高模型的准确性。数据集包括9048对用C语言编写的代码和注释，每对都被注释为“有用”或“无用”。此外，使用大型语言模型架构生成代码和注释对，并对这些生成的对进行标记以指示其实用性。此项工作的成果包括两个分类模型：一个利用原始数据集，另一个利用新生成的代码注释对和标记的扩充数据集。

    This report focuses on enhancing a binary code comment quality classification model by integrating generated code and comment pairs, to improve model accuracy. The dataset comprises 9048 pairs of code and comments written in the C programming language, each annotated as "Useful" or "Not Useful." Additionally, code and comment pairs are generated using a Large Language Model Architecture, and these generated pairs are labeled to indicate their utility. The outcome of this effort consists of two classification models: one utilizing the original dataset and another incorporating the augmented dataset with the newly generated code comment pairs and labels.
    
[^126]: 蛋白质三维图结构学习用于稳健的基于结构的蛋白质性质预测

    Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])

    [http://arxiv.org/abs/2310.11466](http://arxiv.org/abs/2310.11466)

    本文研究了蛋白质基于结构的性质预测中使用预测结构时性能下降的原因，并将其归因为结构嵌入偏差。

    

    蛋白质基于结构的性质预测已经成为各种生物学任务（如蛋白质功能预测和亚细胞定位估计）的一种有希望的方法。现有方法高度依赖实验蛋白质结构数据，在这些数据不可用的情况下失败。利用人工智能工具（如AlphaFold2）预测的蛋白质结构作为替代方案。然而，我们观察到目前的做法，即在推理过程中仅使用准确预测的结构，会导致预测准确性明显下降。虽然类似现象已经在一般领域（如计算机视觉）中进行了广泛研究作为模型的稳健性，但它们对蛋白质性质预测的影响尚未被探索。在本文中，我们首先从结构表示学习的角度研究了在利用预测的结构时性能下降的原因，将其归因为结构嵌入偏差。为了研究这个问题

    Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
    
[^127]: BaitBuster-Bangla:一个包含多特征和多模态分析的用于孟加拉语Clickbait检测的综合数据集

    BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])

    [http://arxiv.org/abs/2310.11465](http://arxiv.org/abs/2310.11465)

    本研究提出了一个大规模的孟加拉语YouTube clickbait多模态数据集，为研究人员提供了在低资源语言中建模clickbait现象的重要价值，并且可以开发出更复杂的跨语言检测方法。

    

    本研究提出了一个大规模的孟加拉语YouTube clickbait多模态数据集，通过使用YouTube API和Python网络自动化框架自动收集了253,070个数据点。该数据集包含了来自58个孟加拉语YouTube频道的单个视频的18个不同的特征，这些特征分类为元数据、主要内容、参与统计和标签。对这些特征进行了严格的预处理，去噪声、去重复和去偏差，确保了无偏倚和可靠的分析。作为迄今为止最大且最强大的孟加拉语clickbait语料库，该数据集对于自然语言处理和数据科学研究人员来说具有重要价值，他们希望在低资源语言中推进clickbait现象的建模。它的多模态性质使得可以对clickbait进行全面的分析，涵盖内容、用户交互和语言维度，以开发具有跨语言应用的更复杂的检测方法。

    This study presents a large multi-modal Bangla YouTube clickbait dataset consisting of 253,070 data points collected through an automated process using the YouTube API and Python web automation frameworks. The dataset contains 18 diverse features categorized into metadata, primary content, engagement statistics, and labels for individual videos from 58 Bangla YouTube channels. A rigorous preprocessing step has been applied to denoise, deduplicate, and remove bias from the features, ensuring unbiased and reliable analysis. As the largest and most robust clickbait corpus in Bangla to date, this dataset provides significant value for natural language processing and data science researchers seeking to advance modeling of clickbait phenomena in low-resource languages. Its multi-modal nature allows for comprehensive analyses of clickbait across content, user interactions, and linguistic dimensions to develop more sophisticated detection methods with cross-linguistic applications.
    
[^128]: 在人工和生物神经系统中识别可解释的视觉特征

    Identifying Interpretable Visual Features in Artificial and Biological Neural Systems. (arXiv:2310.11431v1 [stat.ML])

    [http://arxiv.org/abs/2310.11431](http://arxiv.org/abs/2310.11431)

    本文提出了一种量化视觉可解释性的自动化方法，并找到了卷积神经网络中有意义的方向。

    

    神经网络中的单个神经元通常是“可解释的”，因为它们代表个别直观有意义的特征。然而，许多神经元表现出“混合选择性”，即它们代表多个不相关的特征。最近的假设认为，深度网络中的特征可能以“叠加”的方式表示，即由多个神经元沿非正交轴表示，因为自然数据中可解释的特征数通常大于给定网络中的神经元数。因此，我们应该能够在激活空间中找到与个别神经元不对齐的有意义的方向。在这里，我们提出了（1）一种自动化的方法来量化视觉可解释性，并通过与大量人类心理物理学对神经元可解释性的判断进行验证，以及（2）一种在网络激活空间中寻找有意义方向的方法。我们利用这些方法来发现卷积神经网络中的方向。

    Single neurons in neural networks are often ``interpretable'' in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural
    
[^129]: 理解算法公平性中的公平性代理函数

    Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])

    [http://arxiv.org/abs/2310.11211](http://arxiv.org/abs/2310.11211)

    本文研究了算法公平性中的公平性代理函数，并发现了代理和公平性定义之间存在一个差距。这个差距决定了一个代理函数能否适当替代一个公平性定义。

    

    已观察到机器学习算法对某些人群产生偏见的预测。为了减轻这种偏见并实现可比的准确性，一种有希望的方法是引入涉及公平性定义的代理函数，并解决一个受限制的优化问题。然而，在以往的研究中，一个有趣的问题是这种公平性代理函数可能导致不公平的结果。在本研究中，为了深入理解这个问题，我们以广泛使用的公平性定义——人口统计平等——为例，从理论和实证上证明了公平性定义和公平性代理函数之间存在一个代理-公平性差距。这个"差距"直接决定了一个代理函数是否适合替代一个公平性定义。此外，关于这个"差距"的理论分析和实验结果激发了我们的兴趣，表明无限制的代理函数将受到决策边界远离的点的影响。

    It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
    
[^130]: HGCVAE: 将生成式学习和对比学习整合为一体的异构图学习方法

    HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])

    [http://arxiv.org/abs/2310.11102](http://arxiv.org/abs/2310.11102)

    HGCVAE是一种将生成式学习和对比学习整合为一体的异构图学习方法，通过利用生成式的自监督学习能力来解决异构图学习的挑战。

    

    生成式自监督学习（SSL）在图学习中展示了巨大的潜力和越来越多的关注。本研究旨在探索生成式SSL在异构图学习（HGL）中的问题。以往关于异构图的SSL方法主要依赖对比学习，需要设计复杂的视图来捕捉异质性。然而，现有的生成式SSL方法并未充分利用生成模型的能力来解决HGL的挑战。在本文中，我们提出了HGCVAE，一种新颖的对比变分图自编码器，使HGL摆脱了复杂异质性的负担。HGCVAE不再专注于复杂的异质性，而是充分利用了生成式SSL的潜力。HGCVAE创新地将对比学习与生成式SSL相结合，引入了几个关键创新。首先，我们采用渐进机制生成高质量的hard样本，

    Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
    
[^131]: 在自主赛车中达到极限: 最优控制与强化学习比较

    Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning. (arXiv:2310.10943v1 [cs.RO])

    [http://arxiv.org/abs/2310.10943](http://arxiv.org/abs/2310.10943)

    这项研究比较了最优控制 (OC)和强化学习 (RL)方法在自主无人机赛车中的效果，发现强化学习方法优于最优控制方法。研究表明，强化学习能够直接优化任务层面的目标，并利用领域的随机因素，而最优控制的分解限制了控制器的行为范围。

    

    机器人学中一个核心问题是如何为敏捷移动机器人设计控制系统。本文系统地研究了这个问题，重点是自主无人机赛车。我们发现，在这个设置下，使用强化学习(RL)训练的神经网络控制器胜过最优控制(OC)方法。然后我们调查了哪些基本因素对RL的成功或OC的限制有所贡献。我们的研究表明，RL相对于OC的根本优势不在于优化目标的效果更好，而是在于它优化了一个更好的目标。OC将问题分解为规划和控制，使用一个明确的中间表示，如轨迹，作为接口。这种分解限制了控制器可以表达的行为范围，当面临未建模的影响时，导致控制性能较差。相反，RL可以直接优化任务层面的目标，并且可以利用领域随机因素。

    A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain rando
    
[^132]: 针对跳跃不连续函数的替代主动子空间

    Surrogate Active Subspaces for Jump-Discontinuous Functions. (arXiv:2310.10907v1 [stat.ML])

    [http://arxiv.org/abs/2310.10907](http://arxiv.org/abs/2310.10907)

    该论文提出了一种针对不连续函数的替代主动子空间方法，扩展了活跃子空间的应用范围，并通过数值实验验证了该方法的有效性。

    

    替代建模和活跃子空间已经成为计算科学和工程领域的强大范例。将这些技术应用于社会科学中的计算模型，突显了它们在处理离散输出的Agent-Based模型等不连续模拟器时的局限性。然而，之前的应用研究已经表明，对于这类估计器，替代计算的活跃子空间可以产生有趣的结果。但是，由于活跃子空间是通过梯度定义的，当将该方法应用于不连续模拟器时，估计的是什么量还不清楚。本文首先展示了进行此类分析时可能出现的一些病态情况。这促使我们将活跃子空间扩展到不连续函数上，澄清了在此类分析中实际估计的内容。我们还对合成测试函数进行了数值实验，比较了活跃子空间的高斯过程估计。

    Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active sub
    
[^133]: 从统计学角度揭开中毒后门攻击的神秘面纱

    Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. (arXiv:2310.10780v1 [cs.CR])

    [http://arxiv.org/abs/2310.10780](http://arxiv.org/abs/2310.10780)

    从统计学角度揭开中毒后门攻击的神秘面纱，通过评估任何包含恒定触发器的后门攻击的有效性，确定了后门攻击成功的决定因素、最有效的攻击方向以及几乎不可察觉的人类触发器何时会成功。

    

    在现实世界中，对机器学习的依赖日益增长，强调了理解和确保其安全性的重要性。中毒后门攻击由于其隐蔽性和潜在的严重后果而构成了重大的安全风险。这类攻击涉及将触发器嵌入学习模型中，以在存在活动触发器时引起恶意行为，同时在没有触发器的情况下维持正常功能。本文通过为受损模型在清洁和后门测试数据上的性能建立严格的下限和上限，评估了任何包含恒定触发器的后门攻击的有效性。所开发的理论回答了一系列基本但以前未被充分探索的问题，包括（1）后门攻击成功的决定因素是什么，（2）最有效的后门攻击方向是什么，以及（3）几乎不可察觉的人类触发器何时会成功。我们得到的理解...

    The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understandin
    
[^134]: 深度学习的微扩展数据格式

    Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10537](http://arxiv.org/abs/2310.10537)

    本文评估了Microscaling（MX）数据格式在降低深度学习应用的计算和存储成本方面的可行性。实证结果显示MX数据格式可以作为基线FP32的替代，同时保持低用户摩擦，并且成功在超过两打基准测试中以小于8位的数据格式进行了训练。

    

    窄位宽数据格式对于降低现代深度学习应用的计算和存储成本至关重要。本文评估了将每个块的缩放因子与窄浮点和整数类型相结合的微扩展（MX）数据格式，以满足硬件效率、模型准确性和用户摩擦之间的竞争需求。对于AI推理和训练，MX数据格式在超过两打基准测试中的实证结果证明了其作为基线FP32的可行性，并且使用时用户摩擦小。我们还展示了在最小的准确性损失和无需修改训练配方的情况下，首次训练生成式语言模型在小于8位的权重、激活和渐变上。

    Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
    
[^135]: 用大型语言模型进行语义解析，用于复杂的零样本对话状态跟踪的更新策略

    Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10520](http://arxiv.org/abs/2310.10520)

    本论文提出了ParsingDST方法，利用大型语言模型和语义解析技术，实现了复杂的零样本对话状态跟踪的更新策略，并在实验中展示了明显的改进。

    

    零样本对话状态跟踪（DST）解决了获取和注释面向任务的对话的挑战，这可能耗时费力。然而，DST超出了简单的填槽，需要有效的更新策略来跟踪对话状态随着对话的进行。本文提出了ParsingDST，一种新的In-Context Learning（ICL）方法，以引入额外的复杂更新策略用于零样本DST。我们的方法通过利用强大的大型语言模型（LLMs）并通过语义解析将原始对话文本转换为JSON作为一个中间状态来重新定义DST任务。我们还设计了一个新颖的框架，其中包括更多的模块来确保文本到JSON过程中更新策略的有效性。实验结果表明，我们的方法在MultiWOZ数据集上优于现有的零样本DST方法，在联合目标准确率（JGA）和槽准确度方面与现有的ICL方法相比呈现出显著改进。

    Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
    
[^136]: 使用大型语言模型的文本摘要: MPT-7b-instruct、Falcon-7b-instruct和OpenAI Chat-GPT模型的比较研究

    Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10449](http://arxiv.org/abs/2310.10449)

    本研究通过比较MPT-7b-instruct, Falcon-7b-instruct和OpenAI Chat-GPT模型，在不同的数据集上使用不同的超参数进行了文本摘要实验。实验结果表明，text-davinci-003模型表现最佳，并且提供了大型语言模型在文本摘要中的性能综述。

    

    文本摘要是一项重要的自然语言处理任务，应用范围包括信息检索和内容生成。利用大型语言模型在提升摘要技术方面展示了显著的潜力。本文使用多种大型语言模型（包括MPT-7b-instruct，falcon-7b-instruct和OpenAI ChatGPT text-davinci-003模型）进行文本摘要的探索。实验使用不同的超参数，并使用诸如双语评估衡量（BLEU）分数，面向回忆的视角评估（ROUGE）分数和双向编码器表示转换器（BERT）分数等广泛接受的指标评估生成的摘要。根据实验，text-davinci-003的性能优于其他模型。本次研究涉及CNN Daily Mail和XSum这两个不同的数据集，主要目标是全面了解大型语言模型在文本摘要中的性能。

    Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large
    
[^137]: 跨语言多语言模型中事实知识的跨语言一致性

    Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10378](http://arxiv.org/abs/2310.10378)

    本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。

    

    多语言大规模预训练语言模型（PLM）显示存储了大量的事实知识，但在不同语言之间存在较大的变化。为了确保不同语言背景的用户从同一个模型中获得一致的反馈，我们研究了各种多语言PLM中事实知识的跨语言一致性（CLC）。为此，我们提出了一种基于排序的一致性（RankC）度量，用于独立于准确性评估跨语言间的知识一致性。利用这个度量方法，我们对决定CLC的因素进行了深入分析，包括模型层面和语言对层面。在其他结果中，我们发现增加模型大小可以提高大多数语言中的事实探测准确性，但不能改善跨语言一致性。最后，我们通过模型编辑在PLMs中插入新的事实关联进行了一个CLC的案例研究。对一小部分事实进行了实验。

    Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
    
[^138]: 使用深度生成模型进行中国绘画风格转换

    Chinese Painting Style Transfer Using Deep Generative Models. (arXiv:2310.09978v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.09978](http://arxiv.org/abs/2310.09978)

    本文研究和利用不同的深度生成模型进行中国绘画风格转换，并提出了一种结合了多种模型的算法。在定性和定量方面评估了其性能。

    

    艺术风格转换旨在在保留图像内容的同时修改其风格。自2015年以来，使用深度学习模型进行风格转换已经被广泛研究，大多数应用都集中在像梵高、莫奈、塞尚这样的特定艺术家上。然而，在传统中国绘画风格转换方面的研究和应用较少。本文将研究和利用不同的最新深度生成模型进行中国绘画风格转换，并在定性和定量方面评估其性能。此外，我们还提出了一种结合了几种风格转换模型的算法。具体而言，我们将把传统中国绘画的两种主要风格，即“工笔”和“水墨”，应用到现代图像中，如自然对象、肖像和风景。

    Artistic style transfer aims to modify the style of the image while preserving its content. Style transfer using deep learning models has been widely studied since 2015, and most of the applications are focused on specific artists like Van Gogh, Monet, Cezanne. There are few researches and applications on traditional Chinese painting style transfer. In this paper, we will study and leverage different state-of-the-art deep generative models for Chinese painting style transfer and evaluate the performance both qualitatively and quantitatively. In addition, we propose our own algorithm that combines several style transfer models for our task. Specifically, we will transfer two main types of traditional Chinese painting style, known as "Gong-bi" and "Shui-mo" (to modern images like nature objects, portraits and landscapes.
    
[^139]: 评估特征选择在股市价格预测中的性能，以确定最有效的技术指标

    Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction. (arXiv:2310.09903v1 [q-fin.ST])

    [http://arxiv.org/abs/2310.09903](http://arxiv.org/abs/2310.09903)

    本研究评估了特征选择方法在股市价格预测中的性能，通过选择最佳的技术指标组合来实现最少误差的预测。研究结果表明，不同的包装器特征选择方法在不同的机器学习方法中具有不同的表现。

    

    鉴于技术指标对股市预测的影响，特征选择对选择最佳指标至关重要。一种考虑在特征选择过程中模型性能的特征选择方法是包装器特征选择方法。本研究旨在通过特征选择鉴定出最少误差的预测股市价格的最佳股市指标组合。为评估包装器特征选择技术对股市预测的影响，本文在过去10年苹果公司的数据上使用了10个评估器和123个技术指标进行了SFS和SBS的考察。此外，通过提出的方法，将由3天时间窗口创建的数据转化为适用于回归方法的输入。从观察结果可以得出：（1）每种包装器特征选择方法在不同的机器学习方法中具有不同的结果，每种方法在不同的预测准确性上也有所不同。

    Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 10 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is mor
    
[^140]: Edge-InversionNet：使InversionNet在边缘设备上实现高效推理

    Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge Devices. (arXiv:2310.09667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09667](http://arxiv.org/abs/2310.09667)

    本论文提出了Edge-InversionNet，通过采用结构化修剪算法得到了InversionNet的轻量化版本，在资源受限的边缘设备上实现了高效的推理。实验结果显示，修剪后的InversionNet在性能略有下降的情况下，可以实现高达98.2%的计算资源减少。

    

    地震全波形反演(FWI)是地球物理学中广泛使用的一种技术，用于从地震数据中推断地下结构。而InversionNet是最成功的数据驱动机器学习模型之一，应用于地震FWI。然而，高计算成本使得InversionNet难以有效部署到通常资源受限的边缘设备上。因此，我们提出采用结构化修剪算法获得InversionNet的轻量化版本，以在边缘设备上进行高效推理。我们还使用树莓派制作了一个运行轻量化InversionNet的原型。实验结果表明，修剪后的InversionNet可以实现高达98.2%的计算资源减少，而模型性能略有下降。

    Seismic full waveform inversion (FWI) is a widely used technique in geophysics for inferring subsurface structures from seismic data. And InversionNet is one of the most successful data-driven machine learning models that is applied to seismic FWI. However, the high computing costs to run InversionNet have made it challenging to be efficiently deployed on edge devices that are usually resource-constrained. Therefore, we propose to employ the structured pruning algorithm to get a lightweight version of InversionNet, which can make an efficient inference on edge devices. And we also made a prototype with Raspberry Pi to run the lightweight InversionNet. Experimental results show that the pruned InversionNet can achieve up to 98.2 % reduction in computing resources with moderate model performance degradation.
    
[^141]: LLM训练中的分词选择：微不足道还是至关重要？

    Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])

    [http://arxiv.org/abs/2310.08754](http://arxiv.org/abs/2310.08754)

    在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。

    

    近期LLM的成功主要是由于策划训练数据集、扩展模型架构和数据集规模，以及预训练目标的进步，而分词器的影响则是一个盲点。通过对24个单语和多语言LLM进行训练，并对不同的分词器算法和参数进行大范围实验，我们对分词器选择对LLM的后续性能、训练和推理成本的影响进行了全面研究。我们的研究表明，分词器选择对模型的后续性能、训练和推理成本有着显著影响。特别是，我们发现常见的分词器评估指标（如丰富度和平等性）并不总是对模型的后续性能具有预测能力，这使得这些指标成为对分词器评估的可疑选择。此外，我们还展示了针对五种最常见的欧洲语言训练的多语言分词器需要词汇表的大小。

    The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
    
[^142]: 基于思维链的Transformer的表达能力

    The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])

    [http://arxiv.org/abs/2310.07923](http://arxiv.org/abs/2310.07923)

    本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。

    

    最近的理论研究发现了一些出人意料地简单的推理问题，例如检查图中是否存在连接的两个节点，或模拟有限状态机，这些问题被证明无法由立即读取输入后回答的标准Transformer解决。然而，在实践中，通过允许Transformer使用“思维链”或“草稿纸”，即在回答之前生成并依赖一系列中间token，可以改善其推理能力。基于此，我们问：这种中间生成是否从根本上扩展了仅有解码器的Transformer的计算能力？我们表明答案是肯定的，但增加的程度关键取决于中间生成的数量。例如，我们发现相对于输入长度来说，具有对数级解码步骤的Transformer解码器仅略微推动了标准Transformer的极限，而线性数量的解码步骤则增加了明显的新能力（在标准计算复杂度下）。

    Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
    
[^143]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^144]: Crystal: 以自我反馈为增强的内省推理器

    Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04921](http://arxiv.org/abs/2310.04921)

    提出了一种名为Crystal的内省型常识推理器，通过内省知识和基于知识的推理相结合，提高了常识推理的性能和解释能力。

    

    大量工作表明，通过知识增强的推理方法可以提高常识推理的性能和可解释性，其中推理过程的基础知识明确表达和利用。然而，现有的实现，包括"思维链"及其变种，未能捕捉到常识推理中所需的内省性质，也未能解释知识生成和利用之间的相互适应。我们提出了一种新颖的方法来开发内省型常识推理器 Crystal。为了解决常识问题，它首先内省与给定问题相关的知识陈述，然后基于先前内省的知识进行知情预测。模型的知识内省和基于知识的推理模式通过强化学习进行调整，其中奖励来自反馈。

    Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including "chain-of-thought" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback
    
[^145]: 面向大规模多任务数据集的分子学习基础模型的研究

    Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])

    [http://arxiv.org/abs/2310.04292](http://arxiv.org/abs/2310.04292)

    本研究提出了七个新颖的数据集，分别是ToyMix、LargeMix和UltraLarge，这些数据集在规模和有监督标签的多样性方面突破了界限，涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。

    

    最近，预训练的基础模型在多个领域取得了显著的进展。然而，在分子机器学习中，数据集通常是手工策划的，因此规模较小，缺乏带有标记特征和管理这些数据集的代码库，制约了基础模型的发展。在这项工作中，我们提出了七个新颖的数据集，分为三个不同的类别：ToyMix、LargeMix和UltraLarge。这些数据集在规模和有监督标签的多样性方面突破了界限。它们涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。相比之下，我们的数据集的数据点数量是广泛使用的OGB-LSC PCQM4Mv2数据集的300倍，也是仅包含量子数据的QM1B数据集的13倍。此外，为了支持基于我们提出的基础模型的开发，

    Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed 
    
[^146]: 超越单节点：在分布式系统上实现大规模机器学习模型加速

    MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems. (arXiv:2310.02784v1 [cs.DC])

    [http://arxiv.org/abs/2310.02784](http://arxiv.org/abs/2310.02784)

    该研究提出了一个性能建模框架，在分布式系统上实现了大规模机器学习模型的加速，获得了2.24倍和5.27倍的吞吐量提升潜力。

    

    训练和部署大规模机器学习（ML）模型是耗时且需要大量分布式计算基础设施。根据实际情况在数据中心规模基础设施上进行大模型训练，我们发现14~32%的GPU小时用于通信，没有重叠计算。为了尽量减少等待通信延迟，本研究开发了一个灵活的性能建模框架，指导并行化和硬件软件共同设计策略。利用最先进的GPU训练硬件上的一套实际大规模ML模型，我们展示了预训练和推断场景分别可以提高2.24倍和5.27倍的吞吐量。

    Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.
    
[^147]: 使用元胞自动机的智能客户端选择进行联邦学习

    Intelligent Client Selection for Federated Learning using Cellular Automata. (arXiv:2310.00627v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00627](http://arxiv.org/abs/2310.00627)

    本研究提出了一种使用元胞自动机的智能客户端选择算法，用于解决联邦学习中由于传感器数量增加带来的通信和资源分配问题。

    

    联邦学习（FL）已成为解决隐私增强和延迟最小化等各种实际应用（如交通、通信和医疗）中的问题的有希望的解决方案。FL通过利用来自数百万设备和物联网传感器的数据将机器学习（ML）引入边缘，从而实现对动态环境的快速响应，并产生高度个性化的结果。然而，不同应用中传感器数量的增加在通信和资源分配方面带来了挑战，阻碍了所有设备参与联邦过程的能力，进而需要有效的FL客户端选择。为了解决这个问题，我们提出了基于元胞自动机的客户端选择（CA-CS），这是一种新颖的客户端选择算法，它利用元胞自动机（CA）作为模型，以有效地捕捉快速演变环境中的时空变化。CA-CS考虑了计算资源和通信能力。

    Federated Learning (FL) has emerged as a promising solution for privacy-enhancement and latency minimization in various real-world applications, such as transportation, communications, and healthcare. FL endeavors to bring Machine Learning (ML) down to the edge by harnessing data from million of devices and IoT sensors, thus enabling rapid responses to dynamic environments and yielding highly personalized results. However, the increased amount of sensors across diverse applications poses challenges in terms of communication and resource allocation, hindering the participation of all devices in the federated process and prompting the need for effective FL client selection. To address this issue, we propose Cellular Automaton-based Client Selection (CA-CS), a novel client selection algorithm, which leverages Cellular Automata (CA) as models to effectively capture spatio-temporal changes in a fast-evolving environment. CA-CS considers the computational resources and communication capacity
    
[^148]: 自适应线性模型的统计极限：低维估计和推断

    Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference. (arXiv:2310.00532v1 [math.ST] CROSS LISTED)

    [http://arxiv.org/abs/2310.00532](http://arxiv.org/abs/2310.00532)

    该论文研究了自适应数据对于线性模型中低维参数估计性能的影响，并确定了条件使得估计误差与独立同分布数据情况相匹配。

    

    当数据被自适应地收集时，统计学中的估计和推断面临重大挑战。即使在线性模型中，最小二乘估计器（OLS）在单个坐标估计和误差膨胀方面可能无法展现渐近正态性。最近的极小极限显示了这个问题，指出当数据允许任意自适应性时，估计单个坐标的误差可以增加一个$\sqrt{d}$倍，与i.i.d.情况相比。我们的工作探讨了在利用i.i.d.和自适应数据时估计性能之间的显著差异。我们研究了数据收集中的自适应程度如何影响高维线性模型中估计低维参数组成的性能。我们确定了数据收集机制条件，使得低维参数组成的估计误差在i.i.d.情况下与其相匹配，只差一个f倍。

    Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of $\sqrt{d}$ when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a f
    
[^149]: 一种用于医学图像中一般移动目标分割的基础模型

    A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])

    [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)

    本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果

    

    医学图像分割旨在描绘感兴趣的解剖或病理结构，在临床诊断中起着关键作用。构建高精度的深度分割模型需要大量高质量的注释数据。然而，医学注释非常繁琐耗时，特别是对于医学视频或3D体积，由于巨大的标签空间和差的帧间一致性。最近，在自然图像中，一个名为Moving Object Segmentation (MOS)的基本任务在技术上取得了重大进展。它的目标是在图像序列中从背景中描绘移动物体，只需要最小的注释。在本文中，我们提出了第一个用于医学图像中MOS的基础模型，名为iMOS。对一个大规模多模态医学数据集进行的大量实验验证了所提出的iMOS的有效性。具体而言，只需对序列中少量的图像进行注释，iMOS就可以实现了

    Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
    
[^150]: SEPT: 为运动预测的高效场景表示学习

    SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])

    [http://arxiv.org/abs/2309.15289](http://arxiv.org/abs/2309.15289)

    SEPT是一个利用自监督学习进行场景表示学习的建模框架，通过预训练的编码器捕捉轨迹的运动学特征、道路网络的空间结构以及道路和代理之间的交互作用，实现了在运动预测任务上的最先进性能。

    

    运动预测对于自动驾驶汽车在复杂交通环境中安全运行至关重要。提取交通元素之间的有效时空关系是准确预测的关键。本文受到预训练大型语言模型成功应用的启发，提出了SEPT，这是一个利用自监督学习来开发复杂交通场景中强大的时空理解能力的建模框架。具体而言，我们的方法涉及到在场景输入上进行三个掩码重构建模任务，包括代理路径和道路网络，预训练场景编码器以捕捉轨迹的运动学特征，道路网络的空间结构以及道路和代理之间的交互作用。预训练的编码器然后在下游预测任务上进行微调。大量实验证明，SEPT在Argoverse 1和Argoverse上无需精心设计的架构或手动特征工程，达到了最先进的性能水平。

    Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
    
[^151]: 让PPO变得更好：基于值导向的Monte-Carlo Tree Search解码

    Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])

    [http://arxiv.org/abs/2309.15028](http://arxiv.org/abs/2309.15028)

    本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。

    

    在生成自然语言文本时，使用最新的强化学习算法，如Proximal Policy Optimization (PPO)，因此可以认为推理时间的搜索算法，如Monte-Carlo Tree Search (MCTS) 是不必要的。本文证明了通过在PPO之上集成MCTS，可以进一步提升PPO的性能。关键思想是在解码文本时，不要丢弃值网络，即PPO训练时用于评估部分输出序列的副产品，而是将其与策略网络紧密结合。具体而言，本文提出了一种称为PPO-MCTS的新颖的值导向解码算法，可以将来自PPO的值网络与推理时间产生的策略网络紧密结合。与基于MCTS的控制文本生成的先前方法相比，我们的方法的关键优势在于减少了训练和测试之间部分输出的评分机制的基本不匹配。在四个文本生成任务上的评估结果表明，PPO-MCTS可以显著提升性能。

    Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
    
[^152]: 决策树策略在IBMDP中的Actor-Critic算法的局限性（arXiv:2309.13365v2 [cs.LG] UPDATED）

    Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13365](http://arxiv.org/abs/2309.13365)

    该论文研究了在IBMDP中使用Actor-Critic算法学习决策树策略的局限性。结果表明，即使是在简单的玩具任务上，深度RL也可能失败。

    

    AI模型的可解释性可以通过用户安全检查来建立对这些AI的信任。特别是，决策树（DT）提供了对学习模型的整体视角，并透明地揭示了哪些输入特征对于做出决策至关重要。然而，如果决策树过大，可解释性就会受到影响。为了学习紧凑的决策树，最近提出了一种强化学习（RL）框架，用于使用深度RL探索DT的空间。该框架通过增加动作来收集关于隐藏输入特征的信息，通过适当地对这些动作进行惩罚，代理学习如何在树的大小和性能之间进行最优权衡。在实践中，仍然存在一个开放问题，即需要学习部分可观察马尔可夫决策过程（MDP）的反应性策略。本文表明，即使在这一类简单的玩具任务上，深度RL也可能失败。

    Interpretability of AI models allows for user safety checks to build trust in such AIs. In particular, Decision Trees (DTs) provide a global look at the learned model and transparently reveal which features of the input are critical for making a decision. However, interpretability is hindered if the DT is too large. To learn compact trees, a recent Reinforcement Learning (RL) framework has been proposed to explore the space of DTs using deep RL. This framework augments a decision problem (e.g. a supervised classification task) with additional actions that gather information about the features of an otherwise hidden input. By appropriately penalizing these actions, the agent learns to optimally trade-off size and performance of DTs. In practice, a reactive policy for a partially observable Markov decision process (MDP) needs to be learned, which is still an open problem. We show in this paper that deep RL can fail even on simple toy tasks of this class. However, when the underlying deci
    
[^153]: 基于贝叶斯纵向张量响应回归的神经可塑性建模

    Bayesian longitudinal tensor response regression for modeling neuroplasticity. (arXiv:2309.10065v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.10065](http://arxiv.org/abs/2309.10065)

    提出了一种基于贝叶斯纵向张量响应回归的新方法，利用汇集空间分布的体素信息来推断显著的变化，并对协变量进行调整。该方法使用马尔科夫链蒙特卡洛采样，利用低秩分解减少维度并保持维度的空间配置，通过满足后验分布形状的联合可信区域实现特征选择，从而实现更准确的推断。

    

    纵向神经影像研究中对于由治疗和其他因素引起的体素级神经可塑性的调查是一个重要的研究兴趣。然而，传统的体素级方法存在一些缺陷，可能会影响这些方法的准确性。我们提出一种新颖的基于贝叶斯张量响应回归的纵向影像数据建模方法，通过汇集空间分布的体素信息来推断显著的变化，并对协变量进行调整。该方法采用马尔科夫链蒙特卡洛（MCMC）采样实现，利用低秩分解来减少维度并保持估计系数中体素的空间配置。它还通过满足后验分布形状的联合可信区域实现特征选择，以实现更准确的推断。除了群体水平的推断，该方法还能够推断个体水平的神经可塑性，从而可以进行个体水平的分析。

    A major interest in longitudinal neuroimaging studies involves investigating voxel-level neuroplasticity due to treatment and other factors across visits. However, traditional voxel-wise methods are beset with several pitfalls, which can compromise the accuracy of these approaches. We propose a novel Bayesian tensor response regression approach for longitudinal imaging data, which pools information across spatially-distributed voxels to infer significant changes while adjusting for covariates. The proposed method, which is implemented using Markov chain Monte Carlo (MCMC) sampling, utilizes low-rank decomposition to reduce dimensionality and preserve spatial configurations of voxels when estimating coefficients. It also enables feature selection via joint credible regions which respect the shape of the posterior distributions for more accurate inference. In addition to group level inferences, the method is able to infer individual-level neuroplasticity, allowing for examination of pers
    
[^154]: 缺失数据下的不确定性交通预测

    Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])

    [http://arxiv.org/abs/2309.06800](http://arxiv.org/abs/2309.06800)

    本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。

    

    交通预测是一个重要的课题，因为它在交通领域有广泛的应用。近期，许多研究取得了很好的结果。然而，大多数研究假设预测位置有完整或至少部分的历史记录，不能扩展到无历史记录的位置。在现实场景中，由于预算限制和安装可行性问题，传感器的部署可能受限，这使得大多数当前模型不适用。虽然少数文献尝试在缺失位置上插补交通状态，但这些方法需要与传感器位置同时观测的数据，使它们不适用于预测任务。另一个缺点是缺乏对预测不确定性的测量，使得之前的工作不适用于风险敏感的任务或涉及决策的情况。为了填补这一空白，受到先前的归纳图神经网络的启发，本文提出了一种考虑不确定性的方法。

    Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
    
[^155]: HAct：带有神经网络激活直方图的数据集外检测

    HAct: Out-of-Distribution Detection with Neural Net Activation Histograms. (arXiv:2309.04837v1 [cs.LG])

    [http://arxiv.org/abs/2309.04837](http://arxiv.org/abs/2309.04837)

    本论文提出了一种用于神经网络的数据集外检测方法，使用HAct激活直方图描述符对OOD进行检测，具有简单高效和准确性，并且在多个OOD图像分类基准测试中表现优于先前最先进的方法。

    

    我们提出了一种简单、高效、准确的方法，用于检测经过训练的神经网络对于数据集外（OOD）数据的检测，这是用于OOD泛化方法的潜在第一步。我们提出了一种新颖的描述符，即HAct激活直方图，用于OOD检测，即通过直方图来近似表示神经网络层输出值的概率分布。我们证明HAct在多种OOD图像分类基准测试上比最先进的方法更准确。例如，在标准的OOD基准测试上，我们的方法使用Resnet-50实现了95%的真正例率（TPR），而只有0.05%的误报率，使得其在误报率上相较于之前最先进的方法提高了20.66%（在相同的95%TPR下）。低计算复杂度和易于实现使得HAct适合大规模实践中在线监测部署的神经网络。

    We propose a simple, efficient, and accurate method for detecting out-of-distribution (OOD) data for trained neural networks, a potential first step in methods for OOD generalization. We propose a novel descriptor, HAct activation histograms, for OOD detection, that is, probability distributions (approximated by histograms) of output values of neural network layers under the influence of incoming data. We demonstrate that HAct is significantly more accurate than state-of-the-art on multiple OOD image classification benchmarks. For instance, our approach achieves a true positive rate (TPR) of 95% with only 0.05% false-positives using Resnet-50 on standard OOD benchmarks, outperforming previous state-of-the-art by 20.66% in the false positive rate (at the same TPR of 95%). The low computational complexity and the ease of implementation make HAct suitable for online implementation in monitoring deployed neural networks in practice at scale.
    
[^156]: 混合方差流用于离散变量

    Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])

    [http://arxiv.org/abs/2308.15613](http://arxiv.org/abs/2308.15613)

    本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。

    

    变分流允许从事者学习复杂的连续分布，但是近似离散分布仍然是一个挑战。目前的方法通常将离散目标嵌入连续空间中-通常是通过连续松弛或去量化-然后应用连续流动。这些方法涉及一个可能无法捕捉到原始离散目标的替代目标，可能具有偏倚或不稳定的梯度，并且可能会创建一个困难的优化问题。在这项工作中，我们开发了一种针对离散分布的变分流族，而不需要任何连续嵌入。首先，我们开发了一个保持度量的离散可逆映射，使离散目标保持不变，然后基于该映射创建了一个混合变分流(MAD Mix)。我们还开发了一个扩展，用于处理联合离散和连续模型。我们的实验表明，MAD Mix产生了比连续嵌入流更可靠的近似。

    Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
    
[^157]: 基于量子力学视角的量化优化方法

    Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v1 [quant-ph])

    [http://arxiv.org/abs/2308.11594](http://arxiv.org/abs/2308.11594)

    基于量子力学视角的量化优化方法在全局优化中利用薛定谔方程推导的隧道效应，从而能够避免局部最小值。

    

    基于热力学的统计和随机分析一直是随机全局优化的主要分析框架。最近，出现了用于全局优化的量子退火或量子隧道算法，我们需要一个新的研究框架来进行全局优化算法。在本文中，我们提供了基于薛定谔方程的量化优化的分析，以揭示量子力学中的哪些属性使全局优化成为可能。我们提出，薛定谔方程推导出的隧道效应使得量化优化能够逃离局部最小值。此外，我们确认这种隧道效应是包含在基于量子力学的全局优化中的相同属性。对标准多模态基准函数进行的实验表明了所提出的分析的有效性。

    Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
    
[^158]: 使用大规模未标记的自然图像增强医疗AI模型的网络初始化

    Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])

    [http://arxiv.org/abs/2308.07688](http://arxiv.org/abs/2308.07688)

    该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。

    

    预训练数据集（如ImageNet）已成为医学图像分析的黄金标准。然而，自监督学习（SSL）的出现提供了通过利用未标记数据来学习强大特征的机会，从而可以绕过繁重的标注过程。在这项研究中，我们探索了SSL预训练在非医学图像上是否可以应用于胸部X射线，并与非医学图像和医学图像上的监督预训练进行了比较。我们利用视觉变换器，并根据以下方式初始化其权重：（i）基于自然图像的SSL预训练（DINOv2）、（ii）基于自然图像的监督预训练（ImageNet数据集），以及（iii）基于MIMIC-CXR数据库中的胸部X射线的监督预训练。我们在来自六个全球大型数据集的800,000多张胸部X射线上测试了我们的方法，诊断了20多种不同的影像所见。我们的SSL预训练在经过筛选的图像上不仅表现出色，而且超过了基于ImageNet的预训练（对所有数据集，P<0.001），而且在某些数据集上还超过了基于医学图像的预训练。

    Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in cert
    
[^159]: 架起可信度与开放世界学习的桥梁：一种探索性神经方法，用于增强可解释性、泛化性和鲁棒性

    Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])

    [http://arxiv.org/abs/2308.03666](http://arxiv.org/abs/2308.03666)

    该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。

    

    随着研究人员努力缩小机器智能与人类之间的差距，通过发展人工智能技术，我们必须认识到可信度在开放世界中的关键重要性，在日常生活的各个方面对每个人都已经无处不在。然而，目前的人工智能系统存在几个挑战，可能会导致信任危机：1）对预测结果的解释不足；2）学习模型的泛化性不足；3）对不确定环境的适应能力差。因此，我们探索了一种神经程序，用于架起可信度与开放世界学习之间的桥梁，从单模态扩展到多模态场景，以供读者使用。1）为了增强设计级可解释性，我们首先定制了具有特定物理含义的可信网络；2）然后，通过灵活的学习正则化器设计环境福祉任务接口，以改善可信网络的泛化性能。

    As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
    
[^160]: 动态MRI重建中基于遮蔽图像模型的全局k空间插值

    Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling. (arXiv:2307.12672v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.12672](http://arxiv.org/abs/2307.12672)

    本文提出了一种基于遮蔽图像模型的全局k空间插值方法，通过连接遮蔽图像建模和k空间插值，使用Transformer网络来学习2D+t k空间的全局依赖关系，并提出了k空间迭代细化模块来增强高频成分的学习。

    

    在动态磁共振成像中，由于扫描时间有限，通常会对k空间进行欠采样，导致图像领域中出现伪影。因此，动态MR重建不仅需要建模k空间x和y方向的空间频率分量，还需要考虑时间冗余。大多数之前的工作依赖于图像域正则化器（先验）进行MR重建。相反，我们专注于在获取傅里叶变换图像之前对欠采样的k空间进行插值。在这项工作中，我们将遮蔽图像建模与k空间插值相连接，并提出了一种新颖的基于Transformer的k空间全局插值网络，称为k-GIN。我们的k-GIN学习了2D+t k空间的低频和高频成分之间的全局依赖关系，并用于插值未采样的数据。此外，我们提出了一种新颖的k空间迭代细化模块（k-IRM），以增强高频成分的学习。我们在92个内部2D+t

    In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t 
    
[^161]: JoinGym: 一种高效的强化学习查询优化环境

    JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning. (arXiv:2307.11704v1 [cs.LG])

    [http://arxiv.org/abs/2307.11704](http://arxiv.org/abs/2307.11704)

    本文介绍了JoinGym，一种高效的强化学习查询优化环境。通过将加入顺序选择问题形式化为马尔可夫决策过程，我们提供了一种实现，该实现完全基于离线跟踪，并且无需设置系统即可进行测试。此外，研究还提供了3300个新SQL查询的所有可能加入追踪。

    

    本文介绍了JoinGym，一种高效且轻量级的强化学习查询优化环境。加入顺序选择（JOS）是一个经典的NP-hard组合优化问题，用于数据库查询优化，可作为RL算法泛化能力的实际测试平台。我们描述了如何将左深和繁茂的JOS问题的每个变种形式化为马尔可夫决策过程（MDP），并提供符合标准Gymnasium API的实现。我们强调我们的实现JoinGym完全基于所有可能连接的离线追踪，使RL从业者能够轻松快速地在一个真实的数据管理问题上测试他们的方法，而无需设置任何系统。此外，我们还提供了从IMDB数据集生成的3300个新SQL查询的所有可能连接追踪。在对流行的RL算法进行基准测试时，我们发现至少有一种方法可以获得。

    In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obta
    
[^162]: 大型语言模型在提取分子相互作用和通路知识方面的比较性能评估

    Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])

    [http://arxiv.org/abs/2307.08813](http://arxiv.org/abs/2307.08813)

    本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。

    

    理解蛋白质相互作用和通路知识对于揭示生物系统的复杂性和研究生物功能和复杂疾病的基本机制至关重要。尽管现有的数据库提供了来自文献和其他源的策划生物数据，但它们往往不完整且维护工作繁重，因此需要替代方法。在本研究中，我们提出利用大型语言模型的能力，通过自动从相关科学文献中提取这些知识来解决这些问题。为了实现这个目标，在这项工作中，我们调查了不同大型语言模型在识别蛋白质相互作用、通路和基因调控关系等任务中的有效性。我们对不同模型的性能进行了彻底评估，突出了重要的发现，并讨论了这种方法所面临的未来机遇和挑战。代码和数据集链接可在论文中找到。

    Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
    
[^163]: 使用解耦的语言预训练为视觉语言学习引入引导策略

    Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])

    [http://arxiv.org/abs/2307.07063](http://arxiv.org/abs/2307.07063)

    本文提出了一种新的方法，通过解耦语言预训练，集中在语言组件上，提供了优化应用大型语言模型的框架，有效改善了视觉语言学习的性能。

    

    本论文提出了一种新颖的方法，旨在优化冻结的大型语言模型（LLMs）在资源密集型视觉语言（VL）预训练中的应用。当前的范式使用视觉特征作为提示来引导语言模型，重点是确定与相应文本最相关的视觉特征。我们的方法不同，集中在语言组件上，具体是确定与视觉特征对齐的最佳提示。我们引入了Prompt-Transformer（P-Former），一种可以预测这些理想提示的模型，该模型仅在语言数据上进行训练，避免了图像-文本配对的需要。这种策略将端到端的VL训练过程巧妙地分为了额外的独立阶段。我们的实验证明，我们的框架显著提高了稳健的图像到文本基线（BLIP-2）的性能，并有效地缩小了使用4M或129M图像-文本对进行训练的模型之间的性能差距。

    We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
    
[^164]: 大型语言模型作为属性化训练数据生成器：多样性和偏差的故事

    Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])

    [http://arxiv.org/abs/2306.15895](http://arxiv.org/abs/2306.15895)

    本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。

    

    近期大型语言模型(LLMs)被广泛应用于各种自然语言处理(NLP)任务的训练数据生成。尽管之前的研究探索了使用生成数据进行模型训练的不同方法，但它们通常依赖于简单的类别条件提示，这可能限制了生成数据的多样性，并且继承了LLM的系统性偏差。因此，我们研究了使用具有多样属性的提示(例如指定长度和风格等属性)进行训练数据生成，这有潜力产生多样和归因的生成数据。我们的研究关注具有高基数和多样领域的数据集，在这方面，我们证明了属性化提示在生成模型性能方面优于简单的类别条件提示。此外，我们还展示了一项包括偏差、多样性和效率等关键方面的全面实证研究，并强调了三个关键观察：首先，系统性偏差在生成数据中存在；其次，多样性和效率之间存在权衡；最后，进行属性化训练数据生成可以改善模型性能。

    Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
    
[^165]: 有限内存贪婪拟牛顿方法与非渐进超线性收敛速率

    Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate. (arXiv:2306.15444v1 [math.OC])

    [http://arxiv.org/abs/2306.15444](http://arxiv.org/abs/2306.15444)

    有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。

    

    非渐进收敛分析表明，拟牛顿方法的显式超线性速率为O$((1/\sqrt{t})^t)$。然而，获得这一速率的方法存在一个众所周知的缺点：它们需要存储先前的黑塞近似矩阵，或者存储所有过去的曲率信息以形成当前的黑塞逆近似。有限内存的拟牛顿方法（如著名的L-BFGS）通过利用有限窗口的过去曲率信息来构造黑塞逆近似，从而缓解了这个问题。因此，它们的每次迭代复杂度和存储需求为O$(\tau d)$，其中$\tau \le d$ 是窗口的大小，$d$ 是问题的维数，从而降低了标准拟牛顿方法的O$(d^2)$ 计算成本和内存需求。然而，据我们所知，没有结果表明有限内存拟牛顿方法存在非渐进超线性收敛速率。

    Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit superlinear rate of O$((1/\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is O$(\tau d)$ where $\tau \le d$ is the size of the window and $d$ is the problem dimension reducing the O$(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for a
    
[^166]: DORSal: 基于扩散的物体中心场景表示

    DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])

    [http://arxiv.org/abs/2306.08068](http://arxiv.org/abs/2306.08068)

    DORSal提出了一种基于扩散模型的物体中心场景表示方法，可以呈现高保真新视图，并在较大程度上保留了诸如基于物体的场景编辑之类的优点。

    

    最近在三维场景理解方面取得的进展使跨大量不同场景的数据集的可扩展表示学习成为可能。因此，对于未见过的场景和物体的泛化，仅通过单个或少数图像渲染新视图，以及支持编辑的可控场景生成现在成为可能。然而，联合训练大量场景通常会在渲染质量上妥协，而与单个场景优化模型（如NeRF）相比。在本文中，我们利用最近扩散模型的进展，使三维场景表示学习模型具备呈现高保真新视图的能力，同时在较大程度上保留了诸如基于物体的场景编辑之类的优点。特别地，我们提出了DORSal，它基于扩散视频架构，为基于物体中心的场景插槽表示的三维场景生成提供适应性。我们在复杂的合成多物体场景和现实世界大规模街景数据集上证明，我们的模型能够生成高质量的场景新视图，同时支持物体级别的编辑，并保留细粒度的纹理和反射等细节。

    Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
    
[^167]: 分布式随机分布的异构奖励多智能体多臂赌博机

    Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards. (arXiv:2306.05579v1 [cs.LG])

    [http://arxiv.org/abs/2306.05579](http://arxiv.org/abs/2306.05579)

    本文提出了一种处理多智能体多臂赌博机问题的算法框架，通过稳健模拟生成随机图并将加权技术结合UCB算法，以协作方式减小整个系统的总遗憾。

    

    本文研究了分布式多智能体多臂赌博机问题，多个客户端通过由环境提供的时间依赖性随机图进行连接。每个臂的奖励分布因客户而异，并且奖励是根据包括亚指数和亚高斯分布在内的分布，由环境独立地随时间生成的。每个客户端都会拉动一个臂，并根据由环境提供的图与邻居进行通信。我们的目标是通过协作来减小整个系统的总遗憾。为此，我们引入了一种新的算法框架，该框架首先提供了使用快速混合马尔可夫链或随机图模型生成随机图的稳健仿真方法，然后将基于平均一致性方法和新提出的加权技术以及上置信限结合起来，提供了一种UCB类型的解决方案。我们的算法考虑到了图形中的随机性，消除了限制条件。

    We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing Markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the con
    
[^168]: 序数势函数的玩家评级方法

    Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])

    [http://arxiv.org/abs/2306.05366](http://arxiv.org/abs/2306.05366)

    该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。

    

    如果对于任意纯策略$x$、$y$和$z$，如果$x$比$y$更好，$y$比$z$更好，则$x$比$z$更好，则两个对称的零和博弈是可传递的。最近观察到，Elo评级未能保持策略之间的传递关系，因此不能正确提取游戏的传递组件。我们的第一个贡献是表明当在正确的空间中计算Elo评级时，Elo评级确实能够保持传递性。具体而言，我们首先使用合适的可逆映射$\varphi$将游戏应用于$\varphi$，然后计算Elo评级，最后通过应用$\varphi^{-1}$回到原始空间。我们将可传递游戏的表征为势游戏的一个弱变体，其势函数是加性可分离的。利用这一洞见，我们引入了传递序数的概念，即将可传递游戏的收益转化为其差异所需的最小可逆映射数。

    A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
    
[^169]: 使用近邻嵌入估计传染效应

    Contagion Effect Estimation Using Proximal Embeddings. (arXiv:2306.02479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02479](http://arxiv.org/abs/2306.02479)

    本文介绍了一种使用近邻嵌入方法来估计社交网络中的传染效应。我们提出了ProEmb框架，通过将变分自动编码器（VAEs）和对抗网络集成在一起，生成高维代理变量的平衡低维表示，并解决了传染效应估计中的偏差问题。

    

    传染效应指的是社交网络中同伴行为对个体结果的因果影响。在观察研究中估计传染效应的显著方法通常假设没有未测量的混杂因素，但由于潜在的同质性，传染可能会被混淆：同质网络中的节点倾向于与具有相似属性的同伴建立联系，并且可以在不互相影响的情况下表现出相似的行为。解决潜在同质性的一种方法是考虑未观察混杂因素的代理变量。然而，在存在高维代理变量时，基于代理的方法可能会导致传染效应估计的严重偏差，正如我们在本文中演示的那样。为解决这个问题，我们引入了新颖的近邻嵌入（ProEmb）框架，该框架将变分自动编码器（VAEs）和对抗网络集成在一起，生成不同处理组高维代理变量的平衡低维表示，并且在因果推论中考虑了传染效应的估计偏差。

    Contagion effect refers to the causal effect of peers' behavior on the outcome of an individual in social networks. While prominent methods for estimating contagion effects in observational studies often assume that there are no unmeasured confounders, contagion can be confounded due to latent homophily: nodes in a homophilous network tend to have ties to peers with similar attributes and can behave similarly without influencing one another. One way to account for latent homophily is by considering proxies for the unobserved confounders. However, in the presence of high-dimensional proxies, proxy-based methods can lead to substantially biased estimation of contagion effects, as we demonstrate in this paper. To tackle this issue, we introduce the novel Proximal Embeddings (ProEmb), a framework which integrates Variational Autoencoders (VAEs) and adversarial networks to generate balanced low-dimensional representations of high-dimensional proxies for different treatment groups and identi
    
[^170]: MuZero学到了什么模型？

    What model does MuZero learn?. (arXiv:2306.00840v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00840](http://arxiv.org/abs/2306.00840)

    本文研究了MuZero算法，发现它学习到的模型无法有效推广到评估未见策略，限制了其对当前策略的进一步改进。

    

    近年来，基于模型的强化学习引起了广泛关注，因为它有望提高样本效率。此外，当使用深度学习模型时，有可能从复杂的传感器数据中学习到紧凑的模型。然而，这些学习到的模型的有效性，特别是它们规划能力的提升当前策略的能力，仍然不清楚。在本研究中，我们研究了MuZero这个著名的基于深度模型的强化学习算法，并探讨了它在实现值等价模型的学习目标上的成就以及学习到的模型对策略改进的实用性。在诸多其他观点中，我们得出结论：MuZero学到的模型无法有效地推广到评估未见策略，这限制了我们通过模型规划来进一步改进当前策略的程度。

    Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model.
    
[^171]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^172]: 连续强化学习的策略优化

    Policy Optimization for Continuous Reinforcement Learning. (arXiv:2305.18901v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18901](http://arxiv.org/abs/2305.18901)

    本研究提出了连续强化学习领域的占用时间概念，并在此基础上扩展了离散强化学习中的PG、TRPO和PPO方法，为连续强化学习领域的研究提供了新的思路和方法。

    

    我们研究了连续时间和空间下的强化学习，采用折扣奖励和随机微分方程的基本动态。在连续强化学习的最新进展的基础上，我们提出了占用时间的概念（特别是针对折扣奖励）并展示了如何有效地使用它来导出性能差异和局部逼近公式。我们还将这些结果扩展到了 PG（策略梯度）、TRPO（信任区域策略优化）和 PPO（近端策略优化）方法，这些方法在离散强化学习中是熟知和强大的工具，但在连续强化学习中尚未得到充分发展。通过数字实验，我们展示了我们的方法的有效性和优势。

    We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.
    
[^173]: Koopa: 使用Koopman预测器学习非平稳时间序列动态

    Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors. (arXiv:2305.18803v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18803](http://arxiv.org/abs/2305.18803)

    本论文提出了一种名为Koopa的Koopman预测器，通过使用傅里叶滤波器将非平稳时间序列中的时间变体和时间不变成分分离出来，并使用Koopman算子作为线性表述，成功解决了深度预测模型在处理非平稳时间序列时面临的挑战。

    

    实际时间序列具有内在的非平稳性，对于深度预测模型而言是一个主要挑战。我们使用现代Koopman理论来处理非平稳时间序列，该理论从根本上考虑了底层的时间变化动态。受到描述复杂动力系统的Koopman理论的启发，我们通过傅里叶滤波器将非平稳序列中的时间变体和时间不变成分分离出来，并设计了Koopman预测器来推进各自的动态。技术上，我们提出了一种名为Koopa的新型Koopman预测器，它由可堆叠的块组成，学习分层动态。Koopa寻找Koopman嵌入的测量函数，并利用Koopman算子作为隐含转换的线性表述。为了处理具有强局部性的时间变体动态，Koopa在时间邻域中计算上下文感知算子。

    Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory of portraying complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by Fourier Filter and design Koopman Predictor to advance respective dynamics forward. Technically, we propose Koopa as a novel Koopman forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal ne
    
[^174]: 子采样与岭回归的广义等价性研究

    Generalized equivalences between subsampling and ridge regularization. (arXiv:2305.18496v1 [math.ST])

    [http://arxiv.org/abs/2305.18496](http://arxiv.org/abs/2305.18496)

    此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。

    

    我们针对举集岭估计器，建立了子采样和岭回归之间的精确结构和风险等价性。具体而言，我们证明了，当用不同的岭正则化水平$\lambda$和子采样比例$\psi$拟合子样岭估计器的线性和二次泛函，在$(\lambda,\psi)$-平面上沿着特定路径渐近等价（其中$\psi$是特征维度与子采样大小的比率）。我们的结果仅要求特征和响应分布具有有界矩，并允许任意联合分布。此外，我们提供了一种数据相关的方法来确定$(\lambda,\psi)$的等价路径。我们结果的间接含义是，在数据方面比例中，调优的岭回归呈现出单调预测风险。这解决了Nakkiran等人提出的一个近期未解决的开放性问题，在一般数据分布和温和的正则条件下。

    We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda, \psi )$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a datadependent method to determine the equivalent paths of $(\lambda, \psi )$. An indirect implication of our equivalences is that optimally-tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. under general data distributions and a mild regularity condition that
    
[^175]: 面向方向的多目标学习：简单且可证明的随机算法

    Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])

    [http://arxiv.org/abs/2305.18409](http://arxiv.org/abs/2305.18409)

    本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。

    

    多目标优化（MOO）已成为许多与多个目标相关的机器学习问题（如多标准学习和多任务学习（MTL））中一个有影响力的框架。本文提出了一种新的面向方向的多目标问题，通过在一个方向的邻域内限制公共下降方向来规范线性组合目标的最优方向，例如MTL中的平均损失。 这个公式包括GD和MGDA作为特殊情况，享受像CAGrad中的面向方向的好处，以及有利于随机算法的设计。为了解决这个问题，我们提出了随机方向导向多目标梯度下降（SDMGrad），它使用简单的SGD类型的更新算法，以及在目标数量较多的情况下，使用高效的目标采样的SDMGrad-OS算法。 对于恒定的正则化参数λ，我们证明SDMGrad和SDMGrad-OS确实收敛到帕累托稳定点。

    Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
    
[^176]: 双保真度变分自编码器用于不确定性量化

    Bi-fidelity Variational Auto-encoder for Uncertainty Quantification. (arXiv:2305.16530v1 [stat.ML])

    [http://arxiv.org/abs/2305.16530](http://arxiv.org/abs/2305.16530)

    本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。

    

    在模型验证中，量化物理系统感兴趣的量的不确定性是一个主要目标。然而，实现这一目标需要平衡计算效率和数值精度之间的需求。为了解决这个问题，我们提出了一种新颖的双保真度变分自编码器（BF-VAE）公式，旨在从物理系统中低、高保真度样本中估计与量感兴趣的量有关的不确定性。该模型通过利用从低保真度样本得出的信息来逼近高保真度量的统计信息。具体而言，我们设计了一个在潜在空间中的双保真度自回归模型，将其整合到VAE的概率编码-解码结构中。我们提出了一种有效的算法，以在存在有限高保真度数据的情况下，最大化高保真度对数似然的变分下界，从而以较低的计算成本合成高保真度的实现。此外，我们在各种数值示例中证明了我们方法的有效性，包括非线性随机系统和计算流体动力学模拟。

    Quantifying the uncertainty of quantities of interest (QoIs) from physical systems is a primary objective in model validation. However, achieving this goal entails balancing the need for computational efficiency with the requirement for numerical accuracy. To address this trade-off, we propose a novel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to estimate the uncertainty associated with a QoI from low-fidelity (LF) and high-fidelity (HF) samples of the QoI. This model allows for the approximation of the statistics of the HF QoI by leveraging information derived from its LF counterpart. Specifically, we design a bi-fidelity auto-regressive model in the latent space that is integrated within the VAE's probabilistic encoder-decoder structure. An effective algorithm is proposed to maximize the variational lower bound of the HF log-likelihood in the presence of limited HF data, resulting in the synthesis of HF realizations with a reduced computational cost. Addit
    
[^177]: 聚焦您的注意力（通过自适应IIR滤波器）

    Focus Your Attention (with Adaptive IIR Filters). (arXiv:2305.14952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14952](http://arxiv.org/abs/2305.14952)

    该论文提出了使用自适应IIR滤波器来聚焦注意力的新层。这些滤波器基于输入序列的前几个块来确定系数，能够将注意力集中在相关的序列元素上，并且相比于其他网络具有更少的参数和次二次时间复杂度。该层在多个长程序列问题上表现优异，超越了Heyna、GPT2和Mega等层。

    

    我们提出了一种新的层，在其中使用动态（即输入相关的）二阶无限冲激响应（IIR）滤波器来处理输入序列，然后再应用传统的注意力机制。输入被分成块，并且这些滤波器的系数基于前面的块来确定以保持因果性。尽管它们相对较低阶，但这些因果自适应滤波器被证明可以将注意力集中在相关的序列元素上。这一新层基于控制理论，并展示了能够推广对角状态空间层。该层的表现与最先进的网络相当，但参数量较少，并且时间复杂度随输入大小的增长是次二次的。所得到的层在多个长程序列问题上的参数数量和性能水平都优于Heyna、GPT2和Mega等层。

    We present a new layer in which dynamic (i.e.,input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control theory, and is shown to generalize diagonal state-space layers. The layer performs on-par with state-of-the-art networks, with a fraction of their parameters and with time complexity that is sub-quadratic with input size. The obtained layer is favorable to layers such as Heyna, GPT2, and Mega, both with respect to the number of parameters and the obtained level of performance on multiple long-range sequence problems.
    
[^178]: 差异性遮挡：选择在继续预训练中遮挡什么

    Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])

    [http://arxiv.org/abs/2305.14577](http://arxiv.org/abs/2305.14577)

    本文提出了一种自动选取遮蔽数据的方法（Difference-Masking），以提高在继续预训练中的自监督学习模型的性能，方法是通过考虑未标记的目标域与预训练域的不同之处来进行。实验证明，该方法可以有效地提升继续预训练任务的性能，且具有跨任务的适用性。

    

    自监督学习(SSL)，特别是遮挡预测目标的目标，已经在各种下游任务中证明了很好的性能，然而，大多数方法都是随机地进行标记和遮挡，而在教育领域有强烈的直觉认为，决定什么需要遮挡可以实质性地改善学习结果。我们引入了差异遮挡(Difference-Masking)，一种自动选择遮挡什么的方法，在继续预训练中通过考虑未标记的目标域与预训练域的不同之处来实现。实证上，我们发现差异遮挡在四个不同的语言和多模态视频任务的继续预训练设置中优于基线。差异性遮挡的跨任务适用性支持我们的框架在语言、视觉和其他领域的SSL预训练中的有效性。

    Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
    
[^179]: 无监督异常检测与拒绝

    Unsupervised Anomaly Detection with Rejection. (arXiv:2305.13189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13189](http://arxiv.org/abs/2305.13189)

    本文提出了一种无监督异常检测方法，通过选择一个恒定的置信度度量和拒绝阈值来解决无标签情况下的异常检测中的挑战。

    

    异常检测旨在检测数据中的意外行为。由于异常检测通常是无监督任务，传统的异常检测器通过采用基于直觉的启发式方法来学习决策边界，这在实践中很难验证。这引入了一些不确定性，特别是在决策边界附近，可能降低用户对检测器预测的信任。抵御这种情况的一种方法是允许检测器拒绝具有高不确定性的示例（学习拒绝）。这需要使用一个能够捕捉到与决策边界的距离的置信度度量，并设置一个拒绝阈值来拒绝低置信度的预测。然而，在没有标签的情况下选择适当的度量和设置拒绝阈值是具有挑战性的任务。在这篇论文中，我们通过在ExCeeD计算的稳定度量上设置一个恒定的拒绝阈值来解决这些挑战。我们的洞察力依赖于对这种度量的理论分析。此外，我们设置了一个基于稳定性度量的拒绝阈值，以此来解决这些挑战。

    Anomaly detection aims at detecting unexpected behaviours in the data. Because anomaly detection is usually an unsupervised task, traditional anomaly detectors learn a decision boundary by employing heuristics based on intuitions, which are hard to verify in practice. This introduces some uncertainty, especially close to the decision boundary, that may reduce the user trust in the detector's predictions. A way to combat this is by allowing the detector to reject examples with high uncertainty (Learning to Reject). This requires employing a confidence metric that captures the distance to the decision boundary and setting a rejection threshold to reject low-confidence predictions. However, selecting a proper metric and setting the rejection threshold without labels are challenging tasks. In this paper, we solve these challenges by setting a constant rejection threshold on the stability metric computed by ExCeeD. Our insight relies on a theoretical analysis of such a metric. Moreover, set
    
[^180]: 用于从稀疏远程传感器数据重建非线性海洋波浪表面相位的机器学习方法

    Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data. (arXiv:2305.11913v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.11913](http://arxiv.org/abs/2305.11913)

    本文提出了一种基于神经网络的方法，利用高度现实的合成训练数据对稀疏雷达数据进行相位相关的波浪表面重建。

    

    准确预测相位相关的水波条件对于海洋工程的决策至关重要。然而，远程监测波浪预测模型的初始化首先需要从类似雷达的稀疏测量中重建波浪表面。现有的重建方法要么依赖于计算密集型的优化过程，要么依赖于简化的模型假设，这会影响整个预测过程的实时性或准确性。因此，我们提出了一种基于U-Net和Fourier神经算子（FNO）结构的神经网络方法，用于相位相关的波浪表面重建。我们的方法利用具有高度现实性的合成训练数据，这些数据在均匀的一维网格上由波浪模拟的高阶谱方法和几何雷达建模方法生成。研究结果表明，两种模型都可以提供准确的波浪重建结果。

    Accurate short-term prediction of phase-resolved water wave conditions is crucial for decision-making in ocean engineering. However, the initialization of remote-sensing-based wave prediction models first requires a reconstruction of wave surfaces from sparse measurements like radar. Existing reconstruction methods either rely on computationally intensive optimization procedures or simplistic modeling assumptions that compromise real-time capability or accuracy of the entire prediction process. We therefore address these issues by proposing a novel approach for phase-resolved wave surface reconstruction using neural networks based on the U-Net and Fourier neural operator (FNO) architectures. Our approach utilizes synthetic yet highly realistic training data on uniform one-dimensional grids, that is generated by the high-order spectral method for wave simulation and a geometric radar modeling approach. The investigation reveals that both models deliver accurate wave reconstruction resul
    
[^181]: Clifford群等变神经网络

    Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11141](http://arxiv.org/abs/2305.11141)

    我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。

    

    我们引入了Clifford群等变神经网络：一种构建O(n)和E(n)等变模型的新方法。我们确定并研究了Clifford群，它是Clifford代数中的一个子群，其定义经过调整以实现多个有利属性。主要地，该群的作用形成了一个正交自同构，扩展到整个Clifford代数，同时尊重多矢分级。这导致了对应于多矢分解的多个非等价子表示。此外，我们证明该作用不仅尊重Clifford代数的向量空间结构，还尊重其乘法结构，即几何乘积。这些发现意味着我们可以得到在任意维的内积空间中优雅地推广的表达层。我们特别展示了从一个sin

    We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
    
[^182]: 面向剧集式马尔可夫决策过程的在线资源分配问题

    Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])

    [http://arxiv.org/abs/2305.10744](http://arxiv.org/abs/2305.10744)

    本文将长期资源分配问题形式化为剧集式MDP模型，提出在面临转换和奖励特性不确定的情况下进行在线资源分配问题的解决方案。该方案基于占有度量提供在线镜像下降算法，其期望遗憾受到界限约束 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .

    

    本文研究了一个长期的资源分配问题，它需要在多个时间段内进行多阶段的决策过程。我们将这个问题形式化为一个剧集式有限时间段的马尔可夫决策过程中的在线资源分配问题，其中转换和奖励以及每一次的资源消耗函数都是非定态的。我们提供了一种等效的在线线性规划重构方法，基于占有度量，为此我们开发了一种在线镜像下降算法。我们的资源分配在线镜像下降算法处理了在估算真实可行集时的不确定性和误差，这是相对独立的。我们证明，对于随机奖励和资源消耗函数，在线镜像下降算法的期望遗憾受到界限约束，其界限受到 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ 的约束，其中 $\rho\in(0,1)$ 是预算参数，$H$ 是地平线长度，$S$ 和 $A$ 是. . .

    This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
    
[^183]: 搜索UGLE真相：无监督GNN学习环境的调查

    Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])

    [http://arxiv.org/abs/2305.06026](http://arxiv.org/abs/2305.06026)

    本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。

    

    图神经网络 (GNN) 是任何机器学习任务中的一个重要工具，因为它们能够学习图结构上的函数，这是一种强大和表达性强的数据表示。社区检测是一种无监督任务，越来越多地使用GNN进行。利用节点特征的多维度与图的连接性对图中的节点进行聚类，对从社交网络到基因组学的真实世界任务有许多应用。不幸的是，目前文献中缺乏公平且严谨评估基于GNN的社区检测的充分基准环境，从而可能阻碍这一新兴领域的进展。我们观察到这种情况下的特定困难是模糊的超参数调整环境与性能和评估数据集的冲突指标。在这项工作中，我们提出和评估了框架，用于在GNN学习环境中进行一致的社区检测算法比较。我们提供了一个基准数据集，并提出了评估指标，反映了检测到的社区的内在质量以及聚类的准确性。

    Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
    
[^184]: DeformerNet: 学习三维可塑物体的双手操纵

    DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04449](http://arxiv.org/abs/2305.04449)

    本论文介绍了一种名为DeformerNet的神经网络架构，通过学习三维可塑物体的低维表示来实现机器人对物体形状的操纵。这种方法不需要手工特征和物体特定的控制模型，可在仿真和真实机器人上进行演示和应用。

    

    从家庭护理到仓库配送再到外科手术助理等领域，应用需要机器人可靠地操纵三维可塑物体的形状。弹性三维可塑物体的分析模型需要大量参数来描述决定物体形状的可能无限自由度。以往的3D形状控制尝试依赖于手工特征来表示物体形状，并需要训练物体特定的控制模型。我们通过使用我们的新型DeformerNet神经网络架构来克服这些问题，该架构在被操纵物体的部分视图点云和目标形状的点云上运行，学习物体形状的低维表示。这个形状嵌入使机器人能够学习一种视觉伺服控制器，该控制器计算出所需的机器人末端执行器动作，将物体迭代地变形向目标形状。我们在仿真和真实机器人上演示了这一点。

    Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
    
[^185]: 从成对相关性中推断局部结构

    Inferring Local Structure from Pairwise Correlations. (arXiv:2305.04386v2 [physics.data-an] UPDATED)

    [http://arxiv.org/abs/2305.04386](http://arxiv.org/abs/2305.04386)

    本研究展示了即使在严重欠采样的情况下，成对相关性提供了恢复局部结构的足够信息，对于理解模型复杂多变量系统的建模和现代关注机制机器学习方法的成功具有重要贡献。

    

    为了构建大型多变量复杂系统的模型，比如生物学中的系统，我们需要限制允许相互作用的变量。这可以被看作是检测变量之间的“局部”结构。在一个简单的二维自然和合成图像的玩具模型的背景下，我们展示了即使在严重欠采样的情况下，变量之间的成对相关性也提供足够的信息来恢复局部关系，包括数据的维度，并且能够重构完全混乱的图像中像素的排列。尽管我们的数据中存在高阶相互作用结构，但这种方法被证明是成功的。我们对成功背后的原因进行了直观分析，希望能对建模复杂多变量系统以及解释现代基于关注机制的机器学习方法的成功有所贡献。

    To construct models of large, multivariate complex systems, such as those in biology, one needs to constrain which variables are allowed to interact. This can be viewed as detecting "local" structures among the variables. In the context of a simple toy model of 2D natural and synthetic images, we show that pairwise correlations between the variables -- even when severely undersampled -- provide enough information to recover local relations, including the dimensionality of the data, and to reconstruct arrangement of pixels in fully scrambled images. This proves to be successful even though higher order interaction structures are present in our data. We build intuition behind the success, which we hope might contribute to modeling complex, multivariate systems and to explaining the success of modern attention-based machine learning approaches.
    
[^186]: 神经网络何时在表格数据上胜过增强树？

    When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])

    [http://arxiv.org/abs/2305.02997](http://arxiv.org/abs/2305.02997)

    这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。

    

    表格数据是机器学习中最常用的数据类型之一。尽管神经网络（NN）在表格数据上取得了最近的进展，但人们仍在积极讨论NN是否通常优于梯度提升决策树（GBDT）在表格数据上的表现，一些最近的工作要么认为GBDT在表格数据上一贯优于NN，要么认为NN优于GBDT。在这项工作中，我们退一步问：'这重要吗？'我们通过对176个数据集比较19种算法，进行了迄今为止最大的表格数据分析，并发现'NN vs. GBDT'争论被过分强调：令人惊讶的是，在相当多的数据集中，GBDT和NN之间的性能差异要么可以忽略不计，要么GBDT的轻微超参数调整比选择最佳算法更重要。接下来，我们分析了965个元特征，以确定数据集的哪些特性使NN或GBDT更适合表现良好。例如，我们发现GBDT要比NN在高维稀疏数据上表现更好。

    Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
    
[^187]: IMAP: 内在驱动的对抗策略

    IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])

    [http://arxiv.org/abs/2305.02605](http://arxiv.org/abs/2305.02605)

    IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。

    

    强化学习（RL）代理在部署过程中容易受到规避攻击的影响。在单智能体环境中，攻击者可以对策略或值网络的输入或输出注入无法察觉的扰动；在多智能体环境中，攻击者可以通过控制对手间接影响受害者的观察。 对抗性策略为解决此类攻击提供了一种有前途的解决方案。然而，目前的方法要么需要受害者政策的完美或部分知识，要么由于任务相关奖励的稀疏性而导致样本效率低下。为克服这些局限性，我们提出了内在驱动的对抗政策（IMAP），用于单智能体和多智能体环境中高效的黑盒规避攻击，而不需任何关于受害者策略的知识。 IMAP利用基于状态覆盖率，策略覆盖率，风险和政策分歧的四个内在目标，以鼓励探索并发现更强的攻击技能。我们还描述了一种处理多个具有不同实力的对手的可推广算法。我们的实验表明，IMAP在单智能体和多智能体环境中均优于最先进的方法，包括两个Atari游戏，一个机器人运动任务和一个多智能体游戏。

    Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des
    
[^188]: 利用愚蠢合同进行学习

    Schooling to Exploit Foolish Contracts. (arXiv:2304.10737v1 [cs.CR])

    [http://arxiv.org/abs/2304.10737](http://arxiv.org/abs/2304.10737)

    SCooLS是一个智能合约学习引擎，使用半监督学习和图神经网络，可以直接分析以太坊合约字节码并识别易受攻击的功能，性能优于现有工具，准确度高达98.4%，是第一个基于深度学习的漏洞分析器。

    

    我们介绍了SCooLS，即我们的智能合约学习（半监督）引擎。SCooLS使用神经网络来分析以太坊合约字节码并识别特定的易受攻击的功能。SCooLS包括两个关键元素：半监督学习和图神经网络（GNN）。半监督学习比无监督学习产生更准确的模型，同时不需要大型的标记训练集，而有监督学习则需要。GNN使得可以直接分析智能合约字节码，而不需要任何手动特征工程、预定义的模式或专家规则。SCooLS是半监督学习应用于智能合约漏洞分析的首个应用，也是第一个基于深度学习的漏洞分析器，可以识别特定易受攻击的功能。SCooLS的性能优于现有工具，准确度达到了98.4%，F1得分达到了90.5%，假阳性率仅为0.8%。此外，SCooLS速度很快，

    We introduce SCooLS, our Smart Contract Learning (Semi-supervised) engine. SCooLS uses neural networks to analyze Ethereum contract bytecode and identifies specific vulnerable functions. SCooLS incorporates two key elements: semi-supervised learning and graph neural networks (GNNs). Semi-supervised learning produces more accurate models than unsupervised learning, while not requiring the large oracle-labeled training set that supervised learning requires. GNNs enable direct analysis of smart contract bytecode without any manual feature engineering, predefined patterns, or expert rules.  SCooLS is the first application of semi-supervised learning to smart contract vulnerability analysis, as well as the first deep learning-based vulnerability analyzer to identify specific vulnerable functions. SCooLS's performance is better than existing tools, with an accuracy level of 98.4%, an F1 score of 90.5%, and an exceptionally low false positive rate of only 0.8%. Furthermore, SCooLS is fast, an
    
[^189]: 通过知识蒸馏进行低内存设备的混合硅片分类

    Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation. (arXiv:2303.13974v1 [cs.LG])

    [http://arxiv.org/abs/2303.13974](http://arxiv.org/abs/2303.13974)

    本文提出了一种通过知识蒸馏技术，将复杂预训练模型的知识转移到轻量级模型上，从而使低内存设备也能进行复杂缺陷分类，且无需大量标记数据。

    

    制造硅片是一个复杂的任务，涉及数千个步骤。硅片地图的缺陷模式识别对于确定生产缺陷的根本原因至关重要，这可能进一步为硅片工厂的产量提高提供见解。在制造过程中，各种缺陷可能单独出现在硅片中，也可能以不同的组合形式出现。识别硅片中的多个缺陷通常比识别单个缺陷更难。最近，深度学习方法在混合类型DPR方面获得了显着的进展。然而，这些缺陷的复杂性需要大型复杂模型，使它们很难在通常用于制造实验室的低内存嵌入式设备上运行。另一个常见问题是缺乏标记数据来训练复杂网络。在这项工作中，我们提出了一种无监督训练程序，将复杂预训练模型的知识蒸馏到轻量级可部署模型中。我们凭经验证明，这种方法导致分类模型具有与最先进模型相当的性能，同时也足够高效，可在低内存设备上运行。

    Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial for determining the root cause of production defects, which may further provide insight for yield improvement in wafer foundry. During manufacturing, various defects may appear standalone in the wafer or may appear as different combinations. Identifying multiple defects in a wafer is generally harder compared to identifying a single defect. Recently, deep learning methods have gained significant traction in mixed-type DPR. However, the complexity of defects requires complex and large models making them very difficult to operate on low-memory embedded devices typically used in fabrication labs. Another common issue is the unavailability of labeled data to train complex networks. In this work, we propose an unsupervised training routine to distill the knowledge of complex pre-trained models to lightweight deployment-ready models. We empirically show that this 
    
[^190]: 语义转换混淆AI生成文本检测，而检索是一种有效的防御方法

    Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])

    [http://arxiv.org/abs/2303.13408](http://arxiv.org/abs/2303.13408)

    本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。

    

    近期有多种方法被提出来用于识别恶意使用大型语言模型 (例如虚假内容创建或学术抄袭)中的AI生成文本，包括通过水印或统计异常点。本文探究这些文本检测算法对于AI生成文本的含义转换的鲁棒性。为了测试这些检测器的性能，我们首先训练了一个11B参数的语义转换生成模型(DIPPER)，该模型可以将段落进行语义转换，可选择利用周围文本(例如用户写的提示)作为上下文。DIPPER还使用标量旋钮来控制语义转换中词汇多样性和重新排列的程度。通过使用DIPPER来进行三种大型语言模型生成文本的语义转换，成功地混淆了多个文本检测器，包括水印检测、GPTZero、DetectGPT和OpenAI的文本分类器。例如，DIPPER将DetectGPT的检测准确率从70.3%降至4.6%（在恒定的1%误报率下）。

    To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
    
[^191]: 结合基于结构的编码器和预训练的蛋白质语言模型的增强

    Enhancing Protein Language Models with Structure-based Encoder and Pre-training. (arXiv:2303.06275v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06275](http://arxiv.org/abs/2303.06275)

    本文提出了一种结合基于结构的编码器和预训练的蛋白质语言模型，以明确地编码蛋白质结构，获得更好的结构感知蛋白质表示，并在实验中验证了其有效性。

    This paper proposes enhancing protein language models with structure-based encoder and pre-training to explicitly encode protein structures for better structure-aware protein representations, and empirically verifies its effectiveness.

    在大规模蛋白质序列语料库上预训练的蛋白质语言模型（PLMs）在各种下游蛋白质理解任务中取得了令人印象深刻的表现。尽管能够隐式地捕获残基间的接触信息，但基于变压器的PLMs不能明确地编码蛋白质结构，以获得更好的结构感知蛋白质表示。此外，尽管结构对于确定功能很重要，但尚未探索在可用蛋白质结构上进行预训练以改进这些PLMs的能力。为了解决这些限制，我们在本文中使用基于结构的编码器和预训练来增强PLMs。

    Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNe
    
[^192]: 使用机器学习的未来引力波任务重建哈勃参数

    Reconstructing the Hubble parameter with future Gravitational Wave missions using Machine Learning. (arXiv:2303.05169v2 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/2303.05169](http://arxiv.org/abs/2303.05169)

    本研究使用机器学习算法，通过未来的引力波任务重建了哈勃参数，得出了高斯过程在重建宇宙膨胀历史方面的稳健性，同时也发现未来任务能够提供与当前数据集相竞争的对哈勃参数和哈勃常数的约束。

    

    我们研究了使用高斯过程（GP），一种机器学习算法，通过两个即将到来的引力波任务，即进化的激光干涉空间天线（eLISA）和爱因斯坦望远镜（ET），重建哈勃参数$H(z)$的前景。在假设各种背景宇宙模型的情况下，使用GP以非参数化的方式重建了哈勃参数，并利用每个任务的真实生成目录。我们重点分别关注了早期和后期的先验对$H(z)$和哈勃常数($H_0$)重建的影响。我们的分析表明，在考虑的特定任务的观测窗口范围内，GP在重建宇宙的膨胀历史方面非常稳健。我们进一步确认，eLISA和ET都将能够提供对$H(z)$和$H_0$的约束，这将与当前数据集得出的约束相竞争。尤其是，w

    We study the prospects of Gaussian processes (GP), a machine learning (ML) algorithm, as a tool to reconstruct the Hubble parameter $H(z)$ with two upcoming gravitational wave missions, namely the evolved Laser Interferometer Space Antenna (eLISA) and the Einstein Telescope (ET). Assuming various background cosmological models, the Hubble parameter has been reconstructed in a non-parametric manner with the help of GP using realistically generated catalogs for each mission. The effects of early-time and late-time priors on the reconstruction of $H(z)$, and hence on the Hubble constant ($H_0$), have also been focused on separately. Our analysis reveals that GP is quite robust in reconstructing the expansion history of the Universe within the observational window of the specific missions under consideration. We further confirm that both eLISA and ET would be able to provide constraints on $H(z)$ and $H_0$ which would be competitive to those inferred from current datasets. In particular, w
    
[^193]: 模型无关的联邦学习

    Model-Agnostic Federated Learning. (arXiv:2303.04906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04906](http://arxiv.org/abs/2303.04906)

    这篇论文提出了一种模型无关的联邦学习系统（MAFL），该系统不仅适用于深度神经网络（DNN）和决策树等特定类型的机器学习模型，还能在非DNN场景中应用，并且通过优化实现了较高的性能。

    

    自从2016年引入以来，联邦学习（FL）一直与深度神经网络（DNN）的内部工作紧密相关。一方面，这使得其得以在DNN广泛使用的同时发展和推广。另一方面，这忽视了所有那些不可行或不具优势使用DNN的情况。目前大部分FL框架仅允许训练DNN，加剧了这个问题。为了解决非DNN场景下缺乏FL解决方案的问题，我们提出了MAFL（模型无关的联邦学习）。MAFL将一个模型无关的FL算法AdaBoost.F与一种开放的工业级FL框架Intel OpenFL结合起来。MAFL是第一个不与任何特定类型的机器学习模型绑定的FL系统，允许探索超越DNN和树的FL场景。我们从多个角度测试了MAFL，评估了其正确性、灵活性和可扩展性，最多达到64个节点。我们对基础软件进行了优化，在标准FL场景中实现了5.5倍的加速。MAFL与

    Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs). On the one hand, this allowed its development and widespread use as DNNs proliferated. On the other hand, it neglected all those scenarios in which using DNNs is not possible or advantageous. The fact that most current FL frameworks only allow training DNNs reinforces this problem. To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a model-agnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel OpenFL. MAFL is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees. We test MAFL from multiple points of view, assessing its correctness, flexibility and scaling properties up to 64 nodes. We optimised the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is compatible wi
    
[^194]: Taylor TD学习

    Taylor TD-learning. (arXiv:2302.14182v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14182](http://arxiv.org/abs/2302.14182)

    Taylor TD是一个基于模型的RL框架，通过使用TD更新的泰勒级数展开，减少了连续状态-动作设置中的方差，并具有与标准TD学习相同的稳定学习保证。TaTD3是Taylor TD与TD3算法相结合所形成的方法，其表现优于一些最先进的无模型和基于模型的基准。

    

    许多强化学习方法依赖于时间差分（TD）学习来学习一个评论家。然而，TD学习的更新可能具有较高的方差。在这里，我们引入了一个基于模型的RL框架，即Taylor TD，它减少了连续状态-动作设置中的方差。Taylor TD使用TD更新的一阶泰勒级数展开。该展开允许Taylor TD在行动选择的随机性和每个TD更新的初始状态和动作的状态分布的一些随机性上进行分析积分。我们提供理论和实证证据，证明Taylor TD的更新确实比标准的TD更新具有较低的方差。此外，我们还展示了在合理的假设下，Taylor TD具有与线性函数逼近下的标准TD学习相同的稳定学习保证。接下来，我们将Taylor TD与TD3算法相结合，形成TaTD3。我们展示TaTD3的表现与几种最先进的无模型和基于模型的基准相当，甚至更好。

    Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based basel
    
[^195]: 通过监督稀疏多粒度学习进行视频文本检索

    Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09473](http://arxiv.org/abs/2302.09473)

    本文提出了一种新的多粒度稀疏学习框架S3MA，用于视频文本检索。该框架通过学习共享的稀疏空间和多粒度相似度来改进检索效果，在多个基准测试中表现优于现有方法。

    

    对于视频文本检索，最近在探索更好的表示学习方面取得了进展。本文提出了一个新的多粒度稀疏学习框架S3MA，用于学习视频和文本之间的共享稀疏空间，从而实现视频文本检索。共享稀疏空间通过有限数量的稀疏概念进行初始化，每个概念都对应一些词语。利用现有的文本数据，我们以监督方式学习和更新共享稀疏空间，使用提出的相似度和对齐损失函数。此外，为了实现多粒度对齐，我们将帧表示方法纳入模型，更好地对视频模态进行建模和计算细粒度和粗粒度的相似度。通过学习得到的共享稀疏空间和多粒度相似度，我们在多个视频文本检索基准上进行了广泛实验，实验结果表明S3MA优于现有方法。我们的代码可以在https://github.com/yim上找到。

    While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at https://github.com/yim
    
[^196]: Marich：一种使用公共数据的查询效率高的分布等价模型提取攻击

    Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data. (arXiv:2302.08466v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08466](http://arxiv.org/abs/2302.08466)

    本研究设计了一种黑盒模型提取攻击方法Marich，它使用公共数据集中的最小数量查询来创建一个与目标模型具有信息丰富度和分布等价性的副本，实验结果表明Marich能提取具有60-95%真实模型准确性的模型。

    

    我们研究设计黑盒模型提取攻击，该攻击可以通过一个预测API从一个公开可用的数据集向目标ML模型发送最小数量的查询，以创建一个具有信息丰富度和分布等价性的目标副本。首先，我们定义了分布等价和最大信息模型提取攻击，并将它们简化为一个变分优化问题。攻击者顺序解决这个优化问题，选择最具信息量的查询，同时最大化熵和降低目标和盗窃模型之间的不匹配。这导致了一种基于主动抽样的查询选择算法Marich，它是模型无关的。然后，我们在不同的文本和图像数据集以及不同的模型上评估了Marich。Marich提取的模型实现了真实模型准确性的60-95％，并使用了来自公开可用数据集的1,000-8,500个查询。

    We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\sim 60-95\%$ of true model's accuracy and uses $\sim 1,000 - 8,500$ queries from the publicly available datasets, which are differen
    
[^197]: 用于分散式机器学习的新兴RISC-V系统的实验

    Experimenting with Emerging RISC-V Systems for Decentralised Machine Learning. (arXiv:2302.07946v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2302.07946](http://arxiv.org/abs/2302.07946)

    该研究介绍了一种特定领域语言，用于将分散式机器学习方案映射到FastFlow并行编程库。通过在不同处理器平台上生成不同的DML方案，研究者评估了所提出方案和系统的性能和能源效率，并成功移植了PyTorch框架到RISC-V平台。

    

    分散式机器学习（DML）使合作机器学习摆脱了集中式输入数据。联合学习（FL）和边缘推断是DML的示例。虽然DML工具（特别是FL）开始蓬勃发展，但许多工具不够灵活和便携，无法用于实验新型处理器（例如RISC-V），非全连接网络拓扑和异步协作方案。我们通过一种特定领域的语言克服了这些限制，将DML方案映射到基础中间件（即FastFlow并行编程库）。我们通过在x86-64和ARM平台以及新兴的RISC-V平台上生成不同的DML方案来进行实验。我们表征了所提出的方案和系统的性能和能源效率。作为附带产品，我们介绍了PyTorch框架的RISC-V移植，这是我们所知道的第一个公开可用的移植。

    Decentralised Machine Learning (DML) enables collaborative machine learning without centralised input data. Federated Learning (FL) and Edge Inference are examples of DML. While tools for DML (especially FL) are starting to flourish, many are not flexible and portable enough to experiment with novel processors (e.g., RISC-V), non-fully connected network topologies, and asynchronous collaboration schemes. We overcome these limitations via a domain-specific language allowing us to map DML schemes to an underlying middleware, i.e. the FastFlow parallel programming library. We experiment with it by generating different working DML schemes on x86-64 and ARM platforms and an emerging RISC-V one. We characterise the performance and energy efficiency of the presented schemes and systems. As a byproduct, we introduce a RISC-V porting of the PyTorch framework, the first publicly available to our knowledge.
    
[^198]: 面向药物靶向相互作用的联邦学习基准

    A Federated Learning Benchmark for Drug-Target Interaction. (arXiv:2302.07684v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07684](http://arxiv.org/abs/2302.07684)

    本文提出在DTI领域中采用联邦学习来汇集制药数据，相对于最佳非隐私保护替代方法可获得高达15%的性能提升，且非IID数据分布不会降低联邦学习的性能。

    

    在药物靶向相互作用(DTI)领域中汇集制药数据具有提供挽救生命的突破的潜力。然而，由于监管限制和商业利益，这是非常困难的。本文提出了联邦学习的应用，认为这与行业的限制是和解的，因为它不需要共享任何信息，这些信息可以揭示实体的数据或任何其他的高水平总结。当运用于代表性的GraphDTA模型和KIBA数据集时，相对于最佳的非隐私保护替代方法，它可以实现高达15%的性能提升。我们广泛的一系列实验表明，在DTI数据集中，与其他领域不同的是，非IID数据分布不会降低联邦学习的性能。此外，我们确定了添加新数据的益处与添加更多客户的成本之间的实质性平衡。

    Aggregating pharmaceutical data in the drug-target interaction (DTI) domain has the potential to deliver life-saving breakthroughs. It is, however, notoriously difficult due to regulatory constraints and commercial interests. This work proposes the application of federated learning, which we argue to be reconcilable with the industry's constraints, as it does not require sharing of any information that would reveal the entities' data or any other high-level summary of it. When used on a representative GraphDTA model and the KIBA dataset it achieves up to 15% improved performance relative to the best available non-privacy preserving alternative. Our extensive battery of experiments shows that, unlike in other domains, the non-IID data distribution in the DTI datasets does not deteriorate FL performance. Additionally, we identify a material trade-off between the benefits of adding new data, and the cost of adding more clients.
    
[^199]: 回顾链将语言模型与反馈对齐

    Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02676](http://arxiv.org/abs/2302.02676)

    该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。

    

    从人类偏好中学习对于语言模型具有重要意义，这样才能对人类有所帮助并符合人类和社会价值观。先前的研究通过从人类反馈中学习来理解和遵循指令取得了显著成功。然而，这些方法要么是基于被人类注释者喜欢的手动挑选的模型生成，使得它们在数据利用方面效果不佳且普遍应用具有挑战性，要么依赖于奖励函数和强化学习，这容易出现奖励函数不完美和极难优化的问题。在本文中，我们提出了一种新颖的技术，“回顾链”，它易于优化，并可以从任何形式的反馈中学习，而不受其极性的影响。我们的想法受到了人类如何从以语言形式呈现的广泛反馈中学习的启发。我们将所有类型的反馈转换成句子，然后用它们来微调模型，从而利用这种方法。

    Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
    
[^200]: 神经关系图：识别标签噪音和异常数据的统一框架

    Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12321](http://arxiv.org/abs/2301.12321)

    本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。

    

    诊断和清理数据是构建健壮的机器学习系统的关键步骤。但是，由于存在复杂问题，如标签错误、欠表示和异常值，因此在具有真实世界分布的大规模数据集中识别问题具有挑战性。在本文中，我们提出了一种利用特征嵌入空间中数据的关系结构这一被忽视的信息来源，来识别有问题的数据的统一方法。为此，我们提出了基于数据的关系图结构来检测标签错误和异常数据的可扩展和有效的算法。我们进一步引入了一种可视化工具，提供特征嵌入空间中数据点的上下文信息，作为交互式诊断数据的有效工具。我们在大规模图像、语音和语言领域任务中评估了我们方法的标签错误和离群值/分布外（OOD）检测性能。

    Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
    
[^201]: 关于时变博弈中无悔学习动态的收敛性问题

    On the Convergence of No-Regret Learning Dynamics in Time-Varying Games. (arXiv:2301.11241v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11241](http://arxiv.org/abs/2301.11241)

    本文研究了时变博弈中乐观梯度下降法（OGD）的收敛性，提出了明确的收敛界限，并建立了适用于时变总和多人博弈和元学习的新型双线性公式。

    

    大多数关于博弈学习的文献都集中于底层重复博弈不发生变化的严格模式下。对于动态多智体游戏中无悔学习算法的收敛性问题，我们知之甚少。在本文中，我们研究了时变博弈中乐观梯度下降法（OGD）的收敛性。我们的框架针对自然变化度量的博弈序列的均衡间隙，为OGD提供了明确的收敛界限，从而涵盖了静态博弈的已知结果。此外，只要每场游戏都进行了多次，我们还通过强凸性-强凹性建立了改进的二阶变化界限。我们的结果还适用于时变的总和多人博弈，通过相关均衡的双线性公式，这对元学习以及获得针对变化依赖性后悔界限的精细需求具有新颖意义。

    Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of optimistic gradient descent (OGD) in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved second-order variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also apply to time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally,
    
[^202]: 高性能计算中的神话与传说

    Myths and Legends in High-Performance Computing. (arXiv:2301.02432v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2301.02432](http://arxiv.org/abs/2301.02432)

    这篇文章讨论了高性能计算社区中流传的神话和传说，这些神话往往不基于科学事实，而是基于一些证据或论证。虽然有些问题仍然是无休止的哲学辩论，但新的方向正在出现，如算法的规模化或新的架构研究。

    

    在这篇发人深省的文章中，我们讨论了高性能计算社区成员间流传的一些神话和传说。我们从会议和会议上的对话、产品广告、论文以及社区内外的推特、博客和新闻文章中收集了这些神话。我们认为它们代表了当前时代的时代精神，这个时代正在经历许多规模定律的终结，如Dennard定律和摩尔定律。虽然一些定律结束了，但新的方向正在出现，比如算法的规模化或新的架构研究。然而，这些神话很少基于科学事实，而是基于一些证据或论证。事实上，我们认为这正是许多神话存在的原因，也是为什么它们无法得到明确答案的原因。虽然每个神话都应该有明确的答案，但有些问题可能仍然是无休止的哲学辩论，比如贝多芬比谁更好。

    In this thought-provoking article, we discuss certain myths and legends that are folklore among members of the high-performance computing community. We gathered these myths from conversations at conferences and meetings, product advertisements, papers, and other communications such as tweets, blogs, and news articles within and beyond our community. We believe they represent the zeitgeist of the current era of massive change, driven by the end of many scaling laws such as Dennard scaling and Moore's law. While some laws end, new directions are emerging, such as algorithmic scaling or novel architecture research. Nevertheless, these myths are rarely based on scientific facts, but rather on some evidence or argumentation. In fact, we believe that this is the very reason for the existence of many myths and why they cannot be answered clearly. While it feels like there should be clear answers for each, some may remain endless philosophical debates, such as whether Beethoven was better than
    
[^203]: 学习用于排名的列表级别领域不变表示

    Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.10764](http://arxiv.org/abs/2212.10764)

    本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。

    

    领域适应旨在将在（数据丰富）源领域学到的知识转移到（资源有限）目标领域，一种常用的方法是不变表示学习，它匹配并对齐特征空间上的数据分布。尽管这种方法在分类和回归问题上得到了广泛研究和应用，但在排名问题上的应用却是零散的，并且现有的几种实现缺乏理论上的证明。本文重新审视了用于排名的不变表示学习。在审查之前的工作时，我们发现他们实施了我们称之为项目级别对齐的方法，该方法在聚合的所有列表中对进行排名的项目分布进行对齐，但忽略了列表的结构。然而，列表的结构应该被利用，因为它是排名问题的固有特性，其中数据和度量是在列表上定义和计算的，而不是在项目本身上。为了解决这一不一致，我们提出了列表级别对齐的学习

    Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
    
[^204]: GRACE：离散键值适配器实现的终身模型编辑

    Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11031](http://arxiv.org/abs/2211.11031)

    本论文提出了一种名为GRACE的方法来实现终身模型编辑，它通过在流式错误上执行目标编辑来修复部署模型的问题，生成一个离散、本地的编辑编码本，而不会改变模型权重，在进行数千个顺序编辑时表现为最先进的性能。

    

    部署的模型随时间推移会衰退，原因是输入的变化、用户需求不断改变、或由于出现知识空缺。当发现有害行为时，需要进行有针对性的编辑。然而，当前的模型编辑器在多次编辑中会降低模型的性能。我们提出了GRACE，一种终身模型编辑方法，它在部署模型的流式错误上实现了问题修补，确保对不相关的输入的影响最小化。GRACE将新的映射项写入预训练模型的潜在空间，创建了一个离散的、本地的编码本，而不会改变模型权重。这是第一种只使用流式错误实现数千个顺序编辑的方法。我们在T5、BERT和GPT模型上进行了实验，结果表明GRACE在进行并保留编辑方面的性能处于最先进水平，同时可以推广到未见过的输入。我们的代码可在https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace中获得。

    Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.
    
[^205]: FairMILE: 实现公平的图表示学习效率框架

    FairMILE: Towards an Efficient Framework for Fair Graph Representation Learning. (arXiv:2211.09925v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09925](http://arxiv.org/abs/2211.09925)

    提出了一种名为FairMILE的多层范式框架，用于解决图表示学习中的公平性问题，并且在保持效用的同时实现了高效的图表示学习。

    

    图表示学习模型在许多实际应用中展示了强大的能力。然而，先前的研究表明，这些模型可能会学习到偏见的表示，导致歧视性的结果。已经提出了一些方法来减轻图表示中的偏见。然而，大多数现有方法在训练和微调时需要异常的时间和计算资源。针对这个问题，我们研究了有效的公平图表示学习问题，并提出了一种新的框架FairMILE。FairMILE是一个多层范式，可以在保持公平性和保持实用性的同时有效地学习图表示。它可以与任何无监督嵌入方法配合使用，并且可以适应各种公平性约束。在不同的下游任务上进行了大量实验，结果显示FairMILE在运行时间方面明显优于现有方法，同时在公平性和效果之间实现了更优的平衡。

    Graph representation learning models have demonstrated great capability in many real-world applications. Nevertheless, prior research indicates that these models can learn biased representations leading to discriminatory outcomes. A few works have been proposed to mitigate the bias in graph representations. However, most existing works require exceptional time and computing resources for training and fine-tuning. To this end, we study the problem of efficient fair graph representation learning and propose a novel framework FairMILE. FairMILE is a multi-level paradigm that can efficiently learn graph representations while enforcing fairness and preserving utility. It can work in conjunction with any unsupervised embedding approach and accommodate various fairness constraints. Extensive experiments across different downstream tasks demonstrate that FairMILE significantly outperforms state-of-the-art baselines in terms of running time while achieving a superior trade-off between fairness 
    
[^206]: FedFA: 针对异构数据的特征锚定联邦学习

    FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data. (arXiv:2211.09299v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09299](http://arxiv.org/abs/2211.09299)

    本文提出了一种名为 FedFA 的联邦学习框架，通过特征锚定来对齐特征映射并校准分类器，解决了在异构数据时分类器和特征映射之间的恶性循环问题。在实验中表明，该方法能够提高准确性和收敛速度。

    

    联邦学习允许多个客户端在不交换数据的情况下协作训练模型，从而保护数据隐私。然而，在客户端存在异构数据时，它会遭受明显的性能下降。本文发现，常见的本地训练解决方案通过设计特定的辅助损失函数来规范权重差异或特征不一致性，但这些方法忽略了分类器和特征映射不一致之间的恶性循环，导致客户端模型在特征空间和分类器差异的不一致特征空间中更新。我们提出了一个称为 FedFA 的简单而有效的框架，在本地训练过程中通过特征锚定来对齐客户端之间的特征映射并校准分类器，从而使客户端模型在共享的特征空间和一致的分类器下更新。我们证明，与先前的方法相比，在异构数据情况下，这种修改后的联邦学习方法提高了准确性和收敛速度。

    Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation under heterogeneous data at clients. Common solutions in local training involve designing a specific auxiliary loss to regularize weight divergence or feature inconsistency. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between classifier divergence and feature mapping inconsistency across clients, such that client models are updated in inconsistent feature space with diverged classifiers. We then propose a simple yet effective framework named Federated learning with Feature Anchors (FedFA) to align the feature mappings and calibrate classifier across clients during local training, which allows client models updating in a shared feature space with consistent classifiers. We demonstrate that this modific
    
[^207]: 深度距离敏感预测算法

    Deep Distance Sensitivity Oracles. (arXiv:2211.02681v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02681](http://arxiv.org/abs/2211.02681)

    这篇论文介绍了一种使用深度学习技术构建距离敏感预测算法的方法，利用了替代路径的组合结构。

    

    最基本的图问题之一是寻找从源节点到目标节点的最短路径。虽然问题的基本形式已经被广泛研究并且已知有效的算法，但是当图的某些部分容易失败时，问题变得更加困难。尽管可以在每次故障后重新计算替代路径的最短路径，但这在时间和/或存储上非常低效。解决这个问题的一种方法是将计算负担从查询转移到预处理步骤中，其中计算出一个数据结构，允许快速查询替代路径，通常称为距离敏感预测算法（DSO）。尽管在理论计算机科学界对DSO进行了广泛研究，但据我们所知，这是第一个使用深度学习技术构建DSO的工作。我们展示了如何利用深度学习来利用替代路径的组合结构。

    One of the most fundamental graph problems is finding a shortest path from a source to a target node. While in its basic forms the problem has been studied extensively and efficient algorithms are known, it becomes significantly harder as soon as parts of the graph are susceptible to failure. Although one can recompute a shortest replacement path after every outage, this is rather inefficient both in time and/or storage. One way to overcome this problem is to shift computational burden from the queries into a pre-processing step, where a data structure is computed that allows for fast querying of replacement paths, typically referred to as a Distance Sensitivity Oracle (DSO). While DSOs have been extensively studied in the theoretical computer science community, to the best of our knowledge this is the first work to construct DSOs using deep learning techniques. We show how to use deep learning to utilize a combinatorial structure of replacement paths. More specifically, we utilize the
    
[^208]: 无监督异常检测中污染因子分布的估计

    Estimating the Contamination Factor's Distribution in Unsupervised Anomaly Detection. (arXiv:2210.10487v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10487](http://arxiv.org/abs/2210.10487)

    该论文提出了一种从贝叶斯的角度估计无监督异常检测中污染因子的后验分布的方法，并使用多个异常检测器的输出作为表示，并通过特定的混合形式进行估计。在22个数据集的实证研究中，该方法表现良好。

    

    异常检测方法在无监督情况下通过为示例分配基于各种启发式规则的实值异常分数来识别不符合预期行为的示例。这些分数需要通过阈值化转换为实际预测，从而使被标记为异常的示例比例等于预期的异常比例，称为污染因子。不幸的是，目前没有好的方法来估计污染因子本身。我们从贝叶斯的角度来解决这个问题，引入了一种估计给定无标签数据集的污染因子的后验分布的方法。我们利用多个异常检测器的输出作为已经捕捉到异常性的表示，并使用特定的混合形式来估计污染。在22个数据集的实证研究中，我们表明估计的分布是良好校准的，并且通过设置阈值使用这种方法可以取得良好的性能。

    Anomaly detection methods identify examples that do not follow the expected behaviour, typically in an unsupervised fashion, by assigning real-valued anomaly scores to the examples based on various heuristics. These scores need to be transformed into actual predictions by thresholding, so that the proportion of examples marked as anomalies equals the expected proportion of anomalies, called contamination factor. Unfortunately, there are no good methods for estimating the contamination factor itself. We address this need from a Bayesian perspective, introducing a method for estimating the posterior distribution of the contamination factor of a given unlabeled dataset. We leverage on outputs of several anomaly detectors as a representation that already captures the basic notion of anomalousness and estimate the contamination using a specific mixture formulation. Empirically on 22 datasets, we show that the estimated distribution is well-calibrated and that setting the threshold using the
    
[^209]: 评估分子图嵌入的自监督学习

    Evaluating Self-Supervised Learning for Molecular Graph Embeddings. (arXiv:2206.08005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08005](http://arxiv.org/abs/2206.08005)

    评估分子图嵌入的自监督学习方法的MOLGRAPHEVAL揭示了在不同的下游任务中GSSL方法性能存在显著的不一致性，并为未来研究提供了新的方向。

    

    图形自监督学习（GSSL）为获取嵌入提供了一个强大的途径，无需专家标注，这种能力对于分子图具有深刻的影响，因为潜在分子的数量惊人，并且获取标签的成本很高。然而，GSSL方法不是为了在特定领域内进行优化，而是为了在各种下游任务之间进行可转移性。这种广泛适用性使它们的评估变得复杂。为了解决这一挑战，我们提出了“分子图表示评估”（MOLGRAPHEVAL），生成具有可解释和多样化属性的分子图嵌入的详细剖析。MOLGRAPHEVAL提供了一组探测任务，分为三类：（i）通用图形，（ii）分子亚结构和（iii）嵌入空间属性。通过利用MOLGRAPHEVAL来基准化现有的GSSL方法，对比当前的下游数据集以及我们的任务套件，我们发现GSSL方法性能存在显著的不一致性，并为未来的研究提供了新的方向。

    Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inco
    
[^210]: COVID-Net生化：基于可解释性的框架构建机器学习模型，用于预测COVID-19患者的生存和肾损伤状况的临床和生化数据

    COVID-Net Biochem: An Explainability-driven Framework to Building Machine Learning Models for Predicting Survival and Kidney Injury of COVID-19 Patients from Clinical and Biochemistry Data. (arXiv:2204.11210v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.11210](http://arxiv.org/abs/2204.11210)

    COVID-Net Biochem是一个可解释的框架，通过整合领域专业知识，利用临床和生化数据来预测COVID-19患者的生存和肾损伤风险。

    

    自从世界卫生组织在2020年宣布COVID-19为全球大流行病以来，全球社会一直面临着控制和缓解SARS-CoV-2病毒及其不断进化的亚型和重组物传播的持续挑战。在大流行期间的一个重要挑战不仅是准确检测阳性病例，还包括高效预测并发症风险和患者生存概率。这些任务需要相当量的临床资源分配和关注。在本研究中，我们介绍了COVID-Net生化，这是一个灵活且可解释的框架，用于构建机器学习模型。我们使用临床和生化数据以透明、系统的方式，利用这个框架来预测COVID-19患者的生存和住院期间发展急性肾损伤的可能性。所提出的方法通过将领域专业知识与可解释性方法无缝集成，推进了机器学习模型设计的发展。

    Since the World Health Organization declared COVID-19 a pandemic in 2020, the global community has faced ongoing challenges in controlling and mitigating the transmission of the SARS-CoV-2 virus, as well as its evolving subvariants and recombinants. A significant challenge during the pandemic has not only been the accurate detection of positive cases but also the efficient prediction of risks associated with complications and patient survival probabilities. These tasks entail considerable clinical resource allocation and attention.In this study, we introduce COVID-Net Biochem, a versatile and explainable framework for constructing machine learning models. We apply this framework to predict COVID-19 patient survival and the likelihood of developing Acute Kidney Injury during hospitalization, utilizing clinical and biochemical data in a transparent, systematic approach. The proposed approach advances machine learning model design by seamlessly integrating domain expertise with explainabi
    
[^211]: 用两样本检验解决强化学习中的最大化偏差问题

    Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing. (arXiv:2201.08078v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.08078](http://arxiv.org/abs/2201.08078)

    本文提出了一种解决强化学习中最大化偏差问题的方法，使用了两样本检验的估计器，能够灵活地插值过度估计和欠估计之间的关系，并在$Q$学习和引导化深度Q网络中得到了验证。

    

    基于值的强化学习算法在游戏、机器人学和其他现实世界应用中取得了强大的结果。过度估计偏差是这些算法面临的已知威胁，可能导致性能急剧下降甚至完全失败。我们将偏差问题从统计学角度进行框架化，将其视为估计一组随机变量的最大期望值（MEV）的实例。我们提出了基于两样本检验的$T$-估计器（TE），通过调整底层假设检验的显著性水平，灵活地插值过度估计和欠估计之间的关系。一种命名为$K$-估计器（KE）的推广遵守与TE相同的偏差和方差界限，同时依赖于几乎任意的核函数。我们介绍了使用TE和KE的$Q$学习和引导化深度Q网络（BDQN）的修改，并在表格设置中证明其收敛性。此外，我们提出了一种自适应变体的基于TE的BDQN。

    Value-based reinforcement-learning algorithms have shown strong results in games, robotics, and other real-world applications. Overestimation bias is a known threat to those algorithms and can lead to dramatic performance decreases or even complete algorithmic failure. We frame the bias problem statistically and consider it an instance of estimating the maximum expected value (MEV) of a set of random variables. We propose the $T$-Estimator (TE) based on two-sample testing for the mean, that flexibly interpolates between over- and underestimation by adjusting the significance level of the underlying hypothesis tests. A generalization, termed $K$-Estimator (KE), obeys the same bias and variance bounds as the TE while relying on a nearly arbitrary kernel function. We introduce modifications of $Q$-Learning and the Bootstrapped Deep $Q$-Network (BDQN) using the TE and the KE, and prove convergence in the tabular setting. Furthermore, we propose an adaptive variant of the TE-based BDQN that
    
[^212]: CTR预测中基于稀疏分组Lasso的神经网络自适应优化器

    Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.14432](http://arxiv.org/abs/2107.14432)

    本论文提出了一种新的框架，在神经网络的CTR预测中加入了稀疏分组Lasso的正则项，并创建了一类新的自适应优化器。实验证明，这些优化器在相同稀疏水平下可以显著提升模型性能，并且能够实现极高的稀疏性。

    

    我们在深度学习中开发了一个新的框架，将稀疏分组Lasso的正则项加入到一系列自适应优化器中，如Momentum、Adagrad、Adam、AMSGrad、AdaHessian等，并创建了一类新的优化器，分别命名为Group Momentum、Group Adagrad、Group Adam、Group AMSGrad和Group AdaHessian等。我们基于原始-对偶方法在随机凸设置下建立了理论上的收敛保证。我们使用最先进的深度学习模型，在三个大规模真实广告点击数据集上评估了我们新优化器的正则效果。实验结果表明，与使用幅度修剪方法的原始优化器相比，模型在相同稀疏水平上的性能可以显著提升。此外，与没有幅度修剪的情况相比，我们的方法可以实现极高的稀疏性，同时具有更好或更高的性能。

    We develop a novel framework that adds the regularizers of the sparse group lasso to a family of adaptive optimizers in deep learning, such as Momentum, Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group AdaHessian, etc., accordingly. We establish theoretically proven convergence guarantees in the stochastic convex settings, based on primal-dual methods. We evaluate the regularized effect of our new optimizers on three large-scale real-world ad click datasets with state-of-the-art deep learning models. The experimental results reveal that compared with the original optimizers with the post-processing procedure which uses the magnitude pruning method, the performance of the models can be significantly improved on the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, our methods can achieve extremely high sparsity with significantly better or hig
    
[^213]: 量化方法的比较评估

    A Comparative Evaluation of Quantification Methods. (arXiv:2103.03223v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03223](http://arxiv.org/abs/2103.03223)

    本研究通过对24种不同量化方法在超过40个数据集上进行全面实证比较，填补了量化方法比较研究的空白。我们发现在二分类设置中，基于阈值选择的Median Sweep和TSMax方法、DyS框架和弗里德曼的方法表现最佳；而在多分类设置中，Generaliz方法表现良好。

    

    量化是指在数据集中预测类别分布的问题。它也代表着一个在监督式机器学习中不断发展的研究领域，近年来提出了大量不同的算法。然而，目前还没有一份全面的实证比较量化方法的研究，以支持算法选择。在本研究中，我们通过对超过40个数据集进行了24种不同量化方法的彻底实证性性能比较，包括二分类和多分类量化设置，填补了这一研究空白。我们观察到没有单一算法能够在所有竞争对手中始终表现最佳，但我们确定了一组在二分类设置中表现最佳的方法，包括基于阈值选择的Median Sweep和TSMax方法、DyS框架和弗里德曼的方法。对于多分类设置，我们观察到另一组算法表现良好，包括Generaliz方法。

    Quantification represents the problem of predicting class distributions in a dataset. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on overall more than 40 data sets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the threshold selection-based Median Sweep and TSMax methods, the DyS framework, and Friedman's method that performs best in the binary setting. For the multiclass setting, we observe that a different group of algorithms yields good performance, including the Generaliz
    
[^214]: 基于欧几里德范数诱导的Schatten-p准范则正则化在低秩张量补全和张量鲁棒主成分分析中的应用

    Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis. (arXiv:2012.03436v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.03436](http://arxiv.org/abs/2012.03436)

    本文提出了一种新的张量秩正则化方法，通过计算张量的CP分量向量的欧几里德范数，间接最小化了Schatten-p准范，用于低秩张量补全和张量鲁棒主成分分析。该方法在处理大型张量时具有可扩展性，并且提供了比核范数更精确的秩代理。同时，通过比较理论分析，证明了该方法在LRTC的泛化能力上的优势。

    

    核范数和Schatten-p准范是低秩矩阵恢复中常用的秩代理。然而，在理论和实践中，计算张量的核范数或Schatten-p准范都很困难，阻碍了它们在低秩张量补全(LRTC)和张量鲁棒主成分分析(TRPCA)中的应用。本文提出了一种基于张量的CP分量向量的欧几里德范数的新类张量秩正则化器，并且证明了这些正则化器是张量Schatten-p准范的单调变换。这种连接使得我们能够通过分量向量隐式地最小化LRTC和TRPCA中的Schatten-p准范。该方法适用于大型张量，并且与核范数相比，在低秩张量恢复中提供了任意更尖锐的秩代理。另一方面，我们研究了具有Schatten-p准范正则化器和该提议正则化器的LRTC的泛化能力。定理表明

    The nuclear norm and Schatten-$p$ quasi-norm are popular rank proxies in low-rank matrix recovery. However, computing the nuclear norm or Schatten-$p$ quasi-norm of a tensor is hard in both theory and practice, hindering their application to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose a new class of tensor rank regularizers based on the Euclidean norms of the CP component vectors of a tensor and show that these regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm. This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the nuclear norm. On the other hand, we study the generalization abilities of LRTC with the Schatten-$p$ quasi-norm regularizer and LRTC with the proposed regularizers. The theorems show that
    
[^215]: 功能性轮廓建模和可解释形状变化检测：结合Fréchet均值与形状不变模型的方法

    Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}. (arXiv:2010.02968v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2010.02968](http://arxiv.org/abs/2010.02968)

    该论文提出了一种结合Fréchet均值和形状不变模型的方法，用于检测功能性轮廓中的形状变化，并构建了功能性数据的控制图，可解释性强且能识别潜在变化。

    

    提出了一种适用于检测功能性轮廓中形状变化的建模框架，结合了Fréchet均值概念和变形模型的概念。利用Fréchet均值提供的广义均值感知能够捕捉研究对象轮廓的典型模式，而变形模型的概念，特别是形状不变模型，允许对轮廓与典型形状之间的偏差进行可解释的参数化。构建和提出了与数据的功能性特性和所采用的变形模型相兼容的EWMA类型控制图，利用研究对象的轮廓在广义均值感知下的某些形状特征，实现对形状和/或变形过程潜在变化的识别。进一步将形状变形过程的潜在变化区分为与幅度和/或相位相关的显著变化。

    A modelling framework suitable for detecting shape shifts in functional profiles combining the notion of Fr\'echet mean and the concept of deformation models is developed and proposed. The generalized mean sense offerred by the Fr\'echet mean notion is employed to capture the typical pattern of the profiles under study, while the concept of deformation models, and in particular of the shape invariant model, allows for interpretable parameterizations of profile's deviations from the typical shape. EWMA-type control charts compatible with the functional nature of data and the employed deformation model are built and proposed, exploiting certain shape characteristics of the profiles under study with respect to the generalised mean sense, allowing for the identification of potential shifts concerning the shape and/or the deformation process. Potential shifts in the shape deformation process, are further distingu\-ished to significant shifts with respect to amplitude and/or the phase of the
    
[^216]: 在拟合和学习中采用健壮性和最优性

    Adopting Robustness and Optimality in Fitting and Learning. (arXiv:1510.03826v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1510.03826](http://arxiv.org/abs/1510.03826)

    该论文提出了一种通过优化准最小函数，将健壮-最优指数推至负无穷来实现对异常值的鲁棒性的方法，并通过在Hessian矩阵中扩展凸性区域来保证最优性。通过实验结果验证了该方法的有效性。

    

    我们通过将健壮-最优（RO）指数λ推至负无穷，以通过优化准最小函数实现对异常值的鲁棒性，从而推广了修改的指数化估计器。通过RO指数自适应地实现和控制鲁棒性，而无需预定义的阈值。通过在Hessian矩阵中扩展凸性区域，以避免局部最优解，保证最优性。对鲁棒性和最优性的详细定量分析被提供。在三个有噪非凸函数和MNIST数据集上的拟合任务以及数字识别任务上进行的实验结果证实了结论。

    We generalized a modified exponentialized estimator by pushing the robust-optimal (RO) index $\lambda$ to $-\infty$ for achieving robustness to outliers by optimizing a quasi-Minimin function. The robustness is realized and controlled adaptively by the RO index without any predefined threshold. Optimality is guaranteed by expansion of the convexity region in the Hessian matrix to largely avoid local optima. Detailed quantitative analysis on both robustness and optimality are provided. The results of proposed experiments on fitting tasks for three noisy non-convex functions and the digits recognition task on the MNIST dataset consolidate the conclusions.
    

