# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Analyzing the Roles of Language and Vision in Learning from Limited Data](https://arxiv.org/abs/2403.19669) | 研究人工智能中的复杂视觉-语言模型，发现即使缺乏视觉输入，利用所有组件的语言模型能够恢复大部分VLM的性能，表明语言通过提供对先前知识和推理的访问来对学习新任务有贡献 |
| [^2] | [Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic](https://arxiv.org/abs/2403.11925) | 通过多级Actor-Critic框架和多级蒙特卡罗梯度估计器，本研究成功解决了平均奖励MDPs全局收敛中对混合时间预测的依赖性，展现出最严格的依赖关系。 |
| [^3] | [From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?](https://arxiv.org/abs/2403.11894) | 该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。 |
| [^4] | [CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability](https://arxiv.org/abs/2403.07632) | 提出了CardioGenAI，一个基于机器学习的框架，用于减少药物的hERG活性并保留药理活性。 |
| [^5] | [HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2403.04207) | 该论文研究了在联邦学习中由于设备硬件和软件配置的不同导致的系统诱导数据异质性，证明了这种异质性对模型的性能产生负面影响，并提出了名为HeteroSwitch的解决方案。 |
| [^6] | [RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval](https://arxiv.org/abs/2402.18510) | 本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。 |
| [^7] | [An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey](https://arxiv.org/abs/2402.17045) | 分析了过去10年针对不同类型网络攻击检测的各种最先进机器学习模型，着重比较了最新的工作。 |
| [^8] | [Information-Theoretic Safe Bayesian Optimization](https://arxiv.org/abs/2402.15347) | 提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。 |
| [^9] | [PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem](https://arxiv.org/abs/2402.10450) | 将时间动作抽象视为序列压缩问题，使用Primitive Sequence Encoding (PRISE)方法结合连续动作量化与BPE来学习强大的动作抽象，并在多任务模仿学习和少样本模仿学习中取得显著性能提升 |
| [^10] | [Nearest Neighbor Representations of Neural Circuits](https://arxiv.org/abs/2402.08751) | 该论文介绍了一种受大脑结构启发的新的计算方法——最近邻表示法，通过建立与神经网络的对应关系和明确的构造，可以表示深度为2的阈值电路等不同函数。 |
| [^11] | [Nearest Neighbor Representations of Neurons](https://arxiv.org/abs/2402.08748) | 这个论文研究了最近邻（NN）表示用于表示神经元的复杂性，并证明了著名的阈值函数可以用多项式规模的锚点和对数分辨率实现。 |
| [^12] | [Riemann-Lebesgue Forest for Regression](https://arxiv.org/abs/2402.04550) | 提出了一种新颖的集成方法Riemann-Lebesgue Forest (RLF)用于回归问题，通过划分函数的值域为多个区间来逼近可测函数的思想，开发了一种新的树学习算法Riemann-Lebesgue Tree。通过Hoeffding分解和Stein方法推导了RLF在不同参数设置下的渐近性能，并在仿真数据和真实世界数据集上的实验中证明了RLF与原始随机森林相比具有竞争力的性能。 |
| [^13] | [Partially Stochastic Infinitely Deep Bayesian Neural Networks](https://arxiv.org/abs/2402.03495) | 本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。 |
| [^14] | [Taylor Videos for Action Recognition](https://arxiv.org/abs/2402.03019) | Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。 |
| [^15] | [Procedural Fairness Through Decoupling Objectionable Data Generating Components](https://arxiv.org/abs/2311.14688) | 通过解耦可抗议的数据生成组件，本研究提出了一个框架来防止伪装的程序不公平，并强调了满足程序公平要求的重要性 |
| [^16] | [Compressing Sign Information in DCT-based Image Coding via Deep Sign Retrieval](https://arxiv.org/abs/2209.10712) | 该论文提出了一种名为“符号检索”的方法来高效压缩离散余弦变换（DCT）系数的符号信息。通过排除符号信息并在解码器中补充，该方法在符号位数和计算成本方面优于以前的方法。 |
| [^17] | [Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition.](http://arxiv.org/abs/2401.15108) | 本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。 |
| [^18] | [Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality.](http://arxiv.org/abs/2401.09556) | 本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。 |
| [^19] | [Efficient Reinforcemen Learning with Decoupling Exploration and Utilization.](http://arxiv.org/abs/2312.15965) | 本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。 |
| [^20] | [Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects.](http://arxiv.org/abs/2310.15234) | 本文提出了一种使用星系目录进行场级别的基于模拟的推断方法，能够鲁棒地推断出宇宙学参数，解决了观测受到的系统效应的影响。 |
| [^21] | [Neurosymbolic Grounding for Compositional World Models.](http://arxiv.org/abs/2310.12690) | 本论文介绍了一种名为Cosmos的框架，用于对象为中心的世界建模，通过使用神经符号化基础和视觉-语言基础模型，实现了在未见过的输入场景上的高性能组合泛化能力。 |
| [^22] | [From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers.](http://arxiv.org/abs/2310.11984) | 本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。 |
| [^23] | [Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN.](http://arxiv.org/abs/2310.09163) | JEI-DNN是一种联合学习退出和推断的动态神经网络架构，通过允许模型在中间层进行部分预测，解决了大型预训练模型每次推断所需大量资源的问题。 |
| [^24] | [Faithfulness Measurable Masked Language Models.](http://arxiv.org/abs/2310.07819) | 本论文提出了一种可度量忠实性的掩码语言模型，通过使用一种新颖的微调方法，将屏蔽令牌作为设计使其成为分布内，以解决解释自然语言处理模型时常见的问题。 |
| [^25] | [Invariant Learning via Probability of Sufficient and Necessary Causes.](http://arxiv.org/abs/2309.12559) | 本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。 |
| [^26] | [Preserving Tumor Volumes for Unsupervised Medical Image Registration.](http://arxiv.org/abs/2309.10153) | 该论文提出了一种保持肿瘤体积的无监督医学图像配准方法，通过两个阶段的过程来解决肿瘤区域体积变化失衡的问题。 |
| [^27] | [The Regular Expression Inference Challenge.](http://arxiv.org/abs/2308.07899) | 正则表达式推理挑战是一个以找到最小正则表达式为目标的任务，具有应用广泛、难度可调、适用于代码/语言建模和机器学习的特点。 |
| [^28] | [Calibration in Deep Learning: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2308.01222) | 本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。 |
| [^29] | [DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2308.01140) | 本研究提出了一种余弦相似度依赖的温度调整函数，用于改进自监督对比学习中的性能，并优化了样本在特征空间中的分布。 |
| [^30] | [Flexible and efficient spatial extremes emulation via variational autoencoders.](http://arxiv.org/abs/2307.08079) | 本文提出了一种新的空间极端值模型，通过集成在变分自动编码器的结构中，可以灵活、高效地模拟具有非平稳相关性的极端事件。实验证明，在时间效率和性能上，相对于传统的贝叶斯推断和许多具有平稳相关性的空间极端值模型，我们的方法具有优势。 |
| [^31] | [DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models.](http://arxiv.org/abs/2306.04642) | 本文提出了一种针对生成式扩散模型的版权保护数字水印方法DiffusionShield，保护图像免受侵权，简单易学，难以检测，可广泛应用于各行各业。 |
| [^32] | [Phylo2Vec: a vector representation for binary trees.](http://arxiv.org/abs/2304.12693) | Phylo2Vec是一种新的二叉树简明表示方法，它能够轻松采样二叉树，并以系统性的方法遍历树空间。这种方法用于构建深度神经网络，能够显著提高蛋白质类别预测的性能。 |
| [^33] | [Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable.](http://arxiv.org/abs/2304.01910) | 神经网络训练的运行变化在实际中更少遇到问题，我们提出了一种简化的统计假设并证明方差主要由于训练过程对初始条件的高敏感性所导致。 |
| [^34] | [Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning.](http://arxiv.org/abs/2303.15579) | 本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。 |

# 详细

[^1]: 分析语言和视觉在从有限数据中学习中的作用

    Analyzing the Roles of Language and Vision in Learning from Limited Data

    [https://arxiv.org/abs/2403.19669](https://arxiv.org/abs/2403.19669)

    研究人工智能中的复杂视觉-语言模型，发现即使缺乏视觉输入，利用所有组件的语言模型能够恢复大部分VLM的性能，表明语言通过提供对先前知识和推理的访问来对学习新任务有贡献

    

    arXiv:2403.19669v1 公告类型：交叉摘要：语言是否有助于理解视觉世界？实际观察世界需要看到实际情况，而不是用文字描述吗？关于智能本质的这些基本问题很难回答，因为我们只有一个智能系统的例子——人类——以及有限的独立语言或视觉的案例。然而，人工智能研究人员开发出复杂的视觉-语言模型（VLMs）为我们提供了新的机会，探索语言和视觉对于学习世界的贡献。我们从这些模型的认知架构中切除组件，以确定它们对从有限数据中学习新任务的贡献。我们发现，利用所有组件的语言模型恢复了大部分VLM的性能，尽管它缺乏视觉输入，而语言似乎可以通过提供对先前知识和推理的访问来实现这一点。

    arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
    
[^2]: 在平均奖励强化学习中实现全局最优性而无需混合时间预测：基于多级Actor-Critic方法

    Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic

    [https://arxiv.org/abs/2403.11925](https://arxiv.org/abs/2403.11925)

    通过多级Actor-Critic框架和多级蒙特卡罗梯度估计器，本研究成功解决了平均奖励MDPs全局收敛中对混合时间预测的依赖性，展现出最严格的依赖关系。

    

    在平均奖励强化学习的背景下，对于混合时间的预测的oracle知识要求，即度量马尔可夫链在固定策略下达到其稳态分布所需的时间，对于策略梯度方法的全球收敛构成了重大挑战。为了解决这一限制，我们考虑了多级Actor-Critic（MAC）框架，该框架结合了多级蒙特卡洛（MLMC）梯度估计器。通过我们的方法，实现了对混合时间知识的依赖性的有效减轻，这是平均奖励MDPs全局收敛的首次尝试。此外，我们的方法展现出最严格的$\mathcal{O}$依赖关系。

    arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
    
[^3]: 从可解释到可解释的深度学习在医疗自然语言处理中的应用：现实有多远？

    From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?

    [https://arxiv.org/abs/2403.11894](https://arxiv.org/abs/2403.11894)

    该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。

    

    深度学习（DL）通过解决各种自然语言处理（NLP）任务，极大地增强了医疗保健研究。然而，基于DL的NLP方法日益复杂，需要透明的模型解释性，或至少是可解释性，以进行可靠的决策制定。本文对医疗健康NLP中的可解释和可解释的DL进行了彻底的范围审查。引入了术语“XIAI”（eXplainable和Interpretable Artificial Intelligence）以区分XAI和IAI。方法根据其功能（模型、输入、输出为基础）和范围（局部、全局）进一步分类。我们的分析表明，注意机制是最主要的新兴IAI。此外，IAI越来越多地用于对抗XAI。确定的主要挑战是大多数XIAI不探索“全局”建模过程，缺乏最佳实践，并且需要系统评估和基准测试。

    arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
    
[^4]: CardioGenAI：基于机器学习的框架用于减少hERG毒性的药物再设计

    CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability

    [https://arxiv.org/abs/2403.07632](https://arxiv.org/abs/2403.07632)

    提出了CardioGenAI，一个基于机器学习的框架，用于减少药物的hERG活性并保留药理活性。

    

    药物诱导的心脏毒性是一个重要的健康问题，可能导致严重不良反应，包括通过阻滞电压门控的hERG钾离子通道导致生命威胁的心律失常。因此，开发早期阶段鉴定hERG活性化合物的先进方法，以及优化商业化药物以减少hERG活性非常重要。在这项工作中，我们提出了CardioGenAI，这是一个基于机器学习的框架，用于再设计开发中和已上市药物，以减少hERG活性同时保留其药理活性。该框架结合了用于预测hERG通道活性的最新判别模型，以及钠离子通道NaV1.5和钙离子通道CaV1.2活性，因为它们在调节由hERG通道阻滞引起的心律失常潜在影响中具有潜在意义。这些模型还可以se

    arXiv:2403.07632v1 Announce Type: new  Abstract: Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also se
    
[^5]: HeteroSwitch：对联邦学习中系统诱导数据异质性的表征与调控

    HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning

    [https://arxiv.org/abs/2403.04207](https://arxiv.org/abs/2403.04207)

    该论文研究了在联邦学习中由于设备硬件和软件配置的不同导致的系统诱导数据异质性，证明了这种异质性对模型的性能产生负面影响，并提出了名为HeteroSwitch的解决方案。

    

    联邦学习（FL）是一种实用的方法，可以跨用户设备协作训练深度学习模型，通过在设备上保留原始数据来保护用户隐私。在FL中，参与的用户设备在硬件和软件配置方面高度碎片化。这种碎片化引入了一种新型数据异质性，即\textit{系统诱导的数据异质性}，因为每个设备根据其硬件和软件配置生成不同的数据。在本文中，我们首先表征了系统诱导数据异质性对FL模型性能的影响。我们使用具有跨供应商和性能层级变化的异构设备收集了数据集。通过使用这个数据集，我们证明了\textit{系统诱导的数据异质性}对准确性产生负面影响，并恶化了FL中的公平性和域泛化问题。为了解决这些挑战，我们提出了HeteroSwitch，

    arXiv:2403.04207v1 Announce Type: new  Abstract: Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, 
    
[^6]: RNNs还不是Transformer：在上下文检索中的关键瓶颈

    RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval

    [https://arxiv.org/abs/2402.18510](https://arxiv.org/abs/2402.18510)

    本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。

    

    本文探讨循环神经网络（RNNs）和Transformer在解决算法问题时的表示能力差距。我们重点关注RNNs是否能在处理长序列时，通过Chain-of-Thought (CoT)提示，与Transformer的性能相匹配。我们的理论分析显示CoT可以改进RNNs，但无法弥补与Transformer之间的差距。关键瓶颈在于RNNs无法完全从上下文中检索信息，即使经过CoT的增强：对于几个明确或隐式需要这种能力的任务，如联想召回和确定图是否为树，我们证明RNNs表达能力不足以解决这些任务，而Transformer可以轻松解决。相反，我们证明采用增强RNNs上下文检索能力的技术，包括

    arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
    
[^7]: 对各类网络攻击检测的最先进机器学习方法性能的研究：一项调查

    An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey

    [https://arxiv.org/abs/2402.17045](https://arxiv.org/abs/2402.17045)

    分析了过去10年针对不同类型网络攻击检测的各种最先进机器学习模型，着重比较了最新的工作。

    

    为了保护计算机和信息系统免受攻击者利用系统中的漏洞进行网络犯罪的侵害，已经提出了几种用于实时检测漏洞以提高信息系统安全性的方法。在所有提出的方法中，机器学习是实现系统安全的效果最好的方法，其能力范围从早期检测软件漏洞到实时检测系统中正在进行的妥协。由于存在不同类型的网络攻击，每种现有最先进的机器学习模型都依赖于不同的训练算法，这也影响了它们对特定类型网络攻击的适用性。在这项研究中，我们分析了过去10年针对不同类型网络攻击检测的每一个当前最先进的机器学习模型，重点放在最近的工作上以进行比较。

    arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
    
[^8]: 信息论安全贝叶斯优化

    Information-Theoretic Safe Bayesian Optimization

    [https://arxiv.org/abs/2402.15347](https://arxiv.org/abs/2402.15347)

    提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。

    

    我们考虑了一个顺序决策任务，其目标是在不评估违反先验未知（安全）约束的参数的情况下优化未知函数。一个常见的方法是在未知函数上放置高斯过程先验，并且仅允许在高概率安全区域内进行评估。大多数当前方法依赖于对域的离散化，并且不能直接扩展到连续情况。此外，它们利用约束的规则假设的方式引入了一个额外的关键超参数。在本文中，我们提出了一个信息论安全探索准则，该准则直接利用GP后验来识别最具信息的安全参数进行评估。将这一探索准则与众所周知的贝叶斯优化收益函数结合起来，产生了一种新颖的安全贝叶斯优化选择准则。

    arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
    
[^9]: PRISE：将时间动作抽象视为序列压缩问题

    PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem

    [https://arxiv.org/abs/2402.10450](https://arxiv.org/abs/2402.10450)

    将时间动作抽象视为序列压缩问题，使用Primitive Sequence Encoding (PRISE)方法结合连续动作量化与BPE来学习强大的动作抽象，并在多任务模仿学习和少样本模仿学习中取得显著性能提升

    

    时间动作抽象以及信念状态表示是序贯决策中的强大知识共享机制。本文提出了一个新颖的观点，将诱导时间动作抽象视为序列压缩问题。为此，我们将LLM训练流水线的一个微妙但至关重要的组成部分 -- 输入标记化通过字节对编码（BPE） -- 带到了连续控制领域中学习可变时间跨度技能的 seemingly distant 任务。我们引入一种称为Primitive Sequence Encoding（PRISE）的方法，该方法将连续动作量化与BPE相结合，学习强大的动作抽象。我们通过实验证明，PRISE从一组机器人操作演示中发现的高级技能显著提升了多任务模仿学习以及在未见任务上的少样本模仿学习的性能。我们的代码将在 https: 放出

    arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/
    
[^10]: 神经电路的最近邻表示法

    Nearest Neighbor Representations of Neural Circuits

    [https://arxiv.org/abs/2402.08751](https://arxiv.org/abs/2402.08751)

    该论文介绍了一种受大脑结构启发的新的计算方法——最近邻表示法，通过建立与神经网络的对应关系和明确的构造，可以表示深度为2的阈值电路等不同函数。

    

    神经网络成功地捕捉到了人脑在许多任务中的计算能力。受到大脑结构的启发，最近邻（NN）表示法是一种新的计算方法。我们在NN表示法与神经网络之间建立了更牢固的对应关系。虽然已知如何使用NN表示法表示单个神经元，但对于小规模深度神经网络尚无结果。具体地，针对深度为2的阈值电路，我们提供了其NN表示的明确构造，并给出了表示所需位数的明确界限。示例函数包括凸多面体的NN表示（阈值门的AND）、IP2、阈值门的OR以及线性或精确决策列表。

    arXiv:2402.08751v1 Announce Type: cross Abstract: Neural networks successfully capture the computational power of the human brain for many tasks. Similarly inspired by the brain architecture, Nearest Neighbor (NN) representations is a novel approach of computation. We establish a firmer correspondence between NN representations and neural networks. Although it was known how to represent a single neuron using NN representations, there were no results even for small depth neural networks. Specifically, for depth-2 threshold circuits, we provide explicit constructions for their NN representation with an explicit bound on the number of bits to represent it. Example functions include NN representations of convex polytopes (AND of threshold gates), IP2, OR of threshold gates, and linear or exact decision lists.
    
[^11]: 神经元的最近邻表示

    Nearest Neighbor Representations of Neurons

    [https://arxiv.org/abs/2402.08748](https://arxiv.org/abs/2402.08748)

    这个论文研究了最近邻（NN）表示用于表示神经元的复杂性，并证明了著名的阈值函数可以用多项式规模的锚点和对数分辨率实现。

    

    最近邻（NN）表示是一种受到大脑启发的新兴计算模型。我们研究了使用NN表示来表示神经元（阈值函数）的复杂性。已知，两个锚点（NN计算的点）足以表示阈值函数的NN表示，然而分辨率（锚点条目所需的最大比特数）是$O(n\log{n})$。在这项研究中，我们调查了锚点数和阈值函数的NN表示的分辨率之间的权衡。我们证明了著名的阈值函数EQUALITY，COMPARISON和ODD-MAX-BIT可以通过$n$和$O(\log{n})$的分辨率以多项式规模的锚点表示，这些函数需要2或3个锚点。

    arXiv:2402.08748v1 Announce Type: cross Abstract: The Nearest Neighbor (NN) Representation is an emerging computational model that is inspired by the brain. We study the complexity of representing a neuron (threshold function) using the NN representations. It is known that two anchors (the points to which NN is computed) are sufficient for a NN representation of a threshold function, however, the resolution (the maximum number of bits required for the entries of an anchor) is $O(n\log{n})$. In this work, the trade-off between the number of anchors and the resolution of a NN representation of threshold functions is investigated. We prove that the well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which require 2 or 3 anchors and resolution of $O(n)$, can be represented by polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We conjecture that for all threshold functions, there are NN representations with polynomially large size and logarithmic reso
    
[^12]: Riemann-Lebesgue Forest回归方法的研究

    Riemann-Lebesgue Forest for Regression

    [https://arxiv.org/abs/2402.04550](https://arxiv.org/abs/2402.04550)

    提出了一种新颖的集成方法Riemann-Lebesgue Forest (RLF)用于回归问题，通过划分函数的值域为多个区间来逼近可测函数的思想，开发了一种新的树学习算法Riemann-Lebesgue Tree。通过Hoeffding分解和Stein方法推导了RLF在不同参数设置下的渐近性能，并在仿真数据和真实世界数据集上的实验中证明了RLF与原始随机森林相比具有竞争力的性能。

    

    我们提出了一种新颖的用于回归问题的集成方法，称为Riemann-Lebesgue Forest (RLF)。RLF的核心思想是通过将函数的值域划分为几个区间来模拟可测函数的逼近方式。基于这个思想，我们开发了一种新的树学习算法，称为Riemann-Lebesgue Tree，它在每个非叶节点上有机会从响应Y或特征空间X中的方向进行切割。我们通过Hoeffding分解和Stein方法来推导不同参数设置下RLF的渐近性能。当底层函数Y=f(X)遵循加法回归模型时，RLF与Scornet等人的论证（2014年）保持一致。通过在仿真数据和真实世界数据集上的实验证明，RLF与原始随机森林相比具有竞争力的性能。

    We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.
    
[^13]: 部分随机的无限深度贝叶斯神经网络

    Partially Stochastic Infinitely Deep Bayesian Neural Networks

    [https://arxiv.org/abs/2402.03495](https://arxiv.org/abs/2402.03495)

    本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。

    

    在本文中，我们提出了一种部分随机的无限深度贝叶斯神经网络，这是一种将部分随机性整合到无限深度神经网络框架中的新型架构。我们的新型架构旨在改善现有架构在训练和推理时间上的计算效率限制。为实现这一目标，我们利用了部分随机性在无限深度极限下的优势，包括全随机性的好处，如鲁棒性、不确定性量化和内存效率，同时改善了它们在训练和推理时间上的计算效率限制。我们提出了多种架构配置，提供了网络设计的灵活性，包括不同的权重划分方法。我们还通过确立我们的网络家族符合通用条件分布近似器的数学保证，对我们的模型的表达能力进行了证明。

    In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
    
[^14]: 用于动作识别的Taylor视频

    Taylor Videos for Action Recognition

    [https://arxiv.org/abs/2402.03019](https://arxiv.org/abs/2402.03019)

    Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。

    

    从视频中有效地提取动作是动作识别中一个重要且长期存在的问题。这个问题非常具有挑战性，因为动作(i)没有明确的形式，(ii)拥有诸如位移、速度和加速度等各种概念，(iii)通常会受到不稳定像素引起的噪声的干扰。为了解决这些挑战，我们提出了Taylor视频，一种新的视频格式，它突出显示了每个帧中的主要动作（例如挥手）被称为Taylor帧。Taylor视频的命名来源于Taylor级数，它使用重要的项来近似给定点上的函数。在视频的情境中，我们定义了一个隐含的动作提取函数，旨在从视频时间块中提取动作。在这个块中，我们使用帧、差分帧和高阶差分帧进行Taylor展开，以近似计算起始帧上的这个函数。我们展示了Taylor级数中高阶项的求和给我们提供了...

    Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
    
[^15]: 通过解耦可抗议的数据生成组件来实现程序公平

    Procedural Fairness Through Decoupling Objectionable Data Generating Components

    [https://arxiv.org/abs/2311.14688](https://arxiv.org/abs/2311.14688)

    通过解耦可抗议的数据生成组件，本研究提出了一个框架来防止伪装的程序不公平，并强调了满足程序公平要求的重要性

    

    我们揭示并解决了经常被忽视但重要的问题，即伪装的程序不公平，即对数据生成过程中的中立（即不成问题的）方面的可能无意的改变，和/或对最不利利益个体的实现没有程序保证。受约翰·罗尔斯对纯程序公正的倡导启发，我们将自动决策视为社会制度的缩影，并考虑数据生成过程本身如何满足程序公平的要求。我们提出了一个框架，通过利用参考点和相关的价值实例化规则，将可抗议的数据生成组件与中立的数据生成组件解耦。我们的发现强调了防止伪装的程序不公平的必要性，不仅引起了我们力图缓解的可抗议的数据生成组件的注意

    arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
    
[^16]: 通过深度符号检索在基于DCT的图像编码中压缩符号信息

    Compressing Sign Information in DCT-based Image Coding via Deep Sign Retrieval

    [https://arxiv.org/abs/2209.10712](https://arxiv.org/abs/2209.10712)

    该论文提出了一种名为“符号检索”的方法来高效压缩离散余弦变换（DCT）系数的符号信息。通过排除符号信息并在解码器中补充，该方法在符号位数和计算成本方面优于以前的方法。

    

    由于符号的等概率特性，压缩离散余弦变换（DCT）系数的符号信息是图像编码方案中一个棘手的问题。为了克服这个困难，我们提出了一种名为“符号检索”的符号信息高效压缩方法。该方法受到相位恢复的启发，相位恢复是从其幅度中找到离散傅里叶变换系数的相位信息的经典信号恢复问题。在编码器中，所有DCT系数的符号信息被排除在比特流之外，并且通过我们的符号检索方法在解码器中补充。通过实验证明，我们的方法在符号位数和计算成本方面优于先前的方法。我们使用Python语言实现的方法可以在https://github.com/ctsutake/dsr获得。

    Compressing the sign information of discrete cosine transform (DCT) coefficients is an intractable problem in image coding schemes due to the equiprobable characteristics of the signs. To overcome this difficulty, we propose an efficient compression method for the sign information called "sign retrieval." This method is inspired by phase retrieval, which is a classical signal restoration problem of finding the phase information of discrete Fourier transform coefficients from their magnitudes. The sign information of all DCT coefficients is excluded from a bitstream at the encoder and is complemented at the decoder through our sign retrieval method. We show through experiments that our method outperforms previous ones in terms of the bit amount for the signs and computation cost. Our method, implemented in Python language, is available from https://github.com/ctsutake/dsr.
    
[^17]: 多智能体深度强化学习在竞争中为快速充电电动车中心的动态定价中的应用

    Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])

    [http://arxiv.org/abs/2401.15108](http://arxiv.org/abs/2401.15108)

    本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。

    

    快速充电站将成为全球新建交通电气化基础设施的一部分。这些充电站将承载许多直流快速充电设备，仅可供电动车辆充电使用。类似于汽油加油站，同一地区的快速充电站将根据竞争调整价格以吸引同一群电动车主。这些充电站将与电力网络进行交互，通过预测性购买在前一天电力市场上的电力需求，并在实时市场上满足差额需求。充电站可能配备补充电池储能系统用于套利。本文针对充电站竞争中开发了一个两步数据驱动的动态定价方法。首先通过求解随机的前一天电力需求模型得到纳入承诺，然后通过将游戏建模为竞争来得到充电站的价格策略。

    Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
    
[^18]: 深度学习增强的混合整数优化: 学习减少模型维度

    Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])

    [http://arxiv.org/abs/2401.09556](http://arxiv.org/abs/2401.09556)

    本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。

    

    本研究介绍了一种利用深度学习解决混合整数规划模型中的计算复杂性的框架。我们比较了前馈神经网络(ANN)和卷积神经网络(CNN)在近似混合整数规划问题中的作用。我们利用多标签分类来考虑多个活动维度。为了提高框架的性能，我们采用贝叶斯优化进行超参数调优，以最大化样本级准确性。主要目标是训练神经网络准确地预测所有活动维度，从而最大化全局最优解的出现频率。我们将该框架应用于描述个性化医学供应链中的长期投资规划和中期战术规划的基于流的设施位置分配混合整数线性规划(MILP)问题。

    This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
    
[^19]: 有效的强化学习方法：探索与利用分离

    Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15965](http://arxiv.org/abs/2312.15965)

    本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。

    

    当前的离线强化学习技术过于保守，对现有数据集的处理限制了深度神经网络(DNN)的泛化能力。这种方法常常导致算法只适应某个特定数据集的次优解。同样，在在线强化学习中，之前的惩罚性悲观主义也剥夺了模型的探索潜力。我们的研究提出了一种新的框架，乐观与悲观演员强化学习（OPARL）。OPARL采用独特的双演员方法：乐观演员专注于探索，悲观演员专注于利用，从而有效区分探索和利用策略。这种组合在强化学习方法中促进了更平衡和高效的方法。它通过悲观的利用策略优化着重于产生高奖励动作的策略。

    Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi
    
[^20]: 使用星系目录进行场级别的基于模拟的推断：系统效应的影响

    Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects. (arXiv:2310.15234v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.15234](http://arxiv.org/abs/2310.15234)

    本文提出了一种使用星系目录进行场级别的基于模拟的推断方法，能够鲁棒地推断出宇宙学参数，解决了观测受到的系统效应的影响。

    

    最近的研究表明，一种从星系红移调查中限制宇宙学参数的有效方法是训练图神经网络进行场级别的无似然推断，而不对尺度进行剪切。具体而言，德桑蒂等人（2023年）开发了能够准确推断出通过仅包含星系位置和径向速度的目录来确定$\Omega_{\rm m}$值的模型，这些模型具有对天体物理和亚网格模型的不确定性具有鲁棒性。然而，观测受到许多效应的影响，包括1）掩蔽效应，2）特异速度和径向距离的不确定性，以及3）不同的星系选择。此外，观测只允许我们测量红移，纠缠了星系的径向位置和速度。在本文中，我们使用来自CAMELS项目中不同代码运行的最新水动力学模拟生成的星系目录来训练和测试我们的模型，这些模拟考虑了这些观测效应。

    It has been recently shown that a powerful way to constrain cosmological parameters from galaxy redshift surveys is to train graph neural networks to perform field-level likelihood-free inference without imposing cuts on scale. In particular, de Santi et al. (2023) developed models that could accurately infer the value of $\Omega_{\rm m}$ from catalogs that only contain the positions and radial velocities of galaxies that are robust to uncertainties in astrophysics and subgrid models. However, observations are affected by many effects, including 1) masking, 2) uncertainties in peculiar velocities and radial distances, and 3) different galaxy selections. Moreover, observations only allow us to measure redshift, intertwining galaxies' radial positions and velocities. In this paper we train and test our models on galaxy catalogs, created from thousands of state-of-the-art hydrodynamic simulations run with different codes from the CAMELS project, that incorporate these observational effect
    
[^21]: 神经符号化基础上的组合式世界建模

    Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])

    [http://arxiv.org/abs/2310.12690](http://arxiv.org/abs/2310.12690)

    本论文介绍了一种名为Cosmos的框架，用于对象为中心的世界建模，通过使用神经符号化基础和视觉-语言基础模型，实现了在未见过的输入场景上的高性能组合泛化能力。

    

    我们引入了Cosmos，一个针对组合泛化（CG）设计的以对象为中心的世界建模框架，即在通过已知的视觉“原子”组合获得的未见过的输入场景上具有高性能。Cosmos的核心洞察力是使用一种新颖的神经符号化基础。具体来说，该框架引入了两个新工具：（i）神经符号化场景编码，使用神经编码器计算每个场景中的实体的实向量表示，并使用描述实体属性的可组合符号向量，以及（ii）神经符号化注意机制，将这些实体与学习到的交互规则绑定起来。Cosmos是端到端可微分的；此外，与传统的神经符号化方法需要手动将表示映射为符号不同，它使用视觉-语言基础模型计算实体的符号属性。通过对已建立的blocks场景进行两种不同形式的CG评估，我们验证了Cosmos的有效性。

    We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
    
[^22]: 从插值到外推：算术Transformer的完整长度泛化

    From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])

    [http://arxiv.org/abs/2310.11984](http://arxiv.org/abs/2310.11984)

    本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。

    

    自从提出以来，Transformer模型在各种任务中展现出了优秀的性能。然而，在算法任务中，长度泛化仍存在一些未解决的问题。在本文中，我们研究了Transformer模型在学习算术算法（如加法和乘法）方面的内在能力。通过实验证明和注意力分析，我们确定了实现最佳长度泛化的几个关键因素。我们展示了Transformer模型能够通过目标指向偏置来泛化到长长度。然后，我们引入了Attention Bias Calibration（ABC），这是一个校准阶段，使模型能够自动学习适当的注意力偏置，我们将其与相对位置编码的机制联系起来。我们证明使用ABC，Transformer模型可以在某些算术任务上实现前所未有的完美长度泛化。

    Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
    
[^23]: 动态神经网络的联合学习退出和推断：JEI-DNN

    Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN. (arXiv:2310.09163v1 [cs.LG])

    [http://arxiv.org/abs/2310.09163](http://arxiv.org/abs/2310.09163)

    JEI-DNN是一种联合学习退出和推断的动态神经网络架构，通过允许模型在中间层进行部分预测，解决了大型预训练模型每次推断所需大量资源的问题。

    

    大型预训练模型结合微调已成为主导的机器学习架构。尽管这些模型表现出令人印象深刻的性能，但它们的实际应用通常受到每次推断所需的巨大资源的限制。早期退出动态神经网络（EDNN）通过允许模型从中间层进行部分预测（即早期退出）来绕过这个问题。训练EDNN架构具有挑战性，因为它包括两个相互交织的组件：控制早期退出决策的门控机制（GM）和执行中间表示推断的中间推断模块（IMs）。因此，大多数现有方法依赖于门控机制的阈值置信度度量，并努力改进基本的骨干网络和推断模块。尽管取得了成功，但这种方法有两个基本缺点：1）门控机制和中间推断模块不能共同学习和优化，2）GM在一定程度上依赖于IMS和骨干网络的质量，导致模型性能上的损失。

    Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs 
    
[^24]: 可度量忠实性的掩码语言模型

    Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])

    [http://arxiv.org/abs/2310.07819](http://arxiv.org/abs/2310.07819)

    本论文提出了一种可度量忠实性的掩码语言模型，通过使用一种新颖的微调方法，将屏蔽令牌作为设计使其成为分布内，以解决解释自然语言处理模型时常见的问题。

    

    解释自然语言处理模型的常见方法是使用重要性度量来表达哪些令牌对于预测很重要。然而，尽管这些解释具有说服力，但往往是错误的。因此，测量它们的忠实性至关重要。其中一种度量标准是如果令牌确实很重要，那么屏蔽它们应该导致模型性能变差。然而，令牌屏蔽会引入区域外问题，而现有的解决方案在计算上很昂贵并且使用代理模型。此外，其他指标的适用范围非常有限。在这项工作中，我们提出了一种固有的忠实性可度量模型来应对这些挑战。通过使用一种新颖的微调方法来实现这一目标，该方法将屏蔽令牌作为设计使其成为分布内。这与现有方法不同，现有方法完全与模型无关，但在实践中不适用。我们通过将其应用于各种任务和数据集来证明我们方法的普适性。

    A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and val
    
[^25]: 通过充分因素和必要因素的概率进行不变学习

    Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])

    [http://arxiv.org/abs/2309.12559](http://arxiv.org/abs/2309.12559)

    本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。

    

    在野外学习中，对于未知的、与训练分布不同的测试分布，外部分布（OOD）泛化是不可或缺的。最近从因果性引发的方法在实现OOD泛化方面显示出了巨大的潜力。然而，现有方法主要关注因果性的不变性属性，而在很大程度上忽视了充分性和必要性条件的属性。换句话说，一个必要但不充分的原因（特征）对于分布转换是不变的，但可能没有所需的准确度。相反，一个充分但不必要的原因（特征）倾向于很好地适应特定数据，但可能存在适应新领域的风险。为了捕捉充分和必要因素的信息，我们采用了经典概念——充分和必要因素的概率（PNS），它指示了一个因素是必要和充分原因的概率。为了将PNS与OOD泛化联系起来，我们提出了一种方法

    Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
    
[^26]: 保持肿瘤体积的无监督医学图像配准

    Preserving Tumor Volumes for Unsupervised Medical Image Registration. (arXiv:2309.10153v1 [eess.IV])

    [http://arxiv.org/abs/2309.10153](http://arxiv.org/abs/2309.10153)

    该论文提出了一种保持肿瘤体积的无监督医学图像配准方法，通过两个阶段的过程来解决肿瘤区域体积变化失衡的问题。

    

    医学图像配准是估计图像间空间对应关系的关键任务。然而，目前传统和基于深度学习的方法都依赖相似度度量来生成变形场，这往往导致不相似区域的体积变化失衡，特别是在肿瘤区域。这些改变会显著改变肿瘤大小和底层解剖结构，限制了图像配准在临床诊断中的实际应用。为了解决这个问题，我们将以肿瘤为约束条件的图像配准问题。我们提出的策略包括两个阶段的过程。在第一个阶段，我们使用基于相似度的配准来通过体积变化识别潜在的肿瘤区域，生成相应的软肿瘤掩膜。在第二阶段，我们提出一种保持体积的配准方法，采用一种新颖的自适应体积保持损失函数。

    Medical image registration is a critical task that estimates the spatial correspondence between pairs of images. However, current traditional and deep-learning-based methods rely on similarity measures to generate a deforming field, which often results in disproportionate volume changes in dissimilar regions, especially in tumor regions. These changes can significantly alter the tumor size and underlying anatomy, which limits the practical use of image registration in clinical diagnosis. To address this issue, we have formulated image registration with tumors as a constraint problem that preserves tumor volumes while maximizing image similarity in other normal regions. Our proposed strategy involves a two-stage process. In the first stage, we use similarity-based registration to identify potential tumor regions by their volume change, generating a soft tumor mask accordingly. In the second stage, we propose a volume-preserving registration with a novel adaptive volume-preserving loss t
    
[^27]: 正则表达式推理挑战

    The Regular Expression Inference Challenge. (arXiv:2308.07899v1 [cs.LG])

    [http://arxiv.org/abs/2308.07899](http://arxiv.org/abs/2308.07899)

    正则表达式推理挑战是一个以找到最小正则表达式为目标的任务，具有应用广泛、难度可调、适用于代码/语言建模和机器学习的特点。

    

    我们提出将正则表达式推理（REI）作为代码/语言建模以及更广泛的机器学习社区的挑战。REI是一个有监督的机器学习和程序合成任务，它提出了从示例中找到最小正则表达式的问题：给定两个有限字符串集合P和N以及一个成本函数cost(·)，任务是生成一个接受P中所有字符串并拒绝N中所有字符串的表达式r，而不存在其他表达式r'，使得cost(r')<cost(r)。REI作为一个挑战问题具有以下优势：（i）正则表达式是众所周知、广泛使用的，是代码的自然理想化；（ii）REI的渐近最坏情况复杂性已被充分理解；（iii）REI具有一小部分易于理解的参数（例如P或N的基数、示例的字符串长度或成本函数），这使得我们可以轻松调整REI的难度；（iv）对于基于深度学习的M模型而言，REI是一个未解决的问题。

    We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')<\text{cost}(r)$.  REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based M
    
[^28]: 深度学习中的校准：最新研究综述

    Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])

    [http://arxiv.org/abs/2308.01222](http://arxiv.org/abs/2308.01222)

    本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。

    

    在构建可靠、鲁棒的安全关键应用的人工智能系统中，深度神经模型的校准起着重要作用。最近的研究表明，具有高预测能力的现代神经网络的校准性较差，产生不可靠的模型预测。尽管深度学习模型在各种基准测试中取得了显著的性能，但对模型的校准性和可靠性的研究相对较少。理想的深度模型不仅应具有高预测性能，还应具有良好的校准性。最近提出了一些使用不同机制进行深度模型校准的方法。在本综述中，我们回顾了最新的校准方法，并解释了它们执行模型校准的原理。首先，我们从模型校准的定义开始，解释了模型校准不准确的根本原因。然后，我们介绍了可以衡量模型校准性的关键指标。接下来，我们总结了一些校准方法的方法和实践。

    Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
    
[^29]: DySTreSS: 自监督对比学习中的动态温度调整

    DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])

    [http://arxiv.org/abs/2308.01140](http://arxiv.org/abs/2308.01140)

    本研究提出了一种余弦相似度依赖的温度调整函数，用于改进自监督对比学习中的性能，并优化了样本在特征空间中的分布。

    

    在当代的自监督对比算法中，诸如SimCLR、MoCo等，平衡两个语义相似样本之间的吸引力和两个不同类别样本之间的排斥力的任务主要受到硬负样本的影响。虽然已经证明InfoNCE损失可以根据困难程度施加惩罚，但温度超参数是调节惩罚和均匀性与容忍度之间权衡的关键。在这项工作中，我们着眼于改进SSL中InfoNCE损失的性能，研究温度超参数值的影响。我们提出了一种余弦相似度依赖的温度调整函数，以有效优化特征空间中样本的分布。我们进一步分析均匀性和容忍度度量，以研究余弦相似度空间中更好优化的最佳区域。此外，我们还对局部和全局行为进行了全面的研究。

    In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and g
    
[^30]: 通过变分自动 编码器实现灵活高效的空间极端值模拟

    Flexible and efficient spatial extremes emulation via variational autoencoders. (arXiv:2307.08079v1 [stat.ML])

    [http://arxiv.org/abs/2307.08079](http://arxiv.org/abs/2307.08079)

    本文提出了一种新的空间极端值模型，通过集成在变分自动编码器的结构中，可以灵活、高效地模拟具有非平稳相关性的极端事件。实验证明，在时间效率和性能上，相对于传统的贝叶斯推断和许多具有平稳相关性的空间极端值模型，我们的方法具有优势。

    

    许多现实世界的过程具有复杂的尾依赖结构，这种结构无法使用传统的高斯过程来描述。更灵活的空间极端值模型， 如高斯尺度混合模型和单站点调节模型，具有吸引人的极端依赖性质，但往往难以拟合和模拟。本文中，我们提出了一种新的空间极端值模型，具有灵活和非平稳的相关性属性，并将其集成到变分自动编码器 (extVAE) 的编码-解码结构中。 extVAE 可以作为一个时空模拟器，对潜在的机制模型输出状态的分布进行建模，并产生具有与输入相同属性的输出，尤其是在尾部区域。通过广泛的模拟研究，我们证明我们的extVAE比传统的贝叶斯推断更高效，并且在具有 平稳相关性结构的许多空间极端值模型中表现 更好。

    Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence str
    
[^31]: DiffusionShield：一种针对生成式扩散模型的版权保护数字水印方法

    DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])

    [http://arxiv.org/abs/2306.04642](http://arxiv.org/abs/2306.04642)

    本文提出了一种针对生成式扩散模型的版权保护数字水印方法DiffusionShield，保护图像免受侵权，简单易学，难以检测，可广泛应用于各行各业。

    

    近来，生成式扩散模型(GDMs)在学习和生成图像方面展示了其卓越的能力。大量的GDMs社区自然而然地出现，并促进了GDMs在各个领域的多样化应用。然而，这种无限制的扩散引起了有关版权保护的严重关注。例如，艺术家(包括画家和摄影师)越来越担心GDMs可以毫不费力地复制他们独特的创意作品而无需授权。为了应对这些挑战，我们介绍了一种针对GDMs量身定制的新型水印方案——DiffusionShield。通过将所有权信息编码成一个不可察觉的水印并将其注入图像中，DiffusionShield保护图像免受GDMs侵权。它的水印可以被GDMs轻松地学习并重现在它们生成的图像中。通过从生成的图像中检测水印，可以揭露版权侵权行为。

    Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
    
[^32]: Phylo2Vec: 一种二叉树的向量表示方法

    Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])

    [http://arxiv.org/abs/2304.12693](http://arxiv.org/abs/2304.12693)

    Phylo2Vec是一种新的二叉树简明表示方法，它能够轻松采样二叉树，并以系统性的方法遍历树空间。这种方法用于构建深度神经网络，能够显著提高蛋白质类别预测的性能。

    

    从生物数据推断得到的二叉进化树对于理解生物之间共享的进化历史至关重要。根据最大似然等某个最优性准则推断出树中潜在节点的位置是NP-hard问题，这推动了大量启发式方法的发展。然而，这些启发式方法通常缺乏一种系统性的方法来均匀采样随机树或有效地探索指数级增长的树空间，这对于机器学习等优化问题至关重要。因此，我们提出了Phylo2Vec，这是一种新的简明表示方法来表示进化树。Phylo2Vec将任何具有n个叶子的二叉树映射到长度为n的整数向量。我们证明了Phylo2Vec在空间中既是良定的又是双射的。Phylo2Vec的优点是：i）轻松均匀采样二叉树；ii）以非常大或小的步长系统地遍历树空间。作为概念验证，我们使用Phylo2Vec构建了一个深度神经网络，以从氨基酸序列预测蛋白质类别。我们证明了Phylo2Vec显著提高了网络的性能，超过了之前的最优结果。

    Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
    
[^33]: 校准混乱：神经网络训练的运行变化在无意中且无害

    Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable. (arXiv:2304.01910v1 [cs.LG])

    [http://arxiv.org/abs/2304.01910](http://arxiv.org/abs/2304.01910)

    神经网络训练的运行变化在实际中更少遇到问题，我们提出了一种简化的统计假设并证明方差主要由于训练过程对初始条件的高敏感性所导致。

    

    典型的神经网络训练在重复测试时会有显著的测试集性能差异，影响模型超参数的比较和训练的可重复性。本文通过以下比较来解释这种变化：（1）尽管在测试集上有显著的方差，但标准的CIFAR-10与ImageNet训练在测试分布上的表现却非常一致，这表明方差不像之前想象的那么严重。（2）我们提出了一种简化的统计假设，以紧密近似测试集准确性分布结构。（3）我们认为，在以下两个意义上，测试集方差是不可避免的。首先，我们展示了方差主要是由于训练过程对初始条件的高敏感性而不是特定的随机源（如数据排序和扩充）所导致的。其次，我们证明了方差是频率极限的，但可以通过训练多个模型来减少。

    Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which those test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We argue that test-set variance is inevitable in the following two senses. First, we show that variance is largely caused by high sensitivity of the training process to initial conditions, rather than by specific sources of randomness like the data order and augmentations. Second, we prove that variance is u
    
[^34]: 统计学习中的调整Wasserstein分布鲁棒估计

    Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])

    [http://arxiv.org/abs/2303.15579](http://arxiv.org/abs/2303.15579)

    本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。

    

    我们在统计学习中提出了一种调整的Wasserstein分布鲁棒估计——基于Wasserstein分布鲁棒估计（WDRO）的非线性转换。这种转换将提高WDRO的统计性能，因为调整后的WDRO估计器渐进无偏并且均方误差趋近于零。调整后的WDRO不会削弱WDRO的样本外性能保证。我们提出了调整WDRO估计器的存在的充分条件，并给出了计算调整WDRO估计器的过程。具体而言，我们将展示如何在广义线性模型中开发调整WDRO估计器。数值实验表明，调整后的估计器比经典估计器具有更好的实际性能。

    We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
    

