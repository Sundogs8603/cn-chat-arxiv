# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Convergence Analysis of Flow Matching in Latent Space with Transformers](https://arxiv.org/abs/2404.02538) | 该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。 |
| [^2] | [Block-Diagonal Guided DBSCAN Clustering](https://arxiv.org/abs/2404.01341) | 该研究提出了一种改进版本的DBSCAN，利用相似性图的块对角属性引导聚类过程，通过构建块对角图并进行聚类排序，易于确定聚类结构。 |
| [^3] | [Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems](https://arxiv.org/abs/2404.01224) | 提出了一种协同帕累托集学习(CoPSL)框架，可以同时学习多个多目标优化问题的帕累托集，通过共享和特定层的结构，实现了不同MOP之间的协同学习。 |
| [^4] | [Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences](https://arxiv.org/abs/2403.19871) | 通过混合整数优化算法，以保持一致的分析洞见为重点，在重新训练机器学习模型中实现比贪婪训练更强稳定性，同时在模型性能上有小幅、可控的牺牲。 |
| [^5] | [Sample complexity of quantum hypothesis testing](https://arxiv.org/abs/2403.17868) | 本文研究了量子假设检验的样本复杂度，得出了对称和非对称设置中的二进制量子假设检验的样本复杂度与反错误概率的对数和保真度的负对数的关系。 |
| [^6] | [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations](https://arxiv.org/abs/2403.17124) | 这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。 |
| [^7] | [An incremental MaxSAT-based model to learn balanced rules](https://arxiv.org/abs/2403.16418) | 提出了一种基于递增式MaxSAT的学习平衡规则模型IMLIB，结合了SAT和MaxSAT方法，限制规则大小以实现平衡，并提高模型性能。 |
| [^8] | [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638) | 提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。 |
| [^9] | [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608) | 大型模型参数高效微调（PEFT）是通过调整预训练模型的参数，以适应特定任务，并减少引入的附加参数或计算资源数量的实用解决方案。 |
| [^10] | [DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics](https://arxiv.org/abs/2403.14353) | 该论文提出了一种在自主系统中加速视频分析的持续学习方法，通过利用轻量级“学生”模型进行部署推理，利用更大的“教师”模型进行数据标记，实现对不断变化场景的持续自适应。 |
| [^11] | [Tackling Noisy Labels with Network Parameter Additive Decomposition](https://arxiv.org/abs/2403.13241) | 本论文提出了一种使用网络参数附加分解来解耦干净数据和错误标记数据的记忆，从而减少嘈杂标签对深度网络训练的副作用。 |
| [^12] | [Is Mamba Effective for Time Series Forecasting?](https://arxiv.org/abs/2403.11144) | Mamba模型作为一种状态空间模型在时间序列预测中具有捕捉复杂依赖关系、近线性复杂度以及性能优势的潜力。 |
| [^13] | [Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization](https://arxiv.org/abs/2403.10063) | 本文提出了统一的无投影Frank-Wolfe类型算法，用于对抗性DR-次模优化，在不同场景下取得了令人瞩目的次线性 $\alpha$-后悔上界，并在单调设置中实现了无投影算法的最新次线性 $\alpha$-后悔上界。 |
| [^14] | [CHAI: Clustered Head Attention for Efficient LLM Inference](https://arxiv.org/abs/2403.08058) | CHAI提出了Clustered Head Attention（CHAI）方法，通过在运行时结合具有高相关性的注意力头部，实现了减少内存需求和计算量，能够在不需要微调的情况下将存储K,V缓存的内存需求降低21.4％，推理时间延迟降低1.73倍。 |
| [^15] | [Inverse-Free Fast Natural Gradient Descent Method for Deep Learning](https://arxiv.org/abs/2403.03473) | 本文提出一种快速自然梯度下降（FNGD）方法，在深度学习中仅需要在第一个时代计算逆运算，避免了迭代求逆操作。 |
| [^16] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^17] | [Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations](https://arxiv.org/abs/2403.02090) | 提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。 |
| [^18] | [Improving LLM Code Generation with Grammar Augmentation](https://arxiv.org/abs/2403.01632) | SynCode是一个新框架，结合编程语言的语法和DFA mask store，在LLMs中生成代码过程中获得96.07%的句法错误降低，并展现出提高句法精度的重大影响。 |
| [^19] | [Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures](https://arxiv.org/abs/2402.17992) | 本研究提出了一种物理启发的机器学习方法，将科学原理和物理定律融入深度神经网络，用于建模非线性结构的地震响应。 |
| [^20] | [InstructEdit: Instruction-based Knowledge Editing for Large Language Models](https://arxiv.org/abs/2402.16123) | InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。 |
| [^21] | [EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems](https://arxiv.org/abs/2402.15164) | EasyRL4Rec是一个面向基于强化学习的推荐系统的用户友好和高效库，提供了多样化的RL环境、全面的核心模块、一致的评估标准和定制解决方案，旨在帮助简化模型开发并改善长期用户参与度。 |
| [^22] | [Stick to your Role! Stability of Personal Values Expressed in Large Language Models](https://arxiv.org/abs/2402.14846) | 本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。 |
| [^23] | [Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics](https://arxiv.org/abs/2402.11515) | 该研究专注于优化深度强化学习在流体力学中主动流控制中的并行设置，通过拆解DRL框架、进行扩展性基准测试、提出混合并行化配置并优化多环境DRL训练中的I/O操作，提出了有效的并行化策略。 |
| [^24] | [Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection](https://arxiv.org/abs/2402.10862) | 该论文介绍了一种差分私有联邦迁移学习框架，结合差分隐私和迁移学习，以提升心理健康监测中的数据隐私性和数据充足性。 |
| [^25] | [Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals](https://arxiv.org/abs/2402.09474) | 本研究使用视觉变压器方法解读心率信号，提高心脏疾病检测模型的解释性和可靠性。 |
| [^26] | [AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction](https://arxiv.org/abs/2402.08698) | 本研究提出了一种模块化的模型无关的轨迹预测框架，使用专家混合来解决长尾效应问题，提高对于包含挑战性场景的数据的预测性能。 |
| [^27] | [RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks](https://arxiv.org/abs/2402.08367) | 本论文介绍了RBF-PINN方法，在物理信息神经网络中应用非Fourier位置嵌入，解决了基于Fourier特征映射的局限性，并在各种正向和反向问题中取得了有效结果。 |
| [^28] | [ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design](https://arxiv.org/abs/2402.03479) | 本研究探索了深度强化学习代理的零样本迁移能力，并发现通过根据值损失优先选择级别，可以改善代理的推广能力。此外，无监督环境设计方法对改善代理表现也具有重要作用。 |
| [^29] | [Preference-free Alignment Learning with Regularized Relevance Reward](https://arxiv.org/abs/2402.03469) | 无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。 |
| [^30] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^31] | [Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing](https://arxiv.org/abs/2402.02985) | 本论文提出了一种无监督的路况解析框架，利用视觉语言模型和基本计算机视觉模型来解决无人机高分辨率影像下的路况场景解析问题。该框架首先利用视觉语言模型快速检测路况感兴趣区域，然后利用视觉基础模型生成路况区域掩模，然后采用自监督表示学习网络提取特征表示，最后通过无监督聚类算法对特征进行聚类和标记。 |
| [^32] | [Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating](https://arxiv.org/abs/2402.02678) | 本研究提出了一种新的可解释人工智能框架，通过使用反事实概率和额外的因果结构先验信息，克服了因果图未知的问题，可以解释黑盒机器学习模型并应用于信贷评级等领域。 |
| [^33] | [Explaining Text Classifiers with Counterfactual Representations](https://arxiv.org/abs/2402.00711) | 本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。 |
| [^34] | [Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data](https://arxiv.org/abs/2402.00205) | 本文提出了一种用于多医院数据的分散、协作和保护隐私的机器学习方法（DeCaPH），它可以允许不同方在不共享私有数据集的情况下协作训练机器学习模型，并通过限制数据泄露和隐私侵犯来保护患者隐私。 |
| [^35] | [Simple Policy Optimization](https://arxiv.org/abs/2401.16025) | SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。 |
| [^36] | [The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise](https://arxiv.org/abs/2401.07844) | 本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。 |
| [^37] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^38] | [Adaptive Interventions with User-Defined Goals for Health Behavior Change](https://arxiv.org/abs/2311.09483) | 该论文介绍了一种修改过的Thompson抽样算法，强调通过优化个性化奖励函数实现个性化目标设定，为支持目标设定提供了一个平衡方法，并证明此修改仅对累积遗憾产生恒定的惩罚。 |
| [^39] | [Riemannian Laplace Approximation with the Fisher Metric](https://arxiv.org/abs/2311.02766) | 黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。 |
| [^40] | [Enhancing Group Fairness in Online Settings Using Oblique Decision Forests](https://arxiv.org/abs/2310.11401) | 提出了Aranyani，一种斜裁集成的方法，用于解决在在线环境中优化群体公平性目标所面临的挑战 |
| [^41] | [Learning to Detect Slip through Tactile Estimation of the Contact Force Field and its Entropy](https://arxiv.org/abs/2303.00935) | 通过光学触觉传感器结合物理数据驱动方法，实时连续检测滑动，从滑动事件中提取不均匀特征解决滑动检测问题。 |
| [^42] | [Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints](https://arxiv.org/abs/2212.04672) | 提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。 |
| [^43] | [OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering.](http://arxiv.org/abs/2401.16719) | 本研究提出了一种名为OptiState的腿式机器人状态估计方法，该方法通过整合Kalman滤波、优化和学习模式的混合解决方案，结合本体感和外感信息，以精确估计机器人主体的状态。该方法利用关节编码器、IMU测量和基于凸规划的模型预测控制优化，通过Gate循环单元和Vision Transformer自编码器改进了估计结果。研究结果表明，该方法能够提供准确的机器人状态估计，并减小传感器测量和模型简化引起的非线性误差。 |
| [^44] | [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning.](http://arxiv.org/abs/2401.10862) | 本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。 |
| [^45] | [Adjustable Molecular Representation for Unified Pre-training Strategy.](http://arxiv.org/abs/2401.06166) | AdaMR是一种可调整粒度的分子模型，它在原子和亚结构水平上学习分子表示。通过预训练和分子规范化任务，AdaMR可以改善对多个下游任务的效果，包括模型属性预测和分子生成。 |
| [^46] | [L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages.](http://arxiv.org/abs/2401.02254) | L3Cube-IndicNews是一个面向印度语系的多语种文本分类数据集，包括短标题、长文档和长段落三个数据集。它提供了10种印度语言的新闻文章，每个数据集包含10个或更多类别的文章。这个数据集可以用于深入分析和评估。 |
| [^47] | [TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA.](http://arxiv.org/abs/2312.17670) | 这项研究提出了TopCoW挑战，通过发布具有13种血管组分注释的Willis循环（CoW）数据集，并使用虚拟现实（VR）技术进行拓扑感知解剖分割，解决了手动和耗时的CoW表征问题。 |
| [^48] | [Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks.](http://arxiv.org/abs/2312.16020) | 本研究通过在剪枝过程中应用梯度采样技术，实现了在资源有限的场景中保持高准确性水平的鲁棒剪枝。相关实验证明，采用梯度采样技术进行优化的模型比传统优化方法更能有效地保持准确性。这一创新方法在各种数据集和神经架构上得到了验证，并且理论上解释了梯度采样技术对模型鲁棒性的贡献。 |
| [^49] | [Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA.](http://arxiv.org/abs/2311.00696) | 本研究针对家庭保健机构面临的护理员分配问题，提出了一种决策支持框架，通过考虑护理员对访问顺序的灵活性，旨在减少行驶里程、增加访问次数，并保持护理的连续性。 |
| [^50] | [Stochastic Gradient Descent for Gaussian Processes Done Right.](http://arxiv.org/abs/2310.20581) | 本文研究了使用随机梯度下降方法优化高斯过程回归问题，并引入了一种特定的随机对偶梯度下降算法，该方法在标准回归基准和贝叶斯优化任务上表现出很高的竞争力。 |
| [^51] | [Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM.](http://arxiv.org/abs/2310.16588) | 该论文展示了一种基于微环的时间延迟储备计算方案，能够同时解决时间序列预测、分类和无线通道均衡等任务，并且在每个通道上实现了最先进的性能。 |
| [^52] | [Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment.](http://arxiv.org/abs/2310.13193) | 本论文提出了一种基于异构图神经网络的数据驱动交通分配和交通流学习方法，该方法能够准确捕捉不同链路之间的空间交通模式，优于其他传统神经网络模型，在大规模网络中有着广泛的应用潜力。 |
| [^53] | [Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning.](http://arxiv.org/abs/2310.11684) | 本研究探索了无限时域平均奖励强化学习中量子加速的潜力。我们提出了一种创新的量子框架，通过高效的量子均值估计技术，实现了指数级改进的遗憾保证。所提出的量子算法相较于经典算法，在遗憾界限上有显著改进。 |
| [^54] | [ZeroSwap: Data-driven Optimal Market Making in DeFi.](http://arxiv.org/abs/2310.09413) | ZeroSwap 是第一个基于数据驱动算法的 DeFi 市场做市方案，在保持市场做市商零利润的情况下，通过适应交易者行为来解决了流动性提供者遭受套利损失的问题。 |
| [^55] | [Safe Deep Policy Adaptation.](http://arxiv.org/abs/2310.08602) | 该论文提出了SafeDPA，一种新颖的强化学习和控制框架，用于同时解决策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，并使用少量真实数据进行微调。在真实世界部署过程中，通过引入基于控制屏障函数的安全过滤器，确保了SafeDPA的安全性。 |
| [^56] | [Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning.](http://arxiv.org/abs/2310.07902) | "Unraveling the Single Tangent Space Fallacy"论文分析和澄清了在机器人学习中应用黎曼几何的误区，该误区是指将数据仅投影到单一切空间中的方法。 |
| [^57] | [Generative Modeling with Phase Stochastic Bridges.](http://arxiv.org/abs/2310.07805) | 通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。 |
| [^58] | [Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages.](http://arxiv.org/abs/2310.07418) | 本文对视觉强化学习中的可塑性进行了研究，发现数据增强对保持可塑性至关重要，评论者的可塑性损失是高效训练的主要限制因素，并且未及时恢复评论者的可塑性将导致灾难性结果。这为解决高重放比困境提供了新的策略。 |
| [^59] | [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors.](http://arxiv.org/abs/2310.02980) | 本文研究表明使用随机初始化会导致对架构差异的严重高估，而使用标准消噪目标进行预训练可以在多种架构上实现显著的性能提升，并将Transformers与状态空间模型之间的差距缩小到很小。与之前的研究不同的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上的性能与S4相匹配，并且在PathX-256任务上改进了SSMs的最佳结果20个百分点。 |
| [^60] | [Learning Quantum Processes with Quantum Statistical Queries.](http://arxiv.org/abs/2310.02075) | 本文提出了第一个在量子统计查询模型内学习量子过程的框架，并提供了一个高效的学习器和可证明的性能保证。通过在密码分析中的应用，揭示了经典读出量子物理不可克隆函数的脆弱性，这是量子硬件安全领域一个重要的开放问题的解决方法。 |
| [^61] | [ResBit: Residual Bit Vector for Categorical Values.](http://arxiv.org/abs/2309.17196) | 本论文提出了一种名为ResBit的残差位向量方法，用于解决在深度学习中表示离散数据维度增加和无法恢复原始类别值的问题。 |
| [^62] | [Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images.](http://arxiv.org/abs/2309.12245) | 本论文研究了生成对抗网络中的模式塌陷问题对合成X射线图像多样性的影响。通过实验证明了将自适应输入图像归一化方法与深度模型结合的优势。 |
| [^63] | [Single-Image based unsupervised joint segmentation and denoising.](http://arxiv.org/abs/2309.10511) | 本文提出一种基于单图像的无监督方法，实现了联合分割和去噪。该方法不需要大量标记样本，且能够处理高噪声和通用纹理，并在显微镜图像上表现出色。 |
| [^64] | [A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification.](http://arxiv.org/abs/2309.08415) | 本研究提出了一种使用机器学习和不确定性量化建模的多阶段决策过程方法，用于预测心力衰竭患者对心脏再同步治疗的反应。该模型能够推荐收集额外的SPECT MPI变量，以提高预测准确性。 |
| [^65] | [Physics-constrained robust learning of open-form PDEs from limited and noisy data.](http://arxiv.org/abs/2309.07672) | 该论文提出了一种名为R-DISCOVER的框架，能够从有限且有噪声的数据中稳健地揭示开放式偏微分方程。该框架通过符号表示和强化学习引导下的混合PDE生成器，以及神经网络预测模型来实现。实验结果表明该方法能够高效地发现和嵌入PDE，并选择表现最佳的PDE进行系统响应预测。 |
| [^66] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^67] | [Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach.](http://arxiv.org/abs/2309.00848) | 本文提出了一种基于YOLOv8模型和创新的后处理技术的孟加拉文档布局分析方法，通过数据增强和两阶段预测策略实现了准确的元素分割。该方法优于单个基础架构，并解决了BaDLAD数据集中的问题，有助于提高OCR和文档理解能力。 |
| [^68] | [Bayesian Reasoning for Physics Informed Neural Networks.](http://arxiv.org/abs/2308.13222) | 本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。 |
| [^69] | [Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference.](http://arxiv.org/abs/2308.12066) | 预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。 |
| [^70] | [Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System.](http://arxiv.org/abs/2308.07523) | 深层操作符网络（DeepONet）作为一种强大的替代建模方法，在核系统数字孪生技术中展示出了显著的预测精度和计算效率。然而，挑战仍然存在，包括最佳传感器放置和模型评估。 |
| [^71] | [Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers.](http://arxiv.org/abs/2308.04103) | 提出了一种可解释的机器学习方法，基于吸收光谱来加速测量掺杂聚合物材料的电导率。分类模型能够准确分类电导率大于~25至100 S/cm的样品，回归模型能够精确预测高导电性样品的电导率。该方法经过实验证实了效果。 |
| [^72] | [PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload.](http://arxiv.org/abs/2308.01917) | PePNet是一种支持罕见重负载的工作负载预测网络，通过周期性感知机制和融合多尺度序列学习的能力提高了整体特别是重负载的准确性。 |
| [^73] | [A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks.](http://arxiv.org/abs/2307.12114) | 这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。 |
| [^74] | [Latent Space Representations of Neural Algorithmic Reasoners.](http://arxiv.org/abs/2307.08874) | 这项工作对神经算法推理器中执行算法时产生的潜在空间结构进行了详细分析，并提出了解决两种故障模式的方法。通过使用softmax聚合器解决分辨率丧失问题，以及衰减潜在空间来处理超出范围的值，这些改变在标准CLRS-30基准测试中大多数算法上实现了改进。 |
| [^75] | [Synthetic Lagrangian Turbulence by Generative Diffusion Models.](http://arxiv.org/abs/2307.08529) | 该论文提出了一种基于机器学习的方法，通过生成扩散模型在高雷诺数下合成拉格朗日湍流，成功实现了粒子轨迹的统计和拓扑性质的准确重现。 |
| [^76] | [Capafoldable: self-tracking foldable smart textiles with capacitive sensing.](http://arxiv.org/abs/2307.05370) | 这项工作提出了一种创新的自追踪可折叠智能纺织品，通过结合折叠织物结构和电容传感，利用深度学习技术来检测结构运动。实验结果显示，我们的方法可以从电容信号中准确重构出片段的几何形状。 |
| [^77] | [Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks.](http://arxiv.org/abs/2307.04050) | 本文为快递运营中的负载规划问题提出了一种基于优化的学习方法，全面考虑了负载和流程规划的挑战，并开发了一个决策支持工具。研究发现在网络中存在大量的对称性，导致优化求解器返回不同的解决方案，降低了规划人员对优化求解的信任度。 |
| [^78] | [Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization.](http://arxiv.org/abs/2307.03571) | 本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。 |
| [^79] | [Understanding Generalization in the Interpolation Regime using the Rate Function.](http://arxiv.org/abs/2306.10947) | 本文利用大偏差理论，提出一种基于函数的平滑模型特征描述方法，解释了为什么一些插值器有很好的泛化能力以及现代学习技术为什么能够找到它们。 |
| [^80] | [Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks.](http://arxiv.org/abs/2306.08157) | 本文提出了一种基于动态贝叶斯网络的方法，来预测加密货币价格方向，以帮助投资者做出明智的投资决策。 |
| [^81] | [On Achieving Optimal Adversarial Test Error.](http://arxiv.org/abs/2306.07544) | 本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。 |
| [^82] | [PriSampler: Mitigating Property Inference of Diffusion Models.](http://arxiv.org/abs/2306.05208) | 本文是第一项针对扩散模型属性推断攻击的隐私研究，攻击者将从模型中提取训练集的敏感全局属性，结果表明各种扩散模型及其取样器容易受到攻击的影响。 |
| [^83] | [Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing.](http://arxiv.org/abs/2305.18584) | Coeditor开发了一个多轮代码自动编辑模型，利用同一代码库中的最近变化来预测对代码区域的编辑，表现出更高的准确率。 |
| [^84] | [Subsampling Error in Stochastic Gradient Langevin Diffusions.](http://arxiv.org/abs/2305.13882) | 该研究分析了随机梯度 Langevin 动力学在大型数据环境下使用子采样产生的误差。研究者提出了一种新的连续时间马尔可夫过程，该过程切换数据子集并可用于扩散子采样 MCMC 方法，并证明了该方法的收敛性。 |
| [^85] | [AnyPredict: Foundation Model for Tabular Prediction.](http://arxiv.org/abs/2305.12081) | 本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。 |
| [^86] | [Aligned Diffusion Schr\"odinger Bridges.](http://arxiv.org/abs/2302.11419) | 本文提出一种新的算法框架，首次能够在考虑对齐数据的同时解决扩散薛定谔桥问题，相对于之前的方法，有更简单、方差更低的训练过程，并使用原则性的正则化方案，在实验中取得了显着的改进。 |
| [^87] | [DTAAD: Dual Tcn-Attention Networks for Anomaly Detection in Multivariate Time Series Data.](http://arxiv.org/abs/2302.10753) | 这项研究提出了一种基于Transformer和双重TCN-Attention网络的DTAAD模型，用于多变量时间序列数据的异常检测和诊断。通过集成设计和引入缩放方法和反馈机制，该模型实现了快速准确定位异常，并提高了预测精度和扩大了相关差异。 |
| [^88] | [TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation.](http://arxiv.org/abs/2302.09832) | TAMUNA是首个联合利用网络压缩和少量通信配合加速分布式梯度下降算法，并允许部分参与的算法。 |
| [^89] | [Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment.](http://arxiv.org/abs/2302.07491) | 该论文提出了一种自监督的时间图学习方法，通过提取时间和结构信息来学习更具信息量的节点表示。 |
| [^90] | [Enhancing Exploration in Latent Space Bayesian Optimization.](http://arxiv.org/abs/2302.02399) | 本文提出了一种新的方法来提高潜空间贝叶斯优化（LSBO）的探索能力。方法包括潜在一致性感知获取函数（LCA-AF）和增加一致性点的潜空间生成方法（LCA-VAE），将它们结合起来形成了LCA-LSBO。实验证明LCA-LSBO在图像生成和全新的化学设计任务中表现出改进的性能。 |
| [^91] | [Graph-based Time-Series Anomaly Detection: A Survey.](http://arxiv.org/abs/2302.00058) | 本文综述了基于图的时间序列异常检测，主要探讨了图表示学习的潜力和最先进的图异常检测技术在时间序列中的应用。 |
| [^92] | [Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning.](http://arxiv.org/abs/2212.01168) | 通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。 |
| [^93] | [Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks.](http://arxiv.org/abs/2211.11869) | 研究了个性化任务中强化学习智能体的策略熵，并发现策略优化智能体在训练过程中往往具有低熵策略，然而Q学习智能体对此影响较小，通常保持高熵策略。 |
| [^94] | [Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation.](http://arxiv.org/abs/2211.01939) | 本文在二元治疗条件下条件平均处理效应估计的场景中，对因果推断的模型选择问题进行了实证分析，利用最新的生成建模进展，提出了新的度量方法，证明了新的模型选择策略的有效性。 |
| [^95] | [Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models.](http://arxiv.org/abs/2210.03921) | 数据选择是一种令人惊讶的有效且通用的构建小型可解释模型的策略，它通过学习训练分布而非测试分布的数据，提高了传统基准模型的准确性，并在多个任务中展现出竞争力。 |
| [^96] | [On the Convergence of the ELBO to Entropy Sums.](http://arxiv.org/abs/2209.03077) | 本论文研究了 ELBO 收敛到熵和的问题，证明了对于一类广泛的生成模型，ELBO 在所有学习的稳定点处都等于一系列熵的和，为无监督学习的学习算法的基本属性提供了深入的洞察。 |
| [^97] | [Early heart disease prediction using hybrid quantum classification.](http://arxiv.org/abs/2208.08882) | 本文提出了两种量子机器学习方法，分别适用于高维和低维问题，并在Cleveland和Statlog数据集上实验表明这些方法更适合于早期心脏疾病预测，可获得高达96.43％和97.78％的曲线下面积。 |
| [^98] | [Emergent segmentation from participation dynamics and multi-learner retraining.](http://arxiv.org/abs/2206.02667) | 该论文通过研究基于数据驱动服务的参与和重新训练动态，发现当学习者和用户子群具有风险减少性质时，唯一的稳定均衡是细分的，将子群分配给单个学习者。功利主义社会最优是一个稳定均衡。 |
| [^99] | [Restructuring Graph for Higher Homophily via Adaptive Spectral Clustering.](http://arxiv.org/abs/2206.02386) | 本文提出一种新颖的图重构方法，集成于任何类型的图神经网络中，以增加同质性。方法包括自适应谱聚类、密度感知同质性度量方法和基于聚类结果的邻接矩阵重构。 |

# 详细

[^1]: 流匹配在潜空间中的收敛性分析与Transformer

    Convergence Analysis of Flow Matching in Latent Space with Transformers

    [https://arxiv.org/abs/2404.02538](https://arxiv.org/abs/2404.02538)

    该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。

    

    我们提出了ODE-based生成模型，特别是流匹配的理论收敛性保证。我们使用预训练的自编码器网络将高维原始输入映射到低维潜空间，其中一个Transformer网络被训练来预测从标准正态分布到目标潜空间分布的变换速度场。我们的误差分析展示了这种方法的有效性，表明通过估计的ODE流生成样本的分布在温斯坦-2距离下收敛到目标分布，这在温和且实际的假设下成立。此外，我们展示了具有利普希茨连续性的Transformer网络可以有效地逼近任意光滑函数，这可能是独立感兴趣的。

    arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.
    
[^2]: 块对角引导的DBSCAN聚类

    Block-Diagonal Guided DBSCAN Clustering

    [https://arxiv.org/abs/2404.01341](https://arxiv.org/abs/2404.01341)

    该研究提出了一种改进版本的DBSCAN，利用相似性图的块对角属性引导聚类过程，通过构建块对角图并进行聚类排序，易于确定聚类结构。

    

    集群分析在数据库挖掘中起着至关重要的作用，而在这一领域中最广泛使用的算法之一是DBSCAN。然而，DBSCAN存在一些局限性，例如难以处理高维大规模数据、对输入参数敏感以及在产生聚类结果时缺乏稳健性。本文引入了一种改进版本的DBSCAN，利用了相似性图的块对角属性来引导DBSCAN的聚类过程。其关键思想是构建一个图，衡量高维大规模数据点之间的相似性，并有可能通过未知置换转换为块对角形式，随后通过一个聚类排序过程来生成期望的置换。聚类结构可以通过识别置换后图中的对角块来轻松确定。我们提出了一种基于梯度下降的方法来解决这个问题。

    arXiv:2404.01341v1 Announce Type: cross  Abstract: Cluster analysis plays a crucial role in database mining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing clustering results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity graph to guide the clustering procedure of DBSCAN. The key idea is to construct a graph that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The clustering structure can be easily determined by identifying the diagonal blocks in the permuted graph. We propose a gradient descent-based method to solve the propose
    
[^3]: 多目标优化问题中的协同帕累托集学习

    Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems

    [https://arxiv.org/abs/2404.01224](https://arxiv.org/abs/2404.01224)

    提出了一种协同帕累托集学习(CoPSL)框架，可以同时学习多个多目标优化问题的帕累托集，通过共享和特定层的结构，实现了不同MOP之间的协同学习。

    

    帕累托集学习(PSL)是多目标优化中一个新兴研究领域，专注于训练神经网络学习从偏好向量到帕累托最优解的映射。然而，现有的PSL方法仅限于一次解决单个多目标优化问题(MOP)。面对多个MOP时，这种限制不仅导致显著的低效，而且未能利用横跨不同MOP的潜在协同效应。本文提出了一种协同帕累托集学习(CoPSL)框架，它以协同方式同时学习多个MOP的帕累托集。CoPSL采用了一个架构，包括共享和MOP特定层，其中共享层旨在协同捕捉MOP之间的公共关系，而MOP特定层处理这些关系以生成每个MOP的解集。这种协同方法使得CoPSL能够高效地...

    arXiv:2404.01224v1 Announce Type: new  Abstract: Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to effi
    
[^4]: 通过缓慢变化的序列实现稳定的机器学习模型重新训练

    Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences

    [https://arxiv.org/abs/2403.19871](https://arxiv.org/abs/2403.19871)

    通过混合整数优化算法，以保持一致的分析洞见为重点，在重新训练机器学习模型中实现比贪婪训练更强稳定性，同时在模型性能上有小幅、可控的牺牲。

    

    重新训练机器学习模型仍然是实际机器学习模型部署的重要任务。现有方法主要关注贪婪方法，以找到表现最佳的模型，而不考虑通过不同的重新训练演变来保持训练模型结构的稳定性。在这项研究中，我们开发了一种混合整数优化算法，全面考虑了通过不同的数据批次更新重新训练机器学习模型的问题。我们的方法侧重于保留一致的分析洞见 - 这对于模型可解释性、实施简易性和与用户建立信任至关重要 - 通过使用可以直接纳入优化问题的自定义定义的距离度量。重要的是，我们的方法在真实的生产案例研究中表现出比贪婪训练模型更强的稳定性，同时在模型性能上有小幅、可控的牺牲。

    arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
    
[^5]: 量子假设检验的样本复杂度

    Sample complexity of quantum hypothesis testing

    [https://arxiv.org/abs/2403.17868](https://arxiv.org/abs/2403.17868)

    本文研究了量子假设检验的样本复杂度，得出了对称和非对称设置中的二进制量子假设检验的样本复杂度与反错误概率的对数和保真度的负对数的关系。

    

    传统上，人们从信息论的角度研究量子假设检验，在这种情况下，人们对错误概率的最优衰减速率感兴趣，这个速率是未知状态的样本数量函数。本文研究了量子假设检验的样本复杂度，旨在确定达到所需错误概率所需的最少样本数量。通过利用已有文献中关于量子假设检验的丰富知识，我们表征了对称和非对称设置中的二进制量子假设检验的样本复杂度，并提供了多个量子假设检验的样本复杂度的界限。更详细地说，我们证明了对称二进制量子假设检验的样本复杂度对反错误概率的对数和保真度的负对数的对数。

    arXiv:2403.17868v1 Announce Type: cross  Abstract: Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. 
    
[^6]: 将语言计划基于演示通过反事实干扰进行落实

    Grounding Language Plans in Demonstrations Through Counterfactual Perturbations

    [https://arxiv.org/abs/2403.17124](https://arxiv.org/abs/2403.17124)

    这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。

    

    将大型语言模型的常识推理基于物理领域落实在体现智能的人工智能中仍然是一个至关重要但尚未解决的问题。相较于先前的工作专注于直接利用LLMs在符号空间内规划，这项工作使用LLMs指导任务结构的搜索，隐含在多步演示中的约束。具体而言，我们借鉴了操纵规划文献中的模式族的概念，它按照特定运动约束将机器人配置分组，作为LLM高级语言表示和机器人低级物理轨迹之间的抽象层。通过用合成干扰重新播放少量人类演示，我们可以覆盖演示的状态空间，并额外生成成功执行以及未完成任务的反事实情况。我们的基于解释的学习框架训练了一个端到端可微分神经网络。

    arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
    
[^7]: 一种基于递增式MaxSAT的学习平衡规则模型

    An incremental MaxSAT-based model to learn balanced rules

    [https://arxiv.org/abs/2403.16418](https://arxiv.org/abs/2403.16418)

    提出了一种基于递增式MaxSAT的学习平衡规则模型IMLIB，结合了SAT和MaxSAT方法，限制规则大小以实现平衡，并提高模型性能。

    

    机器学习领域的不断发展导致了众多应用程序的开发，能够有效地解决各种问题并进行准确预测。然而，在某些情况下，仅准确性可能不足够。许多实际问题还需要解释和可解释性。本文旨在提出一种基于MaxSAT的增量模型用于学习可解释且平衡的规则，称为IMLIB。这个新模型基于另外两种方法，一种是基于SAT的，另一种是基于MaxSAT的。基于SAT的方法限制了每个生成规则的大小，使得可以平衡它们。我们认为这样一组规则比一个混合了大规则和小规则更容易理解。基于MaxSAT的方法，称为IMLI，提出了一种提高性能的技术。

    arXiv:2403.16418v1 Announce Type: cross  Abstract: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance th
    
[^8]: 大型语言模型的差分私有下一个标记预测

    Differentially Private Next-Token Prediction of Large Language Models

    [https://arxiv.org/abs/2403.15638](https://arxiv.org/abs/2403.15638)

    提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。

    

    确保大型语言模型（LLMs）的隐私日益重要。DP-SGD是实现这一目标的最广泛采用的技术，它以一种保证差分隐私的方式训练模型。然而，DP-SGD需要比SGD更长的训练时间和更大的内存需求，同时过高估计对手具有白盒访问模型的能力。更现实的场景假设只有对隐私敏感的LLM进行黑盒访问。在这些观察的基础上，我们提出了私有混合集合分布（PMixED）：一种通过将模型的每个输出分布从一个经过精细调整的LLM集合投影到公共LLM输出分布周围的集合上，然后对投影分布进行平均并从中抽样来实现实际的下一个标记预测的私有预测协议。我们的方法比DP-SGD更轻量化，因为它与模型无关。

    arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
    
[^9]: 大型模型的参数高效微调：一项全面调研

    Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey

    [https://arxiv.org/abs/2403.14608](https://arxiv.org/abs/2403.14608)

    大型模型参数高效微调（PEFT）是通过调整预训练模型的参数，以适应特定任务，并减少引入的附加参数或计算资源数量的实用解决方案。

    

    大型模型在多个应用领域代表了一项突破性的进展，使得在各种任务中取得了显著成就。然而，它们空前的规模带来了巨大的计算成本。这些模型通常由数十亿个参数组成，需要大量的计算资源来执行。特别是，在为特定下游任务定制大型模型时，尤其是在受到计算能力限制的硬件平台上，规模庞大和计算要求巨大构成了重大挑战。参数高效微调（PEFT）提供了一个实用解决方案，可以有效地调整大型模型以适应各种下游任务。具体而言，PEFT是指调整预训练大型模型的参数，使其适应特定任务的过程，同时尽量减少引入的附加参数或所需的计算资源数量。

    arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
    
[^10]: DaCapo：加快自主系统在视频分析中的持续学习

    DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics

    [https://arxiv.org/abs/2403.14353](https://arxiv.org/abs/2403.14353)

    该论文提出了一种在自主系统中加速视频分析的持续学习方法，通过利用轻量级“学生”模型进行部署推理，利用更大的“教师”模型进行数据标记，实现对不断变化场景的持续自适应。

    

    深度神经网络（DNN）视频分析对于自动驾驶车辆、无人机（UAV）和安防机器人等自主系统至关重要。然而，由于其有限的计算资源和电池功率，实际部署面临挑战。为了解决这些挑战，持续学习利用在部署（推理）中的轻量级“学生”模型，利用更大的“教师”模型对采样数据进行标记（标记），并不断重新训练学生模型以适应不断变化的场景。

    arXiv:2403.14353v1 Announce Type: cross  Abstract: Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight "student" model at deployment (inference), leverages a larger "teacher" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for
    
[^11]: 使用网络参数附加分解解决有嘈杂标签问题

    Tackling Noisy Labels with Network Parameter Additive Decomposition

    [https://arxiv.org/abs/2403.13241](https://arxiv.org/abs/2403.13241)

    本论文提出了一种使用网络参数附加分解来解耦干净数据和错误标记数据的记忆，从而减少嘈杂标签对深度网络训练的副作用。

    

    考虑到具有嘈杂标签的数据，过参性深度网络会因为过度拟合错误标记的数据而导致泛化能力不佳。深度网络的记忆效应表明，尽管网络能够记忆所有嘈杂数据，但它们首先会记忆干净的训练数据，然后逐渐记忆错误标记的训练数据。一种利用记忆效应来对抗嘈杂标签的简单有效方法是早停止。然而，早停止无法区分对干净数据和错误标记数据的记忆，导致网络仍然在早期训练阶段不可避免地过度拟合错误标记的数据。在本文中，为了解耦干净数据和错误标记数据的记忆，并进一步减少错误标记数据的副作用，我们对网络参数进行了附加分解。即，将所有参数分解为两组，即参数 $\mathbf{w}$ 被分开始解

    arXiv:2403.13241v1 Announce Type: new  Abstract: Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data. A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters. Namely, all parameters are additively decomposed into two groups, i.e., parameters $\mathbf{w}$ are deco
    
[^12]: Mamba在时间序列预测中的有效性如何？

    Is Mamba Effective for Time Series Forecasting?

    [https://arxiv.org/abs/2403.11144](https://arxiv.org/abs/2403.11144)

    Mamba模型作为一种状态空间模型在时间序列预测中具有捕捉复杂依赖关系、近线性复杂度以及性能优势的潜力。

    

    在时间序列预测（TSF）领域中，由于Transformer模型能够聚焦全局环境，有效捕捉时间序列中长距离依赖关系以及辨别多变量之间的相关性，因此它一直展现出强大的性能。然而，由于Transformer模型的低效率和关于其捕捉依赖关系能力的质疑，对Transformer架构的不断完善工作仍在进行中。最近，状态空间模型（SSMs）如Mamba因其能够像Transformer一样捕捉序列中的复杂依赖关系，同时又保持近线性的复杂度而备受推崇。在文本和图像任务中，基于Mamba的模型可以提高性能并节约成本，实现双赢局面。这引起了我们对探索SSM在TSF任务中潜力的兴趣。在本文中，我们介绍了两种基于SSM的简单模型，S-Mamba和......

    arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and 
    
[^13]: 统一的无投影算法用于对抗性DR-次模优化

    Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization

    [https://arxiv.org/abs/2403.10063](https://arxiv.org/abs/2403.10063)

    本文提出了统一的无投影Frank-Wolfe类型算法，用于对抗性DR-次模优化，在不同场景下取得了令人瞩目的次线性 $\alpha$-后悔上界，并在单调设置中实现了无投影算法的最新次线性 $\alpha$-后悔上界。

    

    本文介绍了统一的无投影Frank-Wolfe类型算法，用于对抗性连续DR-次模优化，涵盖了诸如全信息和（半）强敌反馈、单调和非单调函数、不同约束以及类型的随机查询等场景。在非单调设置中考虑的每个问题中，所提出的算法要么是第一个具有证明的次线性 $\alpha$-后悔上界的算法，要么具有比现有技术更好的 $\alpha$-后悔上界，其中 $\alpha$ 是离线设置中的相应近似上界。在单调设置中，所提出的方法在8个考虑的情况中的7种中是无投影算法的最新次线性 $\alpha$-后悔上界，同时与剩余情况的结果相匹配。此外，本文还研究了对抗性DR-次模优化的半强敌和强敌反馈，推进了对这一问题的理解。

    arXiv:2403.10063v1 Announce Type: cross  Abstract: This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\alpha$-regret bounds or have better $\alpha$-regret bounds than the state of the art, where $\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this
    
[^14]: CHAI：高效LLM推理的聚类头部注意力

    CHAI: Clustered Head Attention for Efficient LLM Inference

    [https://arxiv.org/abs/2403.08058](https://arxiv.org/abs/2403.08058)

    CHAI提出了Clustered Head Attention（CHAI）方法，通过在运行时结合具有高相关性的注意力头部，实现了减少内存需求和计算量，能够在不需要微调的情况下将存储K,V缓存的内存需求降低21.4％，推理时间延迟降低1.73倍。

    

    大语言模型(LLMs)拥有数百亿参数改变了机器学习领域。然而，在推理时为这些模型提供服务既需要计算又需要内存，一个请求可能需要多个GPU和数十GB的内存。多头注意力是LLMs的关键组件之一，可以占LLMs内存和计算需求的50%以上。我们观察到在各头之间对注意力的关注有很高的冗余性。基于这一观察，我们提出了Clustered Head Attention (CHAI)。CHAI在运行时将具有高相关性的头部结合进行自注意力，从而减少内存和计算。在我们的实验中，我们展示了CHAI能够将存储K,V缓存的内存需求降低多达21.4%，推理时延迟降低多达1.73倍，而无需任何微调。CHAI实现了最多3.2%的偏差。

    arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
    
[^15]: 无逆矩阵快速自然梯度下降方法用于深度学习

    Inverse-Free Fast Natural Gradient Descent Method for Deep Learning

    [https://arxiv.org/abs/2403.03473](https://arxiv.org/abs/2403.03473)

    本文提出一种快速自然梯度下降（FNGD）方法，在深度学习中仅需要在第一个时代计算逆运算，避免了迭代求逆操作。

    

    二阶方法通过包含二阶导数或统计量可以比一阶方法收敛得更快，但由于计算效率低，它们在深度学习中很少被使用。为了解决这个问题，现有的许多解决方案都集中在减小需要求逆的矩阵的大小。然而，仍然需要在每次迭代中执行求逆操作。本文提出了一种快速自然梯度下降（FNGD）方法，只需在第一个时代计算逆运算。首先，我们将自然梯度下降（NGD）的梯度预处理公式重构为使用Sherman-Morrison-Woodbury公式的每个样本梯度的加权和。基于此，为了避免涉及计算系数的迭代逆操作，这些加权系数在整个时代共享而不影响经验性能。FNGD将NGD近似为f

    arXiv:2403.03473v1 Announce Type: new  Abstract: Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a f
    
[^16]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^17]: 建模多模态社交互动：具有密集对齐表示的新挑战和基线

    Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations

    [https://arxiv.org/abs/2403.02090](https://arxiv.org/abs/2403.02090)

    提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。

    

    理解涉及言语和非言语线索的社交互动对有效解释社交情境至关重要。然而，大多数关于多模态社交线索的先前工作主要集中在单人行为上，或依赖于与多方环境中的话语密切对齐的整体视觉表示。它们在建模多方互动的复杂动态方面存在局限。在本文中，我们介绍了三个新的具有挑战性的任务，以建模多人之间的细粒度动态：话语目标识别、代词指代消解和提及玩家预测。我们为社交推理游戏设置中的这些新挑战提供了广泛的数据注释。此外，我们提出了一种新颖的多模态基线，通过将视觉特征与其对应的话语同步，利用密集对齐的语言-视觉表示，这有助于

    arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
    
[^18]: 通过语法增强改进LLM代码生成

    Improving LLM Code Generation with Grammar Augmentation

    [https://arxiv.org/abs/2403.01632](https://arxiv.org/abs/2403.01632)

    SynCode是一个新框架，结合编程语言的语法和DFA mask store，在LLMs中生成代码过程中获得96.07%的句法错误降低，并展现出提高句法精度的重大影响。

    

    我们提出了 SynCode，一个用于高效和通用地解码大型语言模型（LLMs）代码的新框架。SynCode利用编程语言的语法，利用离线构建的基于语言语法终结符的高效查找表DFA mask store。我们展示了SynCode在给定编程语言的上下文无关文法（CFG）的完备性和正确性，展示其在保留语义上有效令牌的同时拒绝无效令牌的能力。该框架与由CFG定义的任何语言无缝集成，验证了针对Python和Go的CFG实验。结果突出了当SynCode与最先进的LLMs结合时，语法错误减少96.07%，彰显了其对提高代码生成中的句法精度的重大影响。

    arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
    
[^19]: 物理启发的机器学习用于预测非线性钢框架结构的地震响应

    Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures

    [https://arxiv.org/abs/2402.17992](https://arxiv.org/abs/2402.17992)

    本研究提出了一种物理启发的机器学习方法，将科学原理和物理定律融入深度神经网络，用于建模非线性结构的地震响应。

    

    由于传统数值模拟的大量计算成本，人们越来越关注利用机器学习（ML）方法进行结构元模型建模。现有的数据驱动策略显示出模型稳健性和可解释性以及丰富数据依赖性的潜在限制。为了解决这些挑战，本文提出了一种新颖的物理启发机器学习（PiML）方法，将科学原理和物理定律融入深度神经网络中，用于建模非线性结构的地震响应。基本概念是将ML模型的解空间约束在已知的物理范围内。这是通过三个主要特点实现的，即模型降阶、长短期记忆（LSTM）网络和牛顿第二定律（例如，运动方程）。模型降阶对处理具有固有冗余性和增强性的结构系统至关重要。

    arXiv:2402.17992v1 Announce Type: cross  Abstract: There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enh
    
[^20]: InstructEdit：针对大型语言模型的基于指令的知识编辑

    InstructEdit: Instruction-based Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2402.16123](https://arxiv.org/abs/2402.16123)

    InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。

    

    大型语言模型的知识编辑可以提供一种有效的解决方案，以改变模型的行为而不会对整体性能产生消极影响。然而，当前的方法在跨任务的通用性方面存在问题，需要为每个任务设计一个独特的编辑器，这显著阻碍了更广泛的应用。为了解决这一问题，我们首先分析了知识编辑中的多任务泛化问题。具体地，我们开发了一种基于指令的编辑技术，称为InstructEdit，通过简单的指令促进编辑器同时适应各种任务的表现。通过为每个LLM只使用一个统一的编辑器，我们在实证方面表明，InstructEdit可以提高编辑器的控制能力，从而在多任务编辑设置中平均提高可靠性14.86%。此外，涉及保留未见任务的实验说明，InstructEdi

    arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
    
[^21]: EasyRL4Rec：面向基于强化学习的推荐系统的用户友好代码库

    EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems

    [https://arxiv.org/abs/2402.15164](https://arxiv.org/abs/2402.15164)

    EasyRL4Rec是一个面向基于强化学习的推荐系统的用户友好和高效库，提供了多样化的RL环境、全面的核心模块、一致的评估标准和定制解决方案，旨在帮助简化模型开发并改善长期用户参与度。

    

    强化学习（RL）-基础的推荐系统（RSs）越来越被认可其提高长期用户参与度的能力。然而，这个领域面临挑战，如缺乏易用的框架、评估标准不一致以及复制以前的工作的复杂性。为解决这些障碍，我们提出了EasyRL4Rec，一个专为基于RL的RSs量身定制的用户友好和高效的库。EasyRL4Rec具有基于五个广泛使用的公共数据集构建的轻量级、多样化的RL环境，并配备了全面的核心模块，提供丰富的选项来简化模型的开发。它建立了一致的评估标准，重点关注长期影响，并引入了针对推荐系统定制的状态建模和行为表示的定制解决方案。此外，我们分享了通过与当前方法进行的大量实验获得的宝贵见解。EasyRL4Rec旨在促进

    arXiv:2402.15164v1 Announce Type: cross  Abstract: Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly recognized for their ability to improve long-term user engagement. Yet, the field grapples with challenges such as the absence of accessible frameworks, inconsistent evaluation standards, and the complexity of replicating prior work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight, diverse RL environments built on five widely-used public datasets, and is equipped with comprehensive core modules that offer rich options to ease the development of models. It establishes consistent evaluation criteria with a focus on long-term impacts and introduces customized solutions for state modeling and action representation tailored to recommender systems. Additionally, we share valuable insights gained from extensive experiments with current methods. EasyRL4Rec aims to facil
    
[^22]: 坚持你的角色！个人价值在大型语言模型中的稳定性

    Stick to your Role! Stability of Personal Values Expressed in Large Language Models

    [https://arxiv.org/abs/2402.14846](https://arxiv.org/abs/2402.14846)

    本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。

    

    通过基准测试或心理问卷的标准方式研究大型语言模型(LLMs)是提供许多来源于类似最小背景的不同查询（例如多项选择问题）。然而，由于LLM高度依赖于背景，因此从这种最小背景评估中得出的结论可能对模型在部署中的行为（在那里它将暴露于许多新背景）的说明很少。我们认为，依赖于背景的特性应该作为LLM比较的另一个维度来研究，而不是其他维度，如认知能力、知识或模型大小。在本文中，我们提出了一个关于在不同背景下（模拟对不同话题的对话）价值表达稳定性的案例研究，并使用标准心理学问卷（PVQ）和行为下游任务进行测量。我们考虑了来自五个家族的19个开源LLM。借鉴心理学方法，我们研究了等级稳定性。

    arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
    
[^23]: 深度强化学习在流体力学中主动流控制中的最佳并行化策略

    Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics

    [https://arxiv.org/abs/2402.11515](https://arxiv.org/abs/2402.11515)

    该研究专注于优化深度强化学习在流体力学中主动流控制中的并行设置，通过拆解DRL框架、进行扩展性基准测试、提出混合并行化配置并优化多环境DRL训练中的I/O操作，提出了有效的并行化策略。

    

    深度强化学习（DRL）已被证明是处理高动态和非线性主动流控制（AFC）问题的一种有前途的方法。然而，与训练DRL模型相关的计算成本构成了重要的性能瓶颈。为了应对这一挑战并在高性能计算架构上实现有效的扩展，本研究侧重于优化并行设置中的基于DRL的算法。我们验证了用于AFC问题的现有最先进的DRL框架，并讨论了其效率瓶颈。随后，通过拆解整体框架，并为各个组件进行广泛的可扩展性基准测试，我们研究了各种混合并行化配置，并提出了有效的并行化策略。此外，我们优化了多环境DRL训练中的输入/输出（I/O）操作，以解决与数据移动相关的关键开销。

    arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
    
[^24]: 在日常环境中的差分隐私联邦迁移学习用于心理健康监测：以压力检测为案例研究

    Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection

    [https://arxiv.org/abs/2402.10862](https://arxiv.org/abs/2402.10862)

    该论文介绍了一种差分私有联邦迁移学习框架，结合差分隐私和迁移学习，以提升心理健康监测中的数据隐私性和数据充足性。

    

    心理健康状况在各个人群中普遍存在，需要有效的监测来减轻其对生活质量的不利影响。数据驱动的心理健康监测方法的兴起强调了在处理敏感健康数据时隐私保护技术的重要性。尽管联邦学习在心理健康监测方面取得了进展，但现有方法在应对特定网络攻击和现实应用中的数据不足方面存在困难。本文介绍了一种差分私有联邦迁移学习框架，用于增强数据隐私性并丰富数据充足性。为实现这一目标，我们将联邦学习与差分隐私（通过将噪声引入更新）和迁移学习（利用预训练的通用模型）两个关键元素相结合，以应对数据不平衡和缺如问题。

    arXiv:2402.10862v1 Announce Type: new  Abstract: Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality. The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data. Despite strides in federated learning for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications. In this paper, we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency. To accomplish this, we integrate federated learning with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) transfer learning, employing a pre-trained universal model to adeptly address issues of data imbalance and in
    
[^25]: 解读心率信号：一种基于视觉变压器技术的可解释性房颤检测方法

    Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals

    [https://arxiv.org/abs/2402.09474](https://arxiv.org/abs/2402.09474)

    本研究使用视觉变压器方法解读心率信号，提高心脏疾病检测模型的解释性和可靠性。

    

    基于可穿戴单导联心电图（ECG）设备的远程患者监测在结合人工智能（AI）方法进行自动心脏疾病检测方面具有巨大潜力。先前的研究已经应用基于深度学习的AI方法进行心脏疾病检测，但由于目前AI算法的黑盒特性，这些模型尚未被广泛接受作为临床诊断的可靠辅助工具。尤其需要确定ECG信号中贡献于准确诊断的关键特征，从而提高模型的可解释性。本研究开发了一种基于视觉变压器的方法，通过单导联ECG数据识别房颤，并提出了一种残差网络（ResNet）方法以作对比。

    arXiv:2402.09474v1 Announce Type: cross  Abstract: Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a vision transformer approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the visi
    
[^26]: AMEND：一种用于长尾轨迹预测的专家混合框架

    AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction

    [https://arxiv.org/abs/2402.08698](https://arxiv.org/abs/2402.08698)

    本研究提出了一种模块化的模型无关的轨迹预测框架，使用专家混合来解决长尾效应问题，提高对于包含挑战性场景的数据的预测性能。

    

    准确地预测行人未来的动向对于智能驾驶系统至关重要。开发这个任务的模型需要包含多样样本的丰富数据集。然而，现有的自然轨迹预测数据集通常对简单样本偏重，并缺乏具有挑战性的场景。这种长尾效应导致预测模型在包含安全关键场景的数据分布的尾部部分表现不佳。以前的方法使用对比学习和类条件超网络等方法解决了长尾问题。然而，这些方法不是模块化的，不能应用于许多机器学习架构。在这项工作中，我们提出了一种模块化的模型无关的轨迹预测框架，利用专门的专家混合。在我们的方法中，每个专家都通过针对特定部分的特殊技能进行训练。

    arXiv:2402.08698v1 Announce Type: cross Abstract: Accurate prediction of pedestrians' future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as contrastive learning and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part
    
[^27]: RBF-PINN：物理信息神经网络中的非Fourier位置嵌入

    RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.08367](https://arxiv.org/abs/2402.08367)

    本论文介绍了RBF-PINN方法，在物理信息神经网络中应用非Fourier位置嵌入，解决了基于Fourier特征映射的局限性，并在各种正向和反向问题中取得了有效结果。

    

    虽然最近许多物理信息神经网络（PINNs）的变体在求解偏微分方程方面取得了显著的成功，但来自更广泛的神经表示研究的特征映射的经验优势在很大程度上被忽视了。我们强调了在某些情况下广泛使用的基于Fourier的特征映射的局限性，并建议使用具有条件正定性质的径向基函数。实证发现表明我们的方法在各种正向和反向问题案例中的有效性。我们的方法可以无缝集成到基于坐标的输入神经网络中，并为PINNs研究的更广泛领域做出贡献。

    While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked. We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function. The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases. Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research.
    
[^28]: ICED: 通过上下文环境设计实现强化学习的零样本迁移

    ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design

    [https://arxiv.org/abs/2402.03479](https://arxiv.org/abs/2402.03479)

    本研究探索了深度强化学习代理的零样本迁移能力，并发现通过根据值损失优先选择级别，可以改善代理的推广能力。此外，无监督环境设计方法对改善代理表现也具有重要作用。

    

    使用深度强化学习训练的自主代理通常缺乏成功地推广到新环境的能力，即使这些环境与它们在训练过程中遇到的环境具有相似的特征。本研究探讨了个体环境实例（或级别）的采样对强化学习代理的零样本推广能力的影响。我们发现，对于共享基本层的深度演员-评论家架构，根据其值损失优先选择级别，可以最小化代理的内部表示与生成的训练数据中的训练级别之间的互信息。这为某些自适应采样策略实现的隐式正则化提供了新颖的理论解释。然后，我们将注意力转向无监督环境设计（UED）方法，这些方法对数据生成机制具有更多控制。我们发现现有的UED方法可以显著改变训练数据中的环境实例，从而影响代理的表现能力。

    Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
    
[^29]: 无偏好对齐学习与正则化相关奖励

    Preference-free Alignment Learning with Regularized Relevance Reward

    [https://arxiv.org/abs/2402.03469](https://arxiv.org/abs/2402.03469)

    无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。

    

    从人类偏好中学习被认为是将大规模语言模型（LLMs）与人类价值观对齐的关键。然而，与普遍的观点相反，我们的初步研究发现，基于人类偏好数据集训练的奖励模型倾向于给长的与主题无关的回复更高的分数，而给短的与主题相关的回复较低分。在这一观察的驱动下，我们探索了一种无偏好的方法，利用“相关性”作为对齐的一个关键目标。在我们的第一次尝试中，我们发现仅仅通过检索得到的相关性得分容易受到奖励欺骗的影响，即过度优化到不期望的捷径上，当我们将该得分作为奖励用于强化学习。为了缓解这个问题，我们将有效的归纳偏差整合到常规的相关性中，互相正则化，形成了一种混合奖励函数：正则化相关奖励（$R^3$）。$R^3$通过提供稳健的奖励信号，显著提高了在偏好基准测试中的性能。值得注意的是，$R^3$不需要

    Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
    
[^30]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^31]: 无人机高分辨率影像的无监督语义分割用于路况场景解析

    Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing

    [https://arxiv.org/abs/2402.02985](https://arxiv.org/abs/2402.02985)

    本论文提出了一种无监督的路况解析框架，利用视觉语言模型和基本计算机视觉模型来解决无人机高分辨率影像下的路况场景解析问题。该框架首先利用视觉语言模型快速检测路况感兴趣区域，然后利用视觉基础模型生成路况区域掩模，然后采用自监督表示学习网络提取特征表示，最后通过无监督聚类算法对特征进行聚类和标记。

    

    在无人机图像中解析路况场景存在两个挑战。首先，无人机图像的高分辨率使得处理困难。其次，监督深度学习方法需要大量手动标注来训练强大而准确的模型。本文引入了一种无监督的路况解析框架，利用了近期在视觉语言模型和基本计算机视觉模型方面的进展。首先，采用视觉语言模型高效处理超大分辨率无人机图像，快速检测图像中的路况感兴趣区域。接下来，利用视觉基础模型SAM为没有类别信息的路况区域生成掩模。随后，采用自监督表示学习网络从所有掩模区域中提取特征表示。最后，应用无监督的聚类算法对这些特征表示进行聚类并为每个簇分配ID。然后，将掩模区域合并。

    Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined w
    
[^32]: 使用因果发现解释黑盒机器学习模型的反事实解释与信誉评级应用

    Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating

    [https://arxiv.org/abs/2402.02678](https://arxiv.org/abs/2402.02678)

    本研究提出了一种新的可解释人工智能框架，通过使用反事实概率和额外的因果结构先验信息，克服了因果图未知的问题，可以解释黑盒机器学习模型并应用于信贷评级等领域。

    

    可解释的人工智能（XAI）有助于阐明机器学习算法的内部机制，通过展示其预测基础来增强其可靠性。几种XAI模型考虑使用因果关系来解释模型，通过研究预测模型的输入输出关系和特征之间的依赖关系。这些模型大多基于反事实概率来解释，并假设因果图已知。然而，这种假设增加了这些模型在实际数据应用中的复杂性，因为大多数情况下特征之间的因果关系是未知的。因此，本研究提出了一种新颖的XAI框架，放宽了因果图已知的约束。该框架利用反事实概率和关于因果结构的额外先验信息，通过因果发现方法估计出的因果图与黑盒分类模型进行集成。

    Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classifica
    
[^33]: 使用反事实表示解释文本分类器

    Explaining Text Classifiers with Counterfactual Representations

    [https://arxiv.org/abs/2402.00711](https://arxiv.org/abs/2402.00711)

    本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。

    

    一种基于反事实的解释方法可以为分类器提供合理的解释，其中反事实是指除了一个分类特征之外，与真实观察完全相同的假设事件。然而，在文本领域构建这种反事实存在特定挑战，因为某些属性值可能与现实世界的事件不一致。在这篇论文中，我们提出了一种简单的方法，通过对文本表示进行干预来生成反事实，从而克服了这个限制。我们认为我们的干预方法是最小程度的干扰，并且在理论上是可靠的，因为它们与Pearl的因果推断框架中定义的反事实是一致的。为了验证我们的方法，我们首先在合成数据集上进行实验，比较了基于真实反事实（通过明确的文本干预获得）和我们的反事实（通过对文本表示的干预得到）的分类器预测。

    One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
    
[^34]: 分散、协作和保护隐私的多医院数据机器学习

    Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data

    [https://arxiv.org/abs/2402.00205](https://arxiv.org/abs/2402.00205)

    本文提出了一种用于多医院数据的分散、协作和保护隐私的机器学习方法（DeCaPH），它可以允许不同方在不共享私有数据集的情况下协作训练机器学习模型，并通过限制数据泄露和隐私侵犯来保护患者隐私。

    

    机器学习在医疗数据分析方面展现出巨大潜力。来自不同源头和环境的大型数据集对于医疗保健领域的机器学习模型实现更高的准确性和泛化能力至关重要。由于复杂且多变的隐私和监管要求，跨不同医疗机构共享数据是具有挑战性的。因此，允许多个方参与合作训练机器学习模型，在不直接共享数据集或通过合作损害数据集隐私的情况下利用各方现有的私有数据集具有困难但至关重要。在本文中，我们通过提出用于多医院数据的分散、协作和保护隐私的机器学习方法（DeCaPH）来解决这个问题。它具有以下关键优点：（1）允许不同方在不传输私有数据集的情况下协作训练机器学习模型；（2）通过限制潜在的数据泄露和隐私侵犯来保护患者隐私。

    Machine Learning (ML) has demonstrated its great potential on medical data analysis. Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability. Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements. Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration. In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential pr
    
[^35]: 简单策略优化

    Simple Policy Optimization

    [https://arxiv.org/abs/2401.16025](https://arxiv.org/abs/2401.16025)

    SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。

    

    PPO（Proximal Policy Optimization）算法在许多领域表现出色，被认为是TRPO（Trust Region Policy Optimization）算法的简化版本。然而，PPO中的比率剪切操作并不总是有效地强制执行信任区域约束，这可能会影响算法的稳定性。本文提出了一种新颖的剪切方法，即Simple Policy Optimization（SPO）算法，用于旧策略和当前策略之间的KL散度。在Atari 2600环境中进行的大量实验结果表明，与PPO的主流变体相比，SPO实现了更好的样本效率，极低的KL散度和更高的策略熵，并且对网络深度或复杂度的增加具有鲁棒性。更重要的是，SPO保持了无约束一阶算法的简单性。

    arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
    
[^36]: 使用ODE方法进行带有马尔可夫噪声的随机逼近和强化学习

    The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise

    [https://arxiv.org/abs/2401.07844](https://arxiv.org/abs/2401.07844)

    本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。

    

    随机逼近是一类通过迭代、增量和随机更新向量的算法，包括随机梯度下降和时序差分学习。分析随机逼近算法的一个主要挑战是确保其稳定性，即证明随机向量迭代几乎必定有界。本文将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，大大提高了其在强化学习中的适用性，特别是那些具有线性函数逼近和资格迹的离策略强化学习算法。我们的分析的核心在于少数函数的渐进变化速率下降，这一点由大数定律和常用的V4 Lyapunov漂移条件隐含，并在马尔可夫链是有限且不可约时显然成立。

    Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
    
[^37]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^38]: 具有用户定义目标的自适应干预用于健康行为改变

    Adaptive Interventions with User-Defined Goals for Health Behavior Change

    [https://arxiv.org/abs/2311.09483](https://arxiv.org/abs/2311.09483)

    该论文介绍了一种修改过的Thompson抽样算法，强调通过优化个性化奖励函数实现个性化目标设定，为支持目标设定提供了一个平衡方法，并证明此修改仅对累积遗憾产生恒定的惩罚。

    

    身体活动不足仍然是一个主要的公共健康问题，与心血管疾病和2型糖尿病等不良健康结果相关。移动健康应用程序为低成本、可扩展的身体活动促进提供了一个有希望的途径，然而通常效果较小，粘附率低，特别是与人类辅导相比。目标设定是健康辅导的一个关键组成部分，在移动健康干预的自适应算法中一直未充分利用。本文介绍了对Thompson抽样算法的修改，重点放在通过优化个性化奖励函数实现个性化目标设定。作为支持目标设定的一步，本文提供了一个可以利用共享结构同时优化个人偏好和目标的平衡方法。我们证明，我们的修改只对累积遗憾造成一个常数惩罚。

    arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
    
[^39]: 具有Fisher度量的黎曼拉普拉斯逼近

    Riemannian Laplace Approximation with the Fisher Metric

    [https://arxiv.org/abs/2311.02766](https://arxiv.org/abs/2311.02766)

    黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。

    

    Laplace方法用高斯分布在其模式处对目标密度进行近似。基于Bernstein-von Mises定理，它在贝叶斯推断中是计算效率高且渐近准确的，但对于复杂的目标和有限数据后验，它往往是一种过于粗糙的近似。最近对Laplace逼近的一般化是根据选择的黎曼几何对高斯近似进行转换，提供了更丰富的近似族，同时保持计算效率。然而，正如本文所示，其性质严重依赖于所选择的度量，实际上，在先前研究中采用的度量导致的逼近即使在无限数据量的极限下也过于狭窄且存在偏差。我们通过进一步发展逼近族，推导出两种在无限数据极限下精确的替代变种，扩展了理论分析。

    arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
    
[^40]: 利用斜裁决策森林增强在线环境中的群体公平性

    Enhancing Group Fairness in Online Settings Using Oblique Decision Forests

    [https://arxiv.org/abs/2310.11401](https://arxiv.org/abs/2310.11401)

    提出了Aranyani，一种斜裁集成的方法，用于解决在在线环境中优化群体公平性目标所面临的挑战

    

    公平性，特别是群体公平性，在机器学习系统中是一个重要的考虑因素。目前最常见的群体公平性增强技术是通过在训练过程中依赖公平目标（例如人口统计平等）和任务特定目标（例如交叉熵）的混合方法。然而，在数据以在线方式一次一个实例到达时，优化这样的公平性目标面临着几个挑战。特别是，群体公平性目标是通过不同人口统计群体的预测期望来定义的。在在线环境中，算法每次只能访问一个实例，估计群体公平性目标需要额外的存储和比任务特定目标更多的计算（例如前向/后向传递）在每个时间步上。

    arXiv:2310.11401v2 Announce Type: replace  Abstract: Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of obliq
    
[^41]: 通过接触力场和熵的触觉估计学习检测滑动

    Learning to Detect Slip through Tactile Estimation of the Contact Force Field and its Entropy

    [https://arxiv.org/abs/2303.00935](https://arxiv.org/abs/2303.00935)

    通过光学触觉传感器结合物理数据驱动方法，实时连续检测滑动，从滑动事件中提取不均匀特征解决滑动检测问题。

    

    在物体抓取和操作过程中检测滑动对于物体处理起着至关重要的作用。我们提出了一种新颖的受物理启发的数据驱动方法来实时连续检测滑动。我们使用GelSight Mini，一种光学触觉传感器，连接到自定义设计的夹具上以收集触觉数据。我们利用滑动事件期间触觉传感器读数的不均匀性来开发独特特征，将滑动检测建模为一个分类问题。

    arXiv:2303.00935v3 Announce Type: replace-cross  Abstract: Detection of slip during object grasping and manipulation plays a vital role in object handling. Existing solutions primarily rely on visual information to devise a strategy for grasping. However, for robotic systems to attain a level of proficiency comparable to humans, especially in consistently handling and manipulating unfamiliar objects, integrating artificial tactile sensing is increasingly essential. We introduce a novel physics-informed, data-driven approach to detect slip continuously in real time. We employ the GelSight Mini, an optical tactile sensor, attached to custom-designed grippers to gather tactile data. Our work leverages the inhomogeneity of tactile sensor readings during slip events to develop distinctive features and formulates slip detection as a classification problem. To evaluate our approach, we test multiple data-driven models on 10 common objects under different loading conditions, textures, and mate
    
[^42]: Primal Dual Alternating Proximal Gradient算法用于具有耦合线性约束的非光滑非凸极小极大问题

    Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints

    [https://arxiv.org/abs/2212.04672](https://arxiv.org/abs/2212.04672)

    提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。

    

    非凸极小极大问题近年来在机器学习、信号处理和许多其他领域引起了广泛关注。本文提出了一种用于解决非光滑非凸（强）凹和非凸线性极小极大问题的原始对偶交替近端梯度（PDAPG）算法和原始对偶近端梯度（PDPG-L）算法，分别用于具有耦合线性约束的情况。这两种算法的迭代复杂度证明为 $\mathcal{O}\left( \varepsilon ^{-2} \right)$ （对应 $\mathcal{O}\left( \varepsilon ^{-4} \right)$）在非凸强凹 （对应非凸凹）情况下，以及 $\mathcal{O}\left( \varepsilon ^{-3} \right)$ 在非凸线性情况下，分别达到 $\varepsilon$-稳态点。据我们所知，它们是用于解决具有耦合线性约束的非凸极小极大问题的第一批具有迭代复杂度保证的算法。

    arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
    
[^43]: OptiState：基于门控网络、Transformer视觉和卡尔曼滤波的腿式机器人状态估计

    OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering. (arXiv:2401.16719v1 [cs.RO])

    [http://arxiv.org/abs/2401.16719](http://arxiv.org/abs/2401.16719)

    本研究提出了一种名为OptiState的腿式机器人状态估计方法，该方法通过整合Kalman滤波、优化和学习模式的混合解决方案，结合本体感和外感信息，以精确估计机器人主体的状态。该方法利用关节编码器、IMU测量和基于凸规划的模型预测控制优化，通过Gate循环单元和Vision Transformer自编码器改进了估计结果。研究结果表明，该方法能够提供准确的机器人状态估计，并减小传感器测量和模型简化引起的非线性误差。

    

    由于腿式机器人的高动态运动和传感器精度的局限性，腿式机器人的状态估计具有挑战性。通过整合卡尔曼滤波、优化和基于学习的模态，我们提出了一种混合解决方案，结合了本体感和外感信息，用于估计机器人主体的状态。借助关节编码器和IMU测量，我们的卡尔曼滤波器通过单刚体模型进行增强，该模型还结合了基于凸规划的模型预测控制优化的接地反力控制输出。通过门控循环单元进一步改进估计结果，该方法还考虑了从深度图像上应用视觉Transformer自编码器获得的语义洞察和机器人高度。该框架不仅提供准确的机器人状态估计，包括不确定性评估，还可以通过学习来减小传感器测量和模型简化引起的非线性误差。所提出的方法经过评估。

    State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated
    
[^44]: 基于剪枝的保护: 在不进行微调的情况下增加对齐的LLMs的越狱抵抗力

    Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])

    [http://arxiv.org/abs/2401.10862](http://arxiv.org/abs/2401.10862)

    本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。

    

    大型语言模型（LLMs）容易受到“越狱”提示的攻击，这种攻击可以诱使这些模型生成有害和违法内容。本文表明，剪枝LLM参数多达20％可以显著增加它们对此类攻击的抵抗力，而无需额外训练并且不损害其在标准基准测试中的性能。有趣的是，我们发现剪枝后观察到的增强安全性与模型的初始安全训练水平相关，这暗示剪枝的效果可能更普遍，也可能适用于超出安全性范畴的其他LLM行为。另外，我们还介绍了一个包含五个类别、插入到十个不同越狱提示中的225个有害任务的精选数据集，表明剪枝有助于LLMs集中注意力在越狱提示中与任务相关的标记上。最后，我们的实验揭示了突出的聊天模型（如LLaMA-2 Chat，Vicuna和Mistral Instruct）具有很高的易感性。

    Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
    
[^45]: 可调整的分子表示方法用于统一的预训练策略

    Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.06166](http://arxiv.org/abs/2401.06166)

    AdaMR是一种可调整粒度的分子模型，它在原子和亚结构水平上学习分子表示。通过预训练和分子规范化任务，AdaMR可以改善对多个下游任务的效果，包括模型属性预测和分子生成。

    

    我们提出了一种新的大规模分子模型，名为AdaMR，它代表可调整的分子表示方法用于统一的预训练策略。与最近使用单一分子编码的大规模分子模型不同，AdaMR采用了可调整粒度的分子编码器，在原子和亚结构水平上学习分子表示。对于预训练过程，我们设计了一个分子规范化的任务，涉及将多个通用分子表示转化为规范表示。通过调整分子编码的粒度，训练得到的模型可以提高对多个下游任务的效果，如模型属性预测和分子生成。亚结构水平的分子表示保留了决定化学性质和具有类似功能的特定原子组或排列的信息，对于性质预测等任务是有益的。同时，原子级表示将原子的特异信息纳入考虑，使模型能够更好地捕捉原子间的相互作用。

    We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combin
    
[^46]: L3Cube-IndicNews：印度语系新闻短文和长文分类数据集

    L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])

    [http://arxiv.org/abs/2401.02254](http://arxiv.org/abs/2401.02254)

    L3Cube-IndicNews是一个面向印度语系的多语种文本分类数据集，包括短标题、长文档和长段落三个数据集。它提供了10种印度语言的新闻文章，每个数据集包含10个或更多类别的文章。这个数据集可以用于深入分析和评估。

    

    在这项工作中，我们介绍了L3Cube-IndicNews，这是一个多语种文本分类语料库，旨在为印度地区的各大方言语言提供高质量的数据集，特别关注新闻标题和文章。我们的工作主要集中在10种主要的印度语言上，包括印地语、孟加拉语、马拉地语、泰卢固语、泰米尔语、古吉拉特语、卡纳达语、奥里亚语、马拉雅拉姆语和旁遮普语。每个新闻数据集包含10个或更多类别的新闻文章。L3Cube-IndicNews提供了3个不同数据集，针对不同的文档长度进行分类：短标题分类（SHC）数据集包含新闻标题和新闻类别，长文档分类（LDC）数据集包含整个新闻文章和新闻类别，长段落分类（LPC）数据集包含新闻的子文章和新闻类别。我们在所有3个数据集中都保持了一致的标签，以进行深入的基于长度的分析。我们使用4个指标对每个印度语言数据集进行评估。

    In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
    
[^47]: TopCoW：基于拓扑感知解剖分割的Willis循环（CoW）在CTA和MRA中的基准测试

    TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA. (arXiv:2312.17670v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.17670](http://arxiv.org/abs/2312.17670)

    这项研究提出了TopCoW挑战，通过发布具有13种血管组分注释的Willis循环（CoW）数据集，并使用虚拟现实（VR）技术进行拓扑感知解剖分割，解决了手动和耗时的CoW表征问题。

    

    Willis循环（CoW）是连接大脑主要循环的重要动脉网络。其血管结构被认为影响着严重神经血管疾病的风险、严重程度和临床结果。然而，对高度变化的CoW解剖进行表征仍然是一项需要手动和耗时的专家任务。CoW通常通过两种血管造影成像模式进行成像，即磁共振血管成像（MRA）和计算机断层血管造影（CTA），但是关于CTA的CoW解剖的公共数据集及其注释非常有限。因此，我们在2023年组织了TopCoW挑战赛，并发布了一个带有注释的CoW数据集。TopCoW数据集是第一个具有13种可能的CoW血管组分的体素级注释的公共数据集，通过虚拟现实（VR）技术实现。它也是第一个带有来自同一患者的成对MRA和CTA的大型数据集。TopCoW挑战将CoW表征问题形式化为多类问题。

    The Circle of Willis (CoW) is an important network of arteries connecting major circulations of the brain. Its vascular architecture is believed to affect the risk, severity, and clinical outcome of serious neuro-vascular diseases. However, characterizing the highly variable CoW anatomy is still a manual and time-consuming expert task. The CoW is usually imaged by two angiographic imaging modalities, magnetic resonance angiography (MRA) and computed tomography angiography (CTA), but there exist limited public datasets with annotations on CoW anatomy, especially for CTA. Therefore we organized the TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The TopCoW dataset was the first public dataset with voxel-level annotations for thirteen possible CoW vessel components, enabled by virtual-reality (VR) technology. It was also the first large dataset with paired MRA and CTA from the same patients. TopCoW challenge formalized the CoW characterization problem as a multiclas
    
[^48]: 基于梯度采样优化的残差神经网络的鲁棒剪枝

    Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks. (arXiv:2312.16020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16020](http://arxiv.org/abs/2312.16020)

    本研究通过在剪枝过程中应用梯度采样技术，实现了在资源有限的场景中保持高准确性水平的鲁棒剪枝。相关实验证明，采用梯度采样技术进行优化的模型比传统优化方法更能有效地保持准确性。这一创新方法在各种数据集和神经架构上得到了验证，并且理论上解释了梯度采样技术对模型鲁棒性的贡献。

    

    在本研究中，我们探讨了一种创新的神经网络优化方法，重点是在剪枝过程中应用梯度采样技术，类似于StochGradAdam中的技术。我们的主要目标是在资源有限的场景中保持剪枝模型的高准确性水平，这是一个关键性挑战。我们的广泛实验表明，采用梯度采样技术优化的模型在剪枝过程中比使用传统优化方法更有效地保持准确性。这一发现强调了梯度采样在促进鲁棒学习和使网络在复杂度大大降低后仍能保留关键信息方面的重要性。我们通过各种数据集和神经架构验证了我们的方法，展示了它的广泛适用性和效果。该论文还深入探讨了理论方面，解释了梯度采样技术如何增强模型的鲁棒性。

    In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of m
    
[^49]: 决策支持框架在家庭保健护理员分配中的应用：田纳西州HHC机构的案例研究

    Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA. (arXiv:2311.00696v1 [cs.LG])

    [http://arxiv.org/abs/2311.00696](http://arxiv.org/abs/2311.00696)

    本研究针对家庭保健机构面临的护理员分配问题，提出了一种决策支持框架，通过考虑护理员对访问顺序的灵活性，旨在减少行驶里程、增加访问次数，并保持护理的连续性。

    

    人口老龄化是一个全球性的挑战，导致对老年人的医疗和社会服务需求增加。家庭保健护理（HHC）作为一种专门为这一人群提供服务的重要解决方案正逐渐兴起。鉴于对HHC的需求激增，有效地协调和管理护理员的分配至关重要，这对于预算优化的规划和确保提供高质量的护理至关重要。本研究回答了家庭保健机构面临的一个关键问题：“在护理员偏好灵活的访问顺序的情况下，如何优化他们的分配？”之前的研究提出了刚性的访问顺序，而我们的研究引入了一种决策支持框架，通过一种混合方法对护理员进行分配，考虑了访问顺序的灵活性，旨在减少行驶里程、增加每个规划周期的访问次数，并保持连续护理-这是衡量患者情况的关键指标。

    Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly. Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment. Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently. This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care. This research addresses a key question faced by home health agencies (HHAs): "How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?". While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient s
    
[^50]: 高斯过程的随机梯度下降的正确实现

    Stochastic Gradient Descent for Gaussian Processes Done Right. (arXiv:2310.20581v1 [cs.LG])

    [http://arxiv.org/abs/2310.20581](http://arxiv.org/abs/2310.20581)

    本文研究了使用随机梯度下降方法优化高斯过程回归问题，并引入了一种特定的随机对偶梯度下降算法，该方法在标准回归基准和贝叶斯优化任务上表现出很高的竞争力。

    

    本文研究了使用平方损失函数的高斯过程回归的优化问题。目前，解决这个问题的最常见方法是应用精确求解器，比如共轭梯度下降，要么直接应用，要么应用于问题的降阶版本。最近，在深度学习的成功推动下，随机梯度下降作为一种替代方法获得了广泛关注。本文展示了当正确使用时（我们指的是利用优化和核函数领域的特定见解），这种方法是非常有效的。因此，我们引入了一种特定的随机对偶梯度下降算法，可以使用任何深度学习框架的几行代码实现。我们通过消融实验解释了我们的设计决策的优势，并表明新方法具有很高的竞争力。我们对标准回归基准和贝叶斯优化任务进行评估，证明了我们的方法的优越性。

    We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apa
    
[^51]: 多并行任务延迟储备计算结合硅微环与WDM的方案

    Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM. (arXiv:2310.16588v1 [cs.NE])

    [http://arxiv.org/abs/2310.16588](http://arxiv.org/abs/2310.16588)

    该论文展示了一种基于微环的时间延迟储备计算方案，能够同时解决时间序列预测、分类和无线通道均衡等任务，并且在每个通道上实现了最先进的性能。

    

    我们通过数值模拟展示了一种基于微环的时间延迟储备计算方案，能够同时解决涉及时间序列预测、分类和无线通道均衡的三个任务。在每个进行波长复用的通道上，通过优化功率和频率失谐，实现了最先进的性能。

    We numerically demonstrate a microring-based time-delay reservoir computing scheme that simultaneously solves three tasks involving time-series prediction, classification, and wireless channel equalization. Each task performed on a wavelength-multiplexed channel achieves state-of-the-art performance with optimized power and frequency detuning.
    
[^52]: 基于异构图神经网络的数据驱动交通分配研究

    Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment. (arXiv:2310.13193v1 [cs.LG])

    [http://arxiv.org/abs/2310.13193](http://arxiv.org/abs/2310.13193)

    本论文提出了一种基于异构图神经网络的数据驱动交通分配和交通流学习方法，该方法能够准确捕捉不同链路之间的空间交通模式，优于其他传统神经网络模型，在大规模网络中有着广泛的应用潜力。

    

    交通分配问题是交通流分析的重要组成部分之一，已经提出了各种解决方法。然而，将这些方法应用于大规模网络面临重大挑战。在本文中，我们利用异构图神经网络的强大能力，提出了一种新颖的基于数据驱动的交通分配和交通流学习方法。所提出的模型能够捕捉不同链路之间的空间交通模式，从而产生高度准确的结果。我们在城市交通网络上进行了数值实验，并展示了该异构图神经网络模型在收敛速度、训练损失和预测准确度方面优于其他传统神经网络模型的表现。值得注意的是，所提出的异构图神经网络模型还可以推广到不同的网络拓扑。这种方法为复杂交通流分析和预测提供了一种有希望的解决方案。

    The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and predictio
    
[^53]: 无限时域平均奖励强化学习的量子加速

    Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])

    [http://arxiv.org/abs/2310.11684](http://arxiv.org/abs/2310.11684)

    本研究探索了无限时域平均奖励强化学习中量子加速的潜力。我们提出了一种创新的量子框架，通过高效的量子均值估计技术，实现了指数级改进的遗憾保证。所提出的量子算法相较于经典算法，在遗憾界限上有显著改进。

    

    本文研究量子加速在解决无限时域Markov决策过程（MDPs）中提高平均奖励结果的潜力。我们引入了一种创新的量子框架，用于代理与未知MDP的互动，扩展了传统的交互范式。我们的方法涉及设计一种基于乐观主导的具有量子信号的表格强化学习算法，通过高效的量子均值估计技术获取代理获取的量子信号。通过深入的理论分析，我们证明了量子均值估计的优势能够在无限时域强化学习中导致遗憾保证的指数进展。具体地，所提出的量子算法实现了一个遗憾界为$\tilde{\mathcal{O}}(1)$的性能，这是相对于经典对应算法所展示的$\tilde{\mathcal{O}}(\sqrt{T})$界限的显著改进。

    This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
    
[^54]: ZeroSwap: 基于数据驱动的 DeFi 中的最优市场做市

    ZeroSwap: Data-driven Optimal Market Making in DeFi. (arXiv:2310.09413v1 [cs.LG])

    [http://arxiv.org/abs/2310.09413](http://arxiv.org/abs/2310.09413)

    ZeroSwap 是第一个基于数据驱动算法的 DeFi 市场做市方案，在保持市场做市商零利润的情况下，通过适应交易者行为来解决了流动性提供者遭受套利损失的问题。

    

    自动做市商 (AMMs) 是去中心化金融中匹配流动性供给和需求的主要中心。它们的功能主要依赖于流动性提供者 (LPs) 将其资产投资于流动性池。然而，池中资产交易的价格通常比集中化和更流动的交易所价格延迟更多。这导致流动性提供者遭受套利损失。我们通过采用 Glosten 和 Milgrom 的经典市场微观结构模型，将市场价格适应于交易者行为，从而解决了这个问题。在本文中，我们提出了第一个最优贝叶斯和第一个无模型数据驱动算法来最优地跟踪资产的外部价格。我们使用的最优性概念在市场做市商的价格上强制执行了零利润条件，因此取名为 ZeroSwap。这确保了市场做市商在损失知情交易者的同时从噪声交易者那里获得利润。

    Automated Market Makers (AMMs) are major centers of matching liquidity supply and demand in Decentralized Finance. Their functioning relies primarily on the presence of liquidity providers (LPs) incentivized to invest their assets into a liquidity pool. However, the prices at which a pooled asset is traded is often more stale than the prices on centralized and more liquid exchanges. This leads to the LPs suffering losses to arbitrage. This problem is addressed by adapting market prices to trader behavior, captured via the classical market microstructure model of Glosten and Milgrom. In this paper, we propose the first optimal Bayesian and the first model-free data-driven algorithm to optimally track the external price of the asset. The notion of optimality that we use enforces a zero-profit condition on the prices of the market maker, hence the name ZeroSwap. This ensures that the market maker balances losses to informed traders with profits from noise traders. The key property of our 
    
[^55]: 安全深度策略适应

    Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])

    [http://arxiv.org/abs/2310.08602](http://arxiv.org/abs/2310.08602)

    该论文提出了SafeDPA，一种新颖的强化学习和控制框架，用于同时解决策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，并使用少量真实数据进行微调。在真实世界部署过程中，通过引入基于控制屏障函数的安全过滤器，确保了SafeDPA的安全性。

    

    自主和人工智能的一个重要目标是使自主机器人能够在动态和不确定的环境中快速适应。经典的自适应控制和安全控制提供了稳定性和安全性保证，但仅限于特定的系统类别。相比之下，基于强化学习（RL）的策略适应提供了通用性和泛化性，但同时也带来了安全性和稳健性的挑战。我们提出了SafeDPA，一种新颖的RL和控制框架，同时解决了策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，预测环境配置，并使用少量真实数据对动力学模型进行微调。在RL策略之上引入基于控制屏障函数（CBF）的安全过滤器，以确保在真实世界部署过程中的安全性。我们提供了SafeDPA的理论安全性保证，并展示了SafeDPA对学习误差的稳健性。

    A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
    
[^56]: 揭示单切平面误区：在机器人学习中应用黎曼几何的分析和澄清

    Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])

    [http://arxiv.org/abs/2310.07902](http://arxiv.org/abs/2310.07902)

    "Unraveling the Single Tangent Space Fallacy"论文分析和澄清了在机器人学习中应用黎曼几何的误区，该误区是指将数据仅投影到单一切空间中的方法。

    

    在机器人领域，许多后续的机器人任务利用机器学习方法来处理、建模或合成数据。这些数据通常包含固体方向表示四元数的单位范数条件或刚度和可操纵性椭球的正定性等几何约束。有效处理这样的几何约束需要将微分几何工具纳入机器学习方法的制定中。在这个背景下，黎曼流形成为处理这种几何约束的强大数学框架。然而，最近在机器人学习中对其采用过程中存在的一个数学上的缺陷化简现象，被称为“单切平面误区”。这种方法仅涉及将感兴趣的数据投影到一个单一切空间（欧几里得空间）上，然后使用现成的学习算法

    In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algor
    
[^57]: 具有相位随机桥的生成建模

    Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])

    [http://arxiv.org/abs/2310.07805](http://arxiv.org/abs/2310.07805)

    通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。

    

    扩散模型（DMs）是用于连续输入的最先进的生成模型。DMs通过在输入空间（即位置空间）中构建随机微分方程（SDE），并使用神经网络进行反演来工作。在这项工作中，我们介绍了一种基于相位空间动力学的新型生成建模框架，其中相位空间被定义为一个包括位置和速度的增强空间。利用随机最优控制的洞察力，我们构建了相位空间中的路径测度，实现了高效的采样。与DMs相比，我们的框架在动力传播的早期阶段就能够生成逼真的数据点。这种早期预测为通过沿轨迹利用额外的速度信息实现高效的数据生成奠定了基础。在标准图像生成基准测试中，我们的模型在小函数评估数量的范围内表现出优秀的性能。

    Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
    
[^58]: 重审视视觉强化学习中的可塑性：数据、模块和训练阶段

    Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])

    [http://arxiv.org/abs/2310.07418](http://arxiv.org/abs/2310.07418)

    本文对视觉强化学习中的可塑性进行了研究，发现数据增强对保持可塑性至关重要，评论者的可塑性损失是高效训练的主要限制因素，并且未及时恢复评论者的可塑性将导致灾难性结果。这为解决高重放比困境提供了新的策略。

    

    可塑性，神经网络随新数据演进的能力，对于高性能和样本高效的视觉强化学习(VRL)至关重要。虽然重置和正则化等方法可能能够缓解可塑性损失，但VRL框架内各种组件对代理的可塑性的影响仍然知之甚少。在这项工作中，我们进行了系统的经验性探索，重点关注了三个主要尚未充分探索的方面，并得出以下有深入见解的结论：(1)数据增强对于保持可塑性至关重要；(2)评论者的可塑性损失是阻碍高效训练的主要瓶颈；(3)在早期阶段没有及时干预以恢复评论者的可塑性，其损失将变得灾难性。这些见解提出了一种应对高重放比（RR）困境的新策略，其中加剧的可塑性损失妨碍了通过增加重放数量带来的样本效率的潜在改进。

    Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
    
[^59]: 永远不要从头开始训练：公正比较长序列模型需要数据驱动的先验知识

    Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])

    [http://arxiv.org/abs/2310.02980](http://arxiv.org/abs/2310.02980)

    本文研究表明使用随机初始化会导致对架构差异的严重高估，而使用标准消噪目标进行预训练可以在多种架构上实现显著的性能提升，并将Transformers与状态空间模型之间的差距缩小到很小。与之前的研究不同的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上的性能与S4相匹配，并且在PathX-256任务上改进了SSMs的最佳结果20个百分点。

    

    建模序列之间的长程依赖一直是机器学习中的目标，并导致了一些架构，如状态空间模型，在处理长序列时比Transformers有显著的优势。然而，这些令人印象深刻的经验性进展主要是在随机初始化并通过预测输入序列的目标标签进行训练的基准测试（例如Long Range Arena）上展示出来的。在这项工作中，我们展示了随机初始化导致对架构之间差异的严重高估，并且使用标准消噪目标进行预训练（仅使用下游任务数据）可以在多种架构上实现显著的收益，并且可以在Transformers和状态空间模型（SSMs）之间得到很小的差距。与之前的研究形成鲜明对比的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上与S4的性能相匹配，并且我们在PathX-256任务上将SSMs的最佳报告结果提高了20个百分点。

    Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
    
[^60]: 使用量子统计查询学习量子过程

    Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2310.02075](http://arxiv.org/abs/2310.02075)

    本文提出了第一个在量子统计查询模型内学习量子过程的框架，并提供了一个高效的学习器和可证明的性能保证。通过在密码分析中的应用，揭示了经典读出量子物理不可克隆函数的脆弱性，这是量子硬件安全领域一个重要的开放问题的解决方法。

    

    学习复杂的量子过程是量子计算和量子机器学习的许多领域的一个核心挑战，应用于量子基准测试、密码分析和变分量子算法。本文引入了第一个学习框架，用于在量子统计查询（QSQ）模型内研究量子过程学习，提供了对量子过程（QPSQs）进行统计查询的第一个正式定义。该框架使我们能够提出一种高效的QPSQ学习器，适用于任意量子过程，并附带可证明的性能保证。我们还提供了数值模拟来展示该算法的有效性。通过在密码分析中应用该框架，突出了经典读出量子物理不可克隆函数（CR-QPUFs）的脆弱性，解决了量子硬件安全领域中的一个重要开放问题。这项工作是朝着深入理解量子过程学习迈出的重要一步。

    Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
    
[^61]: ResBit: 基于残差位向量的离散值表示方法

    ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])

    [http://arxiv.org/abs/2309.17196](http://arxiv.org/abs/2309.17196)

    本论文提出了一种名为ResBit的残差位向量方法，用于解决在深度学习中表示离散数据维度增加和无法恢复原始类别值的问题。

    

    长期以来，独热编码向量一直广泛应用于机器学习中，作为一种简单且通用的表示离散数据的方法。然而，这种方法会导致维度随着要表示的离散数据线性增加，这在深度学习中视为空间计算复杂性的问题，而深度学习需要大量的数据。最近，基于扩散模型的高表达能力，提出了一种用位序列表示离散数据的方法，即Analog Bits。然而，由于在生成任务中要表示的类别类型数量不一定是2的幂次，导致Analog Bits能够表示的范围与类别数据的范围存在差异。如果生成了这样的值，问题就是无法恢复原始的类别值。为了解决这个问题，我们提出了残差位向量（ResBit），它是一种分层的位表示方法。

    The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
    
[^62]: 适应性输入图像归一化方法解决基于GAN的X射线图像的模式塌陷问题

    Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])

    [http://arxiv.org/abs/2309.12245](http://arxiv.org/abs/2309.12245)

    本论文研究了生成对抗网络中的模式塌陷问题对合成X射线图像多样性的影响。通过实验证明了将自适应输入图像归一化方法与深度模型结合的优势。

    

    由于疾病的罕见性，生物医学图像数据集可能存在不平衡。生成对抗网络通过生成合成图像来扩充数据集，起到了解决这种不平衡的关键作用。生成合成图像需要包含多样化的特征，以准确表示训练图像中存在的特征分布。此外，合成图像中缺乏多样性的特征会降低机器学习分类器的性能。模式塌陷问题影响生成对抗网络生成多样化图像的能力，并分为类内和类间两种类型。本文研究了这两种模式塌陷问题，并评估了它们对合成X射线图像多样性的影响。本研究将自适应输入图像归一化方法与深度模型相结合，通过实验证明了其在解决模式塌陷问题上的优势。

    Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep
    
[^63]: 基于单图像的无监督联合分割和去噪方法

    Single-Image based unsupervised joint segmentation and denoising. (arXiv:2309.10511v1 [cs.CV])

    [http://arxiv.org/abs/2309.10511](http://arxiv.org/abs/2309.10511)

    本文提出一种基于单图像的无监督方法，实现了联合分割和去噪。该方法不需要大量标记样本，且能够处理高噪声和通用纹理，并在显微镜图像上表现出色。

    

    本文提出了一种基于单图像的无监督方法，用于联合分割和去噪。为此，我们将变分分割方法的优势与自监督、基于单图像的深度学习方法的能力结合起来。我们方法的一个主要优点在于，与需要大量标记样本的数据驱动方法不同，我们的模型可以在没有任何训练数据库的情况下将图像分割成多个有意义的区域。此外，我们引入了一种新颖的能量函数，其中去噪和分割以一种相互受益的方式耦合在一起。通过与自监督图像去噪的特定组合，我们解决了现有的单图像基于变分分割方法对高噪声或通用纹理的限制。我们提出了一个统一的优化策略，并展示了在显微镜中可用的非常嘈杂的图像上，我们的方法表现出色。

    In this work, we develop an unsupervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of a variational segmentation method with the power of a self-supervised, single-image based deep learning approach. One major strength of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, our model can segment an image into multiple meaningful regions without any training database. Further, we introduce a novel energy functional in which denoising and segmentation are coupled in a way that both tasks benefit from each other. The limitations of existing single-image based variational segmentation methods, which are not capable of dealing with high noise or generic texture, are tackled by this specific combination with self-supervised image denoising. We propose a unified optimisation strategy and show that, especially for very noisy images available in microscopy, our p
    
[^64]: 使用机器学习和不确定性量化对CRT的多阶段决策过程进行建模的新方法

    A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])

    [http://arxiv.org/abs/2309.08415](http://arxiv.org/abs/2309.08415)

    本研究提出了一种使用机器学习和不确定性量化建模的多阶段决策过程方法，用于预测心力衰竭患者对心脏再同步治疗的反应。该模型能够推荐收集额外的SPECT MPI变量，以提高预测准确性。

    

    目的。本研究旨在创建一个多阶段的机器学习模型，用于预测心力衰竭（HF）患者心脏再同步治疗（CRT）的反应。该模型利用不确定性量化来推荐在基线临床变量和心电图（ECG）的特征不足时收集额外的单光子发射计算机体层摄影心肌灌注显像（SPECT MPI）变量。方法。本研究纳入了218名接受静息门控SPECT MPI的患者。CRT反应被定义为6个月随访时左室射血分数（LVEF）增加> 5%。通过组合两个集成模型创建了一个多阶段的机器学习模型。结果。CRT的反应率为55.5%（n = 121），整体男性占61.0%（n = 133），平均年龄62.0岁，LVEF为27.7。该多阶段模型的性能与集成模型2（利用了额外的SPECT数据）相似，AUC分别为0.75和0.77，准确性分别为0.71和...

    Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) > 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
    
[^65]: 从有限且有噪声的数据中基于物理约束的稳健学习开放式偏微分方程

    Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])

    [http://arxiv.org/abs/2309.07672](http://arxiv.org/abs/2309.07672)

    该论文提出了一种名为R-DISCOVER的框架，能够从有限且有噪声的数据中稳健地揭示开放式偏微分方程。该框架通过符号表示和强化学习引导下的混合PDE生成器，以及神经网络预测模型来实现。实验结果表明该方法能够高效地发现和嵌入PDE，并选择表现最佳的PDE进行系统响应预测。

    

    揭示非线性动态系统的基本控制方程在遇到噪声观测和没有现成先验知识的情况下仍然是一个巨大的挑战。本研究提出了R-DISCOVER，这是一个旨在从有限和有噪声的数据中稳健地揭示开放式偏微分方程的框架。该框架通过两个交替更新过程进行操作：发现和嵌入。发现阶段利用符号表示和强化学习（RL）引导下的混合PDE生成器，高效地产生具有树结构的多样化的开放式偏微分方程。基于神经网络的预测模型适应系统响应并作为生成的PDE的奖励评估器。利用拟合效果较好的PDE通过RL方法进行迭代优化生成器，并通过无参数稳定度指标选择表现最佳的PDE。嵌入阶段将最初从发现过程中确定的PDE与逼近真实系统进行频谱分析。

    Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge, especially when encountering noisy observations and no prior knowledge available. This study proposes R-DISCOVER, a framework designed to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with superior fits are utilized to iteratively optimize the generator via the RL method and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process a
    
[^66]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^67]: 孟加拉文档布局分析-一种基于YOLOv8的集成方法

    Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.00848](http://arxiv.org/abs/2309.00848)

    本文提出了一种基于YOLOv8模型和创新的后处理技术的孟加拉文档布局分析方法，通过数据增强和两阶段预测策略实现了准确的元素分割。该方法优于单个基础架构，并解决了BaDLAD数据集中的问题，有助于提高OCR和文档理解能力。

    

    本文侧重于利用YOLOv8模型和创新的后处理技术提升孟加拉文档布局分析（DLA）。我们通过数据增强以应对孟加拉复杂文字独特的挑战，经过严格的验证集评估，对完整数据集进行微调，实现准确的元素分割的两阶段预测策略。我们的集成模型结合后处理性能优于单个基础架构，解决了BaDLAD数据集中的问题。通过利用这种方法，我们旨在推动孟加拉文档分析的发展，提高OCR和文档理解能力，同时BaDLAD作为基础资源有助于未来的研究。此外，我们的实验为将新策略纳入现有解决方案提供了关键见解。

    This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
    
[^68]: 用于物理信息神经网络的贝叶斯推理

    Bayesian Reasoning for Physics Informed Neural Networks. (arXiv:2308.13222v1 [physics.comp-ph])

    [http://arxiv.org/abs/2308.13222](http://arxiv.org/abs/2308.13222)

    本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。

    

    本文提出了一种基于贝叶斯公式的物理信息神经网络（PINN）方法。我们采用了MacKay在Neural Computation（1992年）中提出的贝叶斯神经网络框架。通过拉普拉斯近似法，得到后验密度。对于每个模型（拟合），计算所谓的证据。它是一种分类假设的度量。最优解具有最大的证据值。贝叶斯框架使我们能够控制边界对总损失的影响。事实上，贝叶斯算法通过微调损失组件的相对权重。我们解决了热力学、波动和Burger方程。所得结果与精确解基本一致。所有解都提供了在贝叶斯框架内计算的不确定性。

    Physics informed neural network (PINN) approach in Bayesian formulation is presented. We adopt the Bayesian neural network framework formulated by MacKay (Neural Computation 4 (3) (1992) 448). The posterior densities are obtained from Laplace approximation. For each model (fit), the so-called evidence is computed. It is a measure that classifies the hypothesis. The most optimal solution has the maximal value of the evidence. The Bayesian framework allows us to control the impact of the boundary contribution to the total loss. Indeed, the relative weights of loss components are fine-tuned by the Bayesian algorithm. We solve heat, wave, and Burger's equations. The obtained results are in good agreement with the exact solutions. All solutions are provided with the uncertainties computed within the Bayesian framework.
    
[^69]: 预门控MoE：快速且可扩展混合专家推理的算法和系统共同设计

    Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])

    [http://arxiv.org/abs/2308.12066](http://arxiv.org/abs/2308.12066)

    预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。

    

    在最近几年中，基于transformers的大型语言模型（LLMs）取得了重大进展，其成功源于模型规模的扩大。尽管算法性能很高，但LLMs的计算和存储需求带来了前所未有的挑战。为了解决LLMs的高计算需求，引入了混合专家（MoE）架构，能够在不成比例地扩大计算需求的情况下扩展模型大小。然而，MoE的高存储需求和稀疏专家的动态激活限制了其在实际问题中的适用性。之前的解决方案将MoE的内存占用高的专家参数转移到CPU内存上，但是从CPU迁移已激活的专家到GPU的延迟导致了高性能开销。我们提出的预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。

    Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
    
[^70]: 深层操作符网络在核系统数字孪生技术中的潜力

    Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System. (arXiv:2308.07523v1 [stat.ML])

    [http://arxiv.org/abs/2308.07523](http://arxiv.org/abs/2308.07523)

    深层操作符网络（DeepONet）作为一种强大的替代建模方法，在核系统数字孪生技术中展示出了显著的预测精度和计算效率。然而，挑战仍然存在，包括最佳传感器放置和模型评估。

    

    本研究在核工程的数字孪生系统中引入了深层操作符网络（DeepONet）作为一种强大的替代建模方法。随着核能作为一种碳中和解决方案的重要性不断增加，采用数字孪生技术对于提高核工程应用中的运营效率、安全性和预测能力变得至关重要。DeepONet具有显著的预测精度，优于传统的机器学习方法。通过广泛的基准测试和评估，本研究展示了DeepONet在解决复杂粒子传输问题中的可扩展性和计算效率。通过将函数作为输入数据并使用训练数据构建操作符G，DeepONet能够有效处理多样化和复杂的场景。然而，DeepONet的应用也揭示了与最佳传感器放置和模型评估相关的挑战，这是实际实施中的关键问题。

    This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing 
    
[^71]: 可解释的机器学习以实现掺杂共轭聚合物的高通量电导率优化

    Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers. (arXiv:2308.04103v1 [physics.app-ph])

    [http://arxiv.org/abs/2308.04103](http://arxiv.org/abs/2308.04103)

    提出了一种可解释的机器学习方法，基于吸收光谱来加速测量掺杂聚合物材料的电导率。分类模型能够准确分类电导率大于~25至100 S/cm的样品，回归模型能够精确预测高导电性样品的电导率。该方法经过实验证实了效果。

    

    高通量实验技术和机器学习的结合最近开启了加速材料发现的新时代，使得能够识别具有尖端性能的材料成为可能。然而，某些物理量的测量仍然具有挑战性。具体而言，要实现掺杂聚合物材料中的最佳电导率，需要谨慎的工艺控制、实验和繁琐的测量。我们提出了一种机器学习方法，依赖于易于测量的吸收光谱，加速了测量电导率的工作流程。第一个机器学习模型（分类模型）能够准确地分类电导率大于~25至100 S/cm的样品，达到100%的准确率。对于具有高导电性的子集样品，我们使用了第二个机器学习模型（回归模型）来预测它们的电导率，得到了令人印象深刻的测试R2值为0.984。为了验证这种方法，我们进行了实验来比较在电导率优化方面使用机器学习方法的效果。

    The combination of high-throughput experimentation techniques and machine learning (ML) has recently ushered in a new era of accelerated material discovery, enabling the identification of materials with cutting-edge properties. However, the measurement of certain physical quantities remains challenging to automate. Specifically, meticulous process control, experimentation and laborious measurements are required to achieve optimal electrical conductivity in doped polymer materials. We propose a ML approach, which relies on readily measured absorbance spectra, to accelerate the workflow associated with measuring electrical conductivity. The first ML model (classification model), accurately classifies samples with a conductivity >~25 to 100 S/cm, achieving a maximum of 100% accuracy rate. For the subset of highly conductive samples, we employed a second ML model (regression model), to predict their conductivities, yielding an impressive test R2 value of 0.984. To validate the approach, we
    
[^72]: PePNet: 一种支持罕见重负载的周期性感知工作负载预测网络

    PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload. (arXiv:2308.01917v1 [cs.DC])

    [http://arxiv.org/abs/2308.01917](http://arxiv.org/abs/2308.01917)

    PePNet是一种支持罕见重负载的工作负载预测网络，通过周期性感知机制和融合多尺度序列学习的能力提高了整体特别是重负载的准确性。

    

    云提供商可以从准确的工作负载预测中获得巨大的好处。然而，云服务器的工作负载高度变化，有时会发生重负载突发事件，这使得工作负载预测具有挑战性。目前有两种主要的工作负载预测方法：统计方法和基于神经网络的方法。前者依赖于强大的数学假设，当预测高度变化的工作负载时，其准确性较低。而后者在整体准确性上更高，但容易受到重负载和常见负载之间数据不平衡的影响，这会影响神经网络模型对重负载的预测准确性。无论是统计方法的整体不准确性还是基于神经网络的模型对重负载的不准确性都会导致服务级别协议的违规。因此，我们提出了PePNet来提高整体特别是重负载预测的准确性。它具有两个独特的特点：周期性感知机制和融合多尺度序列学习的能力。

    Cloud providers can greatly benefit from accurate workload prediction. However, the workload of cloud servers is highly variable, with occasional heavy workload bursts. This makes workload prediction challenging.  There are mainly two categories of workload prediction methods: statistical methods and neural-network-based ones. The former ones rely on strong mathematical assumptions and have reported low accuracy when predicting highly variable workload. The latter ones offer higher overall accuracy, yet they are vulnerable to data imbalance between heavy workload and common one. This impairs the prediction accuracy of neural network-based models on heavy workload.  Either the overall inaccuracy of statistic methods or the heavy-workload inaccuracy of neural-network-based models can cause service level agreement violations.  Thus, we propose PePNet to improve overall especially heavy workload prediction accuracy. It has two distinctive characteristics:  (i) A Periodicity-Perceived Mecha
    
[^73]: 零样本和少样本情况下应用于临床和生物医学任务的指导细调大型语言模型的研究

    A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])

    [http://arxiv.org/abs/2307.12114](http://arxiv.org/abs/2307.12114)

    这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。

    

    我们评估了四种最先进的指导细调大型语言模型（LLM）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在13个实际世界的临床和生物医学自然语言处理（NLP）任务中的表现，例如命名实体识别（NER）、问答（QA）、关系抽取（RE）等。我们的综合结果表明，在大多数任务的零样本和少样本情况下，评估的LLM开始接近最先进模型的性能，尤其对于QA任务表现得特别好，即使它们之前没有见过这些任务的示例。然而，我们观察到分类和关系抽取任务的表现低于特定训练于医学领域的模型（如PubMedBERT）可以达到的水平。最后，我们注意到没有一个LLM在所有研究任务上都胜过其他模型，有些模型更适合于特定的任务。

    We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
    
[^74]: 神经算法推理器的潜在空间表示

    Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])

    [http://arxiv.org/abs/2307.08874](http://arxiv.org/abs/2307.08874)

    这项工作对神经算法推理器中执行算法时产生的潜在空间结构进行了详细分析，并提出了解决两种故障模式的方法。通过使用softmax聚合器解决分辨率丧失问题，以及衰减潜在空间来处理超出范围的值，这些改变在标准CLRS-30基准测试中大多数算法上实现了改进。

    

    神经算法推理（NAR）是一个研究领域，专注于设计能够可靠地捕捉经典计算的神经架构，通常通过学习执行算法来实现。典型的方法是依赖于图神经网络（GNN）架构，它们将输入编码为高维潜在空间，在算法执行期间反复转换。在这项工作中，我们对GNN在执行算法时导致的潜在空间结构进行了详细分析。我们发现了两种可能的故障模式：（i）分辨率丧失，使得难以区分相似的值；（ii）无法处理训练期间未观察到的值范围之外的值。我们提出通过依赖softmax聚合器来解决第一个问题，并建议衰减潜在空间以处理超出范围的值。我们展示了这些变化在使用最先进的方法时，在标准CLRS-30基准测试中大多数算法上的改进。

    Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art
    
[^75]: 通过生成扩散模型实现的合成拉格朗日湍流

    Synthetic Lagrangian Turbulence by Generative Diffusion Models. (arXiv:2307.08529v1 [physics.flu-dyn] CROSS LISTED)

    [http://arxiv.org/abs/2307.08529](http://arxiv.org/abs/2307.08529)

    该论文提出了一种基于机器学习的方法，通过生成扩散模型在高雷诺数下合成拉格朗日湍流，成功实现了粒子轨迹的统计和拓扑性质的准确重现。

    

    拉格朗日湍流是涉及工程、生物流体、大气、海洋和天体物理领域中的分散和混合物理的应用和基础性问题。尽管过去三十年进行了卓越的理论、数值和实验研究，但没有现有模型能够忠实地重现湍流中的粒子轨迹所展示的统计和拓扑性质。我们提出了一种基于最先进的扩散模型的机器学习方法，以在高雷诺数下生成三维湍流中的单粒子轨迹，从而避免了获取可靠的拉格朗日数据所需的直接数值模拟或实验。我们的模型表明，它能够定量地重现整个时间尺度范围内的所有相关统计基准，包括速度增量的尾部分布、异常幂律和增强

    Lagrangian turbulence lies at the core of numerous applied and fundamental problems related to the physics of dispersion and mixing in engineering, bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional theoretical, numerical, and experimental efforts conducted over the past thirty years, no existing models are capable of faithfully reproducing statistical and topological properties exhibited by particle trajectories in turbulence. We propose a machine learning approach, based on a state-of-the-art Diffusion Model, to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, thereby bypassing the need for direct numerical simulations or experiments to obtain reliable Lagrangian data. Our model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of
    
[^76]: Capafoldable: 具有电容传感能力的自追踪可折叠智能纺织品

    Capafoldable: self-tracking foldable smart textiles with capacitive sensing. (arXiv:2307.05370v1 [cs.HC])

    [http://arxiv.org/abs/2307.05370](http://arxiv.org/abs/2307.05370)

    这项工作提出了一种创新的自追踪可折叠智能纺织品，通过结合折叠织物结构和电容传感，利用深度学习技术来检测结构运动。实验结果显示，我们的方法可以从电容信号中准确重构出片段的几何形状。

    

    折叠是一种独特的结构技术，可以使平面材料具有运动或三维力学特性。基于纺织品的电容传感已经被证明对导电纺织品的几何形变和相对运动非常敏感。在这项工作中，我们提出了一种创新的自追踪可折叠智能纺织品，将折叠织物结构和电容传感相结合，利用先进的传感电路和深度学习技术来检测结构运动。我们创建了两种折叠模式，手风琴和齿形，每种模式中都有两种布局的电容传感器，以热粘附的导电纺织品片的形式存在。在手动移动折叠模式的片段的实验中，我们开发了深度神经网络来学习和重构片段的视觉跟踪形状。通过我们的方法，可以从电容信号中重构定义片段形状的几何原语，R-squared值可达95％，22.5cm长片段的追踪误差为1cm。

    Folding is an unique structural technique to enable planer materials with motion or 3D mechanical properties. Textile-based capacitive sensing has shown to be sensitive to the geometry deformation and relative motion of conductive textiles. In this work, we propose a novel self-tracking foldable smart textile by combining folded fabric structures and capacitive sensing to detect the structural motions using state-of-the-art sensing circuits and deep learning technologies. We created two folding patterns, Accordion and Chevron, each with two layouts of capacitive sensors in the form of thermobonded conductive textile patches. In an experiment of manually moving patches of the folding patterns, we developed deep neural network to learn and reconstruct the vision-tracked shape of the patches. Through our approach, the geometry primitives defining the patch shape can be reconstructed from the capacitive signals with R-squared value of up to 95\% and tracking error of 1cm for 22.5cm long pa
    
[^77]: 基于优化的学习用于卡车运输服务网络中的动态负载规划

    Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])

    [http://arxiv.org/abs/2307.04050](http://arxiv.org/abs/2307.04050)

    本文为快递运营中的负载规划问题提出了一种基于优化的学习方法，全面考虑了负载和流程规划的挑战，并开发了一个决策支持工具。研究发现在网络中存在大量的对称性，导致优化求解器返回不同的解决方案，降低了规划人员对优化求解的信任度。

    

    负载规划问题是快递运营中服务网络设计的一个关键挑战：它决定了在终端之间如何在时间上分配多少辆拖车（或负载）进行派遣。另一个关键挑战是确定一个流程计划，它指定了如何将包裹体积分配给计划的负载。本文考虑到了动态负载规划问题（DLPP），它同时考虑了负载和流程规划的挑战，以在操作日之前随着需求预测的变化而调整负载和流程。本文旨在开发一个决策支持工具，为网络中各个终端的规划人员提供决策支持。本文将DLPP形式化为一个MIP，并证明它在每个商品都可以通过主路径和备用路径进行路由的网络中有大量的对称性。因此，优化求解器可能会对密切相关的问题返回根本不同的解决方案，使规划人员感到困惑，降低对优化求解的信任度。

    The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimiz
    
[^78]: 平滑边缘：利用Hadamard超参数化在稀疏正则化的平滑优化中的一般框架

    Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])

    [http://arxiv.org/abs/2307.03571](http://arxiv.org/abs/2307.03571)

    本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。

    

    本文介绍了一种用于（结构化）稀疏正则化问题中的$\ell_q$和$\ell_{p,q}$正则化的平滑方法。这些非平滑且可能非凸的问题的优化通常依赖于专门的过程。相比之下，我们的一般框架与主流的一阶优化方法（如随机梯度下降和加速变体）兼容，无需任何修改。这是通过平滑优化转移实现的，其中选定模型参数的超参数化使用Hadamard乘积和惩罚的改变。在超参数问题中，通过用替代参数进行平滑和凸性的$\ell_2$正则化，能够在原始参数化中引入非平滑和非凸性的$\ell_q$或$\ell_{p,q}$正则化。我们证明了我们的方法不仅能够得到匹配的全局最小值，还能得到等价的局部最小值。这在非凸稀疏正则化中尤其有用，因为在这种情况下找到全局最小值非常困难。

    This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
    
[^79]: 使用速率函数理解插值区间的泛化

    Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])

    [http://arxiv.org/abs/2306.10947](http://arxiv.org/abs/2306.10947)

    本文利用大偏差理论，提出一种基于函数的平滑模型特征描述方法，解释了为什么一些插值器有很好的泛化能力以及现代学习技术为什么能够找到它们。

    

    本文基于大偏差理论的基本原理，提出了一种模型平滑度的新特征描述方法。与以往的工作不同，以往的工作通常用实数值（如权重范数）来表征模型的平滑度，我们表明可以用简单的实值函数来描述平滑度。基于模型平滑度的这一概念，我们提出了一个统一的理论解释，为什么一些插值器表现出非常好的泛化能力，以及为什么广泛使用的现代学习技术（如随机梯度下降，$\ell_2$-规范化，数据增强，不变的架构和超参数化）能够找到它们。我们得出的结论是，所有这些方法都提供了互补的过程，这些过程使优化器偏向于更平滑的插值器，而根据这种理论分析，更平滑的插值器是具有更好的泛化误差的插值器。

    In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
    
[^80]: 使用动态贝叶斯网络进行加密货币价格方向因果特征工程

    Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])

    [http://arxiv.org/abs/2306.08157](http://arxiv.org/abs/2306.08157)

    本文提出了一种基于动态贝叶斯网络的方法，来预测加密货币价格方向，以帮助投资者做出明智的投资决策。

    

    加密货币在各个领域，特别是金融和投资领域中越来越受到关注。其独特的区块链相关特性，如隐私、去中心化和不可追踪性，部分原因是其受欢迎的原因。然而，由于加密货币价格的波动性和不确定性，加密货币仍然是一种高风险投资。本文提出了一个动态贝叶斯网络（DBN）方法，可以在多元设置下模拟复杂系统，以预测五种流行加密货币的价格运动方向，以解决这个问题。

    Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
    
[^81]: 关于实现最优对抗测试误差的研究

    On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])

    [http://arxiv.org/abs/2306.07544](http://arxiv.org/abs/2306.07544)

    本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。

    

    本文首先阐述了最优对抗预测器的各种基本特性：最优对抗凸预测器的结构、将对抗凸损失与对抗0-1损失相关联的界限以及连续预测器可以在凸和0-1损失下无限接近最优对抗误差。本文还将这些结果与对抗训练在初始化附近的新Rademacher复杂度界限相结合，证明了对于一般的数据分布和扰动集，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。相比之下，先前的理论工作只考虑了特定的数据分布或仅提供了训练误差的保证。

    We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.
    
[^82]: PriSampler: 缓解扩散模型的属性推断问题

    PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])

    [http://arxiv.org/abs/2306.05208](http://arxiv.org/abs/2306.05208)

    本文是第一项针对扩散模型属性推断攻击的隐私研究，攻击者将从模型中提取训练集的敏感全局属性，结果表明各种扩散模型及其取样器容易受到攻击的影响。

    

    扩散模型在数据合成方面取得了巨大成功。这些成功也促使扩散模型应用于敏感数据，例如人脸数据，但这可能带来严重的隐私问题。本文系统地介绍了针对扩散模型的属性推断攻击的第一项隐私研究，其中攻击者旨在从扩散模型中提取训练集的敏感全局属性，例如某些敏感属性的训练数据比例。具体而言，我们考虑了最实用的攻击场景：攻击者只能获得合成数据。在现实场景下，我们对不同类型的取样器和扩散模型进行了属性推断攻击的评估。广泛的评估范围表明，各种扩散模型及其取样器都容易受到属性推断攻击的影响。此外，对现成的预训练扩散模型进行一项案例研究也展示了攻击的实际效果。

    Diffusion models have been remarkably successful in data synthesis. Such successes have also driven diffusion models to apply to sensitive data, such as human face data, but this might bring about severe privacy concerns. In this work, we systematically present the first privacy study about property inference attacks against diffusion models, in which adversaries aim to extract sensitive global properties of the training set from a diffusion model, such as the proportion of the training data for certain sensitive properties. Specifically, we consider the most practical attack scenario: adversaries are only allowed to obtain synthetic data. Under this realistic scenario, we evaluate the property inference attacks on different types of samplers and diffusion models. A broad range of evaluations shows that various diffusion models and their samplers are all vulnerable to property inference attacks. Furthermore, one case study on off-the-shelf pre-trained diffusion models also demonstrates
    
[^83]: Coeditor：利用上下文变化进行多轮代码自动编辑

    Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing. (arXiv:2305.18584v1 [cs.SE])

    [http://arxiv.org/abs/2305.18584](http://arxiv.org/abs/2305.18584)

    Coeditor开发了一个多轮代码自动编辑模型，利用同一代码库中的最近变化来预测对代码区域的编辑，表现出更高的准确率。

    

    开发人员经常花费大量时间来维护和重构现有代码。然而，大多数关于生成模型的先前工作都仅关注于创建新代码，忽略了对编辑现有代码的独特要求。在这项工作中，我们探索了一个多轮代码自动编辑的设置，旨在基于同一代码库中的最近变化来预测对代码区域的编辑。我们的模型Coeditor是一个经过细化的CodeT5模型，具有专门设计用于代码编辑任务的增强功能。我们使用行差异格式对代码更改进行编码，并采用静态分析来形成大型定制模型上下文，以确保适当的预测信息。我们从1650个开源Python项目的提交历史中收集了一个代码编辑数据集用于训练和评估。在简化的单轮单编辑任务中，Coeditor的准确性显著优于最佳的代码完成方法，准确率近乎翻倍，即使使用的模型更小。

    Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, neglecting the unique requirements of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned CodeT5 model with enhancements specifically designed for code editing tasks. We encode code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms the best code completion approach -- nearly doubling its exact-match accuracy, despite using a much smaller model 
    
[^84]: 随机梯度 Langevin 扩散中的子采样误差

    Subsampling Error in Stochastic Gradient Langevin Diffusions. (arXiv:2305.13882v1 [stat.ML])

    [http://arxiv.org/abs/2305.13882](http://arxiv.org/abs/2305.13882)

    该研究分析了随机梯度 Langevin 动力学在大型数据环境下使用子采样产生的误差。研究者提出了一种新的连续时间马尔可夫过程，该过程切换数据子集并可用于扩散子采样 MCMC 方法，并证明了该方法的收敛性。

    

    随机梯度 Langevin 动力学 (SGLD) 通常用于大规模数据的统计学习中近似贝叶斯后验分布。与许多常规马尔可夫链蒙特卡罗 (MCMC) 算法不同，SGLD 对于后验分布不是稳定的。它有两个错误来源：第一个错误是由 Euler-Maruyama 离散化 Langevin 扩散过程引入的，第二个错误来自于数据子采样，这使得它适用于大规模数据环境。在本文中，我们考虑了 SGLD 的理想化版本，以分析该方法的纯子采样误差，我们可以将其视为基于扩散的子采样 MCMC 方法的最佳情况误差。事实上，我们引入并研究了随机梯度 Langevin 扩散 (SGLDiff)，这是一个连续时间马尔可夫过程，它遵循与数据子集相应的 Langevin 扩散，并在指数等待时间后切换该数据子集。在此，我们证明了瓦瑟斯坦距离 (Was)

    The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler--Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method's pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show that the Was
    
[^85]: AnyPredict: 表格预测的基础模型

    AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])

    [http://arxiv.org/abs/2305.12081](http://arxiv.org/abs/2305.12081)

    本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。

    

    基础模型是在大规模数据上预先训练的模型，可以在许多下游任务中表现良好。它们在自然语言处理和计算机视觉方面取得了显著的成功。然而，这种模型在表格预测任务中的使用受到限制，主要问题包括 (1) 缺乏大规模和多样化的带有标准标签的表格数据集，以及 (2) 不同领域之间的模式不匹配和预测目标的异质性。本文提出了一种方法，用于构建基于 AnyPredict 的表格预测基础模型的大规模训练数据，包括领域内和广泛的领域外数据集。该方法使用数据引擎，利用大型语言模型 (LLM) 来整合表格样本，克服了不同模式表格之间的障碍，并使用“学习，注释和审计”流程将领域外数据与目标任务对齐。扩展的训练数据使预训练的 AnyPredict 能够支持每个表格领域。

    Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
    
[^86]: 对齐扩散薛定谔桥

    Aligned Diffusion Schr\"odinger Bridges. (arXiv:2302.11419v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11419](http://arxiv.org/abs/2302.11419)

    本文提出一种新的算法框架，首次能够在考虑对齐数据的同时解决扩散薛定谔桥问题，相对于之前的方法，有更简单、方差更低的训练过程，并使用原则性的正则化方案，在实验中取得了显着的改进。

    

    最近，通过在不同时间点的边际观察恢复随机动态的扩散薛定谔桥(DSB)已经成为一个强大的框架。尽管有许多成功的应用，但现有算法尚未利用多种生物现象中自然出现的对齐数据结构来解决DSB。在本文中，我们提出了一个新的算法框架，首次在考虑对齐数据的同时解决了DSB问题。我们的方法依靠两个二十年历史的思想结合起来：经典的薛定谔桥理论和Doob的$h$-变换。相对于之前的方法，我们的方法导致训练过程更简单，方差更低，并且我们还使用了原则性的正则化方案。这最终在合成和真实数据的实验中导致了显着的改进，包括刚性蛋白质对接和细胞分化过程的时间演化等任务。

    Diffusion Schr\"odinger bridges (DSB) have recently emerged as a powerful framework for recovering stochastic dynamics via their marginal observations at different time points. Despite numerous successful applications, existing algorithms for solving DSBs have so far failed to utilize the structure of aligned data, which naturally arises in many biological phenomena. In this paper, we propose a novel algorithmic framework that, for the first time, solves DSBs while respecting the data alignment. Our approach hinges on a combination of two decades-old ideas: The classical Schr\"odinger bridge theory and Doob's $h$-transform. Compared to prior methods, our approach leads to a simpler training procedure with lower variance, which we further augment with principled regularization schemes. This ultimately leads to sizeable improvements across experiments on synthetic and real data, including the tasks of rigid protein docking and temporal evolution of cellular differentiation processes.
    
[^87]: DTAAD: 双重TCN-Attention网络用于多变量时间序列数据的异常检测

    DTAAD: Dual Tcn-Attention Networks for Anomaly Detection in Multivariate Time Series Data. (arXiv:2302.10753v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10753](http://arxiv.org/abs/2302.10753)

    这项研究提出了一种基于Transformer和双重TCN-Attention网络的DTAAD模型，用于多变量时间序列数据的异常检测和诊断。通过集成设计和引入缩放方法和反馈机制，该模型实现了快速准确定位异常，并提高了预测精度和扩大了相关差异。

    

    异常检测技术能够对多变量时间序列数据进行有效的异常检测和诊断，对当今工业应用具有重要意义。然而，由于缺乏异常标签、数据的高维复杂性、实际硬件的内存瓶颈以及快速推理的需求，建立一个能够迅速准确定位的异常检测系统是一个具有挑战性的问题。本文提出了一种基于Transformer和双重时间卷积网络（TCN）的异常检测和诊断模型--DTAAD。我们的整体模型是一个集成设计，在此基础上，自回归模型（AR）与自编码器（AE）结构相结合，并引入了缩放方法和反馈机制，以提高预测精度和扩大相关差异。我们构建的双重TCN-Attention网络（DTA）在基准实验中仅使用了单层Transformer编码器。

    Anomaly detection techniques enable effective anomaly detection and diagnosis in multi-variate time series data, which are of major significance for today's industrial applications. However, establishing an anomaly detection system that can be rapidly and accurately located is a challenging problem due to the lack of outlier tags, the high dimensional complexity of the data, memory bottlenecks in the actual hardware, and the need for fast reasoning. We have proposed an anomaly detection and diagnosis model -- DTAAD in this paper, based on Transformer, and Dual Temporal Convolutional Network(TCN). Our overall model will be an integrated design in which autoregressive model(AR) combines autoencoder(AE) structures, and scaling methods and feedback mechanisms are introduced to improve prediction accuracy and expand correlation differences. Constructed by us, the Dual TCN-Attention Network (DTA) only uses a single layer of Transformer encoder in our baseline experiment, that belongs to an u
    
[^88]: TAMUNA: 带有局部训练、压缩和部分参与的双倍加速联邦学习

    TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation. (arXiv:2302.09832v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09832](http://arxiv.org/abs/2302.09832)

    TAMUNA是首个联合利用网络压缩和少量通信配合加速分布式梯度下降算法，并允许部分参与的算法。

    

    在联邦学习中，大量用户合作学习全局模型。他们交替进行本地计算和与远程服务器的通信。通信是该设置中的主要瓶颈，它可以慢且昂贵。为了减少通信负载并加速分布式梯度下降，使用两种策略很受欢迎：1）更少地通信，即在通信轮之间执行几个本地计算的迭代；2）传输压缩信息而不是完整维度的矢量。我们提出了TAMUNA，这是第一个分布式优化和联邦学习算法，它联合利用这两种策略，同时允许部分参与。TAMUNA以线性速度收敛到精确解决方案。

    In federated learning, a large number of users collaborate to learn a global model. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. In addition to communication-efficiency, a robust algorithm should allow for partial participation, the desirable feature that not all clients need to participate to every round of the training process. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. We propose TAMUNA, the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and allows for partial participation. TAMUNA converges linearly to an exact solution in the stron
    
[^89]: 自监督的时间图学习与时间和结构强度对齐

    Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment. (arXiv:2302.07491v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07491](http://arxiv.org/abs/2302.07491)

    该论文提出了一种自监督的时间图学习方法，通过提取时间和结构信息来学习更具信息量的节点表示。

    

    时间图学习旨在生成用于基于图的任务的高质量表示，同时包含动态信息，最近引起了越来越多的关注。与静态图不同，时间图通常以连续时间上的节点交互序列的形式组织，而不是邻接矩阵。大多数时间图学习方法通过在时间上组合历史信息来建模当前的交互。然而，这些方法仅考虑了一阶时间信息，而忽视了重要的高阶结构信息，导致性能不佳。为了解决这个问题，我们提出了一种自监督方法，名为S2T，通过提取时间和结构信息来学习更具信息量的节点表示。

    Temporal graph learning aims to generate high-quality representations for graph-based tasks along with dynamic information, which has recently drawn increasing attention. Unlike the static graph, a temporal graph is usually organized in the form of node interaction sequences over continuous time instead of an adjacency matrix. Most temporal graph learning methods model current interactions by combining historical information over time. However, such methods merely consider the first-order temporal information while ignoring the important high-order structural information, leading to sub-optimal performance. To solve this issue, by extracting both temporal and structural information to learn more informative node representations, we propose a self-supervised method termed S2T for temporal graph learning. Note that the first-order temporal information and the high-order structural information are combined in different ways by the initial node representations to calculate two conditional 
    
[^90]: 增强潜空间贝叶斯优化中的探索能力

    Enhancing Exploration in Latent Space Bayesian Optimization. (arXiv:2302.02399v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02399](http://arxiv.org/abs/2302.02399)

    本文提出了一种新的方法来提高潜空间贝叶斯优化（LSBO）的探索能力。方法包括潜在一致性感知获取函数（LCA-AF）和增加一致性点的潜空间生成方法（LCA-VAE），将它们结合起来形成了LCA-LSBO。实验证明LCA-LSBO在图像生成和全新的化学设计任务中表现出改进的性能。

    

    潜空间贝叶斯优化（LSBO）将生成模型（通常是变分自编码器）与贝叶斯优化相结合，以生成感兴趣的全新对象。然而，由于贝叶斯优化和变分自编码器之间的目标不匹配，导致了LSBO面临挑战和推广能力的减弱。本文提出了增强LSBO效率并克服这一挑战的新思路。首先，我们引入了潜在的一致性和不一致性的概念，这是LSBO中一个关键的问题，起源于BO-VAE之间的不匹配。为了解决这个问题，我们提出了潜在的一致意识获取函数（LCA-AF），利用LSBO中的一致性区域。此外，我们提出了LCA-VAE，一种新的VAE方法，它生成具有增加的一致性点的潜空间，提高了BO的推广能力。结合LCA-VAE和LCA-AF，我们发展了LCA-LSBO。实验评估证实了LCA-LSBO在图像生成和全新的化学设计任务中的改进性能。

    Latent Space Bayesian Optimization (LSBO) combines generative models, typically Variational Autoencoders (VAE), with Bayesian Optimization (BO) to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor extrapolation capabilities. In this paper, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the BO-VAE mismatch. To address this, we propose the Latent Consistent Aware-Acquisition Function (LCA-AF) that leverages consistent regions in LSBO. Additionally, we present LCA-VAE, a novel VAE method that generates a latent space with increased consistent points, improving BO's extrapolation capabilities. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Experimental evaluations validate the improved performance of LCA-LSBO in image generation and de-novo chem
    
[^91]: 基于图的时间序列异常检测：综述(arXiv：2302.00058v2 [cs.LG]更新)

    Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00058](http://arxiv.org/abs/2302.00058)

    本文综述了基于图的时间序列异常检测，主要探讨了图表示学习的潜力和最先进的图异常检测技术在时间序列中的应用。

    

    随着技术的发展，许多系统持续收集大量时间序列数据，如电子商务、网络安全、车辆维护和医疗监测等领域，时间序列异常检测已成为重要的任务。但由于需要同时考虑变量内部和变量间的依赖性，这一任务非常具有挑战性。近年来，基于图的方法在解决该领域的难题方面取得了重要进展。本综述全面而最新地回顾了基于图的时间序列异常检测(G-TSAD)。首先探讨了图表示学习在时间序列数据中的巨大潜力，然后在时间序列背景下回顾了最先进的图异常检测技术，并讨论了它们的优点和缺点。最后，讨论了这些技术如何应用于实际系统中。

    With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
    
[^92]: 通过元学习在哈密顿流形中识别普遍的神经表示

    Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01168](http://arxiv.org/abs/2212.01168)

    通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。

    

    最近物理学中深度学习的进展集中在通过将物理先验或归纳偏见引入神经网络来发现目标系统的共享表示。然而，这些方法特定于系统，不允许轻松适应由不同物理法则驱动的新物理系统。例如，训练于质点弹簧系统的神经网络无法准确预测双体系统或任何具有不同物理法则的系统的行为。在本研究中，我们使用图神经网络模拟我们的系统，并采用元学习算法使模型在一系列任务中积累经验，并使其适应新的物理系统。我们的方法旨在学习跨各种哈密顿流形的通用表示，这是哈密顿系统数据分布的共同特征。我们使用由不同系统组成的数据集训练模型，每个系统都有其自身固有的动力学，并评估其性能。

    Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
    
[^93]: 研究强化学习智能体在个性化任务中的策略熵

    Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks. (arXiv:2211.11869v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11869](http://arxiv.org/abs/2211.11869)

    研究了个性化任务中强化学习智能体的策略熵，并发现策略优化智能体在训练过程中往往具有低熵策略，然而Q学习智能体对此影响较小，通常保持高熵策略。

    

    本文着重研究了强化学习系统在个性化环境中的行为，并详细描述了不同学习算法所关联的策略熵的差异。我们证明了在训练过程中，策略优化智能体往往具有低熵策略，实际上导致智能体优先考虑某些动作而避免其他动作。相反地，我们也表明了Q学习智能体对这种行为的影响要小得多，并且通常在训练过程中保持高熵策略，这在实际应用中往往更可取。我们提供了各种数值实验以及理论上的证明，以表明这些熵差异是由所采用的学习类型所导致的。

    This effort is focused on examining the behavior of reinforcement learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized. We demonstrate that Policy Optimization agents often possess low-entropy policies during training, which in practice results in agents prioritizing certain actions and avoiding others. Conversely, we also show that Q-Learning agents are far less susceptible to such behavior and generally maintain high-entropy policies throughout training, which is often preferable in real-world applications. We provide a wide range of numerical experiments as well as theoretical justification to show that these differences in entropy are due to the type of learning being employed.
    
[^94]: 异质因果效应估计中模型选择的实证分析

    Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation. (arXiv:2211.01939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01939](http://arxiv.org/abs/2211.01939)

    本文在二元治疗条件下条件平均处理效应估计的场景中，对因果推断的模型选择问题进行了实证分析，利用最新的生成建模进展，提出了新的度量方法，证明了新的模型选择策略的有效性。

    

    我们研究了因果推断中的模型选择问题，特别是针对二元治疗条件下条件平均处理效应（CATE）估计的情况。与机器学习中的模型选择不同，由于我们无法观察到任何数据点的反事实潜在结果，因此没有完美的交叉验证模型。为此，文献中提出了各种代理度量方法，这些方法取决于从观察到的数据中估计的辅助干扰模型（倾向性得分模型、结果回归模型）。然而，这些度量方法的有效性仅在我们可以访问反事实数据的合成数据集上进行了研究。我们进行了广泛的实证分析，以评估文献中介绍的这些度量方法以及本研究中介绍的新方法的性能，在实现多个逼真数据集的最新生成建模进展基础上进行。我们的分析表明了新的模型选择策略的出现。

    We study the problem of model selection in causal inference, specifically for the case of conditional average treatment effect (CATE) estimation under binary treatments. Unlike model selection in machine learning, there is no perfect analogue of cross-validation as we do not observe the counterfactual potential outcome for any data point. Towards this, there have been a variety of proxy metrics proposed in the literature, that depend on auxiliary nuisance models estimated from the observed data (propensity score model, outcome regression model). However, the effectiveness of these metrics has only been studied on synthetic datasets as we can access the counterfactual data for them. We conduct an extensive empirical analysis to judge the performance of these metrics introduced in the literature, and novel ones introduced in this work, where we utilize the latest advances in generative modeling to incorporate multiple realistic datasets. Our analysis suggests novel model selection strate
    
[^95]: 数据选择：一种令人惊讶的有效且通用的构建小型可解释模型的原则。

    Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models. (arXiv:2210.03921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03921](http://arxiv.org/abs/2210.03921)

    数据选择是一种令人惊讶的有效且通用的构建小型可解释模型的策略，它通过学习训练分布而非测试分布的数据，提高了传统基准模型的准确性，并在多个任务中展现出竞争力。

    

    我们提供了令人信服的实证证据，证明了一种构建准确小型模型的有效且通用的策略。这种模型对于可解释性具有吸引力，并且在资源受限的环境中也有用途。该策略是学习训练分布而不是使用测试分布的数据。分布学习算法不是这项工作的贡献；我们强调这种简单策略在各种任务上的广泛适用性，并且基于严格的实证结果，这些结果是我们的贡献。我们将其应用于以下任务：（1）构建聚类解释树，（2）基于原型的分类，以及（3）使用随机森林进行分类，并且展示了它提高了弱传统基准的准确性，使它们令人惊讶地与专业的现代技术相竞争。此策略也适用于模型大小的概念。在前两个任务中，模型大小通过树中叶子节点的数量来确定。

    We present convincing empirical evidence for an effective and general strategy for building accurate small models. Such models are attractive for interpretability and also find use in resource-constrained environments. The strategy is to learn the training distribution instead of using data from the test distribution. The distribution learning algorithm is not a contribution of this work; we highlight the broad usefulness of this simple strategy on a diverse set of tasks, and as such these rigorous empirical results are our contribution. We apply it to the tasks of (1) building cluster explanation trees, (2) prototype-based classification, and (3) classification using Random Forests, and show that it improves the accuracy of weak traditional baselines to the point that they are surprisingly competitive with specialized modern techniques.  This strategy is also versatile wrt the notion of model size. In the first two tasks, model size is identified by number of leaves in the tree and th
    
[^96]: 关于ELBO收敛到熵和的研究

    On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.03077](http://arxiv.org/abs/2209.03077)

    本论文研究了 ELBO 收敛到熵和的问题，证明了对于一类广泛的生成模型，ELBO 在所有学习的稳定点处都等于一系列熵的和，为无监督学习的学习算法的基本属性提供了深入的洞察。

    

    变分下界（又称ELBO或自由能）是许多经典和新颖的无监督学习算法的核心目标。学习算法可以改变模型参数，使变分下界增加。通常，学习进行到参数收敛到接近学习动态的稳定点值。在本文的理论贡献中，我们证明了（对于一类非常广泛的生成模型），变分下界在所有学习的稳定点处均等于一系列熵的和。对于具有一组潜在变量和一组观测变量的标准机器学习模型，这个和包括三个熵: (A) 变分分布的熵（平均熵），(B) 模型先验分布的负熵和 (C) 可观测分布的（期望）负熵。所得到的结果适用于包括：有限数量的数据点，在学习的任意阶段和各种不同的生成模型等真实条件。本研究为无监督学习的学习算法的基本属性提供了深入洞察，是对优化推理和学习的理论分析的第一步。

    The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many established as well as many novel algorithms for unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases. Learning usually proceeds until parameters have converged to values close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For standard machine learning models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at an
    
[^97]: 早期心脏疾病预测的混合量子分类方法

    Early heart disease prediction using hybrid quantum classification. (arXiv:2208.08882v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.08882](http://arxiv.org/abs/2208.08882)

    本文提出了两种量子机器学习方法，分别适用于高维和低维问题，并在Cleveland和Statlog数据集上实验表明这些方法更适合于早期心脏疾病预测，可获得高达96.43％和97.78％的曲线下面积。

    

    心脏发病率和心脏死亡率的增加明显影响了全球公共健康和世界经济。早期预测对于减少心脏发病率和死亡率至关重要。本文提出了两种量子机器学习方法：混合量子神经网络和混合随机森林量子神经网络，用于早期检测心脏疾病。这些方法在Cleveland和Statlog数据集上应用。结果表明，混合量子神经网络和混合随机森林量子神经网络分别适用于高维和低维问题。混合量子神经网络对异常数据敏感，而混合随机森林对异常数据具有鲁棒性。与不同机器学习方法的比较表明，所提出的量子方法更适合早期心脏疾病预测，在Cleveland和Statlog数据集上分别获得了96.43％和97.78％的曲线下面积。

    The rate of heart morbidity and heart mortality increases significantly which affect the global public health and world economy. Early prediction of heart disease is crucial for reducing heart morbidity and mortality. This paper proposes two quantum machine learning methods i.e. hybrid quantum neural network and hybrid random forest quantum neural network for early detection of heart disease. The methods are applied on the Cleveland and Statlog datasets. The results show that hybrid quantum neural network and hybrid random forest quantum neural network are suitable for high dimensional and low dimensional problems respectively. The hybrid quantum neural network is sensitive to outlier data while hybrid random forest is robust on outlier data. A comparison between different machine learning methods shows that the proposed quantum methods are more appropriate for early heart disease prediction where 96.43% and 97.78% area under curve are obtained for Cleveland and Statlog dataset respect
    
[^98]: 从参与度动态和多学习者重新训练中产生的紧急细分

    Emergent segmentation from participation dynamics and multi-learner retraining. (arXiv:2206.02667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02667](http://arxiv.org/abs/2206.02667)

    该论文通过研究基于数据驱动服务的参与和重新训练动态，发现当学习者和用户子群具有风险减少性质时，唯一的稳定均衡是细分的，将子群分配给单个学习者。功利主义社会最优是一个稳定均衡。

    

    在基于数据驱动的服务中选择参与，往往基于该服务的质量，影响了服务学习和改进的能力。我们研究了当学习者和用户子群都具有风险减少性质时，参与和重新训练的动态生成的情况，其中包括了梯度下降、乘法权重等广泛的更新方法。举个例子，假设个体选择在社交媒体平台上花费时间的比例与每个平台对他们的工作效果成比例。每个平台还会收集其活跃用户的数据，并用梯度步骤更新参数。对于这个例子和我们的一般动态类别，我们展示了唯一的渐近稳定均衡是细分的，将子群分配给单个学习者。在温和的假设下，功利主义社会最优是一个稳定均衡。与先前的工作相反，先前的工作显示重复的风险最小化可能不会对韧性和利益进行任何保证。

    The choice to participate in a data-driven service, often made on the basis of quality of that service, influences the ability of the service to learn and improve. We study the participation and retraining dynamics that arise when both the learners and sub-populations of users are \emph{risk-reducing}, which cover a broad class of updates including gradient descent, multiplicative weights, etc. Suppose, for example, that individuals choose to spend their time amongst social media platforms proportionally to how well each platform works for them. Each platform also gathers data about its active users, which it uses to update parameters with a gradient step. For this example and for our general class of dynamics, we show that the only asymptotically stable equilibria are segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can
    
[^99]: 基于自适应谱聚类的图重构提高同质性

    Restructuring Graph for Higher Homophily via Adaptive Spectral Clustering. (arXiv:2206.02386v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02386](http://arxiv.org/abs/2206.02386)

    本文提出一种新颖的图重构方法，集成于任何类型的图神经网络中，以增加同质性。方法包括自适应谱聚类、密度感知同质性度量方法和基于聚类结果的邻接矩阵重构。

    

    尽管越来越多的文献研究了适用于同质性和异质性图的新型图神经网络，但在将传统图神经网络适应于 less-homophilic 图方面做得很少。虽然传统图神经网络处理 less-homophilic 图的能力受到限制，但仍具有效率高、简单、可解释性好等多个优点。本文提出了一种新颖的图重构方法，可集成到任何类型的图神经网络中，包括传统图神经网络，以发挥现有图神经网络的优点同时减轻其局限性。我们的贡献有三个方面: a) 学习拟合节点标签的自适应谱聚类的 pseudo-eigenvector 权重，b) 提出一种新的密度感知的同质性度量方法，具有良好的标签不平衡性鲁棒性，c) 基于自适应谱聚类的结果重构邻接矩阵，以最大化同质性分数。

    While a growing body of literature has been studying new Graph Neural Networks (GNNs) that work on both homophilic and heterophilic graphs, little has been done on adapting classical GNNs to less-homophilic graphs. Although the ability to handle less-homophilic graphs is restricted, classical GNNs still stand out in several nice properties such as efficiency, simplicity, and explainability. In this work, we propose a novel graph restructuring method that can be integrated into any type of GNNs, including classical GNNs, to leverage the benefits of existing GNNs while alleviating their limitations. Our contribution is threefold: a) learning the weight of pseudo-eigenvectors for an adaptive spectral clustering that aligns well with known node labels, b) proposing a new density-aware homophilic metric that is robust to label imbalance, and c) reconstructing the adjacency matrix based on the result of adaptive spectral clustering to maximize the homophilic scores. The experimental results 
    

