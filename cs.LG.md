# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA.](http://arxiv.org/abs/2304.06027) | 本文提出了一种新方法C-LoRA，用于持续自定制文本到图像扩散模型，并避免了新概念加入后过去相似概念的图像质量下降的问题。 |
| [^2] | [Crowd Counting with Sparse Annotation.](http://arxiv.org/abs/2304.06021) | 本文提出了一种新的稀疏标注方法，可用于人群计数，其可减少标注工作并获取更多多样化信息。同时，我们提出了一个基于点的渐进点匹配网络（PPM），在相同注释的情况下，PPM可表现出更好的性能。 |
| [^3] | [PD-ADSV: An Automated Diagnosing System Using Voice Signals and Hard Voting Ensemble Method for Parkinson's Disease.](http://arxiv.org/abs/2304.06016) | PD-ADSV是一种使用语音信号自动诊断帕金森病的系统，它采用四个机器学习分类器和硬投票集成方法达到最高精度。 |
| [^4] | [An Improved Heart Disease Prediction Using Stacked Ensemble Method.](http://arxiv.org/abs/2304.06015) | 本研究使用机器学习方法构建了一个基于心脏病数据集的心脏病诊断系统，并采用堆叠集成技术，通过组合多个基模型提高了预测准确率，达到了91.8%的高预测准确率。 |
| [^5] | [Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.06011) | 提出了一种新颖的基于模型的MARL算法，BiLL，它学习一个双层潜变量模型，以提高样本效率。该模型在顶层学习全局状态的潜在表示，底层学习每个智能体的潜在表示，并生成潜在轨迹用于策略学习。在SMAC和Flatland环境中，我们的算法在样本效率方面优于现有的无模型和基于模型的基准算法，包括在Super Hard SMAC地图上。 |
| [^6] | [Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing.](http://arxiv.org/abs/2304.06009) | 该论文进行了计算机视觉在物流和仓储中应用领域的文献综述，将文献分为监视和操作两个领域，并指出了未来研究方向和计算机视觉的最新发展。研究结果对于物流从业者也有参考价值。 |
| [^7] | [Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems.](http://arxiv.org/abs/2304.05991) | 本论文提出了在PINNs中使用MLE的方法，消除了超参数调整。通过ODE耦合矩阵的SVD分解降维，增加了PINNs预测的稳定性和泛化能力。 |
| [^8] | [Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings.](http://arxiv.org/abs/2304.05989) | 本文通过无监督学习相似物体交互，诱导物体能力簇，使用新颖的深度感知定性空间表示法构建活动图并进行聚类，从而实现具有不确定交互开放集合的类别无关物体的能力分类。 |
| [^9] | [Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes.](http://arxiv.org/abs/2304.05986) | 本研究使用机器学习模型解决了30天内再入院的问题，并对基于敏感属性的子组进行了公平审计，结果发现该模型在不同组别中表现出差异，强调了建立更好的公正和偏差缓解策略的必要性。 |
| [^10] | [Neural Attention Forests: Transformer-Based Forest Improvement.](http://arxiv.org/abs/2304.05980) | 本论文提出一种名为神经注意森林的新方法，将注意力机制引入到随机森林中，通过神经网络计算获得注意权重，进而解决回归和分类任务。 |
| [^11] | [ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation.](http://arxiv.org/abs/2304.05977) | ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。 |
| [^12] | [Boosted Prompt Ensembles for Large Language Models.](http://arxiv.org/abs/2304.05970) | 本文提出了一种增强提示集成方法，可以使用小型数据集构建集成提示提高大型语言模型的性能，优于单提示输出空间集成和袋装提示空间集成。 |
| [^13] | [Localizing Model Behavior with Path Patching.](http://arxiv.org/abs/2304.05969) | 本文介绍了一种新的技术——路径修补，用于表达和定量测试表明行为被定位到一组路径的一类自然假设。 |
| [^14] | [CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning.](http://arxiv.org/abs/2304.05949) | 本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。 |
| [^15] | [Explicitly Minimizing the Blur Error of Variational Autoencoders.](http://arxiv.org/abs/2304.05939) | 该论文提出了一种新的变分自编码器的重构项公式，它特别针对生成模糊图像进行惩罚，同时仍然最大化建模分布下的ELBO。 实验证明，该方法在三个数据集上优于VAE的几个最近提出的重构损失。 |
| [^16] | [A Phoneme-Informed Neural Network Model for Note-Level Singing Transcription.](http://arxiv.org/abs/2304.05917) | 本文提出了一种在唱歌转录中更准确地找到音符起点的方法，使用了梅尔尺度谱图和音素后验图作为输入，后者是由预先训练的网络生成的，并证明语言特征对起始检测有影响。 |
| [^17] | [Diffusion models with location-scale noise.](http://arxiv.org/abs/2304.05907) | 本研究研究了用于生成模型的不同噪声分布，证明了高斯分布在扩散模型（DMs）中表现最佳。 |
| [^18] | [Evaluating Classifier Confidence for Surface EMG Pattern Recognition.](http://arxiv.org/abs/2304.05898) | 本文研究了在表面肌电图案识别中评估分类器分类的置信度，并发现基于比例混合模型的分类器产生了更准确的置信度。 |
| [^19] | [Towards Understanding How Data Augmentation Works with Imbalanced Data.](http://arxiv.org/abs/2304.05895) | 本文对数据增强在不平衡数据中的作用机理做了全面探究，结果表明增强了分类器发现和补偿不平衡数据中类别特定特征的能力。 |
| [^20] | [Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks.](http://arxiv.org/abs/2304.05894) | 本论文提出了一种扩展混合成员随机块模型来推断动态标签网络的方法，具有很好的鲁棒性和良好的性能，相对于静态标签网络，对数据的训练需求较少。 |
| [^21] | [Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL.](http://arxiv.org/abs/2304.05889) | 本文提出了一种名为MusIK的新算法，利用多步逆运动学实现表示学习和系统探索相结合，达到计算高效和样本复杂度最优的效果，成功应用于富观测强化学习任务中。 |
| [^22] | [FetMRQC: Automated Quality Control for fetal brain MRI.](http://arxiv.org/abs/2304.05879) | FetMRQC 是一种针对胎儿脑 MRI 的自动图像质量评估机器学习框架，通过提取一系列质量指标可以预测专家评分，并能够在大部分数据集上实现准确评估，并发布了一个新的机器学习数据集。 |
| [^23] | [Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data.](http://arxiv.org/abs/2304.05874) | 本文提出了一种自适应门控图卷积网络(AGGCN)，该网络结合卷积节点特征增强和功能连接度量自适应学习图结构，实现了高精度的阿尔茨海默病诊断，并提供了重要的脑区信息。 |
| [^24] | [Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics.](http://arxiv.org/abs/2304.05872) | 本文提出了一种基于图神经网络（GNN）的、用于多智能体交互式海洋废弃物清理的通信机制，使得不同代理之间可以协作竞争并实现收集废弃物的最大化。 |
| [^25] | [Edge-cloud Collaborative Learning with Federated and Centralized Features.](http://arxiv.org/abs/2304.05871) | 本文提出了一种边缘云协作知识转移框架（ECCT），使得共享特征嵌入和预测日志的双向知识传输成为可能，从而实现了个性化增强、模型的异构性、容忍训练的异步性和缓解通信负担的功能。 |
| [^26] | [LMR: Lane Distance-Based Metric for Trajectory Prediction.](http://arxiv.org/abs/2304.05869) | LMR是一种基于车道距离的度量，适用于轨迹预测中的结构化环境，相比传统的欧氏距离度量更准确。 |
| [^27] | [NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs.](http://arxiv.org/abs/2304.05866) | 本文提出了一种名为NoisyTwins的方法，可以通过增强类别嵌入并使用自我监督在W空间中去相关化潜空间来改善大规模长尾数据集上StyleGANs的性能表现，从而在图像生成中保留了类内多样性和一致性。 |
| [^28] | [Scale-Equivariant Deep Learning for 3D Data.](http://arxiv.org/abs/2304.05864) | 该论文提出了一种用于三维数据的尺度等变卷积网络层，可在处理不同尺度的对象和对象部分时提高数据效率和产生更好的结果。 |
| [^29] | [RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction.](http://arxiv.org/abs/2304.05856) | 这篇论文提出了一种新的用于条件驾驶行为预测的基于集合法的轨迹预测方法RESET，可以预测灵活数量的轨迹而不影响运行时间或值域。 |
| [^30] | [Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning.](http://arxiv.org/abs/2304.05839) | 本文提出了一种用于探索决策树空间的强化学习框架来学习紧凑的决策树并最佳化可解释性与性能之间的平衡。 |
| [^31] | [DartsReNet: Exploring new RNN cells in ReNet architectures.](http://arxiv.org/abs/2304.05838) | 本文使用 DARTS 方法对标准 RNN 单元进行改进，提出了用于 ReNet 架构的新的 RNN 单元，有效提高了在 CIFAR-10 和 SVHN 数据集上的分类效果。 |
| [^32] | [A Game-theoretic Framework for Federated Learning.](http://arxiv.org/abs/2304.05836) | 本文提出了一个名为联邦学习安全博弈（FLSG）的博弈论框架，该框架同时考虑到联邦学习的保护者和攻击者的收益，包括计算成本、FL模型效用和隐私泄漏风险，并提出了一个实用算法来近似oracle并保持隐私。研究表明该算法对于预防和检测现实世界中的联邦学习攻击具有有效性。 |
| [^33] | [DiscoGen: Learning to Discover Gene Regulatory Networks.](http://arxiv.org/abs/2304.05823) | 本文中介绍了DiscoGen，它是一种神经网络-based GRN发现方法，可以处理嘈杂的基因表达测量数据并处理干预数据，能够更准确地识别GRN。 |
| [^34] | [GDP nowcasting with artificial neural networks: How much does long-term memory matter?.](http://arxiv.org/abs/2304.05805) | 通过比较四种人工神经网络和动态因子模型对美国GDP季度增长的预测表现，研究发现在平衡经济增长期间，更长的输入序列能够实现更准确的预测，但是这种效果会在不到两年的时间内消失。在经济动荡时期，长期记忆的效果变得明显。 |
| [^35] | [Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series.](http://arxiv.org/abs/2304.05800) | Proximity Forest 2.0是一种新的有效且可扩展的基于相似性的时间序列分类器，优于先前最先进的基于相似性的分类器以及最先进的基于内核、神经网络和混合方法在特定数据集上的表现。 |
| [^36] | [Deep neural network approximation of composite functions without the curse of dimensionality.](http://arxiv.org/abs/2304.05790) | 本文发现了一种高维连续函数，可以用DNN逼近而无需担心“维度诅咒”，方法是将许多特殊函数组合在一起。 |
| [^37] | [Function Space and Critical Points of Linear Convolutional Networks.](http://arxiv.org/abs/2304.05752) | 研究了具有一维卷积层的线性网络的函数空间，分析了网络架构对函数空间的影响并证明了对于步幅大于一且数据一般的架构，该优化问题的非零临界点是函数空间的平滑内部点。 |
| [^38] | [Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation.](http://arxiv.org/abs/2304.05749) | 本研究提出了一种插入式模块——不确定性掩蔽混合（UmmU），它能够在中间层嵌入中进行不确定性估计，进而增强嵌入的不确定性，使其具有更好的泛化性能，从而显著提高了连续时间动态图网络的长期预测性能。 |
| [^39] | [Few-shot Class-incremental Learning for Cross-domain Disease Classification.](http://arxiv.org/abs/2304.05734) | 本研究提出了一种跨域增强约束和跨域数据增强方法，以解决跨领域少样本增量学习问题，该方法在MedMNIST实验中表现出较好的性能。 |
| [^40] | [Dynamic Graph Representation Learning with Neural Networks: A Survey.](http://arxiv.org/abs/2304.05729) | 这篇综述文章总结了近年来动态图表示学习的发展，介绍了动态图的重要性和状态，并回顾了各种动态图学习方法，包括监督、无监督和自监督学习。文章还探讨了动态图学习的当前问题和未来研究方向。 |
| [^41] | [Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks.](http://arxiv.org/abs/2304.05727) | 本文提出了一种新方法，Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化，从而大大减少了对隐藏Clever Hans策略的依赖，并实现了更高的性能。 |
| [^42] | [Localisation of Regularised and Multiview Support Vector Machine Learning.](http://arxiv.org/abs/2304.05655) | 本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。 |
| [^43] | [Self Optimisation and Automatic Code Generation by Evolutionary Algorithms in PLC based Controlling Processes.](http://arxiv.org/abs/2304.05638) | 提出了一种基于遗传算法的方法，用于自我优化复杂过程的系统逻辑。该方法能够解析遗传结果，生成程序代码实现系统，实现了多目标优化问题。 |
| [^44] | [Multi-agent Policy Reciprocity with Theoretical Guarantee.](http://arxiv.org/abs/2304.05632) | 本论文提出了一种新的多智能体策略互惠（PR）框架，能够通过定义邻接空间和设计即插即用模块在不匹配的状态下充分利用交叉智能体策略，提高了性能，同时具有理论保障，能在个体感知奖励的情况下稳定收敛于最优值函数。 |
| [^45] | [SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM.](http://arxiv.org/abs/2304.05622) | 介绍了Segment Any Medical Model (SAMM)，它是用于3D Slicer的SAM的扩展。SAMM在医学图像分割上表现良好，在实时性和通用性方面都有很好的性能，可以推断出掩模。 |
| [^46] | [Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning.](http://arxiv.org/abs/2304.05600) | 该论文通过增加配音来增强跨模态对比学习，结果表明这种方法可以提高音频和视听任务的性能。 |
| [^47] | [Learned multiphysics inversion with differentiable programming and machine learning.](http://arxiv.org/abs/2304.05592) | 本文提出了一个名为SLIM的开源软件框架，将可微编程和机器学习应用于多物理反演技术，提出了一个可伸缩的原型用于从时间连续的跨井地震数据中估计渗透率。 |
| [^48] | [Semantic Feature Verification in FLAN-T5.](http://arxiv.org/abs/2304.05591) | 本研究表明大规模语言模型可以极大地增强传统的语义特征规范验证方法，使之能够捕捉超过人类规范范畴的信息，对于我们理解人类和机器的概念表示具有重要意义。 |
| [^49] | [Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification.](http://arxiv.org/abs/2304.05578) | 本文研究基于主动学习方法的教育对话行为分类，提出了一种新的方法来选择信息样本，并且能够优于随机抽样方法和其他AL方法。 |
| [^50] | [A Predictive Model using Machine Learning Algorithm in Identifying Students Probability on Passing Semestral Course.](http://arxiv.org/abs/2304.05565) | 本研究基于分类数据挖掘技术和决策树算法，提出一个预测模型，可以预测学生在学期初阶段通过所选课程的概率，该模型可靠、准确、可推荐。 |
| [^51] | [On the Adversarial Inversion of Deep Biometric Representations.](http://arxiv.org/abs/2304.05561) | 本研究研究了对抗性背景下，无法直接访问原始数据集和模型的情况下的生物特征的反演，证明在一定程度的攻击背景知识下，黑盒模型生物特征表示仍然很容易被反演。 |
| [^52] | [Taxonomic Class Incremental Learning.](http://arxiv.org/abs/2304.05547) | 研究提出一种基于分类学类别树的学习任务设置方法，命名为分类学类别增量学习（TCIL），使用参数继承方案实现对祖先类别知识对后代类别的增量转移，相对于现有SOTA方法，在CIFAR-100和ImageNet-100上，实现了2％和3％的准确率提升。 |
| [^53] | [MEMA Runtime Framework: Minimizing External Memory Accesses for TinyML on Microcontrollers.](http://arxiv.org/abs/2304.05544) | 我们提出了MEMA框架，可以快速且容易地推导出在微控制器上实现TinyML时最小化外部内存访问的有效运行时，这可以使神经网络基准测试在ARM Cortex-M4上加速高达1.8倍和减少44％的能量消耗。 |
| [^54] | [CLCLSA: Cross-omics Linked embedding with Contrastive Learning and Self Attention for multi-omics integration with incomplete multi-omics data.](http://arxiv.org/abs/2304.05542) | 本文提出了一种基于对比学习和自注意力的深度学习方法CLCLSA，可用于不完整多组学数据的一体化，达到最先进的性能。 |
| [^55] | [Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box.](http://arxiv.org/abs/2304.05527) | 本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。 |
| [^56] | [Understanding Causality with Large Language Models: Feasibility and Opportunities.](http://arxiv.org/abs/2304.05524) | 本文评估了大型语言模型回答因果问题的能力，发现它们可以回答基于已有因果知识的问题，但对于发现新知识或高风险决策任务的高精度要求不足。未来可以通过启用显式和隐式的因果模块以及深度因果感知的LLMs来解决这些问题。 |
| [^57] | [Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model.](http://arxiv.org/abs/2304.05516) | 本文提出了一个个性化隐私保护联邦学习框架，可在保证本地隐私的同时实现强中心隐私保证，并利用Shuffle模型进行隐私扩增。 |
| [^58] | [Training Large Language Models Efficiently with Sparsity and Dataflow.](http://arxiv.org/abs/2304.05511) | 本论文提出了一种利用稀疏性和数据流训练大型语言模型的方法，不仅可以有效利用计算资源，提高计算效率，还能达到与稠密模型类似的结果。 |
| [^59] | [Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability.](http://arxiv.org/abs/2304.05509) | 本文提出了一种称为控制不变集增强强化学习的方法，通过控制不变集的应用提高稳定性保证和采样效率。 |
| [^60] | [DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification.](http://arxiv.org/abs/2304.05503) | DistHD是一种适用于资源受限设备的学习感知动态编码技术，可以有效地识别和重构影响学习质量的维度，以显著较低的维度提高准确性和加速学习过程。 |
| [^61] | [Machine learning for structure-property relationships: Scalability and limitations.](http://arxiv.org/abs/2304.05502) | 该论文提出了一种可扩展的机器学习框架，通过分而治之方法实现线性规模计算，利用局部性假设将系统划分为可以分别解决的子域，从而用于预测多体系统的密集性质和分类相。模型适用性由块大小和在临界点附近训练ML模型的块数决定。 |
| [^62] | [GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery.](http://arxiv.org/abs/2304.05498) | GraphGANFed是一个整合了GCN、GAN和联邦学习的框架，可在不共享数据的情况下高效训练生成对抗网络，助力基于图的分子的药物发现。 |
| [^63] | [Revisiting Single-gated Mixtures of Experts.](http://arxiv.org/abs/2304.05497) | 本文重新审视了单门限专家混合模型(MoE)，提出了一种更加实用的训练方式，且实验证明该模型的效率-准确性权衡具备竞争力并优于非混合基线。 |
| [^64] | [Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training.](http://arxiv.org/abs/2304.05492) | 本研究利用级联指导下的对抗训练方法，增强了串联推荐模型的鲁棒性和准确性，取得了比已有方法更好的结果。 |
| [^65] | [An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery.](http://arxiv.org/abs/2304.05463) | 提出了一个自动导向和质量评估系统，使用改进的 Faster R-CNN 算法来建议脐动脉多普勒流门的位置，并评估多普勒波形质量，有效地填补了经验不足的超声医生的缺陷。 |
| [^66] | [Amortized Learning of Dynamic Feature Scaling for Image Segmentation.](http://arxiv.org/abs/2304.05448) | 该研究提出了一种新的超网络策略，可以根据缩放因子快速生成 Pareto 前沿，无需训练多个网络。该方法能够在使用更少的参数和计算的情况下实现最先进的结果。 |
| [^67] | [Collaborative Machine Learning Model Building with Families Using Co-ML.](http://arxiv.org/abs/2304.05444) | Co-ML是一个基于平板电脑的应用程序，用于协同构建ML图像分类器，可以帮助学习者在合作中发掘新的想法和方法，解决数据表现和多样性等关键问题。 |
| [^68] | [Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation.](http://arxiv.org/abs/2304.05430) | 本研究提出了适用于异构特征的迁移学习方法，在新的目标硬件上通过学习联合神经网络和硬件特征，解决了张量程序生成的自动调整问题。 |
| [^69] | [Differentiable graph-structured models for inverse design of lattice materials.](http://arxiv.org/abs/2304.05422) | 本文提出了一种使用图形表示结构和属性的晶格材料的计算方法，使用可微分传递算法计算机械属性以实现反向设计，进而实现了可扩展的、具有前所未有的结构和功能多样性的晶格材料设计。 |
| [^70] | [Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search.](http://arxiv.org/abs/2304.05405) | 本文综述了最近在不同iable神经架构搜索中的研究进展，提出了一种新的基于挑战的分类法，对DARTS方法的贡献和影响进行了讨论，并探讨了未来的研究方向。 |
| [^71] | [Boosting Cross-task Transferability of Adversarial Patches with Visual Relations.](http://arxiv.org/abs/2304.05402) | VRAP是一种多任务的对抗贴片生成方法，利用场景图将物体识别为基础的对抗贴片组合成更大更复杂的对抗贴片，从而提高对抗贴片在不同任务之间的转移能力。 |
| [^72] | [Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space.](http://arxiv.org/abs/2304.05398) | 本研究基于Bures-Wasserstein空间，开发了(随机)正反高斯变分推断算法，同时分析了算法在不同分布下的收敛保证。 |
| [^73] | [Accelerating Hybrid Federated Learning Convergence under Partial Participation.](http://arxiv.org/abs/2304.05397) | 本文提出了一种适用于部分参与环境下的混合联邦学习框架，其包括自适应采样方法和部分聚合方法，有助于加快模型的收敛速度，从而减少通信成本且不影响模型精度。 |
| [^74] | [SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model.](http://arxiv.org/abs/2304.05396) | SAM.MD是Segment Anything Model的医学图像分割版本，可以进行零样本分割，虽然性能不是最优，但是可以为医学领域的半自动分割工具提供潜在催化剂。 |
| [^75] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^76] | [Controllable Textual Inversion for Personalized Text-to-Image Generation.](http://arxiv.org/abs/2304.05265) | 本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。 |
| [^77] | [r-softmax: Generalized Softmax with Controllable Sparsity Rate.](http://arxiv.org/abs/2304.05243) | 本文提出了一种新的广义Softmax函数r-softmax，可以输出具有可控稀疏度的概率分布，相较于现有的替代方案效果更好，在多标签数据集上表现突出，在预训练转换语言模型的自我注意模块中具有重要应用。 |
| [^78] | [A priori compression of convolutional neural networks for wave simulators.](http://arxiv.org/abs/2304.04964) | 该论文提出了一种在训练神经网络之前压缩卷积层的张量格式方法，通过替换多维核为一维滤波器来减少CNN模型的大小和减少过度拟合，从而提高实时准确预测的速度。 |
| [^79] | [Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis.](http://arxiv.org/abs/2304.04468) | 提出了一种通用的COhort Representation lEarning（CORE）框架，用于增强EHR表示学习，支持针对不同队列的特征进行可解释性分析。 |
| [^80] | [NeRF applied to satellite imagery for surface reconstruction.](http://arxiv.org/abs/2304.04133) | 本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。 |
| [^81] | [Replicability and stability in learning.](http://arxiv.org/abs/2304.03757) | 该论文研究了机器学习中的可复制性和全局稳定性，并证明许多学习任务只能弱化地实现全局稳定性。 |
| [^82] | [Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series.](http://arxiv.org/abs/2304.03069) | 本文提出了一种适用于非平稳时间序列的自适应学生t分布方法，基于方法的一般自适应矩可以使用廉价的指数移动平均值（EMA）来估计参数。 |
| [^83] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^84] | [FedIN: Federated Intermediate Layers Learning for Model Heterogeneity.](http://arxiv.org/abs/2304.00759) | FedIN是一种新型的联邦学习方法，支持异构模型，无需公共数据集。在FedIN中，提取器和分类器的模型结构在所有设备中都相同，而中间层的架构可以根据异构设备的资源容量而变化。IN训练可用于利用特征知识，实验结果表明了该方法在图像分类任务上的有效性。 |
| [^85] | [Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR.](http://arxiv.org/abs/2304.00668) | 本文通过量化不同区域对目标识别的贡献和解释数据偏差和模型偏差对非因果性的影响，提供了改善深度学习在SAR ATR中鲁棒性和泛化能力的重要见解。 |
| [^86] | [A Closer Look at Parameter-Efficient Tuning in Diffusion Models.](http://arxiv.org/abs/2303.18181) | 本文研究了大规模扩散模型的参数效率微调，通过插入小型可学习模块来实现。研究表明，适配器的输入位置是影响下游任务性能的关键因素，而将输入位置放在交叉注意力块之后可获得最佳性能。 |
| [^87] | [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos.](http://arxiv.org/abs/2303.16897) | 该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。 |
| [^88] | [Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field.](http://arxiv.org/abs/2303.16869) | 本文探讨了数据驱动的代理建模在高维物理问题中的应用，提出了一种基于自动化机器学习框架的概率建模方法，以降低计算成本和提高预测准确性和精度。 |
| [^89] | [Projections of Model Spaces for Latent Graph Inference.](http://arxiv.org/abs/2303.11754) | 本文将双曲和球形模型空间的立体投影以及Riemannian隐空间的乘积应用于潜在图推断，实现了与非投影空间相当的性能并提供理论保证。 |
| [^90] | [Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer.](http://arxiv.org/abs/2303.08622) | 本文提出了一种适用于文本引导图像风格迁移中的零样本对比损失方法，可以在不需要额外训练的情况下生成具有相同语义内容的图像。 |
| [^91] | [n-Step Temporal Difference Learning with Optimal n.](http://arxiv.org/abs/2303.07068) | 本文提出了使用SPSA算法求解n步时序差分学习中的最优n值的算法SDPSA，并证明了其收敛性和有效性。 |
| [^92] | [Identification of Systematic Errors of Image Classifiers on Rare Subgroups.](http://arxiv.org/abs/2303.05072) | 该论文提出了一种名为PromptAttack的组合测试方法，通过在被低频率覆盖的子群体中搜索找到目标模型在这些条件下表现不佳的子群体，以此识别出分类器可能存在的系统误差问题。 |
| [^93] | [Evolutionary Reinforcement Learning: A Survey.](http://arxiv.org/abs/2303.04150) | 这篇文章系统地总结了最新的进化计算方法在解决强化学习中的关键挑战方面所取得的良好性能。 |
| [^94] | [One-4-All: Neural Potential Fields for Embodied Navigation.](http://arxiv.org/abs/2303.04011) | 本文提出了一种新方法 One-4-All (O4A)，利用自我监督和流形学习，实现了一种无图形、端到端的导航程序。其通过贪婪最小化潜在空间内的潜力函数来进行导航，在真实世界的导航中具有重要应用价值。 |
| [^95] | [Finding Heterophilic Neighbors via Confidence-based Subgraph Matching for Semi-supervised Node Classification.](http://arxiv.org/abs/2302.09755) | 本论文提出了一种基于置信度子图匹配的双阶段算法，通过确定边权系数和改善标签传播机制来解决了异质结构邻居识别的问题。 |
| [^96] | [Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media.](http://arxiv.org/abs/2302.07731) | 本文针对机器生成的虚假评论提出了一种用高质量餐厅评论生成虚假评论并微调GPT输出检测器的方法，该方法预测虚假评论的性能优于现有解决方案。同时，我们还探索了预测非精英评论的模型，并在几个维度上对这些评论进行分析，此类机器生成的虚假评论是社交媒体平台面临的持续挑战。 |
| [^97] | [Continual Pre-training of Language Models.](http://arxiv.org/abs/2302.03241) | 本文研究了语言模型的持续预训练，提出了一种新颖的持续领域自适应预训练方法，使用一系列未标记的领域语料库来逐步适应领域以提高LM在领域内的终端任务表现。该方法通过一个软掩蔽机制来直接控制LM的更新，并使用一种新颖的代理来保留LM中的整体知识，同时对比已学习领域知识和当前全网络的知识来实现知识整合。 |
| [^98] | [Benchmarking optimality of time series classification methods in distinguishing diffusions.](http://arxiv.org/abs/2301.13112) | 本研究提出使用似然比检验为基准测试TSC算法在区分扩散过程中的最优性。随机森林、ResNet和ROCKET算法在单变量时间序列和多元高斯过程中可以实现LRT最优性，但在分类高维非线性多元时间序列时是次优的。 |
| [^99] | [Online Learning in Stackelberg Games with an Omniscient Follower.](http://arxiv.org/abs/2301.11518) | 本文研究了在线学习在两个玩家的分散合作Stackelberg博弈中的应用，假设追随者具有全知，结果表明其存在可以从常数到指数级地改变遗憾最小化的样本复杂度，存在独特挑战。 |
| [^100] | [LDMIC: Learning-based Distributed Multi-view Image Coding.](http://arxiv.org/abs/2301.09799) | LDMIC是一种基于学习的分布式多视图图像编码框架，通过独立编码器和联合上下文传输模块实现了全局视图间的相关性捕捉，对几何关系不敏感。 |
| [^101] | [Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification.](http://arxiv.org/abs/2301.07609) | 该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。 |
| [^102] | [A Novel Sparse Regularizer.](http://arxiv.org/abs/2301.07285) | 提出一种新颖的正则化方法，关注权重矩阵内权重的空间排列，可以显著减少模型参数的非零数量。 |
| [^103] | [Extreme Image Transformations Affect Humans and Machines Differently.](http://arxiv.org/abs/2212.13967) | 本文研究了一些极端的图像变换，发现机器和人类在这些变换下的表现差异较大，机器在某些变换下比人类表现更好，但在人类容易处理的变换下表现不如人类，同时提出了一些方法来改善机器的性能。 |
| [^104] | [BaCO: A Fast and Portable Bayesian Compiler Optimization Framework.](http://arxiv.org/abs/2212.11142) | BaCO是一个快速可移植的自动化编译器优化框架，使用贝叶斯优化算法，能够以小搜索预算提供1.36x-1.56x更快的代码并以2.9x-3.9x的速度达到专家级别性能。 |
| [^105] | [Hierarchical Policy Blending As Optimal Transport.](http://arxiv.org/abs/2212.01938) | 提出了一种分层策略混合方法，通过不平衡的最优传输实现策略混合，巩固底层黎曼运动策略的比例，有效调整黎曼矩阵，决定专家和代理人之间的优先级，保证安全和任务成功，并在一系列机器人控制的应用场景中超过现有基线。 |
| [^106] | [Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph.](http://arxiv.org/abs/2212.01923) | 基于查询的多模态路径融合算法可以有效地对知识库进行补全，提高了性能，并且使用了基于查询的技术来提高系统的效率。 |
| [^107] | [Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors.](http://arxiv.org/abs/2211.12005) | 本文提出自我集成保护技术，通过对训练数据添加不可感知的扰动，通过模型检查点的梯度发现这些样本，可以有效地防止竞争对手在数据上训练高性能模型。 |
| [^108] | [Are we certain it's anomalous?.](http://arxiv.org/abs/2211.09224) | 本论文提出了一种新的异常检测方法HypAD，它采用超几何不确定性来评估异常，利用无监督学习来重新建立输入信号，这种方法可以有效解决时间序列中的异常检测问题。 |
| [^109] | [Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion.](http://arxiv.org/abs/2211.07098) | 本文提出了一种使用基于网络问答和多模态融合的方法填补知识库中的缺失信息。通过设计一个多模态特征和问题模板的基于网络问答的系统来达到更高效的知识库补全，同时结合了知识库中的结构化信息来提高抽取质量。 |
| [^110] | [An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum.](http://arxiv.org/abs/2211.05207) | 该论文设计了一种可解释的深度学习模型，以预测ICU脑电监测中常见的6种脑波图案的存在，并提供高质量的解释和三种解释方法，这对于建立AI的信任和临床采用至关重要。 |
| [^111] | [Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models.](http://arxiv.org/abs/2211.02048) | 提出了空间稀疏推理（SSI）的通用技术，该技术选择性地为编辑区域执行计算并加速各种生成模型，包括条件GAN和扩散模型。通过缓存和重复使用原始图像的特征图，我们将卷积滤波器稀疏地应用于编辑区域，并在未编辑的区域中重复使用缓存特征，从而通过约$1\%$的区域编辑来减少计算资源的浪费。 |
| [^112] | [Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering.](http://arxiv.org/abs/2210.10462) | 该论文提出了一种新颖的自监督异构图预训练方法SHGP，它通过结构聚类产生伪标签来指导模型学习对象嵌入和注意力系数，不需要生成任何正例或负例，具有前景应用价值。 |
| [^113] | [An Empirical Evaluation of Multivariate Time Series Classification with Input Transformation across Different Dimensions.](http://arxiv.org/abs/2210.07713) | 本文通过实证结果证明，多元时间序列分类中，额外的通道维度远非微不足道，最佳的结果并不是通过单独对每个通道进行缩放，而是通过利用通道之间的交互信息进行缩放来实现的。 |
| [^114] | [Global Explainability of GNNs via Logic Combination of Learned Concepts.](http://arxiv.org/abs/2210.07147) | 本文提出了全局逻辑GNN解释器GLGExplainer，它是第一个能够以任意布尔组合的形式生成GNN学习的图形概念的解释器。GLGExplainer提供了准确可解释的全局解释，与GNN的组合性质相一致，并可应用于任何图分类或回归问题。 |
| [^115] | [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis.](http://arxiv.org/abs/2210.02186) | TimesNet使用时间二维变化建模，将一维时间序列转换为基于多个周期的一组二维张量，从而使二维变化可以轻松地通过二维卷积核进行建模，可以用于广泛的时间序列分析任务。 |
| [^116] | [Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations.](http://arxiv.org/abs/2209.11908) | 本文提出了一种快速生涯适应性逆强化学习框架，从学习的策略中构建多样策略的组合实现了对新的演示的快速适应，同时整合演示中的共性知识，实现准确的任务推断，还能够在大规模部署中通过维护一个精简的原型策略集合并通过策略组合来逼近所有行为。 |
| [^117] | [A methodology for identifying resiliency in renewable electrical distribution system using complex network.](http://arxiv.org/abs/2208.11543) | 本文提出一种使用复杂网络理论来识别可再生电力分布系统弹性的方法，可以识别系统中太阳能电池板的托管能力，从而有助于提高系统的韧性。 |
| [^118] | [A Meta-Analysis of Solar Forecasting Based on Skill Score.](http://arxiv.org/abs/2208.10536) | 这项研究进行了关于太阳能预测的元分析，发现预测时间段是影响技能得分最重要的因素，表明应该分别针对不同的时间段进行预测分析。气候区变量与技能得分存在显著相关性。历史数据和时空信息在太阳能预测中非常有帮助，天空和卫星图像则在日内预测中最重要，而数值气象预测和实测值在日前预测中最重要。 |
| [^119] | [Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology.](http://arxiv.org/abs/2208.05140) | 本文提出了一种名为Medical X-VL的视觉语言模型，使用自监督单模型和融合编码器，以实现放射学中的零样本监督人工智能。 |
| [^120] | [Rank-based Decomposable Losses in Machine Learning: A Survey.](http://arxiv.org/abs/2207.08768) | 这篇综述回顾了机器学习中基于排名可分解损失函数的研究并提供了分类法。它讨论了可分解性与优化算法之间的相互作用，并列举了排名损失在深度学习中的最新应用和面临的挑战。 |
| [^121] | [Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification.](http://arxiv.org/abs/2206.09098) | 本文证明了对抗性代理风险的存在性、正则性和极小化定理，这一结果为对抗鲁棒性的理论提供了支持，并且可以指导算法的发展。 |
| [^122] | [Unfooling Perturbation-Based Post Hoc Explainers.](http://arxiv.org/abs/2205.14772) | 本文介绍了一种检测和防御扰动基础解释器的恶意攻击的方法，并在图像分类任务上进行了实验验证。该方法提供了对人工智能系统的可靠性和信任性的保障。 |
| [^123] | [Do we need Label Regularization to Fine-tune Pre-trained Language Models?.](http://arxiv.org/abs/2205.12428) | 本文研究了不同的标签规范化技术，发现标签规范化通常会提高微调模型的性能，尤其是当使用较小的预训练语言模型时。提出了一种新的无教师网络的知识蒸馏方法，并发现其效果与知识蒸馏相当，但取决于教师网络的大小和下游任务。 |
| [^124] | [CNNs Avoid Curse of Dimensionality by Learning on Patches.](http://arxiv.org/abs/2205.10760) | 本研究解释了卷积神经网络在图像分类上的泛化性能，认为CNN在学习图像块后避免了维数灾难，并提供了支持的定量和定性证据。 |
| [^125] | [AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks.](http://arxiv.org/abs/2204.08358) | 本文对六种AutoML框架在100个数据集上进行了全面的实验比较，发现不同框架在不同性能指标方面各有优势和劣势，框架的选择应根据特定任务要求来进行。 |
| [^126] | [Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems.](http://arxiv.org/abs/2204.01815) | 本文介绍了一种新的一致性方法来解决矩阵和张量补全问题，在推荐系统应用中，我们证明了通过保留单位比例和一致性两个约束条件可以实现解的存在性与唯一性。 |
| [^127] | [Analytic theory for the dynamics of wide quantum neural networks.](http://arxiv.org/abs/2203.16711) | 本研究分析了训练误差的梯度下降动态，提出了一个简单的分析公式，可以捕捉到宽量子神经网络损失函数的平均行为。我们预测并表征了随机量子电路残余训练误差作为系统参数的指数衰减。 |
| [^128] | [Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition.](http://arxiv.org/abs/2203.09829) | COWERAGE算法提出，通过训练错误率保证样本覆盖度，实现在自监督语音识别中高效微调。 |
| [^129] | [OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification.](http://arxiv.org/abs/2202.07165) | 本文通过分析FL中TEE的漏洞，并在TEE中引入Oblivious Memory Access（OMA）以保护免受稀疏化风险的影响，提出了OLIVE算法，该算法在通信效率和模型精度方面优于最先进的安全聚合和差分隐私FL算法。 |
| [^130] | [Robust Hybrid Learning With Expert Augmentation.](http://arxiv.org/abs/2202.03881) | 该论文介绍了一种名为“专家增强”的混合数据增强策略，可以将其纳入混合系统以提高泛化性能，该方法可以有效克服混合模型性能仅限于训练分布的限制。作者在三个控制实验中从常微分方程和偏微分方程建模动态系统，并在真实双摆数据集上评估了该方法的潜在应用。 |
| [^131] | [A Hybrid Physics Machine Learning Approach for Macroscopic Traffic State Estimation.](http://arxiv.org/abs/2202.01888) | 本文提出了一种基于混合物理与机器学习的宏观交通状态估计方法，结合交通物理模型和机器学习方法， 使用来自交通传感器的信息作为输入以构建准确且完整的高速公路系统交通状态估计。 |
| [^132] | [Causal Inference Despite Limited Global Confounding via Mixture Models.](http://arxiv.org/abs/2112.11602) | 本论文提出了一种基于混合模型的因果推断方法，通过解决混合问题和恢复概率分布，可以确定原本无法确定的因果关系。 |
| [^133] | [Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning.](http://arxiv.org/abs/2112.09943) | 本文研究了一种通过专家引导的对称检测算法进行数据增强的方法来提高离线强化学习的性能，针对非确定性MDP提出了一种结合监督和无监督学习的检测算法，实验证明该方法在一组基准任务上明显提高了强化学习算法的性能。 |
| [^134] | [MDPFuzz: Testing Models Solving Markov Decision Processes.](http://arxiv.org/abs/2112.02807) | MDPFuzz是第一个用于测试解决MDP的模型的黑盒模糊测试框架。该框架通过检查这些模型是否进入异常危险状态来生成测试预言，并使用高斯混合模型及动态期望最大化的方法来量化状态序列的“新鲜度”以决定保留哪个突变状态。 |
| [^135] | [Combined Scaling for Zero-shot Transfer Learning.](http://arxiv.org/abs/2111.10050) | 提出了一种名为 BASIC 的综合缩放方法，在零样本迁移学习任务中实现了85.7%的高准确率，并在稳健性基准测试中表现出显着的改进。为了实现这些结果，该方法在数据大小、模型大小和批量大小三个维度上对比学习框架进行了放大。 |
| [^136] | [Temporal Abstraction in Reinforcement Learning with the Successor Representation.](http://arxiv.org/abs/2110.05740) | 本研究表明，基于状态访问模式对状态进行编码的继承表示（SR）可以被看作发现和使用时间抽象的自然基质，并且可以用于发现有助于进行暂时扩展探索或规划的选项。 |
| [^137] | [Curvature-Aware Derivative-Free Optimization.](http://arxiv.org/abs/2109.13391) | 本文介绍了曲率感知的无导数优化算法——曲率感知随机搜索（CARS），该算法使用一阶和二阶有限差分逼近来计算候选步长，证明了其对于强凸目标函数的线性收敛性，并提出了无需变量转换的立方正则化变体CARS-CR，可以以O(k^-1)的速率收敛。 |
| [^138] | [In-Network Learning: Distributed Training and Inference in Networks.](http://arxiv.org/abs/2107.03433) | 提出了一种在移动设备和分布式网络中实现分布式训练和推断的算法和架构，并实现了推断的传播和融合。与现有技术相比，该方法具有更好的性能表现。 |
| [^139] | [GitTables: A Large-Scale Corpus of Relational Tables.](http://arxiv.org/abs/2106.07258) | GitTables是一个从GitHub中提取的大型语料库，包含1M个关系表格，可以有效训练和评估高容量模型，以改进关系表格任务 |
| [^140] | [Vec2GC -- A Graph Based Clustering Method for Text Representations.](http://arxiv.org/abs/2104.09439) | 本文介绍了一种文本表示聚类方法Vec2GC，将聚类算法与基于文本表示学习创建的术语或文档加权图的社区检测相结合，可以用于无监督的文档处理。 |
| [^141] | [Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff.](http://arxiv.org/abs/2102.12736) | 该论文提出了一种基于贝叶斯后验共识分布的插补方法，可以在控制方差和前瞻性偏差权衡的同时进行时间序列插补，适用于金融等领域。 |
| [^142] | [SoK: Certified Robustness for Deep Neural Networks.](http://arxiv.org/abs/2009.04131) | 本文系统地研究了深度神经网络的认证鲁棒性，并提供了全面的基准测试。论文总结了关于凸松弛、混合整数规划和随机平滑等方法的最新研究进展，并讨论了其未来研究方向和应用。 |
| [^143] | [The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization.](http://arxiv.org/abs/2006.01244) | 本研究提出使用阶乘幂作为新的参数设置工具，可以简化或提高动量法和随机优化方法的收敛速度。 |
| [^144] | [Improving Certified Robustness via Statistical Learning with Logical Reasoning.](http://arxiv.org/abs/2003.00120) | 本研究通过将统计 ML 模型与逻辑推理组件集成，提出了方法来进一步提高 ML 模型的认证鲁棒性，同时给出了首个适用于马尔可夫逻辑网络的鲁棒性界限。 |
| [^145] | [Estimating uncertainty of earthquake rupture using Bayesian neural network.](http://arxiv.org/abs/1911.09660) | 本文使用贝叶斯神经网络估计地震断层不确定性，通过解决数据不足问题和确定导致断层的参数组合，并使用两千次的断层模拟来训练和测试模型，最终得分0.83。 |

# 详细

[^1]: 持续扩散：使用C-LoRA进行文本到图像扩散的持续定制

    Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])

    [http://arxiv.org/abs/2304.06027](http://arxiv.org/abs/2304.06027)

    本文提出了一种新方法C-LoRA，用于持续自定制文本到图像扩散模型，并避免了新概念加入后过去相似概念的图像质量下降的问题。

    

    最近的研究表明，在只提供少量示例图像的情况下，自定义文本到图像扩散模型具有显着的能力。我们的研究表明，当使用多个细粒度概念以连续方式（即持续性地）自定义这样的模型时，最近的文本到图像定制技术会遭受灾难性的遗忘。为了解决这个问题，我们提出了一种新方法，C-LoRA，采用流行的稳定扩散模型中的跨注意力层中的连续自我正则化低秩适应。此外，我们使用不包括自定义对象的单词（即“人”用于人脸数据集）并初始化为完全随机嵌入的定制提示。重要的是，我们的方法只引入了微小的额外参数成本。

    Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter cost
    
[^2]: 稀疏标注下的人群计数

    Crowd Counting with Sparse Annotation. (arXiv:2304.06021v1 [cs.CV])

    [http://arxiv.org/abs/2304.06021](http://arxiv.org/abs/2304.06021)

    本文提出了一种新的稀疏标注方法，可用于人群计数，其可减少标注工作并获取更多多样化信息。同时，我们提出了一个基于点的渐进点匹配网络（PPM），在相同注释的情况下，PPM可表现出更好的性能。

    

    本文提出了一种称为稀疏标注（SA）的新标注方法，用于人群计数，通过对图像中个体进行稀疏标注来减少人工标注工作。我们认为稀疏标注可以减少完全标注的冗余，并从未被完全捕捉的远处个体中捕获更多多样化的信息，而这是部分标注方法无法达到的。此外，我们提出了一个基于点的渐进点匹配网络（PPM），以更好地从整个图像中探索稀疏标注的人群，其中包括提议匹配网络（PMN）和性能恢复网络（PRN）。PMN使用基本的点分类器生成伪点样本，而PRN则使用伪点对点分类器进行细化以最大化性能。我们的实验结果表明，在相同数量的注释下，PPM比先前的半监督人群计数方法表现更好，并且获得了与最先进的全监督方法相竞争的性能。

    This paper presents a new annotation method called Sparse Annotation (SA) for crowd counting, which reduces human labeling efforts by sparsely labeling individuals in an image. We argue that sparse labeling can reduce the redundancy of full annotation and capture more diverse information from distant individuals that is not fully captured by Partial Annotation methods. Besides, we propose a point-based Progressive Point Matching network (PPM) to better explore the crowd from the whole image with sparse annotation, which includes a Proposal Matching Network (PMN) and a Performance Restoration Network (PRN). The PMN generates pseudo-point samples using a basic point classifier, while the PRN refines the point classifier with the pseudo points to maximize performance. Our experimental results show that PPM outperforms previous semi-supervised crowd counting methods with the same amount of annotation by a large margin and achieves competitive performance with state-of-the-art fully-supervi
    
[^3]: PD-ADSV：一种使用语音信号和硬投票集成方法进行帕金森病自动诊断的系统(arXiv:2304.06016v1 [cs.SD])

    PD-ADSV: An Automated Diagnosing System Using Voice Signals and Hard Voting Ensemble Method for Parkinson's Disease. (arXiv:2304.06016v1 [cs.SD])

    [http://arxiv.org/abs/2304.06016](http://arxiv.org/abs/2304.06016)

    PD-ADSV是一种使用语音信号自动诊断帕金森病的系统，它采用四个机器学习分类器和硬投票集成方法达到最高精度。

    

    帕金森病（PD）是最常见的运动障碍和仅次于阿尔茨海默病的第二常见神经退行性疾病。运动症状和成像技术是诊断该疾病最常用的方法。然而，它们不准确和快速，并且可能仅对少数人可用。本研究提供了一种基于语音信号进行PD诊断的自主系统PD-ADSV，该系统使用了四个机器学习分类器和硬投票集成方法以获得最高精度。PD-ADSV使用Python和Gradio web框架开发。

    Parkinson's disease (PD) is the most widespread movement condition and the second most common neurodegenerative disorder, following Alzheimer's. Movement symptoms and imaging techniques are the most popular ways to diagnose this disease. However, they are not accurate and fast and may only be accessible to a few people. This study provides an autonomous system, i.e., PD-ADSV, for diagnosing PD based on voice signals, which uses four machine learning classifiers and the hard voting ensemble method to achieve the highest accuracy. PD-ADSV is developed using Python and the Gradio web framework.
    
[^4]: 一种使用堆叠集成方法改进心脏疾病预测的方法

    An Improved Heart Disease Prediction Using Stacked Ensemble Method. (arXiv:2304.06015v1 [cs.LG])

    [http://arxiv.org/abs/2304.06015](http://arxiv.org/abs/2304.06015)

    本研究使用机器学习方法构建了一个基于心脏病数据集的心脏病诊断系统，并采用堆叠集成技术，通过组合多个基模型提高了预测准确率，达到了91.8%的高预测准确率。

    

    心脏疾病已经取代癌症成为全球最大的死因。及早发现和治疗可以降低多种心脏病相关的并发症、死亡率和诊断成本。医疗行业收集了大量医疗数据，但尚未很好地开发。在这些数据中发现之前未知的模式和联系可以帮助更好地预测心脏疾病的风险。在本研究中，我们使用心脏疾病数据集构建了一个基于机器学习的心脏病诊断系统。我们使用了许多数据预处理技术，如异常值检测和去除、缺失值检测和去除、特征规范化、交叉验证等，并使用9种分类算法和8种分类器性能度量指标。为了提高预测准确率，我们采用了堆叠集成技术，将多个基模型的预测结果进行组合，这些基模型使用不同的算法类型和参数设置进行训练。实验结果表明，所提出的集成模型胜过了单个模型，并使用Matthews相关系数作为性能度量指标，实现了91.8%的高预测准确率。

    Heart disorder has just overtaken cancer as the world's biggest cause of mortality. Several cardiac failures, heart disease mortality, and diagnostic costs can all be reduced with early identification and treatment. Medical data is collected in large quantities by the healthcare industry, but it is not well mined. The discovery of previously unknown patterns and connections in this information can help with an improved decision when it comes to forecasting heart disorder risk. In the proposed study, we constructed an ML-based diagnostic system for heart illness forecasting, using a heart disorder dataset. We used data preprocessing techniques like outlier detection and removal, checking and removing missing entries, feature normalization, cross-validation, nine classification algorithms like RF, MLP, KNN, ETC, XGB, SVC, ADB, DT, and GBM, and eight classifier measuring performance metrics like ramification accuracy, precision, F1 score, specificity, ROC, sensitivity, log-loss, and Matth
    
[^5]: 多智能体强化学习的双层潜变量模型，提高样本效率

    Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06011v1 [cs.LG])

    [http://arxiv.org/abs/2304.06011](http://arxiv.org/abs/2304.06011)

    提出了一种新颖的基于模型的MARL算法，BiLL，它学习一个双层潜变量模型，以提高样本效率。该模型在顶层学习全局状态的潜在表示，底层学习每个智能体的潜在表示，并生成潜在轨迹用于策略学习。在SMAC和Flatland环境中，我们的算法在样本效率方面优于现有的无模型和基于模型的基准算法，包括在Super Hard SMAC地图上。

    

    多智能体强化学习(MARL)算法在实际应用中具有很大的潜力，但通常具有较高的样本复杂度。为了解决这个问题，我们提出了一种新颖的基于模型的MARL算法：BiLL(双层潜变量模型学习)，该算法从高维度输入中学习双层潜在变量模型。在顶层，该模型学习全局状态的潜在表示，该表示编码与行为学习相关的全局信息。在底层，模型学习每个智能体的潜在表示，给定来自顶层的全局潜在表示。模型生成潜在轨迹以用于策略学习。我们在具有挑战性的SMAC和Flatland环境中评估了我们的算法，结果表明我们的算法在样本效率方面优于现有的无模型和基于模型的基准算法，包括在两个极具挑战性的Super Hard SMAC地图上。

    Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel model-based MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bi-level latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-of-the-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.
    
[^6]: 文献综述：计算机视觉在物流和仓储中的应用

    Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing. (arXiv:2304.06009v1 [cs.CV])

    [http://arxiv.org/abs/2304.06009](http://arxiv.org/abs/2304.06009)

    该论文进行了计算机视觉在物流和仓储中应用领域的文献综述，将文献分为监视和操作两个领域，并指出了未来研究方向和计算机视觉的最新发展。研究结果对于物流从业者也有参考价值。

    

    计算机视觉在物流和仓储中的应用具有巨大的自动化潜力。为了利用这个潜力，我们对该领域的研究进行了结构化的文献综述。所有文献都被分类为应用和用于解决任务的计算机视觉技术。关于应用，我们将文献分为两个领域：监视和操作。此外，我们指出了未来研究的方向，并链接到适用于物流的计算机视觉的最新发展。最后，我们提供现有数据集和工业解决方案的概述。我们得出结论，虽然已经研究了许多研究领域，但未来研究的潜力仍然巨大。我们的分析结果对物流从业者也很有帮助，因为我们提供了开发和测试的现有解决方案的概述。

    Computer vision applications in transportation logistics and warehousing have a huge potential for process automation. We present a structured literature review on research in the field to help leverage this potential. All literature is categorized w.r.t. the application, i.e. the task it tackles and w.r.t. the computer vision techniques that are used. Regarding applications, we subdivide the literature in two areas: Monitoring, i.e. observing and retrieving relevant information from the environment, and manipulation, where approaches are used to analyze and interact with the environment. In addition to that, we point out directions for future research and link to recent developments in computer vision that are suitable for application in logistics. Finally, we present an overview of existing datasets and industrial solutions. We conclude that while already many research areas have been investigated, there is still huge potential for future research. The results of our analysis are als
    
[^7]: 物理信息神经网络中的最大似然估计器用于高维反问题求解

    Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems. (arXiv:2304.05991v1 [cs.LG])

    [http://arxiv.org/abs/2304.05991](http://arxiv.org/abs/2304.05991)

    本论文提出了在PINNs中使用MLE的方法，消除了超参数调整。通过ODE耦合矩阵的SVD分解降维，增加了PINNs预测的稳定性和泛化能力。

    

    物理信息神经网络(PINNs)已被证明是解决反常(ODE)和偏微分方程(PDE)的合适数学框架。典型的反向PINNs被制定为带有几个超参数的软约束多目标优化问题。在本文中，我们证明反向PINNs可以用极大似然估计器(MLE)的形式来表达，通过Taylor展开，将插值误差明确地传播到物理模型空间中，而无需进行超参数调整。我们探讨了其应用于高维耦合ODEs的情况，这些ODEs受到在瞬态化学和生物动力学中常见的微分代数方程的限制。此外，我们还展示了ODE耦合矩阵(反应化学计量矩阵)的奇异值分解(SVD)提供了减少的不相关子空间，在其中可以表示PINNs解，并可以对残差进行投影。最后，SVD基函数作为先验约束增强了预测的稳定性和泛化能力。

    Physics-informed neural networks (PINNs) have proven a suitable mathematical scaffold for solving inverse ordinary (ODE) and partial differential equations (PDE). Typical inverse PINNs are formulated as soft-constrained multi-objective optimization problems with several hyperparameters. In this work, we demonstrate that inverse PINNs can be framed in terms of maximum-likelihood estimators (MLE) to allow explicit error propagation from interpolation to the physical model space through Taylor expansion, without the need of hyperparameter tuning. We explore its application to high-dimensional coupled ODEs constrained by differential algebraic equations that are common in transient chemical and biological kinetics. Furthermore, we show that singular-value decomposition (SVD) of the ODE coupling matrices (reaction stoichiometry matrix) provides reduced uncorrelated subspaces in which PINNs solutions can be represented and over which residuals can be projected. Finally, SVD bases serve as pr
    
[^8]: 通过无监督学习图嵌入进行物体无关能力分类

    Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings. (arXiv:2304.05989v1 [cs.AI])

    [http://arxiv.org/abs/2304.05989](http://arxiv.org/abs/2304.05989)

    本文通过无监督学习相似物体交互，诱导物体能力簇，使用新颖的深度感知定性空间表示法构建活动图并进行聚类，从而实现具有不确定交互开放集合的类别无关物体的能力分类。

    

    获取关于物体交互和能力的知识可以促进场景理解和人机协作任务。在日常生活场景中，人们倾向于根据场景和物体的可用性以多种不同的方式使用物体。针对存在开放互动和物体的类别不确定的情况下，学习物体的能力是一项具有挑战性的任务。本文提出了一种方法来实现具有不确定交互开放集合的类别无关物体的能力分类，通过无监督学习物体交互之间的相似之处从而诱导物体能力簇。采用了一种新颖的深度感知定性空间表示法来构建活动图（AGs），这些图从RGB-D视频中抽象出时空交互的连续表示。然后对这些AGs进行聚类，以获取具有类似能力的一组物体。我们在实际场景中的实验表明，

    Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects' availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that o
    
[^9]: 在临床数据库中审计ICU再入院率：风险因素和临床结果分析

    Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes. (arXiv:2304.05986v1 [cs.LG])

    [http://arxiv.org/abs/2304.05986](http://arxiv.org/abs/2304.05986)

    本研究使用机器学习模型解决了30天内再入院的问题，并对基于敏感属性的子组进行了公平审计，结果发现该模型在不同组别中表现出差异，强调了建立更好的公正和偏差缓解策略的必要性。

    

    本研究提出了一个机器学习（ML）流程用于临床数据分类，目的是解决30天内再入院的问题，并对基于敏感属性的子组进行了公平审计。该分类问题使用了一系列ML模型，并在模型预测上进行了公平审计。针对MIMIC III数据库中的性别、种族、语言和保险组等属性，公平审计发现了机会平等、预测平等、误报率平等和漏报率平等等标准存在差异。结果显示了该模型在不同组别中表现的差异，并强调了建立更好的公正和偏差缓解策略的必要性。本研究建议研究人员、政策制定者和实践者进行合作，解决人工智能系统中的偏差和公正问题。

    This study presents a machine learning (ML) pipeline for clinical data classification in the context of a 30-day readmission problem, along with a fairness audit on subgroups based on sensitive attributes. A range of ML models are used for classification and the fairness audit is conducted on the model predictions. The fairness audit uncovers disparities in equal opportunity, predictive parity, false positive rate parity, and false negative rate parity criteria on the MIMIC III dataset based on attributes such as gender, ethnicity, language, and insurance group. The results identify disparities in the model's performance across different groups and highlights the need for better fairness and bias mitigation strategies. The study suggests the need for collaborative efforts among researchers, policymakers, and practitioners to address bias and fairness in artificial intelligence (AI) systems.
    
[^10]: 神经注意森林：基于Transformer的森林改进

    Neural Attention Forests: Transformer-Based Forest Improvement. (arXiv:2304.05980v1 [cs.LG])

    [http://arxiv.org/abs/2304.05980](http://arxiv.org/abs/2304.05980)

    本论文提出一种名为神经注意森林的新方法，将注意力机制引入到随机森林中，通过神经网络计算获得注意权重，进而解决回归和分类任务。

    

    该论文提出了一种名为神经注意森林（NAF）的新方法，用于解决基于表格形式的训练数据的回归和分类任务。该模型的主要思想是将注意力机制引入到随机森林中，在 Nadaraya-Watson 核回归框架下，通过将特定形式的神经网络计算得到的注意权重分配到决策树中的叶子数据和随机森林本身中。与现有的基于注意力随机森林的模型不同，注意权重和 Nadaraya-Watson 回归采用神经网络的形式表示，其权重可以视为可训练参数。共享权重的神经网络的第一部分用于所有决策树的训练，并计算叶子数据的注意力权重。第二部分聚合树网络的输出，并旨在最小化随机森林预测与训练集目标真值之间的差异。

    A new approach called NAF (the Neural Attention Forest) for solving regression and classification tasks under tabular training data is proposed. The main idea behind the proposed NAF model is to introduce the attention mechanism into the random forest by assigning attention weights calculated by neural networks of a specific form to data in leaves of decision trees and to the random forest itself in the framework of the Nadaraya-Watson kernel regression. In contrast to the available models like the attention-based random forest, the attention weights and the Nadaraya-Watson regression are represented in the form of neural networks whose weights can be regarded as trainable parameters. The first part of neural networks with shared weights is trained for all trees and computes attention weights of data in leaves. The second part aggregates outputs of the tree networks and aims to minimize the difference between the random forest prediction and the truth target value from a training set. 
    
[^11]: ImageReward：学习和评估文本到图像生成的人类喜好

    ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])

    [http://arxiv.org/abs/2304.05977](http://arxiv.org/abs/2304.05977)

    ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。

    

    本文提出一种通用的文本到图像生成人类喜好奖励模型ImageReward，旨在解决生成模型中存在的各种问题，并使其与人类价值和偏好保持一致。该奖励模型的训练基于我们的系统注释流程，其中包括评分和排名组件，迄今已收集了137k的专家比较数据集。在人类评估中，ImageReward的表现优于现有的评分方法（例如比CLIP高38.6\%），因此它是一种有前途的用于评估和改进文本到图像合成的自动度量标准。该奖励模型通过\texttt {image-reward}程序包公开提供，网址为\url{https://github.com/THUDM/ImageReward}。

    We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
    
[^12]: 大型语言模型的增强提示集成

    Boosted Prompt Ensembles for Large Language Models. (arXiv:2304.05970v1 [cs.CL])

    [http://arxiv.org/abs/2304.05970](http://arxiv.org/abs/2304.05970)

    本文提出了一种增强提示集成方法，可以使用小型数据集构建集成提示提高大型语言模型的性能，优于单提示输出空间集成和袋装提示空间集成。

    

    链式思维提示和自一致性等方法已经推动了语言模型推理性能的前沿，而且没有额外的训练。为了进一步提高性能，我们建议为大型语言模型提供一种提示集成方法，该方法使用小型数据集来构建一组少量的提示，这些提示共同构成了一个“增强的提示集成”。每个提示的少数样例是通过渐进式方式选择的，以便在上一个步骤的集成结果不确定时成为“困难”样例。我们证明这种方法在GSM8k和AQuA数据集等方面优于单提示输出空间集成和袋装提示空间集成。我们提出了训练时间和测试时间版本的增强提示，并使用不同级别的可用注释进行了详细的实证研究。

    Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.
    
[^13]: 基于路径修补的模型行为定位

    Localizing Model Behavior with Path Patching. (arXiv:2304.05969v1 [cs.LG])

    [http://arxiv.org/abs/2304.05969](http://arxiv.org/abs/2304.05969)

    本文介绍了一种新的技术——路径修补，用于表达和定量测试表明行为被定位到一组路径的一类自然假设。

    

    将神经网络的行为定位到网络组件的某个子集或组件之间的某个交互的子集是分析网络机制和可能失效模式的自然第一步。现有工作常常是定性且临时的，对于评估定位声明的适当方式没有共识。我们引入了路径修补技术，用于表达和定量测试表明行为被定位到一组路径的一类自然假设。我们改进了感应头的解释，表征了GPT-2的行为，并开源了一个框架，以便高效地运行类似的实验。

    Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.
    
[^14]: CMOS + 随机纳米磁体：概率推理与学习异构计算机

    CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2304.05949](http://arxiv.org/abs/2304.05949)

    本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。

    

    随着摩尔定律的放缓，利用新兴的纳米技术（X）增强互补金属氧化物半导体（CMOS）晶体管变得越来越重要。本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型。尽管sMTJs设备间存在差异，我们的异构计算机成功地执行了概率推理和异步Boltzmann学习。使用CMOS预测流程设计套件（PDK）进行全面比较，数字CMOS-based p-bits模拟高质量随机性需要超过10,000个晶体管，每生成一个随机数的能量比使用只消耗2fJ的sMTJ-based p-bits高约两个数量级。我们的方法的缩放和集成版本可以显着推进概率性的推理。

    With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
    
[^15]: 显式最小化变分自编码器的模糊误差

    Explicitly Minimizing the Blur Error of Variational Autoencoders. (arXiv:2304.05939v1 [cs.CV])

    [http://arxiv.org/abs/2304.05939](http://arxiv.org/abs/2304.05939)

    该论文提出了一种新的变分自编码器的重构项公式，它特别针对生成模糊图像进行惩罚，同时仍然最大化建模分布下的ELBO。 实验证明，该方法在三个数据集上优于VAE的几个最近提出的重构损失。

    

    变分自编码器（VAEs）是强大的生成建模方法，但与它们所训练的图像相比，它们会产生模糊的生成样本和重构图像。已经投入了大量的研究努力来创建更灵活的模型以增加生成能力，但通常灵活性的代价是更高的复杂性和计算成本。几项工作集中在改变证据下界（ELBO）的重构项上，但往往是以损失将样本最大似然的数学联系为代价。在这里，我们提出一种针对VAE的重构项的新公式，特别惩罚生成模糊图像，同时仍然在建模分布下最大化ELBO。我们展示了所提出的损失在三个不同的数据集上的潜力，其中它优于VAE的几个最近提出的重构损失。

    Variational autoencoders (VAEs) are powerful generative modelling methods, however they suffer from blurry generated samples and reconstructions compared to the images they have been trained on. Significant research effort has been spent to increase the generative capabilities by creating more flexible models but often flexibility comes at the cost of higher complexity and computational cost. Several works have focused on altering the reconstruction term of the evidence lower bound (ELBO), however, often at the expense of losing the mathematical link to maximizing the likelihood of the samples under the modeled distribution. Here we propose a new formulation of the reconstruction term for the VAE that specifically penalizes the generation of blurry images while at the same time still maximizing the ELBO under the modeled distribution. We show the potential of the proposed loss on three different data sets, where it outperforms several recently proposed reconstruction losses for VAEs.
    
[^16]: 一种基于音素的神经网络模型来进行音符级歌唱转录

    A Phoneme-Informed Neural Network Model for Note-Level Singing Transcription. (arXiv:2304.05917v1 [cs.SD])

    [http://arxiv.org/abs/2304.05917](http://arxiv.org/abs/2304.05917)

    本文提出了一种在唱歌转录中更准确地找到音符起点的方法，使用了梅尔尺度谱图和音素后验图作为输入，后者是由预先训练的网络生成的，并证明语言特征对起始检测有影响。

    

    音符级别的自动音乐转录是最具代表性的音乐信息检索（MIR）任务之一，已经研究了各种乐器来理解音乐。然而，由于缺乏高质量的标注数据，许多乐器的转录仍然是一项具有挑战性的任务。特别是对于唱歌，由于其在音高、音色和动态方面的表现力，很难找到准确的音符。在本文中，我们提出了一种方法，通过利用仅在唱歌中可见的语言特征，更准确地找到唱歌声音的音符起点。所提出的模型使用了梅尔尺度谱图和音素后验图（PPG），即音素的帧级似然，作为起始检测网络的输入，而PPG是通过使用唱歌和语音数据进行预训练网络生成的。为了验证语言特征如何影响起始检测，我们通过具有不同语言的数据集比较了评估结果。

    Note-level automatic music transcription is one of the most representative music information retrieval (MIR) tasks and has been studied for various instruments to understand music. However, due to the lack of high-quality labeled data, transcription of many instruments is still a challenging task. In particular, in the case of singing, it is difficult to find accurate notes due to its expressiveness in pitch, timbre, and dynamics. In this paper, we propose a method of finding note onsets of singing voice more accurately by leveraging the linguistic characteristics of singing, which are not seen in other instruments. The proposed model uses mel-scaled spectrogram and phonetic posteriorgram (PPG), a frame-wise likelihood of phoneme, as an input of the onset detection network while PPG is generated by the pre-trained network with singing and speech data. To verify how linguistic features affect onset detection, we compare the evaluation results through the dataset with different languages
    
[^17]: 带有定位-尺度噪声的扩散模型

    Diffusion models with location-scale noise. (arXiv:2304.05907v1 [cs.LG])

    [http://arxiv.org/abs/2304.05907](http://arxiv.org/abs/2304.05907)

    本研究研究了用于生成模型的不同噪声分布，证明了高斯分布在扩散模型（DMs）中表现最佳。

    

    扩散模型(DMs)是一种强大的生成模型，它通过添加高斯噪声到数据中，并学会去除噪声。我们想确定哪种噪声分布（高斯或非高斯）在DMs中导致更好的生成数据。由于DMs的设计不适用于非高斯噪声，因此我们构建了一个框架，允许使用非高斯定位-尺度噪声来逆转扩散过程。我们使用该框架展示高斯分布在各种其它分布（拉普拉斯、均匀、t、广义高斯）中表现最佳。

    Diffusion Models (DMs) are powerful generative models that add Gaussian noise to the data and learn to remove it. We wanted to determine which noise distribution (Gaussian or non-Gaussian) led to better generated data in DMs. Since DMs do not work by design with non-Gaussian noise, we built a framework that allows reversing a diffusion process with non-Gaussian location-scale noise. We use that framework to show that the Gaussian distribution performs the best over a wide range of other distributions (Laplace, Uniform, t, Generalized-Gaussian).
    
[^18]: 评估表面肌电图案识别分类器置信度

    Evaluating Classifier Confidence for Surface EMG Pattern Recognition. (arXiv:2304.05898v1 [eess.SP])

    [http://arxiv.org/abs/2304.05898](http://arxiv.org/abs/2304.05898)

    本文研究了在表面肌电图案识别中评估分类器分类的置信度，并发现基于比例混合模型的分类器产生了更准确的置信度。

    

    表面肌电图（EMG）可以通过模式识别应用为各种设备和软件的接口信号。在基于EMG的模式识别中，分类器不仅应准确，还应输出适当的置信度（即正确性的概率）进行预测。如果置信度能够精确地反映真正正确性的可能性，那么它将在各种应用任务中派上用场，例如运动拒绝和在线适应。本文旨在确定哪种类型的分类器在EMG模式识别中提供更高的准确度和更好的置信度。我们定量和定性地评估了四个EMG数据集上各种区分性和生成式分类器的性能。分析结果表明，虽然基于深度神经网络的区分性分类器表现出较高的准确性，但其输出的置信度与真实概率不同。相反，基于比例混合模型的分类器，它是

    Surface electromyogram (EMG) can be employed as an interface signal for various devices and software via pattern recognition. In EMG-based pattern recognition, the classifier should not only be accurate, but also output an appropriate confidence (i.e., probability of correctness) for its prediction. If the confidence accurately reflects the likelihood of true correctness, then it will be useful in various application tasks, such as motion rejection and online adaptation. The aim of this paper is to identify the types of classifiers that provide higher accuracy and better confidence in EMG pattern recognition. We evaluate the performance of various discriminative and generative classifiers on four EMG datasets, both visually and quantitatively. The analysis results show that while a discriminative classifier based on a deep neural network exhibits high accuracy, it outputs a confidence that differs from true probabilities. By contrast, a scale mixture model-based classifier, which is a 
    
[^19]: 探索数据增强在不平衡数据中的作用机理

    Towards Understanding How Data Augmentation Works with Imbalanced Data. (arXiv:2304.05895v1 [cs.LG])

    [http://arxiv.org/abs/2304.05895](http://arxiv.org/abs/2304.05895)

    本文对数据增强在不平衡数据中的作用机理做了全面探究，结果表明增强了分类器发现和补偿不平衡数据中类别特定特征的能力。

    

    数据增强是许多现代机器学习训练流程的基石，然而它的作用机理尚不清楚。本研究对三种不同的分类器卷积神经网络、支持向量机和逻辑回归模型在图像和表格数据集上进行全面评估，结果表明，当应用于不平衡数据时，数据增强会产生模型权重、支持向量和特征选择等方面的显著改变。该研究认为数据增强不仅会生成新样本，更重要的是增强了分类器发现和补偿不平衡数据中的类别特定特征的能力。

    Data augmentation forms the cornerstone of many modern machine learning training pipelines; yet, the mechanisms by which it works are not clearly understood. Much of the research on data augmentation (DA) has focused on improving existing techniques, examining its regularization effects in the context of neural network over-fitting, or investigating its impact on features. Here, we undertake a holistic examination of the effect of DA on three different classifiers, convolutional neural networks, support vector machines, and logistic regression models, which are commonly used in supervised classification of imbalanced data. We support our examination with testing on three image and five tabular datasets. Our research indicates that DA, when applied to imbalanced data, produces substantial changes in model weights, support vectors and feature selection; even though it may only yield relatively modest changes to global metrics, such as balanced accuracy or F1 measure. We hypothesize that 
    
[^20]: 带权标签网络的动态混合成员随机块模型

    Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks. (arXiv:2304.05894v1 [cs.LG])

    [http://arxiv.org/abs/2304.05894](http://arxiv.org/abs/2304.05894)

    本论文提出了一种扩展混合成员随机块模型来推断动态标签网络的方法，具有很好的鲁棒性和良好的性能，相对于静态标签网络，对数据的训练需求较少。

    

    大多数现实中的网络都是随时间变化的。现有的动态网络模型要么没有标签，要么假定只有一个成员结构。另一方面，一种新的混合成员随机块模型（MMSBM）家族允许在混合成员聚类的假设下模拟静态标签网络。在本文中，我们提出将这种模型扩展到在混合成员假设下推断动态标签网络的模型类。我们的方法采用模型参数的时间先验形式，并依赖于动力学不是突然的单一假设。我们展示了我们的方法与现有方法显著不同，并且可以模拟更复杂的系统——动态标记网络。我们在合成和现实数据集上进行了几个实验，证明了我们方法的鲁棒性。我们方法的一个关键优势是，它只需要很少的训练数据就能产生良好的结果。与静态标签网络相比，我们方法在动态标签网络下的性能提升显著。

    Most real-world networks evolve over time. Existing literature proposes models for dynamic networks that are either unlabeled or assumed to have a single membership structure. On the other hand, a new family of Mixed Membership Stochastic Block Models (MMSBM) allows to model static labeled networks under the assumption of mixed-membership clustering. In this work, we propose to extend this later class of models to infer dynamic labeled networks under a mixed membership assumption. Our approach takes the form of a temporal prior on the model's parameters. It relies on the single assumption that dynamics are not abrupt. We show that our method significantly differs from existing approaches, and allows to model more complex systems --dynamic labeled networks. We demonstrate the robustness of our method with several experiments on both synthetic and real-world datasets. A key interest of our approach is that it needs very few training data to yield good results. The performance gain under 
    
[^21]: 多步逆运动学表示学习：富观测强化学习的高效优化方法

    Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL. (arXiv:2304.05889v1 [cs.LG])

    [http://arxiv.org/abs/2304.05889](http://arxiv.org/abs/2304.05889)

    本文提出了一种名为MusIK的新算法，利用多步逆运动学实现表示学习和系统探索相结合，达到计算高效和样本复杂度最优的效果，成功应用于富观测强化学习任务中。

    

    本研究探讨了在块MDP问题下，针对富有高维度观测而设计的样本高效算法的问题。现有算法存在1）计算复杂度过高，2）具有不必要的强统计假设，或3）具有次优的样本复杂度。我们通过提供第一个在计算上高效的算法来解决这些问题，并在最小统计假设的前提下实现了与所需精度水平相对应的速率最优样本复杂度。我们的算法MusIK将系统探索与基于多步逆运动学的表示学习相结合，这是一种学习目标，即从当前观察和（可能遥远的）未来观察中预测学习者自己的行动。MusIK简单而灵活，可以高效地利用通用函数逼近。我们的分析利用了几种新技术，包括一种新的链条论证方法，用于限制时间和表示空间中的误差传播，以及一种新的探索充分条件，用于捕捉低维几何结构。我们在一系列具有挑战性的基准测试中展示了MusIK的优势，无论是在样本复杂度还是墙钟时间方面都优于先前的方法。

    We study the design of sample-efficient algorithms for reinforcement learning in the presence of rich, high-dimensional observations, formalized via the Block MDP problem. Existing algorithms suffer from either 1) computational intractability, 2) strong statistical assumptions that are not necessarily satisfied in practice, or 3) suboptimal sample complexity. We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions. Our algorithm, MusIK, combines systematic exploration with representation learning based on multi-step inverse kinematics, a learning objective in which the aim is to predict the learner's own action from the current observation and observations in the (potentially distant) future. MusIK is simple and flexible, and can efficiently take advantage of general-purpose function approximation. Our analysis leverages several new tec
    
[^22]: FetMRQC: 自动化胎儿脑 MRI 质量控制

    FetMRQC: Automated Quality Control for fetal brain MRI. (arXiv:2304.05879v1 [eess.IV])

    [http://arxiv.org/abs/2304.05879](http://arxiv.org/abs/2304.05879)

    FetMRQC 是一种针对胎儿脑 MRI 的自动图像质量评估机器学习框架，通过提取一系列质量指标可以预测专家评分，并能够在大部分数据集上实现准确评估，并发布了一个新的机器学习数据集。

    

    质量控制一直被认为是确保神经影像学研究可靠性的重要因素。对于胎儿脑 MRI 尤为重要，因为胎动频繁且不可预测，会导致图像中产生严重伪影。现有的胎儿脑质量评估方法仅从 \textit{层面} 考虑，无法全面了解图像质量，而评估整个脑容积才能实现这一点。本文提出了 FetMRQC，一种针对胎儿脑 MRI 的自动图像质量评估机器学习框架，该框架提取一系列质量指标，并使用这些指标预测专家评分。基于在两个不同机构收集的超过 1000 个低分辨率胎儿脑 MRI 样本的手动评分，我们发现 FetMRQC 能够一般化到其他数据集，并且是可解释的和高效的。我们还发布了一个新颖的机器学习数据集，其中包括高质量和低质量的胎儿脑 MRI 容积和专家评分。

    Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where large and unpredictable fetal motion can lead to substantial artifacts in the acquired images. Existing methods for fetal brain quality assessment operate at the \textit{slice} level, and fail to get a comprehensive picture of the quality of an image, that can only be achieved by looking at the \textit{entire} brain volume. In this work, we propose FetMRQC, a machine learning framework for automated image quality assessment tailored to fetal brain MRI, which extracts an ensemble of quality metrics that are then used to predict experts' ratings. Based on the manual ratings of more than 1000 low-resolution stacks acquired across two different institutions, we show that, compared with existing quality metrics, FetMRQC is able to generalize out-of-domain, while being interpretable and data efficient. We also release a novel ma
    
[^23]: 自适应门控图卷积网络用于基于EEG数据的阿尔茨海默病可解释诊断

    Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.05874](http://arxiv.org/abs/2304.05874)

    本文提出了一种自适应门控图卷积网络(AGGCN)，该网络结合卷积节点特征增强和功能连接度量自适应学习图结构，实现了高精度的阿尔茨海默病诊断，并提供了重要的脑区信息。

    

    近来，图神经网络(GNN)模型越来越多地被用于分类脑电图(EEG)数据，然而，基于GNN的神经系统疾病，如阿尔茨海默病(AD)的诊断仍然是相对未开发的领域。因此，本文提出了一种新颖的自适应门控图卷积网络(AGGCN)，该网络可以提供可解释的预测结果。AGGCN通过将基于卷积的节点特征增强与基于功能连接性的著名相关度量相结合来自适应学习图结构。此外，门控图卷积可以动态地加权考虑各种空间尺度的贡献。实验结果表明，该模型在闭眼和睁眼状态下均能取得较高的精度，表明学习到的表征结果的稳定性。最后，我们证明了所提出的AGGCN模型可以提供有关AD最受影响的脑区的重要见解。

    Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
    
[^24]: 在竞争性多智能体环境中学习沟通和协作以清理海洋废弃塑料

    Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics. (arXiv:2304.05872v1 [cs.AI])

    [http://arxiv.org/abs/2304.05872](http://arxiv.org/abs/2304.05872)

    本文提出了一种基于图神经网络（GNN）的、用于多智能体交互式海洋废弃物清理的通信机制，使得不同代理之间可以协作竞争并实现收集废弃物的最大化。

    

    在许多实际应用中，协作与竞争之间的平衡对于人工智能代理至关重要。本文使用多智能体强化学习（MARL）建立在一个高影响问题上，通过对海洋废弃塑料的收集实现了协作与竞争的平衡。我们提出了一种基于图神经网络（GNN）的通信机制，它增加了代理的观察空间。在我们自定义的环境中，代理控制着收集塑料的船只。这种通信机制使代理能够使用二进制信号来开发通信协议。虽然代理的集体目标是尽可能地清理海洋废弃塑料，但代理会因个人收集到的废弃塑料数量而获得奖励。因此，代理必须学会有效地沟通并保持竞争关系。

    Finding a balance between collaboration and competition is crucial for artificial agents in many real-world applications. We investigate this using a Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact problem. The accumulation and yearly growth of plastic in the ocean cause irreparable damage to many aspects of oceanic health and the marina system. To prevent further damage, we need to find ways to reduce macroplastics from known plastic patches in the ocean. Here we propose a Graph Neural Network (GNN) based communication mechanism that increases the agents' observation space. In our custom environment, agents control a plastic collecting vessel. The communication mechanism enables agents to develop a communication protocol using a binary signal. While the goal of the agent collective is to clean up as much as possible, agents are rewarded for the individual amount of macroplastics collected. Hence agents have to learn to communicate effectively while maintai
    
[^25]: 带有联邦和集中特征的边缘云协作学习

    Edge-cloud Collaborative Learning with Federated and Centralized Features. (arXiv:2304.05871v1 [cs.LG])

    [http://arxiv.org/abs/2304.05871](http://arxiv.org/abs/2304.05871)

    本文提出了一种边缘云协作知识转移框架（ECCT），使得共享特征嵌入和预测日志的双向知识传输成为可能，从而实现了个性化增强、模型的异构性、容忍训练的异步性和缓解通信负担的功能。

    

    联邦学习（FL）是一种受欢迎的边缘计算方式，不会危及用户的隐私。目前的FL范例假定数据仅驻留在边缘，而云服务器仅执行模型平均。但是，在诸如推荐系统之类的实际情况下，云服务器具有存储历史和交互特征的能力。本文提出的Edge-Cloud Collaborative Knowledge Transfer Framework（ECCT）弥合了边缘和云之间的差距，使其能够在两者之间进行双向知识传输，共享特征嵌入和预测日志。 ECCT巩固了各种好处，包括增强个性化，实现模型异构性，容忍培训异步性和缓解通信负担。对公共和工业数据集的广泛实验表明ECCT的有效性和学术和工业使用的潜力。

    Federated learning (FL) is a popular way of edge computing that doesn't compromise users' privacy. Current FL paradigms assume that data only resides on the edge, while cloud servers only perform model averaging. However, in real-life situations such as recommender systems, the cloud server has the ability to store historical and interactive features. In this paper, our proposed Edge-Cloud Collaborative Knowledge Transfer Framework (ECCT) bridges the gap between the edge and cloud, enabling bi-directional knowledge transfer between both, sharing feature embeddings and prediction logits. ECCT consolidates various benefits, including enhancing personalization, enabling model heterogeneity, tolerating training asynchronization, and relieving communication burdens. Extensive experiments on public and industrial datasets demonstrate ECCT's effectiveness and potential for use in academia and industry.
    
[^26]: LMR: 基于车道距离的轨迹预测度量

    LMR: Lane Distance-Based Metric for Trajectory Prediction. (arXiv:2304.05869v1 [cs.CV])

    [http://arxiv.org/abs/2304.05869](http://arxiv.org/abs/2304.05869)

    LMR是一种基于车道距离的度量，适用于轨迹预测中的结构化环境，相比传统的欧氏距离度量更准确。

    

    轨迹预测方法的开发需要度量来验证和比较它们的性能。目前已经确定的度量基于欧氏距离，这意味着在所有方向上都给出了相同的误差权重。欧几里得度量对于像道路这样的结构化环境是不足够的，因为它们没有妥善捕捉到与底层车道相关的操作员意图。为了针对下游规划任务合理评估轨迹预测方法，我们提出了一种新的度量，即基于车道距离的车道错过率（LMR）。对于LMR的计算，将地面实测和预测端点分配给车道线段，更确切地说是它们的中心线。通过沿车道线段的距离测量，预测与实测之间的距离在一定阈值范围内的预测被称为命中，否则称为错过。LMR则定义为产生错过的序列的比率。我们在三个不同的数据集上的结果表明，相对于传统的基于欧氏距离的度量，LMR是适用于类似车道这样的结构化环境的轨迹预测更为合适的度量。

    The development of approaches for trajectory prediction requires metrics to validate and compare their performance. Currently established metrics are based on Euclidean distance, which means that errors are weighted equally in all directions. Euclidean metrics are insufficient for structured environments like roads, since they do not properly capture the agent's intent relative to the underlying lane. In order to provide a reasonable assessment of trajectory prediction approaches with regard to the downstream planning task, we propose a new metric that is lane distance-based: Lane Miss Rate (LMR). For the calculation of LMR, the ground-truth and predicted endpoints are assigned to lane segments, more precisely their centerlines. Measured by the distance along the lane segments, predictions that are within a certain threshold distance to the ground-truth count as hits, otherwise they count as misses. LMR is then defined as the ratio of sequences that yield a miss. Our results on three s
    
[^27]: NoisyTwins: 通过StyleGANs实现一致类别和多样化图像生成

    NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs. (arXiv:2304.05866v1 [cs.CV])

    [http://arxiv.org/abs/2304.05866](http://arxiv.org/abs/2304.05866)

    本文提出了一种名为NoisyTwins的方法，可以通过增强类别嵌入并使用自我监督在W空间中去相关化潜空间来改善大规模长尾数据集上StyleGANs的性能表现，从而在图像生成中保留了类内多样性和一致性。

    

    StyleGANs是可控图片生成的前沿技术，其产生了一个语义解耦的潜空间，适用于图像编辑和操作。但是，当通过类别条件在大规模长尾数据集上进行训练时，StyleGANs的性能严重下降。我们发现导致性能下降的一个原因是在W潜空间中每个类别的潜空间坍塌。使用NoisyTwins，我们首先引入了一种有效且低成本的类别嵌入增强策略，然后基于W空间的自我监督来去相关化潜空间。这种去相关化缓解了坍塌，确保我们的方法在图像生成中保留了类内差异和一致性。我们展示了我们的方法在ImageNet-LT和iNaturalist 2019大规模真实世界长尾数据集上的有效性，在FID上优于其他方法约19％，建立了新的最高纪录。

    StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the $\mathcal{W}$ latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the $\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by $\sim 19\%$ on FID, establishing a new state-of-the-art.
    
[^28]: 三维数据的尺度等变深度学习

    Scale-Equivariant Deep Learning for 3D Data. (arXiv:2304.05864v1 [cs.CV])

    [http://arxiv.org/abs/2304.05864](http://arxiv.org/abs/2304.05864)

    该论文提出了一种用于三维数据的尺度等变卷积网络层，可在处理不同尺度的对象和对象部分时提高数据效率和产生更好的结果。

    

    卷积神经网络的平移等变性是它识别图像中位置不变的物体的能力，而群等变卷积神经网络将这种等变性转换为输入的其他变换。恰当处理不同尺度的对象和对象部分具有挑战性，而尺度可以因多种原因而变化，例如基础对象的大小或成像模态的分辨率。在本文中，我们提出了一种用于三维数据的尺度等变卷积网络层，保证三维卷积神经网络具有尺度等变性。尺度等变性减轻了分别学习每种可能尺度的负担，使神经网络专注于更高层次的学习目标，从而产生更好的结果和更好的数据效率。我们提供了有关尺度等变神经网络在二维领域的理论基础和科学工作的概述，然后将其转移到三维领域，并进行实验证明了其有效性。

    The ability of convolutional neural networks (CNNs) to recognize objects regardless of their position in the image is due to the translation-equivariance of the convolutional operation. Group-equivariant CNNs transfer this equivariance to other transformations of the input. Dealing appropriately with objects and object parts of different scale is challenging, and scale can vary for multiple reasons such as the underlying object size or the resolution of the imaging modality. In this paper, we propose a scale-equivariant convolutional network layer for three-dimensional data that guarantees scale-equivariance in 3D CNNs. Scale-equivariance lifts the burden of having to learn each possible scale separately, allowing the neural network to focus on higher-level learning goals, which leads to better results and better data-efficiency. We provide an overview of the theoretical foundations and scientific work on scale-equivariant neural networks in the two-dimensional domain. We then transfer
    
[^29]: 重访轨迹集合，用于条件驾驶行为预测

    RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction. (arXiv:2304.05856v1 [cs.CV])

    [http://arxiv.org/abs/2304.05856](http://arxiv.org/abs/2304.05856)

    这篇论文提出了一种新的用于条件驾驶行为预测的基于集合法的轨迹预测方法RESET，可以预测灵活数量的轨迹而不影响运行时间或值域。

    

    预测自动驾驶车辆的不同轨迹条件下交通参与者的行为是必要的。这可以使得下游的规划器更准确地评估其决策的影响。最近的条件行为预测方法依赖于回归解码器，这意味着坐标或多项式系数会被回归。本文重新考虑基于集合的轨迹预测，其中预定义轨迹集合中每种轨迹的概率由分类模型决定，并首次将其应用于条件驾驶行为预测任务。我们提出了RESET，它结合了一种新的度量驱动的轨迹集合生成算法和基于图形的编码器。对于无条件预测，RESET的表现与基于回归的方法相当。由于基于集合法的特性，它具有可预测灵活数量轨迹的优势，而不会影响运行时间或值域。

    It is desirable to predict the behavior of traffic participants conditioned on different planned trajectories of the autonomous vehicle. This allows the downstream planner to estimate the impact of its decisions. Recent approaches for conditional behavior prediction rely on a regression decoder, meaning that coordinates or polynomial coefficients are regressed. In this work we revisit set-based trajectory prediction, where the probability of each trajectory in a predefined trajectory set is determined by a classification model, and first-time employ it to the task of conditional behavior prediction. We propose RESET, which combines a new metric-driven algorithm for trajectory set generation with a graph-based encoder. For unconditional prediction, RESET achieves comparable performance to a regression-based approach. Due to the nature of set-based approaches, it has the advantageous property of being able to predict a flexible number of trajectories without influencing runtime or comple
    
[^30]: 使用黑盒强化学习的分类树的最佳可解释性 - 性能权衡

    Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning. (arXiv:2304.05839v1 [cs.LG])

    [http://arxiv.org/abs/2304.05839](http://arxiv.org/abs/2304.05839)

    本文提出了一种用于探索决策树空间的强化学习框架来学习紧凑的决策树并最佳化可解释性与性能之间的平衡。

    

    AI模型的可解释性可以建立对这些模型的信任，从而允许进行用户安全检查。决策树（DT）特别提供了关于学习模型的全局视图，并清晰地概述了对于分类给定数据至关重要的特征的角色。然而，如果DT太大，则这种可解释性会受到阻碍。最近提出了一种强化学习（RL）框架来探索DT空间以学习紧凑的树。一个给定的监督分类任务被建模为一个马尔可夫决策问题（MDP），然后添加了收集关于特征信息的额外动作，相当于构建DT。通过适当地惩罚这些操作，RL代理学习最佳权衡DT的大小和性能。但是，要做到这一点，这个RL代理需要解决一个部分可观察的MDP。本文的主要贡献是证明，解决一个完全可观察的问题就足以学习一个优化可解释性 - 性能权衡的DT。

    Interpretability of AI models allows for user safety checks to build trust in these models. In particular, decision trees (DTs) provide a global view on the learned model and clearly outlines the role of the features that are critical to classify a given data. However, interpretability is hindered if the DT is too large. To learn compact trees, a Reinforcement Learning (RL) framework has been recently proposed to explore the space of DTs. A given supervised classification task is modeled as a Markov decision problem (MDP) and then augmented with additional actions that gather information about the features, equivalent to building a DT. By appropriately penalizing these actions, the RL agent learns to optimally trade-off size and performance of a DT. However, to do so, this RL agent has to solve a partially observable MDP. The main contribution of this paper is to prove that it is sufficient to solve a fully observable problem to learn a DT optimizing the interpretability-performance tr
    
[^31]: DartsReNet：在ReNet架构中探索新的RNN单元

    DartsReNet: Exploring new RNN cells in ReNet architectures. (arXiv:2304.05838v1 [cs.CV])

    [http://arxiv.org/abs/2304.05838](http://arxiv.org/abs/2304.05838)

    本文使用 DARTS 方法对标准 RNN 单元进行改进，提出了用于 ReNet 架构的新的 RNN 单元，有效提高了在 CIFAR-10 和 SVHN 数据集上的分类效果。

    

    我们使用一种神经架构搜索（NAS）方法 DARTS，为图像分类提出了一种新的递归神经网络（RNN）单元，该单元用于 ReNet 架构。我们对 ReNet 架构感兴趣，它是一种基于 RNN 的方法，作为卷积和池化步骤的替代方案。我们使用 DARTS 来发现新的单元设计以克服标准 RNN 单元针对一维序列数据而非图像分类这种二维数据的局限性。我们将结果与使用 GRU 和 LSTM 单元的 ReNet 进行比较。我们发现的新单元在 CIFAR-10 和 SVHN 上优于标准 RNN 单元，而对 SVHN 结果的改进表明其具有推广性，因为我们是从 CIFAR-10 推导出 RNN 单元设计的，而没有针对 SVHN 进行新的单元搜索。

    We present new Recurrent Neural Network (RNN) cells for image classification using a Neural Architecture Search (NAS) approach called DARTS. We are interested in the ReNet architecture, which is a RNN based approach presented as an alternative for convolutional and pooling steps. ReNet can be defined using any standard RNN cells, such as LSTM and GRU. One limitation is that standard RNN cells were designed for one dimensional sequential data and not for two dimensions like it is the case for image classification. We overcome this limitation by using DARTS to find new cell designs. We compare our results with ReNet that uses GRU and LSTM cells. Our found cells outperform the standard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate generalizability, as we derived the RNN cell designs from CIFAR-10 without performing a new cell search for SVHN.
    
[^32]: 一种联邦学习的博弈论框架

    A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])

    [http://arxiv.org/abs/2304.05836](http://arxiv.org/abs/2304.05836)

    本文提出了一个名为联邦学习安全博弈（FLSG）的博弈论框架，该框架同时考虑到联邦学习的保护者和攻击者的收益，包括计算成本、FL模型效用和隐私泄漏风险，并提出了一个实用算法来近似oracle并保持隐私。研究表明该算法对于预防和检测现实世界中的联邦学习攻击具有有效性。

    

    在联邦学习中，良性参与者旨在协同优化全局模型。然而，在存在半诚实的对手时，\textit{隐私泄漏}的风险是不可忽视的。现有研究要么专注于设计保护机制，要么专注于发明攻击机制。虽然保护者与攻击者之间的斗争似乎永无止境，但我们关心一个关键问题：是否可能事先预防潜在的攻击？为了解决这个问题，我们提出了一个博弈论框架，同时考虑FL保护者和攻击者的相应收益，其中包括计算成本、FL模型效用和隐私泄漏风险。我们将此游戏称为联邦学习安全博弈（FLSG），在其中保护者和攻击者都不知道所有参与者的收益。为了处理这种情况固有的\textit{不完全信息}，我们建议将FLSG与一个\textit{oracle}相关联，该oracle具有所有参与者的收益知识。我们分析了在各种效用函数和攻击模型组合下FLSG的纳什均衡存在性和唯一性。此外，我们提出了一个实用算法来近似oracle并保持隐私。实验结果说明了我们的算法在预防和检测现实世界中的FL场景中的攻击方面的有效性。

    In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
    
[^33]: DiscoGen: 学习发现基因调控网络

    DiscoGen: Learning to Discover Gene Regulatory Networks. (arXiv:2304.05823v1 [q-bio.MN])

    [http://arxiv.org/abs/2304.05823](http://arxiv.org/abs/2304.05823)

    本文中介绍了DiscoGen，它是一种神经网络-based GRN发现方法，可以处理嘈杂的基因表达测量数据并处理干预数据，能够更准确地识别GRN。

    

    准确地推断基因调控网络（GRN）是生物学中关键且具有挑战性的任务。GRN模型描述了基因之间的促进和抑制作用，并天然地具有因果关系。为了准确地识别GRN，需要进行干预数据。然而，大多数GRN发现方法只是基于观察数据。最近，基于神经网络的因果发现方法的进步显著改善了因果发现的准确性，包括处理干预数据、改善性能和可伸缩性。然而，在生物学中应用最先进（SOTA）的因果发现方法存在着挑战，例如嘈杂的数据和大量的样本。因此，必须改进因果发现方法来处理这些挑战。在本文中，我们介绍了DiscoGen，它是一种基于神经网络的GRN发现方法，可以去噪基因表达测量数据并处理干预数据。我们证明了我们的模型优于最先进的神经网络因果发现方法。

    Accurately inferring Gene Regulatory Networks (GRNs) is a critical and challenging task in biology. GRNs model the activatory and inhibitory interactions between genes and are inherently causal in nature. To accurately identify GRNs, perturbational data is required. However, most GRN discovery methods only operate on observational data. Recent advances in neural network-based causal discovery methods have significantly improved causal discovery, including handling interventional data, improvements in performance and scalability. However, applying state-of-the-art (SOTA) causal discovery methods in biology poses challenges, such as noisy data and a large number of samples. Thus, adapting the causal discovery methods is necessary to handle these challenges. In this paper, we introduce DiscoGen, a neural network-based GRN discovery method that can denoise gene expression measurements and handle interventional data. We demonstrate that our model outperforms SOTA neural network-based causal
    
[^34]: 用人工神经网络预测国内生产总值：长期记忆有多大的作用？

    GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])

    [http://arxiv.org/abs/2304.05805](http://arxiv.org/abs/2304.05805)

    通过比较四种人工神经网络和动态因子模型对美国GDP季度增长的预测表现，研究发现在平衡经济增长期间，更长的输入序列能够实现更准确的预测，但是这种效果会在不到两年的时间内消失。在经济动荡时期，长期记忆的效果变得明显。

    

    在本研究中，我们将不同的统计模型应用于美国经济季度国内生产总值（GDP）增长预测。使用每月的FRED-MD数据库，我们比较了动态因子模型（DFM）和四个人工神经网络（ANNs）的预测表现：多层感知机（MLP）、一维卷积神经网络（1D CNN）、长短期记忆网络（LSTM）和门控循环单元（GRU）。实证分析呈现了两个不同评估周期的结果。第一个周期（2010年第1季度至2019年第4季度）具有平衡的经济增长，而第二个周期（2010年第1季度至2022年第3季度）还包括COVID-19衰退期间的时间。根据我们的结果，更长的输入序列在平衡经济增长期间能够实现更准确的预测。然而，在一个相对较低的阈值值（约六个季度或十八个月）以后，这种效应会消失。在经济动荡期（如COVID-19衰退期间），长期记忆的效果会变得较为明显。

    In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
    
[^35]: Proximity Forest 2.0：一种新的有效且可扩展的基于相似性的时间序列分类器

    Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series. (arXiv:2304.05800v1 [cs.LG])

    [http://arxiv.org/abs/2304.05800](http://arxiv.org/abs/2304.05800)

    Proximity Forest 2.0是一种新的有效且可扩展的基于相似性的时间序列分类器，优于先前最先进的基于相似性的分类器以及最先进的基于内核、神经网络和混合方法在特定数据集上的表现。

    

    时间序列分类（TSC）由于可能与不同分类任务相关的特征类型的多样性而具有挑战性，包括趋势、方差、频率、幅度和各种模式。为了应对这一挑战，已经开发了几种替代方法类别，包括基于相似性、特征和间隔、形状、字典、内核、神经网络和混合方法。本文提出了一种新的基于相似性的分类器Proximity Forest版本2.0（PF 2.0），它在UCR基准测试中优于先前最先进的基于相似性的分类器，并在基准测试中优于最先进的基于内核、神经网络和混合方法的特定数据集，这些数据集最适合使用基于相似性的方法。PF 2.0 合并了时间序列相似性最近的三个进展……

    Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed, including similarity-based, features and intervals, shapelets, dictionary, kernel, neural network, and hybrid approaches. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms state-of-the-art kernel, neural network, and hybrid methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series simi
    
[^36]: 深度神经网络在无“维度诅咒”情况下逼近复合函数

    Deep neural network approximation of composite functions without the curse of dimensionality. (arXiv:2304.05790v1 [math.NA])

    [http://arxiv.org/abs/2304.05790](http://arxiv.org/abs/2304.05790)

    本文发现了一种高维连续函数，可以用DNN逼近而无需担心“维度诅咒”，方法是将许多特殊函数组合在一起。

    

    本文鉴定了一类高维连续函数，可以用带有修正线性单元（ReLU）激活的深度神经网络（DNN）逼近，无需担心“维度诅咒”。换言之，DNN参数的数量最多在输入维度和逼近误差中以多项式形式增长。我们的类别包括一些特殊函数的无限组合，包括乘积、极大值和一定范围内的Lipschitz连续函数。

    In this article we identify a general class of high-dimensional continuous functions that can be approximated by deep neural networks (DNNs) with the rectified linear unit (ReLU) activation without the curse of dimensionality. In other words, the number of DNN parameters grows at most polynomially in the input dimension and the approximation error. The functions in our class can be expressed as a potentially unbounded number of compositions of special functions which include products, maxima, and certain parallelized Lipschitz continuous functions.
    
[^37]: 线性卷积网络的函数空间和临界点

    Function Space and Critical Points of Linear Convolutional Networks. (arXiv:2304.05752v1 [cs.LG])

    [http://arxiv.org/abs/2304.05752](http://arxiv.org/abs/2304.05752)

    研究了具有一维卷积层的线性网络的函数空间，分析了网络架构对函数空间的影响并证明了对于步幅大于一且数据一般的架构，该优化问题的非零临界点是函数空间的平滑内部点。

    

    我们研究了具有一维卷积层的线性网络的几何结构。这些网络的函数空间可以被认为是具有稀疏因子分解的半代数多项式族。我们分析了网络架构对函数空间的维度、边界和奇异点的影响。我们还描述了网络参数化映射的临界点。此外，我们研究了使用平方误差损失训练网络的优化问题。我们证明了对于所有步幅大于一且数据一般的架构，该优化问题的非零临界点是函数空间的平滑内部点。对于稠密的线性网络和步幅为一的线性卷积网络，这种特性被认为是错误的。

    We study the geometry of linear networks with one-dimensional convolutional layers. The function spaces of these networks can be identified with semi-algebraic families of polynomials admitting sparse factorizations. We analyze the impact of the network's architecture on the function space's dimension, boundary, and singular points. We also describe the critical points of the network's parameterization map. Furthermore, we study the optimization problem of training a network with the squared error loss. We prove that for architectures where all strides are larger than one and generic data, the non-zero critical points of that optimization problem are smooth interior points of the function space. This property is known to be false for dense linear networks and linear convolutional networks with stride one.
    
[^38]: 通过数据增强提高连续时间动态图网络长期预测性能

    Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation. (arXiv:2304.05749v1 [cs.LG])

    [http://arxiv.org/abs/2304.05749](http://arxiv.org/abs/2304.05749)

    本研究提出了一种插入式模块——不确定性掩蔽混合（UmmU），它能够在中间层嵌入中进行不确定性估计，进而增强嵌入的不确定性，使其具有更好的泛化性能，从而显著提高了连续时间动态图网络的长期预测性能。

    

    本研究侧重于对连续时间动态图网络（CTDGNs）进行长期预测（LTF），这对于现实世界建模非常重要。现有的CTDGN对于建模时间图数据非常有效，因为它们能够捕捉复杂的时间相关性，但由于对历史数据的实质要求，它们在LTF方面表现不佳，在大多数情况下也不切实际。为了解决这个问题，最直观的方法是数据增强。在本研究中，我们提出了一种插入式模块——不确定性掩蔽混合（UmmU），它能够进行不确定性估计，将不确定性引入CTDGN的中间层嵌入中，并进行掩蔽混合以进一步增强嵌入的不确定性，使其能够适用于更多情况，而且可以轻松地插入到任意CTDGN中，而不增加参数数量。我们在三个真实世界的动态图数据集上进行了全面的实验，并表明UmmU在LTF任务上明显优于现有方法。

    This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph dat
    
[^39]: 跨领域疾病分类的小样本增量学习

    Few-shot Class-incremental Learning for Cross-domain Disease Classification. (arXiv:2304.05734v1 [cs.CV])

    [http://arxiv.org/abs/2304.05734](http://arxiv.org/abs/2304.05734)

    本研究提出了一种跨域增强约束和跨域数据增强方法，以解决跨领域少样本增量学习问题，该方法在MedMNIST实验中表现出较好的性能。

    

    从有限的样本中增量学习新类别的能力对于开发用于实际临床应用的人工智能系统至关重要。虽然现有增量学习技术已经尝试解决这个问题，但是当样本来自不同的领域并且样本标记较少时，它们仍然存在困难。在本文中，我们探讨了跨域少样本增量学习问题。跨域少样本增量学习需要模型从极少量标记样本中增量学习新的类别，并且这些新的类别可能与目标空间大不相同。为了解决这个难题，我们提出了一种跨域增强约束和跨域数据增强方法。在MedMNIST实验中，该方法的分类性能优于其他类似的增量学习方法。

    The ability to incrementally learn new classes from limited samples is crucial to the development of artificial intelligence systems for real clinical application. Although existing incremental learning techniques have attempted to address this issue, they still struggle with only few labeled data, particularly when the samples are from varied domains. In this paper, we explore the cross-domain few-shot incremental learning (CDFSCIL) problem. CDFSCIL requires models to learn new classes from very few labeled samples incrementally, and the new classes may be vastly different from the target space. To counteract this difficulty, we propose a cross-domain enhancement constraint and cross-domain data augmentation method. Experiments on MedMNIST show that the classification performance of this method is better than other similar incremental learning methods.
    
[^40]: 动态图表示学习综述：基于神经网络的方法

    Dynamic Graph Representation Learning with Neural Networks: A Survey. (arXiv:2304.05729v1 [cs.LG])

    [http://arxiv.org/abs/2304.05729](http://arxiv.org/abs/2304.05729)

    这篇综述文章总结了近年来动态图表示学习的发展，介绍了动态图的重要性和状态，并回顾了各种动态图学习方法，包括监督、无监督和自监督学习。文章还探讨了动态图学习的当前问题和未来研究方向。

    

    近年来，动态图表征由于其能够将拓扑信息和时间信息集成为一个紧凑的表示，因此在建模动态系统方面得到了越来越广泛的应用。动态图的出现使得可以高效地处理诸如社交网络预测、推荐系统、交通预测或脑电图分析等无法通过标准数字表示解决的应用。随着动态图表示的出现，动态图学习已经成为一种新的机器学习问题，结合了顺序/时间数据处理和静态图学习的挑战。在这个领域中，动态图神经网络是最先进的方法，近年来提出了大量的模型。本文旨在综述与动态图学习相关的问题和模型，并回顾各种动态图监督、无监督和自监督学习方法。最后，我们讨论了当前的开放问题和基于DGNN的动态图表示学习的未来研究方向。

    In recent years, Dynamic Graph (DG) representations have been increasingly used for modeling dynamic systems due to their ability to integrate both topological and temporal information in a compact representation. Dynamic graphs allow to efficiently handle applications such as social network prediction, recommender systems, traffic forecasting or electroencephalography analysis, that can not be adressed using standard numeric representations. As a direct consequence of the emergence of dynamic graph representations, dynamic graph learning has emerged as a new machine learning problem, combining challenges from both sequential/temporal data processing and static graph learning. In this research area, Dynamic Graph Neural Network (DGNN) has became the state of the art approach and plethora of models have been proposed in the very recent years. This paper aims at providing a review of problems and models related to dynamic graph learning. The various dynamic graph supervised learning sett
    
[^41]: 在深度神经网络中预防性修剪Clever Hans策略

    Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])

    [http://arxiv.org/abs/2304.05727](http://arxiv.org/abs/2304.05727)

    本文提出了一种新方法，Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化，从而大大减少了对隐藏Clever Hans策略的依赖，并实现了更高的性能。

    

    可解释的AI已成为验证机器学习模型的流行工具。解释模型的决策策略与用户的领域知识之间的不匹配（例如Clever Hans效应）也被认为是改进错误模型的起点。然而，当用户和解释达成一致时，要怎么做就不那么清楚了。本文通过展示用户接受解释并不保证ML模型的良好功能，特别是一些隐藏的Clever Hans效应可能仍然未被发现，证明了这一点。我们通过贡献一个新方法Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化。自然画像数据的实验表明，我们的方法导致模型大大减少了对隐藏的Clever Hans策略的依赖，并因此实现了更高的性能。

    Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
    
[^42]: 正则化和多视角支持向量机学习的本地化

    Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])

    [http://arxiv.org/abs/2304.05655](http://arxiv.org/abs/2304.05655)

    本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。

    

    本文证明了 H.Q. Minh、L. Bazzani 和 V. Murino 在《机器学习研究》（Journal of Machine Learning Research）中介绍的一种涉及算子值正定核及其再生核希尔伯特空间的正则化和多视角支持向量机学习问题的本地化版本的一些表示定理。结果涉及到考虑凸或非凸损失函数以及有限或无限维输入空间的一般情况。我们展示了该一般框架允许一些特殊情况下的无限维输入空间和非凸损失函数，特别是当损失函数为 Gâteaux 可微函数时。对导致部分非线性问题的指数最小二乘损失函数进行了详细计算。

    We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
    
[^43]: PLC基控制过程中的自我优化和自动代码生成的进化算法

    Self Optimisation and Automatic Code Generation by Evolutionary Algorithms in PLC based Controlling Processes. (arXiv:2304.05638v1 [cs.NE])

    [http://arxiv.org/abs/2304.05638](http://arxiv.org/abs/2304.05638)

    提出了一种基于遗传算法的方法，用于自我优化复杂过程的系统逻辑。该方法能够解析遗传结果，生成程序代码实现系统，实现了多目标优化问题。

    

    自动化的数字化转型对工业过程中的数据采集和处理提出了新的要求。获得的数据与周期性的过程序列之间的逻辑关系必须得到正确的解释和评估。为了解决这个问题，提出了一种基于进化算法的新方法，用于自我优化复杂过程的系统逻辑。基于遗传结果，通过解码解决方案来得到系统实现的程序代码。这是通过一个具有上游、中游和下游单元的灵活系统结构实现的。在中间单元中，一个有向学习过程与系统副本和评估函数在闭环中交互。代码生成策略由冗余和优先级、排序和性能派生表示。该方法在一个工业液体站过程上进行了评估，该过程面临多目标优化问题。

    The digital transformation of automation places new demands on data acquisition and processing in industrial processes. Logical relationships between acquired data and cyclic process sequences must be correctly interpreted and evaluated. To solve this problem, a novel approach based on evolutionary algorithms is proposed to self optimise the system logic of complex processes. Based on the genetic results, a programme code for the system implementation is derived by decoding the solution. This is achieved by a flexible system structure with an upstream, intermediate and downstream unit. In the intermediate unit, a directed learning process interacts with a system replica and an evaluation function in a closed loop. The code generation strategy is represented by redundancy and priority, sequencing and performance derivation. The presented approach is evaluated on an industrial liquid station process subject to a multi-objective optimisation problem.
    
[^44]: 具有理论保障的多智能体策略互惠算法

    Multi-agent Policy Reciprocity with Theoretical Guarantee. (arXiv:2304.05632v1 [cs.AI])

    [http://arxiv.org/abs/2304.05632](http://arxiv.org/abs/2304.05632)

    本论文提出了一种新的多智能体策略互惠（PR）框架，能够通过定义邻接空间和设计即插即用模块在不匹配的状态下充分利用交叉智能体策略，提高了性能，同时具有理论保障，能在个体感知奖励的情况下稳定收敛于最优值函数。

    

    现代多智能体强化学习算法具有解决各种实际问题的潜力，但它们未能充分利用交叉智能体知识来降低样本复杂度和提高性能。为了解决这个问题，我们提出了一种新的多智能体策略互惠（PR）框架，每个智能体可以充分利用交叉智能体策略，即使在不匹配的状态下。我们定义了一种不匹配状态的邻接空间，并设计了一个用于价值迭代的即插即用模块，使智能体可以推断更精确的回报。为了提高PR的可扩展性，我们提出了连续控制任务的深层PR算法。另外，理论分析表明，在个体感知奖励的情况下，智能体可以渐进地达成共识并收敛于最优值函数，这分别意味着PR的稳定性和有效性。实验结果表明，PR在多个基准任务上取得了优异的性能。

    Modern multi-agent reinforcement learning (RL) algorithms hold great potential for solving a variety of real-world problems. However, they do not fully exploit cross-agent knowledge to reduce sample complexity and improve performance. Although transfer RL supports knowledge sharing, it is hyperparameter sensitive and complex. To solve this problem, we propose a novel multi-agent policy reciprocity (PR) framework, where each agent can fully exploit cross-agent policies even in mismatched states. We then define an adjacency space for mismatched states and design a plug-and-play module for value iteration, which enables agents to infer more precise returns. To improve the scalability of PR, deep PR is proposed for continuous control tasks. Moreover, theoretical analysis shows that agents can asymptotically reach consensus through individual perceived rewards and converge to an optimal value function, which implies the stability and effectiveness of PR, respectively. Experimental results o
    
[^45]: SAMM（Segment Any Medical Model）：用于SAM的3D Slicer集成

    SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])

    [http://arxiv.org/abs/2304.05622](http://arxiv.org/abs/2304.05622)

    介绍了Segment Any Medical Model (SAMM)，它是用于3D Slicer的SAM的扩展。SAMM在医学图像分割上表现良好，在实时性和通用性方面都有很好的性能，可以推断出掩模。

    

    Segment Anything Model（SAM）是一个新的图像分割工具，使用迄今为止最大的分割数据集进行训练。该模型表明它可以创建高质量的图像分割掩模，具有良好的实时性和通用性。然而，在医学图像上的性能需要进一步验证。为了协助在医学图像上开发，评估和利用SAM，我们介绍了Segment Any Medical Model（SAMM），它是SAM在3D Slicer上的扩展。3D Slicer是一个广泛使用于医学影像处理和可视化软件的开源软件。这个开源扩展程序及其演示已发布在GitHub上（https://github.com/bingogome/samm）。SAMM在完整周期中实现了0.6秒的延迟，并可以实时推断出图像掩模。

    The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
    
[^46]: 相貌相似，声音不同：利用反事实跨模态对学习音视频表示

    Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning. (arXiv:2304.05600v1 [cs.SD])

    [http://arxiv.org/abs/2304.05600](http://arxiv.org/abs/2304.05600)

    该论文通过增加配音来增强跨模态对比学习，结果表明这种方法可以提高音频和视听任务的性能。

    

    音视频表示学习通常依赖于视听之间的对应关系。然而，在一个视觉场景中可能存在多个声音轨道与之对应。例如，在同一拥挤的街道上有不同的交谈声。这些反事实对于音视频表示学习的影响尚未研究。为了研究这个问题，我们使用电影的配音版本来增加跨模态对比学习的方法。我们的方法学习表示类似于相同视频的仅在语音内容上不同的替代音轨。我们的实验结果表明，增加配音的训练在一系列听觉和视听任务中提高了性能，而对语言任务的整体表现影响不大。我们还将这种方法与在预训练之前去除语音的强大基线进行了比较，发现增加配音的训练更有效，包括语音外语和视听任务，其中语音恢复任务性能提高了。

    Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech re
    
[^47]: 利用可微编程和机器学习的多物理反演技术的学习

    Learned multiphysics inversion with differentiable programming and machine learning. (arXiv:2304.05592v1 [cs.MS])

    [http://arxiv.org/abs/2304.05592](http://arxiv.org/abs/2304.05592)

    本文提出了一个名为SLIM的开源软件框架，将可微编程和机器学习应用于多物理反演技术，提出了一个可伸缩的原型用于从时间连续的跨井地震数据中估计渗透率。

    

    我们提出了一个名为Seismic Laboratory for Imaging and Modeling/Monitoring (SLIM)的开源软件框架，用于计算地球物理学和更一般的包括波动方程逆问题(例如地震和医学超声)、学习先验正则化和学习神经代理多相流模拟。通过整合多种抽象层次，我们的软件旨在既可读性又可伸缩性。这使得研究人员可以轻松地以抽象的方式构思并利用最新的高性能计算技术。我们通过建立一个可扩展的原型来从时间连续的跨井地震数据中估计渗透率来说明和证明我们的设计原则及其益处，该原型涉及波动物理和多相流的耦合以及机器学习。

    We present the Seismic Laboratory for Imaging and Modeling/Monitoring (SLIM) open-source software framework for computational geophysics and, more generally, inverse problems involving the wave-equation (e.g., seismic and medical ultrasound), regularization with learned priors, and learned neural surrogates for multiphase flow simulations. By integrating multiple layers of abstraction, our software is designed to be both readable and scalable. This allows researchers to easily formulate their problems in an abstract fashion while exploiting the latest developments in high-performance computing. We illustrate and demonstrate our design principles and their benefits by means of building a scalable prototype for permeability inversion from time-lapse crosswell seismic data, which aside from coupling of wave physics and multiphase flow, involves machine learning.
    
[^48]: FLAN-T5中语义特征验证的研究

    Semantic Feature Verification in FLAN-T5. (arXiv:2304.05591v1 [cs.CL])

    [http://arxiv.org/abs/2304.05591](http://arxiv.org/abs/2304.05591)

    本研究表明大规模语言模型可以极大地增强传统的语义特征规范验证方法，使之能够捕捉超过人类规范范畴的信息，对于我们理解人类和机器的概念表示具有重要意义。

    

    本研究评估了大规模语言模型在生成语义特征规范方面的潜力——这是评价认知科学中概念结构的关键工具。我们基于现有的人工生成数据集进行构建，结果表明，机器验证的规范捕捉了概念结构的某些方面，超越了仅考虑人类规范所能涵盖的范围，并且更好地解释了在远距离相关的项目之间的语义相似度人类判断。该结果表明LLM可以极大地增强传统的语义特征规范验证方法，这对于我们理解人类和机器的概念表示具有重要意义。

    This study evaluates the potential of a large language model for aiding in generation of semantic feature norms - a critical tool for evaluating conceptual structure in cognitive science. Building from an existing human-generated dataset, we show that machine-verified norms capture aspects of conceptual structure beyond what is expressed in human norms alone, and better explain human judgments of semantic similarity amongst items that are distally related. The results suggest that LLMs can greatly enhance traditional methods of semantic feature norm verification, with implications for our understanding of conceptual representation in humans and machines.
    
[^49]: 信息量是否重要？基于主动学习的教育对话行为分类

    Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification. (arXiv:2304.05578v1 [cs.CL])

    [http://arxiv.org/abs/2304.05578](http://arxiv.org/abs/2304.05578)

    本文研究基于主动学习方法的教育对话行为分类，提出了一种新的方法来选择信息样本，并且能够优于随机抽样方法和其他AL方法。

    

    基于对话行为（DA），可以解释专家导师在辅导过程中做了什么以及学生知道什么。然而，现有的实证研究多采用随机抽样法来获得手动注释DA的句子样本，然后用于培训DA分类器。本文提出了一种基于主动学习（AL）方法，用于选择教育对话行为分类的信息样本。结果表明，他们的方法优于随机抽样方法和其他最新的AL方法。

    Dialogue Acts (DAs) can be used to explain what expert tutors do and what students know during the tutoring process. Most empirical studies adopt the random sampling method to obtain sentence samples for manual annotation of DAs, which are then used to train DA classifiers. However, these studies have paid little attention to sample informativeness, which can reflect the information quantity of the selected samples and inform the extent to which a classifier can learn patterns. Notably, the informativeness level may vary among the samples and the classifier might only need a small amount of low informative samples to learn the patterns. Random sampling may overlook sample informativeness, which consumes human labelling costs and contributes less to training the classifiers. As an alternative, researchers suggest employing statistical sampling methods of Active Learning (AL) to identify the informative samples for training the classifiers. However, the use of AL methods in educational D
    
[^50]: 利用机器学习算法的预测模型来确定学生通过学期课程的概率

    A Predictive Model using Machine Learning Algorithm in Identifying Students Probability on Passing Semestral Course. (arXiv:2304.05565v1 [cs.LG])

    [http://arxiv.org/abs/2304.05565](http://arxiv.org/abs/2304.05565)

    本研究基于分类数据挖掘技术和决策树算法，提出一个预测模型，可以预测学生在学期初阶段通过所选课程的概率，该模型可靠、准确、可推荐。

    

    本研究旨在确定一个预测模型，以在学期初阶段学生通过所选课程的概率。该研究采用分类数据挖掘技术和决策树算法。利用新发现的预测模型，学生通过当前课程的预测准确率为0.7619，精度为0.8333，召回率为0.8823，f1分数为0.8571，表明预测模型是可靠、准确和可推荐的。

    This study aims to determine a predictive model to learn students probability to pass their courses taken at the earliest stage of the semester. To successfully discover a good predictive model with high acceptability, accurate, and precision rate which delivers a useful outcome for decision making in education systems, in improving the processes of conveying knowledge and uplifting students academic performance, the proponent applies and strictly followed the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology. This study employs classification for data mining techniques, and decision tree for algorithm. With the utilization of the newly discovered predictive model, the prediction of students probabilities to pass the current courses they take gives 0.7619 accuracy, 0.8333 precision, 0.8823 recall, and 0.8571 f1 score, which shows that the model used in the prediction is reliable, accurate, and recommendable. Considering the indicators and the results, it can be not
    
[^51]: 关于深度生物特征表示的对抗性反演

    On the Adversarial Inversion of Deep Biometric Representations. (arXiv:2304.05561v1 [cs.CV])

    [http://arxiv.org/abs/2304.05561](http://arxiv.org/abs/2304.05561)

    本研究研究了对抗性背景下，无法直接访问原始数据集和模型的情况下的生物特征的反演，证明在一定程度的攻击背景知识下，黑盒模型生物特征表示仍然很容易被反演。

    

    生物认证服务提供商通常声称，无法从其数学（特征空间）表示提取出用户的原始生物特征样本，例如指纹或面部图像。本文在深度神经网络（DNN）嵌入的特定示例上研究了这一主张。现有研究利用了原始模型的所有层以及所有可能的数据集信息，研究反演DNN嵌入以解释深度图像表示或合成标准化图像。对于生物认证用例，我们需要在对抗性设置下进行研究，其中攻击者可以访问特征空间表示，但无法直接访问确切的原始数据集或原始学习模型。

    Biometric authentication service providers often claim that it is not possible to reverse-engineer a user's raw biometric sample, such as a fingerprint or a face image, from its mathematical (feature-space) representation. In this paper, we investigate this claim on the specific example of deep neural network (DNN) embeddings. Inversion of DNN embeddings has been investigated for explaining deep image representations or synthesizing normalized images. Existing studies leverage full access to all layers of the original model, as well as all possible information on the original dataset. For the biometric authentication use case, we need to investigate this under adversarial settings where an attacker has access to a feature-space representation but no direct access to the exact original dataset nor the original learned model. Instead, we assume varying degree of attacker's background knowledge about the distribution of the dataset as well as the original learned model (architecture and t
    
[^52]: 分类学类别增量学习

    Taxonomic Class Incremental Learning. (arXiv:2304.05547v1 [cs.LG])

    [http://arxiv.org/abs/2304.05547](http://arxiv.org/abs/2304.05547)

    研究提出一种基于分类学类别树的学习任务设置方法，命名为分类学类别增量学习（TCIL），使用参数继承方案实现对祖先类别知识对后代类别的增量转移，相对于现有SOTA方法，在CIFAR-100和ImageNet-100上，实现了2％和3％的准确率提升。

    

    近年来，连续学习问题引起了越来越多的关注。然而，很少有人质疑基于随机类任务课程的常用学习设置。这与人类连续学习有很大区别，后者是由分类学课程引导的。在本文中，我们提出了分类学类别增量学习（TCIL）问题。在TCIL中，任务序列基于分类学类别树进行组织。我们将现有的CIL和分类学习方法统一为参数继承方案，并引入一种新的TCIL学习方案。这使得从类别分类学的祖先转移到子孙类别的知识的增量转移成为可能。在CIFAR-100和ImageNet-100上的实验表明，所提出的TCIL方法具有很好的效果，其最终准确率在CIFAR-100上比现有SOTA方法高出2％，在ImageNet-100上比现有SOTA方法高出3％。

    The problem of continual learning has attracted rising attention in recent years. However, few works have questioned the commonly used learning setup, based on a task curriculum of random class. This differs significantly from human continual learning, which is guided by taxonomic curricula. In this work, we propose the Taxonomic Class Incremental Learning (TCIL) problem. In TCIL, the task sequence is organized based on a taxonomic class tree. We unify existing approaches to CIL and taxonomic learning as parameter inheritance schemes and introduce a new such scheme for the TCIL learning. This enables the incremental transfer of knowledge from ancestor to descendant class of a class taxonomy through parameter inheritance. Experiments on CIFAR-100 and ImageNet-100 show the effectiveness of the proposed TCIL method, which outperforms existing SOTA methods by 2% in terms of final accuracy on CIFAR-100 and 3% on ImageNet-100.
    
[^53]: MEMA运行时框架：在微控制器上实现TinyML时最小化外部内存访问

    MEMA Runtime Framework: Minimizing External Memory Accesses for TinyML on Microcontrollers. (arXiv:2304.05544v1 [cs.LG])

    [http://arxiv.org/abs/2304.05544](http://arxiv.org/abs/2304.05544)

    我们提出了MEMA框架，可以快速且容易地推导出在微控制器上实现TinyML时最小化外部内存访问的有效运行时，这可以使神经网络基准测试在ARM Cortex-M4上加速高达1.8倍和减少44％的能量消耗。

    

    我们提出了MEMA框架，用于在TinyML系统上矩阵乘法的推断运行时快速且容易地推导出最小化外部内存访问的有效运行时。该框架考虑硬件资源限制和问题大小，通过分析确定最优化的调度和内核，以最小化内存访问。MEMA提供了当前实践中已知问题的解决方案，即仅通过耗时且启发式的搜索大量调度空间才能找到最优调度。我们将MEMA推导出的运行时性能与基于ARM的TinyML系统上现有的最先进的库进行了比较。例如在ARM Cortex-M4上进行神经网络基准测试，我们实现了比CMSIS-NN更高达1.8倍的加速和44％的能量减少。

    We present the MEMA framework for the easy and quick derivation of efficient inference runtimes that minimize external memory accesses for matrix multiplication on TinyML systems. The framework accounts for hardware resource constraints and problem sizes in analytically determining optimized schedules and kernels that minimize memory accesses. MEMA provides a solution to a well-known problem in the current practice, that is, optimal schedules tend to be found only through a time consuming and heuristic search of a large scheduling space. We compare the performance of runtimes derived from MEMA to existing state-of-the-art libraries on ARM-based TinyML systems. For example, for neural network benchmarks on the ARM Cortex-M4, we achieve up to a 1.8x speedup and 44% energy reduction over CMSIS-NN.
    
[^54]: CLCLSA：交叉组学链接的对比学习和自注意力多组学数据不完整情况下一体化方法

    CLCLSA: Cross-omics Linked embedding with Contrastive Learning and Self Attention for multi-omics integration with incomplete multi-omics data. (arXiv:2304.05542v1 [cs.LG])

    [http://arxiv.org/abs/2304.05542](http://arxiv.org/abs/2304.05542)

    本文提出了一种基于对比学习和自注意力的深度学习方法CLCLSA，可用于不完整多组学数据的一体化，达到最先进的性能。

    

    高维异质多组学数据的整合对于理解遗传数据变得越来越重要。然而，多组学数据集中存在未配对的数据，这是由于仪器灵敏度和成本所导致的。本文提出了一种基于对比学习和自注意力的深度学习方法CLCLSA，可用于不完整多组学数据的一体化。该方法利用完整数据作为监督，采用交叉组学自编码器跨越不同类型的生物数据学习特征表示，然后引入多组学对比学习目标，联合学习已配对和未配对数据的公共表示，有效地集成不完整数据。CLCLSA在多个基准多组学数据集上达到了最先进的性能。

    Integration of heterogeneous and high-dimensional multi-omics data is becoming increasingly important in understanding genetic data. Each omics technique only provides a limited view of the underlying biological process and integrating heterogeneous omics layers simultaneously would lead to a more comprehensive and detailed understanding of diseases and phenotypes. However, one obstacle faced when performing multi-omics data integration is the existence of unpaired multi-omics data due to instrument sensitivity and cost. Studies may fail if certain aspects of the subjects are missing or incomplete. In this paper, we propose a deep learning method for multi-omics integration with incomplete data by Cross-omics Linked unified embedding with Contrastive Learning and Self Attention (CLCLSA). Utilizing complete multi-omics data as supervision, the model employs cross-omics autoencoders to learn the feature representation across different types of biological data. The multi-omics contrastive
    
[^55]: 一种使用确定性目标的黑匣子变分推断：更快，更精确，更黑。

    Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])

    [http://arxiv.org/abs/2304.05527](http://arxiv.org/abs/2304.05527)

    本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。

    

    自动微分变分推断（ADVI）提供了多种现代概率编程语言中快速易用的后验近似方法。然而它的随机优化器缺乏明确的收敛标准，并且需要调整参数。此外，ADVI继承了均值场变分贝叶斯（MFVB）的较差后验不确定性估计。我们引入了“确定性ADVI”（DADVI）来解决这些问题。DADVI用固定的蒙特卡罗近似替换了MFVB的不可解目标，这一技术在随机优化文献中被称为“样本平均近似”（SAA）。通过优化近似但确定的目标，DADVI可以使用现成的二阶优化，而且与标准均值场ADVI不同的是，可以适用于更准确的后验线性响应（LR）协方差估计。与现有的最坏情况理论相反，我们表明，在某些常见的统计问题类别上，DADVI和SAA可以表现得更好。

    Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
    
[^56]: 用大规模语言模型理解因果关系: 可行性和机遇

    Understanding Causality with Large Language Models: Feasibility and Opportunities. (arXiv:2304.05524v1 [cs.LG])

    [http://arxiv.org/abs/2304.05524](http://arxiv.org/abs/2304.05524)

    本文评估了大型语言模型回答因果问题的能力，发现它们可以回答基于已有因果知识的问题，但对于发现新知识或高风险决策任务的高精度要求不足。未来可以通过启用显式和隐式的因果模块以及深度因果感知的LLMs来解决这些问题。

    

    本文通过分析大型语言模型(LLMs)在回答三种类型因果问题时的优缺点，评估了它们回答因果问题的能力。我们认为，当前的LLMs可以像领域专家一样回答基于已有因果知识的因果问题。但是，它们还不能为发现新知识或高精度高风险决策任务提供令人满意的答案。我们讨论了未来可能的方向和机遇，例如启用显式和隐式的因果模块以及深度因果感知的LLMs。这些不仅可以使LLMs回答更多类型的因果问题以实现更大的影响力，还可以使LLMs在一般情况下更加值得信任和高效。

    We assess the ability of large language models (LLMs) to answer causal questions by analyzing their strengths and weaknesses against three types of causal question. We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision. We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs. These will not only enable LLMs to answer many different types of causal questions for greater impact but also enable LLMs to be more trustworthy and efficient in general.
    
[^57]: 邻居的回响：基于Shuffle模型的个性化隐私保护联邦学习中的隐私扩增

    Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model. (arXiv:2304.05516v1 [cs.CR])

    [http://arxiv.org/abs/2304.05516](http://arxiv.org/abs/2304.05516)

    本文提出了一个个性化隐私保护联邦学习框架，可在保证本地隐私的同时实现强中心隐私保证，并利用Shuffle模型进行隐私扩增。

    

    联邦学习是一种流行的协同训练范例，但会受到隐私攻击。为了满足用户对于不同隐私需求的本地需求，需要保留个性化的本地差分隐私，同时还需要为全局模型提供严格的隐私保证。本文提出了一个通用框架（APES）来加强个性化本地隐私保护条件下的模型隐私，利用Shuffle模型的隐私扩增效果。为了增强隐私保证，我们量化每个用户对中心隐私的异构贡献，并通过扰动“回声”来描述用户的特征。在各种数据集上的实验表明了我们的框架的有效性。

    Federated Learning, as a popular paradigm for collaborative training, is vulnerable against privacy attacks. Different privacy levels regarding users' attitudes need to be satisfied locally, while a strict privacy guarantee for the global model is also required centrally. Personalized Local Differential Privacy (PLDP) is suitable for preserving users' varying local privacy, yet only provides a central privacy guarantee equivalent to the worst-case local privacy level. Thus, achieving strong central privacy as well as personalized local privacy with a utility-promising model is a challenging problem. In this work, a general framework (APES) is built up to strengthen model privacy under personalized local privacy by leveraging the privacy amplification effect of the shuffle model. To tighten the privacy bound, we quantify the heterogeneous contributions to the central privacy user by user. The contributions are characterized by the ability of generating "echos" from the perturbation of e
    
[^58]: 利用稀疏性和数据流高效训练大型语言模型

    Training Large Language Models Efficiently with Sparsity and Dataflow. (arXiv:2304.05511v1 [cs.LG])

    [http://arxiv.org/abs/2304.05511](http://arxiv.org/abs/2304.05511)

    本论文提出了一种利用稀疏性和数据流训练大型语言模型的方法，不仅可以有效利用计算资源，提高计算效率，还能达到与稠密模型类似的结果。

    

    针对大型基础语言模型的训练需要大量计算资源和机器学习专家的专业知识, 稀疏性技术可以缓解训练中的计算要求，但稀疏性会引入新的挑战。本文提出了一种基于稀疏性和数据流的端到端训练方法，在有效利用计算资源的同时，实现13亿GPT语言模型训练，且稀疏性技术和稠密模型对比具有相当的结果和计算资源和记忆要求的减少。

    Large foundation language models have shown their versatility in being able to be adapted to perform a wide variety of downstream tasks, such as text generation, sentiment analysis, semantic search etc. However, training such large foundational models is a non-trivial exercise that requires a significant amount of compute power and expertise from machine learning and systems experts. As models get larger, these demands are only increasing. Sparsity is a promising technique to relieve the compute requirements for training. However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts. Furthermore, sparsity drops the operation intensity and introduces irregular memory access patterns that makes it challenging to efficiently utilize compute resources. This paper demonstrates an end-to-end training flow on a large language model - 13 billion GPT - using sparsity and dataflow. The dataflow execution model and architecture enables effi
    
[^59]: 控制不变集增强强化学习在过程控制方面的应用：提高采样效率并保证稳定性

    Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability. (arXiv:2304.05509v1 [eess.SY])

    [http://arxiv.org/abs/2304.05509](http://arxiv.org/abs/2304.05509)

    本文提出了一种称为控制不变集增强强化学习的方法，通过控制不变集的应用提高稳定性保证和采样效率。

    

    强化学习是一个受到广泛关注的研究领域，尤其是安全强化学习因其处理实际应用中关键的安全性约束的能力而备受关注。本文提出了一种新的强化学习训练方法，称为控制不变集增强强化学习。它利用控制不变集的优点来提高稳定性保证和采样效率。该方法分为离线阶段和在线阶段。离线阶段将控制不变集纳入奖励设计、初始状态采样和状态重置程序中。在在线阶段，当状态在控制不变集之外时，重新训练强化学习以满足稳定性标准。利用控制不变集的显式形式得到了一个备份表来保证在线稳定性。为了评估该方法，将其应用于模拟化学反应器中。结果表明，离线训练中采样效率显著提高。

    Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications of RL algorithms. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve stability guarantees and sampling efficiency. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. In the online stage, RL is retrained whenever the state is outside of CIS, which serves as a stability criterion. A backup table that utilizes the explicit form of CIS is obtained to ensure the online stability. To evaluate the proposed approach, we apply it to a simulated chemical reactor. The results show a significant improvement in sampling efficiency during offline training 
    
[^60]: DistHD: 一种适用于超高维分类的学习感知动态编码方法

    DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification. (arXiv:2304.05503v1 [cs.LG])

    [http://arxiv.org/abs/2304.05503](http://arxiv.org/abs/2304.05503)

    DistHD是一种适用于资源受限设备的学习感知动态编码技术，可以有效地识别和重构影响学习质量的维度，以显著较低的维度提高准确性和加速学习过程。

    

    受启发于人脑的超高维计算技术(HDC)被认为是一种适用于资源受限设备的有前途的学习方法。然而，现有方法使用静态编码器，在学习过程中从不更新。因此，需要非常高的维度才能获得足够的准确性，严重降低编码和训练效率。在本文中，我们提出DistHD，一种新颖的动态编码技术，用于HDC自适应学习，能有效地识别和重构对分类产生误导并影响学习质量的维度。我们提出的算法DistHD成功加速了学习过程，并以显著较低的维度达到了所期望的准确性。

    Brain-inspired hyperdimensional computing (HDC) has been recently considered a promising learning approach for resource-constrained devices. However, existing approaches use static encoders that are never updated during the learning process. Consequently, it requires a very high dimensionality to achieve adequate accuracy, severely lowering the encoding and training efficiency. In this paper, we propose DistHD, a novel dynamic encoding technique for HDC adaptive learning that effectively identifies and regenerates dimensions that mislead the classification and compromise the learning quality. Our proposed algorithm DistHD successfully accelerates the learning process and achieves the desired accuracy with considerably lower dimensionality.
    
[^61]: 用于结构-性质关系的机器学习：可扩展性和限制

    Machine learning for structure-property relationships: Scalability and limitations. (arXiv:2304.05502v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2304.05502](http://arxiv.org/abs/2304.05502)

    该论文提出了一种可扩展的机器学习框架，通过分而治之方法实现线性规模计算，利用局部性假设将系统划分为可以分别解决的子域，从而用于预测多体系统的密集性质和分类相。模型适用性由块大小和在临界点附近训练ML模型的块数决定。

    

    我们提出了一种可扩展的机器学习（ML）框架，用于预测多体系统的密集性质，特别是分类相。可扩展性和可转移性是ML方法无前例的计算效率的核心。一般来说，通过分而治之方法可以实现线性规模的计算，物理性质的局部性是将系统划分为可以分别解决的子域的关键。基于局部性假设，开发了ML模型，用于预测有限尺寸块的密集性质。可以通过对系统随机采样块的ML模型结果进行平均得出大规模系统的预测。我们表明，该方法的适用性取决于ML模型的块大小是否大于系统的特征长度尺度。特别是在跨越临界点的相位识别的情况下，ML预测的精度取决于块大小以及在临界点附近训练ML模型的块数。

    We present a scalable machine learning (ML) framework for predicting intensive properties and particularly classifying phases of many-body systems. Scalability and transferability are central to the unprecedented computational efficiency of ML methods. In general, linear-scaling computation can be achieved through the divide and conquer approach, and the locality of physical properties is key to partitioning the system into sub-domains that can be solved separately. Based on the locality assumption, ML model is developed for the prediction of intensive properties of a finite-size block. Predictions of large-scale systems can then be obtained by averaging results of the ML model from randomly sampled blocks of the system. We show that the applicability of this approach depends on whether the block-size of the ML model is greater than the characteristic length scale of the system. In particular, in the case of phase identification across a critical point, the accuracy of the ML predictio
    
[^62]: GraphGANFed: 针对基于图的分子的联合生成框架，助力高效药物发现

    GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery. (arXiv:2304.05498v1 [cs.LG])

    [http://arxiv.org/abs/2304.05498](http://arxiv.org/abs/2304.05498)

    GraphGANFed是一个整合了GCN、GAN和联邦学习的框架，可在不共享数据的情况下高效训练生成对抗网络，助力基于图的分子的药物发现。

    

    深度学习的最新进展加速了它在各个领域应用，如细胞图像分析和分子发现。在分子发现中，生成对抗网络（GAN）因其能够高效地从大型分子数据集中学习并生成保留相似性质的新分子的能力而成为首选技术。然而，不同的制药公司可能不愿意或无法共享其本地数据集，这是由于分子数据集的地理分布和敏感性质，这使得在集中式方式中训练GAN变得不可能。在本文中，我们提出了一种基于图卷积神经网络（GCN）、GAN和联邦学习（FL）作为整体系统的GraphGANFed框架，用于生成分子。

    Recent advances in deep learning have accelerated its use in various applications, such as cellular image analysis and molecular discovery. In molecular discovery, a generative adversarial network (GAN), which comprises a discriminator to distinguish generated molecules from existing molecules and a generator to generate new molecules, is one of the premier technologies due to its ability to learn from a large molecular data set efficiently and generate novel molecules that preserve similar properties. However, different pharmaceutical companies may be unwilling or unable to share their local data sets due to the geo-distributed and sensitive nature of molecular data sets, making it impossible to train GANs in a centralized manner. In this paper, we propose a Graph convolutional network in Generative Adversarial Networks via Federated learning (GraphGANFed) framework, which integrates graph convolutional neural Network (GCN), GAN, and federated learning (FL) as a whole system to genera
    
[^63]: 重新审视单门限专家混合模型

    Revisiting Single-gated Mixtures of Experts. (arXiv:2304.05497v1 [cs.CV])

    [http://arxiv.org/abs/2304.05497](http://arxiv.org/abs/2304.05497)

    本文重新审视了单门限专家混合模型(MoE)，提出了一种更加实用的训练方式，且实验证明该模型的效率-准确性权衡具备竞争力并优于非混合基线。

    

    专家混合模型（MoE）由于其能够在训练极其大规模模型的同时，允许合理的推理计算成本，因而越来越受欢迎。但是，最近的最先进方法通常假设有很多专家，并要求联合训练所有专家，这经常导致路由器坍塌等训练不稳定问题。相反，在这项工作中，我们建议重新审视简单的单门限MoE，这使得其训练更为实用。我们的工作的主要优势在于：（i）基础模型分为早期退出和集成正则化方案，（ii）一个简单且高效的异步训练管道，无路由器崩溃问题，以及（iii）每个样本基于聚类的初始化。实验证明，所提出的模型获得了与其他更复杂的MoE相当的效率-准确性权衡，并且优于非混合基线。这展示了即使是简单的单门限MoE的优点，并激励进一步的探索。

    Mixture of Experts (MoE) are rising in popularity as a means to train extremely large-scale models, yet allowing for a reasonable computational cost at inference time. Recent state-of-the-art approaches usually assume a large number of experts, and require training all experts jointly, which often lead to training instabilities such as the router collapsing In contrast, in this work, we propose to revisit the simple single-gate MoE, which allows for more practical training. Key to our work are (i) a base model branch acting both as an early-exit and an ensembling regularization scheme, (ii) a simple and efficient asynchronous training pipeline without router collapse issues, and finally (iii) a per-sample clustering-based initialization. We show experimentally that the proposed model obtains efficiency-to-accuracy trade-offs comparable with other more complex MoE, and outperforms non-mixture baselines. This showcases the merits of even a simple single-gate MoE, and motivates further ex
    
[^64]: 改进串联推荐的鲁棒性和准确性: 伴随级联指导的对抗训练方法

    Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v1 [cs.IR])

    [http://arxiv.org/abs/2304.05492](http://arxiv.org/abs/2304.05492)

    本研究利用级联指导下的对抗训练方法，增强了串联推荐模型的鲁棒性和准确性，取得了比已有方法更好的结果。

    

    串联推荐模型是一种通过学习用户与物品间的时间顺序互动来进行推荐的模型，其已经在许多领域中展现出了良好的表现。然而，近期串联推荐模型的鲁棒性备受质疑。这种模型的两个特性使其容易受到攻击 - 在训练中会产生级联效应，在模型过度依赖时间信息的同时会忽略其他特征。为了解决这些问题，本文提出了一种针对串联推荐模型的级联指导下的对抗训练的方法。我们的方法利用串联建模中的内在级联效应，在训练过程中产生战略性的对抗性扰动来影响物品嵌入。在使用不同的公共数据集训练四种最先进的串联模型实验中，我们的训练方法产生了比现有方法更高的模型鲁棒性，并获得了更好的性能。

    Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ran
    
[^65]: 自动导向和质量评估系统用于脐动脉多普勒成像

    An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery. (arXiv:2304.05463v1 [eess.IV])

    [http://arxiv.org/abs/2304.05463](http://arxiv.org/abs/2304.05463)

    提出了一个自动导向和质量评估系统，使用改进的 Faster R-CNN 算法来建议脐动脉多普勒流门的位置，并评估多普勒波形质量，有效地填补了经验不足的超声医生的缺陷。

    

    在胎儿超声筛查中，通过脐带进行监测的脐动脉多普勒图像对于监测胎儿的血液供应非常重要。然而，为了捕捉脐动脉多普勒图像，需要正确地执行多个步骤：在超声图像中放置门，以获取血流波形，并判断多普勒波形质量。这些步骤都依赖于操作者的经验。经验不足的超声医生的短缺因此产生了机器辅助的需求。我们提出了一个自动系统来填补这个缺口。使用改进的 Faster R-CNN 方法，我们得到了一个算法，它建议多普勒流门的位置。我们随后评估多普勒波形质量。我们在国家超声筛查数据库上对所提出的系统进行了验证，涵盖了657个扫描结果。实验结果表明，我们的系统在指导操作者捕捉脐动脉多普勒图像和评估图像质量方面是有用的。

    In fetal ultrasound screening, Doppler images on the umbilical artery (UA) are important for monitoring blood supply through the umbilical cord. However, to capture UA Doppler images, a number of steps need to be done correctly: placing the gate at a proper location in the ultrasound image to obtain blood flow waveforms, and judging the Doppler waveform quality. Both of these rely on the operator's experience. The shortage of experienced sonographers thus creates a demand for machine assistance. We propose an automatic system to fill this gap. Using a modified Faster R-CNN we obtain an algorithm that suggests Doppler flow gate locations. We subsequently assess the Doppler waveform quality. We validate the proposed system on 657 scans from a national ultrasound screening database. The experimental results demonstrate that our system is useful in guiding operators for UA Doppler image capture and quality assessment.
    
[^66]: 动态特征缩放的分段图像方法的实现与学习

    Amortized Learning of Dynamic Feature Scaling for Image Segmentation. (arXiv:2304.05448v1 [cs.CV])

    [http://arxiv.org/abs/2304.05448](http://arxiv.org/abs/2304.05448)

    该研究提出了一种新的超网络策略，可以根据缩放因子快速生成 Pareto 前沿，无需训练多个网络。该方法能够在使用更少的参数和计算的情况下实现最先进的结果。

    

    卷积神经网络已成为图像分割任务中卓越的模型。大多数卷积神经网络分割架构通过固定的因子将空间维度调整为二来聚合空间上下文。为了提高特定应用程序的模型准确性，最近的研究探讨了使用其他调整因子。然而，找到合适的调整因子通常需要为许多不同的因子训练单独的网络，并比较每个模型的性能。这些模型的计算负荷意味着在实践中很少这样做，而且只考虑了几个不同的缩放因子。在这项工作中，我们提出了一种超网络策略，可以用来轻松快速地生成在调整因子变化时，在准确度和效率之间的 Pareto 前沿。我们展示了如何训练一个单独的超网络，该网络生成条件于调整因子的 CNN 参数。这使得用户可以快速选择他们的特定应用程序的缩放因子，而无需训练多个网络。我们的方法能够在比现有方法使用更少的参数和计算的情况下实现最先进的结果。

    Convolutional neural networks (CNN) have become the predominant model for image segmentation tasks. Most CNN segmentation architectures resize spatial dimensions by a fixed factor of two to aggregate spatial context. Recent work has explored using other resizing factors to improve model accuracy for specific applications. However, finding the appropriate rescaling factor most often involves training a separate network for many different factors and comparing the performance of each model. The computational burden of these models means that in practice it is rarely done, and when done only a few different scaling factors are considered.  In this work, we present a hypernetwork strategy that can be used to easily and rapidly generate the Pareto frontier for the trade-off between accuracy and efficiency as the rescaling factor varies. We show how to train a single hypernetwork that generates CNN parameters conditioned on a rescaling factor. This enables a user to quickly choose a rescalin
    
[^67]: 使用Co-ML协同机器学习模型构建家庭的合作模型构建

    Collaborative Machine Learning Model Building with Families Using Co-ML. (arXiv:2304.05444v1 [cs.HC])

    [http://arxiv.org/abs/2304.05444](http://arxiv.org/abs/2304.05444)

    Co-ML是一个基于平板电脑的应用程序，用于协同构建ML图像分类器，可以帮助学习者在合作中发掘新的想法和方法，解决数据表现和多样性等关键问题。

    

    现有的针对新手友好的机器学习（ML）建模工具，侧重于单一用户体验，一个单一用户仅收集自己的数据来构建模型。然而，单独建模经历限制了学习者共同工作时会遇到的交替想法和方法的宝贵机会。因此，当不同的观点体现在群体构建的数据集中时，往往排除了ML围绕数据表现和多样性的关键问题。为解决这个问题，我们创建了Co-ML——一个面向学习者的基于平板电脑的应用程序，通过端对端的迭代模型构建流程，协同构建ML图像分类器。在本文中，我们通过介绍一个家庭（由两个11和14岁的孩子与父母一起工作）在家中使用Co-ML进行引导性介绍ML活动的深入案例研究，展示了协作建模的可行性和潜在丰富性。我们分享了Co-ML系统的d。

    Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML -- a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system d
    
[^68]: 适用于异构特征的迁移学习，实现高效的张量程序生成

    Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v1 [cs.PL])

    [http://arxiv.org/abs/2304.05430](http://arxiv.org/abs/2304.05430)

    本研究提出了适用于异构特征的迁移学习方法，在新的目标硬件上通过学习联合神经网络和硬件特征，解决了张量程序生成的自动调整问题。

    

    改进张量程序生成需要在目标硬件上为给定程序搜索各种可能的程序转换组合，以优化张量程序的执行。由于庞大的搜索空间和指数级别的变换组合，自动调整张量程序的生成变得更加困难，尤其是当需要面对异构的目标时。本研究旨在通过学习联合神经网络和硬件特征，并将它们转移到新的目标硬件上，从而解决这些问题。我们广泛研究现有的最先进数据集TenSet，在测试集分割策略上进行比较分析，并提出优化数据集的方法。我们采用注意力启发式方法，为调整张量程序提供支持，使它们能够融入神经网络和硬件特定特征。我们的方法能够将数据集的基线精简高达45％，而不会影响Pairwise Comparis。

    Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution. It is already a complex process because of the massive search space and exponential combinations of transformations make auto-tuning tensor program generation more challenging, especially when we have a heterogeneous target. In this research, we attempt to address these problems by learning the joint neural network and hardware features and transferring them to the new target hardware. We extensively study the existing state-of-the-art dataset, TenSet, perform comparative analysis on the test split strategies and propose methodologies to prune the dataset. We adopt an attention-inspired approach for tuning the tensor programs enabling them to embed neural network and hardware-specific features. Our approach could prune the dataset up to 45\% of the baseline without compromising the Pairwise Comparis
    
[^69]: 可区分的图结构模型用于晶格材料反设计

    Differentiable graph-structured models for inverse design of lattice materials. (arXiv:2304.05422v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2304.05422](http://arxiv.org/abs/2304.05422)

    本文提出了一种使用图形表示结构和属性的晶格材料的计算方法，使用可微分传递算法计算机械属性以实现反向设计，进而实现了可扩展的、具有前所未有的结构和功能多样性的晶格材料设计。

    

    处于深空恶劣环境中能够根据需要自适应的物理化学性质的材料将在定义未来的空间探索方面变得至关重要。在自然界中，微妙的微观结构和格子几何形状是设计适应于特定环境材料的令人兴奋的灵感来源。然而，由于这种不规则拓扑覆盖的巨大设计空间，在分析上进行探索是具有挑战性的。因此，迄今为止，大多数合成晶格材料都是基于周期性结构。在本文中，我们提出了一种计算方法，使用图形表示对规则和不规则晶格材料进行建模。我们的方法使用可微分传递算法计算力学性质，因此可以使用自动微分来调整单个晶格元素的几何结构和属性，从而设计具有所需属性的材料。引入对晶格结构和材料属性的隐式可学习几何表示，结合反设计框架，实现了可扩展的、具有前所未有的结构和功能多样性的晶格材料设计方法。

    Materials possessing flexible physico-chemical properties that adapt on-demand to the hostile environmental conditions of deep space will become essential in defining the future of space exploration. A promising venue for inspiration towards the design of environment-specific materials is in the intricate micro-architectures and lattice geometry found throughout nature. However, the immense design space covered by such irregular topologies is challenging to probe analytically. For this reason, most synthetic lattice materials have to date been based on periodic architectures instead. Here, we propose a computational approach using a graph representation for both regular and irregular lattice materials. Our method uses differentiable message passing algorithms to calculate mechanical properties, and therefore allows using automatic differentiation to adjust both the geometric structure and attributes of individual lattice elements to design materials with desired properties. The introdu
    
[^70]: 不同iable神经架构搜索中神经网络设计的高效自动化:一项概述研究(arXiv: 2304.05405v1 [cs.LG])

    Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search. (arXiv:2304.05405v1 [cs.LG])

    [http://arxiv.org/abs/2304.05405](http://arxiv.org/abs/2304.05405)

    本文综述了最近在不同iable神经架构搜索中的研究进展，提出了一种新的基于挑战的分类法，对DARTS方法的贡献和影响进行了讨论，并探讨了未来的研究方向。

    

    在过去的几年中，不同iable神经架构搜索（DNAS）迅速成为自动发现深度神经网络结构的流行方法。 这种崛起主要归功于DARTS，这是第一个重要的DNAS方法之一。 与基于强化学习或进化算法的以前的作品相比，DNAS速度快了数个数量级，并且使用的计算资源更少。 在这篇全面的综述中，我们专门关注DNAS并审查了该领域的最新方法。 此外，我们提出了一种基于挑战的分类法来分类DNAS方法。 我们还讨论了过去几年对DNAS带来的贡献以及其对全球NAS领域的影响。 最后，我们通过提供一些未来研究方向的见解来做出结论。

    In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.
    
[^71]: 基于视觉关系的跨任务对抗贴片的转移增强方法

    Boosting Cross-task Transferability of Adversarial Patches with Visual Relations. (arXiv:2304.05402v1 [cs.CV])

    [http://arxiv.org/abs/2304.05402](http://arxiv.org/abs/2304.05402)

    VRAP是一种多任务的对抗贴片生成方法，利用场景图将物体识别为基础的对抗贴片组合成更大更复杂的对抗贴片，从而提高对抗贴片在不同任务之间的转移能力。

    

    对抗性样本的可转移性是评估深度学习系统鲁棒性的关键方面，尤其是在黑盒场景中，目前虽然提出了多种方法来增强跨模型的可转移性，但很少关注对抗样本在不同任务之间的可转移性。为了解决这一问题，我们提出了一种基于视觉关系的跨任务对抗贴片生成方法VRAP，旨在评估各种视觉任务的鲁棒性，特别是那些涉及视觉推理的任务，例如视觉问答和图像字幕生成。VRAP利用场景图将物体识别为基础的对抗贴片组合成更大更复杂的对抗贴片，从而扰乱目标模型的推理过程。实验结果表明，与单一任务贴片相比，VRAP生成的对抗贴片在任务间具有更高的可转移性，即使这些任务涉及不同的推理函数或输入格式。

    The transferability of adversarial examples is a crucial aspect of evaluating the robustness of deep learning systems, particularly in black-box scenarios. Although several methods have been proposed to enhance cross-model transferability, little attention has been paid to the transferability of adversarial examples across different tasks. This issue has become increasingly relevant with the emergence of foundational multi-task AI systems such as Visual ChatGPT, rendering the utility of adversarial samples generated by a single task relatively limited. Furthermore, these systems often entail inferential functions beyond mere recognition-like tasks. To address this gap, we propose a novel Visual Relation-based cross-task Adversarial Patch generation method called VRAP, which aims to evaluate the robustness of various visual tasks, especially those involving visual reasoning, such as Visual Question Answering and Image Captioning. VRAP employs scene graphs to combine object recognition-b
    
[^72]: 通过在Bures-Wasserstein空间中使用JKO进行正反高斯变分推断

    Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space. (arXiv:2304.05398v1 [math.ST])

    [http://arxiv.org/abs/2304.05398](http://arxiv.org/abs/2304.05398)

    本研究基于Bures-Wasserstein空间，开发了(随机)正反高斯变分推断算法，同时分析了算法在不同分布下的收敛保证。

    

    变分推断旨在通过可追溯的分布族中的元素逼近目标分布$\pi$。统计学和机器学习中的主要关注点是高斯变分推断，它通过在高斯空间上最小化Kullback-Leibler散度到$\pi$来逼近$\pi$。本文开发了(随机)正反高斯变分推断(FB-GVI)算法来解决高斯变分推断问题，我们的方法利用了KL散度的复合结构，KL散度可被写作高斯在Bures-Wasserstein(BW)空间上的势函数与熵函数的和。对于我们提出的算法，当$\pi$是log-smooth和log-concave时，我们获得了最先进的收敛性保证，同时在$\pi$只是log-smooth时，我们首次获得了到一阶稳定解的收敛保证。

    Variational inference (VI) seeks to approximate a target distribution $\pi$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates $\pi$ by minimizing the Kullback-Leibler (KL) divergence to $\pi$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when $\pi$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when $\pi$ is only log-smooth.
    
[^73]: 部分参与下加速混合联邦学习的收敛

    Accelerating Hybrid Federated Learning Convergence under Partial Participation. (arXiv:2304.05397v1 [cs.DC])

    [http://arxiv.org/abs/2304.05397](http://arxiv.org/abs/2304.05397)

    本文提出了一种适用于部分参与环境下的混合联邦学习框架，其包括自适应采样方法和部分聚合方法，有助于加快模型的收敛速度，从而减少通信成本且不影响模型精度。

    

    近年来，联邦学习(Federated Learning，FL)已成为一种常见的分布式机器学习范式。FL涉及一组有着分散数据的客户端，通过集中服务器的协调合作学习一个公共模型，其目的是通过确保本地数据集永远不会离开客户端，只有服务器进行模型聚合来保护客户端的隐私。然而，在现实场景中，服务器可能能够收集少量数据以近似总体分布，并具有更强的计算能力来执行学习过程。为了解决这个问题，本文聚焦于混合FL框架。在先前混合FL工作中，已经证明了客户端和服务器的交替训练可以增加收敛速度，但是它仅关注客户端完全参与的情况，而忽略了部分参与的负面影响。本文提出了一种自适应采样方法和部分聚合方法来改进混合FL的收敛速度，并对其进行了理论分析。我们提出的方法在减少通信成本的同时不会造成太大精度损失，与传统FL方法相比，我们方法的有效性在实际数据集上得到了证明。

    Over the past few years, Federated Learning (FL) has become a popular distributed machine learning paradigm. FL involves a group of clients with decentralized data who collaborate to learn a common model under the coordination of a centralized server, with the goal of protecting clients' privacy by ensuring that local datasets never leave the clients and that the server only performs model aggregation. However, in realistic scenarios, the server may be able to collect a small amount of data that approximately mimics the population distribution and has stronger computational ability to perform the learning process. To address this, we focus on the hybrid FL framework in this paper. While previous hybrid FL work has shown that the alternative training of clients and server can increase convergence speed, it has focused on the scenario where clients fully participate and ignores the negative effect of partial participation. In this paper, we provide theoretical analysis of hybrid FL under
    
[^74]: SAM.MD：Segment Anything Model的零样本医学图像分割能力

    SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model. (arXiv:2304.05396v1 [eess.IV])

    [http://arxiv.org/abs/2304.05396](http://arxiv.org/abs/2304.05396)

    SAM.MD是Segment Anything Model的医学图像分割版本，可以进行零样本分割，虽然性能不是最优，但是可以为医学领域的半自动分割工具提供潜在催化剂。

    

    基础模型由于提示的灵活性已经占据了自然语言处理和图像生成领域。随着Segment Anything Model（SAM）的最近推出，这种提示驱动的范例已经进入了具有大量未开发能力的图像分割领域。本文的目的是通过评估SAM在腹部CT器官分割任务中的表现，通过点或边界框提示，对SAM的开箱即用的零样本能力进行初步评估，展示SAM在CT数据上的泛化能力良好，成为促进临床医生半自动分割工具进步的潜在催化剂。虽然SAM在我们的研究中未达到最先进的分割性能，但我们相信这种基础模型可以作为医学领域此类模型进一步适应复杂性的强大起点。

    Foundation models have taken over natural language processing and image generation domains due to the flexibility of prompting. With the recent introduction of the Segment Anything Model (SAM), this prompt-driven paradigm has entered image segmentation with a hitherto unexplored abundance of capabilities. The purpose of this paper is to conduct an initial evaluation of the out-of-the-box zero-shot capabilities of SAM for medical image segmentation, by evaluating its performance on an abdominal CT organ segmentation task, via point or bounding box based prompting. We show that SAM generalizes well to CT data, making it a potential catalyst for the advancement of semi-automatic segmentation tools for clinicians. We believe that this foundation model, while not reaching state-of-the-art segmentation performance in our investigations, can serve as a highly potent starting point for further adaptations of such models to the intricacies of the medical domain. Keywords: medical image segmenta
    
[^75]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^76]: 个性化文本到图像生成的可控文本反转

    Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])

    [http://arxiv.org/abs/2304.05265](http://arxiv.org/abs/2304.05265)

    本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。

    

    最近，大规模生成模型在以文本为引导的高保真图像的生成方面取得了前所未有的性能。当引导信息包含用户定义的、未见过的或长尾概念标记时，文本反转成为一种有效的个性化生成技术。尽管如此，我们发现并展示了文本反转的部署仍充满了“黑魔法”，例如额外数据集的严苛要求，在循环中需要艰苦的人力成本和缺乏鲁棒性等。在这项工作中，我们提出了一种名为可控文本反转的大大增强版反转，解决了所有上述问题，并反过来提供了一个强大，数据效率高，易于使用的框架。COTI的核心是基于理论的损失目标，具有全面和新颖的加权评分机制，并由主动学习范式所提取。广泛的结果表明，COTI的性能比之前技术有了显著的提升，尤其是在数据少的情况下。

    The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
    
[^77]: r-softmax: 具有可控稀疏率的广义Softmax

    r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])

    [http://arxiv.org/abs/2304.05243](http://arxiv.org/abs/2304.05243)

    本文提出了一种新的广义Softmax函数r-softmax，可以输出具有可控稀疏度的概率分布，相较于现有的替代方案效果更好，在多标签数据集上表现突出，在预训练转换语言模型的自我注意模块中具有重要应用。

    

    如今，人工神经网络模型在许多领域取得了显著的成果。将模型提供的表示映射到概率分布的函数是深度学习解决方案的不可分割的方面。虽然softmax是机器学习社区中通常接受的概率映射函数，但它不能返回稀疏的输出，并且总是将正概率分散到所有位置。在本文中，我们提出了r-softmax，这是softmax的一种修改，它输出具有可控稀疏度的稀疏概率分布。与现有的稀疏概率映射函数相比，我们提供了一种直观的机制来控制输出稀疏度。我们在几个多标签数据集上展示了r-softmax优于其他稀疏的softmax替代方案，并且与原始的softmax相比具有高竞争力。我们还将r-softmax应用于预训练转换语言模型的自我注意模块中，并展示了它在自然语言处理方面的应用。

    Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l
    
[^78]: 卷积神经网络在波形模拟器中的先验压缩

    A priori compression of convolutional neural networks for wave simulators. (arXiv:2304.04964v1 [cs.LG])

    [http://arxiv.org/abs/2304.04964](http://arxiv.org/abs/2304.04964)

    该论文提出了一种在训练神经网络之前压缩卷积层的张量格式方法，通过替换多维核为一维滤波器来减少CNN模型的大小和减少过度拟合，从而提高实时准确预测的速度。

    

    卷积神经网络现在被广泛应用于各个领域，包括图像分类、面部和物体识别、医学成像分析等。此外，像物理信息模拟器这样的应用需要实时准确预测，但现有的神经网络模型包含数以百万计的参数，这使得在具有有限内存的设备上安装此类模型变得困难。压缩技术可能通过减少贡献到模型复杂性的参数数量来减小CNN模型的大小来解决这些问题。我们提出了一种压缩的卷积层张量格式，先于神经网络的训练进行。卷积层中的三维或二维核被一维过滤器替换。过度拟合现象也将减少。预测所需的时间或计算机仿真和预测所需的时间也将减少。

    Convolutional neural networks are now seeing widespread use in a variety of fields, including image classification, facial and object recognition, medical imaging analysis, and many more. In addition, there are applications such as physics-informed simulators in which accurate forecasts in real time with a minimal lag are required. The present neural network designs include millions of parameters, which makes it difficult to install such complex models on devices that have limited memory. Compression techniques might be able to resolve these issues by decreasing the size of CNN models that are created by reducing the number of parameters that contribute to the complexity of the models. We propose a compressed tensor format of convolutional layer, a priori, before the training of the neural network. 3-way kernels or 2-way kernels in convolutional layers are replaced by one-way fiters. The overfitting phenomena will be reduced also. The time needed to make predictions or time required fo
    
[^79]: 实现队列智能化：一种针对电子病历分析的通用群体表示学习框架

    Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04468](http://arxiv.org/abs/2304.04468)

    提出了一种通用的COhort Representation lEarning（CORE）框架，用于增强EHR表示学习，支持针对不同队列的特征进行可解释性分析。

    

    电子病历（EHR）是从临床常规护理中生成的，记录了广泛的病人人群有价值的信息，为改善临床实践中的病人管理和干预策略提供了丰富的机会。为了利用EHR数据的巨大潜力，机器学习中流行的EHR数据分析范式是EHR表示学习，它首先利用单个病人的EHR数据通过一个主干学习信息丰富的表示，并支持建立在这些表示的多样化的医疗下游任务。然而，这种范式无法深入分析病人的相关性，通常在临床实践中被称为队列研究。具体来说，同一队列中的病人倾向于具有相似的特征，表明他们在医疗条件（如症状或疾病）方面具有相似之处。在本文中，我们提出了一种通用COhort Representation lEarning (CORE)框架来增强EHR表示学习，通过使用队列表示学习算法，对群体信息进行建模并支持针对不同队列的特征进行可解释性分析。

    Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
    
[^80]: 基于NeRF技术的卫星图像表面重建

    NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])

    [http://arxiv.org/abs/2304.04133](http://arxiv.org/abs/2304.04133)

    本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。

    

    本文提出了Sat-NeRF模型，是对最近引入的S-NeRF模型的修改实现。该模型能够从稀疏的卫星图像集合中合成新的视角，同时考虑到图片中的光照变化。训练好的模型还能够精确地估计场景表面的高程，这对卫星观测应用非常有帮助。S-NeRF方法改进了标准的NeRF方法，将辐射强度考虑为高反射率和入射辐照度的函数。这两个量都是模型的全连接神经网络枝条的输出，而后者则被视为来自太阳的直接光线和来自天空的漫反射颜色函数。该实现基于用缩放-裁剪技术增强的卫星图像数据集。对NeRF进行了超参数研究，得出了一些有趣的观察结果。

    We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
    
[^81]: 学习中的可复制性和稳定性

    Replicability and stability in learning. (arXiv:2304.03757v1 [cs.LG])

    [http://arxiv.org/abs/2304.03757](http://arxiv.org/abs/2304.03757)

    该论文研究了机器学习中的可复制性和全局稳定性，并证明许多学习任务只能弱化地实现全局稳定性。

    

    可复制性是科学中的关键，因为它使我们能够验证和验证研究结果。Impagliazzo、Lei、Pitassi和Sorrell（'22）最近开始研究机器学习中的可复制性。如果同一算法在两个独立同分布输入上使用相同的内部随机性时通常产生相同的输出，则学习算法是可复制的。我们研究了一种不涉及固定随机性的可复制性变体。如果一个算法在两个独立同分布的输入上（不固定内部随机性）应用时通常产生相同的输出，则算法满足这种形式的可复制性。这个变种被称为全局稳定性，并在差分隐私的上下文中由Bun、Livni和Moran（'20）介绍。 Impagliazzo等人展示了如何提高任何可复制算法的效果，以使其产生的输出概率无限接近于1。相反，我们证明了对于许多学习任务，只能弱化地实现全局稳定性，这里输出只有相同的部分。

    Replicability is essential in science as it allows us to validate and verify research findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recently initiated the study of replicability in machine learning. A learning algorithm is replicable if it typically produces the same output when applied on two i.i.d. inputs using the same internal randomness. We study a variant of replicability that does not involve fixing the randomness. An algorithm satisfies this form of replicability if it typically produces the same output when applied on two i.i.d. inputs (without fixing the internal randomness). This variant is called global stability and was introduced by Bun, Livni and Moran (`20) in the context of differential privacy.  Impagliazzo et al. showed how to boost any replicable algorithm so that it produces the same output with probability arbitrarily close to 1. In contrast, we demonstrate that for numerous learning tasks, global stability can only be accomplished weakly, where the same o
    
[^82]: 自适应学生t分布与方法矩移动估计器用于非平稳时间序列

    Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series. (arXiv:2304.03069v1 [stat.ME])

    [http://arxiv.org/abs/2304.03069](http://arxiv.org/abs/2304.03069)

    本文提出了一种适用于非平稳时间序列的自适应学生t分布方法，基于方法的一般自适应矩可以使用廉价的指数移动平均值（EMA）来估计参数。

    

    真实的时间序列通常是非平稳的，这带来了模型适应的难题。传统方法如GARCH假定任意类型的依赖性。为了避免这种偏差，我们将着眼于最近提出的不可知的移动估计器哲学：在时间$t$找到优化$F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$移动对数似然的参数，随时间演化。例如，它允许使用廉价的指数移动平均值（EMA）来估计参数，例如绝对中心矩$E[|x-\mu|^p]$随$p\in\mathbb{R}^+$的变化而演化$m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$。这种基于方法的一般自适应矩的应用将呈现在学生t分布上，尤其是在经济应用中流行，这里应用于DJIA公司的对数收益率。

    The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like GARCH assume arbitrary type of dependence. To prevent such bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $E[|x-\mu|^p]$ evolving with $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$ for one or multiple powers $p\in\mathbb{R}^+$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies.
    
[^83]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^84]: FedIN：用于模型异构的联邦中间层学习

    FedIN: Federated Intermediate Layers Learning for Model Heterogeneity. (arXiv:2304.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.00759](http://arxiv.org/abs/2304.00759)

    FedIN是一种新型的联邦学习方法，支持异构模型，无需公共数据集。在FedIN中，提取器和分类器的模型结构在所有设备中都相同，而中间层的架构可以根据异构设备的资源容量而变化。IN训练可用于利用特征知识，实验结果表明了该方法在图像分类任务上的有效性。

    

    联邦学习（FL）使得边缘设备能够合作训练全局共享模型，同时在本地和私密地保留训练数据。然而，在FL中一个普遍但不切实际的假设是参与边缘设备拥有相同的必需资源并共享相同的全局模型架构。本研究提出了一种名为Federated Intermediate Layers Learning（FedIN）的新型FL方法，支持异构模型而不使用任何公共数据集。FedIN中的训练模型分为三部分，包括提取器、中间层和分类器。提取器和分类器的模型结构在所有设备中都相同，以保持中间层特征的一致性，而中间层的架构可以根据异构设备的资源容量而变化。为了利用特征知识，我们提出了IN训练，以IN标准化为基础训练中间层。在图像分类任务上的实验结果表明了我们方法在精度和效率方面的有效性。

    Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line 
    
[^85]: 在SAR ATR中发现和解释深度学习的非因果性

    Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR. (arXiv:2304.00668v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00668](http://arxiv.org/abs/2304.00668)

    本文通过量化不同区域对目标识别的贡献和解释数据偏差和模型偏差对非因果性的影响，提供了改善深度学习在SAR ATR中鲁棒性和泛化能力的重要见解。

    

    近年来，深度学习在SAR ATR中被广泛使用，并在MSTAR数据集上取得了优异的性能。然而，由于受限的成像条件，MSTAR存在背景相关等数据偏见，即背景杂波特性与目标类别存在虚假相关性。深度学习可以过度拟合杂波以减少训练误差。因此，杂波的过度拟合程度反映了SAR ATR中深度学习的非因果性。现有方法仅定性分析此现象。本文基于Shapley值量化不同区域对目标识别的贡献。杂波的Shapley值可以衡量其过度拟合程度。此外，我们解释了数据偏差和模型偏差对非因果性的影响。简言之，数据偏差导致训练集和测试集中的信杂比和杂波纹理可比，而各种模型结构对这些偏差的过拟合程度不同。非因果性的解释为提高深度学习在SAR ATR中的鲁棒性和泛化能力提供了重要的见解。

    In recent years, deep learning has been widely used in SAR ATR and achieved excellent performance on the MSTAR dataset. However, due to constrained imaging conditions, MSTAR has data biases such as background correlation, i.e., background clutter properties have a spurious correlation with target classes. Deep learning can overfit clutter to reduce training errors. Therefore, the degree of overfitting for clutter reflects the non-causality of deep learning in SAR ATR. Existing methods only qualitatively analyze this phenomenon. In this paper, we quantify the contributions of different regions to target recognition based on the Shapley value. The Shapley value of clutter measures the degree of overfitting. Moreover, we explain how data bias and model bias contribute to non-causality. Concisely, data bias leads to comparable signal-to-clutter ratios and clutter textures in training and test sets. And various model structures have different degrees of overfitting for these biases. The exp
    
[^86]: 关于扩散模型参数效率微调的进一步探究

    A Closer Look at Parameter-Efficient Tuning in Diffusion Models. (arXiv:2303.18181v1 [cs.CV])

    [http://arxiv.org/abs/2303.18181](http://arxiv.org/abs/2303.18181)

    本文研究了大规模扩散模型的参数效率微调，通过插入小型可学习模块来实现。研究表明，适配器的输入位置是影响下游任务性能的关键因素，而将输入位置放在交叉注意力块之后可获得最佳性能。

    

    大规模扩散模型如Stable Diffusion在许多实际应用中都表现出足够的强大，然而在对这些模型进行个性化微调时却需要耗费大量内存和时间。受自然语言处理领域近期的进展启发，我们通过插入小型可学习模块(称作适配器)来研究大规模扩散模型的参数效率微调。具体来说，我们将适配器的设计空间分解为正交因子——输入位置、输出位置以及函数形式，并进行ANOVA分析，一种分析离散(设计选项)与连续变量(评估指标)之间相关性的经典统计方法。我们的分析表明，适配器的输入位置是影响下游任务性能的关键因素。接着，我们仔细研究了输入位置的选择，发现将输入位置放在交叉注意力块后可以使性能达到最佳。

    Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the bes
    
[^87]: 物理驱动的扩散模型用于从视频中合成冲击声

    Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])

    [http://arxiv.org/abs/2303.16897](http://arxiv.org/abs/2303.16897)

    该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。

    

    对物体相互作用发出的声音进行建模对于实际世界和虚拟世界中的沉浸式感官体验至关重要。传统的冲击声合成方法使用物理模拟来获得一组能够表示和合成声音的物理参数。然而，它们需要物体的细节和冲击位置，这在真实世界中很少可用，并且无法应用于从普通视频中合成冲击声。另一方面，现有的视频驱动深度学习方法只能捕捉到视觉内容和冲击声之间的弱对应关系，因为它们缺乏物理知识。在这项工作中，我们提出了一种物理驱动的扩散模型，可以为静态视频剪辑合成高保真的冲击声。除了视频内容外，我们还提出使用额外的物理先验知识来指导冲击声合成过程，这些先验包括既可控制物理参数，同时也能保证音效质量的噪声扰动。

    Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are
    
[^88]: 概率建模与自动化机器学习框架在高维应力场中的应用

    Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field. (arXiv:2303.16869v1 [cs.CE])

    [http://arxiv.org/abs/2303.16869](http://arxiv.org/abs/2303.16869)

    本文探讨了数据驱动的代理建模在高维物理问题中的应用，提出了一种基于自动化机器学习框架的概率建模方法，以降低计算成本和提高预测准确性和精度。

    

    现代计算方法采用高度复杂的数学公式，使得建模复杂的物理现象、预测关键性能和设计优化成为可能。然而，这些计算模型的高保真度使得查询成本极高，通常会采用简化模型以换取预测精度和精确度的代价。为解决这一问题，数据驱动的代理建模方法在仿真昂贵的计算模型行为方面表现出了很大的优势。然而，这种方法的一个主要瓶颈是无法处理高维度的输入和输出量，需要相对较大的数据集。对于此类问题，常用的代理建模方法会要求大量的计算评估，使得其他数值处理变得困难。

    Modern computational methods, involving highly sophisticated mathematical formulations, enable several tasks like modeling complex physical phenomenon, predicting key properties and design optimization. The higher fidelity in these computer models makes it computationally intensive to query them hundreds of times for optimization and one usually relies on a simplified model albeit at the cost of losing predictive accuracy and precision. Towards this, data-driven surrogate modeling methods have shown a lot of promise in emulating the behavior of the expensive computer models. However, a major bottleneck in such methods is the inability to deal with high input dimensionality and the need for relatively large datasets. With such problems, the input and output quantity of interest are tensors of high dimensionality. Commonly used surrogate modeling methods for such problems, suffer from requirements like high number of computational evaluations that precludes one from performing other nume
    
[^89]: 潜在图推断中的模型空间投影。

    Projections of Model Spaces for Latent Graph Inference. (arXiv:2303.11754v1 [cs.LG])

    [http://arxiv.org/abs/2303.11754](http://arxiv.org/abs/2303.11754)

    本文将双曲和球形模型空间的立体投影以及Riemannian隐空间的乘积应用于潜在图推断，实现了与非投影空间相当的性能并提供理论保证。

    

    图神经网络利用图的连接结构作为归纳偏差。潜在图推断关注于学习一个合适的图结构来扩散信息并提高模型的下游性能。本文利用双曲和球形模型空间的立体投影，以及Riemannian隐空间的乘积，用于潜在图推断。在避免曲率趋于零时空间发散的理论保证下，立体投影模型空间能实现与非投影对应模型空间相当的性能。我们在同构和异构图上进行实验。

    Graph Neural Networks leverage the connectivity structure of graphs as an inductive bias. Latent graph inference focuses on learning an adequate graph structure to diffuse information on and improve the downstream performance of the model. In this work we employ stereographic projections of the hyperbolic and spherical model spaces, as well as products of Riemannian manifolds, for the purpose of latent graph inference. Stereographically projected model spaces achieve comparable performance to their non-projected counterparts, while providing theoretical guarantees that avoid divergence of the spaces when the curvature tends to zero. We perform experiments on both homophilic and heterophilic graphs.
    
[^90]: 零样本对比损失用于文本引导扩散图像风格迁移

    Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])

    [http://arxiv.org/abs/2303.08622](http://arxiv.org/abs/2303.08622)

    本文提出了一种适用于文本引导图像风格迁移中的零样本对比损失方法，可以在不需要额外训练的情况下生成具有相同语义内容的图像。

    

    扩散模型在文本引导图像风格迁移中表现出极大的潜力，但由于其随机性而存在风格转换和内容保护之间的权衡。现有方法需要计算密集的扩散模型微调或附加神经网络。为了解决这个问题，我们在扩散模型中提出了一种零样本对比损失，它不需要额外的微调或辅助网络。通过利用预训练的扩散模型中生成样本和原始图像嵌入之间的图块对比损失，我们的方法可以以零样本的方式生成具有与源图像相同语义内容的图像。我们的方法在保留内容且不需要额外训练的同时，在图像风格迁移、图像到图像的转换和操作中均优于现有方法。我们的实验结果证实了我们提出的方法的有效性。

    Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
    
[^91]: 《具有最优n的n步时序差分学习》

    n-Step Temporal Difference Learning with Optimal n. (arXiv:2303.07068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07068](http://arxiv.org/abs/2303.07068)

    本文提出了使用SPSA算法求解n步时序差分学习中的最优n值的算法SDPSA，并证明了其收敛性和有效性。

    

    本文考虑了在n步时序差分算法中找到最优n值的问题。我们采用了模型自由优化技术，即同时扰动随机逼近（SPSA）方法来寻找最优n。我们采用了一个模拟SPSA程序，将其原始连续优化框架引入离散优化框架，但并结合了循环扰动序列。我们证明了我们提出的算法SDPSA的收敛性，并表明它可以在任意初始值的情况下找到n步TD中的最优n值。通过实验，我们展示了SDPSA能够实现最优n值的求解。

    We consider the problem of finding the optimal value of n in the n-step temporal difference (TD) algorithm. We find the optimal n by resorting to the model-free optimization technique of simultaneous perturbation stochastic approximation (SPSA). We adopt a one-simulation SPSA procedure that is originally for continuous optimization to the discrete optimization framework but incorporates a cyclic perturbation sequence. We prove the convergence of our proposed algorithm, SDPSA, and show that it finds the optimal value of n in n-step TD. Through experiments, we show that the optimal value of n is achieved with SDPSA for any arbitrary initial value of the same.
    
[^92]: 发现图像分类器在罕见子群体上的系统性错误

    Identification of Systematic Errors of Image Classifiers on Rare Subgroups. (arXiv:2303.05072v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05072](http://arxiv.org/abs/2303.05072)

    该论文提出了一种名为PromptAttack的组合测试方法，通过在被低频率覆盖的子群体中搜索找到目标模型在这些条件下表现不佳的子群体，以此识别出分类器可能存在的系统误差问题。

    

    尽管许多图像分类器具有出色的平均性能，但它们在训练数据中未被充分覆盖的语义连贯的子群体上的性能可能会显著降低。这些系统误差可能会影响民族少数群体的公平性，以及在领域转移时的鲁棒性和安全性。我们利用文本到图像模型，搜索数据中被低频率覆盖的子群体（“提示”），找到目标模型在这些条件下表现不佳的子群体，以此识别出分类器可能存在的系统误差问题。我们采用组合测试解决了子群体数量呈指数级增长的挑战，并将这个过程称为PromptAttack。我们在受控的实验环境下研究了PromptAttack的子群体涵盖率和可识别性。

    Despite excellent average-case performance of many image classifiers, their performance can substantially deteriorate on semantically coherent subgroups of the data that were under-represented in the training data. These systematic errors can impact both fairness for demographic minority groups as well as robustness and safety under domain shift. A major challenge is to identify such subgroups with subpar performance when the subgroups are not annotated and their occurrence is very rare. We leverage recent advances in text-to-image models and search in the space of textual descriptions of subgroups ("prompts") for subgroups where the target model has low performance on the prompt-conditioned synthesized data. To tackle the exponentially growing number of subgroups, we employ combinatorial testing. We denote this procedure as PromptAttack as it can be interpreted as an adversarial attack in a prompt space. We study subgroup coverage and identifiability with PromptAttack in a controlled 
    
[^93]: 进化强化学习综述

    Evolutionary Reinforcement Learning: A Survey. (arXiv:2303.04150v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.04150](http://arxiv.org/abs/2303.04150)

    这篇文章系统地总结了最新的进化计算方法在解决强化学习中的关键挑战方面所取得的良好性能。

    

    强化学习是一种通过与环境交互训练智能体最大化累积奖励的机器学习方法。最近将强化学习与深度学习相结合，在棋盘游戏、街机游戏和机器人控制等各种挑战性任务中取得了令人瞩目的成就。尽管取得了成功，但仍存在一些关键挑战，包括由敏感超参数导致的脆弱收敛特性，长时间跨度和稀疏奖励的时间分配困难，特别是在连续搜索空间场景中的多样性探索不足，多智能体强化学习中的信用分配困难以及奖励冲突目标。进化计算维护着一群学习智能体，已展现出解决这些限制的良好性能。本文介绍了集成进化计算的最新方法的全面综述。

    Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC
    
[^94]: 一招鲜：神经潜力场用于实体导航

    One-4-All: Neural Potential Fields for Embodied Navigation. (arXiv:2303.04011v2 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2303.04011](http://arxiv.org/abs/2303.04011)

    本文提出了一种新方法 One-4-All (O4A)，利用自我监督和流形学习，实现了一种无图形、端到端的导航程序。其通过贪婪最小化潜在空间内的潜力函数来进行导航，在真实世界的导航中具有重要应用价值。

    

    机器人的基本任务之一是在两个位置之间导航。特别地，真实世界的导航可能需要使用高维RGB图像进行长期规划，这对于端到端的学习方法构成了重大挑战。目前的半参数方法通过将学习模块与环境的拓扑记忆相结合来实现长期规划，通常使用以先前收集的图像为基础的图形表示。然而，实际上使用这些图表通常需要调整大量的修剪启发式方法，以避免虚假的边缘，限制运行时内存使用并允许相当快速的图形查询。在这项工作中，我们提出了One-4-All（O4A），一种利用自我监督和流形学习来获得无图形、端到端导航管道的方法，其中目标被指定为一幅图像。导航通过贪婪地最小化连续定义在O4A潜在空间上的潜力函数来实现。我们的系统是离线训练的。

    A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice typically involves tuning a number of pruning heuristics to avoid spurious edges, limit runtime memory usage and allow reasonably fast graph queries. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over the O4A latent space. Our system is trained offline on 
    
[^95]: 基于置信度子图匹配的异质结构邻居发现算法用于半监督节点分类

    Finding Heterophilic Neighbors via Confidence-based Subgraph Matching for Semi-supervised Node Classification. (arXiv:2302.09755v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09755](http://arxiv.org/abs/2302.09755)

    本论文提出了一种基于置信度子图匹配的双阶段算法，通过确定边权系数和改善标签传播机制来解决了异质结构邻居识别的问题。

    

    图神经网络在许多基于图的应用中表现出了强大的能力。然而，在异质结构设置下（即邻居节点具有不同的标签），它们的泛化能力不佳。为了解决这一问题，本文提出了一种双阶段算法。首先，我们使用一个补充模块通过子图匹配来确定边缘系数。然后，我们采用具有修改标签传播机制的GNNs来有效利用边缘系数。具体来说，我们的补充模块根据给定的置信度比例确定某些任务不相关的边缘。使用剩余的边缘，我们采用广泛使用的最优运输来计算两个节点及其子图之间的相似度。最后，利用系数作为GNNs的补充信息，我们改善了标签传播机制，可以防止两个节点。

    Graph Neural Networks (GNNs) have proven to be powerful in many graph-based applications. However, they fail to generalize well under heterophilic setups, where neighbor nodes have different labels. To address this challenge, we employ a confidence ratio as a hyper-parameter, assuming that some of the edges are disassortative (heterophilic). Here, we propose a two-phased algorithm. Firstly, we determine edge coefficients through subgraph matching using a supplementary module. Then, we apply GNNs with a modified label propagation mechanism to utilize the edge coefficients effectively. Specifically, our supplementary module identifies a certain proportion of task-irrelevant edges based on a given confidence ratio. Using the remaining edges, we employ the widely used optimal transport to measure the similarity between two nodes with their subgraphs. Finally, using the coefficients as supplementary information on GNNs, we improve the label propagation mechanism which can prevent two nodes 
    
[^96]: AI对抗AI：在社交媒体上打击机器生成的虚假餐厅评论

    Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07731](http://arxiv.org/abs/2302.07731)

    本文针对机器生成的虚假评论提出了一种用高质量餐厅评论生成虚假评论并微调GPT输出检测器的方法，该方法预测虚假评论的性能优于现有解决方案。同时，我们还探索了预测非精英评论的模型，并在几个维度上对这些评论进行分析，此类机器生成的虚假评论是社交媒体平台面临的持续挑战。

    

    最近生成模型（如GPT）的发展使得以更低的成本制造出难以区分的虚假顾客评论，从而对社交媒体平台检测这些机器生成的虚假评论造成挑战。本文提出利用Yelp验证的高质量的精英餐厅评论来生成OpenAI GPT评论生成器的虚假评论，并最终微调GPT输出检测器来预测明显优于现有解决方案的虚假评论。我们进一步将模型应用于预测非精英评论，并在几个维度（如评论、用户和餐厅特征以及写作风格）上识别模式。我们展示了社交媒体平台正在不断面临机器生成的虚假评论的挑战，尽管他们可能实施检测系统以过滤出可疑的评论。

    Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
    
[^97]: 语言模型的持续预训练

    Continual Pre-training of Language Models. (arXiv:2302.03241v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03241](http://arxiv.org/abs/2302.03241)

    本文研究了语言模型的持续预训练，提出了一种新颖的持续领域自适应预训练方法，使用一系列未标记的领域语料库来逐步适应领域以提高LM在领域内的终端任务表现。该方法通过一个软掩蔽机制来直接控制LM的更新，并使用一种新颖的代理来保留LM中的整体知识，同时对比已学习领域知识和当前全网络的知识来实现知识整合。

    

    语言模型（LMs）是自然语言处理快速发展的关键。本文研究LMs的持续预训练，特别是持续领域自适应预训练（或持续DAP训练）。现有研究表明，使用领域语料库进一步预训练LMs以使其适应于领域，可以提高领域内的最终任务性能。本文提出了一种新颖的方法，使用一系列未标记的领域语料库来持续DAP训练LMs，以使其适应于这些领域，从而提高它们的终端任务性能。我们方法的关键创新在于一种软掩蔽机制，可直接控制LMs的更新。还提出了一种新颖的代理来保留原始LMs中的普通知识。此外，它对先前学习的领域知识（包括预先训练的LMs中的普通知识）和来自当前全网络的知识进行对比，以实现知识整合。

    Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The m
    
[^98]: 时间序列分类方法在分辨扩散过程中的最优性基准测试

    Benchmarking optimality of time series classification methods in distinguishing diffusions. (arXiv:2301.13112v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13112](http://arxiv.org/abs/2301.13112)

    本研究提出使用似然比检验为基准测试TSC算法在区分扩散过程中的最优性。随机森林、ResNet和ROCKET算法在单变量时间序列和多元高斯过程中可以实现LRT最优性，但在分类高维非线性多元时间序列时是次优的。

    

    统计最优性基准测试对于分析和设计时间序列分类（TSC）算法至关重要。本研究提出使用似然比检验（LRT）基准测试TSC算法在区分扩散过程中的最优性。LRT是根据Neyman-Pearson引理得出的最优分类器。LRT基准测试计算效率高，因为LRT不需要训练，并且扩散过程可以进行高效模拟，可以灵活反映出真实世界应用的特定特征。我们使用三种常用的TSC算法进行基准测试：随机森林、ResNet和ROCKET。这些算法可以实现单变量时间序列和多元高斯过程的LRT最优性。但是，这些模型无关的算法在分类高维非线性多元时间序列时是次优的。此外，LRT基准测试提供了工具来分析分类准确性与时间依赖性的关系。

    Statistical optimality benchmarking is crucial for analyzing and designing time series classification (TSC) algorithms. This study proposes to benchmark the optimality of TSC algorithms in distinguishing diffusion processes by the likelihood ratio test (LRT). The LRT is an optimal classifier by the Neyman-Pearson lemma. The LRT benchmarks are computationally efficient because the LRT does not need training, and the diffusion processes can be efficiently simulated and are flexible to reflect the specific features of real-world applications. We demonstrate the benchmarking with three widely-used TSC algorithms: random forest, ResNet, and ROCKET. These algorithms can achieve the LRT optimality for univariate time series and multivariate Gaussian processes. However, these model-agnostic algorithms are suboptimal in classifying high-dimensional nonlinear multivariate time series. Additionally, the LRT benchmark provides tools to analyze the dependence of classification accuracy on the time 
    
[^99]: 具有全知追随者的Stackelberg Games中的在线学习

    Online Learning in Stackelberg Games with an Omniscient Follower. (arXiv:2301.11518v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11518](http://arxiv.org/abs/2301.11518)

    本文研究了在线学习在两个玩家的分散合作Stackelberg博弈中的应用，假设追随者具有全知，结果表明其存在可以从常数到指数级地改变遗憾最小化的样本复杂度，存在独特挑战。

    

    本文研究了两个玩家的分散合作Stackelberg博弈中的在线学习问题。在每一轮中，领导者首先采取行动，之后追随者在观察领导者的行动后采取他们的行动。领导者的目标是通过互动历史来学习如何最小化累积遗憾。不同于传统的重复Stackelberg博弈的表述，我们假设追随者是全知的，具有真实奖励的全部知识，他们总是最佳响应领导者的行动。我们分析了该重复Stackelberg博弈中遗憾最小化的样本复杂度。我们表明，根据奖励结构，全知追随者的存在可能会使样本复杂度从常数到指数级发生巨大变化，即使是对于线性合作Stackelberg博弈也是如此。这为领导者的学习过程和随后的遗憾分析带来了独特的挑战。

    We study the problem of online learning in a two-player decentralized cooperative Stackelberg game. In each round, the leader first takes an action, followed by the follower who takes their action after observing the leader's move. The goal of the leader is to learn to minimize the cumulative regret based on the history of interactions. Differing from the traditional formulation of repeated Stackelberg games, we assume the follower is omniscient, with full knowledge of the true reward, and that they always best-respond to the leader's actions. We analyze the sample complexity of regret minimization in this repeated Stackelberg game. We show that depending on the reward structure, the existence of the omniscient follower may change the sample complexity drastically, from constant to exponential, even for linear cooperative Stackelberg games. This poses unique challenges for the learning process of the leader and the subsequent regret analysis.
    
[^100]: LDMIC：基于学习的分布式多视图图像编码

    LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.09799](http://arxiv.org/abs/2301.09799)

    LDMIC是一种基于学习的分布式多视图图像编码框架，通过独立编码器和联合上下文传输模块实现了全局视图间的相关性捕捉，对几何关系不敏感。

    LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.

    多视图图像压缩在3D相关应用中起着至关重要的作用。现有方法采用预测编码架构，需要联合编码压缩相应的视差和残差信息。这要求相机之间进行协作，并强制执行不同视图之间的极线几何约束，这使得在具有随机重叠视野的分布式相机系统中部署这些方法具有挑战性。同时，分布式源编码理论表明，可以通过独立编码和联合解码实现相关源的高效数据压缩，这激发了我们设计基于学习的分布式多视图图像编码（LDMIC）框架的动机。通过独立编码器，LDMIC引入了一个简单而有效的基于交叉注意机制的联合上下文传输模块，以有效捕捉全局视图间的相关性，对几何关系不敏感。

    Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
    
[^101]: 物理学知识作为不确定性量化模型的信息场理论

    Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07609](http://arxiv.org/abs/2301.07609)

    该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    

    数据驱动的方法结合物理学知识是建模系统的强有力技术。此类模型的目标是通过将测量结果与已知物理定律相结合，高效地求解基本场。由于许多系统包含未知元素，如缺失参数、嘈杂数据或不完整的物理定律，因此这通常被视为一种不确定性量化问题。处理所有变量的常见技术通常取决于用于近似后验的数值方案，并且希望有一种不依赖于任何离散化的方法。信息场理论（IFT）提供了对不一定是高斯场的场进行统计学的工具。我们通过将描述场的物理定律的信息编码为函数先验来扩展IFT到物理信息场理论（PIFT）。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
    
[^102]: 一种新颖的稀疏正则化方法

    A Novel Sparse Regularizer. (arXiv:2301.07285v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07285](http://arxiv.org/abs/2301.07285)

    提出一种新颖的正则化方法，关注权重矩阵内权重的空间排列，可以显著减少模型参数的非零数量。

    

    本文介绍了一种不基于 $L_{p}$-norm 的新颖正则化方法。与传统方法只考虑模型中各权重值的度量不同，这种正则化方法关注权重矩阵内权重的空间排列。该方法的加入项可以用于损失函数中，可微分，简单快速计算，与尺度无关，仅需要微小的额外内存，容易并行化。经实验证明，在相同精度水平下，该方法可以使模型参数的非零数量提高一个数量级。

    $L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and $L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniques such as weight decay and group LASSO compute a quantity which de pends on model weights considered in isolation from one another. This paper describes a novel regularizer which is not based on an $L_{p}$-norm. In contrast with $L_{p}$-norm-based regularization, this regularizer is concerned with the spatial arrangement of weights within a weight matrix. This regularizer is an additive term for the loss function and is differentiable, simple and fast to compute, scale-invariant, requires a trivial amount of additional memory, and can easily be parallelized. Empirically this method yields approximately a one order-of-magnitude improvement in the number of nonzero model parameters at a given level of accuracy.
    
[^103]: 极端图像变换在人类和机器上产生不同影响

    Extreme Image Transformations Affect Humans and Machines Differently. (arXiv:2212.13967v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.13967](http://arxiv.org/abs/2212.13967)

    本文研究了一些极端的图像变换，发现机器和人类在这些变换下的表现差异较大，机器在某些变换下比人类表现更好，但在人类容易处理的变换下表现不如人类，同时提出了一些方法来改善机器的性能。

    

    一些最近的人工神经网络(ANN)声称模拟灵长类动物神经和人类的表现数据。然而，它们在对象识别方面的成功依赖于以人类不常采用的方式利用低级特征来解决视觉任务。因此，对于ANN来说，超出分布或对抗性输入经常具有挑战性。相反，人类学习抽象的模式，并且对许多极端图像畸变的影响很小。我们引入了一组新的图像变换，受神经生理学发现的启发，并在对象识别任务上评估了人类和ANN。我们表明，对于某些变换，机器的表现比人类更好，而对于某些对人类简单的变换，机器难以与人类表现相当。我们量化了人类和机器的准确性差异，并为我们的变换数据找到了一个难度排名。我们还建议如何改变人类视觉处理的某些特征以提高ANN的性能。

    Some recent artificial neural networks (ANNs) claim to model aspects of primate neural and human performance data. Their success in object recognition is, however, dependent on exploiting low-level features for solving visual tasks in a way that humans do not. As a result, out-of-distribution or adversarial input is often challenging for ANNs. Humans instead learn abstract patterns and are mostly unaffected by many extreme image distortions. We introduce a set of novel image transforms inspired by neurophysiological findings and evaluate humans and ANNs on an object recognition task. We show that machines perform better than humans for certain transforms and struggle to perform at par with humans on others that are easy for humans. We quantify the differences in accuracy for humans and machines and find a ranking of difficulty for our transforms for human data. We also suggest how certain characteristics of human visual processing can be adapted to improve the performance of ANNs for o
    
[^104]: BaCO: 一个快速可移植的贝叶斯编译器优化框架

    BaCO: A Fast and Portable Bayesian Compiler Optimization Framework. (arXiv:2212.11142v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2212.11142](http://arxiv.org/abs/2212.11142)

    BaCO是一个快速可移植的自动化编译器优化框架，使用贝叶斯优化算法，能够以小搜索预算提供1.36x-1.56x更快的代码并以2.9x-3.9x的速度达到专家级别性能。

    

    我们介绍了贝叶斯编译器优化框架（BaCO），它是一个通用的自动调谐器，适用于面向CPU、GPU和FPGA的现代编译器。BaCO提供了处理现代自动调谐任务所需的灵活性。特别地，它处理排列、有序和连续参数类型以及已知和未知参数约束。为了推理这些参数类型并高效地生成高质量的代码，BaCO使用针对自动调谐领域专门优化的贝叶斯优化算法。我们在三个现代编译器系统：TACO、RISE＆ELEVATE和CPU、GPU和FPGA的HPVM2FPGA上展示了BaCO的有效性。对于这些领域，BaCO通过使用很小的搜索预算，平均提供1.36x-1.56x更快的代码，并且BaCO能够比专家水平的性能更快地实现2.9x-3.9x的表现。

    We introduce the Bayesian Compiler Optimization framework (BaCO), a general purpose autotuner for modern compilers targeting CPUs, GPUs, and FPGAs. BaCO provides the flexibility needed to handle the requirements of modern autotuning tasks. Particularly, it deals with permutation, ordered, and continuous parameter types along with both known and unknown parameter constraints. To reason about these parameter types and efficiently deliver high-quality code, BaCO uses Bayesian optimiza tion algorithms specialized towards the autotuning domain. We demonstrate BaCO's effectiveness on three modern compiler systems: TACO, RISE & ELEVATE, and HPVM2FPGA for CPUs, GPUs, and FPGAs respectively. For these domains, BaCO outperforms current state-of-the-art autotuners by delivering on average 1.36x-1.56x faster code with a tiny search budget, and BaCO is able to reach expert-level performance 2.9x-3.9x faster.
    
[^105]: 分级策略混合作为最优输运

    Hierarchical Policy Blending As Optimal Transport. (arXiv:2212.01938v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.01938](http://arxiv.org/abs/2212.01938)

    提出了一种分层策略混合方法，通过不平衡的最优传输实现策略混合，巩固底层黎曼运动策略的比例，有效调整黎曼矩阵，决定专家和代理人之间的优先级，保证安全和任务成功，并在一系列机器人控制的应用场景中超过现有基线。

    

    我们提出了分级策略混合作为最优输运（HiPBOT）。这种分层框架通过对专家策略的低级反应适应权重，在专家策略和代理人的参数空间上添加了一个前瞻规划层。我们的高层规划者通过不平衡的最优传输实现策略混合，巩固底层黎曼运动策略的比例，有效调整它们的黎曼矩阵，并在专家和代理人之间决定优先级，保证安全和任务成功。我们在从低维导航到高维全身控制的一系列应用场景中的实验结果展示了HiPBOT的功效和效率，该方法优于执行概率推断或定义专家树结构的现有基线，为最优输运在机器人控制中的新应用铺平了道路。

    We present hierarchical policy blending as optimal transport (HiPBOT). This hierarchical framework adapts the weights of low-level reactive expert policies, adding a look-ahead planning layer on the parameter space of a product of expert policies and agents. Our high-level planner realizes a policy blending via unbalanced optimal transport, consolidating the scaling of underlying Riemannian motion policies, effectively adjusting their Riemannian matrix, and deciding over the priorities between experts and agents, guaranteeing safety and task success. Our experimental results in a range of application scenarios from low-dimensional navigation to high-dimensional whole-body control showcase the efficacy and efficiency of HiPBOT, which outperforms state-of-the-art baselines that either perform probabilistic inference or define a tree structure of experts, paving the way for new applications of optimal transport to robot control. More material at https://sites.google.com/view/hipobot
    
[^106]: 基于查询的多模态路径融合知识库补全

    Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph. (arXiv:2212.01923v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2212.01923](http://arxiv.org/abs/2212.01923)

    基于查询的多模态路径融合算法可以有效地对知识库进行补全，提高了性能，并且使用了基于查询的技术来提高系统的效率。

    

    在过去的几年中，大型知识库已经被构建来存储大量的知识。然而，这些知识库高度不完整，例如Freebase中有70%的人没有出生地点。为了解决这个问题，我们提出了一个使用结构化和非结构化信息的多模态融合的、基于查询驱动的知识库补全系统。为了有效地融合来自Web的非结构化信息和知识库中的结构化信息以实现良好的性能，我们的系统基于问答和规则推理构建了多模态知识图。我们提出了一个多模态路径融合算法，在多模态知识图中基于不同的路径对候选答案进行排名，取得了比问答、规则推理和基线融合算法更好的性能。为了提高系统效率，我们使用了基于查询的技术来减少系统的运行时间，为用户查询提供快速响应。

    Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Ex
    
[^107]: 自我集成保护：训练检查点是良好的数据保护者

    Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors. (arXiv:2211.12005v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12005](http://arxiv.org/abs/2211.12005)

    本文提出自我集成保护技术，通过对训练数据添加不可感知的扰动，通过模型检查点的梯度发现这些样本，可以有效地防止竞争对手在数据上训练高性能模型。

    

    随着数据变得越来越重要，公司在发布数据时通常会非常谨慎，因为竞争对手可以使用它来训练高性能模型，从而对公司的商业竞争力造成巨大威胁。为了防止在数据上训练良好的模型，我们可以对其添加不可感知的扰动。由于这样的扰动旨在伤害整个训练过程，因此它们应该反映DNN训练的脆弱性，而不是单个模型的脆弱性。基于这个新想法，我们寻找在训练中始终无法识别（从未被正确分类）的扰动样本。在本文中，我们通过模型检查点的梯度发现这些样本，形成所提出的自我集成保护（SEP）。该方法非常有效，因为（1）在正常训练过程中忽略的示例上进行学习往往会产生不忽略正常示例的DNN；（2）检查点之间跨模型的梯度与正交接近，表示它们与具有不同体系结构的DNN一样多样化。

    As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. Th
    
[^108]: “我们是否确信这是异常？（arXiv:2211.09224v3 [cs.LG] UPDATED）”

    Are we certain it's anomalous?. (arXiv:2211.09224v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09224](http://arxiv.org/abs/2211.09224)

    本论文提出了一种新的异常检测方法HypAD，它采用超几何不确定性来评估异常，利用无监督学习来重新建立输入信号，这种方法可以有效解决时间序列中的异常检测问题。

    

    最近，对于时间序列和结构化数据序列的建模取得了进展，这大大推进了异常检测领域的研究。该任务用于识别金融序列、IT系统、航天测量以及医疗领域中的异常行为，其中异常检测可以帮助隔离出抑郁症和老年人的特殊病例。由于非常不确定性的时间相关性，以及异常的定义有时是主观的，所以时间序列中的异常检测是一项复杂的任务。我们在这里提出了一个新的方法，即使用“超几何不确定性”进行异常检测（HypAD）。HypAD通过无监督学习来重建输入信号。我们采用现有技术的最佳实践，通过LSTM建立序列，同时学习解码器来重建信号，并借助GAN评论家的帮助。利用超几何神经网络来进行端到端的不确定性估计。通过使用不确定性，HypAD可以评估是否存在异常。

    The progress in modelling time series and, more generally, sequences of structured data has recently revamped research in anomaly detection. The task stands for identifying abnormal behaviors in financial series, IT systems, aerospace measurements, and the medical domain, where anomaly detection may aid in isolating cases of depression and attend the elderly. Anomaly detection in time series is a complex task since anomalies are rare due to highly non-linear temporal correlations and since the definition of anomalous is sometimes subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input signal. We adopt best practices from the state-of-the-art to encode the sequence by an LSTM, jointly learned with a decoder to reconstruct the signal, with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a hyperbolic neural network. By using uncertainty, HypAD may assess whether it is
    
[^109]: 使用基于网络问答和多模态融合完成知识库

    Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion. (arXiv:2211.07098v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.07098](http://arxiv.org/abs/2211.07098)

    本文提出了一种使用基于网络问答和多模态融合的方法填补知识库中的缺失信息。通过设计一个多模态特征和问题模板的基于网络问答的系统来达到更高效的知识库补全，同时结合了知识库中的结构化信息来提高抽取质量。

    

    近年来，大型知识库已经建立来存储大量知识，然而这些知识库非常不完整。为了解决这个问题，我们提出一种使用基于网络问答和多模态融合的方法填补知识库中的缺失信息。为了利用网络上的非结构化信息完成知识库，我们设计了一个基于网络问答的系统，使用多模态特征和问题模板来提取缺失的事实，仅仅通过非常少的问题就可以达到良好的效果。同时，该问答系统还使用知识库中的结构化信息，比如实体类型和实体之间的关联性，以帮助提高抽取质量。

    Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.
    
[^110]: 一种可解释的机器学习系统来识别癫痫-间隙-损伤连续状态下的脑电图图案

    An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05207](http://arxiv.org/abs/2211.05207)

    该论文设计了一种可解释的深度学习模型，以预测ICU脑电监测中常见的6种脑波图案的存在，并提供高质量的解释和三种解释方法，这对于建立AI的信任和临床采用至关重要。

    

    在许多医学领域，人们呼吁在用于临床工作的机器学习系统中增加可解释性。在本文中，我们设计了一个可解释的深度学习模型，用于预测ICU脑电监测中常见的6种脑波图案（癫痫、LPD、GPD、LRDA、GRDA、其他）的存在。每个预测都配有一个高质量的解释，借助于专门的用户界面提供支持。此新型模型架构学习了一组原型示例（“原型”），并通过将新的EEG片段与这些原型进行比较来做出决策。这些原型可以是单类（仅与一个类相关）或双类（与两个类相关）。我们提出了三种主要的模型解释方法：1）使用全局结构保持方法，将1275维cEEG潜在特征映射到二维空间中，可视化癫痫-间隙-损伤连续状态，从而深入了解其高维结构。2）我们提出了一种交互式解释方法，使人类专家能够查询模型预测的不同方面，并以自然语言接收经过专家验证的解释。3）我们可视化了导致模型做出某个决策的输入的最重要特征，允许详细检查输入和输出之间的关系。总的来说，我们展示了解释性模型分类EEG图案和提供专家友好的解释的实用性，这两个方面对于建立AI的信任和临床采用至关重要。

    In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
    
[^111]: 有效稀疏推理用于条件GAN和扩散模型

    Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.02048](http://arxiv.org/abs/2211.02048)

    提出了空间稀疏推理（SSI）的通用技术，该技术选择性地为编辑区域执行计算并加速各种生成模型，包括条件GAN和扩散模型。通过缓存和重复使用原始图像的特征图，我们将卷积滤波器稀疏地应用于编辑区域，并在未编辑的区域中重复使用缓存特征，从而通过约$1\%$的区域编辑来减少计算资源的浪费。

    

    在图像编辑中，现有的深度生成模型往往会从头开始重新合成整个输出，包括未编辑的区域。这导致计算资源的浪费，尤其是对于较小的编辑操作。在这项工作中，我们提出了空间稀疏推理（SSI）的通用技术，该技术选择性地为编辑区域执行计算并加速各种生成模型，包括条件GAN和扩散模型。我们的关键观察是用户倾向于逐渐改变输入图像，这激发了我们缓存和重复使用原始图像的特征图的想法。给定编辑过的图像，我们稀疏地将卷积滤波器应用于编辑区域，同时重复使用未编辑区域的缓存特征。基于我们的算法，我们进一步提出了稀疏渐进式生成引擎（SIGE）来将计算减少转化为在现成硬件上的延迟减少。我们的方法通过约$1\%$的区域编辑，减少了计算资源的浪费。

    During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users tend to gradually change the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about $1\%$-area edits, our method reduces the comp
    
[^112]: 基于结构聚类的自监督异构图预训练

    Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering. (arXiv:2210.10462v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10462](http://arxiv.org/abs/2210.10462)

    该论文提出了一种新颖的自监督异构图预训练方法SHGP，它通过结构聚类产生伪标签来指导模型学习对象嵌入和注意力系数，不需要生成任何正例或负例，具有前景应用价值。

    

    最近，基于自监督的HIN预训练方法在异构信息网络上表现出有希望的竞争力，超过了传统的半监督异构图神经网络。然而，它们的性能严重依赖于各种策略的精细定制，以生成高质量的正例和负例，这明显限制了它们的灵活性和泛化能力。本文提出了一种新颖的自监督异构图预训练方法SHGP，它不需要生成任何正例或负例。它由两个模块组成，共享相同的注意力聚合机制。在每一次迭代中，Att-LPA模块通过结构聚类产生伪标签，作为自监督信号来指导Att-HGNN模块学习对象嵌入和注意力系数。两个模块可以有效地利用和增强彼此，促进模型学习。

    Recent self-supervised pre-training methods on Heterogeneous Information Networks (HINs) have shown promising competitiveness over traditional semi-supervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately, their performance heavily depends on careful customization of various strategies for generating high-quality positive examples and negative examples, which notably limits their flexibility and generalization ability. In this work, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-training approach, which does not need to generate any positive examples or negative examples. It consists of two modules that share the same attention-aggregation scheme. In each iteration, the Att-LPA module produces pseudo-labels through structural clustering, which serve as the self-supervision signals to guide the Att-HGNN module to learn object embeddings and attention coefficients. The two modules can effectively utilize and enhance each other, promoting the model to learn 
    
[^113]: 多元时间序列分类及其在不同维度下的特点的实证评估

    An Empirical Evaluation of Multivariate Time Series Classification with Input Transformation across Different Dimensions. (arXiv:2210.07713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07713](http://arxiv.org/abs/2210.07713)

    本文通过实证结果证明，多元时间序列分类中，额外的通道维度远非微不足道，最佳的结果并不是通过单独对每个通道进行缩放，而是通过利用通道之间的交互信息进行缩放来实现的。

    

    在当前研究中，针对时间数据的机器学习和深度学习解决方案，从单通道数据集（单元）向有多个信息通道的问题（多元）转移。其中大部分研究着重方法的创新和架构，输入数据的格式常常被隐含地对待。特别地，多元数据集常常被当作一系列单元时间序列的堆栈来进行输入预处理，采用单独对每个通道采用缩放方法。本研究旨在证明额外的通道维度远非微不足道，而不同的缩放方法可以显著影响解决方案的准确性。为此，我们在四个不同的时间维度上测试了七种不同的数据转换方法，并研究了它们对五种最近方法的分类准确性的影响。我们发现，对于大多数测试数据集，最佳的结果不是通过单独对每个通道进行缩放，而是通过利用通道之间的交互信息进行缩放来实现的。

    In current research, machine and deep learning solutions for the classification of temporal data are shifting from single-channel datasets (univariate) to problems with multiple channels of information (multivariate). The majority of these works are focused on the method novelty and architecture, and the format of the input data is often treated implicitly. Particularly, multivariate datasets are often treated as a stack of univariate time series in terms of input preprocessing, with scaling methods applied across each channel separately. In this evaluation, we aim to demonstrate that the additional channel dimension is far from trivial and different approaches to scaling can lead to significantly different results in the accuracy of a solution. To that end, we test seven different data transformation methods on four different temporal dimensions and study their effect on the classification accuracy of five recent methods. We show that, for the large majority of tested datasets, the be
    
[^114]: 通过学习概念的逻辑组合，全局解释GNNs

    Global Explainability of GNNs via Logic Combination of Learned Concepts. (arXiv:2210.07147v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07147](http://arxiv.org/abs/2210.07147)

    本文提出了全局逻辑GNN解释器GLGExplainer，它是第一个能够以任意布尔组合的形式生成GNN学习的图形概念的解释器。GLGExplainer提供了准确可解释的全局解释，与GNN的组合性质相一致，并可应用于任何图分类或回归问题。

    

    虽然GNN的实例级解释是一个被广泛研究的问题，并且已经开发了很多方法，但是提供GNN行为的全局解释却很少被探讨，尽管它在可解释性和调试方面具有潜力。现有的解决方案要么仅列出给定类别的局部解释，要么生成一个具有给定类别最大分数的合成原型图，完全忽略了GNN可能已经学习的任何组合性方面。在这项工作中，我们提出了GLGExplainer（基于全局逻辑的GNN解释器），它是第一个能够生成学习过的图形概念的任意布尔组合的解释的全局解释器。GLGExplainer是一个全可微架构，它以局部解释作为输入，并将它们组合成基于图形概念的逻辑公式，表示为局部解释的集群。与现有解决方案相反，GLGExplainer提供了准确和人类可解释的全局解释，与GNN的组合性质相一致，并可应用于任何图分类或回归问题。我们在多个基准数据集上展示了GLGExplainer的有效性，在其中几个数据集上实现了最先进的性能。

    While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are
    
[^115]: TimesNet: 通用时间序列分析的时间二维变化建模

    TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. (arXiv:2210.02186v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02186](http://arxiv.org/abs/2210.02186)

    TimesNet使用时间二维变化建模，将一维时间序列转换为基于多个周期的一组二维张量，从而使二维变化可以轻松地通过二维卷积核进行建模，可以用于广泛的时间序列分析任务。

    

    时间序列分析在天气预报、异常检测和动作识别等广泛应用中具有巨大重要性。本文关注于时间变化建模，这是广泛分析任务的共同关键问题。以往的方法尝试直接从一维时间序列中完成此任务，但由于复杂的时间模式，这是极具挑战性的。基于对时间序列的多周期性观察，我们将复杂的时间变化分解为多个周期内和周期间的变化。为了解决一维时间序列在表示能力方面的限制，我们将时间变化分析扩展到了二维空间中，将一维时间序列转换成基于多个周期的一组二维张量。这种转换可以将周期内和周期间的变化嵌入到二维张量的列和行中，使得二维变化可以轻松地通过二维卷积核进行建模。

    Time series analysis is of immense importance in extensive applications, such as weather forecasting, anomaly detection, and action recognition. This paper focuses on temporal variation modeling, which is the common key problem of extensive analysis tasks. Previous methods attempt to accomplish this directly from the 1D time series, which is extremely challenging due to the intricate temporal patterns. Based on the observation of multi-periodicity in time series, we ravel out the complex temporal variations into the multiple intraperiod- and interperiod-variations. To tackle the limitations of 1D time series in representation capability, we extend the analysis of temporal variations into the 2D space by transforming the 1D time series into a set of 2D tensors based on multiple periods. This transformation can embed the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively, making the 2D-variations to be easily modeled by 2D kernels. Technicall
    
[^116]: 来自演示的快速生涯适应性逆强化学习

    Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations. (arXiv:2209.11908v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11908](http://arxiv.org/abs/2209.11908)

    本文提出了一种快速生涯适应性逆强化学习框架，从学习的策略中构建多样策略的组合实现了对新的演示的快速适应，同时整合演示中的共性知识，实现准确的任务推断，还能够在大规模部署中通过维护一个精简的原型策略集合并通过策略组合来逼近所有行为。

    

    演示学习（LfD）方法使终端用户通过所需行为的演示来教授机器人新任务，从而使机器人技术的使用面更广。然而，当前的LfD框架无法快速适应异构的人类演示，也不能在普适的机器人应用中进行大规模部署。在本文中，我们提出了一种新的LfD框架——快速生涯适应性逆强化学习（FLAIR）。我们的方法(1)利用学习策略构建多样策略的组合，从而快速适应新的演示，允许快速的终端用户个性化，(2)整合演示中的共性知识，实现准确的任务推断；(3)在终身部署中只在需要时扩展其模型，通过策略组合维护一个精简的原型策略集合，并能够逼近所有行为。我们在实验证明，FLAIR实现了适应性（即机器人适应了异构的、特定于用户的任务）并且在大规模部署中节省了模型大小。

    Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task
    
[^117]: 一种利用复杂网络识别可再生电力分布系统弹性的方法

    A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2208.11543](http://arxiv.org/abs/2208.11543)

    本文提出一种使用复杂网络理论来识别可再生电力分布系统弹性的方法，可以识别系统中太阳能电池板的托管能力，从而有助于提高系统的韧性。

    

    近年来，电力配电系统广泛采用分布式能源资源（DER）以满足能源需求，普遍认为这可以提高系统的弹性。然而，由于各种因素（如间歇性可用性、天气条件的动态变化、非线性等）可能对电网运营产生不利影响。本文提出了一种使用复杂网络理论来识别带有太阳能光伏发电的配电系统弹性的方法。我们根据不同条件获得了不同条件下的复杂相关网络，并计算了各种网络参数，以识别网络的弹性。所提出的方法可以识别系统中太阳能电池板的托管能力，并在不同不良条件下保持系统的弹性，从而有助于提高系统的韧性。

    Recently, Electrical Distribution Systems are extensively penetrated with the Distributed Energy Resources (DERs) to cater the energy demands with general perception that it enhances the system resiliency. However, it may be adverse for the grid operation due to various factors like its intermittent availability, dynamics in weather condition, introduction of nonlinearity, complexity etc. This needs a detailed understanding of system resiliency that our method proposes here. We introduce a methodology using complex network theory to identify the resiliency of distribution system when incorporated with Solar PV generation under various undesirable configurations. Complex correlated networks for different conditions were obtained and various network parameters were computed for identifying the resiliency of those networks. The proposed methodology identifies the hosting capacity of solar panels in the system while maintaining the resiliency under different unwanted conditions hence helps
    
[^118]: 基于技能得分的太阳能预测的元分析

    A Meta-Analysis of Solar Forecasting Based on Skill Score. (arXiv:2208.10536v2 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2208.10536](http://arxiv.org/abs/2208.10536)

    这项研究进行了关于太阳能预测的元分析，发现预测时间段是影响技能得分最重要的因素，表明应该分别针对不同的时间段进行预测分析。气候区变量与技能得分存在显著相关性。历史数据和时空信息在太阳能预测中非常有帮助，天空和卫星图像则在日内预测中最重要，而数值气象预测和实测值在日前预测中最重要。

    

    我们进行了基于技能得分的确定性太阳能预测的第一项综合性元分析，筛选了谷歌学术上的1,447篇论文，并对320篇论文进行全文审阅以进行数据提取。建立了一个包含4,687个数据点的数据库，并运用多元自适应回归样条建模、部分依赖图和线性回归进行了分析。定量测量了十个因素对技能得分的边际影响。分析结果显示了数据库中变量之间的非线性和复杂交互作用。预测时间段对技能得分有中心影响并支配了其他因素的影响。因此，太阳能预测的分析应该针对每个时间段单独进行。气候区变量与技能得分具有显著的相关性。在输入方面，历史数据和时空信息非常有帮助。对于日内预测，天空和卫星图像最重要。对于日前预测，则是数值气象预测和本地实测值最重要。

    We conduct the first comprehensive meta-analysis of deterministic solar forecasting based on skill score, screening 1,447 papers from Google Scholar and reviewing the full texts of 320 papers for data extraction. A database of 4,687 points was built and analyzed with multivariate adaptive regression spline modelling, partial dependence plots, and linear regression. The marginal impacts on skill score of ten factors were quantified. The analysis shows the non-linearity and complex interaction between variables in the database. Forecast horizon has a central impact and dominates other factors' impacts. Therefore, the analysis of solar forecasts should be done separately for each horizon. Climate zone variables have statistically significant correlation with skill score. Regarding inputs, historical data and spatial temporal information are highly helpful. For intra-day, sky and satellite images show the most importance. For day-ahead, numerical weather predictions and locally measured me
    
[^119]: 自监督的多模态训练，利用未经整理的图像和报告实现放射学零样本监督人工智能

    Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology. (arXiv:2208.05140v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2208.05140](http://arxiv.org/abs/2208.05140)

    本文提出了一种名为Medical X-VL的视觉语言模型，使用自监督单模型和融合编码器，以实现放射学中的零样本监督人工智能。

    

    监管型AI是放射学中的新兴概念，其中AI通过不断支持放射学家的决策，形成与放射学家的共生关系。视觉语言模型的最新进展揭示了监管性AI的长期问题，即它们理解视觉和文本概念及其语义对应关系。然而，将视觉语言模型应用于医学领域的成功案例还很有限，因为目前的视觉语言模型和学习策略需要图像和文本对的网络规模数据语料库，这在医学领域通常难以实现。因此，我们提出了一种被称为医学交叉关注视觉语言模型（Medical X-VL）的模型，利用适用于医学领域的关键组件。我们的医学X-VL模型基于以下组件：医学领域的自监督单模型和融合编码器，以构建多模态视觉语言模型。

    Oversight AI is an emerging concept in radiology where the AI forms a symbiosis with radiologists by continuously supporting radiologists in their decision-making. Recent advances in vision-language models sheds a light on the long-standing problems of the oversight AI by the understanding both visual and textual concepts and their semantic correspondences. However, there have been limited successes in the application of vision-language models in the medical domain, as the current vision-language models and learning strategies for photographic images and captions call for the web-scale data corpus of image and text pairs which was not often feasible in the medical domain. To address this, here we present a model dubbed Medical Cross-attention Vision-Language model (Medical X-VL), leveraging the key components to be tailored for the medical domain. Our medical X-VL model is based on the following components: self-supervised uni-modal models in medical domain and fusion encoder to bridge
    
[^120]: 机器学习中的基于排名可分解损失函数：一项综述

    Rank-based Decomposable Losses in Machine Learning: A Survey. (arXiv:2207.08768v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08768](http://arxiv.org/abs/2207.08768)

    这篇综述回顾了机器学习中基于排名可分解损失函数的研究并提供了分类法。它讨论了可分解性与优化算法之间的相互作用，并列举了排名损失在深度学习中的最新应用和面临的挑战。

    

    近期的研究揭示了损失函数设计中区分个体损失与聚合损失的重要范式。个体损失评估模型在样本上的质量，而聚合损失将每个训练样本的个体损失/分数组合起来。两者都有一个共同的过程，将一组个体值聚合成单个数字值。排名顺序反映了设计损失中个体值之间最基本的关系。此外，可分解性，其中损失可以分解为一组个体项的集合，成为组织损失/分数的重要属性。本综述系统全面地回顾了机器学习中的基于排名可分解损失函数。具体而言，我们提供了一个按聚合损失和个体损失视角的新损失函数分类法。我们确定了形成这种损失的聚合器，它们是集合函数的示例。我们组织了有监督学习和无监督学习中的基于排名的可分解损失。我们讨论了损失的可分解性与优化算法之间的相互作用。最后，我们概述了排名损失在深度学习中的最新应用和面临的挑战。

    Recent works have revealed an essential paradigm in designing loss functions that differentiate individual losses vs. aggregate losses. The individual loss measures the quality of the model on a sample, while the aggregate loss combines individual losses/scores over each training sample. Both have a common procedure that aggregates a set of individual values to a single numerical value. The ranking order reflects the most fundamental relation among individual values in designing losses. In addition, decomposability, in which a loss can be decomposed into an ensemble of individual terms, becomes a significant property of organizing losses/scores. This survey provides a systematic and comprehensive review of rank-based decomposable losses in machine learning. Specifically, we provide a new taxonomy of loss functions that follows the perspectives of aggregate loss and individual loss. We identify the aggregator to form such losses, which are examples of set functions. We organize the rank
    
[^121]: 对抗性代理风险在二元分类中的存在性和极小化定理

    Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (arXiv:2206.09098v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09098](http://arxiv.org/abs/2206.09098)

    本文证明了对抗性代理风险的存在性、正则性和极小化定理，这一结果为对抗鲁棒性的理论提供了支持，并且可以指导算法的发展。

    

    对抗性训练是训练鲁棒性强的方法之一，然而，它从理论角度并不为人们所熟知。本文证明了对抗性代理风险的存在性、正则性和极小化定理。我们的结果解释了之前研究中有关对抗鲁棒性的一些经验观察，并提出了算法发展的新方向。此外，我们的结果将之前已知的对抗分类风险的存在和极小化定理扩展到了代理风险。

    Adversarial training is one of the most popular methods for training methods robust to adversarial attacks, however, it is not well-understood from a theoretical perspective. We prove and existence, regularity, and minimax theorems for adversarial surrogate risks. Our results explain some empirical observations on adversarial robustness from prior work and suggest new directions in algorithm development. Furthermore, our results extend previously known existence and minimax theorems for the adversarial classification risk to surrogate risks.
    
[^122]: 无欺骗性基于扰动的事后解释器

    Unfooling Perturbation-Based Post Hoc Explainers. (arXiv:2205.14772v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.14772](http://arxiv.org/abs/2205.14772)

    本文介绍了一种检测和防御扰动基础解释器的恶意攻击的方法，并在图像分类任务上进行了实验验证。该方法提供了对人工智能系统的可靠性和信任性的保障。

    

    人工智能技术的巨大进步吸引了医生、贷款人、法官和其他专业人员的关注。然而，熟悉人工智能系统的人对其决策过程缺乏透明度表示担忧。扰动基础事后解释器提供了一种对这些系统进行解释的方法，只需要查询级别访问权限即可解释任何模型。然而，最近的研究表明，这些解释器可以被恶意攻击。这一发现对审计员、监管机构和其他监督者产生了严重影响。因此，几个自然问题显而易见——我们如何审计这些黑匣子系统？我们如何确定审计对象是否是真诚配合审计的？在这项工作中，我们严格规范了这个问题，并设计了一种抵御扰动基础解释器的恶意攻击的防御方法。我们提出了检测（CAD-Detect）和防御（CAD-Defense）算法。我们在图像分类任务上的实验表明，我们的方法不仅可以高精度地检测出恶意攻击，而且还可以有效地抵御它们。我们的工作为实现人工智能系统的可靠性和信任性提供了一步。

    Monumental advancements in artificial intelligence (AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise - how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection (CAD-Detect) and de
    
[^123]: 我们需要标签规范化来微调预训练语言模型吗？

    Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12428](http://arxiv.org/abs/2205.12428)

    本文研究了不同的标签规范化技术，发现标签规范化通常会提高微调模型的性能，尤其是当使用较小的预训练语言模型时。提出了一种新的无教师网络的知识蒸馏方法，并发现其效果与知识蒸馏相当，但取决于教师网络的大小和下游任务。

    

    知识蒸馏是一种重要的神经模型压缩技术，它严重依赖于教师网络的预测来指导学生模型的训练。然而，由于预训练语言模型的不断增大，知识蒸馏经常被采用在许多涉及预训练语言模型的自然语言处理任务中。本文通过对各种预训练语言模型进行全面的实验，展示了标签规范化通常会提高微调模型的性能，即使使用较小的预训练语言模型。我们还发现，该方法的好处取决于教师网络的大小，并提出了一种新的无教师网络的知识蒸馏方法，称为“联合微调”，它使用学生模型的集合来代替教师网络，取得了与知识蒸馏相当的结果。

    Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of exp
    
[^124]: CNN在学习图像块后避免了维数灾难

    CNNs Avoid Curse of Dimensionality by Learning on Patches. (arXiv:2205.10760v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.10760](http://arxiv.org/abs/2205.10760)

    本研究解释了卷积神经网络在图像分类上的泛化性能，认为CNN在学习图像块后避免了维数灾难，并提供了支持的定量和定性证据。

    

    尽管卷积神经网络（CNN）在许多计算机视觉任务中取得了成功并具有非凡的泛化性能，但目前为止，有关预测CNN泛化错误的尝试只限于事后分析。事先解释深度神经网络的泛化性能的理论大多忽略了卷积性方面，并且不指明CNN在计算机视觉任务（如图像分类，其中图像维数为数千）中为何能够似乎克服维数灾难。我们的工作试图在假设CNN在图像块的域上运作的情况下解释CNN在图像分类上的泛化性能。我们的工作是我们所知道的第一个为CNN的泛化误差推导先验误差界的工作，并我们提供了定量和定性证据支持我们的理论。我们的基于块的理论还为什么数据增强表现出可靠性提供了解释。

    Despite the success of convolutional neural networks (CNNs) in numerous computer vision tasks and their extraordinary generalization performances, several attempts to predict the generalization errors of CNNs have only been limited to a posteriori analyses thus far. A priori theories explaining the generalization performances of deep neural networks have mostly ignored the convolutionality aspect and do not specify why CNNs are able to seemingly overcome curse of dimensionality on computer vision tasks like image classification where the image dimensions are in thousands. Our work attempts to explain the generalization performance of CNNs on image classification under the hypothesis that CNNs operate on the domain of image patches. Ours is the first work we are aware of to derive an a priori error bound for the generalization error of CNNs and we present both quantitative and qualitative evidences in the support of our theory. Our patch-based theory also offers explanation for why data
    
[^125]: AutoMLBench：自动化机器学习框架的全面实验评估

    AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks. (arXiv:2204.08358v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.08358](http://arxiv.org/abs/2204.08358)

    本文对六种AutoML框架在100个数据集上进行了全面的实验比较，发现不同框架在不同性能指标方面各有优势和劣势，框架的选择应根据特定任务要求来进行。

    

    由于机器学习应用需求的迅速增长，已经意识到有足够知识的数据科学家的数量无法跟随数字世界中不断增长的数据量和应用需求而扩展。为满足这一需求，已经开发出了几种自动机器学习（AutoML）框架，通过自动化构建机器学习流程来填补人类专业知识的差距。每个框架都带有不同的基于启发式的设计决策。在本研究中，我们对六种流行的AutoML框架（AutoWeka、AutoSKlearn、TPOT、Recipe、ATM和SmartML）在来自已建立的AutoML基准套件的100个数据集上执行了全面的评估和比较。我们的实验评估考虑了不同方面的比较，包括多个设计决策的性能影响，如时间预算、搜索空间的大小、元学习和集成构建。我们的评估结果表明，每个AutoML框架在不同性能指标方面都有其优点和缺点，框架的选择可能取决于手头任务的特定要求。

    With the booming demand for machine learning applications, it has been recognized that the number of knowledgeable data scientists can not scale with the growing data volumes and application needs in our digital world. In response to this demand, several automated machine learning (AutoML) frameworks have been developed to fill the gap of human expertise by automating the process of building machine learning pipelines. Each framework comes with different heuristics-based design decisions. In this study, we present a comprehensive evaluation and comparison of the performance characteristics of six popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe, ATM, and SmartML, across 100 data sets from established AutoML benchmark suites. Our experimental evaluation considers different aspects for its comparison, including the performance impact of several design decisions, including time budget, size of search space, meta-learning, and ensemble construction. The results of our
    
[^126]: 具有可证明的一致性和公平保证的推荐系统张量补全

    Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.01815](http://arxiv.org/abs/2204.01815)

    本文介绍了一种新的一致性方法来解决矩阵和张量补全问题，在推荐系统应用中，我们证明了通过保留单位比例和一致性两个约束条件可以实现解的存在性与唯一性。

    

    我们引入了一种新的基于一致性的方法来定义和解决非负/正矩阵和张量补全问题。该框架的新颖之处在于，我们不是人为地将问题形式化为任意优化问题，例如，最小化一个结构量，如秩或范数，而是展示了一个单一的属性/约束：保留单位比例一致性，保证了解的存在，并在相对较弱的支持假设下保证了解的唯一性。该框架和解算法也直接推广到任意维度的张量中，同时保持了固定维度 d 的问题规模的线性计算复杂性。在推荐系统应用中，我们证明了两个合理的性质，这些性质应该适用于任何 RS 问题的解，足以允许在我们的框架内建立唯一性保证。关键理论贡献是展示了这些约束下解的存在性与唯一性。

    We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
    
[^127]: 宽量子神经网络动力学的分析理论

    Analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.16711](http://arxiv.org/abs/2203.16711)

    本研究分析了训练误差的梯度下降动态，提出了一个简单的分析公式，可以捕捉到宽量子神经网络损失函数的平均行为。我们预测并表征了随机量子电路残余训练误差作为系统参数的指数衰减。

    

    参数量子电路可用作量子神经网络，并有潜力在解决学习问题时优于它们的经典对应物。迄今为止，大部分关于它们在实际问题上表现的结果是启发式的。特别是，对于量子神经网络的训练收敛率还没有完全理解。在这里，我们分析梯度下降的动态，研究一类可变量量子机器学习模型的训练误差。我们将宽量子神经网络定义为带有大量量子位和可变参数的参数化量子电路极限。我们然后发现了一个简单的分析公式，可以捕捉到它们损失函数的平均行为，并讨论了我们研究的结果的后果。例如，对于随机量子电路，我们预测并表征了残余训练误差作为系统参数的指数衰减。我们最终通过各种量子电路的数值模拟验证了我们的分析理论，并展示了与我们的预测一致的结果。

    Parameterized quantum circuits can be used as quantum neural networks and have the potential to outperform their classical counterparts when trained for addressing learning problems. To date, much of the results on their performance on practical problems are heuristic in nature. In particular, the convergence rate for the training of quantum neural networks is not fully understood. Here, we analyze the dynamics of gradient descent for the training error of a class of variational quantum machine learning models. We define wide quantum neural networks as parameterized quantum circuits in the limit of a large number of qubits and variational parameters. We then find a simple analytic formula that captures the average behavior of their loss function and discuss the consequences of our findings. For example, for random quantum circuits, we predict and characterize an exponential decay of the residual training error as a function of the parameters of the system. We finally validate our analy
    
[^128]: 自监督语音识别中高效微调的代表性子集选择

    Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition. (arXiv:2203.09829v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09829](http://arxiv.org/abs/2203.09829)

    COWERAGE算法提出，通过训练错误率保证样本覆盖度，实现在自监督语音识别中高效微调。

    

    自监督语音识别模型需要大量标注数据才能学习高保真的语音识别表征，这需要很大的计算量和时间成本。本文研究了在自监督语音模型中识别最佳数据子集以实现高效微调的任务。我们发现，在视觉任务中用于抽样最具信息的例子的数据集修剪策略不如随机子集选择在自监督语音识别中效果好。我们提出COWERAGE算法，以在自监督语音识别中选择代表性子集。COWERAGE基于我们的发现，在早期的训练轮数中基于训练字错误率(WER)保证例子覆盖度可以实现更好的泛化表现。对TIMIT、Librispeech和LJSpeech数据集上使用wav2vec 2.0和HuBERT模型进行的大量实验表明了COWERAGE的有效性和可传递性。

    Self-supervised speech recognition models require considerable labeled training data for learning high-fidelity representations for Automatic Speech Recognition (ASR) which is computationally demanding and time-consuming. We consider the task of identifying an optimal subset of data for efficient fine-tuning in self-supervised speech models for ASR. We discover that the dataset pruning strategies used in vision tasks for sampling the most informative examples do not perform better than random subset selection on fine-tuning self-supervised ASR. We then present the COWERAGE algorithm for representative subset selection in self-supervised ASR. COWERAGE is based on our finding that ensuring the coverage of examples based on training Word Error Rate (WER) in the early training epochs leads to better generalization performance. Extensive experiments with the wav2vec 2.0 and HuBERT model on TIMIT, Librispeech, and LJSpeech datasets show the effectiveness of COWERAGE and its transferability a
    
[^129]: OLIVE: 基于可信执行环境的隐私保护联邦学习防范稀疏性风险

    OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification. (arXiv:2202.07165v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07165](http://arxiv.org/abs/2202.07165)

    本文通过分析FL中TEE的漏洞，并在TEE中引入Oblivious Memory Access（OMA）以保护免受稀疏化风险的影响，提出了OLIVE算法，该算法在通信效率和模型精度方面优于最先进的安全聚合和差分隐私FL算法。

    

    结合可信执行环境(TEE)的联邦学习(FL)是实现隐私保护FL的一种有前途的方法，近年来引起了广泛的学术关注。在服务器端实现TEE可以使每轮FL在不将客户端梯度信息暴露给不可信的服务器的情况下进行。这解决了现有安全聚合方案中存在的可用性差距以及差分隐私FL中的效用差距。然而，为了解决使用TEE的问题，需要考虑服务器端TEE的漏洞，这在FL的背景下尚未得到充分的研究。本研究的主要技术贡献是分析FL中TEE的漏洞和防御。首先，我们在理论上分析了内存访问模式的泄漏，揭示了稀疏梯度的风险，稀疏梯度通常用于增强通信效率和模型精度。其次，我们设计了一个推理攻击以保护免受稀疏化风险的影响，该攻击使用TEE中的混淆RAM引入了Oblivious Memory Access(OMA)。我们对真实数据集的实验表明，我们提出的算法OLIVE在通信效率和模型精度方面都优于最先进的安全聚合和差分隐私FL算法。

    Combining Federated Learning (FL) with a Trusted Execution Environment (TEE) is a promising approach for realizing privacy-preserving FL, which has garnered significant academic attention in recent years. Implementing the TEE on the server side enables each round of FL to proceed without exposing the client's gradient information to untrusted servers. This addresses usability gaps in existing secure aggregation schemes as well as utility gaps in differentially private FL. However, to address the issue using a TEE, the vulnerabilities of server-side TEEs need to be considered -- this has not been sufficiently investigated in the context of FL. The main technical contribution of this study is the analysis of the vulnerabilities of TEE in FL and the defense. First, we theoretically analyze the leakage of memory access patterns, revealing the risk of sparsified gradients, which are commonly used in FL to enhance communication efficiency and model accuracy. Second, we devise an inference at
    
[^130]: 强韧的混合学习：专家增强

    Robust Hybrid Learning With Expert Augmentation. (arXiv:2202.03881v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03881](http://arxiv.org/abs/2202.03881)

    该论文介绍了一种名为“专家增强”的混合数据增强策略，可以将其纳入混合系统以提高泛化性能，该方法可以有效克服混合模型性能仅限于训练分布的限制。作者在三个控制实验中从常微分方程和偏微分方程建模动态系统，并在真实双摆数据集上评估了该方法的潜在应用。

    

    混合建模通过将专家模型与从数据中学习的机器学习（ML）组件相结合，减少了专家模型的错误建模。与许多机器学习算法类似，混合模型的性能保证仅限于训练分布。利用专家模型通常在训练域外也适用的见解，我们通过引入一种名为专家增强的混合数据增强策略克服了这个限制。基于混合建模的概率形式化，我们证明了可以将专家增强纳入现有的混合系统以提高泛化性能。我们还在三个控制实验中从常微分方程和偏微分方程建模动态系统。最后，我们评估了专家增强在真实双摆数据集上的潜在现实世界应用。

    Hybrid modelling reduces the misspecification of expert models by combining them with machine learning (ML) components learned from data. Similarly to many ML algorithms, hybrid model performance guarantees are limited to the training distribution. Leveraging the insight that the expert model is usually valid even outside the training domain, we overcome this limitation by introducing a hybrid data augmentation strategy termed \textit{expert augmentation}. Based on a probabilistic formalization of hybrid modelling, we demonstrate that expert augmentation, which can be incorporated into existing hybrid systems, improves generalization. We empirically validate the expert augmentation on three controlled experiments modelling dynamical systems with ordinary and partial differential equations. Finally, we assess the potential real-world applicability of expert augmentation on a dataset of a real double pendulum.
    
[^131]: 一种基于混合物理与机器学习的宏观交通状态估计方法

    A Hybrid Physics Machine Learning Approach for Macroscopic Traffic State Estimation. (arXiv:2202.01888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01888](http://arxiv.org/abs/2202.01888)

    本文提出了一种基于混合物理与机器学习的宏观交通状态估计方法，结合交通物理模型和机器学习方法， 使用来自交通传感器的信息作为输入以构建准确且完整的高速公路系统交通状态估计。

    

    完整的交通状态信息（例如流量、速度和密度）对于智能交通系统（ITS）的正常运行至关重要。然而，由于大多数区域安装的交通检测器不足，因此只能采集到不完整的交通信息，这是普及ITS的主要障碍。为解决这个问题，本文介绍了一种创新的交通状态估计（TSE）框架，该框架利用交通物理模型和机器学习方法（如人工神经网络（ANN）、随机森林（RF）和支持向量机（SVM））并结合来自交通传感器的数量有限的信息作为输入，构建准确且完整的高速公路系统交通状态估计。

    Full-field traffic state information (i.e., flow, speed, and density) is critical for the successful operation of Intelligent Transportation Systems (ITS) on freeways. However, incomplete traffic information tends to be directly collected from traffic detectors that are insufficiently installed in most areas, which is a major obstacle to the popularization of ITS. To tackle this issue, this paper introduces an innovative traffic state estimation (TSE) framework that hybrid regression machine learning techniques (e.g., artificial neural network (ANN), random forest (RF), and support vector machine (SVM)) with a traffic physics model (e.g., second-order macroscopic traffic flow model) using limited information from traffic sensors as inputs to construct accurate and full-field estimated traffic state for freeway systems. To examine the effectiveness of the proposed TSE framework, this paper conducted empirical studies on a real-world data set collected from a stretch of I-15 freeway in S
    
[^132]: 通过混合模型进行有限全局混淆的因果推断

    Causal Inference Despite Limited Global Confounding via Mixture Models. (arXiv:2112.11602v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11602](http://arxiv.org/abs/2112.11602)

    本论文提出了一种基于混合模型的因果推断方法，通过解决混合问题和恢复概率分布，可以确定原本无法确定的因果关系。

    

    贝叶斯网络是一组$n$个随机变量（图的顶点）上的有向无环图（DAG）; 贝叶斯网络分布（BND）是在图上马尔可夫的随机变量的概率分布。这种模型的有限$k$-混合由一个更大的图形式化表示，该图具有一个额外的“隐藏”（或“潜在”）随机变量$U$，其范围为$\{1,\ldots,k\}$，并且$U$到每个其他顶点都有一个有向边。这种类型的模型在因果推断中是基本的，其中$U$模拟了多个群体的未观察到的混淆效应，使得可观察的DAG中的因果关系变得模糊不清。通过解决混合问题并恢复$U$上的联合概率分布，传统上无法确定的因果关系变得可确定。通过将其约化为更为研究的“空”图中的“乘积”情况，我们提出了第一个学习非空DAG的混合算法。

    A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random variables (the vertices); a Bayesian Network Distribution (BND) is a probability distribution on the random variables that is Markovian on the graph. A finite $k$-mixture of such models is graphically represented by a larger graph which has an additional "hidden" (or "latent") random variable $U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other vertex. Models of this type are fundamental to causal inference, where $U$ models an unobserved confounding effect of multiple populations, obscuring the causal relationships in the observable DAG. By solving the mixture problem and recovering the joint probability distribution on $U$, traditionally unidentifiable causal relationships become identifiable. Using a reduction to the more well-studied "product" case on empty graphs, we give the first algorithm to learn mixtures of non-empty DAGs.
    
[^133]: 通过专家引导的对称检测进行数据增强以提高离线强化学习的性能

    Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning. (arXiv:2112.09943v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09943](http://arxiv.org/abs/2112.09943)

    本文研究了一种通过专家引导的对称检测算法进行数据增强的方法来提高离线强化学习的性能，针对非确定性MDP提出了一种结合监督和无监督学习的检测算法，实验证明该方法在一组基准任务上明显提高了强化学习算法的性能。

    

    非确定性马尔可夫决策过程（MDP）的离线动态模型估计是一项非常困难的任务，它在很大程度上取决于学习阶段可用的数据。有时，模型的动态特性与当前状态和动作的某些变换是不变的。最近的研究表明，一种基于密度估计方法的专家引导流水线，如基于深度神经网络的归一化流，可有效检测确定性环境中的这种结构，包括类别和连续值。所获得的知识可以被利用来增强原始数据集，最终导致真实模型和学习模型之间的分布偏移减少。这种数据增强技术可以作为一个初步过程，在采用离线强化学习架构之前执行，提高其性能。在本研究中，我们将这种范例扩展到解决非确定性MDP，特别是1）我们提出了一种结合监督和无监督学习的检测算法来识别和利用对称性，2）我们展示了所提出的方法在一组基准任务上有效地提高了离线强化学习算法的性能。

    Offline estimation of the dynamical model of a Markov Decision Process (MDP) is a non-trivial task that greatly depends on the data available in the learning phase. Sometimes the dynamics of the model is invariant with respect to some transformations of the current state and action. Recent works showed that an expert-guided pipeline relying on Density Estimation methods as Deep Neural Network based Normalizing Flows effectively detects this structure in deterministic environments, both categorical and continuous-valued. The acquired knowledge can be exploited to augment the original data set, leading eventually to a reduction in the distributional shift between the true and the learned model. Such data augmentation technique can be exploited as a preliminary process to be executed before adopting an Offline Reinforcement Learning architecture, increasing its performance. In this work we extend the paradigm to also tackle non-deterministic MDPs, in particular, 1) we propose a detection 
    
[^134]: MDPFuzz: 用于测试解决马尔可夫决策过程的模型的黑盒模糊测试框架

    MDPFuzz: Testing Models Solving Markov Decision Processes. (arXiv:2112.02807v4 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2112.02807](http://arxiv.org/abs/2112.02807)

    MDPFuzz是第一个用于测试解决MDP的模型的黑盒模糊测试框架。该框架通过检查这些模型是否进入异常危险状态来生成测试预言，并使用高斯混合模型及动态期望最大化的方法来量化状态序列的“新鲜度”以决定保留哪个突变状态。

    

    马尔可夫决策过程（MDP）提供了一种数学框架，用于建模顺序决策问题，其中许多问题对于安全和可靠性是至关重要的，例如自动驾驶和机器人控制。人工智能研究的快速发展已经创造出了解决MDP的有效方法，例如深度神经网络（DNNs）、强化学习（RL）和模仿学习（IL）。然而，这些受欢迎的解决MDP的模型既没有得到彻底的测试，也不是严格可靠的。本文提出MDPFuzz，这是一种用于模型解决MDP的黑盒模糊测试框架。MDPFuzz通过检查目标模型是否进入异常和危险状态来形成测试预言。在模糊化过程中，MDPFuzz通过测量是否可以减少累积奖励或形成新的状态序列来决定保留哪个突变状态。我们设计了有效的技术来使用高斯混合模型（GMMs）和动态期望最大化量化状态序列的“新鲜度”。

    The Markov decision process (MDP) provides a mathematical framework for modeling sequential decision-making problems, many of which are crucial to security and safety, such as autonomous driving and robot control. The rapid development of artificial intelligence research has created efficient methods for solving MDPs, such as deep neural networks (DNNs), reinforcement learning (RL), and imitation learning (IL). However, these popular models solving MDPs are neither thoroughly tested nor rigorously reliable.  We present MDPFuzz, the first blackbox fuzz testing framework for models solving MDPs. MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states. During fuzzing, MDPFuzz decides which mutated state to retain by measuring if it can reduce cumulative rewards or form a new state sequence. We design efficient techniques to quantify the "freshness" of a state sequence using Gaussian mixture models (GMMs) and dynamic expectation-maximization 
    
[^135]: 零样本迁移学习中的综合缩放技术

    Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.10050](http://arxiv.org/abs/2111.10050)

    提出了一种名为 BASIC 的综合缩放方法，在零样本迁移学习任务中实现了85.7%的高准确率，并在稳健性基准测试中表现出显着的改进。为了实现这些结果，该方法在数据大小、模型大小和批量大小三个维度上对比学习框架进行了放大。

    

    我们提出了一种综合缩放方法（称为 BASIC），在不学习任何标记的 ImageNet 示例的情况下，该方法在 ImageNet ILSVRC-2012 验证集上实现了85.7%的 top-1 准确率。该准确率超过了最佳发布的类似模型（CLIP 和 ALIGN）9.3%。我们的 BASIC 模型还在稳健性基准测试中表现出了显著的改进。例如，在 5 个具有自然分布偏移的测试集（例如 ImageNet-{A,R,V2, Sketch} 和 ObjectNet）上，我们的模型实现了84.3% 的 top-1 平均准确率，只有一个小小的跌落，与其原始的 ImageNet 准确率相比。为了实现这些结果，我们在三个维度上放大了 CLIP 和 ALIGN 的对比学习框架：数据大小，模型大小和批量大小。我们的数据集拥有 66 亿个带有噪声的图像-文本对，比 ALIGN 大 4 倍，比 CLIP 大 16 倍。我们最大的模型有 30 亿个权重，参数比 ALIGN 和 CLIP 多出 3.75 倍，FLOPs 多出 8 倍。最后，我们的批处理大小为 65536，比 CLIP 多 2 倍，比 ALIGN 多 4 倍。

    We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than
    
[^136]: 基于继承表示的强化学习中的时间抽象

    Temporal Abstraction in Reinforcement Learning with the Successor Representation. (arXiv:2110.05740v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.05740](http://arxiv.org/abs/2110.05740)

    本研究表明，基于状态访问模式对状态进行编码的继承表示（SR）可以被看作发现和使用时间抽象的自然基质，并且可以用于发现有助于进行暂时扩展探索或规划的选项。

    

    多层次的时间抽象是智能的一个重要特征。在强化学习中，这通常通过被称为选项的暂时性行动来建模。选项允许智能体在环境中进行预测并在不同的抽象层次上操作。然而，基于选项框架的方法通常从已知的合理选项集开始假设。当没有这种情况时，对于应该考虑哪些选项，没有明确的答案。本文认为，基于状态访问模式对状态进行编码的继承表示（SR）可以被视为发现和使用时间抽象的自然基质。为了支持我们的观点，我们从近期研究的全局视角出发，展示了如何使用SR发现有助于进行暂时扩展探索或规划的选项。我们将这些结果结合到一个完整的实验中，并在多个基准环境下进行了测试。

    Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation (SR), which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the SR can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these re
    
[^137]: 曲率感知的无导数优化

    Curvature-Aware Derivative-Free Optimization. (arXiv:2109.13391v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2109.13391](http://arxiv.org/abs/2109.13391)

    本文介绍了曲率感知的无导数优化算法——曲率感知随机搜索（CARS），该算法使用一阶和二阶有限差分逼近来计算候选步长，证明了其对于强凸目标函数的线性收敛性，并提出了无需变量转换的立方正则化变体CARS-CR，可以以O(k^-1)的速率收敛。

    

    本文讨论无导数优化问题，即仅通过函数评价而非梯度或方向导数来最小化一个函数。经典的无导数优化算法，如Nelder-Mead和直接搜索类似于梯度下降，对于高维问题的可扩展性有限。随着大规模机器学习应用的需求，零阶方法越来越受到青睐，本文重点讨论这些方法中步长αk的选择。所提出的方法名为曲率感知随机搜索（CARS），使用一阶和二阶有限差分逼近来计算候选步长α+。我们证明，对于强凸目标函数，只要搜索方向来自满足非常温和条件的分布，CARS就可以线性收敛。我们还提出了CARS-CR，它是CARS的立方正则化变体，无需进行变量转换即可以O(k^-1)的速率收敛。

    The paper discusses derivative-free optimization (DFO), which involves minimizing a function without access to gradients or directional derivatives, only function evaluations. Classical DFO methods, which mimic gradient-based methods, such as Nelder-Mead and direct search have limited scalability for high-dimensional problems. Zeroth-order methods have been gaining popularity due to the demands of large-scale machine learning applications, and the paper focuses on the selection of the step size $\alpha_k$ in these methods. The proposed approach, called Curvature-Aware Random Search (CARS), uses first- and second-order finite difference approximations to compute a candidate $\alpha_{+}$. We prove that for strongly convex objective functions, CARS converges linearly provided that the search direction is drawn from a distribution satisfying very mild conditions. We also present a Cubic Regularized variant of CARS, named CARS-CR, which converges in a rate of $\mathcal{O}(k^{-1})$ without t
    
[^138]: 网络内学习：分布式训练和网络中的推断

    In-Network Learning: Distributed Training and Inference in Networks. (arXiv:2107.03433v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.03433](http://arxiv.org/abs/2107.03433)

    提出了一种在移动设备和分布式网络中实现分布式训练和推断的算法和架构，并实现了推断的传播和融合。与现有技术相比，该方法具有更好的性能表现。

    

    现代机器学习技术的成功使得移动设备和无线网络能够实现重要的新服务，然而，由于数据和处理能力在无线网络中高度分布，这也带来了重大挑战。本文提出了一种学习算法和架构，利用多个数据流和处理单元，不仅在训练阶段而且在推断阶段进行推断传播和融合。我们研究了提出方法的设计准则及其对带宽的要求。同时，我们还讨论了在典型的无线电接入中使用神经网络的实现方面，并提供了实验证明了本方法相比现有技术的优势。

    It is widely perceived that leveraging the success of modern machine learning techniques to mobile devices and wireless networks has the potential of enabling important new services. This, however, poses significant challenges, essentially due to that both data and processing power are highly distributed in a wireless network. In this paper, we develop a learning algorithm and an architecture that make use of multiple data streams and processing units, not only during the training phase but also during the inference phase. In particular, the analysis reveals how inference propagates and fuses across a network. We study the design criterion of our proposed method and its bandwidth requirements. Also, we discuss implementation aspects using neural networks in typical wireless radio access; and provide experiments that illustrate benefits over state-of-the-art techniques.
    
[^139]: GitTables：一个大型关系表语料库

    GitTables: A Large-Scale Corpus of Relational Tables. (arXiv:2106.07258v5 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2106.07258](http://arxiv.org/abs/2106.07258)

    GitTables是一个从GitHub中提取的大型语料库，包含1M个关系表格，可以有效训练和评估高容量模型，以改进关系表格任务

    

    深度学习的成功引发了对使用训练于大型表格语料库的表格表示模型改进关系表格任务（如数据准备和搜索）的兴趣。现有的表格语料库主要包含从HTML页面中提取的表格，限制了表示离线数据库表格的能力。为了训练和评估超出Web应用的高容量模型，我们需要拥有类似于关系数据库表格的表格资源。在这里，我们介绍了GitTables，这是一个从GitHub中提取的1M个关系表格的语料库。我们持续更新旨在将此语料库扩展至至少10M个表格。对GitTables的分析显示其结构、内容和主题覆盖范围与现有表格语料库存在显著差异。我们通过Schema.org和DBpedia对GitTables中的表格列进行语义类型、层次关系和描述注释。我们在T2Dv2基准评估上的注释流程验证表明，我们的方法可以提供高质量的结果。

    The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce GitTables, a corpus of 1M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of GitTables show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotate table columns in GitTables with semantic types, hierarchical relations and descriptions from Schema.org and DBpedia. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results o
    
[^140]: Vec2GC -- 基于图的文本表示聚类方法

    Vec2GC -- A Graph Based Clustering Method for Text Representations. (arXiv:2104.09439v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2104.09439](http://arxiv.org/abs/2104.09439)

    本文介绍了一种文本表示聚类方法Vec2GC，将聚类算法与基于文本表示学习创建的术语或文档加权图的社区检测相结合，可以用于无监督的文档处理。

    

    在有限或没有标签数据的NLP流水线中，需要依赖无监督方法进行文档处理。无监督方法通常依赖于术语或文档的聚类。本文介绍了一种新的聚类算法，Vec2GC (Vector to Graph Communities)，它是一个端到端的流水线，可以针对任何给定的文本语料库聚类术语或文档。我们的方法使用基于文本表示学习创建的术语或文档加权图的社区检测。Vec2GC聚类算法是一种基于密度的方法，同时支持层次聚类。

    NLP pipelines with limited or no labeled data, rely on unsupervised methods for document processing. Unsupervised approaches typically depend on clustering of terms or documents. In this paper, we introduce a novel clustering algorithm, Vec2GC (Vector to Graph Communities), an end-to-end pipeline to cluster terms or documents for any given text corpus. Our method uses community detection on a weighted graph of the terms or documents, created using text representation learning. Vec2GC clustering algorithm is a density based approach, that supports hierarchical clustering as well.
    
[^141]: 用Wasserstein插值进行时间序列插补以达到最佳远期偏差和方差平衡

    Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff. (arXiv:2102.12736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.12736](http://arxiv.org/abs/2102.12736)

    该论文提出了一种基于贝叶斯后验共识分布的插补方法，可以在控制方差和前瞻性偏差权衡的同时进行时间序列插补，适用于金融等领域。

    

    缺失的时间序列数据是一个普遍存在的实际问题。时间序列数据中的插补方法通常被应用于完整的面板数据，目的是为了训练一个用于下游样本外任务的模型。例如，在金融中，缺失收益的插补可能被应用于在训练组合优化模型之前。然而，这种做法可能会导致未来性能上的前瞻性偏差。使用完整数据集进行插补存在使用只训练数据的插补中较大的方差之间的固有权衡。通过连接时间中显示的信息层次，我们提出了一个贝叶斯后验共识分布，它在插补中最优地控制了方差和前瞻性偏差的权衡。我们展示了我们的方法在合成和真实金融数据中的优点。

    Missing time-series data is a prevalent practical problem. Imputation methods in time-series data often are applied to the full panel data with the purpose of training a model for a downstream out-of-sample task. For example, in finance, imputation of missing returns may be applied prior to training a portfolio optimization model. Unfortunately, this practice may result in a look-ahead-bias in the future performance on the downstream task. There is an inherent trade-off between the look-ahead-bias of using the full data set for imputation and the larger variance in the imputation from using only the training data. By connecting layers of information revealed in time, we propose a Bayesian posterior consensus distribution which optimally controls the variance and look-ahead-bias trade-off in the imputation. We demonstrate the benefit of our methodology both in synthetic and real financial data.
    
[^142]: SoK: 深度神经网络的认证鲁棒性

    SoK: Certified Robustness for Deep Neural Networks. (arXiv:2009.04131v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.04131](http://arxiv.org/abs/2009.04131)

    本文系统地研究了深度神经网络的认证鲁棒性，并提供了全面的基准测试。论文总结了关于凸松弛、混合整数规划和随机平滑等方法的最新研究进展，并讨论了其未来研究方向和应用。

    

    深度神经网络在各种任务上取得了最先进的性能，但最近的研究表明，深度神经网络容易受到对抗攻击，这在将这些模型部署到自动驾驶等安全关键型应用时引起了重大关注。不同的防御方法已被提出来对抗对抗攻击，包括经验性防御和认证鲁棒性防御。本文系统化研究了认证鲁棒性防御方法及相关的实际和理论意义和发现。同时，我们还首次对各种数据集上的现有鲁棒性认证和训练方法进行了全面的基准测试。特别是，我们关注基于凸松弛、混合整数规划和随机平滑的认证鲁棒性方法。此外，我们总结了对更先进的深度神经网络，如图卷积神经网络（GCNN）和生成模型的认证鲁棒性的最新进展。最后，我们讨论了未来的研究方向和认证鲁棒性DNN的潜在应用。

    Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which can usually be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In par
    
[^143]: 阶乘幂的威力: (随机) 优化的新参数设置。

    The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization. (arXiv:2006.01244v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.01244](http://arxiv.org/abs/2006.01244)

    本研究提出使用阶乘幂作为新的参数设置工具，可以简化或提高动量法和随机优化方法的收敛速度。

    

    凸优化和非凸优化方法的收敛速度取决于许多常数的选择，包括步长、Lyapunov函数常数和动量常数。在本文中，我们提出使用阶乘幂作为定义出现在收敛证明中的常数的灵活工具。我们列举了这些数列具有的一些显著性质，并展示了如何将它们应用于加速梯度方法、动量法和随机方差缩减方法（SVRG）的收敛证明中，以简化或改进收敛速度。

    The convergence rates for convex and non-convex optimization methods depend on the choice of a host of constants, including step sizes, Lyapunov function constants and momentum constants. In this work we propose the use of factorial powers as a flexible tool for defining constants that appear in convergence proofs. We list a number of remarkable properties that these sequences enjoy, and show how they can be applied to convergence proofs to simplify or improve the convergence rates of the momentum method, accelerated gradient and the stochastic variance reduced method (SVRG).
    
[^144]: 通过逻辑推理与统计学习提高认证鲁棒性

    Improving Certified Robustness via Statistical Learning with Logical Reasoning. (arXiv:2003.00120v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2003.00120](http://arxiv.org/abs/2003.00120)

    本研究通过将统计 ML 模型与逻辑推理组件集成，提出了方法来进一步提高 ML 模型的认证鲁棒性，同时给出了首个适用于马尔可夫逻辑网络的鲁棒性界限。

    

    最近，针对复杂 ML 模型的认证鲁棒性快速提高需要进行密集的算法工作。然而，目前鲁棒性认证方法只能在有限的扰动半径内进行认证。考虑到现有的纯数据驱动的统计方法已经达到瓶颈，在本文中，我们提出使用马尔可夫逻辑网络（MLN）将统计 ML 模型与知识（通过逻辑规则表示）作为推理组件进行集成，以进一步提高整体的认证鲁棒性。这引发了关于认证这种范式（特别是推理组件，如 MLN）鲁棒性的新的研究问题。作为理解这些问题的第一步，我们首先证明了证明 MLN 鲁棒性计算复杂度是 #P-难的。在这个难度结果的指导下，我们通过仔细分析不同的模型规则，推导出了 MLN 的第一个认证的鲁棒性界限。最后，我们在两个广泛使用的数据集上进行了实验验证。

    Intensive algorithmic efforts have been made to enable the rapid improvements of certificated robustness for complex ML models recently. However, current robustness certification methods are only able to certify under a limited perturbation radius. Given that existing pure data-driven statistical approaches have reached a bottleneck, in this paper, we propose to integrate statistical ML models with knowledge (expressed as logical rules) as a reasoning component using Markov logic networks (MLN, so as to further improve the overall certified robustness. This opens new research questions about certifying the robustness of such a paradigm, especially the reasoning component (e.g., MLN). As the first step towards understanding these questions, we first prove that the computational complexity of certifying the robustness of MLN is #P-hard. Guided by this hardness result, we then derive the first certified robustness bound for MLN by carefully analyzing different model regimes. Finally, we c
    
[^145]: 使用贝叶斯神经网络估计地震断层的不确定性

    Estimating uncertainty of earthquake rupture using Bayesian neural network. (arXiv:1911.09660v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1911.09660](http://arxiv.org/abs/1911.09660)

    本文使用贝叶斯神经网络估计地震断层不确定性，通过解决数据不足问题和确定导致断层的参数组合，并使用两千次的断层模拟来训练和测试模型，最终得分0.83。

    

    贝叶斯神经网络（BNN）是一种结合了神经网络（NN）和随机过程优势的概率模型。因此，BNN可以解决过度拟合问题，并在数据有限的应用中表现良好。地震断层研究就是这样一个数据不足的问题，科学家必须依靠许多试验和错误的数值或物理模型来进行研究。在这项工作中，使用BNN，（1）解决数据问题，（2）找出导致地震断裂的参数组合，（3）估计地震断层的不确定性。使用了两千次的断层模拟来训练和测试模型，在每个模拟中，一个简单的2D断层结构被考虑，在中心处具有高斯几何异质性，8个参数在每次模拟中变化。BNN的测试F1得分为0.83。

    Bayesian neural networks (BNN) are the probabilistic model that combines the strengths of both neural network (NN) and stochastic processes. As a result, BNN can combat overfitting and perform well in applications where data is limited. Earthquake rupture study is such a problem where data is insufficient, and scientists have to rely on many trial and error numerical or physical models. Lack of resources and computational expenses, often, it becomes hard to determine the reasons behind the earthquake rupture. In this work, a BNN has been used (1) to combat the small data problem and (2) to find out the parameter combinations responsible for earthquake rupture and (3) to estimate the uncertainty associated with earthquake rupture. Two thousand rupture simulations are used to train and test the model. A simple 2D rupture geometry is considered where the fault has a Gaussian geometric heterogeneity at the center, and eight parameters vary in each simulation. The test F1-score of BNN (0.83
    

