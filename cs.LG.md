# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SCENES: Subpixel Correspondence Estimation With Epipolar Supervision.](http://arxiv.org/abs/2401.10886) | SCENES提出了一种使用极线监督的亚像素对应关系估计方法，通过仅使用相机姿态信息而不需要3D结构，以放宽对数据集的特征要求。 |
| [^2] | [Applications of flow models to the generation of correlated lattice QCD ensembles.](http://arxiv.org/abs/2401.10874) | 本论文介绍了将机器学习的归一化流应用于格量子场论中，生成相关的格规场态集合，并且演示了利用这些相关性可以减少计算观测量时的方差。同时通过三个具体应用证明了机器学习流明显降低了统计不确定性。 |
| [^3] | [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning.](http://arxiv.org/abs/2401.10862) | 本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。 |
| [^4] | [Ensembler: Combating model inversion attacks using model ensemble during collaborative inference.](http://arxiv.org/abs/2401.10859) | Ensembler是一个防止模型反演攻击的可扩展框架，通过利用模型集成和引入扰动的方式，在协作推理过程中有效地保护数据隐私。 |
| [^5] | [Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis.](http://arxiv.org/abs/2401.10844) | 该论文研究了神经群体解码对于癌症亚型诊断的影响，通过采用取胜带走(WTA)网络和基因相似性网络的方法对多组学数据进行分析，展现了WTA网络在分类性能上的重要作用。 |
| [^6] | [Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency.](http://arxiv.org/abs/2401.10843) | 本论文提出了一个通用训练框架，通过在有限的时间步内增强特征学习和激活效率，为更加节能的脉冲神经网络（SNNs）提供了解决方案。 |
| [^7] | [Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media.](http://arxiv.org/abs/2401.10841) | 这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。 |
| [^8] | [Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems.](http://arxiv.org/abs/2401.10840) | 本文提出了一种符号认知诊断（SCD）框架，通过利用符号树明确表示学生与练习互动，以及梯度优化方法学习参数，同时提高了泛化能力和可解释性。 |
| [^9] | [Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework.](http://arxiv.org/abs/2401.10839) | Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials. |
| [^10] | [Understanding Video Transformers via Universal Concept Discovery.](http://arxiv.org/abs/2401.10831) | 本文研究了视频Transformer的可解释性问题，引入了视频Transformer概念发现算法来解释其决策过程，并揭示了时空推理机制和对象为中心的表示。 |
| [^11] | [A survey on recent advances in named entity recognition.](http://arxiv.org/abs/2401.10825) | 这篇综述调查了最近的命名实体识别研究进展，并提供了对不同算法性能的深度比较，还探讨了数据集特征对方法行为的影响。 |
| [^12] | [Optimisation in Neurosymbolic Learning Systems.](http://arxiv.org/abs/2401.10819) | 神经符号学习系统的优化研究了如何将深度学习与符号推理相结合，并探讨了模糊推理和概率推理在神经符号学习中的应用，得到了一些令人惊讶的结果，如与乌鸦悖论的联系。 |
| [^13] | [Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes.](http://arxiv.org/abs/2401.10816) | 该研究通过使用基于图神经网络的推荐系统和来自可穿戴设备的健康行为数据，设计并实施了一个人工智能驱动平台，实现了个性化和情境引导，能够提高参与者的日常活动水平和中等至剧烈运动时长。 |
| [^14] | [Simulation Based Bayesian Optimization.](http://arxiv.org/abs/2401.10811) | 本文介绍了基于仿真的贝叶斯优化（SBBO）作为一种新方法，用于通过仅需基于采样的访问来优化获取函数。 |
| [^15] | [Neglected Hessian component explains mysteries in Sharpness regularization.](http://arxiv.org/abs/2401.10809) | 这篇论文研究了在深度学习中，明确或隐含地惩罚二阶信息可以提高泛化性能，而权重噪声和梯度惩罚则很少能带来这样的好处。作者通过对损失的黑塞矩阵的结构进行解释，提出了特征的开发和特征的探索之间的量化分离。同时，作者发现忽视的非线性建模误差矩阵 (NME) 实际上很重要，可以解释为什么梯度惩罚对激活函数的选择非常敏感。此外，作者通过设计干预措施来改进性能，并提供了证据挑战了以往的观点，认为权重噪声和梯度惩罚是等效的。 |
| [^16] | [Learning to Visually Connect Actions and their Effects.](http://arxiv.org/abs/2401.10805) | 该论文提出了视觉连接动作和其效果的概念（CATE），用于视频理解。研究表明，不同的任务形式产生了捕捉直观动作特性的表示，但模型表现不佳，人类的表现明显优于它们。该研究为未来的努力奠定了基础，并希望能激发出高级形式和模型的灵感。 |
| [^17] | [Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm.](http://arxiv.org/abs/2401.10800) | 本研究通过结合TAMS和Next-Generation Reservoir Computing技术，利用稀事件算法估计来源于数据的确定函数，来计算大西洋经度翻转环流（AMOC）在指定时间窗口内崩溃的概率。 |
| [^18] | [Novel Representation Learning Technique using Graphs for Performance Analytics.](http://arxiv.org/abs/2401.10799) | 该论文提出了一种使用图形来表示学习的新技术，该技术能够解决高性能计算中的性能分析问题。通过将表格性能数据转化为图形，并利用图神经网络技术来捕捉样本之间的复杂关系，实现了不依赖于特征工程和预处理步骤的迁移学习。 |
| [^19] | [Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health Monitoring Systems.](http://arxiv.org/abs/2401.10794) | 这篇论文提出了一个深度强化学习增强的活动感知动态健康监测系统，通过使用SlowFast模型实现精确监测和成本效益的平衡。 |
| [^20] | [Early alignment in two-layer networks training is a two-edged sword.](http://arxiv.org/abs/2401.10791) | 本文研究了两层网络训练中的早期对齐现象，发现在小初始化和一个隐藏的ReLU层网络中，神经元会在训练的早期阶段向关键方向进行对齐，导致网络稀疏表示以及梯度流在收敛时的隐含偏好。然而，这种稀疏诱导的对齐也使得训练目标的最小化变得困难。 |
| [^21] | [Measuring the Impact of Scene Level Objects on Object Detection: Towards Quantitative Explanations of Detection Decisions.](http://arxiv.org/abs/2401.10790) | 本研究提出了一种用于评估场景级对象对目标检测模型影响的解释性方法，通过比较模型在有和没有特定场景级对象的测试数据上的准确性，揭示了这些对象对模型性能的贡献。 |
| [^22] | [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.](http://arxiv.org/abs/2401.10774) | Medusa是一个能够提升LLM推理性能的简洁框架，通过增加多个解码头以实现并行预测多个后续标记，并通过树状注意力机制和并行处理来减少解码步骤。 |
| [^23] | [Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection.](http://arxiv.org/abs/2401.10765) | Starlit是一个新的可扩展隐私保护的联邦学习机制，解决了对于金融欺诈检测中的几个限制，包括缺乏正式的安全定义和证明、假定冻结账户、规模扩大、身份对齐阶段和难以抵抗客户端退出。 |
| [^24] | [Data Augmentation for Traffic Classification.](http://arxiv.org/abs/2401.10754) | 这项工作通过对交通分类任务中的数据增强进行分析和实验，发现时间序列顺序和掩码的增强在交通分类中更适用，同时提出了简单的潜在空间分析可以解释增强效果的思路。 |
| [^25] | [BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation.](http://arxiv.org/abs/2401.10753) | BoolGebra是一种方法，使用属性图学习和神经网络来改进布尔代数操作的逻辑综合。它通过减小搜索空间并高效定位优化空间，实现了跨设计推论的通用性和规模化潜力。 |
| [^26] | [ReliCD: A Reliable Cognitive Diagnosis Framework with Confidence Awareness.](http://arxiv.org/abs/2401.10749) | 该论文提出了一种可靠的认知诊断框架ReliCD，能够量化诊断反馈的信心，并适用于不同的认知诊断功能。 |
| [^27] | [Fast gradient-free activation maximization for neurons in spiking neural networks.](http://arxiv.org/abs/2401.10748) | 本论文提出了一个快速无梯度激活最大化的方法，用于探索神经网络中神经元的特化。在一个人工脉冲神经网络上成功测试了这个方法，并提供了一个有效的设计框架。 |
| [^28] | [Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach.](http://arxiv.org/abs/2401.10747) | 本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。 |
| [^29] | [A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding.](http://arxiv.org/abs/2401.10746) | 本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。 |
| [^30] | [Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models.](http://arxiv.org/abs/2401.10745) | 本文探讨了如何利用道德人工智能原则和指南来解决高级大型语言模型的治理和利用问题。 |
| [^31] | [Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response.](http://arxiv.org/abs/2401.10726) | 本研究通过优化聚合灵活性提供策略和评估HVAC系统的分散灵活性提供，为聚合器在可再生能源不确定性下实现需求响应提供了实用工具，从而实现了稳健的脱碳和增强能源系统的韧性。 |
| [^32] | [Real-Time Zero-Day Intrusion Detection System for Automotive Controller Area Network on FPGAs.](http://arxiv.org/abs/2401.10724) | 这篇论文介绍了一种基于无监督学习的卷积自编码器架构，用于实时检测零日攻击。它解决了汽车控制区域网络中零日攻击的检测问题，该问题变得尤为重要，因为随着主动注入攻击的复杂性不断增加，及时检测并防止传播成为了一项挑战。 |
| [^33] | [Generative Model for Constructing Reaction Path from Initial to Final States.](http://arxiv.org/abs/2401.10721) | 本文提出了一种利用神经网络生成反应路径初始猜测的创新方法，在有机反应的复杂反应路径中取得了良好的效果。 |
| [^34] | [Classification with neural networks with quadratic decision functions.](http://arxiv.org/abs/2401.10710) | 本文研究了使用具有二次决策函数的神经网络进行分类的方法，通过在MNIST数据集上测试和比较在手写数字分类和亚种分类上的表现，证明了其在紧凑基本几何形状识别方面的优势。 |
| [^35] | [Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model.](http://arxiv.org/abs/2401.10700) | 本论文提出了一种安全离线强化学习方法，通过可行性导向的扩散模型来平衡安全约束满足、奖励最大化和离线数据集的行为规范化。通过可达性分析，将硬安全约束转化为在离线数据集中识别最大可行区域。 |
| [^36] | [Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models.](http://arxiv.org/abs/2401.10690) | 本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。 |
| [^37] | [A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid FPGAs.](http://arxiv.org/abs/2401.10689) | 本论文研究了一种在混合FPGAs上部署的轻量级多攻击CAN入侵检测系统，利用量化的机器学习模型，能够检测并缓解CAN中的多个攻击向量。该系统消耗较低的功耗，避免了对GPU等专用计算单元的依赖。 |
| [^38] | [Manipulating Sparse Double Descent.](http://arxiv.org/abs/2401.10686) | 本文研究了两层神经网络中的双下降现象，探讨了L1正则化和表示维度的影响，并提出了稀疏双下降现象。它的发现有助于更好地理解神经网络的训练和优化。 |
| [^39] | [Towards End-to-End GPS Localization with Neural Pseudorange Correction.](http://arxiv.org/abs/2401.10685) | 本论文提出了一个端到端的GPS定位框架E2E-PrNet，通过直接训练神经网络PrNet来进行伪距修正，实验结果表明其优于现有端到端GPS定位方法。 |
| [^40] | [Deep Learning-based Embedded Intrusion Detection System for Automotive CAN.](http://arxiv.org/abs/2401.10674) | 本文提出了一种基于深度学习的嵌入式汽车CAN网络入侵检测系统，通过专用的硬件加速器实现了深度卷积神经网络入侵检测模型的透明集成，平均检测准确率超过99%。 |
| [^41] | [FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks.](http://arxiv.org/abs/2401.10657) | 本文通过对抗攻击展示了基因组学中人工智能模型的脆弱性，同时提出了一种基于输入转换的攻击方法和使用变分自动编码器模型生成有毒数据的增强方法。实验结果表明模型性能下降，准确率降低，假阳性和假阴性增加。 |
| [^42] | [Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech Detection.](http://arxiv.org/abs/2401.10653) | 这项研究基于Transformer框架和"关注融合"层，采用多模态方法识别仇恨言论，突破了传统的文本分析限制。 |
| [^43] | [AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference.](http://arxiv.org/abs/2401.10652) | AutoChunk是一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。 |
| [^44] | [Area Modeling using Stay Information for Large-Scale Users and Analysis for Influence of COVID-19.](http://arxiv.org/abs/2401.10648) | 本文提出了一种名为Area2Vec的新颖区域建模方法，通过模型化用户的位置数据对区域进行建模，从而更好地捕捉区域使用的变化。 |
| [^45] | [Empowering HWNs with Efficient Data Labeling: A Clustered Federated Semi-Supervised Learning Approach.](http://arxiv.org/abs/2401.10646) | 设计了一种聚类联邦半监督学习框架，用于更真实的硬件网络场景，克服了设备缺乏准确标签的问题，提高了收敛速度和处理时间。 |
| [^46] | [A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges.](http://arxiv.org/abs/2401.10643) | 本文通过全面调查了基于深度学习的车辆再识别方法，包括分类、数据集、评估标准以及未来的挑战和研究方向 |
| [^47] | [Towards Universal Unsupervised Anomaly Detection in Medical Imaging.](http://arxiv.org/abs/2401.10637) | 本文提出了一种无监督异常检测方法，通过创建伪健康重建实现对更广泛病变范围的检测，并在不同的成像模态中展示了卓越的性能。 |
| [^48] | [Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach.](http://arxiv.org/abs/2401.10632) | 本文提出了一个框架，用于在部分已知的真实因果图情况下实现因果公平性。通过使用部分有向无环图（PDAG）建模和衡量因果公平性，并通过受限优化问题平衡公平性和准确性。 |
| [^49] | [Polytopic Autoencoders with Smooth Clustering for Reduced-order Modelling of Flows.](http://arxiv.org/abs/2401.10620) | 我们提出了一种多角形自编码器架构，包括一个轻量级的非线性编码器、一个凸组合解码器和一个平滑聚类网络。该模型能够确保所有重建状态都位于一个多面体内，并提供了一个衡量多面体质量的指标。与传统的POD方法相比，我们的模型能够在保证重建误差可接受的情况下，使用最少的凸坐标进行多面体线性参数变化系统的建模。数值实验验证了我们模型的性能。 |
| [^50] | [ZnTrack -- Data as Code.](http://arxiv.org/abs/2401.10603) | ZnTrack是一个Python驱动的数据版本控制工具，通过简化大型数据集为Python脚本的形式，提供了一个以数据为代码的概念，实现了对参数跟踪、工作流设计以及数据存储和共享的用户友好界面。 |
| [^51] | [Adversarially Robust Signed Graph Contrastive Learning from Balance Augmentation.](http://arxiv.org/abs/2401.10590) | 本研究提出了一种名为BA-SGCL的鲁棒SGNN框架，通过结合图对比学习原则和平衡增强技术，解决了带符号图对抗性攻击中平衡相关信息不可逆的问题。 |
| [^52] | [PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks.](http://arxiv.org/abs/2401.10586) | PuriDefense是一种高效的防御机制，通过使用轻量级净化模型进行随机路径净化，减缓基于查询的攻击的收敛速度，并有效防御黑盒基于查询的攻击。 |
| [^53] | [Robust Multi-Modal Density Estimation.](http://arxiv.org/abs/2401.10566) | 本文提出了一种名为ROME的鲁棒多模态密度估计方法，该方法利用聚类将多模态样本集分割成多个单模态样本集，并通过简单的KDE估计来估计整体分布。这种方法解决了多模态、非正态和高相关分布估计的挑战。 |
| [^54] | [OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy.](http://arxiv.org/abs/2401.10559) | OrchMoE通过利用模块化技能架构和自动任务识别，提升了参数效率微调领域的性能，实现了对多任务学习的重大进展。 |
| [^55] | [Unified View Imputation and Feature Selection Learning for Incomplete Multi-view Data.](http://arxiv.org/abs/2401.10549) | 提出了一种新的多视图无监督特征选择方法UNIFIER，可以直接处理存在不完整多视图数据的情况，通过利用特征选择获取的局部结构信息引导补全过程，从而改善特征选择性能，并且充分利用了特征空间的局部性质。 |
| [^56] | [PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization.](http://arxiv.org/abs/2401.10547) | PhoGAD是一种基于图形的异常行为检测框架，通过持久同调优化来澄清行为边界，设计相邻边权重以减轻局部异质性的影响，并解决噪声问题。 |
| [^57] | [I-SplitEE: Image classification in Split Computing DNNs with Early Exits.](http://arxiv.org/abs/2401.10541) | 本文提出了一种创新的统一方法，将提前退出和分割计算相结合，以应对资源受限设备上的图像分类问题，并引入了I-SplitEE算法来适应不同的环境失真。 |
| [^58] | [The "Colonial Impulse" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases.](http://arxiv.org/abs/2401.10535) | 本研究审查了在孟加拉社群中经历殖民主义影响的情感分析工具，发现它们可能存在基于身份的偏见，并提出了警示。 |
| [^59] | [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.](http://arxiv.org/abs/2401.10529) | Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。 |
| [^60] | [FARe: Fault-Aware GNN Training on ReRAM-based PIM Accelerators.](http://arxiv.org/abs/2401.10522) | 本文提出了一种故障感知框架FARe，用于在基于ReRAM的PIM加速器上训练GNN。实验结果显示，FARe框架相较于其他方法在准确性和时间开销上都具有优势。 |
| [^61] | [Spatial-temporal Forecasting for Regions without Observations.](http://arxiv.org/abs/2401.10518) | 本文研究了在没有历史观测数据的区域进行时空预测的问题，并提出了一种名为STSM的模型，采用对比学习方法。 |
| [^62] | [Episodic Reinforcement Learning with Expanded State-reward Space.](http://arxiv.org/abs/2401.10516) | 通过引入扩展的状态-奖励空间，我们提出了一种高效的情节式强化学习（DRL）框架，可以改善DRL中状态与奖励空间之间的不对齐问题，从而提高值的估计准确性和策略性能。 |
| [^63] | [A match made in consistency heaven: when large language models meet evolutionary algorithms.](http://arxiv.org/abs/2401.10510) | 大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。 |
| [^64] | [Causal Layering via Conditional Entropy.](http://arxiv.org/abs/2401.10495) | 本文提出了一种通过条件熵Oracle来恢复图层析的方法，用于离散分布情况下的因果发现。算法通过比较节点的条件熵和噪声的无条件熵，通过删除源或汇点来实现节点的分离。算法具有可证明的正确性和二次时间复杂度。 |
| [^65] | [Generalization Error Guaranteed Auto-Encoder-Based Nonlinear Model Reduction for Operator Learning.](http://arxiv.org/abs/2401.10490) | 本文提出了一种基于Auto-Encoder的非线性模型降维算法，用于运算符学习，并且通过数值实验验证了其准确学习非线性偏微分方程的解算符的能力。 |
| [^66] | [Budgeted Online Model Selection and Fine-Tuning via Federated Learning.](http://arxiv.org/abs/2401.10478) | 本文提出了一种基于预算的在线模型选择和调优框架，通过联邦学习实现。该框架允许在内存受限的边缘设备上进行模型选择，并实现了客户端和服务器的合作来对模型进行适应非平稳环境的调优。 |
| [^67] | [LDReg: Local Dimensionality Regularized Self-Supervised Learning.](http://arxiv.org/abs/2401.10474) | 本文提出了一种叫做LDReg的本地维度正则化方法，用于解决自监督学习中的维度坍缩问题。通过增加局部内在维度，LDReg能够改善表示的性能。 |
| [^68] | [Learning Backdoors for Mixed Integer Programs with Contrastive Learning.](http://arxiv.org/abs/2401.10467) | 本论文提出了使用对比学习方法来学习混合整数规划的后门，通过收集用于训练的后门并训练图注意力网络模型来预测后门，取得了比Gurobi和先前模型更好的性能改进。 |
| [^69] | [Critical Data Size of Language Models from a Grokking Perspective.](http://arxiv.org/abs/2401.10463) | 本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。 |
| [^70] | [Ultra-lightweight Neural Differential DSP Vocoder For High Quality Speech Synthesis.](http://arxiv.org/abs/2401.10460) | 本文提出了一种超轻型差分DSP声码器，通过联合优化的声学模型和DSP声码器实现高质量的语音合成，而无需提取声道频谱特征。该模型在音频质量上接近神经声码器，且性能高效。 |
| [^71] | [Contrastive Unlearning: A Contrastive Approach to Machine Unlearning.](http://arxiv.org/abs/2401.10458) | 该论文提出了一种对比反学习的框架，通过对比反学习样本和剩余样本的嵌入，将反学习样本推离原始类别并拉向其他类别，从而有效地消除其影响，同时保持了剩余样本学习到的表示。实验证明，对比反学习在反学习效果和效率方面表现最佳。 |
| [^72] | [Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach.](http://arxiv.org/abs/2401.10451) | 本研究提出了一种学习辅助的贝叶斯优化方法，用于解决大规模容量扩展问题。通过构建和求解可行的时间聚合代理问题，识别出低成本的规划决策。通过在验证集和测试预测上评估解决的规划结果，实现了随机容量扩展问题的可行解决。 |
| [^73] | [Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition.](http://arxiv.org/abs/2401.10447) | 本研究研究了语音识别中低秩适应的训练策略和模型鲁棒性。通过引入不同的LoRA训练策略，实现了相对词错误率的降低，并研究了模型对输入扰动的稳定性。实验结果表明，高级LoRA变体导致了某些扰动的性能下降。 |
| [^74] | [Large Language Models are Efficient Learners of Noise-Robust Speech Recognition.](http://arxiv.org/abs/2401.10446) | 本文通过引入噪声信息作为条件器，并从N-best列表中提取语言空间噪声嵌入，教会了大型语言模型（LLMs）进行噪声去除，从而实现了噪声鲁棒语音识别的生成式错误纠正（GER）。 |
| [^75] | [Path Choice Matters for Clear Attribution in Path Methods.](http://arxiv.org/abs/2401.10442) | 本论文介绍了路径方法中路径选择对于清晰归因的重要性。通过引入“集中原则”，我们提出了一种模型无关的解释器SAMP，并使用无穷小约束和动量策略来提高严谨性和优化性。实验证明我们的方法可以精确揭示DNNs，且优于其他方法。 |
| [^76] | [A2Q+: Improving Accumulator-Aware Weight Quantization.](http://arxiv.org/abs/2401.10432) | A2Q+是一种改进的累加器感知权重量化方法，通过改进约束和初始化策略，实现了在保持模型准确性的同时提高硬件效率。 |
| [^77] | [M3BUNet: Mobile Mean Max UNet for Pancreas Segmentation on CT-Scans.](http://arxiv.org/abs/2401.10419) | M3BUNet是MobileNet和U-Net神经网络的融合，使用新颖的MM注意力机制，在两个阶段逐步细分胰腺CT图像，并使用掩码指导进行目标检测，从而提高了分割性能。 |
| [^78] | [Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation.](http://arxiv.org/abs/2401.10405) | 本研究旨在评估差分隐私和对抗性鲁棒机器学习的效果。通过将对抗训练和差分隐私训练相结合来应对同时攻击，实证结果表明该方法在性能上优于其他方法，并且在隐私保证方面与非鲁棒私有模型相当。这项研究还强调了对动态训练范式中隐私保证的需求。 |
| [^79] | [Deep Dict: Deep Learning-based Lossy Time Series Compressor for IoT Data.](http://arxiv.org/abs/2401.10396) | 提出了一种深度学习-based的有损时间序列压缩器Deep Dict，通过引入伯努利变换自编码器（BTAE）和失真约束的方式，实现了高压缩比和保持预定范围内的解压缩误差，并通过引入量化熵损失（QEL）来提高鲁棒性。与最先进的有损压缩器相比，在多个时间序列数据集上表现更好。 |
| [^80] | [Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels.](http://arxiv.org/abs/2401.10394) | 本文提出了一种基于分布一致性的自训练方法，用于解决图神经网络中少样本节点分类的挑战，通过消除训练集和测试集之间的分布转移，提高自训练的有效性。 |
| [^81] | [Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments.](http://arxiv.org/abs/2401.10393) | 本研究在自然学习环境中通过回忆方法减轻了灾难性干扰，该方法受到功率法则的启发。 |
| [^82] | [Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning.](http://arxiv.org/abs/2401.10386) | 本研究提出了一种基于随机森林机器学习的非侵入性诊断急性间室综合征的方法。使用压力传感电阻器检测肌肉间室压力，并通过蓝牙传输结果到Web应用程序。该诊断方法在准确率、灵敏度和F1得分等关键性能指标方面表现出色。 |
| [^83] | [Approximation of Solution Operators for High-dimensional PDEs.](http://arxiv.org/abs/2401.10385) | 提出了一种有限维度控制为基础的方法来近似高维PDE的解算符，通过使用深度神经网络等降阶模型和神经常微分方程的计算技术，在一般的二阶非线性PDE类中证明了近似精度的合理性，并通过数值结果验证了所提方法的准确性和效率。 |
| [^84] | [Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis.](http://arxiv.org/abs/2401.10383) | 本文提出了一种解决多智能体图形赌博机问题的算法Multi-G-UCB，并通过数值实验验证了其有效性。 |
| [^85] | [Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats.](http://arxiv.org/abs/2401.10375) | 本文研究基于基础模型集成的联邦学习在敌对威胁下的漏洞，提出了一种新的攻击策略，揭示了该模型在不同配置的联邦学习下对敌对威胁的高敏感性。 |
| [^86] | [Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation.](http://arxiv.org/abs/2401.10373) | 本文提出了一种鲁棒且具普适性的医学图像分割方法，通过协调空间和光谱表示，引入光谱相关系数目标来提高对中阶特征和上下文长程依赖的捕捉能力，从而显著增强了泛化能力。 |
| [^87] | [Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning.](http://arxiv.org/abs/2401.10371) | Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。 |
| [^88] | [Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review.](http://arxiv.org/abs/2401.10370) | 本文比较了深度生成建模在金融时间序列中的应用，提出了新的条件时间序列生成方法，并介绍了在VaR预测中的使用。 |
| [^89] | [Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations.](http://arxiv.org/abs/2401.10364) | 本文讨论了使用LLM进行代码生成的可行性，并特别应用于设计RISC处理器。经实验证实，LLM生成的代码存在显著错误，需要人为干预来修复。LLM可用于辅助程序员的代码设计。 |
| [^90] | [Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs.](http://arxiv.org/abs/2401.10361) | 本文介绍了一种在多跳基于集群的车联网中的分层联邦学习（HFL）框架，该框架利用平均相对速度和余弦相似度对FL模型参数进行加权组合以作为聚类度量，以解决数据多样性和高车辆移动性带来的挑战。 |
| [^91] | [Excuse me, sir? Your language model is leaking (information).](http://arxiv.org/abs/2401.10360) | 这项研究介绍了一种加密方法，可以在大型语言模型的响应中隐藏秘密载荷，且不影响生成文本的质量。 |
| [^92] | [Intelligent Optimization and Machine Learning Algorithms for Structural Anomaly Detection using Seismic Signals.](http://arxiv.org/abs/2401.10355) | 本研究使用智能优化技术和机器学习算法，通过比较结构振动的实验测量值和数值模拟，实现了对简单结构中异常情况的检测。 |
| [^93] | [Towards providing reliable job completion time predictions using PCS.](http://arxiv.org/abs/2401.10354) | 本文提出了PCS调度框架，旨在提供可靠的任务完成时间预测。PCS使用加权公平队列调度，并通过模拟辅助的搜索策略来寻找满足特定目标的配置。通过在DNN作业调度上的实施和评估，PCS展示了其在提供可预测性方面的潜力。 |
| [^94] | [MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online Anomaly Detection with Multivariate Time Series.](http://arxiv.org/abs/2401.10338) | MELODY是一个半监督混合模型，用于基于实体级别的在线异常检测，解决了部署的异构性、低延迟容忍度、模糊的异常定义和有限的监督等挑战。 |
| [^95] | [Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition.](http://arxiv.org/abs/2401.10337) | 该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。 |
| [^96] | [DrugAssist: A Large Language Model for Molecule Optimization.](http://arxiv.org/abs/2401.10334) | DrugAssist是一个交互式分子优化模型，通过人机对话实现优化，利用LLM的强交互性和泛化能力，在药物发现中取得了领先的结果。 |
| [^97] | [Improving One-class Recommendation with Multi-tasking on Various Preference Intensities.](http://arxiv.org/abs/2401.10316) | 本研究提出了一个多任务框架，考虑了隐式反馈中不同偏好强度的情况，并在其中引入了注意力图卷积层来探索高阶关系。这使得表示更具鲁棒性和泛化性。 |
| [^98] | [LangProp: A code optimization framework using Language Models applied to driving.](http://arxiv.org/abs/2401.10314) | LangProp是一种用于自动驾驶的代码优化框架，利用语言模型迭代优化生成的代码。它通过评估代码性能和捕捉异常来改进生成的代码，展示了在CARLA中实现自动驾驶的概念验证。 |
| [^99] | [Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to Identify Trajectory Prediction Vulnerabilities for Autonomous Driving Security.](http://arxiv.org/abs/2401.10313) | 本文通过对两个轨迹预测模型进行敏感性分析，发现尽管图像地图对于这两个模型的预测输出可能只有轻微的贡献，但使用快速梯度符号法制作的不可检测的图像地图扰动可以导致预测误差大幅增加，从而破坏自动驾驶系统的轨迹预测性能。 |
| [^100] | [Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement.](http://arxiv.org/abs/2401.10310) | 这篇论文探讨了在社会和司法约束下，对深度学习进行数学算法设计的挑战。研究者提出了算法透明性的要求，并使用数学框架来分析在计算模型中实现透明实施的可行性。 |
| [^101] | [Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations.](http://arxiv.org/abs/2401.10306) | 本研究提出了一种物理约束卷积神经网络（PC-CNN），用于解决非线性且时空变化的偏微分方程中的两种反问题。该网络可以揭示受偏差影响的真实状态，并在给定稀疏信息的情况下以高分辨率重建解。 |
| [^102] | [Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach.](http://arxiv.org/abs/2401.10305) | 该研究通过手机传感器收集的活动数据可靠地预测了个性特征，这些研究成果为社会科学研究提供了新的途径。通过使用智能手机传感和机器学习技术，可以以成本效益高、无问卷调查的方式对个性相关问题进行研究。这些发现有助于推动个性研究，并且可以为从业者和研究人员提供信息。 |
| [^103] | [On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning.](http://arxiv.org/abs/2401.10304) | 本研究分析了科学数据文档如何满足机器学习社区和监管机构对其在机器学习技术中使用的需求，并提出了一套建议指南。 |
| [^104] | [A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems.](http://arxiv.org/abs/2401.10300) | 本研究提出了一个分层框架，通过学习系统和代理的表示，使用时空一致性学习来捕捉复杂适应性系统中的现象，并解决了现有方法不能捕捉空间模式和建模非线性关系的问题。 |
| [^105] | [An attempt to generate new bridge types from latent space of generative flow.](http://arxiv.org/abs/2401.10299) | 本研究通过归一化流生成潜在空间中的新桥梁类型，并解决了高维矩阵行列式计算和神经网络可逆变换的挑战。 |
| [^106] | [Machine learning approach to detect dynamical states from recurrence measures.](http://arxiv.org/abs/2401.10298) | 这项研究提出了一种将机器学习方法与非线性时间序列分析相结合的方法，通过循环测量来对不同的动力学状态进行分类，发现衡量循环点密度的特征是最相关的，且训练后的算法能够成功预测动力学状态。 |
| [^107] | [Learning Non-myopic Power Allocation in Constrained Scenarios.](http://arxiv.org/abs/2401.10297) | 我们提出了一个学习非远见功率分配的框架，用于在临时约束下的干扰网络中进行高效的功率分配。我们采用了演员-评论家算法来解决这个受约束的序贯决策问题。 |
| [^108] | [Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of Gaussians Mechanisms.](http://arxiv.org/abs/2401.10294) | 本研究提供了一种计算DP-SGD组级别限制的方法，并且证明了这个方法在使用泊松抽样或固定批量大小抽样时是紧密的。 |
| [^109] | [Symmetry breaking in geometric quantum machine learning in the presence of noise.](http://arxiv.org/abs/2401.10293) | 本研究研究了噪声存在下EQNN模型的行为，并展示了对称性破缺随着层数和噪声强度呈线性增加的现象。同时，我们提供了增强EQNN模型对称性保护的策略。 |
| [^110] | [Early Prediction of Geomagnetic Storms by Machine Learning Algorithms.](http://arxiv.org/abs/2401.10290) | 本研究旨在利用大数据和机器学习算法可靠地尽早预测所有类型的地磁暴。通过融合全球多个地面站收集的关于太阳测量的不同方面的大量数据，并使用带有特征选择和数据降采样的随机森林回归方法对较小的地磁暴实例进行处理，我们实现了82.55%的准确度。 |
| [^111] | [Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics.](http://arxiv.org/abs/2401.10289) | 本论文设计了一种光电神经处理器，用于模拟经图像检测训练过的生物神经网络在混合机器人中的应用。通过光遗传学实现精确激活的反向传播STDP算法，实现了与传统神经网络训练算法相媲美的准确度。 |
| [^112] | [CLAN: A Contrastive Learning based Novelty Detection Framework for Human Activity Recognition.](http://arxiv.org/abs/2401.10288) | CLAN是一种基于对比学习的新颖性检测框架，用于处理人体活动识别中的挑战，并构建对挑战具有不变性的已知活动的表示方法。 |
| [^113] | [Open-Source Fermionic Neural Networks with Ionic Charge Initialization.](http://arxiv.org/abs/2401.10287) | 本文将FermiNet模型集成到开源库DeepChem中，并提出了新的初始化技术，用于解决离子的电子分配问题。 |
| [^114] | [Analyzing Brain Activity During Learning Tasks with EEG and Machine Learning.](http://arxiv.org/abs/2401.10285) | 本研究利用EEG和机器学习技术，分析了STEM活动中的脑活动，研究结果表明不同任务的分类可行性。研究发现右额叶在数学处理和规划中表现出色，左额叶在认知灵活性和心理灵活性方面表现出色，左颞顶叶在连接方面表现出色。通过研究脑活动与学习任务之间的关系，有助于对脑活动和学习的理解更深入。 |
| [^115] | [MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems.](http://arxiv.org/abs/2401.10284) | MorpheusNet是一个资源效率的睡眠阶段分类器，可以在嵌入式系统上实时预测睡眠阶段，适用于边缘计算，消耗少量能源。 |
| [^116] | [Window Stacking Meta-Models for Clinical EEG Classification.](http://arxiv.org/abs/2401.10283) | 本论文提出了一种窗口堆叠的元模型，解决了在临床脑电图分类中使用窗口化技术时的计算开销和标签不准确的问题。通过测试，我们的方法将基准准确率从89.8%提升到99.0%，为临床应用提供了突破性的性能。 |
| [^117] | [BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis.](http://arxiv.org/abs/2401.10282) | BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。 |
| [^118] | [EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model.](http://arxiv.org/abs/2401.10278) | 本论文提出了一种名为EEGFormer的脑电基础模型，通过在大规模的复合EEG数据上进行预训练，该模型能够学习到可迁移和可解释的通用表示，为脑电信号的分析提供了有力的工具。 |
| [^119] | [Migrating Birds Optimization-Based Feature Selection for Text Classification.](http://arxiv.org/abs/2401.10270) | 本文提出了一种基于迁徙鸟优化的特征选择方法，通过结合朴素贝叶斯作为内部分类器，解决了文本分类中特征选择的挑战。实验结果表明，与其他方法相比，该方法在特征减少和分类准确性方面具有明显优势。研究还提供了有关增强特征选择方法的宝贵见解，为文本分类提供了可扩展和有效的解决方案。 |
| [^120] | [Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies.](http://arxiv.org/abs/2401.10266) | 本论文综述了工业厂房智能状态监测和故障检测和诊断方法，重点关注了Tennessee Eastman Process。调研总结了最流行和最先进的深度学习和机器学习算法，并探讨了算法的优劣势。还讨论了不平衡数据和无标记样本等挑战，以及深度学习模型如何应对。比较了不同算法在Tennessee Eastman Process上的准确性和规格。 |
| [^121] | [The Best Time for an Update: Risk-Sensitive Minimization of Age-Based Metrics.](http://arxiv.org/abs/2401.10265) | 该论文研究了基于风险敏感的年龄度量最小化问题，提出了一种新的风险状态概念和风险度量方法，并介绍了两种风险敏感策略。 |
| [^122] | [Null Space Properties of Neural Networks with Applications to Image Steganography.](http://arxiv.org/abs/2401.10262) | 本文研究了神经网络的零空间特性，发现了神经网络的固有弱点，并针对此弱点提出了一种图像隐写术方法。 |
| [^123] | [Curriculum Design Helps Spiking Neural Networks to Classify Time Series.](http://arxiv.org/abs/2401.10257) | 本文研究了脉冲神经网络（SNNs）上课程学习（Curriculum Learning）的作用，通过设计一种名为CSNN的新方法，在学习顺序和神经元活动编码方面模拟了人类学习的过程。实验证明，这种方法在多个时间序列数据上取得了优秀的分类结果。 |
| [^124] | [Nowcasting Madagascar's real GDP using machine learning algorithms.](http://arxiv.org/abs/2401.10255) | 该研究基于马达加斯加的宏观经济领先指标，使用不同的机器学习算法进行国内生产总值（GDP）预测，并发现集成模型相对于传统计量经济模型具有更高的准确性和实时性。 |
| [^125] | [Beyond the Frame: Single and mutilple video summarization method with user-defined length.](http://arxiv.org/abs/2401.10254) | 本文研究了使用多种NLP技术和视频处理技术将长视频转换为摘要视频的方法，以减少观看/回顾视频所需的时间。 |
| [^126] | [Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation.](http://arxiv.org/abs/2401.10253) | 本文提出了一种基于图神经网络的混合任务元学习算法，用于可扩展和可转移的带宽分配。通过引入GNN和HML算法，该方法在不同的通信场景下具有较好的性能和采样效率。 |
| [^127] | [Resolution Chromatography of Diffusion Models.](http://arxiv.org/abs/2401.10247) | 本文介绍了分辨率色谱层析，用于解释生成过程中的粗糙到细致行为和设计时间依赖的调制。通过实验证明了该理论，并提出了一些应用方法。 |
| [^128] | [Zero Bubble Pipeline Parallelism.](http://arxiv.org/abs/2401.10241) | 本论文介绍了一种新的调度策略，在同步训练语义下成功实现了零管道泡沫，通过将反向计算分为两部分，设计了优于基线方法的新颖管道调度。此外，还提出了一种自动找到最优调度的算法，并引入新颖技术绕过同步操作实现零泡沫。实验证明，在相似条件下，本方法在吞吐量方面优于1F1B调度23%。 |
| [^129] | [Interplay between Cryptocurrency Transactions and Online Financial Forums.](http://arxiv.org/abs/2401.10238) | 本研究关注加密货币论坛与加密货币价值波动之间的相互作用，发现Bitcointalk论坛的活动与比特币的趋势有直接关系。 |
| [^130] | [Divide and not forget: Ensemble of selectively trained experts in Continual Learning.](http://arxiv.org/abs/2401.10191) | 连续学习中，我们提出了一种名为SEED的新方法，通过选择性训练最优的专家来解决遗忘和计算负担的问题，并在实验中展示了其高性能。 |
| [^131] | [Interplay between depth and width for interpolation in neural ODEs.](http://arxiv.org/abs/2401.09902) | 本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\varepsilon$的关系有关。 |
| [^132] | [A Fast, Performant, Secure Distributed Training Framework For Large Language Model.](http://arxiv.org/abs/2401.09796) | 本文提出了一种基于模型切片的安全分布式语言模型训练框架。通过在客户端和服务器端部署可信执行环境（TEE）并使用轻量级加密进行安全通信，解决了恶意窃取模型参数和数据的问题。此外，采用分割微调和稀疏化参数微调的方法，在降低设备成本的同时提高了模型性能和准确性。 |
| [^133] | [Imitation Learning Inputting Image Feature to Each Layer of Neural Network.](http://arxiv.org/abs/2401.09691) | 本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。 |
| [^134] | [CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder.](http://arxiv.org/abs/2401.08897) | CFASL是一种用于解缠学习的新方法，它将对称性学习与VAE集成，无需任何数据集因子信息的先验知识，具有三个新特征：对齐潜在向量维度到可学习对称代码簿中的对称性，学习复合对称性来表达未知因素的变化，以及引入群等变编码器和解码器来训练VAE。 |
| [^135] | [Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning.](http://arxiv.org/abs/2401.08216) | 本文提出了一种高效且可验证地从恶意客户端的毒化攻击中恢复联邦学习模型的方法，通过选择性利用历史信息以及未受恶意客户端影响的历史模型，能够在恶意客户端被识别后实现准确的全局模型恢复。 |
| [^136] | [Statistical Test for Attention Map in Vision Transformer.](http://arxiv.org/abs/2401.08169) | 本研究提出了一种Vision Transformer中注意力图的统计检验方法，可以将注意力作为可靠的定量证据指标用于决策，并通过p值进行统计显著性量化。 |
| [^137] | [Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs.](http://arxiv.org/abs/2401.07961) | 这项研究将概率Lambert问题与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程等领域连接起来，从而解决了概率Lambert问题的解的存在和唯一性，并提供了数值求解的方法。 |
| [^138] | [Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks.](http://arxiv.org/abs/2401.07494) | 通过结合输入凸性和Lipschitz连续性的优势，我们开发了一种名为输入凸性Lipschitz循环神经网络的新型网络结构，在计算效率和对抗鲁棒性方面优于现有的循环单元，并适用于多种工程任务。 |
| [^139] | [Deep Efficient Private Neighbor Generation for Subgraph Federated Learning.](http://arxiv.org/abs/2401.04336) | 本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。 |
| [^140] | [Diffusion Model with Perceptual Loss.](http://arxiv.org/abs/2401.00110) | 本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。 |
| [^141] | [Privacy-Preserving Neural Graph Databases.](http://arxiv.org/abs/2312.15591) | 隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。 |
| [^142] | [Meta-Learning with Versatile Loss Geometries for Fast Adaptation Using Mirror Descent.](http://arxiv.org/abs/2312.13486) | 本论文提出了一种使用镜像下降的多功能损失几何元的元学习方法，通过学习非线性镜像映射来快速适应提取的先验知识，以便在少量优化步骤内训练特定任务的模型。在少样本学习数据集上的实验中表明，该方法具有更强的表达能力和收敛性。 |
| [^143] | [Pre-training of Molecular GNNs via Conditional Boltzmann Generator.](http://arxiv.org/abs/2312.13110) | 本研究提出了一种针对分子GNN的预训练方法，使用现有的分子构象数据集生成对多个构象通用的潜变量，以解决获取多个构象的计算成本问题。 |
| [^144] | [Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective.](http://arxiv.org/abs/2312.10401) | 本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。 |
| [^145] | [Let's do the time-warp-attend: Learning topological invariants of dynamical systems.](http://arxiv.org/abs/2312.09234) | 该论文提出了一个数据驱动、基于物理信息的深度学习框架，用于分类和表征动力学变化的拓扑不变特征提取，特别关注超临界霍普分歧。这个方法可以帮助预测系统的质变和常发行为变化。 |
| [^146] | [EZ-CLIP: Efficient Zeroshot Video Action Recognition.](http://arxiv.org/abs/2312.08010) | EZ-CLIP是一个简单高效的CLIP改进方案，通过引入时间上的视觉提示来解决视频领域中的挑战，并保持了CLIP的泛化能力。 |
| [^147] | [Neural Spectral Methods: Self-supervised learning in the spectral domain.](http://arxiv.org/abs/2312.05225) | 神经谱方法将经典谱方法与机器学习相结合，通过谱损失实现更高效的求导，大大降低了训练复杂度，并在速度和准确性方面显著超过之前的机器学习方法。 |
| [^148] | [A ripple in time: a discontinuity in American history.](http://arxiv.org/abs/2312.01185) | 该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。 |
| [^149] | [Predicting breast cancer with AI for individual risk-adjusted MRI screening and early detection.](http://arxiv.org/abs/2312.00067) | 本研究使用人工智能算法，基于MRI图像预测患者一年内是否会发展为乳腺癌，以减轻筛查负担并实现早期检测。 |
| [^150] | [Convergence Analysis of Fractional Gradient Descent.](http://arxiv.org/abs/2311.18426) | 本论文通过分析不同环境下的分数梯度下降方法，建立了分数导数与整数导数之间的新界限，并证明了在平滑且凸、平滑且强凸以及平滑且非凸环境下的收敛性，为分数梯度下降的收敛性分析提供了新的理论支持。 |
| [^151] | [Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision.](http://arxiv.org/abs/2311.15497) | 本研究提出了一种自适应图像配准的混合方法，将深度学习和优化函数相结合，通过使用学习方法的输出作为优化的初始参数，并在计算上重点处理损失最大的图像对，取得了1.6%的性能提升和1.0%的变形场平滑度提升。 |
| [^152] | [A Survey of Graph Meets Large Language Model: Progress and Future Directions.](http://arxiv.org/abs/2311.12399) | 本综述对将大型语言模型(LLMs)与图结合的现有方法进行了全面的回顾和分析，提出了一个新的分类法，并讨论了未来研究的有希望的方向。 |
| [^153] | [LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly Detector.](http://arxiv.org/abs/2311.11809) | LogLead是一种快速且集成的日志加载器、增强器和异常检测器，可用于高效的日志分析基准测试。通过使用不同的数据集、日志表示方法和异常检测器，LogLead方便了日志分析研究中的全面基准测试。与过去的解决方案相比，LogLead在日志加载速度和Drain解析速度上都取得了显著的提升。 |
| [^154] | [Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control.](http://arxiv.org/abs/2311.07202) | 本研究提出了一种基于输入凸LSTM的基于Lyapunov的模型预测控制方法，通过减少收敛时间和缓解梯度消失/爆炸问题来改善MPC的性能。 |
| [^155] | [A Foundation Graph Model.](http://arxiv.org/abs/2311.03976) | 本文提出了一个基于对抗性对比学习的基础图模型FoToM，该模型通过节点和边特征排除进行图预训练，在多个领域上实现了正向迁移，并取得了显著的性能提升。 |
| [^156] | [Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing.](http://arxiv.org/abs/2310.13384) | 本文介绍了一种名为“盐化DNN”的方法，通过在推理时让客户控制DNN输出的语义解释，同时保持准确性和效率与标准DNN几乎一致，提升了移动计算中隐私和计算效率的问题。 |
| [^157] | [Towards Robust Offline Reinforcement Learning under Diverse Data Corruption.](http://arxiv.org/abs/2310.12955) | 本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。 |
| [^158] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^159] | [MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks.](http://arxiv.org/abs/2310.04965) | 该论文提出了一个新的基准挑战MultiScript，旨在解决现有脚本学习方法对于开放领域日常任务的限制。论文介绍了两个多模式脚本学习任务，并提供了对应的输入和输出要求。 |
| [^160] | [BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph.](http://arxiv.org/abs/2310.03320) | BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。 |
| [^161] | [A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling.](http://arxiv.org/abs/2310.03298) | 提出了一种基于潜变量的方法，用于非层次化多保真度自适应采样。该方法能够利用不同保真度模型之间的相关性以更高效地探索和利用设计空间。 |
| [^162] | [Unified Uncertainty Calibration.](http://arxiv.org/abs/2310.01202) | 该论文提出了一种统一的不确定性校准（U2C）框架，用于合并可知和认知不确定性，实现了面对困难样例时的准确预测和校准。 |
| [^163] | [LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models.](http://arxiv.org/abs/2309.14393) | 本研究提出了LLMCarbon，一个针对密集型和MoE LLMs设计的端到端碳足迹预测模型，解决了现有工具的限制，并显著提升了估计的准确性。 |
| [^164] | [Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation.](http://arxiv.org/abs/2309.11623) | 本研究利用自我注意力机制和负面反馈，提出了用于顺序音乐推荐的Transformer模型，并采用对比学习任务来提高推荐准确性。 |
| [^165] | [Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition.](http://arxiv.org/abs/2309.07988) | 本论文提出了一种名为折叠注意力的技术，在基于Transformer的流式语音识别模型中，通过减少线性投影层的数量，显著减小了模型大小，提高了内存和功耗效率，实验证明可以将模型大小减小24%、功耗减小23%。 |
| [^166] | [Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks.](http://arxiv.org/abs/2309.04452) | 本研究使用置换不变神经网络对集合天气预测进行后处理，不同于之前的方法，该网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。在地表温度和风速预测的案例研究中，我们展示了最先进的预测质量。 |
| [^167] | [IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency.](http://arxiv.org/abs/2308.12871) | 提出了一种名为IPA的在线深度学习推理管道自适应系统，通过动态配置批处理大小、复制和模型变体，以优化准确性、最小化成本并满足用户定义的延迟要求。 |
| [^168] | [TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures.](http://arxiv.org/abs/2306.17248) | TemperatureGAN是一个生成对抗网络，使用地面以上2m的大气温度数据，能够生成具有良好空间表示和与昼夜周期一致的时间动态的高保真样本。 |
| [^169] | [Interpreting Deep Neural Networks with the Package innsight.](http://arxiv.org/abs/2306.10822) | innsight是一个通用的R包，能够独立于深度学习库，解释来自任何R包的模型，并提供了丰富的可视化工具，以揭示深度神经网络预测的变量解释。 |
| [^170] | [Optimal Sets and Solution Paths of ReLU Networks.](http://arxiv.org/abs/2306.00119) | 本研究开发了一个分析框架，通过凸规划表征所有ReLU网络的最优集合和解路径，并提供了最小网络的最优剪枝算法，建立了ReLU网络正则化路径连续的条件，并为最小ReLU网络提供了灵敏度结果。 |
| [^171] | [Improving Speech Emotion Recognition Performance using Differentiable Architecture Search.](http://arxiv.org/abs/2305.14402) | 该论文提出使用DARTS优化联合CNN和LSTM的体系结构以提高语音情绪识别性能，并在实验中证明了其优于以往最好的结果。 |
| [^172] | [Explaining dark matter halo density profiles with neural networks.](http://arxiv.org/abs/2305.03077) | 使用可解释的神经网络将暗物质晕的演化历史与其密度分布相连接，网络发现超过渗透半径的轮廓由一个单一参数描述，这展示了在复杂的天体物理数据集中机器协助科学发现的潜力。 |
| [^173] | [Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method.](http://arxiv.org/abs/2304.11171) | 本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。 |
| [^174] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^175] | [Utilizing synthetic training data for the supervised classification of rat ultrasonic vocalizations.](http://arxiv.org/abs/2303.03183) | 利用合成训练数据可以提高大鼠超声波声音的监督分类效果。 |
| [^176] | [$\alpha$-divergence Improves the Entropy Production Estimation via Machine Learning.](http://arxiv.org/abs/2303.02901) | 本研究通过机器学习提出了一种基于$\alpha$-散度的损失函数，在估计随机熵产生时表现出更加稳健的性能，尤其在强非平衡驱动或者缓慢动力学的情况下。选择$\alpha=-0.5$能获得最优结果。 |
| [^177] | [Prismer: A Vision-Language Model with An Ensemble of Experts.](http://arxiv.org/abs/2303.02506) | Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。 |
| [^178] | [A Deep Neural Network Based Reverse Radio Spectrogram Search Algorithm.](http://arxiv.org/abs/2302.13854) | 本论文提出了一种基于深度神经网络的反向无线电频谱搜索算法，用于在射电数据中快速找到类似感兴趣信号。算法通过训练一个B-Variational Autoencoder以及使用位置嵌入层和附加元数据来实现。这种搜索算法能够减轻繁重的手动信号审查工作。 |
| [^179] | [Knowledge from Large-Scale Protein Contact Prediction Models Can Be Transferred to the Data-Scarce RNA Contact Prediction Task.](http://arxiv.org/abs/2302.06120) | 本文发现基于蛋白质共进化的Transformer深度神经网络学习的知识可以转移到RNA接触预测任务中，显著提高数据稀缺的RNA接触预测的准确性。 |
| [^180] | [Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences.](http://arxiv.org/abs/2212.09726) | 通过控制无关句子的混淆效应，本文提出了一种改进抽象摘要准确性的方法，并在AnswerSumm数据集上实现了20\%的准确性提升。 |
| [^181] | [Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks.](http://arxiv.org/abs/2212.01521) | 本文提出了一种用于解决生成对抗网络中模式崩溃问题的分布拟合方法。通过全局分布拟合和局部分布拟合，可以限制生成的数据分布，从而提高生成对抗网络的性能。 |
| [^182] | [Are you using test log-likelihood correctly?.](http://arxiv.org/abs/2212.00219) | 使用测试对数似然进行比较可能与其他指标相矛盾，并且高测试对数似然不意味着更准确的后验近似。 |
| [^183] | [Choreographer: Learning and Adapting Skills in Imagination.](http://arxiv.org/abs/2211.13350) | 本论文提出了一种建模师代理，利用世界模型在想象中学习并适应技能。该方法通过解耦探索和技能学习过程，能够在模型的潜在状态空间中发现技能，并通过元控制器进行高效的适应。建模师能够从离线数据和同时收集数据中学习技能。 |
| [^184] | [Orthogonal Non-negative Matrix Factorization: a Maximum-Entropy-Principle Approach.](http://arxiv.org/abs/2210.02672) | 本文提出了一种新的解决正交非负矩阵分解问题的方法，该方法使用了基于最大熵原则的解决方案，并保证了矩阵的正交性和稀疏性以及非负性。该方法在不影响近似质量的情况下具有较好的性能速度和优于文献中类似方法的稀疏性、正交性。 |
| [^185] | [Algorithmic Assistance with Recommendation-Dependent Preferences.](http://arxiv.org/abs/2208.07626) | 本研究提出了一个联合人机决策的委托代理模型，探讨了算法推荐对选择的影响和设计，特别关注算法对偏好的改变，以解决算法辅助可能带来的意外后果。 |
| [^186] | [Hybrid Models for Mixed Variables in Bayesian Optimization.](http://arxiv.org/abs/2206.01409) | 本文提出了一种新型的混合模型，用于混合变量贝叶斯优化，并且在搜索和代理模型阶段都具有创新之处。数值实验证明了混合模型的优越性。 |
| [^187] | [Group-level Brain Decoding with Deep Learning.](http://arxiv.org/abs/2205.14102) | 本文提出了一种深度学习方法，利用主体嵌入技术解决个体差异带来的群体级脑解码问题，并在脑磁图像数据上进行了验证。 |
| [^188] | [Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections.](http://arxiv.org/abs/2205.05359) | 本文介绍了一种使用动态线性投影方法来分析流行的非线性模型的局部解释的方法，探索预测变量之间的交互如何影响变量重要性估计，这对于理解模型的可解释性非常有用。 |
| [^189] | [How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network.](http://arxiv.org/abs/2203.16031) | 本文通过实验调查了一种先进的深度卷积神经网络在识别现代概念艺术作品时的能力，结果表明该模型主要使用形状和颜色等展示特征进行分类，而忽略了历史背景和艺术家意图等非展示特征。 |
| [^190] | [Decompositional Quantum Graph Neural Network.](http://arxiv.org/abs/2201.05158) | 本论文提出了一种新型的基于量子计算的神经网络，能够处理基于图的数据，通过使用自我图和张量积降低模型参数数量，提出一种从现实世界数据到希尔伯特空间的新型映射方式。 |
| [^191] | [Active Restoration of Lost Audio Signals Using Machine Learning and Latent Information.](http://arxiv.org/abs/2111.10891) | 本论文提出了一种使用隐写术、抖动和最先进的浅层学习和深度学习方法相结合的方式来主动恢复丢失的音频信号。通过评估不同方法的结果，观察显示所提出的解决方案在通过隐写术提供的边缘信息的帮助下能够有效增强音频信号的重建。 |
| [^192] | [Few-shot Quality-Diversity Optimization.](http://arxiv.org/abs/2109.06826) | 本文提出了一种几乎零样本的质量多样性优化方法，通过利用参数空间中的优化路径信息构建先验种群，可以在未见环境中进行少样本适应。 |
| [^193] | [Efficient Attention: Attention with Linear Complexities.](http://arxiv.org/abs/1812.01243) | 本文提出了一种高效注意力机制，可以在大幅减少内存和计算成本的情况下实现与传统点积注意力等效的效果。这种高效机制的应用使得注意力模块可以更广泛地集成到网络中，从而提高准确性，并且在物体检测和实例分割等任务中取得了显著的性能提升。 |

# 详细

[^1]: SCENES: 使用极线监督的亚像素对应关系估计

    SCENES: Subpixel Correspondence Estimation With Epipolar Supervision. (arXiv:2401.10886v1 [cs.CV])

    [http://arxiv.org/abs/2401.10886](http://arxiv.org/abs/2401.10886)

    SCENES提出了一种使用极线监督的亚像素对应关系估计方法，通过仅使用相机姿态信息而不需要3D结构，以放宽对数据集的特征要求。

    

    从场景的两个或更多视角中提取点对应关系是一个基础的计算机视觉问题，特别重要的是相对相机姿态估计和结构运动。现有的基于局部特征匹配的方法，通过在大规模数据集上进行对应关系监督训练，能够在测试集上获得高度准确的匹配。然而，它们在不同特征的新数据集上往往无法很好地泛化，这与经典的特征提取器不同。相反，它们需要微调，假设可以获得地面实况对应关系或地面实况相机姿态和3D结构。我们通过取消对3D结构（如深度图或点云）的要求，只需要相机姿态信息（可以从里程计获得）来放松这个假设。我们通过将对应关系损失替换为极线损失来实现，极线损失鼓励假设匹配点位于相关的极线上。虽然它相对来说比对应关系较弱，但一定程度上可以在无需3D结构的情况下估计出相机的相对姿态。

    Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than corresponde
    
[^2]: 将流模型应用于相关格QCD态的生成

    Applications of flow models to the generation of correlated lattice QCD ensembles. (arXiv:2401.10874v1 [hep-lat])

    [http://arxiv.org/abs/2401.10874](http://arxiv.org/abs/2401.10874)

    本论文介绍了将机器学习的归一化流应用于格量子场论中，生成相关的格规场态集合，并且演示了利用这些相关性可以减少计算观测量时的方差。同时通过三个具体应用证明了机器学习流明显降低了统计不确定性。

    

    机器学习的归一化流可以用于格量子场论的背景下，在不同作用参数下生成统计相关的格规场态集合。本研究演示了如何利用这些相关性来减少计算观测量时的方差。采用了一种新颖的残差流结构，在连续极限的规范理论、QCD观测量的质量依赖性和基于费曼-赫尔曼方法的强子矩阵元等三个概念证明应用中。在所有三种情况下，与使用不相关集合或直接重加权进行的相同计算相比，使用机器学习流的统计不确定性显著降低。

    Machine-learned normalizing flows can be used in the context of lattice quantum field theory to generate statistically correlated ensembles of lattice gauge fields at different action parameters. This work demonstrates how these correlations can be exploited for variance reduction in the computation of observables. Three different proof-of-concept applications are demonstrated using a novel residual flow architecture: continuum limits of gauge theories, the mass dependence of QCD observables, and hadronic matrix elements based on the Feynman-Hellmann approach. In all three cases, it is shown that statistical uncertainties are significantly reduced when machine-learned flows are incorporated as compared with the same calculations performed with uncorrelated ensembles or direct reweighting.
    
[^3]: 基于剪枝的保护: 在不进行微调的情况下增加对齐的LLMs的越狱抵抗力

    Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])

    [http://arxiv.org/abs/2401.10862](http://arxiv.org/abs/2401.10862)

    本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。

    

    大型语言模型（LLMs）容易受到“越狱”提示的攻击，这种攻击可以诱使这些模型生成有害和违法内容。本文表明，剪枝LLM参数多达20％可以显著增加它们对此类攻击的抵抗力，而无需额外训练并且不损害其在标准基准测试中的性能。有趣的是，我们发现剪枝后观察到的增强安全性与模型的初始安全训练水平相关，这暗示剪枝的效果可能更普遍，也可能适用于超出安全性范畴的其他LLM行为。另外，我们还介绍了一个包含五个类别、插入到十个不同越狱提示中的225个有害任务的精选数据集，表明剪枝有助于LLMs集中注意力在越狱提示中与任务相关的标记上。最后，我们的实验揭示了突出的聊天模型（如LLaMA-2 Chat，Vicuna和Mistral Instruct）具有很高的易感性。

    Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
    
[^4]: Ensembler:使用模型集成在协作推理过程中防止模型反演攻击

    Ensembler: Combating model inversion attacks using model ensemble during collaborative inference. (arXiv:2401.10859v1 [cs.CR])

    [http://arxiv.org/abs/2401.10859](http://arxiv.org/abs/2401.10859)

    Ensembler是一个防止模型反演攻击的可扩展框架，通过利用模型集成和引入扰动的方式，在协作推理过程中有效地保护数据隐私。

    

    深度学习模型在各个领域展示出了卓越的性能。然而，庞大的模型大小促使边缘设备将推理过程的大部分转移到云端。虽然这种做法带来了许多优势，但也引发了关于用户数据隐私的重要问题。在云服务器的可信度受到质疑的情况下，保护数据隐私的实用和适应性方法变得至关重要。在本文中，我们介绍了Ensembler，这是一个可扩展的框架，旨在大大增加对抗方进行模型反演攻击的难度。Ensembler利用在对抗服务器上运行的模型组合，与现有的在协作推理过程中引入扰动到敏感数据的方法并行。我们的实验表明，即使与基本的高斯噪声相结合，Ensembler也可以有效地保护图像免受重建攻击。

    Deep learning models have exhibited remarkable performance across various domains. Nevertheless, the burgeoning model sizes compel edge devices to offload a significant portion of the inference process to the cloud. While this practice offers numerous advantages, it also raises critical concerns regarding user data privacy. In scenarios where the cloud server's trustworthiness is in question, the need for a practical and adaptable method to safeguard data privacy becomes imperative. In this paper, we introduce Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties. Ensembler leverages model ensembling on the adversarial server, running in parallel with existing approaches that introduce perturbations to sensitive data during colloborative inference. Our experiments demonstrate that when combined with even basic Gaussian noise, Ensembler can effectively shield images from reconstruction attacks, 
    
[^5]: 神经群体解码和不平衡的多组学数据用于癌症亚型诊断

    Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis. (arXiv:2401.10844v1 [cs.NE])

    [http://arxiv.org/abs/2401.10844](http://arxiv.org/abs/2401.10844)

    该论文研究了神经群体解码对于癌症亚型诊断的影响，通过采用取胜带走(WTA)网络和基因相似性网络的方法对多组学数据进行分析，展现了WTA网络在分类性能上的重要作用。

    

    最近在神经计算领域取得了重要进展，采用了取胜带走(WTA)电路来促进分层贝叶斯推断和尖峰神经网络的统一，作为信息处理的神经生物学合理模型。目前的研究通常通过分类任务，特别是MNIST数据集，来验证这些网络的性能。然而，研究人员尚未就如何最佳地将这些网络的随机响应转化为离散决策达成一致，这个过程被称为群体解码。尽管群体解码是SNNs常常被忽视的部分，但在这项工作中，我们显示群体解码对WTA网络的分类性能有很大影响。为此，我们将WTA网络应用于多组学数据的癌症亚型诊断问题，使用了来自癌症基因图谱数据库(TCGA)的数据集。在这个过程中，我们利用了一种新颖的基因相似性网络实现。

    Recent strides in the field of neural computation has seen the adoption of Winner Take All (WTA) circuits to facilitate the unification of hierarchical Bayesian inference and spiking neural networks as a neurobiologically plausible model of information processing. Current research commonly validates the performance of these networks via classification tasks, particularly of the MNIST dataset. However, researchers have not yet reached consensus about how best to translate the stochastic responses from these networks into discrete decisions, a process known as population decoding. Despite being an often underexamined part of SNNs, in this work we show that population decoding has a significanct impact on the classification performance of WTA networks. For this purpose, we apply a WTA network to the problem of cancer subtype diagnosis from multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing so we utilise a novel implementation of gene similarity networks, a featu
    
[^6]: 以改进效率和最小延迟训练通用脉冲神经网络

    Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency. (arXiv:2401.10843v1 [cs.NE])

    [http://arxiv.org/abs/2401.10843](http://arxiv.org/abs/2401.10843)

    本论文提出了一个通用训练框架，通过在有限的时间步内增强特征学习和激活效率，为更加节能的脉冲神经网络（SNNs）提供了解决方案。

    

    最近，以事件驱动方式运行并采用二进制脉冲表示的脉冲神经网络(SNNs)被认为是节能计算的有希望候选者。然而，高性能SNNs的训练存在成本瓶颈：训练SNN模型需要大量的时间步数以及通常的学习迭代，这限制了它们的能效。本文提出了一个通用训练框架，在有限的时间步内增强特征学习和激活效率，为更加节能的SNNs提供了新的解决方案。我们的框架允许SNN中的神经元从不同的感受野学习强大的脉冲特征，并通过利用当前刺激和来自其他神经元的循环信息来更新神经元状态。该设置在单个时间步内连续补充信息。此外，我们提出了一个投影函数来合并这两个刺激，以平滑优化神经元权重。

    Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs. Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spi
    
[^7]: 使用LLMs发现极端社交媒体中的编码反犹太恶意言论的出现

    Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])

    [http://arxiv.org/abs/2401.10841](http://arxiv.org/abs/2401.10841)

    这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。

    

    网络仇恨言论的蔓延给社交媒体平台带来了一个难题。一个特殊的挑战与使用编码语言的群体有关，这些群体既想为其用户创造归属感，又想回避检测。编码语言发展迅速，并且随着时间的推移使用方式不同。本文提出了一种检测新出现的编码恶意术语的方法论。该方法在在线反犹太言论的环境中进行了测试。该方法考虑了从社交媒体平台上抓取的帖子，通常是极端主义用户使用的。帖子是使用与以前已知的针对犹太人的仇恨言论相关的种子表达式进行抓取的。该方法首先通过识别每个帖子最具代表性的表达式，并计算它们在整个语料库中的频率。过滤掉语法不一致的表达式和之前遇到过的表达式，以便关注新出现的良好形式的术语。然后进行了语义评估。

    Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
    
[^8]: 通过混合优化实现符号认知诊断的智能教育系统

    Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems. (arXiv:2401.10840v1 [cs.CY])

    [http://arxiv.org/abs/2401.10840](http://arxiv.org/abs/2401.10840)

    本文提出了一种符号认知诊断（SCD）框架，通过利用符号树明确表示学生与练习互动，以及梯度优化方法学习参数，同时提高了泛化能力和可解释性。

    

    认知诊断评估是学生学习的基本且关键的任务。它建模了学生与练习的互动，并发现了学生对每个知识属性的熟练程度。在实际智能教育系统中，认知诊断方法的泛化能力和可解释性同样重要。然而，由于复杂的学生与练习互动，大多数现有方法很难二者兼顾。为此，本文提出了一种符号认知诊断（SCD）框架，以同时提高泛化能力和可解释性。该SCD框架利用符号树明确地表示复杂的学生与练习互动函数，并利用梯度优化方法有效地学习学生和练习参数。与此同时，挑战在于我们需要传递离散的符号表示和连续的参数优化。

    Cognitive diagnosis assessment is a fundamental and crucial task for student learning. It models the student-exercise interaction, and discovers the students' proficiency levels on each knowledge attribute. In real-world intelligent education systems, generalization and interpretability of cognitive diagnosis methods are of equal importance. However, most existing methods can hardly make the best of both worlds due to the complicated student-exercise interaction. To this end, this paper proposes a symbolic cognitive diagnosis~(SCD) framework to simultaneously enhance generalization and interpretability. The SCD framework incorporates the symbolic tree to explicably represent the complicated student-exercise interaction function, and utilizes gradient-based optimization methods to effectively learn the student and exercise parameters. Meanwhile, the accompanying challenge is that we need to tunnel the discrete symbolic representation and continuous parameter optimization. To address thi
    
[^9]: 弹性的基于代理的分布式机器学习框架：Holonic Learning

    Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework. (arXiv:2401.10839v1 [cs.DC])

    [http://arxiv.org/abs/2401.10839](http://arxiv.org/abs/2401.10839)

    Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials.

    

    在过去的十年中，数据和计算资源的普及性不断增加，推动了机器学习范式向更分布式的方法转变。这种转变不仅旨在解决可扩展性和资源分布挑战，还要解决紧迫的隐私和安全问题。为了对这一持续讨论做出贡献，本文介绍了Holonic Learning (HoL)，这是一个专为训练深度学习模型而设计的协作和注重隐私的学习框架。通过利用Holonic概念，HoL框架在学习过程中建立了一个结构化的自相似层次结构，通过每个holon的个体模型聚合方法以及它们的内部holon承诺和通信模式，实现对协作的更细致控制。Holonic Learning在其通用形式中提供了广泛的设计和灵活性潜力。为了经验分析和展示其效果，本文实现了Holo

    Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements Holo
    
[^10]: 通过通用概念发现理解视频Transformer

    Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])

    [http://arxiv.org/abs/2401.10831](http://arxiv.org/abs/2401.10831)

    本文研究了视频Transformer的可解释性问题，引入了视频Transformer概念发现算法来解释其决策过程，并揭示了时空推理机制和对象为中心的表示。

    

    本文研究了基于概念的视频Transformer表示的可解释性问题。具体而言，我们试图解释基于自动发现的高层时空概念的视频Transformer的决策过程。以往关于基于概念的可解释性的研究仅集中在图像级任务上。相比之下，视频模型处理了额外的时间维度，增加了复杂性，并在识别动态概念方面面临挑战。在这项工作中，我们通过引入第一个视频Transformer概念发现(VTCD)算法系统地解决了这些挑战。为此，我们提出了一种有效的无监督方法，用于识别视频Transformer表示的单元（概念）并对其对模型输出的重要性进行排名。得到的概念具有很强的可解释性，揭示了视频中的时空推理机制和以对象为中心的表示。

    This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
    
[^11]: 最新进展的命名实体识别综述

    A survey on recent advances in named entity recognition. (arXiv:2401.10825v1 [cs.CL])

    [http://arxiv.org/abs/2401.10825](http://arxiv.org/abs/2401.10825)

    这篇综述调查了最近的命名实体识别研究进展，并提供了对不同算法性能的深度比较，还探讨了数据集特征对方法行为的影响。

    

    命名实体识别旨在从文本中提取出命名真实世界对象的子字符串，并确定其类型（例如，是否指人物或组织）。在本综述中，我们首先概述了最近流行的方法，同时还关注了基于图和变换器的方法，包括很少在其他综述中涉及的大型语言模型（LLMs）。其次，我们重点介绍了针对稀缺注释数据集设计的方法。第三，我们评估了主要命名实体识别实现在各种具有不同特征（领域、规模和类别数）的数据集上的性能。因此，我们提供了一种从未同时考虑的算法的深度比较。我们的实验揭示了数据集特征如何影响我们比较的方法的行为。

    Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that are never considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare.
    
[^12]: 神经符号学习系统的优化

    Optimisation in Neurosymbolic Learning Systems. (arXiv:2401.10819v1 [cs.AI])

    [http://arxiv.org/abs/2401.10819](http://arxiv.org/abs/2401.10819)

    神经符号学习系统的优化研究了如何将深度学习与符号推理相结合，并探讨了模糊推理和概率推理在神经符号学习中的应用，得到了一些令人惊讶的结果，如与乌鸦悖论的联系。

    

    神经符号人工智能旨在将深度学习与符号人工智能相结合。这种集成有许多优势，例如减少训练神经网络所需的数据量，提高模型答案的可解释性和可理解性以及验证训练系统的正确性。本研究探讨了神经符号学习，在其中我们同时使用符号语言表示数据和背景知识。我们如何连接符号和神经组件以传达这些知识呢？其中一种方法是模糊推理，它研究的是真实程度。例如，身高并不是二元概念。相反，概率推理研究的是某件事情成为真实或将发生的概率。我们的第一个研究问题探讨了不同形式的模糊推理与学习的结合。我们发现一些令人惊讶的结果，例如与乌鸦悖论的联系，即当我们观察到一个绿色的苹果时，我们确认“乌鸦是黑色的”。在本研究中，我们没有使用背景知识。

    Neurosymbolic AI aims to integrate deep learning with symbolic AI. This integration has many promises, such as decreasing the amount of data required to train a neural network, improving the explainability and interpretability of answers given by models and verifying the correctness of trained systems. We study neurosymbolic learning, where we have both data and background knowledge expressed using symbolic languages. How do we connect the symbolic and neural components to communicate this knowledge? One option is fuzzy reasoning, which studies degrees of truth. For example, being tall is not a binary concept. Instead, probabilistic reasoning studies the probability that something is true or will happen. Our first research question studies how different forms of fuzzy reasoning combine with learning. We find surprising results like a connection to the Raven paradox stating we confirm "ravens are black" when we observe a green apple. In this study, we did not use the background knowledg
    
[^13]: Co-Pilot for Health: 个性化算法AI引导改善健康结果

    Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])

    [http://arxiv.org/abs/2401.10816](http://arxiv.org/abs/2401.10816)

    该研究通过使用基于图神经网络的推荐系统和来自可穿戴设备的健康行为数据，设计并实施了一个人工智能驱动平台，实现了个性化和情境引导，能够提高参与者的日常活动水平和中等至剧烈运动时长。

    

    在全球范围内自动塑造大型人群的健康行为，跨可穿戴设备类型和疾病状况具有巨大的潜力来改善全球健康结果。我们设计并实施了一个基于图神经网络（GNN）推荐系统和来自可穿戴健身设备的精细健康行为数据的人工智能驱动平台，用于数字算法引导。在此我们描述了该平台在新加坡针对$n=84,764$个个体的12周期间进行个性化和情境引导的有效性结果。我们统计验证了目标组中接受此类AI优化日常引导的参与者相较于控制组中未接受任何引导的匹配参与者，其每天的步数增加了6.17%（$p = 3.09\times10^{-4}$），每周中等至剧烈运动（MVPA）分钟增加了7.61%（$p = 1.16\times10^{-2}$）。此外，此类引导非常可行。

    The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
    
[^14]: 基于仿真的贝叶斯优化

    Simulation Based Bayesian Optimization. (arXiv:2401.10811v1 [stat.ML])

    [http://arxiv.org/abs/2401.10811](http://arxiv.org/abs/2401.10811)

    本文介绍了基于仿真的贝叶斯优化（SBBO）作为一种新方法，用于通过仅需基于采样的访问来优化获取函数。

    

    贝叶斯优化是一种将先验知识与持续函数评估相结合的强大方法，用于优化黑盒函数。贝叶斯优化通过构建与协变量相关的目标函数的概率代理模型来指导未来评估点的选择。对于平滑连续的搜索空间，高斯过程经常被用作代理模型，因为它们提供对后验预测分布的解析访问，从而便于计算和优化获取函数。然而，在涉及对分类或混合协变量空间进行优化的复杂情况下，高斯过程可能不是理想的选择。本文介绍了一种名为基于仿真的贝叶斯优化（SBBO）的新方法，该方法仅需要对后验预测分布进行基于采样的访问，以优化获取函数。

    Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimizations over categorical or mixed covariate spaces, GPs may not be ideal.  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires \emph{sampling-based} access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for co
    
[^15]: 被忽视的黑塞 (Hessian) 组件解释了锐化正则化中的谜团

    Neglected Hessian component explains mysteries in Sharpness regularization. (arXiv:2401.10809v1 [cs.LG])

    [http://arxiv.org/abs/2401.10809](http://arxiv.org/abs/2401.10809)

    这篇论文研究了在深度学习中，明确或隐含地惩罚二阶信息可以提高泛化性能，而权重噪声和梯度惩罚则很少能带来这样的好处。作者通过对损失的黑塞矩阵的结构进行解释，提出了特征的开发和特征的探索之间的量化分离。同时，作者发现忽视的非线性建模误差矩阵 (NME) 实际上很重要，可以解释为什么梯度惩罚对激活函数的选择非常敏感。此外，作者通过设计干预措施来改进性能，并提供了证据挑战了以往的观点，认为权重噪声和梯度惩罚是等效的。

    

    最近的研究表明，像SAM这样明确或隐含地惩罚二阶信息的方法可以提高深度学习中的泛化性能。类似的方法，如权重噪声和梯度惩罚，经常无法提供这样的好处。我们展示了这些差异可以通过损失的黑塞矩阵的结构来解释。首先，我们展示了常用的黑塞矩阵的分解可以定量解释为将特征的开发和特征的探索分开。特征的探索可以通过非线性建模误差矩阵(NME)来描述，这在文献中通常被忽视，因为它在插值中消失。我们的工作表明，NME事实上很重要，因为它可以解释为什么梯度惩罚对激活函数的选择敏感。利用这一见解，我们设计了改进性能的干预措施。我们也提供了证据来挑战了长期以来权重噪声和梯度惩罚的等效性。

    Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties.
    
[^16]: 学习视觉连接动作和其效果

    Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])

    [http://arxiv.org/abs/2401.10805](http://arxiv.org/abs/2401.10805)

    该论文提出了视觉连接动作和其效果的概念（CATE），用于视频理解。研究表明，不同的任务形式产生了捕捉直观动作特性的表示，但模型表现不佳，人类的表现明显优于它们。该研究为未来的努力奠定了基础，并希望能激发出高级形式和模型的灵感。

    

    在这项工作中，我们引入了视觉连接动作和其效果（CATE）的新概念，用于视频理解。CATE可以在任务规划和从示范中学习等领域中应用。我们提出了不同基于CATE的任务形式，如动作选择和动作指定，其中视频理解模型以语义和细粒度的方式连接动作和效果。我们观察到不同的形式产生了捕捉直观动作特性的表示。我们还设计了各种基线模型用于动作选择和动作指定。尽管任务具有直观性，但我们观察到模型困难重重，人类表现明显优于它们。本研究旨在为未来的努力奠定基础，展示了连接视频理解中动作和效果的灵活性和多功能性，希望能激发出高级形式和模型的灵感。

    In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
    
[^17]: 使用基于机器学习的稀事件算法估计AMOC转换概率

    Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm. (arXiv:2401.10800v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.10800](http://arxiv.org/abs/2401.10800)

    本研究通过结合TAMS和Next-Generation Reservoir Computing技术，利用稀事件算法估计来源于数据的确定函数，来计算大西洋经度翻转环流（AMOC）在指定时间窗口内崩溃的概率。

    

    大西洋经度翻转环流（AMOC）是全球气候的重要组成部分，被认为是一个临界因素，可以在全球变暖下崩溃。本研究的主要目标是使用一种稀事件算法（Trajectory-Adaptive Multilevel Splitting，TAMS）计算AMOC在指定时间窗口内崩溃的概率。然而，TAMS的效率和准确性取决于得分函数的选择。虽然已知最佳得分函数的定义，称为“确定函数”，但通常无法先验地计算它。在这里，我们将TAMS与下一代水库计算技术相结合，该技术可以从稀事件算法生成的数据中估计确定函数。我们在AMOC的随机盒模型中测试了这种技术，该模型存在两种转变类型，称为F（快速）转变和S（缓慢）转变。F转变的结果与那些进行了有利比较。

    The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming. The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS). However, the efficiency and accuracy of TAMS depend on the choice of the score function. Although the definition of the optimal score function, called ``committor function" is known, it is impossible in general to compute it a priori. Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm. We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions. Results for the F-transtions compare favorably with those 
    
[^18]: 使用图形的新型表示学习技术用于性能分析

    Novel Representation Learning Technique using Graphs for Performance Analytics. (arXiv:2401.10799v1 [cs.LG])

    [http://arxiv.org/abs/2401.10799](http://arxiv.org/abs/2401.10799)

    该论文提出了一种使用图形来表示学习的新技术，该技术能够解决高性能计算中的性能分析问题。通过将表格性能数据转化为图形，并利用图神经网络技术来捕捉样本之间的复杂关系，实现了不依赖于特征工程和预处理步骤的迁移学习。

    

    高性能计算（HPC）中的性能分析领域使用表格数据来解决回归问题，例如预测执行时间。现有的机器学习（ML）技术利用给定表格数据集中的特征之间的相关性，而不是直接利用样本之间的关系。此外，由于从原始特征中获得高质量的嵌入可以提高下游预测模型的准确性，现有方法依赖于广泛的特征工程和预处理步骤，耗费时间和人工努力。为了填补这两个差距，我们提出了一种新颖的想法，将表格性能数据转化为图形，以利用基于图神经网络（GNN）技术在捕捉特征和样本之间复杂关系方面的进展。与其他应用领域（例如社交网络）不同，图形是不存在的；相反，我们需要构建它。为了弥补这一差距，我们提出了构建图形的方法，其中节点代表样本。

    The performance analytics domain in High Performance Computing (HPC) uses tabular data to solve regression problems, such as predicting the execution time. Existing Machine Learning (ML) techniques leverage the correlations among features given tabular datasets, not leveraging the relationships between samples directly. Moreover, since high-quality embeddings from raw features improve the fidelity of the downstream predictive models, existing methods rely on extensive feature engineering and pre-processing steps, costing time and manual effort. To fill these two gaps, we propose a novel idea of transforming tabular performance data into graphs to leverage the advancement of Graph Neural Network-based (GNN) techniques in capturing complex relationships between features and samples. In contrast to other ML application domains, such as social networks, the graph is not given; instead, we need to build it. To address this gap, we propose graph-building methods where nodes represent samples
    
[^19]: 深度强化学习增强的活动感知动态健康监测系统

    Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health Monitoring Systems. (arXiv:2401.10794v1 [cs.LG])

    [http://arxiv.org/abs/2401.10794](http://arxiv.org/abs/2401.10794)

    这篇论文提出了一个深度强化学习增强的活动感知动态健康监测系统，通过使用SlowFast模型实现精确监测和成本效益的平衡。

    

    在智能医疗中，健康监测利用各种工具和技术分析患者的实时生物信号数据，实现即时的操作和干预。现有的监测方法是在医疗设备同时跟踪多个健康指标的前提下设计的，以适应其指定的功能范围。这意味着它们报告该范围内的所有相关健康数值，这可能导致过多的资源使用和收集不相关的健康数据。在这种情况下，我们提出了基于深度强化学习和SlowFast模型的动态活动感知健康监测策略（DActAHM），以在用户活动基础上确保精确监测，实现监测性能和成本效益之间的平衡。具体来说，DActAHM使用SlowFast模型高效地识别个体活动，并捕捉这些结果以进行增强处理。

    In smart healthcare, health monitoring utilizes diverse tools and technologies to analyze patients' real-time biosignal data, enabling immediate actions and interventions. Existing monitoring approaches were designed on the premise that medical devices track several health metrics concurrently, tailored to their designated functional scope. This means that they report all relevant health values within that scope, which can result in excess resource use and the gathering of extraneous data due to monitoring irrelevant health metrics. In this context, we propose Dynamic Activity-Aware Health Monitoring strategy (DActAHM) for striking a balance between optimal monitoring performance and cost efficiency, a novel framework based on Deep Reinforcement Learning (DRL) and SlowFast Model to ensure precise monitoring based on users' activities. Specifically, with the SlowFast Model, DActAHM efficiently identifies individual activities and captures these results for enhanced processing. Subsequen
    
[^20]: 两层网络训练中的早期对齐是一把双刃剑

    Early alignment in two-layer networks training is a two-edged sword. (arXiv:2401.10791v1 [cs.LG])

    [http://arxiv.org/abs/2401.10791](http://arxiv.org/abs/2401.10791)

    本文研究了两层网络训练中的早期对齐现象，发现在小初始化和一个隐藏的ReLU层网络中，神经元会在训练的早期阶段向关键方向进行对齐，导致网络稀疏表示以及梯度流在收敛时的隐含偏好。然而，这种稀疏诱导的对齐也使得训练目标的最小化变得困难。

    

    使用一阶优化方法训练神经网络是深度学习成功的核心。初始化的规模是一个关键因素，因为小的初始化通常与特征学习模式相关，在这种模式下，梯度下降对简单解隐含偏好。本文提供了早期对齐阶段的普遍和量化描述，最初由Maennel等人提出。对于小初始化和一个隐藏的ReLU层网络，训练动态的早期阶段导致神经元向关键方向进行对齐。这种对齐引发了网络的稀疏表示，这与梯度流在收敛时的隐含偏好直接相关。然而，这种稀疏诱导的对齐是以在最小化训练目标方面遇到困难为代价的：我们还提供了一个简单的数据示例，其中超参数网络无法收敛到全局最小值。

    Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and
    
[^21]: 衡量场景级对象对目标检测的影响：走向对检测决策的定量解释

    Measuring the Impact of Scene Level Objects on Object Detection: Towards Quantitative Explanations of Detection Decisions. (arXiv:2401.10790v1 [cs.CV])

    [http://arxiv.org/abs/2401.10790](http://arxiv.org/abs/2401.10790)

    本研究提出了一种用于评估场景级对象对目标检测模型影响的解释性方法，通过比较模型在有和没有特定场景级对象的测试数据上的准确性，揭示了这些对象对模型性能的贡献。

    

    虽然准确度和其他常见指标可以提供关于目标检测模型性能的有用窗口，但它们缺乏对模型决策过程的深入视角。无论训练数据和过程的质量如何，无法保证目标检测模型学到的特征。模型可能会学习到某些背景上下文（即场景级对象）与标记类别的存在之间的关系。此外，标准的性能验证和指标无法识别这一现象。本文提出了一种新的黑盒可解释性方法，用于进一步验证目标检测模型，并找出场景级对象对图像中物体识别的影响。通过比较模型在有和没有特定场景级对象的测试数据上的准确性，可以更清楚地了解这些对象对模型性能的贡献。本实验将评估建筑物对模型性能的影响。

    Although accuracy and other common metrics can provide a useful window into the performance of an object detection model, they lack a deeper view of the model's decision process. Regardless of the quality of the training data and process, the features that an object detection model learns cannot be guaranteed. A model may learn a relationship between certain background context, i.e., scene level objects, and the presence of the labeled classes. Furthermore, standard performance verification and metrics would not identify this phenomenon. This paper presents a new black box explainability method for additional verification of object detection models by finding the impact of scene level objects on the identification of the objects within the image. By comparing the accuracies of a model on test data with and without certain scene level objects, the contributions of these objects to the model's performance becomes clearer. The experiment presented here will assess the impact of buildings 
    
[^22]: Medusa: 多解码头的简洁LLM推理加速框架

    Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])

    [http://arxiv.org/abs/2401.10774](http://arxiv.org/abs/2401.10774)

    Medusa是一个能够提升LLM推理性能的简洁框架，通过增加多个解码头以实现并行预测多个后续标记，并通过树状注意力机制和并行处理来减少解码步骤。

    

    大型语言模型（LLMs）中的推理过程通常受限于自回归解码过程中的并行性缺失，使得大多数操作受限于加速器的内存带宽。虽然已经提出了类似于推测解码的方法来解决这个问题，但由于获得和维护独立的草稿模型所涉及的挑战，它们的实施受到了阻碍。在本文中，我们提出了一种高效的方法，通过添加额外的解码头来增强LLM推理，以并行预测多个后续标记。Medusa利用基于树的注意力机制，在每个解码步骤中同时构造多个候选延续并进行验证。通过利用并行处理，Medusa在单步延迟方面仅引入了最小的开销，同时大大降低了所需的解码步骤数。

    The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
    
[^23]: Starlit: 隐私保护的联邦学习以增强金融欺诈检测

    Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection. (arXiv:2401.10765v1 [cs.LG])

    [http://arxiv.org/abs/2401.10765](http://arxiv.org/abs/2401.10765)

    Starlit是一个新的可扩展隐私保护的联邦学习机制，解决了对于金融欺诈检测中的几个限制，包括缺乏正式的安全定义和证明、假定冻结账户、规模扩大、身份对齐阶段和难以抵抗客户端退出。

    

    联邦学习（FL）是一种数据最小化方法，能够在具有本地数据的各个客户端之间进行协作模型训练，避免直接数据交换。然而，用于识别欺诈金融交易的最新FL解决方案存在以下一些限制：缺乏正式的安全定义和证明，假定金融机构事先冻结可疑客户的账户（限制了解决方案的采用），规模不断扩大，涉及$O(n^2)$的计算昂贵的模块指数运算（其中$n$是金融机构的总数）或者高效率低的完全同态加密，假设各方已经完成了身份对齐阶段，因此将其排除在实施、性能评估和安全分析之外，并且难以抵抗客户端的退出。本文引入了一种新颖的可扩展隐私保护FL机制——Starlit，克服了这些限制。

    Federated Learning (FL) is a data-minimization approach enabling collaborative model training across diverse clients with local data, avoiding direct data exchange. However, state-of-the-art FL solutions to identify fraudulent financial transactions exhibit a subset of the following limitations. They (1) lack a formal security definition and proof, (2) assume prior freezing of suspicious customers' accounts by financial institutions (limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$ computationally expensive modular exponentiation (where $n$ is the total number of financial institutions) or highly inefficient fully homomorphic encryption, (4) assume the parties have already completed the identity alignment phase, hence excluding it from the implementation, performance evaluation, and security analysis, and (5) struggle to resist clients' dropouts. This work introduces Starlit, a novel scalable privacy-preserving FL mechanism that overcomes these limitations
    
[^24]: 交通分类的数据增强

    Data Augmentation for Traffic Classification. (arXiv:2401.10754v1 [cs.LG])

    [http://arxiv.org/abs/2401.10754](http://arxiv.org/abs/2401.10754)

    这项工作通过对交通分类任务中的数据增强进行分析和实验，发现时间序列顺序和掩码的增强在交通分类中更适用，同时提出了简单的潜在空间分析可以解释增强效果的思路。

    

    数据增强（DA）是一种广泛应用于计算机视觉（CV）和自然语言处理（NLP）任务中以提高模型性能的技术，通过添加合成样本来丰富训练数据。然而，在网络环境中，特别是在交通分类（TC）任务中，DA很难获得广泛应用。在本研究中，我们通过将18种增强函数应用于3个TC数据集的数据包时间序列作为输入表示，并考虑各种训练条件，来填补这个空白。我们的结果表明，（i）DA可以获得以前未被探索的好处，（ii）作用于时间序列的顺序和掩码的增强在TC中更合适，以及（iii）简单的潜在空间分析可以提供关于为什么增强会产生积极或消极影响的一些线索。

    Data Augmentation (DA) -- enriching training data by adding synthetic samples -- is a technique widely adopted in Computer Vision (CV) and Natural Language Processing (NLP) tasks to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets using packet time series as input representation and considering a variety of training conditions. Our results show that (i) DA can reap benefits previously unexplored with (ii) augmentations acting on time series sequence order and masking being a better suit for TC and (iii) simple latent space analysis can provide hints about why augmentations have positive or negative effects.
    
[^25]: BoolGebra: 用于布尔代数操作的属性图学习

    BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation. (arXiv:2401.10753v1 [cs.AR])

    [http://arxiv.org/abs/2401.10753](http://arxiv.org/abs/2401.10753)

    BoolGebra是一种方法，使用属性图学习和神经网络来改进布尔代数操作的逻辑综合。它通过减小搜索空间并高效定位优化空间，实现了跨设计推论的通用性和规模化潜力。

    

    布尔代数操作是电子设计自动化（EDA）设计流程中的核心。现有方法往往难以充分利用优化机会，并且往往受到搜索空间爆炸和可扩展性效率有限的困扰。本文提出了BoolGebra，一种新颖的用于布尔代数操作的属性图学习方法，旨在改进基础逻辑综合。BoolGebra结合了图神经网络（GNNs），并将结构和功能信息的初始特征嵌入作为输入。全连接神经网络被用作直接优化结果预测的预测器，显著减少搜索空间并高效定位优化空间。实验涉及培训BoolGebra模型关于设计特定和跨设计推论，使用训练模型的BoolGebra展示了跨设计推论的通用性和规模化潜力。

    Boolean algebraic manipulation is at the core of logic synthesis in Electronic Design Automation (EDA) design flow. Existing methods struggle to fully exploit optimization opportunities, and often suffer from an explosive search space and limited scalability efficiency. This work presents BoolGebra, a novel attributed graph-learning approach for Boolean algebraic manipulation that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph Neural Networks (GNNs) and takes initial feature embeddings from both structural and functional information as inputs. A fully connected neural network is employed as the predictor for direct optimization result predictions, significantly reducing the search space and efficiently locating the optimization space. The experiments involve training the BoolGebra model w.r.t design-specific and cross-design inferences using the trained model, where BoolGebra demonstrates generalizability for cross-design inference and its potential to scale 
    
[^26]: 可靠性认知诊断框架ReliCD:具有信心认知的可靠性认知诊断框架

    ReliCD: A Reliable Cognitive Diagnosis Framework with Confidence Awareness. (arXiv:2401.10749v1 [cs.CY])

    [http://arxiv.org/abs/2401.10749](http://arxiv.org/abs/2401.10749)

    该论文提出了一种可靠的认知诊断框架ReliCD，能够量化诊断反馈的信心，并适用于不同的认知诊断功能。

    

    在过去几十年中，认知诊断建模吸引了计算教育界的越来越多关注，可以量化学生的学习状况和知识掌握水平。近年来，神经网络的最新进展通过学习学生和练习的深层表示大大提高了传统认知诊断模型的性能。然而，现有方法常常在预测学生掌握水平时存在过于自信的问题，这主要是由现实中学生-练习交互数据中不可避免的噪声和稀疏性引起的，严重阻碍了认知反馈的教育应用。为了解决这个问题，我们提出了一种新颖的可靠性认知诊断框架ReliCD，可以量化诊断反馈的信心，并适应不同的认知诊断功能。

    During the past few decades, cognitive diagnostics modeling has attracted increasing attention in computational education communities, which is capable of quantifying the learning status and knowledge mastery levels of students. Indeed, the recent advances in neural networks have greatly enhanced the performance of traditional cognitive diagnosis models through learning the deep representations of students and exercises. Nevertheless, existing approaches often suffer from the issue of overconfidence in predicting students' mastery levels, which is primarily caused by the unavoidable noise and sparsity in realistic student-exercise interaction data, severely hindering the educational application of diagnostic feedback. To address this, in this paper, we propose a novel Reliable Cognitive Diagnosis(ReliCD) framework, which can quantify the confidence of the diagnosis feedback and is flexible for different cognitive diagnostic functions. Specifically, we first propose a Bayesian method to
    
[^27]: 针对脉冲神经网络中神经元的快速无梯度激活最大化

    Fast gradient-free activation maximization for neurons in spiking neural networks. (arXiv:2401.10748v1 [cs.NE])

    [http://arxiv.org/abs/2401.10748](http://arxiv.org/abs/2401.10748)

    本论文提出了一个快速无梯度激活最大化的方法，用于探索神经网络中神经元的特化。在一个人工脉冲神经网络上成功测试了这个方法，并提供了一个有效的设计框架。

    

    神经网络（NNs），无论是生物还是人工的，都是由神经元构成的复杂系统，每个神经元都有自己的专业化。揭示这些专业化对于理解NNs的内部工作机制至关重要。对于一个生物系统，其对刺激的神经响应不是已知的（更不用说是可微分的函数），唯一的方式是建立一个反馈循环，将其暴露于刺激之中，其性质可以迭代地变化，以寻求最大响应的方向。为了在一个生物网络上测试这样的循环，首先需要学会快速和高效地运行它，以在最少的迭代次数内达到最有效的刺激（最大化某些神经元的激活）。我们提出了一个具有有效设计的框架，成功地在人工脉冲神经网络（SNN，模拟生物大脑NNs行为的模型）上测试了它。我们用于激活最大化（AM）的优化方法是基于快梯度方法的。

    Neural networks (NNs), both living and artificial, work due to being complex systems of neurons, each having its own specialization. Revealing these specializations is important for understanding NNs inner working mechanisms. The only way to do this for a living system, the neural response of which to a stimulus is not a known (let alone differentiable) function is to build a feedback loop of exposing it to stimuli, the properties of which can be iteratively varied aiming in the direction of maximal response. To test such a loop on a living network, one should first learn how to run it quickly and efficiently, reaching most effective stimuli (ones that maximize certain neurons activation) in least possible number of iterations. We present a framework with an effective design of such a loop, successfully testing it on an artificial spiking neural network (SNN, a model that mimics the behaviour of NNs in living brains). Our optimization method used for activation maximization (AM) was ba
    
[^28]: 缺失模态下的多模态情感分析:一种知识迁移方法

    Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])

    [http://arxiv.org/abs/2401.10747](http://arxiv.org/abs/2401.10747)

    本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。

    

    多模态情感分析旨在通过视觉、语言和声音线索来识别个体表达的情绪。然而，现有研究大多假设在训练和测试过程中所有模态都是可用的，这使得它们的算法容易受到缺失模态的影响。在本文中，我们提出了一种新颖的知识迁移网络，用于在不同模态之间进行翻译，以重构缺失的音频模态。此外，我们还开发了一种跨模态注意机制，以保留重构和观察到的模态的最大信息，用于情感预测。在三个公开数据集上进行的大量实验证明了相对于基线算法的显著改进，并实现了与具有完整多模态监督的先前方法相媲美的结果。

    Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
    
[^29]: 利用深度学习对脑电解码中的欧几里得对齐进行系统评估

    A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])

    [http://arxiv.org/abs/2401.10746](http://arxiv.org/abs/2401.10746)

    本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。

    

    脑电图（EEG）信号经常用于各种脑机接口（BCI）任务。尽管深度学习（DL）技术显示出有希望的结果，但它们受到大量数据要求的限制。通过利用来自多个受试者的数据，迁移学习能够更有效地训练DL模型。一种越来越受欢迎的技术是欧几里得对齐（EA），因为它易于使用、计算复杂度低并且与深度学习模型兼容。然而，很少有研究评估其对共享和个体DL模型的训练效果的影响。在这项工作中，我们系统地评估了EA与DL相结合在解码BCI信号中的效果。我们使用EA来训练来自多个受试者的共享模型，并评估其对新受试者的可迁移性。我们的实验结果表明，它将目标受试者的解码率提高了4.33％，并且收敛时间缩短了超过70％。我们还为个体模型进行了训练。

    Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
    
[^30]: 对高级大型语言模型的治理和利用的道德人工智能原则和指南

    Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])

    [http://arxiv.org/abs/2401.10745](http://arxiv.org/abs/2401.10745)

    本文探讨了如何利用道德人工智能原则和指南来解决高级大型语言模型的治理和利用问题。

    

    鉴于ChatGPT、LaMDA和其他大型语言模型（LLMs）的成功，技术行业和其他行业对LLMs的开发和使用有所增加。虽然LLMs的水平尚未超过人类智能，但总有一天会达到这一点。这种LLMs可以称为高级LLMs。目前，由于尚未达到这一点，使用道德人工智能（AI）原则和指南来解决高级LLMs的问题还受到限制。然而，这是一个问题，因为一旦达到这一点，我们将无法充分准备好以道德和最佳方式处理其产生的后果，这将导致不可预期的后果。本文讨论了如何利用道德人工智能原则和指南来解决高级LLMs的问题。

    Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
    
[^31]: 用基于数据驱动的实用工具增强聚合器能力：利用聚合与分散的灵活性实现需求响应

    Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response. (arXiv:2401.10726v1 [eess.SY])

    [http://arxiv.org/abs/2401.10726](http://arxiv.org/abs/2401.10726)

    本研究通过优化聚合灵活性提供策略和评估HVAC系统的分散灵活性提供，为聚合器在可再生能源不确定性下实现需求响应提供了实用工具，从而实现了稳健的脱碳和增强能源系统的韧性。

    

    本研究探讨了在可再生能源带来不确定性的情况下，聚合器和建筑物居住者通过需求响应（DR）方案激活灵活性的关键相互作用，着重于实现稳健的脱碳和增强能源系统的韧性。首先，引入了一种在数据有限的环境中优化聚合灵活性提供策略的方法，利用离散傅里叶变换（DFT）和聚类技术识别建筑物居民的活动模式。其次，研究评估了DR事件期间供热通风空调（HVAC）系统的分散灵活性提供，采用机器学习和优化技术进行精确的设备级分析。第一种方法为聚合器在整个建筑物消费仅有一个智能电表的环境中提供灵活性服务提供了一条非侵入性途径。

    This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while t
    
[^32]: 面向FPGA的汽车控制区域网络的实时零日入侵检测系统

    Real-Time Zero-Day Intrusion Detection System for Automotive Controller Area Network on FPGAs. (arXiv:2401.10724v1 [cs.CR])

    [http://arxiv.org/abs/2401.10724](http://arxiv.org/abs/2401.10724)

    这篇论文介绍了一种基于无监督学习的卷积自编码器架构，用于实时检测零日攻击。它解决了汽车控制区域网络中零日攻击的检测问题，该问题变得尤为重要，因为随着主动注入攻击的复杂性不断增加，及时检测并防止传播成为了一项挑战。

    

    随着汽车对外部世界的连接增加，增加了车辆自动化程度，暴露了以前独立存在的汽车网络，如控制区域网络（CAN）的漏洞。CAN的特性，如基于广播的电子控制单元（ECU）之间的通信降低了部署成本，现在被利用来进行主动注入攻击，如拒绝服务（DoS），模糊测试和欺骗攻击。研究文献提出了多个以监督机器学习模型为基础的入侵检测系统（IDS）来检测这种恶意活动; 但是，这些模型很大程度上局限于识别先前已知的攻击向量。随着主动注入攻击复杂性的不断增加，实时检测这些网络中的零日（新颖）攻击（以防止传播）成为一个特别感兴趣的问题。本文提出了一种基于无监督学习的卷积自编码器架构，用于实时检测零日攻击。

    Increasing automation in vehicles enabled by increased connectivity to the outside world has exposed vulnerabilities in previously siloed automotive networks like controller area networks (CAN). Attributes of CAN such as broadcast-based communication among electronic control units (ECUs) that lowered deployment costs are now being exploited to carry out active injection attacks like denial of service (DoS), fuzzing, and spoofing attacks. Research literature has proposed multiple supervised machine learning models deployed as Intrusion detection systems (IDSs) to detect such malicious activity; however, these are largely limited to identifying previously known attack vectors. With the ever-increasing complexity of active injection attacks, detecting zero-day (novel) attacks in these networks in real-time (to prevent propagation) becomes a problem of particular interest. This paper presents an unsupervised-learning-based convolutional autoencoder architecture for detecting zero-day attac
    
[^33]: 从初始状态到最终状态的反应路径的生成模型

    Generative Model for Constructing Reaction Path from Initial to Final States. (arXiv:2401.10721v1 [physics.comp-ph])

    [http://arxiv.org/abs/2401.10721](http://arxiv.org/abs/2401.10721)

    本文提出了一种利用神经网络生成反应路径初始猜测的创新方法，在有机反应的复杂反应路径中取得了良好的效果。

    

    绘制反应路径及其相应的活化能垒是分子模拟的一个重要方面。由于其固有的复杂性和非线性，甚至生成这些路径的初始猜测都仍然是一个具有挑战性的问题。本文提出了一种创新方法，利用神经网络生成这些反应路径的初始猜测。该方法通过输入初始状态的坐标，随后逐步对其结构进行改变。这个迭代过程最终生成了对反应路径的近似表示以及最终状态的坐标。该方法的应用范围扩展到有机反应所示的复杂反应路径。训练是在Transition1x数据集上进行的，该数据集包含有机反应路径数据。结果显示生成的反应与相应的测试数据具有相当的相似性。该方法具有灵活性。

    Mapping out reaction pathways and their corresponding activation barriers is a significant aspect of molecular simulation. Given their inherent complexity and nonlinearity, even generating a initial guess of these paths remains a challenging problem. Presented in this paper is an innovative approach that utilizes neural networks to generate initial guess for these reaction pathways. The proposed method is initiated by inputting the coordinates of the initial state, followed by progressive alterations to its structure. This iterative process culminates in the generation of the approximate representation of the reaction path and the coordinates of the final state. The application of this method extends to complex reaction pathways illustrated by organic reactions. Training was executed on the Transition1x dataset, an organic reaction pathway dataset. The results revealed generation of reactions that bore substantial similarities with the corresponding test data. The method's flexibility 
    
[^34]: 使用具有二次决策函数的神经网络进行分类

    Classification with neural networks with quadratic decision functions. (arXiv:2401.10710v1 [cs.LG])

    [http://arxiv.org/abs/2401.10710](http://arxiv.org/abs/2401.10710)

    本文研究了使用具有二次决策函数的神经网络进行分类的方法，通过在MNIST数据集上测试和比较在手写数字分类和亚种分类上的表现，证明了其在紧凑基本几何形状识别方面的优势。

    

    神经网络通过使用二次决策函数作为标准神经网络的替代品来实现一种优势，当需要识别的对象具有紧凑基本几何形状（如圆形、椭圆形等）时，这种优势更加明显。本文研究了在分类问题上使用这种假设函数的方法。特别地，我们在MNIST数据集上测试和比较了该算法在手写数字分类和亚种分类上的表现。我们还展示了在Tensorflow和Keras软件中可以基于神经网络结构进行实现。

    Neural network with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear one. They are advantageous when the objects to be identified are of compact basic geometries like circles, ellipsis etc. In this paper we investigate the use of such ansatz functions for classification. In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies. We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively.
    
[^35]: 安全离线强化学习与可行性导向扩散模型

    Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])

    [http://arxiv.org/abs/2401.10700](http://arxiv.org/abs/2401.10700)

    本论文提出了一种安全离线强化学习方法，通过可行性导向的扩散模型来平衡安全约束满足、奖励最大化和离线数据集的行为规范化。通过可达性分析，将硬安全约束转化为在离线数据集中识别最大可行区域。

    

    安全离线强化学习是一种有效避免风险的在线交互以实现安全策略学习的方法。大多数现有的方法只强制软约束，即将期望的安全违规约束在预定的阈值以下。然而，这可能导致潜在的不安全结果，在安全关键场景中是不可接受的。另一种选择是强制零违规的硬约束。然而，在离线设置中，这可能具有挑战性，因为需要在安全约束满足、奖励最大化和离线数据集所施加的行为正则化之间取得适当的平衡。有趣的是，我们发现通过安全控制理论的可达性分析，硬安全约束可以等效地转化为识别给定离线数据集的最大可行区域。这无缝地将原始的三重问题转化为依赖于可行性的目标，即在可行性范围内最大化奖励值。

    Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the f
    
[^36]: 超越RMSE和MAE：引入EAUC来揭示偏见和不公平的迪亚德回归模型中的隐藏因素

    Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])

    [http://arxiv.org/abs/2401.10690](http://arxiv.org/abs/2401.10690)

    本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。

    

    迪亚德回归模型用于预测一对实体的实值结果，在许多领域中都是基础的（例如，在推荐系统中预测用户对产品的评分），在许多其他领域中也有许多潜力但尚未深入探索（例如，在个性化药理学中近似确定患者的适当剂量）。本研究中，我们证明个体实体观察值分布的非均匀性导致了最先进模型中的严重偏见预测，偏向于实体的观察过去值的平均值，并在另类但同样重要的情况下提供比随机预测更差的预测能力。我们表明，全局错误度量标准如均方根误差（RMSE）和平均绝对误差（MAE）不足以捕捉到这种现象，我们将其命名为另类偏见，并引入另类-曲线下面积（EAUC）作为一个新的补充度量，可以在所有研究的模型中量化它。

    Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
    
[^37]: 一个轻量级的多攻击CAN入侵检测系统在混合FPGAs上的研究

    A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid FPGAs. (arXiv:2401.10689v1 [cs.CR])

    [http://arxiv.org/abs/2401.10689](http://arxiv.org/abs/2401.10689)

    本论文研究了一种在混合FPGAs上部署的轻量级多攻击CAN入侵检测系统，利用量化的机器学习模型，能够检测并缓解CAN中的多个攻击向量。该系统消耗较低的功耗，避免了对GPU等专用计算单元的依赖。

    

    车辆中的日益连接性使得新的能力成为可能，例如连接的自主驾驶和高级驾驶辅助系统（ADAS），以提高下一代车辆的安全性和可靠性。然而，这种对车内功能的增加让使用遗留车内网络（如CAN）的关键能力受到了威胁，因为CAN没有固有的安全性或认证机制。使用机器学习模型的入侵检测和缓解方法，在检测CAN中的多个攻击向量方面显示出了良好的结果，因为它们能够推广到新的向量。然而，大多数部署需要像GPU这样的专用计算单元执行线速检测，消耗更高的功耗。本文介绍了一种轻量级的多攻击量化机器学习模型，该模型使用赛灵思的深度学习处理单元IP在Zynq Ultrascale+（XCZU3EG）FPGA上部署，并使用p进行训练和验证。

    Rising connectivity in vehicles is enabling new capabilities like connected autonomous driving and advanced driver assistance systems (ADAS) for improving the safety and reliability of next-generation vehicles. This increased access to in-vehicle functions compromises critical capabilities that use legacy invehicle networks like Controller Area Network (CAN), which has no inherent security or authentication mechanism. Intrusion detection and mitigation approaches, particularly using machine learning models, have shown promising results in detecting multiple attack vectors in CAN through their ability to generalise to new vectors. However, most deployments require dedicated computing units like GPUs to perform line-rate detection, consuming much higher power. In this paper, we present a lightweight multi-attack quantised machine learning model that is deployed using Xilinx's Deep Learning Processing Unit IP on a Zynq Ultrascale+ (XCZU3EG) FPGA, which is trained and validated using the p
    
[^38]: 操纵稀疏双下降现象

    Manipulating Sparse Double Descent. (arXiv:2401.10686v1 [cs.LG])

    [http://arxiv.org/abs/2401.10686](http://arxiv.org/abs/2401.10686)

    本文研究了两层神经网络中的双下降现象，探讨了L1正则化和表示维度的影响，并提出了稀疏双下降现象。它的发现有助于更好地理解神经网络的训练和优化。

    

    本文研究了两层神经网络中的双下降现象，重点关注L1正则化和表示维度的作用。它探讨了一种名为稀疏双下降的替代双下降现象。研究强调了模型复杂性、稀疏性和泛化之间的复杂关系，并提出了进一步研究更多样化的模型和数据集的建议。这些发现有助于更深入地理解神经网络的训练和优化。

    This paper investigates the double descent phenomenon in two-layer neural networks, focusing on the role of L1 regularization and representation dimensions. It explores an alternative double descent phenomenon, named sparse double descent. The study emphasizes the complex relationship between model complexity, sparsity, and generalization, and suggests further research into more diverse models and datasets. The findings contribute to a deeper understanding of neural network training and optimization.
    
[^39]: 用神经假伪距修正实现端到端GPS定位

    Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])

    [http://arxiv.org/abs/2401.10685](http://arxiv.org/abs/2401.10685)

    本论文提出了一个端到端的GPS定位框架E2E-PrNet，通过直接训练神经网络PrNet来进行伪距修正，实验结果表明其优于现有端到端GPS定位方法。

    

    伪距误差是GPS定位不准确的根本原因。以往的数据驱动方法使用手工制作的中间标签进行伪距误差回归和消除。与之不同的是，我们提出了一个端到端的GPS定位框架E2E-PrNet，通过使用GPS接收机状态的真实值计算最终任务损失，直接训练一个用于伪距修正的神经网络PrNet。损失对可学习参数的梯度通过可微非线性最小二乘优化器反向传播到PrNet。通过使用Android手机收集的GPS数据进行验证，结果显示E2E-PrNet优于最先进的端到端GPS定位方法。

    Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.
    
[^40]: 基于深度学习的嵌入式汽车CAN网络入侵检测系统

    Deep Learning-based Embedded Intrusion Detection System for Automotive CAN. (arXiv:2401.10674v1 [cs.CR])

    [http://arxiv.org/abs/2401.10674](http://arxiv.org/abs/2401.10674)

    本文提出了一种基于深度学习的嵌入式汽车CAN网络入侵检测系统，通过专用的硬件加速器实现了深度卷积神经网络入侵检测模型的透明集成，平均检测准确率超过99%。

    

    车载电子设备的复杂性的增加使得像自动驾驶和主动安全这样的新能力得以实现。然而，不断增加的自动化也增加了安全威胁的风险，这在CAN等传统网络中缺乏内置的安全措施的情况下更加严重，允许攻击者观察、篡改和修改在这些广播网络上共享的信息。各种入侵检测方法已被提出来检测和解决这些威胁，其中机器学习模型被证明非常有效。然而，部署机器学习模型将需要通过高端处理器或GPU提供高处理能力以逼近线速率。本文提出了一种混合FPGA-based ECU方法，通过一个专用的现成硬件加速器来透明地集成IDS功能，并实现了深度卷积神经网络入侵检测模型。我们的结果表明，所提出的方法在多个攻击数据集上提供了平均精度超过99%。

    Rising complexity of in-vehicle electronics is enabling new capabilities like autonomous driving and active safety. However, rising automation also increases risk of security threats which is compounded by lack of in-built security measures in legacy networks like CAN, allowing attackers to observe, tamper and modify information shared over such broadcast networks. Various intrusion detection approaches have been proposed to detect and tackle such threats, with machine learning models proving highly effective. However, deploying machine learning models will require high processing power through high-end processors or GPUs to perform them close to line rate. In this paper, we propose a hybrid FPGA-based ECU approach that can transparently integrate IDS functionality through a dedicated off-the-shelf hardware accelerator that implements a deep-CNN intrusion detection model. Our results show that the proposed approach provides an average accuracy of over 99% across multiple attack dataset
    
[^41]: FIMBA:通过特征重要性对抗攻击评估基因组学中人工智能的鲁棒性

    FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks. (arXiv:2401.10657v1 [cs.LG])

    [http://arxiv.org/abs/2401.10657](http://arxiv.org/abs/2401.10657)

    本文通过对抗攻击展示了基因组学中人工智能模型的脆弱性，同时提出了一种基于输入转换的攻击方法和使用变分自动编码器模型生成有毒数据的增强方法。实验结果表明模型性能下降，准确率降低，假阳性和假阴性增加。

    

    随着人工智能在生物技术应用中的不断增长和基因组测序的广泛采用，越来越多基于人工智能的算法和工具进入研究和生产阶段，影响着药物发现和临床结果等重要决策流程。本文通过在公认的基因组学数据集上进行模型对抗攻击，展示了人工智能模型在下游任务中的脆弱性。我们通过采用模拟真实数据并混淆模型决策的输入转换攻击，从而大幅度降低了模型的性能。此外，我们通过使用变分自动编码器模型来生成有毒数据来增强我们的攻击方法。实验结果明确表明模型性能下降，准确率降低，假阳性和假阴性增加。此外，我们还分析了生成的对抗样本。

    With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes. This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets. We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance. Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model. Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives. Furthermore, we analyze the resulting adversarial samples vi
    
[^42]: 基于Transformer的多模态仇恨言论检测方法：关注融合

    Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech Detection. (arXiv:2401.10653v1 [cs.CL])

    [http://arxiv.org/abs/2401.10653](http://arxiv.org/abs/2401.10653)

    这项研究基于Transformer框架和"关注融合"层，采用多模态方法识别仇恨言论，突破了传统的文本分析限制。

    

    随着社交媒体使用量的激增和指数增长，审查社交媒体内容中是否存在任何仇恨内容变得非常重要。研究者们自过去十年以来一直致力于区分宣传仇恨和非宣传仇恨的内容。传统上，主要关注的是对文本内容的分析。然而，最近的研究也开始涉及对基于音频的内容的识别。然而，研究表明仅仅依赖音频或基于文本的内容可能是无效的，因为近期的激增表明个体在言辞和写作中经常使用讽刺。为了克服这些挑战，我们提出了一种基于Transformer框架的方法，利用音频和文本表示来判断一段言辞是否宣传了仇恨，同时引入了我们自己的层称为"关注融合"。

    With the recent surge and exponential growth of social media usage, scrutinizing social media content for the presence of any hateful content is of utmost importance. Researchers have been diligently working since the past decade on distinguishing between content that promotes hatred and content that does not. Traditionally, the main focus has been on analyzing textual content. However, recent research attempts have also commenced into the identification of audio-based content. Nevertheless, studies have shown that relying solely on audio or text-based content may be ineffective, as recent upsurge indicates that individuals often employ sarcasm in their speech and writing. To overcome these challenges, we present an approach to identify whether a speech promotes hate or not utilizing both audio and textual representations. Our methodology is based on the Transformer framework that incorporates both audio and text sampling, accompanied by our very own layer called "Attentive Fusion". Th
    
[^43]: AutoChunk: 自动激活块用于内存高效的长序列推断

    AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])

    [http://arxiv.org/abs/2401.10652](http://arxiv.org/abs/2401.10652)

    AutoChunk是一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。

    

    大型深度学习模型在各种应用中取得了令人瞩目的性能。然而，它们对内存的大量需求，包括参数内存和激活内存，已经成为实际应用中的重大挑战。现有方法主要处理参数内存，对激活内存的重要性却被忽视了。特别是对于长输入序列，随着序列长度的增加，激活内存预计会经历显著的指数增长。在这个方法中，我们提出了AutoChunk，一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。所提出的系统通过多个阶段的优化生成块计划。在每个阶段，块搜索通过探索所有可能的块候选项，块选择通过识别最佳块进行。运行时，AutoChunk采用代码生成自动应用块策略。

    Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
    
[^44]: 使用停留信息进行大规模用户区域建模及COVID-19影响分析

    Area Modeling using Stay Information for Large-Scale Users and Analysis for Influence of COVID-19. (arXiv:2401.10648v1 [cs.LG])

    [http://arxiv.org/abs/2401.10648](http://arxiv.org/abs/2401.10648)

    本文提出了一种名为Area2Vec的新颖区域建模方法，通过模型化用户的位置数据对区域进行建模，从而更好地捕捉区域使用的变化。

    

    了解人们在城市中如何使用区域是一种宝贵的信息，在从市场营销到城市规划等各个领域都具有重要价值。区域的使用会随着时间的推移而发生变化，受到季节变化和大流行等各种事件的影响。在智能手机普及之前，这些数据是通过问卷调查来收集的。然而，从时间效率和成本效益来看，这种方法并不可持续。目前存在许多关于区域建模的研究，这些研究使用兴趣点（POI）或区域间移动数据等某种信息来描述区域。然而，由于POI数据与空间静态绑定，区域间移动数据忽略了区域内人们的行为，现有方法在捕捉区域使用变化方面并不足够。在本文中，我们提出了一种新颖的区域建模方法Area2Vec，受Word2Vec启发，该方法基于人们的位置数据对区域进行建模。

    Understanding how people use area in a city can be a valuable information in a wide range of fields, from marketing to urban planning. Area usage is subject to change over time due to various events including seasonal shifts and pandemics. Before the spread of smartphones, this data had been collected through questionnaire survey. However, this is not a sustainable approach in terms of time to results and cost. There are many existing studies on area modeling, which characterize an area with some kind of information, using Point of Interest (POI) or inter-area movement data. However, since POI is data that is statically tied to space, and inter-area movement data ignores the behavior of people within an area, existing methods are not sufficient in terms of capturing area usage changes. In this paper, we propose a novel area modeling method named Area2Vec, inspired by Word2Vec, which models areas based on people's location data. This method is based on the discovery that it is possible 
    
[^45]: 以高效数据标注赋能硬件网络：一种聚类联邦半监督学习的方法

    Empowering HWNs with Efficient Data Labeling: A Clustered Federated Semi-Supervised Learning Approach. (arXiv:2401.10646v1 [cs.NI])

    [http://arxiv.org/abs/2401.10646](http://arxiv.org/abs/2401.10646)

    设计了一种聚类联邦半监督学习框架，用于更真实的硬件网络场景，克服了设备缺乏准确标签的问题，提高了收敛速度和处理时间。

    

    聚类联邦多任务学习（CFL）已经引起了相当大的关注，作为一种有效的克服统计挑战的策略，特别是在处理多用户之间存在非独立同分布（non IID）数据时。然而，现有的CFL研究大多基于一个不切实际的假设，即设备可以访问准确的真实标签。这个假设在分层无线网络（HWNs）中尤为有问题，其中边缘网络包含大量未标记的数据，导致收敛速度较慢和处理时间增加，特别是在两层模型聚合时。为了解决这些问题，我们引入了一种新颖的框架，聚类联邦半监督学习（CFSL），用于更真实的HWN场景。我们的方法利用了一种表现最佳的专门模型算法，其中每个设备被分配一个专门的模型，该模型在生成准确标签方面非常熟练。

    Clustered Federated Multitask Learning (CFL) has gained considerable attention as an effective strategy for overcoming statistical challenges, particularly when dealing with non independent and identically distributed (non IID) data across multiple users. However, much of the existing research on CFL operates under the unrealistic premise that devices have access to accurate ground truth labels. This assumption becomes especially problematic in hierarchical wireless networks (HWNs), where edge networks contain a large amount of unlabeled data, resulting in slower convergence rates and increased processing times, particularly when dealing with two layers of model aggregation. To address these issues, we introduce a novel framework, Clustered Federated Semi-Supervised Learning (CFSL), designed for more realistic HWN scenarios. Our approach leverages a best-performing specialized model algorithm, wherein each device is assigned a specialized model that is highly adept at generating accura
    
[^46]: 基于深度学习的车辆再识别综合调查：模型、数据集和挑战

    A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges. (arXiv:2401.10643v1 [cs.CV])

    [http://arxiv.org/abs/2401.10643](http://arxiv.org/abs/2401.10643)

    本文通过全面调查了基于深度学习的车辆再识别方法，包括分类、数据集、评估标准以及未来的挑战和研究方向

    

    车辆再识别（ReID）旨在将来自分布在不同交通环境的摄像头网络中的车辆图像进行关联。在车辆中心技术范畴中，该任务具有重要意义，在部署智能交通系统（ITS）和推进智慧城市倡议方面发挥着关键作用。近年来，深度学习的快速发展显著推动了车辆 ReID 技术的演进。因此，对基于深度学习的车辆再识别方法进行全面调查已变得迫切且不可避免。本文深入探讨了应用于车辆 ReID 的深度学习技术，概述了这些方法的分类，包括有监督和无监督方法，深入研究了这些分类中的现有研究，介绍了数据集和评估标准，并勾勒了未来的挑战和潜在的研究方向

    Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments. This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives. Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years. Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable. This paper extensively explores deep learning techniques applied to vehicle ReID. It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research dire
    
[^47]: 在医学成像中实现普适的无监督异常检测

    Towards Universal Unsupervised Anomaly Detection in Medical Imaging. (arXiv:2401.10637v1 [eess.IV])

    [http://arxiv.org/abs/2401.10637](http://arxiv.org/abs/2401.10637)

    本文提出了一种无监督异常检测方法，通过创建伪健康重建实现对更广泛病变范围的检测，并在不同的成像模态中展示了卓越的性能。

    

    医学成像数据的复杂性日益增加，强调了需要先进的异常检测方法来自动识别各种病理。目前的方法在捕捉广泛异常范围方面存在挑战，通常限制其在脑部扫描中特定病变类型的使用。为了解决这个挑战，我们引入了一种新颖的无监督方法，称为“反向自编码器（RA）”，旨在创建逼真的伪健康重建，从而实现对更广泛病变范围的检测。我们评估了这种方法在包括脑部磁共振成像（MRI）、儿童腕关节X射线和胸部X射线在内的各种成像模态中，并且证明相比现有最先进方法，具有优越的异常检测性能。我们的无监督异常检测方法可以通过识别更广泛范围的未知病理来提高医学成像的诊断准确性。我们的代码公开可用: \url{htt

    The increasing complexity of medical imaging data underscores the need for advanced anomaly detection methods to automatically identify diverse pathologies. Current methods face challenges in capturing the broad spectrum of anomalies, often limiting their use to specific lesion types in brain scans. To address this challenge, we introduce a novel unsupervised approach, termed \textit{Reversed Auto-Encoders (RA)}, designed to create realistic pseudo-healthy reconstructions that enable the detection of a wider range of pathologies. We evaluate the proposed method across various imaging modalities, including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray, and chest X-ray, and demonstrate superior performance in detecting anomalies compared to existing state-of-the-art methods. Our unsupervised anomaly detection approach may enhance diagnostic accuracy in medical imaging by identifying a broader range of unknown pathologies. Our code is publicly available at: \url{htt
    
[^48]: 部分已知因果图上的干预公平性: 一种受限优化方法

    Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach. (arXiv:2401.10632v1 [cs.LG])

    [http://arxiv.org/abs/2401.10632](http://arxiv.org/abs/2401.10632)

    本文提出了一个框架，用于在部分已知的真实因果图情况下实现因果公平性。通过使用部分有向无环图（PDAG）建模和衡量因果公平性，并通过受限优化问题平衡公平性和准确性。

    

    公平机器学习旨在防止基于敏感属性（如性别和种族）对个体或子群体进行歧视。近年来，因果推断方法在公平机器学习中的应用越来越多，用于通过因果效应来衡量不公平性。然而，当前方法假设已知真实的因果图，而这在实际应用中往往不成立。为了解决这个限制，本文提出了一个框架，用于在部分已知的真实因果图情况下实现因果公平性。所提出的方法涉及使用部分有向无环图（PDAG）来建模公平预测，具体来说，是从观测数据和领域知识中学习的一类因果有向无环图。PDAG用于衡量因果公平性，并通过一个受限优化问题来平衡公平性和准确性。模拟和真实世界数据集的结果表明...

    Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current methods assume that the true causal graph is given, which is often not true in real-world applications. To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known. The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph (PDAG), specifically, a class of causal DAGs that can be learned from observational data combined with domain knowledge. The PDAG is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy. Results on both simulated and real-world datasets demonstrate th
    
[^49]: 具有平滑聚类的多角形自编码器用于流体简化建模

    Polytopic Autoencoders with Smooth Clustering for Reduced-order Modelling of Flows. (arXiv:2401.10620v1 [cs.LG])

    [http://arxiv.org/abs/2401.10620](http://arxiv.org/abs/2401.10620)

    我们提出了一种多角形自编码器架构，包括一个轻量级的非线性编码器、一个凸组合解码器和一个平滑聚类网络。该模型能够确保所有重建状态都位于一个多面体内，并提供了一个衡量多面体质量的指标。与传统的POD方法相比，我们的模型能够在保证重建误差可接受的情况下，使用最少的凸坐标进行多面体线性参数变化系统的建模。数值实验验证了我们模型的性能。

    

    随着神经网络的进步，应用自编码器进行简化模型的研究出版物在数量和种类方面显著增加。我们提出了一种多角形自编码器架构，包括一个轻量级的非线性编码器、一个凸组合解码器和一个平滑聚类网络。通过多项证明支持的模型架构确保所有重建状态都位于多面体内，并附带一个指示构建的多面体质量的指标，称为多面体误差。此外，它为多面体线性参数变化系统提供了最小数量的凸坐标，同时实现了与POD相比可接受的重建误差。为了验证我们提出的模型，我们进行了涉及不可压缩Navier-Stokes方程的两种流动场景的仿真。数值结果证明了模型的保证性质。

    With the advancement of neural networks, there has been a notable increase, both in terms of quantity and variety, in research publications concerning the application of autoencoders to reduced-order models. We propose a polytopic autoencoder architecture that includes a lightweight nonlinear encoder, a convex combination decoder, and a smooth clustering network. Supported by several proofs, the model architecture ensures that all reconstructed states lie within a polytope, accompanied by a metric indicating the quality of the constructed polytopes, referred to as polytope error. Additionally, it offers a minimal number of convex coordinates for polytopic linear-parameter varying systems while achieving acceptable reconstruction errors compared to proper orthogonal decomposition (POD). To validate our proposed model, we conduct simulations involving two flow scenarios with the incompressible Navier-Stokes equation. Numerical results demonstrate the guaranteed properties of the model, l
    
[^50]: ZnTrack -- 数据即代码

    ZnTrack -- Data as Code. (arXiv:2401.10603v1 [cs.SE])

    [http://arxiv.org/abs/2401.10603](http://arxiv.org/abs/2401.10603)

    ZnTrack是一个Python驱动的数据版本控制工具，通过简化大型数据集为Python脚本的形式，提供了一个以数据为代码的概念，实现了对参数跟踪、工作流设计以及数据存储和共享的用户友好界面。

    

    过去十年来，计算机领域取得了巨大的突破，而且没有迹象表明这种趋势会在短期内减慢。机器学习、大规模计算资源和增加的行业关注导致了对数据管理、模拟和模型生成的计算机驱动解决方案的投资增加。然而，随着计算能力的增长，数据的扩大也带来了数据存储、共享和追踪的复杂性。在这项工作中，我们介绍了一个名为ZnTrack的Python驱动数据版本控制工具。ZnTrack基于已建立的版本控制系统，提供了一个用户友好且易于使用的界面，用于跟踪实验中的参数、设计工作流、以及存储和共享数据。通过将大型数据集简化为一个简单的Python脚本，产生了“数据即代码”的概念，这是本文所介绍的工作的核心组成部分，也是计算时代继续演变的一个无疑重要的概念。ZnTrack提供了一种方法，可以将数据当作代码处理。

    The past decade has seen tremendous breakthroughs in computation and there is no indication that this will slow any time soon. Machine learning, large-scale computing resources, and increased industry focus have resulted in rising investments in computer-driven solutions for data management, simulations, and model generation. However, with this growth in computation has come an even larger expansion of data and with it, complexity in data storage, sharing, and tracking. In this work, we introduce ZnTrack, a Python-driven data versioning tool. ZnTrack builds upon established version control systems to provide a user-friendly and easy-to-use interface for tracking parameters in experiments, designing workflows, and storing and sharing data. From this ability to reduce large datasets to a simple Python script emerges the concept of Data as Code, a core component of the work presented here and an undoubtedly important concept as the age of computation continues to evolve. ZnTrack offers an
    
[^51]: Adversarially Robust Signed Graph Contrastive Learning from Balance Augmentation（从平衡增强中提取对抗性鲁棒的带符号图对比学习）

    Adversarially Robust Signed Graph Contrastive Learning from Balance Augmentation. (arXiv:2401.10590v1 [cs.LG])

    [http://arxiv.org/abs/2401.10590](http://arxiv.org/abs/2401.10590)

    本研究提出了一种名为BA-SGCL的鲁棒SGNN框架，通过结合图对比学习原则和平衡增强技术，解决了带符号图对抗性攻击中平衡相关信息不可逆的问题。

    

    带符号图由边和符号组成，可以分为结构信息和平衡相关信息。现有的带符号图神经网络（SGNN）通常依赖于平衡相关信息来生成嵌入。然而，最近的对抗性攻击对平衡相关信息产生了不利影响。类似于结构学习可以恢复无符号图，通过改进被污染图的平衡度，可以将平衡学习应用于带符号图。然而，这种方法面临着“平衡相关信息的不可逆性”挑战-尽管平衡度得到改善，但恢复的边可能不是最初受到攻击影响的边，导致防御效果差。为了解决这个挑战，我们提出了一个鲁棒的SGNN框架，称为平衡增强带符号图对比学习（BA-SGCL），它将图对比学习原则与平衡增强相结合。

    Signed graphs consist of edges and signs, which can be separated into structural information and balance-related information, respectively. Existing signed graph neural networks (SGNNs) typically rely on balance-related information to generate embeddings. Nevertheless, the emergence of recent adversarial attacks has had a detrimental impact on the balance-related information. Similar to how structure learning can restore unsigned graphs, balance learning can be applied to signed graphs by improving the balance degree of the poisoned graph. However, this approach encounters the challenge "Irreversibility of Balance-related Information" - while the balance degree improves, the restored edges may not be the ones originally affected by attacks, resulting in poor defense effectiveness. To address this challenge, we propose a robust SGNN framework called Balance Augmented-Signed Graph Contrastive Learning (BA-SGCL), which combines Graph Contrastive Learning principles with balance augmentati
    
[^52]: PuriDefense：用于防御黑盒基于查询的攻击的随机局部隐式对抗净化

    PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])

    [http://arxiv.org/abs/2401.10586](http://arxiv.org/abs/2401.10586)

    PuriDefense是一种高效的防御机制，通过使用轻量级净化模型进行随机路径净化，减缓基于查询的攻击的收敛速度，并有效防御黑盒基于查询的攻击。

    

    黑盒基于查询的攻击对机器学习作为服务系统构成重大威胁，因为它们可以生成对抗样本而不需要访问目标模型的架构和参数。传统的防御机制，如对抗训练、梯度掩盖和输入转换，要么带来巨大的计算成本，要么损害非对抗输入的测试准确性。为了应对这些挑战，我们提出了一种高效的防御机制PuriDefense，在低推理成本的级别上使用轻量级净化模型的随机路径净化。这些模型利用局部隐式函数并重建自然图像流形。我们的理论分析表明，这种方法通过将随机性纳入净化过程来减缓基于查询的攻击的收敛速度。对CIFAR-10和ImageNet的大量实验验证了我们提出的净化器防御的有效性。

    Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
    
[^53]: 鲁棒的多模态密度估计

    Robust Multi-Modal Density Estimation. (arXiv:2401.10566v1 [cs.LG])

    [http://arxiv.org/abs/2401.10566](http://arxiv.org/abs/2401.10566)

    本文提出了一种名为ROME的鲁棒多模态密度估计方法，该方法利用聚类将多模态样本集分割成多个单模态样本集，并通过简单的KDE估计来估计整体分布。这种方法解决了多模态、非正态和高相关分布估计的挑战。

    

    多模态概率预测模型的发展引发了对综合评估指标的需求。虽然有几个指标可以表征机器学习模型的准确性（例如，负对数似然、Jensen-Shannon散度），但这些指标通常作用于概率密度上。因此，将它们应用于纯粹基于样本的预测模型需要估计底层密度函数。然而，常见的方法如核密度估计（KDE）已被证明在鲁棒性方面存在不足，而更复杂的方法在多模态估计问题中尚未得到评估。在本文中，我们提出了一种非参数的密度估计方法ROME（RObust Multi-modal density Estimator），它解决了估计多模态、非正态和高相关分布的挑战。ROME利用聚类将多模态样本集分割成多个单模态样本集，然后结合简单的KDE估计来得到总体的估计结果。

    Development of multi-modal, probabilistic prediction models has lead to a need for comprehensive evaluation metrics. While several metrics can characterize the accuracy of machine-learned models (e.g., negative log-likelihood, Jensen-Shannon divergence), these metrics typically operate on probability densities. Applying them to purely sample-based prediction models thus requires that the underlying density function is estimated. However, common methods such as kernel density estimation (KDE) have been demonstrated to lack robustness, while more complex methods have not been evaluated in multi-modal estimation problems. In this paper, we present ROME (RObust Multi-modal density Estimator), a non-parametric approach for density estimation which addresses the challenge of estimating multi-modal, non-normal, and highly correlated distributions. ROME utilizes clustering to segment a multi-modal set of samples into multiple uni-modal ones and then combines simple KDE estimates obtained for i
    
[^54]: OrchMoE：具有任务-技能协同效应的高效多适配器学习

    OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])

    [http://arxiv.org/abs/2401.10559](http://arxiv.org/abs/2401.10559)

    OrchMoE通过利用模块化技能架构和自动任务识别，提升了参数效率微调领域的性能，实现了对多任务学习的重大进展。

    

    我们通过创新的多适配器方法OrchMoE推进了参数效率微调（PEFT）领域，利用模块化技能架构增强神经网络的前向传递。与依赖显式任务识别输入的先前模型不同，OrchMoE自动识别任务类别，简化学习过程。这是通过一个整合机制实现的，包括自动任务分类模块和任务-技能分配模块，共同推断任务特定的分类并调整技能分配矩阵。我们在“超自然指令”数据集上进行了广泛的评估，该数据集包含1,600个多样的指令任务，结果表明OrchMoE在性能和样本利用效率方面明显优于可比的多适配器基线，并且在相同参数限制下运行。这些发现表明，OrchMoE在多任务学习方面取得了重大进展。

    We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le
    
[^55]: 不完整多视图数据的统一补全和特征选择学习

    Unified View Imputation and Feature Selection Learning for Incomplete Multi-view Data. (arXiv:2401.10549v1 [cs.LG])

    [http://arxiv.org/abs/2401.10549](http://arxiv.org/abs/2401.10549)

    提出了一种新的多视图无监督特征选择方法UNIFIER，可以直接处理存在不完整多视图数据的情况，通过利用特征选择获取的局部结构信息引导补全过程，从而改善特征选择性能，并且充分利用了特征空间的局部性质。

    

    尽管多视图无监督特征选择（MUFS）是一种在机器学习中降维的有效技术，但现有方法无法直接处理存在不完整多视图数据的情况，其中某些视图中会有一些样本丢失。这些方法应首先对缺失的数据应用预定值进行填补，然后在完整的数据集上进行特征选择。将补全和特征选择过程分离无法充分利用潜在的协同作用，即通过特征选择获得的局部结构信息可以引导补全过程，从而进一步改善特征选择性能。此外，先前的方法仅关注于利用样本的局部结构信息，而忽视了特征空间的固有局部性质。为了解决这些问题，提出了一种新的MUFS方法，称为统一视图补全和特征选择学习（UNIFIER）。

    Although multi-view unsupervised feature selection (MUFS) is an effective technology for reducing dimensionality in machine learning, existing methods cannot directly deal with incomplete multi-view data where some samples are missing in certain views. These methods should first apply predetermined values to impute missing data, then perform feature selection on the complete dataset. Separating imputation and feature selection processes fails to capitalize on the potential synergy where local structural information gleaned from feature selection could guide the imputation, thereby improving the feature selection performance in turn. Additionally, previous methods only focus on leveraging samples' local structure information, while ignoring the intrinsic locality of the feature space. To tackle these problems, a novel MUFS method, called UNified view Imputation and Feature selectIon lEaRning (UNIFIER), is proposed. UNIFIER explores the local structure of multi-view data by adaptively le
    
[^56]: PhoGAD: 基于图形的异常行为检测与持久同调优化

    PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization. (arXiv:2401.10547v1 [cs.LG])

    [http://arxiv.org/abs/2401.10547](http://arxiv.org/abs/2401.10547)

    PhoGAD是一种基于图形的异常行为检测框架，通过持久同调优化来澄清行为边界，设计相邻边权重以减轻局部异质性的影响，并解决噪声问题。

    

    各种有害的在线行为，从网络攻击到匿名流量和垃圾邮件，严重干扰了网络的正常运行。由于网络行为的发件人-收件人本质，常常使用基于图形的框架来检测异常行为。然而，在现实场景中，正常行为和异常行为之间的边界往往模糊不清。图的局部异质性干扰了检测过程，而基于节点或边的现有方法在表示结果中引入了不必要的噪声，从而影响了检测的有效性。为了解决这些问题，我们提出了PhoGAD，一种基于图形的异常检测框架。PhoGAD利用持久同调优化来澄清行为边界。在此基础上，设计了相邻边的权重以减轻局部异质性的影响。随后，为了解决噪声问题，我们进行了形式分析并提出了一个解耦的重新...

    A multitude of toxic online behaviors, ranging from network attacks to anonymous traffic and spam, have severely disrupted the smooth operation of networks. Due to the inherent sender-receiver nature of network behaviors, graph-based frameworks are commonly used for detecting anomalous behaviors. However, in real-world scenarios, the boundary between normal and anomalous behaviors tends to be ambiguous. The local heterophily of graphs interferes with the detection, and existing methods based on nodes or edges introduce unwanted noise into representation results, thereby impacting the effectiveness of detection. To address these issues, we propose PhoGAD, a graph-based anomaly detection framework. PhoGAD leverages persistent homology optimization to clarify behavioral boundaries. Building upon this, the weights of adjacent edges are designed to mitigate the effects of local heterophily. Subsequently, to tackle the noise problem, we conduct a formal analysis and propose a disentangled re
    
[^57]: I-SplitEE：在具有提前退出功能的分割计算DNN中的图像分类

    I-SplitEE: Image classification in Split Computing DNNs with Early Exits. (arXiv:2401.10541v1 [cs.LG])

    [http://arxiv.org/abs/2401.10541](http://arxiv.org/abs/2401.10541)

    本文提出了一种创新的统一方法，将提前退出和分割计算相结合，以应对资源受限设备上的图像分类问题，并引入了I-SplitEE算法来适应不同的环境失真。

    

    深度神经网络（DNN）的最新进展源于其在各个领域的出色表现。然而，其本身庞大的体积阻碍了在资源受限设备（如边缘、移动和物联网平台）上部署这些网络。从部分云计算卸载（分割计算）到在DNN层中集成提前退出，出现了各种策略。我们的工作提出了一种创新的统一方法，将提前退出与分割计算相结合。我们确定了“分割层”，即用于边缘设备计算的DNN的最佳深度，并根据准确性、计算效率和通信成本决定是在边缘设备进行推理还是在云端进行推理。此外，图像分类面临着影响因素如白天时间、照明和天气等多样的环境失真。为了适应这些失真，我们引入了I-SplitEE，这是一种适用于缺少基准的场景和具有连续性的在线无监督算法。

    The recent advances in Deep Neural Networks (DNNs) stem from their exceptional performance across various domains. However, their inherent large size hinders deploying these networks on resource-constrained devices like edge, mobile, and IoT platforms. Strategies have emerged, from partial cloud computation offloading (split computing) to integrating early exits within DNN layers. Our work presents an innovative unified approach merging early exits and split computing. We determine the 'splitting layer', the optimal depth in the DNN for edge device computations, and whether to infer on edge device or be offloaded to the cloud for inference considering accuracy, computational efficiency, and communication costs. Also, Image classification faces diverse environmental distortions, influenced by factors like time of day, lighting, and weather. To adapt to these distortions, we introduce I-SplitEE, an online unsupervised algorithm ideal for scenarios lacking ground truths and with sequentia
    
[^58]: 自然语言处理中的“殖民冲动”: 孟加拉情感分析工具及其基于身份的偏见的审查

    The "Colonial Impulse" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases. (arXiv:2401.10535v1 [cs.CL])

    [http://arxiv.org/abs/2401.10535](http://arxiv.org/abs/2401.10535)

    本研究审查了在孟加拉社群中经历殖民主义影响的情感分析工具，发现它们可能存在基于身份的偏见，并提出了警示。

    

    尽管殖民主义在社会历史上对人们的身份产生了各种影响，但这些殖民的价值观和偏见仍通过社会技术系统得到了延续。一类社会技术系统，情感分析工具，也可能延续殖民的价值观和偏见，然而，对这些工具可能如何与殖民主义的延续相关联的关注却较少，尽管它们经常被用来指导各种实践（例如，内容管理）。在本文中，我们探讨了在经历和继续经历殖民主义影响的孟加拉社群背景下，情感分析工具可能存在的偏见。我们根据当地孟加拉社群中受殖民主义影响最大的身份类别，重点关注了性别、宗教和国籍。我们对在Python包索引(PyPI)和GitHub上提供的所有孟加拉情感分析工具进行了算法审查。尽管语义内容相似，但我们发现这些工具在处理与孟加拉社群有关的情感时存在偏见。

    While colonization has sociohistorically impacted people's identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems--sentiment analysis tools--can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities that have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and
    
[^59]: Mementos: 一种针对图像序列的多模态大型语言模型推理的综合基准测试

    Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])

    [http://arxiv.org/abs/2401.10529](http://arxiv.org/abs/2401.10529)

    Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。

    

    多模态大型语言模型（MLLMs）在处理各种视觉语言任务方面展示了高超的能力。然而，目前的MLLM基准测试主要用于评估基于单个图像的静态信息的推理能力，而现代MLLM在从图像序列中进行推断的能力，在理解不断变化的世界方面的重要性却被较少研究。为了解决这一挑战，本文引入了一个新的基准测试Mementos，用于评估MLLM的序列图像推理能力。Mementos包括4761个具有不同长度的多样的图像序列。我们还采用了GPT-4辅助方法来评估MLLM的推理性能。通过对Mementos中包括GPT-4V和Gemini在内的九个最新MLLM进行仔细评估，我们发现它们在准确描述所给图像序列的动态信息方面存在困难，往往导致对象及其对应行为的错误描述或错觉。

    Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
    
[^60]: FARe: 在基于ReRAM的PIM加速器上进行故障感知的GNN训练

    FARe: Fault-Aware GNN Training on ReRAM-based PIM Accelerators. (arXiv:2401.10522v1 [cs.AR])

    [http://arxiv.org/abs/2401.10522](http://arxiv.org/abs/2401.10522)

    本文提出了一种故障感知框架FARe，用于在基于ReRAM的PIM加速器上训练GNN。实验结果显示，FARe框架相较于其他方法在准确性和时间开销上都具有优势。

    

    基于电阻式随机存储器(ReRAM)的处理内存(PIM)架构是在边缘平台上训练图神经网络(GNN)的一种有吸引力的解决方案。然而，不成熟的制造工艺和有限的写入耐久性使得ReRAM容易发生硬件故障，从而限制了它们在GNN训练中的广泛应用。此外，现有的容错解决方案在存在故障时无法有效地训练GNN。在本文中，我们提出了一种称为FARe的故障感知框架，可以在GNN训练过程中减轻故障的影响。实验结果表明，与无故障情况相比，FARe框架可以将GNN测试准确性提高47.6%，并且时间开销仅为1%。

    Resistive random-access memory (ReRAM)-based processing-in-memory (PIM) architecture is an attractive solution for training Graph Neural Networks (GNNs) on edge platforms. However, the immature fabrication process and limited write endurance of ReRAMs make them prone to hardware faults, thereby limiting their widespread adoption for GNN training. Further, the existing fault-tolerant solutions prove inadequate for effectively training GNNs in the presence of faults. In this paper, we propose a fault-aware framework referred to as FARe that mitigates the effect of faults during GNN training. FARe outperforms existing approaches in terms of both accuracy and timing overhead. Experimental results demonstrate that FARe framework can restore GNN test accuracy by 47.6% on faulty ReRAM hardware with a ~1% timing overhead compared to the fault-free counterpart.
    
[^61]: 没有观测数据的区域的时空预测

    Spatial-temporal Forecasting for Regions without Observations. (arXiv:2401.10518v1 [cs.LG])

    [http://arxiv.org/abs/2401.10518](http://arxiv.org/abs/2401.10518)

    本文研究了在没有历史观测数据的区域进行时空预测的问题，并提出了一种名为STSM的模型，采用对比学习方法。

    

    时空预测在许多现实应用中起着重要作用，如交通预测、空气污染物预测、人群流量预测等。现有的时空预测模型采用数据驱动方法，并且严重依赖数据的可用性。当数据不完整时，这些模型的准确性会受到影响，而实际情况中由于部署和维护传感器收集数据的高昂成本，数据不完整是很常见的。一些最近的研究尝试解决数据不完整的问题。它们通常假设在一个感兴趣区域内要么有一段时间的某些数据可用，要么在一些地点有一些数据可用。本文进一步研究了在没有任何历史观测的感兴趣区域进行时空预测的问题，以解决区域发展不均衡、传感器逐步部署或缺乏开放数据等情况。我们提出了一种名为STSM的模型来完成这个任务。该模型采用对比学习方法。

    Spatial-temporal forecasting plays an important role in many real-world applications, such as traffic forecasting, air pollutant forecasting, crowd-flow forecasting, and so on. State-of-the-art spatial-temporal forecasting models take data-driven approaches and rely heavily on data availability. Such models suffer from accuracy issues when data is incomplete, which is common in reality due to the heavy costs of deploying and maintaining sensors for data collection. A few recent studies attempted to address the issue of incomplete data. They typically assume some data availability in a region of interest either for a short period or at a few locations. In this paper, we further study spatial-temporal forecasting for a region of interest without any historical observations, to address scenarios such as unbalanced region development, progressive deployment of sensors or lack of open data. We propose a model named STSM for the task. The model takes a contrastive learning-based approach to 
    
[^62]: 扩展状态-奖励空间的情节式强化学习

    Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])

    [http://arxiv.org/abs/2401.10516](http://arxiv.org/abs/2401.10516)

    通过引入扩展的状态-奖励空间，我们提出了一种高效的情节式强化学习（DRL）框架，可以改善DRL中状态与奖励空间之间的不对齐问题，从而提高值的估计准确性和策略性能。

    

    借助深度神经网络的力量，深度强化学习（DRL）在游戏、医疗保健和自动驾驶等各个领域取得了巨大的经验成功。尽管取得了这些进展，但是由于有效策略需要大量的环境样本，DRL仍被认为是数据效率低下的。最近，基于情节控制（EC）的无模型DRL方法通过从情节记忆中回顾过去的经验实现了样本效率。然而，现有的基于EC的方法由于忽略了利用（过去的）检索状态的广泛信息，存在状态和奖励空间之间的潜在不对齐的限制，这可能导致值估计不准确和策略性能下降。为了解决这个问题，我们引入了一种具有扩展状态-奖励空间的高效EC型DRL框架，其中用作输入的扩展状态和用于训练的扩展奖励都包含历史和当前信息。

    Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current informa
    
[^63]: 天作之合：大型语言模型与进化算法的结合

    A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])

    [http://arxiv.org/abs/2401.10510](http://arxiv.org/abs/2401.10510)

    大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。

    

    预训练的大型语言模型（LLMs）在生成创造性的自然文本方面具有强大的能力。进化算法（EAs）可以发现复杂实际问题的多样解决方案。本文通过比较文本序列生成和进化的共同特点和方向性，阐述了LLMs与EAs之间的强大一致性，包括多个一对一的核心特征：标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化。在这种一致性视角下，分析了现有的耦合研究，包括进化微调和LLM增强型EAs。借助这些洞见，我们概述了未来在LLMs和EAs耦合方面的基本研究路线，并突出了其中的关键挑战。

    Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
    
[^64]: 条件熵下的因果层析

    Causal Layering via Conditional Entropy. (arXiv:2401.10495v1 [cs.LG])

    [http://arxiv.org/abs/2401.10495](http://arxiv.org/abs/2401.10495)

    本文提出了一种通过条件熵Oracle来恢复图层析的方法，用于离散分布情况下的因果发现。算法通过比较节点的条件熵和噪声的无条件熵，通过删除源或汇点来实现节点的分离。算法具有可证明的正确性和二次时间复杂度。

    

    因果发现旨在从可观测到的数据中恢复关于未观测到的因果图的信息。层析是对变量进行排序，将因果放在效应之前。在本文中，我们提供了通过访问条件熵Oracle来恢复图层析的方法，当分布是离散的时候。我们的算法通过不断从图中删除源或汇点来工作。在适当的假设和条件下，我们可以通过比较它们的条件熵与噪声的无条件熵来将源或汇点与其余节点分离开来。我们的算法在最坏情况下的时间复杂度是二次的，并且经过证明是正确的。主要的假设是忠实性和单射噪声，以及已知噪声熵或沿着有向路径弱单调递增的噪声熵之一。此外，我们需要忠实性的一个非常温和的扩展，或者严格单调递增的噪声熵，或者扩展到无限值。

    Causal discovery aims to recover information about an unobserved causal graph from the observable data it generates. Layerings are orderings of the variables which place causes before effects. In this paper, we provide ways to recover layerings of a graph by accessing the data via a conditional entropy oracle, when distributions are discrete. Our algorithms work by repeatedly removing sources or sinks from the graph. Under appropriate assumptions and conditioning, we can separate the sources or sinks from the remainder of the nodes by comparing their conditional entropy to the unconditional entropy of their noise. Our algorithms are provably correct and run in worst-case quadratic time. The main assumptions are faithfulness and injective noise, and either known noise entropies or weakly monotonically increasing noise entropies along directed paths. In addition, we require one of either a very mild extension of faithfulness, or strictly monotonically increasing noise entropies, or expan
    
[^65]: 基于Auto-Encoder的非线性模型降维算法用于运算符学习的泛化误差保证

    Generalization Error Guaranteed Auto-Encoder-Based Nonlinear Model Reduction for Operator Learning. (arXiv:2401.10490v1 [cs.LG])

    [http://arxiv.org/abs/2401.10490](http://arxiv.org/abs/2401.10490)

    本文提出了一种基于Auto-Encoder的非线性模型降维算法，用于运算符学习，并且通过数值实验验证了其准确学习非线性偏微分方程的解算符的能力。

    

    在科学和工程中，许多物理过程可以自然地由无限维函数空间之间的运算符表示。在这个背景下，运算符学习的问题是从经验数据中提取这些物理过程，由于数据的无限或高维度，这是具有挑战性的。解决这个挑战的一个重要组成部分是模型降维，它可以减少数据的维度和问题的大小。在本文中，我们利用模型降维中的低维非线性结构，通过研究基于Auto-Encoder的神经网络(AENet)。AENet首先学习输入数据的潜变量，然后学习从这些潜变量到相应输出数据的转换。我们的数值实验验证了AENet准确学习非线性偏微分方程的解算符的能力。此外，我们建立了一个数学和统计估计理论，分析了泛化误差。

    Many physical processes in science and engineering are naturally represented by operators between infinite-dimensional function spaces. The problem of operator learning, in this context, seeks to extract these physical processes from empirical data, which is challenging due to the infinite or high dimensionality of data. An integral component in addressing this challenge is model reduction, which reduces both the data dimensionality and problem size. In this paper, we utilize low-dimensional nonlinear structures in model reduction by investigating Auto-Encoder-based Neural Network (AENet). AENet first learns the latent variables of the input data and then learns the transformation from these latent variables to corresponding output data. Our numerical experiments validate the ability of AENet to accurately learn the solution operator of nonlinear partial differential equations. Furthermore, we establish a mathematical and statistical estimation theory that analyzes the generalization e
    
[^66]: 基于预算的在线模型选择和调优通过联邦学习

    Budgeted Online Model Selection and Fine-Tuning via Federated Learning. (arXiv:2401.10478v1 [cs.LG])

    [http://arxiv.org/abs/2401.10478](http://arxiv.org/abs/2401.10478)

    本文提出了一种基于预算的在线模型选择和调优框架，通过联邦学习实现。该框架允许在内存受限的边缘设备上进行模型选择，并实现了客户端和服务器的合作来对模型进行适应非平稳环境的调优。

    

    在线模型选择涉及从候选模型集合中选择一个模型“即时”对数据流进行预测。候选模型的选择对性能有重要影响。虽然使用更大的候选模型集合自然在模型选择中具有更灵活性，但在预测任务在内存有限的边缘设备上执行时，这可能是不可行的。面临这一挑战，本文提出了一种在线联邦模型选择框架，其中一组学习者（客户端）与具有足够内存的服务器进行交互，使得服务器存储所有候选模型。然而，每个客户端只选择存储可以适配其内存的模型子集，并使用其中一个存储模型执行自己的预测任务。此外，使用所提出的算法，客户端和服务器合作对模型进行微调以调整到非平稳环境。

    Online model selection involves selecting a model from a set of candidate models 'on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis 
    
[^67]: LDReg: 本地维度正则化的自监督学习

    LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])

    [http://arxiv.org/abs/2401.10474](http://arxiv.org/abs/2401.10474)

    本文提出了一种叫做LDReg的本地维度正则化方法，用于解决自监督学习中的维度坍缩问题。通过增加局部内在维度，LDReg能够改善表示的性能。

    

    通过自监督学习（SSL）学习的表示可能容易出现维度坍缩，其中学习的表示子空间维度极低，因此无法表示完整的数据分布和模态。维度坍缩也被称为“填充不足”现象，是下游任务性能下降的主要原因之一。之前的工作在全局层面上研究了SSL的维度坍缩问题。在本文中，我们证明表示可以在全局上覆盖高维空间，但在局部上会坍缩。为了解决这个问题，我们提出了一种称为“本地维度正则化（LDReg）”的方法。我们的公式是基于Fisher-Rao度量的推导，用于比较和优化每个数据点在渐进小半径处的局部距离分布。通过增加局部内在维度，我们通过一系列实验证明LDReg可以改善表示。

    Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
    
[^68]: 使用对比学习学习混合整数规划的后门

    Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])

    [http://arxiv.org/abs/2401.10467](http://arxiv.org/abs/2401.10467)

    本论文提出了使用对比学习方法来学习混合整数规划的后门，通过收集用于训练的后门并训练图注意力网络模型来预测后门，取得了比Gurobi和先前模型更好的性能改进。

    

    许多现实世界中的问题可以有效地建模为混合整数规划（MIP）并使用分支定界方法进行求解。先前的研究表明存在MIP后门，即一小组变量，如果优先在可能的情况下在它们上进行分支，则可以加快运行时间。然而，寻找能提高运行时间的高质量后门仍是一个未解决的问题。先前的工作通过排名学习估计随机采样的后门相对求解器速度，然后决定是否使用。本文中，我们利用蒙特卡洛树搜索方法收集用于训练的后门，而不是依赖随机采样，并且采用对比学习框架训练图注意力网络模型来预测后门。我们的方法在四个常见的MIP问题领域上进行评估，表现出对比Gurobi和先前模型的性能改进。

    Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
    
[^69]: 从理解的角度探讨语言模型的关键数据规模

    Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])

    [http://arxiv.org/abs/2401.10463](http://arxiv.org/abs/2401.10463)

    本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。

    

    我们探讨了语言模型中的关键数据规模，这是从快速记忆到缓慢泛化的一个基本转变阈值。我们将这个相变形式化为Grokking配置下的数据效率假说，并确定了语言模型训练动力学中的数据不足、充足和过剩阶段。我们通过重新调整初始化和权重衰减，开发出了一种Grokking配置，稳定地在简化的语言模型上重现了Grokking。我们表明只有当语言模型达到关键大小时才会发生泛化。我们分析了样本级和模型级的Grokking，验证了提出的数据效率假说。我们的实验揭示了语言数据集的关键数据集大小处发生的更平滑的相变。随着模型大小的增加，这个临界点也变得更大，这表明更大的模型需要更多的数据。我们的研究结果加深了对语言模型训练的理解，提供了一种新颖的视角。

    We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
    
[^70]: 高质量语音合成的超轻型神经差分DSP声码器

    Ultra-lightweight Neural Differential DSP Vocoder For High Quality Speech Synthesis. (arXiv:2401.10460v1 [cs.SD])

    [http://arxiv.org/abs/2401.10460](http://arxiv.org/abs/2401.10460)

    本文提出了一种超轻型差分DSP声码器，通过联合优化的声学模型和DSP声码器实现高质量的语音合成，而无需提取声道频谱特征。该模型在音频质量上接近神经声码器，且性能高效。

    

    神经声码器模拟原始音频波形并合成高质量音频，但即使是高效率的声码器，如MB-MelGAN和LPCNet，也无法在低端设备（如智能眼镜）上实时运行。基于纯数字信号处理（DSP）的声码器可以通过轻量级的快速傅里叶变换（FFT）来实现，因此比任何神经声码器都快得多。DSP声码器通常由于使用过度平滑的声学模型预测和音道的近似表示而导致音频质量较低。在本文中，我们提出了一种超轻差分DSP声码器（DDSP声码器），它使用了经过联合优化的声学模型和DSP声码器，并且在不需要提取声道频谱特征的情况下进行训练。该模型在作为DSP声码器的同时，实现了与神经声码器相当的音频质量，平均MOS高达4.36。我们的C++实现，在没有任何硬件特定的优化情况下，达到了15 MFLOPS，超过MB-MelGAN 3个性能单位。

    Neural vocoders model the raw audio waveform and synthesize high-quality audio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to run real-time on a low-end device like a smartglass. A pure digital signal processing (DSP) based vocoder can be implemented via lightweight fast Fourier transforms (FFT), and therefore, is a magnitude faster than any neural vocoder. A DSP vocoder often gets a lower audio quality due to consuming over-smoothed acoustic model predictions of approximate representations for the vocal tract. In this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder that uses a jointly optimized acoustic model with a DSP vocoder, and learns without an extracted spectral feature for the vocal tract. The model achieves audio quality comparable to neural vocoders with a high average MOS of 4.36 while being efficient as a DSP vocoder. Our C++ implementation, without any hardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 3
    
[^71]: 对比反训练：一种对比方法用于机器反学习

    Contrastive Unlearning: A Contrastive Approach to Machine Unlearning. (arXiv:2401.10458v1 [cs.LG])

    [http://arxiv.org/abs/2401.10458](http://arxiv.org/abs/2401.10458)

    该论文提出了一种对比反学习的框架，通过对比反学习样本和剩余样本的嵌入，将反学习样本推离原始类别并拉向其他类别，从而有效地消除其影响，同时保持了剩余样本学习到的表示。实验证明，对比反学习在反学习效果和效率方面表现最佳。

    

    机器反学习旨在消除训练样本子集（即反学习样本）对已训练模型的影响。有效且高效地移除反学习样本，同时不会对整体模型性能产生负面影响仍然具有挑战性。在本文中，我们提出了一种对比反学习框架，利用表示学习的概念来实现更有效的反学习。该方法通过将反学习样本的嵌入与剩余样本进行对比，将它们从原始类别中推开并拉向其他类别，从而消除其影响。通过直接优化表示空间，它有效地去除了反学习样本的影响，同时保持了从剩余样本中学到的表示。在各种数据集和模型上进行的实验证明，对比反学习能够以最低的性能损失实现最好的反学习效果和效率。

    Machine unlearning aims to eliminate the influence of a subset of training samples (i.e., unlearning samples) from a trained model. Effectively and efficiently removing the unlearning samples without negatively impacting the overall model performance is still challenging. In this paper, we propose a contrastive unlearning framework, leveraging the concept of representation learning for more effective unlearning. It removes the influence of unlearning samples by contrasting their embeddings against the remaining samples so that they are pushed away from their original classes and pulled toward other classes. By directly optimizing the representation space, it effectively removes the influence of unlearning samples while maintaining the representations learned from the remaining samples. Experiments on a variety of datasets and models on both class unlearning and sample unlearning showed that contrastive unlearning achieves the best unlearning effects and efficiency with the lowest perfo
    
[^72]: 学习辅助的随机容量扩展规划：一种贝叶斯优化方法

    Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v1 [eess.SY])

    [http://arxiv.org/abs/2401.10451](http://arxiv.org/abs/2401.10451)

    本研究提出了一种学习辅助的贝叶斯优化方法，用于解决大规模容量扩展问题。通过构建和求解可行的时间聚合代理问题，识别出低成本的规划决策。通过在验证集和测试预测上评估解决的规划结果，实现了随机容量扩展问题的可行解决。

    

    解决大规模的容量扩展问题对于区域能源系统的成本效益低碳化至关重要。为了确保容量扩展问题的预期结果，建模考虑到天气相关的可再生能源供应和能源需求的不确定性变得至关重要。然而，由此产生的随机优化模型通常比确定性模型难以计算。在这里，我们提出了一种学习辅助的近似解法来可行地解决两阶段随机容量扩展问题。我们的方法通过构建和求解一系列可行的时间聚合代理问题，识别出低成本的规划决策。我们采用贝叶斯优化方法搜索时间序列聚合超参数的空间，并计算在供需预测的验证集上最小化成本的近似解。重要的是，我们在一组保留的测试预测上评估解决的规划结果。

    Solving large-scale capacity expansion problems (CEPs) is central to cost-effective decarbonization of regional-scale energy systems. To ensure the intended outcomes of CEPs, modeling uncertainty due to weather-dependent variable renewable energy (VRE) supply and energy demand becomes crucially important. However, the resulting stochastic optimization models are often less computationally tractable than their deterministic counterparts. Here, we propose a learning-assisted approximate solution method to tractably solve two-stage stochastic CEPs. Our method identifies low-cost planning decisions by constructing and solving a sequence of tractable temporally aggregated surrogate problems. We adopt a Bayesian optimization approach to searching the space of time series aggregation hyperparameters and compute approximate solutions that minimize costs on a validation set of supply-demand projections. Importantly, we evaluate solved planning outcomes on a held-out set of test projections. We 
    
[^73]: 语音识别中低秩适应的训练策略和模型鲁棒性研究

    Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])

    [http://arxiv.org/abs/2401.10447](http://arxiv.org/abs/2401.10447)

    本研究研究了语音识别中低秩适应的训练策略和模型鲁棒性。通过引入不同的LoRA训练策略，实现了相对词错误率的降低，并研究了模型对输入扰动的稳定性。实验结果表明，高级LoRA变体导致了某些扰动的性能下降。

    

    随着资源有限的硬件设备的普及，使用低秩适应（LoRA）与冻结预训练语言模型（PLMs）已成为一种主流、资源高效的建模方法。本研究首先探讨了如何通过引入各种LoRA训练策略来提高模型性能，在公开的Librispeech数据集上实现了相对词错误率降低3.50％，在消息领域的内部数据集上实现了3.67％的降低。为了进一步评估基于LoRA的二次传递语音识别模型的稳定性，我们研究了对输入扰动的鲁棒性。这些扰动源于同音字替代和一种名为N-best Perturbation-based Rescoring Robustness（NPRR）的新度量标准，这两种方法都用于衡量重评分模型性能的相对降解程度。我们的实验结果表明，虽然LoRA的高级变体（例如动态秩分配的LoRA）导致了$1$-best扰动的性能下降。

    The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\% on the public Librispeech dataset and of 3.67\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati
    
[^74]: 大型语言模型是噪声鲁棒语音识别的高效学习者

    Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])

    [http://arxiv.org/abs/2401.10446](http://arxiv.org/abs/2401.10446)

    本文通过引入噪声信息作为条件器，并从N-best列表中提取语言空间噪声嵌入，教会了大型语言模型（LLMs）进行噪声去除，从而实现了噪声鲁棒语音识别的生成式错误纠正（GER）。

    

    最近对大型语言模型（LLMs）的进展促进了自动语音识别（ASR）的生成式错误纠正（GER），利用LLMs的丰富语言知识和强大的推理能力来改善识别结果。最新的研究提出了一个GER基准测试，并使用HyPoradise数据集通过高效的LLM微调从ASR N-best假设到地面真实转录的映射，这显示出极大的效果，但在噪声鲁棒ASR方面缺乏具体性。在这项工作中，我们将基准测试扩展到噪声条件下，并研究是否可以教会LLMs像噪声鲁棒ASR一样执行去噪。其中一个解决方案是将噪声信息作为条件器引入LLM中。然而，直接从音频编码器中引入噪声嵌入可能会对LLM微调造成损害，因为存在跨模态差距。因此，我们提出了从N-best列表中提取语言空间噪声嵌入来表示源语音的噪声条件的方法。

    Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic
    
[^75]: 路径选择对于路径方法中的清晰归因至关重要

    Path Choice Matters for Clear Attribution in Path Methods. (arXiv:2401.10442v1 [cs.CV])

    [http://arxiv.org/abs/2401.10442](http://arxiv.org/abs/2401.10442)

    本论文介绍了路径方法中路径选择对于清晰归因的重要性。通过引入“集中原则”，我们提出了一种模型无关的解释器SAMP，并使用无穷小约束和动量策略来提高严谨性和优化性。实验证明我们的方法可以精确揭示DNNs，且优于其他方法。

    

    严谨性和清晰性对于解释深度神经网络（DNNs）以产生人类信任都是至关重要的。路径方法常用于生成满足三个公理的严谨归因。然而，由于路径选择的不同，归因的含义仍然模糊不清。为了解决这种模糊性，我们引入了“集中原则”，将高度重要的特征集中分配给高度归因，从而赋予美感和稀疏性。然后，我们提出了一种模型无关的解释器“SAMP”，它可以从预定义的一组操纵路径中高效地搜索近乎最优的路径。此外，我们提出了无穷小约束（IC）和动量策略（MS）来改善严谨性和优化性。可视化结果显示，SAMP可以通过准确定位显著的图像像素来精确揭示DNNs。我们还进行了定量实验，观察到我们的方法显著优于对应方法。代码：https://github.com/zbr17/SAMP。

    Rigorousness and clarity are both essential for interpretations of DNNs to engender human trust. Path methods are commonly employed to generate rigorous attributions that satisfy three axioms. However, the meaning of attributions remains ambiguous due to distinct path choices. To address the ambiguity, we introduce \textbf{Concentration Principle}, which centrally allocates high attributions to indispensable features, thereby endowing aesthetic and sparsity. We then present \textbf{SAMP}, a model-agnostic interpreter, which efficiently searches the near-optimal path from a pre-defined set of manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and momentum strategy (MS) to improve the rigorousness and optimality. Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient image pixels. We also perform quantitative experiments and observe that our method significantly outperforms the counterparts. Code: https://github.com/zbr17/SAMP.
    
[^76]: A2Q+: 改进的累加器感知权重量化方法

    A2Q+: Improving Accumulator-Aware Weight Quantization. (arXiv:2401.10432v1 [cs.LG])

    [http://arxiv.org/abs/2401.10432](http://arxiv.org/abs/2401.10432)

    A2Q+是一种改进的累加器感知权重量化方法，通过改进约束和初始化策略，实现了在保持模型准确性的同时提高硬件效率。

    

    量化技术通常通过限制权重和激活函数的精度来减少神经网络的推理成本。最近的研究表明，减少累加器的精度还可以进一步提高硬件效率，但会增加数值溢出的风险，从而降低模型的准确性。为了在保持准确性的同时避免数值溢出，最近的研究提出了累加器感知量化（A2Q），这是一种量化感知训练方法，通过在训练期间约束模型权重，以在推理期间安全地使用目标累加器位宽。尽管这显示出很大的潜力，但我们证明了A2Q依赖于过于限制性的约束和次优的权重初始化策略，这些都引入了不必要的量化误差。为了解决这些缺点，我们引入了以下改进：（1）一种改进的约束方式，缓解累加器约束而不损害溢出避免；（2）一种新的权重初始化策略。

    Quantization techniques commonly reduce the inference costs of neural networks by restricting the precision of weights and activations. Recent studies show that also reducing the precision of the accumulator can further improve hardware efficiency at the risk of numerical overflow, which introduces arithmetic errors that can degrade model accuracy. To avoid numerical overflow while maintaining accuracy, recent work proposed accumulator-aware quantization (A2Q), a quantization-aware training method that constrains model weights during training to safely use a target accumulator bit width during inference. Although this shows promise, we demonstrate that A2Q relies on an overly restrictive constraint and a sub-optimal weight initialization strategy that each introduce superfluous quantization error. To address these shortcomings, we introduce: (1) an improved bound that alleviates accumulator constraints without compromising overflow avoidance; and (2) a new strategy for initializing qua
    
[^77]: M3BUNet: 用于CT扫描胰腺分割的移动均值最大UNet

    M3BUNet: Mobile Mean Max UNet for Pancreas Segmentation on CT-Scans. (arXiv:2401.10419v1 [eess.IV])

    [http://arxiv.org/abs/2401.10419](http://arxiv.org/abs/2401.10419)

    M3BUNet是MobileNet和U-Net神经网络的融合，使用新颖的MM注意力机制，在两个阶段逐步细分胰腺CT图像，并使用掩码指导进行目标检测，从而提高了分割性能。

    

    在CT扫描图像中分割器官是多个下游医学图像分析任务中必要的过程。目前，手动CT扫描分割由放射科医生广泛应用，尤其是对于像胰腺这样的器官，由于器官尺寸小、遮挡和形状变化等因素需要高水平的领域专业知识进行可靠的分割。当转向自动胰腺分割时，这些因素导致可靠标注数据有限，难以训练有效的分割模型。因此，现代胰腺分割模型的性能仍未达到可接受范围。为了改善这一点，我们提出了M3BUNet，它是MobileNet和U-Net神经网络的融合，配备了一种新颖的均值最大(MM)注意力机制，通过两个阶段逐步细分胰腺CT图像，并使用掩码指导进行目标检测。这种方法使网络能够超越类似网络架构所实现的分割性能。

    Segmenting organs in CT scan images is a necessary process for multiple downstream medical image analysis tasks. Currently, manual CT scan segmentation by radiologists is prevalent, especially for organs like the pancreas, which requires a high level of domain expertise for reliable segmentation due to factors like small organ size, occlusion, and varying shapes. When resorting to automated pancreas segmentation, these factors translate to limited reliable labeled data to train effective segmentation models. Consequently, the performance of contemporary pancreas segmentation models is still not within acceptable ranges. To improve that, we propose M3BUNet, a fusion of MobileNet and U-Net neural networks, equipped with a novel Mean-Max (MM) attention that operates in two stages to gradually segment pancreas CT images from coarse to fine with mask guidance for object detection. This approach empowers the network to surpass segmentation performance achieved by similar network architecture
    
[^78]: 差分隐私和对抗性鲁棒的机器学习：经验评估

    Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation. (arXiv:2401.10405v1 [cs.LG])

    [http://arxiv.org/abs/2401.10405](http://arxiv.org/abs/2401.10405)

    本研究旨在评估差分隐私和对抗性鲁棒机器学习的效果。通过将对抗训练和差分隐私训练相结合来应对同时攻击，实证结果表明该方法在性能上优于其他方法，并且在隐私保证方面与非鲁棒私有模型相当。这项研究还强调了对动态训练范式中隐私保证的需求。

    

    恶意对手可以通过发起一系列的逃逸攻击来推断敏感信息或破坏机器学习模型。尽管各种工作致力于解决隐私和安全问题，但它们集中在个别防御上，而实际上，模型可能同时遭受多个攻击。本研究探讨对抗训练和差分隐私训练的组合，以应对同时攻击。虽然DP-Adv中提出的差分隐私对抗训练在性能上优于其他最先进的方法，但它缺乏正式的隐私保证和实证验证。因此，在本文中，我们使用成员推断攻击来基准测试该技术的性能，并经验证明该方法与非鲁棒私有模型一样具有隐私性。本研究还强调了探索动态训练范式中隐私保证的需求。

    Malicious adversaries can attack machine learning models to infer sensitive information or damage the system by launching a series of evasion attacks. Although various work addresses privacy and security concerns, they focus on individual defenses, but in practice, models may undergo simultaneous attacks. This study explores the combination of adversarial training and differentially private training to defend against simultaneous attacks. While differentially-private adversarial training, as presented in DP-Adv, outperforms the other state-of-the-art methods in performance, it lacks formal privacy guarantees and empirical validation. Thus, in this work, we benchmark the performance of this technique using a membership inference attack and empirically show that the resulting approach is as private as non-robust private models. This work also highlights the need to explore privacy guarantees in dynamic training paradigms.
    
[^79]: 深度字典: 基于深度学习的物联网数据的有损时间序列压缩器

    Deep Dict: Deep Learning-based Lossy Time Series Compressor for IoT Data. (arXiv:2401.10396v1 [eess.SP])

    [http://arxiv.org/abs/2401.10396](http://arxiv.org/abs/2401.10396)

    提出了一种深度学习-based的有损时间序列压缩器Deep Dict，通过引入伯努利变换自编码器（BTAE）和失真约束的方式，实现了高压缩比和保持预定范围内的解压缩误差，并通过引入量化熵损失（QEL）来提高鲁棒性。与最先进的有损压缩器相比，在多个时间序列数据集上表现更好。

    

    我们提出了深度字典，一种基于深度学习的有损时间序列压缩器，旨在实现高压缩比的同时保持解压缩误差在预定范围内。深度字典包括两个关键组件：伯努利变换自编码器（BTAE）和失真约束。BTAE从时间序列数据中提取伯努利表示，相对于传统自编码器，减小了表示的大小。失真约束限制了BTAE的预测误差在期望范围内。此外，为了解决常见回归损失（如L1/L2）的局限性，我们引入了一种称为量化熵损失（QEL）的新型损失函数。QEL考虑了问题的特定特性，增强了对异常值的鲁棒性，并减轻了优化挑战。我们对来自不同领域的10个多样化的时间序列数据集进行了深度字典的评估，结果显示深度字典在压缩率方面优于最先进的有损压缩器。

    We propose Deep Dict, a deep learning-based lossy time series compressor designed to achieve a high compression ratio while maintaining decompression error within a predefined range. Deep Dict incorporates two essential components: the Bernoulli transformer autoencoder (BTAE) and a distortion constraint. BTAE extracts Bernoulli representations from time series data, reducing the size of the representations compared to conventional autoencoders. The distortion constraint limits the prediction error of BTAE to the desired range. Moreover, in order to address the limitations of common regression losses such as L1/L2, we introduce a novel loss function called quantized entropy loss (QEL). QEL takes into account the specific characteristics of the problem, enhancing robustness to outliers and alleviating optimization challenges. Our evaluation of Deep Dict across ten diverse time series datasets from various domains reveals that Deep Dict outperforms state-of-the-art lossy compressors in te
    
[^80]: 基于分布一致性的稀疏标签图神经网络自训练

    Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])

    [http://arxiv.org/abs/2401.10394](http://arxiv.org/abs/2401.10394)

    本文提出了一种基于分布一致性的自训练方法，用于解决图神经网络中少样本节点分类的挑战，通过消除训练集和测试集之间的分布转移，提高自训练的有效性。

    

    少样本节点分类对于图神经网络(GNNs)来说是一个重大挑战，因为标记和未标记节点之间的监督不足和潜在的分布转移问题。自训练是一种广泛流行的框架，利用大量未标记数据扩展训练集，通过给选定的未标记节点分配伪标签。已经开展了许多基于置信度、信息增益等选择策略的努力。然而，这些方法中没有一个考虑到训练和测试节点集之间的分布转移。伪标签步骤可能增加这种转移甚至引入新的转移，从而阻碍自训练的有效性。因此，在这项研究中，我们探索了在自训练过程中明确地消除扩展训练集和测试集之间的分布转移的潜力。为此，我们提出了一种新颖的分布一致图自训练(DC-GST)框架，用于辨别

    Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identif
    
[^81]: 自然的功率法则学习环境中能够减轻灾难性干扰

    Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])

    [http://arxiv.org/abs/2401.10393](http://arxiv.org/abs/2401.10393)

    本研究在自然学习环境中通过回忆方法减轻了灾难性干扰，该方法受到功率法则的启发。

    

    神经网络通常遭受灾难性干扰（CI）：在学习新任务时，先前学习任务的表现显著下降。这与人类形成鲜明对比，人类可以连续学习新任务而不会明显忘记先前的任务。先前的工作已经探索了各种减轻CI的技术，例如正则化、回忆、生成性回放和浓缩方法。本研究采用了一种不同的方法，该方法受到认知科学研究的指导，该研究表明在自然环境中，遇到任务的概率与最后一次执行任务的时间成功率法则递减。我们认为，在模拟自然学习环境中进行减轻CI技术的真实评估是必要的。因此，我们评估了在类似人类面临的功率法则环境中训练简单的回忆方法时，CI的减轻程度。我们的工作探索了这种基于回忆的新方法。

    Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
    
[^82]: 通过随机森林机器学习非侵入性诊断急性间室综合征

    Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning. (arXiv:2401.10386v1 [cs.LG])

    [http://arxiv.org/abs/2401.10386](http://arxiv.org/abs/2401.10386)

    本研究提出了一种基于随机森林机器学习的非侵入性诊断急性间室综合征的方法。使用压力传感电阻器检测肌肉间室压力，并通过蓝牙传输结果到Web应用程序。该诊断方法在准确率、灵敏度和F1得分等关键性能指标方面表现出色。

    

    急性间室综合征（ACS）是一种骨科急症，由肌肉间室内的压力升高引起，导致永久组织损伤并最终致死。ACS的诊断主要依赖患者报告的症状，这种方法在临床上不可靠，通常需要通过侵入性肌肉间室压力测量进行补充。本研究提出了一种连续、客观、非侵入性的ACS诊断方法。该设备通过一个使用贴在皮肤上的压力传感电阻器（FSR）的随机森林机器学习模型检测ACS。最终诊断结果通过蓝牙以实时方式传输到Web应用程序。为了验证诊断结果，创建了一组包含FSR测量和相应的模拟肌肉间室压力的数据集。该诊断方法的准确率达到97%，与侵入性的金标准持平。该设备在关键性能指标包括准确度、灵敏度和F1得分方面表现优异。

    Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured 
    
[^83]: 高维PDE的解算符近似方法

    Approximation of Solution Operators for High-dimensional PDEs. (arXiv:2401.10385v1 [math.NA])

    [http://arxiv.org/abs/2401.10385](http://arxiv.org/abs/2401.10385)

    提出了一种有限维度控制为基础的方法来近似高维PDE的解算符，通过使用深度神经网络等降阶模型和神经常微分方程的计算技术，在一般的二阶非线性PDE类中证明了近似精度的合理性，并通过数值结果验证了所提方法的准确性和效率。

    

    我们提出了一种有限维度控制为基础的方法来近似演化型偏微分方程（PDE）的解算符，特别是在高维情况下。通过采用一般的降阶模型，如深度神经网络，我们将模型参数的演化与相应函数空间中的轨迹联系起来。利用神经常微分方程的计算技术，我们学习控制参数空间，使得从任意初始点开始，受控轨迹能够紧密逼近PDE的解。对于一般的二阶非线性PDE类，我们证明了近似精度的合理性。我们还给出了几个高维PDE的数值结果，包括用于求解Hamilton-Jacobi-Bellman方程的真实应用。这些结果演示了所提方法的准确性和效率。

    We propose a finite-dimensional control-based method to approximate solution operators for evolutional partial differential equations (PDEs), particularly in high-dimensions. By employing a general reduced-order model, such as a deep neural network, we connect the evolution of the model parameters with trajectories in a corresponding function space. Using the computational technique of neural ordinary differential equation, we learn the control over the parameter space such that from any initial starting point, the controlled trajectories closely approximate the solutions to the PDE. Approximation accuracy is justified for a general class of second-order nonlinear PDEs. Numerical results are presented for several high-dimensional PDEs, including real-world applications to solving Hamilton-Jacobi-Bellman equations. These are demonstrated to show the accuracy and efficiency of the proposed method.
    
[^84]: 合作多智能体图形赌博机：UCB算法和遗憾分析

    Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis. (arXiv:2401.10383v1 [cs.LG])

    [http://arxiv.org/abs/2401.10383](http://arxiv.org/abs/2401.10383)

    本文提出了一种解决多智能体图形赌博机问题的算法Multi-G-UCB，并通过数值实验验证了其有效性。

    

    本文将多智能体图形赌博机问题建模为Zhang、Johansson和Li在[CISS 57, 1-6 (2023)]中提出的图形赌博机问题的多智能体扩展。在我们的模型中，N个合作智能体在一个连通的图G上移动，图G有K个节点。抵达每个节点时，智能体观察到从一个与节点相关的概率分布中随机抽取的奖励。系统奖励被建模为智能体观测到的奖励的加权和，其中权重表达了多个智能体同时对同一节点进行采样的边际减少奖励。我们提出了一个基于上限置信区间（UCB）的学习算法，称为Multi-G-UCB，并证明了在T步内其期望遗憾被界定为$O(N\log(T)[\sqrt{KT} + DK])$，其中D是图G的直径。最后，我们通过与其他方法进行比较对算法进行了数值测试。

    In this paper, we formulate the multi-agent graph bandit problem as a multi-agent extension of the graph bandit problem introduced by Zhang, Johansson, and Li [CISS 57, 1-6 (2023)]. In our formulation, $N$ cooperative agents travel on a connected graph $G$ with $K$ nodes. Upon arrival at each node, agents observe a random reward drawn from a node-dependent probability distribution. The reward of the system is modeled as a weighted sum of the rewards the agents observe, where the weights capture the decreasing marginal reward associated with multiple agents sampling the same node at the same time. We propose an Upper Confidence Bound (UCB)-based learning algorithm, Multi-G-UCB, and prove that its expected regret over $T$ steps is bounded by $O(N\log(T)[\sqrt{KT} + DK])$, where $D$ is the diameter of graph $G$. Lastly, we numerically test our algorithm by comparing it to alternative methods.
    
[^85]: 基于基础模型集成的联邦学习在敌对威胁下的漏洞

    Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats. (arXiv:2401.10375v1 [cs.CR])

    [http://arxiv.org/abs/2401.10375](http://arxiv.org/abs/2401.10375)

    本文研究基于基础模型集成的联邦学习在敌对威胁下的漏洞，提出了一种新的攻击策略，揭示了该模型在不同配置的联邦学习下对敌对威胁的高敏感性。

    

    联邦学习是解决与数据隐私和安全相关的机器学习的重要问题，但在某些情况下存在数据不足和不平衡问题。基础模型的出现为现有联邦学习框架的局限性提供了潜在的解决方案，例如通过生成合成数据进行模型初始化。然而，由于基础模型的内在安全性问题，将基础模型集成到联邦学习中可能引入新的风险，这方面的研究尚属未开发。为了填补这一空白，我们首次研究基于基础模型集成的联邦学习在敌对威胁下的漏洞。基于基础模型集成的联邦学习的统一框架，我们引入了一种新的攻击策略，利用基础模型的安全性问题来破坏联邦学习客户端模型。通过在图像和文本领域中使用知名模型和基准数据集进行广泛实验，我们揭示了基于基础模型集成的联邦学习在不同配置的联邦学习下对这种新威胁的高敏感性。

    Federated Learning (FL) addresses critical issues in machine learning related to data privacy and security, yet suffering from data insufficiency and imbalance under certain circumstances. The emergence of foundation models (FMs) offers potential solutions to the limitations of existing FL frameworks, e.g., by generating synthetic data for model initialization. However, due to the inherent safety concerns of FMs, integrating FMs into FL could introduce new risks, which remains largely unexplored. To address this gap, we conduct the first investigation on the vulnerability of FM integrated FL (FM-FL) under adversarial threats. Based on a unified framework of FM-FL, we introduce a novel attack strategy that exploits safety issues of FM to compromise FL client models. Through extensive experiments with well-known models and benchmark datasets in both image and text domains, we reveal the high susceptibility of the FM-FL to this new threat under various FL configurations. Furthermore, we f
    
[^86]: 鲁棒且具普适性的医学图像分割的空间和光谱学习的协调

    Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation. (arXiv:2401.10373v1 [eess.IV])

    [http://arxiv.org/abs/2401.10373](http://arxiv.org/abs/2401.10373)

    本文提出了一种鲁棒且具普适性的医学图像分割方法，通过协调空间和光谱表示，引入光谱相关系数目标来提高对中阶特征和上下文长程依赖的捕捉能力，从而显著增强了泛化能力。

    

    深度学习在医学图像分割方面取得了显著的成就。然而，由于类内变异性和类间独立性，现有的深度学习模型在泛化能力上存在困难，同一类在不同样本中表现不同，难以捕捉不同对象之间的复杂关系，从而导致更高的错误负例。本文提出了一种新的方法，通过协调空间和光谱表示来增强领域通用的医学图像分割。我们引入了创新的光谱相关系数目标，以提高模型捕捉中阶特征和上下文长程依赖的能力。这个目标通过融入有价值的光谱信息来补充传统的空间目标。大量实验证明，优化这个目标与现有的UNet和TransUNet架构显著提高了泛化能力。

    Deep learning has demonstrated remarkable achievements in medical image segmentation. However, prevailing deep learning models struggle with poor generalization due to (i) intra-class variations, where the same class appears differently in different samples, and (ii) inter-class independence, resulting in difficulties capturing intricate relationships between distinct objects, leading to higher false negative cases. This paper presents a novel approach that synergies spatial and spectral representations to enhance domain-generalized medical image segmentation. We introduce the innovative Spectral Correlation Coefficient objective to improve the model's capacity to capture middle-order features and contextual long-range dependencies. This objective complements traditional spatial objectives by incorporating valuable spectral information. Extensive experiments reveal that optimizing this objective with existing architectures like UNet and TransUNet significantly enhances generalization, 
    
[^87]: Langevin遗忘：噪声梯度下降的机器遗忘新视角

    Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])

    [http://arxiv.org/abs/2401.10371](http://arxiv.org/abs/2401.10371)

    Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。

    

    随着采用确保“被遗忘权”的法律，机器遗忘引起了极大的兴趣。研究人员提供了一个概率性的近似遗忘定义，类似于差分隐私（DP）的定义，其中隐私被定义为对重新训练的统计不可区分性。我们提出了Langevin遗忘，这是一个基于噪声梯度下降的近似遗忘问题的隐私保证的遗忘框架。Langevin遗忘在算法上统一了DP学习过程和隐私认证的遗忘过程。其中包括非凸问题的近似认证遗忘，相对于重新训练的复杂度节省，以及用于多个遗忘请求的顺序和批量遗忘。我们通过在基准数据集上的实验验证了Langevin遗忘的实用性，并展示了它对梯度下降的优势。

    Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
    
[^88]: 深度生成建模用于金融时间序列：VaR的比较研究

    Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review. (arXiv:2401.10370v1 [q-fin.CP])

    [http://arxiv.org/abs/2401.10370](http://arxiv.org/abs/2401.10370)

    本文比较了深度生成建模在金融时间序列中的应用，提出了新的条件时间序列生成方法，并介绍了在VaR预测中的使用。

    

    在金融服务业中，基于历史数据和当前市场环境来预测风险因素分布是市场风险建模和价值-at-risk (VaR) 模型的关键。作为商业银行中最广泛采用的VaR模型之一，历史模拟 (HS) 使用历史窗口内每日收益的经验分布作为下一天风险因素收益的预测分布。金融时间序列生成的目标是生成具有良好变化性、与原始历史数据类似分布和动态的合成数据路径。本文应用了多种现有的深度生成方法 (例如CGAN、CWGAN、Diffusion和Signature WGAN) 进行条件时间序列生成，并提出并测试了两种新的条件多步时间序列生成方法，即Encoder-Decoder CGAN和Conditional TimeVAE。此外，我们介绍了一个全面的框架及其在VaR预测中的应用。

    In the financial services industry, forecasting the risk factor distribution conditional on the history and the current market environment is the key to market risk modeling in general and value at risk (VaR) model in particular. As one of the most widely adopted VaR models in commercial banks, Historical simulation (HS) uses the empirical distribution of daily returns in a historical window as the forecast distribution of risk factor returns in the next day. The objectives for financial time series generation are to generate synthetic data paths with good variety, and similar distribution and dynamics to the original historical data. In this paper, we apply multiple existing deep generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for conditional time series generation, and propose and test two new methods for conditional multi-step time series generation, namely Encoder-Decoder CGAN and Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a 
    
[^89]: 使用类似ChatGPT的LLM设计和实现RISC处理器：执行、挑战和限制

    Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations. (arXiv:2401.10364v1 [cs.LG])

    [http://arxiv.org/abs/2401.10364](http://arxiv.org/abs/2401.10364)

    本文讨论了使用LLM进行代码生成的可行性，并特别应用于设计RISC处理器。经实验证实，LLM生成的代码存在显著错误，需要人为干预来修复。LLM可用于辅助程序员的代码设计。

    

    本文讨论了使用大型语言模型LLM进行代码生成的可行性，其中特别应用于设计RISC处理器。文章还回顾了与代码生成相关的步骤，如解析、标记化、编码、注意机制、抽样以及代码生成期间的迭代。通过测试平台和FPGA板上的硬件实现验证了生成的RISC组件的代码。使用了四个度量参数：第一次迭代的正确输出、代码中嵌入的错误数量、实现代码所需的尝试次数以及三次迭代后无法生成代码的失败情况，以比较使用LLM编程的效率。在所有情况下，生成的代码都存在显著错误，总是需要人为干预修复错误。因此，LLM可用于辅助程序员的代码设计。

    This paper discusses the feasibility of using Large Language Models LLM for code generation with a particular application in designing an RISC. The paper also reviews the associated steps such as parsing, tokenization, encoding, attention mechanism, sampling the tokens and iterations during code generation. The generated code for the RISC components is verified through testbenches and hardware implementation on a FPGA board. Four metric parameters Correct output on the first iteration, Number of errors embedded in the code, Number of trials required to achieve the code and Failure to generate the code after three iterations, are used to compare the efficiency of using LLM in programming. In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs. LLM can therefore be used to complement a programmer code design.
    
[^90]: 在多跳基于集群的车联网中的分层联邦学习

    Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs. (arXiv:2401.10361v1 [cs.LG])

    [http://arxiv.org/abs/2401.10361](http://arxiv.org/abs/2401.10361)

    本文介绍了一种在多跳基于集群的车联网中的分层联邦学习（HFL）框架，该框架利用平均相对速度和余弦相似度对FL模型参数进行加权组合以作为聚类度量，以解决数据多样性和高车辆移动性带来的挑战。

    

    在车联网中使用分层联邦学习（FL）引起了研究界的极大兴趣，因为通过通信本地数据集梯度而不是原始数据，可以减少传输开销并保护用户隐私。然而，在车联网中实施FL面临着有限的通信资源、高车辆移动性和数据分布的统计多样性等挑战。为了解决这些问题，本文引入了一种新的分层联邦学习（HFL）框架，用于基于多跳聚类的车联网。所提出的方法利用FL模型参数的平均相对速度和余弦相似度的加权组合作为聚类度量，以考虑数据多样性和高车辆移动性。这个度量可以在非独立同分布（non-IID）数据场景中保证最小变化簇头的收敛性，并解决与该场景相关的复杂性问题。

    The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has garnered significant interest in research due to the advantages of reducing transmission overhead and protecting user privacy by communicating local dataset gradients instead of raw data. However, implementing FL in VANETs faces challenges, including limited communication resources, high vehicle mobility, and the statistical diversity of data distributions. In order to tackle these issues, this paper introduces a novel framework for hierarchical federated learning (HFL) over multi-hop clustering-based VANET. The proposed method utilizes a weighted combination of the average relative speed and cosine similarity of FL model parameters as a clustering metric to consider both data diversity and high vehicle mobility. This metric ensures convergence with minimum changes in cluster heads while tackling the complexities associated with non-independent and identically distributed (non-IID) data scenarios. Additionall
    
[^91]: 对不起，先生？你的语言模型正在泄漏（信息）

    Excuse me, sir? Your language model is leaking (information). (arXiv:2401.10360v1 [cs.CR])

    [http://arxiv.org/abs/2401.10360](http://arxiv.org/abs/2401.10360)

    这项研究介绍了一种加密方法，可以在大型语言模型的响应中隐藏秘密载荷，且不影响生成文本的质量。

    

    我们引入了一种加密方法，将任意秘密载荷隐藏在大型语言模型（LLM）的响应中。提取模型响应中的载荷需要一个秘密密钥，没有密钥是无法区分原始LLM和隐藏载荷的LLM响应的。特别地，生成文本的质量不会受到载荷的影响。我们的方法扩展了Christ、Gunn和Zamir（2023）的最新结果，他们提出了一种无法检测到的LLM水印方案。

    We introduce a cryptographic method to hide an arbitrary secret payload in the response of a Large Language Model (LLM). A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload. In particular, the quality of generated text is not affected by the payload. Our approach extends a recent result of Christ, Gunn and Zamir (2023) who introduced an undetectable watermarking scheme for LLMs.
    
[^92]: 使用地震信号进行结构异常检测的智能优化和机器学习算法

    Intelligent Optimization and Machine Learning Algorithms for Structural Anomaly Detection using Seismic Signals. (arXiv:2401.10355v1 [eess.SP])

    [http://arxiv.org/abs/2401.10355](http://arxiv.org/abs/2401.10355)

    本研究使用智能优化技术和机器学习算法，通过比较结构振动的实验测量值和数值模拟，实现了对简单结构中异常情况的检测。

    

    在机械化隧道工程中缺乏异常检测方法可能导致财务损失和钻削时间的亏损。现场挖掘需要在钻探之前识别出硬障碍物，以避免损坏隧道掘进机并调整传播速度。利用智能优化技术和机器学习可以提高结构异常检测的效率。本研究通过使用参数估计方法将结构振动的实验测量值与数值模拟进行比较，来检测简单结构中的异常情况。

    The lack of anomaly detection methods during mechanized tunnelling can cause financial loss and deficits in drilling time. On-site excavation requires hard obstacles to be recognized prior to drilling in order to avoid damaging the tunnel boring machine and to adjust the propagation velocity. The efficiency of the structural anomaly detection can be increased with intelligent optimization techniques and machine learning. In this research, the anomaly in a simple structure is detected by comparing the experimental measurements of the structural vibrations with numerical simulations using parameter estimation methods.
    
[^93]: 使用PCS提供可靠的任务完成时间预测

    Towards providing reliable job completion time predictions using PCS. (arXiv:2401.10354v1 [cs.DC])

    [http://arxiv.org/abs/2401.10354](http://arxiv.org/abs/2401.10354)

    本文提出了PCS调度框架，旨在提供可靠的任务完成时间预测。PCS使用加权公平队列调度，并通过模拟辅助的搜索策略来寻找满足特定目标的配置。通过在DNN作业调度上的实施和评估，PCS展示了其在提供可预测性方面的潜力。

    

    本文建立了一个案例，为云用户提供任务完成时间的预测，类似于包裹的送达日期或订车的到达时间。我们的分析表明，提供可预测性可能会导致性能和公平性的损失。现有的云调度系统在权衡空间中优化极端点，使其要么极不可预测，要么不实用。为了解决这个挑战，我们提出了PCS，一个新的调度框架，旨在提供可预测性的同时平衡其他传统目标。PCS的关键思想是使用加权公平队列调度（WFQ），找到适当的WFQ参数配置（例如类别权重），以满足特定的可预测性目标。它使用模拟辅助的搜索策略，有效地发现处于这些目标之间权衡空间帕累托前沿的WFQ配置。我们在DNN作业调度的GP上实施和评估PCS。

    In this paper we build a case for providing job completion time predictions to cloud users, similar to the delivery date of a package or arrival time of a booked ride. Our analysis reveals that providing predictability can come at the expense of performance and fairness. Existing cloud scheduling systems optimize for extreme points in the trade-off space, making them either extremely unpredictable or impractical.  To address this challenge, we present PCS, a new scheduling framework that aims to provide predictability while balancing other traditional objectives. The key idea behind PCS is to use Weighted-Fair-Queueing (WFQ) and find a suitable configuration of different WFQ parameters (e.g., class weights) that meets specific goals for predictability. It uses a simulation-aided search strategy, to efficiently discover WFQ configurations that lie on the Pareto front of the trade-off space between these objectives. We implement and evaluate PCS in the context of DNN job scheduling on GP
    
[^94]: MELODY: 强大的半监督混合模型用于多变量时间序列中基于实体级别的在线异常检测

    MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online Anomaly Detection with Multivariate Time Series. (arXiv:2401.10338v1 [cs.LG])

    [http://arxiv.org/abs/2401.10338](http://arxiv.org/abs/2401.10338)

    MELODY是一个半监督混合模型，用于基于实体级别的在线异常检测，解决了部署的异构性、低延迟容忍度、模糊的异常定义和有限的监督等挑战。

    

    在大型IT系统中，软件部署是在线服务的关键过程，因为它们的代码经常更新。然而，一个有故障的代码更改可能会降低目标服务的性能，并在下游服务中引起连锁故障。因此，应全面监测软件部署，并及时检测出异常。本文研究了部署的异常检测问题。我们首先确定了与该异常检测问题相关的实体级别（例如，部署）相对于更典型的多变量时间序列（MTS）异常检测问题的独特挑战。这些独特的挑战包括部署的异构性、低延迟容忍度、模糊的异常定义和有限的监督。为了解决这些问题，我们提出了一种新颖的框架，用于基于实体级别的在线异常检测的半监督混合模型（MELODY）。MELODY首先将不同实体的MTS转换为相同的形式，

    In large IT systems, software deployment is a crucial process in online services as their code is regularly updated. However, a faulty code change may degrade the target service's performance and cause cascading outages in downstream services. Thus, software deployments should be comprehensively monitored, and their anomalies should be detected timely. In this paper, we study the problem of anomaly detection for deployments. We begin by identifying the challenges unique to this anomaly detection problem, which is at entity-level (e.g., deployments), relative to the more typical problem of anomaly detection in multivariate time series (MTS). The unique challenges include the heterogeneity of deployments, the low latency tolerance, the ambiguous anomaly definition, and the limited supervision. To address them, we propose a novel framework, semi-supervised hybrid Model for Entity-Level Online Detection of anomalY (MELODY). MELODY first transforms the MTS of different entities to the same 
    
[^95]: 基于噪声对比估计的低资源安全攻击模式识别匹配框架

    Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])

    [http://arxiv.org/abs/2401.10337](http://arxiv.org/abs/2401.10337)

    该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。

    

    战术、技术和程序（TTPs）是网络安全领域中复杂的攻击模式，在文本知识库中有详细的描述。在网络安全写作中识别TTPs，通常称为TTP映射，是一个重要而具有挑战性的任务。传统的学习方法通常以经典的多类或多标签分类设置为目标。由于存在大量的类别（即TTPs），标签分布的不均衡和标签空间的复杂层次结构，这种设置限制了模型的学习能力。我们采用了一种不同的学习范式来解决这个问题，其中将文本与TTP标签之间的直接语义相似度决定为文本分配给TTP标签，从而减少了仅仅在大型标签空间上竞争的复杂性。为此，我们提出了一种具有有效的基于采样的学习比较机制的神经匹配架构，促进学习过程。

    Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
    
[^96]: DrugAssist：一个用于分子优化的大型语言模型

    DrugAssist: A Large Language Model for Molecule Optimization. (arXiv:2401.10334v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.10334](http://arxiv.org/abs/2401.10334)

    DrugAssist是一个交互式分子优化模型，通过人机对话实现优化，利用LLM的强交互性和泛化能力，在药物发现中取得了领先的结果。

    

    最近，大型语言模型（LLMs）在各种任务上展现出令人印象深刻的性能，吸引了越来越多的尝试将LLMs应用于药物发现领域。然而，在药物发现流程中，分子优化是一个关键任务，但目前LLMs在这个领域的参与很少。大多数现有方法仅关注捕捉数据中提供的化学结构的潜在模式，而没有利用专家反馈。这些非交互式方法忽视了药物发现过程实际上需要专家经验和迭代改进的事实。为了填补这个空白，我们提出了DrugAssist，一个通过人机对话利用LLM的强交互性和泛化能力进行分子优化的交互式模型。DrugAssist在单一和多个性质优化方面取得了领先的结果，同时展示了巨大的潜力。

    Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense pot
    
[^97]: 在不同偏好强度上通过多任务提升单类推荐系统

    Improving One-class Recommendation with Multi-tasking on Various Preference Intensities. (arXiv:2401.10316v1 [cs.IR])

    [http://arxiv.org/abs/2401.10316](http://arxiv.org/abs/2401.10316)

    本研究提出了一个多任务框架，考虑了隐式反馈中不同偏好强度的情况，并在其中引入了注意力图卷积层来探索高阶关系。这使得表示更具鲁棒性和泛化性。

    

    在单类推荐问题中，需要根据用户的隐式反馈来进行推荐，该反馈是通过用户的行为和不行为进行推断的。现有的方法通过编码来自训练数据中观察到的积极和消极交互来获取用户和物品的表示。然而，这些方法假设隐式反馈中的所有积极信号都反映了固定的偏好强度，这是不现实的。因此，用这些方法学习到的表示通常无法捕捉反映不同偏好强度的信息性实体特征。在本文中，我们提出了一个多任务框架，考虑了隐式反馈中每个信号的不同偏好强度。实体的表示需要同时满足每个子任务的目标，使其更加稳健和泛化。此外，我们还将注意力图卷积层引入到用户-物品的高阶关系中进行探索。

    In the one-class recommendation problem, it's required to make recommendations basing on users' implicit feedback, which is inferred from their action and inaction. Existing works obtain representations of users and items by encoding positive and negative interactions observed from training data. However, these efforts assume that all positive signals from implicit feedback reflect a fixed preference intensity, which is not realistic. Consequently, representations learned with these methods usually fail to capture informative entity features that reflect various preference intensities.  In this paper, we propose a multi-tasking framework taking various preference intensities of each signal from implicit feedback into consideration. Representations of entities are required to satisfy the objective of each subtask simultaneously, making them more robust and generalizable. Furthermore, we incorporate attentive graph convolutional layers to explore high-order relationships in the user-item
    
[^98]: LangProp: 一种应用于自动驾驶的使用语言模型的代码优化框架

    LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])

    [http://arxiv.org/abs/2401.10314](http://arxiv.org/abs/2401.10314)

    LangProp是一种用于自动驾驶的代码优化框架，利用语言模型迭代优化生成的代码。它通过评估代码性能和捕捉异常来改进生成的代码，展示了在CARLA中实现自动驾驶的概念验证。

    

    LangProp是一个框架，用于在监督/强化学习环境中迭代优化大型语言模型(LLM)生成的代码。虽然LLM能够零-shot地生成合理的解决方案，但这些解决方案往往是次优的。特别是对于代码生成任务，初始代码可能在某些边缘情况下失败。LangProp自动评估数据集上的代码性能，并捕捉任何异常，并将结果反馈给LLM进行训练，以使LLM可以迭代改进其生成的代码。通过采用基于度量和数据驱动的训练范式来进行代码优化过程，可以轻松地借鉴传统机器学习技术，如模仿学习、DAgger和强化学习。我们展示了在CARLA中自动驾驶的代码优化的第一个概念验证，证明了LangProp可以生成可解释和透明的驾驶代码。

    LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting. While LLMs can generate sensible solutions zero-shot, the solutions are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, as well as catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metricand data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA, showing that LangProp can generate interpretable and transparent dri
    
[^99]: 黑客攻击预测器意味着黑客攻击汽车：利用敏感性分析识别自动驾驶安全中的轨迹预测漏洞

    Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to Identify Trajectory Prediction Vulnerabilities for Autonomous Driving Security. (arXiv:2401.10313v1 [cs.CR])

    [http://arxiv.org/abs/2401.10313](http://arxiv.org/abs/2401.10313)

    本文通过对两个轨迹预测模型进行敏感性分析，发现尽管图像地图对于这两个模型的预测输出可能只有轻微的贡献，但使用快速梯度符号法制作的不可检测的图像地图扰动可以导致预测误差大幅增加，从而破坏自动驾驶系统的轨迹预测性能。

    

    对基于学习的轨迹预测器的对抗攻击已经得到了证明，但是关于除了状态历史以外的轨迹预测器输入的扰动效果以及这些攻击对下游规划和控制的影响仍然存在一些问题。在本文中，我们对两个轨迹预测模型Trajectron++和AgentFormer进行了敏感性分析。我们观察到，在所有的输入中，Trajectron++的几乎所有扰动敏感性仅限于最近的状态历史时间点，而AgentFormer的扰动敏感性则分布在随时间变化的状态历史中。我们还演示了尽管对状态历史的扰动具有主导的敏感性，但使用快速梯度符号法制作的不可检测的图像地图扰动可以导致这两个模型中的预测误差大幅增加。尽管图像地图对于这两个模型的预测输出可能只有轻微的贡献，但这个结果揭示了一个问题：攻击者可以通过扰动图像地图来破坏自动驾驶系统的轨迹预测性能。

    Adversarial attacks on learning-based trajectory predictors have already been demonstrated. However, there are still open questions about the effects of perturbations on trajectory predictor inputs other than state histories, and how these attacks impact downstream planning and control. In this paper, we conduct a sensitivity analysis on two trajectory prediction models, Trajectron++ and AgentFormer. We observe that between all inputs, almost all of the perturbation sensitivities for Trajectron++ lie only within the most recent state history time point, while perturbation sensitivities for AgentFormer are spread across state histories over time. We additionally demonstrate that, despite dominant sensitivity on state history perturbations, an undetectable image map perturbation made with the Fast Gradient Sign Method can induce large prediction error increases in both models. Even though image maps may contribute slightly to the prediction output of both models, this result reveals that
    
[^100]: 在社会和司法约束下对深度学习进行数学算法设计: 算法透明性要求

    Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement. (arXiv:2401.10310v1 [cs.LG])

    [http://arxiv.org/abs/2401.10310](http://arxiv.org/abs/2401.10310)

    这篇论文探讨了在社会和司法约束下，对深度学习进行数学算法设计的挑战。研究者提出了算法透明性的要求，并使用数学框架来分析在计算模型中实现透明实施的可行性。

    

    深度学习在可信度方面仍然存在缺陷，这描述了一种可理解、公平、安全和可靠的方法。为了减轻人工智能的潜在风险，通过监管指南提出了与可信度相关的明确义务，例如,在欧洲AI法案中。因此，一个核心问题是可以实现多大程度上的可信度深度学习。建立构成可信度的描述性属性要求能够追溯影响算法计算的因素，即算法的实现是透明的。受当前深度学习模型演化需要改变计算技术的观察启发，我们得出一个数学框架，使我们能够分析在计算模型中是否有可能实现透明实施。我们应用我们的可信度框架来分析数字和模拟计算模型中逆问题的深度学习方法。

    Deep learning still has drawbacks in terms of trustworthiness, which describes a comprehensible, fair, safe, and reliable method. To mitigate the potential risk of AI, clear obligations associated to trustworthiness have been proposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a central question is to what extent trustworthy deep learning can be realized. Establishing the described properties constituting trustworthiness requires that the factors influencing an algorithmic computation can be retraced, i.e., the algorithmic implementation is transparent. Motivated by the observation that the current evolution of deep learning models necessitates a change in computing technology, we derive a mathematical framework which enables us to analyze whether a transparent implementation in a computing model is feasible. We exemplarily apply our trustworthiness framework to analyze deep learning approaches for inverse problems in digital and analog computing models represe
    
[^101]: 物理约束卷积神经网络用于时空偏微分方程中的反问题

    Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations. (arXiv:2401.10306v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.10306](http://arxiv.org/abs/2401.10306)

    本研究提出了一种物理约束卷积神经网络（PC-CNN），用于解决非线性且时空变化的偏微分方程中的两种反问题。该网络可以揭示受偏差影响的真实状态，并在给定稀疏信息的情况下以高分辨率重建解。

    

    我们提出了一种物理约束卷积神经网络（PC-CNN）来解决偏微分方程中两种类型的反问题，这些方程在空间和时间上都是非线性且变化的。在第一个反问题中，我们给出了受空间变化的系统误差（即偏差，也称为认识不确定性）偏移的数据。任务是从偏差数据中揭示真实状态，即PDE的解。在第二个反问题中，我们给出了PDE解的稀疏信息。任务是以高分辨率重建空间中的解。首先，我们介绍了PC-CNN，它通过简单的时间窗口方案约束PDE来处理时序数据。其次，我们分析了PC-CNN在从偏差数据中揭示解的性能。我们分析了线性和非线性对流扩散方程以及纳维-斯托克斯方程，后者描述了湍流流动的时空混沌动力学。

    We propose a physics-constrained convolutional neural network (PC-CNN) to solve two types of inverse problems in partial differential equations (PDEs), which are nonlinear and vary both in space and time. In the first inverse problem, we are given data that is offset by spatially varying systematic error (i.e., the bias, also known the epistemic uncertainty). The task is to uncover from the biased data the true state, which is the solution of the PDE. In the second inverse problem, we are given sparse information on the solution of a PDE. The task is to reconstruct the solution in space with high-resolution. First, we present the PC-CNN, which constrains the PDE with a simple time-windowing scheme to handle sequential data. Second, we analyse the performance of the PC-CNN for uncovering solutions from biased data. We analyse both linear and nonlinear convection-diffusion equations, and the Navier-Stokes equations, which govern the spatiotemporally chaotic dynamics of turbulent flows. W
    
[^102]: 通过手机传感器推断个性特征：一种机器学习方法

    Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach. (arXiv:2401.10305v1 [eess.SP])

    [http://arxiv.org/abs/2401.10305](http://arxiv.org/abs/2401.10305)

    该研究通过手机传感器收集的活动数据可靠地预测了个性特征，这些研究成果为社会科学研究提供了新的途径。通过使用智能手机传感和机器学习技术，可以以成本效益高、无问卷调查的方式对个性相关问题进行研究。这些发现有助于推动个性研究，并且可以为从业者和研究人员提供信息。

    

    该研究提供了证据表明，可以通过手机传感器收集的活动数据可靠地预测个性特征。通过加速度计记录和运动模式计算得出一组知情指标，我们能够在两类问题上预测用户的个性特征，达到了0.78的F1分数。鉴于手机收集数据的快速增长，我们的新颖个性特征指标为社会科学未来研究开辟了令人兴奋的途径。我们的研究结果揭示了不同行为模式，它们被证明有差异性地预测了五大人格特征。它们有潜力以成本效益高、无问卷调查的方式，在前所未有的规模上研究与个性相关的问题。总体而言，该论文展示了使用智能手机传感和机器学习技术获得丰富行为数据的组合如何帮助推动个性研究，并且可以向从业者和研究人员提供信息。

    This study provides evidence that personality can be reliably predicted from activity data collected through mobile phone sensors. Employing a set of well informed indicators calculable from accelerometer records and movement patterns, we were able to predict users' personality up to a 0.78 F1 score on a two class problem. Given the fast growing number of data collected from mobile phones, our novel personality indicators open the door to exciting avenues for future research in social sciences. Our results reveal distinct behavioral patterns that proved to be differentially predictive of big five personality traits. They potentially enable cost effective, questionnaire free investigation of personality related questions at an unprecedented scale. Overall, this paper shows how a combination of rich behavioral data obtained with smartphone sensing and the use of machine learning techniques can help to advance personality research and can inform both practitioners and researchers about th
    
[^103]: 论科学数据在机器学习中的公正和透明使用的准备情况

    On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])

    [http://arxiv.org/abs/2401.10304](http://arxiv.org/abs/2401.10304)

    本研究分析了科学数据文档如何满足机器学习社区和监管机构对其在机器学习技术中使用的需求，并提出了一套建议指南。

    

    为了确保机器学习系统的公平性和可信性，最近的立法举措和机器学习社区的相关研究指出需要记录用于训练机器学习模型的数据。此外，为了实现可重复性，许多科学领域的数据共享实践近年来也有了发展。在这个意义上，学术机构采用了这些实践，鼓励研究人员将他们的数据和技术文件发布在同行评议的出版物上，如数据论文。本研究分析了科学数据文档如何满足机器学习社区和监管机构对其在机器学习技术中使用的需求。我们对4041篇不同领域的数据论文样本进行了评估，评估其完整性和覆盖范围，并研究了近年来的趋势，特别关注了最多和最少被记录的方面。作为结果，我们提出了一套数据创建者的建议指南。

    To ensure the fairness and trustworthiness of machine learning (ML) systems, recent legislative initiatives and relevant research in the ML community have pointed out the need to document the data used to train ML models. Besides, data-sharing practices in many scientific domains have evolved in recent years for reproducibility purposes. In this sense, the adoption of these practices by academic institutions has encouraged researchers to publish their data and technical documentation in peer-reviewed publications such as data papers. In this study, we analyze how this scientific data documentation meets the needs of the ML community and regulatory bodies for its use in ML technologies. We examine a sample of 4041 data papers of different domains, assessing their completeness and coverage of the requested dimensions, and trends in recent years, putting special emphasis on the most and least documented dimensions. As a result, we propose a set of recommendation guidelines for data creato
    
[^104]: 用于复杂适应性系统中的现象检测的具有时空一致性学习的分层框架

    A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])

    [http://arxiv.org/abs/2401.10300](http://arxiv.org/abs/2401.10300)

    本研究提出了一个分层框架，通过学习系统和代理的表示，使用时空一致性学习来捕捉复杂适应性系统中的现象，并解决了现有方法不能捕捉空间模式和建模非线性关系的问题。

    

    在由交互代理组成的复杂适应性系统（CAS）中，现象是一种全局属性，在现实世界的动态系统中很普遍，例如网络层次的交通拥堵。检测它的形成和消散有助于监测系统的状态，并发出有害现象的警报信号。由于CAS没有集中式控制器，基于每个代理的局部观察来检测现象是可取但具有挑战性的。现有的工作不能捕捉与现象相关的空间模式，并且无法建模代理之间的非线性关系。本文提出了一个分层框架，通过学习系统表示和代理表示来解决这两个问题，其中时空一致性学习器针对代理的非线性关系和系统的复杂演化进行了定制。通过保留最新100个代理的状态和历史状态来学习代理和系统的表示，

    Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent's local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Especially, spatio-temporal encoders are tailored to capture agents' nonlinear relationships and the system's complex evolution. Representations of the agents and the system are learned by preserving the
    
[^105]: 从生成流的潜在空间中生成新的桥梁类型的尝试

    An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])

    [http://arxiv.org/abs/2401.10299](http://arxiv.org/abs/2401.10299)

    本研究通过归一化流生成潜在空间中的新桥梁类型，并解决了高维矩阵行列式计算和神经网络可逆变换的挑战。

    

    通过介绍不同分布之间的坐标和概率变换的示例，简明扼要地介绍了归一化流的基本原理。从随机变量函数的分布的角度解释了概率变换的本质，并引入了概率变换的缩放因子雅可比行列式。将数据集视为来自总体的样本，获取归一化流本质上是通过采样调查对总体的数值特征进行统计推断，然后使用最大似然估计方法建立损失函数。本文介绍了归一化流如何巧妙地解决了高维矩阵行列式计算和神经网络可逆变换两个主要应用挑战。利用三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集，构建了新的生成桥梁类型。

    Through examples of coordinate and probability transformation between different distributions, the basic principle of normalizing flow is introduced in a simple and concise manner. From the perspective of the distribution of random variable function, the essence of probability transformation is explained, and the scaling factor Jacobian determinant of probability transformation is introduced. Treating the dataset as a sample from the population, obtaining normalizing flow is essentially through sampling surveys to statistically infer the numerical features of the population, and then the loss function is established by using the maximum likelihood estimation method. This article introduces how normalizing flow cleverly solves the two major application challenges of high-dimensional matrix determinant calculation and neural network reversible transformation. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge, constr
    
[^106]: 从循环测量中检测动力学状态的机器学习方法

    Machine learning approach to detect dynamical states from recurrence measures. (arXiv:2401.10298v1 [physics.data-an])

    [http://arxiv.org/abs/2401.10298](http://arxiv.org/abs/2401.10298)

    这项研究提出了一种将机器学习方法与非线性时间序列分析相结合的方法，通过循环测量来对不同的动力学状态进行分类，发现衡量循环点密度的特征是最相关的，且训练后的算法能够成功预测动力学状态。

    

    我们将机器学习方法与非线性时间序列分析相结合，特别利用循环测量来对从时间序列中出现的各种动力学状态进行分类。我们在这项研究中实施了三种机器学习算法逻辑回归，随机森林和支持向量机。输入特征是从非线性时间序列的循环量化和对应的循环网络的特征量得出的。对于训练和测试，我们从标准非线性动力系统生成合成数据，并评估机器学习算法在将时间序列分类为周期性，混沌，超混沌或噪声类别方面的效率和性能。此外，我们探讨了分类方案中输入特征的显著性，并发现衡量循环点密度的特征最相关。此外，我们展示了经过训练的算法如何成功预测d

    We integrate machine learning approaches with nonlinear time series analysis, specifically utilizing recurrence measures to classify various dynamical states emerging from time series. We implement three machine learning algorithms Logistic Regression, Random Forest, and Support Vector Machine for this study. The input features are derived from the recurrence quantification of nonlinear time series and characteristic measures of the corresponding recurrence networks. For training and testing we generate synthetic data from standard nonlinear dynamical systems and evaluate the efficiency and performance of the machine learning algorithms in classifying time series into periodic, chaotic, hyper-chaotic, or noisy categories. Additionally, we explore the significance of input features in the classification scheme and find that the features quantifying the density of recurrence points are the most relevant. Furthermore, we illustrate how the trained algorithms can successfully predict the d
    
[^107]: 在约束场景中学习非远见的功率分配

    Learning Non-myopic Power Allocation in Constrained Scenarios. (arXiv:2401.10297v1 [eess.SP])

    [http://arxiv.org/abs/2401.10297](http://arxiv.org/abs/2401.10297)

    我们提出了一个学习非远见功率分配的框架，用于在临时约束下的干扰网络中进行高效的功率分配。我们采用了演员-评论家算法来解决这个受约束的序贯决策问题。

    

    我们提出了一个基于学习的框架，用于在临时约束下的自组干扰网络中进行高效的功率分配。最近，针对瞬时约束下最大化给定网络效用度量的最优功率分配问题引起了广泛关注。已经提出了几种可学习的算法，以获得快速、有效和接近最优的性能。然而，在整个时间耦合约束下，需要优化整个情节下的效用度量。在这种情况下，需要调节瞬时功率，以便在满足所有时刻的约束条件下优化给定的效用度。独立地解决每个实例将是远见的，因为长期约束无法调节这样的解决方案。相反，我们将其框架化为一个受约束的序贯决策问题，并采用演员-评论家算法来获得约束条件下的最优功率分配。

    We propose a learning-based framework for efficient power allocation in ad hoc interference networks under episodic constraints. The problem of optimal power allocation -- for maximizing a given network utility metric -- under instantaneous constraints has recently gained significant popularity. Several learnable algorithms have been proposed to obtain fast, effective, and near-optimal performance. However, a more realistic scenario arises when the utility metric has to be optimized for an entire episode under time-coupled constraints. In this case, the instantaneous power needs to be regulated so that the given utility can be optimized over an entire sequence of wireless network realizations while satisfying the constraint at all times. Solving each instance independently will be myopic as the long-term constraint cannot modulate such a solution. Instead, we frame this as a constrained and sequential decision-making problem, and employ an actor-critic algorithm to obtain the constrain
    
[^108]: 使用高斯混合机制的DP-SGD抽样的DP级别限制

    Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of Gaussians Mechanisms. (arXiv:2401.10294v1 [cs.CR])

    [http://arxiv.org/abs/2401.10294](http://arxiv.org/abs/2401.10294)

    本研究提供了一种计算DP-SGD组级别限制的方法，并且证明了这个方法在使用泊松抽样或固定批量大小抽样时是紧密的。

    

    我们提供了一种计算DP-SGD的组级别$(\epsilon, \delta)$-DP限制的方法，当使用泊松抽样或固定批量大小抽样时。在实现中，除了离散化错误之外，通过此过程计算的DP限制是紧密的（假设我们发布了每个中间迭代）。

    We give a procedure for computing group-level $(\epsilon, \delta)$-DP guarantees for DP-SGD, when using Poisson sampling or fixed batch size sampling. Up to discretization errors in the implementation, the DP guarantees computed by this procedure are tight (assuming we release every intermediate iterate).
    
[^109]: 在噪声存在的情况下，几何量子机器学习中的对称性破缺

    Symmetry breaking in geometric quantum machine learning in the presence of noise. (arXiv:2401.10293v1 [quant-ph])

    [http://arxiv.org/abs/2401.10293](http://arxiv.org/abs/2401.10293)

    本研究研究了噪声存在下EQNN模型的行为，并展示了对称性破缺随着层数和噪声强度呈线性增加的现象。同时，我们提供了增强EQNN模型对称性保护的策略。

    

    基于等变量量子神经网络（EQNN）的几何量子机器学习最近成为量子机器学习中的一个有前景的方向。尽管取得了令人鼓舞的进展，但目前的研究仍限于理论，并且尚未探索EQNN训练中硬件噪声的作用。本工作研究了EQNN模型在噪声存在的情况下的行为。我们展示了特定的EQNN模型可以在Pauli通道下保持等变性，而在阻尼通道下不可能实现。我们声称对称性破缺随着层数和噪声强度呈线性增长。我们通过数值模拟和高达64个量子比特的硬件实验数据支持我们的论断。此外，我们提供了在噪声存在的情况下增强EQNN模型对称性保护的策略。

    Geometric quantum machine learning based on equivariant quantum neural networks (EQNN) recently appeared as a promising direction in quantum machine learning. Despite the encouraging progress, the studies are still limited to theory, and the role of hardware noise in EQNN training has never been explored. This work studies the behavior of EQNN models in the presence of noise. We show that certain EQNN models can preserve equivariance under Pauli channels, while this is not possible under the amplitude damping channel. We claim that the symmetry breaking grows linearly in the number of layers and noise strength. We support our claims with numerical data from simulations as well as hardware up to 64 qubits. Furthermore, we provide strategies to enhance the symmetry protection of EQNN models in the presence of noise.
    
[^110]: 机器学习算法对地磁暴的早期预测

    Early Prediction of Geomagnetic Storms by Machine Learning Algorithms. (arXiv:2401.10290v1 [cs.LG])

    [http://arxiv.org/abs/2401.10290](http://arxiv.org/abs/2401.10290)

    本研究旨在利用大数据和机器学习算法可靠地尽早预测所有类型的地磁暴。通过融合全球多个地面站收集的关于太阳测量的不同方面的大量数据，并使用带有特征选择和数据降采样的随机森林回归方法对较小的地磁暴实例进行处理，我们实现了82.55%的准确度。

    

    地磁暴是指太阳风扰动地球磁层所引起的现象。地磁暴可能对卫星、电网和通信基础设施造成严重损害。美国每天大规模地磁暴的经济影响超过400亿美元。早期预测对于预防和减轻灾害至关重要。然而，目前的方法要么只能提前几小时预测但无法识别所有类型的地磁暴，要么只能在地磁暴发生前的短时间内进行预测，例如提前一小时。本研究旨在利用大数据和机器学习算法可靠地尽早预测所有类型的地磁暴。通过融合全球多个地面站收集的关于太阳测量的不同方面的大量数据，并使用带有特征选择和数据降采样的随机森林回归方法对较小的地磁暴实例进行处理（这些实例占据了大多数数据），我们在2021年收集的数据上实现了82.55%的准确度。

    Geomagnetic storms (GS) occur when solar winds disrupt Earth's magnetosphere. GS can cause severe damages to satellites, power grids, and communication infrastructures. Estimate of direct economic impacts of a large scale GS exceeds $40 billion a day in the US. Early prediction is critical in preventing and minimizing the hazards. However, current methods either predict several hours ahead but fail to identify all types of GS, or make predictions within short time, e.g., one hour ahead of the occurrence. This work aims to predict all types of geomagnetic storms reliably and as early as possible using big data and machine learning algorithms. By fusing big data collected from multiple ground stations in the world on different aspects of solar measurements and using Random Forests regression with feature selection and downsampling on minor geomagnetic storm instances (which carry majority of the data), we are able to achieve an accuracy of 82.55% on data collected in 2021 when making ear
    
[^111]: 光电神经处理器的设计与开发，用于在混合机器人中实现经图像检测训练过的神经网络的模拟

    Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics. (arXiv:2401.10289v1 [cs.ET])

    [http://arxiv.org/abs/2401.10289](http://arxiv.org/abs/2401.10289)

    本论文设计了一种光电神经处理器，用于模拟经图像检测训练过的生物神经网络在混合机器人中的应用。通过光遗传学实现精确激活的反向传播STDP算法，实现了与传统神经网络训练算法相媲美的准确度。

    

    神经网络已广泛应用于图像处理、运动控制、物体检测等各种处理应用中。生物神经网络具有低功耗、更快的处理速度和生物现实性的优势。光遗传学为生物神经元提供高度的空间和时间控制，并在训练活生生的神经网络方面有潜力。本研究提出了一种通过光遗传学实现精确激活的反向传播STDP算法间接训练的模拟活生生的神经网络，其准确度可与传统神经网络训练算法相媲美。

    Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.
    
[^112]: CLAN:基于对比学习的用于人体活动识别的新颖性检测框架

    CLAN: A Contrastive Learning based Novelty Detection Framework for Human Activity Recognition. (arXiv:2401.10288v1 [cs.LG])

    [http://arxiv.org/abs/2401.10288](http://arxiv.org/abs/2401.10288)

    CLAN是一种基于对比学习的新颖性检测框架，用于处理人体活动识别中的挑战，并构建对挑战具有不变性的已知活动的表示方法。

    

    在环境辅助生活中，从时间序列传感器数据进行人体活动识别主要集中于预定义的活动，往往忽略了新的活动模式。我们提出了CLAN，一种基于对比学习的新颖性检测框架，其中包含了不同类型的负样本对于人体活动识别。该框架针对人体活动特征的挑战进行了优化，包括时间和频率特征的重要性、复杂的活动动态、活动之间共享的特征，以及传感器模态的变化。该框架旨在构建对挑战具有不变性的已知活动的表示方法。为了生成合适的负样本对，它根据每个数据集的时间和频率特征选择数据增强方法。它通过对比和分类损失的表示学习以及基于评分函数的新颖性检测，从中导出针对无意义动态的关键表示。

    In ambient assisted living, human activity recognition from time series sensor data mainly focuses on predefined activities, often overlooking new activity patterns. We propose CLAN, a two-tower contrastive learning-based novelty detection framework with diverse types of negative pairs for human activity recognition. It is tailored to challenges with human activity characteristics, including the significance of temporal and frequency features, complex activity dynamics, shared features across activities, and sensor modality variations. The framework aims to construct invariant representations of known activity robust to the challenges. To generate suitable negative pairs, it selects data augmentation methods according to the temporal and frequency characteristics of each dataset. It derives the key representations against meaningless dynamics by contrastive and classification losses-based representation learning and score function-based novelty detection that accommodate dynamic number
    
[^113]: 具有离子电荷初始化的开源费米子神经网络

    Open-Source Fermionic Neural Networks with Ionic Charge Initialization. (arXiv:2401.10287v1 [cs.LG])

    [http://arxiv.org/abs/2401.10287](http://arxiv.org/abs/2401.10287)

    本文将FermiNet模型集成到开源库DeepChem中，并提出了新的初始化技术，用于解决离子的电子分配问题。

    

    在发现重要的分子和材料能量和特性方面，精确求解电子薛定谔方程起着重要作用。因此，解决具有大量电子的系统变得越来越重要。变分蒙特卡洛(VMC)方法，尤其是通过深度神经网络逼近的方法，在这方面具有潜力。在本文中，我们旨在将一种名为FermiNet的模型，一种后哈特里-福克(HF)深度神经网络(DNN)模型，集成到一个标准且广泛使用的开源库DeepChem中。我们还提出了新的初始化技术，以克服与离子超额或欠电子的分配相关的困难。

    Finding accurate solutions to the electronic Schr\"odinger equation plays an important role in discovering important molecular and material energies and characteristics. Consequently, solving systems with large numbers of electrons has become increasingly important. Variational Monte Carlo (VMC) methods, especially those approximated through deep neural networks, are promising in this regard. In this paper, we aim to integrate one such model called the FermiNet, a post-Hartree-Fock (HF) Deep Neural Network (DNN) model, into a standard and widely used open source library, DeepChem. We also propose novel initialization techniques to overcome the difficulties associated with the assignment of excess or lack of electrons for ions.
    
[^114]: 使用EEG和机器学习分析学习任务中的脑活动

    Analyzing Brain Activity During Learning Tasks with EEG and Machine Learning. (arXiv:2401.10285v1 [eess.SP])

    [http://arxiv.org/abs/2401.10285](http://arxiv.org/abs/2401.10285)

    本研究利用EEG和机器学习技术，分析了STEM活动中的脑活动，研究结果表明不同任务的分类可行性。研究发现右额叶在数学处理和规划中表现出色，左额叶在认知灵活性和心理灵活性方面表现出色，左颞顶叶在连接方面表现出色。通过研究脑活动与学习任务之间的关系，有助于对脑活动和学习的理解更深入。

    

    本研究旨在分析各种STEM活动中的脑活动，探索分类不同任务的可行性。收集了二十个受试者参与五种认知任务时的EEG脑数据，并将其分割为4秒的片段。然后分析了脑频率波的功率谱密度。通过使用XGBoost、Random Forest和Bagging Classifier测试不同的k间隔，发现Random Forest在间隔大小为两个时表现最佳，达到了91.07%的测试准确率。当利用所有四个EEG通道进行分析时，认知灵活性最为明显。任务特定的分类准确度显示右额叶在数学处理和规划方面表现出色，左额叶在认知灵活性和心理灵活性方面表现出色，左颞顶叶在连接方面表现出色。值得注意的是，在STEM活动中观察到了大量额叶和颞顶叶之间的连接。这项研究对于更深入地理解脑活动和学习任务之间的关系具有重要意义。

    This study aimed to analyze brain activity during various STEM activities, exploring the feasibility of classifying between different tasks. EEG brain data from twenty subjects engaged in five cognitive tasks were collected and segmented into 4-second clips. Power spectral densities of brain frequency waves were then analyzed. Testing different k-intervals with XGBoost, Random Forest, and Bagging Classifier revealed that Random Forest performed best, achieving a testing accuracy of 91.07% at an interval size of two. When utilizing all four EEG channels, cognitive flexibility was most recognizable. Task-specific classification accuracy showed the right frontal lobe excelled in mathematical processing and planning, the left frontal lobe in cognitive flexibility and mental flexibility, and the left temporoparietal lobe in connections. Notably, numerous connections between frontal and temporoparietal lobes were observed during STEM activities. This study contributes to a deeper understandi
    
[^115]: MorpheusNet：用于嵌入式在线系统的资源效率的睡眠阶段分类器

    MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems. (arXiv:2401.10284v1 [eess.SP])

    [http://arxiv.org/abs/2401.10284](http://arxiv.org/abs/2401.10284)

    MorpheusNet是一个资源效率的睡眠阶段分类器，可以在嵌入式系统上实时预测睡眠阶段，适用于边缘计算，消耗少量能源。

    

    睡眠阶段分类（SSC）是一项耗时的任务，需要专家检查数小时的电生理记录进行手动分类。这在利用睡眠阶段进行治疗目的时是一个限制因素。随着可穿戴设备的价格越来越可承受和扩张，自动化SSC可能使得规模扩大的睡眠基础治疗成为可能。深度学习作为一种潜在的自动化方法已经引起了越来越多的关注，先前的研究已经显示出与手动专家评分相当的准确性。然而，以前的方法需要大量的内存和计算资源，这限制了实时分类和在边缘部署模型的能力。为了填补这个空白，我们的目标是提供一个能够在实时预测睡眠阶段的模型，而不需要访问外部计算资源（例如移动手机、云）。该算法具有低功耗的特点，能够在嵌入式电池供电系统上使用。

    Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts to examine hours of electrophysiological recordings for manual classification. This is a limiting factor when it comes to leveraging sleep stages for therapeutic purposes. With increasing affordability and expansion of wearable devices, automating SSC may enable deployment of sleep-based therapies at scale. Deep Learning has gained increasing attention as a potential method to automate this process. Previous research has shown accuracy comparable to manual expert scores. However, previous approaches require sizable amount of memory and computational resources. This constrains the ability to classify in real time and deploy models on the edge. To address this gap, we aim to provide a model capable of predicting sleep stages in real-time, without requiring access to external computational sources (e.g., mobile phone, cloud). The algorithm is power efficient to enable use on embedded battery powered systems. Our
    
[^116]: 临床脑电图分类的窗口堆叠元模型

    Window Stacking Meta-Models for Clinical EEG Classification. (arXiv:2401.10283v1 [eess.SP])

    [http://arxiv.org/abs/2401.10283](http://arxiv.org/abs/2401.10283)

    本论文提出了一种窗口堆叠的元模型，解决了在临床脑电图分类中使用窗口化技术时的计算开销和标签不准确的问题。通过测试，我们的方法将基准准确率从89.8%提升到99.0%，为临床应用提供了突破性的性能。

    

    窗口化是脑电图机器学习分类和其他时间序列任务中常用的技术。然而，当使用这种技术时会遇到一个挑战：计算开销限制了学习整个记录或一组记录的全局关系。此外，窗口从其父记录继承的标签可能无法准确反映该窗口在单独情况下的内容。为了解决这些问题，我们引入了多阶段模型架构，结合了针对时间窗口化数据聚合的元学习原理。我们进一步测试了两种不同的策略来缓解这些问题：延长窗口和利用重叠来增加数据。我们的方法在Temple University Hospital Abnormal EEG Corpus (TUAB)上的测试中，将基准准确率从89.8％提升到99.0％。这一突破性表现超过了此数据集的先前性能预测，并为临床应用铺平了道路。

    Windowing is a common technique in EEG machine learning classification and other time series tasks. However, a challenge arises when employing this technique: computational expense inhibits learning global relationships across an entire recording or set of recordings. Furthermore, the labels inherited by windows from their parent recordings may not accurately reflect the content of that window in isolation. To resolve these issues, we introduce a multi-stage model architecture, incorporating meta-learning principles tailored to time-windowed data aggregation. We further tested two distinct strategies to alleviate these issues: lengthening the window and utilizing overlapping to augment data. Our methods, when tested on the Temple University Hospital Abnormal EEG Corpus (TUAB), dramatically boosted the benchmark accuracy from 89.8 percent to 99.0 percent. This breakthrough performance surpasses prior performance projections for this dataset and paves the way for clinical applications of
    
[^117]: BioDiffusion：用于生物医学信号合成的多功能扩散模型

    BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])

    [http://arxiv.org/abs/2401.10282](http://arxiv.org/abs/2401.10282)

    BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。

    

    生物医学信号的机器学习任务通常面临有限的数据可用性、不平衡的数据集、标签复杂性和测量噪声的干扰等问题。这些挑战经常阻碍机器学习算法的最佳训练。为了解决这些问题，我们引入了BioDiffusion，这是一种针对合成多变量生物医学信号进行优化的基于扩散的概率模型。BioDiffusion在产生高保真度、非稳态的多变量信号方面表现出色，可用于无条件、标签条件和信号条件生成等多个任务。利用这些合成的信号为上述挑战提供了一个显著的解决方案。我们的研究包括对合成数据质量进行的定性和定量评估，强调其在与生物医学信号相关的机器学习任务中提高准确性的能力。此外，与当前主流的时间系信息生成模型相比，BioDiffusion在质量和效率方面表现出更好的性能。

    Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
    
[^118]: EEGFormer: 实现可迁移和可解释的大规模脑电基础模型

    EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])

    [http://arxiv.org/abs/2401.10278](http://arxiv.org/abs/2401.10278)

    本论文提出了一种名为EEGFormer的脑电基础模型，通过在大规模的复合EEG数据上进行预训练，该模型能够学习到可迁移和可解释的通用表示，为脑电信号的分析提供了有力的工具。

    

    自我监督学习在自然语言处理和计算机视觉领域中被证明是一种高效的方法。在脑电图(EEG)数据等大量无标签数据存在的情况下，它也适用于脑信号，这些数据在从癫痫检测到波形分析等各种真实医学应用中存在。现有的利用自我监督学习进行EEG建模的工作主要集中在对应于单个下游任务的每个独立数据集的预训练上，这无法充分利用丰富的数据，而且可能会导致缺乏泛化性的次优解决方案。此外，这些方法依赖于难以理解的端到端模型学习。本文介绍了一种新颖的EEG基础模型，名为EEGFormer，它在大规模复合EEG数据上进行预训练。预训练模型不仅可以学习适应性能的EEG信号的通用表示，还可以提供可解释性。

    Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance 
    
[^119]: 基于迁徙鸟优化的文本分类特征选择研究

    Migrating Birds Optimization-Based Feature Selection for Text Classification. (arXiv:2401.10270v1 [cs.NE])

    [http://arxiv.org/abs/2401.10270](http://arxiv.org/abs/2401.10270)

    本文提出了一种基于迁徙鸟优化的特征选择方法，通过结合朴素贝叶斯作为内部分类器，解决了文本分类中特征选择的挑战。实验结果表明，与其他方法相比，该方法在特征减少和分类准确性方面具有明显优势。研究还提供了有关增强特征选择方法的宝贵见解，为文本分类提供了可扩展和有效的解决方案。

    

    本研究引入了一种新颖的方法，MBO-NB，在处理具有大量特征的文本分类中利用迁徙鸟优化（MBO）与朴素贝叶斯作为内部分类器来解决特征选择挑战。我们通过使用信息增益算法对原始数据进行预处理，从平均62221个特征降低到2089个，以便提高计算效率。实验证明，与其他现有技术相比，MBO-NB在特征减少方面具有效果，并强调了分类准确性的提高。朴素贝叶斯嵌入MBO中的成功整合提供了一个全面的解决方案。在与粒子群优化（PSO）进行个体对比时，MBO-NB在四种设置中平均表现优于6.9％。本研究为增强特征选择方法提供了有价值的见解，为文本分类提供了可扩展和有效的解决方案。

    This research introduces a novel approach, MBO-NB, that leverages Migrating Birds Optimization (MBO) coupled with Naive Bayes as an internal classifier to address feature selection challenges in text classification having large number of features. Focusing on computational efficiency, we preprocess raw data using the Information Gain algorithm, strategically reducing the feature count from an average of 62221 to 2089. Our experiments demonstrate MBO-NB's superior effectiveness in feature reduction compared to other existing techniques, emphasizing an increased classification accuracy. The successful integration of Naive Bayes within MBO presents a well-rounded solution. In individual comparisons with Particle Swarm Optimization (PSO), MBO-NB consistently outperforms by an average of 6.9% across four setups. This research offers valuable insights into enhancing feature selection methods, providing a scalable and effective solution for text classification
    
[^120]: 工业厂房智能状态监测: 方法论和不确定性管理策略综述

    Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])

    [http://arxiv.org/abs/2401.10266](http://arxiv.org/abs/2401.10266)

    本论文综述了工业厂房智能状态监测和故障检测和诊断方法，重点关注了Tennessee Eastman Process。调研总结了最流行和最先进的深度学习和机器学习算法，并探讨了算法的优劣势。还讨论了不平衡数据和无标记样本等挑战，以及深度学习模型如何应对。比较了不同算法在Tennessee Eastman Process上的准确性和规格。

    

    状态监测在现代工业系统的安全性和可靠性中起着重要作用。人工智能（AI）方法作为一种在工业应用中日益受到学术界和行业关注的增长主题和一种强大的故障识别方式。本文概述了工业厂房智能状态监测和故障检测和诊断方法，重点关注开源基准Tennessee Eastman Process（TEP）。在这项调查中，总结了用于工业厂房状态监测、故障检测和诊断的最流行和最先进的深度学习（DL）和机器学习（ML）算法，并研究了每种算法的优点和缺点。还涵盖了不平衡数据、无标记样本以及深度学习模型如何处理这些挑战。最后，比较了利用Tennessee Eastman Process的不同算法的准确性和规格。

    Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process 
    
[^121]: 最佳更新时间：基于风险敏感的年龄度量最小化

    The Best Time for an Update: Risk-Sensitive Minimization of Age-Based Metrics. (arXiv:2401.10265v1 [cs.IT])

    [http://arxiv.org/abs/2401.10265](http://arxiv.org/abs/2401.10265)

    该论文研究了基于风险敏感的年龄度量最小化问题，提出了一种新的风险状态概念和风险度量方法，并介绍了两种风险敏感策略。

    

    流行的量化传输数据质量的方法包括信息年龄（Age of Information，AoI），查询信息年龄（Query Age of Information，QAoI）和不正确信息年龄（Age of Incorrect Information，AoII）。我们考虑在点对点无线通信系统中使用这些度量，发送方监视一个进程并向接收方发送状态更新。我们的挑战是决定更新的最佳时间，平衡传输能量和接收方的年龄度量。由于高年龄度量值引起的不稳定系统状态等问题的固有风险，我们引入了新的风险状态的概念来表示年龄度量高的状态。我们使用这个新的风险状态概念来量化和最小化高年龄度量的风险，通过直接导出风险状态的频率作为一种新的风险度量。在此基础上，我们引入了两种针对AoI，QAoI和AoII的风险敏感策略。第一种策略使用系统知识，

    Popular methods to quantify transmitted data quality are the Age of Information (AoI), the Query Age of Information (QAoI), and the Age of Incorrect Information (AoII). We consider these metrics in a point-to-point wireless communication system, where the transmitter monitors a process and sends status updates to a receiver. The challenge is to decide on the best time for an update, balancing the transmission energy and the age-based metric at the receiver. Due to the inherent risk of high age-based metric values causing complications such as unstable system states, we introduce the new concept of risky states to denote states with high age-based metric. We use this new notion of risky states to quantify and minimize this risk of experiencing high age-based metrics by directly deriving the frequency of risky states as a novel risk-metric. Building on this foundation, we introduce two risk-sensitive strategies for AoI, QAoI and AoII. The first strategy uses system knowledge, i.e., chann
    
[^122]: 神经网络的零空间特性及其在图像隐写术中的应用

    Null Space Properties of Neural Networks with Applications to Image Steganography. (arXiv:2401.10262v1 [cs.CV])

    [http://arxiv.org/abs/2401.10262](http://arxiv.org/abs/2401.10262)

    本文研究了神经网络的零空间特性，发现了神经网络的固有弱点，并针对此弱点提出了一种图像隐写术方法。

    

    本文研究了神经网络的零空间特性。我们将零空间定义从线性映射扩展到非线性映射，并讨论了神经网络中零空间的存在。给定神经网络的零空间可以告诉我们哪些输入数据对最终预测没有任何贡献，从而可以利用它来欺骗神经网络。这揭示了神经网络中的固有弱点，可以被利用。本文描述了一个应用，即图像隐写术的方法。通过在诸如MNIST之类的图像数据集上进行实验证明，我们可以使用零空间成分来强制神经网络选择所选的隐藏图像类，即使整体图像看起来完全不同。最后，我们通过显示人类观察者能够看到的内容与神经网络用于进行预测的图像部分之间的比较，展示了神经网络的“观测”情况。

    This paper explores the null space properties of neural networks. We extend the null space definition from linear to nonlinear maps and discuss the presence of a null space in neural networks. The null space of a given neural network can tell us the part of the input data that makes no contribution to the final prediction so that we can use it to trick the neural network. This reveals an inherent weakness in neural networks that can be exploited. One application described here leads to a method of image steganography. Through experiments on image datasets such as MNIST, we show that we can use null space components to force the neural network to choose a selected hidden image class, even though the overall image can be made to look like a completely different image. We conclude by showing comparisons between what a human viewer would see, and the part of the image that the neural network is actually using to make predictions and, hence, show that what the neural network ``sees'' is com
    
[^123]: 课程设计帮助脉冲神经网络对时间序列进行分类

    Curriculum Design Helps Spiking Neural Networks to Classify Time Series. (arXiv:2401.10257v1 [cs.NE])

    [http://arxiv.org/abs/2401.10257](http://arxiv.org/abs/2401.10257)

    本文研究了脉冲神经网络（SNNs）上课程学习（Curriculum Learning）的作用，通过设计一种名为CSNN的新方法，在学习顺序和神经元活动编码方面模拟了人类学习的过程。实验证明，这种方法在多个时间序列数据上取得了优秀的分类结果。

    

    脉冲神经网络(SNNs)在建模时间序列数据方面比人工神经网络(ANNs)具有更大的潜力，这是由于其固有的神经元动力学和低能量消耗。然而，由于目前的努力主要集中在设计更好的网络结构上，很难证明它们在分类准确性上的优势。在这项工作中，我们借鉴了大脑启发式科学的思想，发现不仅结构，学习过程也应该像人类一样。为了实现这一点，我们通过设计一种名为CSNN的新方法，通过两个理论上保证的机制探索了课程学习(CL)在SNNs上的潜力：主动-休眠训练顺序使课程类似于人类的学习和适合脉冲神经元；基于值的区域编码使神经元的活动在学习连续数据时模拟大脑的记忆。我们在多个时间序列源上进行了实验，包括模拟、传感器、运动和医疗保健数据。

    Spiking Neural Networks (SNNs) have a greater potential for modeling time series data than Artificial Neural Networks (ANNs), due to their inherent neuron dynamics and low energy consumption. However, it is difficult to demonstrate their superiority in classification accuracy, because current efforts mainly focus on designing better network structures. In this work, enlighten by brain-inspired science, we find that, not only the structure but also the learning process should be human-like. To achieve this, we investigate the power of Curriculum Learning (CL) on SNNs by designing a novel method named CSNN with two theoretically guaranteed mechanisms: The active-to-dormant training order makes the curriculum similar to that of human learning and suitable for spiking neurons; The value-based regional encoding makes the neuron activity to mimic the brain memory when learning sequential data. Experiments on multiple time series sources including simulated, sensor, motion, and healthcare dem
    
[^124]: 使用机器学习算法进行马达加斯加实时国内生产总值预测

    Nowcasting Madagascar's real GDP using machine learning algorithms. (arXiv:2401.10255v1 [econ.GN])

    [http://arxiv.org/abs/2401.10255](http://arxiv.org/abs/2401.10255)

    该研究基于马达加斯加的宏观经济领先指标，使用不同的机器学习算法进行国内生产总值（GDP）预测，并发现集成模型相对于传统计量经济模型具有更高的准确性和实时性。

    

    我们研究了不同的机器学习算法在预测马达加斯加国内生产总值（GDP）方面的预测能力。我们使用了流行的回归模型进行训练，包括线性正则化回归（Ridge，Lasso，Elastic-net），降维模型（主成分回归），k最近邻算法（k-NN回归），支持向量回归（线性SVR）和基于树的集成模型（随机森林和XGBoost回归），并将简单的计量经济模型作为基准。我们通过计算均方根误差（RMSE），平均绝对误差（MAE）和平均绝对百分比误差（MAPE）来衡量每个模型的预测准确性。我们的研究结果表明，通过聚合个体预测形成的集成模型始终优于传统的计量经济模型。我们得出结论，机器学习模型可以提供更准确和及时的预测。

    We investigate the predictive power of different machine learning algorithms to nowcast Madagascar's gross domestic product (GDP). We trained popular regression models, including linear regularized regression (Ridge, Lasso, Elastic-net), dimensionality reduction model (principal component regression), k-nearest neighbors algorithm (k-NN regression), support vector regression (linear SVR), and tree-based ensemble models (Random forest and XGBoost regressions), on 10 Malagasy quarterly macroeconomic leading indicators over the period 2007Q1--2022Q4, and we used simple econometric models as a benchmark. We measured the nowcast accuracy of each model by calculating the root mean square error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). Our findings reveal that the Ensemble Model, formed by aggregating individual predictions, consistently outperforms traditional econometric models. We conclude that machine learning models can deliver more accurate and timely
    
[^125]: 超越画面：使用用户定义的长度的单个和多个视频摘要方法

    Beyond the Frame: Single and mutilple video summarization method with user-defined length. (arXiv:2401.10254v1 [cs.CV])

    [http://arxiv.org/abs/2401.10254](http://arxiv.org/abs/2401.10254)

    本文研究了使用多种NLP技术和视频处理技术将长视频转换为摘要视频的方法，以减少观看/回顾视频所需的时间。

    

    视频摘要是一种减少视频观看/回顾时间的关键方法。随着发表的视频数量每天都在增加，这种方法变得越来越重要。可以使用多种技术，从多模态音视频技术到自然语言处理方法，将单个或多个视频总结为相对较短的视频。音视频技术可用于识别重要的视觉事件并选择最重要的部分，而NLP技术可用于评估音频转录并从原始视频中提取主要句子（时间戳）和相应的视频帧。另一种方法是同时使用领域中的最佳技术。这意味着我们可以同时使用音视频线索和视频转录来提取和总结视频。在本文中，我们将各种NLP技术（抽取和基于内容的摘要生成器）与视频处理技术相结合，将长视频转换为摘要视频。

    Video smmarization is a crucial method to reduce the time of videos which reduces the spent time to watch/review a long video. This apporach has became more important as the amount of publisehed video is increasing everyday. A single or multiple videos can be summarized into a relatively short video using various of techniques from multimodal audio-visual techniques, to natural language processing approaches. Audiovisual techniques may be used to recognize significant visual events and pick the most important parts, while NLP techniques can be used to evaluate the audio transcript and extract the main sentences (timestamps) and corresponding video frames from the original video. Another approach is to use the best of both domain. Meaning that we can use audio-visual cues as well as video transcript to extract and summarize the video. In this paper, we combine a variety of NLP techniques (extractive and contect-based summarizers) with video processing techniques to convert a long video 
    
[^126]: 混合任务元学习：一种用于可扩展和可转移带宽分配的图神经网络方法

    Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation. (arXiv:2401.10253v1 [cs.NI])

    [http://arxiv.org/abs/2401.10253](http://arxiv.org/abs/2401.10253)

    本文提出了一种基于图神经网络的混合任务元学习算法，用于可扩展和可转移的带宽分配。通过引入GNN和HML算法，该方法在不同的通信场景下具有较好的性能和采样效率。

    

    本文提出了一种基于深度学习的带宽分配策略，该策略具有以下特点：1）随着用户数量的增加具有可扩展性；2）能够在不同的通信场景下进行转移，例如非平稳的无线信道、不同的服务质量要求和动态可用资源。为了支持可扩展性，带宽分配策略采用了图神经网络（GNN）进行表示，训练参数的数量随用户数量的增加而不变。为了实现GNN的泛化能力，我们开发了一种混合任务元学习（HML）算法，在元训练过程中使用不同的通信场景来训练GNN的初始参数。然后，在元测试过程中，使用少量样本对GNN进行微调以适应未见过的通信场景。仿真结果表明，与现有基准相比，我们的HML方法可以将初始性能提高8.79％，并提高采样效率73％。在微调后，

    In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\%$, and sampling efficiency by $73\%$, compared with existing benchmarks. After fine-tuning,
    
[^127]: 扩散模型的分辨率色谱层析

    Resolution Chromatography of Diffusion Models. (arXiv:2401.10247v1 [cs.CV])

    [http://arxiv.org/abs/2401.10247](http://arxiv.org/abs/2401.10247)

    本文介绍了分辨率色谱层析，用于解释生成过程中的粗糙到细致行为和设计时间依赖的调制。通过实验证明了该理论，并提出了一些应用方法。

    

    扩散模型通过迭代随机过程生成高分辨率图像。特别地，去噪方法是最流行的方法之一，它在每个时间步骤中预测样本中的噪声并对其进行去噪。常常观察到生成样本的分辨率随时间而变化，开始模糊和粗糙，然后变得更加清晰和细致。本文引入了"分辨率色谱层析"的概念，指示了每个分辨率的信号生成速率，这对于数学上解释生成过程中粗粒到细粒行为、理解噪声时间表的作用以及设计时间依赖的调制非常有帮助。利用分辨率色谱层析，我们确定了在特定时间步骤中哪个分辨率水平变得主导，并通过文本到图像扩散模型实验证明了我们的理论。我们还提出了一些直接应用该概念的方法：将预训练模型升级到更高的分辨率等。

    Diffusion models generate high-resolution images through iterative stochastic processes. In particular, the denoising method is one of the most popular approaches that predicts the noise in samples and denoises it at each time step. It has been commonly observed that the resolution of generated samples changes over time, starting off blurry and coarse, and becoming sharper and finer. In this paper, we introduce "resolution chromatography" that indicates the signal generation rate of each resolution, which is very helpful concept to mathematically explain this coarse-to-fine behavior in generation process, to understand the role of noise schedule, and to design time-dependent modulation. Using resolution chromatography, we determine which resolution level becomes dominant at a specific time step, and experimentally verify our theory with text-to-image diffusion models. We also propose some direct applications utilizing the concept: upscaling pre-trained models to higher resolutions and 
    
[^128]: 零泡沫管道并行性

    Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])

    [http://arxiv.org/abs/2401.10241](http://arxiv.org/abs/2401.10241)

    本论文介绍了一种新的调度策略，在同步训练语义下成功实现了零管道泡沫，通过将反向计算分为两部分，设计了优于基线方法的新颖管道调度。此外，还提出了一种自动找到最优调度的算法，并引入新颖技术绕过同步操作实现零泡沫。实验证明，在相似条件下，本方法在吞吐量方面优于1F1B调度23%。

    

    管道并行性是大规模分布式训练的关键组成部分之一，然而其效率受到被视为不可避免的管道泡沫的影响。在这项工作中，我们引入了一种调度策略，据我们所知，这是第一次在同步训练语义下成功实现零管道泡沫。这个改进背后的关键思想是将反向计算分为两部分，一部分计算输入的梯度，另一部分计算参数的梯度。基于这个思想，我们手工设计了新颖的管道调度方法，显著优于基线方法。我们还开发了一个根据特定的模型配置和内存限制自动找到最优调度的算法。此外，为了真正实现零泡沫，我们引入了一种新颖的技术，在优化器步骤中绕过同步操作。实验评估表明，我们的方法在吞吐量方面比1F1B调度高出多达23%。

    Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a simila
    
[^129]: 加密货币交易和在线金融论坛的相互作用

    Interplay between Cryptocurrency Transactions and Online Financial Forums. (arXiv:2401.10238v1 [q-fin.GN])

    [http://arxiv.org/abs/2401.10238](http://arxiv.org/abs/2401.10238)

    本研究关注加密货币论坛与加密货币价值波动之间的相互作用，发现Bitcointalk论坛的活动与比特币的趋势有直接关系。

    

    加密货币是一种使用密码技术提供安全性和匿名性的数字货币。尽管加密货币代表了一项突破并提供了一些重要的好处，但它们的使用也带来了一些风险，这是因为缺乏监管机构和透明度所导致的。由于虚假信息和价格波动会让个人投资者感到沮丧，因此加密货币的出现与在线用户社区和论坛的蓬勃发展同时进行，这些地方可以共享信息以减轻用户的不信任。本研究重点研究了加密货币论坛与加密货币价值波动之间的相互作用。具体来说，我们分析了最流行的加密货币比特币（BTC）和相关活跃的讨论社区Bitcointalk。研究表明，Bitcointalk论坛的活动与BTC的趋势保持直接关系，因此分析这种交互作用将是一个完美的方法。

    Cryptocurrencies are a type of digital money meant to provide security and anonymity while using cryptography techniques. Although cryptocurrencies represent a breakthrough and provide some important benefits, their usage poses some risks that are a result of the lack of supervising institutions and transparency. Because disinformation and volatility is discouraging for personal investors, cryptocurrencies emerged hand-in-hand with the proliferation of online users' communities and forums as places to share information that can alleviate users' mistrust. This research focuses on the study of the interplay between these cryptocurrency forums and fluctuations in cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin (BTC) and a related active discussion community, Bitcointalk, are analyzed. This study shows that the activity of Bitcointalk forum keeps a direct relationship with the trend in the values of BTC, therefore analysis of this interaction would be a perfec
    
[^130]: 分而不忘：连续学习中选择性训练专家的集成方法

    Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])

    [http://arxiv.org/abs/2401.10191](http://arxiv.org/abs/2401.10191)

    连续学习中，我们提出了一种名为SEED的新方法，通过选择性训练最优的专家来解决遗忘和计算负担的问题，并在实验中展示了其高性能。

    

    随着类增量学习的流行，模型能够拓宽应用范围，同时不忘记已经学到的知识。在这个领域中的一个趋势是使用混合专家技术，不同的模型共同解决任务。然而，这些专家通常会一次性使用整个任务的数据进行训练，这样会增加遗忘的风险和计算负担。为了解决这个问题，我们提出了一种名为SEED的新方法。SEED仅选择一个被认为最优的专家来处理给定的任务，并使用该任务的数据对这个专家进行微调。为此，每个专家用高斯分布表示每个类别，并根据这些分布的相似性选择最优专家。因此，SEED在保持集成方法的高稳定性的同时增加了专家之间的多样性和异质性。大量实验证明SEED达到了最先进的性能。

    Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performan
    
[^131]: 深度和宽度在神经ODE插值中的相互作用

    Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])

    [http://arxiv.org/abs/2401.09902](http://arxiv.org/abs/2401.09902)

    本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\varepsilon$的关系有关。

    

    神经常微分方程(neural ODEs)已经成为从控制角度进行监督学习的自然工具，然而对其最佳结构的完全理解仍然难以捉摸。在这项工作中，我们研究了宽度$p$和层之间的过渡次数$L$（实际上是深度$L+1$）之间的相互作用。具体来说，我们评估了模型的表达能力，以其能够在Wasserstein误差边界$\varepsilon>0$内插值一个包含$N$对点的有限数据集$D$或两个概率测度在$\mathbb{R}^d$中。我们的发现揭示了$p$和$L$之间的平衡折衷关系，在数据集插值中，$L$随着$O(1+N/p)$的比例增长，而在测度插值中，$L=O\left(1+(p\varepsilon^d)^{-1}\right)$。在自主情况下，$L=0$，需要进行单独的研究，我们将重点关注数据集插值。我们解决了$\varepsilon$-近似控制性的放松问题，并建立了误差

    Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon>0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error 
    
[^132]: 一种快速、高性能、安全的分布式训练框架用于大型语言模型

    A Fast, Performant, Secure Distributed Training Framework For Large Language Model. (arXiv:2401.09796v1 [cs.LG])

    [http://arxiv.org/abs/2401.09796](http://arxiv.org/abs/2401.09796)

    本文提出了一种基于模型切片的安全分布式语言模型训练框架。通过在客户端和服务器端部署可信执行环境（TEE）并使用轻量级加密进行安全通信，解决了恶意窃取模型参数和数据的问题。此外，采用分割微调和稀疏化参数微调的方法，在降低设备成本的同时提高了模型性能和准确性。

    

    分布式（联邦式）LLM是一种重要的方法，用于使用隔离数据共同训练领域特定的LLM。然而，恶意地从服务器或客户端窃取模型参数和数据已经成为一个迫切需要解决的问题。在本文中，我们提出了一种基于模型切片的安全分布式LLM。在这种情况下，我们在客户端和服务器端都部署了可信执行环境（TEE），并将微调的结构（LoRA或P-tuning v2的嵌入）放入TEE中。然后，通过轻量级加密，在TEE和通用环境中进行安全通信。为了进一步降低设备成本并提高模型性能和准确性，我们提出了一种分割微调方案。特别是，我们通过层次进行LLM的分割，将后面的层次放在服务器端TEE中（客户端不需要TEE）。然后，我们将提出的稀疏化参数微调（SPF）与LoRA部分相结合，以提高推断的准确性。

    The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the down
    
[^133]: 将图像特征输入到神经网络的每一层的模仿学习

    Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])

    [http://arxiv.org/abs/2401.09691](http://arxiv.org/abs/2401.09691)

    本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。

    

    模仿学习使得机器人能够通过训练数据学习并复制人类行为。最近机器学习的进展使得能够直接处理高维观测数据（如图像）的端到端学习方法成为可能。然而，在处理多个模态的数据时，这些方法面临着一个关键挑战，即在使用短采样周期时无意中忽略与期望输出的相关性较低的数据。本文提出了一种有效的方法来解决这个挑战，通过将数据输入到每个神经网络层中，放大与输出相关性较低的数据的影响。所提出的方法有效地将多样的数据源纳入到学习过程中。通过使用原始图像和关节信息作为输入进行简单的拾取放置操作的实验，即使处理来自短采样周期的数据，也证明了成功率显著提高。

    Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
    
[^134]: CFASL：用于变分自编码器中的解缠学习的复合因子对齐对称学习

    CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])

    [http://arxiv.org/abs/2401.08897](http://arxiv.org/abs/2401.08897)

    CFASL是一种用于解缠学习的新方法，它将对称性学习与VAE集成，无需任何数据集因子信息的先验知识，具有三个新特征：对齐潜在向量维度到可学习对称代码簿中的对称性，学习复合对称性来表达未知因素的变化，以及引入群等变编码器和解码器来训练VAE。

    

    输入和潜在向量的对称性为VAE中的解缠学习提供了宝贵的见解。然而，只有少数几篇论文提出了一种无监督方法，甚至这些方法在训练数据中也需要已知的因子信息。我们提出了一种新的方法，Composite Factor-Aligned Symmetry Learning (CFASL)，将其集成到VAE中，用于学习基于对称性的解缠，无监督学习中不需要任何数据集因子信息的知识。CFASL包括三个用于学习基于对称性的解缠的新特征：1)注入归纳偏置，将潜在向量维度对齐到明确可学习的对称代码簿中的因子对齐对称性；2)学习一个复合对称性，通过学习代码簿中的因子对齐对称性，来表达两个随机样本之间的未知因素的变化；3)在训练VAE时，引入具有群等变编码器和解码器的两个条件。此外，我们提出了一种扩展的评估指标。

    Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
    
[^135]: 高效且可验证地从毒化攻击中恢复联邦学习中的模型

    Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning. (arXiv:2401.08216v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.08216](http://arxiv.org/abs/2401.08216)

    本文提出了一种高效且可验证地从恶意客户端的毒化攻击中恢复联邦学习模型的方法，通过选择性利用历史信息以及未受恶意客户端影响的历史模型，能够在恶意客户端被识别后实现准确的全局模型恢复。

    

    联邦学习面临着毒化攻击的威胁，恶意客户端通过操纵更新来影响全局模型。目前存在各种方法用于检测这些恶意客户端，但识别恶意客户端需要足够的模型更新，因此一旦检测到恶意客户端，联邦学习模型已经被污染。因此，需要一种在识别恶意客户端后恢复准确全局模型的方法。当前的恢复方法依赖于（i）参与联邦学习的所有历史信息和（ii）未受到恶意客户端影响的初始模型，导致对存储和计算资源的需求很高。本文中，我们展示了在选择性历史信息而不是所有历史信息以及基于未被恶意客户端显著影响的历史模型而不是初始模型的情况下，仍然可以实现高效的恢复。

    Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaini
    
[^136]: Vision Transformer中的注意力图统计检验

    Statistical Test for Attention Map in Vision Transformer. (arXiv:2401.08169v1 [stat.ML])

    [http://arxiv.org/abs/2401.08169](http://arxiv.org/abs/2401.08169)

    本研究提出了一种Vision Transformer中注意力图的统计检验方法，可以将注意力作为可靠的定量证据指标用于决策，并通过p值进行统计显著性量化。

    

    Vision Transformer（ViT）在各种计算机视觉任务中展示出了出色的性能。注意力对于ViT捕捉图像补丁之间复杂广泛的关系非常重要，使得模型可以权衡图像补丁的重要性，并帮助我们理解决策过程。然而，当将ViT的注意力用作高风险决策任务（如医学诊断）中的证据时，面临一个挑战，即注意机制可能错误地关注无关的区域。在本研究中，我们提出了一种ViT注意力的统计检验，使我们能够将注意力作为可靠的定量证据指标用于ViT的决策，并严格控制误差率。使用选择性推理框架，我们以p值的形式量化注意力的统计显著性，从而能够理论上基于假阳性检测概率量化注意力。

    The Vision Transformer (ViT) demonstrates exceptional performance in various computer vision tasks. Attention is crucial for ViT to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. However, when utilizing the attention of ViT as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. In this study, we propose a statistical test for ViT's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for ViT's decision-making with a rigorously controlled error rate. Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentio
    
[^137]: 概率Lambert问题的解决方案：与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程的连接

    Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs. (arXiv:2401.07961v1 [math.OC])

    [http://arxiv.org/abs/2401.07961](http://arxiv.org/abs/2401.07961)

    这项研究将概率Lambert问题与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程等领域连接起来，从而解决了概率Lambert问题的解的存在和唯一性，并提供了数值求解的方法。

    

    Lambert问题涉及通过速度控制在规定的飞行时间内将航天器从给定的初始位置转移到给定的终端位置，受到重力力场的限制。我们考虑了Lambert问题的概率变种，其中位置向量的端点约束的知识被它们各自的联合概率密度函数所替代。我们证明了具有端点联合概率密度约束的Lambert问题是一个广义的最优质量传输（OMT）问题，从而将这个经典的天体动力学问题与现代随机控制和随机机器学习的新兴研究领域联系起来。这个新发现的连接使我们能够严格建立概率Lambert问题的解的存在性和唯一性。同样的连接还帮助通过扩散正规化数值求解概率Lambert问题，即通过进一步的连接来利用。

    Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection 
    
[^138]: 输入凸性Lipschitz RNN: 一种用于工程任务的快速和鲁棒的方法

    Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks. (arXiv:2401.07494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07494](http://arxiv.org/abs/2401.07494)

    通过结合输入凸性和Lipschitz连续性的优势，我们开发了一种名为输入凸性Lipschitz循环神经网络的新型网络结构，在计算效率和对抗鲁棒性方面优于现有的循环单元，并适用于多种工程任务。

    

    计算效率和对抗鲁棒性是真实世界工程应用中的关键因素。然而，传统的神经网络往往在同时或分别解决这两个问题方面存在不足。通过从自然物理系统和现有文献中获取的见解，已知输入凸性结构增强了计算效率，而Lipschitz约束结构增强了对抗鲁棒性。通过利用凸性和Lipschitz连续性的优点，我们开发了一种新的网络结构，称为输入凸性Lipschitz循环神经网络。该模型在计算效率和对抗鲁棒性方面表现优于现有的循环单元，适用于一系列工程任务，包括基准MNIST图像分类、新加坡LHT Holdings公司的实际太阳能光伏系统规划中的实时太阳辐射预测，以及化学反应器的实时模型预测控制优化等。

    Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.
    
[^139]: 深度高效的私密领域生成用于子图联邦学习

    Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])

    [http://arxiv.org/abs/2401.04336](http://arxiv.org/abs/2401.04336)

    本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。

    

    在现实应用中，巨大图通常以非中心化子图的形式由多个数据所有者分散存储。为了保护数据隐私，在不损害数据隐私的前提下，考虑到子图联邦学习（subgraph FL）场景是很自然的，其中每个本地客户端持有整个全局图的子图，以获取全局一般化的图挖掘模型。为了解决由于缺少跨子图邻居而导致的局部子图上的信息传播不完整的独特挑战，以前的工作通过缺失邻居生成器和GNN的联合FL来增加本地邻域。然而，它们在FL的效用性、效率性和隐私目标方面存在深层次的限制。在这项工作中，我们提出了FedDEP来全面解决子图FL中的这些挑战。FedDEP包括一系列新颖的技术设计：(1) 利用潜在缺失邻居的GNN嵌入进行深度邻居生成；(2) Effic...

    Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
    
[^140]: 使用感知损失的扩散模型

    Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00110](http://arxiv.org/abs/2401.00110)

    本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。

    

    使用均方误差损失训练的扩散模型倾向于生成不真实的样本。目前的最先进模型依靠无分类器指导来改善样本质量，然而其惊人的效果尚未完全理解。本文中，我们展示了无分类器指导的有效性在一定程度上源自其作为一种隐式感知指导的形式。因此，我们可以直接在扩散训练中加入感知损失来提高样本质量。由于扩散训练中使用的分数匹配目标与无监督训练感知网络时使用的去噪自动编码器目标非常相似，因此扩散模型本身就是一个感知网络，并可以用于生成有意义的感知损失。我们提出了一种新颖的自感知目标，其结果是扩散模型能够生成更真实的样本。对于条件生成，我们的方法仅改善样本质量，而不与条件绑定。

    Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
    
[^141]: 隐私保护的神经图数据库

    Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2312.15591](http://arxiv.org/abs/2312.15591)

    隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。

    

    在大数据和快速发展的信息系统时代，高效准确地检索数据变得日益重要。神经图数据库（NGDB）是一种强大的范式，将图数据库（图形数据库）和神经网络的优势相结合，实现了对图结构数据的高效存储、检索和分析。神经嵌入存储和复杂神经逻辑查询回答为NGDB提供了泛化能力。当图形不完整时，神经图数据库可以通过提取潜在模式和表示来填补图结构中的空缺，揭示隐藏的关系并实现准确的查询回答。然而，这种能力也带来了潜在的隐私风险，因为恶意攻击者可以使用精心设计的组合查询推断出更多敏感信息，例如通过比较图数据库中Turing奖得主的答案集。

    In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
    
[^142]: 使用镜像下降的多功能损失几何元进行快速自适应的元学习

    Meta-Learning with Versatile Loss Geometries for Fast Adaptation Using Mirror Descent. (arXiv:2312.13486v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13486](http://arxiv.org/abs/2312.13486)

    本论文提出了一种使用镜像下降的多功能损失几何元的元学习方法，通过学习非线性镜像映射来快速适应提取的先验知识，以便在少量优化步骤内训练特定任务的模型。在少样本学习数据集上的实验中表明，该方法具有更强的表达能力和收敛性。

    

    元学习是一种基于相关任务中提取的任务不变先验知识的原则性框架，它可以在数据记录有限的情况下训练新任务。元学习中的一个基本挑战是如何快速“调整”提取的先验知识，以便在几次优化步骤内训练特定任务的模型。现有方法使用预处理器来处理这个挑战，以增强每个任务训练过程的收敛性。尽管在表示二次训练损失时有效，但这些简单的线性预处理器很难捕捉复杂的损失几何元。本文通过学习非线性镜像映射来解决这个限制，该映射引入了一种多功能距离度量，可以捕捉和优化各种损失几何元，从而促进每个任务的训练。在几种少样本学习数据集上的数值测试表明，所提出的方法具有更强的表达能力和收敛性。

    Utilizing task-invariant prior knowledge extracted from related tasks, meta-learning is a principled framework that empowers learning a new task especially when data records are limited. A fundamental challenge in meta-learning is how to quickly "adapt" the extracted prior in order to train a task-specific model within a few optimization steps. Existing approaches deal with this challenge using a preconditioner that enhances convergence of the per-task training process. Though effective in representing locally a quadratic training loss, these simple linear preconditioners can hardly capture complex loss geometries. The present contribution addresses this limitation by learning a nonlinear mirror map, which induces a versatile distance metric to enable capturing and optimizing a wide range of loss geometries, hence facilitating the per-task training. Numerical tests on few-shot learning datasets demonstrate the superior expressiveness and convergence of the advocated approach.
    
[^143]: 通过条件玻尔兹曼生成器进行分子GNN的预训练

    Pre-training of Molecular GNNs via Conditional Boltzmann Generator. (arXiv:2312.13110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13110](http://arxiv.org/abs/2312.13110)

    本研究提出了一种针对分子GNN的预训练方法，使用现有的分子构象数据集生成对多个构象通用的潜变量，以解决获取多个构象的计算成本问题。

    

    使用深度学习学习分子结构的表示是分子性质预测任务中的一个基本问题。分子本质上是存在于现实世界中的三维结构；此外，它们不是静态的，而是在三维欧几里德空间中不断运动，形成一个势能表面。因此，希望事先生成多个构象，并使用一个包含多个构象的4D-QSAR模型提取分子表示。然而，由于获取多个构象的计算成本，这种方法在药物和材料发现任务中是不可行的。为了解决这个问题，我们提出了一种使用现有分子构象数据集对分子GNN进行预训练的方法，从2D分子图生成对多个构象通用的潜变量。我们的方法称为玻尔兹曼GNN，通过最大化条件生成概率的条件边缘似然来建模。

    Learning representations of molecular structures using deep learning is a fundamental problem in molecular property prediction tasks. Molecules inherently exist in the real world as three-dimensional structures; furthermore, they are not static but in continuous motion in the 3D Euclidean space, forming a potential energy surface. Therefore, it is desirable to generate multiple conformations in advance and extract molecular representations using a 4D-QSAR model that incorporates multiple conformations. However, this approach is impractical for drug and material discovery tasks because of the computational cost of obtaining multiple conformations. To address this issue, we propose a pre-training method for molecular GNNs using an existing dataset of molecular conformations to generate a latent vector universal to multiple conformations from a 2D molecular graph. Our method, called Boltzmann GNN, is formulated by maximizing the conditional marginal likelihood of a conditional generative 
    
[^144]: 从因果角度重新思考图对比学习中的维度理论

    Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10401](http://arxiv.org/abs/2312.10401)

    本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。

    

    图对比学习是一种在图中捕捉不变信息的学习范式。最近的研究专注于探索图的结构理论，从而增加不变信息的可区分性。然而，这些方法可能导致图模型朝向解释图的可解释性进行错误学习，因此学习到的噪声和与任务无关的信息干扰了图的预测。为了探索图的内在理论，我们提出了从图中捕捉维度理论的方法，但这在文献中并没有得到足够的关注。通过实验验证了上述路径的可行性。为了阐明维度理论对性能改进的内在机制，我们从因果角度重新思考图对比学习中的维度理论。

    Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
    
[^145]: 做时间扭曲吧：学习动力系统的拓扑不变量

    Let's do the time-warp-attend: Learning topological invariants of dynamical systems. (arXiv:2312.09234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.09234](http://arxiv.org/abs/2312.09234)

    该论文提出了一个数据驱动、基于物理信息的深度学习框架，用于分类和表征动力学变化的拓扑不变特征提取，特别关注超临界霍普分歧。这个方法可以帮助预测系统的质变和常发行为变化。

    

    科学领域中的动力系统，从电路到生态网络，当其基本参数跨越阈值时，会发生质变和常发性的行为变化，称为分歧。现有方法能够预测单个系统中即将发生的灾难，但主要基于时间序列，并且在分类不同系统的定性动力学变化和推广到真实数据方面存在困难。为了应对这一挑战，我们提出了一个数据驱动的、基于物理信息的深度学习框架，用于对动力学变化进行分类并表征分歧边界的拓扑不变特征提取。我们专注于超临界霍普分歧的典型案例，其用于模拟广泛应用的周期性动力学。我们的卷积关注方法经过了数据增强训练，鼓励学习可以用于检测分歧边界的拓扑不变量。

    Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes in individual systems but are primarily time-series-based and struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data. To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features. We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications. Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boun
    
[^146]: EZ-CLIP: 高效的零样本视频动作识别

    EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.08010](http://arxiv.org/abs/2312.08010)

    EZ-CLIP是一个简单高效的CLIP改进方案，通过引入时间上的视觉提示来解决视频领域中的挑战，并保持了CLIP的泛化能力。

    

    最近，在图像-文本配对数据的大规模预训练中取得的进展展示了惊人的零样本任务的泛化能力。借鉴这一成功，研究人员努力将图像为基础的视觉语言模型（如CLIP）应用于视频，扩展其在零样本任务上的能力。虽然这些改进表现出了很有前景的结果，但它们需要巨大的计算成本，并且在有效建模视频领域内关键的时间方面方面存在困难。在这项研究中，我们提出了EZ-CLIP，这是一个简单高效的CLIP改进方案，解决了这些挑战。EZ-CLIP利用时间上的视觉提示进行无缝的时间适应，无需对核心CLIP架构进行基本改动，同时保持其卓越的泛化能力。此外，我们引入了一种新的学习目标，对时间视觉提示进行引导，以专注于捕捉动作。

    Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the crucial temporal aspects inherent to the video domain. In this study, we present EZ-CLIP, a simple and efficient adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal visual prompting for seamless temporal adaptation, requiring no fundamental alterations to the core CLIP architecture while preserving its remarkable generalization abilities. Moreover, we introduce a novel learning objective that guides the temporal visual prompts to focus on capturing motion, thereb
    
[^147]: 神经谱方法: 谱域中的自监督学习

    Neural Spectral Methods: Self-supervised learning in the spectral domain. (arXiv:2312.05225v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05225](http://arxiv.org/abs/2312.05225)

    神经谱方法将经典谱方法与机器学习相结合，通过谱损失实现更高效的求导，大大降低了训练复杂度，并在速度和准确性方面显著超过之前的机器学习方法。

    

    我们提出了神经谱方法，这是一种在经典谱方法基础上解决参数化偏微分方程（PDE）问题的技术。我们的方法使用正交基来学习PDE解作为谱系数之间的映射。与当前的机器学习方法相比，这些方法通过在时空域中最小化残差的数值积分来强制PDE约束条件，我们利用Parseval恒等式并引入一种新的训练策略，即谱损失。我们的谱损失通过神经网络实现了更高效的求导，并极大地降低了训练复杂度。在推理时间，我们的方法的计算成本保持恒定，不受时空域分辨率的影响。我们的实验结果表明，我们的方法在速度和准确性方面显著优于之前的机器学习方法，多个不同问题上的性能提高了一到两个数量级。

    We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. Our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, we leverage Parseval's identity and introduce a new training strategy through a \textit{spectral loss}. Our spectral loss enables more efficient differentiation through the neural network, and substantially reduces training complexity. At inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain. Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by one to two orders of magnitude on multiple different problems. When 
    
[^148]: 时间中的涟漪：美国历史中的不连续性

    A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01185](http://arxiv.org/abs/2312.01185)

    该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。

    

    在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\% - 95\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。

    In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
    
[^149]: 使用人工智能预测个体风险调整MRI筛查和早期检测的乳腺癌

    Predicting breast cancer with AI for individual risk-adjusted MRI screening and early detection. (arXiv:2312.00067v2 [physics.med-ph] UPDATED)

    [http://arxiv.org/abs/2312.00067](http://arxiv.org/abs/2312.00067)

    本研究使用人工智能算法，基于MRI图像预测患者一年内是否会发展为乳腺癌，以减轻筛查负担并实现早期检测。

    

    高风险患者进行额外年度筛查性MRI以预防乳腺癌。我们提出根据当前MRI预测一年内患乳腺癌的风险，目的是减少筛查负担和促进早期发现。基于12年累积的12,694名患者的53,858只乳房上进行的筛查或诊断性MRI，培训了一个AI算法，其中包括2,331例确认的癌症。首先训练了一个U-Net来分割病变和识别关注区域。然后训练了第二个卷积网络来使用U-Net提取的特征检测恶性癌症。然后对该网络进行微调，以估计在放射学家认为正常或可能是良性的情况下一年内患癌症的风险。从未被用于训练的高风险筛查群体的9,183只乳房进行了此AI的风险预测的回顾性分析。统计分析侧重于权衡

    Women with an increased life-time risk of breast cancer undergo supplemental annual screening MRI. We propose to predict the risk of developing breast cancer within one year based on the current MRI, with the objective of reducing screening burden and facilitating early detection. An AI algorithm was developed on 53,858 breasts from 12,694 patients who underwent screening or diagnostic MRI and accrued over 12 years, with 2,331 confirmed cancers. A first U-Net was trained to segment lesions and identify regions of concern. A second convolutional network was trained to detect malignant cancer using features extracted by the U-Net. This network was then fine-tuned to estimate the risk of developing cancer within a year in cases that radiologists considered normal or likely benign. Risk predictions from this AI were evaluated with a retrospective analysis of 9,183 breasts from a high-risk screening cohort, which were not used for training. Statistical analysis focused on the tradeoff betwe
    
[^150]: 分数梯度下降法的收敛性分析

    Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2311.18426](http://arxiv.org/abs/2311.18426)

    本论文通过分析不同环境下的分数梯度下降方法，建立了分数导数与整数导数之间的新界限，并证明了在平滑且凸、平滑且强凸以及平滑且非凸环境下的收敛性，为分数梯度下降的收敛性分析提供了新的理论支持。

    

    分数导数是整数阶导数的一种广义推广。对于优化问题，研究使用分数导数的梯度下降方法的收敛性是非常有意义的。目前，对于分数梯度下降的收敛性分析在研究方法和研究环境方面都存在限制。本文旨在填补这些空白，分析了平滑且凸、平滑且强凸以及平滑且非凸环境下的分数梯度下降的变种。首先，我们将建立将分数导数与整数导数联系起来的新界限。然后，利用这些界限证明了对于平滑且强凸函数的线性收敛性和对于平滑且凸函数的O(1/T)收敛性。此外，通过使用一种更适合分数导数的扩展平滑度概念-Holder平滑度，我们还证明了对于平滑且非凸函数的O(1/T)收敛性。

    Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivative
    
[^151]: 自适应图像配准：将深度学习和优化函数相结合的混合方法以提高精度

    Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.15497](http://arxiv.org/abs/2311.15497)

    本研究提出了一种自适应图像配准的混合方法，将深度学习和优化函数相结合，通过使用学习方法的输出作为优化的初始参数，并在计算上重点处理损失最大的图像对，取得了1.6%的性能提升和1.0%的变形场平滑度提升。

    

    图像配准传统上采用两种不同的方法：依赖于强大深度神经网络的学习方法和应用复杂数学变换来使图像变形的优化方法。当然，这两种范式都有优点和缺点，在这项工作中，我们试图将它们各自的优势结合在一个简化的框架中，使用学习方法的输出作为优化的初始参数，同时优先考虑对损失最大的图像对进行计算。我们的研究结果显示，在测试数据上提高了1.6%的性能，同时保持相同的推理时间，并且变形场平滑度方面获得了1.0%的性能提升。

    Image registration has traditionally been done using two distinct approaches: learning based methods, relying on robust deep neural networks, and optimization-based methods, applying complex mathematical transformations to warp images accordingly. Of course, both paradigms offer advantages and disadvantages, and, in this work, we seek to combine their respective strengths into a single streamlined framework, using the outputs of the learning based method as initial parameters for optimization while prioritizing computational power for the image pairs that offer the greatest loss. Our investigations showed improvements of up to 1.6% in test data, while maintaining the same inference time, and a substantial 1.0% points performance gain in deformation field smoothness.
    
[^152]: 图遇上大型语言模型：进展与未来方向的综述

    A Survey of Graph Meets Large Language Model: Progress and Future Directions. (arXiv:2311.12399v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.12399](http://arxiv.org/abs/2311.12399)

    本综述对将大型语言模型(LLMs)与图结合的现有方法进行了全面的回顾和分析，提出了一个新的分类法，并讨论了未来研究的有希望的方向。

    

    图在表示和分析诸如引用网络、社交网络和生物数据等实际应用中扮演着重要角色。最近，大型语言模型(LLMs)在各个领域取得了巨大的成功，并且已经被应用于图相关任务中，超越了基于图神经网络(GNNs)的传统方法，并取得了最先进的性能。在本综述中，我们首先对将LLMs与图结合的现有方法进行全面的回顾和分析。首先，我们提出了一个新的分类法，根据LLMs在图相关任务中扮演的角色(即增强器、预测器和对齐组件)，将现有方法组织为三个类别。然后，我们系统地调查了分类法三个类别中的代表性方法。最后，我们讨论了现有研究的局限性，并突出了未来研究的有希望的方向。

    Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are 
    
[^153]: LogLead -- 快速且集成的日志加载器、增强器和异常检测器。

    LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly Detector. (arXiv:2311.11809v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2311.11809](http://arxiv.org/abs/2311.11809)

    LogLead是一种快速且集成的日志加载器、增强器和异常检测器，可用于高效的日志分析基准测试。通过使用不同的数据集、日志表示方法和异常检测器，LogLead方便了日志分析研究中的全面基准测试。与过去的解决方案相比，LogLead在日志加载速度和Drain解析速度上都取得了显著的提升。

    

    本论文介绍了一种名为LogLead的工具，旨在进行高效的日志分析基准测试。LogLead结合了日志处理的三个关键步骤：加载、增强和异常检测。该工具利用了高速DataFrame库Polars。我们目前已经公开提供了八个系统的加载器（HDFS、Hadoop、BGL、Thunderbird、Spirit、Liberty、TrainTicket和GC Webshop）。我们拥有多个增强器，包括三个解析器（Drain、Spell、LenMa）、Bert嵌入创建以及词袋等其他日志表示技术。LogLead集成了SKLearn中的五种监督学习和四种无监督学习算法，用于进行异常检测。通过集成不同的数据集、日志表示方法和异常检测器，LogLead为日志分析研究提供了全面的基准测试。我们展示了相比过去的解决方案，使用LogLead从原始文件加载到DataFrame的速度提升了超过10倍。我们演示了Drain解析速度大约提升了2倍。

    This paper introduces LogLead, a tool designed for efficient log analysis benchmarking. LogLead combines three essential steps in log processing: loading, enhancing, and anomaly detection. The tool leverages Polars, a high-speed DataFrame library. We currently have Loaders for eight systems that are publicly available (HDFS, Hadoop, BGL, Thunderbird, Spirit, Liberty, TrainTicket, and GC Webshop). We have multiple enhancers with three parsers (Drain, Spell, LenMa), Bert embedding creation and other log representation techniques like bag-of-words. LogLead integrates to five supervised and four unsupervised machine learning algorithms for anomaly detection from SKLearn. By integrating diverse datasets, log representation methods and anomaly detectors, LogLead facilitates comprehensive benchmarking in log analysis research. We show that log loading from raw file to dataframe is over 10x faster with LogLead compared to past solutions. We demonstrate roughly 2x improvement in Drain parsing s
    
[^154]: 输入凸LSTM：一种快速基于Lyapunov模型预测控制的凸方法

    Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.07202](http://arxiv.org/abs/2311.07202)

    本研究提出了一种基于输入凸LSTM的基于Lyapunov的模型预测控制方法，通过减少收敛时间和缓解梯度消失/爆炸问题来改善MPC的性能。

    

    利用输入凸神经网络（ICNN），基于ICNN的模型预测控制（MPC）通过在MPC框架中保持凸性成功实现全局最优解。然而，当前的ICNN架构存在梯度消失/爆炸问题，限制了它们作为复杂任务的深度神经网络的能力。此外，当前基于神经网络的MPC，包括传统的基于神经网络的MPC和基于ICNN的MPC，与基于第一原理模型的MPC相比面临较慢的收敛速度。在本研究中，我们利用ICNN的原理提出了一种新的基于输入凸LSTM的基于Lyapunov的MPC，旨在减少收敛时间、缓解梯度消失/爆炸问题并确保闭环稳定性。通过对非线性化学反应器的模拟研究，我们观察到了梯度消失/爆炸问题的缓解和收敛时间的减少，收敛时间平均降低了一定的百分之。

    Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
    
[^155]: 一个基础图模型

    A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03976](http://arxiv.org/abs/2311.03976)

    本文提出了一个基于对抗性对比学习的基础图模型FoToM，该模型通过节点和边特征排除进行图预训练，在多个领域上实现了正向迁移，并取得了显著的性能提升。

    

    无监督图表示学习的主要优势是在数据或标签稀缺的情况下，可以对预训练模型进行微调。现有的方法是针对特定领域的，保持预训练和目标数据集之间的节点和边属性一致。这使得无法在其他领域进行迁移。能够在任意任务和领域上实现正向迁移的模型将成为第一个基础图模型。在这项工作中，我们使用对抗性对比学习提出了FoToM，一种基于节点和边特征排除的图预训练方法。我们使用FoToM在多个图领域上进行预训练，得到了第一个基础图模型。我们在来自多个领域的评估数据集上展示了正向迁移。在所有数据集上，性能最差时与有监督基线相当，76%的数据集在95%置信度下都显著优于有监督基线（P≤0.01），误差减少了8%至40%。

    The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
    
[^156]: Salted Inference: 在移动计算中提升隐私并保持分割推理的效率的方法

    Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing. (arXiv:2310.13384v1 [cs.LG])

    [http://arxiv.org/abs/2310.13384](http://arxiv.org/abs/2310.13384)

    本文介绍了一种名为“盐化DNN”的方法，通过在推理时让客户控制DNN输出的语义解释，同时保持准确性和效率与标准DNN几乎一致，提升了移动计算中隐私和计算效率的问题。

    

    分割推理将深度神经网络（DNN）划分为两部分，边缘部分在设备上运行，后续部分在云端运行。这满足了设备上机器学习的两个关键需求：输入隐私和计算效率。然而，在分割推理中，一个未解决的问题是输出隐私，因为DNN的输出可见于云端。尽管加密计算可以保护输出隐私，但需要大量的计算和通信资源。本文引入了“盐化DNN”：一种新颖的方法，可以让客户在推理时控制DNN输出的语义解释，同时保持准确性和效率与标准DNN几乎一致。在图像和传感器数据上进行的实验评估表明，盐化DNN在分类准确性上与标准DNN非常接近，尤其是当盐化层位于较早阶段以满足分割推理的要求时。我们的方法是通用的，可以应用于各种领域。

    Split inference partitions a deep neural network (DNN) to run the early part at the edge and the later part in the cloud. This meets two key requirements for on-device machine learning: input privacy and compute efficiency. Still, an open question in split inference is output privacy, given that the output of a DNN is visible to the cloud. While encrypted computing can protect output privacy, it mandates extensive computation and communication resources. In this paper, we introduce "Salted DNNs": a novel method that lets clients control the semantic interpretation of DNN output at inference time while maintaining accuracy and efficiency very close to that of a standard DNN. Experimental evaluations conducted on both image and sensor data show that Salted DNNs achieve classification accuracy very close to standard DNNs, particularly when the salted layer is positioned within the early part to meet the requirements of split inference. Our method is general and can be applied to various D
    
[^157]: 构建具有多样数据损坏情况下鲁棒性的离线强化学习

    Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.12955](http://arxiv.org/abs/2310.12955)

    本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。

    

    离线强化学习（RL）是一种有前途的方法，可以从离线数据集中学习强化策略，而无需与环境进行昂贵或不安全的交互。然而，人们在真实环境中收集的数据集往往存在噪声，甚至可能被恶意损坏，这可能会严重影响离线强化学习的性能。本研究首先对当前离线强化学习算法在包括状态、动作、奖励和动力学在内的全面数据损坏情况下的性能进行了调查。我们的大量实验显示，隐式Q-learning（IQL）在各种离线强化学习算法中表现出了可靠的抗数据损坏能力。此外，我们还进行了经验和理论分析，以了解IQL的鲁棒性能，并将其监督策略学习方案确定为关键因素。尽管相对鲁棒，但IQL在动力学损坏下仍然存在Q函数的重尾目标问题。

    Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
    
[^158]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^159]: MULTISCRIPT: 多模式脚本学习用于支持开放领域的日常任务

    MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04965](http://arxiv.org/abs/2310.04965)

    该论文提出了一个新的基准挑战MultiScript，旨在解决现有脚本学习方法对于开放领域日常任务的限制。论文介绍了两个多模式脚本学习任务，并提供了对应的输入和输出要求。

    

    从视频演示中自动生成脚本（即文本描述的关键步骤序列）并推理后续步骤对于现代AI虚拟助手来引导人们完成日常任务，尤其是陌生任务至关重要。然而，当前的生成式脚本学习方法很大程度上依赖于结构良好的前置步骤的文本和/或图像描述，或者限于特定领域，导致与真实世界中用户场景存在差距。为了解决这些限制，我们提出了一个新的基准挑战——MultiScript，其中包含两个关于面向任务的多模式脚本学习的新任务：（1）多模式脚本生成，和（2）后续步骤预测。对于这两个任务，输入包括目标任务名称和演示视频，预期输出为（1）基于演示视频的结构化步骤描述的序列，和（2）针对每个步骤的单一文本描述。

    Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th
    
[^160]: BioBridge: 通过知识图谱桥接生物医学基础模型

    BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])

    [http://arxiv.org/abs/2310.03320](http://arxiv.org/abs/2310.03320)

    BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。

    

    基础模型(FMs)能够利用大量的无标签数据，在各种任务上展现出优秀的性能。然而，用于生物医学领域的FMs主要仍处于单模态状态，即独立训练并用于处理蛋白质序列、小分子结构或临床数据等单一任务。为了克服生物医学FMs的这种局限性，我们提出了一种新颖的参数高效学习框架BioBridge，通过利用知识图谱(KG)来学习不需要微调任何底层单模态FMs的转换，从而桥接独立训练的单模态FMs以建立多模态行为。我们的实证结果表明，BioBridge在跨模态检索任务中可以击败最佳基线KG嵌入方法（平均提高约76.3%）。我们还发现，BioBridge表现出领域外的泛化能力，可以推广到未见的模态或关系中。

    Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
    
[^161]: 一种用于非层次化多保真度自适应采样的潜变量方法

    A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])

    [http://arxiv.org/abs/2310.03298](http://arxiv.org/abs/2310.03298)

    提出了一种基于潜变量的方法，用于非层次化多保真度自适应采样。该方法能够利用不同保真度模型之间的相关性以更高效地探索和利用设计空间。

    

    多保真度（MF）方法在提高替代模型和设计优化方面越来越受欢迎，通过整合来自不同低保真度（LF）模型的数据。尽管大多数现有的MF方法假定了一个固定的数据集，但是动态分配资源在不同保真度模型之间可以实现更高的探索和利用设计空间的效率。然而，大多数现有的MF方法依赖于保真度级别的层次假设，或者无法捕捉多个保真度级别之间的相互关系并利用其来量化未来样本的价值和导航自适应采样。为了解决这个障碍，我们提出了一个基于不同保真度模型的潜变量嵌入和相关的先验-后验分析的框架，以显式地利用它们的相关性进行自适应采样。在这个框架中，每个填充采样迭代包括两个步骤：首先我们确定具有最大潜力影响的位置。

    Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
    
[^162]: 统一的不确定性校准

    Unified Uncertainty Calibration. (arXiv:2310.01202v1 [stat.ML])

    [http://arxiv.org/abs/2310.01202](http://arxiv.org/abs/2310.01202)

    该论文提出了一种统一的不确定性校准（U2C）框架，用于合并可知和认知不确定性，实现了面对困难样例时的准确预测和校准。

    

    为了构建健壮，公平和安全的人工智能系统，我们希望在面对困难或超出训练类别的测试样例时，分类器能够说“我不知道”。普遍的预测不确定性策略是简单的“拒绝或分类”规则：如果认知不确定性高，则放弃预测，否则进行分类。然而，这种方法不允许不同的不确定性来源相互通信，会产生未校准的预测，并且不能纠正不确定性估计中的错误。为了解决这三个问题，我们引入了统一的不确定性校准（U2C）的整体框架，用于合并可知和认知不确定性。U2C能够进行清晰的学习理论分析不确定性估计，并且在各种ImageNet基准测试中优于拒绝或分类方法。

    To build robust, fair, and safe AI systems, we would like our classifiers to say ``I don't know'' when facing test examples that are difficult or fall outside of the training classes.The ubiquitous strategy to predict under uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify otherwise.Unfortunately, this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. To address these three issues, we introduce \emph{unified uncertainty calibration (U2C)}, a holistic framework to combine aleatoric and epistemic uncertainties. U2C enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of ImageNet benchmarks.
    
[^163]: LLMCarbon: 对大型语言模型的端到端碳足迹建模

    LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])

    [http://arxiv.org/abs/2309.14393](http://arxiv.org/abs/2309.14393)

    本研究提出了LLMCarbon，一个针对密集型和MoE LLMs设计的端到端碳足迹预测模型，解决了现有工具的限制，并显著提升了估计的准确性。

    

    大型语言模型（LLMs）的碳足迹是一个重要关注点，包括它们的训练、推理、实验和存储过程中的排放，包括运营和固定碳排放。一个重要方面是在LLMs训练之前准确估计其碳影响，这在很大程度上依赖于GPU的使用。现有研究已报告了LLMs训练的碳足迹，但只有一个工具mlco2能够在实际训练之前预测新的神经网络的碳足迹。然而，mlco2存在一些严重的限制。它不能扩展其对密集或专家混合（MoE）LLMs的估计，忽视了关键的架构参数，仅关注GPU，并不能建模固化的碳足迹。为了解决这些问题，我们引入了LLMCarbon，一个为密集型和MoE LLMs设计的端到端碳足迹预测模型。与mlco2相比，LLMCarbon显著增强了准确性。

    The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
    
[^164]: 利用自注意力机制对连续音乐推荐进行负面信号的利用

    Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation. (arXiv:2309.11623v1 [cs.IR])

    [http://arxiv.org/abs/2309.11623](http://arxiv.org/abs/2309.11623)

    本研究利用自我注意力机制和负面反馈，提出了用于顺序音乐推荐的Transformer模型，并采用对比学习任务来提高推荐准确性。

    

    音乐流媒体服务仰赖其推荐引擎连续向用户提供内容。因此，顺序推荐已经引起了当前文献的相当关注，而当今最先进的方法主要集中在利用上下文信息（如长期和短期用户历史和项目特征）的自我关注模型上；然而，大多数研究集中在长格式内容领域（零售、电影等）而不是短格式，例如音乐。此外，许多研究未探索在训练过程中如何融入负面会话级反馈。在本研究中，我们研究了基于Transformer的自我关注体系结构，以学习用于顺序音乐推荐的隐式会话级信息。此外，我们还提出了一种对比学习任务，以融入负面反馈（例如跳过的曲目）以促进正面命中并惩罚负面命中。这个任务被形式化为一个简单的损失项，可以加入到训练中。

    Music streaming services heavily rely on their recommendation engines to continuously provide content to their consumers. Sequential recommendation consequently has seen considerable attention in current literature, where state of the art approaches focus on self-attentive models leveraging contextual information such as long and short-term user history and item features; however, most of these studies focus on long-form content domains (retail, movie, etc.) rather than short-form, such as music. Additionally, many do not explore incorporating negative session-level feedback during training. In this study, we investigate the use of transformer-based self-attentive architectures to learn implicit session-level information for sequential music recommendation. We additionally propose a contrastive learning task to incorporate negative feedback (e.g skipped tracks) to promote positive hits and penalize negative hits. This task is formulated as a simple loss term that can be incorporated in
    
[^165]: 折叠注意力：面向设备的Transformer流式语音识别的内存和功耗优化

    Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])

    [http://arxiv.org/abs/2309.07988](http://arxiv.org/abs/2309.07988)

    本论文提出了一种名为折叠注意力的技术，在基于Transformer的流式语音识别模型中，通过减少线性投影层的数量，显著减小了模型大小，提高了内存和功耗效率，实验证明可以将模型大小减小24%、功耗减小23%。

    

    基于Transformer的模型在语音识别中表现出色。现有的用于优化Transformer推断的努力，通常针对长上下文应用，主要集中在简化注意力得分计算上。然而，流式语音识别模型通常每次只处理有限数量的令牌，因此注意力得分计算在这种情况下并不是瓶颈所在。相反，瓶颈在于多头注意力和前馈网络的线性投影层，它们构成了模型大小的相当部分，并对计算、内存和功耗的使用产生重要影响。为了解决这一瓶颈，我们提出了折叠注意力，这是一种针对这些线性层的技术，显著减小了模型大小，并提高了内存和功耗效率。设备上的基于Transformer的流式语音识别模型的实验证明，折叠注意力可以将模型大小（和相应的内存消耗）减小多达24%，并将功耗减小多达23%，而无需进行补充。

    Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp
    
[^166]: 使用置换不变神经网络对集合天气预测进行后处理

    Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks. (arXiv:2309.04452v1 [stat.ML])

    [http://arxiv.org/abs/2309.04452](http://arxiv.org/abs/2309.04452)

    本研究使用置换不变神经网络对集合天气预测进行后处理，不同于之前的方法，该网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。在地表温度和风速预测的案例研究中，我们展示了最先进的预测质量。

    

    统计后处理用于将原始数值天气预报的集合转化为可靠的概率预测分布。本研究中，我们考察了使用置换不变神经网络进行这一任务的方法。与以往的方法不同，通常基于集合概要统计信息并忽略集合分布的细节，我们提出的网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。我们通过校准度和锐度评估所获得的预测分布的质量，并将模型与经典的基准方法和基于神经网络的方法进行比较。通过处理地表温度和风速预测的案例研究，我们展示了最先进的预测质量。为了加深对学习推理过程的理解，我们进一步提出了基于置换的重要性评估方法。

    Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance
    
[^167]: IPA：推理管道自适应以实现高准确性和成本效益

    IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])

    [http://arxiv.org/abs/2308.12871](http://arxiv.org/abs/2308.12871)

    提出了一种名为IPA的在线深度学习推理管道自适应系统，通过动态配置批处理大小、复制和模型变体，以优化准确性、最小化成本并满足用户定义的延迟要求。

    

    在机器学习生产系统中，高效地优化多模型推理管道以实现快速、准确和成本效益的推理是一个关键挑战，考虑到它们对端到端延迟的严格要求。为了简化准确性和成本之间广阔而复杂的权衡空间的探索，提供者通常选择考虑其中之一。然而，挑战在于协调准确性和成本的权衡。为了应对这一挑战并提出一种有效管理推理管道中模型变体的解决方案，我们提出了IPA，一种在线深度学习推理管道自适应系统，它能够有效地利用每个深度学习任务的模型变体。模型变体是同一深度学习任务的预训练模型的不同版本，其资源需求、延迟和准确性有所不同。IPA通过动态配置批处理大小、复制和模型变体来优化准确性、最小化成本并满足用户定义的延迟SLA。

    Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs
    
[^168]: TemperatureGAN: 区域大气温度的生成建模

    TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures. (arXiv:2306.17248v1 [cs.LG])

    [http://arxiv.org/abs/2306.17248](http://arxiv.org/abs/2306.17248)

    TemperatureGAN是一个生成对抗网络，使用地面以上2m的大气温度数据，能够生成具有良好空间表示和与昼夜周期一致的时间动态的高保真样本。

    

    随机生成器对于估计气候对各个领域的影响非常有用。在各个领域中进行气候风险的预测，例如能源系统，需要准确（与基准真实数据有统计相似性）、可靠（不产生错误样本）和高效的生成器。我们利用来自北美陆地数据同化系统的数据，引入了TemperatureGAN，这是一个以月份、位置和时间段为条件的生成对抗网络，以每小时分辨率生成地面以上2m的大气温度。我们提出了评估方法和指标来衡量生成样本的质量。我们证明TemperatureGAN能够生成具有良好空间表示和与已知昼夜周期一致的时间动态的高保真样本。

    Stochastic generators are useful for estimating climate impacts on various sectors. Projecting climate risk in various sectors, e.g. energy systems, requires generators that are accurate (statistical resemblance to ground-truth), reliable (do not produce erroneous examples), and efficient. Leveraging data from the North American Land Data Assimilation System, we introduce TemperatureGAN, a Generative Adversarial Network conditioned on months, locations, and time periods, to generate 2m above ground atmospheric temperatures at an hourly resolution. We propose evaluation methods and metrics to measure the quality of generated samples. We show that TemperatureGAN produces high-fidelity examples with good spatial representation and temporal dynamics consistent with known diurnal cycles.
    
[^169]: 利用innsight包解释深度神经网络

    Interpreting Deep Neural Networks with the Package innsight. (arXiv:2306.10822v1 [stat.ML])

    [http://arxiv.org/abs/2306.10822](http://arxiv.org/abs/2306.10822)

    innsight是一个通用的R包，能够独立于深度学习库，解释来自任何R包的模型，并提供了丰富的可视化工具，以揭示深度神经网络预测的变量解释。

    

    R包innsight提供了一个通用的工具箱，通过所谓的特征归因方法，揭示了深度神经网络预测的变量解释。除了统一的用户友好的框架外，该包在三个方面脱颖而出：首先，它通常是第一个实现神经网络特征归因方法的R包。其次，它独立于深度学习库，允许解释来自任何R包，包括keras、torch、neuralnet甚至用户定义模型的模型。尽管它很灵活，但innsight在内部从torch包的快速和高效的数组计算中受益，这建立在LibTorch（PyTorch的C++后端）上，而不需要Python依赖。最后，它提供了各种可视化工具，用于表格、信号、图像数据或这些数据的组合。此外，可以使用plotly包以交互方式呈现这些图。

    The R package innsight offers a general toolbox for revealing variable-wise interpretations of deep neural networks' predictions with so-called feature attribution methods. Aside from the unified and user-friendly framework, the package stands out in three ways: It is generally the first R package implementing feature attribution methods for neural networks. Secondly, it operates independently of the deep learning library allowing the interpretation of models from any R package, including keras, torch, neuralnet, and even custom models. Despite its flexibility, innsight benefits internally from the torch package's fast and efficient array calculations, which builds on LibTorch $-$ PyTorch's C++ backend $-$ without a Python dependency. Finally, it offers a variety of visualization tools for tabular, signal, image data or a combination of these. Additionally, the plots can be rendered interactively using the plotly package.
    
[^170]: ReLU网络的最优集合和解路径

    Optimal Sets and Solution Paths of ReLU Networks. (arXiv:2306.00119v1 [cs.LG])

    [http://arxiv.org/abs/2306.00119](http://arxiv.org/abs/2306.00119)

    本研究开发了一个分析框架，通过凸规划表征所有ReLU网络的最优集合和解路径，并提供了最小网络的最优剪枝算法，建立了ReLU网络正则化路径连续的条件，并为最小ReLU网络提供了灵敏度结果。

    

    我们开发了一个分析框架，通过将非凸训练问题重新定义为凸规划问题，来刻画最优ReLU神经网络集合。我们证明了凸参数化的全局最优解由一个多面体集合给出，并将这个表征扩展到了非凸训练目标的最优集。由于ReLU训练问题的所有稳定点都可以表示为子采样凸规划的最优解，因此我们的工作为所有非凸目标的临界点提供了通用表达式。然后，我们利用我们的结果提供了计算最小网络的最优剪枝算法，建立了ReLU网络正则化路径连续的条件，并为最小ReLU网络提供了灵敏度结果。

    We develop an analytical framework to characterize the set of optimal ReLU neural networks by reformulating the non-convex training problem as a convex program. We show that the global optima of the convex parameterization are given by a polyhedral set and then extend this characterization to the optimal set of the non-convex training objective. Since all stationary points of the ReLU training problem can be represented as optima of sub-sampled convex programs, our work provides a general expression for all critical points of the non-convex objective. We then leverage our results to provide an optimal pruning algorithm for computing minimal networks, establish conditions for the regularization path of ReLU networks to be continuous, and develop sensitivity results for minimal ReLU networks.
    
[^171]: 使用不同iable的搜索体系架构提高语音情绪识别性能。

    Improving Speech Emotion Recognition Performance using Differentiable Architecture Search. (arXiv:2305.14402v1 [cs.SD])

    [http://arxiv.org/abs/2305.14402](http://arxiv.org/abs/2305.14402)

    该论文提出使用DARTS优化联合CNN和LSTM的体系结构以提高语音情绪识别性能，并在实验中证明了其优于以往最好的结果。

    

    语音情绪识别(SER)是实现情感感知交互的关键因素。深度学习(DL)改善了SER模型的性能，但设计DL体系结构需要先前的经验和实验评估。鼓励地，神经体系结构搜索(NAS)允许自动搜索最优DL模型。特别地，可区分的体系结构搜索(DARTS)是一种使用NAS搜索最优化模型的有效方法。在本文中，我们提出DARTS用于联合CNN和LSTM的体系结构，以改善SER性能。我们选择CNN LSTM耦合的原因是结果表明类似的模型能够提高性能。虽然SER研究人员已将CNN和RNN分别考虑，但DARTs同时用于CNN和LSTM的可行性仍需要探索。通过对IEMOCAP数据集的实验，我们证明了我们的方法优于使用DA的最佳报告结果。

    Speech Emotion Recognition (SER) is a critical enabler of emotion-aware communication in human-computer interactions. Deep Learning (DL) has improved the performance of SER models by improving model complexity. However, designing DL architectures requires prior experience and experimental evaluations. Encouragingly, Neural Architecture Search (NAS) allows automatic search for an optimum DL model. In particular, Differentiable Architecture Search (DARTS) is an efficient method of using NAS to search for optimised models. In this paper, we propose DARTS for a joint CNN and LSTM architecture for improving SER performance. Our choice of the CNN LSTM coupling is inspired by results showing that similar models offer improved performance. While SER researchers have considered CNNs and RNNs separately, the viability of using DARTs jointly for CNN and LSTM still needs exploration. Experimenting with the IEMOCAP dataset, we demonstrate that our approach outperforms best-reported results using DA
    
[^172]: 用神经网络解释暗物质晕的密度分布

    Explaining dark matter halo density profiles with neural networks. (arXiv:2305.03077v1 [astro-ph.CO])

    [http://arxiv.org/abs/2305.03077](http://arxiv.org/abs/2305.03077)

    使用可解释的神经网络将暗物质晕的演化历史与其密度分布相连接，网络发现超过渗透半径的轮廓由一个单一参数描述，这展示了在复杂的天体物理数据集中机器协助科学发现的潜力。

    

    我们使用可解释的神经网络将暗物质晕的演化历史与其密度分布相连接。该网络捕获密度分布中独立的变化因素，在低维表示中物理地解释了它们，使用了互信息。不需要先验知识，网络恢复了早期组装与内部轮廓之间的已知关系，并发现超过渗透半径的轮廓由一个单一参数描述，该参数表示最近的质量吸积率。结果展示了在复杂的天体物理数据集中机器协助科学发现的潜力。

    We use explainable neural networks to connect the evolutionary history of dark matter halos with their density profiles. The network captures independent factors of variation in the density profiles within a low-dimensional representation, which we physically interpret using mutual information. Without any prior knowledge of the halos' evolution, the network recovers the known relation between the early time assembly and the inner profile, and discovers that the profile beyond the virial radius is described by a single parameter capturing the most recent mass accretion rate. The results illustrate the potential for machine-assisted scientific discovery in complicated astrophysical datasets.
    
[^173]: 颗粒球计算：一种高效、鲁棒和可解释的自适应多粒度表示和计算方法

    Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])

    [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)

    本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。

    

    人类认知具有“先大后小”的认知机制，因此具有自适应的多粒度描述能力。这导致了有效性、鲁棒性和可解释性等计算特性。本文提出了一种新的基于颗粒球计算的自适应多粒度表示和计算方法。他们将这种方法应用于几个机器学习任务，并证明其相对于其他最先进的方法的有效性。

    Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
    
[^174]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^175]: 利用合成训练数据进行大鼠超声波声音的监督分类

    Utilizing synthetic training data for the supervised classification of rat ultrasonic vocalizations. (arXiv:2303.03183v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.03183](http://arxiv.org/abs/2303.03183)

    利用合成训练数据可以提高大鼠超声波声音的监督分类效果。

    

    小鼠产生频率高达120kHz的超声波声音（USVs）。这些叫声在社交行为中很重要，因此它们的分析可以揭示声音通信的功能和功能紊乱。手动识别USVs并将其分类为不同的子类非常耗时。虽然机器学习方法可用于识别和分类，但生成训练数据所需的时间和精力可能很高，并且当前方法的准确性可能存在问题。在本研究中，我们比较了经过训练的人与两种卷积神经网络（CNNs），DeepSqueak和VocalMat，在含有大鼠USVs的音频中的检测和分类性能。此外，我们测试了将合成的USVs插入到VocalMat CNN的训练数据中是否可以减少生成训练集的工作量。我们的实验结果表明

    Murine rodents generate ultrasonic vocalizations (USVs) with frequencies that extend to around 120kHz. These calls are important in social behaviour, and so their analysis can provide insights into the function of vocal communication, and its dysfunction. The manual identification of USVs, and subsequent classification into different subcategories is time consuming. Although machine learning approaches for identification and classification can lead to enormous efficiency gains, the time and effort required to generate training data can be high, and the accuracy of current approaches can be problematic. Here we compare the detection and classification performance of a trained human against two convolutional neural networks (CNNs), DeepSqueak and VocalMat, on audio containing rat USVs. Furthermore, we test the effect of inserting synthetic USVs into the training data of the VocalMat CNN as a means of reducing the workload associated with generating a training set. Our results indicate th
    
[^176]: $\alpha$-散度通过机器学习改进了熵产生估计

    $\alpha$-divergence Improves the Entropy Production Estimation via Machine Learning. (arXiv:2303.02901v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2303.02901](http://arxiv.org/abs/2303.02901)

    本研究通过机器学习提出了一种基于$\alpha$-散度的损失函数，在估计随机熵产生时表现出更加稳健的性能，尤其在强非平衡驱动或者缓慢动力学的情况下。选择$\alpha=-0.5$能获得最优结果。

    

    最近几年，通过机器学习从轨迹数据估计随机熵产生（EP）的算法引起了极大兴趣。这类算法的关键是找到一个损失函数，使其最小化能够保证准确的EP估计。本研究展示了存在一类损失函数，即那些实现了$\alpha$-散度的变分表示的函数，可以用于EP估计。通过将$\alpha$固定为在-1到0之间的值，$\alpha$-NEEP（Entropy Production的神经估计器）在强非平衡驱动或者缓慢动力学的情况下表现出更为稳健的性能，而这些情况对基于Kullback-Leibler散度（$\alpha=0$）的现有方法产生不利影响。特别地，选择$\alpha=-0.5$往往能得到最优结果。为了证实我们的发现，我们还提供了一个可精确求解的EP估计问题简化模型，其损失函数为land

    Recent years have seen a surge of interest in the algorithmic estimation of stochastic entropy production (EP) from trajectory data via machine learning. A crucial element of such algorithms is the identification of a loss function whose minimization guarantees the accurate EP estimation. In this study, we show that there exists a host of loss functions, namely those implementing a variational representation of the $\alpha$-divergence, which can be used for the EP estimation. By fixing $\alpha$ to a value between $-1$ and $0$, the $\alpha$-NEEP (Neural Estimator for Entropy Production) exhibits a much more robust performance against strong nonequilibrium driving or slow dynamics, which adversely affects the existing method based on the Kullback-Leibler divergence ($\alpha = 0$). In particular, the choice of $\alpha = -0.5$ tends to yield the optimal results. To corroborate our findings, we present an exactly solvable simplification of the EP estimation problem, whose loss function land
    
[^177]: Prismer: 一种具有专家集合的视觉语言模型

    Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02506](http://arxiv.org/abs/2303.02506)

    Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。

    Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.

    最近的视觉语言模型展示了令人印象深刻的多模态生成能力。然而，通常它们需要在大规模数据集上训练庞大的模型。作为一种更可扩展的替代方案，我们介绍了Prismer，一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合。Prismer只需要训练少量组件，大部分网络权重从现成的预训练领域专家中继承，并在训练期间保持冻结状态。通过利用来自各种领域的专家，我们展示了Prismer可以有效地汇集这些专家知识并将其适应于各种视觉语言推理任务。在我们的实验中，我们展示了Prismer实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。代码可在https://github.com/NVlabs/prismer获得。

    Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
    
[^178]: 基于深度神经网络的反向无线电频谱搜索算法

    A Deep Neural Network Based Reverse Radio Spectrogram Search Algorithm. (arXiv:2302.13854v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.13854](http://arxiv.org/abs/2302.13854)

    本论文提出了一种基于深度神经网络的反向无线电频谱搜索算法，用于在射电数据中快速找到类似感兴趣信号。算法通过训练一个B-Variational Autoencoder以及使用位置嵌入层和附加元数据来实现。这种搜索算法能够减轻繁重的手动信号审查工作。

    

    现代射电天文仪器产生大量数据，日益复杂的射电频率干扰(RFI)环境需要更加复杂的RFI拒绝算法。对于瞬变和技术信号的搜索具有“大海捞针”的特性，我们需要开发可以确定感兴趣信号是否具有独特属性或是否属于某个更大的有害RFI集合的方法。过去，这种审查需要耗时耗力地手动检查大量的信号。在本文中，我们提出了一种快速和模块化的深度学习算法，用于在射电频谱数据中搜索类似感兴趣信号。首先，我们使用能量检测算法返回的信号对B-Variational Autoencoder进行了训练。然后，我们从经典的Transformer架构中适应了一个位置嵌入层，来嵌入附加的元数据，我们通过使用基于频率的嵌入进行了演示。接下来，我们使用了B-V的编码器组件。

    Modern radio astronomy instruments generate vast amounts of data, and the increasingly challenging radio frequency interference (RFI) environment necessitates ever-more sophisticated RFI rejection algorithms. The "needle in a haystack" nature of searches for transients and technosignatures requires us to develop methods that can determine whether a signal of interest has unique properties, or is a part of some larger set of pernicious RFI. In the past, this vetting has required onerous manual inspection of very large numbers of signals. In this paper we present a fast and modular deep learning algorithm to search for lookalike signals of interest in radio spectrogram data. First, we trained a B-Variational Autoencoder on signals returned by an energy detection algorithm. We then adapted a positional embedding layer from classical Transformer architecture to a embed additional metadata, which we demonstrate using a frequency-based embedding. Next we used the encoder component of the B-V
    
[^179]: 基于大规模蛋白质接触预测模型的知识可以转化到数据稀缺的RNA接触预测任务中

    Knowledge from Large-Scale Protein Contact Prediction Models Can Be Transferred to the Data-Scarce RNA Contact Prediction Task. (arXiv:2302.06120v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2302.06120](http://arxiv.org/abs/2302.06120)

    本文发现基于蛋白质共进化的Transformer深度神经网络学习的知识可以转移到RNA接触预测任务中，显著提高数据稀缺的RNA接触预测的准确性。

    

    RNA通过其结构在许多生物活动中起着重要作用，其功能往往受其结构的影响。预测RNA序列中每个核苷酸之间的结构接近性可以表征RNA的结构信息。传统上，机器学习模型使用专家设计的特征并在数据稀缺的标记数据集上进行训练来解决这个问题。我们发现，基于蛋白质共进化的Transformer深度神经网络学习的知识可以转移到RNA接触预测任务中。由于蛋白质数据集比RNA接触预测的数据集大几个数量级，因此我们的发现和随后的框架极大地减少了数据稀缺的瓶颈。实验证实，使用公开的蛋白质模型进行转移学习的RNA接触预测效果得到了显著提高。我们的发现表明，蛋白质学习的结构模式可以转移到RNA上，为RNA结构预测开辟了潜在的新方向。

    RNA, whose functionality is largely determined by its structure, plays an important role in many biological activities. The prediction of pairwise structural proximity between each nucleotide of an RNA sequence can characterize the structural information of the RNA. Historically, this problem has been tackled by machine learning models using expert-engineered features and trained on scarce labeled datasets. Here, we find that the knowledge learned by a protein-coevolution Transformer-based deep neural network can be transferred to the RNA contact prediction task. As protein datasets are orders of magnitude larger than those for RNA contact prediction, our findings and the subsequent framework greatly reduce the data scarcity bottleneck. Experiments confirm that RNA contact prediction through transfer learning using a publicly available protein model is greatly improved. Our findings indicate that the learned structural patterns of proteins can be transferred to RNAs, opening up potenti
    
[^180]: 通过控制无关句子的混淆效应，提高抽象摘要的准确性

    Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences. (arXiv:2212.09726v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09726](http://arxiv.org/abs/2212.09726)

    通过控制无关句子的混淆效应，本文提出了一种改进抽象摘要准确性的方法，并在AnswerSumm数据集上实现了20\%的准确性提升。

    

    尽管最先进的摘要系统在生成看似流利的摘要方面取得了令人印象深刻的进展，但缺乏事实正确性仍然是一个问题。本文表明，无关的输入文本部分可能导致事实不一致，作为混淆因素。为此，我们利用因果效应的信息理论度量来量化混淆的程度，并准确衡量其对摘要性能的影响。基于从理论结果中得出的见解，当有人工注释的相关句子可用时，我们设计了一个简单的多任务模型来控制这种混淆。关键是，我们对数据分布进行了原则性的刻画，从而确保了使用人工注释的相关句子来生成准确的摘要。我们的方法在AnswerSumm上（参考文献：fabbri2021answersumm）上强基准的基础上将准确性得分提高了20\%。

    Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems despite their impressive progress on generating seemingly fluent summaries. In this paper, we show that factual inconsistency can be caused by irrelevant parts of the input text, which act as confounders. To that end, we leverage information-theoretic measures of causal effects to quantify the amount of confounding and precisely quantify how they affect the summarization performance. Based on insights derived from our theoretical results, we design a simple multi-task model to control such confounding by leveraging human-annotated relevant sentences when available. Crucially, we give a principled characterization of data distributions where such confounding can be large thereby necessitating the use of human annotated relevant sentences to generate factual summaries. Our approach improves faithfulness scores by 20\% over strong baselines on AnswerSumm \citep{fabbri2021answersumm}, a conver
    
[^181]: 用于解决生成对抗网络中的模式崩溃问题的分布拟合方法

    Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks. (arXiv:2212.01521v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01521](http://arxiv.org/abs/2212.01521)

    本文提出了一种用于解决生成对抗网络中模式崩溃问题的分布拟合方法。通过全局分布拟合和局部分布拟合，可以限制生成的数据分布，从而提高生成对抗网络的性能。

    

    模式崩溃是生成对抗网络中一个重要的未解决问题。本文从一种新的角度考察了模式崩溃的原因。由于训练过程中的非均匀采样，一些子分布可能被错过。因此，即使生成的分布与真实分布不同，GAN目标仍然可以达到最小值。为了解决这个问题，我们提出了一种带有惩罚项的全局分布拟合（GDF）方法来限制生成的数据分布。当生成的分布与真实分布不同时，GDF将使目标变得更难达到最小值，而原始的全局最小值不变。针对无法获得整体真实数据的情况，我们还提出了一种局部分布拟合（LDF）方法。在几个基准测试上的实验证明了GDF和LDF的有效性和竞争性能。

    Mode collapse is a significant unsolved issue of generative adversarial networks. In this work, we examine the causes of mode collapse from a novel perspective. Due to the nonuniform sampling in the training process, some sub-distributions may be missed when sampling data. As a result, even when the generated distribution differs from the real one, the GAN objective can still achieve the minimum. To address the issue, we propose a global distribution fitting (GDF) method with a penalty term to confine the generated data distribution. When the generated distribution differs from the real one, GDF will make the objective harder to reach the minimal value, while the original global minimum is not changed. To deal with the circumstance when the overall real data is unreachable, we also propose a local distribution fitting (LDF) method. Experiments on several benchmarks demonstrate the effectiveness and competitive performance of GDF and LDF.
    
[^182]: 你是否正确使用了测试对数似然？

    Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.00219](http://arxiv.org/abs/2212.00219)

    使用测试对数似然进行比较可能与其他指标相矛盾，并且高测试对数似然不意味着更准确的后验近似。

    

    测试对数似然常被用来比较不同模型的同一数据，或者比较拟合同一概率模型的不同近似推断算法。我们通过简单的例子展示了如何基于测试对数似然的比较可能与其他目标相矛盾。具体来说，我们的例子表明：（i）达到更高测试对数似然的近似贝叶斯推断算法不必意味着能够产生更准确的后验近似，（ii）基于测试对数似然比较的预测准确性结论可能与基于均方根误差的结论不一致。

    Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
    
[^183]: 建模师：在想象中学习和适应技能

    Choreographer: Learning and Adapting Skills in Imagination. (arXiv:2211.13350v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13350](http://arxiv.org/abs/2211.13350)

    本论文提出了一种建模师代理，利用世界模型在想象中学习并适应技能。该方法通过解耦探索和技能学习过程，能够在模型的潜在状态空间中发现技能，并通过元控制器进行高效的适应。建模师能够从离线数据和同时收集数据中学习技能。

    

    无监督技能学习旨在在没有外部监督的情况下学习丰富的行为库，使人工智能代理能够控制和影响环境。然而，如果缺乏适当的知识和探索，技能可能只能控制环境的有限区域，限制其适用性。此外，如何利用学得的技能行为以数据高效的方式适应后续任务还不清楚。我们提出了建模师，一种基于模型的代理，利用其世界模型在想象中学习和适应技能。我们的方法将探索和技能学习过程解耦，能够在模型的潜在状态空间中发现技能。在适应过程中，代理使用元控制器来评估和调整通过在想象中并行部署已学得的技能，以高效地适应它们。建模师能够从离线数据和同时收集数据中学习技能。

    Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exp
    
[^184]: 正交非负矩阵分解:最大熵原则方法

    Orthogonal Non-negative Matrix Factorization: a Maximum-Entropy-Principle Approach. (arXiv:2210.02672v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2210.02672](http://arxiv.org/abs/2210.02672)

    本文提出了一种新的解决正交非负矩阵分解问题的方法，该方法使用了基于最大熵原则的解决方案，并保证了矩阵的正交性和稀疏性以及非负性。该方法在不影响近似质量的情况下具有较好的性能速度和优于文献中类似方法的稀疏性、正交性。

    

    本文提出了一种解决正交非负矩阵分解（ONMF）问题的新方法，该问题的目标是通过两个非负矩阵（特征矩阵和混合矩阵）的乘积来近似输入数据矩阵，其中一个矩阵是正交的。我们展示了如何将ONMF解释为特定的设施定位问题，并针对ONMF问题采用基于最大熵原则的FLP解决方案进行了调整。所提出的方法保证了特征矩阵或混合矩阵的正交性和稀疏性，同时确保了两者的非负性。此外，我们的方法还开发了一个定量的“真实”潜在特征数量的特征-超参数用于ONMF。针对合成数据集以及标准的基因芯片数组数据集进行的评估表明，该方法在不影响近似质量的情况下具有较好的稀疏性、正交性和性能速度，相对于文献中类似方法有显著的改善。

    In this paper, we introduce a new methodology to solve the orthogonal nonnegative matrix factorization (ONMF) problem, where the objective is to approximate an input data matrix by a product of two nonnegative matrices, the features matrix and the mixing matrix, where one of them is orthogonal. We show how the ONMF can be interpreted as a specific facility-location problem (FLP), and adapt a maximum-entropy-principle based solution for FLP to the ONMF problem. The proposed approach guarantees orthogonality and sparsity of the features or the mixing matrix, while ensuring nonnegativity of both. Additionally, our methodology develops a quantitative characterization of ``true" number of underlying features - a hyperparameter required for the ONMF. An evaluation of the proposed method conducted on synthetic datasets, as well as a standard genetic microarray dataset indicates significantly better sparsity, orthogonality, and performance speed compared to similar methods in the literature, w
    
[^185]: 算法辅助下的推荐相关偏好

    Algorithmic Assistance with Recommendation-Dependent Preferences. (arXiv:2208.07626v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07626](http://arxiv.org/abs/2208.07626)

    本研究提出了一个联合人机决策的委托代理模型，探讨了算法推荐对选择的影响和设计，特别关注算法对偏好的改变，以解决算法辅助可能带来的意外后果。

    

    当算法提供风险评估时，我们通常将其视为对人类决策的有益输入，例如将风险评分呈现给法官或医生。然而，决策者可能不仅仅只针对算法提供的信息做出反应。决策者还可能将算法推荐视为默认操作，使其难以偏离，例如法官在对被告进行高风险评估的时候不愿意推翻，或医生担心偏离推荐的程序会带来后果。为了解决算法辅助的这种意外后果，我们提出了一个联合人机决策的委托代理模型。在该模型中，我们考虑了算法推荐对选择的影响和设计，这种影响不仅仅是通过改变信念，还通过改变偏好。我们从制度因素和行为经济学中的已有模型等方面进行了这个假设的动机论证。

    When an algorithm provides risk assessments, we typically think of them as helpful inputs to human decisions, such as when risk scores are presented to judges or doctors. However, a decision-maker may not only react to the information provided by the algorithm. The decision-maker may also view the algorithmic recommendation as a default action, making it costly for them to deviate, such as when a judge is reluctant to overrule a high-risk assessment for a defendant or a doctor fears the consequences of deviating from recommended procedures. To address such unintended consequences of algorithmic assistance, we propose a principal-agent model of joint human-machine decision-making. Within this model, we consider the effect and design of algorithmic recommendations when they affect choices not just by shifting beliefs, but also by altering preferences. We motivate this assumption from institutional factors, such as a desire to avoid audits, as well as from well-established models in behav
    
[^186]: 混合变量贝叶斯优化中的混合模型

    Hybrid Models for Mixed Variables in Bayesian Optimization. (arXiv:2206.01409v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01409](http://arxiv.org/abs/2206.01409)

    本文提出了一种新型的混合模型，用于混合变量贝叶斯优化，并且在搜索和代理模型阶段都具有创新之处。数值实验证明了混合模型的优越性。

    

    本文提出了一种新型的混合模型，用于处理混合变量贝叶斯优化中的定量（连续和整数）和定性（分类）类型。我们的混合模型将蒙特卡洛树搜索结构（MCTS）用于分类变量，并将高斯过程（GP）用于连续变量。在搜索阶段中，我们将频率派的上置信度树搜索（UCTS）和贝叶斯狄利克雷搜索策略进行对比，展示了树结构在贝叶斯优化中的融合。在代理模型阶段，我们的创新之处在于针对混合变量贝叶斯优化的在线核选择。我们的创新，包括动态核选择、独特的UCTS（hybridM）和贝叶斯更新策略（hybridD），将我们的混合模型定位为混合变量代理模型的进步。数值实验凸显了混合模型的优越性，凸显了它们的潜力。

    This paper presents a new type of hybrid models for Bayesian optimization (BO) adept at managing mixed variables, encompassing both quantitative (continuous and integer) and qualitative (categorical) types. Our proposed new hybrid models merge Monte Carlo Tree Search structure (MCTS) for categorical variables with Gaussian Processes (GP) for continuous ones. Addressing efficiency in searching phase, we juxtapose the original (frequentist) upper confidence bound tree search (UCTS) and the Bayesian Dirichlet search strategies, showcasing the tree architecture's integration into Bayesian optimization. Central to our innovation in surrogate modeling phase is online kernel selection for mixed-variable BO. Our innovations, including dynamic kernel selection, unique UCTS (hybridM) and Bayesian update strategies (hybridD), position our hybrid models as an advancement in mixed-variable surrogate models. Numerical experiments underscore the hybrid models' superiority, highlighting their potentia
    
[^187]: 深度学习在群体级脑解码中的应用

    Group-level Brain Decoding with Deep Learning. (arXiv:2205.14102v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14102](http://arxiv.org/abs/2205.14102)

    本文提出了一种深度学习方法，利用主体嵌入技术解决个体差异带来的群体级脑解码问题，并在脑磁图像数据上进行了验证。

    

    脑成像数据解码在脑机接口和神经表示研究中应用越来越广泛。由于受到个体之间变异的影响，解码通常是针对个体的，并且在个体间无法很好地推广。克服这一问题的方法不仅可以提供更丰富的神经科学见解，还可以使群体模型优于个体模型。本文提出了一种方法，利用类似自然语言处理中单词嵌入的主体嵌入来学习和利用个体间变化的结构作为解码模型的一部分，我们改编了 WaveNet 架构用于分类。我们将其应用于脑磁图像数据，15 个受试者观看了 118 种不同的图像，每个图像有 30 个样本，使用整个图像呈现后的 1s 窗口进行图像分类。我们表明，深度学习和主体嵌入的结合对于分类具有至关重要的作用。

    Decoding brain imaging data is gaining popularity, with applications in brain-computer interfaces and the study of neural representations. Decoding is typically subject-specific and does not generalise well over subjects, due to high amounts of between subject variability. Techniques that overcome this will not only provide richer neuroscientific insights but also make it possible for group-level models to outperform subject-specific models. Here, we propose a method that uses subject embedding, analogous to word embedding in Natural Language Processing, to learn and exploit the structure in between-subject variability as part of a decoding model, our adaptation of the WaveNet architecture for classification. We apply this to magnetoencephalography data, where 15 subjects viewed 118 different images, with 30 examples per image; to classify images using the entire 1s window following image presentation. We show that the combination of deep learning and subject embedding is crucial to cl
    
[^188]: 使用动态线性投影方法探索非线性模型的局部解释

    Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.05359](http://arxiv.org/abs/2205.05359)

    本文介绍了一种使用动态线性投影方法来分析流行的非线性模型的局部解释的方法，探索预测变量之间的交互如何影响变量重要性估计，这对于理解模型的可解释性非常有用。

    

    机器学习模型的预测能力日益增强，但与参数统计模型相比，其复杂性和可解释性下降。这种折衷导致了可解释的人工智能（XAI）的出现，提供了诸如局部解释（LE）和局部变量归因（LVA）之类的方法，以揭示模型如何使用预测变量进行预测。然而，LVA通常不能有效处理预测变量之间的关联。为了理解预测变量之间的交互如何影响变量重要性估计，可以将LVA转换为线性投影，并使用径向游览。这对于学习模型如何犯错，或异常值的影响，或观测值的聚类也非常有用。本文使用各种流行的非线性模型（包括随机森林和神经网络）的示例来说明这种方法。

    The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quant
    
[^189]: 你的艺术有多深：一个关于单一任务、单一模态神经网络艺术理解限制的实验研究

    How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.16031](http://arxiv.org/abs/2203.16031)

    本文通过实验调查了一种先进的深度卷积神经网络在识别现代概念艺术作品时的能力，结果表明该模型主要使用形状和颜色等展示特征进行分类，而忽略了历史背景和艺术家意图等非展示特征。

    

    艺术作品的意义计算模型是复杂且困难的，因为艺术解释是多维的且高度主观的。本文实验性地调查了一种最先进的深度卷积神经网络（DCNN）在将现代概念艺术作品正确区分为艺术策展人设计的画廊时的能力。提出了两个假设，即DCNN模型使用展示特征（如形状和颜色）进行分类，而不使用非展示特征（如历史背景和艺术家意图）。利用专门设计的方法验证了这两个假设。使用在ImageNet数据集上预训练并鉴别微调的VGG-11 DCNN模型在真实世界概念摄影画廊中设计的手工数据集上进行训练。实验结果支持了这两个假设，表明DCNN模型忽略了非展示特征，仅使用展示特征进行分类。

    Computational modeling of artwork meaning is complex and difficult. This is because art interpretation is multidimensional and highly subjective. This paper experimentally investigated the degree to which a state-of-the-art Deep Convolutional Neural Network (DCNN), a popular Machine Learning approach, can correctly distinguish modern conceptual art work into the galleries devised by art curators. Two hypotheses were proposed to state that the DCNN model uses Exhibited Properties for classification, like shape and color, but not Non-Exhibited Properties, such as historical context and artist intention. The two hypotheses were experimentally validated using a methodology designed for this purpose. VGG-11 DCNN pre-trained on ImageNet dataset and discriminatively fine-tuned was trained on handcrafted datasets designed from real-world conceptual photography galleries. Experimental results supported the two hypotheses showing that the DCNN model ignores Non-Exhibited Properties and uses only
    
[^190]: 分解式量子图神经网络

    Decompositional Quantum Graph Neural Network. (arXiv:2201.05158v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2201.05158](http://arxiv.org/abs/2201.05158)

    本论文提出了一种新型的基于量子计算的神经网络，能够处理基于图的数据，通过使用自我图和张量积降低模型参数数量，提出一种从现实世界数据到希尔伯特空间的新型映射方式。

    

    量子机器学习是一个快速发展的领域，旨在利用量子算法和量子计算解决机器学习问题。由于物理量子比特缺乏和缺乏有效的将现实世界数据从欧几里得空间映射到希尔伯特空间的方法，大多数方法都是基于量子类比或过程模拟，而不是基于量子比特的具体体系结构设计。在本文中，我们提出了一种新型的混合量子-经典算法，用于处理基于图的数据，我们将其称为基于自我图的量子图神经网络（egoQGNN）。egoQGNN使用张量积和单位矩阵表示实现了GNN理论框架，大大减少了所需的模型参数数量。当被经典计算机控制时，egoQGNN可以通过从输入图的自我图中处理来容纳任意大小的图，使用一个中等大小的量子设备。该体系结构基于从现实世界数据到希尔伯特空间的新型映射方式。

    Quantum machine learning is a fast-emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This m
    
[^191]: 使用机器学习和潜在信息的主动恢复丢失音频信号

    Active Restoration of Lost Audio Signals Using Machine Learning and Latent Information. (arXiv:2111.10891v4 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2111.10891](http://arxiv.org/abs/2111.10891)

    本论文提出了一种使用隐写术、抖动和最先进的浅层学习和深度学习方法相结合的方式来主动恢复丢失的音频信号。通过评估不同方法的结果，观察显示所提出的解决方案在通过隐写术提供的边缘信息的帮助下能够有效增强音频信号的重建。

    

    近年来，利用深度学习算法对丢失或损坏的音频信号进行数字重建的研究得到了广泛探讨。然而，传统的线性插值、相位编码和音调插入技术仍然流行。然而，在融合抖动、隐写术和机器学习回归器进行音频信号重建的研究工作中我们发现并没有类似的研究。因此，本论文提出了隐写术、半调（抖动）和最先进的浅层学习和深度学习方法的组合。使用SPAIN、自回归、基于深度学习的、基于图的和其他方法进行了结果评估。结果观察表明，所提出的解决方案是有效的，并能通过隐写术提供的边缘信息（如潜在表示）增强音频信号的重建。此外，本论文提出了一个用于重建音频信号的新框架。

    Digital audio signal reconstruction of a lost or corrupt segment using deep learning algorithms has been explored intensively in recent years. Nevertheless, prior traditional methods with linear interpolation, phase coding and tone insertion techniques are still in vogue. However, we found no research work on reconstructing audio signals with the fusion of dithering, steganography, and machine learning regressors. Therefore, this paper proposes the combination of steganography, halftoning (dithering), and state-of-the-art shallow and deep learning methods. The results (including comparing the SPAIN, Autoregressive, deep learning-based, graph-based, and other methods) are evaluated with three different metrics. The observations from the results show that the proposed solution is effective and can enhance the reconstruction of audio signals performed by the side information (e.g., Latent representation) steganography provides. Moreover, this paper proposes a novel framework for reconstru
    
[^192]: 几乎零样本质量多样性优化

    Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.06826](http://arxiv.org/abs/2109.06826)

    本文提出了一种几乎零样本的质量多样性优化方法，通过利用参数空间中的优化路径信息构建先验种群，可以在未见环境中进行少样本适应。

    

    在过去的几年中，已经有大量的研究致力于利用以前的学习经验和设计少样本学习方法和元学习方法，涉及的问题领域从计算机视觉到基于强化学习的控制。一个明显的例外是质量多样性(QD)优化，我们在这个方向上几乎没有做出努力。QD方法已经被证明是处理强化学习中的欺骗性极小值和稀疏奖励的有效工具。然而，由于它们依赖于本质上具有低样本效率的进化过程，它们仍然很昂贵。我们表明，通过利用参数空间中优化路径的信息，可以构建先验种群，当在未见环境中使用该种群初始化QD方法时，可以进行零样本适应。我们提出的方法不需要反向传播，实现起来简单。

    In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimization. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimization in parameter space can be leveraged to build a prior population, which when used to initialize QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to impl
    
[^193]: 高效注意力：具有线性复杂度的注意力机制

    Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1812.01243](http://arxiv.org/abs/1812.01243)

    本文提出了一种高效注意力机制，可以在大幅减少内存和计算成本的情况下实现与传统点积注意力等效的效果。这种高效机制的应用使得注意力模块可以更广泛地集成到网络中，从而提高准确性，并且在物体检测和实例分割等任务中取得了显著的性能提升。

    

    点积注意力在计算机视觉和自然语言处理中有广泛应用。然而，它的内存和计算成本随着输入大小的增加呈二次增长。这种增长限制了其在高分辨率输入上的应用。为解决这个缺点，本文提出了一种新颖的高效注意力机制，它与点积注意力等效，但内存和计算成本大大降低。其资源效率使得注意力模块能更广泛、灵活地集成到网络中，从而提高准确性。经验证明了其优势的有效性。高效注意力模块显著提升了对MS-COCO 2017上的物体检测器和实例分割器的性能。此外，资源效率使得复杂模型能够使用注意力，而高成本限制了使用点积注意力。以立体视觉为例，一个具有高效注意力的模型实现了最先进的准确性。

    Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo d
    

