# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data](https://rss.arxiv.org/abs/2402.01393) | ALERT-Transformer是一种将异步感知与同步处理相结合的新颖桥接方式，通过ALERT模块、灵活的数据读取和基于块的稀疏性优化，实现了对实时事件驱动时空数据的经典处理，其性能超过竞争对手并具有较低的延迟。 |
| [^2] | [FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning](https://rss.arxiv.org/abs/2402.01350) | FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。 |
| [^3] | [SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](https://arxiv.org/abs/2402.05935) | 本论文介绍了SPHINX-X，一种扩展的多模态大型语言模型系列。通过改进架构和训练效率，我们成功构建了一系列参数大小和多语言能力不同的MLLMs，与数据和参数规模有强相关性。 |
| [^4] | [Classifying Nodes in Graphs without GNNs](https://arxiv.org/abs/2402.05934) | 该论文提出了一种无需GNN的图中节点分类方法，通过平滑约束、伪标签迭代和领域标签直方图三个关键组件，可以在不训练GNN的情况下，在标准流行的基准数据集上达到最先进的准确率。 |
| [^5] | [Time Series Diffusion in the Frequency Domain](https://arxiv.org/abs/2402.05933) | 本论文通过分析时间序列在频域中的表示，探讨了基于评分的扩散模型中的归纳偏差，提出了频域扩散模型，并将其与经典的时间扩散模型进行了比较。 |
| [^6] | [WebLINX: Real-World Website Navigation with Multi-Turn Dialogue](https://arxiv.org/abs/2402.05930) | 本论文提出了对话式网站导航的问题，并设计了一个 WEBLINX 基准测试，用于训练和评估代理。为了解决大量信息的处理瓶颈，文中提出了一个受检索启发的模型。实验结果表明，该模型能够在多种场景下复制人类行为的能力。 |
| [^7] | [An Interactive Agent Foundation Model](https://arxiv.org/abs/2402.05929) | 我们提出了一个交互式智能体基础模型，采用新颖的训练范式，能够跨领域、数据集和任务进行训练，展现出通用性和适应性，且在机器人、游戏 AI 和医疗保健领域表现出色。 |
| [^8] | [Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss](https://arxiv.org/abs/2402.05928) | 本文研究了依赖学习理论中的尖锐率，主要是为了避免样本大小缩减对方差产生影响。当假设类别的拓扑结构符合某些条件时，经验风险最小化者的性能与类别的复杂性和二阶统计量有关。 |
| [^9] | [On the Convergence of Zeroth-Order Federated Tuning in Large Language Models](https://arxiv.org/abs/2402.05926) | 我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。 |
| [^10] | [GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory](https://arxiv.org/abs/2402.05916) | GenEFT是一个有效的理论框架，通过研究泛化相变和表示学习动态，揭示了神经网络泛化的静态和动态特性，这弥合了机器学习理论预测与实践之间的差距。 |
| [^11] | [Efficient Stagewise Pretraining via Progressive Subnetworks](https://arxiv.org/abs/2402.05913) | 通过渐进子网络训练，该方法实现了高效的分阶段预训练，避免了早期阶段无法评估完整模型和模型质量下降等问题。 |
| [^12] | [Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games](https://arxiv.org/abs/2402.05906) | 本论文研究了网络聚合马尔可夫博弈中的风险敏感多智能体强化学习，使用了累积前景理论作为风险度量，并提出了一种分布式嵌套CPT-AC算法。这项工作对于理解人类的损失规避和对概率的高估/低估倾向具有重要意义。 |
| [^13] | [Large Language Model Meets Graph Neural Network in Knowledge Distillation](https://arxiv.org/abs/2402.05894) | 本论文提出了一种新颖的图知识蒸馏框架，使用大规模语言模型作为教师模型、图神经网络作为学生模型，解决了在理解文本-属性图中的节点分类问题中的限制。 |
| [^14] | [EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance](https://arxiv.org/abs/2402.05885) | EUGENE是一种可解释的无监督图编辑距离近似方法，可以通过生成编辑路径来近似计算图编辑距离，同时消除了ground-truth生成和数据特定训练的需求。 |
| [^15] | [Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits](https://arxiv.org/abs/2402.05878) | 本论文研究了在结构化赌博机中的贝叶斯固定预算最佳臂识别问题，提出了一种基于先验信息的固定分配算法，并引入了新的证明方法，以得到更紧密的多臂BAI界限。该方法在各种情况下展现出一致且稳健的性能，加深了我们对于该问题的理解。 |
| [^16] | [Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices](https://arxiv.org/abs/2402.05876) | 本研究探索了联邦学习在离线强化学习中的应用，设计了一种适用于联邦离线强化学习的无模型Q-learning算法FedLCB-Q。通过合作利用多个代理的离线数据集，并使用特定的学习速率调度和聚合方法，FedLCB-Q实现了线性加速。 |
| [^17] | [PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868) | PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。 |
| [^18] | [Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs](https://arxiv.org/abs/2402.05864) | 提出了一种名为Permute-and-Flip（PF）解码器，其具有最佳的鲁棒性和质量-鲁棒性的 tradeoff，且比采样方法更好。还设计了一种针对PF解码器的水印方案，能够保持样本的分布不变，并实现任意低的假阳性率和高的召回率。实验证明PF解码器在困惑度方面明显优于朴素采样，为LLM解码提供了一种有希望的新方法。 |
| [^19] | [Let Your Graph Do the Talking: Encoding Structured Data for LLMs](https://arxiv.org/abs/2402.05862) | 本论文介绍了一种参数高效的编码方法，可以使大型语言模型（LLMs）能够显式地表示结构化数据，并在图推理任务中取得了显著改进。 |
| [^20] | [Learning to Route Among Specialized Experts for Zero-Shot Generalization](https://arxiv.org/abs/2402.05859) | 提出了后续自适应逐标记门控机制（PHATGOOSE），通过学习路由于参数有效微调生成的专家模块之间，从而提高在未见任务上的零样本泛化能力。 |
| [^21] | [Structure-Informed Protein Language Model](https://arxiv.org/abs/2402.05856) | 该论文介绍了一种结构信息驱动的蛋白质语言模型，通过集成远程同源性检测，将结构信息融入模型，从而提高了蛋白质功能预测的准确性。 |
| [^22] | [How Much is Unseen Depends Chiefly on Information About the Seen](https://arxiv.org/abs/2402.05835) | 该论文发现，在未知种群中属于未在训练数据中出现的类的数据点的比例几乎完全取决于训练数据中出现相同次数的类的数量。论文提出了一个遗传算法，能够根据样本找到一个具有最小均方误差的估计量。 |
| [^23] | [Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting](https://arxiv.org/abs/2402.05830) | Sparse-VQ是一种无前馈网络的框架，利用稀疏向量量化技术和反实例归一化来减少噪声影响并捕获足够的统计信息，从而提高时间序列预测的性能。 |
| [^24] | [Discovering Temporally-Aware Reinforcement Learning Algorithms](https://arxiv.org/abs/2402.05828) | 这篇论文研究了发现具有时间意识的强化学习算法，用于改进手动设计的算法，使其能够表达出学习的新原则，并适用于各种不同的设置。 |
| [^25] | [FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting](https://arxiv.org/abs/2402.05823) | 本文提出了一个多模态融合框架，将历史功率数据、数值天气预报和卫星图像整合在一起，显著提高了太阳能发电预测的性能。研究展示了强大的零样本预测能力，对于新安装的电站尤其有用。 |
| [^26] | [Guided Evolution with Binary Discriminators for ML Program Search](https://arxiv.org/abs/2402.05821) | 提出使用二元判别器引导进化进行机器学习程序搜索，通过在训练中区分更好的程序，选择更好的程序并加快进化收敛速度，同时能够编码各种机器学习组件，加速进化过程。 |
| [^27] | [Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model](https://arxiv.org/abs/2402.05819) | 本文提出了PW-HuBERT框架，它通过将伪词级目标整合到训练过程中，从视觉语音模型中提取目标，从而在口语理解任务中展现出了优越性能。 |
| [^28] | [Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning](https://arxiv.org/abs/2402.05817) | 本研究使用了最新的YOLO V7目标检测方法，通过对医学图像进行修改和训练，成功提高了肾脏在磁共振成像中的检测效果，为肾脏疾病的诊断和治疗提供了有力支持。 |
| [^29] | [Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2402.05808) | 本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。 |
| [^30] | [On Calibration and Conformal Prediction of Deep Classifiers](https://arxiv.org/abs/2402.05806) | 本文研究了温度缩放对符合预测方法的影响，通过实证研究发现，校准对自适应C方法产生了有害的影响。 |
| [^31] | [Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence](https://arxiv.org/abs/2402.05802) | 通过无监督机器学习和概率独立性方法，我们发现了2000个潜在源的临床特征，这些特征在肺癌预测中的判别能力优于原始变量，并且能够识别未被诊断的癌症的特征。 |
| [^32] | [How do Transformers perform In-Context Autoregressive Learning?](https://arxiv.org/abs/2402.05787) | 本文研究了Transformers在上下文自回归学习中的表现，并通过训练模型发现了其预测下一个标记的过程。针对不同情况，我们证明了单层线性Transformer实现了梯度下降以及正交性之间的关系。 |
| [^33] | [Limits of Transformer Language Models on Algorithmic Learning](https://arxiv.org/abs/2402.05785) | Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。 |
| [^34] | [Analysing the Sample Complexity of Opponent Shaping](https://arxiv.org/abs/2402.05782) | 本文研究了对手塑造的样本复杂性，并提出了一种适合理论分析的表格化版本R-FOS。 |
| [^35] | [Stable Autonomous Flow Matching](https://arxiv.org/abs/2402.05774) | 本文通过应用随机稳定性工具于时间独立系统的流匹配模型，研究了物理稳定数据点的深度生成模型，并与控制理论原理进行了联系。 |
| [^36] | [Off-policy Distributional Q($\lambda$): Distributional RL without Importance Sampling](https://arxiv.org/abs/2402.05766) | 离线策略分布式 Q($\lambda$) 是一种不使用重要性采样的强化学习算法，具有与有符号测度的有趣交互作用，并在深度强化学习基准测试中展现了有希望的结果。 |
| [^37] | [Latent variable model for high-dimensional point process with structured missingness](https://arxiv.org/abs/2402.05758) | 本文提出了一种针对高维点过程的带有结构缺失的灵活高效的潜变量模型，利用高斯过程捕获时间相关性，并开发了可扩展的变分推理方法进行训练。 |
| [^38] | [Generalized Preference Optimization: A Unified Approach to Offline Alignment](https://arxiv.org/abs/2402.05749) | 广义偏好优化（GPO）是一种离线损失函数，通过参数化一类凸函数来实现统一的偏好优化视角，并提供了新的算法工具和实证洞见。 |
| [^39] | [Real-World Robot Applications of Foundation Models: A Review](https://arxiv.org/abs/2402.05741) | 本文综述了基础模型在真实世界机器人中的应用，重点是替换现有机器人系统中的特定组件。这些基础模型在输入输出关系、感知、运动规划和控制等方面扮演了重要角色。未来的挑战和对实际机器人应用的影响也被讨论到。 |
| [^40] | [Implicit Bias and Fast Convergence Rates for Self-attention](https://arxiv.org/abs/2402.05738) | 该论文研究了在自注意力网络中使用梯度下降训练的隐性偏差以及其收敛速率，通过证明在特定数据设置下收敛性是全局的，并提供了W_t到W_mm的有限时间收敛率。 |
| [^41] | [Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL](https://arxiv.org/abs/2402.05724) | 本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。 |
| [^42] | [In-Context Learning Can Re-learn Forbidden Tasks](https://arxiv.org/abs/2402.05723) | 本研究通过研究禁止任务，即模型设计为拒绝回答的任务，探究了在明确调优模型拒绝禁止任务的情况下，上下文学习（ICL）是否可以用于重新学习禁止任务。研究发现，ICL可以成功地撤销安全培训，从而造成重大的安全风险。 |
| [^43] | [Exact capacity of the \emph{wide} hidden layer treelike neural networks with generic activations](https://arxiv.org/abs/2402.05719) | 该论文研究了宽隐藏层树状神经网络的容量，采用全面提升的随机二重性理论(fl RDT)来对宽(TCM)网络的容量进行刻画，得到了一类通用激活函数的显式、闭式容量计算公式。 |
| [^44] | [REMEDI: Corrective Transformations for Improved Neural Entropy Estimation](https://arxiv.org/abs/2402.05718) | REMEDI是一种用于改进神经熵估计的校正转换方法，通过交叉熵最小化和相对熵估计基模型的偏差，提高了估计任务的准确性和效率。 |
| [^45] | [Collaborative non-parametric two-sample testing](https://arxiv.org/abs/2402.05715) | 本文提出了协同非参数的两样本检验（CTST）框架，该框架有效利用了图结构和最小化了对概率密度函数的假设，通过集成f-分布估计、核方法和多任务学习的要点，将传统的在每个节点独立应用的方法优化，对图结构中的多个两样本检验问题进行了更好的处理和分析。 |
| [^46] | [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713) | 该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。 |
| [^47] | [Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming](https://arxiv.org/abs/2402.05703) | 本研究针对人-机组合中的性能提出了离线风险敏感强化学习算法，通过部分可观察性的建模，解决了多样化人类参与者的挑战。 |
| [^48] | [Fixed width treelike neural networks capacity analysis -- generic activations](https://arxiv.org/abs/2402.05696) | 这篇论文研究了树状神经网络的容量，基于Random Duality Theory提出了通用的容量分析框架，并证明该框架适用于其他类型的激活函数，如二次和ReLU函数。 |
| [^49] | [Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits](https://arxiv.org/abs/2402.05689) | 该论文提出了一种新的策略类别，用于解决无限期平均回报好转胆冒险问题。研究表明，在单臂松弛问题是Unichain和非周期性的情况下，该策略类别具有渐进最优性。 |
| [^50] | [Interpretable classifiers for tabular data via discretization and feature selection](https://arxiv.org/abs/2402.05680) | 通过离散化和特征选择的方法，我们提出了一种从表格数据中计算出准确又易解释的分类器的方法。在实验证明该方法在准确度上与随机森林和XGBoost等现有方法相当，并且在多种情况下实际上超过了参考结果。 |
| [^51] | [Is Adversarial Training with Compressed Datasets Effective?](https://arxiv.org/abs/2402.05675) | 本论文研究了在压缩数据集上训练的模型对对抗鲁棒性的影响，并提出了一种同时提高数据集压缩效率和对抗鲁棒性的方法。 |
| [^52] | [A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs](https://arxiv.org/abs/2402.05674) | 本论文研究了高维模型中的对抗训练，引入了一个可处理的数学模型，并给出了对抗性经验风险最小化器的充分统计的精确渐近描述。研究结果表明存在可以防御而不惩罚准确性的方向，揭示了防御非鲁棒特征的优势。 |
| [^53] | [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668) | 对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。 |
| [^54] | [S$\Omega$I: Score-based O-INFORMATION Estimation](https://arxiv.org/abs/2402.05667) | S$\Omega$I是一种新的信息论量度，可以计算多变量系统中的协同-冗余平衡，突破了传统量度的局限性，并在合成数据上得到了验证。 |
| [^55] | [Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction](https://arxiv.org/abs/2402.05663) | 该论文介绍了一种在实时中尺度交通预测中具有最先进效果的深度预测方法SA-LSTM，通过将自注意力与长短期记忆结合，实现了对多步预测的改进，并在短期和长期预测之间取得了平衡。 |
| [^56] | [Rethinking Propagation for Unsupervised Graph Domain Adaptation](https://arxiv.org/abs/2402.05660) | 通过重新评估GNN在图领域适应中的作用，本论文揭示了传播过程对于适应不同图领域至关重要，并通过理论分析提供了多层GNN的泛化界限的证明。 |
| [^57] | [Investigating Reproducibility in Deep Learning-Based Software Fault Prediction](https://arxiv.org/abs/2402.05645) | 本研究通过系统回顾当前文献，分析了深度学习在软件错误预测领域的可再现性问题，并发现了一些令人担忧的情况。 |
| [^58] | [Improving Token-Based World Models with Parallel Observation Prediction](https://arxiv.org/abs/2402.05643) | 该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。 |
| [^59] | [Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients](https://arxiv.org/abs/2402.05639) | 本文提出了一种通过随机近似梯度最小化投影群体风险的新型非参数仪器变量回归框架，并通过理论和实证实验证明了其竞争性能。此外，本文还处理了二元结果的情况，取得了有希望的结果。 |
| [^60] | [RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization](https://arxiv.org/abs/2402.05628) | RepQuant提出了一种新的后训练量化框架，通过采用量化-推理解耦的范式和量化比例重参数化的方法，实现了准确的量化和高效的推理。 |
| [^61] | [Binding Dynamics in Rotating Features](https://arxiv.org/abs/2402.05627) | 本论文研究了旋转特征中的绑定动力学问题，并提出了一种新的“余弦绑定”机制，以替代传统的“$\chi$-binding”机制。通过显式计算特征之间的对齐和相应的权重调整，这一新机制能够达到与传统机制相同的性能，与自注意力和生物神经学有关。 |
| [^62] | [The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding](https://arxiv.org/abs/2402.05626) | 本文研究了使用ReLU-like激活函数以经验平方损失训练的单隐藏层神经网络的损失景观，提出了稳定点条件和逃逸神经元的定义，并将鞍点逃逸与逃逸神经元的参数变化联系起来。 |
| [^63] | [Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks](https://arxiv.org/abs/2402.05616) | 预训练的小型生成式语言模型可以作为序列型任务的通用学习框架，通过指令微调可以在化学信息学任务中实现接近最先进结果。 |
| [^64] | [Optimizing Delegation in Collaborative Human-AI Hybrid Teams](https://arxiv.org/abs/2402.05605) | 本论文提出了一种优化协作的人工智能-人类混合团队授权的框架，通过引入AI经理（通过强化学习）作为团队的外部观察者，学习团队代理人的行为模型并选择最佳的控制代理人。 |
| [^65] | [AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers](https://arxiv.org/abs/2402.05602) | AttnLRP是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，并具有与单一反向传播相似的计算效率的方法。它通过扩展逐层相关传递归因方法以处理注意力层来解决了黑盒Transformer模型的归因问题，具有超越现有方法的准确性和理解潜在表示的能力。 |
| [^66] | [Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry](https://arxiv.org/abs/2402.05576) | 通过利用离散结构，本论文以真实计算机上的实现为基础，打破了统计学习中的维度诅咒，并给出了无维度率的新的泛化界限。 |
| [^67] | [Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits](https://arxiv.org/abs/2402.05575) | 该论文介绍了一种在随机赌博机中同时实现群体曝光公平性和群内精英主义的方法，通过提供随时的群体曝光公平性保证和在每个群体中实现个体层面的精英主义公平性。 |
| [^68] | [Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study](https://arxiv.org/abs/2402.05571) | 该研究旨在开发高效的机器学习模型，用于对饮食紊乱相关的推文进行分类。研究发现，基于转换器的双向编码器表示在四个类别中实现了最高的F1分数，并表明基于转换器的模型较传统技术更为优越，但需要更多的计算资源。 |
| [^69] | [Hypergraph Node Classification With Graph Neural Networks](https://arxiv.org/abs/2402.05569) | 本研究提出了一种简单高效的框架，利用加权子图扩展的图神经网络(WCE-GNN)实现了超图节点分类。实验证明，WCE-GNN具有优秀的预测效果和较低的计算复杂度。 |
| [^70] | [Succint Interaction-Aware Explanations](https://arxiv.org/abs/2402.05566) | 本文提出了一种结合了SHAP和NSHAP的方法，通过将特征分成显著交互的部分，构建了一种简明、易解释的加性解释，并通过统计检验剪枝次优解，提高了解释的运行效率。 |
| [^71] | [Flashback: Understanding and Mitigating Forgetting in Federated Learning](https://arxiv.org/abs/2402.05558) | 本研究深入探讨了联邦学习中的遗忘问题，强调了遗忘在异质数据环境中的关键性质，提出了"闪回"算法来解决遗忘问题并取得优异的学习结果。 |
| [^72] | [Learning quantum Hamiltonians at any temperature in polynomial time with Chebyshev and bit complexity](https://arxiv.org/abs/2402.05552) | 本文提出了一种新的基于Chebyshev展开的指数函数多项式逼近方法，将学习量子哈密顿量问题转化为多项式优化问题，并证明在温和假设下，在有界度数的双重交互图情况下，可以在多项式时间内学习k-局部哈密顿量。 |
| [^73] | [Offline Actor-Critic Reinforcement Learning Scales to Large Models](https://arxiv.org/abs/2402.05546) | 本文证明了离线演员-评论者强化学习方法可以扩展到大型模型，并且比基线方法在多任务训练中表现更好。通过引入Perceiver-based演员-评论者模型，我们揭示了离线强化学习与自注意机制和跨注意力模块配合的关键模型特征。这项研究的发现表明：离线演员-评论者算法是逐渐摆脱行为克隆范式的一种自然选择，并且通过离线强化学习可以从次优示范或自动生成的数据中学习掌握多个领域的多任务策略。 |
| [^74] | [Machine learning applied to omics data](https://arxiv.org/abs/2402.05543) | 本章介绍了将机器学习技术应用于组学数据的方法，特别是在胰腺癌的基因组学和免疫组学综合分析中，通过随机森林和惩罚性多项式逻辑回归来预测胰腺癌免疫浸润，同时提出了使用关联规则进行预测的方法，以提高预测能力。 |
| [^75] | [Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions](https://arxiv.org/abs/2402.05541) | 本研究提出了一个新的框架——强化联邦学习（RFL），通过利用深度强化学习自适应地优化客户贡献的聚合过程，以增强模型鲁棒性和在非相同分布环境下参与者之间的公平性。 |
| [^76] | [Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts](https://arxiv.org/abs/2402.05536) | 这项研究提出了一种新颖的方法，通过结合知识图谱和深度学习，借助实体识别和链接技术以及上下文化嵌入，增强了机器学习模型对社交媒体帖子的分类能力。该方法主要应用于健康领域的饮食障碍识别，为早期诊断提供帮助。 |
| [^77] | [Asynchronous Diffusion Learning with Agent Subsampling and Local Updates](https://arxiv.org/abs/2402.05529) | 本论文研究了一种以异步方式操作的代理网络，旨在发现适合个体本地数据集的全局模型。通过证明异步扩散策略在均方误差上的稳定性，以及在联邦学习中的性能保证，本研究对于分布式学习的研究具有重要的创新和贡献。 |
| [^78] | [Buffer Overflow in Mixture of Experts](https://arxiv.org/abs/2402.05526) | 专家混合模型中，我们发现具有跨批次依赖的专家路由策略易受攻击，恶意查询可以影响模型对其他良性查询的输出。 |
| [^79] | [Differentially Private Model-Based Offline Reinforcement Learning](https://arxiv.org/abs/2402.05525) | 本研究提出了一种差分隐私的基于模型的离线强化学习方法，通过学习离线数据中的隐私模型以及基于模型的策略优化，实现了从离线数据中训练具有隐私保护的强化学习代理。同时，研究还总结了在这种设置下隐私的代价。 |
| [^80] | [Linearizing Models for Efficient yet Robust Private Inference](https://arxiv.org/abs/2402.05521) | 本文提出了一种名为RLNet的鲁棒线性化网络，通过减少延迟并改善模型在各种情况下的表现，实现了高效而鲁棒的隐私推理。 |
| [^81] | [Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming](https://arxiv.org/abs/2402.05501) | 本文调研了在混合整数线性规划中利用机器学习算法增强分支定界算法的方法，并强调了机器学习和数学优化的融合视野。 |
| [^82] | [Heart disease risk prediction using deep learning techniques with feature augmentation](https://arxiv.org/abs/2402.05495) | 本论文提出使用深度学习方法和特征增强技术预测心脏疾病风险，在大规模人群中取得了显著的改进。 |
| [^83] | [Determining the severity of Parkinson's disease in patients using a multi task neural network](https://arxiv.org/abs/2402.05491) | 本文提出了一种基于声音分析的非侵入性技术，使用多任务神经网络来确定帕金森病患者的严重程度和疾病进展的方法。 |
| [^84] | [A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals](https://arxiv.org/abs/2402.05482) | 本研究提出了一种新的非侵入性神经质量评估模型QASE-net，可以有效预测表面肌电信号的信噪比，实验证明其相比之前的模型具有更低的预测误差和更高的线性相关性。 |
| [^85] | [Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization](https://arxiv.org/abs/2402.05476) | 本文提出了一种新颖的无模型集合强化学习算法，用于解决大型网络中的性能和复杂性挑战。算法使用多个Q-learning算法在多个马尔可夫环境中并行运行，并通过自适应加权融合输出，提供近似最优策略。实验结果表明，该算法可以实现多达55%的平均策略减少。 |
| [^86] | [Implicit Diffusion: Efficient Optimization through Stochastic Sampling](https://arxiv.org/abs/2402.05468) | 本文介绍了一种通过随机采样优化隐含分布的新算法，并提出了一种通用框架，用于在单个循环中同时进行优化和采样步骤。实验结果证明了该方法在真实环境中的有效性。 |
| [^87] | [Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss](https://arxiv.org/abs/2402.05453) | 本论文提出了一种凸凹损失函数的方法，通过梯度下降实现训练损失的高方差，从而降低会员推断攻击中的隐私风险。 |
| [^88] | [Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application](https://arxiv.org/abs/2402.05448) | 本文提出了一种用于Minecraft游戏应用的图像生成和编辑系统"Minecraft-ify"，能够生成针对3D虚拟角色的面部聚焦图像，并支持使用文本进行图像编辑，提供了更自由和优化的用户体验。 |
| [^89] | [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2402.05445) | 本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。 |
| [^90] | [Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport](https://arxiv.org/abs/2402.05443) | 本文介绍了一种可扩展的生成模型，称为Semi-dual JKO (S-JKO)，通过采用半对偶形式的JKO步骤，降低了训练复杂性，并在WGF上有显著的性能改进。 |
| [^91] | [Learning Uncertainty-Aware Temporally-Extended Actions](https://arxiv.org/abs/2402.05439) | 我们提出了一种名为不确定性感知时间扩展（UTE）的算法，在强化学习中解决了动作重复可能降低性能的问题，通过测量不确定性从而让策略根据需求进行选择，实验证明UTE优于现有算法。 |
| [^92] | [GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study](https://arxiv.org/abs/2402.05435) | 本研究通过使用结构化叙事提示，验证了GPT-4生成的叙述在传达生活事件方面的有效性。研究结果表明，大多数叙述能够足够传达提示的意图。同时，通过机器学习模型的训练和验证，可以自动识别有效和无效的叙述。 |
| [^93] | [Mixture Density Networks for Classification with an Application to Product Bundling](https://arxiv.org/abs/2402.05428) | 本论文提出了两个基于混合密度网络的分类模型，这两个模型通过拟合高斯混合分布并使用学习到的分布进行分类，效果略优于或与五个基准分类模型相当。在实际的产品捆绑应用中，我们的模型在学习产品支付意愿分布方面展现出真实的实用效果。 |
| [^94] | [A Sampling Theory Perspective on Activations for Implicit Neural Representations](https://arxiv.org/abs/2402.05427) | 本研究从采样理论的视角对隐式神经表示（INR）中的激活进行了全面分析，发现sinc激活是理论上的最佳选择，并建立了动力系统与INR之间的联系。 |
| [^95] | [Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures](https://arxiv.org/abs/2402.05424) | 提出了神经电路图，一种适用于深度学习架构通信的图形语言，可以准确地显示数据排列的变化、操作在坐标轴上的广播方式和线性操作的重要并行行为。 |
| [^96] | [DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2402.05421) | DiffTOP使用可微分轨迹优化作为策略表示来生成动作，解决了模型基于强化学习算法中的“目标不匹配”问题，并在模仿学习任务上进行了性能基准测试。 |
| [^97] | [Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification](https://arxiv.org/abs/2402.05417) | 本研究提出了一种不依靠分词的基于连接主义时间分类损失的文本验证码分类OCR模型，该模型在处理复杂和扭曲内容方面具有优势，并在公开数据集上取得了很高的准确率。 |
| [^98] | [Version age-based client scheduling policy for federated learning](https://arxiv.org/abs/2402.05407) | 这篇论文提出了基于版本年龄的联邦学习客户端调度策略。该策略考虑了及时性和内容滞后，解决了联邦学习中慢速节点的挑战，并对全局模型收敛性和稳定性产生了深远影响。 |
| [^99] | [Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes](https://arxiv.org/abs/2402.05406) | 本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。 |
| [^100] | [Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data](https://arxiv.org/abs/2402.05401) | 本研究通过研究自适应激活函数在稀疏实验数据预测建模中的应用，填补了当前对其影响了解不足的重要差距，并揭示出个体可调参数的自适应激活函数在预测模型中具有较高的准确性。 |
| [^101] | [Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions](https://arxiv.org/abs/2402.05400) | 通过训练一系列损失函数来优化类别不平衡数据上的ROC曲线，并减少对超参数选择的敏感性。 |
| [^102] | [TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning](https://arxiv.org/abs/2402.05396) | 该论文提出了TASER方法，它是针对动态图表示学习的时间自适应采样技术，在准确性、效率和可扩展性方面进行了优化，解决了现实世界动态图中存在的噪声问题。 |
| [^103] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^104] | [Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts](https://arxiv.org/abs/2402.05382) | 提出了一种基于混合聚类条件专家的任务定制化遮蔽自编码器(MoCE)方法，通过使用聚类条件门将每个专家只训练与语义相关的图像，为不同的下游任务提供定制的预训练模型，取得了比传统MAE更好的性能。 |
| [^105] | [Tradeoffs of Diagonal Fisher Information Matrix Estimators](https://arxiv.org/abs/2402.05379) | 本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。 |
| [^106] | [Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks](https://arxiv.org/abs/2402.05378) | 本文研究了多用户灵活双工网络中的物理层安全问题，提出了基于图神经网络的无监督学习策略，并通过大量数值模拟验证了其在性能和时间复杂度方面的优势。 |
| [^107] | [Reduced-order modeling of unsteady fluid flow using neural network ensembles](https://arxiv.org/abs/2402.05372) | 本论文关注使用神经网络集成方法进行非定常流体流动的降阶建模。研究使用卷积自编码器处理空间分布的数据，并使用长短期记忆网络进行时间序列预测。同时，引入集成学习技术中的装袋方法来解决错误传播的问题。 |
| [^108] | [Attention as Robust Representation for Time Series Forecasting](https://arxiv.org/abs/2402.05370) | 在时间序列预测中，我们提出的方法将注意力权重提升为主要表示，使用全局标志和局部窗口构建的注意力图作为稳健核表示来克服噪声和分布变化，并取得了比现有模型更好的性能 improvement. |
| [^109] | [Noise Contrastive Alignment of Language Models with Explicit Rewards](https://arxiv.org/abs/2402.05369) | 本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。 |
| [^110] | [Principled Preferential Bayesian Optimization](https://arxiv.org/abs/2402.05367) | 本文提出了基于原则的优先贝叶斯优化方法，通过利用偏好反馈构建黑盒函数的置信区间，并开发了一个乐观算法来解决问题。实验证明，该方法在遗憾界限和收敛性上具有显著的性能优势。 |
| [^111] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^112] | [Exploring Learning Complexity for Downstream Data Pruning](https://arxiv.org/abs/2402.05356) | 本文提出了一种将学习复杂性作为分类和回归任务的评分函数，以解决在有限计算资源下微调过程中过度参数化模型的问题。 |
| [^113] | [Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels](https://arxiv.org/abs/2402.05353) | 本文提出了一种称为联邦标签混合正则化（FLR）的策略，在联邦学习中解决标签噪音问题。FLR通过生成新的伪标签，将局部和全局模型预测相结合，提高了全局模型的准确性，并有效对抗噪音标签的记忆化。FLR与现有的标签噪音和联邦学习技术兼容，为处理充满标签不准确性的FL环境提供了改进的泛化能力。 |
| [^114] | [KIX: A Metacognitive Generalization Framework](https://arxiv.org/abs/2402.05346) | 人工智能代理缺乏通用行为，需要利用结构化知识表示。该论文提出了一种元认知泛化框架KIX，通过与对象的交互学习可迁移的交互概念和泛化能力，促进了知识与强化学习的融合，为实现人工智能系统的自主和通用行为提供了潜力。 |
| [^115] | [Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference](https://arxiv.org/abs/2402.05330) | 该论文提出了一种在广义标签转移和干扰参数下进行分类的新方法。通过将分类问题视为带有干扰参数的假设检验问题，我们能够有效地估计分类器在整个干扰参数空间中的接收者操作特性，并设计出在广义标签转移下不变的截断点，从而实现了对事件的可靠分类和不确定性度量。 |
| [^116] | [Learning on Multimodal Graphs: A Survey](https://arxiv.org/abs/2402.05322) | 这篇综述论文对多模态图学习的已有工作进行了对比分析，阐明了多模态学习的方式和主流技术特点，并揭示了其重要应用和未来方向。 |
| [^117] | [Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs](https://arxiv.org/abs/2402.05318) | 本文探讨了信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用，LLMs的整合标志着用户与信息系统交互方式的范式转变。 |
| [^118] | [Investigating Generalization Behaviours of Generative Flow Networks](https://arxiv.org/abs/2402.05309) | 本研究通过实证验证了生成流网络(GFlowNets)的一些泛化机制假设，发现它们学习逼近的函数具有隐含的基础结构，有助于泛化。同时，GFlowNets对于离线和离策略训练敏感，但隐含学习的奖励对训练分布的变化具有鲁棒性。 |
| [^119] | [Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks](https://arxiv.org/abs/2402.05307) | 本文探讨了实现具有可解释性的模型和策略的神经符号强化学习的三个路径，并揭示了学习的连续性和可微性的益处，以及将逻辑与数值仿真结合的难点。 |
| [^120] | [Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making](https://arxiv.org/abs/2402.05306) | Sym-Q是一个基于强化学习的模型，通过将符号回归重新定义为顺序决策任务来解决现有模型在泛化性和适应性方面的挑战。通过利用监督演示和奖励信号，Sym-Q能够根据拟合精度的质量改进表达式。 |
| [^121] | [BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs](https://arxiv.org/abs/2402.05301) | 本文介绍了BIKED++数据集，其中包含了140万个自行车设计的图像和参数化CAD文件。该数据集可以用于训练跨模态预测模型，例如使用参数化表示来准确估计图像的特征嵌入。该数据集也已公开，可供研究者使用。 |
| [^122] | [Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach](https://arxiv.org/abs/2402.05296) | 该论文提出了使用凝聚层次聚类和基于主题的方法对垃圾邮件进行分类的新思路。作者提出了两个新的数据集SPEMC-15K-E和SPEMC-15K-S，并使用凝聚层次聚类将其划分为11个类别。实验结果表明，TF-IDF和LR在英文数据集中达到最佳性能。 |
| [^123] | [An information theoretic approach to quantify the stability of feature selection and ranking algorithms](https://arxiv.org/abs/2402.05295) | 本论文提出了一种基于信息论的方法来量化特征选择和排序算法的稳定性。该方法能够评估不同算法结果中的特征排序的稳定性，包括完整的排名列表、特征子集和部分排名列表。 |
| [^124] | [Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection](https://arxiv.org/abs/2402.05294) | 本文首次分析了多模态联邦学习中的模态不一致性的影响，并揭示了其与参与客户端之间的数据异质性的联系。通过使用不考虑不一致性的信息融合机制和模态插值网络，在解决模态不一致性问题方面取得了一定的成果。 |
| [^125] | [A comparative study on feature selection for a risk prediction model for colorectal cancer](https://arxiv.org/abs/2402.05293) | 这项研究比较了不同特征选择算法在结直肠癌风险预测模型中的性能，并提出了视觉方法评估特征排序技术的稳定性。 |
| [^126] | [Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling](https://arxiv.org/abs/2402.05291) | 本研究开发了图神经网络作为有限元冰盖模拟的快速高保真度的仿真器，并在Pine Island Glacier的瞬态模拟中展示了与传统卷积神经网络和多层感知器相比更准确的重现冰盖厚度和速度的能力。同时，这些GNN成功捕捉到了更高底部熔化速率引起的冰质量损失和加速过程。在图形处理单元上实现的GNN仿真器显示出高达50倍的加速。 |
| [^127] | [Do Transformer World Models Give Better Policy Gradients?](https://arxiv.org/abs/2402.05290) | 在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。 |
| [^128] | [Analyzing Adversarial Inputs in Deep Reinforcement Learning](https://arxiv.org/abs/2402.05284) | 这篇论文通过形式验证的视角，分析了深度强化学习中对抗输入的特征，并提出了一个新的度量标准——对抗率，以及计算该度量标准的一套工具和算法。 |
| [^129] | [No Dimensional Sampling Coresets for Classification](https://arxiv.org/abs/2402.05280) | 本文通过敏感性抽样框架提出了无维度的核心子集用于分类问题，该子集的大小与维度无关，并适用于各种损失函数和分布输入。 |
| [^130] | [Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes](https://arxiv.org/abs/2402.05279) | 本文提出一种基于学习的方法，通过定义判别超平面来实现黑盒动态系统的安全过滤器。我们的方法不仅可以消除对特定证明函数的依赖，还简化了安全过滤器的设计。 |
| [^131] | [Exploring Hierarchical Classification Performance for Time Series Data: Dissimilarity Measures and Classifier Comparisons](https://arxiv.org/abs/2402.05275) | 本研究通过比较层次分类和平铺分类方法在时间序列数据分析中的表现，发现在使用MINIROCKET和TSD的情况下，层次分类表现出显著优势，而在使用STSF和SVM等替代方法时，平铺分类一直保持优势。TSD也在几乎所有情况下表现出比CBD和JSD更好的性能。 |
| [^132] | [Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks](https://arxiv.org/abs/2402.05271) | 了解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。前人的研究表明，在训练过程中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这被称为神经特征分析（NFA）。本研究解释了这种相关性的出现，并发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。在早期训练阶段，可以通过解析的方式预测NFA的发展速度。 |
| [^133] | [AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size](https://arxiv.org/abs/2402.05264) | AdaBatchGrad是一种改进的随机梯度下降方法，通过组合自适应批大小和自适应步长来提高方法的稳健性和稳定性。 |
| [^134] | [Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages](https://arxiv.org/abs/2402.05252) | 本文介绍了一种通过优化有序加权平均值函数，在LTR模型的训练过程中集成高效的公平排名模型，实现公平性、用户效用和运行时效率之间的有利平衡。 |
| [^135] | [QGFN: Controllable Greediness with Action Values](https://arxiv.org/abs/2402.05234) | 本文提出了一种新的算法QGFN，通过将GFN策略与动作值估计相结合，可以生成更多高奖励的样本而不牺牲多样性。 |
| [^136] | [Universal Neural Functionals](https://arxiv.org/abs/2402.05232) | 本文提出了通用神经功能（UNFs），一种能够自动构建适用于任何权重空间的置换等变模型的算法。实验结果显示，在优化小型图像分类器和语言模型时，UNFs能够取得有希望的改进，为学习优化器设计提供了新的思路。 |
| [^137] | [VerAs: Verify then Assess STEM Lab Reports](https://arxiv.org/abs/2402.05224) | VerAs是一个端到端的神经架构，用于验证和评估STEM实验报告。它通过利用多个维度的分析评估标准，以及针对学生提供详细反馈，帮助他们提高科学写作技巧。 |
| [^138] | [On Parameter Estimation in Deviated Gaussian Mixture of Experts](https://arxiv.org/abs/2402.05220) | 在偏离高斯混合专家模型中，本文通过构造Voronoi-based损失函数来解决参数估计问题。 |
| [^139] | [Self-calibrated convolution towards glioma segmentation](https://arxiv.org/abs/2402.05218) | 这项工作通过在nnU-Net网络的不同部分使用自校准卷积，证明了自校准模块可以显著提高肿瘤分割的准确性，同时保持整个肿瘤的分割准确性。 |
| [^140] | [Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models](https://arxiv.org/abs/2402.05210) | 这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。 |
| [^141] | [Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series](https://arxiv.org/abs/2402.05203) | 贝尔曼符合推断（BCI）是一个框架，通过解决一维随机控制问题，利用多步预测来提供校准的时间序列预测区间。BCI在任意分布转换和时间依赖性下实现了长期覆盖，且在波动率预测问题上生成更短的预测区间。 |
| [^142] | [Are LLMs Ready for Real-World Materials Discovery?](https://arxiv.org/abs/2402.05200) | LLMs在材料科学中的应用受限，无法实现实际应用。我们提出了基于材料科学知识和假设测试的MatSci-LLMs框架，并描述了关键的材料科学信息提取挑战。 |
| [^143] | [JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows](https://arxiv.org/abs/2402.05193) | JAX-Fluids 2.0是一个面向可压缩两相流的可微CFD的HPC求解器，通过引入高性能计算能力和增强的两相流建模能力，实现了规模化并行计算和稳定的自动微分梯度计算。 |
| [^144] | [Meta-learning the mirror map in policy mirror descent](https://arxiv.org/abs/2402.05187) | 该论文通过实证研究发现，传统的镜像映射选择（NPG）在标准基准环境中常常导致不理想的结果。通过元学习方法，找到了更高效的镜像映射，提升了性能。 |
| [^145] | [cecilia: A Machine Learning-Based Pipeline for Measuring Metal Abundances of Helium-rich Polluted White Dwarfs](https://arxiv.org/abs/2402.05176) | 本研究提出了cecilia，一种基于机器学习的光谱建模代码，用于测量富氦污染白矮星的金属丰度。该流程通过训练大量基于随机抽样的模型，旨在克服传统方法的限制，并能够扩展到大规模的研究中。 |
| [^146] | [Towards Understanding Inductive Bias in Transformers: A View From Infinity](https://arxiv.org/abs/2402.05173) | 本文研究了Transformer模型的归纳偏差，并发现它们倾向于对称排列函数，对称群的表示理论可以用于分析预测，同时提出了一个简化模型来解决学习曲线和网络输出，并在常见设置中得出学习能力的紧密边界，最后还证明了WikiText数据集具有排列对称性。 |
| [^147] | [A Resource Model For Neural Scaling Law](https://arxiv.org/abs/2402.05164) | 该论文介绍了神经缩放律的资源模型，通过观察实证发现，子任务的损失与分配的神经元成反比，复合任务中子任务获得的资源随模型变大而增长，保持资源比例不变。该模型可以用于预测复合任务的神经缩放律，并成功复制了Chinchilla模型的神经缩放律。该资源模型是表征和诊断神经网络的有用工具。 |
| [^148] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^149] | [What's documented in AI? Systematic Analysis of 32K AI Model Cards](https://arxiv.org/abs/2402.05160) | 本研究对Hugging Face平台上的32,111份AI模型文档进行了全面分析，发现大多数模型提供了模型卡，但信息量不一致。有关环境影响、限制和评估的部分填写率最低，训练部分则填写率最高。 |
| [^150] | [Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types](https://arxiv.org/abs/2402.05158) | 本文介绍了一个孟加拉OCR系统，具有重构文档布局、精确提取、多样化文档类型支持和优化字符与单词识别的特点。 |
| [^151] | [Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks](https://arxiv.org/abs/2402.05155) | 本研究解决了随机梯度下降方法在浅层人工神经网络训练中不收敛于全局最小值的问题，并提出了适用于该问题的局部最小解构造方法。 |
| [^152] | [Estimating On-road Transportation Carbon Emissions from Open Data of Road Network and Origin-destination Flow Data](https://arxiv.org/abs/2402.05153) | 该论文利用开放数据中的道路网络和起始位置-目的地流量数据，结合层次异构图学习方法，提出了一种估算公路交通碳排放的新方法。这个方法可以有效解决传统方法中数据收集困难的问题。 |
| [^153] | [CrashFormer: A Multimodal Architecture to Predict the Risk of Crash](https://arxiv.org/abs/2402.05151) | CrashFormer是一种多模态架构，利用全面的输入数据（如事故历史、天气信息、地图图像和人口信息），可以每6小时预测5.161平方公里地理范围内的未来事故风险。 |
| [^154] | [Designing deep neural networks for driver intention recognition](https://arxiv.org/abs/2402.05150) | 本文研究了深度神经网络架构对司机意图识别性能的影响，并应用神经架构搜索方法进行探索，为具有有限计算能力的安全关键应用提供指导。 |
| [^155] | [FlowPG: Action-constrained Policy Gradient with Normalizing Flows](https://arxiv.org/abs/2402.05149) | 本文提出了使用正则化流的动作约束策略梯度（FlowPG）方法，以解决动作约束强化学习中的挑战。该方法通过学习一个可逆映射和开发多种动作采样方法，有效地解决了在每个强化学习步骤中确保代理采取合理动作的问题。 |
| [^156] | [ApiQ: Finetuning of 2-Bit Quantized Large Language Model](https://arxiv.org/abs/2402.05147) | 这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。 |
| [^157] | [Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving](https://arxiv.org/abs/2402.05146) | 本文提出了一种动态结构化剪枝方法，用于压缩深度强化学习网络，以便在资源受限的自动驾驶设备中实现高效部署。通过逐渐删除不重要的神经元，我们的方法显著减小了内存消耗和计算量。 |
| [^158] | [Online Learning Approach for Survival Analysis](https://arxiv.org/abs/2402.05145) | 本论文介绍了一种在线数学框架，用于实时适应动态环境和有审查数据的生存分析，通过在线牛顿步骤(ONS)算法估计事件时间分布，并提出了保证ONS具有对数随机遗憾界的随机方法和自适应聚合方法。 |
| [^159] | [A Bandit Approach with Evolutionary Operators for Model Selection](https://arxiv.org/abs/2402.05144) | 本文提出了一种使用进化算子的强盗方法来进行模型选择，通过将模型选择问题建模为无穷臂赌博机问题，利用部分训练和准确性作为奖励，最终的算法Mutant-UCB在测试中表现出色，优于固定预算下的最先进技术。 |
| [^160] | [Tensor Completion via Integer Optimization](https://arxiv.org/abs/2402.05141) | 本文开发了一种通过整数优化实现张量补全的新算法，该算法通过在线性步骤中实现了收敛并达到了信息理论速率。 |
| [^161] | [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://arxiv.org/abs/2402.05140) | 本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。 |
| [^162] | [LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics and Cosmology](https://arxiv.org/abs/2402.05137) | LtU-ILI是一个用于天体物理学和宇宙学中机器学习推理的全能框架，包括各种神经架构、训练模式、先验知识和密度估计器，并具有全面的验证指标和易于并行化的设计。实际应用包括从X射线光度法估计星系团质量、推断宇宙学、表征引力波信号中的源和获取银河系中的物理尘埃参数。 |
| [^163] | [Personalized Language Modeling from Personalized Human Feedback](https://arxiv.org/abs/2402.05133) | 该论文提出了一个个性化语言模型的方法，通过在于用户的反馈数据中引入个性化特征来解决强化学习框架在多样化用户偏好下存在的问题。 |
| [^164] | [Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering](https://arxiv.org/abs/2402.05127) | 本研究提出了一种新的抑郁症检测和治疗范式，使用先进的大型语言模型，经过特定提示微调以诊断、解释和建议治疗干预。同时还介绍了一个丰富的数据库，以提供个性化的治疗建议。此方法与患者进行共情对话管理，有效支持抑郁症患者。 |
| [^165] | [Graph Neural Network and NER-Based Text Summarization](https://arxiv.org/abs/2402.05126) | 这个项目使用图神经网络和命名实体识别系统，提出了一种创新的文本摘要方法。图神经网络能够捕获并处理文本信息中的关系数据，而命名实体识别系统通过识别和强调关键实体来保持摘要的重点。 |
| [^166] | [More Agents Is All You Need](https://arxiv.org/abs/2402.05120) | 大型语言模型的性能与代理数量成比例，通过简单的采样和投票方法可以进一步增强性能，这种方法与现有的复杂方法正交。 |
| [^167] | [Unsupervised Motion Retargeting for Human-Robot Imitation](https://arxiv.org/abs/2402.05115) | 该论文研究了无监督运动重定位用于人机仿真的问题，提出了一个编码器-解码器神经网络模型进行领域到领域的转换。这种方法可以在无对应数据的情况下进行人机仿真。 |
| [^168] | [A Light-weight and Unsupervised Method for Near Real-time Behavioral Analysis using Operational Data Measurement](https://arxiv.org/abs/2402.05114) | 这种方法提出了一种轻量级和无监督的近实时行为分析方法，可以适应大型计算系统的变化，并能准确识别出行为异常。 |
| [^169] | [Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems](https://arxiv.org/abs/2402.05067) | 本文提出了利用物理信息神经网络进行多尺度建模的方法，通过解耦大尺度和小尺度动力学，并在正交基函数空间中近似小尺度系统。实验结果表明该方法在处理液体动力学问题以及更复杂的情况下具有较高的有效性和适用性。 |
| [^170] | [Federated Learning Can Find Friends That Are Beneficial](https://arxiv.org/abs/2402.05050) | 本研究介绍了一种新颖的算法，在Federated Learning中使用自适应聚合权重来识别对特定学习目标最有益的客户，证明了该方法的收敛性，并经过实证评估发现，使用该算法引导的合作优于传统方法，这为更加简化和有效的Federated Learning实现奠定了基础。 |
| [^171] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^172] | [Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching](https://arxiv.org/abs/2402.04924) | 本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。 |
| [^173] | [Fast Timing-Conditioned Latent Audio Diffusion](https://arxiv.org/abs/2402.04825) | 本研究提出了一种名为Stable Audio的生成模型，该模型利用潜在扩散和条件化技术，实现了高效生成44.1kHz立体声音乐和音效。与其他最先进的模型不同，Stable Audio能够生成具有结构和立体声的音乐，并在性能评估上表现出色。 |
| [^174] | [Continuous Multidimensional Scaling](https://arxiv.org/abs/2402.04436) | 连续多维标度是关于将距离信息嵌入欧几里得空间的过程，并探讨了在对象集不断增加的情况下，将整个嵌入问题序列视为一个固定空间中的一系列优化问题的方法和结论。 |
| [^175] | [Tempered Calculus for ML: Application to Hyperbolic Model Embedding](https://arxiv.org/abs/2402.04163) | 本论文介绍了基于温和微积分的理论和工具，来改进目前在机器学习中使用的数学扭曲方法，特别强调与几何和机器学习相关的特性。 |
| [^176] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^177] | [A General Theory for Kernel Packets: from state space model to compactly supported basis](https://arxiv.org/abs/2402.04022) | 该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。 |
| [^178] | [Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents](https://arxiv.org/abs/2402.03678) | 本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。 |
| [^179] | [IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images](https://arxiv.org/abs/2402.03227) | IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。 |
| [^180] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^181] | [A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators](https://arxiv.org/abs/2402.03149) | 本文详细分析了基于微环的相干光学GEMM加速器的组织，其通过分裂、聚合、调制、加权和求和等方式操作光信号以加速深度神经网络中的矩阵-矩阵乘法，提高吞吐量和能量效率。 |
| [^182] | [Taylor Videos for Action Recognition](https://arxiv.org/abs/2402.03019) | Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。 |
| [^183] | [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987) | 本文介绍了一种针对 GPT 模型的对话重构攻击，该攻击具有劫持会话和重构对话的两个步骤。通过对该攻击对 GPT 模型的隐私风险进行评估，发现 GPT-4 对该攻击具有一定的鲁棒性。 |
| [^184] | [The Virtues of Pessimism in Inverse Reinforcement Learning](https://arxiv.org/abs/2402.02616) | 本论文提出了一种使用离线强化学习算法来加速逆推强化学习中强化学习子程序的替代方法，通过保持与专家数据分布接近的悲观主义策略，提高了逆推强化学习的样本效率。 |
| [^185] | [SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach](https://arxiv.org/abs/2402.02275) | SudokuSens是一种生成框架，用于通过自动生成训练数据来提高IoT感知应用的深度学习模型鲁棒性。该框架通过模仿实际数据采集过程中未遇到的实验配置，增加了数据的多样性。 |
| [^186] | [Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey](https://arxiv.org/abs/2402.02242) | 本综述调研了面向预训练视觉模型的参数高效微调方法，通过最小参数修改超越全面微调的性能，提供了全面的概述和未来方向，并提供了丰富的资源收藏。 |
| [^187] | [Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking](https://arxiv.org/abs/2402.02152) | 这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。 |
| [^188] | [A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles](https://arxiv.org/abs/2402.01703) | 该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。 |
| [^189] | [ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models](https://arxiv.org/abs/2402.00794) | 本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。 |
| [^190] | [Short: Benchmarking transferable adversarial attacks](https://arxiv.org/abs/2402.00418) | 本研究首次全面评估了可转移性对抗攻击的方面，引入了一个基准框架并系统分类和评估了各种增强对抗攻击可转移性的方法，为不同的模型架构提供了一个标准化的平台。 |
| [^191] | [$\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning](https://arxiv.org/abs/2312.17293) | $\mu$GUIDE是一个通用的贝叶斯框架，利用深度学习进行微结构成像，能够有效估计组织微结构参数的后验分布，并量化参数的不确定性和模糊性。 |
| [^192] | [In-Context Reinforcement Learning for Variable Action Spaces](https://arxiv.org/abs/2312.13327) | 本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。 |
| [^193] | [Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes](https://arxiv.org/abs/2312.12112) | 本论文提出了CLLM方法，利用LLMs和数据筛选在低数据环境中进行表格增强。通过利用大型语言模型的先验知识以及基于学习动态、置信度和不确定度指标的筛选机制，CLLM取得了优越的性能。 |
| [^194] | [Social Learning: Towards Collaborative Learning with Large Language Models](https://arxiv.org/abs/2312.11441) | 本论文在大型语言模型的背景下引入了“社会学习”的框架，通过自然语言相互共享知识，提出了两种知识传递方法，并证明了这些方法的可行性和效果。 |
| [^195] | [How Far Can Fairness Constraints Help Recover From Biased Data?](https://arxiv.org/abs/2312.10396) | 公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。 |
| [^196] | [Event-Based Contrastive Learning for Medical Time Series](https://arxiv.org/abs/2312.10308) | 本论文介绍了一种基于事件的对比学习方法（EBCL），用于学习医学时间序列中关键事件前后的数据编码。研究发现，相对于其他预训练方法，EBCL能够产生性能更好的模型，对于心力衰竭队列的关键下游任务具有更好的微调性能，并能有效地将具有相似风险的患者进行聚类。 |
| [^197] | [Spectral State Space Models](https://arxiv.org/abs/2312.06837) | 本文提出了一种称为光谱状态空间模型的序列预测架构，通过学习具有光谱滤波算法的线性动态系统实现。这些模型具有可证明的鲁棒性和固定卷积滤波器，适用于需要非常长程记忆的预测任务。 |
| [^198] | [SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting](https://arxiv.org/abs/2312.03406) | SVQ是一种利用稀疏回归实现简明表示的稀疏向量量化技术，通过保留关键细节和滤除噪声来提高时空预测性能。在实验中，SVQ在五个空间-时间基准数据集上取得了最先进的结果。 |
| [^199] | [When accurate prediction models yield harmful self-fulfilling prophecies](https://arxiv.org/abs/2312.01210) | 本研究通过调查预测模型的部署对决策产生有害影响的情况，发现这些模型可能会成为有害的自我实现预言。这些模型不会因为对某些患者造成更糟糕的结果而使其预测能力变无效。 |
| [^200] | [MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting](https://arxiv.org/abs/2311.18780) | 本文提出了一种名为MultiResFormer的模型，通过自适应选择最优补丁长度，动态建模时序变化。相比于基于补丁的Transformer和CNN基线，MultiResFormer在长期预测任务上表现出更好的性能。 |
| [^201] | [SMaRt: Improving GANs with Score Matching Regularity](https://arxiv.org/abs/2311.18208) | 本文提出使用分数匹配规则（SMaRt）来改进GANs的优化问题，通过持续将生成的数据点推向真实数据流形，提高了合成性能。 |
| [^202] | [DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks](https://arxiv.org/abs/2311.12944) | 本研究提出了一种用于5G及其后太阳能小型蜂窝网络的最佳无人机负载重分配机制，通过使用无人机上的空中基站进行可靠安全的电力再分配，提高了网络的可靠性和稳健性。 |
| [^203] | [Modeling Choice via Self-Attention](https://arxiv.org/abs/2311.07607) | 本论文提出了一种选择模型，利用自注意力成功地进行了建模，这是在深度学习和选择建模领域中的一个重要的研究空白。 |
| [^204] | [TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings](https://arxiv.org/abs/2310.14450) | 本研究提出了一种通过对比学习以及利用未标记新闻文章数据集来训练主题无关和主题感知嵌入的方法，在立场检测中取得了最先进的性能，并在公开数据集上达到了0.771的F1分数。 |
| [^205] | [Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras](https://arxiv.org/abs/2310.04521) | 本文提出了一种Lie神经元网络，能够以任何半单Lie代数数据为输入，通过伴随操作使其具有等变性。通过推广向量神经元网络和引入新的层，该网络在各个领域具有广泛的适用性和竞争性能。 |
| [^206] | [Is Learning in Biological Neural Networks based on Stochastic Gradient Descent? An analysis using stochastic processes](https://arxiv.org/abs/2309.05102) | 本文研究了随机梯度下降在生物神经网络中的应用，并展示了每个学习机会经过多次局部更新后近似进行梯度步骤。这一结果表明，随机梯度下降可能在优化生物神经网络中起到作用。 |
| [^207] | [OHQ: On-chip Hardware-aware Quantization](https://arxiv.org/abs/2309.01945) | 本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。 |
| [^208] | [Lookbehind-SAM: k steps back, 1 step forward](https://arxiv.org/abs/2307.16704) | 本研究提出了一种名为Lookbehind-SAM的方法，通过多次上升步骤和线性插值来增强最大化和最小化过程，以实现更好的损失锐度折衷。实验证明，该方法在各种任务中都有多种优点，包括提高的泛化性能、更高的鲁棒性和改进的学习过程。 |
| [^209] | [AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725) | AnimateDiff是一个实用的框架，可以动画化个性化的T2I模型，无需特定调整。它包含一个即插即用的运动模块，通过训练使用实际视频中的运动先验。运动模块可以插入到任何个性化T2I模型中，从而形成一个个性化的动画生成器。 |
| [^210] | [Trainable Transformer in Transformer](https://arxiv.org/abs/2307.01189) | 这篇论文介绍了一种名为Transformer in Transformer (TinT)的高效构造方式，它可以让Transformer在推理过程中模拟和微调复杂的内部模型，同时使用创新的近似技术大幅减少了模型参数和内存开销。 |
| [^211] | [Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space](https://arxiv.org/abs/2306.02574) | 本文研究了马尔可夫决策过程中具有可数无限状态空间的最优控制问题，提出了基于汤普森采样和动态大小片段的算法进行贝叶斯学习，解决了在这些模型上的挑战。 |
| [^212] | [ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms](https://arxiv.org/abs/2305.04359) | ParlayANN是一个具有确定性和并行性的基于图的近似最近邻搜索算法库，提供了用于开发这类算法的一套有用工具。 |
| [^213] | [DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural Network Worry-Free?](https://arxiv.org/abs/2303.01213) | DSD$^2$提出了一个学习框架，能够避免稀疏双下降现象，并提高模型的泛化性能；引入了熵度量，使得可以使用传统的停止准则；对相关因素进行了全面的定量分析。 |
| [^214] | [HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline](https://arxiv.org/abs/2302.02104) | HardSATGEN方法提出了一种精细的控制机制，以更好地恢复工业基准的结构和计算特性，此方法在工业SAT公式生成任务中表现出优越性。 |
| [^215] | [Unsupervised 3D Keypoint Discovery with Multi-View Geometry](https://arxiv.org/abs/2211.12829) | 本文提出了一种无监督算法，利用多视图几何约束，从多视图图像中学习发现人体的3D关键点，并通过重新投影到每个视图来估计3D关键点的准确性和可解释性。在多个基准数据集上，与其他无监督方法相比，我们的方法发现了更准确和具有解释性的3D关键点。 |
| [^216] | [Training Overparametrized Neural Networks in Sublinear Time](https://arxiv.org/abs/2208.04508) | 这项研究提出了一种新的训练方法，可以在次线性时间内训练过参数化的神经网络，提高了训练的效率。 |
| [^217] | [Personalized PCA: Decoupling Shared and Unique Features](https://arxiv.org/abs/2207.08041) | 本文介绍了个性化PCA（PerPCA），通过使用全局和局部主成分来编码独特和共享特征，解决了PCA面临的异质性挑战。我们证明，在温和的条件下，我们可以通过受约束优化问题识别和恢复出独特和共享的特征。我们还设计了一个联邦算法来解决这个问题，并证明了算法的线性收敛性。 |
| [^218] | [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) | Matryoshka表示学习（MRL）是一种灵活的表示学习方法，能够在不同计算资源下适应多个下游任务，同时保持准确性和丰富性。 |
| [^219] | [The Fairness of Credit Scoring Models](https://arxiv.org/abs/2205.10200) | 本论文研究了信用评分模型的公平性问题，提出了一种形式化测试算法公平性的方法，并探索了影响公平性的变量。研究结果可以指导信贷商监测算法公平性、监管机构控制公平性，同时提高受保护群体的利益，同时保持高水平的预测准确性。 |
| [^220] | [Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning](https://arxiv.org/abs/2110.05286) | 本论文提出了一个用于训练强化学习代理的自我解释从示范中学习的框架（SERLfD）。通过代理人生成自我解释并识别有价值的高级关系特征作为成功轨迹的解释原因，可以在克服示范中存在的模棱两可情况的同时为强化学习提供指导。 |
| [^221] | [Generalization of LiNGAM that allows confounding.](http://arxiv.org/abs/2401.16661) | 本文提出了一种名为LiNGAM-MMI的方法，可以增强LiNGAM模型以处理混淆问题。该方法使用KL散度量化混淆程度，并通过最短路径问题解决方案高效地确定变量顺序，不论是否存在混淆情况。实验证明，LiNGAM-MMI可以更准确地识别正确的变量顺序。 |
| [^222] | [Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning.](http://arxiv.org/abs/2401.10371) | Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。 |
| [^223] | [Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering.](http://arxiv.org/abs/2401.09071) | 本文重新思考了谱图神经网络，并揭示了谱滤波和空间聚合之间的联系。该研究发现，谱滤波在隐含地将原始图转换成适应性新图，并明确计算用于空间聚合的新图。适应性新图展现出非局部性，并能够反映节点之间的标签一致性。 |
| [^224] | [LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control.](http://arxiv.org/abs/2401.04855) | 提出了一种可学习的感知-行动-通信(LPAC)架构，使用卷积神经网络处理环境感知，图神经网络实现机器人之间的信息交流，浅层多层感知机计算机器人的动作。使用集中式显微算法训练模型，实现机器人群体的协作。 |
| [^225] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^226] | [Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning.](http://arxiv.org/abs/2401.03756) | 该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。 |
| [^227] | [Energy-Preserving Reduced Operator Inference for Efficient Design and Control.](http://arxiv.org/abs/2401.02889) | 本文提出了一种能够节约能量的减少操作推断方法，用于高效设计和控制中的多次计算任务，特别适用于保持能量的偏微分方程控制系统。 |
| [^228] | [Fairness-Aware Job Scheduling for Multi-Job Federated Learning.](http://arxiv.org/abs/2401.02740) | 本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。 |
| [^229] | [Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults.](http://arxiv.org/abs/2401.01172) | 本文提出了一种融合时间频率分析和深度学习技术的方法，用于在实际条件下诊断带有时间变化速度和不同噪声水平的轴承故障。这种方法有效地解析与不同轴承故障相关的独特动态模式。 |
| [^230] | [Learning Collective Behaviors from Observation.](http://arxiv.org/abs/2311.00875) | 本文介绍了一系列学习方法，用于从观察中识别动态系统的结构，并理解复杂系统中相互作用代理的 emergent behaviors。这些方法不仅具备理论收敛保证，还能高效处理高维观测数据，解决观测/随机噪声、复杂的交互规则、丢失的交互特征和现实世界观测数据等问题。通过采用变分逆问题方法设计合适的损失函数，我们的学习方法具备降维能力。 |
| [^231] | [DiffEnc: Variational Diffusion with a Learned Encoder.](http://arxiv.org/abs/2310.19789) | DiffEnc是一种使用学习的编码器的变分扩散模型，通过引入数据和深度相关的均值函数和可调节的噪声方差比率，实现了最先进的可能性。 |
| [^232] | [VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models.](http://arxiv.org/abs/2310.13367) | VFedMH是一种垂直联合学习方法，通过在前向传播过程中聚合参与者的嵌入来处理参与者之间的异构模型，解决了现有VFL方法面临的挑战。 |
| [^233] | [Lag-Llama: Towards Foundation Models for Time Series Forecasting.](http://arxiv.org/abs/2310.08278) | Lag-Llama是一个基于大量时间序列数据训练的通用预测模型，在未见过的数据集上展现出强大的零样本预测能力，并使用光滑断裂幂律模型来拟合和预测扩展行为。 |
| [^234] | [Discovering Mixtures of Structural Causal Models from Time Series Data.](http://arxiv.org/abs/2310.06312) | 这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。 |
| [^235] | [A Language-Agent Approach to Formal Theorem-Proving.](http://arxiv.org/abs/2310.04353) | COPRA是一种面向形式定理证明的语言代理方法，利用大型语言模型进行上下文学习，通过选择策略和检索定义和引理进行证明，在MiniF2F基准和Coq任务上表现出优异的性能。 |
| [^236] | [On Wasserstein distances for affine transformations of random vectors.](http://arxiv.org/abs/2310.03945) | 本研究阐述了关于随机向量之间的Wasserstein距离的下界，重点是仿射变换，并应用于流型学习和手写数据集模拟。 |
| [^237] | [GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior.](http://arxiv.org/abs/2309.13160) | 本文提出了一种基于高斯混合后验的VAE方法，重新定义了ELBO，引入正则化项和PatchGAN鉴别器，能够生成逼真的人脸。 |
| [^238] | [Answering Layer 3 queries with DiscoSCMs.](http://arxiv.org/abs/2309.09323) | 本文介绍了DiscoSCMs，一种用于解决因果查询的模型。它通过扩展结构因果模型和潜在结果框架来解决一致性规则引发的退化问题，并在分析个性化激励场景中的潜在结果时展示了其有效性。通过引入独立潜在噪声条件，可以提高解决Layer 3查询的准确性和可解释性。 |
| [^239] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^240] | [Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images.](http://arxiv.org/abs/2308.07688) | 该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。 |
| [^241] | [Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding.](http://arxiv.org/abs/2308.05893) | 本文综述了在多智能体路径规划中深度强化学习技术的应用。与其他研究不同，我们重点介绍了DRL方法在MAPF中的整合，并解决了MAPF解决方案评估指标缺乏统一性的问题。我们讨论了基于模型的DRL作为未来发展方向，并提供了解决MAPF当前挑战所需的基础理解。 |
| [^242] | [Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems.](http://arxiv.org/abs/2308.01084) | 本论文提出了一种基于数据的方法来学习哈密顿系统，利用提升假设，通过强制实施哈密顿结构和使用辛自编码器，我们实现了在变换的坐标系下具有哈密顿结构的二次动力学，保持系统的长期稳定性和较低的模型复杂度。 |
| [^243] | [Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges.](http://arxiv.org/abs/2308.00031) | 这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。 |
| [^244] | [Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques.](http://arxiv.org/abs/2307.12971) | 本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。 |
| [^245] | [Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability.](http://arxiv.org/abs/2307.07084) | 本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。 |
| [^246] | [On the sample complexity of estimation in logistic regression.](http://arxiv.org/abs/2307.04191) | 本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。 |
| [^247] | [Incentive-Theoretic Bayesian Inference for Collaborative Science.](http://arxiv.org/abs/2307.03748) | 本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。 |
| [^248] | [Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series.](http://arxiv.org/abs/2307.01482) | 提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。 |
| [^249] | [AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities.](http://arxiv.org/abs/2306.13149) | 本文提出了一种名为AmicroN的框架，可以利用传感器数据自动生成微动作注释，弥补现有数据集缺乏细粒度注释的不足。 |
| [^250] | [DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles.](http://arxiv.org/abs/2306.05957) | DDLP算法使用深度潜在粒子(DLP)表示法实现无监督物体中心视频预测，并取得了最先进的预测结果。算法的可解释性使得可以进行“假设”生成，而DLP的紧凑结构使得效率高并可以进行基于扩散的无条件视频生成。 |
| [^251] | [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection.](http://arxiv.org/abs/2306.01710) | 本文提出了一种基于数据驱动的相对不确定性度量，用于误分类检测。该度量可以通过学习软预测的分布模式，识别出被误分类的样本，并展示了在多个图像分类任务中的实证改进，优于现有的误分类检测方法。 |
| [^252] | [Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization.](http://arxiv.org/abs/2305.19838) | 本文提出了一种放松去中心化无遗憾高维贝叶斯优化中加法约束的方法，并且解决了过度探索问题。该方法称为DumBO，并且在实验中展示了竞争性能。 |
| [^253] | [Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape.](http://arxiv.org/abs/2305.19510) | 本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。 |
| [^254] | [NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference.](http://arxiv.org/abs/2305.14405) | NeuralMatrix是一种框架，能够在单个通用矩阵乘法加速器上计算深度神经网络(DNNs)，并可在保持推理准确度的情况下实现高达113倍至19.44倍的性能提升。 |
| [^255] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^256] | [Convergence of Alternating Gradient Descent for Matrix Factorization.](http://arxiv.org/abs/2305.06927) | 本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。 |
| [^257] | [Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification.](http://arxiv.org/abs/2305.06344) | 本文提出了一种新的神经网络结构——频率支持神经网络，它通过加入频率信息适应于非线性系统辨识任务，并在多个任务中表现出优越性。 |
| [^258] | [Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System.](http://arxiv.org/abs/2304.13105) | 本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。 |
| [^259] | [An Introduction to Transformers.](http://arxiv.org/abs/2304.10557) | Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。 |
| [^260] | [Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack.](http://arxiv.org/abs/2303.10555) | 本文针对 LiDAR 欺骗攻击在目标检测领域存在的研究差距进行了实证研究，使用9种 LiDAR 和3种目标检测器进行了大规模测量，展示了一种新的攻击模式，提出了安全的 LiDAR 设计和评估方法。 |
| [^261] | [Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes.](http://arxiv.org/abs/2302.02809) | 本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。 |
| [^262] | [Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons.](http://arxiv.org/abs/2301.11270) | 该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。 |
| [^263] | [Topological Learning in Multi-Class Data Sets.](http://arxiv.org/abs/2301.09734) | 本文将拓扑数据分析技术应用于多类数据集中，通过构建拓扑分类器和简单复合体，研究了拓扑复杂性对前馈深度神经网络学习的影响，并验证了拓扑复杂性与DNN学习之间的负相关性。 |
| [^264] | [Federated Recommendation with Additive Personalization.](http://arxiv.org/abs/2301.09109) | FedRAP是一种新的推荐系统生成方法，它使用联邦学习训练共享项嵌入和本地个性化视图，以捕捉用户对推荐项目感知的个体差异并且降低通信成本和延迟。 |
| [^265] | [Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions.](http://arxiv.org/abs/2210.07996) | 该论文研究了具有不连续分布的网络收入管理问题，并提出了一种在线算法，其在此模型下实现了对数级别的遗憾。这是在不需要任何“退化”假设的情况下，首次在具有连续值的NRM模型中实现对数级别遗憾的结果。 |
| [^266] | [The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information.](http://arxiv.org/abs/2102.10019) | 本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。 |

# 详细

[^1]: ALERT-Transformer: 将异步和同步机器学习桥接在实时事件驱动的时空数据上

    ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data

    [https://rss.arxiv.org/abs/2402.01393](https://rss.arxiv.org/abs/2402.01393)

    ALERT-Transformer是一种将异步感知与同步处理相结合的新颖桥接方式，通过ALERT模块、灵活的数据读取和基于块的稀疏性优化，实现了对实时事件驱动时空数据的经典处理，其性能超过竞争对手并具有较低的延迟。

    

    我们旨在通过稠密机器学习模型，实现对由事件感应器产生的连续超稀疏时空数据的经典处理。我们提出了一种新颖的混合管道，由异步感知和同步处理组成，结合了几个思路：（1）基于PointNet模型的嵌入——ALERT模块，可以通过泄漏机制不断整合新事件并消除旧事件，（2）嵌入数据的灵活读取，可以以任何采样率将始终最新的特征输入到下游模型中，（3）借鉴Vision Transformer的基于块的方法来利用输入的稀疏性以优化方法的效率。这些嵌入然后由一个经过对象和手势识别训练的Transformer模型进行处理。使用这种方法，我们实现了比竞争对手更低的延迟，达到了最新技术水平的性能。我们还证明了我们的异步模型可以以任何所需的采样率进行操作。

    We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
    
[^2]: FedMoE: 数据级别个性化的混合专家模型用于异构模型的个性化联邦学习

    FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning

    [https://rss.arxiv.org/abs/2402.01350](https://rss.arxiv.org/abs/2402.01350)

    FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。

    

    联邦学习广泛应用于分散数据的协同训练，但面临数据、系统和模型异构等挑战。这导致模型异构个性化联邦学习 (MHPFL) 的出现。然而，当前的MHPFL方法在数据和模型隐私、模型性能、通信和计算成本方面仍存在关切。为应对这些问题，我们提出了一种新颖的模型异构个性化联邦学习算法 (FedMoE) ，采用著名的专家混合模型 (MoE) 来增强大型语言模型 (LLM)。它为每个客户端的本地异构大模型分配了一个共享的均匀小特征提取器和一个本地门控网络。(1) 在本地训练过程中，本地异构模型的特征提取器作为个性化特征（表示）提取的本地专家，而共享的均匀小特征提取器则作为广义特征提取的全局专家。

    Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. 
    
[^3]: SPHINX-X: 扩展数据和参数用于一系列多模态大型语言模型

    SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models

    [https://arxiv.org/abs/2402.05935](https://arxiv.org/abs/2402.05935)

    本论文介绍了SPHINX-X，一种扩展的多模态大型语言模型系列。通过改进架构和训练效率，我们成功构建了一系列参数大小和多语言能力不同的MLLMs，与数据和参数规模有强相关性。

    

    我们提出SPHINX-X，一种基于SPHINX开发的广泛多模态大型语言模型（MLLM）系列。为了改善架构和训练效率，我们通过移除冗余的视觉编码器、绕过完全填充的子图像，并将多阶段训练简化成为一阶段的全集合模式，修改了SPHINX框架。为了充分发挥MLLM的潜力，我们组装了一个综合的跨语言、跨视觉和视觉-语言任务的多领域、多模态的数据集，涵盖了公开可用的资源。我们进一步使用我们的OCR密集和Mark数据集丰富这个收集，扩展了多样性和普适性。通过对不同基础LLM进行训练，包括TinyLlama1.1B、InternLM2-7B、LLaMA2-13B和Mixtral8x7B，我们获得了一系列参数大小和多语言能力变化的MLLMs。全面的基准测试揭示了多模态性能与数据和参数规模之间的强相关性。

    We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
    
[^4]: 无需GNN的图中节点分类方法

    Classifying Nodes in Graphs without GNNs

    [https://arxiv.org/abs/2402.05934](https://arxiv.org/abs/2402.05934)

    该论文提出了一种无需GNN的图中节点分类方法，通过平滑约束、伪标签迭代和领域标签直方图三个关键组件，可以在不训练GNN的情况下，在标准流行的基准数据集上达到最先进的准确率。

    

    图神经网络（GNN）是图中节点分类的主要范式，但由于其消息传递架构导致其具有一些不理想的属性。最近，蒸馏方法成功地在测试时消除了对GNN的使用，但仍然需要在训练时使用。我们对GNN在蒸馏方法中的角色进行了仔细分析。这个分析导致我们提出了一种完全不依赖GNN的节点分类方法，无需在训练或测试时使用它们。我们的方法包含三个关键组件：平滑约束、伪标签迭代和领域标签直方图。我们的最终方法可以在标准流行的基准数据集上达到与最先进的准确率，而不需要训练GNN。

    Graph neural networks (GNNs) are the dominant paradigm for classifying nodes in a graph, but they have several undesirable attributes stemming from their message passing architecture. Recently, distillation methods succeeded in eliminating the use of GNNs at test time but they still require them during training. We perform a careful analysis of the role that GNNs play in distillation methods. This analysis leads us to propose a fully GNN-free approach for node classification, not requiring them at train or test time. Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms. Our final approach can match the state-of-the-art accuracy on standard popular benchmarks such as citation and co-purchase networks, without training a GNN.
    
[^5]: 频域中的时间序列扩散

    Time Series Diffusion in the Frequency Domain

    [https://arxiv.org/abs/2402.05933](https://arxiv.org/abs/2402.05933)

    本论文通过分析时间序列在频域中的表示，探讨了基于评分的扩散模型中的归纳偏差，提出了频域扩散模型，并将其与经典的时间扩散模型进行了比较。

    

    傅里叶分析在信号处理的发展中起到了重要的作用。这使我们想知道这个框架是否能够同样有益于生成模型。在本文中，我们通过时间序列扩散模型的范围来探讨这个问题。具体来说，我们分析了在频域中表示时间序列是否对基于评分的扩散模型具有有用的归纳偏差。通过从时间域中扩散的经典SDE公式出发，我们展示了在频域中发生了一种双重扩散过程，并具有一个重要的细微差别：布朗运动被我们称之为镜像布朗运动所取代，其特征是其组分之间的镜像对称性。在此基础上，我们展示了如何通过适应去噪评分匹配方法来实现频域中的扩散模型。这导致了频域扩散模型，我们将其与经典的时间扩散模型进行了比较。我们在实际工作上进行了实证评估。

    Fourier analysis has been an instrumental tool in the development of signal processing. This leads us to wonder whether this framework could similarly benefit generative modelling. In this paper, we explore this question through the scope of time series diffusion models. More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models. By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components. Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. This results in frequency diffusion models, which we compare to canonical time diffusion models. Our empirical evaluation on real-wor
    
[^6]: WebLINX: 多轮对话下的真实世界网站导航

    WebLINX: Real-World Website Navigation with Multi-Turn Dialogue

    [https://arxiv.org/abs/2402.05930](https://arxiv.org/abs/2402.05930)

    本论文提出了对话式网站导航的问题，并设计了一个 WEBLINX 基准测试，用于训练和评估代理。为了解决大量信息的处理瓶颈，文中提出了一个受检索启发的模型。实验结果表明，该模型能够在多种场景下复制人类行为的能力。

    

    我们提出了对话式网站导航的问题，其中数字代理控制着一个网页浏览器，并按照用户的指令以多轮对话的方式解决真实世界任务。为了支持这个问题，我们引入了 WEBLINX - 一个100K交互的大规模基准测试，在2300个专家演示中进行了对话式网站导航的测试。我们的基准涵盖了150多个真实世界网站上的广泛模式，可以用于在不同场景下训练和评估代理。由于存在大量信息，大型语言模型 (LLMs) 无法实时处理整个网页。为了解决这个瓶颈，我们设计了一个受检索启发的模型，通过排名相关元素来高效地修剪 HTML 页面。我们使用选定的元素，以及屏幕截图和操作历史记录，评估各种模型在导航网页时复制人类行为的能力。我们的实验从小型纯文本模型到专有的多模态 LLMs 进行了测试。

    We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
    
[^7]: 一个交互式智能体基础模型

    An Interactive Agent Foundation Model

    [https://arxiv.org/abs/2402.05929](https://arxiv.org/abs/2402.05929)

    我们提出了一个交互式智能体基础模型，采用新颖的训练范式，能够跨领域、数据集和任务进行训练，展现出通用性和适应性，且在机器人、游戏 AI 和医疗保健领域表现出色。

    

    人工智能系统的发展正在从创建静态、任务特定的模型转变为能够在各种应用中表现出色的动态智能体系统。我们提出了一个交互式智能体基础模型，采用了一种新颖的多任务智能体训练范式，用于训练跨领域、数据集和任务的 AI 智能体。我们的训练范式统一了各种预训练策略，包括视觉遮挡自编码器、语言建模和下一步行动预测，实现了一个通用而适应性强的 AI 框架。我们在三个独立领域 - 机器人、游戏 AI 和医疗保健中展示了我们框架的性能。我们的模型在每个领域都展示了生成有意义和上下文相关输出的能力。我们方法的优势在于其广泛性，利用了各种数据源，如机器人序列、游戏数据、大规模视频数据集和文本信息，以实现高效的效果。

    The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effectiv
    
[^8]: 依赖学习理论中的尖锐率：避免样本大小缩减的平方损失

    Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss

    [https://arxiv.org/abs/2402.05928](https://arxiv.org/abs/2402.05928)

    本文研究了依赖学习理论中的尖锐率，主要是为了避免样本大小缩减对方差产生影响。当假设类别的拓扑结构符合某些条件时，经验风险最小化者的性能与类别的复杂性和二阶统计量有关。

    

    本文研究了具有依赖性（β-混合）数据和平方损失的统计学习，在一个假设类别Φ_p的子集F中，其中Φ_p是范数∥f∥_Φ_p≡sup_m≥1 m^{-1/p}∥f∥_L^m，其中p∈[2，∞]。我们的研究动机是在具有依赖性数据的学习中寻找尖锐的噪声交互项或方差代理。在没有任何可实现性假设的情况下，典型的非渐近结果显示出方差代理通过底层协变量过程的混合时间进行了乘积缩减。我们证明，只要在我们的假设类别F上，L^2和Φ_p的拓扑是可比较的，即Φ_p是一个弱亚高斯类别：∥f∥_Φ_p≲∥f∥_L^2^η，其中η∈(0，1]，经验风险最小化者在其主导项中只实现了一种只依赖于类别复杂性和二阶统计量的速率。我们的结果适用于许多依赖性数据模型。

    In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether t
    
[^9]: 关于大规模语言模型中零阶联邦调整的收敛性

    On the Convergence of Zeroth-Order Federated Tuning in Large Language Models

    [https://arxiv.org/abs/2402.05926](https://arxiv.org/abs/2402.05926)

    我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。

    

    联邦学习（FL）和大规模语言模型（LLM）的融合为隐私保护的自然语言处理带来了新时代。然而，精调LLM所需的强大内存要求在部署到边缘设备时会面临重大挑战，因为这些设备的计算资源有限。为了解决这个问题，我们在联邦环境中探索了内存高效的零阶优化的全新整合，我们称之为FedMeZO。我们的研究是第一个在LLM背景下考察FedMeZO的理论基础的研究，涉及到大参数空间对优化行为的影响、收敛性的建立以及为个性化的联邦策略确定关键参数的问题。我们广泛的实证证据支持了这个理论，表明FedMeZO不仅比传统的一阶方法（如SGD）收敛更快，而且明显...

    The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
    
[^10]: GenEFT: 通过有效理论理解模型泛化的静态和动态

    GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory

    [https://arxiv.org/abs/2402.05916](https://arxiv.org/abs/2402.05916)

    GenEFT是一个有效的理论框架，通过研究泛化相变和表示学习动态，揭示了神经网络泛化的静态和动态特性，这弥合了机器学习理论预测与实践之间的差距。

    

    我们提出了GenEFT：一个有效的理论框架，用于揭示神经网络泛化的静态和动态，以图学习为例进行了说明。首先，我们研究了数据规模增加时的泛化相变，将实验结果与基于信息理论的近似进行比较。我们发现，在解码器既不太弱也不太强的“小熊宝贝区域”中存在着泛化。然后，我们介绍了一种表示学习动态的有效理论，将潜在空间表示建模为相互作用粒子（repons），发现它解释了我们在编码器和解码器学习速率扫描时观察到的泛化和过拟合之间的相变。这突出了受物理启发的有效理论在弥合机器学习中理论预测与实践之间的差距方面的力量。

    We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples. We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations. We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful. We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned. This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning.
    
[^11]: 通过渐进子网络实现高效的分阶段预训练

    Efficient Stagewise Pretraining via Progressive Subnetworks

    [https://arxiv.org/abs/2402.05913](https://arxiv.org/abs/2402.05913)

    通过渐进子网络训练，该方法实现了高效的分阶段预训练，避免了早期阶段无法评估完整模型和模型质量下降等问题。

    

    最近大型语言模型的发展引起了人们对高效预训练方法的关注。最近的一个有效范例是进行分阶段训练，即在训练过程中逐渐增加模型的大小（例如逐渐叠加（Reddi等人，2023年））。虽然资源和墙钟时间的节省很吸引人，但它也有局限性，特别是在早期阶段无法评估完整的模型，并且由于初始阶段模型容量较小而导致模型质量下降。在这项工作中，我们提出了一种替代性框架，即渐进子网络训练，在整个训练过程中保持完整的模型，但每个步骤只训练模型中的子网络。我们专注于这个框架的一个简单实例，即随机路径训练（RaPTr），它在每个步骤中只训练一条子路径，逐渐增加路径长度。RaPTr在BERT和UL2语言模型的预训练损失方面取得了更好的效果，同时只需要2

    Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 2
    
[^12]: 网络聚合马尔可夫博弈中的风险敏感多智能体强化学习

    Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games

    [https://arxiv.org/abs/2402.05906](https://arxiv.org/abs/2402.05906)

    本论文研究了网络聚合马尔可夫博弈中的风险敏感多智能体强化学习，使用了累积前景理论作为风险度量，并提出了一种分布式嵌套CPT-AC算法。这项工作对于理解人类的损失规避和对概率的高估/低估倾向具有重要意义。

    

    传统的多智能体强化学习（MARL）假设智能体对风险中性并具有完全客观性。然而，在智能体需要考虑或建模人类经济或社会偏好的情景中，必须将风险概念纳入强化学习优化问题中。在其他人类或非人类智能体参与，可能具有其自己的风险敏感策略的MARL中，这将更加重要。在这项工作中，我们考虑了具有累积前景理论（CPT）的风险敏感和非合作MARL，CPT是一种非凸风险度量，并且是风险协同度量的扩展。CPT能够解释人类的损失规避和他们对小概率/大概率的高估/低估倾向。我们提出了一种使用CPT风险的分布式基于采样的演员-评论家（AC）算法，用于网络聚合马尔可夫博弈（NAMGs），我们称之为分布式嵌套CPT-AC。在一系列假设下，我们证明了算法收敛到一种主观概念。

    Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion 
    
[^13]: 大规模语言模型在知识蒸馏中遇见图神经网络

    Large Language Model Meets Graph Neural Network in Knowledge Distillation

    [https://arxiv.org/abs/2402.05894](https://arxiv.org/abs/2402.05894)

    本论文提出了一种新颖的图知识蒸馏框架，使用大规模语言模型作为教师模型、图神经网络作为学生模型，解决了在理解文本-属性图中的节点分类问题中的限制。

    

    尽管近期学术界对于大规模语言模型（LLMs）在理解文本-属性图（TAG）方面的进展和潜力有所披露，但LLMs在实际应用中的部署受到了计算和存储需求高，推理过程中延迟长的限制。同时，传统的图神经网络（GNNs）虽然轻量且擅长学习图的结构特征，但对于真实应用中TAG复杂语义的把握有所限制。为了解决这些限制，我们聚焦于TAG中节点分类的下游任务，提出了一种新颖的图知识蒸馏框架，称为语言图知识蒸馏（LinguGKD），使用LLMs作为教师模型，GNNs作为学生模型进行知识蒸馏。其中包括对LLM进行TAG定向指导调整以应对设计的节点分类提示，然后对层次化学习的节点特征进行对齐。

    Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
    
[^14]: EUGENE: 可解释的无监督图编辑距离近似方法

    EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance

    [https://arxiv.org/abs/2402.05885](https://arxiv.org/abs/2402.05885)

    EUGENE是一种可解释的无监督图编辑距离近似方法，可以通过生成编辑路径来近似计算图编辑距离，同时消除了ground-truth生成和数据特定训练的需求。

    

    在生物学、化学、推荐系统和社交网络分析等领域，需要识别与查询图结构距离较小的图形。在多种测量图间距离的方法中，图编辑距离（GED）因其可理解性而被认为是首选，但其计算的NP难度限制了其应用。目前最先进的GED近似方法主要采用神经方法，然而，这些方法（i）缺少与近似的GED对应的解释性编辑路径；（ii）需要通过NP难问题生成ground-truth GED进行训练；（iii）需要在每个数据集上进行独立训练。本文提出了一种高效的代数无监督方法EUGENE，它近似计算GED并生成与近似成本对应的编辑路径，同时消除了生成ground-truth和数据特定训练的需求。广泛的实验评估表明，EUGENE的上述优点并不以效力为代价。

    The need to identify graphs having small structural distance from a query arises in biology, chemistry, recommender systems, and social network analysis. Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation. State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training. Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy. Specifica
    
[^15]: 基于先验依赖分配的结构化赌博机中贝叶斯固定预算最佳臂识别

    Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits

    [https://arxiv.org/abs/2402.05878](https://arxiv.org/abs/2402.05878)

    本论文研究了在结构化赌博机中的贝叶斯固定预算最佳臂识别问题，提出了一种基于先验信息的固定分配算法，并引入了新的证明方法，以得到更紧密的多臂BAI界限。该方法在各种情况下展现出一致且稳健的性能，加深了我们对于该问题的理解。

    

    我们研究了在结构化赌博机中的贝叶斯固定预算最佳臂识别（BAI）问题。我们提出了一种算法，该算法基于先验信息和环境结构使用固定分配。我们在多个模型中提供了它在性能上的理论界限，包括线性和分层BAI的首个先验依赖上界。我们的主要贡献是引入了新的证明方法，相比现有方法，它能得到更紧密的多臂BAI界限。我们广泛比较了我们的方法与其他固定预算BAI方法，在各种设置中展示了其一致且稳健的性能。我们的工作改进了对于结构化赌博机中贝叶斯固定预算BAI的理解，并突出了我们的方法在实际场景中的有效性。

    We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios.
    
[^16]: 联邦离线强化学习：合作单一策略即可覆盖

    Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices

    [https://arxiv.org/abs/2402.05876](https://arxiv.org/abs/2402.05876)

    本研究探索了联邦学习在离线强化学习中的应用，设计了一种适用于联邦离线强化学习的无模型Q-learning算法FedLCB-Q。通过合作利用多个代理的离线数据集，并使用特定的学习速率调度和聚合方法，FedLCB-Q实现了线性加速。

    

    离线强化学习通过使用离线数据来学习最优策略，在无法在线收集数据或成本高昂的关键应用中引起了广泛关注。本研究探讨了联邦学习在离线强化学习中的好处，并旨在合作利用多个代理的离线数据集。针对有限时段的表格化马尔可夫决策过程(MDP)，我们设计了FedLCB-Q，这是一种针对联邦离线强化学习量身定制的流行的无模型Q-learning算法的变种。FedLCB-Q使用新颖的学习速率调度在代理处更新本地Q函数，并使用重要性平均和精心设计的悲观惩罚项在中央服务器上聚合它们。我们的样本复杂度分析表明，在适当选择的参数和同步时间表下，FedLCB-Q在代理数量上实现了线性加速，而不需要个别代理拥有高质量的数据集。

    Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual age
    
[^17]: PromptCrypt: 使用表情符号对大型语言模型进行安全通信的提示加密

    PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models

    [https://arxiv.org/abs/2402.05868](https://arxiv.org/abs/2402.05868)

    PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。

    

    基于云的大型语言模型（LLM）如ChatGPT在日常操作中变得越来越重要，成为各种应用程序中的重要工具。虽然这些模型在可访问性和功能性方面带来了重大好处，但它们也引入了重要的隐私问题：在云基础架构中传输和存储用户数据会产生重大的数据泄露和未经授权访问敏感信息的风险；即使数据的传输和存储被加密，LLM服务提供商仍然知道数据的真实内容，从而阻止个人或实体放心使用此类LLM服务。为了解决这些问题，本文提出了一种简单但有效的机制PromptCrypt来保护用户隐私。它使用表情符号对用户输入进行加密，然后将其发送到LLM，有效地使其对人类或LLM的检查无法理解，同时保留原始提示的意图，从而确保用户隐私。

    Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
    
[^18]: Permute-and-Flip：一种具有最佳鲁棒性和可加水印的LLMs解码器

    Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs

    [https://arxiv.org/abs/2402.05864](https://arxiv.org/abs/2402.05864)

    提出了一种名为Permute-and-Flip（PF）解码器，其具有最佳的鲁棒性和质量-鲁棒性的 tradeoff，且比采样方法更好。还设计了一种针对PF解码器的水印方案，能够保持样本的分布不变，并实现任意低的假阳性率和高的召回率。实验证明PF解码器在困惑度方面明显优于朴素采样，为LLM解码提供了一种有希望的新方法。

    

    在本文中，我们提出了一种名为Permute-and-Flip（PF）解码器的新解码方法。它具有与标准采样解码器相似的鲁棒性特性，但在质量和鲁棒性的 tradeoff 上证明比采样方法更好，且永远不会差于任何其他解码器。同时，我们还设计了一种类似于Aaronson的Gumbel水印的加密水印方案，但是针对PF解码器而自然量身定制。该水印方案不改变样本的分布，同时允许任意低的假阳性率和高的召回率，只要生成的文本具有高熵。我们的实验证明，PF解码器（及其带有水印的对应物）在困惑度方面明显优于朴素采样（及其带有Gumbel水印的对应物），同时保持相同的鲁棒性（和可检测性），因此为LLM解码提供了一个有希望的新方法。代码可在https://github.com/XuandongZhao/pf-decoding找到。

    In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
    
[^19]: 让你的图来说话：为LLMs编码结构化数据

    Let Your Graph Do the Talking: Encoding Structured Data for LLMs

    [https://arxiv.org/abs/2402.05862](https://arxiv.org/abs/2402.05862)

    本论文介绍了一种参数高效的编码方法，可以使大型语言模型（LLMs）能够显式地表示结构化数据，并在图推理任务中取得了显著改进。

    

    我们如何最有效地将结构化数据编码成序列形式，以供大型语言模型（LLMs）使用？在这项工作中，我们介绍了一种参数高效的方法，可以明确表示LLMs的结构化数据。我们的方法，GraphToken，学习了一种编码函数，以显式结构化信息扩展提示语。与其他专注于有限领域（例如知识图表示）的工作不同，我们的工作是首次针对一般结构化数据编码进行研究，用于各种推理任务。我们展示了明确表示图结构可以显著改进图推理任务。具体来说，我们在GraphQA基准测试中看到了整体的改进 - 在节点、边和图级任务上高达73%的改进。

    How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.
    
[^20]: 学习通过专家路由来实现零样本泛化

    Learning to Route Among Specialized Experts for Zero-Shot Generalization

    [https://arxiv.org/abs/2402.05859](https://arxiv.org/abs/2402.05859)

    提出了后续自适应逐标记门控机制（PHATGOOSE），通过学习路由于参数有效微调生成的专家模块之间，从而提高在未见任务上的零样本泛化能力。

    

    近年来，“专家”语言模型的广泛应用通过参数有效的微调，使其专门用于特定的任务或领域。我们如何重用大量的专家语言模型来提高在未见任务上的零样本泛化能力呢？在这项工作中，我们提出后续自适应逐标记门控机制，它通过学习路由于通过参数有效微调生成的专家模块之间。与过去学习在专业模型之间路由的方法不同，后续自适应逐标记门控机制探讨了如果通过对每个令牌和模型中的每个层进行自适应选择不同的专家，零样本泛化是否会得到改善的可能性。关键是，我们的方法是后续的，不需要同时访问用于创建专业模型的数据集，而且在训练每个专家模型后只需要适量的额外计算。

    Recently, there has been a widespread proliferation of "expert" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range 
    
[^21]: 结构信息驱动的蛋白质语言模型

    Structure-Informed Protein Language Model

    [https://arxiv.org/abs/2402.05856](https://arxiv.org/abs/2402.05856)

    该论文介绍了一种结构信息驱动的蛋白质语言模型，通过集成远程同源性检测，将结构信息融入模型，从而提高了蛋白质功能预测的准确性。

    

    蛋白质语言模型通过在广阔的蛋白质序列数据集上进行预训练，可以学习蛋白质表示。然而，传统的蛋白质语言模型缺乏明确的结构监督，尽管与蛋白质功能相关。为了解决这个问题，我们引入了远程同源性检测的集成，将结构信息融入蛋白质语言模型，而无需明确的蛋白质结构作为输入。我们评估了这种结构信息驱动训练对下游蛋白质功能预测任务的影响。实验结果显示，对于EC编号和GO术语预测的功能注释准确性均有一致的改善。然而，在突变数据集上的性能则根据目标属性和蛋白质结构的关系而变化。这强调了在应用结构感知训练于蛋白质功能预测任务时考虑这种关系的重要性。

    Protein language models are a powerful tool for learning protein representations through pre-training on vast protein sequence datasets. However, traditional protein language models lack explicit structural supervision, despite its relevance to protein function. To address this issue, we introduce the integration of remote homology detection to distill structural information into protein language models without requiring explicit protein structures as input. We evaluate the impact of this structure-informed training on downstream protein function prediction tasks. Experimental results reveal consistent improvements in function annotation accuracy for EC number and GO term prediction. Performance on mutant datasets, however, varies based on the relationship between targeted properties and protein structures. This underscores the importance of considering this relationship when applying structure-aware training to protein function prediction tasks. Code and model weights are available at
    
[^22]: 不可见数据取决于已知信息的多少

    How Much is Unseen Depends Chiefly on Information About the Seen

    [https://arxiv.org/abs/2402.05835](https://arxiv.org/abs/2402.05835)

    该论文发现，在未知种群中属于未在训练数据中出现的类的数据点的比例几乎完全取决于训练数据中出现相同次数的类的数量。论文提出了一个遗传算法，能够根据样本找到一个具有最小均方误差的估计量。

    

    乍一看可能有些违反直觉：我们发现，在预期中，未知种群中属于在训练数据中没有出现的类的数据点的比例几乎完全由训练数据中出现相同次数的类的数量$f_k$确定。虽然在理论上我们证明了由该估计量引起的偏差在样本大小指数级衰减，但在实践中，高方差阻止我们直接使用它作为样本覆盖估计量。但是，我们对$f_k$之间的依赖关系进行了精确的描述，从而产生了多个不同期望值表示的搜索空间，可以确定地实例化为估计量。因此，我们转向优化，并开发了一种遗传算法，仅根据样本搜索平均均方误差（MSE）最小的估计量。在我们的实验证明，我们的遗传算法发现了具有明显较小方差的估计量。

    It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times. While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. However, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE). In our experiments, our genetic algorithm discovers estimators that have a substantially smalle
    
[^23]: 稀疏向量量化变压器：一种无前馈网络的框架，用于增强时间序列预测

    Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting

    [https://arxiv.org/abs/2402.05830](https://arxiv.org/abs/2402.05830)

    Sparse-VQ是一种无前馈网络的框架，利用稀疏向量量化技术和反实例归一化来减少噪声影响并捕获足够的统计信息，从而提高时间序列预测的性能。

    

    时间序列分析对于许多应用非常重要，而变压器在这个领域中变得越来越突出。领先的方法从自然语言处理和计算机视觉中定制了变压器架构，利用修补技术将连续信号转换为片段。然而，由于分布变化和内在噪声水平的显著变化，时间序列数据具有独特的挑战。为了解决这两个挑战，我们引入了稀疏向量量化的FFN-Free变压器（Sparse-VQ）。我们的方法利用稀疏向量量化技术和反实例归一化（RevIN）来减少噪声影响，并捕获足够的统计信息用于预测，作为变压器架构中前馈层（FFN）的替代方法。我们的无FFN方法削减了参数数量，提高了计算效率，并减少了过拟合。通过对十个基准数据集进行评估，包括新引入的CAISO数据集，Sparse-VQ取得了

    Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ su
    
[^24]: 发现具有时间意识的强化学习算法

    Discovering Temporally-Aware Reinforcement Learning Algorithms

    [https://arxiv.org/abs/2402.05828](https://arxiv.org/abs/2402.05828)

    这篇论文研究了发现具有时间意识的强化学习算法，用于改进手动设计的算法，使其能够表达出学习的新原则，并适用于各种不同的设置。

    

    最近的元学习进展使得根据代理目标函数自动发现参数化的新型强化学习算法成为可能。为了改进手动设计的算法，必须对这个学习到的目标函数的参数化进行改进，使其能够表达出学习的新原则（而不仅仅是恢复已经建立的原则），同时仍然适用于其元训练分布之外的各种设置。然而，现有方法集中于发现类似于强化学习中广泛使用的目标函数，这些目标函数不考虑训练所允许的总步数或“训练视野”。相反，人类在获取新能力的过程中会使用各种不同的学习目标。例如，学生可能会根据考试截止日期和自我评估的能力来改变他们的学习技巧。本文认为...

    Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or "training horizon". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends tha
    
[^25]: FusionSF：在矢量量化框架中融合异质模态以实现可靠的太阳能发电预测

    FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting

    [https://arxiv.org/abs/2402.05823](https://arxiv.org/abs/2402.05823)

    本文提出了一个多模态融合框架，将历史功率数据、数值天气预报和卫星图像整合在一起，显著提高了太阳能发电预测的性能。研究展示了强大的零样本预测能力，对于新安装的电站尤其有用。

    

    准确的太阳能发电预测对于将光伏电站整合到电网中，调度和确保电网安全至关重要。对于缺乏足够数据的新安装的太阳能电站来说，这个问题变得更加紧迫。当前的研究主要依赖于历史太阳能发电数据或数值天气预报，以单一模态的形式，忽略了不同模态提供的互补信息。在本文中，我们提出了一个多模态融合框架，将历史功率数据、数值天气预报和卫星图像进行整合，显著提高了预测性能。我们引入了一个矢量量化框架，使具有不同信息密度的模态对齐，平衡了整合足够信息和避免模型过拟合之间的关系。我们的框架展现了强大的零样本预测能力，对于新安装的电站尤其有用。此外，我们还收集并发布了训练和测试数据集。

    Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release
    
[^26]: 用二元判别器引导进化进行机器学习程序搜索

    Guided Evolution with Binary Discriminators for ML Program Search

    [https://arxiv.org/abs/2402.05821](https://arxiv.org/abs/2402.05821)

    提出使用二元判别器引导进化进行机器学习程序搜索，通过在训练中区分更好的程序，选择更好的程序并加快进化收敛速度，同时能够编码各种机器学习组件，加速进化过程。

    

    如何自动设计更好的机器学习程序是AutoML中的一个开放问题。尽管进化算法已经成为搜索更好的机器学习程序的常用工具，但是在复杂问题上使用学习本身来引导搜索的方法成功较少且理解较少，但有着显著提高优化过程的速度和最终性能的潜力。我们提出使用一个二元判别器来引导进化，该判别器在线训练，通过区分给定一对程序中哪个更好来选择更好的程序。判别器在不进行昂贵的评估的情况下选择更好的程序，从而加速进化的收敛过程。我们的方法可以编码各种机器学习组件，包括符号优化器、神经网络架构、强化学习损失函数和符号回归方程，它们都使用同样的有向无环图表示。通过将这种表示与现代图神经网络和自适应变异策略相结合，我们展示了我们的方法可以加速进化的过程。

    How to automatically design better machine learning programs is an open problem within AutoML. While evolution has been a popular tool to search for better ML programs, using learning itself to guide the search has been less successful and less understood on harder problems but has the promise to dramatically increase the speed and final performance of the optimization process. We propose guiding evolution with a binary discriminator, trained online to distinguish which program is better given a pair of programs. The discriminator selects better programs without having to perform a costly evaluation and thus speed up the convergence of evolution. Our method can encode a wide variety of ML components including symbolic optimizers, neural architectures, RL loss functions, and symbolic regression equations with the same directed acyclic graph representation. By combining this representation with modern GNNs and an adaptive mutation strategy, we demonstrate our method can speed up evolutio
    
[^27]: 将自我监督的语音模型与视觉语音模型生成的伪词级目标相结合

    Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model

    [https://arxiv.org/abs/2402.05819](https://arxiv.org/abs/2402.05819)

    本文提出了PW-HuBERT框架，它通过将伪词级目标整合到训练过程中，从视觉语音模型中提取目标，从而在口语理解任务中展现出了优越性能。

    

    最近自我监督的语音模型在许多下游任务中取得了显著的改进。然而，这些模型主要集中在帧级训练目标上，在需要语义理解的口语理解任务中可能不足够。现有的工作通常依赖于额外的语音-文本数据作为中间目标，这在实际环境中成本高昂。为了解决这个挑战，我们提出了Pseudo-Word HuBERT（PW-HuBERT）框架，该框架将伪词级目标整合到训练过程中，其中目标是从视觉语音模型中提取的，因此消除了对语音-文本配对数据的需求。我们在四个口语理解基准测试中的实验结果表明了我们模型在捕捉语义信息方面的优越性。

    Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information.
    
[^28]: 使用YOLO v7在磁共振成像中检测肾脏：一种有监督的对比学习方法

    Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.05817](https://arxiv.org/abs/2402.05817)

    本研究使用了最新的YOLO V7目标检测方法，通过对医学图像进行修改和训练，成功提高了肾脏在磁共振成像中的检测效果，为肾脏疾病的诊断和治疗提供了有力支持。

    

    本研究探索了最新的You Only Look Once (YOLO V7)目标检测方法在医学图像中增强肾脏检测的应用，通过对医学图像格式进行修改，对修改后的YOLO V7进行训练和测试。研究包括878名不同亚型的肾细胞癌（RCC）患者和206名正常肾脏患者。共检索到1084名患者的5657张MRI扫描。从回顾性数据库中选择了326名患有1034个肿瘤的患者，并在其肿瘤周围绘制了边界框。在初始模型上对80%的注释案例进行训练，保留20%用于测试（主要测试集）。然后使用最佳的主要模型在其余861名患者上识别肿瘤，并使用该模型在其扫描中生成边界框坐标。创建了十个基准训练集，其中包含未分割患者上的生成坐标。最终模型用于预测主要测试集中的肾脏。

    Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten benchmark training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predi
    
[^29]: 通过反向课程强化学习训练大型语言模型进行推理

    Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

    [https://arxiv.org/abs/2402.05808](https://arxiv.org/abs/2402.05808)

    本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。

    

    在本文中，我们提出了R$^3$：通过反向课程强化学习（RL）进行推理的学习方法，该方法只使用结果监督来实现大型语言模型的过程监督的好处。将RL应用于复杂推理的核心挑战是确定一系列行动，以获得正向奖励并提供适当的优化监督。结果监督为最终结果提供了稀疏奖励，而不识别错误位置，而过程监督提供了逐步奖励，但需要大量手动注释。R$^3$通过学习正确演示来克服这些限制。具体而言，R$^3$将推理的起始状态从演示的结束滑动到开始，从而在所有阶段都促进了更容易的模型探索。因此，R$^3$建立了一个逐步的课程，使结果监督能够提供阶段级信号并精确定位错误。

    In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
    
[^30]: 关于深度分类器的校准和符合预测研究

    On Calibration and Conformal Prediction of Deep Classifiers

    [https://arxiv.org/abs/2402.05806](https://arxiv.org/abs/2402.05806)

    本文研究了温度缩放对符合预测方法的影响，通过实证研究发现，校准对自适应C方法产生了有害的影响。

    

    在许多分类应用中，深度神经网络（DNN）基于分类器的预测需要伴随一些置信度指示。针对这个目标，有两种流行的后处理方法：1）校准：修改分类器的softmax值，使其最大值（与预测相关）更好地估计正确概率；和2）符合预测（CP）：设计一个基于softmax值的分数，从中产生一组预测，具有理论上保证正确类别边际覆盖的特性。尽管在实践中两种指示都可能是需要的，但到目前为止它们之间的相互作用尚未得到研究。为了填补这一空白，在本文中，我们研究了温度缩放，这是最常见的校准技术，对重要的CP方法的影响。我们首先进行了一项广泛的实证研究，其中显示了一些重要的洞察，其中包括令人惊讶的发现，即校准对流行的自适应C方法产生了有害的影响。

    In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive C
    
[^31]: 无监督发现临床疾病特征的概率独立性方法

    Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence

    [https://arxiv.org/abs/2402.05802](https://arxiv.org/abs/2402.05802)

    通过无监督机器学习和概率独立性方法，我们发现了2000个潜在源的临床特征，这些特征在肺癌预测中的判别能力优于原始变量，并且能够识别未被诊断的癌症的特征。

    

    临床疾病的诊断不够准确可能导致很多治疗失败的情况，即使是常见疾病和治疗。通过使用足够大的数据集，可以使用无监督机器学习来更精确地定义临床疾病模式。我们提出了一种利用概率独立性来解开疾病潜在源因对医疗记录的影响的方法。我们从269,099份电子健康记录中的9195个变量中推断出了2000个潜在源的临床特征。这些学习到的特征在一个肺癌预测任务中比原始变量有更好的判别能力，该任务对推断算法来说是未知的，能够预测出无癌症历史的患者在发现孤立的肺结节之前的3年内的恶性疾病。更重要的是，这些特征的解释能力更强，能够识别出许多患者中明显未被诊断的癌症的结节前特征。

    Insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments. With a large enough dataset, it may be possible to use unsupervised machine learning to define clinical disease patterns more precisely. We present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease. We inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 Electronic Health Records. The learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered. More importantly, the signatures' greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients.
    
[^32]: Transformers在上下文自回归学习中的表现如何？

    How do Transformers perform In-Context Autoregressive Learning?

    [https://arxiv.org/abs/2402.05787](https://arxiv.org/abs/2402.05787)

    本文研究了Transformers在上下文自回归学习中的表现，并通过训练模型发现了其预测下一个标记的过程。针对不同情况，我们证明了单层线性Transformer实现了梯度下降以及正交性之间的关系。

    

    Transformers在语言建模任务中取得了最先进的性能。然而，它们取得巨大成功的原因还不清楚。本文通过在简单的下一个标记预测任务上训练Transformer模型，为了更好地理解这一问题。我们展示了训练后的Transformer如何通过首先在上下文中学习W，然后应用预测映射来预测下一个标记。我们称这个结果为上下文自回归学习。具体来说，我们针对W是交换正交矩阵的情况，首先证明了一个训练后的单层线性Transformer在考虑扩展标记的情况下实现一步梯度下降来最小化内部目标函数。当标记没有扩展时，我们对于一个单层对角线线性多头Transformer的全局最小值进行了表征。重要的是，我们展示了头部之间的正交性。

    Transformers have achieved state-of-the-art performance in language modeling tasks. However, the reasons behind their tremendous success are still unclear. In this paper, towards a better understanding, we train a Transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained Transformer predicts the next token by first learning $W$ in-context, then applying a prediction mapping. We call the resulting procedure in-context autoregressive learning. More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer. Importantly, we exhibit orthogonality between heads and show that posi
    
[^33]: Transformer语言模型在算法学习上的限制

    Limits of Transformer Language Models on Algorithmic Learning

    [https://arxiv.org/abs/2402.05785](https://arxiv.org/abs/2402.05785)

    Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。

    

    我们分析了Transformer语言模型在学习离散算法方面的能力。为此，我们引入了两个要求组合多个离散子任务的新任务。我们通过从头开始训练LLaMA模型和在GPT-4和Gemini上提示来衡量学习学习原语的组合。我们观察到，目前最先进的Transformer语言模型的组合能力非常有限，并且在样本规模方面比为新的算法组合重新学习所有子任务效果更差。我们还提出了一个复杂性理论的定理，证明了记忆前馈模型上的梯度下降可以指数级地浪费数据。

    We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
    
[^34]: 分析对手塑造的样本复杂性

    Analysing the Sample Complexity of Opponent Shaping

    [https://arxiv.org/abs/2402.05782](https://arxiv.org/abs/2402.05782)

    本文研究了对手塑造的样本复杂性，并提出了一种适合理论分析的表格化版本R-FOS。

    

    在一般和博弈中，学习通常会导致集体性的次优结果。对此进行改进，对手塑造（OS）方法积极引导其他智能体的学习过程，经验性地提高了个体和群体在许多情景下的表现。早期的OS方法使用高阶导数来塑造合作玩家的学习，这使得它们不适用于塑造多个学习步骤。后续的工作，即无模型对手塑造（M-FOS），通过将OS问题重新定义为元博弈来解决这些问题。与早期的OS方法相比，对于M-FOS框架的理论理解还很少。为M-FOS提供理论保证是困难的，因为A）元强化学习的理论样本复杂性界限的文献很少 B）M-FOS在连续状态和动作空间中运作，所以理论分析具有挑战性。在这项工作中，我们提出了R-FOS，这是M-FOS的表格化版本，更适合进行理论分析。R-FOS离散化了

    Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises
    
[^35]: 稳定的自主流匹配

    Stable Autonomous Flow Matching

    [https://arxiv.org/abs/2402.05774](https://arxiv.org/abs/2402.05774)

    本文通过应用随机稳定性工具于时间独立系统的流匹配模型，研究了物理稳定数据点的深度生成模型，并与控制理论原理进行了联系。

    

    在表示物理稳定状态的数据样本中，通常假设数据点代表能量景观的局部极小值。在控制论中，众所周知，能量可以作为有效的李亚普诺夫函数。尽管如此，控制论与生成模型之间的联系在文献中很少，尽管有几个具有物理稳定数据点的机器学习应用。在本文中，我们关注这样的数据和一种最近的深度生成模型类别，称为流匹配。我们应用随机稳定性工具于时间独立系统的流匹配模型。通过这样做，我们表征了适应这种处理的流匹配模型的空间，以及与其他控制理论原理的联系。我们在两个示例上展示了我们的理论结果。

    In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.
    
[^36]: 离线策略分布式 Q($\lambda$): 不需重要性采样的分布式强化学习

    Off-policy Distributional Q($\lambda$): Distributional RL without Importance Sampling

    [https://arxiv.org/abs/2402.05766](https://arxiv.org/abs/2402.05766)

    离线策略分布式 Q($\lambda$) 是一种不使用重要性采样的强化学习算法，具有与有符号测度的有趣交互作用，并在深度强化学习基准测试中展现了有希望的结果。

    

    我们引入了离线策略分布式 Q($\lambda$)，它是离线策略分布式评估算法族的新成员。离线策略分布式 Q($\lambda$) 在离线学习时不应用重要性采样，这引入了与有符号测度之间的有趣交互作用。这些独特的特性使得离线策略分布式 Q($\lambda$) 与其他现有的替代方法（如分布式 Retrace）有所区别。我们对离线策略分布式 Q($\lambda$) 的算法特性进行了表征，并通过表格实验验证了理论洞察力。我们展示了离线策略分布式 Q($\lambda$)-C51在深度强化学习基准测试上的有希望的结果。

    We introduce off-policy distributional Q($\lambda$), a new addition to the family of off-policy distributional evaluation algorithms. Off-policy distributional Q($\lambda$) does not apply importance sampling for off-policy learning, which introduces intriguing interactions with signed measures. Such unique properties distributional Q($\lambda$) from other existing alternatives such as distributional Retrace. We characterize the algorithmic properties of distributional Q($\lambda$) and validate theoretical insights with tabular experiments. We show how distributional Q($\lambda$)-C51, a combination of Q($\lambda$) with the C51 agent, exhibits promising results on deep RL benchmarks.
    
[^37]: 高维点过程的带结构缺失的潜变量模型

    Latent variable model for high-dimensional point process with structured missingness

    [https://arxiv.org/abs/2402.05758](https://arxiv.org/abs/2402.05758)

    本文提出了一种针对高维点过程的带有结构缺失的灵活高效的潜变量模型，利用高斯过程捕获时间相关性，并开发了可扩展的变分推理方法进行训练。

    

    纵向数据在医疗保健、社会学和地震学等许多领域中具有重要意义，但是真实世界的数据集对从业人员来说存在明显的挑战，因为它们可能是高维的，包含有结构化的缺失模式，并且测量时间点可能受到未知随机过程的控制。尽管已经提出了各种解决方案，但其中大多数仅考虑了这些挑战中的一个。在这项工作中，我们提出了一种灵活高效的潜变量模型，能够应对所有这些限制。我们的方法利用高斯过程来捕获样本与其关联的缺失模式之间的时间相关性，同时也用于建模底层的点过程。我们将我们的模型构建为一个变分自动编码器，同时使用深度神经网络参数化的编码器和解码器模型，并开发了一个可扩展的变分推理方法来进行高效的模型训练。我们展示了这个模型在各个领域的竞争性能。

    Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. We demonstrate compe
    
[^38]: 广义偏好优化：离线对齐的统一方法

    Generalized Preference Optimization: A Unified Approach to Offline Alignment

    [https://arxiv.org/abs/2402.05749](https://arxiv.org/abs/2402.05749)

    广义偏好优化（GPO）是一种离线损失函数，通过参数化一类凸函数来实现统一的偏好优化视角，并提供了新的算法工具和实证洞见。

    

    离线偏好优化允许直接从离线数据中对大型模型进行微调，并在最近的对齐实践中证明了其有效性。我们提出了广义偏好优化（GPO），这是一类通过一般的凸函数参数化的离线损失函数。GPO提供了对偏好优化的统一视角，涵盖了现有算法（DPO、IPO和SLiC）作为特殊情况，同时自然引入了新的变体。GPO框架还揭示了离线算法如何通过定义损失的凸函数来实施正则化。我们的分析和实验揭示了离线正则化和规范的RLHF公式所意图的KL散度正则化之间的联系和微妙差异。总的来说，我们的结果为对齐实践者提供了新的算法工具和实证洞见。

    Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.
    
[^39]: 基于基础模型的真实世界机器人应用：一项综述

    Real-World Robot Applications of Foundation Models: A Review

    [https://arxiv.org/abs/2402.05741](https://arxiv.org/abs/2402.05741)

    本文综述了基础模型在真实世界机器人中的应用，重点是替换现有机器人系统中的特定组件。这些基础模型在输入输出关系、感知、运动规划和控制等方面扮演了重要角色。未来的挑战和对实际机器人应用的影响也被讨论到。

    

    最近，基于大规模语言模型（LLMs）和视觉语言模型（VLMs）等基础模型的发展，通过对大量数据的训练，为不同任务和模态的灵活应用提供了便利。它们的影响涵盖了包括医疗、教育和机器人等各个领域。本文概述了基础模型在真实世界机器人中的实际应用情况，重点是替换现有机器人系统中的特定组件。总结涵盖了基础模型中的输入输出关系以及它们在机器人领域中的感知、运动规划和控制等方面的作用。本文还讨论了未来挑战和对实际机器人应用的影响。

    Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.
    
[^40]: 隐性偏差与自注意力的快速收敛速率

    Implicit Bias and Fast Convergence Rates for Self-attention

    [https://arxiv.org/abs/2402.05738](https://arxiv.org/abs/2402.05738)

    该论文研究了在自注意力网络中使用梯度下降训练的隐性偏差以及其收敛速率，通过证明在特定数据设置下收敛性是全局的，并提供了W_t到W_mm的有限时间收敛率。

    

    自注意力是transformer的核心机制，它使其与传统神经网络有所区别，并驱动其出色的性能。为了开发自注意力的基本优化原则，我们研究了用梯度下降（GD）训练具有固定线性解码器的自注意力层在二元分类中的隐性偏差。受到在可分离数据上线性逻辑回归中GD的研究启发，最近的工作表明，随着迭代次数t无限接近于无穷大，键-查询矩阵W_t在局部上（相对于初始化方向）收敛到一个硬边界支持向量机解W_mm。我们的工作在四个方面增强了这个结果。首先，我们确定了非平凡的数据设置，对于这些设置，收敛性是全局的，并揭示了优化空间的特性。其次，我们首次提供了W_t到W_mm的有限时间收敛率，并量化了稀疏化的速率。

    Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in t
    
[^41]: 基于模型的强化学习在平均场博弈中并不比单个智能体强化学习更加困难

    Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL

    [https://arxiv.org/abs/2402.05724](https://arxiv.org/abs/2402.05724)

    本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。

    

    我们研究了在平均场博弈中基于模型的函数逼近下强化学习样本复杂度，该方法需要策略性探索以找到纳什均衡策略。我们引入了部分基于模型的Eluder维度（P-MBED），这是一种更有效的概念来描述模型类复杂度。值得注意的是，P-MBED可以衡量从给定的平均场模型类转换而来的单个智能体模型类的复杂度，并且潜在上可能比\citet{huang2023statistical}提出的MBED指数级低。我们提出了一种模型消除算法，具有新颖的探索策略，并建立了与P-MBED相关的样本复杂度结果，这些结果表明，在基本可实现性和Lipschitz连续性假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。我们进一步将我们的结果推广到多类型平均场博弈。

    We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
    
[^42]: 在上下文学习中可以重新学习禁止的任务

    In-Context Learning Can Re-learn Forbidden Tasks

    [https://arxiv.org/abs/2402.05723](https://arxiv.org/abs/2402.05723)

    本研究通过研究禁止任务，即模型设计为拒绝回答的任务，探究了在明确调优模型拒绝禁止任务的情况下，上下文学习（ICL）是否可以用于重新学习禁止任务。研究发现，ICL可以成功地撤销安全培训，从而造成重大的安全风险。

    

    尽管对安全培训进行了大量投入，但在现实世界中部署的大型语言模型（LLMs）仍然存在许多漏洞。有一种观点认为，LLM安全培训可以通过算法禁止模型回答有毒或有害的查询。为了评估安全培训的有效性，本研究研究了禁止任务，即模型设计为拒绝回答的任务。具体而言，我们研究了在明确调优模型拒绝禁止任务的情况下，上下文学习（ICL）是否可以用于重新学习禁止任务。我们首先通过一个拒绝情感分类的玩具示例来演示问题。然后，我们使用ICL来处理一个被细化拒绝总结虚构新闻文章的模型。最后，我们调查ICL是否可以撤销安全培训，这可能代表着重大的安全风险。对于安全任务，我们查看了Vicuna-7B，Starling-7B和Llama2-7B。我们证明了该攻击可以直接对Starlin进行。

    Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starlin
    
[^43]: 宽隐藏层树状神经网络中具有通用激活函数的容量准确性

    Exact capacity of the \emph{wide} hidden layer treelike neural networks with generic activations

    [https://arxiv.org/abs/2402.05719](https://arxiv.org/abs/2402.05719)

    该论文研究了宽隐藏层树状神经网络的容量，采用全面提升的随机二重性理论(fl RDT)来对宽(TCM)网络的容量进行刻画，得到了一类通用激活函数的显式、闭式容量计算公式。

    

    最近对于树状委员会机器(TCM)神经网络的研究表明，随机二重性理论(RDT)及其部分提升变种(pl RDT)是能够用于非常精确的网络容量分析的强大工具。在本文中，我们考虑了宽隐藏层网络，并发现\cite{Stojnictcmspnncapdiffactrdt23}中面临的某些数值困难奇迹般地消失了。特别是，我们采用最近发展的全面提升(fl) RDT来表征宽($d\rightarrow \infty$) TCM网络的容量。我们获得了一类非常通用的隐藏层激活函数的显式、闭式容量刻画。尽管所使用的方法显著降低了所需的数值评估量，但最终的fl RDT的实用性和成功仍然需要可靠的数值计算。

    Recent progress in studying \emph{treelike committee machines} (TCM) neural networks (NN) in \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \emph{fully lifted} (fl) RDT to characterize the \emph{wide} ($d\rightarrow \infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical 
    
[^44]: REMEDI: 改进神经熵估计的校正转换

    REMEDI: Corrective Transformations for Improved Neural Entropy Estimation

    [https://arxiv.org/abs/2402.05718](https://arxiv.org/abs/2402.05718)

    REMEDI是一种用于改进神经熵估计的校正转换方法，通过交叉熵最小化和相对熵估计基模型的偏差，提高了估计任务的准确性和效率。

    

    信息论量在机器学习中起着核心作用。数据和模型复杂性的增加使得准确估计这些量的需求增加。然而，随着维度的增加，估计存在重大挑战，现有方法在相对较低的维度中已经困难重重。为了解决这个问题，在这项工作中，我们引入了REMEDI，用于高效准确地估计微分熵，一种基本的信息论量。该方法结合了简单自适应基模型的交叉熵最小化和其相对熵从数据密度中估计的偏差。我们的方法在各种估计任务中得到了改进，包括对合成数据和自然数据的熵估计。此外，我们将重要的理论一致性结果扩展到我们方法所需的更广义的设置中。我们展示了我们的方法如何提高熵估计的准确性和效率。

    Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how
    
[^45]: 协同非参数的两样本检验

    Collaborative non-parametric two-sample testing

    [https://arxiv.org/abs/2402.05715](https://arxiv.org/abs/2402.05715)

    本文提出了协同非参数的两样本检验（CTST）框架，该框架有效利用了图结构和最小化了对概率密度函数的假设，通过集成f-分布估计、核方法和多任务学习的要点，将传统的在每个节点独立应用的方法优化，对图结构中的多个两样本检验问题进行了更好的处理和分析。

    

    本文针对图结构设置中的多个两样本检验问题进行了研究，这是空间统计和神经科学等领域中的常见情景。在固定图中的每个节点v都涉及到两个特定节点概率密度函数（pdfs）p_v和q_v之间的两样本检验问题。我们的目标是在假设连接的节点会产生类似的检验结果的条件下，确定应拒绝零假设p_v = q_v的节点。我们提出了非参数的协同两样本检验（CTST）框架，该框架有效利用了图结构并最小化了对p_v和q_v的假设。我们的方法集成了f-分布估计、核方法和多任务学习的要点。我们使用合成实验和检测地震活动的真实传感器网络来证明CTST优于最先进的在每个节点独立应用的非参数统计检验方法，因此忽视了图结构的重要性。

    This paper addresses the multiple two-sample test problem in a graph-structured setting, which is a common scenario in fields such as Spatial Statistics and Neuroscience. Each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes. We propose the non-parametric collaborative two-sample testing (CTST) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our methodology integrates elements from f-divergence estimation, Kernel Methods, and Multitask Learning. We use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that CTST outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the g
    
[^46]: 明明就在眼前：对弱势患者群体进行不可检测的对抗性偏见攻击

    Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations

    [https://arxiv.org/abs/2402.05713](https://arxiv.org/abs/2402.05713)

    该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。

    

    人工智能在放射学中的广泛应用揭示了深度学习模型加剧对弱势患者群体的临床偏见的风险。虽然先前的文献主要关注训练的深度学习模型所展示的偏见的量化，但针对特定人口群体的对抗性偏见攻击以及其在临床环境中的影响仍然是一个未被充分研究的医学影像领域。在这项工作中，我们证明了针对人口统计学标签的毒化攻击可以向深度学习模型引入对抗性的诊断不足偏见，并在不影响整体模型性能的情况下降低对被低估群体的性能。此外，我们的结果在多个性能指标和人口群体（如性别、年龄以及其交叉子群）上表明，群体对于不可检测的对抗性偏见攻击的脆弱性与其在模型的训练数据中的表征直接相关。

    The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
    
[^47]: 离线风险敏感强化学习结合部分可观察性，以提高人-机组合的表现

    Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming

    [https://arxiv.org/abs/2402.05703](https://arxiv.org/abs/2402.05703)

    本研究针对人-机组合中的性能提出了离线风险敏感强化学习算法，通过部分可观察性的建模，解决了多样化人类参与者的挑战。

    

    将生理计算整合到混合倡议的人机交互系统中，通过将实时特征作为人类状态观测融入决策系统，可以在自主任务分配中提供有价值的优势。这种方法可以通过智能地分配任务来减轻人类操作员的认知负荷。然而，适应具有不同生理和行为测量结果的多样化人类参与者构成了一个重大挑战。为了解决这个问题，必须利用概率框架，考虑到人类状态的固有不确定性和部分可观察性。最近的研究建议使用离线强化学习（ORL）方法从先前收集的经验数据集中学习一个部分可观测马尔可夫决策过程（POMDP）模型。在本研究中，我们不仅强调部分可观察性表示的潜力，

    The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human's state. Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods. In the present work, we not only highlight the potential of partially observable representations an
    
[^48]: 固定宽度的树状神经网络容量分析-通用激活函数

    Fixed width treelike neural networks capacity analysis -- generic activations

    [https://arxiv.org/abs/2402.05696](https://arxiv.org/abs/2402.05696)

    这篇论文研究了树状神经网络的容量，基于Random Duality Theory提出了通用的容量分析框架，并证明该框架适用于其他类型的激活函数，如二次和ReLU函数。

    

    我们考虑了树状委员会机（TCM）神经网络的容量。基于随机对偶理论（RDT），\cite{Stojnictcmspnncaprdt23}最近提出了一个通用的框架用于它们的容量分析。然后，在\cite{Stojnictcmspnncapliftedrdt23}中提出了一种基于所谓的“部分提升”RDT（pl RDT）的升级版本。这两个工作方向都着重于具有最典型的“符号”激活函数的网络。然而，在这里，我们专注于具有其他更一般类型激活函数的网络，并且证明了\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23}的框架足够强大，可以处理这些情况。除了标准的“线性”激活函数外，我们发现两个广泛使用的激活函数，即“二次”和“修正线性单元（ReLU）”，可以得到特别方便的结果。更具体地说，对于每个激活函数，我们获得了

    We consider the capacity of \emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \emph{partially lifted} RDT (pl RDT) was then presented in \cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \emph{quadratic} and \emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtai
    
[^49]: Unichain和非周期性足以保证平均回报好转胆冒险目标的渐进最优性

    Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits

    [https://arxiv.org/abs/2402.05689](https://arxiv.org/abs/2402.05689)

    该论文提出了一种新的策略类别，用于解决无限期平均回报好转胆冒险问题。研究表明，在单臂松弛问题是Unichain和非周期性的情况下，该策略类别具有渐进最优性。

    

    我们考虑了离散时间下的无限期平均回报的好转胆冒险问题。我们提出了一种新的策略类别，旨在将逐渐扩大的臂子集向最佳分布方向推进。我们证明了我们的策略在N臂问题中是渐进最优的，如果单臂松弛问题是Unichain和非周期性的，那么就会有一个$O(1/\sqrt{N})$的最优间隙。我们的方法不同于大多数现有的研究，这些研究侧重于指数或优先级策略，这些策略依赖于统一全局吸引子属性（UGAP）来保证收敛到最优，或者依赖于最近开发的基于模拟的策略，该策略要求遵循同步假设（SA）。

    We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).
    
[^50]: 通过离散化和特征选择的可解释性表格数据分类器

    Interpretable classifiers for tabular data via discretization and feature selection

    [https://arxiv.org/abs/2402.05680](https://arxiv.org/abs/2402.05680)

    通过离散化和特征选择的方法，我们提出了一种从表格数据中计算出准确又易解释的分类器的方法。在实验证明该方法在准确度上与随机森林和XGBoost等现有方法相当，并且在多种情况下实际上超过了参考结果。

    

    我们引入了一种从表格数据中计算出具有解释性且准确的分类器的方法。所得到的分类器是简短的DNF公式，通过将原始数据离散化为布尔形式，然后使用特征选择结合非常快速的算法来产生最佳的布尔分类器。我们通过14个实验来演示该方法，得到的结果的准确度主要与随机森林、XGBoost以及文献中相同数据集的现有结果相似。在多种情况下，我们的方法实际上在准确度方面优于参考结果，尽管我们研究的主要目标是我们的分类器的即时可解释性。我们还证明了一个关于从现实数据中获得的分类器与来自数据背景分布的最佳分类器相对应的概率的新结果。

    We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
    
[^51]: 压缩数据集的对抗训练是否有效？

    Is Adversarial Training with Compressed Datasets Effective?

    [https://arxiv.org/abs/2402.05675](https://arxiv.org/abs/2402.05675)

    本论文研究了在压缩数据集上训练的模型对对抗鲁棒性的影响，并提出了一种同时提高数据集压缩效率和对抗鲁棒性的方法。

    

    数据集压缩（DC）是指从较大数据集中生成较小的合成数据集的一类最近的数据集压缩方法。这个合成数据集保留了原始数据集的基本信息，使得在其上训练的模型能够达到与在完整数据集上训练的模型相当的性能水平。目前大多数的DC方法主要关注如何在有限的数据预算下实现高测试性能，并没有直接解决对抗鲁棒性的问题。在本工作中，我们研究了在压缩数据集上训练的模型对对抗鲁棒性的影响。我们发现从DC方法获得的压缩数据集对模型的对抗鲁棒性没有有效的传递性。为了同时提高数据集压缩效率和对抗鲁棒性，我们提出了一种基于寻找数据集的最小有限覆盖（MFC）的新型鲁棒性感知数据集压缩方法。

    Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The prop
    
[^52]: 高维模型的对抗训练：几何和权衡

    A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs

    [https://arxiv.org/abs/2402.05674](https://arxiv.org/abs/2402.05674)

    本论文研究了高维模型中的对抗训练，引入了一个可处理的数学模型，并给出了对抗性经验风险最小化器的充分统计的精确渐近描述。研究结果表明存在可以防御而不惩罚准确性的方向，揭示了防御非鲁棒特征的优势。

    

    本研究在高维情况下，即维度$d$和数据点数$n$与固定比例$\alpha = n / d$发散的上下文中，研究了基于边际的线性分类器中的对抗训练。我们引入了一个可处理的数学模型，可以研究数据和对抗攻击者几何之间的相互作用，同时捕捉到对抗鲁棒性文献中观察到的核心现象。我们的主要理论贡献是在通用的凸且非递增损失函数下，对于对抗性经验风险最小化器的充分统计的精确渐近描述。我们的结果使我们能够精确地刻画数据中与更高的泛化/鲁棒性权衡相关的方向，由一个鲁棒性度量和一个有用性度量定义。特别地，我们揭示了存在一些方向，可以进行防御而不惩罚准确性。最后，我们展示了防御非鲁棒特征的优势。

    This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu
    
[^53]: 对LLMs的越狱攻击的综合评估

    Comprehensive Assessment of Jailbreak Attacks Against LLMs

    [https://arxiv.org/abs/2402.05668](https://arxiv.org/abs/2402.05668)

    对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。

    

    对大型语言模型（LLMs）的滥用引起了广泛关注。为了解决这个问题，已经采取了安全措施以确保LLMs符合社会伦理。然而，最近的研究发现了一种绕过LLMs安全措施的不稳定漏洞，被称为越狱攻击。通过应用技术，如角色扮演场景、对抗性样本或对安全目标的微妙破坏作为提示，LLMs可以产生不适当甚至有害的回应。虽然研究人员已经研究了几种越狱攻击的类别，但他们都是孤立地进行的。为了填补这个空白，我们提出了对各种越狱攻击方法的首次大规模测量。我们集中在来自四个类别的13种尖端越狱方法、16种违规类别的160个问题以及六种流行的LLMs上。我们广泛的实验结果表明，优化的越狱提示始终能够达到最高的攻击成功率，并表现出...

    Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
    
[^54]: S$\Omega$I: 基于分数的O-INFORMATION估计

    S$\Omega$I: Score-based O-INFORMATION Estimation

    [https://arxiv.org/abs/2402.05667](https://arxiv.org/abs/2402.05667)

    S$\Omega$I是一种新的信息论量度，可以计算多变量系统中的协同-冗余平衡，突破了传统量度的局限性，并在合成数据上得到了验证。

    

    科学数据和复杂的多变量系统的分析需要捕捉多个随机变量之间关系的信息量。最近，新的信息论量度已被发展出来，以克服传统量度（如互信息）的局限性，后者只考虑成对交互作用。其中，信息协同和冗余的概念对于理解变量之间的高阶依赖关系至关重要。基于这一概念的最著名和多用途的量度之一是O-information，它提供了一种清晰且可扩展的方式来量化多变量系统中的协同-冗余平衡。然而，它在实际应用中受限于简化情况。在本研究中，我们引入了S$\Omega$I，该方法首次允许计算O-information而不受对系统的限制性假设。我们的实验证实了我们的方法在合成数据上的有效性，证明了S$的有效性。

    The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$
    
[^55]: 实时瓶颈和激波预测的中尺度交通预测

    Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction

    [https://arxiv.org/abs/2402.05663](https://arxiv.org/abs/2402.05663)

    该论文介绍了一种在实时中尺度交通预测中具有最先进效果的深度预测方法SA-LSTM，通过将自注意力与长短期记忆结合，实现了对多步预测的改进，并在短期和长期预测之间取得了平衡。

    

    准确的实时交通状态预测在交通控制研究中起着关键作用。特别是CIRCLES联合项目需要预测技术来减轻数据源延迟的影响。在MegaVanderTest实验取得成功之后，本文旨在克服当前系统限制，开发更适合的方法来改善下一轮实验的实时交通状态估计。在本文中，我们介绍了SA-LSTM，这是一种深度预测方法，将自注意力（SA）与长短期记忆（LSTM）在空间维度上结合，可以在实时中尺度交通预测中获得最先进的结果。我们将这种方法扩展到多步预测，使用n-step SA-LSTM，在短期和长期预测之间的平衡中优于传统的多步预测方法，同时实时运行。

    Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.
    
[^56]: 重新思考无监督图领域适应中的传播问题

    Rethinking Propagation for Unsupervised Graph Domain Adaptation

    [https://arxiv.org/abs/2402.05660](https://arxiv.org/abs/2402.05660)

    通过重新评估GNN在图领域适应中的作用，本论文揭示了传播过程对于适应不同图领域至关重要，并通过理论分析提供了多层GNN的泛化界限的证明。

    

    无监督图领域适应（UGDA）旨在将标记源图的知识传输到未标记的目标图中，以解决图领域之间的分布偏移问题。以往的研究主要集中在通过图神经网络（GNN）学习的表示空间中对源图和目标图的数据进行对齐。然而，GNN的内在泛化能力很大程度上被忽视了。在经验分析的基础上，我们重新评估了GNN在图领域适应中的作用，揭示了传播过程在GNN中适应不同图领域中的关键作用。我们对UGDA进行了全面的理论分析，并推导出多层GNN的泛化界限。通过将GNN Lipschitz应用于k层GNNs，我们证明通过在源图中删除传播层并在目标图中堆叠多个传播层，可以使目标风险界限更紧密。基于实证和理论分析结果

    Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis
    
[^57]: 深度学习在软件错误预测中的可再现性研究

    Investigating Reproducibility in Deep Learning-Based Software Fault Prediction

    [https://arxiv.org/abs/2402.05645](https://arxiv.org/abs/2402.05645)

    本研究通过系统回顾当前文献，分析了深度学习在软件错误预测领域的可再现性问题，并发现了一些令人担忧的情况。

    

    过去几年，深度学习方法已经应用于软件工程领域的各种任务，特别是自动预测和定位软件错误的重要任务。然而，随着越来越复杂的机器学习模型的快速采用，学者们越来越难以复现文献中报道的结果。特别是在深度学习模型和评估方法未正确记录、代码和数据未分享的情况下更加困难。鉴于其他应用机器学习领域可再现性的最近且非常令人担忧的发现，本研究的目标是分析软件工程领域，特别是软件错误预测领域是否存在类似问题的程度。因此，我们对当前文献进行了系统回顾，并检查了可再现性的水平。

    Over the past few years, deep learning methods have been applied for a wide range of Software Engineering (SE) tasks, including in particular for the important task of automatically predicting and localizing faults in software. With the rapid adoption of increasingly complex machine learning models, it however becomes more and more difficult for scholars to reproduce the results that are reported in the literature. This is in particular the case when the applied deep learning models and the evaluation methodology are not properly documented and when code and data are not shared. Given some recent -- and very worrying -- findings regarding reproducibility and progress in other areas of applied machine learning, the goal of this work is to analyze to what extent the field of software engineering, in particular in the area of software fault prediction, is plagued by similar problems. We have therefore conducted a systematic review of the current literature and examined the level of reprod
    
[^58]: 通过并行观测预测改进基于令牌的世界模型

    Improving Token-Based World Models with Parallel Observation Prediction

    [https://arxiv.org/abs/2402.05643](https://arxiv.org/abs/2402.05643)

    该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。

    

    受到将Transformer应用于离散符号序列的成功启发，最近提出了基于令牌的世界模型（TBWMs）作为高效样本方法。在TBWMs中，世界模型将代理经验作为一种类似语言的令牌序列进行消耗，其中每个观测构成一个子序列。然而，在想象过程中，通过令牌逐个生成下一个观测的串行方式导致了严重的瓶颈问题，导致训练时间长、GPU利用率低和表示能力有限。为了解决这个瓶颈问题，我们设计了一种新颖的并行观测预测（POP）机制。POP通过一种针对我们的强化学习环境设计的新型前向模式来扩充了保持网络（RetNet）。我们将POP集成到一种名为REM（保持环境模型）的新型TBWM代理中，展示了比以前的TBWMs快15.4倍的想象能力。REM在Atari 100K基准测试的26个游戏中的12个游戏中达到超越人类水平的性能，并且在不到12小时的训练时间内完成训练。

    Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
    
[^59]: 非参数仪器变量回归通过随机近似梯度

    Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients

    [https://arxiv.org/abs/2402.05639](https://arxiv.org/abs/2402.05639)

    本文提出了一种通过随机近似梯度最小化投影群体风险的新型非参数仪器变量回归框架，并通过理论和实证实验证明了其竞争性能。此外，本文还处理了二元结果的情况，取得了有希望的结果。

    

    本文提出了SAGD-IV，这是一种通过使用随机近似梯度来最小化投影群体风险的新型非参数仪器变量（NPIV）回归框架。仪器变量（IV）被广泛应用于计量经济学中，以解决在存在不可观测混淆因素的情况下的估计问题，并且机器学习社区致力于改进现有方法并在NPIV设置下设计新方法，该设置被认为是一个不适定的线性逆问题。我们提供了对我们算法的理论支持，并通过实证实验进一步证明了其竞争性能。此外，我们还处理了二元结果的情况，并取得了有希望的结果，而该情况在社区中没有得到与其连续对应物的同样关注。

    This paper proposes SAGD-IV, a novel framework for conducting nonparametric instrumental variable (NPIV) regression by employing stochastic approximate gradients to minimize the projected populational risk. Instrumental Variables (IVs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the Machine Learning community has devoted significant effort to improving existing methods and devising new ones in the NPIV setting, which is known to be an ill-posed linear inverse problem. We provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments. Furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart.
    
[^60]: RepQuant: 通过比例重参数化实现大型Transformer模型的准确后训练量化

    RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization

    [https://arxiv.org/abs/2402.05628](https://arxiv.org/abs/2402.05628)

    RepQuant提出了一种新的后训练量化框架，通过采用量化-推理解耦的范式和量化比例重参数化的方法，实现了准确的量化和高效的推理。

    

    大型Transformer模型已经展示了卓越的成功。后训练量化（PTQ）是一种有前景的解决方案，用于压缩这些大型模型，它只需要一个小型数据集进行校准，并避免端到端的重新训练。不幸的是，现有的PTQ方法通常会导致非常明显的性能损失。我们发现，性能瓶颈来自于在量化过程中过分考虑硬件兼容性，这迫使它们不情愿地使用简单的量化器，虽然以牺牲准确性为代价。基于上述观察，我们提出了RepQuant，一个新的PTQ框架，通过采用量化-推理解耦范式来解决上述问题。RepQuant在量化过程中使用复杂的量化器，在推理过程中使用简化的量化器，并通过量化比例重参数化在两者之间进行数学上等价的转换，从而确保准确的量化和高效的推理。

    Large transformer models have demonstrated remarkable success. Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference. More specific
    
[^61]: 旋转特征中的绑定动力学

    Binding Dynamics in Rotating Features

    [https://arxiv.org/abs/2402.05627](https://arxiv.org/abs/2402.05627)

    本论文研究了旋转特征中的绑定动力学问题，并提出了一种新的“余弦绑定”机制，以替代传统的“$\chi$-binding”机制。通过显式计算特征之间的对齐和相应的权重调整，这一新机制能够达到与传统机制相同的性能，与自注意力和生物神经学有关。

    

    在人类认知中，绑定问题描述了大脑如何灵活地将各种信息整合成具有连贯性的对象表示的未解之谜。类似地，在机器学习中，人们追求通过无监督学习以学习以对象为中心的表示来实现强大的泛化和推理能力的模型。借鉴神经科学理论，旋转特征通过引入矢量特征来学习这种表示，矢量特征的大小包含对象特征，方向包含对象关联。在架构的每个层中都嵌入了“$\chi$-binding”机制，已被证明至关重要，但了解有限。在本文中，我们提出一种替代的“余弦绑定”机制，该机制显式计算特征之间的对齐并相应地调整权重，并且我们证明它可以达到相同的性能。这使我们能够与自注意力和生物神经学产生直接联系。

    In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations. Analogously, in machine learning, there is a pursuit for models capable of strong generalization and reasoning by learning object-centric representations in an unsupervised manner. Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations. The "$\chi$-binding" mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood. In this paper, we propose an alternative "cosine binding" mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance. This allows us to draw direct connections to self-attention and biological neu
    
[^62]: 浅层ReLU-like神经网络的损失景观：稳定点、鞍点逃逸和网络嵌入

    The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding

    [https://arxiv.org/abs/2402.05626](https://arxiv.org/abs/2402.05626)

    本文研究了使用ReLU-like激活函数以经验平方损失训练的单隐藏层神经网络的损失景观，提出了稳定点条件和逃逸神经元的定义，并将鞍点逃逸与逃逸神经元的参数变化联系起来。

    

    本文研究了使用ReLU-like激活函数以经验平方损失训练的单隐藏层神经网络的损失景观。由于激活函数是不可微的，目前还不清楚如何完全描述稳定点。我们提出了适用于非可微和可微情况的稳定点条件。此外，我们还展示了如果一个稳定点不包含“逃逸神经元”（通过一阶条件定义），那么它必定是一个局部最小值。此外，在标量输出情况下，逃逸神经元的存在保证了稳定点不是局部最小值。我们的结果进一步描述了从无穷小（消失）初始化开始的浅层ReLU-like网络的鞍点到鞍点的训练过程，直接将鞍点逃逸与逃逸神经元的参数变化联系起来。此外，我们还完全讨论了网络嵌入的方式。

    In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain "escape neurons", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network emb
    
[^63]: 预训练的生成式语言模型作为序列型任务的通用学习框架

    Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks

    [https://arxiv.org/abs/2402.05616](https://arxiv.org/abs/2402.05616)

    预训练的小型生成式语言模型可以作为序列型任务的通用学习框架，通过指令微调可以在化学信息学任务中实现接近最先进结果。

    

    我们提出了一个观点，即具有数百万参数的小型预训练基础生成式语言模型可以用作序列型任务的通用学习框架。我们的提议解决了从头开始训练神经网络和语言模型所面临的计算资源、技能需求和时间限制等挑战。此外，我们的方法专注于创建小型且高度专业化的模型，能够准确执行基于任务模型无法完成的挑战性任务。我们证明了使用125M、350M和1.3B参数的预训练基础语言模型进行指令微调，使用10,000到1,000,000个指令示例可以在具有挑战性的化学信息学任务上实现接近最先进结果。我们还展示了连续的语言模型微调时期对改进结果的作用，以及数据格式和预训练基础语言模型选择对指令微调成功的重要性。

    We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.
    
[^64]: 优化协作的人工智能-人类混合团队中的授权

    Optimizing Delegation in Collaborative Human-AI Hybrid Teams

    [https://arxiv.org/abs/2402.05605](https://arxiv.org/abs/2402.05605)

    本论文提出了一种优化协作的人工智能-人类混合团队授权的框架，通过引入AI经理（通过强化学习）作为团队的外部观察者，学习团队代理人的行为模型并选择最佳的控制代理人。

    

    当人类和自主系统作为混合团队共同运作时，我们希望确保团队的成功和效率。我们将团队成员称为代理人。在我们提出的框架中，我们解决了混合团队的情况，即在任何时候，只有一个团队成员（控制代理人）被授权为团队的控制者。为了确定最佳的控制代理人选择，我们提出了引入AI经理（通过强化学习）的想法，该经理作为团队的外部观察者学习。经理通过观察代理人的表现和团队所处的环境/世界来学习行为模型，并基于这些观察结果选择出最理想的控制代理人。为了限定经理的任务，我们引入了一组约束条件。经理的约束条件指示团队的可接受运作方式，因此如果团队进入不可接受并需要经理介入的状态，就会违反约束条件。

    When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To
    
[^65]: AttnLRP: 注意力感知的逐层相关传递用于Transformer

    AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers

    [https://arxiv.org/abs/2402.05602](https://arxiv.org/abs/2402.05602)

    AttnLRP是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，并具有与单一反向传播相似的计算效率的方法。它通过扩展逐层相关传递归因方法以处理注意力层来解决了黑盒Transformer模型的归因问题，具有超越现有方法的准确性和理解潜在表示的能力。

    

    大型语言模型容易产生偏见的预测和幻象，这突显了理解其模型内部推理过程的重要性。然而，实现对整个黑盒Transformer模型的准确归因并保持计算效率是一个尚未解决的挑战。通过扩展逐层相关传递归因方法以处理注意力层，我们有效地解决了这些挑战。虽然存在部分解决方案，但我们的方法是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，同时计算效率与单一反向传播相似。通过对Llama 2、Flan-T5和Vision Transformer架构上与现有方法的广泛评估，我们证明了我们提出的方法在准确性方面超过了其他方法，并能够理解潜在表示，为概念打开了大门。

    Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
    
[^66]: 数字计算机打破维度诅咒：通过有限几何的自适应界限

    Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry

    [https://arxiv.org/abs/2402.05576](https://arxiv.org/abs/2402.05576)

    通过利用离散结构，本论文以真实计算机上的实现为基础，打破了统计学习中的维度诅咒，并给出了无维度率的新的泛化界限。

    

    许多机器学习的基础是建立在理想情况下的前提下，即所有的输入和输出空间都是无穷的，例如$\mathbb{R}^d$。然而，由于有限的机器精度、舍入和有限的存储空间等数字计算机的限制，实际情况下这个核心假设往往被违背。简而言之，数字计算机在$\mathbb{R}^d$上操作的是有限的网格。通过利用这些离散结构，我们展示了在实际计算机上实现模型时，统计学习中的维度诅咒被系统地打破。因此，我们针对在真实世界机器上实现的核函数和深度ReLU MLP回归器获得了新的无维度率的泛化界限。我们的结果应用了一种新的非渐进测度集中性结果，该结果给出了概率测度和其在$N$个独立同分布样本上的经验版本之间的距离为$1$-Wasserstein距离的集中性。

    Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM. In short, digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers. Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines.   Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. samples when measured in the $1$-Wasserstein distance. Unlike standard concentration of measure 
    
[^67]: 在随机赌博机中同时实现群体曝光公平性和群内精英主义

    Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits

    [https://arxiv.org/abs/2402.05575](https://arxiv.org/abs/2402.05575)

    该论文介绍了一种在随机赌博机中同时实现群体曝光公平性和群内精英主义的方法，通过提供随时的群体曝光公平性保证和在每个群体中实现个体层面的精英主义公平性。

    

    现有的随机多臂赌博机（MAB）中的公平性方法主要关注对各个臂的曝光保证。当臂根据某些属性自然分组时，我们提出了双层公平性，它考虑了两个层面的公平性。在第一层面，双层公平性保证每个群体有一定的最低曝光。为了解决群内个体臂的分配不均衡问题，我们在第二层面考虑了精英主义公平性，确保每个臂根据其在群体中的优势被选择。我们的工作表明，通过提供(i) 随时的群体曝光公平性保证和(ii) 在每个群体中实现个体层面的精英主义公平性，我们可以调整基于UCB的算法来实现双层公平性。

    Existing approaches to fairness in stochastic multi-armed bandits (MAB) primarily focus on exposure guarantee to individual arms. When arms are naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which considers two levels of fairness. At the first level, Bi-Level Fairness guarantees a certain minimum exposure to each group. To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic fairness at the second level, which ensures that each arm is pulled according to its merit within the group. Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic Fairness within each group. We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure fairness and (b) regret due to meritocratic fairness within each group. Our proposed algorithm BF-UCB 
    
[^68]: 传统机器学习模型和基于双向编码器表示的转换器(BERT)的自动分类饮食紊乱推文：算法开发和验证研究

    Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study

    [https://arxiv.org/abs/2402.05571](https://arxiv.org/abs/2402.05571)

    该研究旨在开发高效的机器学习模型，用于对饮食紊乱相关的推文进行分类。研究发现，基于转换器的双向编码器表示在四个类别中实现了最高的F1分数，并表明基于转换器的模型较传统技术更为优越，但需要更多的计算资源。

    

    背景：饮食紊乱日益普遍，社交网络提供了宝贵的信息。目标：我们的目标是确定有效的机器学习模型，用于对与饮食紊乱相关的推文进行分类。方法：在三个月内，我们收集了关于饮食紊乱的推文。我们对2,000条推文进行了标记，标记了：(1)由饮食紊乱患者撰写的推文，(2)宣传饮食紊乱的推文，(3)信息性，和(4)科学内容。使用传统机器学习和深度学习模型进行分类，并评估准确度、F1分数和计算时间。结果：从1,058,957条收集到的推文中，基于转换器的双向编码器表示在所有四个类别中实现了最高的F1分数(71.1%-86.4%)。结论：基于转换器的模型在分类与饮食紊乱相关的推文方面表现优于传统技术，但它们需要更多的计算资源。

    Background: Eating disorders are increasingly prevalent, and social networks offer valuable information.   Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders.   Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time.   Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories.   Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.
    
[^69]: 使用图神经网络进行超图节点分类

    Hypergraph Node Classification With Graph Neural Networks

    [https://arxiv.org/abs/2402.05569](https://arxiv.org/abs/2402.05569)

    本研究提出了一种简单高效的框架，利用加权子图扩展的图神经网络(WCE-GNN)实现了超图节点分类。实验证明，WCE-GNN具有优秀的预测效果和较低的计算复杂度。

    

    超图是用来模拟现实世界数据中的高阶相互作用的关键。图神经网络（GNNs）的成功揭示了神经网络处理具有成对交互的数据的能力。这激发了使用神经网络处理具有高阶相互作用的数据的想法，从而导致了超图神经网络（HyperGNNs）的发展。GNNs和HyperGNNs通常被认为是不同的，因为它们被设计用于处理不同几何拓扑的数据。然而，在本文中，我们在理论上证明，在节点分类的上下文中，大多数HyperGNNs可以使用带有超图的加权子图扩展的GNN来近似。这导致了WCE-GNN，一种简单高效的框架，包括一个GNN和一个加权子图扩展（WCE），用于超图节点分类。对于九个真实世界的超图节点分类数据集的实验表明，WCE-GNN不仅具有优秀的预测效果，而且具有较低的计算复杂度。

    Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
    
[^70]: 简明考虑交互的解释

    Succint Interaction-Aware Explanations

    [https://arxiv.org/abs/2402.05566](https://arxiv.org/abs/2402.05566)

    本文提出了一种结合了SHAP和NSHAP的方法，通过将特征分成显著交互的部分，构建了一种简明、易解释的加性解释，并通过统计检验剪枝次优解，提高了解释的运行效率。

    

    SHAP是一种流行的解释黑箱模型的方法，通过揭示各个特征的重要性来进行解释。由于忽略了特征之间的交互作用，SHAP的解释可能会令人困惑甚至误导。另一方面，NSHAP报告了所有特征子集的加性重要性。虽然这包含了所有相互作用的特征集，但也导致了一个指数级大小的难以解释的解释。在本文中，我们提出了将这两个方法的优点结合起来的方法，将特征分成显著交互的部分，并使用这些部分构成简明、易解释的加性解释。我们提出了一个标准来衡量这种分区对模型行为的代表性，折衷于得到的解释的复杂性。为了高效地从超指数数量中找到最佳分区，我们展示了如何使用统计检验来剪枝次优解，不仅提高了运行时间，还有助于解释。

    SHAP is a popular approach to explain black-box models by revealing the importance of individual features. As it ignores feature interactions, SHAP explanations can be confusing up to misleading. NSHAP, on the other hand, reports the additive importance for all subsets of features. While this does include all interacting sets of features, it also leads to an exponentially sized, difficult to interpret explanation. In this paper, we propose to combine the best of these two worlds, by partitioning the features into parts that significantly interact, and use these parts to compose a succinct, interpretable, additive explanation. We derive a criterion by which to measure the representativeness of such a partition for a models behavior, traded off against the complexity of the resulting explanation. To efficiently find the best partition out of super-exponentially many, we show how to prune sub-optimal solutions using a statistical test, which not only improves runtime but also helps to det
    
[^71]: 闪回：理解和减轻联邦学习中的遗忘问题

    Flashback: Understanding and Mitigating Forgetting in Federated Learning

    [https://arxiv.org/abs/2402.05558](https://arxiv.org/abs/2402.05558)

    本研究深入探讨了联邦学习中的遗忘问题，强调了遗忘在异质数据环境中的关键性质，提出了"闪回"算法来解决遗忘问题并取得优异的学习结果。

    

    在联邦学习中，遗忘或者说在不同轮次中的知识丢失阻碍了算法的收敛，尤其在客户端之间存在严重的数据异质性的情况下更为明显。本研究探究了这个问题的复杂性，强调了遗忘在异质数据环境中对联邦学习的低效学习起到的关键作用。知识丢失既发生在客户端局部更新中，也发生在服务器端的聚合步骤中；只解决其中一个而忽略另一个无法有效减轻遗忘问题。我们引入了一个度量遗忘的指标，以确保在新知识获取中明确识别遗忘。借助这些洞察，我们提出了一种名为"闪回"的联邦学习算法，该算法使用动态蒸馏方法来规范化局部模型，并有效地聚合它们的知识。在不同的基准测试中，"闪回"优于其他方法，减轻了遗忘问题，并在6到16个轮次内达到更快的目标准确度。

    In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.
    
[^72]: 在多项式时间内用Chebyshev和比特复杂度学习任意温度下的量子哈密顿量

    Learning quantum Hamiltonians at any temperature in polynomial time with Chebyshev and bit complexity

    [https://arxiv.org/abs/2402.05552](https://arxiv.org/abs/2402.05552)

    本文提出了一种新的基于Chebyshev展开的指数函数多项式逼近方法，将学习量子哈密顿量问题转化为多项式优化问题，并证明在温和假设下，在有界度数的双重交互图情况下，可以在多项式时间内学习k-局部哈密顿量。

    

    本文考虑了在已知逆温度下给定量子哈密顿量的各个Gibbs态副本的学习问题，参考了Haah等人的研究[2108.04842]和Bakshi等人的研究[arXiv:2310.02243]。我们的主要技术贡献是基于Chebyshev展开的指数函数的新型平坦多项式逼近，这使得学习量子哈密顿量可以转化为一个多项式优化问题。进而，这可以受益于使用矩/半正定松弛方法，其多项式比特复杂度需要进行谨慎分析[O'Donnell，ITCS 2017]。最后，我们证明了在温和假设下，学习一个有界度数的双重交互图的k-局部哈密顿量可以在多项式时间内完成。

    We consider the problem of learning local quantum Hamiltonians given copies of their Gibbs state at a known inverse temperature, following Haah et al. [2108.04842] and Bakshi et al. [arXiv:2310.02243]. Our main technical contribution is a new flat polynomial approximation of the exponential function based on the Chebyshev expansion, which enables the formulation of learning quantum Hamiltonians as a polynomial optimization problem. This, in turn, can benefit from the use of moment/SOS relaxations, whose polynomial bit complexity requires careful analysis [O'Donnell, ITCS 2017]. Finally, we show that learning a $k$-local Hamiltonian, whose dual interaction graph is of bounded degree, runs in polynomial time under mild assumptions.
    
[^73]: 离线演员-评论者强化学习扩展到大型模型

    Offline Actor-Critic Reinforcement Learning Scales to Large Models

    [https://arxiv.org/abs/2402.05546](https://arxiv.org/abs/2402.05546)

    本文证明了离线演员-评论者强化学习方法可以扩展到大型模型，并且比基线方法在多任务训练中表现更好。通过引入Perceiver-based演员-评论者模型，我们揭示了离线强化学习与自注意机制和跨注意力模块配合的关键模型特征。这项研究的发现表明：离线演员-评论者算法是逐渐摆脱行为克隆范式的一种自然选择，并且通过离线强化学习可以从次优示范或自动生成的数据中学习掌握多个领域的多任务策略。

    

    我们证明了离线演员-评论者强化学习可以扩展到大型模型，如transformer，并且遵循与监督学习类似的扩展规律。我们发现，离线演员-评论者算法在包含132个连续控制任务的大型数据集上的多任务训练中，可以胜过强大的监督式行为克隆基线，该数据集包含了次优和专家行为。我们引入了一种基于Perceiver的演员-评论者模型，并阐明了使离线强化学习与自注意机制和跨注意力模块配合工作所需的关键模型特征。总的来说，我们发现：i）简单的离线演员评论者算法是逐渐远离当前主流行为克隆范式的自然选择，ii）通过离线强化学习，可以从次优示范或自动生成的数据中学习掌握许多领域的多任务策略，包括真实机器人任务。

    We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.
    
[^74]: 机器学习应用于组学数据

    Machine learning applied to omics data

    [https://arxiv.org/abs/2402.05543](https://arxiv.org/abs/2402.05543)

    本章介绍了将机器学习技术应用于组学数据的方法，特别是在胰腺癌的基因组学和免疫组学综合分析中，通过随机森林和惩罚性多项式逻辑回归来预测胰腺癌免疫浸润，同时提出了使用关联规则进行预测的方法，以提高预测能力。

    

    在本章中，我们介绍了一些机器学习技术在组学数据的背景下的应用。更具体地，我们回顾和评估了随机森林和惩罚性多项式逻辑回归在胰腺癌基因组学和免疫组学的综合分析中的应用。此外，我们提出使用关联规则进行预测，以克服先前提到的模型预测能力不足的问题。最后，我们将回顾的方法应用于来自TCGA的真实数据集，该数据集由107个肿瘤性胰腺样品和117,486个种系SNP组成，展示了所提出方法在预测胰腺癌免疫浸润中的良好性能。

    In this chapter we illustrate the use of some Machine Learning techniques in the context of omics data. More precisely, we review and evaluate the use of Random Forest and Penalized Multinomial Logistic Regression for integrative analysis of genomics and immunomics in pancreatic cancer. Furthermore, we propose the use of association rules with predictive purposes to overcome the low predictive power of the previously mentioned models. Finally, we apply the reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic samples and 117,486 germline SNPs, showing the good performance of the proposed methods to predict the immunological infiltration in pancreatic cancer.
    
[^75]: 强化学习作为鲁棒和公平联邦学习的催化剂：解密客户贡献动力学

    Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions

    [https://arxiv.org/abs/2402.05541](https://arxiv.org/abs/2402.05541)

    本研究提出了一个新的框架——强化联邦学习（RFL），通过利用深度强化学习自适应地优化客户贡献的聚合过程，以增强模型鲁棒性和在非相同分布环境下参与者之间的公平性。

    

    最近在联邦学习（FL）方面的进展产生了模型，通过在多个分散的设备或系统上训练来保护用户隐私并保留本地数据样本。然而，这些策略经常忽视统计异质性和对敌对攻击的脆弱性所带来的困难，这些因素会降低模型的鲁棒性和公平性。个性化的FL策略可以通过调整模型来适应个别客户的特点，但往往忽视了服务器端聚合的脆弱性。为了解决这些问题，我们提出了强化联邦学习（RFL），这是一个利用深度强化学习来自适应优化聚合过程中客户贡献的新框架，从而增强恶意客户下的模型鲁棒性和参与者之间的公平性在非相同分布环境下。为了实现这一目标，我们提出了一种细致的方法，其中包括基于深度确定性策略梯度算法的协同训练，以优化客户贡献的过程。

    Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for co
    
[^76]: 为加强社交媒体帖子中的饮食障碍检测，赋能机器学习模型的语境知识

    Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts

    [https://arxiv.org/abs/2402.05536](https://arxiv.org/abs/2402.05536)

    这项研究提出了一种新颖的方法，通过结合知识图谱和深度学习，借助实体识别和链接技术以及上下文化嵌入，增强了机器学习模型对社交媒体帖子的分类能力。该方法主要应用于健康领域的饮食障碍识别，为早期诊断提供帮助。

    

    社交网络在信息共享方面至关重要，尤其在健康领域用于讨论疾病和治疗。然而，这些平台上的帖子往往是简短的文本，给人工智能在理解上带来了挑战。我们介绍了一种创新的混合方法，结合社区维护的知识图谱（如Wikidata）和深度学习，以增强社交媒体帖子的分类。该方法利用先进的实体识别器和链接器（如Falcon 2.0）将短文帖子中的实体连接到知识图谱。然后使用知识图谱嵌入（KGEs）和上下文化词嵌入（如BERT）来创建丰富的、基于语境的帖子表示。我们的重点是健康领域，特别是识别与饮食障碍相关的帖子（如厌食症、暴食症），以帮助医疗服务提供者进行早期诊断。我们在一个包含2,000条关于饮食障碍的推文的数据集上测试了我们的方法，发现合并单词嵌入和上下文化嵌入可以显著提高检测性能。

    Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts.   Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word em
    
[^77]: 异步扩散学习中的代理子采样与局部更新

    Asynchronous Diffusion Learning with Agent Subsampling and Local Updates

    [https://arxiv.org/abs/2402.05529](https://arxiv.org/abs/2402.05529)

    本论文研究了一种以异步方式操作的代理网络，旨在发现适合个体本地数据集的全局模型。通过证明异步扩散策略在均方误差上的稳定性，以及在联邦学习中的性能保证，本研究对于分布式学习的研究具有重要的创新和贡献。

    

    在本研究中，我们研究了一组以异步方式操作的代理，旨在发现适合个体本地数据集的理想全局模型。我们的假设是每个代理独立选择何时参与算法，并在任何给定时刻选择与之合作的邻域的特定子集。当代理选择参与时，它会在将结果传达给子采样邻域之前经历多次局部更新。在这种设置下，我们证明了得到的异步扩散策略在均方误差意义上是稳定的，并对联邦学习设置提供了性能保证。我们通过数值模拟来说明研究结果。

    In this work, we examine a network of agents operating asynchronously, aiming to discover an ideal global model that suits individual local datasets. Our assumption is that each agent independently chooses when to participate throughout the algorithm and the specific subset of its neighbourhood with which it will cooperate at any given moment. When an agent chooses to take part, it undergoes multiple local updates before conveying its outcomes to the sub-sampled neighbourhood. Under this setup, we prove that the resulting asynchronous diffusion strategy is stable in the mean-square error sense and provide performance guarantees specifically for the federated learning setting. We illustrate the findings with numerical simulations.
    
[^78]: 专家混合模型中的缓冲区溢出问题

    Buffer Overflow in Mixture of Experts

    [https://arxiv.org/abs/2402.05526](https://arxiv.org/abs/2402.05526)

    专家混合模型中，我们发现具有跨批次依赖的专家路由策略易受攻击，恶意查询可以影响模型对其他良性查询的输出。

    

    在保持推理成本稳定的同时，专家混合模型（MoE）已成为扩展大型基础模型的关键要素。我们发现，具有跨批次依赖的专家路由策略易受攻击。如果恶意查询与良性查询分组在同一批次中，恶意查询会影响模型对其他良性查询的输出。我们通过在玩具实验环境中进行概念验证攻击来证明这一点。

    Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious queries can be sent to a model and can affect a model's output on other benign queries if they are grouped in the same batch. We demonstrate this via a proof-of-concept attack in a toy experimental setting.
    
[^79]: 差分隐私的基于模型的离线强化学习

    Differentially Private Model-Based Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.05525](https://arxiv.org/abs/2402.05525)

    本研究提出了一种差分隐私的基于模型的离线强化学习方法，通过学习离线数据中的隐私模型以及基于模型的策略优化，实现了从离线数据中训练具有隐私保护的强化学习代理。同时，研究还总结了在这种设置下隐私的代价。

    

    我们解决了具有隐私保证的离线强化学习问题，目标是训练一个相对于数据集中每个轨迹具有差分隐私的策略。为了实现这一目标，我们引入了DP-MORL，一种带有差分隐私保证的MBRL算法。首先，使用DP-FedAvg从离线数据中学习环境的隐私模型，DP-FedAvg是一种为神经网络提供轨迹级差分隐私保证的训练方法。然后，我们使用基于模型的策略优化从（受罚的）隐私模型中推导出策略，无需进一步与系统交互或访问输入数据。我们经验证明，DP-MORL能够从离线数据中训练出具有隐私保护的RL代理，并进一步概述了在这种情况下隐私的代价。

    We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
    
[^80]: 线性化模型以实现高效而鲁棒的隐私推理

    Linearizing Models for Efficient yet Robust Private Inference

    [https://arxiv.org/abs/2402.05521](https://arxiv.org/abs/2402.05521)

    本文提出了一种名为RLNet的鲁棒线性化网络，通过减少延迟并改善模型在各种情况下的表现，实现了高效而鲁棒的隐私推理。

    

    对数据隐私的日益关注导致了客户端-服务器应用中私有推理（PI）框架的发展，该框架既保护数据隐私又保护模型知识产权。然而，所需的密码原语导致了显著的延迟开销，限制了其广泛应用。同时，不断变化的环境要求PI服务对各种自然发生的和基于梯度的扰动具有鲁棒性。尽管已有一些工作专注于开发适用于PI的延迟-高效模型，但这些模型对鲁棒性的影响尚未被探索。为实现这个目标，本文提出了RLNet，一种鲁棒的线性化网络，通过减少高延迟的ReLU操作提供延迟改进，同时提高模型在清晰图像和损坏图像上的性能。特别地，RLNet模型提供了在清晰图像、自然扰动图像和基于梯度的扰动图像上的分类准确性改善的“三连赢”解决方案。

    The growing concern about data privacy has led to the development of private inference (PI) frameworks in client-server applications which protects both data privacy and model IP. However, the cryptographic primitives required yield significant latency overhead which limits its wide-spread application. At the same time, changing environments demand the PI service to be robust against various naturally occurring and gradient-based perturbations. Despite several works focused on the development of latency-efficient models suitable for PI, the impact of these models on robustness has remained unexplored. Towards this goal, this paper presents RLNet, a class of robust linearized networks that can yield latency improvement via reduction of high-latency ReLU operations while improving the model performance on both clean and corrupted images. In particular, RLNet models provide a "triple win ticket" of improved classification accuracy on clean, naturally perturbed, and gradient-based perturbe
    
[^81]: 机器学习增强的混合整数线性规划中的分支定界

    Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming

    [https://arxiv.org/abs/2402.05501](https://arxiv.org/abs/2402.05501)

    本文调研了在混合整数线性规划中利用机器学习算法增强分支定界算法的方法，并强调了机器学习和数学优化的融合视野。

    

    混合整数线性规划（MILP）是数学优化的基础，为各种应用提供了强大的建模语言。在过去几十年中，MILP求解算法取得了巨大的算法进展，许多商业和学术软件包也存在。然而，来自问题实例和求解器的数据的可用性以及解决新问题和更大（现实生活中）的实例的需求，触发了算法持续发展的需求。MILP求解器使用分支定界作为其主要组成部分。近年来，机器学习算法在增强分支定界算法的所有主要任务中，如原始启发式、分支、割平面、节点选择和求解器配置决策方面，取得了爆炸性的发展。本文通过调研介绍了这些方法，涉及机器学习和数学优化的融合视野。

    Mixed Integer Linear Programming (MILP) is a pillar of mathematical optimization that offers a powerful modeling language for a wide range of applications. During the past decades, enormous algorithmic progress has been made in solving MILPs, and many commercial and academic software packages exist. Nevertheless, the availability of data, both from problem instances and from solvers, and the desire to solve new problems and larger (real-life) instances, trigger the need for continuing algorithmic development. MILP solvers use branch and bound as their main component. In recent years, there has been an explosive development in the use of machine learning algorithms for enhancing all main tasks involved in the branch-and-bound algorithm, such as primal heuristics, branching, cutting planes, node selection and solver configuration decisions. This paper presents a survey of such approaches, addressing the vision of integration of machine learning and mathematical optimization as complement
    
[^82]: 使用深度学习技术和特征增强预测心脏疾病风险

    Heart disease risk prediction using deep learning techniques with feature augmentation

    [https://arxiv.org/abs/2402.05495](https://arxiv.org/abs/2402.05495)

    本论文提出使用深度学习方法和特征增强技术预测心脏疾病风险，在大规模人群中取得了显著的改进。

    

    心血管疾病是普通人群死亡风险最大的疾病之一。对心脏疾病的迟早发现极大地影响着患者的生存机会。年龄、性别、胆固醇水平、血糖水平、心率等因素已知对危及生命的心脏问题有影响，但由于变量的数量较多，专家难以评估每个患者时考虑到这些信息。在本文中，作者提出使用深度学习方法，结合特征增强技术，评估患者是否有患心血管疾病的风险。所提出方法的结果优于其他现有方法4.4%，使准确率达到90%，这在大规模人群受到这种困扰的情况下，是一个显著的改进。

    Cardiovascular diseases state as one of the greatest risks of death for the general population. Late detection in heart diseases highly conditions the chances of survival for patients. Age, sex, cholesterol level, sugar level, heart rate, among other factors, are known to have an influence on life-threatening heart problems, but, due to the high amount of variables, it is often difficult for an expert to evaluate each patient taking this information into account. In this manuscript, the authors propose using deep learning methods, combined with feature augmentation techniques for evaluating whether patients are at risk of suffering cardiovascular disease. The results of the proposed methods outperform other state of the art methods by 4.4%, leading to a precision of a 90%, which presents a significant improvement, even more so when it comes to an affliction that affects a large population.
    
[^83]: 使用多任务神经网络确定帕金森病患者的严重程度

    Determining the severity of Parkinson's disease in patients using a multi task neural network

    [https://arxiv.org/abs/2402.05491](https://arxiv.org/abs/2402.05491)

    本文提出了一种基于声音分析的非侵入性技术，使用多任务神经网络来确定帕金森病患者的严重程度和疾病进展的方法。

    

    在帕金森病的晚期，诊断比较容易，但在早期阶段很难诊断。早期诊断是治疗症状的关键。它影响日常生活，并降低患者和家人的生活质量，是继老年痴呆症后第二常见的神经退行性疾病。目前大多数关于预测帕金森病严重程度的研究都是在疾病的晚期进行的。本研究分析了一组可以通过语音分析轻松提取的变量，使其成为一种非侵入性技术。本文提出了一种基于不同深度学习技术的方法，具有两个目的。一方面，确定一个人是严重帕金森病还是非严重帕金森病；另一方面，通过回归技术确定给定患者疾病的进展程度。

    Parkinson's disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages. Early diagnosis is essential to be able to treat the symptoms. It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60. Most current studies on the prediction of Parkinson's severity are carried out in advanced stages of the disease. In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique. In this paper, a method based on different deep learning techniques is proposed with two purposes. On the one hand, to find out if a person has severe or non-severe Parkinson's disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient. The UPDRS (Unified Parkinson
    
[^84]: 一种非侵入性神经质量评估模型应用于表面肌电信号

    A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals

    [https://arxiv.org/abs/2402.05482](https://arxiv.org/abs/2402.05482)

    本研究提出了一种新的非侵入性神经质量评估模型QASE-net，可以有效预测表面肌电信号的信噪比，实验证明其相比之前的模型具有更低的预测误差和更高的线性相关性。

    

    在涉及测量肌肉表面肌电（sEMG）的实际场景中，尤其是靠近心脏的区域，主要的污染源之一是心电图（ECG）信号的存在。为了更有效地评估实际世界中的sEMG数据质量，本研究提出了QASE-net，一种新的非侵入性模型，可以预测sEMG信号的信噪比。QASE-net将CNN-BLSTM与注意力机制相结合，并采用端到端的训练策略。我们的实验框架利用了两个开放访问数据库的实际世界sEMG和ECG数据，分别是非侵入性适应性假肢数据库和MIT-BIH正常窦性心律数据库。实验结果表明，QASE-net优于先前的评估模型，具有显著降低的预测误差和明显更高的线性相关性。这些发现显示了QASE-net在提高可靠性和的潜力

    In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and 
    
[^85]: 多时间尺度集合Q-learning用于马尔可夫决策过程政策优化

    Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization

    [https://arxiv.org/abs/2402.05476](https://arxiv.org/abs/2402.05476)

    本文提出了一种新颖的无模型集合强化学习算法，用于解决大型网络中的性能和复杂性挑战。算法使用多个Q-learning算法在多个马尔可夫环境中并行运行，并通过自适应加权融合输出，提供近似最优策略。实验结果表明，该算法可以实现多达55%的平均策略减少。

    

    强化学习是解决未知环境下网络控制或策略优化问题的经典工具。原始的Q-learning在非常大的网络中存在性能和复杂性挑战。本文提出了一种新颖的无模型集合强化学习算法，用于处理具有马尔可夫决策过程（MDP）模型的网络中的这些挑战。多个Q-learning算法在多个独立的、合成的和结构相关的马尔可夫环境中并行运行；通过基于Jensen-Shannon散度（JSD）的自适应加权机制来融合输出，以获得具有低复杂性的近似最优策略。论文还提供了算法的理论证明，包括关键统计量和Q函数的收敛性。在多个网络模型上的数值结果显示，所提出的算法可以实现平均策略减少多达55%。

    Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy 
    
[^86]: 隐式扩散: 通过随机采样实现高效优化

    Implicit Diffusion: Efficient Optimization through Stochastic Sampling

    [https://arxiv.org/abs/2402.05468](https://arxiv.org/abs/2402.05468)

    本文介绍了一种通过随机采样优化隐含分布的新算法，并提出了一种通用框架，用于在单个循环中同时进行优化和采样步骤。实验结果证明了该方法在真实环境中的有效性。

    

    我们提出了一种通过参数化随机扩散隐式定义的分布来进行优化的新算法。通过优化这些参数，可以修改采样过程的结果分布。我们引入了一个针对这些过程的一阶优化的通用框架，通过在单个循环中进行优化和采样步骤来实现。这种方法受到双层优化和自动隐式微分的最新进展的启发，利用采样作为在概率分布空间上进行优化的视角。我们提供了关于我们方法性能的理论保证，以及在实际环境中证明其有效性的实验结果。

    We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings.
    
[^87]: 通过凸凹损失函数降低会员推断中的隐私风险

    Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss

    [https://arxiv.org/abs/2402.05453](https://arxiv.org/abs/2402.05453)

    本论文提出了一种凸凹损失函数的方法，通过梯度下降实现训练损失的高方差，从而降低会员推断攻击中的隐私风险。

    

    机器学习模型容易受到会员推断攻击（MIAs），即推断样本是否在训练集中。现有工作利用梯度上升来增大训练数据的损失方差，缓解隐私风险。然而，向相反方向优化可能导致模型参数在局部最小值附近振荡，导致不稳定和次优性能。在本研究中，我们提出了一种新的方法——凸凹损失函数，在梯度下降的过程中实现了训练损失分布的高方差。我们的方法受到理论分析的启发，凸损失函数在训练过程中倾向于减少损失方差。因此，我们在CCL的背后的关键思想是通过凹函数项减小损失函数的凸性。使用CCL训练的神经网络产生训练数据的高方差损失，加强了对MIAs的防御。大量实验证实了CCL的卓越性能，实现了最新的平衡。

    Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i
    
[^88]: Minecraft-ify：用于游戏应用的Minecraft风格图像生成与文本引导的图像编辑

    Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application

    [https://arxiv.org/abs/2402.05448](https://arxiv.org/abs/2402.05448)

    本文提出了一种用于Minecraft游戏应用的图像生成和编辑系统"Minecraft-ify"，能够生成针对3D虚拟角色的面部聚焦图像，并支持使用文本进行图像编辑，提供了更自由和优化的用户体验。

    

    本文首先介绍了面向Minecraft视频游戏的角色纹理生成系统"Minecraft-ify"，该系统可以生成针对具有立方体流形的3D虚拟角色的面部聚焦图像以进行纹理映射。与现有项目或作品只生成纹理不同，提出的系统可以反转用户提供的真实图像，或从学习到的分布生成平均/随机外观。此外，它可以使用StyleGAN和StyleCLIP进行文本引导的操作。这些功能提供了更广泛的用户体验和更多的自由，是一种用户友好的AI工具。

    In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
    
[^89]: 准确的LLMs的LoRA-Finetuning量化通过信息保留

    Accurate LoRA-Finetuning Quantization of LLMs via Information Retention

    [https://arxiv.org/abs/2402.05445](https://arxiv.org/abs/2402.05445)

    本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。

    

    将LLMs的LoRA-finetuning量化研究得到准确但紧凑的LLMs以便在资源受限的硬件上部署。然而，现有的方法导致量化的LLMs严重退化，甚至无法从LoRA的调优中获益。本文提出了一种新颖的IR-QLoRA，通过信息保留推动带有LoRA的量化LLMs变得高度准确。所提出的IR-QLoRA主要依赖于两种从统一信息视角派生的技术：（1）基于统计的信息校准量化允许LLMs的量化参数精确保留原始信息；（2）基于调优的信息弹性连接使LoRA利用具有多样信息的弹性表示转换。综合实验证明，在2-4位宽下，IR-QLoRA可以显著提高LLaMA和LLaMA2系列的准确性，例如，4位LLaMA-7B相比于...

    The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
    
[^90]: 可扩展的不平衡最优输运生成建模中的Wasserstein渐变流

    Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport

    [https://arxiv.org/abs/2402.05443](https://arxiv.org/abs/2402.05443)

    本文介绍了一种可扩展的生成模型，称为Semi-dual JKO (S-JKO)，通过采用半对偶形式的JKO步骤，降低了训练复杂性，并在WGF上有显著的性能改进。

    

    Wasserstein渐变流（WGF）描述了Wasserstein空间中概率密度的梯度动力学。WGF提供了在概率分布上进行优化的有希望的方法。数值上近似连续的WGF需要时间离散化方法。其中最著名的方法是JKO方案。在这方面，以前的WGF模型采用JKO方案，并为每个JKO步骤参数化传输映射。然而，这种方法导致了与JKO步骤数量K成二次增长的训练复杂性$O(K^2)$。这严重限制了WGF模型的可扩展性。在本文中，我们介绍了一种可扩展的基于WGF的生成模型，称为半对偶JKO（S-JKO）。我们的模型基于JKO步骤的半对偶形式，通过JKO步骤与不平衡最优输运之间的等价性得到。我们的方法将训练复杂性降低到$O(K)$。我们证明了我们的模型明显优于现有的基于WGF的生成模型。

    Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based gener
    
[^91]: 学习不确定性感知的时间扩展动作

    Learning Uncertainty-Aware Temporally-Extended Actions

    [https://arxiv.org/abs/2402.05439](https://arxiv.org/abs/2402.05439)

    我们提出了一种名为不确定性感知时间扩展（UTE）的算法，在强化学习中解决了动作重复可能降低性能的问题，通过测量不确定性从而让策略根据需求进行选择，实验证明UTE优于现有算法。

    

    在强化学习中，动作空间中的时间抽象，例如动作重复，是一种通过扩展动作促进策略学习的技术。然而，以前的动作重复研究存在一个主要限制，即当重复次优动作时可能降低性能。这个问题经常抵消了动作重复的优势。为了解决这个问题，我们提出了一种名为不确定性感知时间扩展（UTE）的新算法。UTE使用集成方法在动作扩展期间准确地测量不确定性。这个特性允许策略根据其特定需求，在强调探索或采取不确定性-抵制方法之间进行选择。我们通过在Gridworld和Atari 2600环境中进行实验展示了UTE的有效性。我们的研究结果表明，UTE优于现有的动作重复算法，有效地缓解了它们固有的局限性，并显著提高了效果。

    In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhan
    
[^92]: GPT-4使用结构化叙事提示生成生活事件的叙述：一项验证研究

    GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study

    [https://arxiv.org/abs/2402.05435](https://arxiv.org/abs/2402.05435)

    本研究通过使用结构化叙事提示，验证了GPT-4生成的叙述在传达生活事件方面的有效性。研究结果表明，大多数叙述能够足够传达提示的意图。同时，通过机器学习模型的训练和验证，可以自动识别有效和无效的叙述。

    

    大型语言模型在生成各种叙述方面发挥重要作用，促进了对其在叙述形式中传达生活事件效果的系统探索。本研究利用零-shot结构化叙事提示，使用OpenAI的GPT-4生成了24,000个叙述。从这个数据集中，我们手动分类了2,880个叙述，并评估它们在传达出生、死亡、招聘和解雇事件方面的有效性。令人惊讶的是，87.43%的叙述足够传达结构化提示的意图。为了自动识别有效和无效的叙述，我们对分类数据集训练和验证了九个机器学习模型。利用这些模型，我们扩展了对剩余21,120个叙述的分类预测分析。所有的机器学习模型在将有效的叙述分类为有效方面表现出色，但在同时将无效的叙述分类为无效方面存在挑战。我们的研究结果不仅推进了这一领域的发展，还提供了自动识别有效叙述的有益信息。

    Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
    
[^93]: 混合密度网络用于分类及其在产品捆绑中的应用

    Mixture Density Networks for Classification with an Application to Product Bundling

    [https://arxiv.org/abs/2402.05428](https://arxiv.org/abs/2402.05428)

    本论文提出了两个基于混合密度网络的分类模型，这两个模型通过拟合高斯混合分布并使用学习到的分布进行分类，效果略优于或与五个基准分类模型相当。在实际的产品捆绑应用中，我们的模型在学习产品支付意愿分布方面展现出真实的实用效果。

    

    虽然混合密度网络(MDNs)在回归任务中被广泛使用，但在分类任务中却很少使用。其中一个原因是MDNs在分类任务中的可用性不明确和直接。本文中，我们提出了两个基于MDN的分类模型。这两个模型对数据进行高斯混合拟合，并使用拟合的分布通过评估给定输入特征的学习累积分布函数来对给定样本进行分类。在三个公开可用的数据集上，所提出的基于MDN的模型的性能略优于或与五个基准分类模型相当。然而，我们的模型的真实效用通过一个实际的产品捆绑应用中得以展现。具体而言，我们使用我们的基于MDN的模型从合成销售数据中学习两个产品的支付意愿分布。

    While mixture density networks (MDNs) have been extensively used for regression tasks, they have not been used much for classification tasks. One reason for this is that the usability of MDNs for classification is not clear and straightforward. In this paper, we propose two MDN-based models for classification tasks. Both models fit mixtures of Gaussians to the the data and use the fitted distributions to classify a given sample by evaluating the learnt cumulative distribution function for the given input features. While the proposed MDN-based models perform slightly better than, or on par with, five baseline classification models on three publicly available datasets, the real utility of our models comes out through a real-world product bundling application. Specifically, we use our MDN-based models to learn the willingness-to-pay (WTP) distributions for two products from synthetic sales data of the individual products. The Gaussian mixture representation of the learnt WTP distributions
    
[^94]: 采样理论视角下的隐式神经表示激活

    A Sampling Theory Perspective on Activations for Implicit Neural Representations

    [https://arxiv.org/abs/2402.05427](https://arxiv.org/abs/2402.05427)

    本研究从采样理论的视角对隐式神经表示（INR）中的激活进行了全面分析，发现sinc激活是理论上的最佳选择，并建立了动力系统与INR之间的联系。

    

    隐式神经表示（INR）已经在将信号编码为紧凑的、可微分的实体方面变得流行起来。虽然通常使用傅里叶位置编码或非传统激活函数（如高斯、正弦或小波）等技术来捕捉高频内容，但它们的属性在统一的理论框架中缺乏探索。为了填补这个空白，我们从采样理论的视角对这些激活进行了全面分析。我们的研究发现，在与INR结合时以前未使用的sinc激活是理论上的信号编码最佳选择。此外，我们建立了动力系统和INR之间的联系，利用采样理论来连接这两个范例。

    Implicit Neural Representations (INRs) have gained popularity for encoding signals as compact, differentiable entities. While commonly using techniques like Fourier positional encodings or non-traditional activation functions (e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content, their properties lack exploration within a unified theoretical framework. Addressing this gap, we conduct a comprehensive analysis of these activations from a sampling theory perspective. Our investigation reveals that sinc activations, previously unused in conjunction with INRs, are theoretically optimal for signal encoding. Additionally, we establish a connection between dynamical systems and INRs, leveraging sampling theory to bridge these two paradigms.
    
[^95]: 神经电路图：用于深度学习架构的通信、实现和分析的稳健图示方法

    Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures

    [https://arxiv.org/abs/2402.05424](https://arxiv.org/abs/2402.05424)

    提出了神经电路图，一种适用于深度学习架构通信的图形语言，可以准确地显示数据排列的变化、操作在坐标轴上的广播方式和线性操作的重要并行行为。

    

    图示方法很重要。不幸的是，深度学习界缺乏一种标准的方法来绘制架构图。当前的线性代数符号和临时图示的组合无法提供足够精确的细节，以理解架构的所有细节。然而，这些细节对于忠实的实现、数学分析、进一步的创新和道德保证至关重要。本文提出了神经电路图，这是一种专门针对深度学习架构通信需求的图形语言。神经电路图自然地跟踪数据的变化排列，准确地显示操作如何在坐标轴上广播，并展示线性操作的重要并行行为。现有图示方法的一个悬而未决的问题是无法同时表达坐标轴的细节和数据的自由排列，而神经电路图则解决了这个问题。它们的组合结构类似于代码，创建了一个密切的对应关系。

    Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence bet
    
[^96]: DiffTOP: 可微分轨迹优化在强化学习和模仿学习中的应用

    DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning

    [https://arxiv.org/abs/2402.05421](https://arxiv.org/abs/2402.05421)

    DiffTOP使用可微分轨迹优化作为策略表示来生成动作，解决了模型基于强化学习算法中的“目标不匹配”问题，并在模仿学习任务上进行了性能基准测试。

    

    本文介绍了DiffTOP，它利用可微分轨迹优化作为策略表示，为深度强化学习和模仿学习生成动作。轨迹优化是一种在控制领域中广泛使用的算法，由成本和动力学函数参数化。我们的方法的关键是利用了最近在可微分轨迹优化方面的进展，使得可以计算损失对于轨迹优化的参数的梯度。因此，轨迹优化的成本和动力学函数可以端到端地学习。DiffTOP解决了之前模型基于强化学习算法中的“目标不匹配”问题，因为DiffTOP中的动力学模型通过轨迹优化过程中的策略梯度损失直接最大化任务性能。我们还对DiffTOP在标准机器人操纵任务套件中进行了模仿学习性能基准测试。

    This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
    
[^97]: 不依靠分词的基于连接主义时间分类损失的文本验证码分类OCR模型

    Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification

    [https://arxiv.org/abs/2402.05417](https://arxiv.org/abs/2402.05417)

    本研究提出了一种不依靠分词的基于连接主义时间分类损失的文本验证码分类OCR模型，该模型在处理复杂和扭曲内容方面具有优势，并在公开数据集上取得了很高的准确率。

    

    验证码被广泛用于通过区分计算机响应和人类响应来保护系统免受自动响应。文本、音频、视频、图片等基于光学字符识别（OCR）的验证码用于创建验证码。基于文本的OCR验证码是最常用的验证码，但面临复杂和扭曲内容的问题。目前有尝试使用机器学习和神经网络构建基于验证码检测和分类的系统，需要进行准确性调整。现有系统在识别扭曲字符、处理可变长度验证码和找到验证码中的顺序依赖方面存在挑战。在这项工作中，我们提出了一种基于连接主义时间分类损失技术的文本验证码分类OCR模型，该模型不依赖分词。该模型在公开数据集上进行了训练和测试。在字符级别上，该模型达到了99.80\%的准确率，而在词级别上达到了95\%的准确率。

    Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses. Text, audio, video, picture picture-based Optical Character Recognition (OCR) are used for creating captcha. Text-based OCR captcha are the most often used captcha which faces issues namely, complex and distorted contents. There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy. The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha. In this work, we propose a segmentation-free OCR model for text captcha classification based on the connectionist temporal classification loss technique. The proposed model is trained and tested on a publicly available captcha dataset. The proposed model gives 99.80\% character level accuracy, while 95\% word level accuracy. Th
    
[^98]: 基于版本年龄的联邦学习客户端调度策略

    Version age-based client scheduling policy for federated learning

    [https://arxiv.org/abs/2402.05407](https://arxiv.org/abs/2402.05407)

    这篇论文提出了基于版本年龄的联邦学习客户端调度策略。该策略考虑了及时性和内容滞后，解决了联邦学习中慢速节点的挑战，并对全局模型收敛性和稳定性产生了深远影响。

    

    联邦学习（FL）已经成为一种隐私保护的机器学习范 Paradigm，可以在不共享本地数据的情况下进行多个客户端的协作训练。尽管边缘设备能力的增强，但通信瓶颈导致在聚合大量客户端时出现挑战；每次全局聚合时，只有一部分客户端能够更新其参数。这种现象给FL中的慢速节点（stragglers）带来了重要的挑战，并且客户端调度策略对全局模型收敛性和稳定性有着深远影响。现有的调度策略解决了滞后性问题，但主要集中在及时性或内容上。在此基础上，我们引入了Version Age of Information（VAoI）的新概念来应用于FL。与传统的信息年龄度量不同，VAoI同时考虑了及时性和内容滞后。每个客户端的版本年龄以离散方式进行更新，表示信息的新鲜程度。VAoI被纳入到客户端调度中。

    Federated Learning (FL) has emerged as a privacy-preserving machine learning paradigm facilitating collaborative training across multiple clients without sharing local data. Despite advancements in edge device capabilities, communication bottlenecks present challenges in aggregating a large number of clients; only a portion of the clients can update their parameters upon each global aggregation. This phenomenon introduces the critical challenge of stragglers in FL and the profound impact of client scheduling policies on global model convergence and stability. Existing scheduling strategies address staleness but predominantly focus on either timeliness or content. Motivated by this, we introduce the novel concept of Version Age of Information (VAoI) to FL. Unlike traditional Age of Information metrics, VAoI considers both timeliness and content staleness. Each client's version age is updated discretely, indicating the freshness of information. VAoI is incorporated into the client schedu
    
[^99]: 现在所有人都修剪：仅使用前向传递的LLM结构化修剪

    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes

    [https://arxiv.org/abs/2402.05406](https://arxiv.org/abs/2402.05406)

    本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。

    

    鉴于非专业从业者和最富有资源的机构之间的硬件差距，尺寸不断增长的LLM变得越来越难以使用。虽然提出了许多方法来压缩LLM，以使其资源消耗可管理，但这些方法本身往往耗费资源，使其目标用户群无法接触到。在这项工作中，我们探讨了仅使用前向传递的LLM结构化修剪问题。我们希望让从业者能够修剪模型，使其规模大到硬件仅有足够的内存来运行推理。我们开发了Bonsai，这是一种无梯度、扰动修剪方法，能够生成小、快和准确的修剪模型。我们观察到，Bonsai生成的修剪模型（i）优于更昂贵的梯度-based结构化修剪方法生成的模型，并且（ii）与半结构化修剪模型相比，速度快一倍且准确性相当。

    Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
    
[^100]: 自适应激活函数在稀疏实验数据预测建模中的应用

    Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data

    [https://arxiv.org/abs/2402.05401](https://arxiv.org/abs/2402.05401)

    本研究通过研究自适应激活函数在稀疏实验数据预测建模中的应用，填补了当前对其影响了解不足的重要差距，并揭示出个体可调参数的自适应激活函数在预测模型中具有较高的准确性。

    

    神经网络设计的关键在于选择激活函数，用于引入能够捕捉复杂输入-输出模式的非线性结构。虽然在具有充足数据的领域（例如图像分类问题）中研究了自适应或可调激活函数的有效性，但在数据有限的情况下，对其对分类准确性和预测不确定性的影响仍然存在重大差距。本研究旨在通过研究两种类型的自适应激活函数来填补这些差距。这些函数在每个隐藏层中引入了共享和个体可调参数，并在包含少于一百个训练实例的三个测试平台中进行了探究。我们的研究揭示了个体可调参数的自适应激活函数（例如指数线性单位（ELU）和软加）在预测模型中具有较高的准确性。

    A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. This research aims to address these gaps by investigating the use of two types of adaptive activation functions. These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in acc
    
[^101]: 在类别不平衡数据上通过训练一系列损失函数来优化ROC曲线

    Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions

    [https://arxiv.org/abs/2402.05400](https://arxiv.org/abs/2402.05400)

    通过训练一系列损失函数来优化类别不平衡数据上的ROC曲线，并减少对超参数选择的敏感性。

    

    虽然二元分类在计算机视觉领域已经得到了深入研究，但在严重类别不平衡的情况下训练可靠的分类器仍然是一项具有挑战性的问题。最近的研究提出了通过修改损失函数或优化方法来减轻在不平衡情况下训练的影响的技术。虽然这些研究在多类别情况下整体准确率有了显著改进，但我们观察到这些方法的超参数值的微小变化可能导致在严重不平衡的二元问题上以ROC曲线为指标的性能高度变化。为了降低对超参数选择的敏感性，训练更通用的模型，我们提出了在一系列损失函数上训练，而不是单一损失函数。我们开发了一种在类别不平衡分类问题上应用损失条件训练（Loss Conditional Training，LCT）的方法。

    Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our m
    
[^102]: TASER: 时间自适应采样的快速准确动态图表示学习

    TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning

    [https://arxiv.org/abs/2402.05396](https://arxiv.org/abs/2402.05396)

    该论文提出了TASER方法，它是针对动态图表示学习的时间自适应采样技术，在准确性、效率和可扩展性方面进行了优化，解决了现实世界动态图中存在的噪声问题。

    

    最近，时间图神经网络（TGNN）在包括欺诈检测和内容推荐在内的各种重要应用中展示出了最先进的性能。尽管TGNN取得了成功，但它们容易受到现实世界动态图中普遍存在的噪声的影响，例如时间过时的链接和偏斜的交互分布。这些噪声导致两个关键问题，严重损害了TGNN的准确性：（1）模型受到较差交互的监督，（2）噪声输入导致聚合消息的高方差。然而，目前的TGNN去噪技术并未考虑每个节点的多样化和动态的噪声模式。此外，它们还面临着遍历更多邻居导致产生过多小批量的开销。我们相信快速准确的TGNN的解决方法在于时间自适应采样。在这项工作中，我们提出了TASER，这是第一个针对准确性、效率和可扩展性进行优化的TGNN自适应采样方法。

    Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
    
[^103]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^104]: 基于混合聚类条件专家的任务定制化遮蔽自编码器

    Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts

    [https://arxiv.org/abs/2402.05382](https://arxiv.org/abs/2402.05382)

    提出了一种基于混合聚类条件专家的任务定制化遮蔽自编码器(MoCE)方法，通过使用聚类条件门将每个专家只训练与语义相关的图像，为不同的下游任务提供定制的预训练模型，取得了比传统MAE更好的性能。

    

    遮蔽自编码器(MAE)是一种流行的自监督学习方法，可以在模型预训练中取得有希望的结果。然而，当各种下游任务的数据分布与预训练数据不同时，语义不相关的预训练信息可能导致负迁移，制约了MAE的可扩展性。为了解决这个问题，我们提出了一种新颖的基于MAE的预训练范式，即混合聚类条件专家(MoCE)，它可以仅训练一次，但为不同的下游任务提供定制的预训练模型。与专家混合模型(MoE)不同，我们的MoCE使用聚类条件门只训练每个专家以语义相关的图像。因此，每个下游任务可以分配到其与下游数据最相似的数据预训练模型。在11个下游任务的实验中，MoCE的平均性能比普通MAE提高了2.45%。它还达到了新的最先进水平。

    Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art s
    
[^105]: 对角费舍尔信息矩阵估计器的权衡

    Tradeoffs of Diagonal Fisher Information Matrix Estimators

    [https://arxiv.org/abs/2402.05379](https://arxiv.org/abs/2402.05379)

    本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。

    

    费舍尔信息矩阵描述了神经网络参数空间中的局部几何性质，它提供了理论和工具来理解和优化神经网络。鉴于其计算成本高，实践者通常使用随机估计器，并仅评估对角线条目。我们研究了两种这样的估计器，其准确性和样本复杂性取决于它们关联的方差。我们推导了方差的界限，并在回归和分类网络中实例化它们。我们通过分析和数值研究来权衡这两个估计器。我们发现方差量取决于关于不同参数组的非线性，当估计费舍尔信息时不能忽视它们。

    The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
    
[^106]: 物理层安全在多用户灵活双工网络中的图神经网络研究

    Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks

    [https://arxiv.org/abs/2402.05378](https://arxiv.org/abs/2402.05378)

    本文研究了多用户灵活双工网络中的物理层安全问题，提出了基于图神经网络的无监督学习策略，并通过大量数值模拟验证了其在性能和时间复杂度方面的优势。

    

    本文研究了在灵活双工网络中的物理层安全（PLS），考虑到窃听者的情景。我们的研究围绕着求解总保密速率最大化问题的复杂性展开，特别是面对采用最小均方误差（MMSE）接收器的协调和分布式窃听者时。我们的贡献包括基于迭代经典优化解和基于图神经网络（GNN）的无监督学习策略。据我们所知，这项工作标志着GNN在PLS应用上的初步探索。此外，我们将GNN方法扩展到解决窃听者的信道信息缺失问题。广泛的数值模拟结果显示，在性能和时间复杂度方面，灵活双工通信优于半双工（HD）通信，GNN方法优于传统方法。

    This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers. Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver. Our contributions include an iterative classical optimization solution and an unsupervised learning strategy based on Graph Neural Networks (GNNs). To the best of our knowledge, this work marks the initial exploration of GNNs for PLS applications. Additionally, we extend the GNN approach to address the absence of eavesdroppers' channel knowledge. Extensive numerical simulations highlight FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's superiority over the classical method in both performance and time complexity.
    
[^107]: 使用神经网络集成的降阶建模方法研究非定常流体流动

    Reduced-order modeling of unsteady fluid flow using neural network ensembles

    [https://arxiv.org/abs/2402.05372](https://arxiv.org/abs/2402.05372)

    本论文关注使用神经网络集成方法进行非定常流体流动的降阶建模。研究使用卷积自编码器处理空间分布的数据，并使用长短期记忆网络进行时间序列预测。同时，引入集成学习技术中的装袋方法来解决错误传播的问题。

    

    深度学习在降阶模型中的应用越来越普遍，以便获得完全模型的低维表示。在处理空间分布的数据（包括偏微分方程的解）时，通常使用卷积自编码器（CAEs）。在应用于非定常物理问题时，降阶模型还需要对低维潜变量的时间序列进行预测。长短期记忆（LSTM）网络是一种适用于建模序列数据的循环神经网络，在数据驱动的降阶模型中经常用于自回归时间序列预测。在未知设计点上进行长时间跨度的预测时，经常会遇到错误传播的问题，即早期错误可能会随着时间的推移而不断累积并导致较大的不准确性。在本研究中，我们提出使用集成学习技术中常用的装袋方法来开发一个完全数据驱动的降阶模型。

    The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM f
    
[^108]: 作为时间序列预测的稳健表示的注意力

    Attention as Robust Representation for Time Series Forecasting

    [https://arxiv.org/abs/2402.05370](https://arxiv.org/abs/2402.05370)

    在时间序列预测中，我们提出的方法将注意力权重提升为主要表示，使用全局标志和局部窗口构建的注意力图作为稳健核表示来克服噪声和分布变化，并取得了比现有模型更好的性能 improvement.

    

    时间序列预测在许多实际应用中至关重要，由于Transformer模型在自然语言处理和计算机视觉方面的优秀性能，其在时间序列预测中的应用逐渐增多。Transformers的关键特性，注意力机制，动态地融合嵌入以增强数据表示，通常将注意力权重作为副产品。然而，时间序列数据具有噪声和非平稳性，给预测带来了重大挑战。我们的方法将注意力权重提升为时间序列的主要表示，利用数据点之间的时间关系来改善预测准确性。我们的研究表明，使用全局标志和局部窗口构建的注意力图充当数据点的稳健核表示，能够抵抗噪声和分布移动。我们的方法在多变量时间序列预测中，将均方误差(MSE)显著降低了3.6%，超过了现有模型的表现，而不改变核心

    Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core
    
[^109]: 以显式奖励的噪声对比对齐语言模型

    Noise Contrastive Alignment of Language Models with Explicit Rewards

    [https://arxiv.org/abs/2402.05369](https://arxiv.org/abs/2402.05369)

    本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。

    

    用户意图通常被形式化为需要在微调语言模型时最大化的评估奖励。现有的对齐方法，如直接优化偏好（DPO），主要适用于隐含定义而非明确给定奖励的两两偏好数据。在本文中，我们引入了一个通用的LM对齐框架，利用噪声对比估计（NCE）来解决明确注释有标量评估的奖励数据处理的差距。我们的框架包括两个并行算法，NCA和InfoNCA，两者都能从奖励数据和偏好数据中直接提取LM策略。值得注意的是，我们证明了DPO损失是我们提出的InfoNCA目标在两两偏好设置下的特殊情况，从而集成和扩展了当前的对齐理论。通过对比NCA和InfoNCA，我们展示了InfoNCA和DPO如何在不同响应对于单个指令的相对可能性上进行调整。

    User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
    
[^110]: 基于原则的优先贝叶斯优化

    Principled Preferential Bayesian Optimization

    [https://arxiv.org/abs/2402.05367](https://arxiv.org/abs/2402.05367)

    本文提出了基于原则的优先贝叶斯优化方法，通过利用偏好反馈构建黑盒函数的置信区间，并开发了一个乐观算法来解决问题。实验证明，该方法在遗憾界限和收敛性上具有显著的性能优势。

    

    本文研究了优先贝叶斯优化（BO）问题，其中我们希望仅凭偏好反馈来优化黑盒函数的一对候选解。受到似然比思想的启发，我们使用仅凭偏好反馈构建黑盒函数的置信区间。然后，我们开发了一种乐观算法和高效的计算方法来解决这个问题，它在累积遗憾上具有信息论的界限，这在优先BO中是首次。这个界限进一步允许我们设计一个方案来报告最佳解的估计值，并保证收敛速率。从高斯过程、标准测试函数和一个热舒适度优化问题的实验结果都表明，我们的方法相对于现有的最先进的启发式算法来说，稳定地达到更好或有竞争力的性能。而现有启发式算法在遗憾界限或收敛性上没有理论保证。

    We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence.
    
[^111]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^112]: 探索用于下游数据修剪的学习复杂性

    Exploring Learning Complexity for Downstream Data Pruning

    [https://arxiv.org/abs/2402.05356](https://arxiv.org/abs/2402.05356)

    本文提出了一种将学习复杂性作为分类和回归任务的评分函数，以解决在有限计算资源下微调过程中过度参数化模型的问题。

    

    过度参数化的预训练模型对于有限的计算资源的微调构成了巨大的挑战。一个直观的解决方案是从微调数据集中修剪掉信息较少的样本。提出了一系列基于训练的评分函数来量化数据子集的信息性，但由于参数更新的繁重，修剪成本变得不可忽视。为了高效修剪，将几何方法的相似度评分函数从基于训练的方法适应为无需训练的方法是可行的。然而，我们凭经验证明这种适应扭曲了原始的修剪并导致下游任务表现不佳。在本文中，我们提出将学习复杂性（LC）作为分类和回归任务的评分函数。具体来说，学习复杂性被定义为具有不同容量的子网络的平均预测置信度，它包含了在一个收敛模型中的数据处理。

    The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we
    
[^113]: 在联邦学习遇到嘈杂标签时重温早期学习正则化

    Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels

    [https://arxiv.org/abs/2402.05353](https://arxiv.org/abs/2402.05353)

    本文提出了一种称为联邦标签混合正则化（FLR）的策略，在联邦学习中解决标签噪音问题。FLR通过生成新的伪标签，将局部和全局模型预测相结合，提高了全局模型的准确性，并有效对抗噪音标签的记忆化。FLR与现有的标签噪音和联邦学习技术兼容，为处理充满标签不准确性的FL环境提供了改进的泛化能力。

    

    在联邦学习 (FL) 不断发展的环境中，由于客户端数据的分散和多样性，处理标签噪音带来了独特的挑战。传统的集中式学习方法在FL中受到隐私问题和客户端数据的异质性的限制，无法有效缓解标签噪音。本文重新审视了早期学习正则化，引入了一种创新的策略，即联邦标签混合正则化（FLR）。FLR通过生成新的伪标签，将局部和全局模型预测相结合，灵活适应FL的复杂性。该方法不仅提高了全局模型在i.i.d.和非i.i.d.设置下的准确性，还有效地抵抗了噪音标签的记忆化。FLR与现有的标签噪音和FL技术相兼容，为在充满标签不准确性的FL环境中改善泛化能力铺平了道路。

    In the evolving landscape of federated learning (FL), addressing label noise presents unique challenges due to the decentralized and diverse nature of data collection across clients. Traditional centralized learning approaches to mitigate label noise are constrained in FL by privacy concerns and the heterogeneity of client data. This paper revisits early-learning regularization, introducing an innovative strategy, Federated Label-mixture Regularization (FLR). FLR adeptly adapts to FL's complexities by generating new pseudo labels, blending local and global model predictions. This method not only enhances the accuracy of the global model in both i.i.d. and non-i.i.d. settings but also effectively counters the memorization of noisy labels. Demonstrating compatibility with existing label noise and FL techniques, FLR paves the way for improved generalization in FL environments fraught with label inaccuracies.
    
[^114]: KIX: 一种元认知泛化框架

    KIX: A Metacognitive Generalization Framework

    [https://arxiv.org/abs/2402.05346](https://arxiv.org/abs/2402.05346)

    人工智能代理缺乏通用行为，需要利用结构化知识表示。该论文提出了一种元认知泛化框架KIX，通过与对象的交互学习可迁移的交互概念和泛化能力，促进了知识与强化学习的融合，为实现人工智能系统的自主和通用行为提供了潜力。

    

    人类和其他动物能够灵活解决各种任务，并且能够通过重复使用和应用长期积累的高级知识来适应新颖情境，这表现了一种泛化智能行为。但是人工智能代理更多地是专家，缺乏这种通用行为。人工智能代理需要理解和利用关键的结构化知识表示。我们提出了一种元认知泛化框架，称为Knowledge-Interaction-eXecution (KIX)，并且认为通过与对象的交互来利用类型空间可以促进学习可迁移的交互概念和泛化能力。这是将知识融入到强化学习中的一种自然方式，并有望成为人工智能系统中实现自主和通用行为的推广者。

    Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
    
[^115]: 在无拟样似推断的干扰参数和广义标签转移下的分类问题

    Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference

    [https://arxiv.org/abs/2402.05330](https://arxiv.org/abs/2402.05330)

    该论文提出了一种在广义标签转移和干扰参数下进行分类的新方法。通过将分类问题视为带有干扰参数的假设检验问题，我们能够有效地估计分类器在整个干扰参数空间中的接收者操作特性，并设计出在广义标签转移下不变的截断点，从而实现了对事件的可靠分类和不确定性度量。

    

    在我们的训练数据和目标数据之间，标签和潜在干扰参数的分布不同的情况下，如何以可靠的不确定性度量对事件进行分类是一个科学挑战。我们将这种分布转移称为广义标签转移 (GLS)。直接使用观测数据 $\mathbf{X}$ 进行分类会导致预测结果偏差和标签 $Y$ 的无效不确定性估计。我们通过将分类问题视为带有干扰参数的假设检验问题来克服这些偏差。关键思想是在整个干扰参数空间中估计分类器的接收者操作特性 (ROC)，这使我们能够设计在 GLS 下不变的截断点。我们的方法有效地赋予预训练的分类器领域适应能力，并返回有效的预测集合，同时保持有效的不确定性估计。

    An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maint
    
[^116]: 在多模态图上学习：一项综述

    Learning on Multimodal Graphs: A Survey

    [https://arxiv.org/abs/2402.05322](https://arxiv.org/abs/2402.05322)

    这篇综述论文对多模态图学习的已有工作进行了对比分析，阐明了多模态学习的方式和主流技术特点，并揭示了其重要应用和未来方向。

    

    多模态数据广泛存在于各个领域，包括医疗保健、社交媒体和交通运输等，其中多模态图发挥着关键作用。机器学习在多模态图上的应用，被称为多模态图学习 (MGL)，对于成功的人工智能 (AI) 应用至关重要。这篇综述文章对多模态图学习中已有的工作进行了对比分析，阐明了在不同图类型上实现多模态学习的方式，并探索了主流学习技术的特点。此外，我们还揭示了多模态图学习的重要应用，并对该领域的未来方向提供了深入见解。因此，本文为希望了解现有 MGL 技术及其在不同场景中适用性的研究人员提供了基础资源。

    Multimodal data pervades various domains, including healthcare, social media, and transportation, where multimodal graphs play a pivotal role. Machine learning on multimodal graphs, referred to as multimodal graph learning (MGL), is essential for successful artificial intelligence (AI) applications. The burgeoning research in this field encompasses diverse graph data types and modalities, learning techniques, and application scenarios. This survey paper conducts a comparative analysis of existing works in multimodal graph learning, elucidating how multimodal learning is achieved across different graph types and exploring the characteristics of prevalent learning techniques. Additionally, we delineate significant applications of multimodal graph learning and offer insights into future directions in this domain. Consequently, this paper serves as a foundational resource for researchers seeking to comprehend existing MGL techniques and their applicability across diverse scenarios.
    
[^117]: 航行知识之海：利用LLMs进行行星级答案检索

    Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs

    [https://arxiv.org/abs/2402.05318](https://arxiv.org/abs/2402.05318)

    本文探讨了信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用，LLMs的整合标志着用户与信息系统交互方式的范式转变。

    

    信息检索是一个快速发展的信息检索领域，其特点是从基本的超链接导航到复杂的算法驱动搜索引擎的不断改进。本文旨在全面介绍信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用。LLMs在响应检索和索引领域的整合标志着用户与信息系统交互方式的范式转变。这种范式转变是由像GPT-4这样的大型语言模型的整合驱动的，它们能够理解和生成类似于人类的文本，从而能够提供更直接和情境相关的答案给用户查询。通过这种探索，我们试图阐明技术里程碑。

    Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones 
    
[^118]: 研究生成流网络的泛化行为

    Investigating Generalization Behaviours of Generative Flow Networks

    [https://arxiv.org/abs/2402.05309](https://arxiv.org/abs/2402.05309)

    本研究通过实证验证了生成流网络(GFlowNets)的一些泛化机制假设，发现它们学习逼近的函数具有隐含的基础结构，有助于泛化。同时，GFlowNets对于离线和离策略训练敏感，但隐含学习的奖励对训练分布的变化具有鲁棒性。

    

    生成流网络（GFlowNets，GFNs）是一种用于学习离散空间上非归一化概率质量函数的生成框架。自从它们问世以来，GFlowNets在学习生成模型方面表现出色，特别适用于训练期间大部分离散空间未被访问的应用。这使一些人假设当GFlowNets与深度神经网络（DNNs）配对时，具有良好的泛化性能。本文通过实证验证了GFlowNets的一些泛化机制假设。特别地，我们发现GFlowNets学习逼近的函数具有隐含的基础结构，有助于泛化。我们还发现GFlowNets对于离线和离策略训练很敏感，然而，GFlowNets隐含学习的奖励对训练分布的变化具有鲁棒性。

    Generative Flow Networks (GFlowNets, GFNs) are a generative framework for learning unnormalized probability mass functions over discrete spaces. Since their inception, GFlowNets have proven to be useful for learning generative models in applications where the majority of the discrete space is unvisited during training. This has inspired some to hypothesize that GFlowNets, when paired with deep neural networks (DNNs), have favourable generalization properties. In this work, we empirically verify some of the hypothesized mechanisms of generalization of GFlowNets. In particular, we find that the functions that GFlowNets learn to approximate have an implicit underlying structure which facilitate generalization. We also find that GFlowNets are sensitive to being trained offline and off-policy; however, the reward implicitly learned by GFlowNets is robust to changes in the training distribution.
    
[^119]: 具有可解释模型和策略网络的神经符号强化学习的三个路径

    Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks

    [https://arxiv.org/abs/2402.05307](https://arxiv.org/abs/2402.05307)

    本文探讨了实现具有可解释性的模型和策略的神经符号强化学习的三个路径，并揭示了学习的连续性和可微性的益处，以及将逻辑与数值仿真结合的难点。

    

    神经符号人工智能将经典符号方法的可解释性、简约性和显式推理与数据驱动的神经方法的统计学习相结合。同时可微和可解释的模型和策略可能是这种结合的关键。本文在真实世界的强化学习环境中展示了实现这些模型和策略的三种路径。具体而言，我们研究了一类广泛的神经网络，这些网络在其架构中直接构建可解释的语义。我们揭示和强调了将逻辑、仿真和学习结合起来的潜力和基本困难。一个教训是学习受益于连续性和可微性，但经典逻辑是离散且不可微的。将逻辑松弛为实值的可微表示存在一个权衡；越可学习，越不可解释。另一个教训是在数值仿真环境中使用逻辑

    Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical simulati
    
[^120]: Sym-Q：通过顺序决策进行自适应符号回归

    Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making

    [https://arxiv.org/abs/2402.05306](https://arxiv.org/abs/2402.05306)

    Sym-Q是一个基于强化学习的模型，通过将符号回归重新定义为顺序决策任务来解决现有模型在泛化性和适应性方面的挑战。通过利用监督演示和奖励信号，Sym-Q能够根据拟合精度的质量改进表达式。

    

    符号回归具有从实证数据中揭示潜在数学和物理关系的巨大潜力。虽然现有的基于Transformer的模型在这个领域取得了显著成功，但它们在泛化性和适应性方面面临挑战。通常，当输出表达式不足以适应实验数据时，这些模型缺乏有效的机制来适应或修改表达式。这种缺乏灵活性限制了它们在实际场景中的应用，特别是在发现未知的物理或生物关系方面。受到人类专家如何改进和调整表达式的启发，我们引入了一种新颖的基于强化学习的模型Symbolic Q-network（Sym-Q），将符号回归重新定义为顺序决策任务。Sym-Q利用监督演示并根据奖励信号来改进表达式，奖励信号指示拟合精度的质量。它独特的能力可以处理复杂性。

    Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexi
    
[^121]: BIKED++：一个包含140万个自行车图像和参数化CAD设计的多模态数据集

    BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs

    [https://arxiv.org/abs/2402.05301](https://arxiv.org/abs/2402.05301)

    本文介绍了BIKED++数据集，其中包含了140万个自行车设计的图像和参数化CAD文件。该数据集可以用于训练跨模态预测模型，例如使用参数化表示来准确估计图像的特征嵌入。该数据集也已公开，可供研究者使用。

    

    本文介绍了一个公开数据集，包含了140万个通过参数化表示和JSON文件以及栅格化图像生成的自行车设计。该数据集是通过渲染引擎和BikeCAD软件生成参数化设计的矢量图形而创建的。本文还公开了该渲染引擎。该数据集具有多种应用，其中一个主要目标是训练参数化和基于图像的设计表示之间的跨模态预测模型。例如，我们证明可以通过训练预测模型直接从参数化表示准确估计对比语言-图像预训练（CLIP）嵌入。这样可以建立参数化自行车设计与文本字符串或参考图像之间的相似关系。经过训练的预测模型也已公开。该数据集加入了BIKED数据集系列。

    This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
    
[^122]: 使用凝聚层次聚类和基于主题的方法对垃圾邮件进行分类

    Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach

    [https://arxiv.org/abs/2402.05296](https://arxiv.org/abs/2402.05296)

    该论文提出了使用凝聚层次聚类和基于主题的方法对垃圾邮件进行分类的新思路。作者提出了两个新的数据集SPEMC-15K-E和SPEMC-15K-S，并使用凝聚层次聚类将其划分为11个类别。实验结果表明，TF-IDF和LR在英文数据集中达到最佳性能。

    

    垃圾邮件是未经请求的恼人且有时有害的消息，可能含有恶意软件、钓鱼或恶作剧。与大多数研究解决高效反垃圾邮件过滤器设计的问题不同，我们从不同和新颖的角度来解决垃圾邮件问题。重点关注网络安全单位的需求，我们采用基于主题的方法来对垃圾邮件进行多类别分类。我们提出了两个新颖的数据集SPEMC-15K-E和SPEMC-15K-S，分别包含大约15K封英文和西班牙文邮件，并使用凝聚层次聚类将其划分为11个类别。我们评估了16种流水线，结合了四种文本表示技术-词频-逆文档频率（TF-IDF）、词袋模型、Word2Vec和BERT-以及四种分类器：支持向量机、朴素贝叶斯、随机森林和逻辑回归。实验结果表明，在英文数据集中，TF-IDF和LR的性能最好，

    Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine, N\"aive Bayes, Random Forest and Logistic Regression. Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, 
    
[^123]: 一种量化特征选择和排序算法稳定性的信息论方法

    An information theoretic approach to quantify the stability of feature selection and ranking algorithms

    [https://arxiv.org/abs/2402.05295](https://arxiv.org/abs/2402.05295)

    本论文提出了一种基于信息论的方法来量化特征选择和排序算法的稳定性。该方法能够评估不同算法结果中的特征排序的稳定性，包括完整的排名列表、特征子集和部分排名列表。

    

    特征选择是处理高维数据时的关键步骤。特别是这些技术通过从嘈杂、冗余和无关的特征中选择最相关的特征，简化了从数据中发现知识的过程。在许多实际应用中出现的一个问题是特征选择算法的结果是不稳定的。因此，数据的微小变化可能导致非常不同的特征排序。在上述情况中，评估这些方法的稳定性成为一个重要问题。我们提出了一种基于Jensen Shannon距离的信息论方法来量化这种鲁棒性。与其他稳定性度量不同，这个度量指标适用于不同的算法结果：完整的排名列表、特征子集以及较少研究的部分排名列表。这个广义度量以概率方法量化了相同大小的整个列表集的差异，并提供了反映其中的排序稳定性的结果。

    Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and
    
[^124]: 检验医疗视觉和基于语言的疾病检测的多模态联邦学习中的模态不一致性

    Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection

    [https://arxiv.org/abs/2402.05294](https://arxiv.org/abs/2402.05294)

    本文首次分析了多模态联邦学习中的模态不一致性的影响，并揭示了其与参与客户端之间的数据异质性的联系。通过使用不考虑不一致性的信息融合机制和模态插值网络，在解决模态不一致性问题方面取得了一定的成果。

    

    多模态联邦学习（MMFL）利用每个客户端中的多个模态构建比其单模态对应物更强大的联邦学习（FL）模型。然而，不同客户端缺失模态的影响，也称为模态不一致性，一直被大大忽视。本文首次分析了模态不一致性的影响，并揭示了其与参与客户端之间的数据异质性的联系。我们特别检查了不一致的MMFL与单模态和多模态客户端相比是否更有益于单模态FL。此外，我们还研究了解决这个问题的三个潜在途径。首先，我们研究了各种自注意机制对于不考虑不一致性的信息融合在MMFL中的有效性。其次，我们在多模态客户端中引入了一个预先训练的模态插值网络（MIN）来解决单模态客户端中的模态翻译问题，并研究其减轻缺失模态问题的潜力。第三，我们...

    Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly
    
[^125]: 一项关于结直肠癌风险预测模型特征选择的比较研究

    A comparative study on feature selection for a risk prediction model for colorectal cancer

    [https://arxiv.org/abs/2402.05293](https://arxiv.org/abs/2402.05293)

    这项研究比较了不同特征选择算法在结直肠癌风险预测模型中的性能，并提出了视觉方法评估特征排序技术的稳定性。

    

    背景和目标  风险预测模型旨在识别患上某种疾病风险更高的人群。特征选择对于改善预测模型的性能、避免过拟合以及识别导致癌症风险（和保护）的因素尤为重要。对特征选择/排序算法的稳定性评估成为分析具有更多预测能力特征的重要问题。方法 本研究集中在结直肠癌上，评估了几种特征排序算法在一组风险预测模型（神经网络、支持向量机（SVM）、逻辑回归、k-最近邻和提升树）方面的性能。此外，使用标量稳定度指标和本文提出的视觉方法评估了其稳定性，既可以研究特征排序技术之间的相似性，也可以研究它们的单独稳定性。

    Background and objective   Risk prediction models aim at identifying people at higher risk of developing a target disease. Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors. Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power. Methods   This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest Neighbors and Boosted Trees). Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability. A comparative analysis is 
    
[^126]: 图神经网络作为有限元冰盖模拟的快速高保真度的仿真器

    Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling

    [https://arxiv.org/abs/2402.05291](https://arxiv.org/abs/2402.05291)

    本研究开发了图神经网络作为有限元冰盖模拟的快速高保真度的仿真器，并在Pine Island Glacier的瞬态模拟中展示了与传统卷积神经网络和多层感知器相比更准确的重现冰盖厚度和速度的能力。同时，这些GNN成功捕捉到了更高底部熔化速率引起的冰质量损失和加速过程。在图形处理单元上实现的GNN仿真器显示出高达50倍的加速。

    

    虽然冰盖和海平面系统模型（ISSM）的有限元方法可以快速准确地解决由Stokes方程描述的冰动力学问题，但这种数值建模需要在中央处理单元（CPU）上进行密集的计算。在本研究中，我们开发了图神经网络（GNN）作为快速代理模型来保持ISSM的有限元结构。利用Pine Island Glacier（PIG）的20年瞬态模拟，我们训练和测试了三个GNN：图卷积网络（GCN），图注意力网络（GAT）和等变图卷积网络（EGCN）。这些GNN与经典卷积神经网络（CNN）和多层感知器（MLP）相比，能够更准确地重现冰厚度和速度。特别是，在PIG中，GNN成功捕捉到了由更高底部熔化速率引起的冰质量损失和加速。当我们的GNN仿真器在图形处理单元（GPU）上实现时，它们显示出高达50倍的加速。

    Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN). These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP). In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster c
    
[^127]: 变形器世界模型是否可以给出更好的策略梯度？

    Do Transformer World Models Give Better Policy Gradients?

    [https://arxiv.org/abs/2402.05290](https://arxiv.org/abs/2402.05290)

    在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。

    

    对于强化学习来说，一种自然的方法是通过展开神经网络世界模型来预测未来的奖励，并通过计算图进行反向传播以学习策略。然而，由于典型的世界模型产生了难以优化的损失地形，这种方法在长时间跨度上通常变得不可行。变形器已知可以高效地传播长时间跨度的梯度：它们是否可以解决这个问题呢？令人惊讶的是，我们发现常用的变形器世界模型会产生迂回的梯度路径，这对于长距离的策略梯度是有害的。为了应对这个挑战，我们提出了一类称为Actions World Models (AWMs)的世界模型，旨在提供更直接的梯度传播路径。我们将这种AWMs集成到一个策略梯度的框架中，强调了网络架构与策略梯度更新之间的关系。我们证明了AWMs可以产生可优化的梯度路径。

    A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
    
[^128]: 分析深度强化学习中的对抗输入

    Analyzing Adversarial Inputs in Deep Reinforcement Learning

    [https://arxiv.org/abs/2402.05284](https://arxiv.org/abs/2402.05284)

    这篇论文通过形式验证的视角，分析了深度强化学习中对抗输入的特征，并提出了一个新的度量标准——对抗率，以及计算该度量标准的一套工具和算法。

    

    近年来，深度强化学习（DRL）由于在实际和复杂系统中取得的成功应用而成为机器学习中受欢迎的范例。然而，即使最先进的DRL模型也被证明存在可靠性问题，例如对抗输入的敏感性，即小型且大量的输入扰动会导致模型做出不可预测且潜在危险的决策。这个缺点限制了DRL系统在安全关键环境中的部署，即使是小的错误都是不可容忍的。在这项工作中，我们通过形式验证的视角提出了对对抗输入进行分类的新度量标准——对抗率，并提出了一套用于计算对抗率的工具和算法。我们的分析通过实验证明了对抗输入对DRL模型的影响。

    In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect th
    
[^129]: 无维度抽样核心子集用于分类问题

    No Dimensional Sampling Coresets for Classification

    [https://arxiv.org/abs/2402.05280](https://arxiv.org/abs/2402.05280)

    本文通过敏感性抽样框架提出了无维度的核心子集用于分类问题，该子集的大小与维度无关，并适用于各种损失函数和分布输入。

    

    我们通过敏感性抽样框架对于分类问题的核心子集的已知内容进行了精炼和概括。这种核心子集寻求输入数据的最小可能子集，以便可以在核心子集上优化损失函数，并确保对于原始数据的逼近保证。我们的分析提供了第一个无维度核心子集，因此大小与维度无关。此外，我们的结果是通用的，适用于分布输入并且可以使用独立同分布样本，因此可以提供样本复杂度边界，并适用于各种损失函数。我们开发的关键工具是主要敏感性抽样方法的Radamacher复杂度版本，这可能是一个独立感兴趣的工具。

    We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework. Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data. Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension. Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions. A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest.
    
[^130]: 通过学习判别超平面实现黑盒动态系统的安全过滤器

    Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes

    [https://arxiv.org/abs/2402.05279](https://arxiv.org/abs/2402.05279)

    本文提出一种基于学习的方法，通过定义判别超平面来实现黑盒动态系统的安全过滤器。我们的方法不仅可以消除对特定证明函数的依赖，还简化了安全过滤器的设计。

    

    基于学习的方法正在成为黑盒动态系统的安全过滤器的有效途径。现有方法依赖证明函数（如控制界面函数（CBFs）和哈密顿-雅可比（HJ）可达性值函数）。我们工作的主要动机是认识到最终将安全约束作为每个状态的控制输入约束来强制执行才是最重要的。通过专注于这个约束，我们可以消除对任何特定证明函数设计的依赖性。为了实现这一点，我们定义了一个判别超平面，用于在每个状态上形成控制输入的半空间约束，作为安全的充分条件。这个概念不仅广泛适用传统的安全方法，而且通过消除对特定证明函数的依赖，简化了安全过滤器的设计。我们提出了两种学习判别超平面的策略：（a）监督学习方法，使用预先验证的证明函数信息来训练模型；（b）加速回归的方法，通过迭代生成判别超平面。

    Learning-based approaches are emerging as an effective approach for safety filters for black-box dynamical systems. Existing methods have relied on certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi (HJ) reachability value functions. The primary motivation for our work is the recognition that ultimately, enforcing the safety constraint as a control input constraint at each state is what matters. By focusing on this constraint, we can eliminate dependence on any specific certificate function-based design. To achieve this, we define a discriminating hyperplane that shapes the half-space constraint on control input at each state, serving as a sufficient condition for safety. This concept not only generalizes over traditional safety methods but also simplifies safety filter design by eliminating dependence on specific certificate functions. We present two strategies to learn the discriminating hyperplane: (a) a supervised learning approach, using pre-verified c
    
[^131]: 探索时间序列数据的层次分类性能: 相异度度量和分类器比较

    Exploring Hierarchical Classification Performance for Time Series Data: Dissimilarity Measures and Classifier Comparisons

    [https://arxiv.org/abs/2402.05275](https://arxiv.org/abs/2402.05275)

    本研究通过比较层次分类和平铺分类方法在时间序列数据分析中的表现，发现在使用MINIROCKET和TSD的情况下，层次分类表现出显著优势，而在使用STSF和SVM等替代方法时，平铺分类一直保持优势。TSD也在几乎所有情况下表现出比CBD和JSD更好的性能。

    

    本研究探讨了在时间序列数据分析领域中，层次分类（HC）和平铺分类（FC）方法的比较性能。使用了包括Jensen-Shannon距离（JSD）、任务相似度距离（TSD）和基于分类器的距离（CBD）等相异度度量，以及MINIROCKET、STSF和SVM等不同的分类器。采用了来自UCR存档的数据集子集，重点关注多类别情况下超过两个类别的案例进行分析。观察到一个显著趋势，即在使用TSD和MINIROCKET时，HC相较于FC表现出显著的优势，这与传统理解有所不同。相反，当使用STSF和SVM等替代分类器时，FC在所有配置下都表现出一致的优势。此外，TSD在几乎所有情况下都表现出比CBD和JSD更好的性能，唯独在涉及STSF分类器的情况下，CBD展现出优势。

    The comparative performance of hierarchical classification (HC) and flat classification (FC) methodologies in the realm of time series data analysis is investigated in this study. Dissimilarity measures, including Jensen-Shannon Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance (CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF, and SVM. A subset of datasets from the UCR archive, focusing on multi-class cases comprising more than two classes, is employed for analysis. A significant trend is observed wherein HC demonstrates significant superiority over FC when paired with MINIROCKET utilizing TSD, diverging from conventional understandings. Conversely, FC exhibits consistent dominance across all configurations when employing alternative classifiers such as STSF and SVM. Moreover, TSD is found to consistently outperform both CBD and JSD across nearly all scenarios, except in instances involving the STSF classifier where CBD showcases s
    
[^132]: 梯度下降引发了深度非线性网络权重与经验NTK之间的对齐

    Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks

    [https://arxiv.org/abs/2402.05271](https://arxiv.org/abs/2402.05271)

    了解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。前人的研究表明，在训练过程中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这被称为神经特征分析（NFA）。本研究解释了这种相关性的出现，并发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。在早期训练阶段，可以通过解析的方式预测NFA的发展速度。

    

    理解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。先前的研究已经确定，在一般结构的训练神经网络中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这个说法被称为神经特征分析（NFA）。然而，这些数量在训练过程中如何相关尚不清楚。在这项工作中，我们解释了这种相关性的出现。我们发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。我们证明了先前研究中引入的NFA是由隔离这种对齐的中心化NFA驱动的。我们还展示了在早期训练阶段，可以通过解析的方式预测NFA的发展速度。

    Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
    
[^133]: AdaBatchGrad: 组合自适应批大小和自适应步长的方法

    AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size

    [https://arxiv.org/abs/2402.05264](https://arxiv.org/abs/2402.05264)

    AdaBatchGrad是一种改进的随机梯度下降方法，通过组合自适应批大小和自适应步长来提高方法的稳健性和稳定性。

    

    本文提出了一种新颖的随机梯度下降（SGD）的改进方法，称为AdaBatchGrad。这种改进方法无缝地将自适应步长与可调整的批大小相结合。增加批大小和减小步长是紧束SGD收敛区间和减小其方差的已知技术。R. Byrd和J. Nocedal的一系列研究引入了各种测试技术来评估小批量梯度近似的质量，并在每个步骤选择合适的批量大小。采用准确测试的方法观察到在$O(LR^2/\varepsilon)$迭代内收敛。然而，不准确的测试实现有时导致不收敛和不稳定的性能。为了解决这些挑战，AdaBatchGrad结合了自适应批量和步长的方法，增强了方法的稳健性和稳定性。对于准确测试，我们的方法以$O(LR^2/\varepsilon)$迭代收敛，类似于标准梯度下降方法。

    This paper presents a novel adaptation of the Stochastic Gradient Descent (SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge within $O(LR^2/\varepsilon)$ iterations. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability. For exact tests, our approach converges in $O(LR^2/\varepsilon)$ iterations, analogous to standard gradient descen
    
[^134]: 通过可微分优化有序加权平均值学习公平排名策略

    Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages

    [https://arxiv.org/abs/2402.05252](https://arxiv.org/abs/2402.05252)

    本文介绍了一种通过优化有序加权平均值函数，在LTR模型的训练过程中集成高效的公平排名模型，实现公平性、用户效用和运行时效率之间的有利平衡。

    

    学习排序（LTR）是最广泛使用的机器学习应用之一。它是具有深远社会影响的平台的关键组成部分，包括求职搜索、医疗信息检索和社交媒体内容推送。传统的LTR模型已经显示出产生偏见结果，引发了如何解决仅优先考虑用户相关性的排名系统引入的差异的讨论。然而，虽然已经提出了几种公平学习排序模型，但它们在准确性或效率方面存在缺陷，从而限制了它们在实际排名平台中的适用性。本文展示了如何将基于有序加权平均（OWA）函数的高效可解的公平排名模型集成到LTR模型的训练循环中，以实现公平性、用户效用和运行时效率之间的有利平衡。特别是，本文首次展示了如何通过约束优化反向传播。

    Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constra
    
[^135]: QGFN:具有动作值的可控贪婪算法

    QGFN: Controllable Greediness with Action Values

    [https://arxiv.org/abs/2402.05234](https://arxiv.org/abs/2402.05234)

    本文提出了一种新的算法QGFN，通过将GFN策略与动作值估计相结合，可以生成更多高奖励的样本而不牺牲多样性。

    

    生成流网络（GFlowNets;GFNs）是一种用于组合对象的基于奖励/能量的生成方法，能够生成多样化和高效的样本。然而，偏向于生成高效样本的GFNs并不容易。在这项工作中，我们利用GFNs和强化学习（RL）之间的联系，提出将GFN策略与动作值估计$Q$相结合，从而创建可以通过混合参数控制的贪婪采样策略。我们展示了几个QGFN的变体能够在各种任务中改善生成高奖励样本的数量，同时不牺牲多样性。

    Generative Flow Networks (GFlowNets; GFNs) are a family of reward/energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.
    
[^136]: 通用神经功能

    Universal Neural Functionals

    [https://arxiv.org/abs/2402.05232](https://arxiv.org/abs/2402.05232)

    本文提出了通用神经功能（UNFs），一种能够自动构建适用于任何权重空间的置换等变模型的算法。实验结果显示，在优化小型图像分类器和语言模型时，UNFs能够取得有希望的改进，为学习优化器设计提供了新的思路。

    

    在现代许多机器学习任务中，一个具有挑战性的问题是处理权重空间特征，即从神经网络的权重和梯度中转换或提取信息。最近的研究已经开发出了一些有希望的权重空间模型，这些模型对简单的前馈网络的置换对称性是等变的。然而，它们对于普通架构并不适用，因为权重空间的置换对称性可能会因循环或残差连接而变得复杂。本文提出了一种算法，自动构建置换等变模型，我们称之为通用神经功能（UNFs），适用于任何权重空间。在其他应用中，我们展示了如何将UNFs替代现有的学习优化器设计，并在优化小型图像分类器和语言模型时发现有希望的改进。我们的结果表明，学习优化器可以从考虑（对称）结构的角度受益。

    A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the
    
[^137]: VerAs: 验证然后评估STEM实验报告

    VerAs: Verify then Assess STEM Lab Reports

    [https://arxiv.org/abs/2402.05224](https://arxiv.org/abs/2402.05224)

    VerAs是一个端到端的神经架构，用于验证和评估STEM实验报告。它通过利用多个维度的分析评估标准，以及针对学生提供详细反馈，帮助他们提高科学写作技巧。

    

    随着STEM教育对批判性思维能力的日益关注，科学写作在注重探究技能的课程中发挥着越来越重要的作用。最近发布的一份数据集是基于一套探究型物理课程的两组大学水平的实验报告，依赖于利用多个维度的分析评估标准，指定学科知识和优秀解释的一般组成部分。每个分析维度都以6分制进行评估，以提供详细反馈，帮助学生提高科学写作技巧。手动评估可能较慢，并且在大班中对所有学生进行一致性校准可能很困难。尽管在STEM学科的开放性问题的自动评估上已经有很多工作，但在实验报告等长篇写作中的工作要少得多。我们提出了一个端到端的神经架构，其中包括独立的验证器和评估模块，灵感来源于开放领域问题回答的方法。

    With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
    
[^138]: 关于偏离高斯混合专家模型参数估计的研究

    On Parameter Estimation in Deviated Gaussian Mixture of Experts

    [https://arxiv.org/abs/2402.05220](https://arxiv.org/abs/2402.05220)

    在偏离高斯混合专家模型中，本文通过构造Voronoi-based损失函数来解决参数估计问题。

    

    本文考虑了在偏离高斯混合专家模型中的参数估计问题，其中数据由$(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$生成，其中$X, Y$分别是协变量向量和响应变量，$g_{0}(Y|X)$是已知函数，$\lambda^{\ast} \in [0, 1]$是真实但未知的混合比例，$(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$对于$1 \leq i \leq k^{\ast}$是高斯混合专家模型的未知参数。该问题源自于拟合优度检验，当我们希望检验数据是否由$g_{0}(Y|X)$（零假设）生成，还是由整个混合（备择假设）生成。基于专家函数的代数结构和$g_0$与混合部分的可区分性，我们构造了新的基于Voronoi的损失函数来捕捉c

    We consider the parameter estimation problem in the deviated Gaussian mixture of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$, where $X, Y$ are respectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$ for $1 \leq i \leq k^{\ast}$ are unknown parameters of the Gaussian mixture of experts. This problem arises from the goodness-of-fit test when we would like to test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or they are generated from the whole mixture (alternative hypothesis). Based on the algebraic structure of the expert functions and the distinguishability between $g_0$ and the mixture part, we construct novel Voronoi-based loss functions to capture the c
    
[^139]: 基于自校准卷积的胶质瘤分割

    Self-calibrated convolution towards glioma segmentation

    [https://arxiv.org/abs/2402.05218](https://arxiv.org/abs/2402.05218)

    这项工作通过在nnU-Net网络的不同部分使用自校准卷积，证明了自校准模块可以显著提高肿瘤分割的准确性，同时保持整个肿瘤的分割准确性。

    

    在疾病早期，准确的脑肿瘤分割对治疗的有效性至关重要，可以避免对多种协议（如T1、T2、T2-FLAIR、T1-Gd）的3D MR脑图像进行详尽的视觉检查。存在许多用于胶质瘤分割的网络，其中nnU-Net是最好的之一。在这项工作中，我们评估了在nnU-Net网络的不同部分中使用自校准卷积，以证明在跳跃连接中使用自校准模块可以显著提高增强瘤灶和肿瘤核的分割准确性，同时保持整个肿瘤的分割准确性。

    Accurate brain tumor segmentation in the early stages of the disease is crucial for the treatment's effectiveness, avoiding exhaustive visual inspection of a qualified specialist on 3D MR brain images of multiple protocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma segmentation, being nnU-Net one of the best. In this work, we evaluate self-calibrated convolutions in different parts of the nnU-Net network to demonstrate that self-calibrated modules in skip connections can significantly improve the enhanced-tumor and tumor-core segmentation accuracy while preserving the wholetumor segmentation accuracy.
    
[^140]: 采用分割引导扩散模型的解剖可控医学图像生成

    Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models

    [https://arxiv.org/abs/2402.05210](https://arxiv.org/abs/2402.05210)

    这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。

    

    扩散模型已经实现了非常高质量的医学图像生成，可以通过为小型或不平衡的数据集提供补充，从而帮助减轻获取和注释新图像的费用，同时还可以应用于其他方面。然而，这些模型在生成图像时面临着全局解剖真实性的挑战。因此，我们提出了一种解剖可控的医学图像生成模型。我们的模型在每个采样步骤中遵循多类解剖分割掩模，并采用随机掩模消融训练算法，以实现对所选解剖约束的条件化，同时允许其他解剖区域的灵活性。这也改善了网络在完全无条件（无约束生成）情况下对解剖真实性的学习。通过对乳腺MRI和腹部/颈部到盆腔CT数据集的比较评估，证明了我们模型在解剖真实性和输入掩模保真度方面具有优越性。

    Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
    
[^141]: 贝尔曼符合推断：时间序列预测中预测区间的校准

    Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series

    [https://arxiv.org/abs/2402.05203](https://arxiv.org/abs/2402.05203)

    贝尔曼符合推断（BCI）是一个框架，通过解决一维随机控制问题，利用多步预测来提供校准的时间序列预测区间。BCI在任意分布转换和时间依赖性下实现了长期覆盖，且在波动率预测问题上生成更短的预测区间。

    

    我们引入了贝尔曼符合推断（BCI），这是一个围绕任何时间序列预测模型的框架，可以提供校准的预测区间。与现有方法不同，BCI能够利用多步预测，并通过在每个时间步骤上解决一维随机控制问题（SCP）来显式优化平均区间长度。特别地，我们使用动态规划算法来找到SCP的最优策略。我们证明了在任意分布转换和时间依赖性下，BCI能够实现长期覆盖，即使多步预测较差。我们在实证中发现，与现有方法相比，BCI避免了无信息区间（长度无限）的生成，并在波动率预测问题上生成了明显更短的预测区间。

    We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods.
    
[^142]: LLMs是否准备好应用于真实世界的材料发现？

    Are LLMs Ready for Real-World Materials Discovery?

    [https://arxiv.org/abs/2402.05200](https://arxiv.org/abs/2402.05200)

    LLMs在材料科学中的应用受限，无法实现实际应用。我们提出了基于材料科学知识和假设测试的MatSci-LLMs框架，并描述了关键的材料科学信息提取挑战。

    

    大型语言模型（LLMs）为材料科学中的强大语言处理工具提供了令人兴奋的可能性，加快了材料研究的进展。然而，LLMs在实际应用中仍存在不足，无法成为实用的材料科学工具。本文通过展示LLMs在材料科学中的相关失败案例，揭示了LLMs在理解和推理复杂、相互关联的材料科学知识方面的现有限制。鉴于这些缺点，我们提出了一种开发基于材料科学知识和假设生成与测试的材料科学LLMs（MatSci-LLMs）的框架。实现高性能的MatSci-LLMs的路径在很大程度上取决于建立高质量的多模态数据集，这些数据集来自科学文献，其中存在各种信息提取挑战。因此，我们描述了关键的材料科学信息提取挑战。

    Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
    
[^143]: JAX-Fluids 2.0：面向可压缩两相流的可微CFD的HPC

    JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows

    [https://arxiv.org/abs/2402.05193](https://arxiv.org/abs/2402.05193)

    JAX-Fluids 2.0是一个面向可压缩两相流的可微CFD的HPC求解器，通过引入高性能计算能力和增强的两相流建模能力，实现了规模化并行计算和稳定的自动微分梯度计算。

    

    为了促进机器学习辅助的计算流体力学（CFD），我们推出了第二个版本的JAX-Fluids。JAX-Fluids是一个基于Python的完全可微的CFD求解器，用于可压缩的单相和两相流。在这项工作中，我们扩展了第一个版本，引入了高性能计算（HPC）能力。我们采用利用JAX原始操作的并行化策略，在GPU（最多512个NVIDIA A100显卡）和TPU（最多1024个TPU v3核心）HPC系统上高效扩展。我们进一步展示了在扩展积分轨迹上稳定的自动微分梯度的并行计算。新的代码版本提供了增强的两相流建模能力。特别是，我们加入了一个五方程扩散界面模型，以补充水平集尖锐界面模型。另外的算法改进包括保持正性的限制器，以增加鲁棒性，支持...

    In our effort to facilitate machine learning-assisted computational fluid dynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is a Python-based fully-differentiable CFD solver designed for compressible single- and two-phase flows. In this work, the first version is extended to incorporate high-performance computing (HPC) capabilities. We introduce a parallelization strategy utilizing JAX primitive operations that scales efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024 TPU v3 cores) HPC systems. We further demonstrate the stable parallel computation of automatic differentiation gradients across extended integration trajectories. The new code version offers enhanced two-phase flow modeling capabilities. In particular, a five-equation diffuse-interface model is incorporated which complements the level-set sharp-interface model. Additional algorithmic improvements include positivity-preserving limiters for increased robustness, support f
    
[^144]: 在策略镜像下降中元学习镜像映射

    Meta-learning the mirror map in policy mirror descent

    [https://arxiv.org/abs/2402.05187](https://arxiv.org/abs/2402.05187)

    该论文通过实证研究发现，传统的镜像映射选择（NPG）在标准基准环境中常常导致不理想的结果。通过元学习方法，找到了更高效的镜像映射，提升了性能。

    

    策略镜像下降（PMD）是强化学习中的一种流行框架，作为一种统一视角，它包含了许多算法。这些算法是通过选择一个镜像映射而导出的，并且具有有限时间的收敛保证。尽管它很受欢迎，但对PMD的全面潜力的探索是有限的，大部分研究集中在一个特定的镜像映射上，即负熵，从而产生了著名的自然策略梯度（NPG）方法。目前的理论研究还不确定镜像映射的选择是否会对PMD的有效性产生重大影响。在我们的工作中，我们进行了实证研究，证明了传统的镜像映射选择（NPG）在几个标准基准环境中经常产生不理想的结果。通过应用元学习方法，我们确定了更高效的镜像映射，提高了性能，无论是平均性能还是最佳性能。

    Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along th
    
[^145]: cecilia: 一种基于机器学习的用于测量富氦污染白矮星金属丰度的流程

    cecilia: A Machine Learning-Based Pipeline for Measuring Metal Abundances of Helium-rich Polluted White Dwarfs

    [https://arxiv.org/abs/2402.05176](https://arxiv.org/abs/2402.05176)

    本研究提出了cecilia，一种基于机器学习的光谱建模代码，用于测量富氦污染白矮星的金属丰度。该流程通过训练大量基于随机抽样的模型，旨在克服传统方法的限制，并能够扩展到大规模的研究中。

    

    在过去的几十年中，传统的污染白矮星光谱分析技术已经成为了了解太阳系外星球地质和化学信息的强大工具。然而，尽管这些技术已经证明了自己的能力并具有广泛的科学发现，它们仍然受到手动、耗时且迭代性的限制。因此，它们容易出现人为错误，并且难以扩展到大规模的金属污染研究。本文通过介绍cecilia，一种基于机器学习的光谱建模代码，旨在解决这个问题。cecilia设计用于测量中温（10,000$\leq T_{\rm eff} \leq$20,000 K）、富氦污染白矮星的金属丰度。通过对超过22,000个随机绘制的大气模型和恒星参数进行训练，我们的流程旨在克服传统方法的限制，可以代替从计算昂贵的合成光谱中生成模拟光谱的步骤。

    Over the past several decades, conventional spectral analysis techniques of polluted white dwarfs have become powerful tools to learn about the geology and chemistry of extrasolar bodies. Despite their proven capabilities and extensive legacy of scientific discoveries, these techniques are however still limited by their manual, time-intensive, and iterative nature. As a result, they are susceptible to human errors and are difficult to scale up to population-wide studies of metal pollution. This paper seeks to address this problem by presenting cecilia, the first Machine Learning (ML)-powered spectral modeling code designed to measure the metal abundances of intermediate-temperature (10,000$\leq T_{\rm eff} \leq$20,000 K), Helium-rich polluted white dwarfs. Trained with more than 22,000 randomly drawn atmosphere models and stellar parameters, our pipeline aims to overcome the limitations of classical methods by replacing the generation of synthetic spectra from computationally expensive
    
[^146]: 探索Transformer模型的归纳偏差: 一个来自无穷的视角

    Towards Understanding Inductive Bias in Transformers: A View From Infinity

    [https://arxiv.org/abs/2402.05173](https://arxiv.org/abs/2402.05173)

    本文研究了Transformer模型的归纳偏差，并发现它们倾向于对称排列函数，对称群的表示理论可以用于分析预测，同时提出了一个简化模型来解决学习曲线和网络输出，并在常见设置中得出学习能力的紧密边界，最后还证明了WikiText数据集具有排列对称性。

    

    我们研究了Transformer模型在无穷的过参数化高斯过程极限中的归纳偏差，并指出Transformer模型在序列空间中更倾向于对称排列函数。我们证明了对称群的表示理论可以用于在数据集对token之间的排列具有对称性时给出定量的分析预测。我们提出了一个简化的Transformer模型，并在极限条件下求解模型，包括对学习曲线和网络输出的准确预测。我们展示了在常见的设置中，可以推导出学习能力的紧密边界，以上下文长度作为函数的缩放定律。最后，我们认为WikiText数据集确实具有一定程度的排列对称性。

    We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.
    
[^147]: 神经缩放律的资源模型

    A Resource Model For Neural Scaling Law

    [https://arxiv.org/abs/2402.05164](https://arxiv.org/abs/2402.05164)

    该论文介绍了神经缩放律的资源模型，通过观察实证发现，子任务的损失与分配的神经元成反比，复合任务中子任务获得的资源随模型变大而增长，保持资源比例不变。该模型可以用于预测复合任务的神经缩放律，并成功复制了Chinchilla模型的神经缩放律。该资源模型是表征和诊断神经网络的有用工具。

    

    神经缩放律描述了随着模型规模的增大，模型性能如何提高。受实证观察启发，我们引入了神经缩放的资源模型。一个任务通常是复合任务，可以分解为许多子任务，这些子任务竞争资源（以分配给子任务的神经元数量来衡量）。在玩具问题上，我们经验证实：（1）子任务的损失与其分配的神经元成反比。（2）当复合任务中存在多个子任务时，随着模型变大，每个子任务获得的资源均匀增长，保持获得资源的比例不变。我们假设这些发现在一般情况下是有效的，并建立了一个模型来预测一般复合任务的神经缩放律，并成功复制了arXiv:2203.15556中报告的Chinchilla模型的神经缩放律。我们相信本文中使用的资源概念将是表征和诊断神经网络的有用工具。

    Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural 
    
[^148]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^149]: AI的文档化情况如何？对32,000份AI模型卡进行系统分析

    What's documented in AI? Systematic Analysis of 32K AI Model Cards

    [https://arxiv.org/abs/2402.05160](https://arxiv.org/abs/2402.05160)

    本研究对Hugging Face平台上的32,111份AI模型文档进行了全面分析，发现大多数模型提供了模型卡，但信息量不一致。有关环境影响、限制和评估的部分填写率最低，训练部分则填写率最高。

    

    AI模型的迅速增加突显了充分的文档化的重要性，因为这样可以使用户了解、信任并有效地利用这些模型在各种应用中。虽然开发者鼓励制作模型卡，但目前还不清楚这些卡包含多少信息或者包含哪些信息。在这项研究中，我们对Hugging Face平台上的32,111份AI模型文档进行了全面分析，该平台是分发和部署AI模型的领先平台。我们的调查揭示了普遍的模型卡文档化实践。大多数下载量较大的AI模型提供了模型卡，但这些卡的信息量不一致。我们发现，有关环境影响、限制和评估的部分填写率最低，而训练部分则是填写得最全面的部分。我们对每个部分的内容进行分析，以了解从业者的重点关注内容。有趣的是，有相当多的模型卡在相关部分存在较大的空白。

    The rapid proliferation of AI models has underscored the importance of thorough documentation, as it enables users to understand, trust, and effectively utilize these models in various applications. Although developers are encouraged to produce model cards, it's not clear how much information or what information these cards contain. In this study, we conduct a comprehensive analysis of 32,111 AI model documentations on Hugging Face, a leading platform for distributing and deploying AI models. Our investigation sheds light on the prevailing model card documentation practices. Most of the AI models with substantial downloads provide model cards, though the cards have uneven informativeness. We find that sections addressing environmental impact, limitations, and evaluation exhibit the lowest filled-out rates, while the training section is the most consistently filled-out. We analyze the content of each section to characterize practitioners' priorities. Interestingly, there are substantial
    
[^150]: 提高孟加拉OCR的专用模型和先进技术在不同类型文档中的应用

    Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types

    [https://arxiv.org/abs/2402.05158](https://arxiv.org/abs/2402.05158)

    本文介绍了一个孟加拉OCR系统，具有重构文档布局、精确提取、多样化文档类型支持和优化字符与单词识别的特点。

    

    本研究论文介绍了一个独特的孟加拉OCR系统，该系统具有一些能力。该系统在保留结构、对齐和图像的同时，优秀地重构文档布局。它结合了先进的图像和签名检测技术，以进行精确的提取。针对包括计算机排版、凸版印刷、打字机和手写文档在内的不同类型文档，该系统还包括了专门的单词分割模型。该系统能够处理静态和动态手写输入，并识别各种书写风格。此外，它还能够识别孟加拉复合字符。通过广泛的数据收集工作，提供了多样化的语料库，而先进的技术组件则优化了字符和单词识别。其他贡献包括图像、标志、签名和表格识别、透视校正、布局重建以及用于高效可扩展处理的排队模块。该系统在高效准确的文本提取方面表现出色。

    This research paper presents a unique Bengali OCR system with some capabilities. The system excels in reconstructing document layouts while preserving structure, alignment, and images. It incorporates advanced image and signature detection for accurate extraction. Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents. The system handles static and dynamic handwritten inputs, recognizing various writing styles. Furthermore, it has the ability to recognize compound characters in Bengali. Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition. Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing. The system demonstrates outstanding performance in efficient and accurate text extract
    
[^151]: Adam和随机梯度下降优化在人工神经网络训练中的全局最小值非收敛性及局部最小解构造

    Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks

    [https://arxiv.org/abs/2402.05155](https://arxiv.org/abs/2402.05155)

    本研究解决了随机梯度下降方法在浅层人工神经网络训练中不收敛于全局最小值的问题，并提出了适用于该问题的局部最小解构造方法。

    

    随机梯度下降（SGD）优化方法，如普通的SGD方法和流行的Adam优化器，现在是人工神经网络（ANNs）训练的首选方法。尽管SGD方法在数值模拟中在ANN训练中取得了显著的成功，但在所有实际相关的场景中，严格解释SGD方法为何成功训练ANNs仍然是一个未解决的问题。特别是在大多数实际相关的监督学习问题中，SGD方法似乎以高概率不收敛于ANN训练问题的优化空间的全局最小值。然而，证明SGD方法不会收敛于全局最小值仍然是一个未解决的研究问题。在本研究中，我们解决了这个研究问题，在具有修正线性单元（ReLU）和相关激活函数以及标准均方误差损失的浅层ANNs训练中，推翻了这种结论。

    Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such
    
[^152]: 从道路网络和起始位置-目的地流量数据的开放数据估算公路交通碳排放

    Estimating On-road Transportation Carbon Emissions from Open Data of Road Network and Origin-destination Flow Data

    [https://arxiv.org/abs/2402.05153](https://arxiv.org/abs/2402.05153)

    该论文利用开放数据中的道路网络和起始位置-目的地流量数据，结合层次异构图学习方法，提出了一种估算公路交通碳排放的新方法。这个方法可以有效解决传统方法中数据收集困难的问题。

    

    正确估计公路交通的碳排放是碳排放监测和高效减排政策制定的关键，占总碳排放量的20%以上。然而，现有的估算方法通常依赖于难以收集的个别车辆行驶里程的统计数据来计算排放量，因此数据收集困难较大。为了利用人工智能的强大模式识别能力缓解这个问题，我们结合两个代表交通需求和容量因素的开放数据源，即起始位置-目的地（OD）流量数据和道路网络数据，构建了一种基于层次异构图学习的公路碳排放估算方法（HENCE）。具体而言，我们构建了一个包括道路网络级别、社区级别和区域级别的分层图，以建模多尺度基于道路网络的连接性和空间之间的旅行连接。

    Accounting for over 20% of the total carbon emissions, the precise estimation of on-road transportation carbon emissions is crucial for carbon emission monitoring and efficient mitigation policy formulation. However, existing estimation methods typically depend on hard-to-collect individual statistics of vehicle miles traveled to calculate emissions, thereby suffering from high data collection difficulty. To relieve this issue by utilizing the strong pattern recognition of artificial intelligence, we incorporate two sources of open data representative of the transportation demand and capacity factors, the origin-destination (OD) flow data and the road network data, to build a hierarchical heterogeneous graph learning method for on-road carbon emission estimation (HENCE). Specifically, a hierarchical graph consisting of the road network level, community level, and region level is constructed to model the multi-scale road network-based connectivity and travel connection between spatial a
    
[^153]: CrashFormer: 一种多模态架构用于预测事故风险

    CrashFormer: A Multimodal Architecture to Predict the Risk of Crash

    [https://arxiv.org/abs/2402.05151](https://arxiv.org/abs/2402.05151)

    CrashFormer是一种多模态架构，利用全面的输入数据（如事故历史、天气信息、地图图像和人口信息），可以每6小时预测5.161平方公里地理范围内的未来事故风险。

    

    减少交通事故是一个至关重要的全球公共安全问题。事故预测对于提高交通安全至关重要，可以在事故发生之前采取积极的措施，并提供安全政策、法规和有针对性的干预措施的信息。尽管过去几十年进行了许多关于事故预测的研究，但由于输入数据或问题形式的限制，许多研究在可推广性、可重现性或实际应用上存在局限性。为了解决现有的缺点，我们提出了CrashFormer，这是一种多模态架构，利用全面（但相对容易获取）的输入数据，如事故历史、天气信息、地图图像和人口信息。该模型可以以相对可接受的频率（即每6小时）预测5.161平方公里地理范围内的未来事故风险。CrashFormer由五个组成部分组成：序列编码器用于利用历史事故和天气数据。

    Reducing traffic accidents is a crucial global public safety concern. Accident prediction is key to improving traffic safety, enabling proactive measures to be taken before a crash occurs, and informing safety policies, regulations, and targeted interventions. Despite numerous studies on accident prediction over the past decades, many have limitations in terms of generalizability, reproducibility, or feasibility for practical use due to input data or problem formulation. To address existing shortcomings, we propose CrashFormer, a multi-modal architecture that utilizes comprehensive (but relatively easy to obtain) inputs such as the history of accidents, weather information, map images, and demographic information. The model predicts the future risk of accidents on a reasonably acceptable cadence (i.e., every six hours) for a geographical location of 5.161 square kilometers. CrashFormer is composed of five components: a sequential encoder to utilize historical accidents and weather data
    
[^154]: 设计用于司机意图识别的深度神经网络

    Designing deep neural networks for driver intention recognition

    [https://arxiv.org/abs/2402.05150](https://arxiv.org/abs/2402.05150)

    本文研究了深度神经网络架构对司机意图识别性能的影响，并应用神经架构搜索方法进行探索，为具有有限计算能力的安全关键应用提供指导。

    

    司机意图识别研究越来越多地依赖于深度神经网络。深度神经网络在许多不同的任务中取得了最优性能，但常常不常见地对网络架构的复杂性和性能进行显式分析。因此，本文应用神经架构搜索来研究深度神经网络架构对现实世界中具有有限计算能力的安全关键应用的影响。我们探索了一个预定义的搜索空间，其中包含三种能够处理序列数据的深度神经网络层类型（长短时记忆、时间卷积和时间序列转换层），以及不同数据融合策略对司机意图识别性能的影响。对于两个司机意图识别数据集，我们评估了八种搜索策略。对于这两个数据集，我们观察到没有明显的搜索策略比较好地采样深度神经网络。

    Driver intention recognition studies increasingly rely on deep neural networks. Deep neural networks have achieved top performance for many different tasks, but it is not a common practice to explicitly analyse the complexity and performance of the network's architecture. Therefore, this paper applies neural architecture search to investigate the effects of the deep neural network architecture on a real-world safety critical application with limited computational capabilities. We explore a pre-defined search space for three deep neural network layer types that are capable to handle sequential data (a long-short term memory, temporal convolution, and a time-series transformer layer), and the influence of different data fusion strategies on the driver intention recognition performance. A set of eight search strategies are evaluated for two driver intention recognition datasets. For the two datasets, we observed that there is no search strategy clearly sampling better deep neural network 
    
[^155]: FlowPG: 使用正则化流的动作约束策略梯度

    FlowPG: Action-constrained Policy Gradient with Normalizing Flows

    [https://arxiv.org/abs/2402.05149](https://arxiv.org/abs/2402.05149)

    本文提出了使用正则化流的动作约束策略梯度（FlowPG）方法，以解决动作约束强化学习中的挑战。该方法通过学习一个可逆映射和开发多种动作采样方法，有效地解决了在每个强化学习步骤中确保代理采取合理动作的问题。

    

    动作约束强化学习（ACRL）是解决安全关键和资源分配相关决策问题的常用方法。ACRL的一个主要挑战是确保代理在每个强化学习步骤中采取满足约束条件的有效动作。通常使用在策略网络之上的投影层的方法需要解决一个优化问题，这可能导致训练时间较长、收敛速度慢和零梯度问题。为了解决这个问题，我们首先使用正则化流模型学习一个可逆的、可微分的映射，将可行动作空间映射到一个潜变量上的简单分布的支撑集合，例如高斯分布。其次，学习流模型需要从可行动作空间中进行采样，这也是一个具有挑战性的问题。我们开发了多种方法，基于哈密顿蒙特卡洛和概率表决图，用于凸约束和非凸约束下的动作采样。接下来，我们将学习的流模型与强化学习的策略网络相整合。

    Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learn
    
[^156]: ApiQ：2位量化大型语言模型的微调

    ApiQ: Finetuning of 2-Bit Quantized Large Language Model

    [https://arxiv.org/abs/2402.05147](https://arxiv.org/abs/2402.05147)

    这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。

    

    随着大型语言模型的增大，内存高效的模型微调近年来备受关注，主要是由于GPU内存限制和这些方法与完全微调的可比结果所带来的约束。尽管有了进展，如QLoRA这样的内存高效微调策略在不同位宽的量化和多样化任务中表现不一致。这种不一致主要来自于量化过程对保留知识的有害影响，导致灾难性遗忘，削弱了预训练模型在微调中的利用。在这项工作中，我们引入了一种名为ApiQ的新型量化框架，旨在通过同时初始化LoRA组件和量化LLM的权重来恢复量化损失的信息。这种方法确保了原始LLM的激活精度的维持，同时减轻了误差的传播。

    Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
    
[^157]: 用动态结构化剪枝方法压缩深度强化学习网络，用于自动驾驶

    Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving

    [https://arxiv.org/abs/2402.05146](https://arxiv.org/abs/2402.05146)

    本文提出了一种动态结构化剪枝方法，用于压缩深度强化学习网络，以便在资源受限的自动驾驶设备中实现高效部署。通过逐渐删除不重要的神经元，我们的方法显著减小了内存消耗和计算量。

    

    深度强化学习在复杂的自动驾驶场景中取得了显著的成功。然而，深度强化学习模型不可避免地带来了高内存消耗和计算量，这阻碍了它们在资源受限的自动驾驶设备中的广泛应用。结构化剪枝被认为是一种有用的方法来压缩和加速深度强化学习模型，但是估计一个参数（即神经元）对深度强化学习模型的贡献仍然具有挑战性。在本文中，我们引入了一种新颖的动态结构化剪枝方法，在训练阶段逐渐删除深度强化学习模型中不重要的神经元。我们的方法包括两个步骤，即使用组稀疏正则化器训练深度强化学习模型，以及使用动态剪枝阈值去除不重要的神经元。为了使用少量重要的神经元有效地训练深度强化学习模型，我们采用了一个神经元重要性组稀疏正则化器。与传统的正则化器不同，这个正则化器对冗余参数施加了惩罚。

    Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundan
    
[^158]: 生存分析的在线学习方法

    Online Learning Approach for Survival Analysis

    [https://arxiv.org/abs/2402.05145](https://arxiv.org/abs/2402.05145)

    本论文介绍了一种在线数学框架，用于实时适应动态环境和有审查数据的生存分析，通过在线牛顿步骤(ONS)算法估计事件时间分布，并提出了保证ONS具有对数随机遗憾界的随机方法和自适应聚合方法。

    

    我们引入了一种在线数学框架用于生存分析，可以实时适应动态环境和有审查数据。该框架通过最优二阶在线凸优化算法-在线牛顿步骤(ONS)估计事件时间分布。这种以前未曾探索的方法具有重大优势，包括具有非渐近收敛保证的明确算法。此外，我们分析了ONS超参数的选择，这取决于指数-凸性质并且对遗憾界有显著影响。我们提出了一种保证ONS具有对数随机遗憾界的随机方法。此外，我们引入了一种自适应聚合方法，确保在保持快速遗憾界的同时，在超参数选择方面具有鲁棒性。本文的发现可以超出生存分析领域，并且对于任何具有差的指数-凸性和不稳定ONS的情况都是相关的。

    We introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data. This framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-Online Newton Step (ONS). This approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees. Moreover, we analyze the selection of ONS hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound. We propose a stochastic approach that guarantees logarithmic stochastic regret for ONS. Additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds. The findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ONS. Fi
    
[^159]: 一种使用进化算子的强盗方法进行模型选择

    A Bandit Approach with Evolutionary Operators for Model Selection

    [https://arxiv.org/abs/2402.05144](https://arxiv.org/abs/2402.05144)

    本文提出了一种使用进化算子的强盗方法来进行模型选择，通过将模型选择问题建模为无穷臂赌博机问题，利用部分训练和准确性作为奖励，最终的算法Mutant-UCB在测试中表现出色，优于固定预算下的最先进技术。

    

    本文将模型选择问题建模为无穷臂赌博机问题。模型是臂，选择一个臂对应部分训练模型（资源分配）。奖励是选择模型在部分训练后的准确性。在这个最佳臂识别问题中，遗憾是最优模型的预期准确性与最终选择模型的准确性之间的差距。我们首先考虑了UCB-E在随机无穷臂赌博机问题上的直接推广，并且证明了在基本假设下，期望遗憾的顺序是$T^{-\alpha}$，其中$\alpha \in (0,1/5)$，$T$是要分配的资源数量。从这个基本算法出发，我们介绍了一种算法Mutant-UCB，它结合了进化算法的操作符。在三个开源图片分类数据集上进行的测试表明了这种新颖的组合方法的相关性，该方法优于固定预算下的国际领先技术。

    This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.
    
[^160]: 通过整数优化实现张量补全

    Tensor Completion via Integer Optimization

    [https://arxiv.org/abs/2402.05141](https://arxiv.org/abs/2402.05141)

    本文开发了一种通过整数优化实现张量补全的新算法，该算法通过在线性步骤中实现了收敛并达到了信息理论速率。

    

    张量补全问题的主要挑战在于计算能力和信息理论样本复杂度之间的基本张力。过去的方法要么达到了信息理论的速率但缺乏计算相应解的实际算法，要么具有多项式时间算法，但需要指数级更多的样本以达到低估计误差。本文开发了一种新颖的张量补全算法，通过在线性数目的oracle步骤中同时实现可证的收敛（在数字容差方面）和信息理论速率，从而解决了这个张力。我们的方法通过使用基于规范的张量范数构造了张量补全问题的凸优化问题，这种方法定义了一种允许使用整数线性优化在这种新范数下解决线性分离问题的规范。基于这个洞察力的调整被纳入到一种基于Frank-Wolfe变体的算法中来构建我们的算法。

    The main challenge with the tensor completion problem is a fundamental tension between computation power and the information-theoretic sample complexity rate. Past approaches either achieve the information-theoretic rate but lack practical algorithms to compute the corresponding solution, or have polynomial-time algorithms that require an exponentially-larger number of samples for low estimation error. This paper develops a novel tensor completion algorithm that resolves this tension by achieving both provable convergence (in numerical tolerance) in a linear number of oracle steps and the information-theoretic rate. Our approach formulates tensor completion as a convex optimization problem constrained using a gauge-based tensor norm, which is defined in a way that allows the use of integer linear optimization to solve linear separation problems over the unit-ball in this new norm. Adaptations based on this insight are incorporated into a Frank-Wolfe variant to build our algorithm. We s
    
[^161]: Tag-LLM: 将通用的LLM应用于专业领域的再利用

    Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains

    [https://arxiv.org/abs/2402.05140](https://arxiv.org/abs/2402.05140)

    本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。

    

    大型语言模型（LLMs）在理解和生成自然语言方面表现出了非凡的能力。然而，在专门领域中，如物理学和生物医学科学这样的预训练语料库中未充分涵盖的领域，它们的能力下降。本文探讨了如何将通用LLMs重新用于专业领域的有效任务解决方案。我们介绍了一种新颖的、与模型无关的框架，用于学习自定义的输入标签，这些标签被参数化为连续向量并附加到LLMs的嵌入层，以对LLMs进行条件约束。我们设计了两种类型的输入标签：领域标签用于限定专业表示（例如化学式）并提供领域相关的上下文；功能标签用于表示特定的功能（例如预测分子性质）并压缩功能解决指令。我们使用辅助数据和领域知识开发了一个包括三个阶段的学习这些标签的协议。通过明确将任务领域与任务功能分离，我们的方法能够改善在专业领域中的任务求解能力。

    Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
    
[^162]: LtU-ILI:一种面向天体物理学和宇宙学的隐式推理全能框架

    LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics and Cosmology

    [https://arxiv.org/abs/2402.05137](https://arxiv.org/abs/2402.05137)

    LtU-ILI是一个用于天体物理学和宇宙学中机器学习推理的全能框架，包括各种神经架构、训练模式、先验知识和密度估计器，并具有全面的验证指标和易于并行化的设计。实际应用包括从X射线光度法估计星系团质量、推断宇宙学、表征引力波信号中的源和获取银河系中的物理尘埃参数。

    

    本文介绍了学习宇宙隐含似然推理（LtU-ILI）流程，这是一个用于天体物理学和宇宙学中快速、用户友好和前沿的机器学习（ML）推理的代码库。该流程包括各种神经架构、训练模式、先验知识和密度估计器的软件实现，可轻松适应任何研究工作流程。它包括全面的验证指标，用于评估后验估计覆盖范围，提高推断结果的可靠性。此外，该流程容易实现并行化，并设计用于高效探索建模超参数。为了展示其能力，我们提供了一些应用实例，涵盖了天体物理学和宇宙学的各个问题，如从X射线光度法估计星系团质量；从物质功率谱和暗物质点云中推断宇宙学；对引力波信号中的源的特性进行表征；从银河系中获取物理尘埃参数。

    This paper presents the Learning the Universe Implicit Likelihood Inference (LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edge machine learning (ML) inference in astrophysics and cosmology. The pipeline includes software for implementing various neural architectures, training schema, priors, and density estimators in a manner easily adaptable to any research workflow. It includes comprehensive validation metrics to assess posterior estimate coverage, enhancing the reliability of inferred results. Additionally, the pipeline is easily parallelizable, designed for efficient exploration of modeling hyperparameters. To demonstrate its capabilities, we present real applications across a range of astrophysics and cosmology problems, such as: estimating galaxy cluster masses from X-ray photometry; inferring cosmology from matter power spectra and halo point clouds; characterising progenitors in gravitational wave signals; capturing physical dust parameters from galaxy co
    
[^163]: 个性化语言模型基于个性化人类反馈

    Personalized Language Modeling from Personalized Human Feedback

    [https://arxiv.org/abs/2402.05133](https://arxiv.org/abs/2402.05133)

    该论文提出了一个个性化语言模型的方法，通过在于用户的反馈数据中引入个性化特征来解决强化学习框架在多样化用户偏好下存在的问题。

    

    从个性化人类反馈中进行强化学习（RLHF）是目前主流的框架，用于调整大型语言模型以更好地符合人类偏好。然而，在这个框架下开发的算法的基本前提在用户偏好多样化的情况下可能会出现问题。在本文中，我们旨在通过开发个性化语言模型的方法来解决这个问题。我们首先正式介绍了从个性化人类反馈中学习的任务，并解释了为什么在这种情况下普通的RLHF可能会存在问题。然后，我们提出了一个通用的个性化-RLHF（P-RLHF）框架，需要同时学习用户模型和语言（或奖励）模型。用户模型接收用户信息并输出用户表示。其结构编码了我们对反馈数据中用户偏好的假设。我们为个性化奖励建模和个性化直接偏好优化开发了新的学习目标。

    Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
    
[^164]: Illuminate：一种使用可解释分析和积极治疗的新方法进行抑郁症检测

    Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering

    [https://arxiv.org/abs/2402.05127](https://arxiv.org/abs/2402.05127)

    本研究提出了一种新的抑郁症检测和治疗范式，使用先进的大型语言模型，经过特定提示微调以诊断、解释和建议治疗干预。同时还介绍了一个丰富的数据库，以提供个性化的治疗建议。此方法与患者进行共情对话管理，有效支持抑郁症患者。

    

    本文介绍了一种使用先进的大型语言模型（LLMs）：Generative Pre-trained Transformer 4（GPT-4）、Llama 2 chat和Gemini的抑郁症检测和治疗新范式。这些LLMs通过特定的提示进行微调，以诊断、解释和建议抑郁症的治疗干预。一种独特的少样本提示方法增强了模型根据DSM-5标准分析和解释抑郁症状的能力。在交互阶段，模型采用共情对话管理，利用PsychDB和认知行为疗法（CBT）指南等资源，与患有重度抑郁症的个体进行支持性互动。此外，研究还介绍了Illuminate数据库，其中包含各种CBT模块，可帮助个性化的治疗建议。该研究使用F1分数、精确度、召回率、余弦相似度和召回率导向的差错进行LLM性能评估。

    This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for
    
[^165]: 图神经网络和基于NER的文本摘要

    Graph Neural Network and NER-Based Text Summarization

    [https://arxiv.org/abs/2402.05126](https://arxiv.org/abs/2402.05126)

    这个项目使用图神经网络和命名实体识别系统，提出了一种创新的文本摘要方法。图神经网络能够捕获并处理文本信息中的关系数据，而命名实体识别系统通过识别和强调关键实体来保持摘要的重点。

    

    在当今时代信息和数据的丰富性下，人们或者机器几乎不可能逐行查看所有的数据。通常的做法是试图从行中快速浏览并保留绝对重要的信息，这在更正式的术语中称为摘要。文本摘要是一项重要的任务，旨在将冗长的文档或文章压缩成较短、连贯的表达方式，同时保留核心信息和意义。本项目引入了一种创新的文本摘要方法，利用了图神经网络（GNNs）和命名实体识别（NER）系统的能力。GNNs以其优秀的捕获和处理文本信息中的关系数据的能力，能够理解大型文档中的复杂结构。同时，NER系统通过识别和强调关键实体，确保摘要过程保持专注于重点内容。

    With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization. Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the 
    
[^166]: 更多的代理就是你所需要的

    More Agents Is All You Need

    [https://arxiv.org/abs/2402.05120](https://arxiv.org/abs/2402.05120)

    大型语言模型的性能与代理数量成比例，通过简单的采样和投票方法可以进一步增强性能，这种方法与现有的复杂方法正交。

    

    我们发现，仅通过一种采样和投票的方法，大型语言模型(Large Language Models, LLMs)的性能与实例化的代理数量成比例。此外，这种方法对已有的复杂方法进一步增强LLMs是正交的，而增强的程度与任务的困难程度相关。我们进行了广泛的实验，验证了我们的发现，并研究了能够促进其发生的属性。我们的代码公开在以下网址: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}

    We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
    
[^167]: 无监督运动重定位用于人机仿真

    Unsupervised Motion Retargeting for Human-Robot Imitation

    [https://arxiv.org/abs/2402.05115](https://arxiv.org/abs/2402.05115)

    该论文研究了无监督运动重定位用于人机仿真的问题，提出了一个编码器-解码器神经网络模型进行领域到领域的转换。这种方法可以在无对应数据的情况下进行人机仿真。

    

    这项早期研究旨在通过将人类动作领域的关节位置序列转化为特定机器人可实现的动作领域，从而改进在线人机仿真。借助深度学习方法的泛化能力，我们通过提出一个编码器-解码器神经网络模型进行领域到领域的转换来解决这个问题。为了训练这样的模型，可以使用对应的机器人和人类动作对。然而，在实践中这样的配对数据非常稀缺且收集起来繁琐。因此，我们转向适应于无配对领域到领域转换的深度学习方法，以进行人机仿真。

    This early-stage research work aims to improve online human-robot imitation by translating sequences of joint positions from the domain of human motions to a domain of motions achievable by a given robot, thus constrained by its embodiment. Leveraging the generalization capabilities of deep learning methods, we address this problem by proposing an encoder-decoder neural network model performing domain-to-domain translation. In order to train such a model, one could use pairs of associated robot and human motions. Though, such paired data is extremely rare in practice, and tedious to collect. Therefore, we turn towards deep learning methods for unpaired domain-to-domain translation, that we adapt in order to perform human-robot imitation.
    
[^168]: 一种轻量级和无监督方法，用于使用运维数据测量进行近实时行为分析

    A Light-weight and Unsupervised Method for Near Real-time Behavioral Analysis using Operational Data Measurement

    [https://arxiv.org/abs/2402.05114](https://arxiv.org/abs/2402.05114)

    这种方法提出了一种轻量级和无监督的近实时行为分析方法，可以适应大型计算系统的变化，并能准确识别出行为异常。

    

    监控大型计算系统的状态对于识别意外行为并提高其性能和正常运行至关重要。然而，由于这种计算系统的大规模和分布式设计以及大量的监控参数，应该采用自动化监控方法。这样的自动监控方法还应具有适应计算系统持续变化的能力。此外，它们还应能够在适当的时间内识别出有用的行为异常，以执行相应的反应。本文提出了一种通用的轻量级和无监督方法，利用大型计算系统上的操作数据测量来实现近实时异常检测。所提出的模型只需要4个小时的数据和每个训练过程的50个epochs，就能准确地反映计算系统的行为模式。

    Monitoring the status of large computing systems is essential to identify unexpected behavior and improve their performance and uptime. However, due to the large-scale and distributed design of such computing systems as well as a large number of monitoring parameters, automated monitoring methods should be applied. Such automatic monitoring methods should also have the ability to adapt themselves to the continuous changes in the computing system. In addition, they should be able to identify behavioral anomalies in useful time, to perform appropriate reactions. This work proposes a general lightweight and unsupervised method for near real-time anomaly detection using operational data measurement on large computing systems. The proposed model requires as little as 4 hours of data and 50 epochs for each training process to accurately resemble the behavioral pattern of computing systems.
    
[^169]: 物理信息神经网络的多尺度建模：从复杂系统的大尺度动力学到小尺度预测

    Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems

    [https://arxiv.org/abs/2402.05067](https://arxiv.org/abs/2402.05067)

    本文提出了利用物理信息神经网络进行多尺度建模的方法，通过解耦大尺度和小尺度动力学，并在正交基函数空间中近似小尺度系统。实验结果表明该方法在处理液体动力学问题以及更复杂的情况下具有较高的有效性和适用性。

    

    多尺度现象在各个科学领域中普遍存在，对于准确有效地预测复杂系统中的多尺度动力学提出了普遍的挑战。本文提出了一种通过解耦方法对多尺度动力学进行表征的新的求解模式。通过独立地建模大尺度动力学，并将小尺度动力学视为从属系统，我们开发了一种谱PINN方法，在正交基函数空间中接近小尺度系统。通过大量的数值实验，包括一维Kuramot-Sivashinsky (KS)方程、二维和三维Navier-Stokes (NS)方程，我们展示了该方法的有效性，展示了它在液体动力学问题中的多样性。此外，我们还深入研究了该方法在更复杂问题中的应用，包括非均匀网格、复杂几何形状、带噪声的大尺度数据和高维小尺度动力学。

    Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
    
[^170]: Federated Learning能够找到有益的好友

    Federated Learning Can Find Friends That Are Beneficial

    [https://arxiv.org/abs/2402.05050](https://arxiv.org/abs/2402.05050)

    本研究介绍了一种新颖的算法，在Federated Learning中使用自适应聚合权重来识别对特定学习目标最有益的客户，证明了该方法的收敛性，并经过实证评估发现，使用该算法引导的合作优于传统方法，这为更加简化和有效的Federated Learning实现奠定了基础。

    

    在Federated Learning (FL)中，分布式性质和客户数据的异质性既带来了机会，也带来了挑战。虽然客户之间的合作可以显著增强学习过程，但并不是所有的合作都是有益的；有些甚至可能是有害的。在这项研究中，我们引入了一种新颖的算法，为参与FL训练的客户分配自适应的聚合权重，识别出数据分布对特定学习目标最有益的客户。我们证明了我们的聚合方法的收敛性与仅聚合具有相同数据分布的客户接收的更新的方法不相上下。此外，经验证明，由我们的算法引导的合作优于传统的FL方法。这强调了审慎选择客户的关键作用，并为未来更简化和有效的FL实现奠定了基础。

    In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
    
[^171]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^172]: 两个交易不会困扰：通过构造合理的梯度匹配来压缩图表

    Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching

    [https://arxiv.org/abs/2402.04924](https://arxiv.org/abs/2402.04924)

    本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。

    

    在大规模图表上训练已经在图表表示学习方面取得了显著成果，但其成本和存储引起了越来越多的关注。作为最有前景的方向之一，图表压缩方法通过使用梯度匹配来解决这些问题，目标是将完整的图表压缩成更简洁但信息丰富的合成集。尽管令人鼓舞，但这些策略主要强调梯度的匹配方向，从而导致训练轨迹的偏差。这种偏差进一步由压缩和评估阶段之间的差异放大，最终导致累积误差，对压缩图表的性能产生不利影响。鉴于此，我们提出了一种名为\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory（\textbf{CTRL}）的新型图表压缩方法，它提供了一个更接近原始数据集特征分布的优化起点和一个更精细的策略。

    Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
    
[^173]: 快速定时条件下的潜在音频扩散

    Fast Timing-Conditioned Latent Audio Diffusion

    [https://arxiv.org/abs/2402.04825](https://arxiv.org/abs/2402.04825)

    本研究提出了一种名为Stable Audio的生成模型，该模型利用潜在扩散和条件化技术，实现了高效生成44.1kHz立体声音乐和音效。与其他最先进的模型不同，Stable Audio能够生成具有结构和立体声的音乐，并在性能评估上表现出色。

    

    从文本提示生成长篇44.1kHz立体声音频可能对计算要求很高。此外，大多数先前的工作并没有解决音乐和音效在持续时间上的自然变化问题。我们的研究专注于使用生成模型以高效方式生成长篇、可变长度的44.1kHz立体声音乐和音效。Stable Audio基于潜在扩散，其潜在性质由一个全卷积变分自编码器定义。它不仅基于文本提示进行条件化，还基于时间嵌入，使得可以对生成的音乐和音效的内容和长度进行精细控制。在A100 GPU上，Stable Audio能够在8秒内以44.1kHz的速度渲染长达95秒的立体声信号。尽管计算效率高且推理速度快，但它在两个公开的文本-音乐和音频基准中仍然是最好的，并且与最先进的模型不同，它可以生成具有结构和立体声音乐。

    Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.
    
[^174]: 连续多维标度

    Continuous Multidimensional Scaling

    [https://arxiv.org/abs/2402.04436](https://arxiv.org/abs/2402.04436)

    连续多维标度是关于将距离信息嵌入欧几里得空间的过程，并探讨了在对象集不断增加的情况下，将整个嵌入问题序列视为一个固定空间中的一系列优化问题的方法和结论。

    

    多维标度(MDS)是将关于一组$n$个对象的距离信息嵌入到$d$维欧几里得空间中的过程。最初由心理测量学界构思，MDS关注的是嵌入到一组固定对象上的一组固定距离。现代关注的问题更常涉及到研究与一组不断增加的对象相关联的一系列距离的极限行为，如在随机图的统计推断的渐近理论中出现的问题。点到集合映射理论中的标准结果表明，若$n$固定，则嵌入结构的极限是极限距离的嵌入结构。但如果$n$增加怎么办呢？那么就需要重新制定MDS，以便将整个嵌入问题序列视为一个固定空间中的一系列优化问题。我们提出了这样一种重新制定，并推导出一些结论。

    Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space. As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects. Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities. But what if $n$ increases? It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. We present such a reformulation and derive some consequences.
    
[^175]: Tempered Calculus for ML: 应用于双曲模型嵌入的研究

    Tempered Calculus for ML: Application to Hyperbolic Model Embedding

    [https://arxiv.org/abs/2402.04163](https://arxiv.org/abs/2402.04163)

    本论文介绍了基于温和微积分的理论和工具，来改进目前在机器学习中使用的数学扭曲方法，特别强调与几何和机器学习相关的特性。

    

    大多数在机器学习中使用的数学扭曲本质上都是积分的：$f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances等。在本文中，我们揭示了一个基于温和微积分的理论和工具，可以帮助改进这些扭曲以更好地满足机器学习的要求。我们从广义的黎曼积分开始，这种积分还包括不严格可加的函数，而是更一般地是$t$-可加的，就像非极限统计力学中的情况一样。特别地，这种方法将Volterra的乘积积分作为特例。然后，我们使用（欧几里得）导数的扩展来推广基本定理。这些推广以及一系列更具体的定理为结果提供了基础，展示了如何以简单的方式专门设计、改变或改变扭曲度量的基本特性，特别强调与几何和机器学习相关的特性。

    Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the
    
[^176]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^177]: 一种从状态空间模型到紧支持基的核分组的通用理论

    A General Theory for Kernel Packets: from state space model to compactly supported basis

    [https://arxiv.org/abs/2402.04022](https://arxiv.org/abs/2402.04022)

    该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。

    

    众所周知，高斯过程（GP）的状态空间（SS）模型公式可以将其训练和预测时间降低到O（n）（n为数据点个数）。我们证明了一个m维的GP的SS模型公式等价于我们引入的一个概念，称为通用右核分组（KP）：一种用于GP协方差函数K的变换，使得对于任意$t \leq t_1$，$0 \leq j \leq m-1$和$m+1$个连续点$t_i$，都满足$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$，其中${D}_t^{(j)}f(t)$表示在$t$上作用的第j阶导数。我们将这个思想扩展到了GP的向后SS模型公式，得到了下一个$m$个连续点的左核分组的概念：$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$，对于任意$t\geq t_{2m}$。通过结合左右核分组，可以证明这些协方差函数的适当线性组合产生了$m$个紧支持的核分组函数：对于任意$t\not\in(t_0,t_{2m})$和$j=0,\cdots,m-1$，$\phi^{(j)}(t)=0$。

    It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
    
[^178]: 逻辑规范引导下的强化学习智能体动态任务采样

    Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents

    [https://arxiv.org/abs/2402.03678](https://arxiv.org/abs/2402.03678)

    本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。

    

    强化学习（RL）在使人工智能智能体学习多样化行为方面取得了重要进展。然而，学习有效的策略通常需要大量的环境交互。为了减少样本复杂性问题，最近的方法使用高级任务规范，如线性时态逻辑（LTL$_f$）公式或奖励机器（RM），来指导智能体的学习过程。在这项工作中，我们提出了一种新颖的方法，称为逻辑规范引导下的动态任务采样（LSTS），它通过学习一组强化学习策略，根据高级任务规范指导智能体从初始状态到目标状态，同时最小化环境交互次数。与以前的工作不同，LSTS不假设环境动力学或奖励机器的信息，并动态采样导致成功目标策略的有希望的任务。我们在一个网格世界上评估了LSTS，并展示了它实现了改进的时间到阈值。

    Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
    
[^179]: IGUANe: 一种适用于脑MR图像多中心协调的三维通用CycleGAN模型

    IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images

    [https://arxiv.org/abs/2402.03227](https://arxiv.org/abs/2402.03227)

    IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。

    

    在MRI研究中，来自多个采集点的图像数据的聚合可以增加样本大小，但可能引入阻碍后续分析一致性的与采集点相关的变异。图像翻译的深度学习方法已经成为协调MR图像跨站点的解决方案。在本研究中，我们引入了IGUANe（具有统一对抗网络的图像生成），这是一种原始的三维模型，它结合了域转换的优势和直接应用样式转移方法来实现多中心脑MR图像协调。IGUANe通过多对一策略，集成了任意数量的域进行训练，扩展了CycleGAN架构。在推断过程中，该模型可以应用于任何图像，甚至来自未知采集点，使其成为协调的通用生成器。在由11台不同扫描仪的T1加权图像组成的数据集上进行训练，IGUANe在未见站点的数据上进行了评估。

    In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
    
[^180]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^181]: 基于微环的相干光学GEMM加速器的比较分析

    A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators

    [https://arxiv.org/abs/2402.03149](https://arxiv.org/abs/2402.03149)

    本文详细分析了基于微环的相干光学GEMM加速器的组织，其通过分裂、聚合、调制、加权和求和等方式操作光信号以加速深度神经网络中的矩阵-矩阵乘法，提高吞吐量和能量效率。

    

    提出了几种基于微环谐振器（MRR）的模拟光学架构，以在深度神经网络中加速通用的矩阵-矩阵乘法（GEMM），具有出色的吞吐量和能量效率。为了实现GEMM功能，这些基于MRR的架构一般通过五种不同的方式操作光信号：（i）将多个光信号分裂（复制）以达到某种多分支，（ii）将多个光信号聚合（复用）以达到某种多输入，（iii）调制光信号以将输入值印置于模拟信号幅度上，（iv）对调制的光信号进行加权，以实现模拟输入权重相乘，（v）对光信号进行求和。MRR基于的GEMM加速器以任意顺序执行前四种信号操作，忽略了这些操作顺序对其性能的可能影响。本文对加速器组织进行了详细分析。

    Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organization
    
[^182]: 用于动作识别的Taylor视频

    Taylor Videos for Action Recognition

    [https://arxiv.org/abs/2402.03019](https://arxiv.org/abs/2402.03019)

    Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。

    

    从视频中有效地提取动作是动作识别中一个重要且长期存在的问题。这个问题非常具有挑战性，因为动作(i)没有明确的形式，(ii)拥有诸如位移、速度和加速度等各种概念，(iii)通常会受到不稳定像素引起的噪声的干扰。为了解决这些挑战，我们提出了Taylor视频，一种新的视频格式，它突出显示了每个帧中的主要动作（例如挥手）被称为Taylor帧。Taylor视频的命名来源于Taylor级数，它使用重要的项来近似给定点上的函数。在视频的情境中，我们定义了一个隐含的动作提取函数，旨在从视频时间块中提取动作。在这个块中，我们使用帧、差分帧和高阶差分帧进行Taylor展开，以近似计算起始帧上的这个函数。我们展示了Taylor级数中高阶项的求和给我们提供了...

    Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
    
[^183]: GPT 模型的对话重构攻击

    Conversation Reconstruction Attack Against GPT Models

    [https://arxiv.org/abs/2402.02987](https://arxiv.org/abs/2402.02987)

    本文介绍了一种针对 GPT 模型的对话重构攻击，该攻击具有劫持会话和重构对话的两个步骤。通过对该攻击对 GPT 模型的隐私风险进行评估，发现 GPT-4 对该攻击具有一定的鲁棒性。

    

    最近，在大型语言模型（LLM）领域取得了重要进展，其中 GPT 系列模型代表着最具代表性的成果。为了优化任务执行，用户经常与托管在云环境中的 GPT 模型进行多轮对话。这些多轮对话往往包含私人信息，需要在云中进行传输和存储。然而，这种操作模式引入了额外的攻击面。本文首先介绍了一种针对 GPT 模型的特定对话重构攻击。我们提出的对话重构攻击由两个步骤组成：劫持会话和重构对话。随后，我们对当 GPT 模型遭受该攻击时对话中固有的隐私风险进行了详尽评估。然而，GPT-4 对于该攻击具有一定的鲁棒性。接着，我们引入了两种高级攻击，旨在更好地重构以前的对话。

    In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
    
[^184]: 逆推强化学习中悲观主义的优势

    The Virtues of Pessimism in Inverse Reinforcement Learning

    [https://arxiv.org/abs/2402.02616](https://arxiv.org/abs/2402.02616)

    本论文提出了一种使用离线强化学习算法来加速逆推强化学习中强化学习子程序的替代方法，通过保持与专家数据分布接近的悲观主义策略，提高了逆推强化学习的样本效率。

    

    逆推强化学习（IRL）是从专家演示中学习复杂行为的强大框架。然而，传统上需要在内循环中反复解决计算成本昂贵的强化学习（RL）问题。通过利用专家演示来减少探索负担在内循环的RL中非常可取。例如，最近的工作通过将学习者重置到专家状态来指导学习者在高回报专家状态下工作。然而，这样的方法在真实世界中不可行。在这项工作中，我们考虑了一种加速IRL中RL子程序的替代方法：悲观主义，即保持与专家数据分布接近，通过离线RL算法来实现。我们在离线RL和IRL之间形成了一个连接，使我们能够使用任意离线RL算法来提高IRL的样本效率。我们通过实验证明了我们的理论，展示了样本效率和探索负担之间的强相关性。

    Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the ef
    
[^185]: SudokuSens: 使用生成方法增强IoT感知应用的深度学习鲁棒性

    SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach

    [https://arxiv.org/abs/2402.02275](https://arxiv.org/abs/2402.02275)

    SudokuSens是一种生成框架，用于通过自动生成训练数据来提高IoT感知应用的深度学习模型鲁棒性。该框架通过模仿实际数据采集过程中未遇到的实验配置，增加了数据的多样性。

    

    本文介绍了SudokuSens，一种用于机器学习驱动的物联网（IoT）应用中自动生成训练数据的生成框架，生成的合成数据模仿实际传感器数据采集过程中未遇到的实验配置。该框架提高了深度学习模型的鲁棒性，并适用于数据采集成本高昂的IoT应用。该工作的动机是因为IoT时间序列数据纠缠了观察对象的特征以及周围环境的混杂内在属性和动态环境干扰。为了在IoT训练数据中加入足够的多样性，因此需要考虑与对象数量和可能遇到的环境条件成倍增加的训练用例的组合爆炸。我们的框架大大减少了这些多样性。

    This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these mult
    
[^186]: 面向预训练视觉模型的参数高效微调：一项综述

    Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey

    [https://arxiv.org/abs/2402.02242](https://arxiv.org/abs/2402.02242)

    本综述调研了面向预训练视觉模型的参数高效微调方法，通过最小参数修改超越全面微调的性能，提供了全面的概述和未来方向，并提供了丰富的资源收藏。

    

    大规模预训练的视觉模型（PVMs）展示了在各种下游视觉任务中的适应能力潜力。然而，随着最先进的PVMs达到数十亿甚至数万亿个参数，标准的全面微调范式由于高计算和存储需求变得不可持续。作为响应，研究人员正在探索参数高效微调（PEFT），旨在以最小参数修改超越全面微调的性能。本综述提供了视觉PEFT的全面概述和未来方向，对最新进展进行了系统审查。首先，我们提供了PEFT的正式定义，并讨论了模型预训练方法。然后，我们将现有方法分为三类：基于添加的、基于部分的和基于统一的。最后，我们介绍了常用的数据集和应用，并提出了潜在的未来研究挑战。该综述还提供了丰富的资源收藏。

    Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
    
[^187]: 论文题目：为什么“摸着石头过河”方法主导推荐系统实践；呼吁摒弃反乌托邦思维

    Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking

    [https://arxiv.org/abs/2402.02152](https://arxiv.org/abs/2402.02152)

    这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。

    

    应用推荐系统研究处于一种奇特的境地。尽管在通过A/B测试来衡量性能方面有一个非常严格的协议，但找到要测试的“B”的最佳方法并没有明确地针对性能，而是针对一个代理指标。因此，一个A/B测试的成功或失败完全取决于所提出的代理指标是否与性能相关性更好。没有原则可以在离线情况下确定一个代理指标是否比另一个更好，这使得从业者们摸不着头脑。本论文的目的是质疑这种反乌托邦思维，并主张深度学习堆栈的非标准用法实际上有潜力解锁优化奖励的推荐系统。

    Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
    
[^188]: 一种多角度的机器学习方法用于评估洛杉矶警察与司机的互动

    A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles

    [https://arxiv.org/abs/2402.01703](https://arxiv.org/abs/2402.01703)

    该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。

    

    政府官员与市民之间的互动影响公共福祉和民主社会的正当性。警察是国家最显而易见、最接触市民的代理人，在交通站停期间，他们每年与公众互动超过2000万次。如今，这些互动经常被戴在身上的摄像机记录下来，这被视为提高警察问责制和改善警民互动的手段。然而，由于缺乏可靠的自动化工具来分析这些复杂而有争议的警民互动，这些记录的及时分析受到了阻碍。本文提出了一种新的多角度、多模态机器学习（ML）工具的方法，用于分析来自这些身上摄像机记录的音频、视频和文字信息。我们的方法首先确定与不同利益相关者最相关的沟通方面，包括共同感知互动的标志标记以及具有这些标记的符号。

    Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
    
[^189]: ReAGent: 一个面向生成语言模型的模型无关特征归因方法

    ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

    [https://arxiv.org/abs/2402.00794](https://arxiv.org/abs/2402.00794)

    本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。

    

    特征归因方法（FAs），如梯度和注意力机制，被广泛应用于确定所有输入特征对模型预测的重要性。现有自然语言处理领域的研究主要集中在为仅有编码器的语言模型（LMs）开发和测试FAs，用于分类任务。然而，尚不清楚在文本生成上是否可以使用这些FAs来处理仅有解码器的模型，因为模型架构和任务设置之间存在固有差异。此外，先前的研究已经证明，没有一个通用的FA适用于所有模型和任务。这使得针对大型LMs选择FA计算上非常昂贵，因为输入重要性的推导通常需要多个前向和反向传递，包括可能是限制性的梯度计算。为了解决这些问题，我们提出了一种面向生成LMs的模型无关FA，称为递归归因生成器（ReAGent）。

    Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
    
[^190]: 短文: 基准测试可转移性对抗攻击

    Short: Benchmarking transferable adversarial attacks

    [https://arxiv.org/abs/2402.00418](https://arxiv.org/abs/2402.00418)

    本研究首次全面评估了可转移性对抗攻击的方面，引入了一个基准框架并系统分类和评估了各种增强对抗攻击可转移性的方法，为不同的模型架构提供了一个标准化的平台。

    

    深度学习模型对抗攻击的稳健性仍然是一个关键关注点。本研究首次全面评估了可转移性对抗攻击的方面。它系统地分类和批判性评估了各种增强对抗攻击可转移性的方法。该研究涵盖了一系列技术，包括生成结构、语义相似性、梯度编辑、目标修改和集成方法。与此同时，本文介绍了一个基准框架"TAA-Bench"，集成了十种主要的对抗攻击可转移性方法，从而为不同的模型架构提供了一个标准化和系统化的比较分析平台。通过全面的审查，我们揭示了每种方法的效力和限制，并阐明了它们的操作原理和实际效用。本综述试图成为一种基于多模型架构进行对比分析的标准化平台。

    The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintesse
    
[^191]: $\mu$GUIDE:一种通过深度学习使用广义不确定性驱动的推断来进行微结构成像的框架

    $\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning

    [https://arxiv.org/abs/2312.17293](https://arxiv.org/abs/2312.17293)

    $\mu$GUIDE是一个通用的贝叶斯框架，利用深度学习进行微结构成像，能够有效估计组织微结构参数的后验分布，并量化参数的不确定性和模糊性。

    

    本文提出了$\mu$GUIDE:一种通用的贝叶斯框架，用于从任何给定的生物物理模型或MRI信号表示中估计组织微结构参数的后验分布，通过扩散加权MRI的示例演示。利用一种新的深度学习架构，用于自动信号特征选择，结合基于模拟的推断和后验分布的高效采样，$\mu$GUIDE绕过了传统贝叶斯方法的高计算和时间成本，并且不依赖采集约束来定义模型特定的摘要统计量。获得的后验分布可突出显示模型定义中存在的退化性，并量化所估计参数的不确定性和模糊性。

    This work proposes $\mu$GUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or MRI signal representation, with exemplar demonstration in diffusion-weighted MRI. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.
    
[^192]: 变化的行动空间中的情境式强化学习

    In-Context Reinforcement Learning for Variable Action Spaces

    [https://arxiv.org/abs/2312.13327](https://arxiv.org/abs/2312.13327)

    本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。

    

    最近的研究表明，预先在多样数据集上进行上下文多情节训练的变形金刚网络可以在情境中泛化到新的强化学习任务。先前提出的模型的一个关键限制是它们依赖于预定义的行动空间大小和结构。引入新的行动空间通常需要数据重新收集和模型重新训练，这对于一些应用来说可能是昂贵的。我们的工作表明，通过提出一种只训练一次的Headless-AD模型，可以缓解这个问题，该模型能够泛化到具有可变大小、语义内容和顺序的离散动作空间。通过在伯努利和上下文赌博机以及一个网格世界环境中进行实验，我们展示了Headless-AD在从未遇到的行动空间上表现出显著的泛化能力，甚至在几个环境配置上胜过专门针对特定行动集训练的模型。

    Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
    
[^193]: LLM精选：在超低数据环境中利用LLMs和数据筛选进行表格增强

    Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes

    [https://arxiv.org/abs/2312.12112](https://arxiv.org/abs/2312.12112)

    本论文提出了CLLM方法，利用LLMs和数据筛选在低数据环境中进行表格增强。通过利用大型语言模型的先验知识以及基于学习动态、置信度和不确定度指标的筛选机制，CLLM取得了优越的性能。

    

    低数据情况下的机器学习（ML）仍然是一个被低估但至关重要的问题。因此，增加ML所需的数据样本大小的数据增强方法对于释放ML在数据匮乏的地区和领域的变革潜力至关重要。不幸的是，有限的训练集限制了传统的表格合成数据生成器在生成ML任务所需的大规模且多样化的增强数据集方面的能力。为了解决这个挑战，我们引入了CLLM，它利用大型语言模型（LLMs）在低数据环境中进行数据增强的先验知识。然而，像任何生成模型一样，并非LLMs生成的所有数据都能提高下游的效用。因此，我们引入了一种基于学习动态、置信度和不确定度指标的原则性筛选机制，以获取高质量的数据集。通过多个真实世界数据集的实证，我们展示了CLLM在低数据环境中的优越性能。

    Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the lo
    
[^194]: 社会学习：朝着与大型语言模型的协作学习

    Social Learning: Towards Collaborative Learning with Large Language Models

    [https://arxiv.org/abs/2312.11441](https://arxiv.org/abs/2312.11441)

    本论文在大型语言模型的背景下引入了“社会学习”的框架，通过自然语言相互共享知识，提出了两种知识传递方法，并证明了这些方法的可行性和效果。

    

    我们在大型语言模型（LLMs）的背景下引入了“社会学习”的框架，在隐私保护的前提下，模型通过使用自然语言相互共享知识。我们提出并评估了两种在LLMs之间进行知识传递的方法。在第一种情况下，我们允许模型生成抽象提示以便教授任务。在第二种方法中，模型通过生成合成示例来传递知识。我们跨多个数据集评估了这些方法，并以记忆化作为隐私损失的代理进行量化。这些受到社会学习启发的技术取得了有希望的结果，原始数据的记忆化程度较低。特别是，我们展示了使用这些方法的性能与使用原始标签和提示的结果相媲美。我们的工作证明了社会学习在LLMs中的可行性，建立了基线方法，并突出了几个未开发的领域供未来研究使用。

    We introduce the framework of "social learning" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.
    
[^195]: 公平性约束能够在多大程度上帮助从有偏差的数据中恢复？

    How Far Can Fairness Constraints Help Recover From Biased Data?

    [https://arxiv.org/abs/2312.10396](https://arxiv.org/abs/2312.10396)

    公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。

    

    一般认为，在公平分类中，公平性约束会导致准确性的减少，而有偏差的数据可能会加剧这种情况。然而，Blum＆Stangl（2019）的研究表明，在极度有偏差的数据上，即使采用平等机会约束，也可以恢复到原始数据分布上准确和公平的分类器。他们的研究结果很有趣，因为它证明了公平性约束可以隐式修正数据偏差，同时克服了公平性与准确性之间的平衡问题。他们的数据偏差模型模拟了受压迫人群的表征和标签偏见，并在具有独立标签噪声的简单条件下，针对一个理想化的数据分布展示了上述结果。我们提出了一种通用方法，以扩展Blum＆Stangl（2019）的结果，适用于不同的公平性约束、数据偏差模型、数据分布和假设类别。

    A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum & Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum & Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
    
[^196]: 基于事件的对比学习用于医学时间序列

    Event-Based Contrastive Learning for Medical Time Series

    [https://arxiv.org/abs/2312.10308](https://arxiv.org/abs/2312.10308)

    本论文介绍了一种基于事件的对比学习方法（EBCL），用于学习医学时间序列中关键事件前后的数据编码。研究发现，相对于其他预训练方法，EBCL能够产生性能更好的模型，对于心力衰竭队列的关键下游任务具有更好的微调性能，并能有效地将具有相似风险的患者进行聚类。

    

    在临床实践中，我们经常需要确定某个关键医学事件后患者是否处于不良结果的高风险状态，例如心力衰竭入院后的短期死亡风险。这个任务由于长期医学数据的复杂性、变异性和异质性而具有挑战性，特别是对于像心力衰竭这样的慢性疾病患者。在本文中，我们引入了基于事件的对比学习（EBCL）方法，用于学习不同类型患者数据的嵌入表示，以保留关键索引事件前后的时间信息。我们证明，相对于其他预训练方法，EBCL产生的模型在心力衰竭队列的关键下游任务（包括30天再入院、1年死亡率和1周住院天数）的微调性能更好。我们的研究还揭示了EBCL预训练单独能够有效地将具有相似死亡率和再入院风险的患者进行聚类，从而提供了新的见解。

    In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event; for example, the short-term risk of death after an admission for heart failure. This task is challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL), a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL produces models that yield better fine-tuning performance on critical downstream tasks for a heart failure cohort, including 30-day readmission, 1-year mortality, and 1-week length of stay, relative to other pretraining methods. Our findings also reveal that EBCL pretraining alone can effectively cluster patients with similar mortality and readmission risks, offering 
    
[^197]: 光谱状态空间模型

    Spectral State Space Models

    [https://arxiv.org/abs/2312.06837](https://arxiv.org/abs/2312.06837)

    本文提出了一种称为光谱状态空间模型的序列预测架构，通过学习具有光谱滤波算法的线性动态系统实现。这些模型具有可证明的鲁棒性和固定卷积滤波器，适用于需要非常长程记忆的预测任务。

    

    本文研究了具有长程依赖关系的预测任务的序列建模。我们提出了一种基于学习具有光谱滤波算法的线性动态系统的新形式化状态空间模型（SSMs）。这导致了一种称为光谱状态空间模型的新颖序列预测架构。光谱状态空间模型具有两个主要优势。首先，它们具有可证明的鲁棒性属性，因为它们的性能既不依赖于底层动力学的频谱，也不依赖于问题的维度。其次，这些模型是通过固定的卷积滤波器构建的，不需要学习，同时在理论和实践中仍然优于SSMs。基于光谱过滤算法的Spectral state space models在合成动态系统和各种模态的长程预测任务上进行了评估。这些评估支持了光谱滤波在需要非常长程记忆的任务中的理论优势。

    This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model.   Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.   The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.
    
[^198]: SVQ: 稀疏向量量化用于时空预测

    SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting

    [https://arxiv.org/abs/2312.03406](https://arxiv.org/abs/2312.03406)

    SVQ是一种利用稀疏回归实现简明表示的稀疏向量量化技术，通过保留关键细节和滤除噪声来提高时空预测性能。在实验中，SVQ在五个空间-时间基准数据集上取得了最先进的结果。

    

    时空预测在许多领域中都是关键，取得好的预测结果需要找到微妙的模式并滤除噪声。为了解决这个问题，我们介绍了稀疏回归向量量化（SVQ）这一新技术，它利用稀疏回归来实现简明表示，这一方法在理论和实践上都比传统的基于聚类的向量量化方法更有优势。这种方法使用回归模型保留原始向量的关键细节，同时通过稀疏设计滤除噪声。此外，我们使用两层MLP和一个广泛的码本来近似稀疏回归过程。这种方法不仅大大降低了计算成本，还使得SVQ具有可微性和训练简易性，从而显著提高了性能。我们在五个空间-时间基准数据集上进行的实证研究表明，SVQ取得了最先进的结果。具体来说，在

    Spatio-temporal forecasting, pivotal in numerous fields, hinges on the delicate equilibrium between isolating nuanced patterns and sifting out noise. To tackle this, we introduce Sparse Regression-based Vector Quantization (SVQ), a novel technique that leverages sparse regression for succinct representation, an approach theoretically and practically favored over classical clustering-based vector quantization methods. This approach preserves critical details from the original vectors using a regression model while filtering out noise via sparse design. Moreover, we approximate the sparse regression process using a blend of a two-layer MLP and an extensive codebook. This approach not only substantially cuts down on computational costs but also grants SVQ differentiability and training simplicity, resulting in a notable enhancement of performance. Our empirical studies on five spatial-temporal benchmark datasets demonstrate that SVQ achieves state-of-the-art results. Specifically, on the 
    
[^199]: 当准确的预测模型导致有害的自我实现预言

    When accurate prediction models yield harmful self-fulfilling prophecies

    [https://arxiv.org/abs/2312.01210](https://arxiv.org/abs/2312.01210)

    本研究通过调查预测模型的部署对决策产生有害影响的情况，发现这些模型可能会成为有害的自我实现预言。这些模型不会因为对某些患者造成更糟糕的结果而使其预测能力变无效。

    

    目标：预测模型在医学研究和实践中非常受欢迎。通过为特定患者预测感兴趣的结果，这些模型可以帮助决策困难的治疗决策，并且通常被誉为个性化的、数据驱动的医疗保健的杰出代表。许多预测模型在验证研究中基于其预测准确性而部署用于决策支持。我们调查这是否是一种安全和有效的方法。材料和方法：我们展示了使用预测模型进行决策可以导致有害的决策，即使在部署后这些预测表现出良好的区分度。这些模型是有害的自我实现预言：它们的部署损害了一群患者，但这些患者的更糟糕的结果并不使模型的预测能力无效。结果：我们的主要结果是对这些预测模型集合的形式化描述。接下来，我们展示了在部署前后都进行了良好校准的模型

    Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach.   Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model.   Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment 
    
[^200]: 多分辨率建模的Transformers：适用于一般时间序列预测的自适应多分辨率建模

    MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting

    [https://arxiv.org/abs/2311.18780](https://arxiv.org/abs/2311.18780)

    本文提出了一种名为MultiResFormer的模型，通过自适应选择最优补丁长度，动态建模时序变化。相比于基于补丁的Transformer和CNN基线，MultiResFormer在长期预测任务上表现出更好的性能。

    

    基于Transformer的模型近期在时间序列预测方面取得了重大突破。现有方法通常使用一个固定的或固定集合的补丁长度来将时间序列数据进行编码。然而，这可能导致无法捕捉到现实世界中多周期时间序列中存在的多样细致的时间依赖性。在本文中，我们提出了MultiResFormer，通过自适应地选择最优的补丁长度来动态建模时序变化。具体来说，在每个层的开始时，时间序列数据被编码为几个并行的分支，每个分支使用检测到的周期性，然后通过Transformer编码块。我们对长期和短期预测数据集进行了广泛评估，将MultiResFormer与最先进的基线进行比较。MultiResFormer在长期预测任务上优于基于补丁的Transformer基准，并且始终显著优于CNN基线，同时使用的资源更少。

    Transformer-based models have greatly pushed the boundaries of time series forecasting recently. Existing methods typically encode time series data into $\textit{patches}$ using one or a fixed set of patch lengths. This, however, could result in a lack of ability to capture the variety of intricate temporal dependencies present in real-world multi-periodic time series. In this paper, we propose MultiResFormer, which dynamically models temporal variations by adaptively choosing optimal patch lengths. Concretely, at the beginning of each layer, time series data is encoded into several parallel branches, each using a detected periodicity, before going through the transformer encoder block. We conduct extensive evaluations on long- and short-term forecasting datasets comparing MultiResFormer with state-of-the-art baselines. MultiResFormer outperforms patch-based Transformer baselines on long-term forecasting tasks and also consistently outperforms CNN baselines by a large margin, while usi
    
[^201]: SMaRt: 使用分数匹配规则改进GANs

    SMaRt: Improving GANs with Score Matching Regularity

    [https://arxiv.org/abs/2311.18208](https://arxiv.org/abs/2311.18208)

    本文提出使用分数匹配规则（SMaRt）来改进GANs的优化问题，通过持续将生成的数据点推向真实数据流形，提高了合成性能。

    

    生成对抗网络（GANs）通常在学习高度多样化的复杂数据时遇到困难。本文重新审视了GANs的数学基础，并从理论上揭示了GAN训练的原始对抗损失不能解决生成数据流形的正测度子集落在真实数据流形之外的问题。相反，我们发现分数匹配可以作为解决这个问题的有希望的方法，因为它可以持续将生成的数据点推向真实数据流形。因此，我们提出通过分数匹配规则（SMaRt）来改进GANs的优化。对于经验证据，我们首先设计了一个玩具示例来展示通过辅助一个真实得分函数来训练GANs可以更准确地再现真实数据分布，然后确认我们的方法可以持续提升各种状态的合成性能。

    Generative adversarial networks (GANs) usually struggle in learning from highly diverse data, whose underlying manifold is complex. In this work, we revisit the mathematical foundations of GANs, and theoretically reveal that the native adversarial loss for GAN training is insufficient to fix the problem of subsets with positive Lebesgue measure of the generated data manifold lying out of the real data manifold. Instead, we find that score matching serves as a promising solution to this issue thanks to its capability of persistently pushing the generated data points towards the real data manifold. We thereby propose to improve the optimization of GANs with score matching regularity (SMaRt). Regarding the empirical evidences, we first design a toy example to show that training GANs by the aid of a ground-truth score function can help reproduce the real data distribution more accurately, and then confirm that our approach can consistently boost the synthesis performance of various state-o
    
[^202]: DroneOptiNet: 一种用于5G及其后太阳能小型蜂窝网络的最佳无人机负载重分配机制的框架

    DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks

    [https://arxiv.org/abs/2311.12944](https://arxiv.org/abs/2311.12944)

    本研究提出了一种用于5G及其后太阳能小型蜂窝网络的最佳无人机负载重分配机制，通过使用无人机上的空中基站进行可靠安全的电力再分配，提高了网络的可靠性和稳健性。

    

    第五代及其后的蜂窝网络对功率需求提出了重要的限制，需要能够高效利用能源的解决方案。在本研究中，我们提出了一种新颖的使用无人机上的空中基站（BS）进行可靠安全的电力再分配的用户负载转移方法，以跨越由绿色小型蜂窝BS组成的微网网络。根据用户密度和空中基站的可用性，通过将空中基站从高能耗小区迁移到低能耗小区，来满足能量不足的小区的能量需求。所提出的混合无人机框架将长短期记忆与独特的成本函数结合，使用进化神经网络来有效地管理无人机和基站的能量和负载重分配。所提出的算法减少了基站停电，并保持了一致的吞吐量稳定性，从而展示了其提升无线网络可靠性和稳健性的能力。

    The power requirements posed by the fifth-generation and beyond cellular networks are an important constraint in network deployment and require energy-efficient solutions. In this work, we propose a novel user load transfer approach using airborne base stations (BS) mounted on drones for reliable and secure power redistribution across the micro-grid network comprising green small cell BSs. Depending on the user density and the availability of an aerial BS, the energy requirement of a cell with an energy deficit is accommodated by migrating the aerial BS from a high-energy to a low-energy cell. The proposed hybrid drone-based framework integrates long short-term memory with unique cost functions using an evolutionary neural network for drones and BSs and efficiently manages energy and load redistribution. The proposed algorithm reduces power outages at BSs and maintains consistent throughput stability, thereby demonstrating its capability to boost the reliability and robustness of wirel
    
[^203]: 通过自注意力建模选择

    Modeling Choice via Self-Attention

    [https://arxiv.org/abs/2311.07607](https://arxiv.org/abs/2311.07607)

    本论文提出了一种选择模型，利用自注意力成功地进行了建模，这是在深度学习和选择建模领域中的一个重要的研究空白。

    

    选择模型是运营管理领域中许多经典优化问题的基础输入，包括组合、库存和定价优化。准确地从数据中估计这些模型是在实践中应用这些优化问题的关键步骤。与此同时，深度学习的最新进展引起了将这些技术整合到选择建模中的兴趣。然而，在深度学习和选择建模的交叉点上存在明显的研究空白，尤其是在理论和经验基础上。因此，我们首先提出了一种选择模型，这是第一个成功（从理论和实践两个方面）利用现代神经网络架构概念（自注意力）的模型。在理论上，我们证明了我们基于注意力的选择模型是Halo多项式逻辑（Halo-MNL）模型的低秩推广。我们证明了Halo-MNL模型需要$\Omega(m^2)$的计算量，而我们的模型只需要$\Omega(m)$的计算量。

    Models of choice are a fundamental input to many now-canonical optimization problems in the field of Operations Management, including assortment, inventory, and price optimization. Naturally, accurate estimation of these models from data is a critical step in the application of these optimization problems in practice. Concurrently, recent advancements in deep learning have sparked interest in integrating these techniques into choice modeling. However, there is a noticeable research gap at the intersection of deep learning and choice modeling, particularly with both theoretical and empirical foundations. Thus motivated, we first propose a choice model that is the first to successfully (both theoretically and practically) leverage a modern neural network architectural concept (self-attention). Theoretically, we show that our attention-based choice model is a low-rank generalization of the Halo Multinomial Logit (Halo-MNL) model. We prove that whereas the Halo-MNL requires $\Omega(m^2)$ d
    
[^204]: TATA: 通过主题无关和主题感知的嵌入进行立场检测

    TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings

    [https://arxiv.org/abs/2310.14450](https://arxiv.org/abs/2310.14450)

    本研究提出了一种通过对比学习以及利用未标记新闻文章数据集来训练主题无关和主题感知嵌入的方法，在立场检测中取得了最先进的性能，并在公开数据集上达到了0.771的F1分数。

    

    立场检测对于理解互联网上不同的态度和信仰很重要。然而，鉴于一篇文章对给定主题的立场往往高度依赖于该主题，建立一个能推广到未知主题的立场检测模型是困难的。在这项工作中，我们提出使用对比学习以及涵盖了各种不同主题的未标记新闻文章数据集来训练用于下游立场检测的主题无关（TAG）和主题感知（TAW）的嵌入。将这些嵌入组合在我们的完整TATA模型中，我们在几个公开的立场检测数据集上实现了最先进的性能（在Zero-shot VAST数据集上达到0.771的F1分数）。我们在https://github.com/hanshanley/tata发布了我们的代码和数据。

    Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 $F_1$-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata.
    
[^205]: Lie神经元：半单Lie代数的伴随等变神经网络

    Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras

    [https://arxiv.org/abs/2310.04521](https://arxiv.org/abs/2310.04521)

    本文提出了一种Lie神经元网络，能够以任何半单Lie代数数据为输入，通过伴随操作使其具有等变性。通过推广向量神经元网络和引入新的层，该网络在各个领域具有广泛的适用性和竞争性能。

    

    本文提出了一种等变神经网络，将数据作为输入，该数据存在于任何半单Lie代数中。对应的群通过伴随操作作用于Lie代数上，使得我们提出的网络具有伴随等变性。我们的框架将简单的$\mathrm{SO}(3)$-等变网络——向量神经元从3维欧几里得空间推广到Lie代数空间，利用Killing形式的不变性质。此外，我们还提出了新颖的Lie括号层和几何通道混合层来扩展建模能力。实验在多个任务上对$\mathfrak{so}(3)$和$\mathfrak{sl}(3)$ Lie代数进行了验证，包括拟合等变和不变函数、学习系统动力学、点云配准和基于单应性的形状分类。我们提出的等变网络在各个领域都表现出广泛的适用性和竞争性能。

    This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
    
[^206]: 生物神经网络的学习是基于随机梯度下降的吗？使用随机过程进行分析

    Is Learning in Biological Neural Networks based on Stochastic Gradient Descent? An analysis using stochastic processes

    [https://arxiv.org/abs/2309.05102](https://arxiv.org/abs/2309.05102)

    本文研究了随机梯度下降在生物神经网络中的应用，并展示了每个学习机会经过多次局部更新后近似进行梯度步骤。这一结果表明，随机梯度下降可能在优化生物神经网络中起到作用。

    

    近年来，关于生物神经网络（BNNs）中学习与人工神经网络中学习的区别一直存在激烈的争论。通常认为，大脑中连接的更新仅依赖于局部信息，因此不能使用随机梯度下降类型的优化方法。本文研究了BNNs中监督学习的随机模型。我们展示了当每个学习机会通过许多局部更新进行处理时，（连续的）梯度步骤近似发生。这一结果表明，随机梯度下降可能在优化BNNs中起到一定作用。

    In recent years, there has been an intense debate about how learning in biological neural networks (BNNs) differs from learning in artificial neural networks. It is often argued that the updating of connections in the brain relies only on local information, and therefore a stochastic gradient-descent type optimization method cannot be used. In this paper, we study a stochastic model for supervised learning in BNNs. We show that a (continuous) gradient step occurs approximately when each learning opportunity is processed by many local updates. This result suggests that stochastic gradient descent may indeed play a role in optimizing BNNs.
    
[^207]: OHQ: 芯片上的硬件感知量化

    OHQ: On-chip Hardware-aware Quantization

    [https://arxiv.org/abs/2309.01945](https://arxiv.org/abs/2309.01945)

    本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。

    

    量化成为在资源受限的硬件上部署先进深度模型的最有前途的方法之一。混合精度量化利用多位宽架构来释放量化模型的准确性和效率潜力。然而，现有的混合精度量化存在搜索空间过大的问题，导致巨大的计算开销。因此，量化过程依赖于独立的高性能设备，而不是本地进行，这也导致了考虑的硬件指标与实际部署之间的显著差距。本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），而无需访问在线设备。首先，我们构建了芯片上的量化感知（OQA）流水线，能够感知量化算子在硬件上的实际效率指标。其次，我们提出了基于掩码引导的量化估计（MQE）技术。

    Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
    
[^208]: Lookbehind-SAM: k步回望，1步前进

    Lookbehind-SAM: k steps back, 1 step forward

    [https://arxiv.org/abs/2307.16704](https://arxiv.org/abs/2307.16704)

    本研究提出了一种名为Lookbehind-SAM的方法，通过多次上升步骤和线性插值来增强最大化和最小化过程，以实现更好的损失锐度折衷。实验证明，该方法在各种任务中都有多种优点，包括提高的泛化性能、更高的鲁棒性和改进的学习过程。

    

    锐度感知优化（SAM）方法通过将最小化损失值和损失锐度问题表述为极小极大型目标，得到了越来越多的关注。在本研究中，我们增加了SAM目标中最大化和最小化部分的效率，以实现更好的损失锐度折衷。受Lookahead优化器的启发，该优化器使用多个向前的下降步骤，我们提出了Lookbehind，它在后面执行多个上升步骤，增强了SAM的最大化步骤，并找到了一个具有更高损失的最坏情况扰动。然后，为了减小由于收集到的多个上升步骤的梯度所引起的下降步骤的方差，我们采用线性插值来改进最小化过程。Lookbehind在各种任务中带来了许多好处。特别是，我们展示了提高的泛化性能，对噪声权重的更高鲁棒性，以及在学习过程中改进的效果和较少的灾难性遗忘。

    Sharpness-aware minimization (SAM) methods have gained increasing popularity by formulating the problem of minimizing both loss value and loss sharpness as a minimax objective. In this work, we increase the efficiency of the maximization and minimization parts of SAM's objective to achieve a better loss-sharpness trade-off. By taking inspiration from the Lookahead optimizer, which uses multiple descent steps ahead, we propose Lookbehind, which performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss. Then, to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step. Lookbehind leads to a myriad of benefits across a variety of tasks. Particularly, we show increased generalization performance, greater robustness against noisy weights, as well as improved learning and less catastrophic forgetting in l
    
[^209]: AnimateDiff：无需特定调整即可使个性化的文本到图像扩散模型生成动画

    AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning

    [https://arxiv.org/abs/2307.04725](https://arxiv.org/abs/2307.04725)

    AnimateDiff是一个实用的框架，可以动画化个性化的T2I模型，无需特定调整。它包含一个即插即用的运动模块，通过训练使用实际视频中的运动先验。运动模块可以插入到任何个性化T2I模型中，从而形成一个个性化的动画生成器。

    

    随着文本到图像（T2I）扩散模型（如稳定扩散）和相应的个性化技术（如DreamBooth和LoRA）的推动，每个人都能以合理的成本将他们的想象力展现为高质量的图像。然而，将运动动态添加到现有高质量个性化T2I并使其生成动画仍然是一个公开的挑战。在本文中，我们提出了AnimateDiff，这是一个实用的框架，可以动画化个性化的T2I模型而无需特定的调整。我们框架的核心是一个即插即用的运动模块，只需进行一次训练，即可无缝地集成到来自相同基础T2I的任何个性化T2I中。通过我们提出的训练策略，运动模块可以有效地从现实世界的视频中学习可转移的运动先验。一旦训练完成，该运动模块可以插入到个性化T2I模型中，形成一个个性化的动画生成器。我们进一步提出了MotionLoRA，一种轻量级的...（摘要未完成）

    With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fi
    
[^210]: 可训练的Transformer in Transformer

    Trainable Transformer in Transformer

    [https://arxiv.org/abs/2307.01189](https://arxiv.org/abs/2307.01189)

    这篇论文介绍了一种名为Transformer in Transformer (TinT)的高效构造方式，它可以让Transformer在推理过程中模拟和微调复杂的内部模型，同时使用创新的近似技术大幅减少了模型参数和内存开销。

    

    最近的研究将大型预训练语言模型中的上下文学习能力归因于在推理过程中隐式模拟和微调内部模型（如线性或2层MLP）。然而，这种构造需要大量的内存开销，使得模拟更复杂的内部模型变得困难。在这项工作中，我们提出了一种高效的构造方式，称为Transformer in Transformer（简称TinT），它允许一个Transformer在推理过程中模拟和微调复杂的内部模型（如预训练语言模型）。特别是，我们引入了创新的近似技术，使得一个拥有不到20亿参数的TinT模型能够在单次前向传递中模拟和微调一个拥有1.25亿参数的Transformer模型。TinT适用于许多常见的Transformer变体，其设计思路还改进了Transformer中简单模型的效率。我们进行了端到端实验来验证...

    Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validat
    
[^211]: 在具有可数无限状态空间的马尔可夫决策过程中的贝叶斯学习最优策略

    Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space

    [https://arxiv.org/abs/2306.02574](https://arxiv.org/abs/2306.02574)

    本文研究了马尔可夫决策过程中具有可数无限状态空间的最优控制问题，提出了基于汤普森采样和动态大小片段的算法进行贝叶斯学习，解决了在这些模型上的挑战。

    

    很多现实应用的模型，如通信网络或计算系统的排队模型，都具有可数无限状态空间。目前已经开发的算法和学习过程主要针对有限状态设置，并不能直接应用于这些模型。为了解决这个问题，本文研究了在一个未知参数θ∈Θ控制下的家族离散时间可数状态空间马尔可夫决策过程(MDP)的最优控制问题，该过程定义在可数无限状态空间Xυ={Z+}d上，具有有限动作空间Aυ以及无界成本函数。我们采用贝叶斯观点，将随机未知参数θ*作为给定先验分布在Θ上生成。为了最优地控制未知的MDP，我们提出了一种基于汤普森采样和动态大小片段的算法：在每个片段的开始，根据后验概率分布选择动作，并在每个片段结束时更新后验概率分布。

    Models of many real-life applications, such as queuing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\theta\in\Theta$, and defined on a countably-infinite state space $\mathcal X=\mathbb{Z}_+^d$, with finite action space $\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\boldsymbol{\theta}^*$ generated via a given fixed prior distribution on $\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior
    
[^212]: ParlayANN: 可扩展和确定性的并行基于图的近似最近邻搜索算法

    ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms

    [https://arxiv.org/abs/2305.04359](https://arxiv.org/abs/2305.04359)

    ParlayANN是一个具有确定性和并行性的基于图的近似最近邻搜索算法库，提供了用于开发这类算法的一套有用工具。

    

    近似最近邻搜索（ANNS）算法是现代深度学习系统的关键部分，因为它们可以在高维向量空间表示（即嵌入）的数据上实现高效的相似性搜索。在各种ANNS算法中，基于图的算法被认为在吞吐量和召回率之间取得了最佳的平衡。尽管现代ANNS数据集具有较大规模，但现有的并行基于图的实现由于大量使用锁和其他顺序瓶颈而存在显著的挑战，这两点阻碍了它们有效扩展到大量处理器，并导致在某些应用中不希望出现的非确定性。

    Approximate nearest-neighbor search (ANNS) algorithms are a key part of the modern deep learning stack due to enabling efficient similarity search over high-dimensional vector space representations (i.e., embeddings) of data. Among various ANNS algorithms, graph-based algorithms are known to achieve the best throughput-recall tradeoffs. Despite the large scale of modern ANNS datasets, existing parallel graph based implementations suffer from significant challenges to scale to large datasets due to heavy use of locks and other sequential bottlenecks, which 1) prevents them from efficiently scaling to a large number of processors, and 2) results in nondeterminism that is undesirable in certain applications.   In this paper, we introduce ParlayANN, a library of deterministic and parallel graph-based approximate nearest neighbor search algorithms, along with a set of useful tools for developing such algorithms. In this library, we develop novel parallel implementations for four state-of-th
    
[^213]: DSD$^2$: 我们能够避免稀疏双下降并无忧地压缩神经网络吗？

    DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural Network Worry-Free?

    [https://arxiv.org/abs/2303.01213](https://arxiv.org/abs/2303.01213)

    DSD$^2$提出了一个学习框架，能够避免稀疏双下降现象，并提高模型的泛化性能；引入了熵度量，使得可以使用传统的停止准则；对相关因素进行了全面的定量分析。

    

    最新的研究表明，现代深度学习模型可能会出现稀疏双下降现象。确实，随着模型的稀疏性增加，测试性能首先变差，因为模型过拟合训练数据；然后，过拟合减少，导致性能改善；最后，模型开始遗忘关键信息，导致欠拟合。这种行为阻止了使用传统的提前停止准则。在这项工作中，我们有三个关键贡献。首先，我们提出了一种学习框架，避免了这种现象并提高了泛化性能。其次，我们引入了一种熵度量，提供了对这种现象出现的更多洞察，并使得可以使用传统的停止准则。第三，我们对重新初始化方法、模型宽度和深度以及数据集噪声等相关因素进行了全面的定量分析。这些贡献得到了典型设置中的经验证据支持。

    Neoteric works have shown that modern deep learning models can exhibit a sparse double descent phenomenon. Indeed, as the sparsity of the model increases, the test performance first worsens since the model is overfitting the training data; then, the overfitting reduces, leading to an improvement in performance, and finally, the model begins to forget critical information, resulting in underfitting. Such a behavior prevents using traditional early stop criteria. In this work, we have three key contributions. First, we propose a learning framework that avoids such a phenomenon and improves generalization. Second, we introduce an entropy measure providing more insights into the insurgence of this phenomenon and enabling the use of traditional stop criteria. Third, we provide a comprehensive quantitative analysis of contingent factors such as re-initialization methods, model width and depth, and dataset noise. The contributions are supported by empirical evidence in typical setups. Our cod
    
[^214]: HardSATGEN：理解难SAT公式生成的困难和强的结构困难感知基线

    HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline

    [https://arxiv.org/abs/2302.02104](https://arxiv.org/abs/2302.02104)

    HardSATGEN方法提出了一种精细的控制机制，以更好地恢复工业基准的结构和计算特性，此方法在工业SAT公式生成任务中表现出优越性。

    

    工业SAT公式生成是一项关键但具有挑战性的任务。现有的SAT生成方法很难同时捕捉全局结构特性并保持合理的计算困难度。我们首先对之前学习方法在重现原始实例的计算困难度方面的限制进行了深入分析，这可能源自其采用的分裂合并过程的内在同质性。基于工业公式呈现明确社区结构和过度分割子结构导致逻辑结构语义形成困难的观察，我们提出了HardSATGEN，在SAT公式生成的神经分裂合并范例中引入了精细的控制机制，以更好地恢复工业基准的结构和计算特性。包括私人和实际企业测试基准评估在内的实验表明HardSATGEN的优越性，成为唯一一种能够

    Industrial SAT formula generation is a critical yet challenging task. Existing SAT generation approaches can hardly simultaneously capture the global structural properties and maintain plausible computational hardness. We first present an in-depth analysis for the limitation of previous learning methods in reproducing the computational hardness of original instances, which may stem from the inherent homogeneity in their adopted split-merge procedure. On top of the observations that industrial formulae exhibit clear community structure and oversplit substructures lead to the difficulty in semantic formation of logical structures, we propose HardSATGEN, which introduces a fine-grained control mechanism to the neural split-merge paradigm for SAT formula generation to better recover the structural and computational properties of the industrial benchmarks. Experiments including evaluations on private and practical corporate testbed show the superiority of HardSATGEN being the only method to
    
[^215]: 无监督的多视角几何下的3D关键点发现

    Unsupervised 3D Keypoint Discovery with Multi-View Geometry

    [https://arxiv.org/abs/2211.12829](https://arxiv.org/abs/2211.12829)

    本文提出了一种无监督算法，利用多视图几何约束，从多视图图像中学习发现人体的3D关键点，并通过重新投影到每个视图来估计3D关键点的准确性和可解释性。在多个基准数据集上，与其他无监督方法相比，我们的方法发现了更准确和具有解释性的3D关键点。

    

    分析和训练3D身体姿势模型在很大程度上依赖于关节标签的可用性，这些标签通常通过繁琐的手动标注或者通过经过精心策划的标记和捕捉系统的基于标记的关节定位来获得。然而，这样的标注并不总是可用的，特别是对于进行不寻常活动的人。在本文中，我们提出了一种算法，学习从多视图图像中发现人体的3D关键点，而无需任何监督或标签，仅依靠多视图几何提供的约束。为了确保发现的3D关键点是有意义的，它们被重新投影到每个视图以估计模型在没有监督的情况下最初估计的人的掩模。我们的方法在Human3.6M和MPI-INF-3DHP基准数据集上比其他最先进的无监督方法发现了更有解释性和更准确的3D关键点。

    Analyzing and training 3D body posture models depend heavily on the availability of joint labels that are commonly acquired through laborious manual annotation of body joints or via marker-based joint localization using carefully curated markers and capturing systems. However, such annotations are not always available, especially for people performing unusual activities. In this paper, we propose an algorithm that learns to discover 3D keypoints on human bodies from multiple-view images without any supervision or labels other than the constraints multiple-view geometry provides. To ensure that the discovered 3D keypoints are meaningful, they are re-projected to each view to estimate the person's mask that the model itself has initially estimated without supervision. Our approach discovers more interpretable and accurate 3D keypoints compared to other state-of-the-art unsupervised approaches on Human3.6M and MPI-INF-3DHP benchmark datasets.
    
[^216]: 在次线性时间内训练过参数化的神经网络

    Training Overparametrized Neural Networks in Sublinear Time

    [https://arxiv.org/abs/2208.04508](https://arxiv.org/abs/2208.04508)

    这项研究提出了一种新的训练方法，可以在次线性时间内训练过参数化的神经网络，提高了训练的效率。

    

    深度学习的成功付出了巨大的计算和能源成本，训练过参数化的神经网络的可扩展性正在成为人工智能进展的真正障碍。尽管传统的反向传播通过梯度下降的成本每次迭代很低，但随机梯度下降在非凸设置中具有禁止的收敛速度，无论是在理论上还是实践中。为了缓解这种成本，最近的研究提出了采用具有更快收敛速度但每次迭代成本更高的替代（牛顿类型）训练方法。对于具有$m=\mathrm{poly}(n)$个参数和输入批次$n$个数据点在$\mathbb{R}^d$中的典型神经网络，[Brand, Peng, Song, and Weinstein, ITCS'2021]的先前工作每次迭代需要$\sim mnd+n^3$的时间。在本文中，我们提出了一种新的训练方法，只需要$m^{1-\alpha}nd+n^3$的摊销时间，与先前的方法相比具有更高的效率。

    The success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of artificial intelligence (AI). Despite the popularity and low cost-per-iteration of traditional backpropagation via gradient decent, stochastic gradient descent (SGD) has prohibitive convergence rate in non-convex settings, both in theory and practice.   To mitigate this cost, recent works have proposed to employ alternative (Newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. For a typical neural network with $m=\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\mathbb{R}^d$, the previous work of [Brand, Peng, Song, and Weinstein, ITCS'2021] requires $\sim mnd + n^3$ time per iteration. In this paper, we present a novel training method that requires only $m^{1-\alpha} n d + n^3$ amortized time in the same overparametri
    
[^217]: 个性化PCA:解耦共享和独特特征

    Personalized PCA: Decoupling Shared and Unique Features

    [https://arxiv.org/abs/2207.08041](https://arxiv.org/abs/2207.08041)

    本文介绍了个性化PCA（PerPCA），通过使用全局和局部主成分来编码独特和共享特征，解决了PCA面临的异质性挑战。我们证明，在温和的条件下，我们可以通过受约束优化问题识别和恢复出独特和共享的特征。我们还设计了一个联邦算法来解决这个问题，并证明了算法的线性收敛性。

    

    在这篇论文中，我们解决了PCA面临的一个重要挑战：异质性。当数据来自不同的来源，具有异质的趋势，同时仍然共享某些一致性时，关键是提取共享的知识，同时保留每个来源的独特特征。为此，我们提出了个性化PCA（PerPCA），它使用互相正交的全局和局部主成分来编码独特和共享的特征。我们证明，在温和的条件下，即使协方差矩阵极其不同，也可以通过受约束优化问题识别和恢复出独特和共享的特征。此外，我们设计了一个完全的联邦算法，灵感来自分布式Stiefel梯度下降，用于解决这个问题。该算法引入了一种称为广义回缩的新操作组来处理正交约束，只需要在来源之间共享全局主成分。我们证明了算法在适当的假设下具有线性收敛性。

    In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumpt
    
[^218]: Matryoshka表示学习

    Matryoshka Representation Learning

    [https://arxiv.org/abs/2205.13147](https://arxiv.org/abs/2205.13147)

    Matryoshka表示学习（MRL）是一种灵活的表示学习方法，能够在不同计算资源下适应多个下游任务，同时保持准确性和丰富性。

    

    学习得到的表示是现代机器学习系统中的关键组件，用于服务各种下游任务。在训练这种表示时，往往无法确定每个下游任务的计算和统计约束。在这种情况下，刚性的固定容量表示可能过度或不足地适应当前任务。这使我们产生了一个问题：我们能否设计一种灵活的表示，以适应具有不同计算资源的多个下游任务？我们的主要贡献是Matryoshka表示学习（MRL），它在不同的粒度上编码信息，并允许单个嵌入适应下游任务的计算约束。MRL对现有的表示学习流程进行最小修改，并在推理和部署过程中不增加任何额外成本。MRL学习的粗粒度到细粒度的表示至少与独立训练的低维表示一样准确和丰富。

    Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The f
    
[^219]: 信用评分模型的公平性

    The Fairness of Credit Scoring Models

    [https://arxiv.org/abs/2205.10200](https://arxiv.org/abs/2205.10200)

    本论文研究了信用评分模型的公平性问题，提出了一种形式化测试算法公平性的方法，并探索了影响公平性的变量。研究结果可以指导信贷商监测算法公平性、监管机构控制公平性，同时提高受保护群体的利益，同时保持高水平的预测准确性。

    

    在信用市场中，筛选算法的目标是区分好类型和坏类型的借款人。然而，在这样做的过程中，他们还可能在具有受保护属性的个体（例如性别、年龄、种族起源）和整个人群之间进行歧视。这可能是无意识的，来源于训练数据集或模型本身。我们展示了如何形式化测试评分模型的算法公平性，以及如何确定影响公平性不足的变量。然后，我们利用这些变量来优化公平性和性能之间的权衡。我们的框架提供了关于如何监测信贷商的算法公平性、如何由监管机构控制、如何改善受保护群体的利益，同时仍保持高水平的预测准确性的指导。

    In credit markets, screening algorithms aim to discriminate between good-type and bad-type borrowers. However, when doing so, they can also discriminate between individuals sharing a protected attribute (e.g. gender, age, racial origin) and the rest of the population. This can be unintentional and originate from the training dataset or from the model itself. We show how to formally test the algorithmic fairness of scoring models and how to identify the variables responsible for any lack of fairness. We then use these variables to optimize the fairness-performance trade-off. Our framework provides guidance on how algorithmic fairness can be monitored by lenders, controlled by their regulators, improved for the benefit of protected groups, while still maintaining a high level of forecasting accuracy.
    
[^220]: 通过自我解释引导的强化学习从模棱两可的示范中学习

    Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning

    [https://arxiv.org/abs/2110.05286](https://arxiv.org/abs/2110.05286)

    本论文提出了一个用于训练强化学习代理的自我解释从示范中学习的框架（SERLfD）。通过代理人生成自我解释并识别有价值的高级关系特征作为成功轨迹的解释原因，可以在克服示范中存在的模棱两可情况的同时为强化学习提供指导。

    

    我们的工作旨在有效地利用模棱两可的示范来训练强化学习代理。一个模棱两可的示范通常可以有多重解释，这严重阻碍了强化学习代理的稳定和高效学习。由于最佳示范也可能存在模棱两可的问题，之前将强化学习与从示范中学习相结合的工作可能无法很好地工作。受到人类处理这种情况的启发，我们提出使用自我解释（代理人为自身生成解释）来识别有价值的高级关系特征，作为成功轨迹成功的解释原因。通过这种方式，代理可以为其强化学习提供一些指导。我们的主要贡献是提出了自我解释从示范中学习的强化学习（SERLfD）框架，可以克服传统RLfD工作的局限性。我们的实验结果表明，使用我们的SERLfD框架可以改进RLfD模型。

    Our work aims at efficiently leveraging ambiguous demonstrations for the training of a reinforcement learning (RL) agent. An ambiguous demonstration can usually be interpreted in multiple ways, which severely hinders the RL-Agent from learning stably and efficiently. Since an optimal demonstration may also suffer from being ambiguous, previous works that combine RL and learning from demonstration (RLfD works) may not work well. Inspired by how humans handle such situations, we propose to use self-explanation (an agent generates explanations for itself) to recognize valuable high-level relational features as an interpretation of why a successful trajectory is successful. This way, the agent can provide some guidance for its RL learning. Our main contribution is to propose the Self-Explanation for RL from Demonstrations (SERLfD) framework, which can overcome the limitations of traditional RLfD works. Our experimental results show that an RLfD model can be improved by using our SERLfD fra
    
[^221]: 允许混淆的LiNGAM的泛化

    Generalization of LiNGAM that allows confounding. (arXiv:2401.16661v1 [cs.LG])

    [http://arxiv.org/abs/2401.16661](http://arxiv.org/abs/2401.16661)

    本文提出了一种名为LiNGAM-MMI的方法，可以增强LiNGAM模型以处理混淆问题。该方法使用KL散度量化混淆程度，并通过最短路径问题解决方案高效地确定变量顺序，不论是否存在混淆情况。实验证明，LiNGAM-MMI可以更准确地识别正确的变量顺序。

    

    LiNGAM使用加性噪声模型来确定因果关系的变量顺序，但在混淆方面面临挑战。先前的方法在保持LiNGAM的基本结构的同时，试图识别和处理受混淆影响的变量。结果是，不论是否存在混淆，这些方法都需要大量的计算资源，并且不能确保检测到所有的混淆类型。相比之下，本文通过引入LiNGAM-MMI对LiNGAM进行了增强，该方法使用KL散度量化混淆程度，并安排变量以最小化其影响。该方法通过最短路径问题的形式高效地实现全局最优的变量顺序。在无混淆的情况下，LiNGAM-MMI的处理数据效率与传统LiNGAM相当，同时有效处理混淆情况。我们的实验结果表明，LiNGAM-MMI更准确地确定了正确的变量顺序...

    LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, bo
    
[^222]: Langevin遗忘：噪声梯度下降的机器遗忘新视角

    Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])

    [http://arxiv.org/abs/2401.10371](http://arxiv.org/abs/2401.10371)

    Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。

    

    随着采用确保“被遗忘权”的法律，机器遗忘引起了极大的兴趣。研究人员提供了一个概率性的近似遗忘定义，类似于差分隐私（DP）的定义，其中隐私被定义为对重新训练的统计不可区分性。我们提出了Langevin遗忘，这是一个基于噪声梯度下降的近似遗忘问题的隐私保证的遗忘框架。Langevin遗忘在算法上统一了DP学习过程和隐私认证的遗忘过程。其中包括非凸问题的近似认证遗忘，相对于重新训练的复杂度节省，以及用于多个遗忘请求的顺序和批量遗忘。我们通过在基准数据集上的实验验证了Langevin遗忘的实用性，并展示了它对梯度下降的优势。

    Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
    
[^223]: 用空间自适应滤波重新思考谱图神经网络

    Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])

    [http://arxiv.org/abs/2401.09071](http://arxiv.org/abs/2401.09071)

    本文重新思考了谱图神经网络，并揭示了谱滤波和空间聚合之间的联系。该研究发现，谱滤波在隐含地将原始图转换成适应性新图，并明确计算用于空间聚合的新图。适应性新图展现出非局部性，并能够反映节点之间的标签一致性。

    

    尽管谱图神经网络（GNN）在理论上在谱域中有很好的基础，但它们实际上依赖于多项式逼近，意味着它们与空间域有着深刻的联系。由于以前的研究很少从空间角度研究谱图GNN，因此它们在空间域的可解释性仍然难以捉摸，例如，谱图GNN在空间域中实际上编码了哪些信息？为了回答这个问题，本文在谱滤波和空间聚合之间建立了一个理论上的联系，揭示了谱滤波隐含地将原始图转换成适应性新图的内在交互作用，并明确地计算用于空间聚合的适应性新图。理论和经验研究表明，适应性新图不仅表现出非局部性，还能够容纳有符号的边权重以反映节点之间的标签一致性。因此，这些发现突显了谱图GNN在空间中的可解释性角色。

    Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
    
[^224]: LPAC: 可学习的感知-行动-通信循环及其在覆盖控制中的应用

    LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])

    [http://arxiv.org/abs/2401.04855](http://arxiv.org/abs/2401.04855)

    提出了一种可学习的感知-行动-通信(LPAC)架构，使用卷积神经网络处理环境感知，图神经网络实现机器人之间的信息交流，浅层多层感知机计算机器人的动作。使用集中式显微算法训练模型，实现机器人群体的协作。

    

    覆盖控制是指导机器人群体协同监测未知的感兴趣特征或现象的问题。在有限的通信和感知能力的分散设置中，这个问题具有挑战性。本文提出了一种可学习的感知-行动-通信(LPAC)架构来解决覆盖控制问题。在该解决方案中，卷积神经网络(CNN)处理了环境的局部感知；图神经网络(GNN)实现了邻近机器人之间的相关信息通信；最后，浅层多层感知机(MLP)计算机器人的动作。通信模块中的GNN通过计算应该与邻居通信哪些信息以及如何利用接收到的信息采取适当的行动来实现机器人群体的协作。我们使用一个知晓整个环境的集中式显微算法来进行模型的训练。

    Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
    
[^225]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^226]: 上下文固定预算的最佳臂识别：适应性实验设计与策略学习

    Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])

    [http://arxiv.org/abs/2401.03756](http://arxiv.org/abs/2401.03756)

    该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。

    

    个性化治疗推荐是基于证据的决策中的关键任务。在这项研究中，我们将这个任务作为一个带有上下文信息的固定预算最佳臂识别（Best Arm Identification, BAI）问题来进行建模。在这个设置中，我们考虑了一个给定多个治疗臂的自适应试验。在每一轮中，决策者观察一个刻画实验单位的上下文（协变量），并将该单位分配给其中一个治疗臂。在实验结束时，决策者推荐一个在给定上下文条件下预计产生最高期望结果的治疗臂（最佳治疗臂）。该决策的有效性通过最坏情况下的期望简单遗憾（策略遗憾）来衡量，该遗憾表示在给定上下文条件下，最佳治疗臂和推荐治疗臂的条件期望结果之间的最大差异。我们的初始步骤是推导最坏情况下期望简单遗憾的渐近下界，该下界还暗示着解决该问题的一些思路。

    Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
    
[^227]: 高效设计和控制的节约能量约束下的减少操作推断

    Energy-Preserving Reduced Operator Inference for Efficient Design and Control. (arXiv:2401.02889v1 [math.NA])

    [http://arxiv.org/abs/2401.02889](http://arxiv.org/abs/2401.02889)

    本文提出了一种能够节约能量的减少操作推断方法，用于高效设计和控制中的多次计算任务，特别适用于保持能量的偏微分方程控制系统。

    

    对于设计和控制中需要多次计算的工程系统，计算模型的高效性至关重要。对于由偏微分方程（PDE）控制的系统，常见的高保真数值模型是高维的，对于多次计算而言计算成本过高。因此，需要有效的代理模型以实现设计和控制中的低成本计算。本文提出了一种保持物理约束的减少模型学习方法，针对能量保持的PDE，例如许多流体问题中出现的控制方程。该方法基于操作推断方法，通过最小二乘拟合将减少模型算子与状态快照和时间导数数据匹配。然而，操作推断通常不能学习到具有能量保持属性的减少二次算子，因此我们提出了一种新的能量保持操作推断（EP-O）方法。

    Many-query computations, in which a computational model for an engineering system must be evaluated many times, are crucial in design and control. For systems governed by partial differential equations (PDEs), typical high-fidelity numerical models are high-dimensional and too computationally expensive for the many-query setting. Thus, efficient surrogate models are required to enable low-cost computations in design and control. This work presents a physics-preserving reduced model learning approach that targets PDEs whose quadratic operators preserve energy, such as those arising in governing equations in many fluids problems. The approach is based on the Operator Inference method, which fits reduced model operators to state snapshot and time derivative data in a least-squares sense. However, Operator Inference does not generally learn a reduced quadratic operator with the energy-preserving property of the original PDE. Thus, we propose a new energy-preserving Operator Inference (EP-O
    
[^228]: 为多任务联邦学习提供公平性感知的作业调度

    Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])

    [http://arxiv.org/abs/2401.02740](http://arxiv.org/abs/2401.02740)

    本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。

    

    联邦学习（FL）使多个数据所有者（即FL客户端）能够在不泄露敏感私人数据的情况下共同训练机器学习模型。现有的FL研究主要关注垄断场景，在该场景中，单个FL服务器在每轮训练中选择一部分FL客户端来更新其本地模型。实际上，可能会有多个FL服务器同时尝试从同一个池中选择客户端。本文提出了一种首创的公平感知联邦作业调度（FairFedJS）方法来弥合这一差距。基于Lyapunov优化，它通过同时考虑当前需求和作业付款出价，确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，以防止等待时间过长。基于两个数据集对FairFedJS与四种最先进的方法进行了大量实验证明了其显著优势。它在平均上击败了最佳基准线31.9%和1.0%。

    Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
    
[^229]: 振动信号的二次时间频率分析用于诊断轴承故障

    Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])

    [http://arxiv.org/abs/2401.01172](http://arxiv.org/abs/2401.01172)

    本文提出了一种融合时间频率分析和深度学习技术的方法，用于在实际条件下诊断带有时间变化速度和不同噪声水平的轴承故障。这种方法有效地解析与不同轴承故障相关的独特动态模式。

    

    轴承故障的诊断对于降低维修成本和设备停机至关重要。轴承故障是机器振动的主要原因，分析其信号形态可以揭示其健康状况。然而，现有的方法主要针对控制环境进行优化，忽略了实际条件下的时间变化的转速和振动的非平稳性。本文提出了一种时间频率分析和深度学习技术的融合方法，用于在时间变化速度和不同噪声水平下诊断轴承故障。首先，我们制定了轴承故障引起的振动，并讨论了它们的非平稳性与轴承固有和操作参数之间的联系。我们还阐述了二次时间频率分布，并验证了它们解析与不同轴承故障相关的独特动态模式的有效性。基于此，我们设计了一个时间频率卷积神经网络。

    Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
    
[^230]: 从观察中学习集体行为

    Learning Collective Behaviors from Observation. (arXiv:2311.00875v1 [cs.LG])

    [http://arxiv.org/abs/2311.00875](http://arxiv.org/abs/2311.00875)

    本文介绍了一系列学习方法，用于从观察中识别动态系统的结构，并理解复杂系统中相互作用代理的 emergent behaviors。这些方法不仅具备理论收敛保证，还能高效处理高维观测数据，解决观测/随机噪声、复杂的交互规则、丢失的交互特征和现实世界观测数据等问题。通过采用变分逆问题方法设计合适的损失函数，我们的学习方法具备降维能力。

    

    我们提出了一系列学习方法的综述，用于识别动态系统的结构，旨在理解相互作用代理系统中的 emergent behaviors。这些方法不仅提供了收敛的理论保证，还展示了处理高维观测数据的计算效率。它们可以处理来自一阶和二阶动态系统的观测数据，考虑观测/随机噪声、复杂的交互规则、丢失的交互特征以及与现实世界中相互作用代理系统的观测数据。发展这一系列学习方法的核心在于使用变分逆问题方法设计适当的损失函数，这种方法天然地提供了我们学习方法的降维能力。

    We present a review of a series of learning methods used to identify the structure of dynamical systems, aiming to understand emergent behaviors in complex systems of interacting agents. These methods not only offer theoretical guarantees of convergence but also demonstrate computational efficiency in handling high-dimensional observational data. They can manage observation data from both first- and second-order dynamical systems, accounting for observation/stochastic noise, complex interaction rules, missing interaction features, and real-world observations of interacting agent systems. The essence of developing such a series of learning methods lies in designing appropriate loss functions using the variational inverse problem approach, which inherently provides dimension reduction capabilities to our learning methods.
    
[^231]: DiffEnc: 使用学习的编码器的变分扩散模型

    DiffEnc: Variational Diffusion with a Learned Encoder. (arXiv:2310.19789v1 [cs.LG])

    [http://arxiv.org/abs/2310.19789](http://arxiv.org/abs/2310.19789)

    DiffEnc是一种使用学习的编码器的变分扩散模型，通过引入数据和深度相关的均值函数和可调节的噪声方差比率，实现了最先进的可能性。

    

    扩散模型可以看作是具有两种改进的分层变分自编码器（VAEs）：在生成过程中参数共享的条件分布和在层次结构上独立计算损失。我们对扩散模型进行了两个变化，保留了这些优势的同时增加了模型的灵活性。首先，我们在扩散过程中引入了一个与数据和深度相关的均值函数，从而导致了修改后的扩散损失。我们提出的框架DiffEnc在CIFAR-10上实现了最先进的可能性。其次，我们让反向编码过程的噪声方差与生成过程的比率成为一个自由的权重参数，而不是固定为1。这带来了理论上的洞察力：对于有限深度层次，证据下界（ELBO）可以用作加权扩散损失方法的目标，并用于专门为推理而优化噪声调度。

    Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite
    
[^232]: VFedMH: 垂直联合学习用于训练多参与方异构模型

    VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])

    [http://arxiv.org/abs/2310.13367](http://arxiv.org/abs/2310.13367)

    VFedMH是一种垂直联合学习方法，通过在前向传播过程中聚合参与者的嵌入来处理参与者之间的异构模型，解决了现有VFL方法面临的挑战。

    

    垂直联合学习（VFL）作为一种集成样本对齐和特征合并的新型训练范式，已经引起了越来越多的关注。然而，现有的VFL方法在处理参与者之间存在异构本地模型时面临挑战，这影响了优化收敛性和泛化能力。为了解决这个问题，本文提出了一种名为VFedMH的新方法，用于训练多方异构模型。VFedMH的重点是在前向传播期间聚合每个参与者知识的嵌入，而不是中间结果。主动方，拥有样本的标签和特征，在VFedMH中安全地聚合本地嵌入以获得全局知识嵌入，并将其发送给被动方。被动方仅拥有样本的特征，然后利用全局嵌入在其本地异构网络上进行前向传播。然而，被动方不拥有标签。

    Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
    
[^233]: Lag-Llama: 用于时间序列预测的基础模型

    Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])

    [http://arxiv.org/abs/2310.08278](http://arxiv.org/abs/2310.08278)

    Lag-Llama是一个基于大量时间序列数据训练的通用预测模型，在未见过的数据集上展现出强大的零样本预测能力，并使用光滑断裂幂律模型来拟合和预测扩展行为。

    

    为了构建时间序列预测的基础模型并研究其扩展行为，我们在这里介绍了我们正在进行中的 Lag-Llama 工作，这是一个在大量时间序列数据上训练的通用单变量概率时间序列预测模型。该模型在未见过的“分布外”时间序列数据集上展现出良好的零样本预测能力，优于有监督基线方法。我们使用光滑断裂幂律来拟合和预测模型的扩展行为。开源代码可在 https://github.com/kashif/pytorch-transformer-ts 上获得。

    Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
    
[^234]: 从时间序列数据中发现混合结构因果模型

    Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])

    [http://arxiv.org/abs/2310.06312](http://arxiv.org/abs/2310.06312)

    这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。

    

    在金融、气候科学和神经科学等领域，从时间序列数据中推断因果关系是一个巨大的挑战。尽管现代技术可以处理变量之间的非线性关系和灵活的噪声分布，但它们依赖于简化假设，即数据来自相同的潜在因果模型。在这项工作中，我们放松了这个假设，从来源于不同因果模型混合的时间序列数据中进行因果发现。我们推断了潜在的结构性因果模型，以及每个样本属于特定混合成分的后验概率。我们的方法采用了一个端对端的训练过程，最大化了数据似然的证据下界。通过对合成和真实世界数据集的广泛实验，我们证明了我们的方法在因果发现任务中超越了最先进的基准方法，尤其是当数据来自不同的潜在因果模型时。

    In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
    
[^235]: 一种面向形式定理证明的语言代理方法

    A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])

    [http://arxiv.org/abs/2310.04353](http://arxiv.org/abs/2310.04353)

    COPRA是一种面向形式定理证明的语言代理方法，利用大型语言模型进行上下文学习，通过选择策略和检索定义和引理进行证明，在MiniF2F基准和Coq任务上表现出优异的性能。

    

    语言代理是利用大型语言模型（LLM）进行上下文学习来与外部环境进行交互的方法，最近被认为是一种有前景的控制任务方法。

    Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
    
[^236]: 关于随机向量的仿射变换的Wasserstein距离研究

    On Wasserstein distances for affine transformations of random vectors. (arXiv:2310.03945v1 [stat.ML])

    [http://arxiv.org/abs/2310.03945](http://arxiv.org/abs/2310.03945)

    本研究阐述了关于随机向量之间的Wasserstein距离的下界，重点是仿射变换，并应用于流型学习和手写数据集模拟。

    

    我们阐述了关于在Wasserstein空间中用于数据流型学习的随机向量之间的二次Wasserstein距离的一些已知下界，重点是仿射变换。特别地，我们通过计算协方差矩阵之间的Bures距离，给出了旋转的随机向量在具有不相关分量的$\mathbb{R}^2$空间中的具体下界。我们还得到了仿射变换的组合的上界，从而产生了应用于初始数据测度的丰富的微分同胚。我们将这些界应用于包括在$\mathbb{R}^2$中的一维流型上的各种分布，并说明了这些界的质量。最后，我们提供了一个在流型学习框架中可以应用于模拟手写数字或字母数据集的框架。

    We expound on some known lower bounds of the quadratic Wasserstein distance between random vectors in $\mathbb{R}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in Wasserstein space. In particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices. We also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. We apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{R}^2$ and illustrate the quality of the bounds. Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.
    
[^237]: GAMIX-VAE: 一种基于高斯混合后验的VAE

    GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])

    [http://arxiv.org/abs/2309.13160](http://arxiv.org/abs/2309.13160)

    本文提出了一种基于高斯混合后验的VAE方法，重新定义了ELBO，引入正则化项和PatchGAN鉴别器，能够生成逼真的人脸。

    

    变分自动编码器（VAEs）已成为机器学习中生成建模和表示学习的基石。本文探讨了VAEs的一个细微方面，重点是解释KL Divergence，这是Evidence Lower Bound（ELBO）中的关键组成部分，它控制了重构准确性和正则化之间的权衡。虽然KL Divergence让潜变量分布与先验分布对齐，给整个潜空间加上结构约束，但却不限制各个变量分布。所提出的方法重新定义了带有高斯混合的后验概率的ELBO，引入了正则化项以防止方差崩溃，并使用PatchGAN鉴别器来增强纹理逼真度。实现细节涉及Encoder和Decoder的ResNetV2架构。实验证明了生成逼真的人脸的能力，为提供了一个有希望的解决方案。

    Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
    
[^238]: 用DiscoSCMs回答Layer 3查询

    Answering Layer 3 queries with DiscoSCMs. (arXiv:2309.09323v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09323](http://arxiv.org/abs/2309.09323)

    本文介绍了DiscoSCMs，一种用于解决因果查询的模型。它通过扩展结构因果模型和潜在结果框架来解决一致性规则引发的退化问题，并在分析个性化激励场景中的潜在结果时展示了其有效性。通过引入独立潜在噪声条件，可以提高解决Layer 3查询的准确性和可解释性。

    

    在当代因果推断研究中，解决Pearl因果层次（PCH）下的关联、干预和反事实的因果查询是一个核心任务。本文针对一致性规则引发的退化问题，引入了分布一致性结构因果模型（DiscoSCMs），扩展了结构因果模型（SCM）和潜在结果框架。以个性化激励场景中潜在结果的相关模式$P(y_x, y'_{x'})$为案例研究。尽管反事实不再退化，但仍无法确定。因此，将独立潜在噪声条件纳入DiscoSCM。发现通过适应分布的嵌入式推断，可以极大地提高解决Layer 3查询的准确性和可解释性。

    Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e., associational, interventional and counterfactual), which is formalized as \Layer{} Valuations, is a central task in contemporary causal inference research. Counterfactual questions, in particular, pose a significant challenge as they often necessitate a complete knowledge of structural equations. This paper identifies \textbf{the degeneracy problem} caused by the consistency rule. To tackle this, the \textit{Distribution-consistency Structural Causal Models} (DiscoSCMs) is introduced, which extends both the structural causal models (SCM) and the potential outcome framework. The correlation pattern of potential outcomes in personalized incentive scenarios, described by $P(y_x, y'_{x'})$, is used as a case study for elucidation. Although counterfactuals are no longer degenerate, they remain indeterminable. As a result, the condition of independent potential noise is incorporated into DiscoSCM. It is found that by ad
    
[^239]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^240]: 使用大规模未标记的自然图像增强医疗AI模型的网络初始化

    Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])

    [http://arxiv.org/abs/2308.07688](http://arxiv.org/abs/2308.07688)

    该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。

    

    预训练数据集（如ImageNet）已成为医学图像分析的黄金标准。然而，自监督学习（SSL）的出现提供了通过利用未标记数据来学习强大特征的机会，从而可以绕过繁重的标注过程。在这项研究中，我们探索了SSL预训练在非医学图像上是否可以应用于胸部X射线，并与非医学图像和医学图像上的监督预训练进行了比较。我们利用视觉变换器，并根据以下方式初始化其权重：（i）基于自然图像的SSL预训练（DINOv2）、（ii）基于自然图像的监督预训练（ImageNet数据集），以及（iii）基于MIMIC-CXR数据库中的胸部X射线的监督预训练。我们在来自六个全球大型数据集的800,000多张胸部X射线上测试了我们的方法，诊断了20多种不同的影像所见。我们的SSL预训练在经过筛选的图像上不仅表现出色，而且超过了基于ImageNet的预训练（对所有数据集，P<0.001），而且在某些数据集上还超过了基于医学图像的预训练。

    Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in cert
    
[^241]: 学习基于团队导航：深度强化学习技术在多智能体路径规划中的综述

    Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding. (arXiv:2308.05893v1 [cs.AI])

    [http://arxiv.org/abs/2308.05893](http://arxiv.org/abs/2308.05893)

    本文综述了在多智能体路径规划中深度强化学习技术的应用。与其他研究不同，我们重点介绍了DRL方法在MAPF中的整合，并解决了MAPF解决方案评估指标缺乏统一性的问题。我们讨论了基于模型的DRL作为未来发展方向，并提供了解决MAPF当前挑战所需的基础理解。

    

    多智能体路径规划(MAPF)是许多大规模机器人应用中的关键领域，通常是多智能体系统的基本步骤。然而，在复杂和拥挤的环境中，MAPF的复杂性不断增加，已有解决方案的有效性严重降低。与其他研究不同，我们在本综述论文中重点介绍了DRL方法在MAPF中的应用。此外，我们旨在填补目前在评估MAPF解决方案方面的缺口，通过解决缺乏统一评估指标的问题并对这些指标进行全面阐释。最后，我们的论文讨论了作为未来方向的基于模型的DRL的潜力，并提供了必要的基础理解以应对MAPF中的当前挑战。

    Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our o
    
[^242]: 基于数据的非线性哈密顿系统二次辛表示的数据驱动识别

    Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems. (arXiv:2308.01084v1 [cs.LG])

    [http://arxiv.org/abs/2308.01084](http://arxiv.org/abs/2308.01084)

    本论文提出了一种基于数据的方法来学习哈密顿系统，利用提升假设，通过强制实施哈密顿结构和使用辛自编码器，我们实现了在变换的坐标系下具有哈密顿结构的二次动力学，保持系统的长期稳定性和较低的模型复杂度。

    

    我们提出了一个利用数据学习哈密顿系统的框架。这项工作基于提升假设，即非线性哈密顿系统可以写成具有立方哈密顿量的非线性系统。通过利用这一点，我们得到在变换的坐标系下具有哈密顿结构的二次动力学。为了达到这个目标，对于给定的广义位置和动量数据，我们提出了一种学习二次动力学系统的方法，结合辛自编码器强制实施哈密顿结构。强制实施的哈密顿结构表现出系统的长期稳定性，而立方哈密顿函数提供了相对较低的模型复杂性。对于低维数据，我们确定了一个高阶变换的坐标系，而对于高维数据，我们找到了一个具有所需特性的低阶坐标系。我们通过低维和高维的非线性哈密顿系统示例展示了所提出的方法。

    We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonia
    
[^243]: 生成人工智能的强化学习：现状、机会和开放研究挑战

    Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])

    [http://arxiv.org/abs/2308.00031](http://arxiv.org/abs/2308.00031)

    这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。

    

    生成人工智能（AI）是近十年来计算机科学领域最令人兴奋的发展之一。与此同时，强化学习（RL）在各种机器学习任务中已经成为非常成功的范式。在本调查中，我们讨论了将RL应用于生成AI中的现状、机会和开放的研究问题。具体而言，我们将讨论三种应用类型，即作为一种无特定目标的生成方式，作为一种同时最大化目标函数的输出生成方式，以及作为一种将无法通过目标函数轻松捕捉的期望特征嵌入生成过程的方式。我们在调查结果中对这个迷人的新兴领域中的机会和挑战进行了深入的讨论。

    Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
    
[^244]: 大数据-供应链管理框架的预测：数据预处理和机器学习技术

    Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])

    [http://arxiv.org/abs/2307.12971](http://arxiv.org/abs/2307.12971)

    本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。

    

    本文旨在系统地识别和比较分析最先进的供应链预测策略和技术。提出了一个新的框架，将大数据分析应用于供应链管理中，包括问题识别、数据来源、探索性数据分析、机器学习模型训练、超参数调优、性能评估和优化，以及预测对人力、库存和整个供应链的影响。首先讨论了根据供应链策略收集数据的需求以及如何收集数据。文章讨论了根据周期或供应链目标需要不同类型的预测。推荐使用供应链绩效指标和误差测量系统来优化表现最佳的模型。还讨论了幻影库存对预测的不利影响以及管理决策依赖供应链绩效指标来确定模型性能参数和改进运营管理、透明度的问题。

    This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
    
[^245]: 安全强化学习作为Wasserstein变分推理：可解释性的形式方法

    Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])

    [http://arxiv.org/abs/2307.07084](http://arxiv.org/abs/2307.07084)

    本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。

    

    强化学习或最优控制可以为具有可变动态的顺序决策问题提供有效的推理。然而，在实际实施中，解释奖励函数和相应的最优策略一直是一个持久的挑战。因此，将顺序决策问题形式化为推理具有重要价值，因为概率推理原则上提供了多样且强大的数学工具来推断随机动态，同时提供了奖励设计和策略收敛的概率解释。在本研究中，我们提出了一种新颖的自适应Wasserstein变分优化（AWaVO）方法来解决这些顺序决策中的挑战。我们的方法利用形式方法来解释奖励设计，透明地训练收敛，以及对顺序决策的概率解释。为了证明实用性，我们展示了收敛训练并保证了收敛的训练。

    Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
    
[^246]: 关于逻辑回归中参数估计的样本复杂度研究

    On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])

    [http://arxiv.org/abs/2307.04191](http://arxiv.org/abs/2307.04191)

    本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。

    

    逻辑回归模型是噪声二元分类问题中最常见的数据生成模型之一。本文研究了在标准正态协变量下，以$\ell_2$误差为限，估计逻辑回归模型参数的样本复杂度，考虑了维度和逆温度的影响。逆温度控制了数据生成过程中的信噪比。虽然逻辑回归的广义界限和渐近性能已经有了深入研究，但关于参数估计的非渐近样本复杂度在之前的分析中没有讨论其与误差和逆温度的依赖关系。我们展示了样本复杂度曲线在逆温度方面具有两个转折点（或临界点），明确划分了低、中和高温度区域。

    The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
    
[^247]: 激励理论的贝叶斯推理在协作科学中的应用

    Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])

    [http://arxiv.org/abs/2307.03748](http://arxiv.org/abs/2307.03748)

    本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。

    

    当代科学研究是一项分布式的、协作的工作，由研究团队、监管机构、资助机构、商业合作伙伴和科学机构组成，彼此互动并面对不同的激励。为了保持科学严谨性，统计方法应该认识到这种情况。为此，我们研究了假设检验的情况，其中有一个代理人（例如研究人员或制药公司）对未知参数拥有私人先验知识，还有一个委托人（如政策制定者或监管机构）希望根据参数值做出决策。代理人根据他们的私人先验选择是否进行统计试验，然后试验的结果由委托人用来做出决策。我们展示了委托人如何进行统计推断，利用代理人的战略行为所透露的信息，也就是他们选择是否进行试验。具体来说，我们展示了如何计算p值，从而综合利用代理人的行为和试验的结果进行推理。

    Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the p
    
[^248]: Nexus sine qua non：基于节点识别的神经网络连接的时空预测多变量时间序列

    Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])

    [http://arxiv.org/abs/2307.01482](http://arxiv.org/abs/2307.01482)

    提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。

    

    建模和预测多变量时间序列不仅有助于从业者的决策，还加深我们对底层动态系统的科学理解。时空图神经网络（STGNNs）已经成为强大的预测器，并成为学习时空表示的事实标准模型。然而，现有的STGNNs的架构往往通过堆叠一系列复杂的层次而变得复杂。设计的模型可能多余或难以理解，这给复杂性和可扩展性带来了巨大挑战。这些问题促使我们重新审视现代STGNNs的设计，并确定对强大和高效的神经预测器有所贡献的核心原则。在这里，我们提出了一个紧凑的预测模型，完全由密集编码器-解码器和消息传递层来定义，基于节点识别，没有任何复杂的顺序模块，例如TCNs，RNNs和Transformers。通过实证重新评估该模型的性能，我们证明了该模型的有效性和高效性。

    Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
    
[^249]: AmicroN：一种用于生成细粒度微动作的人体活动识别注释的框架

    AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities. (arXiv:2306.13149v1 [cs.HC])

    [http://arxiv.org/abs/2306.13149](http://arxiv.org/abs/2306.13149)

    本文提出了一种名为AmicroN的框架，可以利用传感器数据自动生成微动作注释，弥补现有数据集缺乏细粒度注释的不足。

    

    利用传感器数据实现高效的人体活动识别需要大量的注释数据。未标记传感器数据的增长挑战了传统的基于人的数据收集方法，通常导致收集更浅的注释。这些浅层注释忽略了组成日常生活中任何复杂活动的细粒度微动作。本文分析了可用的预注释数据集中缺乏细粒度注释的原因和缺陷，并进行了详细的调查以了解与注释相关的人类感知。在此基础上，我们开发了AmicroN框架，该框架可以使用运动特征和可用的粗粒度宏操作标签自动生成微动作注释。在后台，AmicroN应用变点检测，然后进行零样本学习。

    Efficient human activity recognition (HAR) using sensor data needs a significant volume of annotated data. The growing volume of unlabelled sensor data has challenged conventional practices for gathering HAR annotations with human-in-the-loop approaches, often leading to the collection of shallower annotations. These shallower annotations ignore the fine-grained micro-activities that constitute any complex activities of daily living (ADL). Understanding this, we, in this paper, first analyze this lack of granular annotations from available pre-annotated datasets to understand the practical inconsistencies and also perform a detailed survey to look into the human perception surrounding annotations. Drawing motivations from these, we next develop the framework AmicroN that can automatically generate micro-activity annotations using locomotive signatures and the available coarse-grain macro-activity labels. In the backend, AmicroN applies change-point detection followed by zero-shot learn
    
[^250]: DDLP：基于深度动态潜在粒子的无监督物体中心视频预测

    DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles. (arXiv:2306.05957v1 [cs.CV])

    [http://arxiv.org/abs/2306.05957](http://arxiv.org/abs/2306.05957)

    DDLP算法使用深度潜在粒子(DLP)表示法实现无监督物体中心视频预测，并取得了最先进的预测结果。算法的可解释性使得可以进行“假设”生成，而DLP的紧凑结构使得效率高并可以进行基于扩散的无条件视频生成。

    

    本文提出了一种基于深度潜在粒子（DLP）表示的新型物体中心视频预测算法。与现有的基于槽或补丁的表示相比，DLP使用一组关键点模拟场景，学习参数用于属性例如位置和大小，并且既高效又可解释。我们的方法——深度动态潜在粒子(DDLP)，在多个具有挑战性的数据集上实现了最先进的物体中心视频预测结果。DDLP的可解释性使我们能够执行“假设”生成——预测更改初始帧中对象属性的结果，而DLP的紧凑结构使得效率高并可以进行基于扩散的无条件视频生成。视频、代码和预训练模型可在此链接找到：https://taldatech.github.io/ddlp-web

    We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation. In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web
    
[^251]: 基于数据驱动的相对不确定性测度用于误分类检测

    A Data-Driven Measure of Relative Uncertainty for Misclassification Detection. (arXiv:2306.01710v1 [stat.ML])

    [http://arxiv.org/abs/2306.01710](http://arxiv.org/abs/2306.01710)

    本文提出了一种基于数据驱动的相对不确定性度量，用于误分类检测。该度量可以通过学习软预测的分布模式，识别出被误分类的样本，并展示了在多个图像分类任务中的实证改进，优于现有的误分类检测方法。

    

    误分类检测是机器学习中的一个重要问题，它可以识别模型预测不可靠的实例。然而，传统的不确定性测度如香农熵并不能提供一种有效的方式来推断模型预测的实际不确定性。本文提出了一种新颖的数据驱动相对不确定性度量，用于误分类检测。通过学习软预测的分布模式，我们的不确定性度量可以基于预测的类概率标识被误分类的样本。有趣的是，根据所提出的度量，与误分类实例对应的软预测可能具有很大的不确定性，即使它们的香农熵可能很低。我们展示了多个图像分类任务中的实证改进，优于现有的误分类检测方法。

    Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. In this paper, we introduce a novel data-driven measure of relative uncertainty to an observer for misclassification detection. By learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. Interestingly, according to the proposed measure, soft-predictions that correspond to misclassified instances can carry a large amount of uncertainty, even though they may have low Shannon entropy. We demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.
    
[^252]: 放松去中心化无遗憾高维贝叶斯优化中的加法约束

    Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19838](http://arxiv.org/abs/2305.19838)

    本文提出了一种放松去中心化无遗憾高维贝叶斯优化中加法约束的方法，并且解决了过度探索问题。该方法称为DumBO，并且在实验中展示了竞争性能。

    

    贝叶斯优化常用于优化一个未知函数$f$，该函数存在噪声且评估成本高昂，通过利用必须在每个优化步骤中最大化的收获函数来实现。尽管可证明渐进最优的BO算法在优化低维函数方面效率很高，但将其扩展到高维空间仍然是一个待解决的问题，通常通过假设$f$具有加法结构来解决。通过这样做，BO算法通常引入了对加法结构的额外限制性假设，降低了它们的适用范围。本文包含两个主要贡献：（i）放松对$f$加法结构的限制性假设，以减弱收获函数的最大化保证；（ii）解决去中心化BO算法中的过度探索问题。为此，我们提出了DumBO，一种渐进最优的去中心化BO算法，具有非常竞争的性能。

    Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, BO algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This paper contains two main contributions: (i) we relax the restrictive assumptions on the additive structure of $f$, at the expense of weakening the maximization guarantees of the acquisition function, and (ii) we address the over-exploration problem for decentralized BO algorithms. To these ends, we propose DumBO, an asymptotically optimal decentralized BO algorithm that achieves very competitive performance
    
[^253]: 略微超参数化的ReLU网络具有有利的损失景观

    Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])

    [http://arxiv.org/abs/2305.19510](http://arxiv.org/abs/2305.19510)

    本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。

    

    本文研究了有限输入数据集上，二层略微超参数化ReLU神经网络的损失景观，使用了参数映射的Jacobian矩阵的秩来估计局部和全局极小值集的维度。使用随机二进制矩阵的结果，我们证明大多数激活模式对应的参数区域没有坏的可微局部极小值。此外，对于一维输入数据，我们证明了网络可以通过大多数的激活模式实现高维全局极小值集合而不具有坏的局部极小值。我们通过发现大多数区域具有完整的秩或缺乏秩，以实验的方式证实了这些结果，这取决于超参数的数量。

    We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
    
[^254]: NeuralMatrix: 将整个神经网络移动到通用矩阵乘法以实现高效推理

    NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])

    [http://arxiv.org/abs/2305.14405](http://arxiv.org/abs/2305.14405)

    NeuralMatrix是一种框架，能够在单个通用矩阵乘法加速器上计算深度神经网络(DNNs)，并可在保持推理准确度的情况下实现高达113倍至19.44倍的性能提升。

    

    本研究介绍了一种名为NeuralMatrix的新型框架，它使得可以在单个通用矩阵乘法（GEMM）加速器上计算多功能的深度神经网络（DNNs）。该方法克服了基于ASIC的加速器的专用性限制，同时实现了与CPU和GPU等通用处理器相比的应用特定加速水平。我们解决了将DNN计算中的线性和非线性运算映射到通用矩阵乘法以及使用GEMM加速器对DNN推理准确性的影响的挑战。我们在来自三种流行类别的各种DNN模型上进行了大量实验（即CNN，Transformers和GNN）作为示例的支撑模型。我们的结果表明，将DNN转换为通用矩阵乘法后仅会出现高达2.02％的准确度损失，同时将吞吐量与功率的比值与CPU和GPU相比提高了113倍到19.44倍。

    In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
    
[^255]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^256]: 矩阵分解中交替梯度下降的收敛性分析

    Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])

    [http://arxiv.org/abs/2305.06927](http://arxiv.org/abs/2305.06927)

    本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。

    

    本文考虑了应用于不对称矩阵分解目标的具有固定步长$\eta>0$的交替梯度下降（AGD）。我们证明了，对于秩为$r$的矩阵$\mathbf {A}\in \mathbb {R} ^ {m \times n}$，$T=\left(\left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2\log(1/\epsilon)\right)$次交替梯度下降即可从非典型随机初始化高概率地达到$\epsilon$-最优分解$\|\mathbf {A}\mathbf {X}_T^{\vphantom{\intercal}}\mathbf {Y}_T^{\intercal}\|_{\rm F}^2\le\epsilon\|\mathbf {A}\|_{\rm F}^2$。分解中因子的秩为$d>r$，因此$\mathbf{X}_T\in\mathbb{R}^{m \times d}$且$\mathbf{Y}_T\in\mathbb{R}^{n \times d}$。实验表明，我们提出的初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。我们的证明概念上很简单：一致的PL不等式和一致的Lipschitz平滑性。

    We consider alternating gradient descent (AGD) with fixed step size $\eta > 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, $T = \left( \left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2 \log(1/\epsilon)\right)$ iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization $\| \mathbf{A} \mathbf{X}_T^{\vphantom{\intercal}} \mathbf{Y}_T^{\intercal} \|_{\rm F}^2 \leq \epsilon \| \mathbf{A} \|_{\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d>r$ so that $\mathbf{X}_T\in\mathbb{R}^{m \times d}$ and $\mathbf{Y}_T \in\mathbb{R}^{n \times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne
    
[^257]: 面向非线性动态系统辨识的频率支持神经网络

    Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification. (arXiv:2305.06344v1 [cs.LG])

    [http://arxiv.org/abs/2305.06344](http://arxiv.org/abs/2305.06344)

    本文提出了一种新的神经网络结构——频率支持神经网络，它通过加入频率信息适应于非线性系统辨识任务，并在多个任务中表现出优越性。

    

    神经网络是一种非常通用的模型，能够学习多个变量之间的各种关系。其中一种关系在实践中非常有趣，即非线性系统的输入输出关系，具有多种应用。研究能够估计这种关系的模型是一门广泛的学科，具有许多理论和实际结果。神经网络非常通用，但存在多种特殊情况，包括卷积神经网络和循环神经网络，分别用于特定的应用，即图像和序列处理。我们提出了一个假设，即通过将频率信息纳入到通用网络结构中来调整其结构，应该可以得到一种专门适用于非线性系统辨识的网络。此外，我们还表明，从理论上讲，可以添加这种频率信息而不会损失通用性。我们称这种新结构为频率支持神经网络（FSNN），并在多个非线性动态系统辨识任务中展示了其优越性。

    Neural networks are a very general type of model capable of learning various relationships between multiple variables. One example of such relationships, particularly interesting in practice, is the input-output relation of nonlinear systems, which has a multitude of applications. Studying models capable of estimating such relation is a broad discipline with numerous theoretical and practical results. Neural networks are very general, but multiple special cases exist, including convolutional neural networks and recurrent neural networks, which are adjusted for specific applications, which are image and sequence processing respectively. We formulate a hypothesis that adjusting general network structure by incorporating frequency information into it should result in a network specifically well suited to nonlinear system identification. Moreover, we show that it is possible to add this frequency information without the loss of generality from a theoretical perspective. We call this new st
    
[^258]: 利用室内WiFi系统进行无设备穿墙存在检测的注意增强深度学习

    Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])

    [http://arxiv.org/abs/2304.13105](http://arxiv.org/abs/2304.13105)

    本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。

    

    在室内环境中准确检测人员存在对于各种应用非常重要，例如能源管理和安全。本文提出了一种利用WiFi信号的通道状态信息（CSI）进行人员存在检测的新系统。我们的系统名为注意力增强深度学习（ALPD），采用关注机制从CSI数据中自动选择有信息量的子载波，并采用双向长短时记忆（LSTM）网络捕捉CSI中的时间依赖性。此外，我们利用一个静态特征来提高静态状态下人员存在检测的准确性。我们通过部署一对WiFi接入点（AP）来收集CSI数据集来评估所提出的ALPD系统，该系统进一步与几个基准进行比较。结果表明，我们的ALPD系统在准确性方面优于基准，特别是在存在干扰的情况下。此外，双向传输数据不会影响我们系统的性能，证明了其在现实场景中的稳健性。

    Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
    
[^259]: Transformer介绍

    An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])

    [http://arxiv.org/abs/2304.10557](http://arxiv.org/abs/2304.10557)

    Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。

    

    Transformer是一种可以学习序列或数据集表示的神经网络组件。Transformer在自然语言处理、计算机视觉和时空建模方面取得了重大进展。虽然有很多Transformer的介绍，但大多数都缺少对其架构的精确数学描述，其设计选择的直觉也常常缺失。此外，随着研究路径的曲折，Transformer部件的解释可能是异质的。在这篇论文中，我们旨在提供一个数学精确、直观、简洁的Transformer架构描述。

    The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
    
[^260]: 重新审视 LiDAR 欺骗攻击对于目标检测的能力：改进、测量和新攻击

    Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack. (arXiv:2303.10555v1 [cs.CR])

    [http://arxiv.org/abs/2303.10555](http://arxiv.org/abs/2303.10555)

    本文针对 LiDAR 欺骗攻击在目标检测领域存在的研究差距进行了实证研究，使用9种 LiDAR 和3种目标检测器进行了大规模测量，展示了一种新的攻击模式，提出了安全的 LiDAR 设计和评估方法。

    

    LiDAR（光学遥感）是进行精确长距离和宽范围 3D 感应的不可或缺的传感器，直接造福于自动驾驶技术的快速推广。同时，这种安全关键的应用强烈推动了对其安全性的研究。最近的研究表明，攻击者可以通过向 LiDAR 发送恶意激光来操纵 LiDAR 点云并欺骗目标检测。然而，这些研究面临三个关键的研究难点：（1）仅评估特定的 LiDAR（VLP-16）；（2）假设攻击能力未被验证；以及（3）评估受限的数据集上训练的模型。为了填补这些关键的研究难点，我们对总共9种流行的 LiDAR 和3种主要类型的目标检测器进行了第一次大规模的 LiDAR 欺骗攻击能力测量研究。为了进行这个测量，我们通过更加小心的光学和功能电子学，显著提高了 LiDAR 欺骗能力，展示了一种新的攻击模式，可以甚至规避最先进的防御机制。我们的结果提供了关于如何在不同的实际设置中实际进行 LiDAR 欺骗攻击的见解，并呼吁安全的 LiDAR 设计和评估方法。

    LiDAR (Light Detection And Ranging) is an indispensable sensor for precise long- and wide-range 3D sensing, which directly benefited the recent rapid deployment of autonomous driving (AD). Meanwhile, such a safety-critical application strongly motivates its security research. A recent line of research demonstrates that one can manipulate the LiDAR point cloud and fool object detection by firing malicious lasers against LiDAR. However, these efforts face 3 critical research gaps: (1) evaluating only on a specific LiDAR (VLP-16); (2) assuming unvalidated attack capabilities; and (3) evaluating with models trained on limited datasets.  To fill these critical research gaps, we conduct the first large-scale measurement study on LiDAR spoofing attack capabilities on object detectors with 9 popular LiDARs in total and 3 major types of object detectors. To perform this measurement, we significantly improved the LiDAR spoofing capability with more careful optics and functional electronics, whic
    
[^261]: Listen2Scene：交互式物质感知双耳音频传播重构三维场景

    Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.02809](http://arxiv.org/abs/2302.02809)

    本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。

    

    本文提出了一种用于虚拟现实（VR）和增强现实（AR）应用的端到端双耳音频渲染方法（Listen2Scene）。我们提出了一种新颖的基于神经网络的双耳声学传播方法，以生成真实环境的3D模型的声学效果。任何清洁音频或干音频都可以与生成的声学效果卷积，以渲染与真实环境相对应的音频。我们提出了一个图神经网络，利用3D场景的材料和拓扑信息生成场景潜在向量。此外，我们使用条件生成对抗网络（CGAN）从场景潜在向量生成声学效果。我们的网络能够处理重构的三维网格模型中的孔洞或其他伪像。我们提出了一种高效的成本函数，用于生成器网络以整合空间音频效果。给定源和听者位置，我们的基于学习的双耳声音传播方法可以生成与真实环境精度相符的声学输出。

    We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou
    
[^262]: 使用来自成对或$K$元比较的人类反馈的规范强化学习

    Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270)

    该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。

    

    我们为带有人类反馈的强化学习问题提供了一个理论框架。我们的分析表明，当真实奖励函数为线性函数时，最大似然估计（MLE）在Bradley-Terry-Luce（BTL）模型和Plackett-Luce（PL）模型下均收敛。然而，我们发现当基于学得的奖励模型训练策略时，MLE会失败，而基于悲观估计的MLE在一定的覆盖假设下提供性能更好的策略。此外，我们证明在PL模型下，真实MLE和将$k$元比较拆分为成对比较的备选MLE都收敛。而真实MLE是渐近更为高效的。我们的结果验证了现有RLHF算法（如InstructGPT）的实验成功，并为算法设计提供了新的见解。此外，我们的结果统一了RLHF问题和最大熵反向强化学习(IRL)问题，并为其提供了第一个样本复杂度界。

    We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
    
[^263]: 多类数据集中的拓扑学习

    Topological Learning in Multi-Class Data Sets. (arXiv:2301.09734v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09734](http://arxiv.org/abs/2301.09734)

    本文将拓扑数据分析技术应用于多类数据集中，通过构建拓扑分类器和简单复合体，研究了拓扑复杂性对前馈深度神经网络学习的影响，并验证了拓扑复杂性与DNN学习之间的负相关性。

    

    我们将拓扑数据分析技术应用于多类数据集的拓扑复杂性表征问题。通过定义一个拓扑分类器，使用数据集的开放子覆盖，可以构建一个简单复合体，其拓扑特征（如Betti数）提供关于分类问题的信息。我们使用这些拓扑结构来研究拓扑复杂性对前馈深度神经网络（DNN）学习的影响。我们假设拓扑复杂性与全连接前馈深度神经网络正确分类数据的能力呈负相关。我们在多个构建和开源数据集上评估我们的拓扑分类算法。我们还验证了在多个数据集上拓扑复杂性与DNN学习之间的关系。

    We specialize techniques from topological data analysis to the problem of characterizing the topological complexity (as defined in the body of the paper) of a multi-class data set. As a by-product, a topological classifier is defined that uses an open sub-covering of the data set. This sub-covering can be used to construct a simplicial complex whose topological features (e.g., Betti numbers) provide information about the classification problem. We use these topological constructs to study the impact of topological complexity on learning in feedforward deep neural networks (DNNs). We hypothesize that topological complexity is negatively correlated with the ability of a fully connected feedforward deep neural network to learn to classify data correctly. We evaluate our topological classification algorithm on multiple constructed and open source data sets. We also validate our hypothesis regarding the relationship between topological complexity and learning in DNN's on multiple data sets.
    
[^264]: 带增量个性化的联邦推荐

    Federated Recommendation with Additive Personalization. (arXiv:2301.09109v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09109](http://arxiv.org/abs/2301.09109)

    FedRAP是一种新的推荐系统生成方法，它使用联邦学习训练共享项嵌入和本地个性化视图，以捕捉用户对推荐项目感知的个体差异并且降低通信成本和延迟。

    

    基于联邦学习（FL）构建推荐系统是推动下一代互联网服务和隐私保护的新兴挑战。现有的方法通过FL训练共享项嵌入，同时在客户端保持用户嵌入私密性。然而，相同嵌入对所有客户端的项目不能捕捉到用户对同一项目感知的个体差异，因此导致个性化差。此外，FL中的密集项目嵌入导致通信成本和延迟昂贵。为解决这些挑战，我们提出了带增量个性化的联邦推荐（FedRAP），它通过FL学习项目的全局视图并在每个用户本地学习个性化视图。FedRAP通过正则化增加正则化权重来有效地学习局部和全局视图。我们提出了一种有效的课程表来逐渐学习本地和全局视图，并通过正则化促进两种视图之间的不同之处，使全局视图更稀疏以节省FL的通信成本。

    Building recommendation systems via federated learning (FL) is a new emerging challenge for advancing next-generation Internet service and privacy protection. Existing approaches train shared item embedding by FL while keeping the user embedding private on client side. However, item embedding identical for all clients cannot capture users' individual differences on perceiving the same item and thus leads to poor personalization. Moreover, dense item embedding in FL results in expensive communication cost and latency. To address these challenges, we propose Federated Recommendation with Additive Personalization (FedRAP), which learns a global view of items via FL and a personalized view locally on each user. FedRAP enforces sparsity of the global view to save FL's communication cost and encourages difference between the two views through regularization. We propose an effective curriculum to learn the local and global views progressively with increasing regularization weights. To produce
    
[^265]: 退化是可以接受的：带有不连续分布的网络收入管理中的对数遗憾

    Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions. (arXiv:2210.07996v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07996](http://arxiv.org/abs/2210.07996)

    该论文研究了具有不连续分布的网络收入管理问题，并提出了一种在线算法，其在此模型下实现了对数级别的遗憾。这是在不需要任何“退化”假设的情况下，首次在具有连续值的NRM模型中实现对数级别遗憾的结果。

    

    我们研究了具有接受/拒绝决策和T次独立同分布到达的经典网络收入管理（NRM）问题。我们考虑了一个分布形式，每个到达必须属于有限数量的可能类别之一，每个类别具有确定的资源消耗向量，但是一个在区间上连续分布的随机值。我们开发了一种在线算法，该算法在此模型下实现了O(log^2 T)的遗憾，唯一（必要）的假设是概率密度远离0。我们得到了第二个结果，在二阶增长的额外假设下，实现了O(log T)的遗憾。据我们所知，这是第一个在没有任何“退化”假设的情况下，在具有连续值的NRM模型中实现对数级别遗憾的结果。我们的结果是通过包括一种新的边界mypopic regret，离线分配的“半流体”放松以及改进边界的新技术实现的。

    We study the classical Network Revenue Management (NRM) problem with accept/reject decisions and $T$ IID arrivals. We consider a distributional form where each arrival must fall under a finite number of possible categories, each with a deterministic resource consumption vector, but a random value distributed continuously over an interval. We develop an online algorithm that achieves $O(\log^2 T)$ regret under this model, with the only (necessary) assumption being that the probability densities are bounded away from 0. We derive a second result that achieves $O(\log T)$ regret under an additional assumption of second-order growth. To our knowledge, these are the first results achieving logarithmic-level regret in an NRM model with continuous values that do not require any kind of ``non-degeneracy'' assumptions. Our results are achieved via new techniques including a new method of bounding myopic regret, a ``semi-fluid'' relaxation of the offline allocation, and an improved bound on the 
    
[^266]: 不确定性的不平等影响：平权行动与平权信息

    The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.10019](http://arxiv.org/abs/2102.10019)

    本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。

    This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.

    像贷款批准、医疗干预和大学录取这样的关键决策是在存在不确定性的情况下进行预测的。在本文中，我们证明了不确定性具有不平等的影响。虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化：平均结果较高的群体通常被分配更高的假阳性率，而平均结果较低的群体则被分配更高的假阴性率。我们展示了额外的数据获取可以消除这种差异并扩大机会的获取。我们称之为平权信息的策略可以作为平权行动的替代方案。

    Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
    

