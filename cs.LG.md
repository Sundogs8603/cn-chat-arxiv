# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [UAV Pathfinding in Dynamic Obstacle Avoidance with Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2310.16659) | 本文提出了一种基于多智能体强化学习的集中式训练和分布式执行方法，以在线解决动态障碍物避障问题。实验结果验证了该方法的有效性。 |
| [^2] | [A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation.](http://arxiv.org/abs/2310.16656) | 本文提出了一种原则性重写方法来改善图像生成模型的效果，通过使用专门的自动字幕模型重新标注语料库，并在重写后的数据集上训练文本到图像模型，模型在整体图像质量方面得到了显著改善。 |
| [^3] | [Towards Control-Centric Representations in Reinforcement Learning from Images.](http://arxiv.org/abs/2310.16655) | 该论文提出了一个名为ReBis的方法，通过整合无奖励控制信息和奖励特定知识，来捕捉图像中的控制中心信息。ReBis利用变形器架构来建模动力学，并通过分块掩码消除时空冗余。此外，ReBis还结合了等仿函式损失和非对称重建损失，以防止在稀疏奖励环境中的特征崩溃。 |
| [^4] | [How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels.](http://arxiv.org/abs/2310.16652) | 本文研究了联邦学习对上行和下行通信错误的鲁棒性。理论分析表明，鲁棒性取决于客户端数量和模型参数的数值范围。实验证明上行通信可以容忍更高的误码率比下行通信。 |
| [^5] | [Posterior Consistency for Missing Data in Variational Autoencoders.](http://arxiv.org/abs/2310.16648) | 本论文研究了在包含缺失数据的情况下，从数据中学习变分自动编码器的问题。通过正则化编码器的后验分布，提出了一种改进后验一致性的方法，并在实验证明该方法在缺失值设置下可以提高重建质量和性能。 |
| [^6] | [Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach.](http://arxiv.org/abs/2310.16647) | 本文提出了一种使用随机增广拉格朗日方法实现更灵活和高效的神经网络正则化机制的方法，在黑盒和白盒模型上都取得了显著改进。 |
| [^7] | [Model predictive control-based value estimation for efficient reinforcement learning.](http://arxiv.org/abs/2310.16646) | 该论文提出了一种基于模型预测控制的强化学习方法，通过对环境进行数据驱动建模，并进行多步预测以估计值函数并优化策略，展示出更高的学习效率和更快的收敛速度，同时减少了样本容量需求。 |
| [^8] | [Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks.](http://arxiv.org/abs/2310.16639) | 本论文提出了一种使用概念阻塞作为控制命令预测和用户车辆行为解释的方法，通过学习人类可理解的概念层解释顺序驾驶场景，同时获得竞争性性能和可解释性。 |
| [^9] | [Covariate Shift Adaptation Robust to Density-Ratio Estimation.](http://arxiv.org/abs/2310.16638) | 该论文研究了在协变量偏移下的密度比估计的罕见问题，提出了一种适应性方法来减轻密度比估计的偏差对模型的影响。 |
| [^10] | [Photometric Redshifts with Copula Entropy.](http://arxiv.org/abs/2310.16633) | 本论文提出了基于Copula熵的光度红移研究方法，在SDSS类星体数据上验证了该方法的有效性。实验结果表明，通过使用具有高Copula熵的测量结果，可以显著提高光度红移的准确性，尤其是对于高红移样本。 |
| [^11] | [Free-form Flows: Make Any Architecture a Normalizing Flow.](http://arxiv.org/abs/2310.16624) | 本文提出了一种训练过程，通过使用变量转换公式梯度的高效估计器，克服了归一化流设计在解析逆变换方面的限制。这使得任何保持维度的神经网络都可以作为生成模型进行最大似然训练，并在分子生成和反问题基准测试中取得优秀的结果。 |
| [^12] | [SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence.](http://arxiv.org/abs/2310.16620) | SpikingJelly是一个开源的机器学习基础设施平台，提供了丰富的工具和功能，用于处理神经形态数据集、构建深度脉冲神经网络（SNNs）、优化参数并在神经形态芯片上部署SNNs。相比现有方法，SpikingJelly的训练速度提高了11倍，并通过多级继承和半自动代码生成提供了卓越的可扩展性和灵活性。 |
| [^13] | [Performative Prediction: Past and Future.](http://arxiv.org/abs/2310.16608) | 表演性预测是机器学习中一个新兴领域，通过定义和研究预测对目标的影响，提供了对于分布变化和优化挑战的解决方法。 |
| [^14] | [AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term Memory.](http://arxiv.org/abs/2310.16606) | AirFL-Mem通过长期记忆机制解决了联邦学习中的通信瓶颈问题，并具有与理想通信相同的收敛速度。 |
| [^15] | [Parcel loss prediction in last-mile delivery: deep and non-deep approaches with insights from Explainable AI.](http://arxiv.org/abs/2310.16602) | 本文介绍了两种机器学习方法用于准确预测最后一公里交付中的包裹丢失。通过在比利时货运数据上的实证研究，我们发现深度混合集成学习（DHEL）模型表现最佳，可帮助电子商务零售商优化决策政策。 |
| [^16] | [Balancing central and marginal rejection when combining independent significance tests.](http://arxiv.org/abs/2310.16600) | 该论文提出了一种在组合独立显著性检验时平衡中心和边缘拒绝的方法，并提出了一种用于测量两者平衡的组合函数。 |
| [^17] | [Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes.](http://arxiv.org/abs/2310.16597) | 本文扩展了之前的研究，将证明的范围从独立同分布权重扩展到了更大的权重分布类别(PSEUDO-IID)，包括低秩和稀疏设置。作者发现使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。这些结果可以帮助我们识别更广泛的神经网络的边界混沌状态，并进行性能调优。 |
| [^18] | [Over-the-air Federated Policy Gradient.](http://arxiv.org/abs/2310.16592) | 本文提出了一种过空中联合策略梯度算法，通过无线信道广播携带本地信息的模拟信号实现更新策略参数，研究了噪声和信道失真对算法收敛性的影响，并通过仿真结果验证了算法的有效性。 |
| [^19] | [Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM.](http://arxiv.org/abs/2310.16588) | 该论文展示了一种基于微环的时间延迟储备计算方案，能够同时解决时间序列预测、分类和无线通道均衡等任务，并且在每个通道上实现了最先进的性能。 |
| [^20] | [Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations.](http://arxiv.org/abs/2310.16587) | 本论文提出了一种新的框架，通过利用特征表示的统计特性，在潜在表示中使用数据自适应的高维假设检验来进行不确定性估计。这种方法克服了现有不确定性估计方法对低维分布假设的依赖和数据域限制的问题，为不确定性估计提供了更好的泛化能力和性能。 |
| [^21] | [Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression.](http://arxiv.org/abs/2310.16577) | 本文利用磁力计阵列进行磁场映射，通过新颖方法将磁力计的位置信息纳入，提高了地图质量。 |
| [^22] | [Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression.](http://arxiv.org/abs/2310.16574) | 本论文提出了一种使用结构化核插值的高斯过程回归方法，在室内环境中生成大规模磁场地图。通过将结构化核插值与导数相结合，该方法能够在线性时间复杂度内计算预测均值和协方差，并且在模拟中取得了良好的表现。 |
| [^23] | [Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation.](http://arxiv.org/abs/2310.16566) | 这项研究提出了一种模型增强的对比强化学习方法，用于解决推荐系统中的数据稀疏和过估计问题。 |
| [^24] | [Label Propagation for Graph Label Noise.](http://arxiv.org/abs/2310.16560) | 本文研究了图中的标签噪声问题，提出了一种基于标签传播的算法来处理任意异质性的图标签噪声，以纠正噪声标签并为未标记的节点分配标签。 |
| [^25] | [DECWA : Density-Based Clustering using Wasserstein Distance.](http://arxiv.org/abs/2310.16552) | DECWA是一种使用Wasserstein距离的基于密度的聚类方法，它通过新的群集特征描述和聚类算法在任意形状的群集中表现良好，并解决了低密度群集、相似密度群集和高维数据的问题。 |
| [^26] | [Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion.](http://arxiv.org/abs/2310.16546) | 本论文提出了一种通过随机化风险标准的分布式强化学习算法，以避免在风险上的偏向性，并证明了其收敛性和最优性。实验证明，在包括Atari 55游戏在内的各种环境中，该方法优于其他分布式算法。 |
| [^27] | [FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning.](http://arxiv.org/abs/2310.16538) | FedTherapist是一种使用联邦学习在智能手机上进行用户生成的语言表达的精神健康监测的系统。它有效地利用了持续语音和键盘输入，并通过上下文感知语言学习方法来提高预测的准确性。在评估中，比较了非语言特征，结果显示FedTherapist在预测抑郁、压力、焦虑和心情方面的表现更好。 |
| [^28] | [Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents.](http://arxiv.org/abs/2310.16527) | 本文提出了一种深度学习模型，通过多任务预训练来增强文档信息分析的能力，实现了对视觉丰富文档中信息的提取。模型利用transformer-based模型编码文档图像的各种信息，并通过预训练和微调来优化各种文档图像分析任务。另外，模型还包括了额外的任务和集体预训练方案。 |
| [^29] | [Cyclic Directed Probabilistic Graphical Model: A Proposal Based on Structured Outcomes.](http://arxiv.org/abs/2310.16525) | 本文提出了一种基于结构化结果的循环有向概率图模型，该模型可以直接捕捉在结构学习过程中产生的方向性循环依赖关系。 |
| [^30] | [Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data.](http://arxiv.org/abs/2310.16524) | 本文介绍了3S Testing，这是一个深度生成建模框架，通过生成合成测试集和模拟分布偏移来改进模型评估，实验结果表明3S Testing在评估少数子群体的模型性能和在分布偏移下表现更优。 |
| [^31] | [Towards Self-Interpretable Graph-Level Anomaly Detection.](http://arxiv.org/abs/2310.16520) | 本文提出了一个自解释图级异常检测模型（SIGNET），能够识别异常图并生成有意义的解释，解决了当前图级异常检测方法无法提供有效解释的问题。 |
| [^32] | [Particle-based Variational Inference with Generalized Wasserstein Gradient Flow.](http://arxiv.org/abs/2310.16516) | 本文提出了一种基于广义Wasserstein梯度流的ParVI框架，通过引入凸函数引导的更广泛类别的正则化器，解决了传统基于核函数的方法设计困难和限制性的问题，并展示了其具有强大的收敛性保证和高效性能。 |
| [^33] | [Identifying Reasons for Bias: An Argumentation-Based Approach.](http://arxiv.org/abs/2310.16506) | 本文提出了一种基于论证的方法来确定为什么一个个体被分类与相似个体不同，该方法使用定量论证框架来表示个体和与其相似个体的属性-值对，并使用一个众所周知的语义来确定对个体分类产生最大贡献的属性-值对。 |
| [^34] | [Data Optimization in Deep Learning: A Survey.](http://arxiv.org/abs/2310.16499) | 该论文综合了现有的深度学习数据优化方法，并提出了全面的分类体系。 |
| [^35] | [Citizen participation: crowd-sensed sustainable indoor location services.](http://arxiv.org/abs/2310.16496) | 本研究提出了一种基于机器学习的室内定位服务方法，无需额外的硬件设施，提供了小于2米的精确度，并且即使在丢失大量BSSIDs的情况下，仍能保持鲁棒性。 |
| [^36] | [On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection.](http://arxiv.org/abs/2310.16492) | 本文研究了文本异常曝光在视觉外界检测中的应用。通过使用文本异常值代替图像中的异常值，我们揭示了使用文本异常值的好处，并提出了多种生成文本异常值的方法。 |
| [^37] | [TSONN: Time-stepping-oriented neural network for solving partial differential equations.](http://arxiv.org/abs/2310.16491) | TSONN是一种将时间步方法与深度学习相结合的神经网络，通过将原始问题转化为一系列良好条件的子问题来解决PDE求解中的稳定训练和正确结果的挑战。 |
| [^38] | [Hyperparameter Optimization for Multi-Objective Reinforcement Learning.](http://arxiv.org/abs/2310.16487) | 该论文研究了多目标强化学习中的超参数优化的挑战，并提出了一种系统的方法来解决这一问题。 |
| [^39] | [A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP.](http://arxiv.org/abs/2310.16485) | 本文介绍了一个新的深度学习监督方法，用于检测多元时间序列数据中的事件。与现有方法相比，该方法在回归、标记数据集需求和鲁棒性方面具有创新。 |
| [^40] | [Symphony of experts: orchestration with adversarial insights in reinforcement learning.](http://arxiv.org/abs/2310.16473) | 这篇论文介绍了一种利用专家策略进行决策指导的编排方法，通过将对抗性设置中的后悔边界结果转移到表格设置下的编排中，推广了自然策略梯度的分析，并提供了关于样本复杂度的洞察。这种方法的关键点在于其透明的证明。在随机匹配玩具模型中进行了模拟实验。 |
| [^41] | [Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes.](http://arxiv.org/abs/2310.16466) | 本研究提出了一种名为NDP4ND的神经ODE进程，以从稀缺观测中学习连续网络新兴动态。这种方法通过建立数据自适应的随机网络动力学，克服了由于稀疏、不规则采样、部分和噪声观测而导致的学习困难。 |
| [^42] | [Towards Explainability in Monocular Depth Estimation.](http://arxiv.org/abs/2310.16457) | 本文研究了单眼深度估计方法中的可解释性，重点关注了人类感知深度的一个最重要的视觉线索——相对尺寸。通过模拟人类实验和测试最先进的方法，本研究实现了平均准确率约为77%的结果，间接揭示了这些方法的可解释性。 |
| [^43] | [ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training.](http://arxiv.org/abs/2310.16453) | ClearMark是第一种为直观人类评估而设计的DNN水印方法，它通过嵌入可见水印来允许人类决策，同时使用转置模型架构将水印与主要任务交织在一起。 |
| [^44] | [Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph.](http://arxiv.org/abs/2310.16452) | 本文提出了一个名为PEARLM的方法，通过语言建模开展基于路径的知识图谱推荐，解决了现有方法中对预训练知识图谱嵌入的依赖以及未充分利用实体和关系之间相互依赖性的问题，还避免了生成不准确的解释。实验结果表明，与现有方法相比，我们的方法效果显著。 |
| [^45] | [Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding.](http://arxiv.org/abs/2310.16441) | 该论文揭示了在线性网络中的领悟现象，研究了领悟时间与输入输出维度、训练样本量、正则化和网络初始化的关系，并发现泛化准确性的大幅提升并不一定意味着从“记忆”到“理解”的转变。 |
| [^46] | [Non-isotropic Persistent Homology: Leveraging the Metric Dependency of PH.](http://arxiv.org/abs/2310.16437) | 本论文提出了一种非各向异性的持久同调方法，通过变化底层空间上的距离函数并分析持久图的偏移，可以提取额外的拓扑和几何信息。 |
| [^47] | [FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning.](http://arxiv.org/abs/2310.16412) | 本文提出了FlatMatch方法，通过最小化交叉锋利度度量来确保标记数据和未标记数据之间的一致学习性能，提高了半监督学习的泛化性能。 |
| [^48] | [Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero.](http://arxiv.org/abs/2310.16410) | 本研究提出了一种新的方法，可以从AlphaZero中提取新的国际象棋概念，并发现这些概念可以被顶级国际象棋大师所学习和应用。 |
| [^49] | [Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model.](http://arxiv.org/abs/2310.16409) | 该论文研究了将推荐系统与大型语言模型结合以实现顺序推荐的问题。现有工作主要考虑单键情况，而忽略了多键值数据的重要性。本研究的贡献在于解决了实际应用中多键值数据的推荐问题。 |
| [^50] | [Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels.](http://arxiv.org/abs/2310.16407) | 这项工作提出了一种基于信息论的拓扑感知联邦边缘学习的泛化分析方法，并提出了一种名为FedGMIR的正则化方法来增强模型性能。 |
| [^51] | [Graph Neural Networks with a Distribution of Parametrized Graphs.](http://arxiv.org/abs/2310.16401) | 这篇论文提出了一种使用参数化和生成多个图来解决图神经网络仅使用单个观察图的挑战的方法。在节点分类和图回归任务上，通过最大似然估计网络参数，在多个图上使用马尔可夫链蒙特卡洛方法，结合PAC-Bayesian理论的原则，取得了性能改进。 |
| [^52] | [Learning Efficient Surrogate Dynamic Models with Graph Spline Networks.](http://arxiv.org/abs/2310.16397) | 本文提出了一种新颖的深度学习方法GraphSplineNets，通过减少深度替代模型的网格大小和迭代步数来加速物理系统的预测，同时使用两种可微的正交样条插值方法和自适应插值策略提高准确度和速度的权衡。 |
| [^53] | [Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization.](http://arxiv.org/abs/2310.16391) | 这项研究提出了一种名为EVIL的方法，通过探索分布知识中的变量参数，将其排除在不变性学习之外，从而找到一个抵抗分布转移的鲁棒子网络。 |
| [^54] | [Distributed Uncertainty Quantification of Kernel Interpolation on Spheres.](http://arxiv.org/abs/2310.16384) | 本文提出和研究了一种分布式插值方法，用于管理和量化通过插值球面嘈杂数据而带来的不确定性。数值模拟结果表明该方法在处理具有挑战性计算环境中的噪声数据方面是实用和强大的。 |
| [^55] | [A model for multi-attack classification to improve intrusion detection performance using deep learning approaches.](http://arxiv.org/abs/2310.16380) | 该论文提出了一个用深度学习方法改进入侵检测性能的多攻击分类模型，这个模型通过引入新的深度学习方法，在准确度、检测率和低误报率方面取得了优秀的结果 |
| [^56] | [GADY: Unsupervised Anomaly Detection on Dynamic Graphs.](http://arxiv.org/abs/2310.16376) | GADY是一种在动态图上进行无监督异常检测的新方法，通过提出连续动态图模型和引入负采样模块来解决了动态结构构建挑战和负采样挑战。 |
| [^57] | [DyExplainer: Explainable Dynamic Graph Neural Networks.](http://arxiv.org/abs/2310.16375) | DyExplainer是一个新颖的方法，用于解释动态图神经网络。它训练一个动态GNNs骨架，能够提取每个快照中图的表示，并且同时进行探索，以有效捕捉时间依赖性和结构关系。 |
| [^58] | [Joint Distributional Learning via Cramer-Wold Distance.](http://arxiv.org/abs/2310.16374) | 本文引入了克拉默沃尔德距离正则化，以更好地处理高维数据集和观测变量之间的复杂相关结构，并通过两步学习方法提高了先验建模的灵活性和聚合后验与先验分布之间的对齐度。 |
| [^59] | [Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms.](http://arxiv.org/abs/2310.16363) | 本文研究了受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析，证明了这些算法能找到性能函数的一阶稳定点，并且具有较低的样本复杂度。 |
| [^60] | [Neural Potential Field for Obstacle-Aware Local Motion Planning.](http://arxiv.org/abs/2310.16362) | 提出了一种基于神经网络的局部运动规划方法，通过神经潜力场模型返回可微分的碰撞成本，利用神经图像编码器将问题维度降低两个数量级，实验结果表明与现有方法相当。 |
| [^61] | [Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs.](http://arxiv.org/abs/2310.16355) | Redco是一个轻量级工具，旨在自动化分布式训练LLMs，并简化ML流程的开发。 |
| [^62] | [Generative Pre-training for Speech with Flow Matching.](http://arxiv.org/abs/2310.16338) | 本文展示了一种使用流匹配的预训练生成模型，该模型可以适应不同的下游任务并获得强大的性能，通过在60k小时的未转录语音上进行预训练，该模型可以与现有的专家模型在语音增强、分离和合成方面进行匹配或超越。 |
| [^63] | [SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process.](http://arxiv.org/abs/2310.16336) | 提出了SMURF-THP方法来学习Transformer Hawkes过程并量化预测不确定性。通过学习到的分数函数，可以从预测分布中采样事件到达时间，并计算置信区间来量化不确定性。 |
| [^64] | [Defense Against Model Extraction Attacks on Recommender Systems.](http://arxiv.org/abs/2310.16335) | 本文引入了基于梯度的排序优化（GRO）作为第一个防御推荐系统模型抽取攻击的策略，最小化受保护的目标模型的损失，同时最大化攻击者的代理模型的损失。 |
| [^65] | [Corrupting Neuron Explanations of Deep Visual Features.](http://arxiv.org/abs/2310.16332) | 本文通过对神经元解释方法进行鲁棒性分析，发现这些解释可以被随机噪声和精心设计的扰动严重破坏，即使添加小的随机噪声也可以改变高达28％的神经元的概念分配。此外，作者还设计了一种新的污染算法，通过污染不到10％的探测数据可以操纵超过80％的神经元的解释。这引发了在现实生活中对神经元解释方法的信任问题。 |
| [^66] | [Brain-Inspired Reservoir Computing Using Memristors with Tunable Dynamics and Short-Term Plasticity.](http://arxiv.org/abs/2310.16331) | 这项研究提出了一种使用具有可调动力学和短期可塑性的记忆电阻器进行类脑储层计算的方法。使用这种方法可以实现更快的信息处理速度、更低的能耗和更小的面积占用。研究人员通过对输入数据进行编码，使得记忆电阻器的动力学能够自适应地调整，克服了先前实现中的限制。 |
| [^67] | [Reinforcement Learning for SBM Graphon Games with Re-Sampling.](http://arxiv.org/abs/2310.16326) | 本文研究了基于再采样的SBM图动态的强化学习问题，并提出了一种新的学习框架。我们证明了策略镜像上升算法能够找到多种群均场博弈的纳什均衡，并提出了一种高效的样本集成的强化学习算法来解决实际情况下随机块模型未知的情况。 |
| [^68] | [Personalized Federated X -armed Bandit.](http://arxiv.org/abs/2310.16323) | 本文对个性化的联邦多臂赌博问题进行了研究，提出了PF-PNE算法，通过双重淘汰策略和有效的本地目标评估方法，实现了同时优化异质本地目标和鼓励联邦合作，该算法在多个基线算法和实验数据集上都表现出较好的性能。 |
| [^69] | [Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo.](http://arxiv.org/abs/2310.16320) | 本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。 |
| [^70] | [Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder.](http://arxiv.org/abs/2310.16318) | 本文提出了一种与语言形式无关的元学习蒙版自监督学习框架MetaMAE，通过将蒙版自编码器（MAE）的蒙版重构视为元学习任务，并采用转换器元学习技术来改进MAE的自监督学习在不同语言形式上的表现。 |
| [^71] | [Sum-of-Parts Models: Faithful Attributions for Groups of Features.](http://arxiv.org/abs/2310.16316) | Sum-of-Parts模型通过构造保证特征组归因的忠实性，将预测分解为可解释的分数之和，帮助天体物理学家发现了关于星系形成的新知识。 |
| [^72] | [Understanding Code Semantics: An Evaluation of Transformer Models in Summarization.](http://arxiv.org/abs/2310.16314) | 本研究通过评估代码摘要的有效性和引入对抗性案例，研究了基于Transformer模型的代码理解能力。结果显示，模型在理解代码语义方面仍存在挑战，这对于提高软件开发和维护效率具有重要意义。 |
| [^73] | [Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process with Uncertainty Quantification.](http://arxiv.org/abs/2310.16310) | 提出了SMASH方法，基于分数匹配的伪似然度量，用于学习具有不确定性量化的标记时空点过程（STPPs）。该方法解决了现有方法中概率密度函数归一化的问题，并提供了模型预测的不确定性量化。 |
| [^74] | [Dolfin: Diffusion Layout Transformers without Autoencoder.](http://arxiv.org/abs/2310.16305) | Dolfin是一种改进的布局生成模型，通过Transformer-based扩散过程和自回归扩散模型(Dolfin-AR)实现，相比现有方法具有更高的建模能力和较低的复杂性。 |
| [^75] | [Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks.](http://arxiv.org/abs/2310.16302) | 本论文提出了一种低成本的不完美数字孪生辅助强化训练方法，通过引入数字孪生技术来减少实际训练的成本，并通过自然和虚拟生成的无人机混合部署方法来平衡训练成本、数字孪生建设成本和数字孪生偏差对训练的影响。 |
| [^76] | [Instance-wise Linearization of Neural Network for Model Interpretation.](http://arxiv.org/abs/2310.16295) | 这项研究提出了一种实例化线性化的方法，用于解释神经网络模型。通过给模型内部的每个输入特征分配重要得分，揭示了模型如何使用特征做出决策。这种方法有助于解决当前特征归因方法中的局限性，并提高了模型解释的准确性。 |
| [^77] | [Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification.](http://arxiv.org/abs/2310.16293) | Crowd-Certain是一种在众包和集成学习分类中进行标签聚合的新方法，通过比较标注者的一致性与训练好的分类器，确定每个标注者的可靠性得分，并利用预测概率实现了对未来样本数据的重复使用，从而提高了性能和计算效率。 |
| [^78] | [Removing Dust from CMB Observations with Diffusion Models.](http://arxiv.org/abs/2310.16285) | 本文研究了使用扩散模型去除CMB观测中的尘埃的方法，并发现扩散模型可以很好地模拟尘埃前景并进行成分分离。我们还引入了一种与CMB宇宙学条件相结合的模型，在成分分离中表现出更好的性能。 |
| [^79] | [Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions.](http://arxiv.org/abs/2310.16277) | 本研究通过学习网络参数的领域不变后验分布，提出了一种名为PosTerior Generalization的方法，能够更好地泛化到未见过的目标领域。 |
| [^80] | [Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism.](http://arxiv.org/abs/2310.16270) | Attention Lens是一种工具，它能够通过学习的注意力头特定转换将注意力头的输出翻译为词汇标记。使用Attention Lens，我们可以解释注意力头在生成最终标记预测中的作用。注意力头在语言模型中扮演着高度专门化的角色。 |
| [^81] | [A Causal Disentangled Multi-Granularity Graph Classification Method.](http://arxiv.org/abs/2310.16256) | 这篇论文提出了一种因果解缠离散多粒度图分类方法（CDM-GNN），该方法能够解决图数据的多粒度特性，实现了对图中重要子结构和偏差部分的解析，并用于图分类任务中。 |
| [^82] | [Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits & Dueling Bandits.](http://arxiv.org/abs/2310.16252) | 本研究研究了具有噪声的两人零和矩阵游戏中纯策略纳什均衡的样本复杂度问题，设计了一个近最优算法，其样本复杂度与下界相匹配，同时解决了随机多臂赌博机和决斗赌卒中的纯探索问题。 |
| [^83] | [ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality.](http://arxiv.org/abs/2310.16242) | 本文介绍了一种名为ZzzGPT的交互式GPT方法，旨在提高睡眠质量。通过利用大型语言模型进行预测和反馈，该方法融合了先进的机器学习和用户导向设计，以提供准确和有价值的结果。 |
| [^84] | [Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction.](http://arxiv.org/abs/2310.16241) | 本文提出了一种自动任务分组的方法，通过研究任务对多任务学习的亲和力以及任务的固有特征和单任务学习特性，实现了对任务的自动分组。 |
| [^85] | [Attention-Based Ensemble Pooling for Time Series Forecasting.](http://arxiv.org/abs/2310.16231) | 提出了一种基于注意力机制的集成池化方法，用于时间序列预测。该方法通过学习加权平均值来权衡不同预测模型的输出，并在两个时间序列预测问题上进行了测试。在某些情况下，该方法优于现有的集成池化方法。 |
| [^86] | [On the Foundations of Shortcut Learning.](http://arxiv.org/abs/2310.16228) | 该论文研究了快速学习的基础，揭示了模型对哪些特征更偏好，即可预测性和可用性如何相互影响模型的特征使用。 |
| [^87] | [TiC-CLIP: Continual Training of CLIP Models.](http://arxiv.org/abs/2310.16226) | 该论文提出了用于训练视觉-语言模型的大规模时间连续 (TiC) 基准，使用这些基准评估了现有模型的时间鲁棒性，并展示了一种简单有效的排练方法来持续训练模型。 |
| [^88] | [CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset.](http://arxiv.org/abs/2310.16225) | CleanCoNLL是一种几乎无噪声的命名实体识别数据集，通过全面重标记和自动一致性检查来纠正CoNLL-03中的注释错误，提高了最先进方法的F1分数，并减少了因注释缺失而误判的情况。 |
| [^89] | [Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks.](http://arxiv.org/abs/2310.16224) | 这项研究提出了一种全面不可知的框架DIVA，通过分析潜在的毒害数据集来检测机器学习模型遭受的攻击。通过比较模型在受到毒害和干净数据上的准确性，DIVA能够预测未知的干净数据集上的准确性，从而实现对一般毒害攻击的检测。 |
| [^90] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^91] | [Performance Tuning for GPU-Embedded Systems: Machine-Learning-based and Analytical Model-driven Tuning Methodologies.](http://arxiv.org/abs/2310.16214) | 本文开发了一种基于分析模型和基于机器学习的GPU嵌入式系统优化方法，针对并行前缀操作进行了性能评估，并提供了性能洞察。 |
| [^92] | [ELM Ridge Regression Boosting.](http://arxiv.org/abs/2310.16209) | ELM岭回归增强方法显著提高了ELM的分类性能和鲁棒性。 |
| [^93] | [Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder: Theoretical and Empirical Insights.](http://arxiv.org/abs/2310.16194) | 本文提出了一种新的方法，称为低秩自编码器（LoRAE），通过加入低秩正则化项，自适应地重构低维潜在空间，同时保持自编码器的基本目标。该方法在图像生成和下游分类等任务中展现出了优越性。 |
| [^94] | [Efficient deep data assimilation with sparse observations and time-varying sensors.](http://arxiv.org/abs/2310.16187) | 本研究提出了一种新的深度数据同化方案（VIVID），结合了Voronoi-tessellation和卷积神经网络的能力，用于处理稀疏、非结构化和时变的传感器数据，并建立了一个高效的数据同化框架。 |
| [^95] | [Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images.](http://arxiv.org/abs/2310.16186) | 本文提出了一种使用U-Net架构进行粉末X射线衍射图像的图像分割的方法，通过深度学习和卷积神经网络，能够准确识别和排除实验X射线衍射图像中的伪影，并在召回率和假阳性方面表现出显著的改进。 |
| [^96] | [BLP 2023 Task 2: Sentiment Analysis.](http://arxiv.org/abs/2310.16183) | BLP 2023任务2是关于情感分析的共享任务，吸引了71个参与者。参与者通过各种方法，包括经典机器学习模型和大型语言模型，提交了597个运行结果。本文提供了任务的详细设置和参与者提交系统的概述。 |
| [^97] | [G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation.](http://arxiv.org/abs/2310.16175) | G-CASCADE是一种基于图卷积的解码器，用于2D医学图像分割，通过有效的图卷积块逐渐改进分层Transformer编码器生成的特征图，保留长程信息，并且在多个医学图像分割任务上优于其他最先进方法。 |
| [^98] | [On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\epsilon$-Greedy Exploration.](http://arxiv.org/abs/2310.16173) | 本文首次提供了关于实际设置下具有epsilon-greedy策略的Deep Q网络（DQN）的收敛性和样本复杂度分析。 |
| [^99] | [Brainchop: Next Generation Web-Based Neuroimaging Application.](http://arxiv.org/abs/2310.16162) | Brainchop是一种基于浏览器的神经影像工具，采用前端机器学习和预训练模型，在保护数据隐私的同时提供全面的大脑预处理和分割解决方案。 |
| [^100] | [Context-aware feature attribution through argumentation.](http://arxiv.org/abs/2310.16157) | 本论文提出了一种基于论证的上下文感知特征归因方法，以解决机器学习和数据分析中特征归因的挑战。该方法利用广义可加模型和梯度方法与替代模型相结合，同时考虑用户的背景信息，从而提高了归因的准确性和解释性。 |
| [^101] | [Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations.](http://arxiv.org/abs/2310.16154) | 本论文研究了深度学习模型的结构与处理数据中固有结构之间的关系，探索了深度学习的理论基础，旨在打破所谓的维度诅咒，并理解深度学习算法的有效性和其超越传统方法的原因。 |
| [^102] | [FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering.](http://arxiv.org/abs/2310.16152) | 本文提出了一种FLTrojan攻击方法，通过选择性权重篡改，从联邦语言模型中泄露隐私敏感用户数据。通过观察到FL中中间轮次的模型快照可以引起更大的隐私泄露，并发现隐私泄露可以通过篡改模型的选择性权重来加剧。 |
| [^103] | [A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing.](http://arxiv.org/abs/2310.16142) | 开发了一个循环神经语言模型，通过使用单个自我注意头紧密模拟了认知理论中假设的记忆系统，并捕捉到人类句子处理中的干扰。 |
| [^104] | [Online Thermal Field Prediction for Metal Additive Manufacturing of Thin Walls.](http://arxiv.org/abs/2310.16125) | 本文提出了一种在线热场预测方法，通过映射和重建技术，在金属增材制造过程中使用少量传感器实现尚未打印部件的热场预测和性能控制。 |
| [^105] | [Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems.](http://arxiv.org/abs/2310.16123) | 提出了一种锚定空间优化传输（ASOT）问题，通过将分布映射到共享的锚点空间，学习其潜在的共同特征，从而加速了多个传输问题的批处理。 |
| [^106] | [19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics.](http://arxiv.org/abs/2310.16121) | 只需19个参数的微型神经网络在顶夸克喷注的二分类任务上表现出优于数以万计参数的通用架构的性能。 |
| [^107] | [Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations.](http://arxiv.org/abs/2310.16119) | Alquist 5.0是一种新的SocialBot系统，通过将对话树和生成模型相结合，以及引入NRG Barista和支持多模式设备，提高了用户对话体验，并保持了共情和知识型对话能力。 |
| [^108] | [Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos.](http://arxiv.org/abs/2310.16115) | 本文提出了一种新的类增量学习方法，通过使用来自在线安慰剂的旧类别数据进行知识蒸馏，解决了类增量学习中长期记忆的问题，并提高了旧类别知识保留的效率。 |
| [^109] | [Compressed representation of brain genetic transcription.](http://arxiv.org/abs/2310.16113) | 本文研究了大脑基因转录的压缩表示方法，通过比较不同的线性和非线性方法，评估了它们在重建、解剖和预测方面的性能。 |
| [^110] | [Locally Differentially Private Document Generation Using Zero Shot Prompting.](http://arxiv.org/abs/2310.16111) | 本研究提出了一种本地差分隐私文档生成机制，利用预训练的大型语言模型和零阶提示对抗作者去匿名攻击，同时最小化对下游效用的影响。实验证明，该机制在降低攻击成功率的同时能够完全恢复清洁的情感分数，比现有方法更有效。 |
| [^111] | [Decentralized Learning over Wireless Networks with Broadcast-Based Subgraph Sampling.](http://arxiv.org/abs/2310.16106) | 本文介绍了一种在无线网络上进行去中心化学习的通信框架BASS，采用广播传输和概率子图采样。通过控制子集的随机激活和重要性，以及通信成本约束，实现了算法的快速收敛。 |
| [^112] | [Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs.](http://arxiv.org/abs/2310.16105) | 本文提出了一种基于局部差分隐私的梯度跟踪分布式在线学习算法，在保持学习准确性的同时，有效地保护了学习者的隐私。 |
| [^113] | [Anatomically-aware Uncertainty for Semi-supervised Image Segmentation.](http://arxiv.org/abs/2310.16099) | 本研究提出了一种解剖学感知的方法，通过利用分割掩模中的全局信息来估计半监督图像分割的不确定性。该方法克服了传统不确定性估计方法的高计算复杂性和对像素级差异的限制。 |
| [^114] | [Contextual Bandits for Evaluating and Improving Inventory Control Policies.](http://arxiv.org/abs/2310.16096) | 这项研究引入了均衡策略的概念，并提出了一种基于上下文强盗的算法来评估和改进库存控制策略，这在理论上和实证研究中得到了有利的保证。 |
| [^115] | [Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions.](http://arxiv.org/abs/2310.16076) | 线性变换器（LTs）或快速权重程序员（FWPs）是一种特殊的序列处理器，可以用于循环神经网络以及自注意力网络。研究发现，类似于标准变换器的很多结果也适用于LTs/FWPs。循环FWP和自引用权重矩阵的扩展成功地克服了LT的一些限制，例如在奇偶问题上的泛化。 |
| [^116] | [Grid Frequency Forecasting in University Campuses using Convolutional LSTM.](http://arxiv.org/abs/2310.16071) | 该论文介绍了一种创新的方法，利用卷积神经网络和长短期记忆网络建立稳健的时间序列预测模型，用于预测电网频率。个体化的卷积LSTM模型可以独立地为大学校园内的建筑进行训练和评估，并且结果证明了该模型的优越性。 |
| [^117] | [Spatial-Temporal Hypergraph Neural Network for Traffic Forecasting.](http://arxiv.org/abs/2310.16070) | 本文提出了一种名为STHODE的空间-时间超图神经网络用于交通预测，该网络结合了道路网络拓扑和交通动态，能够有效捕捉交通数据中的高阶空间-时间依赖性。 |
| [^118] | [The Hyperdimensional Transform: a Holographic Representation of Functions.](http://arxiv.org/abs/2310.16065) | 这项研究介绍了一种新型的积分变换-超维变换，它将函数转换为噪声鲁棒、全息、高维表示的超维向量，与其他积分变换紧密相关，并为超维领域提供了理论基础和新的洞察。 |
| [^119] | [Enhancing Traffic Prediction with Learnable Filter Module.](http://arxiv.org/abs/2310.16063) | 本论文提出了一种利用可学习滤波模块来适应性地过滤交通数据中的噪声的方法，通过傅里叶变换将数据转换为频域，然后根据噪声的模式进行滤波，最后再通过逆傅里叶变换将数据恢复到时域。这种方法可以提升交通预测模型的输入数据质量。 |
| [^120] | [Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning.](http://arxiv.org/abs/2310.16062) | 本研究提出了一种对抗领域适应与混淆平衡（ADA-CBF）方法，用于在预训练大模型微调中考虑混淆因素。该方法通过联合训练预训练大模型、领域分类器和混淆分类器，并使用对抗损失来改善领域不变的表示学习和平衡混淆因子的分布。 |
| [^121] | [A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially Correlated Faults with Application to Multistation Assembly Systems.](http://arxiv.org/abs/2310.16058) | 本论文提出了一种用于制造系统故障诊断的新方法：空间相关稀疏贝叶斯学习。该方法可应对传感器数量有限和非平稳空间相关故障等挑战，并在多站装配系统中展示了其应用性。 |
| [^122] | [Physically Explainable Deep Learning for Convective Initiation Nowcasting Using GOES-16 Satellite Observations.](http://arxiv.org/abs/2310.16015) | 本研究开发了基于多通道红外卫星观测数据的可解释性深度学习模型，用于预测大气对流起始。通过案例研究，该模型表现出对云层和湿度特性的依赖性，并在误报率上显著优于经典的逻辑模型。 |
| [^123] | [Accented Speech Recognition With Accent-specific Codebooks.](http://arxiv.org/abs/2310.15970) | 本研究提出了一种使用具有专门口音代码本的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。在实验证明了该方法在已见和未见的口音上都能获得显著的性能提升。 |
| [^124] | [Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles.](http://arxiv.org/abs/2310.15952) | 本文引入了一种新颖的三阶段方法，通过变换器和条件扩散模型来改善医学图像分类模型对实际应用中常见成像变异性的鲁棒性。 |
| [^125] | [Online Robust Mean Estimation.](http://arxiv.org/abs/2310.15932) | 本文研究了在线高维健壮均值估计的问题，提出了两个主要结果。 |
| [^126] | [COPF: Continual Learning Human Preference through Optimal Policy Fitting.](http://arxiv.org/abs/2310.15694) | 通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。 |
| [^127] | [VMAF Re-implementation on PyTorch: Some Experimental Results.](http://arxiv.org/abs/2310.15578) | 这项研究重新在PyTorch上实现了VMAF，与标准实现进行比较，结果显示在VMAF单位上的差异小于$10^{-2}$。同时，研究了在使用VMAF作为目标函数时的梯度计算，并证明使用该函数进行训练不会导致梯度不良。 |
| [^128] | [MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks.](http://arxiv.org/abs/2310.15074) | MGAS是一个多粒度架构搜索的统一框架，通过学习特定粒度级别的离散化函数，自适应地确定剩余比例，从而实现同时优化模型大小和模型性能。 |
| [^129] | [Data Pruning via Moving-one-Sample-out.](http://arxiv.org/abs/2310.14664) | 本文提出了一种新颖的数据修剪方法MoSo，它通过评估样本对最优经验风险的影响来确定每个样本的重要性，并提出了一种高效的一阶近似器来计算样本的重要性，该近似器只需要梯度信息。 |
| [^130] | [Attention-Enhancing Backdoor Attacks Against BERT-based Models.](http://arxiv.org/abs/2310.14480) | 本文提出了一种基于BERT模型的增强注意力的后门攻击方法，通过直接操纵注意力模式来增强特洛伊行为，有效提高了攻击成功率和污染率。该方法适用于不同的任务和模型。 |
| [^131] | [InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution.](http://arxiv.org/abs/2310.13276) | 通过反向图卷积进行的鲁棒跨模态检索，解决了表示退化问题，并通过增加数据点之间的距离来有效分离不同模态的表示。 |
| [^132] | [A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation.](http://arxiv.org/abs/2310.12127) | 本研究通过调查机器翻译模型中的性别偏见问题以及缓解性别偏见的方法来填补现有研究的空白。研究发现指导微调模型在默认为男性翻译上存在性别偏见，同时忽视了指示职业性别的代词，并提出了一些可行的缓解策略。 |
| [^133] | [Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability.](http://arxiv.org/abs/2310.11518) | 这篇论文研究了多人游戏中自我对抗的保证问题，通过多矩阵可分解性，在满足一定条件的情况下，通过自我对抗学习的算法能够产生有界脆弱性的策略。 |
| [^134] | [ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors.](http://arxiv.org/abs/2310.10692) | ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。 |
| [^135] | [Infinite Width Graph Neural Networks for Node Regression/ Classification.](http://arxiv.org/abs/2310.08176) | 本研究分析了无限宽度图神经网络在图结构化数据上的应用。通过连接深度学习和高斯过程/核方法，研究推广了神经网络，并推导出了闭式形式的核函数和高斯过程。研究结果表明，高斯过程和核方法在不确定性估计方面更加用户友好，并且可以在多种架构和数据集上进行回归/分类任务。 |
| [^136] | [Are GATs Out of Balance?.](http://arxiv.org/abs/2310.07235) | 本研究揭示了Graph Attention Network (GAT)梯度流动力学的守恒定律，解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。我们还提出了一种平衡GAT网络的初始化方案，使得深层网络更容易进行训练，并且相比标准初始化，具有更快的收敛速度。 |
| [^137] | [Optimal Exploration is no harder than Thompson Sampling.](http://arxiv.org/abs/2310.06069) | 这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？ |
| [^138] | [DSAC-T: Distributional Soft Actor-Critic with Three Refinements.](http://arxiv.org/abs/2310.05858) | 本论文介绍了DSAC-T，通过评论者梯度调整、双值分布学习和基于方差的目标回报裁剪等三个改进对标准DSAC进行了改进，解决了标准DSAC存在的不稳定学习过程和对任务特定奖励缩放的问题，提高了算法的性能和适应性。 |
| [^139] | [Improving Summarization with Human Edits.](http://arxiv.org/abs/2310.05857) | 本文介绍了一种改进摘要生成的方法，使用人工编辑的反馈数据，并通过序列对齐（不）似然训练(SALT)技术将人工编辑数据与模型生成数据结合起来。实验证明了这种方法在医学领域摘要生成中的有效性。 |
| [^140] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^141] | [Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks.](http://arxiv.org/abs/2310.01037) | 本文介绍了一种名为地震图变压器（SeisT）的通用深度学习骨干网络模型，用于多种地震监测任务。SeisT的高效网络架构使其在地震检测、地震相位拾取、首次运动极性分类、震级估计和反方位角估计等任务中表现优秀，特别是在泛化性能方面。 |
| [^142] | [Leave-one-out Distinguishability in Machine Learning.](http://arxiv.org/abs/2309.17310) | 这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。 |
| [^143] | [Learning to Receive Help: Intervention-Aware Concept Embedding Models.](http://arxiv.org/abs/2309.16928) | 这项研究提出了一种干预感知的概念嵌入模型，用于提高神经架构对概念干预的响应性，并解决了概念干预顺序和模型架构的依赖性的问题。 |
| [^144] | [A Diffusion-Model of Joint Interactive Navigation.](http://arxiv.org/abs/2309.12508) | 本文提出了一种基于扩散模型的方法 DJINN，用于生成交通场景。通过联合扩散所有代理的轨迹，并以灵活的状态观察为条件，我们在轨迹预测上取得了最先进的性能。此外，DJINN还能灵活地从多种有价值的条件分布中进行测试时抽样。 |
| [^145] | [Guide Your Agent with Adaptive Multimodal Rewards.](http://arxiv.org/abs/2309.10790) | 本文提出了一种自适应返回条件策略（ARP）框架，通过使用自然语言任务描述和预训练的多模态编码器来提升智能体的泛化能力。通过在预训练的多模态嵌入空间中计算视觉观察和自然语言指令之间的相似度，并将其用作奖励信号，ARP有效缓解了目标误泛化问题，并在面对未知的文本指令时展现出了出色的泛化性能。 |
| [^146] | [A Configurable Library for Generating and Manipulating Maze Datasets.](http://arxiv.org/abs/2309.10498) | 这个论文介绍了一个可配置的库，用于生成和处理迷宫数据集，研究人员可以通过该库生成不同分布的迷宫数据集，并对生成参数和生成规则进行自定义控制。可以支持多种输出格式，适用于不同类型的模型。 |
| [^147] | [Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data.](http://arxiv.org/abs/2309.09725) | 在无约束特征模型的背景下，我们研究了交叉熵损失函数下不均衡数据的神经塌缩现象。 |
| [^148] | [RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability.](http://arxiv.org/abs/2309.00082) | 本文提出了RePo算法，通过正则化后验可预测性的方式，增强了视觉模型基础强化学习方法的弹性。该方法通过学习一个对冗余和伪变化具有弹性的潜在表示，提高了方法对视觉干扰的鲁棒性，使其能够在动态环境中运行。 |
| [^149] | [CL-MAE: Curriculum-Learned Masked Autoencoders.](http://arxiv.org/abs/2308.16572) | 本文提出了一种课程学习的遮罩自编码器（CL-MAE）。我们引入了一种可学习的遮罩模块，通过更新遮罩策略来增加自监督重构任务的复杂性。通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。 |
| [^150] | [Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS).](http://arxiv.org/abs/2308.08561) | 该论文介绍了一种名为QMLS的新概念，通过结合机器学习和量子模拟的方法，可以缩短药物研发的时间和降低成本。通过生成命中物和优化分子的过程，可以大大提高药物发现的效率。 |
| [^151] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^152] | [AgentBench: Evaluating LLMs as Agents.](http://arxiv.org/abs/2308.03688) | AgentBench是一个用于评估LLMs作为代理人的多维度基准，发现在复杂环境中，商业LLMs在充当代理人方面表现强劲，但与开源竞争对手相比，存在显著性能差距。该研究揭示了LLMs在长期推理、决策和指令遵循能力上的瓶颈。 |
| [^153] | [On Single Index Models beyond Gaussian Data.](http://arxiv.org/abs/2307.15804) | 该论文研究了超越高斯数据的单指数模型，探索了对稳定性和对称性的违反情况下的样本复杂性控制。 |
| [^154] | [Efficient Estimation of the Local Robustness of Machine Learning Models.](http://arxiv.org/abs/2307.13885) | 本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。 |
| [^155] | [WebArena: A Realistic Web Environment for Building Autonomous Agents.](http://arxiv.org/abs/2307.13854) | WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。 |
| [^156] | [SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design.](http://arxiv.org/abs/2307.11694) | 本文提出了一种通过上下文学习个性化药物协同作用并进行药物设计的方法，该方法利用小型的个性化数据集，不依赖于文本语料库、分子指纹或蛋白质相互作用的领域特定知识，取得了竞争性的结果。 |
| [^157] | [A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset.](http://arxiv.org/abs/2307.10455) | 提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。 |
| [^158] | [A Vulnerability of Attribution Methods Using Pre-Softmax Scores.](http://arxiv.org/abs/2307.03305) | 这篇论文讨论了使用前softmax分数的归属方法的一个漏洞，该方法用于解释卷积神经网络分类器输出。与对抗性攻击不同，作者关注的是对归属方法进行小修改可能导致的影响，而不会改变模型的输出。 |
| [^159] | [PlanE: Representation Learning over Planar Graphs.](http://arxiv.org/abs/2307.01180) | 本研究的目标是设计用于高效学习平面图完备不变量的架构。 |
| [^160] | [Separable Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.15969) | 这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。 |
| [^161] | [Unsupervised Episode Generation for Graph Meta-learning.](http://arxiv.org/abs/2306.15217) | 本文研究了无监督的剧集生成方法，通过元学习解决没有标签的少样本节点分类问题。它们充分利用所有节点信息，并且通过泛化能力提高性能。 |
| [^162] | [DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology.](http://arxiv.org/abs/2306.13384) | DiffInfinite 是一种能够生成任意大的组织学图像，使用小补丁进行快速训练，并可以更高效地并行化的方法，在组织病理学成像实践中有效解决了大规模信息、昂贵的手动注释以及保护性数据处理等独特挑战。 |
| [^163] | [Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback.](http://arxiv.org/abs/2306.13233) | 本文提出了一种算法，在带有嘈杂贝叶斯反馈的零和矩阵博弈中，实现了对数遗憾策略。 |
| [^164] | [Learning Unseen Modality Interaction.](http://arxiv.org/abs/2306.12795) | 本文提出了一个解决多模态学习中未见过的模态组合的问题的方法。该方法利用一个模块将不同模态的特征投影到一个共享的空间中，并通过伪监督来减少过拟合。实验证明该方法在多个任务和模态上都是有效的。 |
| [^165] | [Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models.](http://arxiv.org/abs/2306.12747) | 本文研究了在超参数模型中解决随机梯度下降（SGD）和Adam的速度问题，提出了一种非单调线搜索方法，取得更快的收敛速度和泛化性能，并结合同步的Polyak初始化步伐实现。 |
| [^166] | [Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift.](http://arxiv.org/abs/2306.12306) | 该论文在多个具有挑战性的分类和回归任务上对现代BDL算法进行了系统性评估，重点关注了在分布偏移下的泛化能力和校准能力，并研究了一种带符号的期望校准误差版本。 |
| [^167] | [Universal adversarial perturbations for multiple classification tasks with quantum classifiers.](http://arxiv.org/abs/2306.11974) | 本文探讨了量子通用对抗扰动，并发现一个精心制作的通用扰动可以成功地欺骗两个不同分类任务上达到最先进准确性的量子分类器，这为构建安全的量子机器学习系统带来潜在威胁。 |
| [^168] | [Understanding Optimization of Deep Learning.](http://arxiv.org/abs/2306.09338) | 本文全面介绍了深度学习中梯度消失和梯度爆炸等优化挑战，并通过提高梯度流和对网络Lipschitz常数施加约束等措施进行了解决。显式优化和隐式优化是两种解决优化问题的不同方式。 |
| [^169] | [Phase Transitions of Civil Unrest across Countries and Time.](http://arxiv.org/abs/2306.08698) | 跨国和时间的社会动荡相变研究探索了集体社会动荡是否可以被描述为一系列具有可测量和可识别特征的循环相变，并证明了宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，以及普遍机制可能支撑着社会动荡的某些方面。 |
| [^170] | [PeFLL: A Lifelong Learning Approach to Personalized Federated Learning.](http://arxiv.org/abs/2306.05515) | PeFLL是个性化联邦学习的一种新方法，通过联合训练嵌入网络和超网络，PeFLL能够学习输出特定于每个客户端的模型，并且在其它新出现的客户端上表现良好。 |
| [^171] | [AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification.](http://arxiv.org/abs/2306.05501) | AMEE是一个模型无关的解释评价框架，用于量化和比较时间序列分类中多种基于显著性的解释方法的信息价值，帮助解决在这一领域中解释方法选择的难题。 |
| [^172] | [Interpretable Alzheimer's Disease Classification Via a Contrastive Diffusion Autoencoder.](http://arxiv.org/abs/2306.03022) | 本研究通过使用对比扩散自编码器实现了对阿尔茨海默病的可解释分类，达到了与黑盒方法相当的分类准确性，并产生人类可解释的模型解释。 |
| [^173] | [Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency.](http://arxiv.org/abs/2306.02109) | 通过自监督模型行为一致性编码时间序列解释，提供离散的归因图，并学习了一种可用于各种方式的解释的潜在空间。 |
| [^174] | [Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction.](http://arxiv.org/abs/2306.01439) | 该论文介绍了一种名为NUDGE的策略，利用训练好的基于神经网络的代理来引导逻辑规则的搜索，实现了可解释和可解释的策略。 |
| [^175] | [Direct Diffusion Bridge using Data Consistency for Inverse Problems.](http://arxiv.org/abs/2305.19809) | 本文提出了一种用于逆问题的直接扩散链桥算法，提高了逆问题求解器的性能，并通过使用数据一致性解决了当前DDB框架存在的关键限制。 |
| [^176] | [Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming.](http://arxiv.org/abs/2305.19706) | 本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。 |
| [^177] | [What Can We Learn from Unlearnable Datasets?.](http://arxiv.org/abs/2305.19254) | 无法学习的数据集方法具有保护数据隐私的潜力，但实际使用受到限制。我们发现神经网络在无法学习的数据集上可以学习到有用的特征，而不仅仅是简单规则，这对图像保护的效果不确定。此外，线性可分的扰动并不是诱导学习捷径的必要条件，因此不能依赖它们。 |
| [^178] | [One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration.](http://arxiv.org/abs/2305.18258) | 提出一种在线强化学习方法Maximize to Explore (MEX)，只需优化一个无约束的目标函数，自动平衡探索和利用，实现次线性遗憾。 |
| [^179] | [Hardware-Efficient Transformer Training via Piecewise Affine Operations.](http://arxiv.org/abs/2305.17190) | 本文提出一种使用分段仿射操作代替传统乘法的高效Transformer训练方法，不需要更改训练超参数即可在视觉和语言任务中成功实现训练，同时消除了整个训练过程中的所有乘法操作。 |
| [^180] | [Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation.](http://arxiv.org/abs/2305.16985) | 本文研究了在多任务模仿学习中的逆动力学预训练方法，通过预训练学习高维观察空间的低维表示，并将其转移到微调数据集进行任务执行。 |
| [^181] | [Most Neural Networks Are Almost Learnable.](http://arxiv.org/abs/2305.16508) | 本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。 |
| [^182] | [Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks.](http://arxiv.org/abs/2305.16475) | 本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。 |
| [^183] | [Optimal Rates for Bandit Nonstochastic Control.](http://arxiv.org/abs/2305.15352) | 本研究解决了带有拟对抗扰动和时变对抗性赌博损失函数的LQR和LQG问题，并提供了一种新颖的具有记忆的赌博凸优化方案。 |
| [^184] | [From Tempered to Benign Overfitting in ReLU Neural Networks.](http://arxiv.org/abs/2305.15141) | 本论文通过对二层ReLU神经网络进行研究，证明了各种假设下过拟合的类型会从一维数据的极端情况下缓和到高维的良性，揭示了输入维度在神经网络过拟合中的关键作用。 |
| [^185] | [An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning.](http://arxiv.org/abs/2305.15016) | 这个论文提出了一种无监督方法，通过利用数据流形的拓扑特征估计数据的类别可分性，该方法与有监督度量具有相关性，可以用于半监督和感知学习。同时，在语言模型微调中应用该方法用于自动停止准则。 |
| [^186] | [S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions.](http://arxiv.org/abs/2305.14095) | S-CLIP是一种利用少量专业字幕进行半监督视觉语言学习的方法，通过两种伪标签策略提高了模型在专业领域的适应性。 |
| [^187] | [TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks.](http://arxiv.org/abs/2305.11430) | 本文提出了一个通用分类法，可以用来设计具有特定属性的提示来执行各种复杂任务，从而解决了LLM在执行复杂任务方面的性能变异巨大的问题。 |
| [^188] | [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining.](http://arxiv.org/abs/2305.10429) | DoReMi方法使用分组分布式鲁棒优化训练小型代理模型以产生域权重，再使用这些权重重新采样数据集训练大型模型，相比使用默认权重的基线模型，在The Pile和GLaM数据集上平均提高了6.5%和4.7%的few-shot下游准确度，分别使用2.6倍和相同的训练步骤达到基线准确度。 |
| [^189] | [StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure.](http://arxiv.org/abs/2305.05588) | 本文开发了StrAE框架，该框架利用句子结构无监督地学习多级节点嵌入，并且发现使用显式结构可以提高嵌入表现，新的对比目标优于标准的交叉熵目标。同时，完全忠实于结构确实能够根据相应模型的性能消除结构类型之间的歧义。 |
| [^190] | [TAPS: Connecting Certified and Adversarial Training.](http://arxiv.org/abs/2305.04574) | TAPS是一种结合了IBP和PGD训练的认证训练方法，通过产生精确但不一定正确的最坏情况损失逼近，从而减少过度正则化，并在许多设置中实现了最先进的认证精度。 |
| [^191] | [Performative Prediction with Bandit Feedback: Learning through Reparameterization.](http://arxiv.org/abs/2305.01094) | 本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。 |
| [^192] | [Leveraging the two timescale regime to demonstrate convergence of neural networks.](http://arxiv.org/abs/2304.09576) | 研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。 |
| [^193] | [Sheaf Neural Networks for Graph-based Recommender Systems.](http://arxiv.org/abs/2304.09097) | 基于Sheaf神经网络的模型提出了一种新的向量空间表示方法，使得其在基准推荐任务上获得最先进的性能表现。 |
| [^194] | [Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models.](http://arxiv.org/abs/2304.03271) | 本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。 |
| [^195] | [Saddle-to-Saddle Dynamics in Diagonal Linear Networks.](http://arxiv.org/abs/2304.00488) | 本文研究了对角线性网络中的鞍点到鞍点动力学，并展示了在初始化趋近于零的情况下，极限流如何从一个训练损失的鞍点跳到另一个鞍点，直到达到最小的$\ell_1$-范数解。通过递归算法和弧长时间重新参数化，我们明确刻画了访问过的鞍点和跳跃时间。我们的分析适用于欠参数化和过参数化的情况，也涵盖了活动数量单调性问题的复杂情况。 |
| [^196] | [Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE.](http://arxiv.org/abs/2304.00083) | 本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。 |
| [^197] | [Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness.](http://arxiv.org/abs/2303.09863) | 本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。 |
| [^198] | [On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level.](http://arxiv.org/abs/2303.03944) | 本文研究了一类非凸双层优化问题，并提出了一种基于动量的梯度双层方法(MGBiO)来解决这些确定性问题，同时提出了一类基于动量的随机梯度双层方法(MSGBiO和VR-MSGBiO)来解决这些随机问题。通过收敛分析，证明了MGBiO方法具有收敛性。 |
| [^199] | [Goal Driven Discovery of Distributional Differences via Language Descriptions.](http://arxiv.org/abs/2302.14233) | 本论文提出了一个新的任务D5，通过目标驱动的方式自动发现两个大型语料库之间的差异。作者构建了一个D5系统，并提出了一套统一的评估指标来衡量其性能。通过实验证明，语言模型可以使用目标驱动的方法来发现语料库差异。 |
| [^200] | [Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts.](http://arxiv.org/abs/2302.13875) | 该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。 |
| [^201] | [(S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability.](http://arxiv.org/abs/2302.08982) | 本文研究了在对角线线性网络上，随机性和大步长对梯度下降(GD)和随机梯度下降(SGD)的隐式正则化的影响。实验结果表明大步长对稀疏回归问题中的SGD有益处，但对GD可能有害。这种影响在接近发散阈值的紧密步长下被放大。 |
| [^202] | [ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval.](http://arxiv.org/abs/2302.02285) | ReDi是一种高效的无学习扩散推断方法，通过轨迹检索来加速推断过程，实验证明了它在提高模型推断效率方面具有2倍的加速，并且能够在零样本跨域图像生成中泛化良好。 |
| [^203] | [Power Laws for Hyperparameter Optimization.](http://arxiv.org/abs/2302.00441) | 该论文提出了一种基于幂律规律的Deep Power Laws（DPL）方法来解决超参数优化问题，并在表格、图像和NLP数据集上展示了最佳结果。 |
| [^204] | [Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?.](http://arxiv.org/abs/2301.12609) | 知识蒸馏和标签平滑被认为是等价的方法，但实验证明它们对模型置信度的影响方向完全相反。知识蒸馏不仅传递知识，还传递了自信心。 |
| [^205] | [Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence.](http://arxiv.org/abs/2301.11476) | 这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。 |
| [^206] | [AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation.](http://arxiv.org/abs/2301.08110) | AtMan是一种通过在生成式Transformer模型中操纵注意力机制来解释预测的方法，相较于传统方法几乎不占用额外内存，可在生产环境中使用。 |
| [^207] | [Myths and Legends in High-Performance Computing.](http://arxiv.org/abs/2301.02432) | 这篇文章讨论了高性能计算社区中流传的神话和传说，这些神话往往不基于科学事实，而是基于一些证据或论证。虽然有些问题仍然是无休止的哲学辩论，但新的方向正在出现，如算法的规模化或新的架构研究。 |
| [^208] | [Numerical Stability of DeepGOPlus Inference.](http://arxiv.org/abs/2212.06361) | 这篇论文研究了用于预测蛋白质功能的 CNN DeepGOPlus 在推理过程中的数值不确定性和数值稳定性，并研究了使用降低精度浮点格式进行推理的可能性。 |
| [^209] | [Variable Decision-Frequency Option Critic.](http://arxiv.org/abs/2212.04407) | 这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。 |
| [^210] | [Welfare and Fairness in Multi-objective Reinforcement Learning.](http://arxiv.org/abs/2212.01382) | 本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。 |
| [^211] | [Data-Driven Network Neuroscience: On Data Collection and Benchmark.](http://arxiv.org/abs/2211.12421) | 本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。 |
| [^212] | [A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation.](http://arxiv.org/abs/2210.12089) | 这篇综述研究了图形反事实解释的概念、方法、评估及其应用于图神经网络的情况，提供了分类法、统一的符号表示、基准数据集和评估指标，并对十四种方法、二十二个数据集和十九个指标进行了讨论和整合。未来的工作主要集中在解决开放的挑战上。 |
| [^213] | [Regularized Data Programming with Automated Bayesian Prior Selection.](http://arxiv.org/abs/2210.08677) | 本文介绍了一种自动贝叶斯先验选择的正则化数据编程方法，通过加入正则化项来提升数据编程在弱监督学习中的性能，在低数据情境中表现出更好的性能，并提供更大的可解释性。 |
| [^214] | [Improving Deep Learning Models for Pediatric Low-Grade Glioma Tumors Molecular Subtype Identification Using 3D Probability Distributions of Tumor Location.](http://arxiv.org/abs/2210.07287) | 本研究提出使用肿瘤位置概率来改进深度学习模型，以提高儿童低级别胶质瘤分子亚型鉴定的准确性。 |
| [^215] | [Label-free segmentation from cardiac ultrasound using self-supervised learning.](http://arxiv.org/abs/2210.04979) | 本研究提出了一种无需手动标注的自监督学习流程，在心脏超声图像分割中取得了可靠的结果，与监督学习方法相比具有相似的测量准确度，并且能够准确检测异常心腔大小和功能。 |
| [^216] | [GLM-130B: An Open Bilingual Pre-trained Model.](http://arxiv.org/abs/2210.02414) | GLM-130B是一个具有1300亿参数的开源双语预训练模型，能够超越GPT-3和最大的中文语言模型ERNIE TITAN 3.0 260B，在多个基准测试中表现出色。 |
| [^217] | [Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty.](http://arxiv.org/abs/2209.15543) | 本论文介绍了将贝叶斯神经网络应用于地热资源评估的方法，利用监督学习问题和已知特征图进行预测，并可用于找出潜力更高的区域进行进一步的调查。 |
| [^218] | [Learning Bilinear Models of Actuated Koopman Generators from Partially-Observed Trajectories.](http://arxiv.org/abs/2209.09977) | 该论文提出了一种从部分观测轨迹中学习作用Koopman生成器的方法，克服了现有方法依赖于基函数选择和观测不完整的限制。 |
| [^219] | [Lottery Aware Sparsity Hunting: Enabling Federated Learning on Resource-Limited Edge.](http://arxiv.org/abs/2208.13092) | 本论文提出了一种名为FLASH的联邦学习框架，该框架通过抽奖感知的稀疏度搜索，在资源有限的边缘环境中训练稀疏子模型，从而保持性能并获得相应的通信优势。 |
| [^220] | [Implicit Two-Tower Policies.](http://arxiv.org/abs/2208.01191) | 隐式双塔策略（ITT）是一种新的结构化强化学习策略体系，通过在策略堆栈中显式区分动作和状态处理，实现了显著的计算效益和更好的性能，在黑盒/进化优化方面表现出色。 |
| [^221] | [Open-radiomics: A Collection of Standardized Datasets and a Technical Protocol for Reproducible Radiomics Machine Learning Pipelines.](http://arxiv.org/abs/2207.14776) | 本研究提出了一套开放放射组学数据集和技术协议，旨在解决放射组学在结果可重复性和可访问性方面所面临的挑战。通过在BraTS 2020数据集上进行实验，研究了放射组学特征提取对结果可重复性的影响。 |
| [^222] | [Unsupervised energy disaggregation via convolutional sparse coding.](http://arxiv.org/abs/2207.09785) | 本文提出了一种无监督能量分解方法，通过卷积稀疏编码将功耗分类为有功或者无功，以实现对私人住宅居民活动和存在情况的报告。实验结果表明该方法在能量泛函最小化和收敛性等方面具有可行性。 |
| [^223] | ["Understanding Robustness Lottery": A Geometric Visual Comparative Analysis of Neural Network Pruning Approaches.](http://arxiv.org/abs/2206.07918) | 本研究通过几何可视化分析，比较了不同神经网络剪枝方法对网络内部特征表示的影响，并对模型性能进行了评估。 |
| [^224] | [Impartial Games: A Challenge for Reinforcement Learning.](http://arxiv.org/abs/2205.12787) | AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations. |
| [^225] | [Feature Extractor Stacking for Cross-domain Few-shot Meta-learning.](http://arxiv.org/abs/2205.05831) | 这篇论文提出了一种新的跨领域少样本元学习方法，称为特征提取器叠加(FES)。FES通过叠加多个主干的信息，可以利用异构预训练的主干，而且不需要维护一个需要重新计算的通用模型。 |
| [^226] | [Limitations of Deep Learning for Inverse Problems on Digital Hardware.](http://arxiv.org/abs/2202.13490) | 本文研究了深度学习在数字硬件上解决反问题的限制，并证明了对于小的松弛参数，有限维反问题无法通过计算方法解决。这些结果还引入了算法可获得准确度的下限。 |
| [^227] | [Forward Composition Propagation for Explainable Neural Reasoning.](http://arxiv.org/abs/2112.12717) | 本文提出了一种被称为前向组合传播（FCP）的算法，用于解释前馈神经网络在结构化分类问题上的预测。该算法通过组合向量描述每个神经元中问题特征的作用，并且模拟结果表明组合值与受保护特征的预期行为紧密对齐。 |
| [^228] | [Bolstering Stochastic Gradient Descent with Model Building.](http://arxiv.org/abs/2111.07058) | 用基于模型构建的方法增强了随机梯度下降算法，适应性地调整步长和搜索方向，提高了收敛速度。 |
| [^229] | [Is Attention always needed? A Case Study on Language Identification from Speech.](http://arxiv.org/abs/2110.03427) | 本研究提出了一种基于卷积循环神经网络的语言识别方法，使用梅尔频率倒谱系数特征。与现有的方法进行了比较分析，并得出了一些结论。 |
| [^230] | [How do I update my model? On the resilience of Predictive Process Monitoring models to change.](http://arxiv.org/abs/2109.03501) | 本研究针对预测过程监控模型的韧性问题，评估了三种不同策略，并发现增量学习算法具有潜力来解决预测模型的更新和可变性问题。 |
| [^231] | [Revisiting Deep Learning Models for Tabular Data.](http://arxiv.org/abs/2106.11959) | 本论文重新审视了用于表格数据的深度学习模型，提出了两种简单且强大的深度学习架构作为性能基准，包括类似于ResNet的架构和适应于表格数据的Transformer架构。这些基准模型在不同问题上表现出有竞争力的性能。 |
| [^232] | [GPT Understands, Too.](http://arxiv.org/abs/2103.10385) | 提出了一种方法P-Tuning，通过使用可学习的连续提示嵌入和离散提示的拼接，稳定了预训练语言模型（PLM）的训练过程，并在多种自然语言理解任务上显著提高了性能。 |
| [^233] | [Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning.](http://arxiv.org/abs/2006.16785) | 离线生成对抗模仿学习中，将学习到的奖励函数强制变成局部利普希茨连续是取得良好表现的必要条件，并且满足奖励的利普希茨性约束对模仿性能具有积极影响。 |
| [^234] | [Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization.](http://arxiv.org/abs/2006.16205) | 本论文提出了一种组合微调方法，通过冻结预训练的降噪自编码器来保留输出结构，从而显著降低预测器的复杂性并提高泛化性能。 |
| [^235] | [A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM).](http://arxiv.org/abs/1912.13490) | 这个论文提出了一个神经计算框架下的意识理论，称为“目标对齐的内部表示操作”（GARIM）。该理论认为意识支持对目标相关的内部表示进行主动操作，使其与追求的目标更加对齐，从而增加目标导向行为的灵活性。 |

# 详细

[^1]: 无人机在多智能体强化学习中的动态避障路径规划

    UAV Pathfinding in Dynamic Obstacle Avoidance with Multi-agent Reinforcement Learning. (arXiv:2310.16659v1 [cs.RO])

    [http://arxiv.org/abs/2310.16659](http://arxiv.org/abs/2310.16659)

    本文提出了一种基于多智能体强化学习的集中式训练和分布式执行方法，以在线解决动态障碍物避障问题。实验结果验证了该方法的有效性。

    

    多智能体强化学习方法在动态和不确定的场景中在线规划智能体的可行且安全的路径具有重要意义。本文提出了一种基于多智能体强化学习的集中式训练和分布式执行方法，以在线解决动态障碍物避障问题。在该方法中，每个智能体仅与中央规划者或其邻居进行通信，以在线规划可行且安全的路径。我们基于模型预测控制的思想改进了我们的方法，以提高智能体的训练效率和采样利用率。在模拟、室内和室外环境中的实验结果验证了我们方法的有效性。

    Multi-agent reinforcement learning based methods are significant for online planning of feasible and safe paths for agents in dynamic and uncertain scenarios. Although some methods like fully centralized and fully decentralized methods achieve a certain measure of success, they also encounter problems such as dimension explosion and poor convergence, respectively. In this paper, we propose a novel centralized training with decentralized execution method based on multi-agent reinforcement learning to solve the dynamic obstacle avoidance problem online. In this approach, each agent communicates only with the central planner or only with its neighbors, respectively, to plan feasible and safe paths online. We improve our methods based on the idea of model predictive control to increase the training efficiency and sample utilization of agents. The experimental results in both simulation, indoor, and outdoor environments validate the effectiveness of our method. The video is available at htt
    
[^2]: 一张图片胜过千言万语：原则性重写改善图像生成

    A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])

    [http://arxiv.org/abs/2310.16656](http://arxiv.org/abs/2310.16656)

    本文提出了一种原则性重写方法来改善图像生成模型的效果，通过使用专门的自动字幕模型重新标注语料库，并在重写后的数据集上训练文本到图像模型，模型在整体图像质量方面得到了显著改善。

    

    文本到图像扩散模型在过去几年中取得了显著的突破，使得从文本提示中高质量且多样化地合成图像成为可能。然而，即使是最先进的模型也常常难以准确地遵循其提示中的所有指令。这些模型中绝大部分是在由（图像，字幕）对组成的数据集上进行训练的，其中图像通常来自网络，而字幕则是它们的HTML替代文本。一个显著的例子是LAION数据集，被Stable Diffusion和其他模型使用。在这项工作中，我们观察到这些字幕通常质量较低，并认为这显著影响了模型理解文本提示中微妙语义的能力。我们展示通过使用专门的自动字幕模型重新标注语料库，并在重写后的数据集上训练文本到图像模型，模型在各个方面都会得到大幅度的改善。首先，在整体图像质量方面：例如FID 14.84 vs. t

    Text-to-image diffusion models achieved a remarkable leap in capabilities over the last few years, enabling high-quality and diverse synthesis of images from a textual prompt. However, even the most advanced models often struggle to precisely follow all of the directions in their prompts. The vast majority of these models are trained on datasets consisting of (image, caption) pairs where the images often come from the web, and the captions are their HTML alternate text. A notable example is the LAION dataset, used by Stable Diffusion and other models. In this work we observe that these captions are often of low quality, and argue that this significantly affects the model's capability to understand nuanced semantics in the textual prompts. We show that by relabeling the corpus with a specialized automatic captioning model and training a text-to-image model on the recaptioned dataset, the model benefits substantially across the board. First, in overall image quality: e.g. FID 14.84 vs. t
    
[^3]: 从图像中实现强化学习的控制中心表示的研究

    Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])

    [http://arxiv.org/abs/2310.16655](http://arxiv.org/abs/2310.16655)

    该论文提出了一个名为ReBis的方法，通过整合无奖励控制信息和奖励特定知识，来捕捉图像中的控制中心信息。ReBis利用变形器架构来建模动力学，并通过分块掩码消除时空冗余。此外，ReBis还结合了等仿函式损失和非对称重建损失，以防止在稀疏奖励环境中的特征崩溃。

    

    基于图像的强化学习是一项实际但具有挑战性的任务。其中一个主要障碍在于提取控制中心的表示，同时忽略不相关的信息。虽然遵循等仿函式原则的方法展示了学习状态表示来解决这个问题的潜力，但它们仍然面临着潜在动力学的有限表达能力和适应稀疏奖励环境的困难。为了解决这些限制，我们引入了ReBis，旨在通过将免奖励控制信息与奖励特定知识集成来捕捉控制中心信息。ReBis利用变形器架构隐式建模动力学，并结合分块掩码消除时空冗余。此外，ReBis将等仿函式损失与非对称重建损失相结合，以防止在稀疏奖励环境中的特征崩溃。在Atari游戏和...

    Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and D
    
[^4]: Federated Learning在通信错误方面的鲁棒性有多强？来自上行和下行通道的比较研究。

    How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels. (arXiv:2310.16652v1 [cs.LG])

    [http://arxiv.org/abs/2310.16652](http://arxiv.org/abs/2310.16652)

    本文研究了联邦学习对上行和下行通信错误的鲁棒性。理论分析表明，鲁棒性取决于客户端数量和模型参数的数值范围。实验证明上行通信可以容忍更高的误码率比下行通信。

    

    由于其保护隐私的能力，联邦学习（FL）在学术界和工业界引起了广泛关注。然而，当在无线网络上实现时，FL能够容忍多少通信错误尚不清楚。本文研究了FL对上行和下行通信错误的鲁棒性。我们的理论分析揭示了鲁棒性取决于两个关键参数，即客户端数量和模型参数的数值范围。同时，我们证明了FL的上行通信可以容忍更高的误码率（BER）比下行通信，并提出了一个定量的差异公式。通过大量实验进一步验证了这些发现和理论分析。

    Because of its privacy-preserving capability, federated learning (FL) has attracted significant attention from both academia and industry. However, when being implemented over wireless networks, it is not clear how much communication error can be tolerated by FL. This paper investigates the robustness of FL to the uplink and downlink communication error. Our theoretical analysis reveals that the robustness depends on two critical parameters, namely the number of clients and the numerical range of model parameters. It is also shown that the uplink communication in FL can tolerate a higher bit error rate (BER) than downlink communication, and this difference is quantified by a proposed formula. The findings and theoretical analyses are further validated by extensive experiments.
    
[^5]: 变分自动编码器中缺失数据的后验一致性

    Posterior Consistency for Missing Data in Variational Autoencoders. (arXiv:2310.16648v1 [cs.LG])

    [http://arxiv.org/abs/2310.16648](http://arxiv.org/abs/2310.16648)

    本论文研究了在包含缺失数据的情况下，从数据中学习变分自动编码器的问题。通过正则化编码器的后验分布，提出了一种改进后验一致性的方法，并在实验证明该方法在缺失值设置下可以提高重建质量和性能。

    

    我们考虑从包含缺失值的数据中学习变分自动编码器（VAE），即一种深度生成模型的问题。由于完整数据通常无法获得或成本太高，这种数据在机器学习的现实应用中普遍存在。我们特别关注改进VAE的摊销后验推断，即在缺失数据情况下可以学习到不一致后验分布的编码器。为此，我们提供了后验一致性的正式定义，并提出一种用于正则化编码器后验分布以促进一致性的方法。我们观察到，所提出的正则化方法在面对缺失值时建议了与文献中通常考虑的训练目标不同的方法。此外，我们在重构质量和d方面的实验证明，我们的正则化方法提高了缺失值设置下的性能。

    We consider the problem of learning Variational Autoencoders (VAEs), i.e., a type of deep generative model, from data with missing values. Such data is omnipresent in real-world applications of machine learning because complete data is often impossible or too costly to obtain. We particularly focus on improving a VAE's amortized posterior inference, i.e., the encoder, which in the case of missing data can be susceptible to learning inconsistent posterior distributions regarding the missingness. To this end, we provide a formal definition of posterior consistency and propose an approach for regularizing an encoder's posterior distribution which promotes this consistency. We observe that the proposed regularization suggests a different training objective than that typically considered in the literature when facing missing values. Furthermore, we empirically demonstrate that our regularization leads to improved performance in missing value settings in terms of reconstruction quality and d
    
[^6]: 实现神经网络中的约束：一种随机增广拉格朗日方法

    Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach. (arXiv:2310.16647v1 [cs.LG])

    [http://arxiv.org/abs/2310.16647](http://arxiv.org/abs/2310.16647)

    本文提出了一种使用随机增广拉格朗日方法实现更灵活和高效的神经网络正则化机制的方法，在黑盒和白盒模型上都取得了显著改进。

    

    正则化深度神经网络（DNNs）对于改善泛化能力和防止过拟合至关重要。固定惩罚方法虽然常见，但缺乏适应性并且对超参数敏感。在本文中，我们提出了一种新颖的DNN正则化方法，将训练过程构建为一个约束优化问题。其中数据保真度项是最小化目标，正则化项作为约束。然后，我们采用随机增广拉格朗日（SAL）方法实现更加灵活和高效的正则化机制。我们的方法不仅适用于黑盒正则化，还在白盒模型中展现了显著的提升，其中权重常常受到硬约束以确保可解释性。在MNIST、CIFAR10和CIFAR100数据集上的图像分类实验结果验证了我们方法的有效性。SAL始终能够获得更高的准确度同时实现更好的约束。

    Regularizing Deep Neural Networks (DNNs) is essential for improving generalizability and preventing overfitting. Fixed penalty methods, though common, lack adaptability and suffer from hyperparameter sensitivity. In this paper, we propose a novel approach to DNN regularization by framing the training process as a constrained optimization problem. Where the data fidelity term is the minimization objective and the regularization terms serve as constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method to achieve a more flexible and efficient regularization mechanism. Our approach extends beyond black-box regularization, demonstrating significant improvements in white-box models, where weights are often subject to hard constraints to ensure interpretability. Experimental results on image-based classification on MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our approach. SAL consistently achieves higher Accuracy while also achieving better constrain
    
[^7]: 基于模型预测控制的值估计用于高效强化学习

    Model predictive control-based value estimation for efficient reinforcement learning. (arXiv:2310.16646v1 [cs.LG])

    [http://arxiv.org/abs/2310.16646](http://arxiv.org/abs/2310.16646)

    该论文提出了一种基于模型预测控制的强化学习方法，通过对环境进行数据驱动建模，并进行多步预测以估计值函数并优化策略，展示出更高的学习效率和更快的收敛速度，同时减少了样本容量需求。

    

    强化学习在实践中存在着与虚拟环境进行交互的次数所带来的局限性。为了解决这个问题，我们设计了一种改进的基于模型预测控制的强化学习方法，通过数据驱动的方式对环境进行建模。基于学习到的环境模型，该方法进行多步预测以估计值函数并优化策略。该方法表现出更高的学习效率，更快的收敛速度以接近最优值，并且在经验回放缓冲区中需要更少的样本容量。在经典数据库和无人机动态避障场景中的实验结果验证了所提出方法的有效性。

    Reinforcement learning suffers from limitations in real practices primarily due to the numbers of required interactions with virtual environments. It results in a challenging problem that we are implausible to obtain an optimal strategy only with a few attempts for many learning method. Hereby, we design an improved reinforcement learning method based on model predictive control that models the environment through a data-driven approach. Based on learned environmental model, it performs multi-step prediction to estimate the value function and optimize the policy. The method demonstrates higher learning efficiency, faster convergent speed of strategies tending to the optimal value, and fewer sample capacity space required by experience replay buffers. Experimental results, both in classic databases and in a dynamic obstacle avoidance scenario for unmanned aerial vehicle, validate the proposed approaches.
    
[^8]: 驾驶通过概念阻塞：解开可解释性瓶颈

    Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])

    [http://arxiv.org/abs/2310.16639](http://arxiv.org/abs/2310.16639)

    本论文提出了一种使用概念阻塞作为控制命令预测和用户车辆行为解释的方法，通过学习人类可理解的概念层解释顺序驾驶场景，同时获得竞争性性能和可解释性。

    

    在可解释的机器学习中，概念阻塞模型通过利用一组人为定义的概念在模型中编码信息，取得了成功。在人类辅助或自动驾驶的背景下，可解释性模型可以帮助用户接受和理解自动驾驶车辆所做的决策，并用于合理化和解释驾驶员或车辆的行为。我们提出了一种新的方法，使用概念阻塞作为控制命令预测和用户车辆行为解释的可视特征。我们学习了一个人类可理解的概念层，用于解释顺序驾驶场景同时学习车辆的控制命令。这种方法可以用来确定人类（或自动驾驶车辆）对首选缝隙或转向命令的改变是否由外部刺激或偏好的改变所引导。在我们的模型设置中，我们获得了具有竞争性的性能，同时获得了可解释性。

    Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup
    
[^9]: 适应密度比估计的协变量偏移适应

    Covariate Shift Adaptation Robust to Density-Ratio Estimation. (arXiv:2310.16638v1 [stat.ME])

    [http://arxiv.org/abs/2310.16638](http://arxiv.org/abs/2310.16638)

    该论文研究了在协变量偏移下的密度比估计的罕见问题，提出了一种适应性方法来减轻密度比估计的偏差对模型的影响。

    

    在一种情况下，我们可以访问具有协变量和结果的训练数据，而测试数据只包含协变量。在这种情况下，我们的主要目标是预测测试数据中缺失的结果。为了实现这个目标，我们在协变量偏移下训练参数回归模型，其中训练数据和测试数据之间的协变量分布不同。对于这个问题，现有研究提出了通过使用密度比的重要性加权来进行协变量偏移适应的方法。该方法通过对训练数据损失进行加权平均，每个权重是训练数据和测试数据之间的协变量密度比的估计，以近似测试数据的风险。尽管它允许我们获得一个最小化测试数据风险的模型，但其性能严重依赖于密度比估计的准确性。此外，即使密度比可以一致地估计，密度比的估计误差也会导致回归模型的估计器产生偏差。

    Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's 
    
[^10]: 基于Copula熵的光度红移研究

    Photometric Redshifts with Copula Entropy. (arXiv:2310.16633v1 [cs.LG])

    [http://arxiv.org/abs/2310.16633](http://arxiv.org/abs/2310.16633)

    本论文提出了基于Copula熵的光度红移研究方法，在SDSS类星体数据上验证了该方法的有效性。实验结果表明，通过使用具有高Copula熵的测量结果，可以显著提高光度红移的准确性，尤其是对于高红移样本。

    

    本文提出将Copula熵（CE）应用于光度红移研究。CE用于测量光度测量和红移之间的相关性，并选择与高CE相关的测量来预测红移。我们在SDSS类星体数据上验证了提出的方法。实验结果显示，与实验中使用了所有测量结果相比，选择使用CE的测量结果显著提高了光度红移的准确性，特别是对于高红移样本。使用CE选择的测量结果包括亮度大小，紫外波段亮度的标准差以及其他四个波段的亮度。由于CE是一个严格定义的数学概念，因此所得到的模型是可解释的。

    In this paper we propose to apply copula entropy (CE) to photometric redshifts. CE is used to measure the correlations between photometric measurements and redshifts and then the measurements associated with high CEs are selected for predicting redshifts. We verified the proposed method on the SDSS quasar data. Experimental results show that the accuracy of photometric redshifts is improved with the selected measurements compared to the results with all the measurements used in the experiments, especially for the samples with high redshifts. The measurements selected with CE include luminosity magnitude, the brightness in ultraviolet band with standard deviation, and the brightness of the other four bands. Since CE is a rigorously defined mathematical concept, the models such derived is interpretable.
    
[^11]: 自由形式流动：使任何架构成为归一化流

    Free-form Flows: Make Any Architecture a Normalizing Flow. (arXiv:2310.16624v1 [cs.LG])

    [http://arxiv.org/abs/2310.16624](http://arxiv.org/abs/2310.16624)

    本文提出了一种训练过程，通过使用变量转换公式梯度的高效估计器，克服了归一化流设计在解析逆变换方面的限制。这使得任何保持维度的神经网络都可以作为生成模型进行最大似然训练，并在分子生成和反问题基准测试中取得优秀的结果。

    

    归一化流是直接最大化可能性的生成模型。以前，归一化流的设计在很大程度上受到对解析逆变换的需要限制。通过使用对变量转换公式的梯度的高效估计器进行训练，我们克服了这个限制。这使得任何保持维度的神经网络都可以通过最大似然训练作为生成模型。我们的方法允许将重点放在精确调整归纳偏见以适应手头的任务上。具体而言，我们在分子生成基准测试中利用$E(n)$-等变网络取得了出色的结果。此外，我们的方法在一个反问题基准测试中也具有竞争力，同时采用现成的ResNet架构。

    Normalizing Flows are generative models that directly maximize the likelihood. Previously, the design of normalizing flows was largely constrained by the need for analytical invertibility. We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula. This enables any dimension-preserving neural network to serve as a generative model through maximum likelihood training. Our approach allows placing the emphasis on tailoring inductive biases precisely to the task at hand. Specifically, we achieve excellent results in molecule generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our method is competitive in an inverse problem benchmark, while employing off-the-shelf ResNet architectures.
    
[^12]: SpikingJelly：一个用于基于脉冲的智能的开源机器学习基础设施平台

    SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence. (arXiv:2310.16620v1 [cs.NE])

    [http://arxiv.org/abs/2310.16620](http://arxiv.org/abs/2310.16620)

    SpikingJelly是一个开源的机器学习基础设施平台，提供了丰富的工具和功能，用于处理神经形态数据集、构建深度脉冲神经网络（SNNs）、优化参数并在神经形态芯片上部署SNNs。相比现有方法，SpikingJelly的训练速度提高了11倍，并通过多级继承和半自动代码生成提供了卓越的可扩展性和灵活性。

    

    脉冲神经网络（SNNs）旨在通过引入神经动力学和脉冲特性，实现神经形态芯片上的类脑智能，具有高能效性。随着新兴的脉冲深度学习范式引起越来越多的关注，传统的编程框架无法满足自动微分、并行计算加速和处理神经形态数据集以及部署的要求。在本研究中，我们提出了SpikingJelly框架来解决上述困境。我们为神经形态数据集的预处理、构建深度SNNs、优化参数以及在神经形态芯片上部署SNNs贡献了一个全栈工具包。与现有方法相比，深度SNNs的训练可以加速11倍，并且SpikingJelly的卓越可扩展性和灵活性使用户能够通过多级继承和半自动代码生成以低成本加速定制模型。SpikingJelly展示了其在多任务上的优越性。

    Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of the automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for pre-processing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated $11\times$, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly pav
    
[^13]: 表演性预测：过去与未来

    Performative Prediction: Past and Future. (arXiv:2310.16608v1 [cs.LG])

    [http://arxiv.org/abs/2310.16608](http://arxiv.org/abs/2310.16608)

    表演性预测是机器学习中一个新兴领域，通过定义和研究预测对目标的影响，提供了对于分布变化和优化挑战的解决方法。

    

    在社会世界中，预测通常会影响预测的目标，这一现象被称为表演性。自我实现和自我否定的预测是表演性的例子。对经济学、金融学和社会科学至关重要的概念在机器学习的发展中一直缺失。在机器学习应用中，表演性通常表现为分布变化。例如，在数字平台上部署的预测模型会影响消费，从而改变数据生成的分布。我们调查了最近成立的表演性预测领域，该领域提供了一个定义和概念框架来研究机器学习中的表演性。表演性预测的一个结果是自然均衡概念的产生，从而产生了新的优化挑战。另一个结果是学习和操控之间的区别，这是表演性预测中的两种机制。

    Predictions in the social world generally influence the target of prediction, a phenomenon known as performativity. Self-fulfilling and self-negating predictions are examples of performativity. Of fundamental importance to economics, finance, and the social sciences, the notion has been absent from the development of machine learning. In machine learning applications, performativity often surfaces as distribution shift. A predictive model deployed on a digital platform, for example, influences consumption and thereby changes the data-generating distribution. We survey the recently founded area of performative prediction that provides a definition and conceptual framework to study performativity in machine learning. A consequence of performative prediction is a natural equilibrium notion that gives rise to new optimization challenges. Another consequence is a distinction between learning and steering, two mechanisms at play in performative prediction. The notion of steering is in turn i
    
[^14]: AirFL-Mem: 通过长期记忆改善通信和学习的权衡

    AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term Memory. (arXiv:2310.16606v1 [cs.IT])

    [http://arxiv.org/abs/2310.16606](http://arxiv.org/abs/2310.16606)

    AirFL-Mem通过长期记忆机制解决了联邦学习中的通信瓶颈问题，并具有与理想通信相同的收敛速度。

    

    在联邦学习中存在的通信瓶颈问题，我们提出了一种新的方案——AirFL-Mem，通过实现一种“长期”记忆机制来缓解深度衰落的影响。我们提供了收敛界限，考虑了长期记忆和现有具有短期记忆的AirFL变种的情况，适用于一般的非凸目标。理论证明了AirFL-Mem表现出与理想通信的联邦平均（FedAvg）相同的收敛速度，而现有方案的性能通常受到错误下限的限制。利用理论结果，我们还提出了一种新的凸优化策略，用于在雷利衰落信道中进行功率控制的截断阈值。实验结果证实了分析的优势，验证了长期记忆机制的优势。

    Addressing the communication bottleneck inherent in federated learning (FL), over-the-air FL (AirFL) has emerged as a promising solution, which is, however, hampered by deep fading conditions. In this paper, we propose AirFL-Mem, a novel scheme designed to mitigate the impact of deep fading by implementing a \emph{long-term} memory mechanism. Convergence bounds are provided that account for long-term memory, as well as for existing AirFL variants with short-term memory, for general non-convex objectives. The theory demonstrates that AirFL-Mem exhibits the same convergence rate of federated averaging (FedAvg) with ideal communication, while the performance of existing schemes is generally limited by error floors. The theoretical results are also leveraged to propose a novel convex optimization strategy for the truncation threshold used for power control in the presence of Rayleigh fading channels. Experimental results validate the analysis, confirming the advantages of a long-term memor
    
[^15]: 最后一公里交付中的包裹丢失预测：深度学习和非深度学习方法结合可解释性AI的洞见

    Parcel loss prediction in last-mile delivery: deep and non-deep approaches with insights from Explainable AI. (arXiv:2310.16602v1 [cs.LG])

    [http://arxiv.org/abs/2310.16602](http://arxiv.org/abs/2310.16602)

    本文介绍了两种机器学习方法用于准确预测最后一公里交付中的包裹丢失。通过在比利时货运数据上的实证研究，我们发现深度混合集成学习（DHEL）模型表现最佳，可帮助电子商务零售商优化决策政策。

    

    在电子商务零售领域，减少最后一公里交付阶段的包裹丢失是一个重要目标。包括产品、客户和订单信息在内的数据的不断增加使得机器学习在包裹丢失预测中的应用成为可能。然而，数据中存在的天然不平衡问题（即只有极少部分包裹丢失）带来了重大挑战。在本文中，我们提出了两种机器学习方法，即数据平衡与有监督学习（DBSL）和深度混合集成学习（DHEL），以准确预测包裹丢失。这些预测的实际意义在于帮助电子商务零售商优化与保险相关的决策政策。我们使用比利时货运的一年数据对所提出的机器学习模型进行了全面评估。研究结果表明，DHEL模型（将前馈自动编码器与随机森林集成方法相结合）的表现最佳。

    Within the domain of e-commerce retail, an important objective is the reduction of parcel loss during the last-mile delivery phase. The ever-increasing availability of data, including product, customer, and order information, has made it possible for the application of machine learning in parcel loss prediction. However, a significant challenge arises from the inherent imbalance in the data, i.e., only a very low percentage of parcels are lost. In this paper, we propose two machine learning approaches, namely, Data Balance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning (DHEL), to accurately predict parcel loss. The practical implication of such predictions is their value in aiding e-commerce retailers in optimizing insurance-related decision-making policies. We conduct a comprehensive evaluation of the proposed machine learning models using one year data from Belgian shipments. The findings show that the DHEL model, which combines a feed-forward autoencoder with a ra
    
[^16]: 在组合独立显著性检验时平衡中心和边缘拒绝

    Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v1 [stat.ME])

    [http://arxiv.org/abs/2310.16600](http://arxiv.org/abs/2310.16600)

    该论文提出了一种在组合独立显著性检验时平衡中心和边缘拒绝的方法，并提出了一种用于测量两者平衡的组合函数。

    

    当原始数据不可用时，评估一组p值的显著性的常见方法是将它们与汇集函数进行组合。这些汇集的p值将p值样本转化为一个表现类似于单变量p值的单一数值。为了明确讨论这些函数，引入了一系列交叉假设，以传达p值中非零证据的强度和普遍性，然后讨论了常规汇集公式。在特定交叉假设的UMP汇集p值中观察到的模式推动了对于中心和边缘拒绝水平在α处的定义和讨论。证明了中心拒绝总是大于等于边缘拒绝，从而提出了一种用于测量两者在汇集的p值中平衡的商。基于χ²_κ分位数变换的组合函数被提出以控制这个商，并且被证明是有效的。

    A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be
    
[^17]: 超越独立同分布权重：稀疏和低秩深度神经网络也是高斯过程

    Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v1 [stat.ML])

    [http://arxiv.org/abs/2310.16597](http://arxiv.org/abs/2310.16597)

    本文扩展了之前的研究，将证明的范围从独立同分布权重扩展到了更大的权重分布类别(PSEUDO-IID)，包括低秩和稀疏设置。作者发现使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。这些结果可以帮助我们识别更广泛的神经网络的边界混沌状态，并进行性能调优。

    

    无限宽神经网络已经被证明是一个有用且可管理的数学模型，使得我们能够理解深度学习中出现的许多现象。其中一个例子是随机深层网络收敛到高斯过程，从而能够对激活函数和网络权重选择对训练动态的影响进行严格分析。在本文中，我们将Matthews等人(2018)的开创性证明扩展到更大的初始权重分布类别(我们称之为PSEUDO-IID)，其中包括独立同分布和正交权重的已有情况，以及因其计算加速优势而受到赞誉的新兴低秩和结构稀疏设置。我们证明，使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。利用我们的结果，可以识别更广泛的神经网络的边界混沌状态，并调整它们的临界性，以增强训练性能。

    The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance the
    
[^18]: 无线联合策略梯度的过空中聚合方法

    Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])

    [http://arxiv.org/abs/2310.16592](http://arxiv.org/abs/2310.16592)

    本文提出了一种过空中联合策略梯度算法，通过无线信道广播携带本地信息的模拟信号实现更新策略参数，研究了噪声和信道失真对算法收敛性的影响，并通过仿真结果验证了算法的有效性。

    

    近年来，过空中聚合在大规模分布式学习、优化和感知中得到了广泛关注。本文提出了一种过空中联合策略梯度算法，其中所有的智能体同时向共享的无线信道广播携带本地信息的模拟信号，中央控制器使用接收到的汇总波形来更新策略参数。我们研究了噪声和信道失真对所提出算法收敛性的影响，并建立了通信和采样的复杂度来找到一个$\epsilon$-近似的稳定点。最后，我们通过一些仿真结果展示了该算法的有效性。

    In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
    
[^19]: 多并行任务延迟储备计算结合硅微环与WDM的方案

    Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM. (arXiv:2310.16588v1 [cs.NE])

    [http://arxiv.org/abs/2310.16588](http://arxiv.org/abs/2310.16588)

    该论文展示了一种基于微环的时间延迟储备计算方案，能够同时解决时间序列预测、分类和无线通道均衡等任务，并且在每个通道上实现了最先进的性能。

    

    我们通过数值模拟展示了一种基于微环的时间延迟储备计算方案，能够同时解决涉及时间序列预测、分类和无线通道均衡的三个任务。在每个进行波长复用的通道上，通过优化功率和频率失谐，实现了最先进的性能。

    We numerically demonstrate a microring-based time-delay reservoir computing scheme that simultaneously solves three tasks involving time-series prediction, classification, and wireless channel equalization. Each task performed on a wavelength-multiplexed channel achieves state-of-the-art performance with optimized power and frequency detuning.
    
[^20]: 自适应高维检验在潜在表示中的不确定性估计

    Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations. (arXiv:2310.16587v1 [cs.LG])

    [http://arxiv.org/abs/2310.16587](http://arxiv.org/abs/2310.16587)

    本论文提出了一种新的框架，通过利用特征表示的统计特性，在潜在表示中使用数据自适应的高维假设检验来进行不确定性估计。这种方法克服了现有不确定性估计方法对低维分布假设的依赖和数据域限制的问题，为不确定性估计提供了更好的泛化能力和性能。

    

    不确定性估计旨在评估训练深度神经网络的置信度。然而，现有的不确定性估计方法依赖于低维分布假设，因此受到潜在特征的高维问题的限制。现有方法往往关注于离散分类概率的不确定性，这导致对于其他任务的不确定性估计的泛化能力较差。此外，大部分文献要求在训练时要看到外域（OOD）数据以更好地估计不确定性，这限制了实际中不确定性估计的性能，因为外域数据通常是未见过的。为了克服这些限制，我们提出了一个新的框架，利用数据自适应的高维假设测试来进行不确定性估计，这利用了特征表示的统计特性。我们的方法直接在潜在表示上操作，因此不需要重新训练特征编码器。

    Uncertainty estimation aims to evaluate the confidence of a trained deep neural network. However, existing uncertainty estimation approaches rely on low-dimensional distributional assumptions and thus suffer from the high dimensionality of latent features. Existing approaches tend to focus on uncertainty on discrete classification probabilities, which leads to poor generalizability to uncertainty estimation for other tasks. Moreover, most of the literature requires seeing the out-of-distribution (OOD) data in the training for better estimation of uncertainty, which limits the uncertainty estimation performance in practice because the OOD data are typically unseen. To overcome these limitations, we propose a new framework using data-adaptive high-dimensional hypothesis testing for uncertainty estimation, which leverages the statistical properties of the feature representations. Our method directly operates on latent representations and thus does not require retraining the feature encode
    
[^21]: 使用具有噪声输入的高斯过程回归的磁力计阵列进行磁场映射

    Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression. (arXiv:2310.16577v1 [stat.ML])

    [http://arxiv.org/abs/2310.16577](http://arxiv.org/abs/2310.16577)

    本文利用磁力计阵列进行磁场映射，通过新颖方法将磁力计的位置信息纳入，提高了地图质量。

    

    室内环境中的铁磁材料会产生环境磁场的扰动。通过使用磁力计测量和有关磁力计位置的信息，可以学习磁场的空间变化幅度。然而，磁力计的位置通常只是大致已知，这对磁场地图的质量产生负面影响。本文研究了如何利用磁力计阵列来提高磁场地图的质量。阵列的位置大致已知，但是阵列上磁力计的相对位置已知。我们在一种新颖的方法中包含了这些信息，以制作环境磁场的地图。我们通过模拟研究了我们的方法的性质，并证明我们的方法可以提高地图质量。我们还通过实验证明了我们方法的有效性。

    Ferromagnetic materials in indoor environments give rise to disturbances in the ambient magnetic field. Maps of these magnetic disturbances can be used for indoor localisation. A Gaussian process can be used to learn the spatially varying magnitude of the magnetic field using magnetometer measurements and information about the position of the magnetometer. The position of the magnetometer, however, is frequently only approximately known. This negatively affects the quality of the magnetic field map. In this paper, we investigate how an array of magnetometers can be used to improve the quality of the magnetic field map. The position of the array is approximately known, but the relative locations of the magnetometers on the array are known. We include this information in a novel method to make a map of the ambient magnetic field. We study the properties of our method in simulation and show that our method improves the map quality. We also demonstrate the efficacy of our method with exper
    
[^22]: 使用结构化核插值的高斯过程回归在大规模环境中生成磁场地图

    Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression. (arXiv:2310.16574v1 [stat.ML])

    [http://arxiv.org/abs/2310.16574](http://arxiv.org/abs/2310.16574)

    本论文提出了一种使用结构化核插值的高斯过程回归方法，在室内环境中生成大规模磁场地图。通过将结构化核插值与导数相结合，该方法能够在线性时间复杂度内计算预测均值和协方差，并且在模拟中取得了良好的表现。

    

    我们提出了一种映射算法，使用近似的高斯过程（GP）回归计算室内环境中的大规模磁场地图。映射环境中环境磁场的空间变化可以用于室内定位算法。为了计算这样的地图，GP回归是一种适合的工具，因为它提供了在新位置的磁场预测以及不确定性量化。由于全GP回归的复杂度随着数据点的数量呈立方增长，因此对于GP的近似方法已被广泛研究。在本文中，我们在结构化核插值（SKI）框架上构建，通过利用高效的Krylov子空间方法加速推断。具体而言，我们将带导数的SKI（D-SKI）纳入了用于磁场建模的标量势模型，并使用线性数据点复杂度计算预测均值和协方差。在我们的模拟中，我们展示了我们的方法。

    We present a mapping algorithm to compute large-scale magnetic field maps in indoor environments with approximate Gaussian process (GP) regression. Mapping the spatial variations in the ambient magnetic field can be used for localization algorithms in indoor areas. To compute such a map, GP regression is a suitable tool because it provides predictions of the magnetic field at new locations along with uncertainty quantification. Because full GP regression has a complexity that grows cubically with the number of data points, approximations for GPs have been extensively studied. In this paper, we build on the structured kernel interpolation (SKI) framework, speeding up inference by exploiting efficient Krylov subspace methods. More specifically, we incorporate SKI with derivatives (D-SKI) into the scalar potential model for magnetic field modeling and compute both predictive mean and covariance with a complexity that is linear in the data points. In our simulations, we show that our metho
    
[^23]: 模型增强的对比强化学习用于序列推荐

    Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation. (arXiv:2310.16566v1 [cs.IR])

    [http://arxiv.org/abs/2310.16566](http://arxiv.org/abs/2310.16566)

    这项研究提出了一种模型增强的对比强化学习方法，用于解决推荐系统中的数据稀疏和过估计问题。

    

    强化学习已经广泛应用于推荐系统，因为其潜力在于优化用户的长期参与度。从强化学习的角度来看，推荐可以被形式化为马尔可夫决策过程(MDP)，其中推荐系统(代理)可以与用户(环境)进行交互，并获得反馈(奖励信号)。然而，出于对用户体验和实现复杂性的考虑，进行在线交互是不切实际的，我们只能使用包含有限奖励信号和状态转换的离线数据集来训练RL推荐者。因此，奖励信号和状态转换的数据稀疏问题非常严重，而这一问题一直被现有的RL推荐系统忽视。更糟糕的是，RL方法通过试错模式来学习，但在隐式反馈推荐任务中无法获得负反馈，进一步加剧了离线RL推荐者的过估计问题。为了解决这些挑战，我们提出了一种模型增强的对比强化学习方法。

    Reinforcement learning (RL) has been widely applied in recommendation systems due to its potential in optimizing the long-term engagement of users. From the perspective of RL, recommendation can be formulated as a Markov decision process (MDP), where recommendation system (agent) can interact with users (environment) and acquire feedback (reward signals).However, it is impractical to conduct online interactions with the concern on user experience and implementation complexity, and we can only train RL recommenders with offline datasets containing limited reward signals and state transitions. Therefore, the data sparsity issue of reward signals and state transitions is very severe, while it has long been overlooked by existing RL recommenders.Worse still, RL methods learn through the trial-and-error mode, but negative feedback cannot be obtained in implicit feedback recommendation tasks, which aggravates the overestimation problem of offline RL recommender. To address these challenges, 
    
[^24]: 图标签传播算法应对图标签噪声问题

    Label Propagation for Graph Label Noise. (arXiv:2310.16560v1 [cs.LG])

    [http://arxiv.org/abs/2310.16560](http://arxiv.org/abs/2310.16560)

    本文研究了图中的标签噪声问题，提出了一种基于标签传播的算法来处理任意异质性的图标签噪声，以纠正噪声标签并为未标记的节点分配标签。

    

    标签噪声是大型数据集中常见的挑战，它会显著降低深度神经网络的泛化能力。大部分现有研究都集中在计算机视觉中的噪声标签，然而，图模型将节点特征和图拓扑结构作为输入，通过消息传递机制更容易受到标签噪声的影响。近期，只有少数几篇文章提出了解决图中标签噪声的方法。其中一个主要限制是它们假设图是同构的，并且标签是平滑分布的。然而，现实世界中的图可能包含不同程度的异质性甚至是异质性的主导，导致当前方法的不足。本文研究任意异质性条件下的图标签噪声问题，旨在纠正噪声标签并为之前未标记的节点分配标签。我们首先进行了两个实证分析，探讨图同质性对图标签噪声的影响。接着，我们提出了一种基于标签传播的算法来处理任意异质性的图标签噪声。

    Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One major limitation is that they assume the graph is homophilous and the labels are smoothly distributed. Nevertheless, real-world graphs may contain varying degrees of heterophily or even be heterophily-dominated, leading to the inadequacy of current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following o
    
[^25]: DECWA: 使用Wasserstein距离的基于密度的聚类方法

    DECWA : Density-Based Clustering using Wasserstein Distance. (arXiv:2310.16552v1 [cs.LG])

    [http://arxiv.org/abs/2310.16552](http://arxiv.org/abs/2310.16552)

    DECWA是一种使用Wasserstein距离的基于密度的聚类方法，它通过新的群集特征描述和聚类算法在任意形状的群集中表现良好，并解决了低密度群集、相似密度群集和高维数据的问题。

    

    聚类是一种通过发现数据组之间的群集来提取知识的数据分析方法。在这些方法中，最先进的基于密度的聚类方法已被证明对任意形状的群集非常有效。尽管结果令人鼓舞，但它们难以找到低密度群集，与相似密度的群集，以及高维数据。我们的提议是一种新的群集特征描述和基于空间密度和概率方法的聚类算法。首先，通过使用点之间的成对距离的概率密度函数（$p.d.f$）表示空间密度，构建子群集。然后提出了一种方法来通过使用子群集的密度（$p.d.f$）和空间距离来聚合相似的子群集。我们提出的关键想法是使用Wasserstein距离度量子群集的$p.d.f$之间的距离，这是一种衡量距离的强大工具。我们展示了我们的方法优于其他最先进的基于密度的聚类方法。

    Clustering is a data analysis method for extracting knowledge by discovering groups of data called clusters. Among these methods, state-of-the-art density-based clustering methods have proven to be effective for arbitrary-shaped clusters. Despite their encouraging results, they suffer to find low-density clusters, near clusters with similar densities, and high-dimensional data. Our proposals are a new characterization of clusters and a new clustering algorithm based on spatial density and probabilistic approach. First of all, sub-clusters are built using spatial density represented as probability density function ($p.d.f$) of pairwise distances between points. A method is then proposed to agglomerate similar sub-clusters by using both their density ($p.d.f$) and their spatial distance. The key idea we propose is to use the Wasserstein metric, a powerful tool to measure the distance between $p.d.f$ of sub-clusters. We show that our approach outperforms other state-of-the-art density-bas
    
[^26]: 乐观主义的陷阱：通过随机化风险标准的分布式强化学习

    Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])

    [http://arxiv.org/abs/2310.16546](http://arxiv.org/abs/2310.16546)

    本论文提出了一种通过随机化风险标准的分布式强化学习算法，以避免在风险上的偏向性，并证明了其收敛性和最优性。实验证明，在包括Atari 55游戏在内的各种环境中，该方法优于其他分布式算法。

    

    分布式强化学习算法试图利用估计的不确定性进行探索，如在面对不确定性时的乐观主义。然而，使用估计的方差进行乐观探索可能导致数据收集的偏差，阻碍收敛或性能。本文提出了一种新颖的分布式强化学习算法，通过随机化风险标准来选择动作，避免在风险上的单向倾向。我们通过扭曲风险度量提供了一个扰动的分布贝尔曼最优性算子，并证明了所提方法具有较弱的收缩性质的收敛性和最优性。我们的理论结果支持，所提方法不会陷入偏向性的探索，并确保收敛到最优回报。最后，我们在包括Atari 55游戏在内的各种环境中通过实验证明了我们的方法优于其他现有的基于分布的算法。

    Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
    
[^27]: FedTherapist：通过联邦学习在智能手机上使用用户生成的语言表达进行精神健康监测

    FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning. (arXiv:2310.16538v1 [cs.CL])

    [http://arxiv.org/abs/2310.16538](http://arxiv.org/abs/2310.16538)

    FedTherapist是一种使用联邦学习在智能手机上进行用户生成的语言表达的精神健康监测的系统。它有效地利用了持续语音和键盘输入，并通过上下文感知语言学习方法来提高预测的准确性。在评估中，比较了非语言特征，结果显示FedTherapist在预测抑郁、压力、焦虑和心情方面的表现更好。

    

    精神科医生通过患者的语言使用来诊断精神疾病。但由于数据隐私问题，现有的被动精神健康监测系统使用手机设备的活动、应用使用和位置等替代特征。我们提出了FedTherapist，一种利用联邦学习以隐私保护方式使用持续语音和键盘输入的移动精神健康监测系统。我们通过比较不同模型设计的性能和开销来克服在智能手机上进行设备内语言模型训练的复杂性。我们还提出了一种上下文感知语言学习（CALL）方法，以有效利用智能手机的大规模和嘈杂文本进行精神健康信号感知。我们在46名参与者中进行了经IRB批准的自我报告抑郁、压力、焦虑和心情的预测评估，结果显示FedTherapist相比于非语言特征的性能提高了0.15 AUROC。

    Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improveme
    
[^28]: 利用多任务预训练增强文档信息分析：一种面向视觉丰富文档中信息提取的鲁棒方法

    Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents. (arXiv:2310.16527v1 [cs.CV])

    [http://arxiv.org/abs/2310.16527](http://arxiv.org/abs/2310.16527)

    本文提出了一种深度学习模型，通过多任务预训练来增强文档信息分析的能力，实现了对视觉丰富文档中信息的提取。模型利用transformer-based模型编码文档图像的各种信息，并通过预训练和微调来优化各种文档图像分析任务。另外，模型还包括了额外的任务和集体预训练方案。

    

    本文介绍了一种专门用于文档信息分析的深度学习模型，重点关注文档分类、实体关系提取和文档视觉问题回答。所提出的模型利用基于transformer的模型来编码文档图像中的所有信息，包括文本、视觉和布局信息。该模型经过预训练后，继续对各种文档图像分析任务进行微调。在预训练阶段，所提出的模型还包括三个额外的任务，包括识别文档图像中不同布局段落的阅读顺序、根据PubLayNet对布局段落进行分类以及生成给定布局段落（文本块）内的文本序列。该模型还采用了一种集体预训练方案，其中考虑了所有考虑的任务的损失，包括使用所有数据集进行的预训练和微调任务。此外，还添加了额外的编码器和解码器块。

    This paper introduces a deep learning model tailored for document information analysis, emphasizing document classification, entity relation extraction, and document visual question answering. The proposed model leverages transformer-based models to encode all the information present in a document image, including textual, visual, and layout information. The model is pre-trained and subsequently fine-tuned for various document image analysis tasks. The proposed model incorporates three additional tasks during the pre-training phase, including reading order identification of different layout segments in a document image, layout segments categorization as per PubLayNet, and generation of the text sequence within a given layout segment (text block). The model also incorporates a collective pre-training scheme where losses of all the tasks under consideration, including pre-training and fine-tuning tasks with all datasets, are considered. Additional encoder and decoder blocks are added to 
    
[^29]: 循环有向概率图模型：基于结构化结果的提案

    Cyclic Directed Probabilistic Graphical Model: A Proposal Based on Structured Outcomes. (arXiv:2310.16525v1 [cs.LG])

    [http://arxiv.org/abs/2310.16525](http://arxiv.org/abs/2310.16525)

    本文提出了一种基于结构化结果的循环有向概率图模型，该模型可以直接捕捉在结构学习过程中产生的方向性循环依赖关系。

    

    在从一组观测数据中构建（结构学习）概率图模型的过程中，常常发现模型的随机变量之间存在方向性的循环依赖关系。现有的图模型如贝叶斯网络和马尔可夫网络可以反映这种依赖关系，但需要复杂化模型，例如添加额外变量或将模型图分割为不同子图。在本文中，我们描述了一种概率图模型——概率关系网络，它允许在结构学习过程中直接捕捉方向性循环依赖关系。该模型基于一个简单的想法，即观测数据的每个样本可以由一个任意图（结构化结果）表示，该图反映了样本中包含的变量依赖的结构。每个结果只包含概率模型结构的一部分，但通过组合这些结果可以得到完整的概率模型图。

    In the process of building (structural learning) a probabilistic graphical model from a set of observed data, the directional, cyclic dependencies between the random variables of the model are often found. Existing graphical models such as Bayesian and Markov networks can reflect such dependencies. However, this requires complicating those models, such as adding additional variables or dividing the model graph into separate subgraphs. Herein, we describe a probabilistic graphical model - probabilistic relation network - that allows the direct capture of directional cyclic dependencies during structural learning. This model is based on the simple idea that each sample of the observed data can be represented by an arbitrary graph (structured outcome), which reflects the structure of the dependencies of the variables included in the sample. Each of the outcomes contains only a part of the graphical model structure; however, a complete graph of the probabilistic model is obtained by combin
    
[^30]: 你能依靠模型评估吗？使用合成测试数据改进模型评估。

    Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data. (arXiv:2310.16524v1 [cs.LG])

    [http://arxiv.org/abs/2310.16524](http://arxiv.org/abs/2310.16524)

    本文介绍了3S Testing，这是一个深度生成建模框架，通过生成合成测试集和模拟分布偏移来改进模型评估，实验结果表明3S Testing在评估少数子群体的模型性能和在分布偏移下表现更优。

    

    在保证公平和可靠性的真实应用中，评估机器学习模型在多样化和代表性不足的子群体上的性能至关重要。然而，由于两个主要问题，准确评估模型性能变得具有挑战性：（1）测试数据稀缺，特别是对于小的子群体，（2）模型在部署环境中可能出现的分布偏移与可用的测试数据不一致。在这项工作中，我们介绍了一个名为3S Testing的深度生成建模框架，通过生成小型子群体的合成测试集和模拟分布偏移来促进模型评估。我们的实验表明，相比于仅使用真实测试数据，3S Testing在评估少数子群体的模型性能和在可能的分布偏移下表现更优。此外，3S Testing提供了性能估计的区间，相比现有方法，展现了更优的覆盖率。

    Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S Testing outperforms traditional baselines -- including real test data alone -- in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing 
    
[^31]: 面向自解释图级异常检测的研究

    Towards Self-Interpretable Graph-Level Anomaly Detection. (arXiv:2310.16520v1 [cs.LG])

    [http://arxiv.org/abs/2310.16520](http://arxiv.org/abs/2310.16520)

    本文提出了一个自解释图级异常检测模型（SIGNET），能够识别异常图并生成有意义的解释，解决了当前图级异常检测方法无法提供有效解释的问题。

    

    图级异常检测旨在识别与集合中的大多数图相比具有显著差异的图形。然而，当前的研究主要关注评估图级异常性，而未能提供有意义的预测解释，这在很大程度上限制了它们的可靠性和应用范围。在本文中，我们研究了一个新的有挑战性的问题，即可解释图级异常检测，其中学习目标是预测每个图样本的异常性，并提供相应的解释，即导致预测的关键子图。为了解决这个具有挑战性的问题，我们提出了一种自解释图级异常检测模型（SIGNET），它可以同时检测异常图并生成信息丰富的解释。具体而言，我们首先引入了多视图子图信息瓶颈（MSIB）框架，作为我们自解释图级异常检测方法的设计基础。

    Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not o
    
[^32]: 基于粒子的广义Wasserstein梯度流的变分推理方法

    Particle-based Variational Inference with Generalized Wasserstein Gradient Flow. (arXiv:2310.16516v1 [stat.ML])

    [http://arxiv.org/abs/2310.16516](http://arxiv.org/abs/2310.16516)

    本文提出了一种基于广义Wasserstein梯度流的ParVI框架，通过引入凸函数引导的更广泛类别的正则化器，解决了传统基于核函数的方法设计困难和限制性的问题，并展示了其具有强大的收敛性保证和高效性能。

    

    基于粒子的变分推理方法（ParVIs），如Stein变分梯度下降（SVGD），通过基于核化的Wasserstein梯度流更新粒子，用于Kullback-Leibler（KL）散度。然而，核函数的设计通常是非平凡的，并且可能对方法的灵活性有限制。最近的研究表明，具有二次形式正则化项的功能梯度流逼近可以提高性能。在本文中，我们提出了一种基于广义Wasserstein梯度流的ParVI框架，称为广义Wasserstein梯度下降（GWG），其可以被视为一种具有凸函数引导的更广泛类别的正则化器的功能梯度方法。我们证明了GWG具有强大的收敛性保证。我们还提供了一种自适应版本，可以自动选择Wasserstein度量来加速收敛。在实验证明了所提出框架的有效性和高效性。

    Particle-based variational inference methods (ParVIs) such as Stein variational gradient descent (SVGD) update the particles based on the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. However, the design of kernels is often non-trivial and can be restrictive for the flexibility of the method. Recent works show that functional gradient flow approximations with quadratic form regularization terms can improve performance. In this paper, we propose a ParVI framework, called generalized Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient flow of the KL divergence, which can be viewed as a functional gradient method with a broader class of regularizers induced by convex functions. We show that GWG exhibits strong convergence guarantees. We also provide an adaptive version that automatically chooses Wasserstein metric to accelerate convergence. In experiments, we demonstrate the effectiveness and efficiency of the proposed framework
    
[^33]: 识别偏见的原因：一种基于论证的方法

    Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])

    [http://arxiv.org/abs/2310.16506](http://arxiv.org/abs/2310.16506)

    本文提出了一种基于论证的方法来确定为什么一个个体被分类与相似个体不同，该方法使用定量论证框架来表示个体和与其相似个体的属性-值对，并使用一个众所周知的语义来确定对个体分类产生最大贡献的属性-值对。

    

    随着算法决策系统在社会中的普及，确保这些系统的公平性变得越来越重要。虽然在构建公平算法决策系统方面已经进行了大量研究，但其中大部分方法需要访问训练数据，包括个人特征，并且对于哪些个体被不公平地分类没有透明度。本文提出了一种新颖的、与模型无关的基于论证的方法，以确定为什么一个个体被分类与相似个体不同。我们的方法使用定量论证框架来表示个体和与其相似个体的属性-值对，并使用一个众所周知的语义来确定对个体分类产生最大贡献的属性-值对。我们在两个在公平领域常用的数据集上评估了我们的方法，并展示了它在识别差异分类方面的有效性。

    As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identi
    
[^34]: 深度学习中的数据优化：一项综述研究

    Data Optimization in Deep Learning: A Survey. (arXiv:2310.16499v1 [cs.LG])

    [http://arxiv.org/abs/2310.16499](http://arxiv.org/abs/2310.16499)

    该论文综合了现有的深度学习数据优化方法，并提出了全面的分类体系。

    

    大规模、高质量的数据被认为是许多深度学习技术成功应用的关键因素。然而，许多实际深度学习任务仍然面临着缺乏足够数量高质量数据的问题。此外，模型的稳健性、公平性和可信度等问题也与训练数据密切相关。因此，大量现有文献中的研究都集中在深度学习任务中的数据方面。一些典型的数据优化技术包括数据增强、逻辑扰动、样本加权和数据压缩。这些技术通常来自于不同的深度学习领域，它们的理论灵感或启发动机可能看似无关。本研究旨在组织广泛的现有数据优化方法论并构建一个全面的分类体系。

    Large-scale, high-quality data are considered an essential factor for the successful application of many deep learning techniques. Meanwhile, numerous real-world deep learning tasks still have to contend with the lack of sufficient amounts of high-quality data. Additionally, issues such as model robustness, fairness, and trustworthiness are also closely related to training data. Consequently, a huge number of studies in the existing literature have focused on the data aspect in deep learning tasks. Some typical data optimization techniques include data augmentation, logit perturbation, sample weighting, and data condensation. These techniques usually come from different deep learning divisions and their theoretical inspirations or heuristic motivations may seem unrelated to each other. This study aims to organize a wide range of existing data optimization methodologies for deep learning from the previous literature, and makes the effort to construct a comprehensive taxonomy for them. T
    
[^35]: 公民参与：基于众感应的可持续室内定位服务

    Citizen participation: crowd-sensed sustainable indoor location services. (arXiv:2310.16496v1 [cs.LG])

    [http://arxiv.org/abs/2310.16496](http://arxiv.org/abs/2310.16496)

    本研究提出了一种基于机器学习的室内定位服务方法，无需额外的硬件设施，提供了小于2米的精确度，并且即使在丢失大量BSSIDs的情况下，仍能保持鲁棒性。

    

    在可持续创新的时代，循环经济范式要求对现有有限资源进行最佳利用和开发。与此同时，向智能基础设施的过渡需要大量的资本、资源和人力投入。在这项工作中，我们提出了一种通用的机器学习方法，用于提供室内定位服务，无需投资额外和专门的硬件设施。我们探索了一些使用案例，访客通过他们的智能手机与可用的WiFi基础设施进行交互，以估计他们的位置，因为室内需求对标准GPS解决方案存在限制。结果表明，我们提出的方法实现了小于2米的精度，并且即使在丢失大量BSSIDs的情况下，模型也具有鲁棒性。

    In the present era of sustainable innovation, the circular economy paradigm dictates the optimal use and exploitation of existing finite resources. At the same time, the transition to smart infrastructures requires considerable investment in capital, resources and people. In this work, we present a general machine learning approach for offering indoor location awareness without the need to invest in additional and specialised hardware. We explore use cases where visitors equipped with their smart phone would interact with the available WiFi infrastructure to estimate their location, since the indoor requirement poses a limitation to standard GPS solutions. Results have shown that the proposed approach achieves a less than 2m accuracy and the model is resilient even in the case where a substantial number of BSSIDs are dropped.
    
[^36]: 关于文本异常曝光在视觉外界检测中的效力

    On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection. (arXiv:2310.16492v1 [cs.CV])

    [http://arxiv.org/abs/2310.16492](http://arxiv.org/abs/2310.16492)

    本文研究了文本异常曝光在视觉外界检测中的应用。通过使用文本异常值代替图像中的异常值，我们揭示了使用文本异常值的好处，并提出了多种生成文本异常值的方法。

    

    成功检测越界数据对于确保神经网络的安全部署变得越来越重要。越界检测的主要挑战之一是神经网络在越界数据上输出过度自信的预测，这使得仅根据预测来确定数据的越界性变得困难。异常曝光通过在训练过程中引入额外的损失，鼓励对越界数据进行低置信度预测，从而解决了这个问题。尽管异常曝光在提高越界检测性能方面显示出了很大潜力，但是之前所有关于异常曝光的研究都限于使用视觉异常。受到视觉语言预训练的最新进展的启发，本文首次探索了文本异常曝光在尚未开拓的领域。首先，我们通过用文本等价物替换图像域中的真实或虚拟异常值来揭示使用文本异常值的好处。接下来，我们提出了多种生成文本异常值的方法。

    Successful detection of Out-of-Distribution (OoD) data is becoming increasingly important to ensure safe deployment of neural networks. One of the main challenges in OoD detection is that neural networks output overconfident predictions on OoD data, make it difficult to determine OoD-ness of data solely based on their predictions. Outlier exposure addresses this issue by introducing an additional loss that encourages low-confidence predictions on OoD data during training. While outlier exposure has shown promising potential in improving OoD detection performance, all previous studies on outlier exposure have been limited to utilizing visual outliers. Drawing inspiration from the recent advancements in vision-language pre-training, this paper venture out to the uncharted territory of textual outlier exposure. First, we uncover the benefits of using textual outliers by replacing real or virtual outliers in the image-domain with textual equivalents. Then, we propose various ways of genera
    
[^37]: TSONN: 时间步导向的神经网络用于求解偏微分方程

    TSONN: Time-stepping-oriented neural network for solving partial differential equations. (arXiv:2310.16491v1 [cs.LG])

    [http://arxiv.org/abs/2310.16491](http://arxiv.org/abs/2310.16491)

    TSONN是一种将时间步方法与深度学习相结合的神经网络，通过将原始问题转化为一系列良好条件的子问题来解决PDE求解中的稳定训练和正确结果的挑战。

    

    深度神经网络（DNNs），尤其是受物理启发的神经网络（PINNs），最近成为求解由偏微分方程（PDEs）决定的正向和反向问题的新热门方法。然而，由于使用基于PDE的软约束来最小化PDE残差，这些方法仍然面临着在许多问题中实现稳定训练和获得正确结果的挑战，因为这使得问题条件不良。与所有现有的直接最小化PDE残差的方法不同，本文将时间步方法与深度学习相结合，将原本的条件不良优化问题转化为一系列在给定伪时间间隔内具有良好条件的子问题。通过跟踪伪时间步进过程的轨迹，模型训练的收敛性得到显著改善，从而得到了稳健的基于优化的PDE求解器。我们的结果表明，所提出的方法在许多问题中实现了稳定的训练和正确的结果。

    Deep neural networks (DNNs), especially physics-informed neural networks (PINNs), have recently become a new popular method for solving forward and inverse problems governed by partial differential equations (PDEs). However, these methods still face challenges in achieving stable training and obtaining correct results in many problems, since minimizing PDE residuals with PDE-based soft constraint make the problem ill-conditioned. Different from all existing methods that directly minimize PDE residuals, this work integrates time-stepping method with deep learning, and transforms the original ill-conditioned optimization problem into a series of well-conditioned sub-problems over given pseudo time intervals. The convergence of model training is significantly improved by following the trajectory of the pseudo time-stepping process, yielding a robust optimization-based PDE solver. Our results show that the proposed method achieves stable training and correct results in many problems that s
    
[^38]: 多目标强化学习的超参数优化

    Hyperparameter Optimization for Multi-Objective Reinforcement Learning. (arXiv:2310.16487v1 [cs.LG])

    [http://arxiv.org/abs/2310.16487](http://arxiv.org/abs/2310.16487)

    该论文研究了多目标强化学习中的超参数优化的挑战，并提出了一种系统的方法来解决这一问题。

    

    强化学习已经成为解决复杂问题的一种强大方法。多目标强化学习(MORL)的引入进一步扩展了强化学习的范围，使代理能够在多个目标之间进行权衡。这一进展不仅扩大了可以解决的问题范围，还为探索和进步创造了许多机会。然而，强化学习代理的有效性严重依赖于适当设置它们的超参数。在实践中，这一任务常常很具有挑战性，导致这些技术在各种实例中无法成功部署。因此，先前的研究在强化学习中探索了超参数优化以解决这一问题。本文针对MORL对超参数优化的挑战进行了初步研究。我们规范化了该问题，突出了其独特的挑战，并提出了一种系统的方法来解决它。

    Reinforcement learning (RL) has emerged as a powerful approach for tackling complex problems. The recent introduction of multi-objective reinforcement learning (MORL) has further expanded the scope of RL by enabling agents to make trade-offs among multiple objectives. This advancement not only has broadened the range of problems that can be tackled but also created numerous opportunities for exploration and advancement. Yet, the effectiveness of RL agents heavily relies on appropriately setting their hyperparameters. In practice, this task often proves to be challenging, leading to unsuccessful deployments of these techniques in various instances. Hence, prior research has explored hyperparameter optimization in RL to address this concern.  This paper presents an initial investigation into the challenge of hyperparameter optimization specifically for MORL. We formalize the problem, highlight its distinctive challenges, and propose a systematic methodology to address it. The proposed me
    
[^39]: 一种用于多元时间序列数据的基于深度学习的事件检测和自然语言处理中的信息检索的全面Python库

    A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP. (arXiv:2310.16485v1 [cs.LG])

    [http://arxiv.org/abs/2310.16485](http://arxiv.org/abs/2310.16485)

    本文介绍了一个新的深度学习监督方法，用于检测多元时间序列数据中的事件。与现有方法相比，该方法在回归、标记数据集需求和鲁棒性方面具有创新。

    

    时间序列数据中的事件检测在金融、医疗保健、网络安全和科学等各个领域都至关重要。准确地识别时间序列数据中的事件对于做出明智决策、检测异常和预测未来趋势至关重要。尽管针对时间序列的事件检测已经进行了广泛的研究，其中深度学习方法是最先进的方法之一，但在这个领域仍有改进和创新的空间。本文提出了一种新的深度学习监督方法，用于检测多元时间序列数据中的事件。与现有的深度学习监督方法相比，我们的方法结合了四个独特的创新。首先，它基于回归而不是二元分类。其次，它不需要标记的数据集，每个数据点都有标签；相反，它只需要定义为时间点或时间间隔的参考事件。第三，它通过使用堆叠集成模型设计得到了鲁棒性。

    Event detection in time series data is crucial in various domains, including finance, healthcare, cybersecurity, and science. Accurately identifying events in time series data is vital for making informed decisions, detecting anomalies, and predicting future trends. Despite extensive research exploring diverse methods for event detection in time series, with deep learning approaches being among the most advanced, there is still room for improvement and innovation in this field. In this paper, we present a new deep learning supervised method for detecting events in multivariate time series data. Our method combines four distinct novelties compared to existing deep-learning supervised methods. Firstly, it is based on regression instead of binary classification. Secondly, it does not require labeled datasets where each point is labeled; instead, it only requires reference events defined as time points or intervals of time. Thirdly, it is designed to be robust by using a stacked ensemble l
    
[^40]: 专家的交响曲：在强化学习中运用对抗性洞察力的编排

    Symphony of experts: orchestration with adversarial insights in reinforcement learning. (arXiv:2310.16473v1 [cs.LG])

    [http://arxiv.org/abs/2310.16473](http://arxiv.org/abs/2310.16473)

    这篇论文介绍了一种利用专家策略进行决策指导的编排方法，通过将对抗性设置中的后悔边界结果转移到表格设置下的编排中，推广了自然策略梯度的分析，并提供了关于样本复杂度的洞察。这种方法的关键点在于其透明的证明。在随机匹配玩具模型中进行了模拟实验。

    

    结构化强化学习利用具有优势特性的策略以达到更好的性能，特别是在探索具有挑战性的场景中。我们通过编排的概念来探索这一领域，其中一组（少量）专家策略指导决策；我们的第一个贡献是建立了此建模。然后，我们通过从对抗性设置中转移后悔边界结果，在表格设置下建立了编排的价值函数后悔边界。我们将对 Agarwal 等人 [2021, 第5.3节] 中自然策略梯度的分析推广并扩展到任意对抗性聚合策略。我们还将其扩展到估计优势函数的情况，提供了关于期望值和高概率下样本复杂度的洞察。我们方法的一个关键点在于其相对于现有方法而言证明较为透明。最后，我们针对随机匹配玩具模型进行了模拟实验。

    Structured reinforcement learning leverages policies with advantageous properties to reach better performance, particularly in scenarios where exploration poses challenges. We explore this field through the concept of orchestration, where a (small) set of expert policies guides decision-making; the modeling thereof constitutes our first contribution. We then establish value-functions regret bounds for orchestration in the tabular setting by transferring regret-bound results from adversarial settings. We generalize and extend the analysis of natural policy gradient in Agarwal et al. [2021, Section 5.3] to arbitrary adversarial aggregation strategies. We also extend it to the case of estimated advantage functions, providing insights into sample complexity both in expectation and high probability. A key point of our approach lies in its arguably more transparent proofs compared to existing methods. Finally, we present simulations for a stochastic matching toy model.
    
[^41]: 从有限观测中学习连续网络新兴动态的数据自适应随机过程

    Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes. (arXiv:2310.16466v1 [cs.LG])

    [http://arxiv.org/abs/2310.16466](http://arxiv.org/abs/2310.16466)

    本研究提出了一种名为NDP4ND的神经ODE进程，以从稀缺观测中学习连续网络新兴动态。这种方法通过建立数据自适应的随机网络动力学，克服了由于稀疏、不规则采样、部分和噪声观测而导致的学习困难。

    

    通过实证结构和时空观测数据学习网络动态对于揭示复杂网络在多个领域中的交互机制至关重要。然而，大多数现有方法只针对学习由特定常微分方程实例生成的网络动态行为，对于新的方程实例效果不佳，并且通常需要密集观测。观测数据，尤其是来自网络新兴动态的数据通常很难获得，这给模型学习带来了困扰。因此，如何通过稀疏、不规则采样、部分和噪声观测来学习准确的网络动态仍然是一个基本挑战。我们引入了神经ODE进程用于网络动力学（NDP4ND），这是一类由随机数据自适应网络动力学控制的随机过程，以克服这个挑战并从有限观测中学习连续网络动力学。

    Learning network dynamics from the empirical structure and spatio-temporal observation data is crucial to revealing the interaction mechanisms of complex networks in a wide range of domains. However, most existing methods only aim at learning network dynamic behaviors generated by a specific ordinary differential equation instance, resulting in ineffectiveness for new ones, and generally require dense observations. The observed data, especially from network emerging dynamics, are usually difficult to obtain, which brings trouble to model learning. Therefore, how to learn accurate network dynamics with sparse, irregularly-sampled, partial, and noisy observations remains a fundamental challenge. We introduce Neural ODE Processes for Network Dynamics (NDP4ND), a new class of stochastic processes governed by stochastic data-adaptive network dynamics, to overcome the challenge and learn continuous network dynamics from scarce observations. Intensive experiments conducted on various network 
    
[^42]: 《单眼深度估计中的可解释性》

    Towards Explainability in Monocular Depth Estimation. (arXiv:2310.16457v1 [cs.CV])

    [http://arxiv.org/abs/2310.16457](http://arxiv.org/abs/2310.16457)

    本文研究了单眼深度估计方法中的可解释性，重点关注了人类感知深度的一个最重要的视觉线索——相对尺寸。通过模拟人类实验和测试最先进的方法，本研究实现了平均准确率约为77%的结果，间接揭示了这些方法的可解释性。

    

    二维图像深度估计一直是计算机视觉中具有挑战性且广泛研究的课题。近年来，随着基于深度学习的方法的出现，取得了显著的进展，并且取得了极高的成功率。本文关注单眼深度估计方法中的可解释性，即人类如何感知深度。这项初步研究强调了最显著的视觉线索之一，即相对尺寸，在几乎所有观察的图像中都非常突出。我们设计了一个特定的实验来模拟人类实验，并测试了最先进的方法以间接评估在所定义的上下文中的可解释性。此外，我们观察到测量准确性需要进一步关注，并提出了一种特殊的方法来解决这个问题。结果表明，在各种方法中，平均准确率达到了约77%，其中一些方法表现出色，从而间接揭示了它们的可解释性。

    The estimation of depth in two-dimensional images has long been a challenging and extensively studied subject in computer vision. Recently, significant progress has been made with the emergence of Deep Learning-based approaches, which have proven highly successful. This paper focuses on the explainability in monocular depth estimation methods, in terms of how humans perceive depth. This preliminary study emphasizes on one of the most significant visual cues, the relative size, which is prominent in almost all viewed images. We designed a specific experiment to mimic the experiments in humans and have tested state-of-the-art methods to indirectly assess the explainability in the context defined. In addition, we observed that measuring the accuracy required further attention and a particular approach is proposed to this end. The results show that a mean accuracy of around 77% across methods is achieved, with some of the methods performing markedly better, thus, indirectly revealing their
    
[^43]: ClearMark：通过转置模型训练的直观且鲁棒的模型水印技术

    ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training. (arXiv:2310.16453v1 [cs.LG])

    [http://arxiv.org/abs/2310.16453](http://arxiv.org/abs/2310.16453)

    ClearMark是第一种为直观人类评估而设计的DNN水印方法，它通过嵌入可见水印来允许人类决策，同时使用转置模型架构将水印与主要任务交织在一起。

    

    由于在数据采集和模型训练过程中需要付出巨大的努力，深度神经网络（DNN）属于模型创建者的知识产权。因此，未经授权的使用、盗窃或修改可能导致法律后果。现有的DNN水印方法往往非直观，嵌入人眼不可见的标记，需要对缺乏人可理解属性的算法评估产生信任，并且依赖于刚性的阈值，易受到部分水印擦除的影响。本文介绍了ClearMark，这是第一种为直观人类评估而设计的DNN水印方法。ClearMark嵌入可见水印，使人类可以进行决策，而无需刚性的值阈值，同时允许技术辅助评估。ClearMark定义了一个转置模型架构，可以将水印与所有模型参数中的主要任务交织在一起。与现有的水印方法相比，

    Due to costly efforts during data acquisition and model training, Deep Neural Networks (DNNs) belong to the intellectual property of the model creator. Hence, unauthorized use, theft, or modification may lead to legal repercussions. Existing DNN watermarking methods for ownership proof are often non-intuitive, embed human-invisible marks, require trust in algorithmic assessment that lacks human-understandable attributes, and rely on rigid thresholds, making it susceptible to failure in cases of partial watermark erasure.  This paper introduces ClearMark, the first DNN watermarking method designed for intuitive human assessment. ClearMark embeds visible watermarks, enabling human decision-making without rigid value thresholds while allowing technology-assisted evaluations. ClearMark defines a transposed model architecture allowing to use of the model in a backward fashion to interwove the watermark with the main task within all model parameters. Compared to existing watermarking methods
    
[^44]: 可解释的基于路径的知识图推荐中的忠实路径语言建模

    Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])

    [http://arxiv.org/abs/2310.16452](http://arxiv.org/abs/2310.16452)

    本文提出了一个名为PEARLM的方法，通过语言建模开展基于路径的知识图谱推荐，解决了现有方法中对预训练知识图谱嵌入的依赖以及未充分利用实体和关系之间相互依赖性的问题，还避免了生成不准确的解释。实验结果表明，与现有方法相比，我们的方法效果显著。

    

    针对知识图谱中的路径推理方法在提高推荐系统透明度方面的潜力，本文提出了一种名为PEARLM的新方法，该方法通过语言建模有效捕获用户行为和产品端知识。我们的方法通过语言模型直接从知识图谱上的路径中学习知识图谱嵌入，并将实体和关系统一在同一优化空间中。序列解码的约束保证了路径对知识图谱的忠实性。在两个数据集上的实验证明了我们方法与现有最先进方法的有效性。

    Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
    
[^45]: 在线性估计器中的领悟——一个可解的模型在不理解的情况下领悟。 （arXiv：2310.16441v1 [stat.ML]）

    Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding. (arXiv:2310.16441v1 [stat.ML])

    [http://arxiv.org/abs/2310.16441](http://arxiv.org/abs/2310.16441)

    该论文揭示了在线性网络中的领悟现象，研究了领悟时间与输入输出维度、训练样本量、正则化和网络初始化的关系，并发现泛化准确性的大幅提升并不一定意味着从“记忆”到“理解”的转变。

    

    领悟是一个有趣的现象，指的是模型在拟合训练数据后仍能泛化。我们通过解析和数值方法表明，即使在简单的师生设置中，具有高斯输入的线性网络执行线性任务时，领悟也会出现。在这种情况下，我们推导出了完整的训练动力学，以训练和泛化数据的协方差矩阵表示。我们提供了关于领悟时间如何取决于输入和输出维度，训练样本量，正则化和网络初始化的精确预测。我们证明了泛化准确性的急剧增加可能并不意味着从“记忆”到“理解”的转变，而只是准确性度量的产物。我们提供了对我们计算的经验证实，并初步的结果表明，某些预测也适用于具有非线性激活函数的更深层次的网络。

    Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with Gaussian inputs. In this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. We present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. We demonstrate that the sharp increase in generalization accuracy may not imply a transition from "memorization" to "understanding", but can simply be an artifact of the accuracy measure. We provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.
    
[^46]: 非各向异性的持久同调：利用持久同调的度量依赖性

    Non-isotropic Persistent Homology: Leveraging the Metric Dependency of PH. (arXiv:2310.16437v1 [math.AT])

    [http://arxiv.org/abs/2310.16437](http://arxiv.org/abs/2310.16437)

    本论文提出了一种非各向异性的持久同调方法，通过变化底层空间上的距离函数并分析持久图的偏移，可以提取额外的拓扑和几何信息。

    

    持久同调是一种广泛使用的拓扑数据分析工具，它基于指定的滤波器创建一个对点云的拓扑性质的简洁描述。大多数用于持久同调的滤波器（隐含地）依赖于选择的度量，通常会以$\mathbb{R}^n$上的标准欧氏度量为选择。最近的研究试图使用距离到度量函数来揭示点云上的“真实”度量，以获得更有意义的持久同调结果。在这里，我们提出了对这个问题的替代性看法：我们认为当将持久同调限制在一个单一（正确的）距离函数上时，点云上的信息会丢失。相反，我们展示了如何通过在底层空间上变化距离函数并分析相应的持久图中的偏移，可以提取额外的拓扑和几何信息。最后，我们通过数值实验证明了非各向异性持久同调能够实现...

    Persistent Homology is a widely used topological data analysis tool that creates a concise description of the topological properties of a point cloud based on a specified filtration. Most filtrations used for persistent homology depend (implicitly) on a chosen metric, which is typically agnostically chosen as the standard Euclidean metric on $\mathbb{R}^n$. Recent work has tried to uncover the 'true' metric on the point cloud using distance-to-measure functions, in order to obtain more meaningful persistent homology results. Here we propose an alternative look at this problem: we posit that information on the point cloud is lost when restricting persistent homology to a single (correct) distance function. Instead, we show how by varying the distance function on the underlying space and analysing the corresponding shifts in the persistence diagrams, we can extract additional topological and geometrical information. Finally, we numerically show that non-isotropic persistent homology can 
    
[^47]: FlatMatch: 使用交叉锋利度来衔接标记和未标记数据的半监督学习

    FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning. (arXiv:2310.16412v1 [cs.LG])

    [http://arxiv.org/abs/2310.16412](http://arxiv.org/abs/2310.16412)

    本文提出了FlatMatch方法，通过最小化交叉锋利度度量来确保标记数据和未标记数据之间的一致学习性能，提高了半监督学习的泛化性能。

    

    半监督学习（SSL）一直是一种有效利用丰富的未标记数据与极其稀缺的标记数据的方法。然而，大多数SSL方法通常基于不同数据转换之间的实例一致性。因此，对标记数据的标签指导很难传播到未标记数据中。结果，标记数据上的学习过程比未标记数据上的学习过程快得多，很可能陷入不利于未标记数据的局部极小值，导致次优的泛化性能。在本文中，我们提出了FlatMatch，通过最小化交叉锋利度度量来确保两个数据集之间的一致学习性能。具体而言，我们增加了标记数据上的经验风险，得到了一个最坏情况模型，即需要增强的失败情况。然后，通过利用未标记数据的丰富性，我们惩罚最坏情况模型与原始模型之间的预测差异（即交叉锋利度）。

    Semi-Supervised Learning (SSL) has been an effective way to leverage abundant unlabeled data with extremely scarce labeled data. However, most SSL methods are commonly based on instance-wise consistency between different data transformations. Therefore, the label guidance on labeled data is hard to be propagated to unlabeled data. Consequently, the learning process on labeled data is much faster than on unlabeled data which is likely to fall into a local minima that does not favor unlabeled data, leading to sub-optimal generalization performance. In this paper, we propose FlatMatch which minimizes a cross-sharpness measure to ensure consistent learning performance between the two datasets. Specifically, we increase the empirical risk on labeled data to obtain a worst-case model which is a failure case that needs to be enhanced. Then, by leveraging the richness of unlabeled data, we penalize the prediction difference (i.e., cross-sharpness) between the worst-case model and the original 
    
[^48]: 弥合人工智能与人类知识的差距：在AlphaZero中进行概念发现和传递

    Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero. (arXiv:2310.16410v1 [cs.AI])

    [http://arxiv.org/abs/2310.16410](http://arxiv.org/abs/2310.16410)

    本研究提出了一种新的方法，可以从AlphaZero中提取新的国际象棋概念，并发现这些概念可以被顶级国际象棋大师所学习和应用。

    

    人工智能系统在各个领域取得了超人类水平的表现，为我们提供了一个进一步提升人类知识和提高人类专家表现的机会。然而，这些高效的人工智能系统所包含的知识往往难以提取，也可能难以理解或学习。在这里，我们提出了一种新方法，可以在AlphaZero中提取新的国际象棋概念，AlphaZero是一个通过自我对弈而掌握国际象棋的人工智能系统。我们的分析表明，AlphaZero可能编码了超越现有人类知识的知识，但这些知识最终并不超出人类的理解范围，并且可以成功地学习。在人类研究中，我们展示了这些概念是可以被顶级国际象棋大师所学习的，因为四名顶级国际象棋大师在解决所呈现的概念原型位置时显示出了进步。

    Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. 
    
[^49]: 融合大型语言模型的推荐系统中的多键值策略

    Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model. (arXiv:2310.16409v1 [cs.IR])

    [http://arxiv.org/abs/2310.16409](http://arxiv.org/abs/2310.16409)

    该论文研究了将推荐系统与大型语言模型结合以实现顺序推荐的问题。现有工作主要考虑单键情况，而忽略了多键值数据的重要性。本研究的贡献在于解决了实际应用中多键值数据的推荐问题。

    

    推荐系统在满足互联网应用中用户信息需求方面发挥着重要作用，通常使用神经网络处理嵌入细节。最近，大型语言模型在计算机视觉和自然语言处理社区取得重大突破。因此，将推荐系统与大型语言模型更好地结合起来成为了新兴的研究方向。尽管一些现有工作对此问题有所贡献，但主要考虑单键情况（如历史交互），特别是在顺序推荐中，多键值数据的情况被简单忽略。然而，多键值数据在实际应用中是主流场景，用户的信息（如年龄、职业等）和物品的信息（如标题、类别等）具有多个键。因此，我们旨在基于多键值数据实现顺序推荐。

    Recommendation system (RS) plays significant roles in matching users information needs for Internet applications, and it usually utilizes the vanilla neural network as the backbone to handle embedding details. Recently, the large language model (LLM) has exhibited emergent abilities and achieved great breakthroughs both in the CV and NLP communities. Thus, it is logical to incorporate RS with LLM better, which has become an emerging research direction. Although some existing works have made their contributions to this issue, they mainly consider the single key situation (e.g. historical interactions), especially in sequential recommendation. The situation of multiple key-value data is simply neglected. This significant scenario is mainstream in real practical applications, where the information of users (e.g. age, occupation, etc) and items (e.g. title, category, etc) has more than one key. Therefore, we aim to implement sequential recommendations based on multiple key-value data by in
    
[^50]: 基于信息论的拓扑感知异构联邦边缘学习在噪声通道上的泛化分析

    Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v1 [cs.IT])

    [http://arxiv.org/abs/2310.16407](http://arxiv.org/abs/2310.16407)

    这项工作提出了一种基于信息论的拓扑感知联邦边缘学习的泛化分析方法，并提出了一种名为FedGMIR的正则化方法来增强模型性能。

    

    随着边缘智能的快速发展，无线网络上的联邦学习部署越来越受关注，被称为联邦边缘学习（FEEL）。在FEEL中，移动设备通过噪声通道传输模型参数和在各种环境中收集数据，这给训练模型的泛化带来了挑战。此外，设备可以通过设备间通信进行分散式联邦学习，而连接设备的通信拓扑也影响模型的泛化能力。然而，大多数最近的理论研究在开展泛化分析时忽视了所有这些效应的纳入。与之相反，我们的工作提出了一种基于信息论的拓扑感知FEEL的泛化分析方法，考虑到了数据异构性和噪声通道的影响。此外，我们还提出了一种名为联邦全局互信息减少（FedGMIR）的新型正则化方法，以提高模型的性能。

    With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models
    
[^51]: 具有参数化图的图神经网络

    Graph Neural Networks with a Distribution of Parametrized Graphs. (arXiv:2310.16401v1 [cs.LG])

    [http://arxiv.org/abs/2310.16401](http://arxiv.org/abs/2310.16401)

    这篇论文提出了一种使用参数化和生成多个图来解决图神经网络仅使用单个观察图的挑战的方法。在节点分类和图回归任务上，通过最大似然估计网络参数，在多个图上使用马尔可夫链蒙特卡洛方法，结合PAC-Bayesian理论的原则，取得了性能改进。

    

    传统上，图神经网络是使用单个观察到的图进行训练的。然而，观察到的图仅代表了一种可能的实现。在许多应用中，图可能会遇到不确定性，例如存在错误或缺失的边，以及提供很少信息价值的边权重。为了解决这些挑战并捕捉先前在观察到的图中缺失的附加信息，我们引入潜在变量来参数化和生成多个图。我们基于多个图的期望最大化（EM）框架，使用最大似然估计网络参数。具体而言，我们使用马尔可夫链蒙特卡洛（MCMC）方法迭代地确定图的分布，结合PAC-Bayesian理论的原则。数值实验在异质图的节点分类和化学数据集的图回归上改进了性能。

    Traditionally, graph neural networks have been trained using a single observed graph. However, the observed graph represents only one possible realization. In many applications, the graph may encounter uncertainties, such as having erroneous or missing edges, as well as edge weights that provide little informative value. To address these challenges and capture additional information previously absent in the observed graph, we introduce latent variables to parameterize and generate multiple graphs. We obtain the maximum likelihood estimate of the network parameters in an Expectation-Maximization (EM) framework based on the multiple graphs. Specifically, we iteratively determine the distribution of the graphs using a Markov Chain Monte Carlo (MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical experiments demonstrate improvements in performance against baseline models on node classification for heterogeneous graphs and graph regression on chemistry datasets.
    
[^52]: 使用图样条网络学习高效替代动态模型

    Learning Efficient Surrogate Dynamic Models with Graph Spline Networks. (arXiv:2310.16397v1 [math.NA])

    [http://arxiv.org/abs/2310.16397](http://arxiv.org/abs/2310.16397)

    本文提出了一种新颖的深度学习方法GraphSplineNets，通过减少深度替代模型的网格大小和迭代步数来加速物理系统的预测，同时使用两种可微的正交样条插值方法和自适应插值策略提高准确度和速度的权衡。

    

    在工程和科学计算中，复杂的物理系统模拟被广泛应用，但它们常常需要耗费大量计算资源，近年来通过深度学习方法解决这个问题。本文提出了一种新颖的深度学习方法GraphSplineNets，通过减少深度替代模型的网格大小和迭代步数来加速物理系统的预测。我们的方法使用两种可微的正交样条插值方法，有效地预测任意时空位置的响应。此外，我们引入了自适应插值策略，以优先从最重要的区域进行采样。GraphSplineNets在预测不断复杂化的各种动态系统中，包括热方程、阻尼波传播、Navier-Stokes方程和真实世界海流，提高了准确度和速度的权衡，适用于规则和非规则域。

    While complex simulations of physical systems have been widely used in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep-learning method to speed up the forecasting of physical systems by reducing the grid size and number of iteration steps of deep surrogate models. Our method uses two differentiable orthogonal spline collocation methods to efficiently predict response at any location in time and space. Additionally, we introduce an adaptive collocation strategy in space to prioritize sampling from the most important regions. GraphSplineNets improve the accuracy-speedup tradeoff in forecasting various dynamical systems with increasing complexity, including the heat equation, damped wave propagation, Navier-Stokes equations, and real-world ocean currents in both regular and irregular domains.
    
[^53]: 通过探索变量参数提高不同分布的不变性学习：从失败彩票中获取的获奖

    Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization. (arXiv:2310.16391v1 [cs.LG])

    [http://arxiv.org/abs/2310.16391](http://arxiv.org/abs/2310.16391)

    这项研究提出了一种名为EVIL的方法，通过探索分布知识中的变量参数，将其排除在不变性学习之外，从而找到一个抵抗分布转移的鲁棒子网络。

    

    不同分布的不变性学习旨在学习到对各种环境都具有很好泛化性能的鲁棒模型，而不是适应特定分布的特征。最近的研究基于"彩票票据假设"解决了此问题，通过最小化学习目标来找到一些对任务关键的参数。然而，在不同分布的问题中，这些解决方案并不是最优的，因为学习任务包含严重的分布噪声，这可能会误导优化过程。因此，除了找到与任务相关的参数（即不变参数）外，我们提出了"探索变量参数进行不变性学习"（EVIL）的方法，该方法还利用分布知识来寻找对分布变化敏感的参数（即变量参数）。一旦将变量参数从不变性学习中剔除，就可以找到一个抵抗分布转移的鲁棒子网络。此外，相对稳定的参数还可以通过跨分布的解耦来显著提高模型性能。

    Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in OOD problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distr
    
[^54]: 分布式的不确定度量化：球面上的核插值

    Distributed Uncertainty Quantification of Kernel Interpolation on Spheres. (arXiv:2310.16384v1 [math.NA])

    [http://arxiv.org/abs/2310.16384](http://arxiv.org/abs/2310.16384)

    本文提出和研究了一种分布式插值方法，用于管理和量化通过插值球面嘈杂数据而带来的不确定性。数值模拟结果表明该方法在处理具有挑战性计算环境中的噪声数据方面是实用和强大的。

    

    对于散乱数据的径向基函数（RBF）核插值，Schaback在1995年证明了可达到的逼近误差和基础插值矩阵的条件数不能同时变小。他将这一发现称为“不确定关系”，其中的一个不可取的结果是RBF核插值容易受到噪声数据的干扰。在本文中，我们提出并研究了一种分布式插值方法，用于管理和量化通过插值非零程度的球面嘈杂数据而带来的不确定性。我们还提供了数值模拟结果，表明我们的方法在处理来自具有挑战性计算环境的噪声数据方面是实用和强大的。

    For radial basis function (RBF) kernel interpolation of scattered data, Schaback in 1995 proved that the attainable approximation error and the condition number of the underlying interpolation matrix cannot be made small simultaneously. He referred to this finding as an "uncertainty relation", an undesirable consequence of which is that RBF kernel interpolation is susceptible to noisy data. In this paper, we propose and study a distributed interpolation method to manage and quantify the uncertainty brought on by interpolating noisy spherical data of non-negligible magnitude. We also present numerical simulation results showing that our method is practical and robust in terms of handling noisy data from challenging computing environments.
    
[^55]: 用深度学习方法改进入侵检测性能的多攻击分类模型

    A model for multi-attack classification to improve intrusion detection performance using deep learning approaches. (arXiv:2310.16380v1 [cs.NI])

    [http://arxiv.org/abs/2310.16380](http://arxiv.org/abs/2310.16380)

    该论文提出了一个用深度学习方法改进入侵检测性能的多攻击分类模型，这个模型通过引入新的深度学习方法，在准确度、检测率和低误报率方面取得了优秀的结果

    

    该提议的模型引入了新颖的深度学习方法。其目标是创建一个可靠的入侵检测机制，以帮助识别恶意攻击。该模型基于深度学习的解决方案框架包括三种方法。第一种方法是带有七种优化器函数（如adamax、SGD、adagrad、adam、RMSprop、nadam和adadelta）的长短期记忆循环神经网络（LSTM-RNN）。该模型在NSL-KDD数据集上进行评估，并进行多攻击分类。该模型在准确度、检测率和低误报率方面优于adamax优化器。将LSTM-RNN与adamax优化器的结果与现有的浅层机器学习和深度学习模型在准确度、检测率和低误报率方面进行了比较。多模型方法包括递归神经网络（RNN）、长短期记忆循环神经网络（LSTM-RNN）和深度神经网络（DNN）。

    This proposed model introduces novel deep learning methodologies. The objective here is to create a reliable intrusion detection mechanism to help identify malicious attacks. Deep learning based solution framework is developed consisting of three approaches. The first approach is Long-Short Term Memory Recurrent Neural Network (LSTM-RNN) with seven optimizer functions such as adamax, SGD, adagrad, adam, RMSprop, nadam and adadelta. The model is evaluated on NSL-KDD dataset and classified multi attack classification. The model has outperformed with adamax optimizer in terms of accuracy, detection rate and low false alarm rate. The results of LSTM-RNN with adamax optimizer is compared with existing shallow machine and deep learning models in terms of accuracy, detection rate and low false alarm rate. The multi model methodology consisting of Recurrent Neural Network (RNN), Long-Short Term Memory Recurrent Neural Network (LSTM-RNN), and Deep Neural Network (DNN). The multi models are eval
    
[^56]: GADY: 动态图上的无监督异常检测

    GADY: Unsupervised Anomaly Detection on Dynamic Graphs. (arXiv:2310.16376v1 [cs.LG])

    [http://arxiv.org/abs/2310.16376](http://arxiv.org/abs/2310.16376)

    GADY是一种在动态图上进行无监督异常检测的新方法，通过提出连续动态图模型和引入负采样模块来解决了动态结构构建挑战和负采样挑战。

    

    动态图上的异常检测是指检测行为明显偏离图中观察到的规范行为的实体和它们的时间信息。这个领域因其在金融、网络安全、社交网络等方面的应用而受到越来越多的关注。然而，现有方法面临两个挑战：动态结构构建挑战-难以捕捉带有复杂时间信息的图结构，以及负采样挑战-无法构建优秀的负采样进行无监督学习。为了解决这些挑战，我们提出了在动态图上的无监督生成异常检测（GADY）。为了解决第一个挑战，我们提出了一个连续的动态图模型来捕捉细粒度信息，突破了现有离散方法的限制。具体地，我们采用了一个消息传递框架结合位置特征来获取边的嵌入，通过解码来识别异常。对于第二个挑战，我们通过引入一个负采样模块来构造优秀的负样本，从而提高无监督学习的性能。

    Anomaly detection on dynamic graphs refers to detecting entities whose behaviors obviously deviate from the norms observed within graphs and their temporal information. This field has drawn increasing attention due to its application in finance, network security, social networks, and more. However, existing methods face two challenges: dynamic structure constructing challenge - difficulties in capturing graph structure with complex time information and negative sampling challenge - unable to construct excellent negative samples for unsupervised learning. To address these challenges, we propose Unsupervised Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first challenge, we propose a continuous dynamic graph model to capture the fine-grained information, which breaks the limit of existing discrete methods. Specifically, we employ a message-passing framework combined with positional features to get edge embeddings, which are decoded to identify anomalies. For the sec
    
[^57]: DyExplainer: 可解释的动态图神经网络

    DyExplainer: Explainable Dynamic Graph Neural Networks. (arXiv:2310.16375v1 [cs.LG])

    [http://arxiv.org/abs/2310.16375](http://arxiv.org/abs/2310.16375)

    DyExplainer是一个新颖的方法，用于解释动态图神经网络。它训练一个动态GNNs骨架，能够提取每个快照中图的表示，并且同时进行探索，以有效捕捉时间依赖性和结构关系。

    

    图神经网络(GNNs)因其能从图结构数据中捕捉表示的能力而重新兴起为一个研究热点。然而，GNNs的黑盒特性在理解和信任这些模型方面存在着显著挑战，从而限制了它们在关键任务场景中的实际应用。虽然近年来在解释GNNs领域取得了显著进展，但其中大部分研究都集中在静态图上，对动态GNNs的解释研究相对较少。动态GNNs以其不断演化的图结构提出了独特的挑战，并需要额外的努力来有效捕捉时间依赖性和结构关系。为了解决这一挑战，我们提出了DyExplainer，一种新颖的动态GNNs解释方法。DyExplainer训练一个动态GNNs骨架，在每个快照中提取图的表示，并同时进行探索。

    Graph Neural Networks (GNNs) resurge as a trending research subject owing to their impressive ability to capture representations from graph-structured data. However, the black-box nature of GNNs presents a significant challenge in terms of comprehending and trusting these models, thereby limiting their practical applications in mission-critical scenarios. Although there has been substantial progress in the field of explaining GNNs in recent years, the majority of these studies are centered on static graphs, leaving the explanation of dynamic GNNs largely unexplored. Dynamic GNNs, with their ever-evolving graph structures, pose a unique challenge and require additional efforts to effectively capture temporal dependencies and structural relationships. To address this challenge, we present DyExplainer, a novel approach to explaining dynamic GNNs on the fly. DyExplainer trains a dynamic GNN backbone to extract representations of the graph at each snapshot, while simultaneously exploring st
    
[^58]: 通过克拉默沃尔德距离进行联合分布学习

    Joint Distributional Learning via Cramer-Wold Distance. (arXiv:2310.16374v1 [cs.LG])

    [http://arxiv.org/abs/2310.16374](http://arxiv.org/abs/2310.16374)

    本文引入了克拉默沃尔德距离正则化，以更好地处理高维数据集和观测变量之间的复杂相关结构，并通过两步学习方法提高了先验建模的灵活性和聚合后验与先验分布之间的对齐度。

    

    在处理高维数据集或观测变量之间复杂相关结构时，基于条件独立性的假设在变分自编码器（VAE）解码器建模中具有局限性。为解决这个问题，我们引入了克拉默沃尔德距离正则化，可以通过闭合形式计算，以促进高维数据集的联合分布学习。此外，我们引入了一个两步学习方法，以实现灵活的先验建模，并提高聚合后验与先验分布之间的对齐度。此外，我们从理论上对该类别中的现有方法进行了区分。为了评估我们提出的方法在合成数据生成方面的性能，我们在具有多个类别变量的高维数据集上进行了实验。考虑到许多现有的数据集和数据科学应用涉及此类数据集，我们的实验有着重要意义。

    The assumption of conditional independence among observed variables, primarily used in the Variational Autoencoder (VAE) decoder modeling, has limitations when dealing with high-dimensional datasets or complex correlation structures among observed variables. To address this issue, we introduced the Cramer-Wold distance regularization, which can be computed in a closed-form, to facilitate joint distributional learning for high-dimensional datasets. Additionally, we introduced a two-step learning method to enable flexible prior modeling and improve the alignment between the aggregated posterior and the prior distribution. Furthermore, we provide theoretical distinctions from existing methods within this category. To evaluate the synthetic data generation performance of our proposed approach, we conducted experiments on high-dimensional datasets with multiple categorical variables. Given that many readily available datasets and data science applications involve such datasets, our experime
    
[^59]: 受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析

    Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])

    [http://arxiv.org/abs/2310.16363](http://arxiv.org/abs/2310.16363)

    本文研究了受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析，证明了这些算法能找到性能函数的一阶稳定点，并且具有较低的样本复杂度。

    

    Actor Critic方法在广泛的强化学习任务中找到了巨大的应用，特别是当状态-动作空间很大的时候。本文考虑使用函数逼近的actor critic和natural actor critic算法来处理涉及不等式约束的马尔可夫决策过程（C-MDP），并在非 i.i.d（马尔可夫）环境中进行了非渐近分析。我们考虑长期平均成本准则，其中目标和约束函数都是某些规定成本函数的适当策略依赖的长期平均。我们使用拉格朗日乘子法处理不等式约束。我们证明这些算法保证能找到性能（拉格朗日）函数$L(\theta,\gamma)$的一阶稳定点（即$\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$），并且其样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.5})$。

    Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of both C
    
[^60]: 基于神经潜力场的考虑障碍物的局部运动规划

    Neural Potential Field for Obstacle-Aware Local Motion Planning. (arXiv:2310.16362v1 [cs.RO])

    [http://arxiv.org/abs/2310.16362](http://arxiv.org/abs/2310.16362)

    提出了一种基于神经网络的局部运动规划方法，通过神经潜力场模型返回可微分的碰撞成本，利用神经图像编码器将问题维度降低两个数量级，实验结果表明与现有方法相当。

    

    模型预测控制（MPC）可能为移动机器人平台提供局部运动规划。挑战在于当障碍物地图和机器人足迹都是任意的情况下，对碰撞成本进行分析表示。我们提出了一种神经潜力场：一个神经网络模型，根据机器人姿态、障碍物地图和机器人足迹返回可微分的碰撞成本。我们模型的可微性允许其在MPC求解器中使用。对于具有非常多参数的问题，计算上非常困难。因此，我们的架构包括神经图像编码器，将障碍物地图和机器人足迹转化为嵌入，从而将问题的维度降低两个数量级。网络训练的参考数据是基于算法计算的有符号距离函数。比较实验表明，所提出的方法与现有的局部规划器相当：它提供轨迹...

    Model predictive control (MPC) may provide local motion planning for mobile robotic platforms. The challenging aspect is the analytic representation of collision cost for the case when both the obstacle map and robot footprint are arbitrary. We propose a Neural Potential Field: a neural network model that returns a differentiable collision cost based on robot pose, obstacle map, and robot footprint. The differentiability of our model allows its usage within the MPC solver. It is computationally hard to solve problems with a very high number of parameters. Therefore, our architecture includes neural image encoders, which transform obstacle maps and robot footprints into embeddings, which reduce problem dimensionality by two orders of magnitude. The reference data for network training are generated based on algorithmic calculation of a signed distance function. Comparative experiments showed that the proposed approach is comparable with existing local planners: it provides trajectories w
    
[^61]: Redco:一个轻量级工具，可在任何GPU/TPUs上自动化分布式训练LLMs

    Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])

    [http://arxiv.org/abs/2310.16355](http://arxiv.org/abs/2310.16355)

    Redco是一个轻量级工具，旨在自动化分布式训练LLMs，并简化ML流程的开发。

    

    人工智能的最新进展主要归功于大型语言模型（LLMs）。然而，它们不断增长的内存需求给机器学习（ML）研究人员和工程师带来了挑战。解决这个问题需要开发人员将大型模型分区以分布在多个GPU或TPU上。这需要使用现有模型并行工具（如Megatron-LM、DeepSpeed和Alpa）进行相当的编码和复杂的配置工作。这些工具需要用户具备机器学习系统（MLSys）的专业知识，给LLM开发带来了瓶颈，特别是对于没有MLSys背景的开发人员。在这项工作中，我们提出了Redco，这是一个轻量级且用户友好的工具，旨在自动化LLMs的分布式训练和推理，以及简化ML流程的开发。Redco的设计强调了两个关键方面。首先，为了自动化模型并行，我们的研究确定了两个简单的规则，用于为任何GPU / TPU生成张量并行策略。

    The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any g
    
[^62]: 带有流匹配的语音生成预训练

    Generative Pre-training for Speech with Flow Matching. (arXiv:2310.16338v1 [eess.AS])

    [http://arxiv.org/abs/2310.16338](http://arxiv.org/abs/2310.16338)

    本文展示了一种使用流匹配的预训练生成模型，该模型可以适应不同的下游任务并获得强大的性能，通过在60k小时的未转录语音上进行预训练，该模型可以与现有的专家模型在语音增强、分离和合成方面进行匹配或超越。

    

    近年来，生成模型在需要估计和抽样数据分布以生成高保真合成数据的任务中取得了显著的成功，因此越来越受到关注。在语音领域，文本到语音合成和神经声码器是生成模型成功应用的典型例子。尽管生成模型已经在语音的不同应用中得到了应用，但还没有一个通用的生成模型可以直接建模语音。在本文中，我们通过展示单一的预训练生成模型可以适应不同的下游任务并获得强大的性能，迈出了这个方向的一步。具体来说，我们使用流匹配和蒙版条件在60k小时的未转录语音上预训练了一个名为SpeechFlow的生成模型。实验结果表明，预训练的生成模型可以通过特定任务数据进行微调，以在语音增强、分离和合成方面达到或超过现有的专家模型的性能。

    Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested 
    
[^63]: SMURF-THP：基于分数匹配的Transformer Hawkes过程不确定性量化研究

    SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process. (arXiv:2310.16336v1 [cs.LG])

    [http://arxiv.org/abs/2310.16336](http://arxiv.org/abs/2310.16336)

    提出了SMURF-THP方法来学习Transformer Hawkes过程并量化预测不确定性。通过学习到的分数函数，可以从预测分布中采样事件到达时间，并计算置信区间来量化不确定性。

    

    Transformer Hawkes过程模型在建模事件序列数据方面取得了成功。然而，大部分现有的训练方法都依赖于最大化事件序列的似然性，这涉及到一些难以计算的积分。此外，现有方法无法为模型预测提供不确定性量化，例如对预测事件到达时间的置信区间。为了解决这些问题，我们提出了SMURF-THP，一种基于分数匹配的方法，用于学习Transformer Hawkes过程并量化预测的不确定性。具体而言，SMURF-THP通过避免难以计算的积分，学习事件到达时间的分数函数。通过这样一个学习的分数函数，我们可以从预测分布中采样事件到达时间。这自然地通过计算生成的样本上的置信区间，实现了不确定性的量化。我们在事件类型预测方面进行了大量实验证明了该方法的有效性。

    Transformer Hawkes process models have shown to be successful in modeling event sequence data. However, most of the existing training methods rely on maximizing the likelihood of event sequences, which involves calculating some intractable integral. Moreover, the existing methods fail to provide uncertainty quantification for model predictions, e.g., confidence intervals for the predicted event's arrival time. To address these issues, we propose SMURF-THP, a score-based method for learning Transformer Hawkes process and quantifying prediction uncertainty. Specifically, SMURF-THP learns the score function of events' arrival time based on a score-matching objective that avoids the intractable computation. With such a learned score function, we can sample arrival time of events from the predictive distribution. This naturally allows for the quantification of uncertainty by computing confidence intervals over the generated samples. We conduct extensive experiments in both event type predic
    
[^64]: 防御推荐系统模型抽取攻击

    Defense Against Model Extraction Attacks on Recommender Systems. (arXiv:2310.16335v1 [cs.LG])

    [http://arxiv.org/abs/2310.16335](http://arxiv.org/abs/2310.16335)

    本文引入了基于梯度的排序优化（GRO）作为第一个防御推荐系统模型抽取攻击的策略，最小化受保护的目标模型的损失，同时最大化攻击者的代理模型的损失。

    

    推荐系统的鲁棒性已成为研究界的突出问题。已经提出了许多敌对攻击方法，但大多数依赖于广泛的先验知识，如所有的白盒攻击或大部分黑盒攻击都假设某些外部知识可用。在这些攻击中，模型抽取攻击是一种有前景且实用的方法，涉及通过反复查询目标模型来训练代理模型。然而，现有文献在防御推荐系统模型抽取攻击方面存在显著差距。在本文中，我们引入了基于梯度的排序优化（GRO）作为第一个针对此类攻击的防御策略。我们将防御形式化为一个优化问题，旨在最小化受保护的目标模型的损失，同时最大化攻击者的代理模型的损失。由于前k个排序列表是不可微分的，并且问题的复杂度很高。

    The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differ
    
[^65]: 污染神经元解释深度视觉特征

    Corrupting Neuron Explanations of Deep Visual Features. (arXiv:2310.16332v1 [cs.LG])

    [http://arxiv.org/abs/2310.16332](http://arxiv.org/abs/2310.16332)

    本文通过对神经元解释方法进行鲁棒性分析，发现这些解释可以被随机噪声和精心设计的扰动严重破坏，即使添加小的随机噪声也可以改变高达28％的神经元的概念分配。此外，作者还设计了一种新的污染算法，通过污染不到10％的探测数据可以操纵超过80％的神经元的解释。这引发了在现实生活中对神经元解释方法的信任问题。

    

    深度神经网络无法解释其黑盒行为的能力，导致近来出现了大量解释性方法。然而，人们越来越担心这些解释性方法缺乏稳健性和可靠性。在这项工作中，我们首次在一个统一的流程下对神经元解释方法进行了鲁棒性分析，并展示了这些解释可以被随机噪声和精心设计的扰动所严重破坏。我们发现，即使添加标准差为0.02的小随机噪声，也可以改变深层中高达28%的神经元所分配的概念。此外，我们设计了一种新颖的污染算法，并证明我们的算法可以通过污染不到10%的探测数据来操纵超过80%的神经元的解释。这引发了在现实生活中对神经元解释方法的信任问题，特别是对于涉及安全和公平重要应用的情况。

    The inability of DNNs to explain their black-box behavior has led to a recent surge of explainability methods. However, there are growing concerns that these explainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Explanation Methods under a unified pipeline and show that these explanations can be significantly corrupted by random noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explanation of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron Explanation Methods in real-life safety and fairness critical applications.
    
[^66]: 基于可调动力学和短期可塑性的类脑储层计算的记忆电阻器

    Brain-Inspired Reservoir Computing Using Memristors with Tunable Dynamics and Short-Term Plasticity. (arXiv:2310.16331v1 [cs.LG])

    [http://arxiv.org/abs/2310.16331](http://arxiv.org/abs/2310.16331)

    这项研究提出了一种使用具有可调动力学和短期可塑性的记忆电阻器进行类脑储层计算的方法。使用这种方法可以实现更快的信息处理速度、更低的能耗和更小的面积占用。研究人员通过对输入数据进行编码，使得记忆电阻器的动力学能够自适应地调整，克服了先前实现中的限制。

    

    最近对储层计算研究的先进进展促使人们需要具有可促进储层物理实现的动力学的模拟设备，以实现更快的信息处理速度，同时消耗较少能量和占用更小的面积。研究表明，具有非线性和短期记忆动力学的动态记忆电阻器，是用于时间分类和预测任务的信息处理设备或储层的理想选择。以往的实现依赖于名义上相同的记忆电阻器对输入数据应用相同的非线性变换，这不足以实现丰富的状态空间。为了解决这个限制，研究人员要么在多个记忆电阻器之间扩展数据编码，要么利用记忆电阻器之间的随机设备变异性。然而，这种方法需要额外的预处理步骤，并且会导致同步问题。因此，最好的解决方案是对输入数据进行编码，使得记忆电阻器的动力学能够自适应地调整。

    Recent advancements in reservoir computing research have created a demand for analog devices with dynamics that can facilitate the physical implementation of reservoirs, promising faster information processing while consuming less energy and occupying a smaller area footprint. Studies have demonstrated that dynamic memristors, with nonlinear and short-term memory dynamics, are excellent candidates as information-processing devices or reservoirs for temporal classification and prediction tasks. Previous implementations relied on nominally identical memristors that applied the same nonlinear transformation to the input data, which is not enough to achieve a rich state space. To address this limitation, researchers either diversified the data encoding across multiple memristors or harnessed the stochastic device-to-device variability among the memristors. However, this approach requires additional pre-processing steps and leads to synchronization issues. Instead, it is preferable to encod
    
[^67]: 基于再采样的SBM图动态的强化学习

    Reinforcement Learning for SBM Graphon Games with Re-Sampling. (arXiv:2310.16326v1 [cs.GT])

    [http://arxiv.org/abs/2310.16326](http://arxiv.org/abs/2310.16326)

    本文研究了基于再采样的SBM图动态的强化学习问题，并提出了一种新的学习框架。我们证明了策略镜像上升算法能够找到多种群均场博弈的纳什均衡，并提出了一种高效的样本集成的强化学习算法来解决实际情况下随机块模型未知的情况。

    

    均场近似是研究大规模人口动态的一种可行方法。然而，它对同质性和所有代理之间的通用连接的假设限制了其在许多实际场景中的适用性。为了解决这些限制，文献中引入了多种群均场博弈模型。当已知基本的随机块模型时，我们证明了一个策略镜像上升算法找到了多种群均场博弈的纳什均衡。在更现实的情况下，即随机块模型未知的情况下，我们提出了从图动态集成与有限的N-player多群均场博弈模型的再采样方案。我们基于图动态与再采样的Game (GGR-S)模型开发了一种新颖的学习框架，该模型捕捉了代理之间的复杂网络结构。我们分析了GGR-S动态并建立了与多种群均场博弈动态的收敛性。利用这个结果，我们提出了一种高效的基于样本的N-player强化学习算法用于GGR-S。

    The Mean-Field approximation is a tractable approach for studying large population dynamics. However, its assumption on homogeneity and universal connections among all agents limits its applicability in many real-world scenarios. Multi-Population Mean-Field Game (MP-MFG) models have been introduced in the literature to address these limitations. When the underlying Stochastic Block Model is known, we show that a Policy Mirror Ascent algorithm finds the MP-MFG Nash Equilibrium. In more realistic scenarios where the block model is unknown, we propose a re-sampling scheme from a graphon integrated with the finite N-player MP-MFG model. We develop a novel learning framework based on a Graphon Game with Re-Sampling (GGR-S) model, which captures the complex network structures of agents' connections. We analyze GGR-S dynamics and establish the convergence to dynamics of MP-MFG. Leveraging this result, we propose an efficient sample-based N-player Reinforcement Learning algorithm for GGR-S wit
    
[^68]: 个性化的联邦多臂赌博问题研究

    Personalized Federated X -armed Bandit. (arXiv:2310.16323v1 [stat.ML])

    [http://arxiv.org/abs/2310.16323](http://arxiv.org/abs/2310.16323)

    本文对个性化的联邦多臂赌博问题进行了研究，提出了PF-PNE算法，通过双重淘汰策略和有效的本地目标评估方法，实现了同时优化异质本地目标和鼓励联邦合作，该算法在多个基线算法和实验数据集上都表现出较好的性能。

    

    本文研究了个性化的联邦多臂赌博问题，其中在联邦学习范式中同时优化了客户端的异质本地目标。我们提出了具有独特双重淘汰策略的PF-PNE算法，通过偏差但有效的本地目标评估，安全地消除非最优区域同时鼓励联邦合作。所提出的PF-PNE算法能够优化具有任意异质性水平的本地目标，并且其有限通信保护了客户端奖励数据的机密性。理论分析表明所提出的算法相对于单客户算法的优势。在实验中，PF-PNE在合成和真实数据集上都优于多个基线算法。

    In this work, we study the personalized federated $\mathcal{X}$-armed bandit problem, where the heterogeneous local objectives of the clients are optimized simultaneously in the federated learning paradigm. We propose the \texttt{PF-PNE} algorithm with a unique double elimination strategy, which safely eliminates the non-optimal regions while encouraging federated collaboration through biased but effective evaluations of the local objectives. The proposed \texttt{PF-PNE} algorithm is able to optimize local objectives with arbitrary levels of heterogeneity, and its limited communications protects the confidentiality of the client-wise reward data. Our theoretical analysis shows the benefit of the proposed algorithm over single-client algorithms. Experimentally, \texttt{PF-PNE} outperforms multiple baselines on both synthetic and real life datasets.
    
[^69]: 增强低精度采样：随机梯度Hamiltonian Monte Carlo

    Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])

    [http://arxiv.org/abs/2310.16320](http://arxiv.org/abs/2310.16320)

    本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。

    

    低精度训练已经成为一种有前景的低成本技术，可以在不牺牲太多准确性的情况下提高深度神经网络的训练效率。其贝叶斯对应物可以进一步提供不确定性量化和改进的泛化准确性。本文研究了在强对数凹和非对数凹分布下，使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)。从理论上讲，我们的结果表明，为了在非对数凹分布下实现2-Wasserstein距离的ε误差，低精度SGHMC相对于低精度采样器（随机梯度Langevin动力学，SGLD）实现了二次改进（$\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$ vs $\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$）。另外，基于真实数据集的实验证明了低精度SGHMC相对于SGLD在非对数凹分布下的优越性。

    Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$). Moreo
    
[^70]: 一种与语言形式无关的元学习蒙版自监督学习方法

    Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder. (arXiv:2310.16318v1 [cs.LG])

    [http://arxiv.org/abs/2310.16318](http://arxiv.org/abs/2310.16318)

    本文提出了一种与语言形式无关的元学习蒙版自监督学习框架MetaMAE，通过将蒙版自编码器（MAE）的蒙版重构视为元学习任务，并采用转换器元学习技术来改进MAE的自监督学习在不同语言形式上的表现。

    

    尽管自监督学习在各种语言形式中具有实际重要性，但近期的研究主要集中在少数经过精选的领域，如视觉和语言，并且常常依赖于特定领域的知识。本文中，我们将蒙版自编码器（MAE）作为一个统一的、与语言形式无关的自监督学习框架进行了拓展。我们认为元学习是解释MAE作为与语言形式无关学习器的关键，并从共同提高其在多种语言形式上的自监督学习能力的动机出发，提出了MetaMAE框架。我们的核心思想是将MAE的蒙版重构视为一个元学习任务：通过对未蒙版标记进行自适应来预测蒙版标记，从而通过转换器元学习实现对其进行总误差减小。基于这一新的解释，我们提出了将两种高级元学习技术结合的方法。

    Despite its practical importance across a wide range of modalities, recent advances in self-supervised learning (SSL) have been primarily focused on a few well-curated domains, e.g., vision and language, often relying on their domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become one of the popular architectures in these domains, but less has explored its potential in other modalities. In this paper, we develop MAE as a unified, modality-agnostic SSL framework. In turn, we argue meta-learning as a key to interpreting MAE as a modality-agnostic learner, and propose enhancements to MAE from the motivation to jointly improve its SSL across diverse modalities, coined MetaMAE as a result. Our key idea is to view the mask reconstruction of MAE as a meta-learning task: masked tokens are predicted by adapting the Transformer meta-learner through the amortization of unmasked tokens. Based on this novel interpretation, we propose to integrate two advanced meta-learning tec
    
[^71]: Sum-of-Parts模型：对特征组的忠实归因

    Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])

    [http://arxiv.org/abs/2310.16316](http://arxiv.org/abs/2310.16316)

    Sum-of-Parts模型通过构造保证特征组归因的忠实性，将预测分解为可解释的分数之和，帮助天体物理学家发现了关于星系形成的新知识。

    

    如果机器学习模型的解释准确反映了其决策过程，则被认为是“忠实”的解释。然而，例如深度学习的特征归因等解释并不能保证忠实，有可能产生具有误导性的解释。在这项工作中，我们开发了Sum-of-Parts（SOP）模型，它是一类模型，其预测具有通过构造保证忠实的特征组归因。该模型将预测分解为可解释的分数之和，每个分数直接归因于一组稀疏特征。我们使用标准可解释性指标对SOP进行评估，并在一个案例研究中，利用SOP提供的忠实解释帮助天体物理学家发现了关于星系形成的新知识。

    An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
    
[^72]: 了解代码语义: 对Transformer模型在摘要中的评估

    Understanding Code Semantics: An Evaluation of Transformer Models in Summarization. (arXiv:2310.16314v1 [cs.LG])

    [http://arxiv.org/abs/2310.16314](http://arxiv.org/abs/2310.16314)

    本研究通过评估代码摘要的有效性和引入对抗性案例，研究了基于Transformer模型的代码理解能力。结果显示，模型在理解代码语义方面仍存在挑战，这对于提高软件开发和维护效率具有重要意义。

    

    本文使用先进的基于Transformer的语言模型深入研究了代码摘要的复杂性。通过实证研究，我们通过改变函数和变量名来评估代码摘要的有效性，以探索模型是否真正理解代码语义，还是仅仅依赖文本线索。我们还引入了死代码和注释代码等对抗性案例，涵盖了三种编程语言(Python、Javascript和Java)，以进一步审查模型的理解能力。最终，我们的研究旨在提供有价值的见解，加强Transformer模型理解代码的能力，从而为更高效的软件开发实践和维护工作流程做出贡献。

    This paper delves into the intricacies of code summarization using advanced transformer-based language models. Through empirical studies, we evaluate the efficacy of code summarization by altering function and variable names to explore whether models truly understand code semantics or merely rely on textual cues. We have also introduced adversaries like dead code and commented code across three programming languages (Python, Javascript, and Java) to further scrutinize the model's understanding. Ultimately, our research aims to offer valuable insights into the inner workings of transformer-based LMs, enhancing their ability to understand code and contributing to more efficient software development practices and maintenance workflows.
    
[^73]: 基于分数匹配的神经标记时空点过程的伪似然度量与不确定性量化

    Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process with Uncertainty Quantification. (arXiv:2310.16310v1 [cs.LG])

    [http://arxiv.org/abs/2310.16310](http://arxiv.org/abs/2310.16310)

    提出了SMASH方法，基于分数匹配的伪似然度量，用于学习具有不确定性量化的标记时空点过程（STPPs）。该方法解决了现有方法中概率密度函数归一化的问题，并提供了模型预测的不确定性量化。

    

    时空点过程（STPPs）是建模和预测具有时间和空间特征的事件的强大数学工具。尽管它们的多功能性，但大多数现有的学习STPPs的方法要么假设了一种受限的时空分布形式，要么由于复杂积分的不准确近似而遭受了问题。这些问题通常来自概率密度函数的归一化项。此外，当前的技术无法为模型预测提供不确定性量化，例如预测事件到达时间的置信区间和事件位置的置信区域，这在考虑到数据的相当随机性的情况下是关键的。为了解决这些挑战，我们引入了SMASH：一种基于分数匹配的伪似然度量，用于学习具有不确定性量化的标记STPPs。具体来说，我们的框架通过估计来自标记数据的采样预测分布来采用无归一化的目标。然后，我们使用分数匹配从模型分布中进行采样并最小化与真实分布之间的分数距离。

    Spatio-temporal point processes (STPPs) are potent mathematical tools for modeling and predicting events with both temporal and spatial features. Despite their versatility, most existing methods for learning STPPs either assume a restricted form of the spatio-temporal distribution, or suffer from inaccurate approximations of the intractable integral in the likelihood training objective. These issues typically arise from the normalization term of the probability density function. Moreover, current techniques fail to provide uncertainty quantification for model predictions, such as confidence intervals for the predicted event's arrival time and confidence regions for the event's location, which is crucial given the considerable randomness of the data. To tackle these challenges, we introduce SMASH: a Score MAtching-based pSeudolikeliHood estimator for learning marked STPPs with uncertainty quantification. Specifically, our framework adopts a normalization-free objective by estimating the
    
[^74]: Dolfin: 无自编码器的扩散布局变换器

    Dolfin: Diffusion Layout Transformers without Autoencoder. (arXiv:2310.16305v1 [cs.CV])

    [http://arxiv.org/abs/2310.16305](http://arxiv.org/abs/2310.16305)

    Dolfin是一种改进的布局生成模型，通过Transformer-based扩散过程和自回归扩散模型(Dolfin-AR)实现，相比现有方法具有更高的建模能力和较低的复杂性。

    

    本文介绍了一种新的生成模型，称为无自编码器的扩散布局变换器（Dolfin），相比现有方法，它在减小复杂性的同时显著提高了建模能力。 Dolfin采用基于Transformer的扩散过程来进行布局生成。除了高效的双向（非因果连接）序列表示外，我们还提出了一种自回归扩散模型（Dolfin-AR），特别擅长捕捉邻近对象的丰富语义相关性，如对齐、大小和重叠。在标准生成布局基准测试中，Dolfin在各种指标（fid、对齐、重叠、MaxIoU和DocSim分数）上显著提高性能，增强了透明度和互操作性。此外，Dolfin的应用不仅局限于布局生成，还适用于建模几何结构，如线段。

    In this paper, we introduce a novel generative model, Diffusion Layout Transformers without Autoencoder (Dolfin), which significantly improves the modeling capability with reduced complexity compared to existing methods. Dolfin employs a Transformer-based diffusion process to model layout generation. In addition to an efficient bi-directional (non-causal joint) sequence representation, we further propose an autoregressive diffusion model (Dolfin-AR) that is especially adept at capturing rich semantic correlations for the neighboring objects, such as alignment, size, and overlap. When evaluated against standard generative layout benchmarks, Dolfin notably improves performance across various metrics (fid, alignment, overlap, MaxIoU and DocSim scores), enhancing transparency and interoperability in the process. Moreover, Dolfin's applications extend beyond layout generation, making it suitable for modeling geometric structures, such as line segments. Our experiments present both qualitati
    
[^75]: 多无人机网络的低成本不完美数字孪生辅助强化训练

    Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks. (arXiv:2310.16302v1 [cs.LG])

    [http://arxiv.org/abs/2310.16302](http://arxiv.org/abs/2310.16302)

    本论文提出了一种低成本的不完美数字孪生辅助强化训练方法，通过引入数字孪生技术来减少实际训练的成本，并通过自然和虚拟生成的无人机混合部署方法来平衡训练成本、数字孪生建设成本和数字孪生偏差对训练的影响。

    

    深度强化学习（DRL）被广泛用于优化多无人机网络的性能。然而，DRL的训练依赖于无人机与环境之间的频繁交互，在实际实验中，由于无人机的飞行和通信，这消耗了大量能源。受数字孪生（DT）技术的启发，该技术可以通过复制物理空间的特征构建数字空间来模拟算法的性能，我们引入了DT来降低实际训练的成本，例如能源和硬件购买。与先前的DT辅助工作不同的是，我们考虑了一个偏离真实物理情况的不完美DT模型来辅助多无人机网络的训练。值得注意的是，为了权衡训练成本、DT构建成本和DT偏差对训练的影响，我们提出了一种自然和虚拟生成的无人机混合部署方法。

    Deep Reinforcement Learning (DRL) is widely used to optimize the performance of multi-UAV networks. However, the training of DRL relies on the frequent interactions between the UAVs and the environment, which consumes lots of energy due to the flying and communication of UAVs in practical experiments. Inspired by the growing digital twin (DT) technology, which can simulate the performance of algorithms in the digital space constructed by coping features of the physical space, the DT is introduced to reduce the costs of practical training, e.g., energy and hardware purchases. Different from previous DT-assisted works with an assumption of perfect reflecting real physics by virtual digital, we consider an imperfect DT model with deviations for assisting the training of multi-UAV networks. Remarkably, to trade off the training cost, DT construction cost, and the impact of deviations of DT on training, the natural and virtually generated UAV mixing deployment method is proposed. Two cascad
    
[^76]: 对于模型解释的神经网络实例化线性化

    Instance-wise Linearization of Neural Network for Model Interpretation. (arXiv:2310.16295v1 [cs.LG])

    [http://arxiv.org/abs/2310.16295](http://arxiv.org/abs/2310.16295)

    这项研究提出了一种实例化线性化的方法，用于解释神经网络模型。通过给模型内部的每个输入特征分配重要得分，揭示了模型如何使用特征做出决策。这种方法有助于解决当前特征归因方法中的局限性，并提高了模型解释的准确性。

    

    神经网络在许多科学领域取得了显著的成功。然而，神经网络模型的可解释性仍然是将这种技术应用于我们日常生活的主要瓶颈。挑战在于神经网络的非线性行为，这提出了一个关键性问题，即模型如何使用输入特征进行决策。解决这一挑战的经典方法是特征归因，它为每个输入特征分配一个重要得分，并揭示其对当前预测的重要性。然而，当前的特征归因方法经常指示每个输入特征的重要性，而没有详细说明它们在模型内部实际上是如何处理的。这些归因方法常常引发一个关注点，即它们是否正确地强调了模型预测的特征。对于神经网络模型，非线性行为通常是由模型的非线性激活单元引起的。然而，预测的计算行为往往是复杂的，这使得解释和理解模型的决策变得困难。

    Neural network have achieved remarkable successes in many scientific fields. However, the interpretability of the neural network model is still a major bottlenecks to deploy such technique into our daily life. The challenge can dive into the non-linear behavior of the neural network, which rises a critical question that how a model use input feature to make a decision. The classical approach to address this challenge is feature attribution, which assigns an important score to each input feature and reveal its importance of current prediction. However, current feature attribution approaches often indicate the importance of each input feature without detail of how they are actually processed by a model internally. These attribution approaches often raise a concern that whether they highlight correct features for a model prediction.  For a neural network model, the non-linear behavior is often caused by non-linear activation units of a model. However, the computation behavior of a predict
    
[^77]: Crowd-Certain: 众包和集成学习分类中的标签聚合方法

    Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification. (arXiv:2310.16293v1 [cs.LG])

    [http://arxiv.org/abs/2310.16293](http://arxiv.org/abs/2310.16293)

    Crowd-Certain是一种在众包和集成学习分类中进行标签聚合的新方法，通过比较标注者的一致性与训练好的分类器，确定每个标注者的可靠性得分，并利用预测概率实现了对未来样本数据的重复使用，从而提高了性能和计算效率。

    

    众包系统已被用于积累大规模标记数据，用于计算机视觉和自然语言处理等应用。然而，由于众包标记的动态性和不确定性，开发一个在大多数情况下都能工作的技术是非常具有挑战性的。在本文中，我们介绍了一种名为Crowd-Certain的新颖方法，用于众包和集成学习分类任务中的标签聚合，该方法在不同的标注者数量和各种数据集上能够提供改进的性能和计算效率。所提出的方法使用标注者的一致性与训练好的分类器进行比较，为每个标注者确定可靠性得分。此外，Crowd-Certain利用预测概率，使得训练好的分类器可以在未来的样本数据上重复使用，从而消除了现有方法中固有的重复模拟过程。我们对我们的方法进行了广泛的评估，与其他十种现有方法进行了对比。

    Crowdsourcing systems have been used to accumulate massive amounts of labeled data for applications such as computer vision and natural language processing. However, because crowdsourced labeling is inherently dynamic and uncertain, developing a technique that can work in most situations is extremely challenging. In this paper, we introduce Crowd-Certain, a novel approach for label aggregation in crowdsourced and ensemble learning classification tasks that offers improved performance and computational efficiency for different numbers of annotators and a variety of datasets. The proposed method uses the consistency of the annotators versus a trained classifier to determine a reliability score for each annotator. Furthermore, Crowd-Certain leverages predicted probabilities, enabling the reuse of trained classifiers on future sample data, thereby eliminating the need for recurrent simulation processes inherent in existing methods. We extensively evaluated our approach against ten existing
    
[^78]: 用扩散模型去除CMB观测中的尘埃

    Removing Dust from CMB Observations with Diffusion Models. (arXiv:2310.16285v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.16285](http://arxiv.org/abs/2310.16285)

    本文研究了使用扩散模型去除CMB观测中的尘埃的方法，并发现扩散模型可以很好地模拟尘埃前景并进行成分分离。我们还引入了一种与CMB宇宙学条件相结合的模型，在成分分离中表现出更好的性能。

    

    在宇宙学中，追寻宇宙微波背景(CMB)观测中的原始$B$-mode，突出了对银河尘埃前景的精确建模的关键需求。我们研究了基于扩散的尘埃前景建模及其在成分分离中的意义。在假设具有已知宇宙学(或协方差矩阵)的高斯CMB的前提下，我们证明了扩散模型可以在尘埃辐射地图的实例上进行训练，使其采样过程直接与成分分离上下文中的后验采样相一致。我们用模拟的尘埃辐射和CMB的混合物来说明这一点。我们展示了通过这个过程可以很好地恢复组分的常见摘要统计量(功率谱、闵可夫斯基函数)。我们还引入了与CMB宇宙学条件相结合的模型，比单一宇宙学训练的模型在成分分离中表现更好。这样的模型将在未来的工作中用于基于扩散的宇宙学推断。

    In cosmology, the quest for primordial $B$-modes in cosmic microwave background (CMB) observations has highlighted the critical need for a refined model of the Galactic dust foreground. We investigate diffusion-based modeling of the dust foreground and its interest for component separation. Under the assumption of a Gaussian CMB with known cosmology (or covariance matrix), we show that diffusion models can be trained on examples of dust emission maps such that their sampling process directly coincides with posterior sampling in the context of component separation. We illustrate this on simulated mixtures of dust emission and CMB. We show that common summary statistics (power spectrum, Minkowski functionals) of the components are well recovered by this process. We also introduce a model conditioned by the CMB cosmology that outperforms models trained using a single cosmology on component separation. Such a model will be used in future work for diffusion-based cosmological inference.
    
[^79]: Bayesian领域不变学习通过参数分布的后验泛化

    Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions. (arXiv:2310.16277v1 [cs.LG])

    [http://arxiv.org/abs/2310.16277](http://arxiv.org/abs/2310.16277)

    本研究通过学习网络参数的领域不变后验分布，提出了一种名为PosTerior Generalization的方法，能够更好地泛化到未见过的目标领域。

    

    领域不变学习旨在学习能够提取各种训练领域中不变特征的模型，从而更好地泛化到未见过的目标领域。最近，贝叶斯神经网络在领域不变学习方面取得了良好的结果，但大多数研究集中在对齐特征分布而不是参数分布。受到贝叶斯神经网络原理的启发，我们试图直接学习网络参数的领域不变后验分布。首先，我们提出了一个定理，表明可以通过聚合不同训练领域上的后验来隐式推断参数的不变后验。我们的假设更具宽松性，可以提取更多的领域不变信息。我们还提出了一种名为"PosTerior Generalization (PTG)"的简单而有效的方法，用于估计不变的参数分布。PTG充分利用了变分推断来近似参数分布。

    Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distribution
    
[^80]: 注意力镜头：一种解释注意力头信息检索机制的工具

    Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])

    [http://arxiv.org/abs/2310.16270](http://arxiv.org/abs/2310.16270)

    Attention Lens是一种工具，它能够通过学习的注意力头特定转换将注意力头的输出翻译为词汇标记。使用Attention Lens，我们可以解释注意力头在生成最终标记预测中的作用。注意力头在语言模型中扮演着高度专门化的角色。

    

    基于Transformer的大型语言模型(LLMs)是自然语言任务的最先进技术。最近的研究尝试通过逆向工程线性层的作用，解码LLMs为文本完成任务做出最终预测的内部机制。然而，关于注意力头在生成最终标记预测中的具体作用还知之甚少。我们提出了Attention Lens，一个工具，可以通过学习的注意力头特定转换(称为镜头)将注意力头的输出翻译为词汇标记。我们训练的镜头的初步发现表明，注意力头在语言模型中扮演着高度专门化的角色。Attention Lens的代码可在github.com/msakarvadia/AttentionLens上获得。

    Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.
    
[^81]: 一种因果解缠离散多粒度图分类方法

    A Causal Disentangled Multi-Granularity Graph Classification Method. (arXiv:2310.16256v1 [cs.LG])

    [http://arxiv.org/abs/2310.16256](http://arxiv.org/abs/2310.16256)

    这篇论文提出了一种因果解缠离散多粒度图分类方法（CDM-GNN），该方法能够解决图数据的多粒度特性，实现了对图中重要子结构和偏差部分的解析，并用于图分类任务中。

    

    实际生活中广泛存在大量数据和复杂结构的图数据。将图数据映射到低维嵌入空间是必要的。图分类是一项关键的图任务，主要依赖于识别图中的重要子结构。目前，一些图分类方法没有结合图数据的多粒度特性。建模中缺乏粒度区分导致模型中关键信息和虚假相关性混淆。因此，实现可信和可解释的模型的目标变得具有挑战性。本文提出了一种因果解缠离散多粒度图表示学习方法（CDM-GNN）来解决这个挑战。CDM-GNN模型从多粒度的角度对图中的重要子结构和偏差部分进行解缠离。CDM-GNN模型的解缠离揭示了重要和偏差部分，为其分类任务奠定了基础。

    Graph data widely exists in real life, with large amounts of data and complex structures. It is necessary to map graph data to low-dimensional embedding. Graph classification, a critical graph task, mainly relies on identifying the important substructures within the graph. At present, some graph classification methods do not combine the multi-granularity characteristics of graph data. This lack of granularity distinction in modeling leads to a conflation of key information and false correlations within the model. So, achieving the desired goal of a credible and interpretable model becomes challenging. This paper proposes a causal disentangled multi-granularity graph representation learning method (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the important substructures and bias parts within the graph from a multi-granularity perspective. The disentanglement of the CDM-GNN model reveals important and bias parts, forming the foundation for its classification task, spe
    
[^82]: Matrix Games中的近最优纯探索：随机赌徒与决斗赌徒的推广

    Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits & Dueling Bandits. (arXiv:2310.16252v1 [cs.LG])

    [http://arxiv.org/abs/2310.16252](http://arxiv.org/abs/2310.16252)

    本研究研究了具有噪声的两人零和矩阵游戏中纯策略纳什均衡的样本复杂度问题，设计了一个近最优算法，其样本复杂度与下界相匹配，同时解决了随机多臂赌博机和决斗赌卒中的纯探索问题。

    

    本研究针对具有噪声的两人零和矩阵游戏中，纯策略纳什均衡（PSNE）的样本复杂度进行了研究。在给定的随机模型中，任何学习器可以对输入矩阵A的某个元素（i，j）进行采样，并观察到A_{i，j} + \eta的值，其中\eta是一个零均值的1-子高斯噪声。学习器的目标是在尽可能少的采样次数下，以高概率确定A的PSNE。我们设计了一个近最优算法，其样本复杂度与下界相匹配，只有对数因子的差距。确定PSNE的问题也推广了随机多臂赌博机和决斗赌徒中的纯探索问题，我们的结果在这两个设置中与最优界限相匹配，只有对数因子的差距。

    We study the sample complexity of identifying the pure strategy Nash equilibrium (PSNE) in a two-player zero-sum matrix game with noise. Formally, we are given a stochastic model where any learner can sample an entry $(i,j)$ of the input matrix $A\in[-1,1]^{n\times m}$ and observe $A_{i,j}+\eta$ where $\eta$ is a zero-mean 1-sub-Gaussian noise. The aim of the learner is to identify the PSNE of $A$, whenever it exists, with high probability while taking as few samples as possible. Zhou et al. (2017) presents an instance-dependent sample complexity lower bound that depends only on the entries in the row and column in which the PSNE lies. We design a near-optimal algorithm whose sample complexity matches the lower bound, up to log factors. The problem of identifying the PSNE also generalizes the problem of pure exploration in stochastic multi-armed bandits and dueling bandits, and our result matches the optimal bounds, up to log factors, in both the settings.
    
[^83]: ZzzGPT: 提高睡眠质量的交互式GPT方法

    ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality. (arXiv:2310.16242v1 [cs.LG])

    [http://arxiv.org/abs/2310.16242](http://arxiv.org/abs/2310.16242)

    本文介绍了一种名为ZzzGPT的交互式GPT方法，旨在提高睡眠质量。通过利用大型语言模型进行预测和反馈，该方法融合了先进的机器学习和用户导向设计，以提供准确和有价值的结果。

    

    在当今世界中，睡眠质量对总体健康至关重要。虽然可穿戴传感器提供实时监测，但它们常常缺乏有针对性的见解，导致用户放弃使用。本文研究了技术在理解睡眠模式方面的作用。我们引入了一个两阶段的框架，利用大型语言模型 (LLMs)，旨在提供准确的睡眠预测和有价值的反馈。利用GLOBEM数据集和LLMs的合成数据，我们展示了与XGBoost等模型相比的增强结果。我们的方法将先进的机器学习与以用户为中心的设计相结合，将科学准确性与实用性融合在一起。

    In today's world, sleep quality is pivotal for overall well-being. While wearable sensors offer real-time monitoring, they often lack actionable insights, leading to user abandonment. This paper delves into the role of technology in understanding sleep patterns. We introduce a two-stage framework, utilizing Large Language Models (LLMs), aiming to provide accurate sleep predictions with actionable feedback. Leveraging the GLOBEM dataset and synthetic data from LLMs, we highlight enhanced results with models like XGBoost. Our approach merges advanced machine learning with user-centric design, blending scientific accuracy with practicality.
    
[^84]: 通过任务亲和力预测实现自动化多任务机器学习的任务分组

    Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction. (arXiv:2310.16241v1 [cs.LG])

    [http://arxiv.org/abs/2310.16241](http://arxiv.org/abs/2310.16241)

    本文提出了一种自动任务分组的方法，通过研究任务对多任务学习的亲和力以及任务的固有特征和单任务学习特性，实现了对任务的自动分组。

    

    当需要同时学习多个相似任务时，多任务学习（MTL）模型可以比单任务学习（STL）模型达到显著更高的准确性。然而，MTL的优势取决于各种因素，如任务的相似性、数据集的大小等；实际上，一些任务可能并不适合MTL，甚至与STL相比会导致准确性下降。因此，问题就出现了：应该将哪些任务一起学习？领域专家可以根据直觉、经验和最佳实践来尝试将任务分组在一起，但手动分组可能既耗时又远非最佳。在本文中，我们提出了一种新颖的自动任务分组方法。首先，我们使用四个在MTL文献中广泛使用的基准数据集研究了任务对MTL的亲和力，重点是基于神经网络的MTL模型。我们确定了任务的固有特征和STL特性，可以帮助我们预测一个任务组是否适合进行MTL。

    When a number of similar tasks have to be learned simultaneously, multi-task learning (MTL) models can attain significantly higher accuracy than single-task learning (STL) models. However, the advantage of MTL depends on various factors, such as the similarity of the tasks, the sizes of the datasets, and so on; in fact, some tasks might not benefit from MTL and may even incur a loss of accuracy compared to STL. Hence, the question arises: which tasks should be learned together? Domain experts can attempt to group tasks together following intuition, experience, and best practices, but manual grouping can be labor-intensive and far from optimal. In this paper, we propose a novel automated approach for task grouping. First, we study the affinity of tasks for MTL using four benchmark datasets that have been used extensively in the MTL literature, focusing on neural network-based MTL models. We identify inherent task features and STL characteristics that can help us to predict whether a gro
    
[^85]: 基于注意力机制的集成池化方法用于时间序列预测

    Attention-Based Ensemble Pooling for Time Series Forecasting. (arXiv:2310.16231v1 [cs.LG])

    [http://arxiv.org/abs/2310.16231](http://arxiv.org/abs/2310.16231)

    提出了一种基于注意力机制的集成池化方法，用于时间序列预测。该方法通过学习加权平均值来权衡不同预测模型的输出，并在两个时间序列预测问题上进行了测试。在某些情况下，该方法优于现有的集成池化方法。

    

    在时间序列预测中，减少模型偏差的常见技术是使用多个预测模型的集成，并将其输出汇总为一个集成预测。然而，在每个预测模型具有不同偏差的情况下，如何在汇总中权衡每个模型的预测并不总是清晰的。我们提出了一种集成池化方法，利用基于注意力的集成池化模型学习预测候选模型的加权平均值。我们将这种方法应用于两个时间序列预测问题：非平稳的Lorenz `63方程的多步预测和COVID-19每周发生的死亡事件的单步预测。我们发现，虽然我们的模型在预测非平稳的Lorenz `63方程时表现出色，但在预测COVID-19每周发生的死亡事件时，并不一直比现有的集成池化方法表现更好。

    A common technique to reduce model bias in time-series forecasting is to use an ensemble of predictive models and pool their output into an ensemble forecast. In cases where each predictive model has different biases, however, it is not always clear exactly how each model forecast should be weighed during this pooling. We propose a method for pooling that performs a weighted average over candidate model forecasts, where the weights are learned by an attention-based ensemble pooling model. We test this method on two time-series forecasting problems: multi-step forecasting of the dynamics of the non-stationary Lorenz `63 equation, and one-step forecasting of the weekly incident deaths due to COVID-19. We find that while our model achieves excellent valid times when forecasting the non-stationary Lorenz `63 equation, it does not consistently perform better than the existing ensemble pooling when forecasting COVID-19 weekly incident deaths.
    
[^86]: 关于快速学习的基础研究

    On the Foundations of Shortcut Learning. (arXiv:2310.16228v1 [cs.LG])

    [http://arxiv.org/abs/2310.16228](http://arxiv.org/abs/2310.16228)

    该论文研究了快速学习的基础，揭示了模型对哪些特征更偏好，即可预测性和可用性如何相互影响模型的特征使用。

    

    深度学习模型可以从数据中提取丰富的特征。模型使用哪些特征不仅取决于预测能力 - 一个特征可靠地指示训练集标签的程度，还取决于可用性 - 一个特征可以从输入中被轻松提取或利用的程度。有关快速学习的文献已经指出了模型偏好一个特征而不是另一个特征的例子，例如在纹理和形状之间以及在图像背景和前景对象之间。在这里，我们测试关于哪些输入属性对于模型更容易获取的假设，并系统地研究预测能力和可用性如何相互作用来塑造模型的特征使用。我们构建了一个最小的、明确的生成框架来合成具有两个潜在特征的分类数据集，这两个特征在预测能力和我们假设与可用性有关的因素上有所不同，并量化了模型的快捷偏差 - 它过度依赖快捷（更可用、不太预测）特征而忽视了核心（不太可用)特征。

    Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity-how reliably a feature indicates train-set labels-but also on availability-how easily the feature can be extracted, or leveraged, from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and quantify a model's shortcut bias-its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less avail
    
[^87]: TiC-CLIP: CLIP模型的持续训练

    TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])

    [http://arxiv.org/abs/2310.16226](http://arxiv.org/abs/2310.16226)

    该论文提出了用于训练视觉-语言模型的大规模时间连续 (TiC) 基准，使用这些基准评估了现有模型的时间鲁棒性，并展示了一种简单有效的排练方法来持续训练模型。

    

    保持大型基础模型与最新数据保持同步本身就是昂贵的。为了避免不断重新训练的高成本，持续训练这些模型至关重要。这个问题被缺乏大规模连续学习基准或基线所加剧。我们引入了用于训练视觉-语言模型的第一批 Web 规模时间连续（TiC）基准：TiC-DataCompt、TiC-YFCC 和 TiC-RedCaps，其中包含超过 127 亿个时间戳图像-文本对，跨越了 9 年的时间（2014-2022）。我们首先使用这些基准来策划各种动态评估，以衡量现有模型的时间鲁棒性。我们展示了 OpenAI 的 CLIP 模型（使用 2020 年的数据进行训练）在我们策划的从 2021 年到 2022 年的检索任务中，失去了约 8% 的零-shot准确率，而与 OpenCLIP 存储库中最近训练的模型相比。然后，我们研究如何高效地对时间连续数据进行训练。我们证明了一种简单的排练方法，从上次的训练中继续训练，可以实现有效的训练。

    Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
    
[^88]: CleanCoNLL: 一种几乎无噪声的命名实体识别数据集

    CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])

    [http://arxiv.org/abs/2310.16225](http://arxiv.org/abs/2310.16225)

    CleanCoNLL是一种几乎无噪声的命名实体识别数据集，通过全面重标记和自动一致性检查来纠正CoNLL-03中的注释错误，提高了最先进方法的F1分数，并减少了因注释缺失而误判的情况。

    

    CoNLL-03语料库被认为是最著名和广泛使用的命名实体识别（NER）基准数据集。然而，之前的研究发现了大量的注释错误、不完整性和数据不一致性。这给客观比较NER方法和分析其错误带来了挑战，因为目前最先进的模型在CoNLL-03中达到的F1分数与估计的噪声水平相当甚至更高。为了解决这个问题，我们通过自动一致性检查辅助的全面重标记工作来纠正英文CoNLL-03中所有标签的7.0％。我们的工作还为了更好地解释NER标签和作为附加保证注释质量而添加了一个实体链接注释层。我们的实验证明，最先进的方法不仅在我们的数据上达到了显著更高的F1分数（97.1％），而且关键是正确预测被错误地计算为错误的比例由于注释的缺失得到了改善。

    The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annota
    
[^89]: 毒药不是无痕的：全面不可知的检测毒害攻击

    Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks. (arXiv:2310.16224v1 [cs.CR])

    [http://arxiv.org/abs/2310.16224](http://arxiv.org/abs/2310.16224)

    这项研究提出了一种全面不可知的框架DIVA，通过分析潜在的毒害数据集来检测机器学习模型遭受的攻击。通过比较模型在受到毒害和干净数据上的准确性，DIVA能够预测未知的干净数据集上的准确性，从而实现对一般毒害攻击的检测。

    

    机器学习模型的性能取决于底层数据的质量。恶意攻击者可以通过毒化训练数据来攻击模型。当前的检测器针对特定数据类型、模型或攻击类型，因此在现实场景中具有有限的适用性。本文提出了一种新颖的全面不可知框架DIVA（检测不可见攻击），仅通过分析潜在毒害数据集来检测攻击。DIVA基于这样一个思想，即通过比较分类器在受到毒害和干净数据上的准确性来检测毒害攻击，并使用复杂性度量预训练元学习器来估计在假设的干净数据集上的未知准确性。该框架适用于一般的毒害攻击。为了评估目的，在本文中，我们在标签翻转攻击上测试了DIVA。

    The performance of machine learning models depends on the quality of the underlying data. Malicious actors can attack the model by poisoning the training data. Current detectors are tied to either specific data types, models, or attacks, and therefore have limited applicability in real-world scenarios. This paper presents a novel fully-agnostic framework, DIVA (Detecting InVisible Attacks), that detects attacks solely relying on analyzing the potentially poisoned data set. DIVA is based on the idea that poisoning attacks can be detected by comparing the classifier's accuracy on poisoned and clean data and pre-trains a meta-learner using Complexity Measures to estimate the otherwise unknown accuracy on a hypothetical clean dataset. The framework applies to generic poisoning attacks. For evaluation purposes, in this paper, we test DIVA on label-flipping attacks.
    
[^90]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^91]: GPU嵌入式系统的性能优化：基于机器学习和分析模型的优化方法

    Performance Tuning for GPU-Embedded Systems: Machine-Learning-based and Analytical Model-driven Tuning Methodologies. (arXiv:2310.16214v1 [cs.DC])

    [http://arxiv.org/abs/2310.16214](http://arxiv.org/abs/2310.16214)

    本文开发了一种基于分析模型和基于机器学习的GPU嵌入式系统优化方法，针对并行前缀操作进行了性能评估，并提供了性能洞察。

    

    GPU嵌入式系统因其高效的能耗而在各个领域中变得流行。然而，为了满足这些系统运行实时或耗时应用程序的需求，将其调整以展现出高性能是至关重要的。本文通过开发和比较两种在GPU嵌入式系统上的优化方法来解决这个问题，并为寻求优化运行在这些架构上的应用程序的开发人员和研究人员提供性能洞察。我们重点关注并行前缀操作，如FFT、扫描原语和三对角系统求解器，这些是许多应用程序中性能关键的组件。本研究引入了一种基于分析模型的优化方法和一种基于机器学习（ML）的优化方法。我们评估了这两种优化方法在NVIDIA Jetson系统中BPLG库的不同并行前缀实现的性能，并将其与现有方法的性能进行比较。

    GPU-embedded systems have gained popularity across various domains due to their efficient power consumption. However, in order to meet the demands of real-time or time-consuming applications running on these systems, it is crucial for them to be tuned to exhibit high performance. This paper addresses the issue by developing and comparing two tuning methodologies on GPU-embedded systems, and also provides performance insights for developers and researchers seeking to optimize applications running on these architectures. We focus on parallel prefix operations, such as FFT, scan primitives, and tridiagonal system solvers, which are performance-critical components in many applications. The study introduces an analytical model-driven tuning methodology and a Machine Learning (ML)-based tuning methodology. We evaluate the performance of the two tuning methodologies for different parallel prefix implementations of the BPLG library in an NVIDIA Jetson system, and compare their performance to t
    
[^92]: ELM岭回归增强

    ELM Ridge Regression Boosting. (arXiv:2310.16209v1 [cs.LG])

    [http://arxiv.org/abs/2310.16209](http://arxiv.org/abs/2310.16209)

    ELM岭回归增强方法显著提高了ELM的分类性能和鲁棒性。

    

    我们讨论了一种对岭回归（RR）方法进行增强的方法，该方法应用于极限学习机（ELM），并且我们展示了该方法显著提高了ELM的分类性能和鲁棒性。

    We discuss a boosting approach for the Ridge Regression (RR) method, with applications to the Extreme Learning Machine (ELM), and we show that the proposed method significantly improves the classification performance and robustness of ELMs.
    
[^93]: 学习低秩潜空间的简单确定性自编码器：理论和经验洞见

    Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder: Theoretical and Empirical Insights. (arXiv:2310.16194v1 [cs.LG])

    [http://arxiv.org/abs/2310.16194](http://arxiv.org/abs/2310.16194)

    本文提出了一种新的方法，称为低秩自编码器（LoRAE），通过加入低秩正则化项，自适应地重构低维潜在空间，同时保持自编码器的基本目标。该方法在图像生成和下游分类等任务中展现出了优越性。

    

    自编码器是一种无监督学习范式，通过最小化重构损失来创建数据的紧凑潜在表示。然而，它往往忽视了大多数数据（图像）被嵌入在一个低维空间中的事实，这对于有效的数据表示至关重要。为了解决这个限制，我们提出了一种称为低秩自编码器（LoRAE）的新方法。在LoRAE中，我们加入了一个低秩正则化项，以自适应地重构一个低维潜在空间，同时保持自编码器的基本目标。这有助于将数据嵌入到一个低维空间中，同时保留重要信息。它是一个学习低秩潜空间的简单自编码器扩展。从理论上讲，我们为我们的模型建立了一个更紧密的误差界。在实证上，我们的模型在图像生成和下游分类等各种任务中展现出了优越性。理论和实践结果都强调了获取低秩潜空间的重要性。

    The autoencoder is an unsupervised learning paradigm that aims to create a compact latent representation of data by minimizing the reconstruction loss. However, it tends to overlook the fact that most data (images) are embedded in a lower-dimensional space, which is crucial for effective data representation. To address this limitation, we propose a novel approach called Low-Rank Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to adaptively reconstruct a low-dimensional latent space while preserving the basic objective of an autoencoder. This helps embed the data in a lower-dimensional space while preserving important information. It is a simple autoencoder extension that learns low-rank latent space. Theoretically, we establish a tighter error bound for our model. Empirically, our model's superiority shines through various tasks such as image generation and downstream classification. Both theoretical and practical outcomes highlight the importance of acquiring low
    
[^94]: 高效的深度数据同化方法在稀疏观测和时变传感器中的应用

    Efficient deep data assimilation with sparse observations and time-varying sensors. (arXiv:2310.16187v1 [cs.LG])

    [http://arxiv.org/abs/2310.16187](http://arxiv.org/abs/2310.16187)

    本研究提出了一种新的深度数据同化方案（VIVID），结合了Voronoi-tessellation和卷积神经网络的能力，用于处理稀疏、非结构化和时变的传感器数据，并建立了一个高效的数据同化框架。

    

    变分数据同化（DA）已被广泛应用于工程问题中，通过对多源噪声数据的加权组合来进行场景重建和预测。近年来，深度学习（DL）技术在数据同化中的应用显示出在高维动力系统中提高效率和准确性的潜力。然而，现有的深度数据同化方法在处理非结构化观测数据时面临困难，特别是当传感器的位置和数量随时间动态变化时。我们引入了一种新的变分数据同化方案，名为Voronoi-tessellation Inverse operator for VariatIonal Data assimilation（VIVID），它将DL逆算子纳入同化目标函数中。通过利用Voronoi-tessellation和卷积神经网络的能力，VIVID能够处理稀疏、非结构化和时变的传感器数据。此外，DL逆算子的引入还建立了一个高效的数据同化框架。

    Variational Data Assimilation (DA) has been broadly used in engineering problems for field reconstruction and prediction by performing a weighted combination of multiple sources of noisy data. In recent years, the integration of deep learning (DL) techniques in DA has shown promise in improving the efficiency and accuracy in high-dimensional dynamical systems. Nevertheless, existing deep DA approaches face difficulties in dealing with unstructured observation data, especially when the placement and number of sensors are dynamic over time. We introduce a novel variational DA scheme, named Voronoi-tessellation Inverse operator for VariatIonal Data assimilation (VIVID), that incorporates a DL inverse operator into the assimilation objective function. By leveraging the capabilities of the Voronoi-tessellation and convolutional neural networks, VIVID is adept at handling sparse, unstructured, and time-varying sensor data. Furthermore, the incorporation of the DL inverse operator establishes
    
[^95]: 使用U-Net架构进行粉末X射线衍射图像的图像分割

    Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images. (arXiv:2310.16186v1 [cs.LG])

    [http://arxiv.org/abs/2310.16186](http://arxiv.org/abs/2310.16186)

    本文提出了一种使用U-Net架构进行粉末X射线衍射图像的图像分割的方法，通过深度学习和卷积神经网络，能够准确识别和排除实验X射线衍射图像中的伪影，并在召回率和假阳性方面表现出显著的改进。

    

    科学研究人员经常使用原位同步辐射高能粉末X射线衍射（XRD）技术来研究功能器件中材料的晶体结构，比如可充电电池材料。我们提出了一种用于识别实验X射线衍射图像中伪影的方法。该方法使用深度学习卷积神经网络架构，如可调节的U-Net来识别伪影。特别是，使用整体真阳性率或召回率对预测的伪影与相应的基准（手动实现）进行评估。结果表明，在测试数据集上，U-Net能够始终产生92.4%的很好的召回率表现，相比传统方法平均假阳性降低了34%。U-Net还将识别和分离伪影所需的时间缩短了50%以上。此外，排除伪影显示出主要的创新点和贡献。

    Scientific researchers frequently use the in situ synchrotron high-energy powder X-ray diffraction (XRD) technique to examine the crystallographic structures of materials in functional devices such as rechargeable battery materials. We propose a method for identifying artifacts in experimental XRD images. The proposed method uses deep learning convolutional neural network architectures, such as tunable U-Nets to identify the artifacts. In particular, the predicted artifacts are evaluated against the corresponding ground truth (manually implemented) using the overall true positive rate or recall. The result demonstrates that the U-Nets can consistently produce great recall performance at 92.4% on the test dataset, which is not included in the training, with a 34% reduction in average false positives in comparison to the conventional method. The U-Nets also reduce the time required to identify and separate artifacts by more than 50%. Furthermore, the exclusion of the artifacts shows majo
    
[^96]: BLP 2023任务2：情感分析

    BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])

    [http://arxiv.org/abs/2310.16183](http://arxiv.org/abs/2310.16183)

    BLP 2023任务2是关于情感分析的共享任务，吸引了71个参与者。参与者通过各种方法，包括经典机器学习模型和大型语言模型，提交了597个运行结果。本文提供了任务的详细设置和参与者提交系统的概述。

    

    我们总结了作为BLP 2023创新工作坊的一部分举办的BLP情感共享任务。该任务的定义是在给定的社交媒体文本中检测情感。该任务吸引了71个参与者的关注，其中在开发和评估阶段分别有29个和30个团队提交了系统。总共，参与者提交了597个运行结果。然而，总共有15个团队提交了系统描述论文。提交的系统涵盖了从经典的机器学习模型、微调预训练模型到在零样本和少样本设置中利用大型语言模型（LLMs）的各种方法。在本文中，我们详细介绍了该任务的设置，包括数据集的开发和评估设置。此外，我们简要概述了参与者提交的系统。共享任务的所有数据集和评估脚本已公开可用。

    We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
    
[^97]: G-CASCADE: 高效的级联图卷积解码器用于2D医学图像分割

    G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation. (arXiv:2310.16175v1 [eess.IV])

    [http://arxiv.org/abs/2310.16175](http://arxiv.org/abs/2310.16175)

    G-CASCADE是一种基于图卷积的解码器，用于2D医学图像分割，通过有效的图卷积块逐渐改进分层Transformer编码器生成的特征图，保留长程信息，并且在多个医学图像分割任务上优于其他最先进方法。

    

    最近几年，医学图像分割已成为计算机辅助诊断领域中的重要应用。本文首次提出了一种基于图卷积的解码器，即级联图卷积注意力解码器（G-CASCADE），用于2D医学图像分割。G-CASCADE通过高效的图卷积块逐渐改进由分层Transformer编码器生成的多阶段特征图。编码器利用自我注意机制捕捉长程依赖性，而解码器通过图卷积块的全局感受野改善特征图，保留长程信息。对我们的解码器在五个医学图像分割任务（即腹部器官、心脏器官、息肉病变、皮损和视网膜血管）上进行了严格的评估，结果显示我们的模型优于其他最先进的方法。我们还证明了我们的解码器实现了

    In recent years, medical image segmentation has become an important application in the field of computer-aided diagnosis. In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves
    
[^98]: 关于具有epsilon-greedy探索的Deep Q网络的收敛性与样本复杂度分析

    On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\epsilon$-Greedy Exploration. (arXiv:2310.16173v1 [cs.LG])

    [http://arxiv.org/abs/2310.16173](http://arxiv.org/abs/2310.16173)

    本文首次提供了关于实际设置下具有epsilon-greedy策略的Deep Q网络（DQN）的收敛性和样本复杂度分析。

    

    本文在深度强化学习中提供了对Deep Q网络（DQN）具有epsilon-greedy探索的理论理解。尽管DQN取得了巨大的实证成就，但其理论描述仍然不完善。首先，现有分析中的探索策略要么不切实际，要么被忽略。其次，与传统的Q学习算法相比，DQN采用目标网络和经验回放来获得训练Q网络所使用的均方贝尔曼误差（MSBE）的无偏估计。然而，现有的DQN理论分析缺乏收敛性分析，或者通过使用计算量非常大的神经网络来规避技术挑战，这在计算效率上并不高效。本文提供了第一个关于DQN实际设置的收敛性和样本复杂度分析，其中包括epsilon-greedy策略。我们证明了一个具有递减epsilon的迭代过程。

    This paper provides a theoretical understanding of Deep Q-Network (DQN) with the $\varepsilon$-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with $\epsilon$-greedy policy. We prove an iterative procedure with decaying $\epsilon$ 
    
[^99]: Brainchop:下一代基于Web的神经影像应用程序

    Brainchop: Next Generation Web-Based Neuroimaging Application. (arXiv:2310.16162v1 [cs.LG])

    [http://arxiv.org/abs/2310.16162](http://arxiv.org/abs/2310.16162)

    Brainchop是一种基于浏览器的神经影像工具，采用前端机器学习和预训练模型，在保护数据隐私的同时提供全面的大脑预处理和分割解决方案。

    

    直接在浏览器中进行体积图像处理，尤其是处理医学数据，与传统的后端工具相比，面临着前所未有的挑战。这些挑战源于浏览器环境中固有的限制，如有限的计算资源和前端机器学习库的可用性。因此，目前缺乏能提供全面端到端解决方案，用于整个大脑预处理和分割，同时保护端用户数据隐私和安全性的神经影像前端工具。在这种背景下，我们引入了Brainchop作为一种创新的基于浏览器的神经影像工具，它使用预训练的全脑深度学习模型对结构性MRI进行体积分析，而无需技术专业知识或复杂的设置程序。除了致力于数据隐私保护，这个前端工具还提供了多个特性，包括可扩展性、低延迟、用户友好的操作界面。

    Performing volumetric image processing directly within the browser, particularly with medical data, presents unprecedented challenges compared to conventional backend tools. These challenges arise from limitations inherent in browser environments, such as constrained computational resources and the availability of frontend machine learning libraries. Consequently, there is a shortage of neuroimaging frontend tools capable of providing comprehensive end-to-end solutions for whole brain preprocessing and segmentation while preserving end-user data privacy and residency. In light of this context, we introduce Brainchop (this http URL) as a groundbreaking in-browser neuroimaging tool that enables volumetric analysis of structural MRI using pre-trained full-brain deep learning models, all without requiring technical expertise or intricate setup procedures. Beyond its commitment to data privacy, this frontend tool offers multiple features, including scalability, low latency, user-friendly op
    
[^100]: 基于论证的上下文感知特征归因

    Context-aware feature attribution through argumentation. (arXiv:2310.16157v1 [cs.LG])

    [http://arxiv.org/abs/2310.16157](http://arxiv.org/abs/2310.16157)

    本论文提出了一种基于论证的上下文感知特征归因方法，以解决机器学习和数据分析中特征归因的挑战。该方法利用广义可加模型和梯度方法与替代模型相结合，同时考虑用户的背景信息，从而提高了归因的准确性和解释性。

    

    特征归因是机器学习和数据分析中的基本任务，涉及确定个别特征或变量对模型输出的贡献。这个过程有助于确定预测结果最重要的特征。特征归因方法的历史可以追溯到广义可加模型 (GAMs)，它通过将因变量和自变量之间的非线性关系纳入模型，扩展了线性回归模型。近年来，基于梯度的方法和替代模型已经被应用于揭示复杂的人工智能 (AI) 系统，但这些方法存在一些局限性。GAMs 往往能够达到较低的准确性，基于梯度的方法很难解释，替代模型通常存在稳定性和保真度问题。此外，大部分现有方法都没有考虑用户的背景，而用户的背景可能会对他们的偏好产生重要影响。为了解决这些限制并推进当前的研究

    Feature attribution is a fundamental task in both machine learning and data analysis, which involves determining the contribution of individual features or variables to a model's output. This process helps identify the most important features for predicting an outcome. The history of feature attribution methods can be traced back to General Additive Models (GAMs), which extend linear regression models by incorporating non-linear relationships between dependent and independent variables. In recent years, gradient-based methods and surrogate models have been applied to unravel complex Artificial Intelligence (AI) systems, but these methods have limitations. GAMs tend to achieve lower accuracy, gradient-based methods can be difficult to interpret, and surrogate models often suffer from stability and fidelity issues. Furthermore, most existing methods do not consider users' contexts, which can significantly influence their preferences. To address these limitations and advance the current s
    
[^101]: 通过学习不变表示打破深度神经网络中的维度诅咒

    Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations. (arXiv:2310.16154v1 [cs.LG])

    [http://arxiv.org/abs/2310.16154](http://arxiv.org/abs/2310.16154)

    本论文研究了深度学习模型的结构与处理数据中固有结构之间的关系，探索了深度学习的理论基础，旨在打破所谓的维度诅咒，并理解深度学习算法的有效性和其超越传统方法的原因。

    

    人工智能，特别是机器学习领域，已经经历了从数据驱动模型到基于数据学习和适应的范式转变。这导致了在自然语言处理和计算机视觉等各个领域取得了前所未有的进展，这主要归功于深度学习，一种特殊的机器学习模型。深度学习通过一系列计算层从原始数据中学习相关特征，可以说超越了传统方法。本论文通过研究这些模型的结构和处理的数据中固有结构之间的关系，探索了深度学习的理论基础。特别地，我们问：是什么使得深度学习算法有效，并使其能够战胜所谓的维度诅咒——即由于维度增加导致对数据点的指数级需求增加，从而在高维度中通常学习函数变得困难。

    Artificial intelligence, particularly the subfield of machine learning, has seen a paradigm shift towards data-driven models that learn from and adapt to data. This has resulted in unprecedented advancements in various domains such as natural language processing and computer vision, largely attributed to deep learning, a special class of machine learning models. Deep learning arguably surpasses traditional approaches by learning the relevant features from raw data through a series of computational layers.  This thesis explores the theoretical foundations of deep learning by studying the relationship between the architecture of these models and the inherent structures found within the data they process. In particular, we ask What drives the efficacy of deep learning algorithms and allows them to beat the so-called curse of dimensionality-i.e. the difficulty of generally learning functions in high dimensions due to the exponentially increasing need for data points with increased dimensio
    
[^102]: FLTrojan: 通过选择性权重篡改对联邦语言模型进行隐私泄露攻击

    FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])

    [http://arxiv.org/abs/2310.16152](http://arxiv.org/abs/2310.16152)

    本文提出了一种FLTrojan攻击方法，通过选择性权重篡改，从联邦语言模型中泄露隐私敏感用户数据。通过观察到FL中中间轮次的模型快照可以引起更大的隐私泄露，并发现隐私泄露可以通过篡改模型的选择性权重来加剧。

    

    联邦学习(Federated learning, FL)正成为许多技术应用中的关键组件，包括语言建模领域，其中个体FL参与者在其本地数据集中往往具有敏感的文本数据。然而，确定联邦语言模型中的隐私泄露程度并不简单，现有的攻击只是试图提取数据，而不考虑数据的敏感性或天真性。为了填补这一空白，在本文中，我们介绍了关于从联邦语言模型中泄露隐私敏感用户数据的两个新发现。首先，我们观察到FL中中间轮次的模型快照比最终训练模型能够造成更大的隐私泄露。其次，我们确定隐私泄露可以通过篡改模型的选择性权重来加剧，这些权重特别负责记忆敏感训练数据。我们展示了恶意客户端如何在FL中泄露其他用户的隐私敏感数据。

    Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
    
[^103]: 有限记忆容量的语言模型捕捉人类句子处理中的干扰

    A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing. (arXiv:2310.16142v1 [cs.CL])

    [http://arxiv.org/abs/2310.16142](http://arxiv.org/abs/2310.16142)

    开发了一个循环神经语言模型，通过使用单个自我注意头紧密模拟了认知理论中假设的记忆系统，并捕捉到人类句子处理中的干扰。

    

    人类句子处理困难的两个核心因素被认为是期望和来自工作记忆的检索。最近的一个尝试，旨在创建一个综合认知模型，将这两个因素整合在一起，依赖于transformer语言模型的自我注意机制和人类句子处理中基于暗示的工作记忆检索理论之间的相似之处。（Ryu and Lewis, 2021）.虽然Ryu和Lewis展示了GPT-2的特殊自注意头中的注意模式与基于相似性的干扰的关键预测一致，这是基于暗示的检索模型，但他们的方法需要识别出句法特化的自注意头，并做出认知上不合理的假设，即数百次的内存检索操作是并行进行的。在本研究中，我们开发了一个具有单个自我注意头的循环神经语言模型，更贴近认知理论所假设的记忆系统。我们展示了我们模型的...

    Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's
    
[^104]: 金属增材制造中薄壁部件的在线热场预测

    Online Thermal Field Prediction for Metal Additive Manufacturing of Thin Walls. (arXiv:2310.16125v1 [cs.LG])

    [http://arxiv.org/abs/2310.16125](http://arxiv.org/abs/2310.16125)

    本文提出了一种在线热场预测方法，通过映射和重建技术，在金属增材制造过程中使用少量传感器实现尚未打印部件的热场预测和性能控制。

    

    本文旨在研究金属增材制造中的一个实际问题，即当只有少数传感器可用时，如何在线预测尚未打印的零件的热场。该研究提出了一种使用映射和重建的在线热场预测方法，可以集成到金属增材制造过程中进行在线性能控制。基于温度曲线的相似性（一个点的温度曲线的曲线段），热场映射应用人工神经网络估计尚未打印层上点的温度曲线，该曲线是由之前打印层上某些点的测量温度得到。利用同一层上几个点的测量/预测温度曲线，热场重建提出了一个降阶模型（ROM）来构建同一层上所有点的温度曲线，从而构建整个层的温度场。ROM的训练是通过极限学习机算法进行的。

    This paper aims to study a practical issue in metal AM, i.e., how to predict the thermal field of yet-to-print parts online when only a few sensors are available. This work proposes an online thermal field prediction method using mapping and reconstruction, which could be integrated into a metal AM process for online performance control. Based on the similarity of temperature curves (curve segments of a temperature profile of one point), the thermal field mapping applies an artificial neural network to estimate the temperature curves of points on the yet-to-print layer from measured temperatures of certain points on the previously printed layer. With measured/predicted temperature profiles of several points on the same layer, the thermal field reconstruction proposes a reduced order model (ROM) to construct the temperature profiles of all points on the same layer, which could be used to build the temperature field of the entire layer. The training of ROM is performed with an extreme le
    
[^105]: 锚定空间优化传输：加速多个传输问题的批处理

    Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems. (arXiv:2310.16123v1 [cs.LG])

    [http://arxiv.org/abs/2310.16123](http://arxiv.org/abs/2310.16123)

    提出了一种锚定空间优化传输（ASOT）问题，通过将分布映射到共享的锚点空间，学习其潜在的共同特征，从而加速了多个传输问题的批处理。

    

    最优传输（OT）理论提供了一种有效的比较定义在指定度量空间上的概率分布的方法，但它的计算复杂度为立方级。尽管Sinkhorn算法极大地降低了OT解的计算复杂度，但在实践中，多个OT问题的解仍然耗时和占用内存。然而，关于OT计算加速的许多工作通常基于单个OT问题的前提，忽视了一个小批量中分布的潜在共同特征。因此，我们提出了一种翻译的OT问题，称为锚定空间优化传输（ASOT）问题，专门用于批处理多个OT问题的解。对于提出的ASOT问题，分布将被映射到一个共享的锚点空间，该空间学习潜在的共同特征，从而帮助加速OT批处理。基于所提出的ASOT，Wasserstei

    The optimal transport (OT) theory provides an effective way to compare probability distributions on a defined metric space, but it suffers from cubic computational complexity. Although the Sinkhorn's algorithm greatly reduces the computational complexity of OT solutions, the solutions of multiple OT problems are still time-consuming and memory-comsuming in practice. However, many works on the computational acceleration of OT are usually based on the premise of a single OT problem, ignoring the potential common characteristics of the distributions in a mini-batch. Therefore, we propose a translated OT problem designated as the anchor space optimal transport (ASOT) problem, which is specially designed for batch processing of multiple OT problem solutions. For the proposed ASOT problem, the distributions will be mapped into a shared anchor point space, which learns the potential common characteristics and thus help accelerate OT batch processing. Based on the proposed ASOT, the Wasserstei
    
[^106]: 只需19个参数：用于粒子物理学的微型神经网络

    19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics. (arXiv:2310.16121v1 [hep-ph])

    [http://arxiv.org/abs/2310.16121](http://arxiv.org/abs/2310.16121)

    只需19个参数的微型神经网络在顶夸克喷注的二分类任务上表现出优于数以万计参数的通用架构的性能。

    

    随着粒子加速器增加碰撞速率和深度学习解决方案的可行性得到证实，对于低延迟任务（如触发器），需要轻量且快速的神经网络架构。我们研究了最近的洛伦兹和置换对称架构PELICAN的潜力，并展示了该架构的几个实例，其中可训练参数仅为19个，在顶夸克喷注的二分类任务上，性能超过了数以万计参数的通用架构。

    As particle accelerators increase their collision rates, and deep learning solutions prove their viability, there is a growing need for lightweight and fast neural network architectures for low-latency tasks such as triggering. We examine the potential of one recent Lorentz- and permutation-symmetric architecture, PELICAN, and present its instances with as few as 19 trainable parameters that outperform generic architectures with tens of thousands of parameters when compared on the binary classification task of top quark jet tagging.
    
[^107]: Alquist 5.0：对话树与生成模型相结合。增强SocialBot对话的一种新方法。

    Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])

    [http://arxiv.org/abs/2310.16119](http://arxiv.org/abs/2310.16119)

    Alquist 5.0是一种新的SocialBot系统，通过将对话树和生成模型相结合，以及引入NRG Barista和支持多模式设备，提高了用户对话体验，并保持了共情和知识型对话能力。

    

    我们介绍了我们的SocialBot- Alquist 5.0-，该系统是为Alexa Prize SocialBot大挑战5开发的。在我们系统的前几个版本基础上，我们引入了NRG Barista，并概述了将Barista整合到我们的SocialBot中的几种创新方法，从而改善了整体的对话体验。此外，我们还扩展了我们的SocialBot以支持多模式设备。本文提供了关于Alquist 5.0开发的见解，该系统在满足用户不断变化的期望的同时，保持了对各种主题的共情和知识型对话能力。

    We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
    
[^108]: 无需过去数据，唤醒过去的概念：从在线安慰剂进行类增量学习

    Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos. (arXiv:2310.16115v1 [cs.CV])

    [http://arxiv.org/abs/2310.16115](http://arxiv.org/abs/2310.16115)

    本文提出了一种新的类增量学习方法，通过使用来自在线安慰剂的旧类别数据进行知识蒸馏，解决了类增量学习中长期记忆的问题，并提高了旧类别知识保留的效率。

    

    在模型不断适应新类别时，不忘记旧类别的知识是类增量学习中的一个关键挑战。解决这个问题的常用技术是知识蒸馏（KD），它惩罚了旧模型和新模型之间的预测不一致性。由于类增量学习中的内存限制严格，旧类别数据极为稀缺，因此这样的预测几乎都是由新类别数据进行的。然而，我们深入研究了知识蒸馏损失，并发现“使用新类别数据进行知识蒸馏”不仅阻碍了模型对新类别的适应，而且对保留旧类别知识的效率非常低。为了解决这个问题，我们提出了“使用旧类别的安慰剂进行知识蒸馏”的方法，其中安慰剂是从自由图像流中选择的，比如谷歌图片，以自动和经济的方式进行选择。为此，我们训练了一个在线安慰剂选择策略来快速评估流式图像的质量（好的或坏的安慰剂），并且只使用好的安慰剂进行一次喂食。

    Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that "using new class data for KD" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by "using the placebos of old classes for KD", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-f
    
[^109]: 大脑基因转录的压缩表示

    Compressed representation of brain genetic transcription. (arXiv:2310.16113v1 [cs.LG])

    [http://arxiv.org/abs/2310.16113](http://arxiv.org/abs/2310.16113)

    本文研究了大脑基因转录的压缩表示方法，通过比较不同的线性和非线性方法，评估了它们在重建、解剖和预测方面的性能。

    

    大脑的结构过于复杂，无法直观地进行观察，需要使用压缩表示将其变化投影到紧凑、可导航的空间中。在高维数据（如基因表达）中，尤其具有挑战性，其中解剖和转录模式的联合复杂性要求最大压缩。目前的实践是使用标准的主成分分析（PCA），其计算效率受到限制，尤其在大压缩比下表现力有限。本研究利用全脑体素级Allen大脑图谱转录数据，系统比较了基于最广泛支持的线性和非线性方法（PCA，核PCA，非负矩阵分解（NMF），t-随机邻居嵌入（t-SNE），统一流形逼近和投影（UMAP），深度自编码）的压缩表示，量化重建保真度，解剖连贯性和预测效果。

    The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility
    
[^110]: 使用零阶提示的本地差分隐私文档生成

    Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v1 [cs.CL])

    [http://arxiv.org/abs/2310.16111](http://arxiv.org/abs/2310.16111)

    本研究提出了一种本地差分隐私文档生成机制，利用预训练的大型语言模型和零阶提示对抗作者去匿名攻击，同时最小化对下游效用的影响。实验证明，该机制在降低攻击成功率的同时能够完全恢复清洁的情感分数，比现有方法更有效。

    

    大量研究已经强调了预训练的大型语言模型所带来的隐私风险。相比之下，我们的研究提供了一个独特的视角，证明了预训练的大型语言模型可以有效地为隐私保护做出贡献。我们提出了一种名为DP-Prompt的本地差分隐私机制，它利用预训练的大型语言模型和零阶提示来对抗作者去匿名攻击，同时最小化对下游效用的影响。当DP-Prompt与像ChatGPT（gpt-3.5）这样的强大语言模型一起使用时，我们观察到去匿名攻击成功率显著降低，并且尽管其设计更简单，但它超过了现有方法的很大程度。例如，在IMDB数据集的情况下，DP-Prompt（使用ChatGPT）完全恢复了清洁的情感F1分数，并在静态攻击者的作者识别F1分数上实现了46％的降低和26％的减少。

    Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\% reduction in author identification F1 score against static attackers and a 26\% reduc
    
[^111]: 无线网络上基于广播子图采样的去中心化学习

    Decentralized Learning over Wireless Networks with Broadcast-Based Subgraph Sampling. (arXiv:2310.16106v1 [cs.LG])

    [http://arxiv.org/abs/2310.16106](http://arxiv.org/abs/2310.16106)

    本文介绍了一种在无线网络上进行去中心化学习的通信框架BASS，采用广播传输和概率子图采样。通过控制子集的随机激活和重要性，以及通信成本约束，实现了算法的快速收敛。

    

    本研究关注无线网络上去中心化学习的通信方面，使用基于共识的去中心化随机梯度下降（D-SGD）。考虑到迭代过程中因网络内信息交换而导致的实际通信成本或延迟，我们的目标是通过每个传输时隙的改进来实现算法的快速收敛。我们提出了BASS，一个用于无线网络上D-SGD的高效通信框架，其中采用广播传输和概率子图采样。在每次迭代中，我们激活多个非干扰节点的子集，将模型更新广播给它们的邻居。这些子集会随机随时间激活，并根据其在网络连通性中的重要性以及通信成本约束（例如每次迭代的平均传输时隙数）进行控制。在共识更新步骤中，仅保留双向链接以保持通信。

    This work centers on the communication aspects of decentralized learning over wireless networks, using consensus-based decentralized stochastic gradient descent (D-SGD). Considering the actual communication cost or delay caused by in-network information exchange in an iterative process, our goal is to achieve fast convergence of the algorithm measured by improvement per transmission slot. We propose BASS, an efficient communication framework for D-SGD over wireless networks with broadcast transmission and probabilistic subgraph sampling. In each iteration, we activate multiple subsets of non-interfering nodes to broadcast model updates to their neighbors. These subsets are randomly activated over time, with probabilities reflecting their importance in network connectivity and subject to a communication cost constraint (e.g., the average number of transmission slots per iteration). During the consensus update step, only bi-directional links are effectively preserved to maintain communic
    
[^112]: 在有向图上的分布式在线学习中基于局部差分隐私的梯度跟踪

    Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs. (arXiv:2310.16105v1 [cs.LG])

    [http://arxiv.org/abs/2310.16105](http://arxiv.org/abs/2310.16105)

    本文提出了一种基于局部差分隐私的梯度跟踪分布式在线学习算法，在保持学习准确性的同时，有效地保护了学习者的隐私。

    

    分布式在线学习在解决涉及流数据的大规模机器学习问题方面非常有效。然而，分布式学习中的学习者之间的信息共享也引起了个体学习者敏感数据可能泄露的担忧。为了减轻这种风险，差分隐私被广泛应用于许多现有的分布式在线学习结果中，被认为是隐私保护的“金标准”。然而，这些结果往往面临学习准确性和隐私之间的基本权衡。在本文中，我们提出了一种基于局部差分隐私梯度跟踪的分布式在线学习算法，成功地避免了这种权衡。我们的分析表明，该算法在均方意义上收敛到精确的最优解，同时确保了严格的局部差分隐私，即使迭代次数增加，累积隐私预算也保证是有限的。

    Distributed online learning has been proven extremely effective in solving large-scale machine learning problems involving streaming data. However, information sharing between learners in distributed learning also raises concerns about the potential leakage of individual learners' sensitive data. To mitigate this risk, differential privacy, which is widely regarded as the "gold standard" for privacy protection, has been widely employed in many existing results on distributed online learning. However, these results often face a fundamental tradeoff between learning accuracy and privacy. In this paper, we propose a locally differentially private gradient tracking based distributed online learning algorithm that successfully circumvents this tradeoff. Our analysis shows that the proposed algorithm converges in mean square to the exact optimal solution while ensuring rigorous local differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iter
    
[^113]: 解剖学感知的半监督图像分割的不确定性研究

    Anatomically-aware Uncertainty for Semi-supervised Image Segmentation. (arXiv:2310.16099v1 [cs.CV])

    [http://arxiv.org/abs/2310.16099](http://arxiv.org/abs/2310.16099)

    本研究提出了一种解剖学感知的方法，通过利用分割掩模中的全局信息来估计半监督图像分割的不确定性。该方法克服了传统不确定性估计方法的高计算复杂性和对像素级差异的限制。

    

    半监督学习通过利用无标签数据来放宽对图像分割大规模像素级标注数据的需求。利用无标签数据的一个重要方法是对模型预测进行规范化。然而，由于无标签数据的预测可能不可靠，通常需要借助不确定性感知的方法逐渐学习有意义和可靠的预测。然而，不确定性估计方法依赖于模型预测的多次推断，每个训练步骤都需要计算，这在计算上是昂贵的。此外，这些不确定性图像捕捉像素级差异，并不能考虑全局信息。本研究提出了一种新的方法，通过利用分割掩模中的全局信息来估计分割不确定性。更确切地说，首先学习解剖学感知的表示来建模可用的分割掩模。然后，该学习表示将新的分割预测映射到一个解剖结构上。

    Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. A prominent way to exploit unlabeled data is to regularize model predictions. Since the predictions of unlabeled data can be unreliable, uncertainty-aware schemes are typically employed to gradually learn from meaningful and reliable predictions. Uncertainty estimation methods, however, rely on multiple inferences from the model predictions that must be computed for each training step, which is computationally expensive. Moreover, these uncertainty maps capture pixel-wise disparities and do not consider global information. This work proposes a novel method to estimate segmentation uncertainty by leveraging global information from the segmentation masks. More precisely, an anatomically-aware representation is first learnt to model the available segmentation masks. The learnt representation thereupon maps the prediction of a new segmentation into an anatomic
    
[^114]: 上下文强盗用于评估和改进库存控制策略

    Contextual Bandits for Evaluating and Improving Inventory Control Policies. (arXiv:2310.16096v1 [stat.ML])

    [http://arxiv.org/abs/2310.16096](http://arxiv.org/abs/2310.16096)

    这项研究引入了均衡策略的概念，并提出了一种基于上下文强盗的算法来评估和改进库存控制策略，这在理论上和实证研究中得到了有利的保证。

    

    解决库存控制问题的解决方案通常涉及对非平稳随机需求、失去销售和具有随机供应商交货时间的周期性检查的动力学进行强假设的逼近或模拟，并应用优化、动态规划或强化学习等方法。因此，分析和评估任何库存控制策略是重要的，特别是看是否有改进的空间。我们引入了均衡策略的概念，这是一个策略的理想特性，直观地意味着事后只改变少部分操作不会产生实质上更多的回报。我们提供了一个轻量级的基于上下文的强盗算法来评估和偶尔微调策略，并证明该方法在理论上和实证研究中均取得了有利的保证。

    Solutions to address the periodic review inventory control problem with nonstationary random demand, lost sales, and stochastic vendor lead times typically involve making strong assumptions on the dynamics for either approximation or simulation, and applying methods such as optimization, dynamic programming, or reinforcement learning. Therefore, it is important to analyze and evaluate any inventory control policy, in particular to see if there is room for improvement. We introduce the concept of an equilibrium policy, a desirable property of a policy that intuitively means that, in hindsight, changing only a small fraction of actions does not result in materially more reward. We provide a light-weight contextual bandit-based algorithm to evaluate and occasionally tweak policies, and show that this method achieves favorable guarantees, both theoretically and in empirical studies.
    
[^115]: 线性变换器及其循环和自指扩展的实用计算能力

    Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions. (arXiv:2310.16076v1 [cs.LG])

    [http://arxiv.org/abs/2310.16076](http://arxiv.org/abs/2310.16076)

    线性变换器（LTs）或快速权重程序员（FWPs）是一种特殊的序列处理器，可以用于循环神经网络以及自注意力网络。研究发现，类似于标准变换器的很多结果也适用于LTs/FWPs。循环FWP和自引用权重矩阵的扩展成功地克服了LT的一些限制，例如在奇偶问题上的泛化。

    

    最近关于循环神经网络（RNN）计算能力的研究揭示了一种给定实时和有限精度假设的RNN体系结构的层次结构。在这里，我们研究了具有线性化注意力的自回归变换器，也称为线性变换器（LT）或快速权重程序员（FWP）。LT在特定意义上是特殊的，因为它们等同于具有固定大小状态的类似RNN的序列处理器，同时也可以被表示为现在流行的自注意力网络。我们展示了许多关于标准变换器的知名结果如何直接转移到LTs/FWPs。我们的形式语言识别实验演示了最近提出的FWP扩展，如循环FWPs和自引用权重矩阵，如何成功克服LT的某些限制，例如在奇偶问题上的泛化。我们的代码是公开的。

    Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.
    
[^116]: 使用卷积LSTM在大学校园中预测电网频率

    Grid Frequency Forecasting in University Campuses using Convolutional LSTM. (arXiv:2310.16071v1 [cs.LG])

    [http://arxiv.org/abs/2310.16071](http://arxiv.org/abs/2310.16071)

    该论文介绍了一种创新的方法，利用卷积神经网络和长短期记忆网络建立稳健的时间序列预测模型，用于预测电网频率。个体化的卷积LSTM模型可以独立地为大学校园内的建筑进行训练和评估，并且结果证明了该模型的优越性。

    

    现代电网面临着越来越复杂的问题，主要源于可再生能源的整合和消费模式的演变。本文引入了一种创新的方法，利用卷积神经网络（CNN）和长短期记忆（LSTM）网络建立稳健的时间序列预测模型，用于电网频率。这些模型有效地捕捉了电网频率数据中固有的时空复杂性，显著提高了预测准确性，增强了电网的可靠性。研究探讨了大学校园内个体化的卷积LSTM（ConvLSTM）模型的潜力和发展，使它们能够独立地针对每一栋建筑进行训练和评估。个体ConvLSTM模型基于每栋校园建筑的用电数据进行训练，并根据历史趋势预测电网频率。结果有力地证明了所提出模型的优越性。

    The modern power grid is facing increasing complexities, primarily stemming from the integration of renewable energy sources and evolving consumption patterns. This paper introduces an innovative methodology that harnesses Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks to establish robust time series forecasting models for grid frequency. These models effectively capture the spatiotemporal intricacies inherent in grid frequency data, significantly enhancing prediction accuracy and bolstering power grid reliability. The research explores the potential and development of individualized Convolutional LSTM (ConvLSTM) models for buildings within a university campus, enabling them to be independently trained and evaluated for each building. Individual ConvLSTM models are trained on power consumption data for each campus building and forecast the grid frequency based on historical trends. The results convincingly demonstrate the superiority of the proposed mode
    
[^117]: 空间-时间超图神经网络用于交通预测

    Spatial-Temporal Hypergraph Neural Network for Traffic Forecasting. (arXiv:2310.16070v1 [cs.LG])

    [http://arxiv.org/abs/2310.16070](http://arxiv.org/abs/2310.16070)

    本文提出了一种名为STHODE的空间-时间超图神经网络用于交通预测，该网络结合了道路网络拓扑和交通动态，能够有效捕捉交通数据中的高阶空间-时间依赖性。

    

    交通预测在智能交通系统中起着至关重要的作用，它依靠移动互联网发展和定位技术，通过收集的交通数据来实现丰富多样的交通应用，并为人们带来便捷的交通服务。目前大部分现有方法通常利用基于图的深度学习网络浅层建模复杂的道路网络进行交通预测。然而，尽管这些方法的有效性，但它们通常在完全捕捉道路网络拓扑引起的高阶空间依赖性和交通动态引起的高阶时空依赖性方面存在局限性。为了解决以上问题，我们关注交通系统的本质，提出了STHODE（Spatio-Temporal Hypergraph Neural Ordinary Differential Equation Network），它结合了道路网络拓扑和交通动态，以捕捉交通数据中的高阶空间-时间依赖性。从技术上讲，STHODE由一个空间部分和一个时间部分组成，在高阶空间依赖性建模方面使用超图表示道路网络拓扑，同时使用普通微分方程网络来建模高阶时间依赖性。

    Traffic forecasting, which benefits from mobile Internet development and position technologies, plays a critical role in Intelligent Transportation Systems. It helps to implement rich and varied transportation applications and bring convenient transportation services to people based on collected traffic data. Most existing methods usually leverage graph-based deep learning networks to model the complex road network for traffic forecasting shallowly. Despite their effectiveness, these methods are generally limited in fully capturing high-order spatial dependencies caused by road network topology and high-order temporal dependencies caused by traffic dynamics. To tackle the above issues, we focus on the essence of traffic system and propose STHODE: Spatio-Temporal Hypergraph Neural Ordinary Differential Equation Network, which combines road network topology and traffic dynamics to capture high-order spatio-temporal dependencies in traffic data. Technically, STHODE consists of a spatial m
    
[^118]: 超维变换：函数的全息表示

    The Hyperdimensional Transform: a Holographic Representation of Functions. (arXiv:2310.16065v1 [cs.LG])

    [http://arxiv.org/abs/2310.16065](http://arxiv.org/abs/2310.16065)

    这项研究介绍了一种新型的积分变换-超维变换，它将函数转换为噪声鲁棒、全息、高维表示的超维向量，与其他积分变换紧密相关，并为超维领域提供了理论基础和新的洞察。

    

    积分变换是将函数映射到更容易表征的空间中的宝贵数学工具。我们引入了超维变换作为一种新型的积分变换。它将可积函数转换为噪声鲁棒、全息、高维表示，称为超维向量。其核心思想是用随机函数的线性组合来逼近一个函数。我们正式引入了一组随机的正交基函数，并定义了超维变换及其逆变换。我们讨论了一般变换相关的性质，如其唯一性、逆变换的逼近性质以及积分和导数的表示。超维变换提供了一个强大而灵活的框架，与傅里叶、拉普拉斯和模糊变换等其他积分变换密切相关。此外，它为超维领域提供了理论基础和新的洞察。

    Integral transforms are invaluable mathematical tools to map functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new kind of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdim
    
[^119]: 利用可学习的滤波模块提升交通预测

    Enhancing Traffic Prediction with Learnable Filter Module. (arXiv:2310.16063v1 [cs.LG])

    [http://arxiv.org/abs/2310.16063](http://arxiv.org/abs/2310.16063)

    本论文提出了一种利用可学习滤波模块来适应性地过滤交通数据中的噪声的方法，通过傅里叶变换将数据转换为频域，然后根据噪声的模式进行滤波，最后再通过逆傅里叶变换将数据恢复到时域。这种方法可以提升交通预测模型的输入数据质量。

    

    对未来交通条件的建模通常依赖于复杂的时空神经网络来捕捉时空相关性，但这可能忽视了数据中固有的噪声。这种噪声通常表现为交通观测中的意外短期波峰或波谷，通常是由交通事故或传感器固有振动导致的。在实践中，由于其随机性，这种噪声很难建模，并且如果神经网络被设计成学习这种行为，会导致过拟合的风险。为了解决这个问题，我们提出了一个可学习的滤波模块，来自适应性地过滤交通数据中的噪声。该模块利用傅里叶变换将数据转换为频域，在此基础上根据噪声的模式进行滤波。经过去噪处理后的数据再通过逆傅里叶变换恢复到时域。我们的方法着重提升交通预测模型的输入数据质量，这是一个关键但常被忽视的因素。

    Modeling future traffic conditions often relies heavily on complex spatial-temporal neural networks to capture spatial and temporal correlations, which can overlook the inherent noise in the data. This noise, often manifesting as unexpected short-term peaks or drops in traffic observation, is typically caused by traffic accidents or inherent sensor vibration. In practice, such noise can be challenging to model due to its stochastic nature and can lead to overfitting risks if a neural network is designed to learn this behavior. To address this issue, we propose a learnable filter module to filter out noise in traffic data adaptively. This module leverages the Fourier transform to convert the data to the frequency domain, where noise is filtered based on its pattern. The denoised data is then recovered to the time domain using the inverse Fourier transform. Our approach focuses on enhancing the quality of the input data for traffic prediction models, which is a critical yet often overloo
    
[^120]: 在预训练大模型微调中的对抗领域适应中进行混淆平衡

    Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning. (arXiv:2310.16062v1 [cs.LG])

    [http://arxiv.org/abs/2310.16062](http://arxiv.org/abs/2310.16062)

    本研究提出了一种对抗领域适应与混淆平衡（ADA-CBF）方法，用于在预训练大模型微调中考虑混淆因素。该方法通过联合训练预训练大模型、领域分类器和混淆分类器，并使用对抗损失来改善领域不变的表示学习和平衡混淆因子的分布。

    

    预训练大模型（PLMs）具有出色的泛化、上下文学习和出现能力，可以处理没有直接训练数据的特定任务，使它们成为对抗领域适应（ADA）方法中将从源领域学习的知识转移至目标领域的更好的基础模型。然而，现有的ADA方法未能正确考虑混淆因素，混淆因素是导致源数据分布与目标领域不同的根本原因。本研究提出了一种用于PLMs微调的对抗领域适应与混淆平衡（ADA-CBF）方法。ADA-CBF包括一个PLM作为特征提取器的基础模型，以及一个领域分类器和一个混淆分类器，它们通过对抗损失进行联合训练。这个损失旨在通过减弱领域分类器中的歧视来改进领域不变的表示学习。同时，对抗损失也平衡混淆因子的分布。

    The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes an adversarial domain adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The ADA-CBF includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distrib
    
[^121]: 一种应用于多站装配系统的稀疏贝叶斯学习方法，用于非平稳和空间相关故障的诊断

    A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially Correlated Faults with Application to Multistation Assembly Systems. (arXiv:2310.16058v1 [cs.LG])

    [http://arxiv.org/abs/2310.16058](http://arxiv.org/abs/2310.16058)

    本论文提出了一种用于制造系统故障诊断的新方法：空间相关稀疏贝叶斯学习。该方法可应对传感器数量有限和非平稳空间相关故障等挑战，并在多站装配系统中展示了其应用性。

    

    传感器技术的发展为制造系统中的故障诊断提供了基础。然而，由于物理限制或不必要的成本，传感器数量有限，这影响了实际过程中的准确诊断。此外，生成非平稳过程故障以及过程中的相关信息，需要考虑制造系统中准确故障诊断。本文提出了一种新颖的故障诊断方法：空间相关稀疏贝叶斯学习（CSSBL），并明确展示了其在容易受到上述挑战的多站装配系统中的适用性。具体而言，该方法基于一个实用假设，即它可能有几个过程故障（稀疏）。此外，CSSBL的分层结构具有多个参数化先验分布以应对上述挑战。由于过程故障的后验分布没有

    Sensor technology developments provide a basis for effective fault diagnosis in manufacturing systems. However, the limited number of sensors due to physical constraints or undue costs hinders the accurate diagnosis in the actual process. In addition, time-varying operational conditions that generate nonstationary process faults and the correlation information in the process require to consider for accurate fault diagnosis in the manufacturing systems. This article proposes a novel fault diagnosis method: clustering spatially correlated sparse Bayesian learning (CSSBL), and explicitly demonstrates its applicability in a multistation assembly system that is vulnerable to the above challenges. Specifically, the method is based on a practical assumption that it will likely have a few process faults (sparse). In addition, the hierarchical structure of CSSBL has several parameterized prior distributions to address the above challenges. As posterior distributions of process faults do not hav
    
[^122]: 使用GOES-16卫星观测数据的可解释性深度学习对大气对流起始进行预测

    Physically Explainable Deep Learning for Convective Initiation Nowcasting Using GOES-16 Satellite Observations. (arXiv:2310.16015v1 [physics.ao-ph] CROSS LISTED)

    [http://arxiv.org/abs/2310.16015](http://arxiv.org/abs/2310.16015)

    本研究开发了基于多通道红外卫星观测数据的可解释性深度学习模型，用于预测大气对流起始。通过案例研究，该模型表现出对云层和湿度特性的依赖性，并在误报率上显著优于经典的逻辑模型。

    

    大气对流起始（CI）的预测在数值天气预报模型和现有的预测算法中仍然是个具有挑战性的问题。本研究开发了基于多通道红外GOES-R卫星观测数据的基于对象的概率深度学习模型，用于预测CI。数据来自于2020年6月和7月以及2021年6月在美国大平原地区多雷达多传感器多普勒天气雷达产品中识别出的潜在CI事件周围的补丁。使用客观的基于雷达的方法来识别这些事件。深度学习模型在提前时间为1小时的情况下显著优于经典的逻辑模型，特别是在误报率上。通过案例研究，深度学习模型展示了对云层和多层次湿度特性的依赖性。模型解释进一步揭示了模型在不同基线下的决策过程。解释结果突出了湿度的重要性。

    Convection initiation (CI) nowcasting remains a challenging problem for both numerical weather prediction models and existing nowcasting algorithms. In this study, object-based probabilistic deep learning models are developed to predict CI based on multichannel infrared GOES-R satellite observations. The data come from patches surrounding potential CI events identified in Multi-Radar Multi-Sensor Doppler weather radar products over the Great Plains region from June and July 2020 and June 2021. An objective radar-based approach is used to identify these events. The deep learning models significantly outperform the classical logistic model at lead times up to 1 hour, especially on the false alarm ratio. Through case studies, the deep learning model exhibits the dependence on the characteristics of clouds and moisture at multiple levels. Model explanation further reveals the model's decision-making process with different baselines. The explanation results highlight the importance of moist
    
[^123]: 使用具有专门口音代码本的口音识别

    Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.15970](http://arxiv.org/abs/2310.15970)

    本研究提出了一种使用具有专门口音代码本的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。在实验证明了该方法在已见和未见的口音上都能获得显著的性能提升。

    

    语音口音对于现有自动语音识别（ASR）系统构成了重要挑战。在代表性不足的口音中的性能下降严重阻碍了ASR的普及应用。本研究提出了一种新颖的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。这些可学习的代码本捕捉了口音特定信息，并被整合到ASR编码器层中。模型在带口音的英语语音上进行训练，而测试数据中也包含了在训练过程中未见过的口音。在Mozilla Common Voice多口音数据集上，我们展示了我们提出的方法在不仅在已见的英语口音中获得显著的性能提升（单词错误率相对提升高达37%），而且在未见的口音上也获得了5%的相对提升。此外，我们还展示了在L2Artic数据集上的零样本迁移设置的好处。我们还进行了对比实验。

    Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
    
[^124]: 通过潜在引导扩散和嵌套集成改进医学图像分类的鲁棒性和可靠性

    Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15952](http://arxiv.org/abs/2310.15952)

    本文引入了一种新颖的三阶段方法，通过变换器和条件扩散模型来改善医学图像分类模型对实际应用中常见成像变异性的鲁棒性。

    

    尽管深度学习模型在各种医学图像分析任务中取得了显著的成功，但在真实临床环境中部署这些模型需要它们对所获取的图像的变异性具有鲁棒性。许多方法会对训练数据应用预定义的转换，以增强测试时的鲁棒性，但这些转换可能无法确保模型对患者图像中的多样性变异性具有鲁棒性。在本文中，我们提出了一种基于变换器和条件扩散模型的新型三阶段方法，旨在提高模型对实践中常见的成像变异性的鲁棒性，而无需预先确定的数据增强策略。为了实现这一目标，多个图像编码器首先学习分层特征表示来构建辨别潜在空间。接下来，一个由潜在代码引导的逆扩散过程作用于有信息先验，并提出预测候选。

    While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
    
[^125]: 在线健壮均值估计

    Online Robust Mean Estimation. (arXiv:2310.15932v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.15932](http://arxiv.org/abs/2310.15932)

    本文研究了在线高维健壮均值估计的问题，提出了两个主要结果。

    

    我们研究了在线设置中高维健壮均值估计的问题。具体而言，我们考虑一个场景，在这个场景中，n个传感器正在测量某个共同的持续现象。在每个时间步t=1,2,...,T，第i个传感器报告其在该时间步的读数x^(i)_t。然后，算法必须对该时刻的真实均值μ_t进行估计。我们假设大部分传感器观测到了来自某个公共分布X的独立样本，但是其中一个ε分数的传感器可能表现出恶意行为。算法希望计算出对真实均值μ*:=E[X]的良好近似值μ。我们注意到，如果允许算法等待到时间T才报告其估计值，那么这就变成了一个已经被广泛研究的健壮均值估计问题。然而，我们的算法要求在数据进来时生成部分估计值，这在很大程度上使情况变得复杂。我们证明了关于这个问题的两个主要结果。

    We study the problem of high-dimensional robust mean estimation in an online setting. Specifically, we consider a scenario where $n$ sensors are measuring some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the $i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The algorithm must then commit to its estimate $\mu_t$ for the true mean value of the process at time $t$. We assume that most of the sensors observe independent samples from some common distribution $X$, but an $\epsilon$-fraction of them may instead behave maliciously. The algorithm wishes to compute a good approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that if the algorithm is allowed to wait until time $T$ to report its estimate, this reduces to the well-studied problem of robust mean estimation. However, the requirement that our algorithm produces partial estimates as the data is coming in substantially complicates the situation.  We prove two main results about 
    
[^126]: COPF: 通过最优策略拟合实现持续学习人类偏好

    COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])

    [http://arxiv.org/abs/2310.15694](http://arxiv.org/abs/2310.15694)

    通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。

    

    强化学习通过人类反馈（RLHF）的技术是改善预训练语言模型（LM）以符合人类偏好的常用方法。然而，当前基于RLHF的LM在引入新的查询或反馈时需要完全重新训练，这是一项具有挑战性的任务，因为人类偏好在不同领域或任务之间可能会有所变化。由于所需的时间和计算资源以及与数据隐私相关的问题，重新训练LM在许多现实世界的情况下存在实际困难。为了解决这个限制，我们提出了一种新的方法，称为持续最优策略拟合（COPF），其中我们使用蒙特卡罗法估计一系列最优策略，然后通过函数正则化不断拟合策略序列。COPF包含一个单一的学习阶段，不需要复杂的强化学习。

    The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
    
[^127]: 在PyTorch上重新实现的VMAF：一些实验结果

    VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])

    [http://arxiv.org/abs/2310.15578](http://arxiv.org/abs/2310.15578)

    这项研究重新在PyTorch上实现了VMAF，与标准实现进行比较，结果显示在VMAF单位上的差异小于$10^{-2}$。同时，研究了在使用VMAF作为目标函数时的梯度计算，并证明使用该函数进行训练不会导致梯度不良。

    

    基于标准的VMAF实现，我们提出了使用PyTorch框架实现VMAF的方法。对于这个实现，与标准的(libvmaf)进行比较，VMAF单位上的差异小于$10^{-2}$。我们研究了在使用VMAF作为目标函数时的梯度计算，并证明使用该函数进行训练不会导致梯度不良。

    Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
    
[^128]: MGAS: 多粒度架构搜索以实现高效且有效的神经网络

    MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15074](http://arxiv.org/abs/2310.15074)

    MGAS是一个多粒度架构搜索的统一框架，通过学习特定粒度级别的离散化函数，自适应地确定剩余比例，从而实现同时优化模型大小和模型性能。

    

    可微分架构搜索(DAS)通过时间高效的自动化改变了神经网络架构搜索(NAS)的方式，从离散候选采样和评估转变为可微分超网络优化和离散化。然而，现有的DAS方法要么只进行粗粒度的操作级搜索，要么手动定义剩余的细粒度的核级和权重级单位的比例，从而无法同时优化模型大小和模型性能。此外，这些方法为了减少内存消耗而牺牲了搜索质量。为了解决这些问题，我们引入了多粒度架构搜索(MGAS)，这是一个统一的框架，旨在全面而内存高效地探索多粒度搜索空间，发现既有效又高效的神经网络。具体来说，我们学习了针对每个粒度级别的离散化函数，根据不断演化的架构自适应地确定剩余的比例。

    Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec
    
[^129]: 通过移除单个样本进行数据修剪

    Data Pruning via Moving-one-Sample-out. (arXiv:2310.14664v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14664](http://arxiv.org/abs/2310.14664)

    本文提出了一种新颖的数据修剪方法MoSo，它通过评估样本对最优经验风险的影响来确定每个样本的重要性，并提出了一种高效的一阶近似器来计算样本的重要性，该近似器只需要梯度信息。

    

    本文提出了一种新颖的数据修剪方法称为移除单个样本(MoSo)，旨在从训练集中识别并移除最不相关的样本。MoSo的核心思想是通过评估样本对最优经验风险的影响来确定每个样本的重要性。这通过衡量从训练集中排除一个特定样本时，经验风险的变化程度来实现。我们提出了一种高效的一阶近似器，它仅需要来自不同训练阶段的梯度信息，而不是使用计算上昂贵的逐个样本重新训练的过程。我们近似的关键思想是，梯度与训练集的平均梯度一致的样本更具信息量，并且应该获得更高的分数，可以直观地理解为：如果来自特定样本的梯度与平均梯度向量一致，则意味着

    In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies
    
[^130]: Attention-Enhancing Backdoor Attacks Against BERT-based Models（基于BERT模型的增强注意力的后门攻击）

    Attention-Enhancing Backdoor Attacks Against BERT-based Models. (arXiv:2310.14480v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14480](http://arxiv.org/abs/2310.14480)

    本文提出了一种基于BERT模型的增强注意力的后门攻击方法，通过直接操纵注意力模式来增强特洛伊行为，有效提高了攻击成功率和污染率。该方法适用于不同的任务和模型。

    

    最近的研究表明，“后门攻击”可能威胁到自然语言处理（NLP）模型的安全性。研究后门攻击的策略将有助于了解模型的脆弱性。大部分现有的文本后门攻击集中在生成隐蔽的触发词或修改模型权重上。本文直接针对神经网络的内部结构和后门机制，提出了一种新颖的特洛伊注意力损失（TAL），通过直接操纵注意力模式来增强特洛伊行为。我们的损失函数可以应用于不同的攻击方法，从而提高它们的攻击成功率和污染率。它不仅适用于传统的脏标签攻击，也适用于更具挑战性的干净标签攻击。我们在不同的骨干模型（BERT，RoBERTa和DistilBERT）和各种任务（情感分析，有害检测和主题分类）上验证了我们的方法。

    Recent studies have revealed that \textit{Backdoor Attacks} can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model's vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).
    
[^131]: 通过反向图卷积实现鲁棒的跨模态检索

    InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])

    [http://arxiv.org/abs/2310.13276](http://arxiv.org/abs/2310.13276)

    通过反向图卷积进行的鲁棒跨模态检索，解决了表示退化问题，并通过增加数据点之间的距离来有效分离不同模态的表示。

    

    近年来，跨模态检索的重大进展主要是通过视觉和语言建模的突破推动的。然而，最近的研究表明，多模态数据表示往往在有限的凸锥内聚集（作为表示退化问题），这由于这些表示的不可分离性而阻碍了检索性能。在我们的研究中，我们首先通过多个跨模态基准和方法经验证实了表示退化问题的存在。接下来，为了解决这个问题，我们引入了一种新方法，称为InvGC，它是一种受图卷积和平均池化启发的后处理技术。具体而言，InvGC在数据集中定义图拓扑，然后应用图卷积以一种减法的方式。这种方法通过增加数据点之间的距离来有效地分离表示。为了提高InvGC的效率和效果，我们提出了一个高级图拓扑，Lo

    Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, Lo
    
[^132]: 代词故事：可解释性指导下的公平指导机器翻译中的性别偏见缓解

    A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])

    [http://arxiv.org/abs/2310.12127](http://arxiv.org/abs/2310.12127)

    本研究通过调查机器翻译模型中的性别偏见问题以及缓解性别偏见的方法来填补现有研究的空白。研究发现指导微调模型在默认为男性翻译上存在性别偏见，同时忽视了指示职业性别的代词，并提出了一些可行的缓解策略。

    

    最近的指导微调模型可在多个NLP任务中解决问题，其中机器翻译（MT）是一个突出的用例。然而，当前的研究通常集中在标准性能基准上，忽视了引人注目的公平和伦理考虑。在MT中，这可能导致性别错误的翻译，从而导致刻板印象和偏见的持续存在。在这项工作中，我们通过调查这些模型在机器翻译中是否存在性别偏见以及如何缓解性别偏见来填补这一空白。具体而言，我们在从英文到德文和西班牙文的WinoMT语料库上计算已建立的性别偏见指标。我们发现指导微调模型默认为男性屈从翻译，甚至忽视女性职业刻板印象。接下来，使用可解释性方法，我们揭示了模型系统性地忽视指示目标职业性别的代词在同时性别错误的翻译中。最后，根据可解释性的发现，我们提出了性别偏见缓解的策略，并将其应用于MT模型中。

    Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
    
[^133]: 通过多矩阵可分解性在多人游戏中对自我对抗的保证

    Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])

    [http://arxiv.org/abs/2310.11518](http://arxiv.org/abs/2310.11518)

    这篇论文研究了多人游戏中自我对抗的保证问题，通过多矩阵可分解性，在满足一定条件的情况下，通过自我对抗学习的算法能够产生有界脆弱性的策略。

    

    自我对抗是一种机器学习在多智能体系统中的技术，其中学习算法通过与自身的副本交互来学习。自我对抗对于生成大量的学习数据很有用，但它的缺点是训练后学习者将面对的智能体可能与通过与自身交互时所期望的智能体行为截然不同。对于两人常和游戏的特殊情况，达到纳什均衡的自我对抗能够保证产生对任何训练后对手表现良好的策略；然而，对于多人游戏来说没有这样的保证存在。我们展示了在近似分解为一组两人常和游戏（称为多矩阵游戏）的游戏中，其中全局 $\epsilon$-纳什均衡在每个子游戏中都与纳什均衡有有界距离的情况下，通过自我对抗学习的无外部遗憾算法将产生一个有界脆弱性的策略。我们的结果首次确定了……

    Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
    
[^134]: ACES: 使用自我目标语言模型和语义描述符生成多样的编程难题

    ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])

    [http://arxiv.org/abs/2310.10692](http://arxiv.org/abs/2310.10692)

    ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。

    

    寻找和选择新颖有趣的问题是好奇心、科学和创新的核心。在Python编程难题的无限空间中，我们研究了自动问题生成。现有的生成模型通常旨在建模参考分布，没有明确的多样性优化。其他方法在有限的手工编码表示空间或不可解释的学习嵌入空间中明确优化多样性，这些嵌入空间可能与人类对有趣变化的感知不符。通过ACES（自我目标代码探索与语义描述符），我们引入了一种新的自我目标生成方法，利用大型语言模型（LLM）生成语义描述符，直接优化有趣的多样性，以及少样本生成。每个难题都标记有10个维度，每个维度捕捉了解决它所需的编程技能。ACES生成并追求新颖可行的目标。

    Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
    
[^135]: 无限宽度图神经网络用于节点回归/分类

    Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])

    [http://arxiv.org/abs/2310.08176](http://arxiv.org/abs/2310.08176)

    本研究分析了无限宽度图神经网络在图结构化数据上的应用。通过连接深度学习和高斯过程/核方法，研究推广了神经网络，并推导出了闭式形式的核函数和高斯过程。研究结果表明，高斯过程和核方法在不确定性估计方面更加用户友好，并且可以在多种架构和数据集上进行回归/分类任务。

    

    本研究分析了图神经网络，在每个全连接层的节点数量趋近无穷大时，它是对图结构化数据上全连接深度神经网的一种推广。无限宽度神经网络将深度学习与高斯过程和核方法相连接，后者都是具有悠久传统和丰富理论基础的机器学习框架。高斯过程和核方法的超参数较少，可用于不确定性估计，使其在应用中更加用户友好。本研究扩展了将高斯过程和核方法与神经网络相连接的研究数量不断增加的趋势。对于多种架构（包括标准图神经网络、具有跳跃连接的图神经网络和图注意力神经网络），推导出了核函数和高斯过程的闭式形式。对这些架构在各种数据集上进行了评估，并进行了回归/分类任务。

    This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task
    
[^136]: GATs是否失衡？

    Are GATs Out of Balance?. (arXiv:2310.07235v1 [cs.LG])

    [http://arxiv.org/abs/2310.07235](http://arxiv.org/abs/2310.07235)

    本研究揭示了Graph Attention Network (GAT)梯度流动力学的守恒定律，解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。我们还提出了一种平衡GAT网络的初始化方案，使得深层网络更容易进行训练，并且相比标准初始化，具有更快的收敛速度。

    

    虽然图神经网络（GNNs）的表达能力和计算能力已经在理论上得到了研究，但它们的优化和学习动态在大多数情况下仍然未被探索。我们的研究针对图注意力网络（GAT），这是一种流行的GNN架构，其中节点的邻域聚合由参数化的注意力系数加权。我们推导出GAT梯度流动力学的守恒定律，这解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。这种效应在深层的GAT中被放大，它们的性能要明显差于浅层的GAT。为了缓解这个问题，我们设计了一种平衡GAT网络的初始化方案。我们的方法 i) 可以更有效地传播梯度，从而使深层网络可训练，ii) 相比于标准初始化，可以实现训练和收敛时间的显著加速。

    While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. O
    
[^137]: 最优探索不比汤普森采样更困难

    Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])

    [http://arxiv.org/abs/2310.06069](http://arxiv.org/abs/2310.06069)

    这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？

    

    在给定一组臂$\mathcal{Z}\subset \mathbb{R}^d$和未知参数向量$\theta_\ast\in\mathbb{R}^d$的情况下，纯探索线性臂问题旨在通过对$x^{\top}\theta_{\ast}$的噪声测量，返回$\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$，并以高概率找到正确解。现有的（渐近）最优方法要求要么为每个臂$z\in \mathcal{Z}$进行潜在昂贵的投影，要么在每个时间点明确地维护一部分正在考虑的$\mathcal{Z}$。这种复杂性与流行且简单的汤普森采样算法用于最小化后悔的情况完全相反，后者只需要访问后验采样和argmax oracle，并且在任何时间点都不需要枚举$\mathcal{Z}$。不幸的是，已知汤普森采样对于纯探索是次优的。在这项工作中，我们提出了一个自然的问题：是否存在一种算法能够进行最优探索，而且只需要相同的计算操作？

    Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput
    
[^138]: DSAC-T: 带有三个改进的分布式软角色扮演者—评论者

    DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05858](http://arxiv.org/abs/2310.05858)

    本论文介绍了DSAC-T，通过评论者梯度调整、双值分布学习和基于方差的目标回报裁剪等三个改进对标准DSAC进行了改进，解决了标准DSAC存在的不稳定学习过程和对任务特定奖励缩放的问题，提高了算法的性能和适应性。

    

    强化学习在处理复杂的决策和控制任务方面已经被证明非常有效。然而，常见的无模型的强化学习方法往往面临严重的性能下降问题，这是由于众所周知的过估计问题所引起的。作为对这个问题的回应，我们最近引入了一种离线策略的强化学习算法，称为分布式软角色扮演者评论者（DSAC或DSAC-v1），它通过学习连续的高斯值分布来有效提高值估计的准确性。然而，标准DSAC也存在一些缺点，包括时而不稳定的学习过程和对任务特定的奖励缩放的需求，这可能会阻碍其在一些特殊任务中的整体性能和适应性。本文进一步引入了三个对标准DSAC的重要改进，以解决这些问题。这些改进包括评论者梯度调整、双值分布学习和基于方差的目标回报裁剪。修改后的强化学习算法称为DSAC-T。

    Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
    
[^139]: 使用人工编辑改进摘要生成

    Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05857](http://arxiv.org/abs/2310.05857)

    本文介绍了一种改进摘要生成的方法，使用人工编辑的反馈数据，并通过序列对齐（不）似然训练(SALT)技术将人工编辑数据与模型生成数据结合起来。实验证明了这种方法在医学领域摘要生成中的有效性。

    

    最近的研究表明，通过人类反馈范式学习可以产生高质量的文本。现有的工作在通用领域抽象化摘要生成中使用人类反馈来训练大型语言模型(LLMs)，并获得了超越传统似然训练的摘要质量。在本文中，我们关注一种较少探索的人类反馈形式——人工编辑。我们提出了一种新颖的技术——序列对齐（不）似然训练(SALT)，在训练循环中同时使用人工编辑和模型生成的数据。此外，我们还展示了使用现有训练数据中的基准摘要来模拟人工编辑，以及在训练后获取的模型生成摘要，以减少对昂贵的人工编辑数据的需求。在实验中，我们将人类反馈的探索从通用领域摘要生成扩展到医学领域摘要生成。我们的结果表明SALT在改进摘要生成方面的有效性。

    Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
    
[^140]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^141]: 地震图变压器：用于多种地震监测任务的通用深度学习骨干网络

    Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks. (arXiv:2310.01037v2 [physics.geo-ph] UPDATED)

    [http://arxiv.org/abs/2310.01037](http://arxiv.org/abs/2310.01037)

    本文介绍了一种名为地震图变压器（SeisT）的通用深度学习骨干网络模型，用于多种地震监测任务。SeisT的高效网络架构使其在地震检测、地震相位拾取、首次运动极性分类、震级估计和反方位角估计等任务中表现优秀，特别是在泛化性能方面。

    

    地震记录，即地震图，是由地震事件引起的地面运动的重要记录，是地震研究和监测的基础。深度学习的最新进展极大地促进了各种地震信号处理任务。本文介绍了一种新颖的骨干神经网络模型，用于各种地震监测任务，名为地震图变压器（SeisT）。由于其高效的网络架构，SeisT在地震检测、地震相位拾取、首次运动极性分类、震级估计和反方位角估计等任务中能够与甚至超过最先进的模型匹配，特别是在超出分布的泛化性能方面。SeisT由多个网络层组成，这些层由不同的基础模块组成，帮助模型理解地震图的多层次特征表达，从低层次到高层次的复杂特征，有效地提取特征。

    Seismic records, known as seismograms, are crucial records of ground motion resulting from seismic events, constituting the backbone of earthquake research and monitoring. The latest advancements in deep learning have significantly facilitated various seismic signal processing tasks. This paper introduces a novel backbone neural network model designed for various seismic monitoring tasks, named Seismogram Transformer (SeisT). Thanks to its efficient network architecture, SeisT matches or even outperforms the state-of-the-art models in earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, and back-azimuth estimation tasks, particularly in terms of out-of-distribution generalization performance. SeisT consists of multiple network layers composed of different foundational blocks, which help the model understand multi-level feature representations of seismograms from low-level to high-level complex features, effectively extracting features
    
[^142]: 机器学习中的一次出训练数据的可辨识性分析

    Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])

    [http://arxiv.org/abs/2309.17310](http://arxiv.org/abs/2309.17310)

    这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。

    

    我们引入了一个新的分析框架，用于量化机器学习算法在训练集中包含少量数据点后输出分布的变化，我们将这个概念定义为一次出训练数据的可辨识性(LOOD)。这个问题对于衡量机器学习中的数据记忆和信息泄漏以及训练数据对模型预测的影响至关重要。我们使用高斯过程模型来建模机器学习算法的随机性，并通过对成员推断攻击使用广泛的经验分析验证了LOOD。我们的理论框架使我们能够研究信息泄漏的原因以及泄漏程度高的位置。例如，我们分析了激活函数对数据记忆的影响。此外，我们的方法允许我们优化...

    We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim
    
[^143]: 学习接受帮助：干预感知的概念嵌入模型

    Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])

    [http://arxiv.org/abs/2309.16928](http://arxiv.org/abs/2309.16928)

    这项研究提出了一种干预感知的概念嵌入模型，用于提高神经架构对概念干预的响应性，并解决了概念干预顺序和模型架构的依赖性的问题。

    

    概念瓶颈模型（CBMs）通过使用一组高级概念构建和解释神经架构的预测，以解决其不透明性的问题。这些模型的一个特殊属性是它们允许概念干预，用户可以纠正被错误预测的概念，从而提高模型的性能。然而，最近的研究表明，干预有效性可能严重依赖于干预概念的顺序以及模型的架构和训练超参数。我们认为，这源于CBM在训练时缺乏模型适应概念干预的激励。为了解决这个问题，我们提出了干预感知的概念嵌入模型（IntCEMs），这是一种基于CBM的新型架构和训练范式，可以提高模型对测试时干预的响应性。我们的模型以端到端的方式学习了一个概念干预策略，从中可以采样有意义的干预轨迹。

    Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
    
[^144]: 一种联合交互式导航的扩散模型

    A Diffusion-Model of Joint Interactive Navigation. (arXiv:2309.12508v1 [cs.LG])

    [http://arxiv.org/abs/2309.12508](http://arxiv.org/abs/2309.12508)

    本文提出了一种基于扩散模型的方法 DJINN，用于生成交通场景。通过联合扩散所有代理的轨迹，并以灵活的状态观察为条件，我们在轨迹预测上取得了最先进的性能。此外，DJINN还能灵活地从多种有价值的条件分布中进行测试时抽样。

    

    模拟自动驾驶系统需要模拟出展现多样和真实行为的交通参与者。在模拟中使用实际世界交通场景确保了真实性，但是安全关键事件的罕见性使得大规模收集驾驶场景具有高昂的成本。在本文中，我们提出了一种基于扩散的生成交通场景的方法DJINN。我们的方法联合扩散所有代理的轨迹，以过去、现在或未来的一系列灵活的状态观察为条件。在流行的轨迹预测数据集上，我们报道了在联合轨迹评估指标上的最先进性能。此外，我们展示了DJINN如何灵活地使得从各种有价值的条件分布中直接进行测试时抽样，包括基于目标的抽样、行为类别抽样和场景编辑。

    Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN - a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing.
    
[^145]: 用自适应多模态奖励引导你的智能体

    Guide Your Agent with Adaptive Multimodal Rewards. (arXiv:2309.10790v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.10790](http://arxiv.org/abs/2309.10790)

    本文提出了一种自适应返回条件策略（ARP）框架，通过使用自然语言任务描述和预训练的多模态编码器来提升智能体的泛化能力。通过在预训练的多模态嵌入空间中计算视觉观察和自然语言指令之间的相似度，并将其用作奖励信号，ARP有效缓解了目标误泛化问题，并在面对未知的文本指令时展现出了出色的泛化性能。

    

    在模仿学习中，开发一个能够适应未知环境的智能体仍然是一个具有挑战性的问题。本文提出了一种名为自适应返回条件策略(ARP)的高效框架，用于通过自然语言任务描述和预训练的多模态编码器来提升智能体的泛化能力。我们的关键思想是在预训练的多模态嵌入空间(例如CLIP)中计算视觉观测和自然语言指令之间的相似度，并将其作为奖励信号。然后，我们使用用多模态奖励标记的专家演示来训练一个返回条件策略。由于多模态奖励在每个时间步提供自适应信号，我们的ARP有效地缓解了目标误泛化问题。与现有的文本条件策略相比，即使面对未知的文本指令，我们的ARP在泛化性能方面也表现出众。为了提高奖励的质量，我们还引入了一种预训练微调方法。

    Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. This work presents Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-traine
    
[^146]: 一个可配置的库用于生成和操作迷宫数据集

    A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])

    [http://arxiv.org/abs/2309.10498](http://arxiv.org/abs/2309.10498)

    这个论文介绍了一个可配置的库，用于生成和处理迷宫数据集，研究人员可以通过该库生成不同分布的迷宫数据集，并对生成参数和生成规则进行自定义控制。可以支持多种输出格式，适用于不同类型的模型。

    

    理解机器学习模型对分布偏移的响应方式是一个重要的研究挑战。由于不同的生成算法提供了一个细致的平台来模拟微妙和显著的分布偏移，迷宫作为一个优秀的测试基准。为了支持对模型在分布偏离数据上行为的系统性研究，我们提出了“maze-dataset”，一个包含迷宫求解任务的生成、处理和可视化数据集的综合库。借助这个库，研究人员可以轻松创建数据集，可以对使用的生成算法、传递给选择算法的参数和生成的迷宫必须满足的筛选器进行广泛的控制。此外，它支持多种输出格式，包括栅格化和基于文本的格式，适用于卷积神经网络和自回归变换模型。这些格式以及用于可视化和转换的工具确保了灵活性和适应性。

    Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
    
[^147]: 无约束特征模型中的交叉熵损失下不受限的神经塌缩机制

    Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data. (arXiv:2309.09725v1 [stat.ML])

    [http://arxiv.org/abs/2309.09725](http://arxiv.org/abs/2309.09725)

    在无约束特征模型的背景下，我们研究了交叉熵损失函数下不均衡数据的神经塌缩现象。

    

    近年来，深度神经网络（DNNs）在计算机视觉和文本处理的各种任务中取得了巨大的成功。有趣的是，这些具有大量参数的DNNs在训练的末期阶段（TPT）的特征表示和末层分类器具有相似的结构特性。具体而言，如果训练数据是平衡的（每个类别具有相同数量的样本），观察到来自同一类别的样本的特征向量收敛到相应的类内均值特征，并且它们的成对角度相同。这一迷人的现象被称为神经塌缩（NC），由Papyan，Han和Donoho在2019年首次提出。最近的许多工作通过采用所谓的无约束特征模型（UFM）在理论上解释了这一现象。在本文中，我们研究了在无约束特征模型的上下文中，弥补了NC现象对不均衡数据在交叉熵损失函数下的拓展。我们的贡献是

    Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is
    
[^148]: RePo: 通过正则化后验可预测性增强弹性模型基础强化学习

    RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. (arXiv:2309.00082v1 [cs.LG])

    [http://arxiv.org/abs/2309.00082](http://arxiv.org/abs/2309.00082)

    本文提出了RePo算法，通过正则化后验可预测性的方式，增强了视觉模型基础强化学习方法的弹性。该方法通过学习一个对冗余和伪变化具有弹性的潜在表示，提高了方法对视觉干扰的鲁棒性，使其能够在动态环境中运行。

    

    视觉模型基础强化学习方法通常将图像观测编码为低维表示方式，这种方式未能消除冗余信息。这使得这些方法容易受到伪变化的影响，即与任务无关的组成部分的变化，如背景干扰因素或光照条件的变化。本文提出了一种视觉模型基础强化学习方法，该方法学习到了一种对这种伪变化具有弹性的潜在表示。我们的训练目标鼓励该表示在动力学和奖励预测方面具有最大的预测性，同时限制了观测到潜在表示的信息流。我们证明了这一目标极大增强了视觉模型基础强化学习方法对视觉干扰的弹性，使其能够在动态环境中运行。然后我们展示了虽然学习到的编码器对伪变化具有弹性，但在显著分布变化下并没有不变性。为了解决这个问题，我们提出了一个简单的奖励方案。

    Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-f
    
[^149]: CL-MAE: 课程学习的遮罩自编码器

    CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])

    [http://arxiv.org/abs/2308.16572](http://arxiv.org/abs/2308.16572)

    本文提出了一种课程学习的遮罩自编码器（CL-MAE）。我们引入了一种可学习的遮罩模块，通过更新遮罩策略来增加自监督重构任务的复杂性。通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。

    

    遮罩图像建模已被证明是一种强大的预文本任务，用于生成能够有效泛化到多个下游任务的鲁棒表示。通常，这种方法涉及在输入图像中随机遮罩补丁（标记），并且遮罩策略在训练过程中保持不变。本文提出了一种课程学习方法，通过更新遮罩策略以持续增加自监督重构任务的复杂性。我们推测，通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。为了实现这一点，我们引入了一种新颖的可学习遮罩模块，具有生成不同复杂度遮罩的能力，并将该模块与遮罩自编码器（MAE）集成。我们的模块与MAE一同训练，同时调整其行为，在训练过程中从MAE的参与者过渡到MAE（优化相同的重构目标）。

    Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
    
[^150]: 未来药物发现的实施：基于量子的机器学习模拟(QMLS)。

    Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.08561](http://arxiv.org/abs/2308.08561)

    该论文介绍了一种名为QMLS的新概念，通过结合机器学习和量子模拟的方法，可以缩短药物研发的时间和降低成本。通过生成命中物和优化分子的过程，可以大大提高药物发现的效率。

    

    药物研发的研究与开发(R&D)阶段是一个漫长而昂贵的过程。为了改革这个过程，我们引入了新概念QMLS，将整个R&D阶段缩短到三到六个月，成本仅为五到八万美元。对于命中产生，机器学习分子生成(MLMG)根据目标蛋白的分子结构生成可能的命中物，而量子模拟(QS)根据与目标蛋白的反应和结合效果过滤原始实验中的分子。然后，对于铅优化，从MLMG和QS生成和过滤的结果分子进行比较，并通过机器学习分子变异(MLMV)将那些出现在两个过程中的分子制成数十种分子变体，而其他分子只制成几种变体。最后，所有优化的分子将经过多轮高标准的QS过滤，以确保反应效果。

    The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
    
[^151]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^152]: AgentBench: 评估LLMs作为代理人

    AgentBench: Evaluating LLMs as Agents. (arXiv:2308.03688v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.03688](http://arxiv.org/abs/2308.03688)

    AgentBench是一个用于评估LLMs作为代理人的多维度基准，发现在复杂环境中，商业LLMs在充当代理人方面表现强劲，但与开源竞争对手相比，存在显著性能差距。该研究揭示了LLMs在长期推理、决策和指令遵循能力上的瓶颈。

    

    大型语言模型(LLMs)变得越来越智能和自主，针对传统的NLP任务之外的现实世界实际任务。因此，迫切需要在互动环境中评估LLMs作为代理人在具有挑战性的任务上的推理和决策能力。我们提出了AgentBench，一个多维度演变的基准，目前包括8个不同的环境，以评估LLM作为代理人在多轮开放式生成设置中的推理和决策能力。我们在27个基于API和开源的LLM上进行了广泛的测试，结果表明，虽然顶级商业LLM在复杂环境中表现出良好的代理人能力，但它们与开源竞争对手之间的性能差距很大。我们找出了环境和LLM中失败的典型原因，表明长期推理、决策和遵循指示能力不佳是开发可用LLM代理人的主要障碍。通过对代码和高质量进行训练

    Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality 
    
[^153]: 关于高斯数据之外的单指数模型

    On Single Index Models beyond Gaussian Data. (arXiv:2307.15804v1 [cs.LG])

    [http://arxiv.org/abs/2307.15804](http://arxiv.org/abs/2307.15804)

    该论文研究了超越高斯数据的单指数模型，探索了对稳定性和对称性的违反情况下的样本复杂性控制。

    

    稀疏高维函数已成为研究使用浅层神经网络的梯度下降方法行为的丰富框架，展示了它们在线性模型之外进行特征学习的能力。其中最简单的是单指数模型 $f(x) = \phi( x \cdot \theta^*)$，其中标签由一个未知的一维投影 $\theta^*$ 应用于任意非线性标量连接函数 $\phi$ 产生。通过专注于高斯数据，最近几项研究工作建立了一个引人注目的图景，将信息指数（与连接函数的正则性相关）与所需的样本复杂性进行了控制。实质上，这些工具利用了高斯分布的稳定性和球对称性。在本研究中，我们从 \cite{arous2020online} 的框架出发，探索了超越高斯设定的这个图景的扩展，其中稳定性或对称性可能被违反。

    Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \phi( x \cdot \theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\phi$ applied to an unknown one-dimensional projection $\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the pl
    
[^154]: 机器学习模型的局部鲁棒性的高效估计

    Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])

    [http://arxiv.org/abs/2307.13885](http://arxiv.org/abs/2307.13885)

    本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    

    机器学习模型通常需要对噪声输入数据具有鲁棒性。现实世界中的噪声（通常是随机的）对模型预测的影响可以通过模型的局部鲁棒性来捕捉，即在输入周围的局部区域内模型预测的一致性。然而，基于蒙特卡罗采样的计算局部鲁棒性的朴素方法在统计上是低效的，对于大规模应用而言计算成本高昂。在这项工作中，我们通过局部线性函数逼近和多元正态CDF开发了首个分析估计器，以高效计算多类别判别模型的局部鲁棒性。通过这些估计器的推导，我们展示了局部鲁棒性与随机平滑和softmax概率等概念的联系。我们还通过实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
    
[^155]: WebArena: 一个用于构建自主智能体的真实网络环境

    WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])

    [http://arxiv.org/abs/2307.13854](http://arxiv.org/abs/2307.13854)

    WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。

    

    随着生成式人工智能的进展，通过自然语言指令进行日常任务的自主智能体的潜力逐渐显现。然而，当前的智能体主要是在简化的合成环境中创建和测试的，严重限制了现实世界场景的表示能力。在本文中，我们构建了一个高度逼真且可复现的智能体指令和控制环境。具体而言，我们关注在网站上执行任务的智能体，我们创建了一个包含来自四个常见领域的完全功能网站的环境，分别是电子商务、社交论坛讨论、协同软件开发和内容管理。我们的环境使用工具（如地图）和外部知识库（如用户手册）来鼓励像人类一样解决任务。在我们的环境基础上，我们发布了一组重点评估任务完成功能正确性的基准任务。我们基准任务具有多样性和长远的视野，并且被设计为鼓励智能体进行更深层次的任务理解和解决。

    With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
    
[^156]: SynerGPT:上下文学习用于个性化药物协同作用预测和药物设计

    SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])

    [http://arxiv.org/abs/2307.11694](http://arxiv.org/abs/2307.11694)

    本文提出了一种通过上下文学习个性化药物协同作用并进行药物设计的方法，该方法利用小型的个性化数据集，不依赖于文本语料库、分子指纹或蛋白质相互作用的领域特定知识，取得了竞争性的结果。

    

    预测药物的协同组合可以加速癌症治疗的发现，特别是通过活检细胞个性化的治疗。在本文中，我们提出了一种新的设置和模型用于上下文中的药物协同学习。我们给出了一个小的“个性化数据集”，其中包含特定癌症靶细胞上下文中的10-20个药物协同关系。我们的目标是预测该上下文中的额外药物协同关系。受最近工作的启发，该工作通过预训练GPT语言模型（LM）来“上下文学习”常见的功能类。我们设计了一种 新的预训练方案，使GPT模型能够上下文学习“药物协同功能”。我们的模型 - 不使用任何文本语料库，分子指纹，蛋白质相互作用或任何其他领域特定的知识 - 能够取得竞争性的结果。我们进一步将我们的上下文方法与遗传算法结合起来，以优化模型提示并选择协同候选项。

    Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates
    
[^157]: 朝着全球生物多样性评估迈出的一步：BIOSCAN-1M昆虫数据集

    A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])

    [http://arxiv.org/abs/2307.10455](http://arxiv.org/abs/2307.10455)

    提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。

    

    为了对昆虫生物多样性进行编目，我们提出了一个新的大型手工标记昆虫图像数据集，即BIOSCAN-Insect数据集。每个记录都由专家进行分类，并且具有相关的遗传信息，包括原始核苷酸条形码序列和分配的条形码索引号，这些是基于遗传的物种分类的代理。本文介绍了一个精选的百万图像数据集，主要用于训练能够提供基于图像的分类评估的计算机视觉模型，但该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。由于数据集固有的生物性质，展现出了具有长尾类别不平衡分布的特征。此外，分类标签是一个分层分类方案，在较低级别上呈现出高度细粒度的分类问题。除了激发对生物多样性研究的兴趣外，该数据集还促进了对机器学习的深入研究。

    In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
    
[^158]: 使用前softmax分数的归属方法的一个漏洞

    A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])

    [http://arxiv.org/abs/2307.03305](http://arxiv.org/abs/2307.03305)

    这篇论文讨论了使用前softmax分数的归属方法的一个漏洞，该方法用于解释卷积神经网络分类器输出。与对抗性攻击不同，作者关注的是对归属方法进行小修改可能导致的影响，而不会改变模型的输出。

    

    我们讨论了一类用于解释卷积神经网络分类器输出的归属方法的一个漏洞。已知这种类型的网络容易受到对抗性攻击的影响，即输入的微小扰动可能会改变模型的输出。与此不同的是，我们关注的是对归属方法进行小修改可能导致的影响，而不会改变模型的输出。

    We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
    
[^159]: PlanE: 平面图的表示学习

    PlanE: Representation Learning over Planar Graphs. (arXiv:2307.01180v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01180](http://arxiv.org/abs/2307.01180)

    本研究的目标是设计用于高效学习平面图完备不变量的架构。

    

    图神经网络是用于图表示学习的杰出模型，其思想是通过一系列变换来迭代计算输入图中节点的表示，从而学习到的图函数在图同构时是不变的，从而使学习到的表示成为图不变量。另一方面，众所周知，这类模型学习到的图不变量是不完备的：存在一些非同构的图对，标准图神经网络无法区分它们。这在对一般图进行同构性测试的计算困难性的情况下并不令人惊讶，但对于一些特殊的图类来说，情况可能有所不同，例如平面图，对于这些图，已知存在高效的图同构测试算法。本文的目标是设计用于高效学习平面图完备不变量的架构。受Hopcroft和

    Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and
    
[^160]: 可分离的物理信息神经网络

    Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])

    [http://arxiv.org/abs/2306.15969](http://arxiv.org/abs/2306.15969)

    这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。

    

    物理信息神经网络(PINNs)最近已经成为有希望的基于数据的PDE求解器，在各种PDE上显示出令人鼓舞的结果。然而，训练PINNs来解决多维PDE和逼近高度复杂解函数存在根本限制。在这些具有挑战性的PDE上所需的训练点数量(配点)大大增加，但由于昂贵的计算成本和庞大的内存开销，其受到严重限制。为了解决这个问题，我们提出了一种用于PINNs的网络架构和训练算法。所提出的方法，可分离的PINN (SPINN)，在多维PDE中按轴逐个处理，从而显著减少了网络传播的数量，不同于传统PINNs中的逐点处理。我们还提出使用正向模式自动微分来降低计算PDE残差的计算成本，从而在单个普通GPU上可以使用大量的配点(>10^7)。

    Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (>10^7) on a single commodity GPU. The
    
[^161]: 无监督的剧集生成方法用于图元学习

    Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])

    [http://arxiv.org/abs/2306.15217](http://arxiv.org/abs/2306.15217)

    本文研究了无监督的剧集生成方法，通过元学习解决没有标签的少样本节点分类问题。它们充分利用所有节点信息，并且通过泛化能力提高性能。

    

    本文研究了无监督的剧集生成方法，通过元学习来解决没有标签的少样本节点分类问题。主流的少样本节点分类的元学习方法是在存在大量有标签节点用于训练的情况下开发的，然而在现实世界中可能无法获得这样的数据。虽然已经提出了一些解决标签稀缺性问题的研究，但它们仍然依赖于有限数量的有标签数据，这限制了对图中所有节点信息的充分利用。尽管自监督学习方法在没有标签的节点分类问题上很有效，但它们主要学习通用的节点嵌入，没有考虑要解决的下游任务，这可能限制了其性能。在这项工作中，我们提出了无监督的剧集生成方法，以利用它们在少样本节点分类任务中的泛化能力，同时解决标签稀缺性问题。我们首先提出了一种利用图增强方法的方法

    In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
    
[^162]: DiffInfinite: 通过并行随机补丁扩散在组织病理学中实现大型蒙版图像合成

    DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology. (arXiv:2306.13384v1 [eess.IV])

    [http://arxiv.org/abs/2306.13384](http://arxiv.org/abs/2306.13384)

    DiffInfinite 是一种能够生成任意大的组织学图像，使用小补丁进行快速训练，并可以更高效地并行化的方法，在组织病理学成像实践中有效解决了大规模信息、昂贵的手动注释以及保护性数据处理等独特挑战。

    

    我们提出了 DiffInfinite，这是一种层次扩散模型，可以生成任意大的组织学图像，同时保持长程相关性结构信息。我们的方法先生成合成分割掩模，随后用作高保真度生成扩散过程的条件。所提出的采样方法可以扩展到任意所需图像尺寸，而只需要小补丁进行快速训练。此外，相较于以往的大型内容生成方法，它可以更高效地并行化，同时避免平铺反射式伪影。训练利用了无分类器指导的方法，使用无标注数据扩充小型、稀疏注释的数据集。我们的方法缓解了组织病理学成像实践中的独特挑战：大规模信息、昂贵的手动注释以及保护性数据处理。DiffInfinite 数据的生物合理性由十名有经验的病理学家进行调查验证，以及下游分割任务。

    We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artefacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is validated in a survey by ten experienced pathologists as well as a downstream segmentation task. Fu
    
[^163]: 基于带有嘈杂贝叶斯反馈的零和矩阵博弈的对数遗憾对策略

    Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback. (arXiv:2306.13233v1 [cs.LG])

    [http://arxiv.org/abs/2306.13233](http://arxiv.org/abs/2306.13233)

    本文提出了一种算法，在带有嘈杂贝叶斯反馈的零和矩阵博弈中，实现了对数遗憾策略。

    

    本文研究了零和矩阵博弈的变种，其中每步行选手选择一行$i$，列选手选择一列$j$，行选手收到平均值为$A_{i,j}$的嘈杂奖励。行选手的目标是尽可能地累积奖励，即使对手是一个对手性列选手。该文提出了一种策略，该策略证明在$m \times n$矩阵博弈中，实现了$O(\sqrt{mnT})$对数遗憾，进一步提高了UCB风格算法所获得的$O(m\sqrt{nT})$对数遗憾。

    This paper considers a variant of zero-sum matrix games where at each timestep the row player chooses row $i$, the column player chooses column $j$, and the row player receives a noisy reward with mean $A_{i,j}$. The objective of the row player is to accumulate as much reward as possible, even against an adversarial column player. If the row player uses the EXP3 strategy, an algorithm known for obtaining $\sqrt{T}$ regret against an arbitrary sequence of rewards, it is immediate that the row player also achieves $\sqrt{T}$ regret relative to the Nash equilibrium in this game setting. However, partly motivated by the fact that the EXP3 strategy is myopic to the structure of the game, O'Donoghue et al. (2021) proposed a UCB-style algorithm that leverages the game structure and demonstrated that this algorithm greatly outperforms EXP3 empirically. While they showed that this UCB-style algorithm achieved $\sqrt{T}$ regret, in this paper we ask if there exists an algorithm that provably ach
    
[^164]: 学习未见过的模态交互

    Learning Unseen Modality Interaction. (arXiv:2306.12795v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2306.12795](http://arxiv.org/abs/2306.12795)

    本文提出了一个解决多模态学习中未见过的模态组合的问题的方法。该方法利用一个模块将不同模态的特征投影到一个共享的空间中，并通过伪监督来减少过拟合。实验证明该方法在多个任务和模态上都是有效的。

    

    多模态学习假定在训练期间可以使用所有感兴趣的模态组合来学习跨模态对应关系。本文对多模态学习中的模态完整性做出了挑战，而是在推理过程中努力实现对未见过的模态组合的泛化。我们提出了未见过的模态交互问题，并介绍了第一个解决方案。它利用一个模块将不同模态的多维特征投影到一个具有多样信息的公共空间中，从而可以通过简单的求和操作对可用的模态进行信息累积。为了减少在训练过程中对较少辨别力的模态组合的过拟合，我们进一步改进了模型学习，通过伪监督指示模态预测的可靠性。我们通过对多模态视频分类、机器人状态回归等进行评估，证明了我们的方法对于不同任务和模态是有效的。

    Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences.In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, a
    
[^165]: 不要太单调：放宽超参数模型中的随机线搜索

    Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models. (arXiv:2306.12747v1 [math.OC])

    [http://arxiv.org/abs/2306.12747](http://arxiv.org/abs/2306.12747)

    本文研究了在超参数模型中解决随机梯度下降（SGD）和Adam的速度问题，提出了一种非单调线搜索方法，取得更快的收敛速度和泛化性能，并结合同步的Polyak初始化步伐实现。

    

    近期的工作表明，线搜索方法可以提高现代超参数设置下的随机梯度下降（SGD）和Adam的速度。但是，由于需要（小批量）目标函数的单调减少，现有的线搜索可能会采取比必要更小的步骤。我们探索了非单调线搜索方法来放宽这个条件，并可能接受更大的步长。尽管缺乏单调递减，但我们证明与单调情况相同的快速收敛速度。我们的实验表明，非单调方法在SGD / Adam的收敛速度和泛化性能方面甚至超越了先前的单调线搜索。我们提出了一种POlyak NOnmonotone随机（PoNoS）方法，通过将非单调线搜索与Polyak初始步长结合而得到。此外，我们开发了一种新的重置技术，在大多数迭代中将回溯的数量减少到零，同时仍保持较大的初始s。

    Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial s
    
[^166]: 超越深度集成——基于分布偏移下贝叶斯深度学习的大规模评估

    Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])

    [http://arxiv.org/abs/2306.12306](http://arxiv.org/abs/2306.12306)

    该论文在多个具有挑战性的分类和回归任务上对现代BDL算法进行了系统性评估，重点关注了在分布偏移下的泛化能力和校准能力，并研究了一种带符号的期望校准误差版本。

    

    贝叶斯深度学习（BDL）是实现在分布偏移数据上进行良好校准预测的有前途的方法。然而，缺乏大规模调查，以系统方式评估最近的 SOTA 方法在多样、现实和具有挑战性的基准任务上的表现。为了清晰了解BDL研究的当前状况，我们在来自WILDS集合的现实世界数据集上评估现代BDL算法，包含具有挑战性的分类和回归任务，重点关注在分布偏移下的泛化性能和校准能力。我们比较了一系列大型，卷积和基于 transformer 的神经网络结构上的算法，并研究了一个带符号的期望校准误差版本，揭示出方法是过度自信还是低振幅，进一步深入研究方法的行为。此外，我们为了首次系统评估BDL在微调大型预训练模型上表现，做了更多的工作。

    Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, 
    
[^167]: 用于多分类任务的量子分类器的通用对抗扰动

    Universal adversarial perturbations for multiple classification tasks with quantum classifiers. (arXiv:2306.11974v1 [quant-ph])

    [http://arxiv.org/abs/2306.11974](http://arxiv.org/abs/2306.11974)

    本文探讨了量子通用对抗扰动，并发现一个精心制作的通用扰动可以成功地欺骗两个不同分类任务上达到最先进准确性的量子分类器，这为构建安全的量子机器学习系统带来潜在威胁。

    

    量子对抗机器学习是一门新兴的领域，它研究了量子学习系统对抗扰动的脆弱性并开发了可能的防御策略。量子通用对抗扰动是一种小的扰动，可以使不同的输入样本成为误导给定量子分类器的对抗示例。尽管此领域之前鲜有探究，但是通用扰动可能极大地简化恶意攻击，对量子机器学习模型造成意想不到的破坏。本文在异构分类任务的背景下，进一步探讨量子通用对抗扰动。特别地，我们发现，几乎在两个不同分类任务上达到最先进准确性的量子分类器，都可以被一个精心制作的通用扰动所诱导成功地欺骗。这一结果已经在计算机视觉和量子机器学习社区中广泛使用的数据集CIFAR-10和Iris中得到明确的证明。我们的发现表明，通用对抗扰动是量子机器学习模型的潜在威胁，并可能给构建安全的量子机器学习系统带来巨大挑战。

    Quantum adversarial machine learning is an emerging field that studies the vulnerability of quantum learning systems against adversarial perturbations and develops possible defense strategies. Quantum universal adversarial perturbations are small perturbations, which can make different input samples into adversarial examples that may deceive a given quantum classifier. This is a field that was rarely looked into but worthwhile investigating because universal perturbations might simplify malicious attacks to a large extent, causing unexpected devastation to quantum machine learning models. In this paper, we take a step forward and explore the quantum universal perturbations in the context of heterogeneous classification tasks. In particular, we find that quantum classifiers that achieve almost state-of-the-art accuracy on two different classification tasks can be both conclusively deceived by one carefully-crafted universal perturbation. This result is explicitly demonstrated with well-
    
[^168]: 深度学习的优化理解

    Understanding Optimization of Deep Learning. (arXiv:2306.09338v1 [cs.LG])

    [http://arxiv.org/abs/2306.09338](http://arxiv.org/abs/2306.09338)

    本文全面介绍了深度学习中梯度消失和梯度爆炸等优化挑战，并通过提高梯度流和对网络Lipschitz常数施加约束等措施进行了解决。显式优化和隐式优化是两种解决优化问题的不同方式。

    

    本文全面介绍了深度学习中的优化理论，主要关注梯度消失和梯度爆炸等问题所带来的模型表示能力降低和训练不稳定性等挑战。我们通过提高梯度流和对网络Lipschitz 常数施加约束等措施来分析这两个挑战。为了帮助理解当前的优化方法，我们将其分为显式优化方法和隐式优化方法。显式优化方法涉及直接操作优化器参数，包括权重、梯度、学习率和权重衰减等。相比之下，隐式优化方法侧重于通过增强网络模块（如残差快捷方式、标准化方法、注意机制和激活）来改善网络整体形势。本文提供了深入的分析和实验，以帮助研究人员更好地了解深度学习模型的优化方法。

    This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth a
    
[^169]: 跨国和时间的社会动荡相变

    Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v2 [physics.soc-ph] UPDATED)

    [http://arxiv.org/abs/2306.08698](http://arxiv.org/abs/2306.08698)

    跨国和时间的社会动荡相变研究探索了集体社会动荡是否可以被描述为一系列具有可测量和可识别特征的循环相变，并证明了宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，以及普遍机制可能支撑着社会动荡的某些方面。

    

    相变是复杂系统中突发转变的特征，尽管在物理和自然科学中已经进行了大量研究，但在社会系统中对这一现象的实证研究相对较少。本研究的目标是探索集体社会动荡的动力学是否可以被合理地描述为一系列循环相变，其中每个阶段具有可测量和可识别的潜在特征。我们引入了一个宏观水平的社会动荡统计模型，并使用包括1946年至2017年在内的170个国家的全面数据集来评估其可行性。我们的研究结果表明，这个宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，并且普遍的机制可能潜在地支撑着社会动荡的某些方面。我们还引入了一个新的量表来衡量一个国家的社会动荡程度。

    Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's lo
    
[^170]: 一种个性化联邦学习的终身学习方法

    PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])

    [http://arxiv.org/abs/2306.05515](http://arxiv.org/abs/2306.05515)

    PeFLL是个性化联邦学习的一种新方法，通过联合训练嵌入网络和超网络，PeFLL能够学习输出特定于每个客户端的模型，并且在其它新出现的客户端上表现良好。

    

    个性化联邦学习（pFL）已成为应对参与客户端数据分布的统计异质性挑战的常用方法。pFL不是学习单个全局模型，而是旨在学习每个客户端的个体模型，同时仍然利用其他客户端可用的数据。在这项工作中，我们提出了PeFLL，这是一种根植于终身学习的新型pFL方法，不仅在训练阶段存在的客户端上表现良好，而且在未来可能出现的客户端上也表现良好。PeFLL通过联合训练嵌入网络和超网络来学习输出特定于客户端的模型。嵌入网络学习以一种反映它们之间相似性的潜在描述符空间中表示客户端。超网络学习从这个潜在空间到可能的客户模型空间的映射。我们的实验证明，与先前的方法相比，PeFLL产生了更高准确率的模型。

    Personalized federated learning (pFL) has emerged as a popular approach to dealing with the challenge of statistical heterogeneity between the data distributions of the participating clients. Instead of learning a single global model, pFL aims to learn an individual model for each client while still making use of the data available at other clients. In this work, we present PeFLL, a new pFL approach rooted in lifelong learning that performs well not only on clients present during its training phase, but also on any that may emerge in the future. PeFLL learns to output client specific models by jointly training an embedding network and a hypernetwork. The embedding network learns to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork learns a mapping from this latent space to the space of possible client models. We demonstrate experimentally that PeFLL produces models of superior accuracy compared to previous methods, es
    
[^171]: AMEE：时间序列分类的解释评价框架

    AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])

    [http://arxiv.org/abs/2306.05501](http://arxiv.org/abs/2306.05501)

    AMEE是一个模型无关的解释评价框架，用于量化和比较时间序列分类中多种基于显著性的解释方法的信息价值，帮助解决在这一领域中解释方法选择的难题。

    

    本文旨在提供一个框架，用于定量评估和排名时间序列分类任务中的解释方法，该任务涉及到卫生保健和金融等关键领域的普遍数据类型。最近对时间序列分类解释方法的研究兴趣激增，提供了各种各样的解释技术。然而，当这些解释技术在特定问题上产生分歧时，仍然不清楚使用哪种技术。比较解释以找到正确答案并不容易。两个关键挑战仍然存在：如何定量和稳健地评估给定解释方法的信息价值（即与分类任务相关性），以及如何并排比较解释方法。我们提出了AMEE，一种模型无关的解释评价框架，用于量化和比较多种应用于时间序列分类的基于显著性的解释方法。在输入时间序列中增加扰动

    This paper aims to provide a framework to quantitatively evaluate and rank explanation methods for the time series classification task, which deals with a prevalent data type in critical domains such as healthcare and finance. The recent surge of research interest in explanation methods for time series classification has provided a great variety of explanation techniques. Nevertheless, when these explanation techniques disagree on a specific problem, it remains unclear which of them to use. Comparing the explanations to find the right answer is non-trivial. Two key challenges remain: how to quantitatively and robustly evaluate the informativeness (i.e., relevance for the classification task) of a given explanation method, and how to compare explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation Evaluation framework for quantifying and comparing multiple saliency-based explanations for time series classification. Perturbation is added to the input time series gu
    
[^172]: 通过对比扩散自编码器实现可解释的阿尔茨海默病分类

    Interpretable Alzheimer's Disease Classification Via a Contrastive Diffusion Autoencoder. (arXiv:2306.03022v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.03022](http://arxiv.org/abs/2306.03022)

    本研究通过使用对比扩散自编码器实现了对阿尔茨海默病的可解释分类，达到了与黑盒方法相当的分类准确性，并产生人类可解释的模型解释。

    

    在可视化对象分类中，人类经常通过将对象与类别内的典型示例进行比较来证明他们的选择。因此，我们可以通过赋予深度学习模型类似的推理方式来增加其可解释性。在这项工作中，我们通过将影像与潜在空间中的训练样本的相似性来对阿尔茨海默病进行分类。我们使用对比损失结合扩散自编码器骨干，生成一个语义有意义的潜在空间，使相邻的潜在空间具有类似的图像级特征。我们在一个2D MRI图像数据集上实现了与黑盒方法相当的分类准确性，同时产生人类可解释的模型解释。因此，这项工作对于在医学影像中准确且可解释的深度学习的相关发展是一项贡献。

    In visual object classification, humans often justify their choices by comparing objects to prototypical examples within that class. We may therefore increase the interpretability of deep learning models by imbuing them with a similar style of reasoning. In this work, we apply this principle by classifying Alzheimer's Disease based on the similarity of images to training examples within the latent space. We use a contrastive loss combined with a diffusion autoencoder backbone, to produce a semantically meaningful latent space, such that neighbouring latents have similar image-level features. We achieve a classification accuracy comparable to black box approaches on a dataset of 2D MRI images, whilst producing human interpretable model explanations. Therefore, this work stands as a contribution to the pertinent development of accurate and interpretable deep learning within medical imaging.
    
[^173]: 通过自监督模型行为一致性对时间序列解释进行编码

    Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency. (arXiv:2306.02109v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02109](http://arxiv.org/abs/2306.02109)

    通过自监督模型行为一致性编码时间序列解释，提供离散的归因图，并学习了一种可用于各种方式的解释的潜在空间。

    

    解释时间序列模型是一项独特的挑战，因为它需要确定驱动模型预测的时间序列信号的位置以及它们与可解释的时间模式的匹配。尽管可以应用其他形式的解释器来解释时间序列，但它们的归纳偏差不适用于时间序列的挑战性解释。我们提出了TimeX，一种用于训练解释器的时间序列一致性模型。TimeX训练一个可解释的代理模型来模仿预训练时间序列模型的行为。它通过引入模型行为一致性来解决模型的忠实度问题，这是一种新颖的公式，它保留了由预训练模型引起的潜在空间中的关系与由TimeX引起的潜在空间中的关系。TimeX提供离散的归因图，不同于现有的可解释性方法，它学习了一种可以以各种方式使用的解释的潜在空间，例如，用于提供可视化的地标。

    Interpreting time series models is uniquely challenging because it requires identifying both the location of time series signals that drive model predictions and their matching to an interpretable temporal pattern. While explainers from other modalities can be applied to time series, their inductive biases do not transfer well to the inherently challenging interpretation of time series. We present TimeX, a time series consistency model for training explainers. TimeX trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It addresses the issue of model faithfulness by introducing model behavior consistency, a novel formulation that preserves relations in the latent space induced by the pretrained model with relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and, unlike existing interpretability methods, it learns a latent space of explanations that can be used in various ways, such as to provide landmarks to visua
    
[^174]: 通过神经引导符号抽象实现可解释和可解释逻辑策略

    Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])

    [http://arxiv.org/abs/2306.01439](http://arxiv.org/abs/2306.01439)

    该论文介绍了一种名为NUDGE的策略，利用训练好的基于神经网络的代理来引导逻辑规则的搜索，实现了可解释和可解释的策略。

    

    神经网络所需要的有限先验使其成为使用强化学习（RL）编码和学习策略的主要选择。然而，它们也是黑匣子，在工作在图像级别时难以理解代理行为。因此，神经符号RL旨在首先创建可解释的策略。不幸的是，可解释性不意味着可解释性。为了实现解释性和可解释性，我们引入了神经引导可微分逻辑策略（NUDGE）。NUDGE利用训练好的基于神经网络的代理来引导候选加权逻辑规则的搜索，然后使用可微分的逻辑来训练逻辑代理。我们的实验评估表明，NUDGE代理可以产生可解释和可解释的策略，同时胜过纯神经代理，并展现出良好的灵活性，以适应不同初始状态和问题大小的环境。

    The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
    
[^175]: 使用数据一致性的直接扩散链桥解决逆问题

    Direct Diffusion Bridge using Data Consistency for Inverse Problems. (arXiv:2305.19809v1 [cs.CV])

    [http://arxiv.org/abs/2305.19809](http://arxiv.org/abs/2305.19809)

    本文提出了一种用于逆问题的直接扩散链桥算法，提高了逆问题求解器的性能，并通过使用数据一致性解决了当前DDB框架存在的关键限制。

    

    基于扩散模型的逆问题求解器表现出令人印象深刻的性能，但速度受限，主要是因为需要从噪声开始进行反向扩散采样。近期的一些工作尝试通过构建扩散过程来直接桥接特定逆问题的清洁和污染数据以减轻这个问题。在本文中，我们首先将这些现有工作统一命名为直接扩散链桥（DDB），证明尽管受不同理论的启发，但由此产生的算法在参数选择上的不同。然后，我们强调当前DDB框架的一个关键限制，即它不能保证数据一致性。为了解决这个问题，我们提出了一种修改的推断程序，它在不需要精细调整的情况下强制数据一致性。我们将得到的方法称为数据一致的DDB（CDDB），它在感知和失真指标方面都优于不一致的对应物，从而有效地推动了逆问题求解器的最新进展。

    Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the
    
[^176]: 可分目标的最优决策树：推动动态规划的极限

    Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])

    [http://arxiv.org/abs/2305.19706](http://arxiv.org/abs/2305.19706)

    本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。

    

    决策树的全局优化在准确性，大小和人类可理解性方面表现出良好的前景。然而，许多方法仍然依赖于通用求解器，可扩展性仍然是一个问题。动态规划方法已被证明具有更好的可扩展性，因为它们通过将子树作为独立的子问题解决来利用树结构。然而，这仅适用于可以分别优化子树的任务。我们详细研究了这种关系，并展示了实现这种可分离约束和目标任意组合的动态规划方法。在四个应用领域的实验表明了这种方法的普适性，同时也比通用求解器具有更好的可扩展性。

    Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
    
[^177]: 无法学习的数据集可以给我们带来哪些启示？

    What Can We Learn from Unlearnable Datasets?. (arXiv:2305.19254v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19254](http://arxiv.org/abs/2305.19254)

    无法学习的数据集方法具有保护数据隐私的潜力，但实际使用受到限制。我们发现神经网络在无法学习的数据集上可以学习到有用的特征，而不仅仅是简单规则，这对图像保护的效果不确定。此外，线性可分的扰动并不是诱导学习捷径的必要条件，因此不能依赖它们。

    

    在普遍进行网络爬虫的时代，无法学习的数据集方法具有保护数据隐私、防止深度神经网络泛化的潜力。但除了一些实际限制使得它们的使用不太可能外，我们发现一些结果对其保护数据能力提出了质疑。首先，人们普遍认为在无法学习的数据集上训练的神经网络只会学习到捷径，即并不适用于泛化的简单规则。然而，我们发现网络实际上可以学习到有用的特征，并且这些特征可以重新加权以获得高测试性能，这表明图像的保护并不能得到保证。无法学习的数据集据信通过添加扰动的线性可分性来诱导学习捷径。我们提供了一个反例，证明了扰动的线性可分性并不是必要条件。为了强调为什么不能依赖线性可分的扰动，我们提出了一个正交投影的方法。

    In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal proje
    
[^178]: 一种融合估计和规划实现探索的最大化目标函数的在线强化学习方法

    One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration. (arXiv:2305.18258v1 [cs.LG])

    [http://arxiv.org/abs/2305.18258](http://arxiv.org/abs/2305.18258)

    提出一种在线强化学习方法Maximize to Explore (MEX)，只需优化一个无约束的目标函数，自动平衡探索和利用，实现次线性遗憾。

    

    在在线强化学习中，平衡探索和利用对于以有效的方式找到最优策略至关重要。为了实现这一目标，现有的在线强化学习算法通常包括三个组成部分：估计、规划和探索。然而，为了应对通用函数逼近器，在大多数情况下都需要使用不切实际的算法组件来激励探索，例如数据相关的级别集内优化或繁琐的采样过程。为了解决这一挑战，我们提出了一种易于实现的强化学习框架，称为Maximize to Explore (MEX) ，它只需要无约束地优化一个集成了估计和规划组件的单一目标函数，同时自动平衡探索和利用。理论上，我们证明了对于马尔可夫决策过程的通用函数逼近，MEX实现了一个次线性的遗憾，进一步：

    In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textit{Maximize to Explore} (\texttt{MEX}), which only needs to optimize \emph{unconstrainedly} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \texttt{MEX} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further 
    
[^179]: 利用分段仿射操作实现高效Transformer训练

    Hardware-Efficient Transformer Training via Piecewise Affine Operations. (arXiv:2305.17190v1 [cs.LG])

    [http://arxiv.org/abs/2305.17190](http://arxiv.org/abs/2305.17190)

    本文提出一种使用分段仿射操作代替传统乘法的高效Transformer训练方法，不需要更改训练超参数即可在视觉和语言任务中成功实现训练，同时消除了整个训练过程中的所有乘法操作。

    

    在神经网络训练和推理中，大多数计算成本都是由乘法所贡献的。近期的研究致力于减少由此带来的成本。本文受Mogami（2020）启发，用一种廉价的分段仿射逼近替换乘法，它通过将浮点数的位表示作为整数相加来实现。我们证明了在视觉和语言任务中，可以使用所得到的修改后的矩阵乘法训练Transformer，几乎没有性能影响，并且不需要更改训练超参数。我们进一步用分段仿射替换了网络中的所有非线性，使它们在输入和权重方面都成为完全联合的分段仿射函数。最后，我们展示了如何消除整个训练过程中的所有乘法操作，包括前向传播、反向传播和优化器更新的操作，展示了现代神经网络架构的首次成功训练。

    Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami (2020), we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in
    
[^180]: 逆动力学预训练为多任务模仿学习学习良好的表示

    Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation. (arXiv:2305.16985v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16985](http://arxiv.org/abs/2305.16985)

    本文研究了在多任务模仿学习中的逆动力学预训练方法，通过预训练学习高维观察空间的低维表示，并将其转移到微调数据集进行任务执行。

    

    近年来，自然语言处理和图像识别等领域普及了使用大型数据集预训练表示的范例，可以有效地转移到下游任务中。在本文中，我们评估了在模仿学习中应该如何进行这样的范例，其中预训练和微调数据都是由专家与未知环境交互产生的轨迹。具体而言，我们考虑了一种情况，预训练语料库由多任务演示组成，每个演示的任务由一个未观察到的潜在上下文变量设定。目标是使用预训练语料库学习高维（例如视觉）观察空间的低维表示，该表示可以转移到微调数据集，以在有限的演示数据上进行微调。在各种可能的预训练目标中，我们认为逆动力学建模是合适的，即根据观察来预测动作。

    In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be effectively transferred to downstream tasks. In this work we evaluate how such a paradigm should be done in imitation learning, where both pretraining and finetuning data are trajectories collected by experts interacting with an unknown environment. Namely, we consider a setting where the pretraining corpus consists of multitask demonstrations and the task for each demonstration is set by an unobserved latent context variable. The goal is to use the pretraining corpus to learn a low dimensional representation of the high dimensional (e.g., visual) observation space which can be transferred to a novel context for finetuning on a limited dataset of demonstrations. Among a variety of possible pretraining objectives, we argue that inverse dynamics modeling -- i.e., predicting an action given the observations appeari
    
[^181]: 大部分神经网络几乎是可学习的

    Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])

    [http://arxiv.org/abs/2305.16508](http://arxiv.org/abs/2305.16508)

    本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。

    

    我们提出了一个PTAS来学习随机常数深度网络。我们证明了对于任何固定的$\epsilon>0$和深度$i$，存在一个多项式时间算法，对于$\sqrt{d} \cdot \mathbb{S}^{d-1}$上的任何分布，学习随机Xavier网络的深度$i$，误差为$\epsilon$。该算法的时间和样本复杂度为$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$，其中$\bar d$是网络的大小。对于某些类似于Sigmoid和ReLU的激活函数，可以将误差界限改进为$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$，从而得到一种几乎多项式时间算法来学习常数深度随机网络。

    We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
    
[^182]: 线性预测器和神经网络的初始化相关样本复杂度

    Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks. (arXiv:2305.16475v1 [cs.LG])

    [http://arxiv.org/abs/2305.16475](http://arxiv.org/abs/2305.16475)

    本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。

    

    我们提供了关于向量值线性预测器(由矩阵参数化)、更一般的神经网络的样本复杂性的新结果。专注于大小无关的界限，在这种情况下，仅控制从某个固定参考矩阵$W_0$的参数的Frobenius范数距离，我们展示了样本复杂度行为可以出人意料地不同于我们在研究标量值线性预测器方面所期望的。这还导致了前馈神经网络的新样本复杂度界限，解决了一些文献中存在的问题，并确立了一个新的凸线性预测问题，证明了它可以在没有统一收敛的情况下被学习。

    We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.
    
[^183]: 带有拟对抗扰动和时变对抗性赌博损失函数的最优率问题

    Optimal Rates for Bandit Nonstochastic Control. (arXiv:2305.15352v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15352](http://arxiv.org/abs/2305.15352)

    本研究解决了带有拟对抗扰动和时变对抗性赌博损失函数的LQR和LQG问题，并提供了一种新颖的具有记忆的赌博凸优化方案。

    

    线性二次调节器（LQR）和线性二次高斯（LQG）控制是最优控制中基础且广泛研究的问题。我们研究了具有拟对抗扰动和时变对抗性赌博损失函数的LQR和LQG问题。已知的最佳亚线性遗憾算法~\cite{gradu2020non}在时间横跨度上具有$T^{\frac{3}{4}}$的依赖关系，其作者提出了一个关于是否可以达到$\sqrt{T}$的紧致率的开放问题。我们回答肯定地，提供了一种对于已知和未知系统都能达到最优（除对数因子外）遗憾的赌博LQR和LQG算法。我们方法的一个核心组成部分是一种具有记忆的赌博凸优化新方案，这具有独立的意义。

    Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm of~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizon dependence, and its authors posed an open question about whether a tight rate of $\sqrt{T}$ could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret (up to logarithmic factors) for both known and unknown systems. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest.
    
[^184]: 从ReLU神经网络的缓和过拟合到良性过拟合

    From Tempered to Benign Overfitting in ReLU Neural Networks. (arXiv:2305.15141v1 [cs.LG])

    [http://arxiv.org/abs/2305.15141](http://arxiv.org/abs/2305.15141)

    本论文通过对二层ReLU神经网络进行研究，证明了各种假设下过拟合的类型会从一维数据的极端情况下缓和到高维的良性，揭示了输入维度在神经网络过拟合中的关键作用。

    

    过参数化神经网络被观察到即使训练模型来完美地适应嘈杂的数据也能很好地推广。这一现象引发了大量关于“良性过拟合”的工作，其中内插预测器实现接近最优性能。最近，有人猜测并经验性地观察到神经网络的行为通常更好地描述为“缓和过拟合”，其中性能既非最优，也非微不足道，并随噪声水平的变化而降低。然而，迄今为止，这一主张尚缺乏关于非线性神经网络理论的证明。在这项工作中，我们提供了几个结果，旨在弥合这些互补的观点。我们研究了一个简单的分类设置，使用二层ReLU神经网络，并证明在各种假设下，过拟合的类型从一维数据的极端情况下缓和到高维的良性。因此，我们证明输入维度在这种情况下有关键作用。

    Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on "benign overfitting", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as "tempered overfitting", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the type of overfitting in thi
    
[^185]: 一种无监督方法用于估计数据集的类别可分性，并应用到LLMs的微调

    An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning. (arXiv:2305.15016v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15016](http://arxiv.org/abs/2305.15016)

    这个论文提出了一种无监督方法，通过利用数据流形的拓扑特征估计数据的类别可分性，该方法与有监督度量具有相关性，可以用于半监督和感知学习。同时，在语言模型微调中应用该方法用于自动停止准则。

    

    本论文提出了一种无监督方法，利用数据流形的拓扑特征来估计数据的类别可分性，而不需要标签信息。在本论文中，对几个数据集进行的实验表明，所提出的方法估计的类别可分性与需要标签信息的监督度量（如Fisher判别比率（FDR）和分类器的交叉验证）之间存在清晰的相关性和一致性。这可以实现从有标签和无标签数据中学习的学习范式，如半监督学习和感知学习。当我们有限的有标签数据和相对较大的无标签数据集可以用来增强学习过程时，这将特别有用。该方法在无监督的设置下，通过监测嵌入空间流形的类别可分性，实现了语言模型微调和自动停止准则的应用。

    This paper proposes an unsupervised method that leverages topological characteristics of data manifolds to estimate class separability of the data without requiring labels. Experiments conducted in this paper on several datasets demonstrate a clear correlation and consistency between the class separability estimated by the proposed method with supervised metrics like Fisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which both require labels. This can enable implementing learning paradigms aimed at learning from both labeled and unlabeled data, like semi-supervised and transductive learning. This would be particularly useful when we have limited labeled data and a relatively large unlabeled dataset that can be used to enhance the learning process. The proposed method is implemented for language model fine-tuning with automated stopping criterion by monitoring class separability of the embedding-space manifold in an unsupervised setting. The proposed methodology has 
    
[^186]: S-CLIP: 使用少量专业字幕的半监督视觉语言学习

    S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions. (arXiv:2305.14095v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.14095](http://arxiv.org/abs/2305.14095)

    S-CLIP是一种利用少量专业字幕进行半监督视觉语言学习的方法，通过两种伪标签策略提高了模型在专业领域的适应性。

    

    视觉语言模型，如对比语言-图像预训练 (CLIP)，在自然图像领域展示了令人印象深刻的结果。然而，这些模型在应用于遥感等专业领域时往往遇到困难，由于训练时可用的图像-文本对数量有限，对这些领域的适应性具有挑战性。为了解决这个问题，我们提出了S-CLIP，一种用于训练CLIP的半监督学习方法，利用了额外的未配对图像。S-CLIP采用两种伪标签策略，专门针对对比学习和语言模态设计。字幕级伪标签由配对图像的字幕组合给出，通过解决未配对和配对图像之间的最优传输问题获得。关键词级伪标签由最近的配对图像字幕中的关键词给出，通过假设候选标签集合进行部分标签学习进行训练，而不是准确的标签。通过结合这两种伪标签，我们实现了在专业领域进行视觉语言学习的效果提升。

    Vision-language models, such as contrastive language-image pre-training (CLIP), have demonstrated impressive results in natural image domains. However, these models often struggle when applied to specialized domains like remote sensing, and adapting to such domains is challenging due to the limited number of image-text pairs available for training. To address this, we propose S-CLIP, a semi-supervised learning method for training CLIP that utilizes additional unpaired images. S-CLIP employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality. The caption-level pseudo-label is given by a combination of captions of paired images, obtained by solving an optimal transport problem between unpaired and paired images. The keyword-level pseudo-label is given by a keyword in the caption of the nearest paired image, trained through partial label learning that assumes a candidate set of labels for supervision instead of the exact one. By combini
    
[^187]: TELeR：用于基准测试复杂任务的LLM提示的通用分类法

    TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])

    [http://arxiv.org/abs/2305.11430](http://arxiv.org/abs/2305.11430)

    本文提出了一个通用分类法，可以用来设计具有特定属性的提示来执行各种复杂任务，从而解决了LLM在执行复杂任务方面的性能变异巨大的问题。

    

    尽管LLM在传统对话环境中理解和生成文本时取得了巨大成功，但它们在执行不明确的复杂任务方面的潜力仍然受到很少的研究。本文提出了一种通用分类法，可以用来设计具有特定属性的提示，以执行各种复杂任务，从而解决了使用不同提示类型/风格和提示提供的不同详细程度时LLM性能变化巨大的问题。这个分类法将使未来的基准测试研究能够报告研究中使用的特定提示类别，从而实现跨不同研究的有意义的比较。

    While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
    
[^188]: DoReMi: 优化数据混合加速语言模型预训练

    DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])

    [http://arxiv.org/abs/2305.10429](http://arxiv.org/abs/2305.10429)

    DoReMi方法使用分组分布式鲁棒优化训练小型代理模型以产生域权重，再使用这些权重重新采样数据集训练大型模型，相比使用默认权重的基线模型，在The Pile和GLaM数据集上平均提高了6.5%和4.7%的few-shot下游准确度，分别使用2.6倍和相同的训练步骤达到基线准确度。

    

    预训练数据域的混合比例（例如，维基百科、图书、网页文本）极大地影响语言模型（LM）性能。在本文中，我们提出了一种称为DoReMi的Domain Reweighting with Minimax Optimization方法，它首先使用分组分布式鲁棒优化（Group DRO）训练一个小代理模型，以产生域权重（混合比例），而不需要知道下游任务的知识。然后我们使用这些域权重重新采样一个数据集，并训练一个更大的，全尺寸的模型。在我们的实验中，我们使用DoReMi在一个280M参数的代理模型上，更有效地找到训练一个8B参数模型（30倍大）的域权重。在The Pile上，即使在减小一些域的比重时，DoReMi也能提高所有域的perplexity。相比使用The Pile的默认域权重训练的基线模型，DoReMi将平均few-shot下游准确度提高了6.5%，并使用2.6倍的训练步骤达到基线准确度。在GLaM数据集上，DoReMi没有任何关于下游任务的知识，提高了4.7%（次于现有最先进的模型）的few-shot准确度，在相同的训练步骤下提高了9.0%的准确度。

    The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
    
[^189]: StrAE：使用显式结构的自编码预训练嵌入的表示学习

    StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])

    [http://arxiv.org/abs/2305.05588](http://arxiv.org/abs/2305.05588)

    本文开发了StrAE框架，该框架利用句子结构无监督地学习多级节点嵌入，并且发现使用显式结构可以提高嵌入表现，新的对比目标优于标准的交叉熵目标。同时，完全忠实于结构确实能够根据相应模型的性能消除结构类型之间的歧义。

    

    本文通过开发StrAE这一自编码框架，探究了在NLP中使用显式结构进行表示学习的实用性，该框架利用句子结构无监督地学习多级节点嵌入。我们在包括一种新的对比损失在内的不同类型句子结构和目标下使用StrAE进行模型训练，并在一系列内在和外在任务上评估所学嵌入。实验结果表明，通过StrAE利用显式结构可以提高嵌入，新的对比目标优于标准的交叉熵目标。此外，与以往的做法相比，我们发现完全忠实于结构确实能够根据相应模型的性能消除结构类型之间的歧义。作为StrAE实用性的进一步证明，我们开发了一个简单的p

    This work explores the utility of explicit structure for representation learning in NLP by developing StrAE -- an autoencoding framework that faithfully leverages sentence structure to learn multi-level node embeddings in an unsupervised fashion. We use StrAE to train models across different types of sentential structure and objectives, including a novel contrastive loss over structure, and evaluate the learnt embeddings on a series of both intrinsic and extrinsic tasks. Our experiments indicate that leveraging explicit structure through StrAE leads to improved embeddings over prior work, and that our novel contrastive objective over structure outperforms the standard cross-entropy objective. Moreover, in contrast to findings from prior work that weakly leverages structure, we find that being completely faithful to structure does enable disambiguation between types of structure based on the corresponding model's performance. As further evidence of StrAE's utility, we develop a simple p
    
[^190]: TAPS: 连接认证和对抗训练

    TAPS: Connecting Certified and Adversarial Training. (arXiv:2305.04574v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04574](http://arxiv.org/abs/2305.04574)

    TAPS是一种结合了IBP和PGD训练的认证训练方法，通过产生精确但不一定正确的最坏情况损失逼近，从而减少过度正则化，并在许多设置中实现了最先进的认证精度。

    

    训练可认证鲁棒性神经网络仍然是一个众所周知的难题。一方面，对抗训练优化最坏情况损失的欠缺逼近导致认证的正则化不足，另一方面，声音认证训练方法优化宽松的逼近，导致过度正则化和精度不佳。在这项工作中，我们提出了TAPS，一种结合了IBP和PGD训练的（不一定正确的）认证训练方法，以产生精确但不一定正确的最坏情况损失逼近，从而减少过度正则化并提高认证和标准精度。根据实验证据，TAPS在许多设置中实现了新的最先进水平，例如，在$\ell_\infty$扰动半径$\epsilon=1/255$的TinyImageNet上实现了$22\%$的认证精度。我们在https://github.com/eth-sri/taps上公开了我们的实现和网络。

    Training certifiably robust neural networks remains a notoriously hard problem. On one side, adversarial training optimizes under-approximations of the worst-case loss, which leads to insufficient regularization for certification, while on the other, sound certified training methods optimize loose over-approximations, leading to over-regularization and poor (standard) accuracy. In this work we propose TAPS, an (unsound) certified training method that combines IBP and PGD training to yield precise, although not necessarily sound, worst-case loss approximations, reducing over-regularization and increasing certified and standard accuracies. Empirically, TAPS achieves a new state-of-the-art in many settings, e.g., reaching a certified accuracy of $22\%$ on TinyImageNet for $\ell_\infty$-perturbations with radius $\epsilon=1/255$. We make our implementation and networks public at https://github.com/eth-sri/taps.
    
[^191]: 通过重新参数化学习实现在线反馈的实现式预测

    Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])

    [http://arxiv.org/abs/2305.01094](http://arxiv.org/abs/2305.01094)

    本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。

    

    本文提出了在数据分布由模型部署自身改变的情形下预测的一个框架——实现式预测。现有研究的重点在于优化准确性，但是其假设往往难以在实践中得到满足。本文针对这类问题，提出了一种两层零阶优化算法，通过重新参数化实现式预测目标，从而将非凸的目标转化为凸的目标。

    Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
    
[^192]: 利用双时间尺度制度证明神经网络的收敛性研究

    Leveraging the two timescale regime to demonstrate convergence of neural networks. (arXiv:2304.09576v1 [math.OC])

    [http://arxiv.org/abs/2304.09576](http://arxiv.org/abs/2304.09576)

    研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。

    

    我们研究了浅层神经网络的训练动态，在内层步长远小于外层步长的双时间尺度制度下。在这个制度下，在简单的单变量环境中，我们证明了梯度流收敛于非凸优化问题的全局最优解。我们的结果不需要神经元数量趋于无限，这使我们的结果不同于最近流行的方法，如神经切向核或平均场制度。我们提供实验说明，显示随机梯度下降按照我们对梯度流的描述进行行为，并因此在双时间尺度制度下收敛于全局最优解，但在此制度之外可能失败。

    We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.
    
[^193]: 基于Sheaf神经网络的基于图的推荐系统

    Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])

    [http://arxiv.org/abs/2304.09097](http://arxiv.org/abs/2304.09097)

    基于Sheaf神经网络的模型提出了一种新的向量空间表示方法，使得其在基准推荐任务上获得最先进的性能表现。

    

    近年来，Graph神经网络在许多应用中得到了广泛应用，包括推荐系统。Graph神经网络对其他方法的优越性在于，推荐系统中的许多问题可以自然地建模为图，其中节点可以是用户或项目，边代表偏好关系。 在当前的Graph神经网络方法中，节点用在训练时学习到的静态向量表示。这种静态向量可能只适用于捕捉定义它们的一些用户或项目的微妙差别。为了克服这个限制，我们建议使用最近提出的启发范畴论的模型：Sheaf神经网络。Sheaf神经网络及其连接的拉普拉斯可以通过将每个节点（以及边）与向量空间而不是单个向量相关联来解决上述问题。向量空间表示更丰富，并允许在推理时选择正确的表示。这种方法使我们的模型更具表现力和灵活性，在几个基准推荐任务上实现了最先进的性能。

    Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
    
[^194]: 使AI“口渴”减少的方法：揭示和解决AI模型的秘密水消耗

    Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])

    [http://arxiv.org/abs/2304.03271](http://arxiv.org/abs/2304.03271)

    本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。

    

    人工智能（AI）模型的碳足迹不断增长，特别是像GPT-3和GPT-4这样的大型模型，已经受到公众的关注。然而，同等重要且巨大的AI模型水印尚未引起人们的注意。例如，在微软最先进的美国数据中心中训练GPT-3可以直接消耗70万升清洁淡水（相当于生产370辆宝马汽车或320辆特斯拉电动汽车），如果在微软的亚洲数据中心进行训练，这个水消耗量将增加三倍，但这样的信息一直被保密。这极其令人担忧，因为淡水短缺已成为在人口迅速增长、水资源减少和老化的水基础设施的背景下，我们所有人面临的最紧迫的挑战之一。为了应对全球水资源的挑战，人工智能模型可以，而且应该，承担社会责任，以身作则解决自己的问题。

    The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
    
[^195]: 对角线性网络中的鞍点到鞍点动力学

    Saddle-to-Saddle Dynamics in Diagonal Linear Networks. (arXiv:2304.00488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.00488](http://arxiv.org/abs/2304.00488)

    本文研究了对角线性网络中的鞍点到鞍点动力学，并展示了在初始化趋近于零的情况下，极限流如何从一个训练损失的鞍点跳到另一个鞍点，直到达到最小的$\ell_1$-范数解。通过递归算法和弧长时间重新参数化，我们明确刻画了访问过的鞍点和跳跃时间。我们的分析适用于欠参数化和过参数化的情况，也涵盖了活动数量单调性问题的复杂情况。

    

    在本文中，我们完全描述了在初始化趋近于零的情况下，梯度流在对角线性网络上的轨迹。我们展示了极限流从一个训练损失的鞍点跳到另一个鞍点，直到到达最小的$\ell_1$-范数解。这种鞍点到鞍点的动力学转化为一个增量式的学习过程，因为每个鞍点对应于在活动集之外坐标必须为零的损失最小化器。我们通过一个递归算法明确刻画了访问过的鞍点以及跳跃时间，这个算法类似于用于计算Lasso路径的LARS算法。我们的证明利用了方便的弧长时间重新参数化，使得我们可以跟踪跳跃之间的异室房转换。我们的分析对数据的要求很低，适用于欠参数化和过参数化的情况，并涵盖了复杂的情况，其中活动数量的单调性问题。

    In this paper we fully describe the trajectory of gradient flow over diagonal linear networks in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum $\ell_1$-norm solution. This saddle-to-saddle dynamics translates to an incremental learning process as each saddle corresponds to the minimiser of the loss constrained to an active set outside of which the coordinates must be zero. We explicitly characterise the visited saddles as well as the jumping times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the heteroclinic transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of acti
    
[^196]: Fides：一种利用安全执行环境对机器学习工作负载进行结果验证的生成框架

    Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])

    [http://arxiv.org/abs/2304.00083](http://arxiv.org/abs/2304.00083)

    本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。

    

    机器学习在敏感领域的部署导致了对其安全性和隐私性的重视，现有解决方案，如多方计算和基于证明的系统，给实时应用带来了很大的计算开销。本文提出了一个名为Fides的框架，用于实时验证外协的机器学习工作负载，其中采用新颖且高效的贪心蒸馏迁移学习技术，实现一种实时验证模型来较少地消耗空间和计算能力，同时运行在可信执行环境中。

    The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
    
[^197]: 通过图表自编码器进行内部数据结构的深度非参数估计：广义误差和鲁棒性。

    Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])

    [http://arxiv.org/abs/2303.09863](http://arxiv.org/abs/2303.09863)

    本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。

    

    自编码器在学习高维数据的低维潜在特征方面已经在各种应用中展现出了显着的成功。假设数据在低维流形附近采样，我们采用图表自编码器，将数据编码为一组图表上的低维潜在特征，从而保留了数据流形的拓扑和几何。我们的论文为图表自编码器的广义误差建立了统计保证，并且通过考虑$d$维流形上$n$个带噪声训练样本及其无噪声对应物来展示它们的去噪能力。通过训练自编码器，我们展示了图表自编码器能够有效地去噪输入数据和正态分布噪声。我们证明，在适当的网络架构下，图表自编码器实现了一个大致为$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$阶的平方广义误差，该误差取决于流形的内在维度，并且仅弱依赖于样本数量$n$。

    Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
    
[^198]: 基于动量的梯度方法在非凸下层双层优化中的应用

    On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level. (arXiv:2303.03944v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.03944](http://arxiv.org/abs/2303.03944)

    本文研究了一类非凸双层优化问题，并提出了一种基于动量的梯度双层方法(MGBiO)来解决这些确定性问题，同时提出了一类基于动量的随机梯度双层方法(MSGBiO和VR-MSGBiO)来解决这些随机问题。通过收敛分析，证明了MGBiO方法具有收敛性。

    

    双层优化是一种常用的两级层次优化方法，已广泛应用于许多机器学习任务，如超参数学习、元学习和持续学习。然而，在下层问题为非凸时，双层方法的研究还不够充分。为此，本文研究了一类非凸双层优化问题，其中上层问题和下层问题均为非凸问题，并且下层问题满足Polyak-Lojasiewicz (PL)条件。我们提出了一种高效的基于动量的梯度双层方法(MGBiO)来解决这些确定性问题。同时，我们提出了一类高效的基于动量的随机梯度双层方法(MSGBiO和VR-MSGBiO)来解决这些随机问题。此外，我们为我们的方法提供了一个有用的收敛分析框架。具体而言，在一些温和的条件下，我们证明了MGBiO方法具有收敛性。

    Bilevel optimization is a popular two-level hierarchical optimization, which has been widely applied to many machine learning tasks such as hyperparameter learning, meta learning and continual learning. Although many bilevel optimization methods recently have been developed, the bilevel methods are not well studied when the lower-level problem is nonconvex. To fill this gap, in the paper, we study a class of nonconvex bilevel optimization problems, where both upper-level and lower-level problems are nonconvex, and the lower-level problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient momentum-based gradient bilevel method (MGBiO) to solve these deterministic problems. Meanwhile, we propose a class of efficient momentum-based stochastic gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic problems. Moreover, we provide a useful convergence analysis framework for our methods. Specifically, under some mild conditions, we prove that our MGBiO m
    
[^199]: 通过基于目标的语言描述发现分布差异

    Goal Driven Discovery of Distributional Differences via Language Descriptions. (arXiv:2302.14233v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.14233](http://arxiv.org/abs/2302.14233)

    本论文提出了一个新的任务D5，通过目标驱动的方式自动发现两个大型语料库之间的差异。作者构建了一个D5系统，并提出了一套统一的评估指标来衡量其性能。通过实验证明，语言模型可以使用目标驱动的方法来发现语料库差异。

    

    挖掘大型语料库可以产生有用的发现，但对人类来说耗时。我们提出了一个新的任务D5，它以目标驱动的方式自动发现两个大型语料库之间的差异。任务输入是一个问题，包括一个研究目标“比较药物A和药物B的副作用”，以及一个语料库对（两个大型患者自报反应的集合）。输出是对这些语料库差异的语言描述（发现）（使用药物A后，患者更经常提到“偏执感”）。我们构建了一个D5系统，并为了定量衡量其性能，我们贡献了一个元数据集OpenD5，聚合了675个开放问题，涵盖了商业、社会科学、人文学科、机器学习和健康等领域，同时我们提出了一套统一的评估指标：有效性、相关性、新颖性和显著性。通过数据集和统一指标，我们确认语言模型可以使用目标驱动的方法来发现语料库差异。

    Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal "$\textit{comparing the side effects of drug A and drug B}$" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A "$\textit{mention feelings of paranoia}$" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goal
    
[^200]: 在结构分布偏移条件下评估图模型的鲁棒性和不确定性

    Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13875](http://arxiv.org/abs/2302.13875)

    该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。

    

    在基于机器学习的可靠决策系统中，模型必须对分布偏移具有鲁棒性或提供其预测的不确定性。在图学习的节点级问题中，分布偏移可能尤为复杂，因为样本是相互依赖的。为了评估图模型的性能，重要的是在各种有意义的分布偏移下对它们进行测试。然而，大多数考虑节点级分布偏移的图基准主要关注节点特征，而结构属性对图问题也很重要。在这项工作中，我们提出了一种基于图结构引出多样化分布偏移的通用方法。我们使用这种方法根据几个节点的结构属性：流行度、局部性和密度来创建数据分割。在我们的实验中，我们全面评估了所提出的分布偏移，并表明它们对于现有的图模型可能非常具有挑战性。我们还修订了一些关于基准测试图模型的先前工作，并提出了一组新的基准测试，考虑了结构分布偏移条件。

    In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
    
[^201]: (S)GD在对角线线性网络上的隐式正则化、大步长和稳定边缘的影响研究

    (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability. (arXiv:2302.08982v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08982](http://arxiv.org/abs/2302.08982)

    本文研究了在对角线线性网络上，随机性和大步长对梯度下降(GD)和随机梯度下降(SGD)的隐式正则化的影响。实验结果表明大步长对稀疏回归问题中的SGD有益处，但对GD可能有害。这种影响在接近发散阈值的紧密步长下被放大。

    

    本文研究了随机性和大步长对梯度下降(GD)和随机梯度下降(SGD)在对角线线性网络上的隐式正则化的影响。我们证明了在过参数化的回归设置中，使用宏观步长的GD和SGD收敛，并通过隐式正则化问题描述它们的解。我们的清晰描述为我们提供了关于随机性和步长对恢复解的影响的定性见解。具体而言，我们表明在稀疏回归问题中，大步长对SGD有稳定的好处，但对GD的稀疏解恢复可能产生阻碍。这些效应在紧密窗口内的步长下被放大，位于“稳定边缘”区域的步长。实验结果支持我们的发现。

    In this paper, we investigate the impact of stochasticity and large stepsizes on the implicit regularisation of gradient descent (GD) and stochastic gradient descent (SGD) over diagonal linear networks. We prove the convergence of GD and SGD with macroscopic stepsizes in an overparametrised regression setting and characterise their solutions through an implicit regularisation problem. Our crisp characterisation leads to qualitative insights about the impact of stochasticity and stepsizes on the recovered solution. Specifically, we show that large stepsizes consistently benefit SGD for sparse regression problems, while they can hinder the recovery of sparse solutions for GD. These effects are magnified for stepsizes in a tight window just below the divergence threshold, in the "edge of stability" regime. Our findings are supported by experimental results.
    
[^202]: ReDi: 高效的无学习扩散推断通过轨迹检索

    ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval. (arXiv:2302.02285v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02285](http://arxiv.org/abs/2302.02285)

    ReDi是一种高效的无学习扩散推断方法，通过轨迹检索来加速推断过程，实验证明了它在提高模型推断效率方面具有2倍的加速，并且能够在零样本跨域图像生成中泛化良好。

    

    扩散模型展现了各种数据的优异生成能力。尽管其生成质量较高，但扩散模型的推断仍然耗时，因为需要大量的采样迭代。为了加速推断过程，我们提出了一种简单但无需学习的基于检索的扩散采样框架（ReDi）。通过预先计算的知识库，ReDi从一个在生成的早期阶段与部分生成轨迹相似的轨迹中检索，跳过大部分中间步骤，并在检索到的轨迹的后续步骤中继续采样。我们在理论上证明了ReDi的生成性能是有保证的。我们的实验表明，ReDi提高了模型推断的效率，加速了2倍。此外，ReDi在零样本跨域图像生成（如图像风格化）中具有良好的泛化能力。

    Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sampling iterations required. To accelerate the inference, we propose ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework. From a precomputed knowledge base, ReDi retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from a later step in the retrieved trajectory. We theoretically prove that the generation performance of ReDi is guaranteed. Our experiments demonstrate that ReDi improves the model inference efficiency by 2x speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain image generation such as image stylization.
    
[^203]: 超参数优化的幂律法则

    Power Laws for Hyperparameter Optimization. (arXiv:2302.00441v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00441](http://arxiv.org/abs/2302.00441)

    该论文提出了一种基于幂律规律的Deep Power Laws（DPL）方法来解决超参数优化问题，并在表格、图像和NLP数据集上展示了最佳结果。

    

    超参数优化是机器学习中一个重要的子领域，它专注于调整所选算法的超参数以实现最佳性能。最近，有一系列方法解决了超参数优化的问题，然而，大多数方法没有利用学习曲线的缩放规律特性。在这项工作中，我们提出Deep Power Laws（DPL），一组神经网络模型，它们的预测遵循一个幂律缩放模式。我们的方法通过利用灰盒评估动态决定暂停和增量训练哪些配置。我们在与3个基准相关的表格，图像和NLP数据集上与7种最先进的竞争对手进行了比较，涵盖59项不同的任务。我们的方法在所有基准测试中都取得了最佳结果，并获得了比所有竞争对手更好的任何时候的结果。

    Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the scaling law property of learning curves. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.
    
[^204]: 知识蒸馏≈标签平滑：事实还是谬误？

    Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12609](http://arxiv.org/abs/2301.12609)

    知识蒸馏和标签平滑被认为是等价的方法，但实验证明它们对模型置信度的影响方向完全相反。知识蒸馏不仅传递知识，还传递了自信心。

    

    最初被提出作为一种从一个模型向另一个模型进行知识传递的方法，一些最近的研究表明知识蒸馏(KD)实际上是一种正则化的形式。最强有力的支持来自于它与标签平滑(LS)方法的明显相似之处。本文通过比较它们所训练的模型的预测置信度，重新考察了这两种方法之间的等价关系。在涉及不同规模模型的四个文本分类任务的实验中，显示出：(a)在大多数设置中，KD和LS会完全相反地影响模型的置信度。(b) 在KD中，学生不仅继承知识，而且还从老师那里继承自信心，加强了传统的知识传递观点。

    Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest support of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.
    
[^205]: 使用Tsallis KL散度的广义Munchausen强化学习

    Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11476](http://arxiv.org/abs/2301.11476)

    这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。

    

    许多强化学习中的策略优化方法都采用Kullback-Leibler（KL）散度到上一个策略，以防止策略变化过快。这个想法最初是在Conservative Policy Iteration的一篇重要论文中提出的，近似算法如TRPO和Munchausen Value Iteration（MVI）给出了有限的方法。我们通过研究一种广义的KL散度 - 称为Tsallis KL散度 - 来继续这一工作，它在定义中使用了$q$-对数。这种方法是一种严格的推广，因为$q = 1$对应于标准的KL散度；$q > 1$提供了一系列新的选项。我们对在Tsallis KL下学习的策略类型进行了表征，并阐述了何时$ q > 1 $可能是有益的。为了获得一个将Tsallis KL正则化纳入实际算法的方法，我们扩展了MVI，它是一种最简单的包含KL正则化的方法之一。我们展示了这种广义MVI（$q$）获得了显著的改进。

    Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q > 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q >1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
    
[^206]: AtMan:通过节约内存的注意力机制理解Transformer的预测

    AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08110](http://arxiv.org/abs/2301.08110)

    AtMan是一种通过在生成式Transformer模型中操纵注意力机制来解释预测的方法，相较于传统方法几乎不占用额外内存，可在生产环境中使用。

    

    生成式的Transformer模型越来越复杂，参数数量大且具备处理多输入模态的能力。目前解释它们的预测的方法资源密集。最重要的是，它们需要过多的额外内存，因为它们依赖反向传播，而反向传播会分配的GPU内存几乎是前向传播的两倍。这使得在生产环境中使用它们非常困难，甚至不可能。我们提出了AtMan，它几乎不会产生额外的成本，用于解释生成式Transformer模型。具体而言，AtMan是一种模态无关的扰动方法，通过操纵Transformer的注意力机制生成与输出预测相关性的重要性图。AtMan不使用反向传播，而是在嵌入空间中应用一种基于余弦相似度邻近性的可并行化基于记号的搜索方法。我们在文本和图像-文本基准测试中进行了详尽的实验

    Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
    
[^207]: 高性能计算中的神话与传说

    Myths and Legends in High-Performance Computing. (arXiv:2301.02432v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2301.02432](http://arxiv.org/abs/2301.02432)

    这篇文章讨论了高性能计算社区中流传的神话和传说，这些神话往往不基于科学事实，而是基于一些证据或论证。虽然有些问题仍然是无休止的哲学辩论，但新的方向正在出现，如算法的规模化或新的架构研究。

    

    在这篇发人深省的文章中，我们讨论了高性能计算社区成员间流传的一些神话和传说。我们从会议和会议上的对话、产品广告、论文以及社区内外的推特、博客和新闻文章中收集了这些神话。我们认为它们代表了当前时代的时代精神，这个时代正在经历许多规模定律的终结，如Dennard定律和摩尔定律。虽然一些定律结束了，但新的方向正在出现，比如算法的规模化或新的架构研究。然而，这些神话很少基于科学事实，而是基于一些证据或论证。事实上，我们认为这正是许多神话存在的原因，也是为什么它们无法得到明确答案的原因。虽然每个神话都应该有明确的答案，但有些问题可能仍然是无休止的哲学辩论，比如贝多芬比谁更好。

    In this thought-provoking article, we discuss certain myths and legends that are folklore among members of the high-performance computing community. We gathered these myths from conversations at conferences and meetings, product advertisements, papers, and other communications such as tweets, blogs, and news articles within and beyond our community. We believe they represent the zeitgeist of the current era of massive change, driven by the end of many scaling laws such as Dennard scaling and Moore's law. While some laws end, new directions are emerging, such as algorithmic scaling or novel architecture research. Nevertheless, these myths are rarely based on scientific facts, but rather on some evidence or argumentation. In fact, we believe that this is the very reason for the existence of many myths and why they cannot be answered clearly. While it feels like there should be clear answers for each, some may remain endless philosophical debates, such as whether Beethoven was better than
    
[^208]: DeepGOPlus 推理的数值稳定性研究

    Numerical Stability of DeepGOPlus Inference. (arXiv:2212.06361v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06361](http://arxiv.org/abs/2212.06361)

    这篇论文研究了用于预测蛋白质功能的 CNN DeepGOPlus 在推理过程中的数值不确定性和数值稳定性，并研究了使用降低精度浮点格式进行推理的可能性。

    

    卷积神经网络 (CNNs) 是目前最广泛使用的神经网络之一，在许多问题上都取得了最先进的性能。虽然最初应用于计算机视觉任务，但 CNNs 与具有空间关系的任何数据都能很好地配合使用，并已应用于不同领域。然而，最近的研究强调了 CNNs，与其他深度学习模型一样，对噪声注入的敏感性可能会危及其性能。本文量化了 DeepGOPlus 的浮点精度不确定性以确定其数值稳定性，DeepGOPlus 是一种用于预测蛋白质功能的 CNN。此外，本文研究了使用降低精度浮点格式进行 DeepGOPlus 推理以减少内存消耗和延迟的可能性。这是通过 Monte Carlo Arithmetic 实现的，该技术实验性地量化了浮点运算错误和 VPR。

    Convolutional neural networks (CNNs) are currently among the most widely-used neural networks available and achieve state-of-the-art performance for many problems. While originally applied to computer vision tasks, CNNs work well with any data with a spatial relationship, besides images, and have been applied to different fields. However, recent works have highlighted how CNNs, like other deep learning models, are sensitive to noise injection which can jeopardise their performance. This paper quantifies the numerical uncertainty of the floating point arithmetic inaccuracies of the inference stage of DeepGOPlus, a CNN that predicts protein function, in order to determine its numerical stability. In addition, this paper investigates the possibility to use reduced-precision floating point formats for DeepGOPlus inference to reduce memory consumption and latency. This is achieved with Monte Carlo Arithmetic, a technique that experimentally quantifies floating point operation errors and VPR
    
[^209]: 可变化决策频率的选项评论者

    Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04407](http://arxiv.org/abs/2212.04407)

    这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。

    

    在传统的强化学习算法中，代理在离散和固定的时间间隔内做出决策。决策之间的持续时间变成了一个关键的超参数，因为设置得太短可能会增加问题的难度，需要代理进行多次决策才能实现其目标，而设置得太长会导致代理失去对系统的控制。然而，物理系统不一定需要恒定的控制频率，对于学习代理来说，一般情况下，当需要时以高频率运行，而在可能时以低频率运行更好。我们提出了一个名为连续时间连续选项 (CTCO) 的框架，其中代理选择选项作为可变持续时间的子策略。这些选项是时间连续的，可以以任何所需频率与系统交互，从而提供平滑的动作变化。我们通过将其性能与传统 RL 和时间抽象 RL 方法进行比较，展示了 CTCO 的有效性。

    In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
    
[^210]: 多目标强化学习中的福利和公平性

    Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.01382](http://arxiv.org/abs/2212.01382)

    本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。

    

    我们研究了公平多目标强化学习，其中一个代理必须学习一个能够在多个维度的向量值奖励上同时获得高回报的策略。受公平资源分配文献的启发，我们将其建模为期望福利最大化问题，针对向量的长期累积奖励的非线性公平福利函数。其中一个经典的例子是纳什社会福利函数，或者几何平均数，其对数变换也被称为比例公平目标。我们表明，即使在表格化的情况下，对期望纳什社会福利进行近似最优化也是计算上难以处理的。尽管如此，我们提供了一种创新的Q-learning改进方法，结合非线性标量化学习更新和非稳态动作选择，以学习有效的优化非线性福利函数的策略。我们证明了我们的算法是可收敛的，并进行了实验证明。

    We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
    
[^211]: 基于数据驱动的网络神经科学：关于数据收集与基准的研究

    Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.12421](http://arxiv.org/abs/2211.12421)

    本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。

    

    本文提供了一份全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。解剖学和功能性磁共振成像被用于理解人脑的功能连接，并且在识别阿尔茨海默氏症、帕金森症和自闭症等潜在的神经退行性疾病方面尤为重要。最近，利用机器学习和图分析研究以脑网络的形式来研究大脑的方法变得越来越流行，特别是用于预测这些疾病的早期发生。作为一个图形表示的脑网络保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了研究人员进行数据驱动的探索。其中主要的困难在于复杂的领域特定的预处理步骤。

    This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
    
[^212]: 图形反事实解释的综述: 定义, 方法, 评估

    A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation. (arXiv:2210.12089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12089](http://arxiv.org/abs/2210.12089)

    这篇综述研究了图形反事实解释的概念、方法、评估及其应用于图神经网络的情况，提供了分类法、统一的符号表示、基准数据集和评估指标，并对十四种方法、二十二个数据集和十九个指标进行了讨论和整合。未来的工作主要集中在解决开放的挑战上。

    

    图神经网络 (GNNs) 在社区检测和分子分类方面表现出色。反事实解释 (CE) 提供反例来克服黑盒模型的透明度限制。由于对图学习的关注不断增长，我们将重点关注 GNNs 的 CE 概念。我们分析了非常规的手段，提供了分类法，统一的符号表示，以及基准数据集和评估指标。我们讨论了十四种方法，它们的评估协议，二十二个数据集和十九个指标。我们整合了大多数方法到 GRETEL 库中，进行了实证评估，以了解它们的优势和缺点。我们强调了开放的挑战和未来的工作。

    Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.
    
[^213]: 自动贝叶斯先验选择的正则化数据编程

    Regularized Data Programming with Automated Bayesian Prior Selection. (arXiv:2210.08677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08677](http://arxiv.org/abs/2210.08677)

    本文介绍了一种自动贝叶斯先验选择的正则化数据编程方法，通过加入正则化项来提升数据编程在弱监督学习中的性能，在低数据情境中表现出更好的性能，并提供更大的可解释性。

    

    在监督学习中，手动数据标记的成本可能是一个重要的障碍。数据编程（DP）提供了一种弱监督的解决方案，用于训练数据集的创建，其中用户定义的编程标签函数（LFs）的输出通过无监督学习进行调和。然而，DP在某些情况下可能无法胜过加权多数投票，包括低数据情境。本文介绍了一种贝叶斯扩展的经典DP，通过增加正则化项来减轻无监督学习的失败。通过最大后验估计和信息先验实现了正则化学习。提出利用多数投票作为自动先验参数选择的代理信号。实验结果表明，正则化的DP相对于最大似然估计和多数投票可以改善性能，提供更大的可解释性，并在低数据情境中增强性能。

    The cost of manual data labeling can be a significant obstacle in supervised learning. Data programming (DP) offers a weakly supervised solution for training dataset creation, wherein the outputs of user-defined programmatic labeling functions (LFs) are reconciled through unsupervised learning. However, DP can fail to outperform an unweighted majority vote in some scenarios, including low-data contexts. This work introduces a Bayesian extension of classical DP that mitigates failures of unsupervised learning by augmenting the DP objective with regularization terms. Regularized learning is achieved through maximum a posteriori estimation with informative priors. Majority vote is proposed as a proxy signal for automated prior parameter selection. Results suggest that regularized DP improves performance relative to maximum likelihood and majority voting, confers greater interpretability, and bolsters performance in low-data regimes.
    
[^214]: 使用肿瘤位置的3D概率分布改进儿童低级别胶质瘤分子亚型鉴定的深度学习模型

    Improving Deep Learning Models for Pediatric Low-Grade Glioma Tumors Molecular Subtype Identification Using 3D Probability Distributions of Tumor Location. (arXiv:2210.07287v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07287](http://arxiv.org/abs/2210.07287)

    本研究提出使用肿瘤位置概率来改进深度学习模型，以提高儿童低级别胶质瘤分子亚型鉴定的准确性。

    

    背景与目的：儿童低级别胶质瘤（pLGG）是儿童最常见的脑肿瘤类型，寻找pLGG的分子标记对于成功的治疗计划至关重要。卷积神经网络（CNN）模型用于pLGG亚型鉴定依赖于肿瘤分割。我们假设肿瘤分割是次优的，因此我们提出使用MRI数据中的肿瘤位置概率来增强CNN模型。材料与方法：我们的经研究伦理委员会批准的回顾性研究包括了143例BRAF融合型和71例BRAF V600E突变型肿瘤的MRI FLAIR序列。肿瘤分割（感兴趣区域（ROI））由一名儿童神经放射医学研究员提供，并由一名高级儿童神经放射医学研究员进行核实。在每个实验中，我们将数据随机分成开发集和测试集，比例为80/20。我们将开发集中每个类别的3D二进制ROI掩模组合起来，以得到概率密度函数（

    Background and Purpose: Pediatric low-grade glioma (pLGG) is the most common type of brain tumor in children, and identification of molecular markers for pLGG is crucial for successful treatment planning. Convolutional Neural Network (CNN) models for pLGG subtype identification rely on tumor segmentation. We hypothesize tumor segmentations are suboptimal and thus, we propose to augment the CNN models using tumor location probability in MRI data.  Materials and Methods: Our REB-approved retrospective study included MRI Fluid-Attenuated Inversion Recovery (FLAIR) sequences of 143 BRAF fused and 71 BRAF V600E mutated tumors. Tumor segmentations (regions of interest (ROIs)) were provided by a pediatric neuroradiology fellow and verified by a senior pediatric neuroradiologist. In each experiment, we randomly split the data into development and test with an 80/20 ratio. We combined the 3D binary ROI masks for each class in the development dataset to derive the probability density functions (
    
[^215]: 无标签的自监督学习在心脏超声图像分割中的应用

    Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.04979](http://arxiv.org/abs/2210.04979)

    本研究提出了一种无需手动标注的自监督学习流程，在心脏超声图像分割中取得了可靠的结果，与监督学习方法相比具有相似的测量准确度，并且能够准确检测异常心腔大小和功能。

    

    心脏超声图像的分割和测量对于心脏超声来说至关重要，但是这些任务耗时且难以重现。神经网络可以提供辅助，但是监督学习方法需要耗费大量人力进行手动标注。本文建立了一个无需手动标注的自监督学习流程，结合了计算机视觉、临床领域知识和深度学习。我们在450个心脏超声图像（93000张图片）上进行了训练，并在8393个心脏超声图像（4476266张图片，平均年龄61岁，女性占51%）上进行了测试，利用分割结果进行生物测量。我们还对来自额外10030名患者的外部图像进行了测试，这些图像具有手动描迹的左室信息。在几种不同的测量指标（r2 0.56-0.84）上，临床测量和我们的流程预测之间的r2值与已报道的临床医生之间的变异程度相似，并且与监督学习的结果相当。检测异常心腔大小和功能的平均准确度为0.85（范围0.71-0.97）。

    Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compar
    
[^216]: GLM-130B: 一个开源的双语预训练模型

    GLM-130B: An Open Bilingual Pre-trained Model. (arXiv:2210.02414v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.02414](http://arxiv.org/abs/2210.02414)

    GLM-130B是一个具有1300亿参数的开源双语预训练模型，能够超越GPT-3和最大的中文语言模型ERNIE TITAN 3.0 260B，在多个基准测试中表现出色。

    

    本文介绍了GLM-130B，一个具有1300亿参数的双语（英语和中文）预训练语言模型。它是为了打开1000亿规模的模型，至少与GPT-3（davinci）一样好，并揭示如何成功地进行如此规模的预训练。在这个过程中，我们遇到了许多意外的技术和工程挑战，特别是在损失峰和发散方面。在本文中，我们介绍了GLM-130B的训练过程，包括设计选择、提高效率和稳定性的训练策略，以及工程努力。GLM-130B模型在广泛的英语基准测试中明显优于GPT-3 175B（davinci），但在OPT-175B和BLOOM-176B中没有观察到性能优势。它还在相关基准测试中始终且显著优于最大的中文语言模型ERNIE TITAN 3.0 260B。最后，我们利用GLM-130B的独特缩放性能进行实验。

    We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to rea
    
[^217]: 贝叶斯神经网络在地热资源评估中的应用：带有不确定性的预测

    Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty. (arXiv:2209.15543v2 [physics.geo-ph] UPDATED)

    [http://arxiv.org/abs/2209.15543](http://arxiv.org/abs/2209.15543)

    本论文介绍了将贝叶斯神经网络应用于地热资源评估的方法，利用监督学习问题和已知特征图进行预测，并可用于找出潜力更高的区域进行进一步的调查。

    

    我们考虑将机器学习应用于地热资源潜力评估。我们定义了一个监督学习问题，利用美国内华达州的10个地质和地球物理特征图来界定广泛区域内的地热潜力。我们有一组相对较小的正样本训练点（已知资源或活跃发电厂）和负样本训练点（已知钻探点但存在不适合的地热条件），并利用这些训练点来约束和优化人工神经网络进行分类任务。主要目标是在已知定义特征的大面积地理区域内预测未知点的地热资源潜力。这些预测可以用于确定有前景的区域进行详细调查。我们描述了我们的工作从定义特定的神经网络架构到训练和优化试验的演变。通过分析，我们揭示了不可避免的问题。

    We consider the application of machine learning to the evaluation of geothermal resource potential. A supervised learning problem is defined where maps of 10 geological and geophysical features within the state of Nevada, USA are used to define geothermal potential across a broad region. We have available a relatively small set of positive training sites (known resources or active power plants) and negative training sites (known drill sites with unsuitable geothermal conditions) and use these to constrain and optimize artificial neural networks for this classification task. The main objective is to predict the geothermal resource potential at unknown sites within a large geographic area where the defining features are known. These predictions could be used to target promising areas for further detailed investigations. We describe the evolution of our work from defining a specific neural network architecture to training and optimization trials. Upon analysis we expose the inevitable pro
    
[^218]: 从部分观测轨迹中学习作用Koopman生成器的双线性模型

    Learning Bilinear Models of Actuated Koopman Generators from Partially-Observed Trajectories. (arXiv:2209.09977v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2209.09977](http://arxiv.org/abs/2209.09977)

    该论文提出了一种从部分观测轨迹中学习作用Koopman生成器的方法，克服了现有方法依赖于基函数选择和观测不完整的限制。

    

    基于逼近潜在的Koopman算子或生成器的非线性动力系统的数据驱动模型已经被证明是成功的预测、特征学习、状态估计和控制工具。众所周知，控制仿射系统的Koopman生成器在输入方面也具有仿射依赖性，进而导致方便的有限维双线性近似动力学。然而，目前对于近似具有作用的Koopman生成器的方法仍然存在两个主要障碍。首先，现有方法的性能严重依赖于选择用于逼近Koopman生成器的基函数，而对于非测度保持的系统目前没有普适的选择方式。其次，如果我们没有观测到完整的状态，可能无法获得足够丰富的这类函数集合来描述动态。这是因为通常情况下我们无法获得描述动态所需的足够丰富的函数集合。

    Data-driven models for nonlinear dynamical systems based on approximating the underlying Koopman operator or generator have proven to be successful tools for forecasting, feature learning, state estimation, and control. It has become well known that the Koopman generators for control-affine systems also have affine dependence on the input, leading to convenient finite-dimensional bilinear approximations of the dynamics. Yet there are still two main obstacles that limit the scope of current approaches for approximating the Koopman generators of systems with actuation. First, the performance of existing methods depends heavily on the choice of basis functions over which the Koopman generator is to be approximated; and there is currently no universal way to choose them for systems that are not measure preserving. Secondly, if we do not observe the full state, we may not gain access to a sufficiently rich collection of such functions to describe the dynamics. This is because the commonly u
    
[^219]: 抽奖感知的稀疏度搜索: 在资源有限的边缘环境中实现联邦学习

    Lottery Aware Sparsity Hunting: Enabling Federated Learning on Resource-Limited Edge. (arXiv:2208.13092v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13092](http://arxiv.org/abs/2208.13092)

    本论文提出了一种名为FLASH的联邦学习框架，该框架通过抽奖感知的稀疏度搜索，在资源有限的边缘环境中训练稀疏子模型，从而保持性能并获得相应的通信优势。

    

    由于边缘设备的分布特性，联邦学习可以极大地受益于它们，然而，由于资源和计算能力的限制，部署存在局限性。解决这个问题的一个可能的方法是在客户端利用现成的稀疏学习算法来满足它们的资源预算。然而，这样朴素地在客户端部署会导致显著的准确度下降，特别是对于资源受限的客户端。我们的研究发现，在客户端之间的稀疏度掩码缺乏共识可能会减慢全局模型的收敛速度并导致显著的准确度下降。基于这些观察，我们提出了一种统一的稀疏学习框架——联邦抽奖感知稀疏度搜索(FLASH)，用于训练一个在超低参数密度下保持性能的稀疏子模型，同时产生相应的通信优势。

    Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop. With these observations, we present \textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different
    
[^220]: 隐式双塔策略

    Implicit Two-Tower Policies. (arXiv:2208.01191v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.01191](http://arxiv.org/abs/2208.01191)

    隐式双塔策略（ITT）是一种新的结构化强化学习策略体系，通过在策略堆栈中显式区分动作和状态处理，实现了显著的计算效益和更好的性能，在黑盒/进化优化方面表现出色。

    

    我们提出了一种新的结构化强化学习策略体系，即隐式双塔（ITT）策略，其中动作基于其可学习的潜在表示与输入状态的注意力分数进行选择。通过在策略堆栈中显式区分动作和状态处理，我们实现了两个主要目标：显著的计算效益和更好的性能。我们的架构适用于离散和连续动作空间。通过在OpenAI Gym和DeepMind Control Suite的15个环境上进行测试，我们展示了ITT架构特别适用于黑盒/进化优化，相应的策略训练算法优于其草率的隐式对应物以及常用的显式策略。我们通过展示如何应用哈希和惰性塔更新等技术，关键依赖于ITT的双塔结构，来补充我们的分析。

    We present a new class of structured reinforcement learning policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are chosen based on the attention scores of their learnable latent representations with those of the input states. By explicitly disentangling action from state processing in the policy stack, we achieve two main goals: substantial computational gains and better performance. Our architectures are compatible with both: discrete and continuous action spaces. By conducting tests on 15 environments from OpenAI Gym and DeepMind Control Suite, we show that ITT-architectures are particularly suited for blackbox/evolutionary optimization and the corresponding policy training algorithms outperform their vanilla unstructured implicit counterparts as well as commonly used explicit policies. We complement our analysis by showing how techniques such as hashing and lazy tower updates, critically relying on the two-tower structure of ITTs, can be applied to obtain add
    
[^221]: 开放放射组学：一系列标准化数据集和可重复放射组学机器学习流程的技术协议

    Open-radiomics: A Collection of Standardized Datasets and a Technical Protocol for Reproducible Radiomics Machine Learning Pipelines. (arXiv:2207.14776v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2207.14776](http://arxiv.org/abs/2207.14776)

    本研究提出了一套开放放射组学数据集和技术协议，旨在解决放射组学在结果可重复性和可访问性方面所面临的挑战。通过在BraTS 2020数据集上进行实验，研究了放射组学特征提取对结果可重复性的影响。

    

    目的：作为医学影像中机器学习流程的一个重要分支，放射组学面临着两个主要挑战，即可重复性和可访问性。在这项工作中，我们介绍了开放放射组学，一套放射组学数据集以及基于我们提出的技术协议的综合放射组学流程，以研究放射组学特征提取对结果可重复性的影响。材料和方法：实验使用BraTS 2020开源磁共振成像（MRI）数据集进行，包括369名患有脑肿瘤的成年患者（76例低级别胶质瘤（LGG）和293例高级别胶质瘤（HGG））。使用PyRadiomics库进行LGG与HGG分类，形成了288个放射组学数据集；其中包括4个MRI序列、3个binWidths、6种图像归一化方法和4个肿瘤次区域的组合。使用随机森林分类器，并为每个放射组学数据集进行训练-验证-测试（60%/20%/20%）实验，采用不同的数据划分和m

    Purpose: As an important branch of machine learning pipelines in medical imaging, radiomics faces two major challenges namely reproducibility and accessibility. In this work, we introduce open-radiomics, a set of radiomics datasets along with a comprehensive radiomics pipeline based on our proposed technical protocol to investigate the effects of radiomics feature extraction on the reproducibility of the results.  Materials and Methods: Experiments are conducted on BraTS 2020 open-source Magnetic Resonance Imaging (MRI) dataset that includes 369 adult patients with brain tumors (76 low-grade glioma (LGG), and 293 high-grade glioma (HGG)). Using PyRadiomics library for LGG vs. HGG classification, 288 radiomics datasets are formed; the combinations of 4 MRI sequences, 3 binWidths, 6 image normalization methods, and 4 tumor subregions.  Random Forest classifiers were used, and for each radiomics dataset the training-validation-test (60%/20%/20%) experiment with different data splits and m
    
[^222]: 无监督的卷积稀疏编码能量分解方法

    Unsupervised energy disaggregation via convolutional sparse coding. (arXiv:2207.09785v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2207.09785](http://arxiv.org/abs/2207.09785)

    本文提出了一种无监督能量分解方法，通过卷积稀疏编码将功耗分类为有功或者无功，以实现对私人住宅居民活动和存在情况的报告。实验结果表明该方法在能量泛函最小化和收敛性等方面具有可行性。

    

    本文提出了一种针对私人住宅中配备智能电表的无监督能量分解方法。该方法旨在将功耗分类为有功或者无功，从而能够报告居民的活动和存在情况，而无需直接交互。这为非侵入式私人住宅的健康监测等应用奠定了基础。所提出的方法基于最小化适当的能量泛函，采用了iPALM（惯性近端交替线性化最小化）算法，证明了满足收敛的各种条件。为了确认所提出方法的可行性，对半合成测试数据集进行了实验，并与现有的监督方法进行了比较。

    In this work, a method for unsupervised energy disaggregation in private households equipped with smart meters is proposed. This method aims to classify power consumption as active or passive, granting the ability to report on the residents' activity and presence without direct interaction. This lays the foundation for applications like non-intrusive health monitoring of private homes.  The proposed method is based on minimizing a suitable energy functional, for which the iPALM (inertial proximal alternating linearized minimization) algorithm is employed, demonstrating that various conditions guaranteeing convergence are satisfied.  In order to confirm feasibility of the proposed method, experiments on semi-synthetic test data sets and a comparison to existing, supervised methods are provided.
    
[^223]: "理解鲁棒性之彩票": 神经网络剪枝方法的几何可视化比较分析

    "Understanding Robustness Lottery": A Geometric Visual Comparative Analysis of Neural Network Pruning Approaches. (arXiv:2206.07918v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2206.07918](http://arxiv.org/abs/2206.07918)

    本研究通过几何可视化分析，比较了不同神经网络剪枝方法对网络内部特征表示的影响，并对模型性能进行了评估。

    

    通过依赖大而过参数化的神经网络，深度学习方法在许多应用中提供了最先进的性能。然而，这样的网络被证明非常脆弱，难以部署在资源有限的平台上。模型剪枝，即减小网络的规模，是一种广泛采用的策略，可以产生更强韧和紧凑的模型。存在许多模型剪枝的启发式方法，但实证研究表明，某些启发式方法可以改善性能，而其他方法可能使模型更加脆弱或产生其他副作用。本研究旨在揭示不同剪枝方法如何改变网络的内部特征表示以及对模型性能的相应影响。为了便于全面比较和描述高维模型特征空间，我们引入了一种基于可视化几何分析的特征表示方法。我们对常见的几何概念进行了分解和评估。

    Deep learning approaches have provided state-of-the-art performance in many applications by relying on large and overparameterized neural networks. However, such networks have been shown to be very brittle and are difficult to deploy on resource-limited platforms. Model pruning, i.e., reducing the size of the network, is a widely adopted strategy that can lead to a more robust and compact model. Many heuristics exist for model pruning, but empirical studies show that some heuristics improve performance whereas others can make models more brittle or have other side effects. This work aims to shed light on how different pruning methods alter the network's internal feature representation and the corresponding impact on model performance. To facilitate a comprehensive comparison and characterization of the high-dimensional model feature space, we introduce a visual geometric analysis of feature representations. We decomposed and evaluated a set of critical geometric concepts from the commo
    
[^224]: 公正游戏：对强化学习的挑战

    Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12787](http://arxiv.org/abs/2205.12787)

    AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.

    

    类似AlphaZero的强化学习算法在各种棋盘游戏中表现出色，但在公正游戏中却面临挑战，这些游戏中玩家共享棋子。我们提供了一个具体的游戏例子，即小孩们玩的尼姆游戏，以及其他一些公正游戏，这些游戏似乎成为AlphaZero和类似的强化学习算法的绊脚石。我们的发现与最近的研究一致，表明AlphaZero-style算法容易受到敌对攻击和敌对扰动的影响，显示了在所有合法状态下学习掌握这些游戏的困难。我们发现尼姆游戏在小型棋盘上可以学习，但当棋盘尺寸增大时，AlphaZero-style算法的学习速度显著减慢。直观上，尼姆等公正游戏与象棋和围棋等党派游戏之间的区别在于，如果系统中添加了微小的噪音（例如，棋盘的一小部分被覆盖），对于公正游戏来说，这是一种典型的情况。

    AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
    
[^225]: 跨领域少样本元学习中的特征提取器叠加

    Feature Extractor Stacking for Cross-domain Few-shot Meta-learning. (arXiv:2205.05831v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.05831](http://arxiv.org/abs/2205.05831)

    这篇论文提出了一种新的跨领域少样本元学习方法，称为特征提取器叠加(FES)。FES通过叠加多个主干的信息，可以利用异构预训练的主干，而且不需要维护一个需要重新计算的通用模型。

    

    跨领域少样本元学习(CDFSML)解决了需要将知识从多个源领域转移到一个实例稀缺的目标领域，而目标领域的数据分布明显不同的学习问题。最近发布的CDFSML方法通常构建一个通用模型，将多个源领域的知识合并到一个主干特征提取器中。这样可以实现高效推理，但需要在添加新的源领域时重新计算主干。其中一些方法还与异构源领域主干架构不兼容。我们提出了特征提取器叠加(FES)，一种将来自一组主干的信息进行组合的新CDFSML方法，它可以直接使用异构预训练的主干，并且不需要维护一个需要在主干集合更新时重新计算的通用模型。我们提出了基本的FES算法，它受经典叠加方法元学习的启发。

    Cross-domain few-shot meta-learning (CDFSML) addresses learning problems where knowledge needs to be transferred from several source domains into an instance-scarce target domain with an explicitly different distribution. Recently published CDFSML methods generally construct a universal model that combines knowledge of multiple source domains into one backbone feature extractor. This enables efficient inference but necessitates re-computation of the backbone whenever a new source domain is added. Some of these methods are also incompatible with heterogeneous source domain backbone architectures. We propose feature extractor stacking (FES), a new CDFSML method for combining information from a collection of backbones, which can utilise heterogeneous pretrained backbones out of the box, and does not maintain a universal model that needs to be re-computed when its backbone collection is updated. We present the basic FES algorithm, which is inspired by the classic stacking approach to meta-
    
[^226]: 深度学习在数字硬件上的反问题存在的限制

    Limitations of Deep Learning for Inverse Problems on Digital Hardware. (arXiv:2202.13490v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.13490](http://arxiv.org/abs/2202.13490)

    本文研究了深度学习在数字硬件上解决反问题的限制，并证明了对于小的松弛参数，有限维反问题无法通过计算方法解决。这些结果还引入了算法可获得准确度的下限。

    

    过去几年，深度神经网络取得了巨大的成功。由于训练是在数字硬件上进行的，本文分析了在当前模拟为图灵机的硬件平台上实际可以计算的内容，这会导致深度学习的固有限制。为此，我们重点研究了反问题类，特别是涵盖了从测量中重构数据的任何任务。我们证明，对于小的松弛参数，有限维反问题无法通过巴拿赫-马兹尔计算方法解决。更重要的是，我们的结果引入了在算法上可以获得的准确度的下界。

    Deep neural networks have seen tremendous success over the last years. Since the training is performed on digital hardware, in this paper, we analyze what actually can be computed on current hardware platforms modeled as Turing machines, which would lead to inherent restrictions of deep learning. For this, we focus on the class of inverse problems, which, in particular, encompasses any task to reconstruct data from measurements. We prove that finite-dimensional inverse problems are not Banach-Mazur computable for small relaxation parameters. Even more, our results introduce a lower bound on the accuracy that can be obtained algorithmically.
    
[^227]: 可解释的神经推理的前向组合传播算法

    Forward Composition Propagation for Explainable Neural Reasoning. (arXiv:2112.12717v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.12717](http://arxiv.org/abs/2112.12717)

    本文提出了一种被称为前向组合传播（FCP）的算法，用于解释前馈神经网络在结构化分类问题上的预测。该算法通过组合向量描述每个神经元中问题特征的作用，并且模拟结果表明组合值与受保护特征的预期行为紧密对齐。

    

    本文提出一种称为前向组合传播（FCP）的算法，用于解释在结构化分类问题上操作的前馈神经网络的预测。在提出的FCP算法中，每个神经元都由一个组合向量描述，该向量指示了每个问题特征在该神经元中的作用。组合向量使用给定的输入实例初始化，并随后通过整个网络传播，直到达到输出层。每个组合值的符号表示相应特征是否激活或抑制神经元，而绝对值 quantifies 了其影响。FCP算法是在后续基础上执行的，即在学习过程完成后。为了说明FCP算法，本文开展了一个关于公平问题中偏见检测的案例研究，其中地面真相是已知的。模拟结果表明，组合值与受保护特征的预期行为紧密对齐。

    This paper proposes an algorithm called Forward Composition Propagation (FCP) to explain the predictions of feed-forward neural networks operating on structured classification problems. In the proposed FCP algorithm, each neuron is described by a composition vector indicating the role of each problem feature in that neuron. Composition vectors are initialized using a given input instance and subsequently propagated through the whole network until reaching the output layer. The sign of each composition value indicates whether the corresponding feature excites or inhibits the neuron, while the absolute value quantifies its impact. The FCP algorithm is executed on a post-hoc basis, i.e., once the learning process is completed. Aiming to illustrate the FCP algorithm, this paper develops a case study concerning bias detection in a fairness problem in which the ground truth is known. The simulation results show that the composition values closely align with the expected behavior of protected
    
[^228]: 用模型构建增强随机梯度下降算法

    Bolstering Stochastic Gradient Descent with Model Building. (arXiv:2111.07058v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.07058](http://arxiv.org/abs/2111.07058)

    用基于模型构建的方法增强了随机梯度下降算法，适应性地调整步长和搜索方向，提高了收敛速度。

    

    随机梯度下降方法及其变种是解决机器学习问题的核心优化算法，能够获得良好的收敛速度。在针对特定应用进行调优时，可以进一步提高这些算法的效果。近期的研究表明，通过线搜索方法迭代调整步长，可以降低调优过程的计算成本。我们提出了一种基于前向步模型构建的随机线搜索的替代方法。这个模型构建步骤融入了二阶信息，不仅可以调整步长，还可以调整搜索方向。我们的方法将深度学习模型参数分组（张量的层），为每个参数组建立模型并计算新的步长。这种新颖的对角化方法使得选择的步长是自适应的。我们提供了收敛速度分析，

    Stochastic gradient descent method and its variants constitute the core optimization algorithms that achieve good convergence rates for solving machine learning problems. These rates are obtained especially when these algorithms are fine-tuned for the application at hand. Although this tuning process can require large computational costs, recent work has shown that these costs can be reduced by line search methods that iteratively adjust the step length. We propose an alternative approach to stochastic line search by using a new algorithm based on forward step model building. This model building step incorporates second-order information that allows adjusting not only the step length but also the search direction. Noting that deep learning model parameters come in groups (layers of tensors), our method builds its model and calculates a new step for each parameter group. This novel diagonalization approach makes the selected step lengths adaptive. We provide convergence rate analysis, a
    
[^229]: 是否总是需要注意力？语言识别案例研究。(arXiv:2110.03427v3 [cs.LG] 更新)

    Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03427](http://arxiv.org/abs/2110.03427)

    本研究提出了一种基于卷积循环神经网络的语言识别方法，使用梅尔频率倒谱系数特征。与现有的方法进行了比较分析，并得出了一些结论。

    

    语言识别（LID）是自动语音识别（ASR）领域中的关键预处理过程，涉及从音频样本中识别出讲话语言。当前能够处理多种语言的系统要求用户在使用之前明确指定一种或多种语言。在多语言环境下，当ASR系统无法理解讲话语言时，LID任务扮演着重要的角色，导致语音识别结果失败。本研究引入了基于卷积循环神经网络（CRNN）的LID方法，设计用于处理音频样本的梅尔频率倒谱系数（MFCC）特征。此外，我们复制了一些最先进的方法，特别是卷积神经网络（CNN）和基于注意力的卷积循环神经网络（带有注意力的CRNN），并与我们的基于CRNN的方法进行了比较分析。我们进行了一项实验

    Language Identification (LID) is a crucial preliminary process in the field of Automatic Speech Recognition (ASR) that involves the identification of a spoken language from audio samples. Contemporary systems that can process speech in multiple languages require users to expressly designate one or more languages prior to utilization. The LID task assumes a significant role in scenarios where ASR systems are unable to comprehend the spoken language in multilingual settings, leading to unsuccessful speech recognition outcomes. The present study introduces convolutional recurrent neural network (CRNN) based LID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC) characteristics of audio samples. Furthermore, we replicate certain state-of-the-art methodologies, specifically the Convolutional Neural Network (CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with attention), and conduct a comparative analysis with our CRNN-based approach. We conducted co
    
[^230]: 我该如何更新我的模型？关于预测过程监控模型对变化的韧性。

    How do I update my model? On the resilience of Predictive Process Monitoring models to change. (arXiv:2109.03501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.03501](http://arxiv.org/abs/2109.03501)

    本研究针对预测过程监控模型的韧性问题，评估了三种不同策略，并发现增量学习算法具有潜力来解决预测模型的更新和可变性问题。

    

    现有的经过充分研究的预测过程监控技术通常基于过去的流程执行构建预测模型，然后使用该模型预测新进行中案例的未来，无法通过新案例的执行来更新它。这使得预测过程监控对于在不断演变和/或随时间展现新变体行为的实际环境中的过程的可变性太过僵化。为解决这个问题，我们评估了三种不同的策略，允许周期性地重新发现或增量构建预测模型，以利用新的可用数据。评估重点放在新学习的预测模型的性能上，包括准确性和时间，与原始模型进行比较，并使用了一些真实和合成的数据集，包括明确的概念漂移和没有明确的概念漂移。结果证明了增量学习算法的潜力。

    Existing well investigated Predictive Process Monitoring techniques typically construct a predictive model based on past process executions, and then use it to predict the future of new ongoing cases, without the possibility of updating it with new cases when they complete their execution. This can make Predictive Process Monitoring too rigid to deal with the variability of processes working in real environments that continuously evolve and/or exhibit new variant behaviours over time. As a solution to this problem, we evaluate the use of three different strategies that allow the periodic rediscovery or incremental construction of the predictive model so as to exploit new available data. The evaluation focuses on the performance of the new learned predictive models, in terms of accuracy and time, against the original one, and uses a number of real and synthetic datasets with and without explicit Concept Drift. The results provide an evidence of the potential of incremental learning algo
    
[^231]: 重新审视用于表格数据的深度学习模型

    Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11959](http://arxiv.org/abs/2106.11959)

    本论文重新审视了用于表格数据的深度学习模型，提出了两种简单且强大的深度学习架构作为性能基准，包括类似于ResNet的架构和适应于表格数据的Transformer架构。这些基准模型在不同问题上表现出有竞争力的性能。

    

    现有关于表格数据的深度学习的文献提出了各种新颖的架构，并在各种数据集上报告了有竞争力的结果。然而，这些提出的模型通常没有进行适当的比较，现有的研究常常使用不同的基准和实验方案。因此，对于研究人员和实践者而言，什么样的模型性能最好是不清楚的。另外，该领域仍然缺乏有效的基准模型，即在不同问题上提供有竞争力性能的易于使用的模型。在这项工作中，我们对表格数据的主要DL架构进行了概述，并通过确定两种简单而强大的深度架构，提高了表格DL的基准。第一种是类似于ResNet的架构，结果显示它是常见的先前工作中常缺失的强基准。第二个模型是我们针对表格数据的Transformer架构的简单适应，在性能上超过了其他解决方案。

    The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solution
    
[^232]: GPT也理解了

    GPT Understands, Too. (arXiv:2103.10385v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2103.10385](http://arxiv.org/abs/2103.10385)

    提出了一种方法P-Tuning，通过使用可学习的连续提示嵌入和离散提示的拼接，稳定了预训练语言模型（PLM）的训练过程，并在多种自然语言理解任务上显著提高了性能。

    

    使用自然语言模式来促使预训练语言模型（PLM）具有自然语言理解（NLU）方面的效果已经被证明是有效的。然而，我们的初步研究发现，手动离散的提示往往导致性能不稳定——例如，在提示中改变一个单词可能导致性能大幅下降。我们提出了一种新方法P-Tuning，它使用训练可学习的连续提示嵌入以及离散提示的拼接。经验证明，P-Tuning不仅通过减小各种离散提示之间的差距来稳定训练，还通过较大幅度的提升在包括LAMA和SuperGLUE在内的各种NLU任务上的性能。无论是冻结的还是调整的语言模型，在全监督和少样本设置下，P-Tuning通常都是有效的。

    Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.
    
[^233]: 唯有利普希茨性能够驯服离线生成对抗模仿学习

    Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.16785](http://arxiv.org/abs/2006.16785)

    离线生成对抗模仿学习中，将学习到的奖励函数强制变成局部利普希茨连续是取得良好表现的必要条件，并且满足奖励的利普希茨性约束对模仿性能具有积极影响。

    

    尽管强化学习在各个领域取得了最近的成功，但是这些方法大多对超参数敏感，并且通常需要进行一些关键的工程操作才能取得成功。我们考虑了离线生成对抗模仿学习的情况，并对该方法进行了深入的定性和定量分析。我们证明，将学习到的奖励函数强制变成局部利普希茨连续是该方法表现良好的必要条件。然后，我们研究了这个必要条件的影响，并提供了涉及状态-价值函数的局部利普希茨性质的几个理论结果。我们通过实证证据证明，奖励的利普希茨性约束的一致满足对模仿性能具有极强的积极影响。最后，我们探讨了一个通用的悲观奖励预处理附加项，形成了一个大类的奖励函数。

    Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward 
    
[^234]: 组合微调：冻结预训练的降噪自编码器以提高泛化性能

    Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. (arXiv:2006.16205v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.16205](http://arxiv.org/abs/2006.16205)

    本论文提出了一种组合微调方法，通过冻结预训练的降噪自编码器来保留输出结构，从而显著降低预测器的复杂性并提高泛化性能。

    

    本文关注受输出有效性约束的结构化输出的预测问题，例如将伪代码翻译为代码时，代码必须能够编译。虽然标记的输入-输出对很难获取，但是“无标签”的输出，即没有对应输入的输出，是免费提供的（例如GitHub上的代码），并且提供了有关输出有效性的信息。我们可以通过预训练降噪器来捕捉输出结构，该降噪器用于去噪无标签输出的损坏版本。我们首先证明了在预训练之后进行标准微调会破坏部分输出结构。然后，我们提出了组合微调方法，该方法将预训练的降噪器与预测器组合进行微调，其中降噪器被冻结以保留输出结构。对于两层ReLU网络，我们证明了组合微调显著降低了预测器的复杂性，从而提高了泛化性能。在实证方面，我们展示了组合微调在两个伪代码到代码翻译任务上优于标准微调。

    We focus on prediction problems with structured outputs that are subject to output validity constraints, e.g. pseudocode-to-code translation where the code must compile. While labeled input-output pairs are expensive to obtain, "unlabeled" outputs, i.e. outputs without corresponding inputs, are freely available (e.g. code on GitHub) and provide information about output validity. We can capture the output structure by pre-training a denoiser to denoise corrupted versions of unlabeled outputs. We first show that standard fine-tuning after pre-training destroys some of this structure. We then propose composed fine-tuning, which fine-tunes a predictor composed with the pre-trained denoiser, which is frozen to preserve output structure. For two-layer ReLU networks, we prove that composed fine-tuning significantly reduces the complexity of the predictor, thus improving generalization. Empirically, we show that composed fine-tuning improves over standard fine-tuning on two pseudocode-to-code 
    
[^235]: 意识的神经计算模型：目标对齐的内部表示操作理论（GARIM）

    A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13490](http://arxiv.org/abs/1912.13490)

    这个论文提出了一个神经计算框架下的意识理论，称为“目标对齐的内部表示操作”（GARIM）。该理论认为意识支持对目标相关的内部表示进行主动操作，使其与追求的目标更加对齐，从而增加目标导向行为的灵活性。

    

    意识作为人类认知的核心要素，已经通过神经科学、心理学、人工智能和机器人技术等多种科学方法进行研究。然而，这些领域之间的不良整合限制了对意识的完整和清晰理解。在这篇论文中，我们通过提出一个神经计算框架下的“目标对齐的内部表示操作”（GARIM）意识理论，为改善这种整合做出了贡献。GARIM理论的核心思想是，意识支持对目标相关的内部表示（如世界状态、对象和行为序列）进行主动操作，使它们与追求的目标更加对齐。这些操作使得意识代理能够在内部产生其所缺乏的知识，以应对新条件和目标，从而增加目标导向行为的灵活性。表示的操作由四个神经功能宏系统（Hierarc...

    Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
    

