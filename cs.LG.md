# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Can Transformers Learn Optimal Filtering for Unknown Systems?.](http://arxiv.org/abs/2308.08536) | 本文研究了使用transformers进行最优输出估计问题，通过训练一个transformer来在未知系统上进行预测，并命名为元输出预测器（MOP）。我们观察到，尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当，在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。 |
| [^2] | [Painter: Teaching Auto-regressive Language Models to Draw Sketches.](http://arxiv.org/abs/2308.08520) | Painter是一种教导自回归语言模型绘制素描的方法，可以通过生成虚拟笔触将文本描述转化成图像，并具有除去、检测和分类对象的功能。 |
| [^3] | [Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database.](http://arxiv.org/abs/2308.08517) | 本文介绍了构建RadiologyNET的方法：利用多模态来源（图像、DICOM元数据和诊断叙述），通过自动的无监督方法，注释了来自克罗地亚里耶卡临床医院中心的大规模医学放射学图像数据库。 |
| [^4] | [Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems.](http://arxiv.org/abs/2308.08511) | 提出了一种两个半顺序基于得分的模型（TOSM），用于解决CT和MRI中的三维体重建问题。通过在二维空间中学习数据分布来减少训练的复杂性，然后在三维空间中更新数据分布以实现准确的三维重建。 |
| [^5] | [Autoencoding a Soft Touch to Learn Grasping from On-land to Underwater.](http://arxiv.org/abs/2308.08510) | 本研究通过使用视觉捕捉和监督式变分自动编码器（SVAE）学习的柔性机器手指，探索了从陆地到水下的抓取知识的可转移性，并展示了在不同环境中相对于商业FT传感器具有更好适应性的优势。 |
| [^6] | [ResBuilder: Automated Learning of Depth with Residual Structures.](http://arxiv.org/abs/2308.08504) | ResBuilder是一种神经架构搜索算法，可以自动生成ResNet深度学习模型，通过删除和插入ResNet块，搜索适合的模型，并实现了接近最先进性能的准确度，同时节约了计算成本。 |
| [^7] | [A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction.](http://arxiv.org/abs/2308.08502) | 该论文提出了一个基于元学习的堆叠回归方法，用于客户终身价值预测，旨在解决现有模型在处理各种输入特征时的限制，并避免深度学习方法的复杂性。 |
| [^8] | [InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models.](http://arxiv.org/abs/2308.08500) | 本文提出了InTune，一个基于强化学习的数据流优化方法，应用于深度推荐模型。通过研究在Netflix计算集群中的DLRM数据处理流程，我们发现目前的流程优化器存在性能不佳、频繁崩溃或需要不切实际的集群重组等问题。 |
| [^9] | [Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes.](http://arxiv.org/abs/2308.08494) | 该论文提出了一种概念化的机器学习方法，通过使用电子健康记录的审计日志作为监督，实现在特定临床背景下、特定时间点的笔记相关性检索。实验证明该方法在预测个别笔记撰写会话中哪些笔记会被阅读方面具有很高的准确性，并且临床医生的用户研究结果显示该框架可以帮助临床医生更高效地检索相关信息。 |
| [^10] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^11] | [Temporal Interest Network for Click-Through Rate Prediction.](http://arxiv.org/abs/2308.08487) | 本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。 |
| [^12] | [TBIN: Modeling Long Textual Behavior Data for CTR Prediction.](http://arxiv.org/abs/2308.08483) | TBIN模型通过局部敏感哈希算法和基于块位移的自注意力方法解决了利用长文本用户行为数据进行CTR预测时的截断问题和模型表达能力问题。 |
| [^13] | [Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features.](http://arxiv.org/abs/2308.08482) | 本研究提出了“快捷去偏”的方法，通过将目标任务对偏见属性的学习从偏见特征转移到快捷特征上，并采用因果干预来消除偏见特征的影响，以解决机器学习模型依赖敏感社会属性进行预测的公平性问题。 |
| [^14] | [Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals.](http://arxiv.org/abs/2308.08480) | 研究探索了在光电容积描记信号中利用标签传播技术进行不平衡类别中伪迹检测的方法，并证明其在标记医疗数据集和伪迹分类方面的有效性。 |
| [^15] | [LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs.](http://arxiv.org/abs/2308.08469) | 这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。 |
| [^16] | [An Expert's Guide to Training Physics-informed Neural Networks.](http://arxiv.org/abs/2308.08468) | 本文提出了一系列的最佳实践，可以显著提高物理知识神经网络(PINN)的训练效率和整体精度。同时，通过展示不同架构选择和训练策略对结果模型的测试精度的影响，提供了强有力的基线进行比较。 |
| [^17] | [On Neural Quantum Support Vector Machines.](http://arxiv.org/abs/2308.08467) | 本文介绍了神经量子支持向量机，利用量子核，扩展了神经支持向量机的训练算法。 |
| [^18] | [Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks.](http://arxiv.org/abs/2308.08465) | 该论文提出了一种基于层次化不确定性估计的医学图像分割网络。通过利用层次化图像表示和跳跃连接模块，实现了对多级不确定性的估计。实验证明，采用该方法可以提高深度学习分割网络的性能。 |
| [^19] | [CDR: Conservative Doubly Robust Learning for Debiased Recommendation.](http://arxiv.org/abs/2308.08461) | 该论文提出了一种保守双重稳健策略（CDR），用于解决推荐系统中存在的有毒插补问题。CDR通过审查插补的均值和方差来过滤插补，结果显示CDR具有降低方差和改进尾部界限的优势，并且能够显著提升性能并减少有毒插补的频率。 |
| [^20] | [Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance.](http://arxiv.org/abs/2308.08448) | 这项研究讨论了在金融领域中应用量子机器学习的新研究方向，通过比较qGAN和QCBM等模型，展示了在金融领域中实现量子优势的潜力。 |
| [^21] | [CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services.](http://arxiv.org/abs/2308.08446) | 本研究提出了CSPM模型，通过对比学习和时空偏好提取来解决按需食品配送CTR预测中的时空信息建模问题。 |
| [^22] | [Accurate synthesis of Dysarthric Speech for ASR data augmentation.](http://arxiv.org/abs/2308.08438) | 本文提出了一种针对口吃患者的ASR训练数据增强的口吃语音合成方法，通过添加严重程度系数和暂停插入模型，实现了不同严重程度口吃语音的合成。 |
| [^23] | [Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning.](http://arxiv.org/abs/2308.08427) | 本文提出了一个新的方法，通过交互式问答来识别代理人的风险规避。在一期情景和无限期情景下，我们通过要求代理人展示她的最优策略来回答问题，使用随机设计的问题来识别代理人的风险规避。这个方法可以通过一个有限的候选集有效地识别出代理人的风险规避。 |
| [^24] | [Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach.](http://arxiv.org/abs/2308.08410) | 本研究引入了一种名为测地线反向传播的方法，以解决心脏电生理模型中的逆问题。该方法适用于GPU加速的机器学习框架，可以通过优化曲面波方程的参数来重建给定的心电图。实验结果表明，测地线反向传播可以以高准确性重建模拟心脏激活，并在真实数据集上取得了积极的结果。 |
| [^25] | [Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities.](http://arxiv.org/abs/2308.08407) | 这篇综述论文讨论了可解释的人工智能在临床风险预测中的应用，包括概念、方法和方式。可解释性对于确保人们对AI系统的信任和可靠性至关重要，除了解释性之外，还涉及公平性、偏见、信任和透明度等方面。该综述还讨论了近期在临床风险预测中可解释模型的进展。 |
| [^26] | [Content-based Recommendation Engine for Video Streaming Platform.](http://arxiv.org/abs/2308.08406) | 本文提出了一种基于内容的推荐引擎，通过计算文档中单词的相关性和使用余弦相似度方法来推荐视频给用户。同时，还通过计算精确率、召回率和F1得分来评估引擎的性能。 |
| [^27] | [Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks.](http://arxiv.org/abs/2308.08391) | 本文提出了使用神经网络（NN）的代理模型方法，以降低计算成本来快速预测已用核燃料（SNF）的特性，包括衰变热和核素浓度。该模型的准确性通过验证和测试得到了证实。 |
| [^28] | [Continuous Sweep: an improved, binary quantifier.](http://arxiv.org/abs/2308.08387) | Continuous Sweep是一种改进的二元量化器，通过使用参数化类别分布、优化决策边界以及计算均值等方法，它在量化学习中取得了更好的性能。 |
| [^29] | [Precision and Recall Reject Curves for Classification.](http://arxiv.org/abs/2308.08381) | 该论文提出了一种在分类问题中评估精确度和召回率的拒绝曲线方法。使用感知量化的原型分类器来验证了该方法在不平衡数据集和医学实际数据上的有效性。 |
| [^30] | [A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks.](http://arxiv.org/abs/2308.08379) | 提出了一种用于神经网络的动态传感器选择方法，并结合使用Gumbel-Softmax技巧通过标准反向传播来学习离散决策。该方法可以增加无线传感器网络的寿命，并提高任务-DNN处理多种可能节点子集的能力。 |
| [^31] | [PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing.](http://arxiv.org/abs/2308.08371) | 该论文提出了一个框架，用于合成包含过程数据和对应程序化知识的数据集，并比较了不同嵌入方法的性能。 |
| [^32] | [Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition.](http://arxiv.org/abs/2308.08366) | 这篇论文提出了一种针对长尾识别问题的双分支温度缩放校准方法（Dual-TS），通过考虑不同类别温度参数的不同性和非通用性，解决了样本稀有问题和训练集验证集温度系数不一致的挑战。 |
| [^33] | [KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution.](http://arxiv.org/abs/2308.08361) | 本文提出了KernelWarehouse，它是一种更通用的动态卷积形式，通过重新定义动态卷积中的卷积核的概念，实现了在参数效率和表示能力之间的有利平衡。 |
| [^34] | [Independent Distribution Regularization for Private Graph Embedding.](http://arxiv.org/abs/2308.08360) | 这篇论文提出了一种独立分布正则化的方法，用于保护私有图嵌入的隐私。通过考虑主要学习和隐私保护，该方法解决了现有方法在训练阶段需拥有所有敏感属性的限制，并解决了隐私保护表示学习中常见的对抗学习技术问题。 |
| [^35] | [Convergence of Two-Layer Regression with Nonlinear Units.](http://arxiv.org/abs/2308.08358) | 本研究研究了两层非线性单元回归的收敛性，提出了一个softmax ReLU回归问题，并证明了Hessian的性质，引入了基于近似牛顿法的贪婪算法，最后证明了收敛性。 |
| [^36] | [Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?.](http://arxiv.org/abs/2308.08354) | 本文研究表明，针对推荐系统中的冷启动问题，元学习技术在处理深度学习模型时已成为最受欢迎的方法。然而，当前的元学习方法在实际推荐系统中并不实用，因为这些系统拥有庞大的用户和物品数量，且有严格的延迟要求。 |
| [^37] | [Graph Out-of-Distribution Generalization with Controllable Data Augmentation.](http://arxiv.org/abs/2308.08344) | 本论文提出了一种名为“OOD-GMixup”的方法，利用可控数据增强来解决图的带外分布泛化问题。该方法通过提取图合理性和生成虚拟样本的方式来消除虚假相关性和稳定性问题。 |
| [^38] | [Learning Logic Programs by Discovering Higher-Order Abstractions.](http://arxiv.org/abs/2308.08334) | 本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。 |
| [^39] | [Warped geometric information on the optimisation of Euclidean functions.](http://arxiv.org/abs/2308.08305) | 使用扭曲几何学的概念，我们提出了一种在高维欧几里德空间中优化函数的方法，并通过在重新定义的黎曼流形上进行计算，找到了函数的最优解。 |
| [^40] | [Robust Bayesian Satisficing.](http://arxiv.org/abs/2308.08291) | 本文提出了一种名为RoBOS的鲁棒贝叶斯满足算法，用于解决在上下文贝叶斯优化中存在分布偏移时的问题。该算法能够在一定的分布偏移量下保证亏得不严重的子线性遗憾，并且具有与分布偏移量无关的较弱遗憾边界。 |
| [^41] | [DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning.](http://arxiv.org/abs/2308.08290) | 我们提出了一种新的分散式联邦学习算法DFedADMM和其改进版本DFedADMM-SAM，用于解决局部不一致性和局部异构过拟合的问题。 |
| [^42] | [CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation.](http://arxiv.org/abs/2308.08283) | CARE是一种提供了大规模直肠癌CT图像数据集和临床适用的基准模型的研究。这个数据集提供了正常和癌性直肠的像素级注释，为算法研究和临床应用开发提供了宝贵的资源。 |
| [^43] | [It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models.](http://arxiv.org/abs/2308.08268) | 生成变换模型在OOD泛化方面存在神秘的性能下降。研究人员观察到模型在训练和泛化数字运算时的行为之间的不一致，并尝试提出解决方案，但仍未解决本质机制。 |
| [^44] | [Graph Relation Aware Continual Learning.](http://arxiv.org/abs/2308.08259) | 连续图学习中的两个挑战是如何处理图的内在属性以及如何在不增加模型复杂度的情况下传输跨图的信息。本文提出了一个新的模型来解决这个问题。 |
| [^45] | [Two Phases of Scaling Laws for Nearest Neighbor Classifiers.](http://arxiv.org/abs/2308.08247) | 最近邻分类器的缩放律可分为两个阶段：第一阶段中，泛化误差多项式地依赖于数据维度并迅速减小；第二阶段中，误差指数地依赖于数据维度并缓慢减小。这表明最近邻分类器在数据分布良好时可以实现泛化误差多项式地依赖于数据维度，而不是指数地依赖于数据维度。 |
| [^46] | [The Expressive Power of Graph Neural Networks: A Survey.](http://arxiv.org/abs/2308.08235) | 本综述调查了图神经网络（GNNs）在不同形式定义下增强表达能力的模型，并对图特征增强、图拓扑增强和GNNs架构增强进行了综述和讨论。 |
| [^47] | [Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey.](http://arxiv.org/abs/2308.08234) | 本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。 |
| [^48] | [SCQPTH: an efficient differentiable splitting method for convex quadratic programming.](http://arxiv.org/abs/2308.08232) | SCQPTH是一种用于凸二次规划的高效可微分的分裂算法，通过交替方向乘法器方法（ADMM）和操作拆分求解器实现。它在大规模问题上具有很高的计算效率提升。 |
| [^49] | [Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance.](http://arxiv.org/abs/2308.08230) | 本论文探索了Winograd卷积在神经网络容错性方面的潜力，并评估了从不同粒度（模型、层、操作类型）进行的综合容错评估。研究发现Winograd卷积能够降低容错设计开销，并与经典的容错设计方法有效结合，实现对软错误的成本效益的神经网络保护。 |
| [^50] | [How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning.](http://arxiv.org/abs/2308.08224) | 本论文研究了半监督学习中的确认偏差问题，并提出了三个现实数据场景中的挑战，分别是类间不平衡、类内不平衡和类间相似性。通过在模拟数据中进行实验，发现随机采样不能解决这些挑战。 |
| [^51] | [HyperSNN: A new efficient and robust deep learning model for resource constrained control applications.](http://arxiv.org/abs/2308.08222) | HyperSNN是一种适用于资源受限控制应用的高效稳健深度学习模型，通过使用脉冲神经网络和高维计算，将能量消耗降低至1.36%-9.96%，同时提高了鲁棒性和准确性。它适用于交互式、移动和可穿戴设备，促进了能量高效和稳健的系统设计。 |
| [^52] | [Epicure: Distilling Sequence Model Predictions into Patterns.](http://arxiv.org/abs/2308.08203) | Epicure是一种将序列模型的预测转化为简单模式的方法，能够准确预测高熵序列分布中的名称，并且在预测函数名称和检测异常名称的任务中相较于最高概率模型预测更准确匹配实际情况。 |
| [^53] | [DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting.](http://arxiv.org/abs/2308.08198) | DeSCo是一个通用且可扩展的深度子图计数方法，它解决了现有神经计数方法在计数准确性、图形区分和出现位置预测方面存在的问题。 |
| [^54] | [Endogenous Macrodynamics in Algorithmic Recourse.](http://arxiv.org/abs/2308.08187) | 这项研究填补了算法补救中的内生动力学和对策影响其他个体的研究空白，提出了一个广义框架，并揭示了对策的隐藏成本。通过模拟实验，验证了该方法的效果。 |
| [^55] | [Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design.](http://arxiv.org/abs/2308.08174) | 该论文提出了一种加速通用图神经网络的方法，通过协同设计架构、编译器和分区方法来解决GNN模型的高带宽需求和多样性的挑战。 |
| [^56] | [Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness.](http://arxiv.org/abs/2308.08173) | 通过对抗鲁棒性研究，揭示了图神经网络的表达能力与传统消息传递神经网络之间的显著差距，并证明了更强大的GNNs无法泛化到小扰动的图结构和分布不一样的图。 |
| [^57] | [AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation.](http://arxiv.org/abs/2308.08172) | 这项研究准备并发布了一款名为AATCT-IDS的基准腹部脂肪CT图像数据集，用于图像去噪、语义分割和放射学评估。通过对数据集中注释的脂肪组织区域进行研究，揭示了在不同任务中各种方法的性能差异，并验证了该数据集在这些任务中的研究潜力。 |
| [^58] | [A Quantum Approximation Scheme for k-Means.](http://arxiv.org/abs/2308.08167) | 这个论文提出了一个量子逼近方案，用于解决经典的k-Means聚类问题，该方案的运行时间与数据点的数量具有多对数依赖关系，并且能够在高概率下输出一个近似最优解，这是第一个具有多对数运行时间的量子算法，并且能够提供一个可证明的逼近保证。 |
| [^59] | [Characteristics of networks generated by kernel growing neural gas.](http://arxiv.org/abs/2308.08163) | 本研究开发了核化的生长神经气体算法，并研究了由此算法生成的网络的特性。研究发现，核生长神经气体算法可以将数据集转化为无向图，并提取数据集的特征作为图形。五种核函数被用于此研究。 |
| [^60] | [Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations.](http://arxiv.org/abs/2308.08162) | 本论文提出了一个用于评估原型部件解释的空间不一致性的解释性基准，并介绍了一种补偿不一致性的方法。通过大量的实证研究，表明了基准的表达能力和补偿方法的有效性。 |
| [^61] | [Benchmarking Adversarial Robustness of Compressed Deep Learning Models.](http://arxiv.org/abs/2308.08160) | 本研究通过开发一个全面的基准测试，证明了在深度学习模型压缩过程中，裁剪模型仍能保持基础模型的对抗鲁棒性水平。 |
| [^62] | [Deep Generative Imputation Model for Missing Not At Random Data.](http://arxiv.org/abs/2308.08158) | 本文介绍了一种用于处理非随机缺失数据的深度生成填充模型，提出了一种从新的角度去处理这个问题的方法，并且通过实验证明了直接将统计方法纳入深度生成模型的次优之处。 |
| [^63] | [Sarcasm Detection in a Disaster Context.](http://arxiv.org/abs/2308.08156) | 本文介绍了一个名为HurricaneSARC的数据集，其中包含了15,000条注释为讽刺意图的推文，并使用预训练语言模型进行了讽刺检测的研究。通过中间任务的迁移学习，我们的最佳模型在数据集上获得了0.70的F1值。 |
| [^64] | [Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series.](http://arxiv.org/abs/2308.08148) | 本文提出了一种改进的基于拓扑的方法，通过引入有限时间序列数据和条件独立性测试，目的是克服传统方法在生成冗余边方面的限制，从而更准确地识别观测数据中的因果关系。 |
| [^65] | [Online Control for Linear Dynamics: A Data-Driven Approach.](http://arxiv.org/abs/2308.08138) | 本文提出了一种数据驱动的方法来解决在线控制线性动力学问题，该方法不需要识别系统模型，而是通过累积扰动来进行决策，证明了算法性能与基于模型的方法相当。 |
| [^66] | [Microstructure-Empowered Stock Factor Extraction and Utilization.](http://arxiv.org/abs/2308.08135) | 本论文提出了一个新的框架，旨在从订单流数据中有效提取关键因子，用于多样化下游任务。 |
| [^67] | [Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?.](http://arxiv.org/abs/2308.08129) | 这项研究探讨了自我监督的预训练方法在分子性质预测中的外推能力，结果发现即使在强大的机器学习模型条件下，准确的外推仍然是一个具有挑战性的问题。 |
| [^68] | [How to Mask in Error Correction Code Transformer: Systematic and Double Masking.](http://arxiv.org/abs/2308.08128) | 该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。 |
| [^69] | [S-Mixup: Structural Mixup for Graph Neural Networks.](http://arxiv.org/abs/2308.08097) | S-Mixup是一种在节点分类任务中使用结构信息的新型Mixup增强方法，通过图神经网络的预测置信度和边梯度来构建Mixup池，并在真实世界基准数据集上进行了验证。 |
| [^70] | [Safety Filter Design for Neural Network Systems via Convex Optimization.](http://arxiv.org/abs/2308.08086) | 本文提出了一种通过凸优化为神经网络系统设计安全过滤器的方法，该过滤器能够捕获建模误差，并通过鲁棒线性模型预测控制来搜索可以保证约束满足的控制器。通过在非线性摆系统上的实验验证了该方法的有效性。 |
| [^71] | [Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation.](http://arxiv.org/abs/2308.08079) | 该论文介绍了一种用于地下数据集的稳定降维方法，通过刚性变换实现了欧几里德不变表示，能够量化地下数据的不确定性并且适应外样本点的扩展。 |
| [^72] | [Decentralized Graph Neural Network for Privacy-Preserving Recommendation.](http://arxiv.org/abs/2308.08072) | 本文提出了一种去中心化图神经网络（DGREC）框架，用于隐私保护推荐，其中用户可以选择公开他们的交互。该框架通过图构建、局部梯度计算和全局梯度传递三个阶段实现，同时引入了名为安全梯度共享的差分隐私机制，保护用户的私密数据。 |
| [^73] | [Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks.](http://arxiv.org/abs/2308.08071) | 本论文提出了一种通过动态图神经网络解决延迟反馈问题的方法，该方法在数据新鲜度和标签准确性之间取得了平衡。 |
| [^74] | [Max-affine regression via first-order methods.](http://arxiv.org/abs/2308.08070) | 本文研究了最大仿射回归问题，并提出了基于梯度下降和随机梯度下降的收敛分析方法。数值实验验证了理论结果的有效性。该方法在运行时间和观测次数较少时都能取得较好的效果。 |
| [^75] | [A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes.](http://arxiv.org/abs/2308.08069) | 本文通过使用强化学习方法，在云计算节点上设计了一个能够限制功耗而不影响应用程序性能的策略。 |
| [^76] | [The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models.](http://arxiv.org/abs/2308.08061) | 在部署大型语言模型时，需要关注泛化、评估和成本最优化。本文提出了一个针对大型语言模型的泛化、评估和成本建模框架，帮助企业深入了解和评估这些因素。 |
| [^77] | [Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation.](http://arxiv.org/abs/2308.08060) | 本文提出了一种鲁棒的贝叶斯张量分解方法，使用零膨胀泊松模型来处理包含过多零值的高维计数数据。为了解决随机性问题，引入了一致聚合的方法。在合成和真实数据集上的实验证明了该方法的优越性能。 |
| [^78] | [Simple online learning with consistency oracle.](http://arxiv.org/abs/2308.08055) | 该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。 |
| [^79] | [Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference.](http://arxiv.org/abs/2308.08053) | 提出了一种基于自然进化策略的黑盒估计器，可以克服变分自动编码器框架下对分布类型的限制。 |
| [^80] | [Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem.](http://arxiv.org/abs/2308.08051) | 该论文介绍了对抗领域适应的方法来解决银行贷款问题中训练集偏差问题，旨在学习无偏但信息丰富的过去数据表示。 |
| [^81] | [Regret Lower Bounds in Multi-agent Multi-armed Bandit.](http://arxiv.org/abs/2308.08046) | 在多智能体多臂赌博机中，我们首次全面研究了后悔下界，并证明了在具有良好连通性属性和随机分布奖励的情况下，存在紧密的实例相关和实例无关的下界。 |
| [^82] | [Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks.](http://arxiv.org/abs/2308.08030) | 本文研究了如何使用深度ReLU神经网络在没有对模型参数施加限制的情况下，对由高斯混合模型生成的无界数据进行二分类。我们首次获得了收敛速度不受维度诅咒影响的非渐近上界，并通过使用高斯分布的特性在无限域上进行了分类分析。 |
| [^83] | [Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning.](http://arxiv.org/abs/2308.08029) | 本论文提出了一种新颖的算法，称为SI SL，用于主动学习和模型优化过程中的规划。该算法通过与贝叶斯强化学习方案的比较证明了其性能的优越性。 |
| [^84] | [Potential Energy Advantage of Quantum Economy.](http://arxiv.org/abs/2308.08025) | 量子计算在能源效率方面具有优势，并且能够在盈利和能源效率上超越经典计算。这使得量子计算成为计算行业更可持续的选择。 |
| [^85] | [Active Inverse Learning in Stackelberg Trajectory Games.](http://arxiv.org/abs/2308.08017) | 这项研究提出了一种在Stackelberg博弈中的主动逆向学习方法，通过活跃地最大化跟随者在不同假设下的轨迹差异来加速领导者的推断过程。 |
| [^86] | [Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling.](http://arxiv.org/abs/2308.08012) | 本文通过设计具有空间金字塔池化网络的卷积神经网络模型，解决了网络稳健性评估中的性能、捕捉稳健性、可扩展性和可转移性等挑战。 |
| [^87] | [GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity.](http://arxiv.org/abs/2308.08010) | GRINN是一种物理信息导向的神经网络，用于在自重存在的情况下求解三维流体动力学系统。它在模拟重力不稳定性和波传播方面取得了良好的结果。 |
| [^88] | [BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis.](http://arxiv.org/abs/2308.08003) | BI-LAVA是一个通过主动学习和视觉分析进行层级图像标签的生物鉴定系统，它解决了标签层级性质、数据处理开销、标记数据缺失等问题，并帮助模型构建。 |
| [^89] | [Monte Carlo guided Diffusion for Bayesian linear inverse problems.](http://arxiv.org/abs/2308.07983) | 本研究提出了一种在贝叶斯框架下利用蒙特卡洛方法解决非完备线性逆问题的算法，该算法通过利用基于得分的生成模型的先验结构和Feynman-Kac模型，并进行顺序蒙特卡洛采样，表现出比竞争对手更好的性能。 |
| [^90] | [An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning.](http://arxiv.org/abs/2308.07980) | 本文提出了一种基于元学习的自适应概率风电功率预测方法，通过离线学习和在线学习过程，在不同的预测任务中展现出优异的适应性能力。数值测试结果验证了该方法的优势。 |
| [^91] | [MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction.](http://arxiv.org/abs/2308.07971) | MultiSChuBERT是一个多模态预测模型，通过结合文本和图像信息，在学术文档质量预测任务上取得了显著改进。我们的工作在结合视觉和文本嵌入、逐渐解冻视觉子模型权重以及采用最新文本嵌入替换标准BERT$_{\textrm{BASE}}$嵌入方面做出了重要贡献。 |
| [^92] | [Portfolio Selection via Topological Data Analysis.](http://arxiv.org/abs/2308.07944) | 通过拓扑数据分析，我们提出了一个基于两阶段方法的投资组合选择系统。实验结果表明，我们的系统优于其他方法，验证了拓扑数据分析作为一个强大的投资组合选择工具的可行性。 |
| [^93] | [Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis.](http://arxiv.org/abs/2308.07942) | 基于图神经网络和规则的归纳知识图谱补全研究了基于规则的方法在实践中的表现不佳的原因，发现不合理的实体没有排名和只考虑最具信息量的路径是影响因素。提出了一些解决这些问题的规则方法的变体，发现其性能接近于基于图神经网络的方法NBFNet。这些变体仅使用了NBFNet所依赖的证据的一小部分。 |
| [^94] | [Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data.](http://arxiv.org/abs/2308.07940) | 该论文尝试通过使用GPT-2模型从头开始训练编码的时空数据，生成受环境因素和个体属性影响的个体轨迹。 |
| [^95] | [Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting.](http://arxiv.org/abs/2308.07939) | Ada-QPacknet是一种自适应剪枝与位宽缩减的高效继续学习方法，通过剪枝和量化技术生成任务子网络，在动态和复杂环境中实现了与浮点数子网络相似的准确性。 |
| [^96] | [One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training.](http://arxiv.org/abs/2308.07934) | 本论文介绍了一种位翻转攻击和模型训练相结合的方法，通过在训练阶段引入对手构建高风险模型，在只进行少量位翻转的情况下，将正常模型转化为恶意模型。实验结果表明，这种攻击方法可以逃避各种检测方法。 |
| [^97] | [Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment.](http://arxiv.org/abs/2308.07933) | 该论文提出了一种新的痴呆检测模型，将图片和描述文本作为输入，并利用大型预训练图像文本对齐模型的知识。通过观察发现，痴呆样本与健康样本在文本与图片相关性和图片焦点区域上存在差异，从而可以提高痴呆检测的准确性。 |
| [^98] | [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation.](http://arxiv.org/abs/2308.07931) | 本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。 |
| [^99] | [Probabilistic Black-Box Checking via Active MDP Learning.](http://arxiv.org/abs/2308.07930) | 这篇论文介绍了一种新的方法，利用主动MDP学习来进行概率黑盒检查。这种方法通过在各个阶段运用不同的技术，增强了传统黑盒检查方法的能力。 |
| [^100] | [Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation.](http://arxiv.org/abs/2308.07929) | 本研究提出了一种快速自适应方法，利用Bradley-Terry偏好模型，通过很少的示例和最小的计算资源高效地微调大型多模态模型，使其更符合用户的偏好，并在多个领域中展示了该方法的能力。 |
| [^101] | [Predictive Modeling of Menstrual Cycle Length: A Time Series Forecasting Approach.](http://arxiv.org/abs/2308.07927) | 本研究通过使用机器学习技术，探索了预测月经周期的方法，结果表明可以准确预测月经周期的开始和持续时间。 |
| [^102] | [SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation.](http://arxiv.org/abs/2308.07896) | SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。 |
| [^103] | [DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models.](http://arxiv.org/abs/2308.07687) | 本论文提出了一种名为DiffGuard的方法，使用预训练的扩散模型进行语义不匹配引导的带外分布检测。实验证明，DiffGuard在小规模数据集上表现出色，但在ImageNet规模的数据集上无法应用。 |
| [^104] | [Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks.](http://arxiv.org/abs/2308.07439) | 本研究提出了一种交互感知的个性化车辆轨迹预测方法，利用时间图神经网络来建模目标车辆与周围交通之间的时空交互，并通过迁移学习来个性化预测。实验结果表明，该方法能够更准确地预测车辆的轨迹。 |
| [^105] | [LCE: An Augmented Combination of Bagging and Boosting in Python.](http://arxiv.org/abs/2308.07250) | LCE是一个用于分类和回归任务的Python包，它实现了局部级联集成方法，通过结合随机森林和XGBoost的优势，采用多样化的方法获得更好的泛化预测性能。 |
| [^106] | [Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases.](http://arxiv.org/abs/2308.07118) | 本文研究了神经辐射场在工业领域的应用，并提供了未来研究的方向。通过学习训练图像来学习3D场景表示，可以解决当前工业3D表示方法的不足。 |
| [^107] | [Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation.](http://arxiv.org/abs/2308.06422) | 本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。 |
| [^108] | [PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems.](http://arxiv.org/abs/2308.00864) | 本论文提出了一种基于个性化剩余策略的合作咨询系统PeRP，用于缓解拥堵。该系统通过结构化建模人类驾驶的相似性，并根据驾驶员的特征为其提供行动建议，以减少交通拥堵。 |
| [^109] | [Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems.](http://arxiv.org/abs/2307.16120) | 本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。 |
| [^110] | [Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities.](http://arxiv.org/abs/2307.13565) | 决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。 |
| [^111] | [LLM Cognitive Judgements Differ From Human.](http://arxiv.org/abs/2307.11787) | 这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。 |
| [^112] | [Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning.](http://arxiv.org/abs/2306.16296) | 本文提出了一种基于类比的方法，通过选择和修剪相关实体，从而引导知识图谱的构建。实证结果显示，这种方法在两个领域和异质种子实体的数据集上优于其他机器学习方法，并具有较低的参数数量。这些结果支持在相关任务中进一步应用类比推理。 |
| [^113] | [Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations.](http://arxiv.org/abs/2306.15891) | 本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），用于解决多尺度时变线性输运问题。该方法通过采用多个局部卷积操作、池化和激活操作来捕捉线性输运问题的扩散行为，并验证了方法的有效性。 |
| [^114] | [Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork.](http://arxiv.org/abs/2306.10698) | 人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。 |
| [^115] | [Latent Dynamical Implicit Diffusion Processes.](http://arxiv.org/abs/2306.07077) | 本文提出了一种新型的潜在变量模型 LDIDPs，利用隐式扩散过程从动态潜在过程中进行采样，然后生成相应的顺序观察样本，相较于最先进的顺序生成模型有更好的性能。 |
| [^116] | [Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection.](http://arxiv.org/abs/2306.05989) | 本文提出了一种名为QBSD的实时预测方法，以在时间序列异常检测中取得最佳平衡。 |
| [^117] | [How does over-squashing affect the power of GNNs?.](http://arxiv.org/abs/2306.03589) | 本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是... |
| [^118] | [Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network.](http://arxiv.org/abs/2306.01631) | 本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。 |
| [^119] | [Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics.](http://arxiv.org/abs/2305.18477) | 本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。 |
| [^120] | [Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization.](http://arxiv.org/abs/2305.18437) | 本文提出了一些数值编码和可视化方法，以支持机器学习算法处理混合数据，并提出了可解释的多分类模型和SRG算法来生成解释性分类模型。 |
| [^121] | [Beyond Individual Input for Deep Anomaly Detection on Tabular Data.](http://arxiv.org/abs/2305.15121) | 本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。 |
| [^122] | [Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models.](http://arxiv.org/abs/2305.13873) | 本文研究揭示了文本转图像模型生成不安全图像和令人憎恶的模因，并且发现这些模型可以生成相当大比例的不安全图像。作者鉴定了一些文本提示因素和模型倾向因素，以揭示不安全内容的生成机理，并且凸显了需要继续研究的必要性。 |
| [^123] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^124] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^125] | [CUTS+: High-dimensional Causal Discovery from Irregular Time-series.](http://arxiv.org/abs/2305.05890) | CUTS+是一种基于Granger因果和图神经网络(MPGNN)的因果发现算法，通过引入粗到细发现（C2FD）技术提高可扩展性。实验结果表明，CUTS+在非规则高维数据上的因果发现性能大幅度提高。 |
| [^126] | [Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber.](http://arxiv.org/abs/2305.04043) | Echoes提出了一种无监督的去偏方法，生成偏差对立样本的伪偏差标签，实现了对数据集中偏差特征的一致性处理，并取得了各项任务和数据集上的最先进性能。 |
| [^127] | [Are demographically invariant models and representations in medical imaging fair?.](http://arxiv.org/abs/2305.01397) | 医学影像模型编码患者人口统计信息，引发有关潜在歧视的担忧。研究表明，不编码人口属性的模型容易损失预测性能，而考虑人口统计属性的反事实模型不变性存在复杂性。人口统计学编码可以被认为是优势。 |
| [^128] | [DuETT: Dual Event Time Transformer for Electronic Health Records.](http://arxiv.org/abs/2304.13017) | DuETT是一个用于EHR的双重事件时间变换器，通过双重注意机制学习不同上下文中相同时间步的不同表示，避免由于时间步伐大而产生的二次放缩问题，并在四个基准EHR数据集上优于以往的单一任务最先进方法。 |
| [^129] | [A Scalable Test Problem Generator for Sequential Transfer Optimization.](http://arxiv.org/abs/2304.08503) | STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。 |
| [^130] | [A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance.](http://arxiv.org/abs/2304.06783) | 提出了一种基于Wasserstein距离的分布鲁棒方法实现后悔最优控制的控制器设计策略。 |
| [^131] | [RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples.](http://arxiv.org/abs/2304.04137) | 该论文介绍了一种基于码率-失真理论，结合分布式点过程的方法，用于多级分类中选择多样化的学习数据样本，相比现有方法具有更好的性能表现。 |
| [^132] | [Architecture-Preserving Provable Repair of Deep Neural Networks.](http://arxiv.org/abs/2304.03496) | 本文提出了一种保体系结构 V-多面体可证明修复深度神经网络的方法。修复只修改 DNN 的参数，具有灵活性，支持多种类型的层，并在多项式时间内运行。 |
| [^133] | [ERM++: An Improved Baseline for Domain Generalization.](http://arxiv.org/abs/2304.01973) | ERM++是一个用于域通用性的改进基准方法，通过更好地利用训练数据、模型参数选择和权重空间正则化等关键技术，在多个数据集上比标准ERM更有效，同时计算复杂度更低，表现也优于最先进方法。 |
| [^134] | [Black Box Few-Shot Adaptation for Vision-Language models.](http://arxiv.org/abs/2304.01752) | 本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。 |
| [^135] | [Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices.](http://arxiv.org/abs/2303.13538) | 该论文捕获了首个公开可访问的商用设备真实射频指纹数据集，这对于识别非法或未授权发射器具有重要意义。 |
| [^136] | [Ablating Concepts in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2303.13516) | 本论文提出了一种有效的方法，在不重新训练模型的情况下实现了预训练模型中的概念消融，可以消除文本到图像生成中的版权问题和样本记忆问题。 |
| [^137] | [Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks.](http://arxiv.org/abs/2303.10310) | 本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。 |
| [^138] | [Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator.](http://arxiv.org/abs/2302.14036) | 本文提出了一种使用集成文本到梅尔频谱生成器的无标记转写领域自适应端到端ASR系统。该系统可以在训练过程中动态生成梅尔频谱，并通过使用新领域的仅文本数据来适应ASR模型。研究结果表明，所提出的训练方法显著提高了ASR准确性，并在自适应质量和训练速度上超过了级联TTS系统与声码器。 |
| [^139] | [3D-aware Blending with Generative NeRFs.](http://arxiv.org/abs/2302.06608) | 这篇论文提出了一种使用生成式NeRF的3D感知融合方法，通过3D感知对齐和融合来解决输入图像不对齐的问题，该方法在FFHQ和AFHQ-Cat上验证了优于现有2D方法的性能。 |
| [^140] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^141] | [Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++.](http://arxiv.org/abs/2301.11118) | Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。 |
| [^142] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^143] | [Time-Synchronized Full System State Estimation Considering Practical Implementation Challenges.](http://arxiv.org/abs/2212.01729) | 本研究提出了一种基于深度神经网络的状态估计器（DeNSE），利用贝叶斯框架综合利用慢速但广泛存在的SCADA数据和快速但局部的PMU数据，实现了对整个系统的亚秒级状态估计。通过考虑实际挑战，如拓扑变化、非高斯测量噪声和错误数据检测与校正，证明了DeNSE方法的优越性。 |
| [^144] | [An ensemble of VisNet, Transformer-M, and pretraining models for molecular property prediction in OGB Large-Scale Challenge @ NeurIPS 2022.](http://arxiv.org/abs/2211.12791) | 本研究通过集成VisNet、Transformer-M和预训练模型，成功预测了给定分子的量子化学性质，取得了显著的性能提升。 |
| [^145] | [Disentangled Representation Learning.](http://arxiv.org/abs/2211.11695) | 解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。 |
| [^146] | [EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones.](http://arxiv.org/abs/2211.09703) | 本文提出了一种泛化课程学习方法，用于高效训练视觉主干网络，通过优先让模型学习“更容易学习”的模式，不断引入更难的模式，从而加速训练过程。 |
| [^147] | [Learning Ability of Interpolating Deep Convolutional Neural Networks.](http://arxiv.org/abs/2210.14184) | 本文研究了深度卷积神经网络（DCNNs）在欠参数和过参数设置下的学习能力，建立了欠参数DCNNs的学习速度，并通过一种新颖的网络加深方案获得了插值DCNN，从而验证了过拟合的DCNN的泛化性能。 |
| [^148] | [DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph.](http://arxiv.org/abs/2210.10592) | 本论文提出了一种用于离散时动态图的分离表示学习框架DyTed，通过设计时间片对比学习任务和结构对比学习任务，有效地识别时间不变和时间变化的表示，并提出分离感知判别器以增强分离性能。 |
| [^149] | [ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system.](http://arxiv.org/abs/2210.09043) | 本文提出了一种名为ST-former的新型transformer架构，用于COVID-19期间的城市轨道交通客流预测。通过引入改进的自注意机制和自适应多图卷积网络，可以准确建模客流的时空依赖关系和复杂空间依赖关系。 |
| [^150] | [Stochastic Constrained DRO with a Complexity Independent of Sample Size.](http://arxiv.org/abs/2210.05740) | 本文提出了一种适用于非凸和凸损失函数的随机算法，用于解决Kullback Leibler散度约束的分布鲁棒优化问题，并且具有与样本大小无关的复杂度，每次迭代只需要恒定的批次大小。实证研究证明了该算法在解决非凸和凸约束DRO问题中的有效性。 |
| [^151] | [Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models.](http://arxiv.org/abs/2210.03921) | 数据选择是一种令人惊讶的有效且通用的构建小型可解释模型的策略，它通过学习训练分布而非测试分布的数据，提高了传统基准模型的准确性，并在多个任务中展现出竞争力。 |
| [^152] | [Federated Learning with Server Learning: Enhancing Performance for Non-IID Data.](http://arxiv.org/abs/2210.02614) | 基于辅助学习的联邦学习可以显著提高在非独立同分布数据上的模型精度和收敛时间 |
| [^153] | [Diffusion Models for Graphs Benefit From Discrete State Spaces.](http://arxiv.org/abs/2210.01549) | 本论文提出了一种在生成离散图时使用离散噪声的方法，相比于之前的方法，实验证明使用离散噪声可以生成更高质量的样本，同时采样过程速度提高了30倍。 |
| [^154] | [AI-Assisted Discovery of Quantitative and Formal Models in Social Science.](http://arxiv.org/abs/2210.00563) | 该论文介绍了在社会科学研究中辅助AI发现定量和形式模型的方法。通过扩展神经符号方法，该系统能够从真实数据中发现可解释的非线性和动态关系，帮助科学家在研究过程中揭示新的关系和探索对照模型。 |
| [^155] | [Active Learning for Optimal Intervention Design in Causal Models.](http://arxiv.org/abs/2209.04744) | 本研究开发了一种因果主动学习策略，通过推断因果模型中的变量之间的关系以及干预对系统的影响，快速确定最优的干预措施，从而解决了在大干预空间下进行实验设计的问题。 |
| [^156] | [Neuro-Dynamic State Estimation for Networked Microgrids.](http://arxiv.org/abs/2208.12288) | 这项研究提出了一种神经动态状态估计算法，用于网络化微电网中的未知子系统。具体贡献包括：基于数据驱动的Neuro-DSE算法、自我完善的Neuro-DSE+算法、神经KalmanNet-DSE算法以及用于联合估计微电网状态和未知参数的增强Neuro-DSE算法。大量实验表明，在不同噪声水平、控制模式、电源和可观测性下，这些算法都具有有效性。 |
| [^157] | [Large-Scale Traffic Congestion Prediction based on Multimodal Fusion and Representation Mapping.](http://arxiv.org/abs/2208.11061) | 本文提出了一种基于卷积神经网络的新型框架，通过学习表示和多模态融合，结合各种全局参考信息，实现了在大规模地图上对任意查询位置进行准确的交通拥堵预测。 |
| [^158] | [Non-linear Embeddings in Hilbert Simplex Geometry.](http://arxiv.org/abs/2203.11434) | 本文研究了将图的距离矩阵嵌入到Hilbert Simplex几何中的表示能力，发现该几何结构在嵌入任务中与其他几何结构相媲美，同时具有快速和数值稳健的特点。 |
| [^159] | [Spatial-Temporal Attention Fusion Network for short-term passenger flow prediction on holidays in urban rail transit systems.](http://arxiv.org/abs/2203.00007) | 提出了一个基于深度学习的模型，利用空时注意力融合网络来预测城市轨道交通系统节假日短期客流。该模型通过多图注意力网络提取客流的复杂空间依赖性，通过卷积-注意力块提取客流的时间依赖性，并通过特征融合块进行信息融合。这个方法对于交通管理具有重要意义。 |
| [^160] | [STG-GAN: A spatiotemporal graph generative adversarial networks for short-term passenger flow prediction in urban rail transit systems.](http://arxiv.org/abs/2202.06727) | STG-GAN提出了一种基于深度学习的时空图生成对抗网络，用于解决短期客流预测的准确性、效率和内存占用问题。 |
| [^161] | [SMGRL: Scalable Multi-resolution Graph Representation Learning.](http://arxiv.org/abs/2201.12670) | 本论文提出了一个可扩展的多分辨率图表示学习框架（SMGRL），通过降低训练成本和利用自相似性在多个分辨率上应用算法，能够高效地学习多分辨率节点嵌入。 |
| [^162] | [STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary 2D Exemplar?.](http://arxiv.org/abs/2102.03973) | 本文提出了一种名为STS-GAN的基于生成对抗网络的框架，能够将给定的2D示例扩展到任意3D实体纹理，并合成具有高保真度和相似特征的实体纹理。 |

# 详细

[^1]: Transformers能否学习用于未知系统的最优滤波？

    Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])

    [http://arxiv.org/abs/2308.08536](http://arxiv.org/abs/2308.08536)

    本文研究了使用transformers进行最优输出估计问题，通过训练一个transformer来在未知系统上进行预测，并命名为元输出预测器（MOP）。我们观察到，尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当，在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。

    

    Transformers在自然语言处理中取得了显著的成功，然而它们在动态系统中的潜力仍然大部分未被探索。本文研究了使用transformers进行最优输出估计问题，它使用过去的所有输出来生成预测。我们使用来自先验分布的各种系统来训练transformer，然后在先前未见过的相同分布的系统上评估其性能。结果表明，获得的transformer就像一个预测算法，它可以在上下文中学习并快速适应和预测不同的系统，因此我们称之为元输出预测器（MOP）。尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当。通过大量的数值实验，我们观察到MOP在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。

    Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
    
[^2]: Painter: 教导自回归语言模型绘制素描

    Painter: Teaching Auto-regressive Language Models to Draw Sketches. (arXiv:2308.08520v1 [cs.CV])

    [http://arxiv.org/abs/2308.08520](http://arxiv.org/abs/2308.08520)

    Painter是一种教导自回归语言模型绘制素描的方法，可以通过生成虚拟笔触将文本描述转化成图像，并具有除去、检测和分类对象的功能。

    

    大型语言模型（LLMs）在自然语言理解方面取得了巨大的进展，它们也成功应用于计算机视觉、机器人、强化学习等其他领域。在这项工作中，我们通过直接生成虚拟的笔触来绘制图像，将LLMs应用于图像生成任务。我们提出了Painter，一种LLM，它可以通过自回归方式生成相应的笔触，将用户的文本描述转化为素描。我们基于预训练大型文本语料库的现成LLM构建了Painter，在保留语言理解能力的同时，通过在新任务上进行微调来应用它。我们创建了一个包含多种对象类型和任务的多对象素描与文本提示配对的数据集。Painter可以根据文本描述生成素描，清除画布上的对象，并检测和分类素描中的对象。尽管这是在图像生成中使用LLMs的史无前例的开创性工作，

    Large language models (LLMs) have made tremendous progress in natural language understanding and they have also been successfully adopted in other domains such as computer vision, robotics, reinforcement learning, etc. In this work, we apply LLMs to image generation tasks by directly generating the virtual brush strokes to paint an image. We present Painter, an LLM that can convert user prompts in text description format to sketches by generating the corresponding brush strokes in an auto-regressive way. We construct Painter based on off-the-shelf LLM that is pre-trained on a large text corpus, by fine-tuning it on the new task while preserving language understanding capabilities. We create a dataset of diverse multi-object sketches paired with textual prompts that covers several object types and tasks. Painter can generate sketches from text descriptions, remove objects from canvas, and detect and classify objects in sketches. Although this is an unprecedented pioneering work in using
    
[^3]: 构建RadiologyNET：一个大规模多模态医学数据库的无监督注释

    Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database. (arXiv:2308.08517v1 [cs.CV])

    [http://arxiv.org/abs/2308.08517](http://arxiv.org/abs/2308.08517)

    本文介绍了构建RadiologyNET的方法：利用多模态来源（图像、DICOM元数据和诊断叙述），通过自动的无监督方法，注释了来自克罗地亚里耶卡临床医院中心的大规模医学放射学图像数据库。

    

    背景与目标：近年来，机器学习在医学诊断和治疗中的应用取得了显著增长，通过开发依赖于注释医学放射学图像的计算机辅助诊断系统。然而，大规模注释图像数据集的可用性仍然是一个主要障碍，因为注释过程耗时且昂贵。本文探讨了如何根据语义相似性自动注释医学放射学图像数据库。材料与方法：采用一种自动的无监督方法，利用多模态来源（包括图像、DICOM元数据和诊断叙述），构建了一个来自克罗地亚里耶卡临床医院中心的大规模注释数据集。对于每个数据来源，测试了多个合适的特征提取器，并使用k-means和k-medoids聚类评估其效用。

    Background and objective: The usage of machine learning in medical diagnosis and treatment has witnessed significant growth in recent years through the development of computer-aided diagnosis systems that are often relying on annotated medical radiology images. However, the availability of large annotated image datasets remains a major obstacle since the process of annotation is time-consuming and costly. This paper explores how to automatically annotate a database of medical radiology images with regard to their semantic similarity.  Material and methods: An automated, unsupervised approach is used to construct a large annotated dataset of medical radiology images originating from Clinical Hospital Centre Rijeka, Croatia, utilising multimodal sources, including images, DICOM metadata, and narrative diagnoses. Several appropriate feature extractors are tested for each of the data sources, and their utility is evaluated using k-means and k-medoids clustering on a representative data sub
    
[^4]: 两个半顺序基于得分的模型用于解决三维不适定反问题

    Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems. (arXiv:2308.08511v1 [eess.IV])

    [http://arxiv.org/abs/2308.08511](http://arxiv.org/abs/2308.08511)

    提出了一种两个半顺序基于得分的模型（TOSM），用于解决CT和MRI中的三维体重建问题。通过在二维空间中学习数据分布来减少训练的复杂性，然后在三维空间中更新数据分布以实现准确的三维重建。

    

    计算机断层扫描（CT）和磁共振成像（MRI）是医学成像领域至关重要的技术。基于得分的模型已经被证明在解决CT和MRI中遇到的不同反问题（如稀疏视野CT和快速MRI重建）方面是有效的。然而，这些模型在实现精确的三维体重建方面面临挑战。现有的基于得分的模型主要关注重建二维数据分布，在重建的三维体积图像中导致相邻切片之间的不一致性。为了克服这一限制，我们提出了一种新颖的两个半顺序基于得分的模型（TOSM）。在训练阶段，我们的TOSM在二维空间中学习数据分布，相较于直接在三维体积上工作，减少了训练的复杂性。然而，在重建阶段，TOSM会在三维空间中更新数据分布，利用三个方向的互补得分（sag）

    Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial technologies in the field of medical imaging. Score-based models have proven to be effective in addressing different inverse problems encountered in CT and MRI, such as sparse-view CT and fast MRI reconstruction. However, these models face challenges in achieving accurate three dimensional (3D) volumetric reconstruction. The existing score-based models primarily focus on reconstructing two dimensional (2D) data distribution, leading to inconsistencies between adjacent slices in the reconstructed 3D volumetric images. To overcome this limitation, we propose a novel two-and-a-half order score-based model (TOSM). During the training phase, our TOSM learns data distributions in 2D space, which reduces the complexity of training compared to directly working on 3D volumes. However, in the reconstruction phase, the TOSM updates the data distribution in 3D space, utilizing complementary scores along three directions (sag
    
[^5]: 对岸到水下：一种自动编码柔软触摸学习抓取的方法

    Autoencoding a Soft Touch to Learn Grasping from On-land to Underwater. (arXiv:2308.08510v1 [cs.RO])

    [http://arxiv.org/abs/2308.08510](http://arxiv.org/abs/2308.08510)

    本研究通过使用视觉捕捉和监督式变分自动编码器（SVAE）学习的柔性机器手指，探索了从陆地到水下的抓取知识的可转移性，并展示了在不同环境中相对于商业FT传感器具有更好适应性的优势。

    

    机器人在探索海洋中扮演着人类操作者的物理代理的关键角色。然而，在高压水环境下完全浸没并且受到少量可见光影响时，可靠地抓取物体仍然具有挑战性，主要是由于指尖与物体表面之间的流体干扰对触觉力学的影响。本研究通过基于视觉的柔性机器手指，使用监督式变分自动编码器（SVAE）学习6D力和力矩（FT）来探索从陆地到水下的抓取知识的可转移性。高帧率摄像机捕捉到机器手指在陆地和水下与物理物体互动时的整体变形。结果表明，训练得到的SVAE模型学到了一系列可从陆地转移到水下的柔性力学的潜在表示，相对商业FT传感器而言，对不断变化的环境具有更好的适应性。通过柔软、精细和反应灵敏的抓取，能够实现...

    Robots play a critical role as the physical agent of human operators in exploring the ocean. However, it remains challenging to grasp objects reliably while fully submerging under a highly pressurized aquatic environment with little visible light, mainly due to the fluidic interference on the tactile mechanics between the finger and object surfaces. This study investigates the transferability of grasping knowledge from on-land to underwater via a vision-based soft robotic finger that learns 6D forces and torques (FT) using a Supervised Variational Autoencoder (SVAE). A high-framerate camera captures the whole-body deformations while a soft robotic finger interacts with physical objects on-land and underwater. Results show that the trained SVAE model learned a series of latent representations of the soft mechanics transferrable from land to water, presenting a superior adaptation to the changing environments against commercial FT sensors. Soft, delicate, and reactive grasping enabled by
    
[^6]: ResBuilder: 使用残差结构自动生成深度学习模型的算法研究

    ResBuilder: Automated Learning of Depth with Residual Structures. (arXiv:2308.08504v1 [cs.LG])

    [http://arxiv.org/abs/2308.08504](http://arxiv.org/abs/2308.08504)

    ResBuilder是一种神经架构搜索算法，可以自动生成ResNet深度学习模型，通过删除和插入ResNet块，搜索适合的模型，并实现了接近最先进性能的准确度，同时节约了计算成本。

    

    在这项工作中，我们开发了一种名为ResBuilder的神经架构搜索算法，可以从头开始开发ResNet架构，以在中等计算成本下实现高准确度。它还可以用于修改现有的架构，并具有删除和插入ResNet块的能力，从而在ResNet架构空间中搜索适合的架构。在我们对不同图像分类数据集的实验中，ResBuilder实现了接近于最先进性能的同时，节约了与现成的ResNet相比的计算成本。值得一提的是，我们在CIFAR10上调节了参数，得到了适用于所有其他数据集的合适默认选择。我们证明了这个属性甚至在工业应用中也是适用的，通过在专有的欺诈检测数据集上应用我们的方法并使用默认参数进行实验。

    In this work, we develop a neural architecture search algorithm, termed Resbuilder, that develops ResNet architectures from scratch that achieve high accuracy at moderate computational cost. It can also be used to modify existing architectures and has the capability to remove and insert ResNet blocks, in this way searching for suitable architectures in the space of ResNet architectures. In our experiments on different image classification datasets, Resbuilder achieves close to state-of-the-art performance while saving computational cost compared to off-the-shelf ResNets. Noteworthy, we once tune the parameters on CIFAR10 which yields a suitable default choice for all other datasets. We demonstrate that this property generalizes even to industrial applications by applying our method with default parameters on a proprietary fraud detection dataset.
    
[^7]: 基于元学习的堆叠回归方法用于客户终身价值预测

    A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction. (arXiv:2308.08502v1 [cs.LG])

    [http://arxiv.org/abs/2308.08502](http://arxiv.org/abs/2308.08502)

    该论文提出了一个基于元学习的堆叠回归方法，用于客户终身价值预测，旨在解决现有模型在处理各种输入特征时的限制，并避免深度学习方法的复杂性。

    

    全球各地的公司都渴望定位潜在的高价值客户，以扩大收入，而这只能通过更好地了解客户来实现。客户终身价值（CLV）是客户在一段规定的时间内与企业进行的交易/购买的总货币价值，用于预测未来客户互动。CLV在多个不同的商业领域中都有应用，如银行，保险，在线娱乐，游戏和电子商务。现有的基于分布和基本（最近性，频率和金额）的模型在处理各种输入特征方面存在限制。此外，更先进的深度学习方法可能过于复杂，在某些应用领域中增加了不必要的复杂性。因此，我们提出了一个既能够有效又全面简单易懂的系统。考虑到这一点，我们开发了一个基于元学习的堆叠回归方法，用于客户终身价值预测。

    Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a m
    
[^8]: InTune:基于强化学习的数据流优化用于深度推荐模型

    InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models. (arXiv:2308.08500v1 [cs.IR])

    [http://arxiv.org/abs/2308.08500](http://arxiv.org/abs/2308.08500)

    本文提出了InTune，一个基于强化学习的数据流优化方法，应用于深度推荐模型。通过研究在Netflix计算集群中的DLRM数据处理流程，我们发现目前的流程优化器存在性能不佳、频繁崩溃或需要不切实际的集群重组等问题。

    

    基于深度学习的推荐模型(DLRM)已经成为许多现代推荐系统的重要组成部分。一些公司正在建设大型计算集群专门用于DLRM训练，进而推动了对成本和时间的节约优化的新兴兴趣。在这个场景中所面临的系统挑战是独特的；尽管典型的深度学习训练任务由模型执行主导，但DLRM训练性能中最重要的因素往往是线上数据摄入。在本文中，我们探讨了该数据摄入问题的独特特征，并深入研究了DLRM训练流程中的性能瓶颈和挑战。我们对Netflix计算集群中真实的DLRM数据处理流程进行了研究，观察了线上摄入的性能影响，并识别出现有流程优化器的不足之处。我们发现当前的工具要么产生次优性能，要么经常崩溃，要么需要不现实的集群重组。

    Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.  In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization 
    
[^9]: 用于动态电子健康记录信息检索的机器学习概念化

    Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes. (arXiv:2308.08494v1 [cs.IR])

    [http://arxiv.org/abs/2308.08494](http://arxiv.org/abs/2308.08494)

    该论文提出了一种概念化的机器学习方法，通过使用电子健康记录的审计日志作为监督，实现在特定临床背景下、特定时间点的笔记相关性检索。实验证明该方法在预测个别笔记撰写会话中哪些笔记会被阅读方面具有很高的准确性，并且临床医生的用户研究结果显示该框架可以帮助临床医生更高效地检索相关信息。

    

    临床医生花费大量时间筛选病人笔记并在电子健康记录（EHR）中记录是临床医生倦怠的主要原因。通过在记录过程中主动和动态地检索相关笔记，我们可以减少查找相关病例历史所需的工作量。在这项工作中，我们概念化了使用EHR审计日志作为机器学习的来源，以监督特定临床背景下、特定时间点的笔记相关性。我们的评估重点放在紧急科室的动态检索上，这是一个具有独特信息检索和笔记编写模式的高重症设置。我们显示我们的方法在预测哪些笔记会在个别笔记撰写会话中被阅读方面可以实现0.963的AUC。此外，我们对多名临床医生进行了用户研究，发现我们的框架可以帮助临床医生更高效地检索相关信息。通过展示我们的框架和...

    The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and m
    
[^10]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^11]: 点击率预测的时间兴趣网络

    Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])

    [http://arxiv.org/abs/2308.08487](http://arxiv.org/abs/2308.08487)

    本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。

    

    用户行为的历史是预测点击率最重要的特征之一，因为它们与目标项目具有强烈的语义和时间相关性。虽然已有文献分别研究了这些相关性，但尚未分析它们的组合，即行为语义、目标语义、行为时间和目标时间的四重相关性。这种相关性对性能的影响以及现有方法学习这种相关性的程度尚不清楚。为了填补这一空白，我们在实践中测量了四重相关性，并观察到直观而强大的四重模式。我们测量了几种代表性的用户行为方法的学习相关性，但令人惊讶的是，它们都没有学习到这样的模式，特别是时间模式。在本文中，我们提出了时间兴趣网络（TIN）来捕捉行为与目标之间的四重语义和时间相关性。

    The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
    
[^12]: TBIN: 模型化长文本行为数据用于CTR预测

    TBIN: Modeling Long Textual Behavior Data for CTR Prediction. (arXiv:2308.08483v1 [cs.IR])

    [http://arxiv.org/abs/2308.08483](http://arxiv.org/abs/2308.08483)

    TBIN模型通过局部敏感哈希算法和基于块位移的自注意力方法解决了利用长文本用户行为数据进行CTR预测时的截断问题和模型表达能力问题。

    

    点击率（CTR）预测在推荐系统的成功中起着关键作用。受到最近语言模型（LMs）的繁荣影响，许多研究通过以文本格式组织用户行为数据，并利用LMs来在语义层面上理解用户兴趣来改进预测。虽然有前景，但这些研究不得不截断文本数据以减少LMs中自注意力的二次计算开销。然而，已经研究表明长时间的用户行为数据可以显著提高CTR预测。此外，这些工作通常将用户的多样化兴趣压缩成一个特征向量，这阻碍了模型的表达能力。本文提出了一种基于文本行为的兴趣切块网络（TBIN），通过结合高效的局部敏感哈希算法和基于块位移的自注意力方法来解决上述限制。得到的用户多样化兴趣是

    Click-through rate (CTR) prediction plays a pivotal role in the success of recommendations. Inspired by the recent thriving of language models (LMs), a surge of works improve prediction by organizing user behavior data in a \textbf{textual} format and using LMs to understand user interest at a semantic level. While promising, these works have to truncate the textual data to reduce the quadratic computational overhead of self-attention in LMs. However, it has been studied that long user behavior data can significantly benefit CTR prediction. In addition, these works typically condense user diverse interests into a single feature vector, which hinders the expressive capability of the model. In this paper, we propose a \textbf{T}extual \textbf{B}ehavior-based \textbf{I}nterest Chunking \textbf{N}etwork (TBIN), which tackles the above limitations by combining an efficient locality-sensitive hashing algorithm and a shifted chunk-based self-attention. The resulting user diverse interests are
    
[^13]: 通过使用快捷特征实现公平视觉识别的良性捷径消除偏见：一篇实例分析

    Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features. (arXiv:2308.08482v1 [cs.LG])

    [http://arxiv.org/abs/2308.08482](http://arxiv.org/abs/2308.08482)

    本研究提出了“快捷去偏”的方法，通过将目标任务对偏见属性的学习从偏见特征转移到快捷特征上，并采用因果干预来消除偏见特征的影响，以解决机器学习模型依赖敏感社会属性进行预测的公平性问题。

    

    机器学习模型经常学会依赖于性别和种族等敏感社会属性进行预测，这在社会应用中，如招聘、银行和刑事司法中，带来重大公平风险。现有工作通过减少模型中与社会属性相关的信息来解决这个问题。然而，目标任务和这些社会属性之间的高相关性使得在目标任务上的学习与去偏不兼容。鉴于模型偏见是由于学习偏见特征（例如性别）来优化目标任务而引起的，我们探讨以下研究问题：我们是否可以利用快捷特征来替代偏见特征在去偏的目标任务优化中的作用？为此，我们提出了“快捷去偏”的方法，首先将目标任务对偏见属性的学习从偏见特征转移到快捷特征上，然后采用因果干预来消除偏见特征的影响。

    Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate sh
    
[^14]: 利用光电容积描记信号中的标签传播技术进行不平衡类别中的伪迹检测

    Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals. (arXiv:2308.08480v1 [cs.LG])

    [http://arxiv.org/abs/2308.08480](http://arxiv.org/abs/2308.08480)

    研究探索了在光电容积描记信号中利用标签传播技术进行不平衡类别中伪迹检测的方法，并证明其在标记医疗数据集和伪迹分类方面的有效性。

    

    光电容积描记信号在医疗保健中被广泛用于监测生命体征，但它们容易受到运动伪迹的影响，从而导致不准确的解释。本研究探索了在不平衡类别场景中使用标签传播技术在PPG样本之间传播标签的方法，其中干净的PPG样本明显少于受伪迹污染的样本。结果显示，在没有伪迹的类别中，精确度为91%，召回率为90%，F1得分为90%，证明了其在标记医疗数据集方面的有效性，即使干净样本很少。对于伪迹的分类，我们的研究比较了传统分类器和神经网络（MLP、Transformers、FCN）等有监督分类器与半监督标签传播算法。KNN有监督模型具有89%的精确度、95%的召回率和92%的F1得分，结果良好，但半监督算法在检测方面表现更好。

    Photoplethysmogram (PPG) signals are widely used in healthcare for monitoring vital signs, but they are susceptible to motion artifacts that can lead to inaccurate interpretations. In this study, the use of label propagation techniques to propagate labels among PPG samples is explored, particularly in imbalanced class scenarios where clean PPG samples are significantly outnumbered by artifact-contaminated samples. With a precision of 91%, a recall of 90% and an F1 score of 90% for the class without artifacts, the results demonstrate its effectiveness in labeling a medical dataset, even when clean samples are rare. For the classification of artifacts our study compares supervised classifiers such as conventional classifiers and neural networks (MLP, Transformers, FCN) with the semi-supervised label propagation algorithm. With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNN supervised model gives good results, but the semi-supervised algorithm performs better in detec
    
[^15]: LLM4TS:使用预训练的LLM进行两阶段微调用于时间序列预测

    LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])

    [http://arxiv.org/abs/2308.08469](http://arxiv.org/abs/2308.08469)

    这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。

    

    在这项工作中，我们利用预训练的大型语言模型（LLMs）来增强时间序列预测。借鉴了自然语言处理和计算机视觉统一模型的日益增长的兴趣，我们设想创建一个类似的模型用于长期时间序列预测。由于缺乏大规模的时间序列数据来构建稳健的基础模型，我们的方法LLM4TS专注于利用预训练的LLMs的优势。通过将时间序列修补与时间编码相结合，我们提高了LLMs处理时间序列数据的能力。受到聊天机器人领域的有监督微调的启发，我们优先进行两阶段的微调过程：首先进行有监督微调以使LLMs适应时间序列数据，然后进行任务特定的下游微调。此外，为了在不进行大量参数调整的情况下发挥预训练LLMs的灵活性，我们采用了几种参数高效微调（PEFT）技术。

    In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
    
[^16]: 训练基于物理知识的神经网络的专家指南

    An Expert's Guide to Training Physics-informed Neural Networks. (arXiv:2308.08468v1 [cs.LG])

    [http://arxiv.org/abs/2308.08468](http://arxiv.org/abs/2308.08468)

    本文提出了一系列的最佳实践，可以显著提高物理知识神经网络(PINN)的训练效率和整体精度。同时，通过展示不同架构选择和训练策略对结果模型的测试精度的影响，提供了强有力的基线进行比较。

    

    物理知识神经网络(PINN)已经成为一种流行的深度学习框架，它可以无缝地合成观测数据和偏微分方程(PDE)约束。然而，它们的实际有效性可能受到训练病态的影响，而且常常因为缺乏深度学习专业知识的用户做出的不良选择而受到影响。在本文中，我们提出了一系列最佳实践，可以显著提高PINN的训练效率和整体精度。我们还提出了一系列具有挑战性的基准问题，突出显示了训练PINN的一些主要困难，并呈现了全面可复现的消融研究，展示了不同架构选择和训练策略对结果模型的测试精度的影响。我们表明，本研究提出的方法和指导原则可以导致最先进的结果，并为未来的研究提供强有力的基线进行比较。

    Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparis
    
[^17]: 论神经量子支持向量机

    On Neural Quantum Support Vector Machines. (arXiv:2308.08467v1 [quant-ph])

    [http://arxiv.org/abs/2308.08467](http://arxiv.org/abs/2308.08467)

    本文介绍了神经量子支持向量机，利用量子核，扩展了神经支持向量机的训练算法。

    

    在 \cite{simon2023algorithms} 中，我们介绍了四种用于训练神经支持向量机（NSVMs）的算法，并证明了其可行性。在本文中，我们引入了神经量子支持向量机，即具有量子核的NSVMs，并将我们的结果扩展到这个情景中。

    In \cite{simon2023algorithms} we introduced four algorithms for the training of neural support vector machines (NSVMs) and demonstrated their feasibility. In this note we introduce neural quantum support vector machines, that is, NSVMs with a quantum kernel, and extend our results to this setting.
    
[^18]: 基于层次化不确定性估计的医学图像分割网络

    Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks. (arXiv:2308.08465v1 [eess.IV])

    [http://arxiv.org/abs/2308.08465](http://arxiv.org/abs/2308.08465)

    该论文提出了一种基于层次化不确定性估计的医学图像分割网络。通过利用层次化图像表示和跳跃连接模块，实现了对多级不确定性的估计。实验证明，采用该方法可以提高深度学习分割网络的性能。

    

    学习医学图像分割模型是一个本质上存在歧义的任务，因为模型训练中存在图像的不确定性（噪声）和手动标注的不确定性（人为错误和偏差）。为了建立一个可信赖的图像分割模型，不仅需要评估其性能，还需要估计模型预测的不确定性。大多数先进的图像分割网络采用层次化编码器结构，从精细到粗糙提取多个分辨率级别的图像特征。在这项工作中，我们利用这种层次化图像表示，并提出了一种简单而有效的多级不确定性估计方法。多级不确定性通过跳跃连接模块建模，然后进行采样以生成预测图像分割的不确定性地图。我们证明，当使用这种层次化不确定性估计模块实现深度学习分割网络（如U-net）时，可以实现较好的性能。

    Learning a medical image segmentation model is an inherently ambiguous task, as uncertainties exist in both images (noise) and manual annotations (human errors and bias) used for model training. To build a trustworthy image segmentation model, it is important to not just evaluate its performance but also estimate the uncertainty of the model prediction. Most state-of-the-art image segmentation networks adopt a hierarchical encoder architecture, extracting image features at multiple resolution levels from fine to coarse. In this work, we leverage this hierarchical image representation and propose a simple yet effective method for estimating uncertainties at multiple levels. The multi-level uncertainties are modelled via the skip-connection module and then sampled to generate an uncertainty map for the predicted image segmentation. We demonstrate that a deep learning segmentation network such as U-net, when implemented with such hierarchical uncertainty estimation module, can achieve a h
    
[^19]: CDR：用于去偏推荐的保守双重稳健学习

    CDR: Conservative Doubly Robust Learning for Debiased Recommendation. (arXiv:2308.08461v1 [cs.IR])

    [http://arxiv.org/abs/2308.08461](http://arxiv.org/abs/2308.08461)

    该论文提出了一种保守双重稳健策略（CDR），用于解决推荐系统中存在的有毒插补问题。CDR通过审查插补的均值和方差来过滤插补，结果显示CDR具有降低方差和改进尾部界限的优势，并且能够显著提升性能并减少有毒插补的频率。

    

    在推荐系统中，用户行为数据往往是观察性的而不是实验性的，导致数据中普遍存在偏差。因此，解决偏差问题已成为推荐系统领域的一个重要挑战。最近，双重稳健学习（DR）由于其卓越的性能和稳健的特性而受到了广泛关注。然而，我们的实验结果表明，现有的DR方法在存在所谓的有毒插补（Poisonous Imputation）时受到严重影响，插补明显偏离真实数据并适得其反。为了解决这个问题，本文提出了一种保守双重稳健策略（CDR），通过审查插补的均值和方差来过滤插补。理论分析表明，CDR可以降低方差并改进尾部界限。此外，我们的实验研究表明，CDR显著提升了性能，并且确实减少了有毒插补的频率。

    In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.  To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
    
[^20]: 在金融领域中实现量子生成对抗网络（qGAN）和QCBM

    Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance. (arXiv:2308.08448v1 [quant-ph])

    [http://arxiv.org/abs/2308.08448](http://arxiv.org/abs/2308.08448)

    这项研究讨论了在金融领域中应用量子机器学习的新研究方向，通过比较qGAN和QCBM等模型，展示了在金融领域中实现量子优势的潜力。

    

    量子机器学习（QML）是一个跨学科的领域，由两个最具创新性的研究领域组成：量子计算和经典机器学习（ML），ML和人工智能（AI）被认为是将受到量子计算机兴起影响的第一个领域。这项工作讨论了在金融中应用量子机器学习（QML）的一些新研究领域，我们讨论了一些已在金融界引起关注的QML模型，以及使用模拟环境中的真实金融数据集对qGAN（量子生成对抗网络）和QCBM（量子电路Born机）等模型进行比较。对于qGAN，我们定义了鉴别器和生成器的量子电路，并展示了未来在金融领域中通过QML实现量子优势的潜力。

    Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
    
[^21]: CSPM: 用于按需食品配送CTR预测的对比时空偏好模型

    CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services. (arXiv:2308.08446v1 [cs.IR])

    [http://arxiv.org/abs/2308.08446](http://arxiv.org/abs/2308.08446)

    本研究提出了CSPM模型，通过对比学习和时空偏好提取来解决按需食品配送CTR预测中的时空信息建模问题。

    

    点击率（CTR）预测是在线按需食品配送（OFD）平台中一项关键任务，用于准确估计用户点击食品项目的概率。与淘宝和亚马逊等通用电商平台不同，OFD平台上的用户行为和兴趣更加与地点和时间相关，这是因为存在有限的配送范围和区域商品供应。然而，现有的OFD场景下的CTR预测算法主要集中于捕捉来自历史行为序列的兴趣，未能有效地对特征中的复杂时空信息进行建模，导致性能较差。为解决这一挑战，本文提出了对比时空偏好模型（CSPM），通过对比时空表示学习（CSRL）、时空偏好提取器（StPE）和时空信息过滤器（StIF）三个模块，对不同搜索状态下的用户偏好进行建模。

    Click-through rate (CTR) prediction is a crucial task in the context of an online on-demand food delivery (OFD) platform for precisely estimating the probability of a user clicking on food items. Unlike universal e-commerce platforms such as Taobao and Amazon, user behaviors and interests on the OFD platform are more location and time-sensitive due to limited delivery ranges and regional commodity supplies. However, existing CTR prediction algorithms in OFD scenarios concentrate on capturing interest from historical behavior sequences, which fails to effectively model the complex spatiotemporal information within features, leading to poor performance. To address this challenge, this paper introduces the Contrastive Sres under different search states using three modules: contrastive spatiotemporal representation learning (CSRL), spatiotemporal preference extractor (StPE), and spatiotemporal information filter (StIF). CSRL utilizes a contrastive learning framework to generate a spatiotem
    
[^22]: ASR数据增强中对口吃性语音的准确合成方法

    Accurate synthesis of Dysarthric Speech for ASR data augmentation. (arXiv:2308.08438v1 [cs.SD])

    [http://arxiv.org/abs/2308.08438](http://arxiv.org/abs/2308.08438)

    本文提出了一种针对口吃患者的ASR训练数据增强的口吃语音合成方法，通过添加严重程度系数和暂停插入模型，实现了不同严重程度口吃语音的合成。

    

    口吃是一种运动言语障碍，常通过对言语产生肌肉的缓慢、不协调的控制来表征语音可理解性降低。自动语音识别（ASR）系统可以帮助口吃患者更有效地进行交流。然而，稳健的针对口吃的ASR需要大量的训练语音，而这对于口吃患者来说并不容易获得。本文提出了一种新的口吃语音合成方法，用于ASR训练数据增强。口吃自发语音在不同严重程度水平上的韵律和声学特征差异是口吃语音建模、合成和增强的重要组成部分。为了进行口吃语音合成，采用了一种修改后的多话者神经文本到语音（TTS）方法，通过添加口吃严重程度系数和暂停插入模型，合成不同严重程度的口吃语音。为了评估合成训练数据的有效性，进行了ASR性能测试。

    Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data fo
    
[^23]: 借助交互式问答通过逆强化学习来引导风险规避

    Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning. (arXiv:2308.08427v1 [stat.ML])

    [http://arxiv.org/abs/2308.08427](http://arxiv.org/abs/2308.08427)

    本文提出了一个新的方法，通过交互式问答来识别代理人的风险规避。在一期情景和无限期情景下，我们通过要求代理人展示她的最优策略来回答问题，使用随机设计的问题来识别代理人的风险规避。这个方法可以通过一个有限的候选集有效地识别出代理人的风险规避。

    

    本文提出了一个新颖的框架，利用交互式问答来识别代理人的风险规避。我们的研究在两种情景中进行：一期情景和无限期情景。在一期情景中，我们假设代理人的风险规避由状态的成本函数和失真风险度量所表征。在无限期情景中，我们用一个额外的成分，折扣因子，来建模风险规避。假设我们可以访问一个包含代理人真实风险规避的有限候选集，我们证明通过要求代理人在各种环境中展示她的最优政策来回答问题，这可以有效地识别代理人的风险规避。具体而言，我们证明了当问题的数量趋近无穷大并且问题是随机设计的时候，可以识别出代理人的风险规避。我们还开发了一个算法用于设计最优问题，并提供了实证证据来支持我们的方法。

    This paper proposes a novel framework for identifying an agent's risk aversion using interactive questioning. Our study is conducted in two scenarios: a one-period case and an infinite horizon case. In the one-period case, we assume that the agent's risk aversion is characterized by a cost function of the state and a distortion risk measure. In the infinite horizon case, we model risk aversion with an additional component, a discount factor. Assuming the access to a finite set of candidates containing the agent's true risk aversion, we show that asking the agent to demonstrate her optimal policies in various environment, which may depend on their previous answers, is an effective means of identifying the agent's risk aversion. Specifically, we prove that the agent's risk aversion can be identified as the number of questions tends to infinity, and the questions are randomly designed. We also develop an algorithm for designing optimal questions and provide empirical evidence that our met
    
[^24]: 从表面心电图中数字双生心脏电生理模型：一种测地线反向传播方法

    Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach. (arXiv:2308.08410v1 [math.OC])

    [http://arxiv.org/abs/2308.08410](http://arxiv.org/abs/2308.08410)

    本研究引入了一种名为测地线反向传播的方法，以解决心脏电生理模型中的逆问题。该方法适用于GPU加速的机器学习框架，可以通过优化曲面波方程的参数来重建给定的心电图。实验结果表明，测地线反向传播可以以高准确性重建模拟心脏激活，并在真实数据集上取得了积极的结果。

    

    曲面波方程已成为精确高效建模心脏电激活的不可或缺工具。通过匹配临床记录的曲面波电图和心电图（ECG），原则上可以以纯非侵入的方式构建患者特异性的心脏电生理模型。然而，拟合过程仍然是一个具有挑战性的任务。本研究介绍了一种新方法，称为测地线反向传播（Geodesic-BP），用于解决逆曲面波问题。Geodesic-BP非常适用于支持GPU加速的机器学习框架，可以优化曲面波方程的参数以重现给定的ECG。我们展示了Geodesic-BP在合成测试案例中，在建模不准确的情况下也能以高准确性重构模拟心脏激活。此外，我们将算法应用于一个公开可用的兔模型数据集，并取得了非常积极的结果。考虑到未来个体化医疗的发展，Geodesic-BP具有良好的前景。

    The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic-BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a rabbit model, with very positive results. Given the future shift towards personalized medicine, Geodesic-BP has t
    
[^25]: 可解释的人工智能在临床风险预测中的应用:概念、方法和方式的调查

    Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities. (arXiv:2308.08407v1 [cs.LG])

    [http://arxiv.org/abs/2308.08407](http://arxiv.org/abs/2308.08407)

    这篇综述论文讨论了可解释的人工智能在临床风险预测中的应用，包括概念、方法和方式。可解释性对于确保人们对AI系统的信任和可靠性至关重要，除了解释性之外，还涉及公平性、偏见、信任和透明度等方面。该综述还讨论了近期在临床风险预测中可解释模型的进展。

    

    最近人工智能在医疗保健领域的应用取得了令人难以置信的成果，在诊断和疾病预测方面超越了人类性能。然而，随着人工智能模型复杂性的增加，对其不透明性、潜在偏见以及可解释性的担忧也日益增加。为了确保人们对人工智能系统的信任和可靠性，尤其是在临床风险预测模型中，可解释性变得至关重要。可解释性通常指的是人工智能系统向人类利益相关者提供其决策逻辑或决策本身的稳健解释能力。在临床风险预测中，公平性、偏见、信任和透明度等其他方面的可解释性也代表了超越可解释性本身的重要概念。在本综述中，我们探讨了这些概念之间的关系，因为它们通常一起或可互换地使用。本综述还讨论了最近在临床风险预测的可解释模型的发展进展，

    Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highligh
    
[^26]: 基于内容的视频流媒体平台推荐引擎

    Content-based Recommendation Engine for Video Streaming Platform. (arXiv:2308.08406v1 [cs.IR])

    [http://arxiv.org/abs/2308.08406](http://arxiv.org/abs/2308.08406)

    本文提出了一种基于内容的推荐引擎，通过计算文档中单词的相关性和使用余弦相似度方法来推荐视频给用户。同时，还通过计算精确率、召回率和F1得分来评估引擎的性能。

    

    推荐引擎使用机器学习算法向用户建议内容、产品或服务。本文提出了一种基于内容的推荐引擎，根据用户之前的兴趣和选择，向用户提供视频推荐。我们将使用TF-IDF文本向量化方法来确定文档中单词的相关性。然后通过计算它们之间的余弦相似度，找出每个内容之间的相似性。最后，根据得到的相似度分数值，引擎将向用户推荐视频。另外，我们将通过计算精确率、召回率和F1得分来衡量引擎的性能。

    Recommendation engine suggest content, product or services to the user by using machine learning algorithm. This paper proposed a content-based recommendation engine for providing video suggestion to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the users based on the obtained similarity score value. In addition, we will measure the engine's performance by computing precision, recall, and F1 core of the proposed system.
    
[^27]: 使用神经网络实现对已用核燃料的快速不确定性量化

    Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks. (arXiv:2308.08391v1 [cs.LG])

    [http://arxiv.org/abs/2308.08391](http://arxiv.org/abs/2308.08391)

    本文提出了使用神经网络（NN）的代理模型方法，以降低计算成本来快速预测已用核燃料（SNF）的特性，包括衰变热和核素浓度。该模型的准确性通过验证和测试得到了证实。

    

    在核能生产、废物管理和核监督的安全、效率和可持续性方面，对已用核燃料（SNF）特性的准确计算和不确定性量化起着关键作用。虽然现有的基于物理模型是可靠的，但计算复杂且耗时。本文提出了一种使用神经网络（NN）的代理模型方法，以降低计算成本来预测一系列SNF特性。通过从CASMO5晶格计算生成的数据进行训练，该NN能够准确预测SNF的衰变热和核素浓度，并具有关键输入参数（如富集度、燃耗、循环之间的冷却时间、平均硼浓度和燃料温度）的函数关系。该模型经过基于物理模型的衰变热模拟验证，并与来自两种不同压水类型反应堆的不同铀氧化物燃料组件的测试结果进行了对比。

    The accurate calculation and uncertainty quantification of the characteristics of spent nuclear fuel (SNF) play a crucial role in ensuring the safety, efficiency, and sustainability of nuclear energy production, waste management, and nuclear safeguards. State of the art physics-based models, while reliable, are computationally intensive and time-consuming. This paper presents a surrogate modeling approach using neural networks (NN) to predict a number of SNF characteristics with reduced computational costs compared to physics-based models. An NN is trained using data generated from CASMO5 lattice calculations. The trained NN accurately predicts decay heat and nuclide concentrations of SNF, as a function of key input parameters, such as enrichment, burnup, cooling time between cycles, mean boron concentration and fuel temperature. The model is validated against physics-based decay heat simulations and measurements of different uranium oxide fuel assemblies from two different pressurized
    
[^28]: Continuous Sweep: 一种改进的二元量化器

    Continuous Sweep: an improved, binary quantifier. (arXiv:2308.08387v1 [stat.ML])

    [http://arxiv.org/abs/2308.08387](http://arxiv.org/abs/2308.08387)

    Continuous Sweep是一种改进的二元量化器，通过使用参数化类别分布、优化决策边界以及计算均值等方法，它在量化学习中取得了更好的性能。

    

    量化是一种监督式机器学习任务，其关注的是估计数据集中类别的普遍性，而不是标记其个体观测。我们引入了Continuous Sweep，这是一种新的参数化二元量化器，受到表现良好的Median Sweep的启发。Median Sweep目前是最好的二元量化器之一，但我们在三个方面改变了这个量化器，即1）使用参数化的类别分布而不是经验分布，2）优化决策边界而不是应用离散的决策规则，3）计算均值而不是中位数。在一般模型假设下，我们推导了Continuous Sweep的偏差和方差的解析表达式。这是量化学习领域中的首次理论贡献之一。此外，这些推导使我们能够找到最优的决策边界。最后，我们的模拟研究表明，在广泛的情况下，Continuous Sweep优于Median Sweep。

    Quantification is a supervised machine learning task, focused on estimating the class prevalence of a dataset rather than labeling its individual observations. We introduce Continuous Sweep, a new parametric binary quantifier inspired by the well-performing Median Sweep. Median Sweep is currently one of the best binary quantifiers, but we have changed this quantifier on three points, namely 1) using parametric class distributions instead of empirical distributions, 2) optimizing decision boundaries instead of applying discrete decision rules, and 3) calculating the mean instead of the median. We derive analytic expressions for the bias and variance of Continuous Sweep under general model assumptions. This is one of the first theoretical contributions in the field of quantification learning. Moreover, these derivations enable us to find the optimal decision boundaries. Finally, our simulation study shows that Continuous Sweep outperforms Median Sweep in a wide range of situations.
    
[^29]: 分类问题中的精确度和召回率拒绝曲线

    Precision and Recall Reject Curves for Classification. (arXiv:2308.08381v1 [cs.LG])

    [http://arxiv.org/abs/2308.08381](http://arxiv.org/abs/2308.08381)

    该论文提出了一种在分类问题中评估精确度和召回率的拒绝曲线方法。使用感知量化的原型分类器来验证了该方法在不平衡数据集和医学实际数据上的有效性。

    

    在某些分类场景中，只使用模型高度确定的分类实例是可取的。为了获得这些高度确定的实例，先前的研究提出了准确度拒绝曲线。拒绝曲线允许评估和比较不同的确定度度量在接受或拒绝分类的一系列阈值上的性能。然而，准确度可能并不适合所有应用程序的评估指标，相反，精确度或召回率可能更可取。这是因为在存在类别分布不平衡的数据中，例如，在不平衡的基准数据和医学实际数据中，我们提出了评估精确度和召回率的拒绝曲线：召回率-拒绝曲线和精确度-拒绝曲线。通过学习向量量化中的原型分类器，我们首先在人工基准数据上将提出的曲线与准确度拒绝曲线作为基准进行验证。然后，我们在存在类别分布不平衡的基准数据和医学实际数据上进行展示。

    For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data 
    
[^30]: 一种用于动态传感器选择的分布式神经网络架构，适用于带宽受限的身体传感器网络。

    A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks. (arXiv:2308.08379v1 [cs.LG])

    [http://arxiv.org/abs/2308.08379](http://arxiv.org/abs/2308.08379)

    提出了一种用于神经网络的动态传感器选择方法，并结合使用Gumbel-Softmax技巧通过标准反向传播来学习离散决策。该方法可以增加无线传感器网络的寿命，并提高任务-DNN处理多种可能节点子集的能力。

    

    我们提出了一种用于深度神经网络（DNN）的动态传感器选择方法，能够针对每个特定输入样本推导出最佳的传感器子集选择，而不是整个数据集的固定选择。这种动态选择与任务模型一起在端到端的方式中联合学习，使用Gumbel-Softmax技巧通过标准反向传播来学习离散决策。然后我们展示了如何利用这种动态选择通过限制每个节点的传输频率来增加无线传感器网络（WSN）的寿命。我们进一步通过包括动态空间滤波器来提高性能，使任务-DNN对于现在需要处理的多种可能节点子集更具鲁棒性。最后，我们解释了如何将最佳通道的选择分布到WSN中的不同节点上。我们在身体传感器网络的一个使用案例中验证了这种方法。

    We propose a dynamic sensor selection approach for deep neural networks (DNNs), which is able to derive an optimal sensor subset selection for each specific input sample instead of a fixed selection for the entire dataset. This dynamic selection is jointly learned with the task model in an end-to-end way, using the Gumbel-Softmax trick to allow the discrete decisions to be learned through standard backpropagation. We then show how we can use this dynamic selection to increase the lifetime of a wireless sensor network (WSN) by imposing constraints on how often each node is allowed to transmit. We further improve performance by including a dynamic spatial filter that makes the task-DNN more robust against the fact that it now needs to be able to handle a multitude of possible node subsets. Finally, we explain how the selection of the optimal channels can be distributed across the different nodes in a WSN. We validate this method on a use case in the context of body-sensor networks, where
    
[^31]: PDPK: 用于制造业的合成过程数据和对应程序化知识的框架

    PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing. (arXiv:2308.08371v1 [cs.AI])

    [http://arxiv.org/abs/2308.08371](http://arxiv.org/abs/2308.08371)

    该论文提出了一个框架，用于合成包含过程数据和对应程序化知识的数据集，并比较了不同嵌入方法的性能。

    

    程序化知识描述了如何完成任务和解决问题。这样的知识通常由领域专家持有，例如制造业中调整参数以达到质量目标的操作员。据我们所知，目前公开可用的包含过程数据和对应程序化知识的真实世界数据集目前还不存在，可能是因为企业对知识进展的损失存在担心。因此，我们提供了一个可以适应不同领域的生成合成数据集的框架。该框架的设计选择受到我们可以访问的两个实际的程序化知识数据集的启发。除了包含符合资源描述框架（RDF）标准的知识图形中的程序化知识的表示外，该框架还模拟参数化过程并提供一致的过程数据。我们比较了在生成的知识图形上的已建立的嵌入方法，详细说明了哪些开箱即用的方法具有重新解释知识潜力。

    Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to rep
    
[^32]: 长尾识别的双分支温度缩放校准方法

    Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition. (arXiv:2308.08366v1 [cs.LG])

    [http://arxiv.org/abs/2308.08366](http://arxiv.org/abs/2308.08366)

    这篇论文提出了一种针对长尾识别问题的双分支温度缩放校准方法（Dual-TS），通过考虑不同类别温度参数的不同性和非通用性，解决了样本稀有问题和训练集验证集温度系数不一致的挑战。

    

    当前深度神经网络的校准问题引起了广泛关注和研究。校准不准确通常会导致模型过于自信。在长尾数据分布的条件下，校准不准确的问题更为突出，因为少数类和多数类样本的置信水平不同，会导致更严重的过于自信。为了解决这个问题，一些当前研究基于温度缩放（TS）方法设计了不同类别的不同温度系数。然而，在少数类中存在稀有样本的情况下，温度系数不具有通用性，并且训练集和验证集之间的温度系数存在很大差异。为了解决这个挑战，本文提出了一种双分支温度缩放校准模型（Dual-TS），考虑到不同类别的温度参数的不同性和非通用性。

    The calibration for deep neural networks is currently receiving widespread attention and research. Miscalibration usually leads to overconfidence of the model. While, under the condition of long-tailed distribution of data, the problem of miscalibration is more prominent due to the different confidence levels of samples in minority and majority categories, and it will result in more serious overconfidence. To address this problem, some current research have designed diverse temperature coefficients for different categories based on temperature scaling (TS) method. However, in the case of rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set. To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which considers the diversities in temperature parameters of different categories and the non-genera
    
[^33]: KernelWarehouse：朝着参数有效的动态卷积迈进

    KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution. (arXiv:2308.08361v1 [cs.CV])

    [http://arxiv.org/abs/2308.08361](http://arxiv.org/abs/2308.08361)

    本文提出了KernelWarehouse，它是一种更通用的动态卷积形式，通过重新定义动态卷积中的卷积核的概念，实现了在参数效率和表示能力之间的有利平衡。

    

    动态卷积学习一种带有样本相关注意力权重的$n$个静态卷积核的线性组合，与普通卷积相比表现出优异性能。然而，现有设计在参数效率上存在问题：它们将卷积参数数量增加了$n$倍。这与优化困难导致了动态卷积领域的研究进展缓慢，无法使用大的$n$值（例如$n>100$而不是典型设置$n<10$）来推动性能边界。本文提出了KernelWarehouse，一种更通用的动态卷积形式，它在参数效率和表示能力之间取得了有利的平衡。其关键思想是从减少卷积核维度和显著增加卷积核数量的角度重新定义了动态卷积中的"$kernels$"和"$assembling$ $kernels$"的基本概念。原则上，KernelWarehouse增强了卷积参数的依赖关系。

    Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n>100$ instead of typical setting $n<10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of "$kernels$" and "$assembling$ $kernels$" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter depen
    
[^34]: 独立分布正则化用于私有图嵌入

    Independent Distribution Regularization for Private Graph Embedding. (arXiv:2308.08360v1 [cs.LG])

    [http://arxiv.org/abs/2308.08360](http://arxiv.org/abs/2308.08360)

    这篇论文提出了一种独立分布正则化的方法，用于保护私有图嵌入的隐私。通过考虑主要学习和隐私保护，该方法解决了现有方法在训练阶段需拥有所有敏感属性的限制，并解决了隐私保护表示学习中常见的对抗学习技术问题。

    

    学习图嵌入是图挖掘任务中的一个关键任务。一个有效的图嵌入模型可以从图结构化数据中学习到低维表示，从而为数据发布带来各种下游应用的好处，如节点分类、链路预测等。然而，最近的研究表明，图嵌入容易受到属性推断攻击的影响，攻击者可以从学习到的图嵌入中推断出私有节点属性。为了解决这些问题，出现了保护隐私的图嵌入方法，通过对抗学习同时考虑主要学习和隐私保护。然而，现有方法大多假设表示模型在训练阶段拥有所有敏感属性的访问权，但由于不同的隐私偏好，这并不总是成立。此外，隐私保护表示学习中常用的对抗学习技术存在问题。

    Learning graph embeddings is a crucial task in graph mining tasks. An effective graph embedding model can learn low-dimensional representations from graph-structured data for data publishing benefiting various downstream applications such as node classification, link prediction, etc. However, recent studies have revealed that graph embeddings are susceptible to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from 
    
[^35]: 两层非线性单元回归的收敛性研究

    Convergence of Two-Layer Regression with Nonlinear Units. (arXiv:2308.08358v1 [cs.LG])

    [http://arxiv.org/abs/2308.08358](http://arxiv.org/abs/2308.08358)

    本研究研究了两层非线性单元回归的收敛性，提出了一个softmax ReLU回归问题，并证明了Hessian的性质，引入了基于近似牛顿法的贪婪算法，最后证明了收敛性。

    

    大型语言模型（LLMs），如ChatGPT和GPT4，在许多人类生活任务中表现出色。注意力计算在训练LLMs中起着重要作用。Softmax单元和ReLU单元是注意力计算的关键结构。受到它们的启发，我们提出了一个softmax ReLU回归问题。总的来说，我们的目标是找到涉及ReLU单元的回归问题的最优解。在这项工作中，我们计算了损失函数的Hessian的闭合形式表示。在一定的假设下，我们证明了Hessian的Lipschitz连续性和PSD性质。然后，我们引入了基于近似牛顿法的贪婪算法，该算法在距离最优解的意义下收敛。最后，我们放宽了Lipschitz条件，并证明了在损失值的意义下的收敛性。

    Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.
    
[^36]: 元学习是否是解决推荐系统中冷启动问题的正确方法？

    Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?. (arXiv:2308.08354v1 [cs.IR])

    [http://arxiv.org/abs/2308.08354](http://arxiv.org/abs/2308.08354)

    本文研究表明，针对推荐系统中的冷启动问题，元学习技术在处理深度学习模型时已成为最受欢迎的方法。然而，当前的元学习方法在实际推荐系统中并不实用，因为这些系统拥有庞大的用户和物品数量，且有严格的延迟要求。

    

    推荐系统已经成为现代在线产品和服务的基础构建模块，并对用户体验产生了重大影响。在过去的几年中，深度学习方法吸引了大量的研究，并在现代实际推荐系统中得到广泛应用。然而，处理冷启动设置下的推荐问题，例如当用户在系统中的互动有限时，仍然是一个远未解决的问题。元学习技术，尤其是基于优化的元学习，最近已成为学术研究文献中处理推荐系统中冷启动问题的最流行方法。然而，目前的元学习方法对于拥有数十亿用户和物品以及严格的延迟要求的现实推荐系统来说并不实用。在本文中，我们展示了在常用基准上获得类似或更高性能是可能的。

    Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchma
    
[^37]: 使用可控数据增强实现图的带外分布泛化

    Graph Out-of-Distribution Generalization with Controllable Data Augmentation. (arXiv:2308.08344v1 [cs.LG])

    [http://arxiv.org/abs/2308.08344](http://arxiv.org/abs/2308.08344)

    本论文提出了一种名为“OOD-GMixup”的方法，利用可控数据增强来解决图的带外分布泛化问题。该方法通过提取图合理性和生成虚拟样本的方式来消除虚假相关性和稳定性问题。

    

    图神经网络（GNN）在图属性分类方面表现出非凡的性能。然而，由于训练和测试数据的选择偏差（例如，在小图上进行训练，在大图上进行测试，或在稠密图上进行训练，在稀疏图上进行测试），分布偏差很普遍。更重要的是，我们经常观察到尽管有单边偏向的数据分区，但却存在着同时具有规模和密度的混合结构分布偏移。混合分布偏移中的伪相关性降低了先前GNN方法的性能，并且在不同数据集之间显示出较大的不稳定性。为了缓解这个问题，我们提出了“OOD-GMixup”在度量空间中以联合操作训练分布的可控数据增强。具体来说，我们首先提取图合理性来消除由于不相关信息而引起的虚假相关性。其次，我们对图合理性进行扰动生成虚拟样本

    Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale r
    
[^38]: 通过发现高阶抽象来学习逻辑程序

    Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])

    [http://arxiv.org/abs/2308.08334](http://arxiv.org/abs/2308.08334)

    本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。

    

    发现新颖的抽象对于人类级别的人工智能至关重要。我们介绍了一种发现高阶抽象（例如map、filter和fold）的方法。我们专注于归纳逻辑编程，即从示例和背景知识中归纳逻辑程序。我们引入了高阶重构问题，目标是通过引入高阶抽象来压缩逻辑程序。我们将我们的方法实现在STEVIE中，它将高阶重构问题建模为约束优化问题。我们在多个领域，包括程序合成和视觉推理，的实验结果表明，与没有重构相比，STEVIE可以提高预测精度27%并将学习时间减少47%。我们还展示了STEVIE可以发现适用于不同领域的抽象。

    Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
    
[^39]: 在欧几里德函数优化中的扭曲几何信息

    Warped geometric information on the optimisation of Euclidean functions. (arXiv:2308.08305v1 [stat.ML])

    [http://arxiv.org/abs/2308.08305](http://arxiv.org/abs/2308.08305)

    使用扭曲几何学的概念，我们提出了一种在高维欧几里德空间中优化函数的方法，并通过在重新定义的黎曼流形上进行计算，找到了函数的最优解。

    

    我们考虑了在潜在高维欧几里德空间中优化实值函数的基本任务，例如许多机器学习任务中的损失函数或统计推断中的概率分布的对数。我们使用扭曲黎曼几何概念，将欧几里德空间上的函数优化问题重新定义为一个带有扭曲度量的黎曼流形，并在该流形上找到函数的最优解。选择用于搜索域的扭曲度量引入了一个计算友好的度量张量，使得在流形上找到最优搜索方向与测地线变得更容易计算。沿测地线进行优化通常是不可行的，但我们表明在这个特定的流形中，我们可以解析地得到高达三阶的泰勒近似。一般情况下，这些对测地线的近似不会位于流形上，但我们构造了合适的回缩方程将这些近似重新映射到流形上。

    We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction m
    
[^40]: 鲁棒贝叶斯满足。

    Robust Bayesian Satisficing. (arXiv:2308.08291v1 [cs.LG])

    [http://arxiv.org/abs/2308.08291](http://arxiv.org/abs/2308.08291)

    本文提出了一种名为RoBOS的鲁棒贝叶斯满足算法，用于解决在上下文贝叶斯优化中存在分布偏移时的问题。该算法能够在一定的分布偏移量下保证亏得不严重的子线性遗憾，并且具有与分布偏移量无关的较弱遗憾边界。

    

    分布偏移对于实现当代机器学习的鲁棒性构成了重大挑战。为了克服这一挑战，鲁棒满足（RS）在实现超过期望阈值的效用的同时，寻求对于未指定的分布偏移的鲁棒解决方案。本文关注在上下文贝叶斯优化中存在真实和参考分布之间的差异时的鲁棒满足（RS）问题。我们提出了一种名为RoBOS的新型噪声黑箱优化的鲁棒贝叶斯满足算法。在某些关于分布偏移量的假设下，我们的算法保证亏得不严重的子线性遗憾。此外，我们定义了一种较弱的遗憾概念，称为鲁棒满足遗憾，其中我们的算法实现了与分布偏移量无关的子线性上界。为了展示我们方法的有效性，我们将其应用于各种学习问题，并与其他方法进行比较，如分布交换方法。

    Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally rob
    
[^41]: DFedADMM：用于分散式联邦学习的双重约束控制模型不一致性

    DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning. (arXiv:2308.08290v1 [cs.LG])

    [http://arxiv.org/abs/2308.08290](http://arxiv.org/abs/2308.08290)

    我们提出了一种新的分散式联邦学习算法DFedADMM和其改进版本DFedADMM-SAM，用于解决局部不一致性和局部异构过拟合的问题。

    

    为了解决联邦学习中的通信负担问题，分散式联邦学习（DFL）舍弃中央服务器，建立分散式通信网络，每个客户端仅与相邻客户端通信。然而，现有的DFL方法仍然面临两个主要挑战：局部不一致性和局部异构过拟合，这些问题尚未从根本上得到解决。为了解决这些问题，我们提出了新的DFL算法DFedADMM及其改进版本DFedADMM-SAM，以提高DFL的性能。DFedADMM算法通过利用双变量来控制由分散的异构数据分布导致的模型不一致性来使用原始-对偶优化（ADMM）。DFedADMM-SAM算法通过使用梯度扰动来生成局部平坦模型并寻找模型来进一步改进DFedADMM。

    To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models
    
[^42]: CARE: 一种用于直肠癌分割的大规模CT图像数据集和临床适用的基准模型

    CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation. (arXiv:2308.08283v1 [eess.IV])

    [http://arxiv.org/abs/2308.08283](http://arxiv.org/abs/2308.08283)

    CARE是一种提供了大规模直肠癌CT图像数据集和临床适用的基准模型的研究。这个数据集提供了正常和癌性直肠的像素级注释，为算法研究和临床应用开发提供了宝贵的资源。

    

    CT图像的直肠癌分割在及时临床诊断、放疗治疗和随访中起着至关重要的作用。尽管目前的分割方法在描绘癌组织方面显示出了一定的潜力，但仍然在达到高分割精度方面面临挑战。这些难题源于直肠复杂的解剖结构以及进行直肠癌鉴别诊断的困难。此外，一个主要障碍是缺乏用于直肠癌分割的大规模、精细标注的CT图像数据集。为了解决这些问题，本研究介绍了一种新型的大规模直肠癌CT图像数据集CARE，并提供了正常和癌性直肠的像素级注释，这为算法研究和临床应用开发提供了宝贵的资源。此外，我们提出了一种名为U-SAM的新型医学癌症病变分割基准模型。该模型专门设计用于应对所面临的挑战。

    Rectal cancer segmentation of CT image plays a crucial role in timely clinical diagnosis, radiotherapy treatment, and follow-up. Although current segmentation methods have shown promise in delineating cancerous tissues, they still encounter challenges in achieving high segmentation precision. These obstacles arise from the intricate anatomical structures of the rectum and the difficulties in performing differential diagnosis of rectal cancer. Additionally, a major obstacle is the lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation. To address these issues, this work introduces a novel large scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed b
    
[^43]: 它其实不那么糟糕：理解生成变换模型对OOD泛化的神秘性能下降

    It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])

    [http://arxiv.org/abs/2308.08268](http://arxiv.org/abs/2308.08268)

    生成变换模型在OOD泛化方面存在神秘的性能下降。研究人员观察到模型在训练和泛化数字运算时的行为之间的不一致，并尝试提出解决方案，但仍未解决本质机制。

    

    生成变换模型在解决各种问题上取得了显著的成就。然而，它们的泛化能力并没有完全被理解，并且并不总是令人满意。研究人员从基本的数学任务（如n位数的加法或乘法）开始，作为重要视角来研究模型的泛化行为。有趣的是，观察到当模型在n位数运算（例如加法）上进行训练时，模型在未见过的长度为n位的输入上可以成功泛化（即内分布泛化），但在长度更长、未见过的情况下（即外分布泛化）会失败并且表现神秘。研究尝试通过修改位置嵌入、微调和引入更广泛或更有指导性的数据来弥合这一差距。然而，如果不解决本质机制，这些解决方案的稳健性几乎没有任何保证。

    Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
    
[^44]: 图关系感知的连续学习

    Graph Relation Aware Continual Learning. (arXiv:2308.08259v1 [cs.LG])

    [http://arxiv.org/abs/2308.08259](http://arxiv.org/abs/2308.08259)

    连续图学习中的两个挑战是如何处理图的内在属性以及如何在不增加模型复杂度的情况下传输跨图的信息。本文提出了一个新的模型来解决这个问题。

    

    连续图学习（CGL）研究了如何从无限的图数据流中学习， consolidainge和将历史知识推广到未来的任务。与此同时，只有当前的图数据可用。虽然最近有一些尝试来处理这个任务，但我们仍然面临两个潜在的挑战：1）大多数现有的作品只在中间图嵌入上操作，忽略了图的内在属性。跨图传输信息是非平凡的。2）最近的尝试采用参数共享策略，在时间步上传输知识，或者根据转移的图分布逐步扩展新的架构。学习单一模型可能会丢失每个图任务的有区别信息，而模型扩展方案则面临模型复杂度高的问题。在本文中，我们指出图边缘后面的潜在关系可以归因于发展中图的一个不变因素，并提出了一个新的模型来解决这个问题。

    Continual graph learning (CGL) studies the problem of learning from an infinite stream of graph data, consolidating historical knowledge, and generalizing it to the future task. At once, only current graph data are available. Although some recent attempts have been made to handle this task, we still face two potential challenges: 1) most of existing works only manipulate on the intermediate graph embedding and ignore intrinsic properties of graphs. It is non-trivial to differentiate the transferred information across graphs. 2) recent attempts take a parameter-sharing policy to transfer knowledge across time steps or progressively expand new architecture given shifted graph distribution. Learning a single model could loss discriminative information for each graph task while the model expansion scheme suffers from high model complexity. In this paper, we point out that latent relations behind graph edges can be attributed as an invariant factor for the evolving graphs and the statistica
    
[^45]: 最近邻分类器的两个阶段的缩放律

    Two Phases of Scaling Laws for Nearest Neighbor Classifiers. (arXiv:2308.08247v1 [stat.ML])

    [http://arxiv.org/abs/2308.08247](http://arxiv.org/abs/2308.08247)

    最近邻分类器的缩放律可分为两个阶段：第一阶段中，泛化误差多项式地依赖于数据维度并迅速减小；第二阶段中，误差指数地依赖于数据维度并缓慢减小。这表明最近邻分类器在数据分布良好时可以实现泛化误差多项式地依赖于数据维度，而不是指数地依赖于数据维度。

    

    缩放律是指当训练数据数量增加时，模型的测试性能会提高的观察结果。快速的缩放律意味着通过增加数据和模型大小就能解决机器学习问题。然而，在许多情况下，增加更多数据的好处可能是微不足道的。在本研究中，我们研究了最近邻分类器的缩放律。我们发现缩放律可能有两个阶段：在第一阶段，泛化误差多项式地依赖于数据维度并且快速减小；而在第二阶段，误差指数地依赖于数据维度并且减小得慢。我们的分析突显了数据分布在决定泛化误差中的复杂性。当数据分布良好时，我们的结果表明最近邻分类器可以实现泛化误差多项式地依赖于数据维度，而不是指数地依赖于数据维度。

    A scaling law refers to the observation that the test performance of a model improves as the number of training data increases. A fast scaling law implies that one can solve machine learning problems by simply boosting the data and the model sizes. Yet, in many cases, the benefit of adding more data can be negligible. In this work, we study the rate of scaling laws of nearest neighbor classifiers. We show that a scaling law can have two phases: in the first phase, the generalization error depends polynomially on the data dimension and decreases fast; whereas in the second phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension.
    
[^46]: 图神经网络的表达能力：一项综述研究

    The Expressive Power of Graph Neural Networks: A Survey. (arXiv:2308.08235v1 [cs.LG])

    [http://arxiv.org/abs/2308.08235](http://arxiv.org/abs/2308.08235)

    本综述调查了图神经网络（GNNs）在不同形式定义下增强表达能力的模型，并对图特征增强、图拓扑增强和GNNs架构增强进行了综述和讨论。

    

    图神经网络（GNNs）是许多与图相关的应用中有效的机器学习模型。尽管在实践中取得了成功，但许多研究工作集中在GNNs的理论限制，即其表达能力。早期的研究主要关注GNNs的图同构识别能力，而最近的研究尝试利用子图计数和连接学习等属性来描述GNNs的表达能力，这更加实际且更接近实际应用。然而，目前还没有关于此方向的综述论文和开源仓库综合总结和讨论这些模型。为了填补这个空白，我们首次对不同形式定义下增强表达能力的模型进行了综述。具体而言，基于三个类别对模型进行了评述，即图特征增强、图拓扑增强和GNNs架构增强。

    Graph neural networks (GNNs) are effective machine learning models for many graph-related applications. Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., Graph feature enhancement, Graph topology enhancement, and GNNs architecture enhancement.
    
[^47]: 在自然语言处理中使用基于Transformer的多任务学习的挑战和机会：一项调研

    Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])

    [http://arxiv.org/abs/2308.08234](http://arxiv.org/abs/2308.08234)

    本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。

    

    自然语言处理（NLP）模型在各个行业中的广泛应用导致从训练到在生产中运行这些模型的机器学习系统需要有效处理。然而，使用基于Transformer的预训练语言模型进行训练、部署和更新多个模型可能复杂、昂贵且耗时，特别是使用多任务学习（MTL）作为改进效率和性能的方法。本调研首先概述了NLP中基于Transformer的MTL方法。然后，我们讨论了在典型的机器学习生命周期中使用MTL方法面临的挑战和机会，重点关注数据工程、模型开发、部署和监控阶段的挑战。本项调研集中于基于Transformer的MTL架构，并据我们所知是首创的。

    The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it 
    
[^48]: SCQPTH:用于凸二次规划的高效可微分分裂方法

    SCQPTH: an efficient differentiable splitting method for convex quadratic programming. (arXiv:2308.08232v1 [math.OC])

    [http://arxiv.org/abs/2308.08232](http://arxiv.org/abs/2308.08232)

    SCQPTH是一种用于凸二次规划的高效可微分的分裂算法，通过交替方向乘法器方法（ADMM）和操作拆分求解器实现。它在大规模问题上具有很高的计算效率提升。

    

    我们提出了SCQPTH：一种用于凸二次规划的可微分一阶分裂方法。SCQPTH框架基于交替方向乘法器方法（ADMM），软件实现受到了最先进的凸二次规划求解器OSQP的启发，该求解器是一种用于凸二次规划的操作拆分求解器。SCQPTH软件作为开源Python包提供，并包含许多类似的功能，包括有效重用矩阵分解、不可行性检测、自动缩放和参数选择。前向传播算法在原始问题空间的维度上进行算子分裂，因此适用于具有100-1000个决策变量和成千上万个约束条件的大规模凸二次规划问题。通过对ADMM固定点映射进行隐式微分来执行反向传播。实验证明，对于大规模凸二次规划问题，SCQPTH在计算效率方面可以提供1倍到10倍的提升。

    We present SCQPTH: a differentiable first-order splitting method for convex quadratic programs. The SCQPTH framework is based on the alternating direction method of multipliers (ADMM) and the software implementation is motivated by the state-of-the art solver OSQP: an operating splitting solver for convex quadratic programs (QPs). The SCQPTH software is made available as an open-source python package and contains many similar features including efficient reuse of matrix factorizations, infeasibility detection, automatic scaling and parameter selection. The forward pass algorithm performs operator splitting in the dimension of the original problem space and is therefore suitable for large scale QPs with $100-1000$ decision variables and thousands of constraints. Backpropagation is performed by implicit differentiation of the ADMM fixed-point mapping. Experiments demonstrate that for large scale QPs, SCQPTH can provide a $1\times - 10\times$ improvement in computational efficiency in com
    
[^49]: 探索Winograd卷积用于成本效益的神经网络容错性

    Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance. (arXiv:2308.08230v1 [cs.LG])

    [http://arxiv.org/abs/2308.08230](http://arxiv.org/abs/2308.08230)

    本论文探索了Winograd卷积在神经网络容错性方面的潜力，并评估了从不同粒度（模型、层、操作类型）进行的综合容错评估。研究发现Winograd卷积能够降低容错设计开销，并与经典的容错设计方法有效结合，实现对软错误的成本效益的神经网络保护。

    

    Winograd通常用于优化卷积性能和计算效率，因为它减少了乘法运算，但通常忽略了Winograd带来的可靠性问题。在这项工作中，我们观察到Winograd卷积在提高神经网络容错性方面具有巨大潜力。基于这一观察，我们首次全面评估了Winograd卷积容错性，从模型、层和操作类型等不同粒度进行评估。然后，我们探索了Winograd卷积的内在容错性如何与经典的容错设计方法（包括三重模块冗余、容错重训练和约束激活函数）有效结合，实现对软错误的成本效益的神经网络保护。根据我们的实验，Winograd卷积可以降低容错设计开销。

    Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead b
    
[^50]: 如何通过主动学习克服半监督图像分类中的确认偏差

    How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning. (arXiv:2308.08224v1 [cs.LG])

    [http://arxiv.org/abs/2308.08224](http://arxiv.org/abs/2308.08224)

    本论文研究了半监督学习中的确认偏差问题，并提出了三个现实数据场景中的挑战，分别是类间不平衡、类内不平衡和类间相似性。通过在模拟数据中进行实验，发现随机采样不能解决这些挑战。

    

    我们是否需要主动学习？强大的深度半监督方法的崛起对有限标记数据设置中主动学习的可用性提出了疑问。结果表明，将半监督学习方法与随机选择进行标记相结合可以胜过现有的主动学习技术。然而，这些结果来自于在公认的基准数据集上进行的实验，可能高估了外部有效性。然而，文献中对主动半监督学习方法在实际数据场景中的性能缺乏足够的研究，这在我们的理解中存在明显的空白。因此，我们提出了三个在现实世界应用中常见的数据挑战：类间不平衡、类内不平衡和类间相似性。这些挑战可能由于确认偏差而损害半监督学习的性能。我们在模拟数据挑战上进行了半监督学习和主动学习的实验，并发现随机采样不能解决这些挑战。

    Do we need active learning? The rise of strong deep semi-supervised methods raises doubt about the usability of active learning in limited labeled data settings. This is caused by results showing that combining semi-supervised learning (SSL) methods with a random selection for labeling can outperform existing active learning (AL) techniques. However, these results are obtained from experiments on well-established benchmark datasets that can overestimate the external validity. However, the literature lacks sufficient research on the performance of active semi-supervised learning methods in realistic data scenarios, leaving a notable gap in our understanding. Therefore we present three data challenges common in real-world applications: between-class imbalance, within-class imbalance, and between-class similarity. These challenges can hurt SSL performance due to confirmation bias. We conduct experiments with SSL and AL on simulated data challenges and find that random sampling does not mi
    
[^51]: HyperSNN：一种适用于资源受限控制应用的高效稳健深度学习模型

    HyperSNN: A new efficient and robust deep learning model for resource constrained control applications. (arXiv:2308.08222v1 [cs.RO])

    [http://arxiv.org/abs/2308.08222](http://arxiv.org/abs/2308.08222)

    HyperSNN是一种适用于资源受限控制应用的高效稳健深度学习模型，通过使用脉冲神经网络和高维计算，将能量消耗降低至1.36%-9.96%，同时提高了鲁棒性和准确性。它适用于交互式、移动和可穿戴设备，促进了能量高效和稳健的系统设计。

    

    鉴于边缘计算在智能家具、机器人和智能家居等领域的日益采用，本文介绍了一种创新的控制任务方法HyperSNN，它使用脉冲神经网络（SNNs）与高维计算相结合。 HyperSNN用8位整数加法替代了昂贵的32位浮点乘法，从而降低了能量消耗，同时增强了稳健性和可能提高准确性。我们的模型在AI Gym基准测试中进行了测试，包括Cartpole、Acrobot、MountainCar和Lunar Lander。 HyperSNN实现了与传统机器学习方法相当的控制准确性，但仅消耗1.36％至9.96％的能量开销。此外，我们的实验显示，使用HyperSNN可以提高鲁棒性。我们认为HyperSNN特别适用于交互式、移动和可穿戴设备，促进了能量高效和稳健的系统设计。而且，它为.....

    In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for th
    
[^52]: Epicure: 将序列模型预测转化成模式

    Epicure: Distilling Sequence Model Predictions into Patterns. (arXiv:2308.08203v1 [cs.LG])

    [http://arxiv.org/abs/2308.08203](http://arxiv.org/abs/2308.08203)

    Epicure是一种将序列模型的预测转化为简单模式的方法，能够准确预测高熵序列分布中的名称，并且在预测函数名称和检测异常名称的任务中相较于最高概率模型预测更准确匹配实际情况。

    

    大多数机器学习模型预测的是关于具体输出的概率分布，并且很难准确预测高熵序列分布中的名称。在这篇论文中，我们探索了寻找这些预测中的抽象、高精度模式，以便进行有用地捕捉罕见序列的抽象预测。在这篇简短的论文中，我们提出了Epicure，一种将序列模型的预测（例如beam search的输出）转化为简单模式的方法。Epicure将模型的预测映射到一个表示越来越一般模式的格点上，这些模式包含了具体模型预测的内容。在预测给定函数的源代码时选择描述性名称和检测给定函数中异常名称的任务上，我们展示了Epicure相较于仅使用最高概率模型预测更准确地命名模式与实际情况相匹配。在误报率为10%的情况下，Epicure预测的模式与实际情况匹配。

    Most machine learning models predict a probability distribution over concrete outputs and struggle to accurately predict names over high entropy sequence distributions. Here, we explore finding abstract, high-precision patterns intrinsic to these predictions in order to make abstract predictions that usefully capture rare sequences. In this short paper, we present Epicure, a method that distils the predictions of a sequence model, such as the output of beam search, into simple patterns. Epicure maps a model's predictions into a lattice that represents increasingly more general patterns that subsume the concrete model predictions.  On the tasks of predicting a descriptive name of a function given the source code of its body and detecting anomalous names given a function, we show that Epicure yields accurate naming patterns that match the ground truth more often compared to just the highest probability model prediction. For a false alarm rate of 10%, Epicure predicts patterns that match 
    
[^53]: DeSCo:面向通用且可扩展的深度子图计数方法

    DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting. (arXiv:2308.08198v1 [cs.LG])

    [http://arxiv.org/abs/2308.08198](http://arxiv.org/abs/2308.08198)

    DeSCo是一个通用且可扩展的深度子图计数方法，它解决了现有神经计数方法在计数准确性、图形区分和出现位置预测方面存在的问题。

    

    子图计数是在大型目标图中计算给定查询图的出现次数的问题。大规模的子图计数在各个领域中都很有用，比如社交网络分析中的图案计数和交易网络上的反洗钱检测中的循环计数。最近，为了解决可扩展子图计数的指数级运行时间复杂度，提出了神经方法。然而，现有的神经计数方法在三个方面存在不足。首先，相同查询的计数在不同目标图上可以从零到数百万，比大多数图回归任务都要困难得多。其次，目前的可扩展图神经网络具有有限的表达能力，无法高效地区分计数预测中的图形。此外，现有的神经方法无法预测查询在目标图中的出现位置。在这里，我们设计了DeSCo，这是一个可扩展的神经深度子图计数流水线，旨在准确地预测子图的计数和出现位置。

    Subgraph counting is the problem of counting the occurrences of a given query graph in a large target graph. Large-scale subgraph counting is useful in various domains, such as motif counting for social network analysis and loop counting for money laundering detection on transaction networks. Recently, to address the exponential runtime complexity of scalable subgraph counting, neural methods are proposed. However, existing neural counting approaches fall short in three aspects. Firstly, the counts of the same query can vary from zero to millions on different target graphs, posing a much larger challenge than most graph regression tasks. Secondly, current scalable graph neural networks have limited expressive power and fail to efficiently distinguish graphs in count prediction. Furthermore, existing neural approaches cannot predict the occurrence position of queries in the target graph.  Here we design DeSCo, a scalable neural deep subgraph counting pipeline, which aims to accurately p
    
[^54]: 算法补救中的内生宏观动力学

    Endogenous Macrodynamics in Algorithmic Recourse. (arXiv:2308.08187v1 [cs.LG])

    [http://arxiv.org/abs/2308.08187](http://arxiv.org/abs/2308.08187)

    这项研究填补了算法补救中的内生动力学和对策影响其他个体的研究空白，提出了一个广义框架，并揭示了对策的隐藏成本。通过模拟实验，验证了该方法的效果。

    

    现有的关于对策解释和算法补救的研究主要集中在静态环境中的单个个体上：在给定一些估计模型的情况下，目标是找到满足不同需求的单个实例的有效对策解释。这些对策解释的能力处理数据和模型漂移等动态问题仍然是一个较少研究的挑战。与此相关的另一个问题是一个个体对策实施的实际影响其他个体的问题，关于这方面的研究相当少。通过这项工作，我们旨在填补这一空白。首先，我们展示现有的许多方法可以被统一描述为一个广义框架。然后，我们认为现有框架没有考虑到一个隐藏的对策成本，在研究群体层面上的内生动力学时才会显现出来。通过涉及各种最先进的对策解释方法的仿真实验...

    Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual
    
[^55]: 通过架构、编译器和分区方法的协同设计加速通用图神经网络

    Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design. (arXiv:2308.08174v1 [cs.AR])

    [http://arxiv.org/abs/2308.08174](http://arxiv.org/abs/2308.08174)

    该论文提出了一种加速通用图神经网络的方法，通过协同设计架构、编译器和分区方法来解决GNN模型的高带宽需求和多样性的挑战。

    

    图神经网络（GNN）在各种图学习领域中显示出了显著的准确性改进，引发了相当大的研究兴趣。为了将这些准确性改进转化为实际应用，开发高性能和高效的硬件加速器对于GNN模型至关重要。然而，设计GNN加速器面临两个基本挑战：GNN模型的高带宽需求和GNN模型的多样性。先前的工作通过使用更昂贵的内存接口来实现更高的带宽来解决第一个挑战。而对于第二个挑战，现有的工作要么支持特定的GNN模型，要么具有硬件利用率低的通用设计。在这项工作中，我们同时解决了这两个挑战。首先，我们确定了一种新型的分区级运算符融合方法，利用它在内部减少GNN的高带宽需求。然后，我们引入了分区级多线程来调度并发操作。

    Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.  In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent
    
[^56]: 通过对抗鲁棒性研究，揭示图神经网络的表达能力

    Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness. (arXiv:2308.08173v1 [cs.LG])

    [http://arxiv.org/abs/2308.08173](http://arxiv.org/abs/2308.08173)

    通过对抗鲁棒性研究，揭示了图神经网络的表达能力与传统消息传递神经网络之间的显著差距，并证明了更强大的GNNs无法泛化到小扰动的图结构和分布不一样的图。

    

    我们首次对图神经网络（GNNs）进行了对抗鲁棒性研究，证明它们在表达能力上比传统的消息传递神经网络（MPNNs）更强大。具体而言，我们使用对抗鲁棒性作为一种工具来揭示它们在理论上可能和经验上实际达到的表达能力之间的显著差距。为此，我们关注GNNs计数特定的子图模式的能力，这是一种已建立的表达能力度量，将对抗鲁棒性的概念扩展到这个任务上。基于此，我们开发了高效的对抗攻击方法来进行子图计数，并展示更强大的GNNs即使在对图结构的小扰动下也无法泛化。在此基础上，我们还表明这样的架构在处理分布不一样的图时也无法计数子结构。

    We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
    
[^57]: AATCT-IDS: 一款用于图像去噪、语义分割和放射学评估的腹部脂肪CT图像数据集的基准

    AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation. (arXiv:2308.08172v1 [eess.IV])

    [http://arxiv.org/abs/2308.08172](http://arxiv.org/abs/2308.08172)

    这项研究准备并发布了一款名为AATCT-IDS的基准腹部脂肪CT图像数据集，用于图像去噪、语义分割和放射学评估。通过对数据集中注释的脂肪组织区域进行研究，揭示了在不同任务中各种方法的性能差异，并验证了该数据集在这些任务中的研究潜力。

    

    本研究准备并发布了一款名为AATCT-IDS的基准腹部脂肪CT图像数据集，其中包含300个受试者。AATCT-IDS公开了13,732个原始CT切片，并且研究人员对其中3,213个具有相同切片距离的切片分别进行了皮下和腹腔内脂肪组织区域的注释，以验证去噪方法，训练语义分割模型，并进行放射学研究。本文通过结合可视化结果和评估数据，比较和分析了各种方法在AATCT-IDS上的性能，从而验证了该数据集在上述三种类型的任务中的研究潜力。

    Methods: In this study, a benchmark \emph{Abdominal Adipose Tissue CT Image Dataset} (AATTCT-IDS) containing 300 subjects is prepared and published. AATTCT-IDS publics 13,732 raw CT slices, and the researchers individually annotate the subcutaneous and visceral adipose tissue regions of 3,213 of those slices that have the same slice distance to validate denoising methods, train semantic segmentation models, and study radiomics. For different tasks, this paper compares and analyzes the performance of various methods on AATTCT-IDS by combining the visualization results and evaluation data. Thus, verify the research potential of this data set in the above three types of tasks.  Results: In the comparative study of image denoising, algorithms using a smoothing strategy suppress mixed noise at the expense of image details and obtain better evaluation data. Methods such as BM3D preserve the original image structure better, although the evaluation data are slightly lower. The results show sig
    
[^58]: 一个用于k-Means的量子逼近方案

    A Quantum Approximation Scheme for k-Means. (arXiv:2308.08167v1 [quant-ph])

    [http://arxiv.org/abs/2308.08167](http://arxiv.org/abs/2308.08167)

    这个论文提出了一个量子逼近方案，用于解决经典的k-Means聚类问题，该方案的运行时间与数据点的数量具有多对数依赖关系，并且能够在高概率下输出一个近似最优解，这是第一个具有多对数运行时间的量子算法，并且能够提供一个可证明的逼近保证。

    

    我们在QRAM模型中提供了一个量子逼近方案（即对于任意ε > 0, 都是 (1 + ε)-逼近），用于经典的k-Means聚类问题，其运行时间仅与数据点的数量具有多对数依赖关系。具体而言，给定一个在QRAM数据结构中存储的具有N个点的数据集V，这个量子算法的运行时间为Õ(2^(Õ(k/ε))η^2d)，并且以高概率输出一个包含k个中心的集合C，满足cost(V, C) ≤ (1+ε) · cost(V, C_OPT)。这里C_OPT表示最优的k个中心，cost(.)表示标准的k-Means代价函数（即点到最近中心的平方距离之和），而η是纵横比（即最远距离与最近距离的比值）。这是第一个具有多对数运行时间的量子算法，并且能够提供一个可证明的(1+ε)逼近保证。

    We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon > 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varep
    
[^59]: 由核生长神经气体生成的网络的特性

    Characteristics of networks generated by kernel growing neural gas. (arXiv:2308.08163v1 [cs.LG])

    [http://arxiv.org/abs/2308.08163](http://arxiv.org/abs/2308.08163)

    本研究开发了核化的生长神经气体算法，并研究了由此算法生成的网络的特性。研究发现，核生长神经气体算法可以将数据集转化为无向图，并提取数据集的特征作为图形。五种核函数被用于此研究。

    

    本研究旨在开发核化的生长神经气体（GNG）算法的核GNG，并研究由核GNG生成的网络的特征。GNG是一种无监督的人工神经网络，可以将数据集转化为无向图，从而提取数据集的特征作为图形。GNG被广泛应用于向量量化、聚类和三维图形中。核方法常用于将数据集映射到特征空间，其中支持向量机是最重要的应用之一。本文介绍了核GNG方法，并探索了由核GNG生成的网络的特性。本研究中使用了五种核函数，包括高斯核、拉普拉斯核、柯西核、反多项式核和对数核。

    This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
    
[^60]: 用于评估原型部件解释的空间不一致性的解释性基准

    Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations. (arXiv:2308.08162v1 [cs.CV])

    [http://arxiv.org/abs/2308.08162](http://arxiv.org/abs/2308.08162)

    本论文提出了一个用于评估原型部件解释的空间不一致性的解释性基准，并介绍了一种补偿不一致性的方法。通过大量的实证研究，表明了基准的表达能力和补偿方法的有效性。

    

    由于其忠实的自解释性，原型部件网络越来越受欢迎。然而，它们的相似性图计算在倒数第二层网络中。因此，原型激活区域的感受野通常依赖于图像区域外的部分，这可能导致误导性解释。我们将这种不希望的行为称为空间解释不一致，并引入了一个解释性基准，提供一套专用指标来量化这一现象。此外，我们提出了一种补偿不一致性的方法，并将其应用于现有的最先进模型。通过大量的实证研究，我们展示了我们基准的表达能力以及提出的补偿方法的有效性。

    Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
    
[^61]: 压缩深度学习模型的对抗鲁棒性基准测试

    Benchmarking Adversarial Robustness of Compressed Deep Learning Models. (arXiv:2308.08160v1 [cs.LG])

    [http://arxiv.org/abs/2308.08160](http://arxiv.org/abs/2308.08160)

    本研究通过开发一个全面的基准测试，证明了在深度学习模型压缩过程中，裁剪模型仍能保持基础模型的对抗鲁棒性水平。

    

    随着深度神经网络(DNNs)规模的增长，对模型压缩的需求日益迫切，特别是在资源受限设备上使用时。同时，DNNs对对抗攻击的敏感性也是一个重要障碍。尽管对于模型压缩和对抗鲁棒性的研究已经很充分，但它们的联合研究仍未得到充分探讨。我们的研究填补了这一空白，旨在了解针对基础模型制作的对抗输入对其裁剪版本的影响。为了研究这种关系，我们开发了一个全面的基准测试，涵盖了不同的对抗攻击和流行的DNN模型。我们独特地关注了之前未经过对抗训练的模型，并应用了针对准确性和性能优化的裁剪方案。我们的研究结果表明，虽然裁剪带来了增强的泛化能力、压缩效果和更快的推理时间，但对抗鲁棒性与基础模型相当。

    The increasing size of Deep Neural Networks (DNNs) poses a pressing need for model compression, particularly when employed on resource constrained devices. Concurrently, the susceptibility of DNNs to adversarial attacks presents another significant hurdle. Despite substantial research on both model compression and adversarial robustness, their joint examination remains underexplored. Our study bridges this gap, seeking to understand the effect of adversarial inputs crafted for base models on their pruned versions. To examine this relationship, we have developed a comprehensive benchmark across diverse adversarial attacks and popular DNN models. We uniquely focus on models not previously exposed to adversarial training and apply pruning schemes optimized for accuracy and performance. Our findings reveal that while the benefits of pruning enhanced generalizability, compression, and faster inference times are preserved, adversarial robustness remains comparable to the base model. This sug
    
[^62]: 深度生成填充模型用于非随机缺失数据

    Deep Generative Imputation Model for Missing Not At Random Data. (arXiv:2308.08158v1 [cs.LG])

    [http://arxiv.org/abs/2308.08158](http://arxiv.org/abs/2308.08158)

    本文介绍了一种用于处理非随机缺失数据的深度生成填充模型，提出了一种从新的角度去处理这个问题的方法，并且通过实验证明了直接将统计方法纳入深度生成模型的次优之处。

    

    数据分析通常面临非随机缺失（MNAR）问题，其中缺失值的原因没有完全观察到。与简单的完全随机缺失（MCAR）问题相比，这更符合实际情况，也更复杂和具有挑战性。现有的统计方法通过不同的完整数据和缺失蒙版的联合分布分解来建模MNAR机制。但我们在经验上发现，直接将这些统计方法纳入深度生成模型是次优的。具体来说，它会忽视MNAR填充过程中重构蒙版的置信度，导致信息提取不足和填充质量不够可靠。在本文中，我们从一种新的角度重新审视MNAR问题，即完整数据和缺失蒙版是两个不完整数据的模态。沿着这条线，我们提出了一种生成模型特定的联合预测框架

    Data analysis usually suffers from the Missing Not At Random (MNAR) problem, where the cause of the value missing is not fully observed. Compared to the naive Missing Completely At Random (MCAR) problem, it is more in line with the realistic scenario whereas more complex and challenging. Existing statistical methods model the MNAR mechanism by different decomposition of the joint distribution of the complete data and the missing mask. But we empirically find that directly incorporating these statistical methods into deep generative models is sub-optimal. Specifically, it would neglect the confidence of the reconstructed mask during the MNAR imputation process, which leads to insufficient information extraction and less-guaranteed imputation quality. In this paper, we revisit the MNAR problem from a novel perspective that the complete data and missing mask are two modalities of incomplete data on an equal footing. Along with this line, we put forward a generative-model-specific joint pr
    
[^63]: 灾难背景下的讽刺检测

    Sarcasm Detection in a Disaster Context. (arXiv:2308.08156v1 [cs.CL])

    [http://arxiv.org/abs/2308.08156](http://arxiv.org/abs/2308.08156)

    本文介绍了一个名为HurricaneSARC的数据集，其中包含了15,000条注释为讽刺意图的推文，并使用预训练语言模型进行了讽刺检测的研究。通过中间任务的迁移学习，我们的最佳模型在数据集上获得了0.70的F1值。

    

    在自然灾害期间，人们经常使用Twitter等社交媒体平台寻求帮助、提供关于灾情的信息，或表达对灾情演变或公共政策和指导方针的蔑视。在灾难背景中理解这种言论形式对改进对灾害相关推文的自然语言理解至关重要。本文介绍了一种名为HurricaneSARC的数据集，该数据集包含了15,000条标记了讽刺意图的推文，并使用预训练语言模型对讽刺检测进行了全面研究。我们的最佳模型在我们的数据集上能够达到0.70的F1值。我们还证明，通过利用中间任务的迁移学习可以提高HurricaneSARC的性能。我们在https://github.com/tsosea2/HurricaneSarc上发布了我们的数据和代码。

    During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
    
[^64]: 有限时间序列的层次化拓扑排序与条件独立性测试

    Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series. (arXiv:2308.08148v1 [cs.LG])

    [http://arxiv.org/abs/2308.08148](http://arxiv.org/abs/2308.08148)

    本文提出了一种改进的基于拓扑的方法，通过引入有限时间序列数据和条件独立性测试，目的是克服传统方法在生成冗余边方面的限制，从而更准确地识别观测数据中的因果关系。

    

    学习有向无环图（DAG）以识别观测数据中的因果关系既至关重要又具有重要挑战。最近，基于拓扑的方法作为一种两步法来发现DAG，首先学习变量的拓扑排序，然后消除冗余边，同时确保图形保持无环。然而，其中一个限制是这些方法会生成大量的虚假边，需要后续修剪。为了克服这一限制，在本文中，我们提出了一种改进的基于拓扑的方法，通过引入有限时间序列数据，其中仅包含两个不必相邻且具有灵活时间的横截面记录。通过将条件工具变量作为外生干预变量，我们旨在为每个变量识别后代节点。在这一基础上，我们提出了一种具有条件独立性测试的层次化拓扑排序算法。

    Learning directed acyclic graphs (DAGs) to identify causal relations underlying observational data is crucial but also poses significant challenges. Recently, topology-based methods have emerged as a two-step approach to discovering DAGs by first learning the topological ordering of variables and then eliminating redundant edges, while ensuring that the graph remains acyclic. However, one limitation is that these methods would generate numerous spurious edges that require subsequent pruning. To overcome this limitation, in this paper, we propose an improvement to topology-based methods by introducing limited time series data, consisting of only two cross-sectional records that need not be adjacent in time and are subject to flexible timing. By incorporating conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable. Following this line, we propose a hierarchical topological ordering algorithm with conditional independence test (
    
[^65]: 在线控制线性动力学：一种数据驱动方法

    Online Control for Linear Dynamics: A Data-Driven Approach. (arXiv:2308.08138v1 [eess.SY])

    [http://arxiv.org/abs/2308.08138](http://arxiv.org/abs/2308.08138)

    本文提出了一种数据驱动的方法来解决在线控制线性动力学问题，该方法不需要识别系统模型，而是通过累积扰动来进行决策，证明了算法性能与基于模型的方法相当。

    

    本文考虑了一个在线控制问题，涉及到具有未知动力学、有界扰动和对抗成本的线性时不变系统。我们提出了一种数据驱动策略来降低控制器的后悔。与基于模型的方法不同，我们的算法不需要识别系统模型，而是利用单个无噪声轨迹来计算扰动的累积，并使用我们设计的累积扰动动作控制器做出决策，其参数通过在线梯度下降进行更新。我们证明，在温和的假设下，我们算法的后悔是$\mathcal{O}(\sqrt{T})$的，这表明它的性能与基于模型的方法相当。

    This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
    
[^66]: 微结构强化的股票因子提取与利用

    Microstructure-Empowered Stock Factor Extraction and Utilization. (arXiv:2308.08135v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.08135](http://arxiv.org/abs/2308.08135)

    本论文提出了一个新的框架，旨在从订单流数据中有效提取关键因子，用于多样化下游任务。

    

    高频量化投资是股票投资的重要方面。特别是订单流数据在高频交易数据中起着关键作用，因为它提供了最详细的信息，包括订单簿和逐笔交易记录的全面数据。订单流数据对市场分析非常有价值，因为它为交易者提供了必要的洞察力，帮助他们做出明智的决策。然而，由于涉及到大量的数据和传统因子挖掘技术的局限性，订单流数据的提取和有效利用存在挑战，这些技术主要是为粗粒度股票数据设计的。为了解决这些挑战，我们提出了一个新的框架，旨在从订单流数据中有效提取关键因子，用于不同粒度和场景的多样化下游任务。我们的方法包括一个上下文编码器和一个因子提取器。上下文编码器学习一个嵌入f

    High-frequency quantitative investment is a crucial aspect of stock investment. Notably, order flow data plays a critical role as it provides the most detailed level of information among high-frequency trading data, including comprehensive data from the order book and transaction records at the tick level. The order flow data is extremely valuable for market analysis as it equips traders with essential insights for making informed decisions. However, extracting and effectively utilizing order flow data present challenges due to the large volume of data involved and the limitations of traditional factor mining techniques, which are primarily designed for coarser-level stock data. To address these challenges, we propose a novel framework that aims to effectively extract essential factors from order flow data for diverse downstream tasks across different granularities and scenarios. Our method consists of a Context Encoder and an Factor Extractor. The Context Encoder learns an embedding f
    
[^67]: 自我监督的预训练对分子性质预测的外推是否有效？

    Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?. (arXiv:2308.08129v1 [cs.LG])

    [http://arxiv.org/abs/2308.08129](http://arxiv.org/abs/2308.08129)

    这项研究探讨了自我监督的预训练方法在分子性质预测中的外推能力，结果发现即使在强大的机器学习模型条件下，准确的外推仍然是一个具有挑战性的问题。

    

    材料性质的预测在电池、半导体、催化剂和药物等各种应用中起着至关重要的作用。最近，人们越来越关注使用机器学习技术结合常规理论计算的数据驱动方法。在材料科学中，未观测的值，通常称为外推，在性质预测中尤其关键，因为它使研究人员能够深入了解超出现有数据限制范围的材料。然而，即使在强大的机器学习模型的最新进展下，准确的外推仍被广泛认为是一个具有挑战性的问题。另一方面，自我监督的预训练是一种机器学习技术，模型首先通过使用相对简单的前提任务在无标签数据上进行训练，然后在带标签的数据上进行目标训练。

    The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target 
    
[^68]: 如何在纠错码变压器中进行遮蔽：系统化与双重遮蔽

    How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])

    [http://arxiv.org/abs/2308.08128](http://arxiv.org/abs/2308.08128)

    该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。

    

    在通信和存储系统中，纠错码（ECC）对于确保数据可靠性至关重要。随着深度学习在不同领域的应用广泛扩展，神经网络解码器已成为研究的焦点，超越传统解码算法。在这些神经解码器中，纠错码变压器（ECCT）已经实现了最先进的性能，大幅超过其他方法。为了进一步提高ECCT的性能，我们提出了两种新方法。首先，利用ECC的系统编码技术，我们引入了一个新的遮蔽矩阵来改善ECCT的性能并减少计算复杂性。其次，我们提出了一种新的ECCT变压器架构，称为双重遮蔽的ECCT。该架构以并行方式使用两个不同的遮蔽矩阵，以学习遮蔽自注意力块中编码字位之间更多样的特征关系。大量实验证明了我们方法的有效性。

    In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
    
[^69]: S-Mixup: 结构Mixup用于图神经网络

    S-Mixup: Structural Mixup for Graph Neural Networks. (arXiv:2308.08097v1 [cs.LG])

    [http://arxiv.org/abs/2308.08097](http://arxiv.org/abs/2308.08097)

    S-Mixup是一种在节点分类任务中使用结构信息的新型Mixup增强方法，通过图神经网络的预测置信度和边梯度来构建Mixup池，并在真实世界基准数据集上进行了验证。

    

    现有的应用Mixup技术于图中的研究主要集中在图分类任务上，而节点分类的研究仍未深入探索。在本文中，我们提出了一种新的节点分类Mixup增强方法，称为结构Mixup（S-Mixup）。其核心思想是在混合节点时考虑结构信息。具体而言，S-Mixup通过图神经网络（GNN）分类器获得图中未标记节点的伪标签和预测置信度。这些伪标签和置信度作为构成跨类和内类Mixup的Mixup池的标准。此外，我们利用GNN训练中获得的边梯度，并提出了一种基于梯度的边选择策略，用于选择连接到Mixup生成的节点的边。通过对真实世界基准数据集的大量实验证明了S-Mixup在节点分类任务上的有效性。我们观察到S-Mixup在节点分类任务上的效果。

    Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-M
    
[^70]: 通过凸优化为神经网络系统设计安全过滤器

    Safety Filter Design for Neural Network Systems via Convex Optimization. (arXiv:2308.08086v1 [eess.SY])

    [http://arxiv.org/abs/2308.08086](http://arxiv.org/abs/2308.08086)

    本文提出了一种通过凸优化为神经网络系统设计安全过滤器的方法，该过滤器能够捕获建模误差，并通过鲁棒线性模型预测控制来搜索可以保证约束满足的控制器。通过在非线性摆系统上的实验验证了该方法的有效性。

    

    随着数据可用性的增加，已经广泛证明神经网络（NN）能够以数据驱动的方式准确捕获复杂的系统动态。然而，NN的架构复杂性和非线性使得对其合成一个能够被证明是安全的控制器变得具有挑战性。在这项工作中，我们提出了一种新颖的安全过滤器，它依靠凸优化来确保NN系统的安全性，同时能够捕获建模误差。我们的方法利用NN验证工具来使用一组线性边界来近似NN的动态，然后应用鲁棒线性模型预测控制（robust linear MPC）来寻找可以保证鲁棒约束满足的控制器。我们通过数值实验在非线性摆系统上展示了所提出框架的有效性。

    With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
    
[^71]: 用于支持地下不确定性量化和解释的稳定低维空间刚性变换

    Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation. (arXiv:2308.08079v1 [cs.LG])

    [http://arxiv.org/abs/2308.08079](http://arxiv.org/abs/2308.08079)

    该论文介绍了一种用于地下数据集的稳定降维方法，通过刚性变换实现了欧几里德不变表示，能够量化地下数据的不确定性并且适应外样本点的扩展。

    

    地下数据集天然地具有大数据特征，如庞大的体积、多样的特征和高速采样速度，受到各种物理、工程和地质输入引起的维数诅咒的影响。在现有的降维方法中，非线性降维方法，特别是度量多维缩放（MDS），是地下数据集中首选的方法，因为它们具有固有的复杂性。虽然MDS保留了内在的数据结构和不确定性的量化，但其局限性包括不稳定的唯一解，不变于欧几里德变换，并且没有外样本点（OOSP）扩展。为了增强地下推理和机器学习工作流程，必须将数据集转化为稳定的、降维的表示，以容纳OOSP。我们的解决方案利用刚性变换实现了LDS的稳定欧几里德不变表示。通过计算MDS输入的不相似度，

    Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.  Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity m
    
[^72]: 基于去中心化图神经网络的隐私保护推荐系统

    Decentralized Graph Neural Network for Privacy-Preserving Recommendation. (arXiv:2308.08072v1 [cs.IR])

    [http://arxiv.org/abs/2308.08072](http://arxiv.org/abs/2308.08072)

    本文提出了一种去中心化图神经网络（DGREC）框架，用于隐私保护推荐，其中用户可以选择公开他们的交互。该框架通过图构建、局部梯度计算和全局梯度传递三个阶段实现，同时引入了名为安全梯度共享的差分隐私机制，保护用户的私密数据。

    

    在不违反用户隐私的情况下构建基于图神经网络（GNN）的推荐系统是具有挑战性的。现有方法可以分为联邦GNN和去中心化GNN两种。然而，这两种方法都存在问题，如通信效率低和隐私泄露。本文提出了一种新的去中心化GNN框架，名为DGREC，用于隐私保护推荐，用户可以选择公开他们的交互。该框架包括三个阶段，即图构建、局部梯度计算和全局梯度传递。第一阶段为每个用户构建了一个本地内部物品超图和一个全局用户间图。第二阶段对用户偏好进行建模，并在每个本地设备上计算梯度。第三阶段设计了一种名为安全梯度共享的本地差分隐私机制，以保护用户的私密数据。我们在三个公共数据集上进行了大量实验，验证了我们框架的一贯优越性。

    Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framewo
    
[^73]: 鲜度或准确性，为什么不能两者兼得？通过动态图神经网络解决延迟反馈问题

    Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks. (arXiv:2308.08071v1 [cs.LG])

    [http://arxiv.org/abs/2308.08071](http://arxiv.org/abs/2308.08071)

    本论文提出了一种通过动态图神经网络解决延迟反馈问题的方法，该方法在数据新鲜度和标签准确性之间取得了平衡。

    

    在在线商业系统中，延迟反馈问题是预测转化率面临的最紧迫挑战之一，因为用户的转化总是延迟发生。虽然新数据有益于持续训练，但是没有完整的反馈信息，即转化标签，训练算法可能会遭受大量的假负面影响。现有方法往往使用多任务学习或设计数据流程来解决延迟反馈问题。然而，这些方法在数据新鲜度和标签准确性之间存在一个折中。在本文中，我们提出了通过动态图神经网络进行延迟反馈建模 （DGDFEM）。它包括三个阶段，即准备数据流程、构建动态图以及训练CVR预测模型。在模型训练中，我们提出了一种名为HLGCN的新颖图卷积方法，它利用高通和低通滤波器来处理转化和非转化关系。所提出的方法实现了数据新鲜度和标签准确性的双重要求。

    The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both 
    
[^74]: 最大仿射回归及其一阶方法

    Max-affine regression via first-order methods. (arXiv:2308.08070v1 [stat.ML])

    [http://arxiv.org/abs/2308.08070](http://arxiv.org/abs/2308.08070)

    本文研究了最大仿射回归问题，并提出了基于梯度下降和随机梯度下降的收敛分析方法。数值实验验证了理论结果的有效性。该方法在运行时间和观测次数较少时都能取得较好的效果。

    

    本文考虑了最大仿射模型的回归问题，该模型通过使用最大化函数将仿射模型组合成分段线性模型。最大仿射模型广泛应用于信号处理和统计学中，包括多类别分类、拍卖问题和凸回归。它还推广了相位恢复和学习整流线性单元激活函数。我们对梯度下降（GD）和小批量随机梯度下降（SGD）方法在最大仿射回归中的非渐近收敛性进行了分析，假设模型以随机位置观测，遵循次高斯分布和具有加性次高斯噪声的反浓度。在这些假设下，适当初始化的GD和SGD能够线性收敛到由相应误差界限确定的目标区域附近。我们提供了与理论发现相一致的数值结果。值得注意的是，SGD不仅在运行时间上收敛更快，而且在观测次数较少时也能获得较好的效果。

    We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alt
    
[^75]: 基于强化学习的数据中心计算节点功耗降低方法

    A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes. (arXiv:2308.08069v1 [cs.DC])

    [http://arxiv.org/abs/2308.08069](http://arxiv.org/abs/2308.08069)

    本文通过使用强化学习方法，在云计算节点上设计了一个能够限制功耗而不影响应用程序性能的策略。

    

    随着超级计算成为现实，云数据中心的计算节点能源需求将继续增长。减少能源需求的一种常见方法是在系统其他部分面临瓶颈时限制硬件组件的功耗。然而，设计一个能够实时检测并限制功耗的资源控制器是一个复杂的问题，还可能对应用程序性能产生不利影响。在本文中，我们探索了使用强化学习来设计云计算节点上的功率限制策略，通过观察当前功耗和瞬时应用程序性能（心跳）来实现。通过利用Argo Node资源管理（NRM）软件堆栈和英特尔运行平均功耗限制（RAPL）硬件控制机制，我们设计了一个代理来控制供应给处理器的最大功率，而不会影响应用程序性能。采用一种接近策略优化算法（Proximal）

    As Exascale computing becomes a reality, the energy needs of compute nodes in cloud data centers will continue to grow. A common approach to reducing this energy demand is to limit the power consumption of hardware components when workloads are experiencing bottlenecks elsewhere in the system. However, designing a resource controller capable of detecting and limiting power consumption on-the-fly is a complex issue and can also adversely impact application performance. In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance. Employing a Proximal
    
[^76]: 高成本困境：大型语言模型的泛化、评估和成本最优部署。

    The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models. (arXiv:2308.08061v1 [cs.CL])

    [http://arxiv.org/abs/2308.08061](http://arxiv.org/abs/2308.08061)

    在部署大型语言模型时，需要关注泛化、评估和成本最优化。本文提出了一个针对大型语言模型的泛化、评估和成本建模框架，帮助企业深入了解和评估这些因素。

    

    在为任何产品/应用程序在生产环境中部署机器学习模型时，通常希望具备三个属性。首先，模型应具有泛化能力，即在我们对领域知识的发展中可以扩展其用途。其次，它们应该是可评估的，这样在生产环境中可以有清晰的性能指标和计算这些指标的可行方式。最后，部署应尽可能地成本最优。在本文中，我们提出这三个目标（即泛化、评估和成本最优化）在大型语言模型中往往是相对独立的，并且对于大型语言模型，尽管其在传统NLP模型上表现出色，但企业在对这项技术进行重大投资之前需要仔细评估所有三个因素。我们提出了一个特别针对大型语言模型的泛化、评估和成本建模框架，为企业提供深入了解这些复杂因素的洞察力。

    When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricaci
    
[^77]: 鲁棒贝叶斯张量分解与零膨胀泊松模型和一致聚合

    Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation. (arXiv:2308.08060v1 [stat.ML])

    [http://arxiv.org/abs/2308.08060](http://arxiv.org/abs/2308.08060)

    本文提出了一种鲁棒的贝叶斯张量分解方法，使用零膨胀泊松模型来处理包含过多零值的高维计数数据。为了解决随机性问题，引入了一致聚合的方法。在合成和真实数据集上的实验证明了该方法的优越性能。

    

    张量分解是一种用于高效表示和分析多维数据的强大工具。然而，传统的基于最大似然估计的张量分解方法在应用于包含过多零值的计数数据（如单细胞RNA测序数据）时表现不佳。此外，张量分解的随机性导致因子在多次运行中变化，使得结果的解释和重现具有挑战性。本文提出了一种新颖的方法，称为零膨胀泊松张量分解（ZIPTF），用于分解具有过多零值的高维计数数据。为了解决随机性挑战，我们引入了一种称为一致零膨胀泊松张量分解（C-ZIPTF）的方法，将ZIPTF与基于一致性的元分析相结合。我们在合成的零膨胀计数数据和合成的真实单细胞RNA测序数据上评估了我们提出的ZIPTF和C-ZIPTF方法。结果显示，ZIPTF方法在性能上始终优于基线矩阵和张量分解方法。

    Tensor factorizations (TF) are powerful tools for the efficient representation and analysis of multidimensional data. However, classic TF methods based on maximum likelihood estimation underperform when applied to zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data. Additionally, the stochasticity inherent in TFs results in factors that vary across repeated runs, making interpretation and reproducibility of the results challenging. In this paper, we introduce Zero Inflated Poisson Tensor Factorization (ZIPTF), a novel approach for the factorization of high-dimensional count data with excess zeros. To address the challenge of stochasticity, we introduce Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF consistently outperforms baseline matrix and tensor factorizatio
    
[^78]: 通过一致性预言机进行简单的在线学习

    Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])

    [http://arxiv.org/abs/2308.08055](http://arxiv.org/abs/2308.08055)

    该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。

    

    我们考虑在只能通过一致性预言机访问类的模型下的在线学习——在任何时刻，预言机都能给出与目前为止看到的所有示例一致的类函数。该模型最近由Assos等人（COLT'23）考虑。这个模型的动机是标准的在线学习方法依赖于计算子类的Littlestone维度，这是一个计算复杂的问题。Assos等人在这个模型中给出了一个在线学习算法，对于Littlestone维度为d的类，最多会犯C^d个错误，其中C是一个未指定的绝对常数且大于0。我们提出了一个新的算法，最多会犯O(256^d)个错误。我们的证明更简单，只使用了Littlestone维度的基本属性。我们还观察到，不存在一个在这个模型中最多会犯2^(d+1)-2个错误的算法。我们还观察到，我们的算法（以及Assos等人的算法）。

    We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
    
[^79]: 自然进化策略作为随机变分推断的黑盒估计器

    Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference. (arXiv:2308.08053v1 [cs.NE])

    [http://arxiv.org/abs/2308.08053](http://arxiv.org/abs/2308.08053)

    提出了一种基于自然进化策略的黑盒估计器，可以克服变分自动编码器框架下对分布类型的限制。

    

    随机变分推断及其变种（如变分自动编码器）能够以高效的方式对大型数据集进行贝叶斯推断。然而，使用变分自动编码器进行推断需要进行某种设计选择（例如重新参数化技巧），以实现无偏和低方差的梯度估计，限制了可以创建的模型类型。为了克服这一挑战，提出了一种基于自然进化策略的替代估计器。该估计器不对所使用的分布类型做出任何假设，从而可以创建在变分自动编码器框架下否则无法实现的模型。

    Stochastic variational inference and its derivatives in the form of variational autoencoders enjoy the ability to perform Bayesian inference on large datasets in an efficient manner. However, performing inference with a VAE requires a certain design choice (i.e. reparameterization trick) to allow unbiased and low variance gradient estimation, restricting the types of models that can be created. To overcome this challenge, an alternative estimator based on natural evolution strategies is proposed. This estimator does not make assumptions about the kind of distributions used, allowing for the creation of models that would otherwise not have been possible under the VAE framework.
    
[^80]: 减少后悔的无偏决策：面向银行贷款问题的对抗领域适应

    Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem. (arXiv:2308.08051v1 [cs.LG])

    [http://arxiv.org/abs/2308.08051](http://arxiv.org/abs/2308.08051)

    该论文介绍了对抗领域适应的方法来解决银行贷款问题中训练集偏差问题，旨在学习无偏但信息丰富的过去数据表示。

    

    在许多实际情景中，基于有限数据进行二元分类决策是基于近实时的，例如在审批贷款申请时。我们专注于一类具有共同特征的问题：只有在数据点被指派为正标签时才能观察到真实标签，例如，只有在我们接受贷款申请之后才能发现申请人是否违约。因此，错误拒绝会变得自我强化，并导致由模型决策不断更新的标记训练集累积偏差。先前的研究通过将乐观主义注入模型来减轻这种效应，但这会增加错误接受率的代价。我们引入对抗乐观主义（AdOpt），通过对抗性领域适应直接解决训练集中的偏差问题。AdOpt的目标是通过减少被接受数据集的分布偏移来学习过去数据的无偏但信息丰富的表示。

    In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted da
    
[^81]: 多智能体多臂赌博机中的后悔下界

    Regret Lower Bounds in Multi-agent Multi-armed Bandit. (arXiv:2308.08046v1 [cs.LG])

    [http://arxiv.org/abs/2308.08046](http://arxiv.org/abs/2308.08046)

    在多智能体多臂赌博机中，我们首次全面研究了后悔下界，并证明了在具有良好连通性属性和随机分布奖励的情况下，存在紧密的实例相关和实例无关的下界。

    

    多臂赌博机激发了具有可证明后悔上界的方法，与之对应的后悔下界在这个背景下也被广泛研究。最近，多智能体多臂赌博机在各个领域中得到了显著的关注，其中个体客户以分布式的方式面临着赌博问题，目标是整体系统的性能，通常用后悔来衡量。尽管已经出现了具有后悔上界的高效算法，但对应的后悔下界却没有得到足够的关注，除了最近针对对抗设置的一个下界，然而，它与已知上界有差距。为此，我们在不同的设置下提供了第一次全面研究后悔下界，并建立了它们的紧密性。具体来说，当图表现出良好的连通性属性且奖励随机分布时，我们证明了一阶为$O(\log T)$的实例相关下界和$O(\log T)$的实例无关下界。

    Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order $O(\log T)$ for instance-dependent bounds and $
    
[^82]: 使用深度ReLU网络对由高斯混合模型生成的数据进行分类

    Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks. (arXiv:2308.08030v1 [stat.ML])

    [http://arxiv.org/abs/2308.08030](http://arxiv.org/abs/2308.08030)

    本文研究了如何使用深度ReLU神经网络在没有对模型参数施加限制的情况下，对由高斯混合模型生成的无界数据进行二分类。我们首次获得了收敛速度不受维度诅咒影响的非渐近上界，并通过使用高斯分布的特性在无限域上进行了分类分析。

    

    本文研究了使用深度ReLU神经网络对由高斯混合模型生成的无界数据进行二分类。我们首次获得了对于没有对模型参数施加限制的分类任务中超出风险（超出错误分类率）的非渐近上界和收敛速率。我们得到的收敛速率不依赖于维度$d$，证明了深度ReLU网络可以克服在分类中的维度诅咒。虽然现有的分类算法的广义分析大多依赖于有界域，但我们通过利用高斯分布的解析性和快速衰减将其应用于无界域。为了便于我们的分析，我们给出了一个使用ReLU网络对一般解析函数的新近似误差界，这可能具有独立的研究价值。高斯分布可以很好地用于建模产生的数据。

    This paper studies the binary classification of unbounded data from ${\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\unicode{x2013}$ for the first time $\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising
    
[^83]: 规划学习：一种新颖的模型优化过程中的主动学习算法

    Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning. (arXiv:2308.08029v1 [cs.AI])

    [http://arxiv.org/abs/2308.08029](http://arxiv.org/abs/2308.08029)

    本论文提出了一种新颖的算法，称为SI SL，用于主动学习和模型优化过程中的规划。该算法通过与贝叶斯强化学习方案的比较证明了其性能的优越性。

    

    主动推理是一种近期的对不确定性情境下规划建模的框架。现在人们已经开始评估这种方法的优缺点以及如何改进它。最近的一个拓展-复杂模型优化算法通过递归决策树搜索在多步规划问题上提高了性能。然而，迄今为止很少有工作对比SI与其他已建立的规划算法。SI算法也主要关注推理而不是学习。本文有两个目标。首先，我们比较SI与旨在解决相似问题的贝叶斯强化学习（RL）方案的性能。其次，我们提出了SI复杂学习（SL）的拓展，该拓展在规划过程中更加充分地引入了主动学习。SL维持对未来观测下每个策略下模型参数如何变化的信念。这允许了一种反事实的回顾性评估。

    Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective in
    
[^84]: 量子经济的势能优势

    Potential Energy Advantage of Quantum Economy. (arXiv:2308.08025v1 [quant-ph])

    [http://arxiv.org/abs/2308.08025](http://arxiv.org/abs/2308.08025)

    量子计算在能源效率方面具有优势，并且能够在盈利和能源效率上超越经典计算。这使得量子计算成为计算行业更可持续的选择。

    

    随着大规模机器学习模型和语言模型的广泛部署，能源成本越来越关键。对于提供计算服务的公司来说，低能耗对于市场增长和政府法规来说都非常重要。本文研究了量子计算与经典计算之间的能源优势。我们在能源效率的背景下重新定义优势，与仅基于计算复杂性的传统量子优势不同。通过一个以能量使用为约束条件的Cournot竞争模型，我们证明量子计算公司在Nash均衡点上在盈利能力和能源效率方面都能超越经典对手。因此，量子计算可能代表计算行业更可持续的发展路径。此外，我们发现量子计算经济的能源利益取决于大规模计算。

    Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation
    
[^85]: Stackelberg轨迹博弈中的主动逆向学习

    Active Inverse Learning in Stackelberg Trajectory Games. (arXiv:2308.08017v1 [cs.GT])

    [http://arxiv.org/abs/2308.08017](http://arxiv.org/abs/2308.08017)

    这项研究提出了一种在Stackelberg博弈中的主动逆向学习方法，通过活跃地最大化跟随者在不同假设下的轨迹差异来加速领导者的推断过程。

    

    博弈论的逆向学习是从玩家的行为中推断出他们的目标的问题。我们在一个Stackelberg博弈中，通过每个玩家的动态系统轨迹来定义一个逆向学习问题，其中包括一个领导者和一个跟随者。我们提出了一种主动逆向学习方法，让领导者推断出一个有限候选集中描述跟随者目标函数的假设。与现有方法使用被动观察到的轨迹不同，所提出的方法主动地最大化不同假设下跟随者轨迹的差异，加速领导者的推断过程。我们在一个递进的重复轨迹博弈中展示了所提出的方法。与均匀随机输入相比，所提供的方法加速了概率收敛到条件于跟随者轨迹的不同假设上的收敛速度。

    Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
    
[^86]: 基于具有空间金字塔池化的卷积神经网络的网络稳健性评估的综合分析

    Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling. (arXiv:2308.08012v1 [cs.CV])

    [http://arxiv.org/abs/2308.08012](http://arxiv.org/abs/2308.08012)

    本文通过设计具有空间金字塔池化网络的卷积神经网络模型，解决了网络稳健性评估中的性能、捕捉稳健性、可扩展性和可转移性等挑战。

    

    连通性稳健性是理解、优化和修复复杂网络的关键方面，传统上通过耗时且常常不切实际的模拟来评估。幸运的是，机器学习为解决这一挑战提供了一条新的途径。然而，仍然存在一些关键问题尚未解决，包括在更一般的边缘删除场景中的性能，通过攻击曲线捕捉稳健性而不是直接训练稳健性，预测任务的可扩展性以及预测能力的可转移性。本文通过设计具有空间金字塔池化网络的卷积神经网络模型(CNN)，调整现有的评估指标，重新设计攻击模式，引入适当的过滤规则，并将稳健性的价值作为训练数据加以解决这些挑战。结果表明，所提出的CNN框架在解决高计算挑战方面具有全面性。

    Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational
    
[^87]: GRINN:一种物理信息导向的神经网络，在自重存在的情况下求解流体动力学系统

    GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity. (arXiv:2308.08010v1 [cs.LG])

    [http://arxiv.org/abs/2308.08010](http://arxiv.org/abs/2308.08010)

    GRINN是一种物理信息导向的神经网络，用于在自重存在的情况下求解三维流体动力学系统。它在模拟重力不稳定性和波传播方面取得了良好的结果。

    

    建模自重气体流动对于回答天体物理学中的许多基本问题是至关重要的。这涉及众多话题，包括行星形成盘、星云形成、星系形成以及宇宙中大尺度结构的发展。然而，重力与流体动力学之间的非线性相互作用对求解所得到的三维时变偏微分方程（PDEs）提出了巨大的挑战。通过在一个无网格框架中利用神经网络的普适逼近能力，物理信息导向的神经网络（PINNs）提供了解决这一挑战的新方法。我们介绍了一种基于PINN的码，名为GRINN，用于模拟三维自重流体动力学系统。在这里，我们特别研究一个等温气体中的重力不稳定性和波传播。我们的结果在线性区域内与线性解相匹配，误差在1%以内，与传统网格码求解的结果一致。

    Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solu
    
[^88]: BI-LAVA：通过主动学习和视觉分析进行层级图像标签的生物鉴定

    BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. (arXiv:2308.08003v1 [cs.HC])

    [http://arxiv.org/abs/2308.08003](http://arxiv.org/abs/2308.08003)

    BI-LAVA是一个通过主动学习和视觉分析进行层级图像标签的生物鉴定系统，它解决了标签层级性质、数据处理开销、标记数据缺失等问题，并帮助模型构建。

    

    在生物医学领域，分类法通过层级结构组织科学图像的获取方式。这些分类法利用大量正确的图像标签，并提供关于科学出版物重要性的重要信息，这可以在生物鉴定任务中使用。然而，标签的层级性质、图像处理的开销、标记数据的缺失或不完整性，以及标记这种类型数据所需要的专业知识都阻碍了用于生物鉴定的有用数据集的创建。根据与生物鉴定员和文本挖掘研究人员多年的合作，我们提出了一种迭代的视觉分析和主动学习策略来解决这些挑战。我们在一个名为BI-LAVA的系统中实现了这个策略，它通过主动学习和视觉分析来使用一小组图像标签、一套层级图像分类器来帮助模型构建。

    In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model build
    
[^89]: 蒙特卡洛引导扩散的贝叶斯线性逆问题研究

    Monte Carlo guided Diffusion for Bayesian linear inverse problems. (arXiv:2308.07983v1 [stat.ML])

    [http://arxiv.org/abs/2308.07983](http://arxiv.org/abs/2308.07983)

    本研究提出了一种在贝叶斯框架下利用蒙特卡洛方法解决非完备线性逆问题的算法，该算法通过利用基于得分的生成模型的先验结构和Feynman-Kac模型，并进行顺序蒙特卡洛采样，表现出比竞争对手更好的性能。

    

    非完备的线性逆问题经常在各种应用中出现，从计算摄影到医学成像。最近的研究集中于使用基于得分的生成模型（SGMs），在填补问题中产生具有感知合理性的图像来解决这些问题。本研究利用SGM定义的先验结构来制定贝叶斯框架下的恢复问题，将其作为Feynman-Kac模型，该模型改编自用于构建基于得分扩散的前向扩散模型。为了解决这个Feynman-Kac问题，我们提出使用顺序蒙特卡洛方法。所提出的算法MCGdiff在理论上是有根据的，并且我们提供了数值模拟结果，表明它在处理非完备逆问题时优于竞争对手的基准方法。

    Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
    
[^90]: 基于元学习的自适应概率风电功率预测方法研究

    An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning. (arXiv:2308.07980v1 [eess.SY])

    [http://arxiv.org/abs/2308.07980](http://arxiv.org/abs/2308.07980)

    本文提出了一种基于元学习的自适应概率风电功率预测方法，通过离线学习和在线学习过程，在不同的预测任务中展现出优异的适应性能力。数值测试结果验证了该方法的优势。

    

    本文研究了一种包括离线学习和在线学习过程的自适应概率风电功率预测方法。在离线学习阶段，通过元学习的内外层更新训练基准预测模型，使其具有适应不同预测任务（即具有不同提前时间或位置的概率风电功率预测）的优异能力。在在线学习阶段，将基准预测模型应用于在线预测，结合增量学习技术，使在线预测充分利用最近的信息和基准预测模型的适应性。基于所提出的方法，分别开发了两个应用：不同提前时间的预测（时间适应性）和新建风电场的预测（空间适应性）。在真实世界的风电数据集上进行了数值测试。仿真结果验证了该方法的优势。

    This paper studies an adaptive approach for probabilistic wind power forecasting (WPF) including offline and online learning procedures. In the offline learning stage, a base forecast model is trained via inner and outer loop updates of meta-learning, which endows the base forecast model with excellent adaptability to different forecast tasks, i.e., probabilistic WPF with different lead times or locations. In the online learning stage, the base forecast model is applied to online forecasting combined with incremental learning techniques. On this basis, the online forecast takes full advantage of recent information and the adaptability of the base forecast model. Two applications are developed based on our proposed approach concerning forecasting with different lead times (temporal adaptation) and forecasting for newly established wind farms (spatial adaptation), respectively. Numerical tests were conducted on real-world wind power data sets. Simulation results validate the advantages i
    
[^91]: MultiSChuBERT: 高效的学术文档质量预测的多模态融合方法

    MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])

    [http://arxiv.org/abs/2308.07971](http://arxiv.org/abs/2308.07971)

    MultiSChuBERT是一个多模态预测模型，通过结合文本和图像信息，在学术文档质量预测任务上取得了显著改进。我们的工作在结合视觉和文本嵌入、逐渐解冻视觉子模型权重以及采用最新文本嵌入替换标准BERT$_{\textrm{BASE}}$嵌入方面做出了重要贡献。

    

    学术文档质量的自动评估是一项具有高潜力影响的困难任务。多模态学习，特别是将视觉信息与文本相结合，已经显示出在学术文档质量预测任务上提高性能的效果。我们提出了一种多模态的预测模型MultiSChuBERT。它结合了基于文本的模型（SChuBERT）和基于图像的模型（Inception V3）。我们的工作在学术文档质量预测方面有三个方面的贡献。首先，我们展示了结合视觉和文本嵌入的方法可以在结果上产生显著影响。其次，我们证明了逐渐解冻视觉子模型的权重可以减少过拟合数据的趋势，从而提高结果。第三，我们展示了采用最新的文本嵌入替换标准BERT$_{\textrm{BASE}}$嵌入时多模态的优势。

    Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding 
    
[^92]: 通过拓扑数据分析进行投资组合选择

    Portfolio Selection via Topological Data Analysis. (arXiv:2308.07944v1 [q-fin.PM])

    [http://arxiv.org/abs/2308.07944](http://arxiv.org/abs/2308.07944)

    通过拓扑数据分析，我们提出了一个基于两阶段方法的投资组合选择系统。实验结果表明，我们的系统优于其他方法，验证了拓扑数据分析作为一个强大的投资组合选择工具的可行性。

    

    投资组合管理是投资决策的重要部分。然而，传统方法往往不能提供合理的表现。这个问题源于这些方法无法考虑到股市多元时间序列数据的独特特征。我们提出了一个两阶段的方法来构建一个由普通股票组成的投资组合。该方法涉及时间序列表示的生成及其后续聚类。我们的方法利用基于拓扑数据分析（TDA）的特征进行表示的生成，使我们能够阐明数据中的拓扑结构。实验结果表明，我们提出的系统优于其他方法。这种优越性在不同的时间框架下都是一致的，说明TDA作为一个强大的投资组合选择工具的可行性。

    Portfolio management is an essential part of investment decision-making. However, traditional methods often fail to deliver reasonable performance. This problem stems from the inability of these methods to account for the unique characteristics of multivariate time series data from stock markets. We present a two-stage method for constructing an investment portfolio of common stocks. The method involves the generation of time series representations followed by their subsequent clustering. Our approach utilizes features based on Topological Data Analysis (TDA) for the generation of representations, allowing us to elucidate the topological structure within the data. Experimental results show that our proposed system outperforms other methods. This superior performance is consistent over different time frames, suggesting the viability of TDA as a powerful tool for portfolio selection.
    
[^93]: 基于图神经网络和规则的归纳知识图谱补全的分析

    Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])

    [http://arxiv.org/abs/2308.07942](http://arxiv.org/abs/2308.07942)

    基于图神经网络和规则的归纳知识图谱补全研究了基于规则的方法在实践中的表现不佳的原因，发现不合理的实体没有排名和只考虑最具信息量的路径是影响因素。提出了一些解决这些问题的规则方法的变体，发现其性能接近于基于图神经网络的方法NBFNet。这些变体仅使用了NBFNet所依赖的证据的一小部分。

    

    归纳知识图谱补全的任务要求模型从训练图谱中学习推理模式，然后可以用来在分离的测试图谱上进行预测。虽然基于规则的方法似乎很适合这个任务，但在实践中，它们的表现明显不如基于图神经网络（GNNs）的最先进方法，如NBFNet。我们假设基于规则的方法表现不佳是由于两个因素：（i）不合理的实体根本没有排名，（ii）在确定给定链接预测答案的置信度时，只考虑了最具信息量的路径。为了分析这些因素的影响，我们研究了一些针对上述问题的规则方法的变体。我们发现，所得到的模型的性能接近NBFNet。至关重要的是，考虑到的变体只使用了NBFNet所依赖的证据的一小部分。

    The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
    
[^94]: 从头开始对编码的时空数据进行训练的使用GPT-2生成个体轨迹

    Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data. (arXiv:2308.07940v1 [cs.LG])

    [http://arxiv.org/abs/2308.07940](http://arxiv.org/abs/2308.07940)

    该论文尝试通过使用GPT-2模型从头开始训练编码的时空数据，生成受环境因素和个体属性影响的个体轨迹。

    

    在此研究中，我们使用Mizuno、Fujimoto和Ishikawa的方法将地理坐标转换为具有不同空间尺度位置特征的独特位置符号。我们使用独特的时间间隔符号将位置符号组合成个体每日轨迹的序列。通过使用自回归语言模型GPT-2的架构，我们从头开始训练这个符号序列，从而构建一个能够顺序生成个体每日轨迹的深度学习模型。通过使用特殊符号表示气象条件和个体属性，比如性别和年龄，并在GPT-2架构上训练这些符号和轨迹，我们可以生成同时受环境因素和个体属性影响的轨迹。

    Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
    
[^95]: Ada-QPacknet -- 自适应剪枝与位宽缩减作为一种高效的继续学习方法，不会遗忘的算法

    Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])

    [http://arxiv.org/abs/2308.07939](http://arxiv.org/abs/2308.07939)

    Ada-QPacknet是一种自适应剪枝与位宽缩减的高效继续学习方法，通过剪枝和量化技术生成任务子网络，在动态和复杂环境中实现了与浮点数子网络相似的准确性。

    

    继续学习（CL）是一个过程，其中人类和深度学习模型之间的效率仍存在巨大差距。最近设计了许多CL算法，大部分都存在在动态和复杂环境中学习的问题。本文描述了一种基于新架构的方法Ada-QPacknet。它通过剪枝提取每个任务的子网络。基于架构的CL方法的关键是容量。在提出的方法中，通过高效的线性和非线性量化方法减小了模型的规模。该方法减小了权重格式的位宽。实验结果显示，混合8位和4位量化在著名的CL场景上实现了与浮点数子网络相似的准确性。据我们所知，这是第一个将剪枝和量化这两种压缩技术应用于生成任务子网络的CL策略。该算法在著名的情节组合上进行了测试。

    Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
    
[^96]: 只需一次位翻转：当位翻转攻击遇到模型训练

    One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training. (arXiv:2308.07934v1 [cs.CR])

    [http://arxiv.org/abs/2308.07934](http://arxiv.org/abs/2308.07934)

    本论文介绍了一种位翻转攻击和模型训练相结合的方法，通过在训练阶段引入对手构建高风险模型，在只进行少量位翻转的情况下，将正常模型转化为恶意模型。实验结果表明，这种攻击方法可以逃避各种检测方法。

    

    深度神经网络广泛部署在现实世界的设备上。对其安全性的关注引起了研究者的极大关注。最近提出了一种新的权重修改攻击称为位翻转攻击（BFA），该攻击利用内存故障注入技术，如行锤击，来攻击部署阶段的量化模型。仅通过少量的位翻转，目标模型可以被渲染为无用的随机猜测者，甚至可以植入恶意功能。在这项工作中，我们试图进一步降低位翻转的数量。我们提出了一种训练辅助的位翻转攻击，在其中，对手参与到训练阶段中，建立一个高风险的释放模型。这个高风险模型与相应的恶意模型结合，表现正常，并且可以逃避各种检测方法。在基准数据集上的实验结果表明，攻击者可以轻松地将这个高风险但正常的模型转化为受害者这边的恶意模型。

    Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipp
    
[^97]: 使用图像文本对齐评估用于痴呆检测的图片描述语音

    Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment. (arXiv:2308.07933v1 [cs.CL])

    [http://arxiv.org/abs/2308.07933](http://arxiv.org/abs/2308.07933)

    该论文提出了一种新的痴呆检测模型，将图片和描述文本作为输入，并利用大型预训练图像文本对齐模型的知识。通过观察发现，痴呆样本与健康样本在文本与图片相关性和图片焦点区域上存在差异，从而可以提高痴呆检测的准确性。

    

    使用图片描述语音进行痴呆检测已经研究了30年。尽管有这么长的历史，先前的模型主要关注正常人和患有痴呆症的患者之间语音模式的差异，但没有直接利用图片信息。在本文中，我们提出了首个将图片和描述文本作为输入，并融入大型预训练图像文本对齐模型的痴呆检测模型。我们观察到痴呆样本和健康样本在文本与图片相关性以及图片的焦点区域方面存在差异。因此，我们认为这种差异可以用于提高痴呆检测的准确性。具体而言，我们使用文本与图片的相关性对样本的句子进行排序和过滤。我们还根据图片的焦点区域确定了话题，并根据焦点区域对句子进行分类。我们提出了三个先进模型进行预处理。

    Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-pr
    
[^98]: 精简特征场使得语言引导的少样本操作成为可能

    Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])

    [http://arxiv.org/abs/2308.07931](http://arxiv.org/abs/2308.07931)

    本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。

    

    自监督和语言监督的图像模型包含了世界的丰富知识，对于泛化很重要。然而，许多机器人任务需要对 3D 几何的详细理解，这在 2D 图像特征中往往缺乏。本研究通过利用精简特征场，将精确的 3D 几何与 2D 基础模型的丰富语义相结合，来弥合机器人操作中的 2D 到 3D 的差距。我们提出一种针对 6 自由度抓取和放置的少样本学习方法，利用这些强大的空间和语义先验，实现对未见过的物体的自然泛化。通过从视觉语言模型 CLIP 中精简的特征，我们展示了一种通过自由文本自然语言指定新颖对象进行操作的方式，并展示了它在未见过的表达和新颖类别的物体上的泛化能力。

    Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
    
[^99]: 运用主动MDP学习的概率黑盒检查方法

    Probabilistic Black-Box Checking via Active MDP Learning. (arXiv:2308.07930v1 [cs.SE])

    [http://arxiv.org/abs/2308.07930](http://arxiv.org/abs/2308.07930)

    这篇论文介绍了一种新的方法，利用主动MDP学习来进行概率黑盒检查。这种方法通过在各个阶段运用不同的技术，增强了传统黑盒检查方法的能力。

    

    我们引入了一种新的方法来测试嵌入式系统中经常遇到的随机黑盒系统。我们的方法增强了已有的黑盒检查方法，以解决随机行为的问题。传统的黑盒检查方法主要通过以下三个阶段来迭代地确定一个违反系统规范的输入：学习阶段用于构建逼近黑盒行为的自动机，合成阶段用于从学习的自动机中确定一个候选的反例，验证阶段用于验证获得的候选反例和学习的自动机是否符合原始黑盒系统。我们的方法，ProbBBC，在学习阶段采用了主动马尔可夫决策过程（MDP）学习方法，合成阶段结合了概率模型检查，验证阶段应用了统计假设检验。

    We introduce a novel methodology for testing stochastic black-box systems, frequently encountered in embedded systems. Our approach enhances the established black-box checking (BBC) technique to address stochastic behavior. Traditional BBC primarily involves iteratively identifying an input that breaches the system's specifications by executing the following three phases: the learning phase to construct an automaton approximating the black box's behavior, the synthesis phase to identify a candidate counterexample from the learned automaton, and the validation phase to validate the obtained candidate counterexample and the learned automaton against the original black-box system. Our method, ProbBBC, refines the conventional BBC approach by (1) employing an active Markov Decision Process (MDP) learning method during the learning phase, (2) incorporating probabilistic model checking in the synthesis phase, and (3) applying statistical hypothesis testing in the validation phase. ProbBBC un
    
[^100]: 在文本到图像分类和生成中，使用Bradley-Terry偏好模型进行快速自适应

    Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation. (arXiv:2308.07929v1 [cs.CV])

    [http://arxiv.org/abs/2308.07929](http://arxiv.org/abs/2308.07929)

    本研究提出了一种快速自适应方法，利用Bradley-Terry偏好模型，通过很少的示例和最小的计算资源高效地微调大型多模态模型，使其更符合用户的偏好，并在多个领域中展示了该方法的能力。

    

    最近，大型多模态模型，如CLIP和Stable Diffusion在基础理论和应用方面取得了巨大成功。然而，随着这些模型的参数大小和计算要求增加，用户为特定任务或偏好个性化它们变得更具挑战性。在这项工作中，我们解决了将之前的模型适应到特定人类偏好集合的问题，将检索或生成的图像与用户的偏好对齐。我们利用Bradley-Terry偏好模型开发了一种快速自适应方法，通过很少的示例和最小的计算资源高效地微调原始模型。通过与多模态文本和图像理解相关的不同领域的实验证据，我们提供了这个框架的能力。

    Recently, large multimodal models, such as CLIP and Stable Diffusion have experimented tremendous successes in both foundations and applications. However, as these models increase in parameter size and computational requirements, it becomes more challenging for users to personalize them for specific tasks or preferences. In this work, we address the problem of adapting the previous models towards sets of particular human preferences, aligning the retrieved or generated images with the preferences of the user. We leverage the Bradley-Terry preference model to develop a fast adaptation method that efficiently fine-tunes the original model, with few examples and with minimal computing resources. Extensive evidence of the capabilities of this framework is provided through experiments in different domains related to multimodal text and image understanding, including preference prediction as a reward model, and generation tasks.
    
[^101]: "月经周期长度的预测建模：一种时间序列预测方法"

    Predictive Modeling of Menstrual Cycle Length: A Time Series Forecasting Approach. (arXiv:2308.07927v1 [cs.LG])

    [http://arxiv.org/abs/2308.07927](http://arxiv.org/abs/2308.07927)

    本研究通过使用机器学习技术，探索了预测月经周期的方法，结果表明可以准确预测月经周期的开始和持续时间。

    

    正确预测月经周期对女性健康至关重要，因为它可以让个体采取预防措施来减少与周期相关的不适。此外，精确的预测对于规划女性生活中的重要事件，如计划生育，也是有用的。本研究探索了使用机器学习技术来预测规律和不规律月经周期的方法。我们实现了一些时间序列预测算法，如自回归综合移动平均、Huber回归、Lasso回归、正交匹配追踪和长短期记忆网络。此外，我们还生成了合成数据来实现我们的目的。结果表明，使用机器学习技术可以准确预测月经周期的开始和持续时间。

    A proper forecast of the menstrual cycle is meaningful for women's health, as it allows individuals to take preventive actions to minimize cycle-associated discomforts. In addition, precise prediction can be useful for planning important events in a woman's life, such as family planning. In this work, we explored the use of machine learning techniques to predict regular and irregular menstrual cycles. We implemented some time series forecasting algorithm approaches, such as AutoRegressive Integrated Moving Average, Huber Regression, Lasso Regression, Orthogonal Matching Pursuit, and Long Short-Term Memory Network. Moreover, we generated synthetic data to achieve our purposes. The results showed that it is possible to accurately predict the onset and duration of menstrual cycles using machine learning techniques.
    
[^102]: SciRE-Solver: 用得分积分求解器和递归导数估计快速采样扩散概率模型

    SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])

    [http://arxiv.org/abs/2308.07896](http://arxiv.org/abs/2308.07896)

    SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。

    

    扩散概率模型(DPMs)是一类强大的生成模型，以其生成高保真图像样本的能力而闻名。DPMs的实现面临的主要挑战是采样过程缓慢。在这项工作中，我们提出了一种高效的DPMs采样器。具体而言，我们针对与DPMs采样过程对应的扩散ODE提出了一个基于得分的精确解决方案范式，该范式为求解扩散ODE的数值算法开发提供了新的视角。为了实现高效的采样器，我们提出了一种递归导数估计(RDE)方法来减小估计误差。通过我们提出的解决方案范式和RDE方法，我们提出了具有收敛顺序保证的得分积分求解器(SciRE-Solver)来解决扩散ODEs。SciRE-Solver在离散时间和连续时间DPMs上获得了最先进的采样性能，并且仅需有限数量的得分函数评估(NFE)。

    Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
    
[^103]: DiffGuard：使用预训练扩散模型进行语义不匹配引导的带外分布检测

    DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])

    [http://arxiv.org/abs/2308.07687](http://arxiv.org/abs/2308.07687)

    本论文提出了一种名为DiffGuard的方法，使用预训练的扩散模型进行语义不匹配引导的带外分布检测。实验证明，DiffGuard在小规模数据集上表现出色，但在ImageNet规模的数据集上无法应用。

    

    本论文针对语义带外（OOD）样本与合法类别内容在语义上的不匹配特征，提出了一种语义不匹配引导的带外分布检测方法DiffGuard。与其他方法相比，DiffGuard直接使用预训练的扩散模型进行语义不匹配引导，相较于条件生成对抗网络，扩散模型更易于训练且适用于各种条件。实验结果表明，在小规模数据集上，DiffGuard取得了显著的带外分布检测性能。然而，由于使用图像和标签作为条件训练条件生成对抗网络的困难性，DiffGuard在ImageNet规模的数据集上无法应用。

    Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
    
[^104]: 使用时间图神经网络的交互感知个性化车辆轨迹预测

    Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])

    [http://arxiv.org/abs/2308.07439](http://arxiv.org/abs/2308.07439)

    本研究提出了一种交互感知的个性化车辆轨迹预测方法，利用时间图神经网络来建模目标车辆与周围交通之间的时空交互，并通过迁移学习来个性化预测。实验结果表明，该方法能够更准确地预测车辆的轨迹。

    

    准确预测车辆轨迹对于先进驾驶辅助系统和自动驾驶汽车至关重要。现有方法主要依赖于从大型数据集中推导出的通用轨迹预测，忽视了个别驾驶员的个性化驾驶模式。为了弥补这一差距，我们提出了一种交互感知的个性化车辆轨迹预测方法，该方法采用了时间图神经网络。我们的方法利用图卷积神经网络（GCN）和长短期记忆（LSTM）来建模目标车辆与周围交通之间的时空交互。为了个性化预测，我们建立了一个利用迁移学习的流程：模型首先在大规模轨迹数据集上进行预训练，然后使用特定驾驶数据针对每个驾驶员进行微调。我们采用人机协同仿真来收集个性化的自然驾驶轨迹及其相应的周围车辆轨迹。

    Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
    
[^105]: LCE: Python中增强版的Bagging和Boosting的结合方法

    LCE: An Augmented Combination of Bagging and Boosting in Python. (arXiv:2308.07250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.07250](http://arxiv.org/abs/2308.07250)

    LCE是一个用于分类和回归任务的Python包，它实现了局部级联集成方法，通过结合随机森林和XGBoost的优势，采用多样化的方法获得更好的泛化预测性能。

    

    lcensemble是一个高性能、可扩展且用户友好的Python包，用于分类和回归的常见任务。该包实现了局部级联集成(LCE)，这是一种机器学习方法，进一步提高了当前最先进的方法如随机森林和XGBoost的预测性能。LCE将它们的优势结合起来，并采用一种互补的多样化方法来获得更好的泛化预测器。该包与scikit-learn兼容, 可以与scikit-learn的流水线和模型选择工具交互。它在Apache 2.0许可下分发，并且其源代码可以在https://github.com/LocalCascadeEnsemble/LCE获得。

    lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.
    
[^106]: 工业和机器人领域中的神经辐射场：应用，研究机会和使用案例

    Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases. (arXiv:2308.07118v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2308.07118](http://arxiv.org/abs/2308.07118)

    本文研究了神经辐射场在工业领域的应用，并提供了未来研究的方向。通过学习训练图像来学习3D场景表示，可以解决当前工业3D表示方法的不足。

    

    如今，诸如扩展现实（XR）之类的技术的普及增加了对高质量三维图形表示的需求。工业3D应用包括计算机辅助设计（CAD），有限元分析（FEA），扫描和机器人技术。然而，目前用于工业3D表示的方法存在实施成本高和对准确3D建模的手工输入依赖的问题。为了解决这些挑战，神经辐射场（NeRFs）已经成为一种有前景的方法，基于提供的训练2D图像来学习3D场景表示。尽管对NeRFs的兴趣不断增长，但它们在各种工业子领域中的潜在应用尚未被探索。在本文中，我们对NeRF在工业应用中进行了全面的研究，并提供了未来研究的方向。我们还展示了一系列概念验证实验，证明了NeRF在工业领域的潜力。

    The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industri
    
[^107]: 使用基于聚类的树状Parzen估计的敏感性感知混合精度量化和宽度优化的深度神经网络

    Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])

    [http://arxiv.org/abs/2308.06422](http://arxiv.org/abs/2308.06422)

    本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。

    

    随着深度学习模型的复杂性和计算需求的提高，对神经网络设计的有效优化方法的需求变得至关重要。本文引入了一种创新的搜索机制，用于自动选择单个神经网络层的最佳位宽和层宽。这导致深度神经网络效率的明显提高。通过利用基于Hessian的剪枝策略，有选择地减少搜索域，确保移除非关键参数。随后，我们通过使用基于聚类的树状Parzen估计器开发有利和不利结果的替代模型。这种策略允许对架构可能性进行简化的探索，并迅速确定表现最好的设计。通过对知名数据集进行严格测试，我们的方法证明了与现有方法相比的明显优势。与领先的压缩策略相比，我们的方法取得了令人瞩目的成果。

    As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
    
[^108]: PeRP：通过合作咨询系统实现个性化剩余策略以缓解拥堵

    PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])

    [http://arxiv.org/abs/2308.00864](http://arxiv.org/abs/2308.00864)

    本论文提出了一种基于个性化剩余策略的合作咨询系统PeRP，用于缓解拥堵。该系统通过结构化建模人类驾驶的相似性，并根据驾驶员的特征为其提供行动建议，以减少交通拥堵。

    

    智能驾驶系统可以通过简单的行动来缓解拥堵，从而改善通勤时间和燃油成本等众多社会经济因素。然而，这些系统假设对自动驾驶车队具有精确的控制，因此在实际中存在限制，因为它们未能考虑到人类行为的不确定性。分段常数（PC）策略通过结构建模人类驾驶的相似性来减少交通拥堵，以提供给人类驾驶员遵循的行动建议。然而，PC策略假设所有驾驶员行为相似。为了实现这一目标，我们开发了一个基于PC策略的合作咨询系统，其中包含一种新型的驾驶员特征相关的个性化剩余策略，即PeRP。PeRP建议驾驶员以减少交通拥堵的方式行驶。我们首先使用变分自动编码器无监督地推断驾驶员如何遵循指令的内在特征。然后，通过将策略与驾驶员特征条件化，实现个性化的行动建议。

    Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
    
[^109]: 深度展开网络与循环动量加速用于非线性反问题

    Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])

    [http://arxiv.org/abs/2307.16120](http://arxiv.org/abs/2307.16120)

    本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。

    

    结合基于模型的迭代算法和数据驱动的深度学习解决方案，深度展开网络(DuNets)已成为解决逆向成像问题的流行工具。虽然DuNets已成功应用于许多线性逆向问题，但非线性问题往往会影响方法的性能。受优化算法中常用的动量加速技术启发，我们提出了一种循环动量加速(RMA)框架，该框架使用长短期记忆循环神经网络(LSTM-RNN)来模拟动量加速过程。RMA模块利用LSTM-RNN学习和保留先前梯度的知识能力。我们将RMA应用于两种流行的DuNets——学习的近端梯度下降(LPGD)和学习的原始-对偶(LPD)方法，分别得到LPGD-RMA和LPD-RMA。我们在两个非线性逆向问题上提供了实验结果：非线性去卷积问题、

    Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
    
[^110]: 决策导向学习：基础、现状、基准和未来机会

    Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])

    [http://arxiv.org/abs/2307.13565](http://arxiv.org/abs/2307.13565)

    决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。

    

    决策导向学习（DFL）是一种新兴的机器学习范式，它训练模型以优化决策，在一个端到端的系统中集成了预测和优化。这个范式有望在许多实际应用中革命性地改变决策制定，这些应用在不确定性下运作，在这些决策模型中估计未知参数经常成为一个重要障碍。本文对DFL进行了全面的回顾。它对各种技术进行了深入分析，以整合机器学习和优化模型，引入了一种根据其独特特征来区分DFL方法的分类法，并对这些方法进行了广泛的实证评估，提出了适用于DFL的合适基准数据集和任务。最后，本研究提供了关于DFL研究中当前和潜在未来方向的宝贵见解。

    Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
    
[^111]: LLM认知判断与人类有所不同

    LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])

    [http://arxiv.org/abs/2307.11787](http://arxiv.org/abs/2307.11787)

    这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。

    

    最近，大型语言模型(LLMs)成为研究人员、企业和消费者关注的焦点。虽然这类模型的语言能力已经得到了广泛的研究，但对它们作为认知主体的调查越来越受关注。在本研究中，我对GPT-3和ChatGPT在一个来自认知科学文献的有限数据归纳推理任务上的能力进行了研究。结果表明，这些模型的认知判断与人类不同。

    Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
    
[^112]: 相关实体选择：通过零样本类比修剪进行知识图谱引导

    Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])

    [http://arxiv.org/abs/2306.16296](http://arxiv.org/abs/2306.16296)

    本文提出了一种基于类比的方法，通过选择和修剪相关实体，从而引导知识图谱的构建。实证结果显示，这种方法在两个领域和异质种子实体的数据集上优于其他机器学习方法，并具有较低的参数数量。这些结果支持在相关任务中进一步应用类比推理。

    

    知识图谱构建可以被视为一个迭代过程，从高质量的核心开始，通过知识提取方法不断改进。这样的核心可以从像Wikidata这样的开放式知识图谱中获得。然而，由于这种通用知识图谱的规模，将其作为整体集成可能会包含无关内容和可扩展性问题。我们提出了一种基于类比的方法，从通用知识图谱中的感兴趣种子实体开始，并保留或修剪其相邻实体。我们通过两个手动标记的数据集 在Wikidata上评估了我们的方法，这些数据集包含领域同质或异质的种子实体。我们从实证上证明了我们的基于类比的方法优于LSTM，随机森林，支持向量机和多层感知器，且参数数量大大减少。我们还在迁移学习环境中评估了其泛化能力。这些结果对于进一步将基于类比的推理集成到相关任务中提供了支持。

    Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
    
[^113]: 渐近保持的卷积Deep Operator网络捕捉多尺度线性输运方程的扩散行为

    Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations. (arXiv:2306.15891v1 [cs.LG])

    [http://arxiv.org/abs/2306.15891](http://arxiv.org/abs/2306.15891)

    本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），用于解决多尺度时变线性输运问题。该方法通过采用多个局部卷积操作、池化和激活操作来捕捉线性输运问题的扩散行为，并验证了方法的有效性。

    

    本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），旨在解决多尺度时变线性输运问题。我们观察到，使用修改过的多层感知器（MLP）的基本物理约束DeepONets可能在保持期望的宏观行为上表现出不稳定性。因此，需要使用渐近保持的损失函数。受扩散方程中的热核的启发，我们提出了一种新的架构，称为卷积Deep Operator网络，它在每个滤波器层中采用多个局部卷积操作而不是全局热核，并结合池化和激活操作。我们的APCON方法的参数数量与网格大小无关，并能够捕捉线性输运问题的扩散行为。最后，通过几个数值实例验证了我们方法的有效性。

    In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to address the multiscale time-dependent linear transport problem. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employ multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.
    
[^114]: 基于任务条件化超网络的多任务记忆深度强化学习

    Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10698](http://arxiv.org/abs/2306.10698)

    人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。

    

    深度强化学习算法通常受到采样效率低下的限制，严重依赖与环境的多次交互才能获得准确的决策能力。相比之下，人类似乎依赖海马体从过去有关任务的经历中检索相关信息，在学习新任务时指导其决策，而不是仅仅依赖于环境交互。然而，为代理设计类似海马体的模块以将过去的经历融入既有的强化学习算法面临两个挑战。第一个挑战涉及选择当前任务最相关的过去经验，第二个是将这些经验与决策网络相结合。为了解决这些问题，我们提出了一种新算法，利用基于任务条件化超网络的检索网络，根据任务调整检索网络的参数。

    Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
    
[^115]: 潜在动态隐式扩散过程

    Latent Dynamical Implicit Diffusion Processes. (arXiv:2306.07077v1 [cs.LG])

    [http://arxiv.org/abs/2306.07077](http://arxiv.org/abs/2306.07077)

    本文提出了一种新型的潜在变量模型 LDIDPs，利用隐式扩散过程从动态潜在过程中进行采样，然后生成相应的顺序观察样本，相较于最先进的顺序生成模型有更好的性能。

    

    潜在动态模型常被用来学习代表一系列噪声数据样本的潜在动态过程的分布。然而，由于潜在的和观测动态的复杂性和变异性，产生具有高保真度的样本具有挑战性。最近，在基于扩散的生成模型（例如DDPM和NCSN）方面取得的进展，展示了一些有前景的替代方法，适用于从先验分布中生成高质量的序列样本，相较于先进的潜在生成模型（如神经ODE、RNN和正则化流网络）。然而，将它们应用于建模具有潜在动态模型的序列数据尚未被探索。因此，本文提出了一种名为潜在动态隐式扩散过程（LDIDPs）的新型潜在变量模型，利用隐式扩散过程从动态潜在过程中进行采样，然后生成相应的顺序观察样本。我们在合成和模拟神经数据上测试了LDIDPs，并证明它优于最先进的顺序生成模型。

    Latent dynamical models are commonly used to learn the distribution of a latent dynamical process that represents a sequence of noisy data samples. However, producing samples from such models with high fidelity is challenging due to the complexity and variability of latent and observation dynamics. Recent advances in diffusion-based generative models, such as DDPM and NCSN, have shown promising alternatives to state-of-the-art latent generative models, such as Neural ODEs, RNNs, and Normalizing flow networks, for generating high-quality sequential samples from a prior distribution. However, their application in modeling sequential data with latent dynamical models is yet to be explored. Here, we propose a novel latent variable model named latent dynamical implicit diffusion processes (LDIDPs), which utilizes implicit diffusion processes to sample from dynamical latent processes and generate sequential observation samples accordingly. We tested LDIDPs on synthetic and simulated neural d
    
[^116]: 基于四分位数的季节性分解用于时间序列预测和异常检测

    Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection. (arXiv:2306.05989v1 [cs.LG])

    [http://arxiv.org/abs/2306.05989](http://arxiv.org/abs/2306.05989)

    本文提出了一种名为QBSD的实时预测方法，以在时间序列异常检测中取得最佳平衡。

    

    在电信领域，及时检测异常非常重要，因为这有助于识别和表征不规则模式、异常行为和网络异常，从而提高服务质量和操作效率。精确地预测和消除可预测的时间序列模式是时间序列异常检测的重要组成部分。本文提出了一种名为基于四分位数的季节性分解（QBSD）的实时预测方法，以在计算复杂度和预测准确率之间取得最佳平衡。本文比较了QBSD与现有预测方法的性能及其适用性。

    The timely detection of anomalies is essential in the telecom domain as it facilitates the identification and characterization of irregular patterns, abnormal behaviors, and network anomalies, contributing to enhanced service quality and operational efficiency. Precisely forecasting and eliminating predictable time series patterns constitutes a vital component of time series anomaly detection. While the state-of-the-art methods aim to maximize forecasting accuracy, the computational performance takes a hit. In a system composed of a large number of time series variables, e.g., cell Key Performance Indicators (KPIs), the time and space complexity of the forecasting employed is of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a live forecasting method proposed in this paper to make an optimal trade-off between computational complexity and forecasting accuracy. This paper compares the performance of QBSD to the state-of-the-art forecasting methods and their applic
    
[^117]: 过度压缩如何影响GNN的能力？

    How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])

    [http://arxiv.org/abs/2306.03589](http://arxiv.org/abs/2306.03589)

    本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是...

    

    图神经网络（GNN）是处理图结构数据的机器学习的最先进模型。最流行的GNN类别是通过相邻节点间的信息交换来操作的，称为消息传递神经网络（MPNN）。鉴于它们的广泛应用，了解MPNN的表达能力是一个关键问题。然而，现有结果通常考虑具有无信息节点特征的环境。在本文中，我们提供了一种严格的分析方法，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。我们通过测量MPNN允许的节点之间的成对交互水平来实现此目的。该测量提供了一种新的量化特性，即所谓的过度压缩效应，该效应被观察到是当大量的信息聚合成固定大小的向量时发生的。使用我们的测量，我们证明，为了保证节点对之间的充分通信，MPNN的容量必须是...

    Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
    
[^118]: Gode -- 将生物化学知识图谱集成到分子图神经网络的预训练中

    Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])

    [http://arxiv.org/abs/2306.01631](http://arxiv.org/abs/2306.01631)

    本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。

    

    分子属性的准确预测对于促进创新治疗方法的发展和理解化学物质和生物系统之间复杂的相互作用至关重要。本研究提出了一种新的方法，将单个分子结构的图表示与生物医学知识图谱 (KG) 的多个领域信息进行集成。通过集成两个级别的信息，我们可以使用自我监督策略预先训练更广泛和更强大的表示，用于分子级和 KG 级预测任务。在性能评估方面，我们在 11 个具有挑战性的化学属性预测任务上微调我们预先训练的模型。我们的框架的结果表明，我们微调的模型优于现有的最先进的模型。

    The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
    
[^119]: 超越元数据：利用游戏设计参数进行跨版本电子竞技分析

    Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])

    [http://arxiv.org/abs/2305.18477](http://arxiv.org/abs/2305.18477)

    本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。

    

    电子竞技游戏是全球游戏市场的重要组成部分，并且是增长最快的游戏细分领域。这导致了电子竞技分析的领域产生，其使用游戏提取的遥测数据来为玩家、教练、播音员和其他利益相关者提供信息。与传统的体育比赛相比，电子竞技游戏的机制和规则经常发生快速变化。由于游戏参数的频繁更改，电子竞技分析模型的使用寿命可能很短，这在文献中很大程度上被忽略了。本文提取游戏设计信息（即补丁说明），利用聚类技术提出了一种新的角色表征形式。以Dota 2游戏中击杀次数的预测为案例，利用这种创新的角色表征技术训练了一个神经网络模型。然后将此模型的性能与包括常规技术在内的两个不同基线进行了评估。这个模型不仅达到了显著的表现水平，还克服了电子竞技游戏中版本更迭的困境。

    Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
    
[^120]: 可解释机器学习在类别和混合数据上的应用：无损可视化

    Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v1 [cs.LG])

    [http://arxiv.org/abs/2305.18437](http://arxiv.org/abs/2305.18437)

    本文提出了一些数值编码和可视化方法，以支持机器学习算法处理混合数据，并提出了可解释的多分类模型和SRG算法来生成解释性分类模型。

    

    为混合数据构建准确可解释的机器学习模型一直是算法面对非数值数据的挑战。本文提出了数值编码方案和无损可视化方法，为机器学习算法支持准确可解释的模型，提出了可解释的多分类模型和演示其重要作用。论文提出了一种分类混合数据类型的方法，并提出了一种工具包，以对混合数据的所有内部操作实现可解释性。论文还提出了一种新的“顺序规则生成（SRG）”算法，用于生成可解释的分类模型，并在多个计算实验中成功评估该算法。

    Building accurate and interpretable Machine Learning (ML) models for heterogeneous/mixed data is a long-standing challenge for algorithms designed for numeric data. This work focuses on developing numeric coding schemes for non-numeric attributes for ML algorithms to support accurate and explainable ML models, methods for lossless visualization of n-D non-numeric categorical data with visual rule discovery in these visualizations, and accurate and explainable ML models for categorical data. This study proposes a classification of mixed data types and analyzes their important role in Machine Learning. It presents a toolkit for enforcing interpretability of all internal operations of ML algorithms on mixed data with a visual data exploration on mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable rule generation with categorical data is proposed and successfully evaluated in multiple computational experiments. This work is one of the steps to the full scope ML alg
    
[^121]: 在表格数据上进行深度异常检测的超越个体输入

    Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15121](http://arxiv.org/abs/2305.15121)

    本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。

    

    异常检测在金融、医疗和网络安全等各个领域都至关重要。在本文中，我们提出了一种新颖的基于非参数转换器（NPTs）的表格数据深度异常检测方法，以捕捉特征与特征之间以及样本与样本之间的依赖关系。在基于重构的框架中，我们训练NPT来重构正常样本的遮蔽特征。以非参数化方式，在推理过程中利用整个训练集，并利用模型在生成异常得分时重构遮蔽特征的能力。据我们所知，我们提出的方法是首个成功结合特征之间和样本之间依赖关系进行表格数据异常检测的方法。我们在31个表格数据集的广泛基准测试中评估了我们的方法，并证明我们的方法在F1得分和AUROC方面优于现有的最先进方法。

    Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
    
[^122]: 不安全扩散：文本转图像模型生成不安全图像和令人憎恶的模因

    Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. (arXiv:2305.13873v1 [cs.CV])

    [http://arxiv.org/abs/2305.13873](http://arxiv.org/abs/2305.13873)

    本文研究揭示了文本转图像模型生成不安全图像和令人憎恶的模因，并且发现这些模型可以生成相当大比例的不安全图像。作者鉴定了一些文本提示因素和模型倾向因素，以揭示不安全内容的生成机理，并且凸显了需要继续研究的必要性。

    

    Stable Diffusion和DALLE·2等最新的文本转图像模型正在彻底改变人们的视觉内容生成方式。同时，社会对对手如何利用这些模型生成不安全图像和令人担忧的模因存在严重的担忧。本研究着眼于揭示文本转图像模型生成不安全图像和令人憎恶的模因。首先，我们构建了一个五种类别的不安全图像分类法(性暴力、暴力、令人不安、令人憎恶和政治)，然后我们使用四个提示数据集评估了四种先进的文本转图像模型生成的不安全图像比例。我们发现这些模型可以生成相当大比例的不安全图像；在四个模型和四个提示数据集中，所有生成的图像中有14.56%是不安全的。在比较这四种模型时，我们发现存在不同的风险水平，其中Stable Diffusion是生成不安全内容最容易的(所有生成的图像中有18.92%是不安全的)。鉴于Stable Diffusion的流行和不安全图像的有害影响，我们进行了对Stable Diffusion的深入分析，以揭示其生成不安全图像的倾向因素。具体而言，我们确定了一些经常导致生成不安全图像的文本提示，以及模型生成某些类型内容的倾向。我们的分析强调了需要继续研究从文本到图像模型的不安全图像生成以及开发强有力的对策的必要性。

    State-of-the-art Text-to-Image models like Stable Diffusion and DALLE$\cdot$2 are revolutionizing how people generate visual content. At the same time, society has serious concerns about how adversaries can exploit such models to generate unsafe images. In this work, we focus on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. We first construct a typology of unsafe images consisting of five categories (sexually explicit, violent, disturbing, hateful, and political). Then, we assess the proportion of unsafe images generated by four advanced Text-to-Image models using four prompt datasets. We find that these models can generate a substantial percentage of unsafe images; across four models and four prompt datasets, 14.56% of all generated images are unsafe. When comparing the four models, we find different risk levels, with Stable Diffusion being the most prone to generating unsafe content (18.92% of all generated images are unsafe). Given Stable 
    
[^123]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^124]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^125]: CUTS+：基于非规则时间序列的高维因果发现

    CUTS+: High-dimensional Causal Discovery from Irregular Time-series. (arXiv:2305.05890v1 [cs.LG])

    [http://arxiv.org/abs/2305.05890](http://arxiv.org/abs/2305.05890)

    CUTS+是一种基于Granger因果和图神经网络(MPGNN)的因果发现算法，通过引入粗到细发现（C2FD）技术提高可扩展性。实验结果表明，CUTS+在非规则高维数据上的因果发现性能大幅度提高。

    

    时间序列中的因果关系发现是机器学习领域中一个根本性的问题，可以在复杂情境下进行因果推理和决策制定。最近，研究人员成功地通过将神经网络与 Granger 因果相结合来发现因果关系，但是当遇到高维数据时，由于高度冗余的网络设计和庞大的因果图，它们的性能会严重下降。此外，观察值中的缺失条目进一步阻碍了因果结构学习。为了克服这些限制，我们提出 CUTS+，它建立在基于 Granger 因果的因果发现方法 CUTS 上，并通过引入一种称为粗到细发现（C2FD）的技术和利用基于消息传递的图神经网络（MPGNN）提高了可扩展性。与之前的方法在模拟、准实际和真实数据集上相比，我们展示了 CUTS+ 在不同类型的非规则高维数据上大大提高了因果发现的性能。

    Causal discovery in time-series is a fundamental problem in the machine learning community, enabling causal reasoning and decision-making in complex scenarios. Recently, researchers successfully discover causality by combining neural networks with Granger causality, but their performances degrade largely when encountering high-dimensional data because of the highly redundant network design and huge causal graphs. Moreover, the missing entries in the observations further hamper the causal structural learning. To overcome these limitations, We propose CUTS+, which is built on the Granger-causality-based causal discovery method CUTS and raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN). Compared to previous methods on simulated, quasi-real, and real datasets, we show that CUTS+ largely improves the causal discovery performance on high-dimensional data with different types of irregula
    
[^126]: Echoes: 基于伪偏差标记的模仿式回声室无监督去偏

    Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])

    [http://arxiv.org/abs/2305.04043](http://arxiv.org/abs/2305.04043)

    Echoes提出了一种无监督的去偏方法，生成偏差对立样本的伪偏差标签，实现了对数据集中偏差特征的一致性处理，并取得了各项任务和数据集上的最先进性能。

    

    当神经网络暴露于有偏训练数据时，通常会学习到不正确的相关性，从而导致在拓展领域数据上表现不佳。本文提出一种名为“Echoes”的简单高效方法，它生成偏差对立样本的伪偏差标签，以强制使伪标签与数据集中的偏差特征一致，并用于去偏。我们的实证研究表明，Echoes实现了各项任务和数据集上的最先进性能，同时使用比以前的方法更少的计算资源。

    Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
    
[^127]: 医学影像中的人口统计学不变模型和表示是否公平？

    Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])

    [http://arxiv.org/abs/2305.01397](http://arxiv.org/abs/2305.01397)

    医学影像模型编码患者人口统计信息，引发有关潜在歧视的担忧。研究表明，不编码人口属性的模型容易损失预测性能，而考虑人口统计属性的反事实模型不变性存在复杂性。人口统计学编码可以被认为是优势。

    

    研究表明，医学成像模型在其潜在表示中编码了有关患者人口统计学信息（年龄、种族、性别），这引发了有关其潜在歧视的担忧。在这里，我们询问是否可行和值得训练不编码人口属性的模型。我们考虑不同类型的与人口统计学属性的不变性，即边际、类条件和反事实模型不变性，并说明它们与算法公平的标准概念的等价性。根据现有理论，我们发现边际和类条件的不变性可被认为是实现某些公平概念的过度限制方法，导致显著的预测性能损失。关于反事实模型不变性，我们注意到对于人口统计学属性，定义医学图像反事实存在复杂性。最后，我们认为人口统计学编码甚至可以被认为是优势。

    Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
    
[^128]: DuETT: 双重事件时间变换器用于电子病历

    DuETT: Dual Event Time Transformer for Electronic Health Records. (arXiv:2304.13017v1 [cs.LG])

    [http://arxiv.org/abs/2304.13017](http://arxiv.org/abs/2304.13017)

    DuETT是一个用于EHR的双重事件时间变换器，通过双重注意机制学习不同上下文中相同时间步的不同表示，避免由于时间步伐大而产生的二次放缩问题，并在四个基准EHR数据集上优于以往的单一任务最先进方法。

    

    医院设置中记录的电子病历（EHR）通常包含广泛的数字时间序列数据，其特征是高稀疏性和不规则观察。这些数据的有效建模必须利用其时间序列特性，不同类型观察之间的语义关系以及数据中稀疏性结构中的信息。自监督变压器在NLP和计算机视觉中的各种结构化任务中表现出了出色的性能。但是，多元时间序列数据包含两个维度上的结构化关系：时间和记录的事件类型，而直接将变压器应用于时间序列数据则不能利用这种独特的结构。自我注意力层的二次放缩还可以显着限制输入序列长度，而没有适当的输入工程。我们引入了DuETT架构，这是变压器的扩展，设计用于在EHR的时间和事件类型维度上关注。DuETT使用双重注意机制，它可以在不同上下文中学习相同时间步的不同表示。通过引入具有内核方法的自我注意力，DuETT避免了自我注意力的二次放缩，并过滤掉不相关的时间步骤。DuETT在四个基准EHR数据集上优于以前的单一任务最先进方法。

    Electronic health records (EHRs) recorded in hospital settings typically contain a wide range of numeric time series data that is characterized by high sparsity and irregular observations. Effective modelling for such data must exploit its time series nature, the semantic relationship between different types of observations, and information in the sparsity structure of the data. Self-supervised Transformers have shown outstanding performance in a variety of structured tasks in NLP and computer vision. But multivariate time series data contains structured relationships over two dimensions: time and recorded event type, and straightforward applications of Transformers to time series data do not leverage this distinct structure. The quadratic scaling of self-attention layers can also significantly limit the input sequence length without appropriate input engineering. We introduce the DuETT architecture, an extension of Transformers designed to attend over both time and event type dimensio
    
[^129]: 一种可扩展的序列转移优化问题生成器

    A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])

    [http://arxiv.org/abs/2304.08503](http://arxiv.org/abs/2304.08503)

    STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。

    

    近年来，序列转移优化(STO)受到越来越多的研究关注，旨在利用储存在数据库中以前求解的优化任务的知识来提高优化性能。然而，尽管算法设计已有重大进展，但STO中的测试问题设计并不完善。它们往往是由其他基准函数随机组合而成，这些基准函数具有相同的最佳值，或者生成自表现出有限变化的实际问题。这些问题中源任务和目标任务的最优解之间的关系是手动配置的，因此单调，限制了它们表征真实问题多样化关系的能力。因此，许多算法在这些问题上取得的有前途的结果具有高度的偏见，并且难以推广到其他问题。鉴于此，我们首先引入了一些表征STO问题的基本概念。

    Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
    
[^130]: 基于Wasserstein距离的分布鲁棒方法实现后悔最优控制

    A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance. (arXiv:2304.06783v1 [math.OC])

    [http://arxiv.org/abs/2304.06783](http://arxiv.org/abs/2304.06783)

    提出了一种基于Wasserstein距离的分布鲁棒方法实现后悔最优控制的控制器设计策略。

    

    本文提出了一种基于分布鲁棒方法的离散时间线性动态系统的后悔最优控制策略，该系统受到随机加性扰动影响，成本为二次型。扰动过程的概率分布未知，但假定位于给定的分布球内，定义为二阶Wasserstein距离。在该框架下，设计具有严格因果线性扰动反馈控制器，以最小化最坏情况下的预期后悔。控制器所产生的后悔是指它对抗扰动时产生的成本与最优非因果控制器在一开始就有完全了解扰动过程后产生的成本之差。建立在最优输运问题的一个很好的对偶理论基础上，我们展示了如何等价地改写这个极小极大的后悔最优控制问题为一个可行的半定规划问题。

    This paper proposes a distributionally robust approach to regret optimal control of discrete-time linear dynamical systems with quadratic costs subject to stochastic additive disturbance on the state process. The underlying probability distribution of the disturbance process is unknown, but assumed to lie in a given ball of distributions defined in terms of the type-2 Wasserstein distance. In this framework, strictly causal linear disturbance feedback controllers are designed to minimize the worst-case expected regret. The regret incurred by a controller is defined as the difference between the cost it incurs in response to a realization of the disturbance process and the cost incurred by the optimal noncausal controller which has perfect knowledge of the disturbance process realization at the outset. Building on a well-established duality theory for optimal transport problems, we show how to equivalently reformulate this minimax regret optimal control problem as a tractable semidefini
    
[^131]: RD-DPP: 码率-失真理论与分布式点过程相结合，实现多样化学习数据样本

    RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples. (arXiv:2304.04137v1 [cs.LG])

    [http://arxiv.org/abs/2304.04137](http://arxiv.org/abs/2304.04137)

    该论文介绍了一种基于码率-失真理论，结合分布式点过程的方法，用于多级分类中选择多样化的学习数据样本，相比现有方法具有更好的性能表现。

    

    在一些实际的学习任务中，如交通视频分析，可用训练样本的数量受到不同因素的限制，如有限的通信带宽和计算能力。因此，选择对学习系统质量做出最大贡献的多样化数据样本是至关重要的。选择多样化样本的一种流行方法是分布式点过程 (DPP)。然而，DPP存在一些已知的缺点，例如将样本数量限制为相似性矩阵的秩，并且不能为特定的学习任务（例如多级分类任务）定制。本文提出了一种基于码率-失真 (RD) 理论的衡量任务定向多样性的新方法，适用于多级分类。为此，我们建立了DPP和RD理论之间的基本关系，设计了RD-DPP，一种基于RD的价值函数，用于评估数据样本的多样性增益。我们还得到了一些特定情况下RD-DPP的闭合解，并在合成和真实数据集上进行了实验，表明我们所提出的方法在多样性优化方面的表现优于现有方法。

    In some practical learning tasks, such as traffic video analysis, the number of available training samples is restricted by different factors, such as limited communication bandwidth and computation power; therefore, it is imperative to select diverse data samples that contribute the most to the quality of the learning system. One popular approach to selecting diverse samples is Determinantal Point Process (DPP). However, it suffers from a few known drawbacks, such as restriction of the number of samples to the rank of the similarity matrix, and not being customizable for specific learning tasks (e.g., multi-level classification tasks). In this paper, we propose a new way of measuring task-oriented diversity based on the Rate-Distortion (RD) theory, appropriate for multi-level classification. To this end, we establish a fundamental relationship between DPP and RD theory, which led to designing RD-DPP, an RD-based value function to evaluate the diversity gain of data samples. We also ob
    
[^132]: 深度神经网络的保体系结构可证明修复方法

    Architecture-Preserving Provable Repair of Deep Neural Networks. (arXiv:2304.03496v1 [cs.LG])

    [http://arxiv.org/abs/2304.03496](http://arxiv.org/abs/2304.03496)

    本文提出了一种保体系结构 V-多面体可证明修复深度神经网络的方法。修复只修改 DNN 的参数，具有灵活性，支持多种类型的层，并在多项式时间内运行。

    

    深度神经网络（DNNs）成为了软件中越来越重要的组成部分，并被认为是解决许多问题（如图像识别）的最先进解决方案。然而，DNN 远非不可错误，DNN 的不正确行为可能会在现实世界中造成灾难性后果。本文解决了保体系结构 V-多面体可证明修复 DNNs 的问题。V-多面体使用其顶点表示法定义了一个凸约束多面体。V-多面体可证明修复保证修复后的 DNN 满足给定 V-多面体中无限点集上的规范。体系结构保持修复仅修改 DNN 的参数，而不修改其体系结构。修复有灵活性，可以修改 DNN 的多个层，并在多项式时间内运行。它支持具有一些线性部分的激活函数，以及完全连接的、卷积的、池化的和残余层的 DNNs。据我们所知，这是第一篇提供 DNN 体系结构保持可证明修复的正式框架的论文。

    Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our 
    
[^133]: ERM++：用于域通用性的改进基准方法

    ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])

    [http://arxiv.org/abs/2304.01973](http://arxiv.org/abs/2304.01973)

    ERM++是一个用于域通用性的改进基准方法，通过更好地利用训练数据、模型参数选择和权重空间正则化等关键技术，在多个数据集上比标准ERM更有效，同时计算复杂度更低，表现也优于最先进方法。

    

    多源域通用性（DG）衡量分类器对于它没有接受过训练的新数据分布的泛化能力，并考虑了多个训练域。虽然已经提出了几种多源DG方法，但是它们在训练过程中使用域标签增加了额外的复杂性。最近的研究表明，经过良好调整的经验风险最小化（ERM）训练过程，即在源域上简单地最小化经验风险，可以胜过大多数现有的DG方法。我们确定了几个关键候选技术，以进一步提高ERM的性能，例如更好地利用训练数据、模型参数选择和权重空间正则化。我们将结果称为ERM ++，并展示它相对于标准ERM在五个多源数据集上将DG的性能显着提高了5％以上，并且尽管计算复杂度更低，但击败了最先进的方法。此外，我们还证明了ERM ++在WILDS-FMOW数据集上的有效性。

    Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
    
[^134]: 视觉-语言模型的黑匣子少样本适应

    Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])

    [http://arxiv.org/abs/2304.01752](http://arxiv.org/abs/2304.01752)

    本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。

    

    通过对比学习训练的视觉-语言模型在少样本情况下表现出很强的学习能力。软提示学习是少样本领域适用的最受欢迎的方法，旨在通过新领域引发的分布偏移来缩小模态差距。虽然该方法性能高效，但仍需要访问模型权重，并且在具有数十亿个参数的大型模型上可能会导致计算上的不可行性。本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的 V-L 少样本适应，不需要访问模型权重，训练速度快数个数量级，适用于有监督和无监督训练，并且还可以用于对单模型计算的图像和文本特征进行对齐。

    Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
    
[^135]: 用于商用设备真实射频指纹的蓝牙和WiFi数据集

    Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices. (arXiv:2303.13538v1 [cs.NI])

    [http://arxiv.org/abs/2303.13538](http://arxiv.org/abs/2303.13538)

    该论文捕获了首个公开可访问的商用设备真实射频指纹数据集，这对于识别非法或未授权发射器具有重要意义。

    

    射频指纹作为一种物理层安全方案，可用于识别在共享RF频谱的非法或未授权发射器。然而，由于缺乏公开可访问的真实世界数据集，大多数研究都集中在使用软件定义无线电（SDR）生成合成波形，这不适用于实际部署环境。另一方面，现有的有限数据集仅关注生成一种波形的芯片组。商用现成（COTS）组合芯片组支持使用共享双频段天线的两种无线标准（例如WiFi和蓝牙），如在笔记本电脑、适配器、无线充电器、树莓派等IoT设备中广泛使用。因此，为跟上现代IoT环境的步伐，迫切需要捕获这些组合芯片组发射异构通信协议的真实世界开放数据集。为此，我们捕获了第一个已知的此类公开可访问数据集，其中包括来自各种商业设备的COTS Wi-Fi和蓝牙组合芯片组的RF发射。

    RF fingerprinting is emerging as a physical layer security scheme to identify illegitimate and/or unauthorized emitters sharing the RF spectrum. However, due to the lack of publicly accessible real-world datasets, most research focuses on generating synthetic waveforms with software-defined radios (SDRs) which are not suited for practical deployment settings. On other hand, the limited datasets that are available focus only on chipsets that generate only one kind of waveform. Commercial off-the-shelf (COTS) combo chipsets that support two wireless standards (for example WiFi and Bluetooth) over a shared dual-band antenna such as those found in laptops, adapters, wireless chargers, Raspberry Pis, among others are becoming ubiquitous in the IoT realm. Hence, to keep up with the modern IoT environment, there is a pressing need for real-world open datasets capturing emissions from these combo chipsets transmitting heterogeneous communication protocols. To this end, we capture the first kno
    
[^136]: 文本到图像扩散模型中的概念消融

    Ablating Concepts in Text-to-Image Diffusion Models. (arXiv:2303.13516v1 [cs.CV])

    [http://arxiv.org/abs/2303.13516](http://arxiv.org/abs/2303.13516)

    本论文提出了一种有效的方法，在不重新训练模型的情况下实现了预训练模型中的概念消融，可以消除文本到图像生成中的版权问题和样本记忆问题。

    

    大规模的文本到图像扩散模型具有强大的组合能力，可以生成高保真度的图片。然而，这些模型通常需要在数量庞大的网络数据上进行训练，往往包含有版权材料、授权图像和个人照片。此外，这些模型已经被发现能够模仿不同艺术家的风格或记住准确的训练样本。如何在不重新训练模型的情况下去除这些版权概念或图像？为了达成这个目标，我们提出了一种有效的方法，在预训练模型中实现概念消融，即防止生成目标概念。我们的算法学习如何匹配一个锚定概念对应的图像分布和与目标风格、实例或文本提示相关的图像分布，以防止模型根据其文本条件生成目标概念。广泛的实验证明，我们的方法可以成功地防止消融概念的生成。

    Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated conce
    
[^137]: 伪监督度量：在无监督跨域分类框架中评估无监督图像翻译模型

    Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])

    [http://arxiv.org/abs/2303.10310](http://arxiv.org/abs/2303.10310)

    本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。

    

    图像分类的准确性和高效性取决于访问大型标记数据集并在模型训练的相同领域上测试数据。当处理来自不同领域的新数据时，分类变得更加具有挑战性，因为收集大型标记数据集并从头训练新分类器耗时、昂贵，有时是不可行或不可能的。跨域分类框架通过利用无监督图像对图像 (UI2I) 翻译模型将输入图像从未标记的域转换为标记域来处理这个数据域漂移问题。这些无监督模型的问题在于它们是无监督的。由于缺少注释，无法使用传统的监督度量来评估这些翻译模型以选择最佳的检查点模型。在本文中，我们介绍了一种新的方法，称为伪监督度量，专门用于评估无监督跨域分类框架中 UI2I 翻译模型的性能。我们通过对几个基准数据集的实验证明了我们的方法的有效性。

    The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
    
[^138]: 使用集成的文本到梅尔频谱生成器的无标记转写领域自适应端到端ASR

    Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator. (arXiv:2302.14036v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.14036](http://arxiv.org/abs/2302.14036)

    本文提出了一种使用集成文本到梅尔频谱生成器的无标记转写领域自适应端到端ASR系统。该系统可以在训练过程中动态生成梅尔频谱，并通过使用新领域的仅文本数据来适应ASR模型。研究结果表明，所提出的训练方法显著提高了ASR准确性，并在自适应质量和训练速度上超过了级联TTS系统与声码器。

    

    我们提出了一个端到端的自动语音识别（ASR）系统，可以用转录的语音数据、仅有文本的数据或者二者的混合数据进行训练。所提出的模型使用了一种集成的文本基础训练辅助模块。该模块结合了非自回归的多说话人文本到梅尔频谱生成器和基于GAN的增强器，以提高频谱质量。该系统可以在训练过程中动态生成梅尔频谱。通过使用该新领域的仅文本数据，可以将ASR模型适应到新领域。我们证明了所提出的训练方法与仅训练于转录语音的系统相比显著提高了ASR准确性。它还在自适应质量和训练速度上超过了级联TTS系统与声码器。

    We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both. The proposed model uses an integrated auxiliary block for text-based training. This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram generator with a GAN-based enhancer to improve the spectrogram quality. The proposed system can generate a mel-spectrogram dynamically during training. It can be used to adapt the ASR model to a new domain by using text-only data from this domain. We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only. It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed.
    
[^139]: 具有生成式NeRF的3D感知融合

    3D-aware Blending with Generative NeRFs. (arXiv:2302.06608v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.06608](http://arxiv.org/abs/2302.06608)

    这篇论文提出了一种使用生成式NeRF的3D感知融合方法，通过3D感知对齐和融合来解决输入图像不对齐的问题，该方法在FFHQ和AFHQ-Cat上验证了优于现有2D方法的性能。

    

    图像融合旨在无缝地合并多个图像。对于现有的基于2D的方法来说，如果输入图像由于3D相机姿态和物体形状的差异而不对齐，仍然具有挑战性。为了解决这些问题，我们提出了一种使用生成式神经辐射场（NeRF）的3D感知融合方法，包括两个关键组件：3D感知对齐和3D感知融合。对于3D感知对齐，我们首先估计与生成式NeRF相关的参考图像的相机姿态，然后对每个部分进行3D局部对齐。为了进一步利用生成式NeRF的3D信息，我们提出了基于3D感知的融合，它直接在NeRF的潜在表示空间上进行图像融合，而不是在原始像素空间上进行。通过对FFHQ和AFHQ-Cat进行广泛的定量和定性评估，我们的方法优于现有的2D基线。

    Image blending aims to combine multiple images seamlessly. It remains challenging for existing 2D-based methods, especially when input images are misaligned due to differences in 3D camera poses and object shapes. To tackle these issues, we propose a 3D-aware blending method using generative Neural Radiance Fields (NeRF), including two key components: 3D-aware alignment and 3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of the reference image with respect to generative NeRFs and then perform 3D local alignment for each part. To further leverage 3D information of the generative NeRF, we propose 3D-aware blending that directly blends images on the NeRF's latent representation space, rather than raw pixel space. Collectively, our method outperforms existing 2D baselines, as validated by extensive quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.
    
[^140]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^141]: Box$^2$EL: EL++描述逻辑中的概念和角色盒子嵌入的概念和角色盒子嵌入方法及其作用

    Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11118](http://arxiv.org/abs/2301.11118)

    Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。

    

    描述逻辑本体论扩展了知识图谱与概念信息和逻辑背景知识。近年来，人们对这种本体论的归纳推理技术越来越感兴趣，这些技术有望补充传统的演绎推理算法。类似于知识图谱的完善，现有的一些方法通过在潜在空间中学习本体论嵌入，同时确保这些嵌入能够准确地捕捉到底层描述逻辑的逻辑语义。然而，它们存在一些问题，主要是由于受限的角色表示。我们提出了Box$^2$EL方法，将概念和角色都表示为盒子（即轴对齐超矩形），并展示了它如何克服之前方法的局限性。我们在理论上证明了我们模型的正确性，并进行了大量的实验评估，在各种数据集上取得了领先的结果。作为我们评估的一部分，我们引入了一个新的基准。

    Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
    
[^142]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^143]: 考虑实际实施挑战的同步全系统状态估计

    Time-Synchronized Full System State Estimation Considering Practical Implementation Challenges. (arXiv:2212.01729v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.01729](http://arxiv.org/abs/2212.01729)

    本研究提出了一种基于深度神经网络的状态估计器（DeNSE），利用贝叶斯框架综合利用慢速但广泛存在的SCADA数据和快速但局部的PMU数据，实现了对整个系统的亚秒级状态估计。通过考虑实际挑战，如拓扑变化、非高斯测量噪声和错误数据检测与校正，证明了DeNSE方法的优越性。

    

    由于相量测量装置（PMUs）通常放置在最高电压的母线上，因此许多低电压级别的大规模电力系统无法被其观测到。这种观测缺失使得同步时间状态估计成为一个具有挑战性的问题。我们提出了一种基于深度神经网络的状态估计器（DeNSE），以解决这个问题。该DeNSE采用贝叶斯框架，通过间接结合来自慢时间尺度但普遍存在的监督控制与数据采集（SCADA）数据与快时间尺度但局部的PMU数据的推断，实现对整个系统的亚秒级情景感知。通过考虑拓扑变化、非高斯测量噪声和错误数据检测和校正，展示了所提方法的实际效用。使用IEEE 118-bus系统得到的结果表明了DeNSE在纯SCADA状态估计器、SCADA-PMU混合状态估计器和仅PMU线性状态估计器方面的优越性。

    As phasor measurement units (PMUs) are usually placed on the highest voltage buses, many lower voltage levels of the bulk power system are not observed by them. This lack of visibility makes time-synchronized state estimation of the full system a challenging problem. We propose a Deep Neural network-based State Estimator (DeNSE) to overcome this problem. The DeNSE employs a Bayesian framework to indirectly combine inferences drawn from slow timescale but widespread supervisory control and data acquisition (SCADA) data with fast timescale but local PMU data to attain sub-second situational awareness of the entire system. The practical utility of the proposed approach is demonstrated by considering topology changes, non-Gaussian measurement noise, and bad data detection and correction. The results obtained using the IEEE 118-bus system show the superiority of the DeNSE over a purely SCADA state estimator, a SCADA-PMU hybrid state estimator, and a PMU-only linear state estimator from a te
    
[^144]: 《VisNet、Transformer-M和预训练模型在OGB大规模挑战中的分子性质预测中的集成》

    An ensemble of VisNet, Transformer-M, and pretraining models for molecular property prediction in OGB Large-Scale Challenge @ NeurIPS 2022. (arXiv:2211.12791v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12791](http://arxiv.org/abs/2211.12791)

    本研究通过集成VisNet、Transformer-M和预训练模型，成功预测了给定分子的量子化学性质，取得了显著的性能提升。

    

    在这份技术报告中，我们提供了我们在OGB-LSC 2022图回归任务中的解决方案。该任务的目标是预测PCQM4Mv2数据集中给定分子的量子化学性质，即HOMO-LUMO能隙。在比赛中，我们设计了两种模型：Transformer-M-ViSNet（一种增强几何图神经网络）和Pretrained-3D-ViSNet（通过提取优化结构的几何信息进行预训练的ViSNet）。通过22种模型的集成，ViSNet团队在测试挑战集上实现了0.0723 eV的MAE，相较于去年比赛中最佳方法，错误减少了39.75%。

    In the technical report, we provide our solution for OGB-LSC 2022 Graph Regression Task. The target of this task is to predict the quantum chemical property, HOMO-LUMO gap for a given molecule on PCQM4Mv2 dataset. In the competition, we designed two kinds of models: Transformer-M-ViSNet which is an geometry-enhanced graph neural network for fully connected molecular graphs and Pretrained-3D-ViSNet which is a pretrained ViSNet by distilling geomeotric information from optimized structures. With an ensemble of 22 models, ViSNet Team achieved the MAE of 0.0723 eV on the test-challenge set, dramatically reducing the error by 39.75% compared with the best method in the last year competition.
    
[^145]: 解缠表示学习

    Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11695](http://arxiv.org/abs/2211.11695)

    解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。

    

    解缠表示学习（DRL）旨在学习一个能够识别和解缠可观测数据中隐藏因素的模型。将变化的潜在要素分离成具有语义意义的变量的过程有助于学习可解释的数据表示，模仿人类观察对象或关系时的有意义理解过程。作为一种通用的学习策略，DRL在多个领域中展示了提高模型可解释性、可控性、鲁棒性以及泛化能力的优势，如计算机视觉、自然语言处理、数据挖掘等。本文综合评述了DRL的各个方面，包括动机、定义、方法论、评估、应用和模型设计。我们讨论了基于两个公认定义（直观定义和群论定义）的DRL方法。我们进一步分析了DRL的开展。

    Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
    
[^146]: 高效训练：探索泛化课程学习来训练视觉主干网络

    EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones. (arXiv:2211.09703v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09703](http://arxiv.org/abs/2211.09703)

    本文提出了一种泛化课程学习方法，用于高效训练视觉主干网络，通过优先让模型学习“更容易学习”的模式，不断引入更难的模式，从而加速训练过程。

    

    现代深度网络的卓越性能通常伴随着昂贵的训练过程。本文提出了一种新的课程学习方法，用于高效训练视觉主干网络（例如视觉Transformer）。本文启发于深度网络的内在学习动力学：我们实验性地展示了在较早的训练阶段，模型主要学习在每个示例中识别一些“更容易学习”的判别模式，例如图像的低频成分和数据增广之前的原始信息。基于此现象，我们提出了一种课程，其中模型总是在每个时期利用所有训练数据，而课程始于仅暴露每个示例的“更容易学习”的模式，并逐渐引入更难的模式。为了实现这个想法，我们1）在输入的傅里叶谱中引入一个裁剪操作，使模型只能从低频组分中进行学习。

    The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo
    
[^147]: 插值深度卷积神经网络的学习能力

    Learning Ability of Interpolating Deep Convolutional Neural Networks. (arXiv:2210.14184v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14184](http://arxiv.org/abs/2210.14184)

    本文研究了深度卷积神经网络（DCNNs）在欠参数和过参数设置下的学习能力，建立了欠参数DCNNs的学习速度，并通过一种新颖的网络加深方案获得了插值DCNN，从而验证了过拟合的DCNN的泛化性能。

    

    高参数神经网络往往具有良好的泛化能力。现有的理论工作主要研究了线性设置或全连接神经网络的情况。本文研究了一类重要的深度神经网络，即深度卷积神经网络（DCNNs）在欠参数和过参数设置下的学习能力。我们在文献中首次建立了无参数或函数变量结构限制的欠参数DCNNs的学习速度。我们还表明，通过向非插值DCNN添加良定义的层，可以获得一些保持非插值DCNN良好学习速度的插值DCNN。这一结果是通过为DCNN设计的一种新颖的网络加深方案实现的。我们的工作在理论上验证了过拟合的DCNN如何良好地进行泛化。

    It is frequently observed that overparameterized neural networks generalize well. Regarding such phenomena, existing theoretical work mainly devotes to linear settings or fully-connected neural networks. This paper studies the learning ability of an important family of deep neural networks, deep convolutional neural networks (DCNNs), under both underparameterized and overparameterized settings. We establish the first learning rates of underparameterized DCNNs without parameter or function variable structure restrictions presented in the literature. We also show that by adding well-defined layers to a non-interpolating DCNN, we can obtain some interpolating DCNNs that maintain the good learning rates of the non-interpolating DCNN. This result is achieved by a novel network deepening scheme designed for DCNNs. Our work provides theoretical verification of how overfitted DCNNs generalize well.
    
[^148]: DyTed:离散时动态图的分离表示学习

    DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. (arXiv:2210.10592v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2210.10592](http://arxiv.org/abs/2210.10592)

    本论文提出了一种用于离散时动态图的分离表示学习框架DyTed，通过设计时间片对比学习任务和结构对比学习任务，有效地识别时间不变和时间变化的表示，并提出分离感知判别器以增强分离性能。

    

    近年来，无监督的动态图表示学习引起了很多研究关注。与静态图相比，动态图既体现了节点的内在稳定特征，又体现了与时间相关的动态偏好。然而，现有方法通常将这两种信息混合到一个表示空间中，这可能导致解释性差、鲁棒性差，并且在应用于不同的下游任务时能力有限。为了解决上述问题，本文提出了一种用于离散时动态图的新型分离表示学习框架DyTed。我们特别设计了一个时间片对比学习任务，并结合了结构对比学习，分别有效地识别时间不变和时间变化的表示。为了进一步增强这两种表示的分离性，我们提出了一个分离感知判别器...

    Unsupervised representation learning for dynamic graphs has attracted a lot of research attention in recent years. Compared with static graph, the dynamic graph is a comprehensive embodiment of both the intrinsic stable characteristics of nodes and the time-related dynamic preference. However, existing methods generally mix these two types of information into a single representation space, which may lead to poor explanation, less robustness, and a limited ability when applied to different downstream tasks. To solve the above problems, in this paper, we propose a novel disenTangled representation learning framework for discrete-time Dynamic graphs, namely DyTed. We specially design a temporal-clips contrastive learning task together with a structure contrastive learning to effectively identify the time-invariant and time-varying representations respectively. To further enhance the disentanglement of these two types of representation, we propose a disentanglement-aware discriminator unde
    
[^149]: ST-former在COVID-19期间用于短期城市轨道交通客流预测

    ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system. (arXiv:2210.09043v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09043](http://arxiv.org/abs/2210.09043)

    本文提出了一种名为ST-former的新型transformer架构，用于COVID-19期间的城市轨道交通客流预测。通过引入改进的自注意机制和自适应多图卷积网络，可以准确建模客流的时空依赖关系和复杂空间依赖关系。

    

    准确预测城市轨道交通的客流对于提高智能交通系统的性能尤为重要，尤其是在疫情期间。如何动态建模客流的复杂时空依赖关系是实现准确客流预测的主要问题。为了解决这个问题，本文提出了一种全新的基于transformer的架构，称为ST-former，专门用于COVID-19期间的客流预测。具体而言，我们开发了一种改进的自注意机制，命名为因果卷积ProbSparse自注意（CPSA），以低计算成本建模客流的多个时间依赖关系。为了捕捉复杂且动态的空间依赖关系，我们引入了一种新颖的自适应多图卷积网络（AMGCN），通过以自适应的方式利用多个图来进行建模。另外，多源数据融合模块将客流数据、COVID-19确诊病例等多种数据进行融合。

    Accurate passenger flow prediction of urban rail transit is essential for improving the performance of intelligent transportation systems, especially during the epidemic. How to dynamically model the complex spatiotemporal dependencies of passenger flow is the main issue in achieving accurate passenger flow prediction during the epidemic. To solve this issue, this paper proposes a brand-new transformer-based architecture called STformer under the encoder-decoder framework specifically for COVID-19. Concretely, we develop a modified self-attention mechanism named Causal-Convolution ProbSparse Self-Attention (CPSA) to model the multiple temporal dependencies of passenger flow with low computational costs. To capture the complex and dynamic spatial dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network (AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally, the Multi-source Data Fusion block fuses the passenger flow data, COVID-19 confirmed case
    
[^150]: 随机约束分布鲁棒优化算法的样本大小无关复杂度

    Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05740](http://arxiv.org/abs/2210.05740)

    本文提出了一种适用于非凸和凸损失函数的随机算法，用于解决Kullback Leibler散度约束的分布鲁棒优化问题，并且具有与样本大小无关的复杂度，每次迭代只需要恒定的批次大小。实证研究证明了该算法在解决非凸和凸约束DRO问题中的有效性。

    

    分布鲁棒优化(DRO)作为一种在训练集和测试集之间进行分布偏移训练鲁棒模型的流行方法，近年来受到了广泛关注。本文提出并分析了适用于解决Kullback Leibler散度约束DRO问题的随机算法，适用于非凸和凸损失函数。与现有的解决方法相比，我们的随机算法不仅具有与样本大小无关的竞争性甚至更好的复杂度，而且在每次迭代中只需要恒定的批次大小，这对于广泛应用更加实用。我们为非凸损失函数找到了一个$\epsilon$稳定解的近乎最优的复杂度界限，并为凸损失函数找到了一个$\epsilon$最优解的最优复杂度。实证研究证明了所提算法在解决非凸和凸约束DRO问题方面的有效性。

    Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.
    
[^151]: 数据选择：一种令人惊讶的有效且通用的构建小型可解释模型的原则。

    Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models. (arXiv:2210.03921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03921](http://arxiv.org/abs/2210.03921)

    数据选择是一种令人惊讶的有效且通用的构建小型可解释模型的策略，它通过学习训练分布而非测试分布的数据，提高了传统基准模型的准确性，并在多个任务中展现出竞争力。

    

    我们提供了令人信服的实证证据，证明了一种构建准确小型模型的有效且通用的策略。这种模型对于可解释性具有吸引力，并且在资源受限的环境中也有用途。该策略是学习训练分布而不是使用测试分布的数据。分布学习算法不是这项工作的贡献；我们强调这种简单策略在各种任务上的广泛适用性，并且基于严格的实证结果，这些结果是我们的贡献。我们将其应用于以下任务：（1）构建聚类解释树，（2）基于原型的分类，以及（3）使用随机森林进行分类，并且展示了它提高了弱传统基准的准确性，使它们令人惊讶地与专业的现代技术相竞争。此策略也适用于模型大小的概念。在前两个任务中，模型大小通过树中叶子节点的数量来确定。

    We present convincing empirical evidence for an effective and general strategy for building accurate small models. Such models are attractive for interpretability and also find use in resource-constrained environments. The strategy is to learn the training distribution instead of using data from the test distribution. The distribution learning algorithm is not a contribution of this work; we highlight the broad usefulness of this simple strategy on a diverse set of tasks, and as such these rigorous empirical results are our contribution. We apply it to the tasks of (1) building cluster explanation trees, (2) prototype-based classification, and (3) classification using Random Forests, and show that it improves the accuracy of weak traditional baselines to the point that they are surprisingly competitive with specialized modern techniques.  This strategy is also versatile wrt the notion of model size. In the first two tasks, model size is identified by number of leaves in the tree and th
    
[^152]: 带有服务器学习的联邦学习：提高非独立同分布数据的性能

    Federated Learning with Server Learning: Enhancing Performance for Non-IID Data. (arXiv:2210.02614v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02614](http://arxiv.org/abs/2210.02614)

    基于辅助学习的联邦学习可以显著提高在非独立同分布数据上的模型精度和收敛时间

    

    联邦学习（FL）已成为使用客户端存储的本地数据进行分布式学习的一种手段，其中协调服务器。最近的研究表明，当训练客户端数据不独立同分布时，FL可能会遭受性能下降和收敛缓慢的问题。在这里，我们考虑一种新的补充方法来减轻这种性能下降，即允许服务器从小数据集上执行辅助学习。我们的分析和实验表明，即使服务器数据集很小且其分布与所有客户端聚合数据不同，这种新方法也可以在模型精度和收敛时间方面实现显着的改进。

    Federated Learning (FL) has emerged as a means of distributed learning using local data stored at clients with a coordinating server. Recent studies showed that FL can suffer from poor performance and slower convergence when training data at clients are not independent and identically distributed. Here we consider a new complementary approach to mitigating this performance degradation by allowing the server to perform auxiliary learning from a small dataset. Our analysis and experiments show that this new approach can achieve significant improvements in both model accuracy and convergence time even when the server dataset is small and its distribution differs from that of the aggregated data from all clients.
    
[^153]: 图的扩散模型受益于离散状态空间

    Diffusion Models for Graphs Benefit From Discrete State Spaces. (arXiv:2210.01549v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01549](http://arxiv.org/abs/2210.01549)

    本论文提出了一种在生成离散图时使用离散噪声的方法，相比于之前的方法，实验证明使用离散噪声可以生成更高质量的样本，同时采样过程速度提高了30倍。

    

    去噪扩散概率模型和评分匹配模型已被证明在生成任务中非常强大。虽然这些方法也被应用于离散图的生成，但迄今为止，它们仍依赖于连续高斯扰动。相反，在本研究中，我们建议使用离散噪声进行前向马尔可夫过程。这确保在每个中间步骤中，图保持离散。与先前的方法相比，我们在四个数据集和多个架构上的实验结果显示，使用离散噪声处理过程生成的样本质量更高，平均 MMDs 降低了1.5倍。此外，去噪步骤的数量从1000减少到32步，导致采样过程速度提高了30倍。

    Denoising diffusion probabilistic models and score-matching models have proven to be very powerful for generative tasks. While these approaches have also been applied to the generation of discrete graphs, they have, so far, relied on continuous Gaussian perturbations. Instead, in this work, we suggest using discrete noise for the forward Markov process. This ensures that in every intermediate step the graph remains discrete. Compared to the previous approach, our experimental results on four datasets and multiple architectures show that using a discrete noising process results in higher quality generated samples indicated with an average MMDs reduced by a factor of 1.5. Furthermore, the number of denoising steps is reduced from 1000 to 32 steps, leading to a 30 times faster sampling procedure.
    
[^154]: 在社会科学中辅助AI发现定量和形式模型

    AI-Assisted Discovery of Quantitative and Formal Models in Social Science. (arXiv:2210.00563v3 [cs.SC] UPDATED)

    [http://arxiv.org/abs/2210.00563](http://arxiv.org/abs/2210.00563)

    该论文介绍了在社会科学研究中辅助AI发现定量和形式模型的方法。通过扩展神经符号方法，该系统能够从真实数据中发现可解释的非线性和动态关系，帮助科学家在研究过程中揭示新的关系和探索对照模型。

    

    在社会科学中，使用形式和定量模型，如描述经济增长和集体行动的模型，以制定机械解释，提供预测，并揭示有关观察到现象的问题。在这里，我们展示了使用机器学习系统辅助发现能够捕捉社会科学数据集中的非线性和动态关系的符号模型的方法。通过将神经符号方法扩展到嘈杂和纵向数据中找到简洁函数和微分方程，我们展示了我们的系统可以从经济学和社会学的真实数据中发现可解释的模型。通过将符号回归与现有工作流相结合，可以帮助发现新的关系，并在科学过程中探索对照模型。我们提出，这种AI辅助框架可以通过系统地探索非线性模型空间来桥接社会科学研究中常用的参数化和非参数化模型。

    In social science, formal and quantitative models, such as ones describing economic growth and collective action, are used to formulate mechanistic explanations, provide predictions, and uncover questions about observed phenomena. Here, we demonstrate the use of a machine learning system to aid the discovery of symbolic models that capture nonlinear and dynamical relationships in social science datasets. By extending neuro-symbolic methods to find compact functions and differential equations in noisy and longitudinal data, we show that our system can be used to discover interpretable models from real-world data in economics and sociology. Augmenting existing workflows with symbolic regression can help uncover novel relationships and explore counterfactual models during the scientific process. We propose that this AI-assisted framework can bridge parametric and non-parametric models commonly employed in social science research by systematically exploring the space of nonlinear models an
    
[^155]: 因果模型中的最优干预设计的主动学习

    Active Learning for Optimal Intervention Design in Causal Models. (arXiv:2209.04744v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.04744](http://arxiv.org/abs/2209.04744)

    本研究开发了一种因果主动学习策略，通过推断因果模型中的变量之间的关系以及干预对系统的影响，快速确定最优的干预措施，从而解决了在大干预空间下进行实验设计的问题。

    

    在科学、工程和公共政策等各个领域，顺序实验设计以发现实现期望结果的干预策略是一个关键问题。当可能的干预空间很大时，完全搜索是不可行的，需要使用实验设计策略。在这种情况下，编码变量之间的因果关系，特别是干预对系统的影响，对于更有效地确定理想的干预是至关重要的。在这里，我们开发了一种因果主动学习策略，通过后干预分布的均值与期望目标均值之间的差异来确定最优干预措施。该方法利用贝叶斯更新进行因果模型的建立，并使用精心设计的，基于因果关系的获取函数来优先考虑干预措施。该获取函数可以以闭合形式进行评估，从而实现快速优化。最终得到的算法在理论上是可行的。

    Sequential experimental design to discover interventions that achieve a desired outcome is a key problem in various domains including science, engineering and public policy. When the space of possible interventions is large, making an exhaustive search infeasible, experimental design strategies are needed. In this context, encoding the causal relationships between the variables, and thus the effect of interventions on the system, is critical for identifying desirable interventions more efficiently. Here, we develop a causal active learning strategy to identify interventions that are optimal, as measured by the discrepancy between the post-interventional mean of the distribution and a desired target mean. The approach employs a Bayesian update for the causal model and prioritizes interventions using a carefully designed, causally informed acquisition function. This acquisition function is evaluated in closed form, allowing for fast optimization. The resulting algorithms are theoreticall
    
[^156]: 网络化微电网的神经动态状态估计

    Neuro-Dynamic State Estimation for Networked Microgrids. (arXiv:2208.12288v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2208.12288](http://arxiv.org/abs/2208.12288)

    这项研究提出了一种神经动态状态估计算法，用于网络化微电网中的未知子系统。具体贡献包括：基于数据驱动的Neuro-DSE算法、自我完善的Neuro-DSE+算法、神经KalmanNet-DSE算法以及用于联合估计微电网状态和未知参数的增强Neuro-DSE算法。大量实验表明，在不同噪声水平、控制模式、电源和可观测性下，这些算法都具有有效性。

    

    我们设计了一个名为神经动态状态估计（Neuro-DSE）的基于学习的动态状态估计（DSE）算法，用于未知子系统的网络化微电网（NM）。我们的贡献包括：1）一种基于数据驱动的Neuro-DSE算法，用于具有部分未知动态模型的NM DSE，该算法将神经常微分方程（ODE-Net）与卡尔曼滤波器相结合；2）一种自我完善的Neuro-DSE算法（Neuro-DSE+），通过建立自动过滤、增加和校正框架，使得在有限且存在噪声测量情况下可以进行数据驱动的DSE；3）一种称为神经KalmanNet-DSE的算法，进一步将KalmanNet与Neuro-DSE相结合，以缓解神经和物理动态模型之间的模型不匹配；4）一种增强的Neuro-DSE算法，用于联合估计NM状态和未知参数（如惯性）。大量案例研究证明了Neuro-DSE及其变体在不同噪声水平、控制模式、电源和可观测性下的有效性。

    We devise neuro-dynamic state estimation (Neuro-DSE), a learning-based dynamic state estimation (DSE) algorithm for networked microgrids (NMs) under unknown subsystems. Our contributions include: 1) a data-driven Neuro-DSE algorithm for NMs DSE with partially unidentified dynamic models, which incorporates the neural-ordinary-differential-equations (ODE-Net) into Kalman filters; 2) a self-refining Neuro-DSE algorithm (Neuro-DSE+) which enables data-driven DSE under limited and noisy measurements by establishing an automatic filtering, augmenting and correcting framework; 3) a Neuro-KalmanNet-DSE algorithm which further integrates KalmanNet with Neuro-DSE to relieve the model mismatch of both neural- and physics-based dynamic models; and 4) an augmented Neuro-DSE for joint estimation of NMs states and unknown parameters (e.g., inertia). Extensive case studies demonstrate the efficacy of Neuro-DSE and its variants under different noise levels, control modes, power sources, observabilitie
    
[^157]: 基于多模态融合和表示映射的大规模交通拥堵预测

    Large-Scale Traffic Congestion Prediction based on Multimodal Fusion and Representation Mapping. (arXiv:2208.11061v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11061](http://arxiv.org/abs/2208.11061)

    本文提出了一种基于卷积神经网络的新型框架，通过学习表示和多模态融合，结合各种全局参考信息，实现了在大规模地图上对任意查询位置进行准确的交通拥堵预测。

    

    随着城市化进程的推进，城市交通系统对城市发展和居民生活质量至关重要。其中，通过分析拥堵因素来判断交通拥堵是最重要的任务之一。最近，已经引入了各种传统和基于机器学习的模型来预测交通拥堵。然而，这些模型要么对大规模拥堵因素聚合不完善，要么无法准确预测大规模空间中每个精确位置的情况。为了缓解这些问题，本文提出了一种基于卷积神经网络的新型端到端框架。通过学习表示，该框架提出了一种新颖的多模态融合模块和一种新颖的表示映射模块，结合各种全局参考信息，实现在大规模地图上对任意查询位置进行交通拥堵预测。

    With the progress of the urbanisation process, the urban transportation system is extremely critical to the development of cities and the quality of life of the citizens. Among them, it is one of the most important tasks to judge traffic congestion by analysing the congestion factors. Recently, various traditional and machine-learning-based models have been introduced for predicting traffic congestion. However, these models are either poorly aggregated for massive congestion factors or fail to make accurate predictions for every precise location in large-scale space. To alleviate these problems, a novel end-to-end framework based on convolutional neural networks is proposed in this paper. With learning representations, the framework proposes a novel multimodal fusion module and a novel representation mapping module to achieve traffic congestion predictions on arbitrary query locations on a large-scale map, combined with various global reference information. The proposed framework achie
    
[^158]: Hilbert Simplex几何中的非线性嵌入

    Non-linear Embeddings in Hilbert Simplex Geometry. (arXiv:2203.11434v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.11434](http://arxiv.org/abs/2203.11434)

    本文研究了将图的距离矩阵嵌入到Hilbert Simplex几何中的表示能力，发现该几何结构在嵌入任务中与其他几何结构相媲美，同时具有快速和数值稳健的特点。

    

    机器学习和计算机视觉的关键技术之一是将离散加权图嵌入到连续空间中进行后续处理。在分层结构嵌入到双曲几何中方面取得了很大的成功，因为已经证明任何加权树都可以以任意低的扭曲程度嵌入到该几何中。我们研究了将图的距离矩阵嵌入到Hilbert Simplex几何中的表示能力。我们的发现表明，Hilbert Simplex几何在嵌入任务中与其他几何结构（如Poincaré双曲球或欧几里德几何）相媲美，同时具有快速和数值稳健的特点。

    A key technique of machine learning and computer vision is to embed discrete weighted graphs into continuous spaces for further downstream processing. Embedding discrete hierarchical structures in hyperbolic geometry has proven very successful since it was shown that any weighted tree can be embedded in that geometry with arbitrary low distortion. Various optimization methods for hyperbolic embeddings based on common models of hyperbolic geometry have been studied. In this paper, we consider Hilbert geometry for the standard simplex which is isometric to a vector space equipped with the variation polytope norm. We study the representation power of this Hilbert simplex geometry by embedding distance matrices of graphs. Our findings demonstrate that Hilbert simplex geometry is competitive to alternative geometries such as the Poincar\'e hyperbolic ball or the Euclidean geometry for embedding tasks while being fast and numerically robust.
    
[^159]: 基于空时注意力融合网络的城市轨道交通系统节假日短期客流预测

    Spatial-Temporal Attention Fusion Network for short-term passenger flow prediction on holidays in urban rail transit systems. (arXiv:2203.00007v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00007](http://arxiv.org/abs/2203.00007)

    提出了一个基于深度学习的模型，利用空时注意力融合网络来预测城市轨道交通系统节假日短期客流。该模型通过多图注意力网络提取客流的复杂空间依赖性，通过卷积-注意力块提取客流的时间依赖性，并通过特征融合块进行信息融合。这个方法对于交通管理具有重要意义。

    

    对于城市轨道交通系统来说，短期客流预测对交通运营和管理至关重要。近年来，基于深度学习的模型提供了提高预测准确性的有效方法。然而，大部分现有模型主要预测普通工作日或周末的客流，很少有研究关注节假日客流的预测，而这是一个具有突发性和不规律性的重要挑战。为此，我们提出了一种基于深度学习的模型，命名为空时注意力融合网络，包括一个新颖的多图注意力网络、卷积-注意力块和特征融合块，用于节假日短期客流预测。

    The short term passenger flow prediction of the urban rail transit system is of great significance for traffic operation and management. The emerging deep learning-based models provide effective methods to improve prediction accuracy. However, most of the existing models mainly predict the passenger flow on general weekdays or weekends. There are only few studies focusing on predicting the passenger flow on holidays, which is a significantly challenging task for traffic management because of its suddenness and irregularity. To this end, we propose a deep learning-based model named Spatial Temporal Attention Fusion Network comprising a novel Multi-Graph Attention Network, a Conv-Attention Block, and Feature Fusion Block for short-term passenger flow prediction on holidays. The multi-graph attention network is applied to extract the complex spatial dependencies of passenger flow dynamically and the conv-attention block is applied to extract the temporal dependencies of passenger flow fro
    
[^160]: STG-GAN：一种用于城市轨道交通系统短期客流预测的时空图生成对抗网络

    STG-GAN: A spatiotemporal graph generative adversarial networks for short-term passenger flow prediction in urban rail transit systems. (arXiv:2202.06727v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06727](http://arxiv.org/abs/2202.06727)

    STG-GAN提出了一种基于深度学习的时空图生成对抗网络，用于解决短期客流预测的准确性、效率和内存占用问题。

    

    短期客流预测是更好管理城市轨道交通系统的重要且具有挑战性的任务。一些新兴的深度学习模型为改善短期预测准确性提供了有益的见解。然而，在城市轨道交通系统中存在许多复杂的时空依赖关系。大多数先前的方法只将真实值和预测值之间的绝对误差作为优化目标，未能考虑到对预测的空间和时间约束。此外，许多现有的预测模型引入了复杂的神经网络层以提高准确性，但忽视了它们的训练效率和内存占用，降低了在现实世界中应用的机会。为了克服这些限制，我们提出了一种新颖的基于深度学习的时空图生成对抗网络（STG-GAN）模型，具有更高的预测准确性、更高的效率和更低的内存占用，用于预测短期客流。

    Short-term passenger flow prediction is an important but challenging task for better managing urban rail transit (URT) systems. Some emerging deep learning models provide good insights to improve short-term prediction accuracy. However, there exist many complex spatiotemporal dependencies in URT systems. Most previous methods only consider the absolute error between ground truth and predictions as the optimization objective, which fails to account for spatial and temporal constraints on the predictions. Furthermore, a large number of existing prediction models introduce complex neural network layers to improve accuracy while ignoring their training efficiency and memory occupancy, decreasing the chances to be applied to the real world. To overcome these limitations, we propose a novel deep learning-based spatiotemporal graph generative adversarial network (STG-GAN) model with higher prediction accuracy, higher efficiency, and lower memory occupancy to predict short-term passenger flows
    
[^161]: SMGRL：可扩展的多分辨率图表示学习

    SMGRL: Scalable Multi-resolution Graph Representation Learning. (arXiv:2201.12670v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12670](http://arxiv.org/abs/2201.12670)

    本论文提出了一个可扩展的多分辨率图表示学习框架（SMGRL），通过降低训练成本和利用自相似性在多个分辨率上应用算法，能够高效地学习多分辨率节点嵌入。

    

    图卷积网络（GCN）可以用于学习具有拓扑感知能力的节点嵌入，对于分类或链接预测非常有用。然而，它们无法捕捉节点之间的长程依赖关系，除非添加额外的层次——而这又导致过度平滑和时间空间复杂度增加。此外，节点之间的复杂依赖关系使得小批量处理变得困难，限制了它们在大型图上的适用性。我们提出了一个可扩展的多分辨率图表示学习（SMGRL）框架，使我们能够高效地学习多分辨率节点嵌入。我们的框架与模型无关，可以应用于任何现有的GCN模型。通过仅在原始图的降维粗化上进行训练，然后利用自相似性在多个分辨率上应用所得算法，我们显著降低了训练成本。最终的多分辨率嵌入可以聚合以产生高质量的节点嵌入。

    Graph convolutional networks (GCNs) allow us to learn topologically-aware node embeddings, which can be useful for classification or link prediction. However, they are unable to capture long-range dependencies between nodes without adding additional layers -- which in turn leads to over-smoothing and increased time and space complexity. Further, the complex dependencies between nodes make mini-batching challenging, limiting their applicability to large graphs. We propose a Scalable Multi-resolution Graph Representation Learning (SMGRL) framework that enables us to learn multi-resolution node embeddings efficiently. Our framework is model-agnostic and can be applied to any existing GCN model. We dramatically reduce training costs by training only on a reduced-dimension coarsening of the original graph, then exploit self-similarity to apply the resulting algorithm at multiple resolutions. The resulting multi-resolution embeddings can be aggregated to yield high-quality node embeddings th
    
[^162]: STS-GAN: 我们能否从任意2D示例合成高保真度的实体纹理？

    STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary 2D Exemplar?. (arXiv:2102.03973v7 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2102.03973](http://arxiv.org/abs/2102.03973)

    本文提出了一种名为STS-GAN的基于生成对抗网络的框架，能够将给定的2D示例扩展到任意3D实体纹理，并合成具有高保真度和相似特征的实体纹理。

    

    实体纹理合成（STS）是将2D示例扩展到3D实体体积的有效方法，在计算摄影中具有优势。然而，现有方法通常无法准确学习任意纹理，这可能导致合成高保真度实体纹理的失败。本文提出了一种新的基于生成对抗网络的框架（STS-GAN），将给定的2D示例扩展到任意3D实体纹理。在STS-GAN中，多尺度2D纹理鉴别器评估给定的2D示例与生成的3D纹理切片之间的相似性，促进3D纹理生成器合成逼真的实体纹理。最后，实验证明所提出的方法能够生成具有与2D示例类似的视觉特征的高保真度实体纹理。

    Solid texture synthesis (STS), an effective way to extend a 2D exemplar to a 3D solid volume, exhibits advantages in computational photography. However, existing methods generally fail to accurately learn arbitrary textures, which may result in the failure to synthesize solid textures with high fidelity. In this paper, we propose a novel generative adversarial nets-based framework (STS-GAN) to extend the given 2D exemplar to arbitrary 3D solid textures. In STS-GAN, multi-scale 2D texture discriminators evaluate the similarity between the given 2D exemplar and slices from the generated 3D texture, promoting the 3D texture generator synthesizing realistic solid textures. Finally, experiments demonstrate that the proposed method can generate high-fidelity solid textures with similar visual characteristics to the 2D exemplar.
    

