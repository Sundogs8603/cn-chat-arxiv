# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting.](http://arxiv.org/abs/2401.10227) | 该论文提出了一种基于稳定扩散的潜在扩散方法，用于全景分割和遮罩修复，通过简化架构来避免复杂性，实现了生成模型解锁遮罩修复功能，具有应用于交互式分割的潜力。 |
| [^2] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^3] | [AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data.](http://arxiv.org/abs/2401.10220) | AutoFT是一种通过优化超参数在OOD数据上进行基础模型微调的方法，以实现鲁棒性能。 |
| [^4] | [Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products.](http://arxiv.org/abs/2401.10216) | 该论文提出了一种加速计算不可约表示张量积的方法，通过将等变操作基础从球形谐波改变为2D傅立叶基础，实现了对E(3)群的等变神经网络的高效建模。 |
| [^5] | [Improving automatic detection of driver fatigue and distraction using machine learning.](http://arxiv.org/abs/2401.10213) | 本文介绍了使用机器学习技术改进的自动检测驾驶员疲劳和注意力分散的方法，包括基于视觉和面部对齐网络的驾驶疲劳检测以及基于卷积神经网络的注意力分散行为识别。实验结果表明了这些方法的有效性。 |
| [^6] | [Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation.](http://arxiv.org/abs/2401.10211) | 本文提出了一种通过多粒度结构和多尺度序列表示耦合的PTM位点预测方法PTM-CMGMS，该方法在结构表示学习和序列表示学习上进行优化，提高了PTM位点预测的准确性。 |
| [^7] | [Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction.](http://arxiv.org/abs/2401.10210) | 通过精通指导的非参数聚类方法，预测学生在问题解决中可能采用的策略，从而实现自适应教学系统的个性化体验。 |
| [^8] | [Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems.](http://arxiv.org/abs/2401.10207) | 本文研究了用于解释性深度神经网络入侵检测系统的折衷规则提取，旨在解决黑盒解释器的不可信任问题。 |
| [^9] | [Divide and not forget: Ensemble of selectively trained experts in Continual Learning.](http://arxiv.org/abs/2401.10191) | 连续学习中，我们提出了一种名为SEED的新方法，通过选择性训练最优的专家来解决遗忘和计算负担的问题，并在实验中展示了其高性能。 |
| [^10] | [A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions.](http://arxiv.org/abs/2401.10190) | 本论文提出了一种启发于Kaczmarz的方法加速神经网络波函数的优化，该方法结合了最小步长随机重构优化器(MinSR)和随机Kaczmarz方法，相对于其他方法在多个小原子和分子上表现更优。 |
| [^11] | [Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction.](http://arxiv.org/abs/2401.10189) | 这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。 |
| [^12] | [Transfer Learning in Human Activity Recognition: A Survey.](http://arxiv.org/abs/2401.10185) | 这项调查论文介绍了在智能家居和可穿戴设备基于人体活动识别的应用领域中所采用的迁移学习方法。通过对贡献和挑战进行分类和呈现，提供了问题-解决方案的视角。 |
| [^13] | [Comprehensive OOD Detection Improvements.](http://arxiv.org/abs/2401.10176) | 本文提出了一种综合性的OOD检测方法，包括基于表示和基于逻辑的方法。其中，在基于表示的方法中，通过降维改进了性能，在基于逻辑的方法中，通过解决一个未被注意到的缺陷提高了性能。在基准测试中，这些方法表现出卓越的性能，并取得了最新的结果。 |
| [^14] | [DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks.](http://arxiv.org/abs/2401.10158) | DISTINQT是一种面向未来移动和无线网络的隐私感知分布式学习框架，用于QoS预测。 |
| [^15] | [A novel hybrid time-varying graph neural network for traffic flow forecasting.](http://arxiv.org/abs/2401.10155) | 本文提出了一种新型的混合时变图神经网络（HTVGNN）用于交通流量预测，解决了现有方法中预定义图和自适应图的学习能力受限的问题。 |
| [^16] | [Multi-Agent Reinforcement Learning for Maritime Operational Technology Cyber Security.](http://arxiv.org/abs/2401.10149) | 本文通过引入一个模拟环境IPMSRL，展示了将自主网络防御应用于工业控制系统的潜力，并探索了多智能体强化学习在海上操作技术网络安全中的应用。 |
| [^17] | [Explicitly Disentangled Representations in Object-Centric Learning.](http://arxiv.org/abs/2401.10148) | 这篇论文提出了一种在物体中心化学习中明确解开形状和纹理成分的方法，通过将潜在空间划分为两个不重叠的子集，使得模型更加稳定和有效。 |
| [^18] | [Exploiting Hierarchical Interactions for Protein Surface Learning.](http://arxiv.org/abs/2401.10144) | 这个论文研究了蛋白质表面学习中的两个关键属性，即原子之间的关系和层次化特征交互，并提出了一个基于深度学习技术的框架。 |
| [^19] | [Spatial-Temporal Large Language Model for Traffic Prediction.](http://arxiv.org/abs/2401.10134) | 本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。 |
| [^20] | [Towards Principled Graph Transformers.](http://arxiv.org/abs/2401.10119) | 边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。 |
| [^21] | [Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study.](http://arxiv.org/abs/2401.10107) | 本研究通过比较标准多导睡眠图（PSG）和耳内脑电信号的相似性，旨在探索一种更少侵入、成本效益高和便携的替代方法。研究确定了评估方法，并通过提取特征进行分析。 |
| [^22] | [Learning shallow quantum circuits.](http://arxiv.org/abs/2401.10095) | 本文提出了一个多项式时间经典算法，可以通过单比特测量数据学习浅层量子电路的描述，包括架构未知的电路和准备未知态的电路。 |
| [^23] | [Optimizing Medication Decisions for Patients with Atrial Fibrillation through Path Development Network.](http://arxiv.org/abs/2401.10014) | 本研究通过使用机器学习算法和路径发展网络，利用心电图数据预测心房颤动患者是否应该推荐抗凝治疗，为医生在药物决策中提供决策支持。 |
| [^24] | [Developing an AI-based Integrated System for Bee Health Evaluation.](http://arxiv.org/abs/2401.09988) | 本研究发展了一个基于人工智能的综合系统，利用视觉和音频信号来评估蜜蜂箱的健康状况，通过注意力机制的多模态神经网络实现了准确的评估。 |
| [^25] | [FLex&Chill: Improving Local Federated Learning Training with Logit Chilling.](http://arxiv.org/abs/2401.09986) | FLex&Chill 提出了一种通过Logit Chilling方法改进本地联合学习训练的方法，可以加快模型收敛并提高推理精度。 |
| [^26] | [Ventricular Segmentation: A Brief Comparison of U-Net Derivatives.](http://arxiv.org/abs/2401.09980) | 本文探讨了深度学习技术在心脏图像分割中的应用，实施了多个U-Net衍生模型以实现对心脏特定部位的全面解剖和功能分析。通过图像、图表和定量指标验证了模型的效果，并讨论了面临的挑战和未来改进策略。 |
| [^27] | [False Discovery Rate Control for Gaussian Graphical Models via Neighborhood Screening.](http://arxiv.org/abs/2401.09979) | 本文介绍了一种通过邻域筛选控制高斯图模型中虚警率的方法，有效解决了已存在的估计器容易出现虚警边缘检测的问题。该方法是无参数的，无需用户调整，并在数值实验中得到验证。 |
| [^28] | [Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification.](http://arxiv.org/abs/2401.09953) | 通过光谱视角，研究了图数据增强方法中的图属性和结构变化的关系，发现保持低频特征值不变可以保留关键属性，提出了双棱镜（DP）增强方法，该方法灵活地保留关键的图属性同时增加图的多样性。 |
| [^29] | [SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning.](http://arxiv.org/abs/2401.09949) | SymbolNet是一种神经网络方法，通过动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性，实现了符号回归。通过引入稀疏正则化项，我们的模型可以自适应调整自身的强度，并收敛到目标稀疏度水平。与现有方法相比，SymbolNet能高效处理具有超过10个输入的数据集。 |
| [^30] | [HGAttack: Transferable Heterogeneous Graph Adversarial Attack.](http://arxiv.org/abs/2401.09945) | 这篇论文介绍了HGAttack，一种针对异构图的攻击方法。通过设计一个拟合模型和利用梯度生成扰动，该方法能够有效利用异构信息，提高攻击的可转移性和效果。 |
| [^31] | [WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV.](http://arxiv.org/abs/2401.09944) | WindSeer是一个名为WindSeer的神经网络，能够在实时预测低空风的同时节省计算资源。通过使用稀疏的测量数据和合成数据进行训练，它可以成功地预测已知地形上的真实风场，并在不同的分辨率和域大小上生成准确的预测结果，无需重新训练。 |
| [^32] | [Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation.](http://arxiv.org/abs/2401.09943) | 提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，并能集成任何幂级数并捕捉长程依赖关系。 |
| [^33] | [Biases in Expected Goals Models Confound Finishing Ability.](http://arxiv.org/abs/2401.09940) | 本研究旨在解决使用期望进球（xG）统计评估射门能力时的限制和偏见。研究发现，持续超出累积xG需要高射门频率。 |
| [^34] | [Probabilistic Truly Unordered Rule Sets.](http://arxiv.org/abs/2401.09918) | 本论文提出了概率性真正无序规则集（TURS）方法，用于解决规则集学习中的三个缺点：强加顺序、重叠冲突和多类别目标分类问题。通过利用规则集的概率特性来解决重叠冲突，并形式化定义学习问题。 |
| [^35] | [Enabling On-device Continual Learning with Binary Neural Networks.](http://arxiv.org/abs/2401.09916) | 本研究提出了一种结合了持续学习和二值神经网络的解决方案，以在资源受限的设备上进行训练，并保持竞争性能。该方法利用二值激活的重放和一种新颖的量化方案。 |
| [^36] | [Qadence: a differentiable interface for digital-analog programs.](http://arxiv.org/abs/2401.09915) | Qadence是一个高级编程接口，用于构建数字模拟量子程序。它具有灵活的接口、本地可微性和对真实设备执行的关注，促进了原生DAQC平台上变分量子算法的研究。 |
| [^37] | [Interplay between depth and width for interpolation in neural ODEs.](http://arxiv.org/abs/2401.09902) | 本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\varepsilon$的关系有关。 |
| [^38] | [A Survey on Hardware Accelerators for Large Language Models.](http://arxiv.org/abs/2401.09890) | 这项论文调查了用于增强大型语言模型性能和能量效率的硬件加速器，并对多种加速器进行了深入分析，为研究人员、工程师和决策者在实际应用中优化大型语言模型的部署提供了宝贵的见解。 |
| [^39] | [Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network.](http://arxiv.org/abs/2401.09886) | 本论文提出了一种基于弹性联邦和多智能体深度强化学习的合作边缘缓存方案，通过训练个性化的本地模型，预测准确受欢迎的内容，并在不同的SBS之间合作缓存热门内容，以达到优化获取内容成本的目标。 |
| [^40] | [GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting.](http://arxiv.org/abs/2401.09881) | GA-SmaAt-GNet是一种生成对抗架构，用于改进极端降水暂时预测的深度学习模型性能。它通过创建一个新的生成器和使用注意力增强鉴别器来利用降水掩码提供的额外信息，提高了预测质量。 |
| [^41] | [Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition.](http://arxiv.org/abs/2401.09880) | 本研究提出了一种基于注意力的循环神经网络用于自动识别下蛋鸡行为。通过声音分析和特征提取，构建了一个鲁棒的行为特征化系统，对下蛋鸡的健康行为进行监测和识别。实验结果表明该模型具有良好的综合性能。 |
| [^42] | [Reconciling Spatial and Temporal Abstractions for Goal Representation.](http://arxiv.org/abs/2401.09870) | 本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。 |
| [^43] | [Improving fine-grained understanding in image-text pre-training.](http://arxiv.org/abs/2401.09865) | 本研究引入了一种名为SPARC的方法，通过在图像-文本对中学习每个令牌的图像组合，以提高图像-文本预训练中的细粒度理解能力。SPARC方法结合了细粒度损失和对比损失，可以以较低的计算成本学习同时编码全局和局部信息的表示。 |
| [^44] | [Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments.](http://arxiv.org/abs/2401.09862) | 本研究提出了一种针对语言模型提示优化的进化多目标方法，通过情感分析为案例研究，实现了生成能够同时体现两种相互冲突情感的提示语，从而提高模型的性能和相关信息的提取能力。 |
| [^45] | [FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction.](http://arxiv.org/abs/2401.09840) | 本研究改进了名为FREED的RL模型，通过复制、审查和简化，使其在蛋白质条件下的分子生成中具有更好的质量和性能 |
| [^46] | [PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning.](http://arxiv.org/abs/2401.09819) | PPNet是一种新颖的神经网络结构，用于解决端到端近似最优路径规划问题。通过将路径规划问题分为两个子问题，并使用两级级联神经网络进行求解，同时引入了一种高效的数据生成方法EDaGe-PP。实验结果表明，PPNet在计算时间和成功率方面比其他方法有显著提升。 |
| [^47] | [Clickbait vs. Quality: How Engagement-Based Optimization Shapes the Content Landscape in Online Platforms.](http://arxiv.org/abs/2401.09804) | 研究表明，基于参与度的优化策略在在线平台中对内容格局产生了重要影响，其既鼓励了质量投资，又奖励了点击驱动等战术。然而，在均衡状态下，内容质量和战术之间呈正相关，而用户消费的平均内容质量可能会下降，基于参与度的优化策略在用户体验方面可能表现得更差。 |
| [^48] | [A Fast, Performant, Secure Distributed Training Framework For Large Language Model.](http://arxiv.org/abs/2401.09796) | 本文提出了一种基于模型切片的安全分布式语言模型训练框架。通过在客户端和服务器端部署可信执行环境（TEE）并使用轻量级加密进行安全通信，解决了恶意窃取模型参数和数据的问题。此外，采用分割微调和稀疏化参数微调的方法，在降低设备成本的同时提高了模型性能和准确性。 |
| [^49] | [PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection.](http://arxiv.org/abs/2401.09793) | PatchAD是一种新颖的基于块的MLP-Mixer体系结构，利用对比学习进行时间序列异常检测。它具有高效和轻量级的架构，并采用创新的双项目约束模块来提高表示能力。 |
| [^50] | [BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images.](http://arxiv.org/abs/2401.09791) | 本研究介绍了一种用于对乳腺Faxitron和组织病理学图像进行配准的深度学习框架，可以改善组织病理学过程，并为病理学家选择取样区域提供了更准确的方法。 |
| [^51] | [Querying Easily Flip-flopped Samples for Deep Active Learning.](http://arxiv.org/abs/2401.09787) | 本文提出了一种基于模型的预测不确定性度量，即最小不一致度量（LDM），用于解决复杂决策边界情况下的主动学习问题。通过查询具有最小LDM的未标记数据，可以提高深度学习模型的性能。 |
| [^52] | [Towards Learning from Graphs with Heterophily: Progress and Future.](http://arxiv.org/abs/2401.09769) | 本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。 |
| [^53] | [Explaining Drift using Shapley Values.](http://arxiv.org/abs/2401.09756) | 本文提出了一个新的框架-DBShap，使用Shapley值来确定模型性能漂移的主要贡献者并量化他们的贡献。通过DBShap提供的解释，可以理解漂移背后的根本原因。 |
| [^54] | [Universally Robust Graph Neural Networks by Preserving Neighbor Similarity.](http://arxiv.org/abs/2401.09754) | 本文通过保持邻居相似性实现了普适鲁棒的图神经网络，并在异类图上探索了图神经网络的脆弱性。理论上证明了负分类损失的更新与基于邻居特征的成对相似性呈负相关，解释了图攻击者连接不相似节点对的行为。通过这种方法，我们新颖地提出了一种解决方案。 |
| [^55] | [Applications of Machine Learning to Optimizing Polyolefin Manufacturing.](http://arxiv.org/abs/2401.09753) | 本文介绍了机器学习在化工和聚烯烃制造优化中的应用，包括核心机器学习组件、各种方法和深度学习网络，并提出了科学引导的机器学习的混合方法来增强模型准确性。 |
| [^56] | [Improving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation.](http://arxiv.org/abs/2401.09752) | 本研究提出了一种动态联合分布适应（DJDA）方法，通过联合分布适应来消除多说话人引起的多域转移挑战，并学习具有辨别性和说话人无关性的语音情感特征。 |
| [^57] | [Exploration and Anti-Exploration with Distributional Random Network Distillation.](http://arxiv.org/abs/2401.09750) | 该论文提出了一种新的深度强化学习探索算法，称为分布式随机网络蒸馏（DRND）。该算法通过蒸馏随机网络的分布和隐式融入伪计数来改进奖励分配的精度，从而增强了探索过程。理论分析和实验结果均表明该方法的优越性。 |
| [^58] | [Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework.](http://arxiv.org/abs/2401.09748) | 引入了一个基于函数图像和操作树序列的科学计算多模态框架（Botfip），应用于符号回归问题，并验证了其在低复杂度问题上的优势，展示了其潜力。这个多模态框架在科学计算问题中具有广泛的应用前景。 |
| [^59] | [Offline Imitation Learning by Controlling the Effective Planning Horizon.](http://arxiv.org/abs/2401.09728) | 本文研究了离线模仿学习中通过控制有效规划范围来提高性能的问题，并提出了修正算法方法，解决了存在的逼近误差问题。 |
| [^60] | [EfficientRec an unlimited user-item scale recommendation system based on clustering and users interaction embedding profile.](http://arxiv.org/abs/2401.09693) | EfficientRec是一种应用了图神经网络和对比学习框架的推荐系统，采用了软聚类架构，能以低计算成本学习用户偏好，并具有较高的准确性和对无限用户和产品的可扩展性。 |
| [^61] | [Imitation Learning Inputting Image Feature to Each Layer of Neural Network.](http://arxiv.org/abs/2401.09691) | 本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。 |
| [^62] | [Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks.](http://arxiv.org/abs/2401.09682) | 本研究比较了不同分类变量编码器在分类和回归任务中的性能，验证了对于ATI模型来说，独热编码器是最佳选择，而目标编码器及其变体是树模型中最合适的编码器。 |
| [^63] | [Harnessing Density Ratios for Online Reinforcement Learning.](http://arxiv.org/abs/2401.09681) | 本文通过提出一种新的算法（GLOW），结合密度比率模型和值函数模型，在线强化学习中解决了瓶颈问题，即如何在没有初始数据集的情况下收集具有良好覆盖度的探索数据集。 |
| [^64] | [Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack.](http://arxiv.org/abs/2401.09673) | 本文提出了一种名为本地自适应对抗颜色攻击（LAACA）的方法，用于保护艺术品免受神经风格转换（NST）的滥用。该方法通过在不可察觉的情况下对图像进行修改，产生对NST具有干扰作用的扰动。 |
| [^65] | [Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach.](http://arxiv.org/abs/2401.09671) | 本研究旨在解决无监督领域转换中的可识别性问题，引入了一个MPA消除理论，解决了CycleGAN及其变体产生内容不对齐的限制。 |
| [^66] | [Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks.](http://arxiv.org/abs/2401.09665) | 本文研究了一种通过自排斥随机游走加速分布式随机优化的方法，该方法用非线性马尔可夫链取代传统的线性马尔可夫链，并在渐近方差方面取得了显著的改进。 |
| [^67] | [Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks.](http://arxiv.org/abs/2401.09656) | 在车联网中，通过收敛性分析，本文证明移动性对分层联邦学习的收敛速度有积极影响，它增加了边缘层异构数据的融合和更快的数据融合速度，从而提高模型准确性。 |
| [^68] | [Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning.](http://arxiv.org/abs/2401.09651) | 本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。 |
| [^69] | [ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change.](http://arxiv.org/abs/2401.09646) | ClimateGPT是一个针对气候变化领域的跨学科研究合成的AI模型，通过优化检索增强和使用级联机器翻译方法，提高了模型的性能和可访问性。 |
| [^70] | [Functional Linear Non-Gaussian Acyclic Model for Causal Discovery.](http://arxiv.org/abs/2401.09641) | 本研究提出了功能性线性非高斯无循环模型（Func-LiNGAM），将变量的概念扩展到了向量和函数，并解决了原始模型无法处理无限维数据集的问题，从而在因果发现中提供了更广泛的应用。 |
| [^71] | [Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using Fusion Strategies and Deep Learning.](http://arxiv.org/abs/2401.09638) | 本研究提出了一种使用融合策略和深度学习的自动化三维多模式超声胎盘分割方法，通过自动识别胎盘变化，有望解决当前超声处理方法的劳动密集、耗时长和容易出错的问题，从而促进早期检测和潜在治疗。 |
| [^72] | [Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic Navigation Systems using Liquid Time-Constant Networks.](http://arxiv.org/abs/2401.09631) | 本研究提出了一种基于物理信息的方法，在磁导航系统中使用Tolles-Lawson系数和液体时间常数网络（LTCs）进行气磁补偿校准。实验结果显示，在实际飞行数据中，我们观察到了高达64％的气磁补偿误差减少，优于传统模型。 |
| [^73] | [Multiple Locally Linear Kernel Machines.](http://arxiv.org/abs/2401.09629) | 本文提出了一种基于多个局部线性分类器组合的新型非线性分类器，提供了可扩展的泛化多核学习训练算法，填补了高准确性但缓慢的非线性分类器和快速但低准确性的线性分类器之间的差距。 |
| [^74] | [SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI.](http://arxiv.org/abs/2401.09627) | SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。 |
| [^75] | [MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks.](http://arxiv.org/abs/2401.09624) | MITS-GAN是一种用于保护医学影像免受篡改的新方法，通过引入适当的高斯噪声作为防护措施，打乱攻击者的生成对抗网络架构的输出。实验结果表明MITS-GAN能够生成耐篡改图像，具有优越的性能。 |
| [^76] | [SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics.](http://arxiv.org/abs/2401.09622) | SMOOTHIE是一种通过考虑损失函数的“光滑度”来引导超参数优化的新型方法，在软件分析中应用可以带来显著的性能改进。 |
| [^77] | [Land Cover Image Classification.](http://arxiv.org/abs/2401.09607) | 本文研究了土地覆盖图像分类和深度学习模型在提高分类准确性和效率方面的应用。通过比较卷积神经网络和基于Transformer的方法，我们在LC研究中展示了它们的优势，并在EuroSAT数据集上取得了最先进的结果。 |
| [^78] | [Robustness Evaluation of Machine Learning Models for Robot Arm Action Recognition in Noisy Environments.](http://arxiv.org/abs/2401.09606) | 本文研究了机器人臂动作识别在嘈杂环境中的鲁棒性评估。通过使用视觉系统追踪机器人的运动和深度学习模型提取臂部的关键点，我们的方法在嘈杂环境中实现了精确的动作分类。 |
| [^79] | [MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical Images with Transformers and Fully Homomorphic Encryption.](http://arxiv.org/abs/2401.09604) | MedBlindTuner通过结合Transformer和全同态加密，在保护病人数据隐私的同时，实现对生物医学图像的隐私保护微调。通过实验验证，MedBlindTuner在加密数据上取得了与非加密数据训练模型相当的准确度，为外包机器学习计算提供了安全的解决方案。 |
| [^80] | [Efficient generative adversarial networks using linear additive-attention Transformers.](http://arxiv.org/abs/2401.09596) | 这项工作提出了一种名为LadaGAN的高效生成对抗网络，它使用了一种名为Ladaformer的新型Transformer块，通过线性加法注意机制来降低计算复杂度并解决训练不稳定性问题。 |
| [^81] | [Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis.](http://arxiv.org/abs/2401.09587) | 该论文提出了一种新的双层优化算法BO-REP，用于解决神经网络中存在的无界平滑性问题，通过使用归一化动量更新上层变量并引入初始化细化和周期性更新两种新技术来更新下层变量。 |
| [^82] | [eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles.](http://arxiv.org/abs/2401.09582) | eipy是一个开源Python包，用于开发多模态数据集成分类模型。它提供了严格和用户友好的框架，并通过嵌套交叉验证来评估并选择最佳的集成方法。 |
| [^83] | [Fully-blind Neural Network Based Equalization for Severe Nonlinear Distortions in 112 Gbit/s Passive Optical Networks.](http://arxiv.org/abs/2401.09579) | 该论文提出了一种对于112 Gbit/s被动光网络中严重非线性失真的全盲神经网络均衡器，并评估了其性能。这种均衡器采用低硬件复杂度的神经网络拓扑结构。 |
| [^84] | [Towards Scalable and Robust Model Versioning.](http://arxiv.org/abs/2401.09574) | 本文探讨了在不获取新的训练数据或更改模型架构的情况下，生成具有不同攻击属性的多个模型版本的可行性，以保护模型所有者免受恶意入侵带来的损失。 |
| [^85] | [Sharing Knowledge in Multi-Task Deep Reinforcement Learning.](http://arxiv.org/abs/2401.09561) | 本研究探讨了在多任务强化学习中分享表示的益处，并提供理论保证和实验评估结果，表明在多任务设置中分享表示可以改进性能。 |
| [^86] | [Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality.](http://arxiv.org/abs/2401.09556) | 本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。 |
| [^87] | [Improving Classification Performance With Human Feedback: Label a few, we label the rest.](http://arxiv.org/abs/2401.09555) | 本文探讨了通过人类反馈来改进分类模型性能的方法。使用少量有标签示例，通过连续反馈循环，我们能够显著提高模型的准确性。在多个数据集上进行评估，结果表明这种方法能够超越零样本大型语言模型，提供更强的文本分类性能。 |
| [^88] | [Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning.](http://arxiv.org/abs/2401.09517) | 本综述通过机器学习和多模态MRI揭示了神经精神疾病和神经退行性疾病的疾病异质性，为我们更好地理解这些疾病做出了贡献 |
| [^89] | [Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling.](http://arxiv.org/abs/2401.09516) | 该论文提出了一种名为排序克里洛夫回收（SKR）的新方法，用于加速神经算子训练的数据生成。该方法解决了现有方法在解决PDE问题时计算冗余的问题，显著提高了数据生成效率。 |
| [^90] | [Community Detection in the Multi-View Stochastic Block Model.](http://arxiv.org/abs/2401.09510) | 本文介绍了一个多视图随机块模型（MVSBM），以信息理论的角度研究了在多个相关图上进行的社区检测问题。我们证明了当模型参数超过阈值时可以实现精确恢复社区，而当参数低于阈值时无法达到精确恢复社区的要求。 |
| [^91] | [Exploration of Activation Fault Reliability in Quantized Systolic Array-Based DNN Accelerators.](http://arxiv.org/abs/2401.09509) | 本文提出了一种综合方法论，用于探索基于量化的系统阵列DNN加速器中激活错误的可靠性，从而全面评估模型精度、可靠性和硬件效率之间的相互影响。 |
| [^92] | [Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising.](http://arxiv.org/abs/2401.09507) | 这篇论文介绍了一种用于解决电子商务广告中多场后处理校准问题的方法。该方法通过训练校准器，并在在线推断过程中应用这些校准器来实现形状校准和数值校准。 |
| [^93] | [Functional Autoencoder for Smoothing and Representation Learning.](http://arxiv.org/abs/2401.09499) | 本研究提出了一种功能自编码器，用于学习功能数据的非线性表示，并避免了预处理的需要。 |
| [^94] | [Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility.](http://arxiv.org/abs/2401.09498) | 本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。 |
| [^95] | [VeriBug: An Attention-based Framework for Bug-Localization in Hardware Designs.](http://arxiv.org/abs/2401.09494) | VeriBug是一种基于注意力机制的硬件设计漏洞定位框架，利用深度学习技术加速寄存器传输级别的调试，并生成可能的根本原因的解释。 |
| [^96] | [Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification.](http://arxiv.org/abs/2401.09493) | 本研究利用线性变分编码器-解码器来学习云辐射反馈对早期热带气旋强化的影响，发现内核深对流和浅云的长波辐射强迫都对强化起到贡献，其中深对流的影响最大。 |
| [^97] | [Uncertainty-Aware Calibration of a Hot-Wire Anemometer With Gaussian Process Regression.](http://arxiv.org/abs/2401.09492) | 本研究针对低成本热线风速计由于气温变化而导致的精度损失问题，采用高斯过程回归进行概率校准，并在实验验证中取得了良好的性能。通过在实际使用前进行校准，可以估计典型环境温度下的风速，并提供每个速度测量的可靠不确定性估计。 |
| [^98] | [PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies.](http://arxiv.org/abs/2401.09489) | PUPAE是一种直观且可操作的时间序列异常解释方法，通过引入领域无关的反事实解释，能够帮助解释和处理异常。 |
| [^99] | [LoMA: Lossless Compressed Memory Attention.](http://arxiv.org/abs/2401.09486) | LoMA是一种无损压缩的内存注意力方法，可以有效地处理长文本并减少资源消耗。 |
| [^100] | [Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning.](http://arxiv.org/abs/2401.09479) | 本文提出了一种使用多模态深度学习进行硬件特洛伊检测的方法，通过生成对抗网络扩充数据，并采用早融合和晚融合策略进行评估。通过估计不确定性量化指标，实现风险感知的决策制定。 |
| [^101] | [Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from MRIs.](http://arxiv.org/abs/2401.09475) | Triamese-ViT是一种创新的ViT模型适应脑龄估计的方法，通过结合来自三个不同方向的ViTs来捕捉三维信息，显著提高了准确性和可解释性。 |
| [^102] | [Brain Tumor Radiogenomic Classification.](http://arxiv.org/abs/2401.09471) | 本研究通过分析多参数mpMRI扫描来预测胶质母细胞瘤中的MGMT生物标志物状态，结果表明ViT3D和Xception模型具有优势，可作为有效的分类工具进行使用。 |
| [^103] | [Self Supervised Vision for Climate Downscaling.](http://arxiv.org/abs/2401.09466) | 这项工作提出了一种深度学习模型，用于下调尺度 ESM 模拟数据，不需要高分辨率的真实数据进行模型优化。 |
| [^104] | [Parametric Constraints for Bayesian Knowledge Tracing from First Principles.](http://arxiv.org/abs/2401.09456) | 本文从一级原理出发，推导出了可以在贝叶斯知识追踪的参数空间上施加的约束，解决了目前算法中存在的一系列问题。 |
| [^105] | [Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach.](http://arxiv.org/abs/2401.09455) | 本论文提出了一种基于约束的多智能体强化学习方法来解决集成卫星-地面网络（ISTN）系统的动态路由问题，有效平衡了快速通信、能源效率和数据包丢失要求。 |
| [^106] | [Voila-A: Aligning Vision-Language Models with User's Gaze Attention.](http://arxiv.org/abs/2401.09454) | 本文介绍了一种使用用户注视注意力对齐视觉-语言模型的方法，在处理复杂场景和多个物体的实际应用中提高了模型的可解释性和效果。 |
| [^107] | [Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings.](http://arxiv.org/abs/2401.09452) | 该论文提出了一种将黎曼几何特征应用于学习翼面压力系数分布的方法，以提高气动系数的预测准确性。 |
| [^108] | [Diffusion-Driven Generative Framework for Molecular Conformation Prediction.](http://arxiv.org/abs/2401.09451) | 本文介绍了一种基于扩散驱动的生成框架\method{}，用于预测分子的三维构象，具有较高的预测精度并改进了传统方法的不足。 |
| [^109] | [Explainable Multimodal Sentiment Analysis on Bengali Memes.](http://arxiv.org/abs/2401.09446) | 这项研究提出了一个多模态方法来解释孟加拉语Memes的情感，以填补此领域中低资源语言的研究空白。对比现有的数据集，提出了一个新的MemoSen数据集并表明其准确率的局限性。这项研究的主要贡献是在孟加拉语Memes情感分析领域引入了多模态方法。 |
| [^110] | [CRD: Collaborative Representation Distance for Practical Anomaly Detection.](http://arxiv.org/abs/2401.09443) | 本文提出了一种基于协同表示模型的图像补丁距离计算方法，避免了查询图像和存储的补丁之间的最近邻搜索所带来的复杂度问题，能够在边缘环境下进行快速部署实用的异常检测。 |
| [^111] | [Voxceleb-ESP: preliminary experiments detecting Spanish celebrities from their voices.](http://arxiv.org/abs/2401.09441) | Voxceleb-ESP是一个从声音中检测西班牙名人的实验，提供了一个全面多样的西班牙语数据集，与原版英语VoxCeleb相当复杂，为说话人识别基准的扩展做出了贡献。 |
| [^112] | [RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models.](http://arxiv.org/abs/2401.09432) | RoleCraft-GLM是一个创新框架，通过大型语言模型实现个性化角色扮演，解决了缺乏个性化互动的问题。通过独特的对话数据集和细致入微的角色发展，它能够生成准确反映角色个性特征和情感的对话，提升用户参与度。 |
| [^113] | [A Smoothing Algorithm for l1 Support Vector Machines.](http://arxiv.org/abs/2401.09431) | 本文提出了一个用于解决具有$\ell^{1}$惩罚的软间隔支持向量机（SVM）优化问题的平滑算法，可以在遍历数据时保持较低的成本，并经过实验证明算法具有很高的测试准确性。 |
| [^114] | [Transduce: learning transduction grammars for string transformation.](http://arxiv.org/abs/2401.09426) | Transduce算法通过构建抽象迁移语法和其泛化，可以从输入输出示例中高效地学习位置转换，并且成功率高于当前最新成果。 |
| [^115] | [Precipitation Prediction Using an Ensemble of Lightweight Learners.](http://arxiv.org/abs/2401.09424) | 本文提出了一个使用多个轻量级学习器的集成预测降水模型，并通过使用卫星图像进行训练，有效地模拟复杂的降雨模式，特别是对于高降水事件。在Weather4Cast 2023竞赛中取得了第一名。 |
| [^116] | [Preparing Lessons for Progressive Training on Language Models.](http://arxiv.org/abs/2401.09192) | 提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。 |
| [^117] | [Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder.](http://arxiv.org/abs/2401.09180) | 该论文提出了一种无监督多领域翻译的方法，通过修改后的变分自编码器实现了受控解缠的两个潜变量，其中一个仅与领域有关，另一个与数据的其他变化因素有关。实验证明该方法在不同的视觉数据集上提高了性能。 |
| [^118] | [MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data.](http://arxiv.org/abs/2401.08727) | 提出了一种新的交通拥堵预测模型，使用车辆轨迹数据以及多邻接关系注意力图卷积网络（MA2GCN）来预测交通拥堵情况，不依赖于传感器数据，提取灵活且准确的交通信息。 |
| [^119] | [NODI: Out-Of-Distribution Detection with Noise from Diffusion.](http://arxiv.org/abs/2401.08689) | 本研究将扩散过程应用于外分布检测任务中，通过将整个训练集的信息集成到预测的噪声向量中，获得稳定的噪声向量，并将其转化为OOD分数。 |
| [^120] | [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture.](http://arxiv.org/abs/2401.08406) | 本文评估了检索增强生成（RAG）和微调两种方法在大型语言模型上的性能差异，并提出了适用于农业数据集的管道和权衡。 |
| [^121] | [Differentially Private Estimation of CATE in Adaptive Experiment.](http://arxiv.org/abs/2401.08224) | 本研究提出在自适应实验中使用差分隐私估计条件平均处理效果(CATE)，平衡社会福利损失和统计功率，并通过匹配上下界以及帕累托最优性概念来获得最优解。 |
| [^122] | [Are self-explanations from Large Language Models faithful?.](http://arxiv.org/abs/2401.07927) | 大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。 |
| [^123] | [Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data.](http://arxiv.org/abs/2401.07231) | 本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法，并扩展了这些方法以应用于时间序列数据。这些方法利用先验知识进行高效因果发现，并具有对因果关系顺序的特殊处理。 |
| [^124] | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training.](http://arxiv.org/abs/2401.05566) | 该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。 |
| [^125] | [LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control.](http://arxiv.org/abs/2401.04855) | 提出了一种可学习的感知-行动-通信(LPAC)架构，使用卷积神经网络处理环境感知，图神经网络实现机器人之间的信息交流，浅层多层感知机计算机器人的动作。使用集中式显微算法训练模型，实现机器人群体的协作。 |
| [^126] | [Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis.](http://arxiv.org/abs/2401.02860) | 该论文提出了一个利用矩阵分析方法的框架，用于推理时间序列中的跟随模式。在模拟数据集和声音记录数据集中，该框架优于基准方法，并能够检测出加密货币数据集中的跟随模式。 |
| [^127] | [TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA.](http://arxiv.org/abs/2312.17670) | 这项研究提出了TopCoW挑战，通过发布具有13种血管组分注释的Willis循环（CoW）数据集，并使用虚拟现实（VR）技术进行拓扑感知解剖分割，解决了手动和耗时的CoW表征问题。 |
| [^128] | [Divergences induced by dual subtractive and divisive normalizations of exponential families and their convex deformations.](http://arxiv.org/abs/2312.12849) | 本文研究了指数族的双重减性和除性归一化方法引起的差异，提出了一对Bregman和Jensen散度。同时，证明了α散度可以通过分配函数引起缩放α偏斜Jensen散度，从而比较了不同归一化方法之间的差异。 |
| [^129] | [FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise.](http://arxiv.org/abs/2312.12838) | 本论文针对联邦医学图像分割中的异构标注噪声问题，提出了FedA3I方法，通过注释质量感知聚合来解决高质量客户端对联邦学习的更大影响。 |
| [^130] | [Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed.](http://arxiv.org/abs/2312.12469) | 本论文提出了一种通用的引导非自回归知识蒸馏（GNARKD）方法，通过知识蒸馏将自回归模型中的关键组件保留在网络架构中，从而获得具有低推理延迟的高性能非自回归车辆路径问题求解器。 |
| [^131] | [Partial Label Learning with a Partner.](http://arxiv.org/abs/2312.11034) | 本文介绍了一种在部分标签学习中识别和纠正错误标记样本的方法，通过引入合作伙伴分类器和互相监督范式，以及使用模糊机制实现分类器之间的协作。 |
| [^132] | [A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML.](http://arxiv.org/abs/2312.06305) | 本文提出了一种元级学习算法SHSR，用于减少AutoML中的超参数空间，减少了约30%的执行时间并且性能损失小于0.1%。 |
| [^133] | [Invariant Random Forest: Tree-Based Model Solution for OOD Generalization.](http://arxiv.org/abs/2312.04273) | 本文引入了一种新颖且有效的解决方案，用于决策树模型的OOD泛化，称为不变决策树（IDT）。该方法通过在树的生长过程中惩罚不同环境下分裂的不稳定行为，构建了不变随机森林（IRF）。验证实验证明，相比非OOD树模型，该方法表现出更好的性能，强调了考虑树模型的OOD泛化的必要性。 |
| [^134] | [DKiS: Decay weight invertible image steganography with private key.](http://arxiv.org/abs/2311.18243) | DKiS是一种基于私钥的图像隐写术，通过引入衰减权重来控制信息的传输，提高了隐写术的性能，并确保隐藏信息的安全性。 |
| [^135] | [Labeling Neural Representations with Inverse Recognition.](http://arxiv.org/abs/2311.13594) | 逆向识别 (INVERT) 是一种可扩展的方法，通过连接学习到的神经表示与人类可理解的概念，实现了对神经表示的标记并提供了统计显著性评估指标。 |
| [^136] | [Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation.](http://arxiv.org/abs/2311.13184) | 本论文提出了一种方法，通过将算法表示集成到算法选择中，从而填补了当前算法选择技术对算法特征的研究空白。 |
| [^137] | [FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification.](http://arxiv.org/abs/2311.10359) | FIKIT是一种基于优先级的实时GPU多任务调度策略，具有内核识别功能，能够在高优先级任务的内核间填充空闲时间。 |
| [^138] | [Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control.](http://arxiv.org/abs/2311.07202) | 本研究提出了一种基于输入凸LSTM的基于Lyapunov的模型预测控制方法，通过减少收敛时间和缓解梯度消失/爆炸问题来改善MPC的性能。 |
| [^139] | [Generalized test utilities for long-tail performance in extreme multi-label classification.](http://arxiv.org/abs/2311.05081) | 这项研究提出了一种解决极端多标签分类中长尾性能问题的通用测试工具，通过分析广义度量预算和预期测试效用框架，推导出最优预测规则。 |
| [^140] | [Improved DDIM Sampling with Moment Matching Gaussian Mixtures.](http://arxiv.org/abs/2311.04938) | 在DDIM框架中使用GMM作为反向转移算子，通过矩匹配可以获得质量更高的样本。在无条件模型和类条件模型上进行了实验，并通过FID和IS指标证明了我们的方法的改进效果。 |
| [^141] | [On the Lipschitz constant of random neural networks.](http://arxiv.org/abs/2311.01356) | 本文研究了随机ReLU神经网络的Lipschitz常数，对于浅层神经网络，我们得到了Lipschitz常数的精确刻画，对于足够宽度的深层神经网络，我们给出了上下界，并匹配一个依赖于深度的对数因子。 |
| [^142] | [On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications.](http://arxiv.org/abs/2311.00964) | 本文研究了在金融科技应用中寻找高质量的双目标 Pareto 最优欺诈预防规则集的问题。通过采用 Pareto 最优性概念和启发式框架 PORS，我们成功提出了一组非支配的规则子集，并通过实证评估证明了其有效性。 |
| [^143] | [Unexpected Improvements to Expected Improvement for Bayesian Optimization.](http://arxiv.org/abs/2310.20708) | 提出了LogEI作为一类新的贝叶斯优化的获得函数，具有与传统的EI函数相同或近似相等的最优解，但数值上更容易进行优化。 |
| [^144] | [BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis.](http://arxiv.org/abs/2310.20496) | BasisFormer提出了一种基于可学习和可解释的基础的注意力机制时间序列预测方法，通过自适应的自监督学习获得基础，并设计了一个模块计算时间序列与基础之间的相似系数。 |
| [^145] | [Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery.](http://arxiv.org/abs/2310.19776) | 本论文提出了一种新颖、高效和自我监督的方法，可以在测试时发现以前未知的类别，通过将最小长度类别代码分配给单个数据实例来增强对类别细粒度的控制。 |
| [^146] | [SpecTr: Fast Speculative Decoding via Optimal Transport.](http://arxiv.org/abs/2310.15141) | 本研究通过最优传输的方法提供了推测性解码的原则性理解，使得从大语言模型中进行自回归采样的过程能够更快速地进行。 |
| [^147] | [Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages.](http://arxiv.org/abs/2310.13897) | 给出了一种新的变换器编码器模型，该模型具有硬注意力和严格未来掩码，并且证明这些网络识别的语言类别正是无星语言。研究还发现，通过添加位置嵌入，这一模型可以扩展到其他研究充分的语言类别。一个关键技术是布尔RASP，通过无星语言的研究，将变换器与一阶逻辑、时态逻辑和代数自动机理论相关联。 |
| [^148] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^149] | [Circuit Component Reuse Across Tasks in Transformer Language Models.](http://arxiv.org/abs/2310.08744) | 这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。 |
| [^150] | [3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information.](http://arxiv.org/abs/2309.17366) | 3D-Mol是一种新颖的基于3D结构的分子建模方法，通过对比学习提高了分子性质预测准确性，并在多个基准数据集上超过了最先进的模型。 |
| [^151] | [Compositional Program Generation for Systematic Generalization.](http://arxiv.org/abs/2309.16467) | 系统化泛化是人类的关键技能之一，组合式程序生成器（CPG）通过模块化、类型抽象和递归组合的特征，能够以少样本的方式对新概念进行系统化的泛化，在各种语言任务上具有生产力。 |
| [^152] | [Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models.](http://arxiv.org/abs/2309.16316) | Astroconformer是一个基于Transformer的深度学习框架，旨在从恒星光曲线中捕捉长程依赖关系。通过应用于Kepler光曲线数据集，实现了对恒星表面重力的准确估计。 |
| [^153] | [ICML 2023 Topological Deep Learning Challenge : Design and Results.](http://arxiv.org/abs/2309.15188) | 本文介绍了ICML 2023拓扑深度学习挑战，该挑战要求参与者在两个月内提供开源实现的拓扑神经网络，吸引了28个合格的提交。 |
| [^154] | [Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models.](http://arxiv.org/abs/2309.14068) | 本文发现了扩散模型在反向降噪中存在一个表达瓶颈，并且推出了一种新的软混合降噪（SMD）模型，该模型在理论上能够很好地逼近任意高斯混合分布，并且在实现上简单高效。 |
| [^155] | [Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes.](http://arxiv.org/abs/2309.12971) | 本文提出了基于花瓣拉普拉斯的高阶图卷积网络，通过利用简单复合体来建模高阶交互，在不同拓扑尺度上识别内在特征，并使用可学习的图滤波器来量化高阶交互强度。 |
| [^156] | [Virchow: A Million-Slide Digital Pathology Foundation Model.](http://arxiv.org/abs/2309.07778) | Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。 |
| [^157] | [Chat Failures and Troubles: Reasons and Solutions.](http://arxiv.org/abs/2309.03708) | 本文研究了人机交互中聊天失败和问题的原因，提出了闭环控制算法和强化学习模型等解决方案以降低错误。 |
| [^158] | [BridgeData V2: A Dataset for Robot Learning at Scale.](http://arxiv.org/abs/2308.12952) | BridgeData V2是一个大规模且多样化的机器人操作行为数据集，广泛应用于机器人学习研究。该数据集具备任务和环境变异性，并且兼容多种学习方法，通过实验表明，使用此数据集可以提高模型性能。 |
| [^159] | [Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models.](http://arxiv.org/abs/2308.10462) | 本文探索了大型语言模型在资源有限的环境下用于代码生成的参数高效微调技术，并提出了参数高效微调作为一种有前途的方法，可以在保持合理资源消耗的同时，高效地将语言模型专门用于任务特定的数据。 |
| [^160] | [LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs.](http://arxiv.org/abs/2308.08469) | 这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。 |
| [^161] | [Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization.](http://arxiv.org/abs/2308.05021) | 扩散模型可能受到错误传播的影响，本文通过理论分析和数据实证验证了这个问题，并提出了一种一致性正则化方法来解决。我们的分析揭示了模型的去噪模块是否具有容错性以及如何减少分布差异。 |
| [^162] | [Linear Convergence Bounds for Diffusion Models via Stochastic Localization.](http://arxiv.org/abs/2308.03686) | 通过随机定位方法，我们提供了一种解决扩散模型中线性收敛界限问题的方法，并证明了这种方法可以在有限二阶矩条件下达到可接受的精度。 |
| [^163] | [Curvature-based Transformer for Molecular Property Prediction.](http://arxiv.org/abs/2307.13275) | 该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。 |
| [^164] | [Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives.](http://arxiv.org/abs/2307.02140) | 本文探讨了开放联邦学习平台的技术和法律观察，提出了基于查询和基于合同的两种适用于开放联邦学习的合作框架，并对构建开放的FL平台的可行性进行了全面评估。 |
| [^165] | [Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data.](http://arxiv.org/abs/2306.05535) | 政治辩论、演讲和访谈中的值得核实的论断可以使用音频数据进行检测和确认，这可帮助主持人、记者和事实核查组织进行工作。 |
| [^166] | [Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares.](http://arxiv.org/abs/2306.04961) | 该论文提出了一种非凸迭代加权最小二乘法用于同时恢复行稀疏和低秩的数据矩阵，能够在最小样本复杂度的情况下局部二次收敛到同时结构化的数据矩阵，并在实验中表现出有利的经验收敛性。 |
| [^167] | [Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation.](http://arxiv.org/abs/2306.00788) | 本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。 |
| [^168] | [Thought Cloning: Learning to Think while Acting by Imitating Human Thinking.](http://arxiv.org/abs/2306.00323) | 本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。 |
| [^169] | [Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization.](http://arxiv.org/abs/2305.19838) | 本文提出了一种放松去中心化无遗憾高维贝叶斯优化中加法约束的方法，并且解决了过度探索问题。该方法称为DumBO，并且在实验中展示了竞争性能。 |
| [^170] | [Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization.](http://arxiv.org/abs/2305.18417) | 本文描述了一个基于点过程注意力和网格编码的算法，在分布外测试集上实现了泛化能力，为理解大脑强泛化能力提供了见解。 |
| [^171] | [A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem.](http://arxiv.org/abs/2305.17198) | 提出了一个基于模型的离线多智能体强化学习方法MOMA-PPO，通过生成合成交互数据并优化智能体的政策，解决了策略一致性和策略微调两个协调问题，在具有挑战性的离线MARL场景中胜过主流的学习方法，提供了实际应用中的可行解决方案。 |
| [^172] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^173] | [GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs.](http://arxiv.org/abs/2305.12788) | 本论文提出了一种名为GraphCare的框架，通过使用个性化知识图谱来改进基于电子健康记录的医疗预测，并通过在两个公共数据集上的实验证明了其有效性。 |
| [^174] | [Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites.](http://arxiv.org/abs/2305.09820) | 这篇论文研究了机器生成文章在虚假新闻和主流新闻网站的普及程度，发现虚假新闻网站上合成文章的使用速度比主流网站上更快。 |
| [^175] | [DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference.](http://arxiv.org/abs/2305.07376) | 本论文提出了一种基于多项式近似的SRAM内数字乘法器，在不依赖于新型存储技术和避免了比特串行计算的情况下，通过内存执行GEMM计算，从而为DNN训练和推理提供了高效的加速器。 |
| [^176] | [Symbolic Regression on FPGAs for Fast Machine Learning Inference.](http://arxiv.org/abs/2305.04099) | 本论文提出了一种全新的利用符号回归的全流程，可在FPGA上进行机器学习推断，具有优化性能-资源平衡的特点。 |
| [^177] | [Explainable Reinforcement Learning via a Causal World Model.](http://arxiv.org/abs/2305.02749) | 本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。 |
| [^178] | [Variations on a Theme by Blahut and Arimoto.](http://arxiv.org/abs/2305.02650) | 本文提出了BA算法的一种新的修改，通过让乘数在每次迭代中通过一维求根来更新，这使得算法能够直接计算所需失真的RD函数，而无需像原始算法一样探索整个RD曲线。 |
| [^179] | [Hyperbolic Image-Text Representations.](http://arxiv.org/abs/2304.09172) | 本文提出了一个使用双曲表示捕捉图像和文本层次结构的对比模型MERU，并证明其在多模态任务上与CLIP相当。 |
| [^180] | [CodeKGC: Code Language Model for Generative Knowledge Graph Construction.](http://arxiv.org/abs/2304.09048) | 本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。 |
| [^181] | [Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box.](http://arxiv.org/abs/2304.05527) | 本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。 |
| [^182] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^183] | [Training Neural Networks is NP-Hard in Fixed Dimension.](http://arxiv.org/abs/2303.17045) | 研究了训练具有ReLU和线性阈值激活函数的两层神经网络的固定维度下的NP难度。 回答了两个问题，证明了这两个问题在二维情况下是NP难的，此外在ReLU案例中证明了固定参数问题的参数化固定复杂度维数和ReLU数量的组合参数。 |
| [^184] | [Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning.](http://arxiv.org/abs/2303.15579) | 本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。 |
| [^185] | [Discovering mesoscopic descriptions of collective movement with neural stochastic modelling.](http://arxiv.org/abs/2303.09906) | 介观集体运动的随机特征对确定性和随机性动力学建模至关重要。作者利用物理启发、神经网络和随机微分方程来研究相互作用个体的群体动力学，并对其进行了鉴定和分析，为这些系统的秩序性质提供了新颖的见解。 |
| [^186] | [ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure.](http://arxiv.org/abs/2303.02472) | ESD是一种无需调参的可训练校准目标损失，通过将校准误差看作两个期望值之间的平方差，可以改善神经网络模型的校准度。 |
| [^187] | [Unboxing Tree Ensembles for interpretability: a hierarchical visualization tool and a multivariate optimal re-built tree.](http://arxiv.org/abs/2302.07580) | 本论文开发了一种层次化可视化工具和多变量最优重建树，用于提高树集成模型的可解释性和洞察力。 |
| [^188] | [Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification.](http://arxiv.org/abs/2302.02070) | SGID是一种使用扩散模型的语义引导生成图像增强方法，旨在在图像分类中平衡图像多样性和语义一致性。实验证明，SGID在ResNet-5上的效果优于最佳增强基线1.72％。 |
| [^189] | [Versatile Energy-Based Probabilistic Models for High Energy Physics.](http://arxiv.org/abs/2302.00695) | 本文提出了一个多功能的能量概率模型，用于描述高能物理事件，可用于参数化的事件生成，异常信号探测以及粒子识别。 |
| [^190] | [Increasing biases can be more efficient than increasing weights.](http://arxiv.org/abs/2301.00924) | 通过增加偏差而不是权重，可以显著提高神经网络模型的性能，并提供了优化神经网络信息流的替代视角。 |
| [^191] | [Detecting Change Intervals with Isolation Distributional Kernel.](http://arxiv.org/abs/2212.14630) | 这篇论文提出了一种基于隔离分布核的CID方法，用于在数据流中高效识别各种类型的变点并容忍异常值。 |
| [^192] | [An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning.](http://arxiv.org/abs/2211.11086) | 本文研究了一种名为SimiS的简单但被忽视的基准方法，通过将伪标签作为标签数据的补充，根据与最频繁类别的类别分布差异，有效地减少了不平衡半监督学习中的类别不平衡，相对于现有方法有显著的性能提升。 |
| [^193] | [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks.](http://arxiv.org/abs/2210.15629) | 本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。 |
| [^194] | [Normality-Guided Distributional Reinforcement Learning for Continuous Control.](http://arxiv.org/abs/2208.13125) | 本论文研究了连续控制任务中的值分布，并发现学习的值分布与正态分布非常接近。基于这一观察，提出了一种正态引导的分布式强化学习方法，利用方差网络预测的方差和回报，以及与标准值函数不同的值分布结构特征来更新策略。这种方法在两种在线算法上产生了显著效果。 |
| [^195] | [Climate-Invariant Machine Learning.](http://arxiv.org/abs/2112.08440) | 本研究提出了一种新的框架——"气候不变"的机器学习，通过将气候过程的知识纳入机器学习算法中，可以在广泛的气候和地理条件下保持高准确性。 |
| [^196] | [Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models.](http://arxiv.org/abs/2011.14238) | 我们提出了一种用于贝叶斯层次回归模型的近似交叉验证均值估计的新方法，通过在方差-协方差参数上进行条件，将交叉验证问题转化为简单的优化问题，从而提高了大型BHRMs的可行性。 |

# 详细

[^1]: 一个简单的潜在扩散方法应用于全景分割和遮罩修复

    A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting. (arXiv:2401.10227v1 [cs.CV])

    [http://arxiv.org/abs/2401.10227](http://arxiv.org/abs/2401.10227)

    该论文提出了一种基于稳定扩散的潜在扩散方法，用于全景分割和遮罩修复，通过简化架构来避免复杂性，实现了生成模型解锁遮罩修复功能，具有应用于交互式分割的潜力。

    

    全景和实例分割网络通常通过专门的目标检测模块，复杂的损失函数和特殊的后处理步骤来训练，以处理实例遮罩的置换不变性。

    Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.
    
[^2]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^3]: AutoFT: 通过优化OOD数据上的超参数实现鲁棒的微调

    AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])

    [http://arxiv.org/abs/2401.10220](http://arxiv.org/abs/2401.10220)

    AutoFT是一种通过优化超参数在OOD数据上进行基础模型微调的方法，以实现鲁棒性能。

    

    基础模型可以通过在特定任务的数据上进行微调来适应所需任务，但是在一种特定数据分布上微调模型往往会损害模型在其他分布上的原始性能。目前的鲁棒微调方法利用手工制定的正则化技术来约束微调过程，以保留基础模型的特征。然而，很难准确指定在微调过程中应保留哪些基础模型的特征，因为这取决于预训练、微调和评估数据分布之间的关系。我们提出了AutoFT，一种数据驱动的方法来引导基础模型的微调。AutoFT通过优化微调超参数来在小的ODD验证集上最大化性能。为了以细粒度的方式指导微调，AutoFT搜索一个高度表达的超参数空间，其中包括许多不同的权重系数。

    Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different lo
    
[^4]: 通过Gaunt张量积在傅里叶基础上实现高效的等变操作

    Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products. (arXiv:2401.10216v1 [cs.LG])

    [http://arxiv.org/abs/2401.10216](http://arxiv.org/abs/2401.10216)

    该论文提出了一种加速计算不可约表示张量积的方法，通过将等变操作基础从球形谐波改变为2D傅立叶基础，实现了对E(3)群的等变神经网络的高效建模。

    

    在建模现实世界应用中的3D数据时，发展E(3)群的等变神经网络起着重要作用。实现这种等变性主要涉及到不可约表示（irreps）的张量积。然而，随着使用高阶张量，这些操作的计算复杂性显著增加。在这项工作中，我们提出了一种系统的方法来大大加速不可约表示的张量积的计算。我们将常用的Clebsch-Gordan系数与Gaunt系数进行了数学上的连接，Gaunt系数是三个球形谐波乘积的积分。通过Gaunt系数，不可约表示的张量积等价于由球形谐波表示的球形函数之间的乘法。这种观点进一步使我们能够将等变操作的基础从球形谐波改变为2D傅立叶基础。因此，球形函数之间的乘法可以在傅立叶基础上进行。

    Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions 
    
[^5]: 使用机器学习改进自动检测驾驶员疲劳和注意力分散

    Improving automatic detection of driver fatigue and distraction using machine learning. (arXiv:2401.10213v1 [cs.CV])

    [http://arxiv.org/abs/2401.10213](http://arxiv.org/abs/2401.10213)

    本文介绍了使用机器学习技术改进的自动检测驾驶员疲劳和注意力分散的方法，包括基于视觉和面部对齐网络的驾驶疲劳检测以及基于卷积神经网络的注意力分散行为识别。实验结果表明了这些方法的有效性。

    

    近年来，信息技术的变化和进步在智能汽车系统的发展中发挥了重要作用。驾驶员疲劳和注意力分散是交通事故中的重要因素。因此，对驾驶行为进行车内监测已成为智能汽车高级驾驶辅助系统的关键组成部分。在本文中，我们介绍了使用基于视觉和机器学习方法同时检测疲劳和注意力分散行为的技术。在驾驶疲劳检测中，我们使用面部对齐网络来识别图像中的面部特征点，并计算面部特征点的距离来检测眼睛和嘴巴的开闭。此外，我们使用基于MobileNet架构的卷积神经网络(CNN)来识别各种注意力分散行为。在基于PC平台的摄像头设置上进行实验，并使用publ结果进行演示。

    Changes and advances in information technology have played an important role in the development of intelligent vehicle systems in recent years. Driver fatigue and distracted driving are important factors in traffic accidents. Thus, onboard monitoring of driving behavior has become a crucial component of advanced driver assistance systems for intelligent vehicles. In this article, we present techniques for simultaneously detecting fatigue and distracted driving behaviors using vision-based and machine learning-based approaches. In driving fatigue detection, we use facial alignment networks to identify facial feature points in the images, and calculate the distance of the facial feature points to detect the opening and closing of the eyes and mouth. Furthermore, we use a convolutional neural network (CNN) based on the MobileNet architecture to identify various distracted driving behaviors. Experiments are performed on a PC based setup with a webcam and results are demonstrated using publ
    
[^6]: 通过多粒度结构和多尺度序列表示的耦合改进PTM位点预测

    Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation. (arXiv:2401.10211v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.10211](http://arxiv.org/abs/2401.10211)

    本文提出了一种通过多粒度结构和多尺度序列表示耦合的PTM位点预测方法PTM-CMGMS，该方法在结构表示学习和序列表示学习上进行优化，提高了PTM位点预测的准确性。

    

    蛋白质翻译后修饰（PTM）位点预测是生物信息学中的重要任务。已经开发了几种计算方法来预测PTM位点。然而，现有方法忽略了结构信息，仅利用蛋白质序列。此外，PTM是发生在原子粒度的生物事件，所以迫切需要设计一种更精细的结构表示学习方法。在本文中，我们提出了一种通过多粒度结构和多尺度序列表示耦合的PTM位点预测方法，简称PTM-CMGMS。具体而言，我们设计了多粒度结构感知表示学习方法，从AlphaFold预测的结构中学习氨基酸、原子和整个蛋白质的邻域结构表示，然后利用对比学习优化结构表示。此外，我们还使用了多尺度序列表示学习来提取上下文序列信息。

    Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context seq
    
[^7]: 通过精通指导的非参数聚类来扩大策略预测规模

    Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction. (arXiv:2401.10210v1 [cs.CY])

    [http://arxiv.org/abs/2401.10210](http://arxiv.org/abs/2401.10210)

    通过精通指导的非参数聚类方法，预测学生在问题解决中可能采用的策略，从而实现自适应教学系统的个性化体验。

    

    预测学生在解决问题时可能使用的策略（概念序列）有助于自适应教学系统（AISs）根据他们的学习能力更好地适应不同类型的学习者。这可以为学生提供更动态、有趣和个性化的学习体验。为了扩大训练一个可以覆盖大规模教育数据集的预测模型（如LSTMs），我们开发了一种非参数方法来对数据中的对称实例进行聚类。具体来说，我们利用基于Node2Vec的表示学习将掌握或技能水平上的对称性编码为策略，因为解决问题时，学生的策略很可能涉及他们已经掌握的概念。利用这种表示，我们使用DP-Means通过对聚类的粗细调整来对对称实例进行分组。我们将我们的模型应用到从MATHia（一家中学数学学习领先的AIS）的大规模数据集中学习数学学习策略。

    Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student's strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning.
    
[^8]: 用于解释性深度神经网络入侵检测系统的折衷规则提取

    Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems. (arXiv:2401.10207v1 [cs.CR])

    [http://arxiv.org/abs/2401.10207](http://arxiv.org/abs/2401.10207)

    本文研究了用于解释性深度神经网络入侵检测系统的折衷规则提取，旨在解决黑盒解释器的不可信任问题。

    

    本文针对黑盒算法和代理解释器在可解释性入侵检测系统(X-IDS)中所引发的信任问题进行研究。虽然可解释的人工智能(XAI)旨在提高透明度，但黑盒代理解释器，如局部可解释的模型无关解释(LIME)和SHapley加法解释(SHAP)，很难信任。这些代理解释器的黑盒特性使得解释生成的过程不透明且难以理解。为了避免这个问题，可以使用透明的白盒算法，如规则提取(RE)。规则提取有三种类型的算法:教育、分解和折衷。教育方法提供快速但不可信赖的白盒解释，而分解规则提取提供了可信赖但可扩展性较差的解释。本研究探讨了折衷规则提取，它在可扩展性和可信赖性之间达到了平衡。通过综合不同的技术方法，

    This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniq
    
[^9]: 分而不忘：连续学习中选择性训练专家的集成方法

    Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])

    [http://arxiv.org/abs/2401.10191](http://arxiv.org/abs/2401.10191)

    连续学习中，我们提出了一种名为SEED的新方法，通过选择性训练最优的专家来解决遗忘和计算负担的问题，并在实验中展示了其高性能。

    

    随着类增量学习的流行，模型能够拓宽应用范围，同时不忘记已经学到的知识。在这个领域中的一个趋势是使用混合专家技术，不同的模型共同解决任务。然而，这些专家通常会一次性使用整个任务的数据进行训练，这样会增加遗忘的风险和计算负担。为了解决这个问题，我们提出了一种名为SEED的新方法。SEED仅选择一个被认为最优的专家来处理给定的任务，并使用该任务的数据对这个专家进行微调。为此，每个专家用高斯分布表示每个类别，并根据这些分布的相似性选择最优专家。因此，SEED在保持集成方法的高稳定性的同时增加了专家之间的多样性和异质性。大量实验证明SEED达到了最先进的性能。

    Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performan
    
[^10]: 一种启发于Kaczmarz的方法加速神经网络波函数的优化

    A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions. (arXiv:2401.10190v1 [physics.comp-ph])

    [http://arxiv.org/abs/2401.10190](http://arxiv.org/abs/2401.10190)

    本论文提出了一种启发于Kaczmarz的方法加速神经网络波函数的优化，该方法结合了最小步长随机重构优化器(MinSR)和随机Kaczmarz方法，相对于其他方法在多个小原子和分子上表现更优。

    

    通过变分蒙特卡罗方法优化的神经网络波函数已被证明在原子和小分子的电子结构方面产生高精度结果，但是优化这种波函数的高成本限制了它们在更大系统中的应用。我们提出了Subsampled Projected-Increment Natural Gradient Descent (SPRING)优化器来减少这个瓶颈。SPRING结合了之前引入的最小步长随机重构优化器(MinSR)和经典的随机Kaczmarz方法解决线性最小二乘问题的思想。我们证明了在多个小原子和分子上，SPRING优于MinSR和流行的Kronecker-Factored Approximate Curvature方法(KFAC)，前提是各种方法的学习率都经过了最佳调整。例如，在氧原子上，SPRING在四万次训练迭代之后达到了化学精度，而MinSR和KFAC甚至在一个小时后也无法做到这一点。

    Neural network wavefunctions optimized using the variational Monte Carlo method have been shown to produce highly accurate results for the electronic structure of atoms and small molecules, but the high cost of optimizing such wavefunctions prevents their application to larger systems. We propose the Subsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer to reduce this bottleneck. SPRING combines ideas from the recently introduced minimum-step stochastic reconfiguration optimizer (MinSR) and the classical randomized Kaczmarz method for solving linear least-squares problems. We demonstrate that SPRING outperforms both MinSR and the popular Kronecker-Factored Approximate Curvature method (KFAC) across a number of small atoms and molecules, given that the learning rates of all methods are optimally tuned. For example, on the oxygen atom, SPRING attains chemical accuracy after forty thousand training iterations, whereas both MinSR and KFAC fail to do so even after one h
    
[^11]: Chem-FINESE: 通过文本重构验证细粒度少样本实体提取

    Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])

    [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)

    这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。

    

    在化学领域中，细粒度少样本实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了一种新颖的基于序列到序列的少样本实体提取方法Chem-FINESE来解决这两个挑战。我们的Chem-FINESE包含两个组件：一个序列到序列的实体提取器用于从输入句子中提取命名实体，以及一个序列到序列的自我验证模块用于从提取的实体中重构原始输入句子。受到一个好的实体提取系统需要忠实提取实体的事实启发，我们的新自我验证模块利用实体提取结果来重构原始输入句子。此外，我们设计了一种新的对比损失来减少在提取过程中的过度复制。

    Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
    
[^12]: 人体活动识别中的迁移学习：一项调查

    Transfer Learning in Human Activity Recognition: A Survey. (arXiv:2401.10185v1 [cs.LG])

    [http://arxiv.org/abs/2401.10185](http://arxiv.org/abs/2401.10185)

    这项调查论文介绍了在智能家居和可穿戴设备基于人体活动识别的应用领域中所采用的迁移学习方法。通过对贡献和挑战进行分类和呈现，提供了问题-解决方案的视角。

    

    基于传感器的人体活动识别（HAR）是一个活跃的研究领域，因其在智能环境、辅助生活、健身和医疗等方面的应用而获得了广泛关注。最近，基于深度学习的端到端训练在计算机视觉和自然语言等领域取得了最先进的性能，其中有大量注释数据可用。然而，对于基于传感器的HAR来说，大量注释数据是不可用的。此外，HAR所执行的真实世界环境在传感器模态、分类任务和目标用户方面存在差异。为了解决这个问题，迁移学习得到了广泛应用。在这项调查中，我们重点关注智能家居和可穿戴设备基于HAR的迁移学习方法。具体而言，我们通过将工作按其贡献和解决的挑战进行分类和呈现，以提供问题-解决方案的视角。

    Sensor-based human activity recognition (HAR) has been an active research area, owing to its applications in smart environments, assisted living, fitness, healthcare, etc. Recently, deep learning based end-to-end training has resulted in state-of-the-art performance in domains such as computer vision and natural language, where large amounts of annotated data are available. However, large quantities of annotated data are not available for sensor-based HAR. Moreover, the real-world settings on which the HAR is performed differ in terms of sensor modalities, classification tasks, and target users. To address this problem, transfer learning has been employed extensively. In this survey, we focus on these transfer learning methods in the application domains of smart home and wearables-based HAR. In particular, we provide a problem-solution perspective by categorizing and presenting the works in terms of their contributions and the challenges they address. We also present an updated view of
    
[^13]: 综合性的OOD检测改进

    Comprehensive OOD Detection Improvements. (arXiv:2401.10176v1 [cs.LG])

    [http://arxiv.org/abs/2401.10176](http://arxiv.org/abs/2401.10176)

    本文提出了一种综合性的OOD检测方法，包括基于表示和基于逻辑的方法。其中，在基于表示的方法中，通过降维改进了性能，在基于逻辑的方法中，通过解决一个未被注意到的缺陷提高了性能。在基准测试中，这些方法表现出卓越的性能，并取得了最新的结果。

    

    随着机器学习在重大决策中的日益普及，识别推理数据是否超出模型预期的输入分布对于给出预测的背景是至关重要的。为此，我们提出了一种分为基于表示和基于逻辑的方法的OOD检测方法。与大多数论文只关注其中一组不同，我们同时解决了两个方法。我们在基于表示的方法中对特征嵌入进行降维，以提高时间性能和性能。此外，我们提出了DICE-COL，这是一种改进了流行的基于逻辑的方法Directed Sparsification (DICE)的方法，解决了一个未被注意到的缺陷。我们在OpenOODv1.5基准框架上展示了我们方法的有效性，结果表明它们显著提高了性能并获得了最新的结果。

    As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.
    
[^14]: DISTINQT: 一种面向未来移动和无线网络的分布式隐私感知学习框架，用于QoS预测

    DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])

    [http://arxiv.org/abs/2401.10158](http://arxiv.org/abs/2401.10158)

    DISTINQT是一种面向未来移动和无线网络的隐私感知分布式学习框架，用于QoS预测。

    

    5G和6G以后的网络将支持依赖一定服务质量（QoS）的新的和具有挑战性的用例和应用程序。及时预测QoS对于安全关键应用（如车辆通信）尤为重要。尽管直到最近，QoS预测一直由集中式人工智能（AI）解决方案完成，但已经出现了一些隐私、计算和运营方面的问题。替代方案已经出现（如分割学习、联邦学习），将复杂度较低的AI任务分布在节点之间，同时保护数据隐私。然而，考虑到未来无线网络的异构性，当涉及可扩展的分布式学习方法时，会出现新的挑战。该研究提出了一种名为DISTINQT的面向QoS预测的隐私感知分布式学习框架。

    Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
    
[^15]: 一种新型的混合时变图神经网络用于交通流量预测

    A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v1 [cs.LG])

    [http://arxiv.org/abs/2401.10155](http://arxiv.org/abs/2401.10155)

    本文提出了一种新型的混合时变图神经网络（HTVGNN）用于交通流量预测，解决了现有方法中预定义图和自适应图的学习能力受限的问题。

    

    实时准确的交通流量预测是确保智能交通系统高效运行的基础。在现有基于图神经网络（GNN）的交通流量预测方法中，通常使用预定义的图来描述城市道路网络中不同交通节点的空间相关性。然而，预定义图描述空间相关性的能力受限于先前的知识和图生成方法。尽管基于数据驱动学习的时变图能部分克服预定义图的缺点，但现有自适应图的学习能力有限。例如，时变图不能充分捕捉交通流量数据中固有的空间相关性。为了解决这些问题，我们提出了一种用于交通流量预测的混合时变图神经网络（HTVGNN）。

    Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation systems.In existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow data.In order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.
    
[^16]: 多智能体强化学习在海上操作技术网络安全中的应用

    Multi-Agent Reinforcement Learning for Maritime Operational Technology Cyber Security. (arXiv:2401.10149v1 [cs.LG])

    [http://arxiv.org/abs/2401.10149](http://arxiv.org/abs/2401.10149)

    本文通过引入一个模拟环境IPMSRL，展示了将自主网络防御应用于工业控制系统的潜力，并探索了多智能体强化学习在海上操作技术网络安全中的应用。

    

    本文展示了将自主网络防御应用于工业控制系统的潜力，并提供了一个基准环境，以进一步探索多智能体强化学习在这个问题领域中的应用。它引入了一个模拟环境IPMSRL，用于一个通用集成平台管理系统(IPMS)，并探索了在通用海上IPMS操作技术(OT)中使用多智能体强化学习进行自主网络防御决策的方法。OT网络防御措施相对于企业IT来说还不够成熟。这是由于OT基础设施相对脆弱，源自于使用传统系统、设计时的工程假设以及缺乏全面的现代安全控制措施。由于不断增长的网络攻击复杂性和传统IT中心网络防御解决方案的局限性，网络安全领域存在许多挑战。

    This paper demonstrates the potential for autonomous cyber defence to be applied on industrial control systems and provides a baseline environment to further explore Multi-Agent Reinforcement Learning's (MARL) application to this problem domain. It introduces a simulation environment, IPMSRL, of a generic Integrated Platform Management System (IPMS) and explores the use of MARL for autonomous cyber defence decision-making on generic maritime based IPMS Operational Technology (OT). OT cyber defensive actions are less mature than they are for Enterprise IT. This is due to the relatively brittle nature of OT infrastructure originating from the use of legacy systems, design-time engineering assumptions, and lack of full-scale modern security controls. There are many obstacles to be tackled across the cyber landscape due to continually increasing cyber-attack sophistication and the limitations of traditional IT-centric cyber defence solutions. Traditional IT controls are rarely deployed on 
    
[^17]: 在物体中心化学习中明确解开的表示

    Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])

    [http://arxiv.org/abs/2401.10148](http://arxiv.org/abs/2401.10148)

    这篇论文提出了一种在物体中心化学习中明确解开形状和纹理成分的方法，通过将潜在空间划分为两个不重叠的子集，使得模型更加稳定和有效。

    

    从原始视觉数据中提取结构化表示是机器学习中一个重要且长期存在的挑战。最近，无监督学习物体中心化表示的技术引起了越来越多的关注。在这个背景下，增强潜在特征的稳定性可以提高下游任务训练的效率和效果。在这个方向上一个有希望的步骤是解开导致数据变化的因素。先前，不变卡槽注意实现了从其他特征中解开位置、尺度和方向。扩展这一方法，我们着重于分离形状和纹理组成部分。特别地，我们提出了一种新颖的架构，将物体中心化模型中的形状和纹理成分偏置为潜在空间维度的两个不重叠子集。这些子集是先验已知的，因此在训练过程之前。在一系列物体中心化测试中进行的实验揭示了...

    Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
    
[^18]: 利用层次交互来进行蛋白质表面学习

    Exploiting Hierarchical Interactions for Protein Surface Learning. (arXiv:2401.10144v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.10144](http://arxiv.org/abs/2401.10144)

    这个论文研究了蛋白质表面学习中的两个关键属性，即原子之间的关系和层次化特征交互，并提出了一个基于深度学习技术的框架。

    

    预测蛋白质之间的相互作用是结构生物信息学中最重要但也最具挑战性的问题之一。蛋白质表面的潜在功能位点由几何和化学特征共同确定。然而，现有的研究仅考虑手工设计或单独学习的原子类型的化学特征，并独立提取几何特征。 在本文中，我们确定了有效蛋白质表面学习的两个关键属性：1）原子之间的关系：原子通过共价键连接在一起形成生物分子，而不是单独出现，因此在化学特征学习中建模原子之间的关系具有重要意义。2）层次化特征交互：邻近残基效应验证了原子之间以及表面点和原子（或残基）之间的层次化特征交互的重要性。我们提出了一个基于深度学习技术的基于原则的框架，即Hierarchical framework。

    Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical
    
[^19]: 空间-时间大语言模型用于交通预测

    Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])

    [http://arxiv.org/abs/2401.10134](http://arxiv.org/abs/2401.10134)

    本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。

    

    交通预测是智能交通系统的关键组成部分，它通过使用历史数据来预测特定位置的未来交通情况。尽管现有的交通预测模型通常强调开发复杂的神经网络结构，但它们的准确性并未相应提高。最近，大型语言模型（LLMs）在时间序列分析方面显示出了出色的能力。与现有模型不同，LLMs主要通过参数扩展和广泛的预训练来进步，同时保持其基本结构。本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测。具体而言，ST-LLM将每个位置的时间步长定义为标记，并结合空间-时间嵌入模块来学习标记的空间位置和全局时间表示。然后，这些表示被融合以为每个标记提供统一的空间和时间信息。

    Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
    
[^20]: 走向基于原则的图形变换器

    Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])

    [http://arxiv.org/abs/2401.10119](http://arxiv.org/abs/2401.10119)

    边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。

    

    基于k维Weisfeiler-Leman（k-WL）层次结构的图形学习架构提供了理论上很好理解的表达能力。然而，这样的架构在真实任务中往往无法提供可靠的预测性能，从而限制了它们的实际影响力。相比之下，基于全局注意力的模型如图形变换器在实践中表现出了强大的性能，但是将它们的表达能力与k-WL层次结构进行比较仍然具有挑战性，尤其是因为这些架构依赖于位置或结构编码来实现其表达能力和预测性能。为了解决这个问题，我们展示了最近提出的边缘变换器，这是一个在节点对而不是节点上进行操作的全局注意力模型，具有至少3-WL的表达能力。经验上，我们证明了边缘变换器在预测性能上超过了其他理论对齐的架构，同时不依赖于位置或结构编码。

    Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
    
[^21]: 标准多导睡眠图与耳内脑电信号的比较分析：初步研究

    Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])

    [http://arxiv.org/abs/2401.10107](http://arxiv.org/abs/2401.10107)

    本研究通过比较标准多导睡眠图（PSG）和耳内脑电信号的相似性，旨在探索一种更少侵入、成本效益高和便携的替代方法。研究确定了评估方法，并通过提取特征进行分析。

    

    研究目的：多导睡眠图（PSG）目前被用作评估睡眠障碍的基准。其不舒适、不适合家庭使用以及在睡眠质量评估中引入偏差的问题需要探索更少侵入性、成本效益高和便携性的替代方法。一种有前景的候选方法是耳内脑电传感器，它在舒适性、固定电极位置、抗电磁干扰性和易于使用性方面均具有优势。本研究旨在建立一种评估耳内脑电信号与标准PSG之间相似性的方法。方法：我们评估PSG和耳内脑电推导的睡眠图之间的一致性。我们从PSG和耳内脑电信号的30秒时域和频域提取特征。我们只考虑在PSG评分员和耳内脑电评分员达成一致时的时段。我们引入一种方法来量化PSG推导和单通道耳内脑电之间的相似性。该方法包括...

    Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
    
[^22]: 学习浅层量子电路

    Learning shallow quantum circuits. (arXiv:2401.10095v1 [quant-ph])

    [http://arxiv.org/abs/2401.10095](http://arxiv.org/abs/2401.10095)

    本文提出了一个多项式时间经典算法，可以通过单比特测量数据学习浅层量子电路的描述，包括架构未知的电路和准备未知态的电路。

    

    尽管学习量子电路具有基础兴趣，但学习浅层量子电路的计算有效算法的存在仍然是一个开放的问题。由于浅层量子电路可以生成经典难以采样的分布，现有的学习算法不适用。在本文中，我们提出了一个多项式时间经典算法，通过在量子电路输出态上进行单比特测量数据，学习任意未知的$n$比特浅层量子电路$U$的描述，其架构未知，且达到小的钻石距离。我们还提供了一个多项式时间经典算法，通过在$\lvert \psi \rangle$的副本上进行单比特测量，学习由浅层量子电路$U$（在2D格子上）准备的任意未知的$n$比特态$\lvert \psi \rangle$的描述，且达到小的迹距离。我们的方法使用了基于局部反转的量子电路表示方法。

    Despite fundamental interests in learning quantum circuits, the existence of a computationally efficient algorithm for learning shallow quantum circuits remains an open question. Because shallow quantum circuits can generate distributions that are classically hard to sample from, existing learning algorithms do not apply. In this work, we present a polynomial-time classical algorithm for learning the description of any unknown $n$-qubit shallow quantum circuit $U$ (with arbitrary unknown architecture) within a small diamond distance using single-qubit measurement data on the output states of $U$. We also provide a polynomial-time classical algorithm for learning the description of any unknown $n$-qubit state $\lvert \psi \rangle = U \lvert 0^n \rangle$ prepared by a shallow quantum circuit $U$ (on a 2D lattice) within a small trace distance using single-qubit measurements on copies of $\lvert \psi \rangle$. Our approach uses a quantum circuit representation based on local inversions an
    
[^23]: 通过路径发展网络优化心房颤动患者的药物决策

    Optimizing Medication Decisions for Patients with Atrial Fibrillation through Path Development Network. (arXiv:2401.10014v1 [cs.LG])

    [http://arxiv.org/abs/2401.10014](http://arxiv.org/abs/2401.10014)

    本研究通过使用机器学习算法和路径发展网络，利用心电图数据预测心房颤动患者是否应该推荐抗凝治疗，为医生在药物决策中提供决策支持。

    

    心房颤动（AF）是一种常见的心律失常，其特征是心房快速不规则收缩。它显著增加了中风的风险，因为心房中的血流减慢，尤其是在易形成血栓的左心房附属物中。这样的血栓可以迁移到脑动脉，引发缺血性中风。为了评估是否应该为心房颤动患者开用抗凝药物，医生通常使用CHA2DS2-VASc评分系统。然而，抗凝药物的使用必须谨慎，因为它可能影响凝血功能。本研究引入了一种使用12导联心电图数据预测心房颤动患者是否应该推荐抗凝治疗的机器学习算法。在该模型中，我们使用STOME来增强时间序列数据，然后通过卷积神经网络（CNN）进行处理。通过引入路径发展层，该模型在NPV为1的条件下实现了30.6%的特异性。

    Atrial fibrillation (AF) is a common cardiac arrhythmia characterized by rapid and irregular contractions of the atria. It significantly elevates the risk of strokes due to slowed blood flow in the atria, especially in the left atrial appendage, which is prone to blood clot formation. Such clots can migrate into cerebral arteries, leading to ischemic stroke. To assess whether AF patients should be prescribed anticoagulants, doctors often use the CHA2DS2-VASc scoring system. However, anticoagulant use must be approached with caution as it can impact clotting functions. This study introduces a machine learning algorithm that predicts whether patients with AF should be recommended anticoagulant therapy using 12-lead ECG data. In this model, we use STOME to enhance time-series data and then process it through a Convolutional Neural Network (CNN). By incorporating a path development layer, the model achieves a specificity of 30.6% under the condition of an NPV of 1. In contrast, LSTM algori
    
[^24]: 发展基于人工智能的综合系统用于蜜蜂健康评估

    Developing an AI-based Integrated System for Bee Health Evaluation. (arXiv:2401.09988v1 [cs.LG])

    [http://arxiv.org/abs/2401.09988](http://arxiv.org/abs/2401.09988)

    本研究发展了一个基于人工智能的综合系统，利用视觉和音频信号来评估蜜蜂箱的健康状况，通过注意力机制的多模态神经网络实现了准确的评估。

    

    蜜蜂是全球约三分之一食物供应的传粉者，但是由于多种因素导致蜜蜂群体在过去十年中惊人地下降了近40%，包括杀虫剂和害虫等。传统的蜜蜂箱监测方法，如人工检查，主观性强，干扰性高，耗时长。为了克服这些限制，人工智能被用于评估蜜蜂箱的健康状况。然而，以往的研究缺乏一个端到端的解决方案，主要依赖于单一数据来源，如蜜蜂图像或声音。本研究引入了一个全面的系统，包括蜜蜂目标检测和健康评估。此外，该系统利用了视觉和音频信号的组合来分析蜜蜂行为。开发了一种基于注意力机制的多模态神经网络（AMNN），能够自适应地从每种信号类型中聚焦于关键特征，以进行准确的蜜蜂健康评估。AMNN实现了92.61%的整体准确率，超过了八种现有单一信号的方法。

    Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signa
    
[^25]: FLex&Chill：通过Logit Chilling改进本地联合学习训练

    FLex&Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])

    [http://arxiv.org/abs/2401.09986](http://arxiv.org/abs/2401.09986)

    FLex&Chill 提出了一种通过Logit Chilling方法改进本地联合学习训练的方法，可以加快模型收敛并提高推理精度。

    

    联合学习由于本地客户端的非iid分布式训练数据而受到数据异质性的阻碍。我们提出了一种新的联合学习模型训练方法FLex&Chill，利用了Logit Chilling方法。通过广泛的评估，我们证明在联合学习系统中固有的非iid数据特征存在的情况下，这种方法可以加快模型收敛并提高推理精度。从我们的实验中，我们观察到全局联合学习模型收敛时间提高了6倍，推理精度提高了3.37%。

    Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
    
[^26]: 心室分割：U-Net衍生模型的简要比较

    Ventricular Segmentation: A Brief Comparison of U-Net Derivatives. (arXiv:2401.09980v1 [eess.IV])

    [http://arxiv.org/abs/2401.09980](http://arxiv.org/abs/2401.09980)

    本文探讨了深度学习技术在心脏图像分割中的应用，实施了多个U-Net衍生模型以实现对心脏特定部位的全面解剖和功能分析。通过图像、图表和定量指标验证了模型的效果，并讨论了面临的挑战和未来改进策略。

    

    医学影像是指用于观察人体及其内部的技术和方法，以诊断、监测甚至治疗医学疾病。本文旨在探讨深度学习技术在心脏短轴磁共振成像图像的语义分割中的应用，旨在提高与心脏相关的医学疾病的诊断、监测和治疗。重点是实施各种U-Net的衍生体系结构，以有效地分离心脏的特定部分，进行全面的解剖和功能分析。通过图像、图表和定量指标的组合展示了模型及其预测的效果。此外，本文还讨论了遇到的挑战，并概述了未来改进的策略。本摘要简要概述了利用深度学习进行心脏图像分割的工作，强调了模型的有效性。

    Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the ac
    
[^27]: 通过邻域筛选控制高斯图模型中的虚警率

    False Discovery Rate Control for Gaussian Graphical Models via Neighborhood Screening. (arXiv:2401.09979v1 [stat.ML])

    [http://arxiv.org/abs/2401.09979](http://arxiv.org/abs/2401.09979)

    本文介绍了一种通过邻域筛选控制高斯图模型中虚警率的方法，有效解决了已存在的估计器容易出现虚警边缘检测的问题。该方法是无参数的，无需用户调整，并在数值实验中得到验证。

    

    高斯图模型在许多领域中被广泛应用。它们将变量之间的统计关系建模成一个图，其中两个变量之间的边表示条件相关性。然而，已经建立的估计器，如图形套索或邻域选择，已知容易出现大量虚警边缘检测。虚警检测可能会导致不准确甚至错误的科学解释，在生物医学或健康护理等应用中具有重大影响。在本文中，我们引入了一种基于节点的变量选择方法来学习图，并在自我估计水平上可控制所选边缘集合的虚警率。一种新颖的个体邻域融合方法产生了一个无向图估计值。所提出的方法是无参数的，不需要用户进行调整。在数值实验中与竞争的虚警率控制方法进行了基准测试。

    Gaussian graphical models emerge in a wide range of fields. They model the statistical relationships between variables as a graph, where an edge between two variables indicates conditional dependence. Unfortunately, well-established estimators, such as the graphical lasso or neighborhood selection, are known to be susceptible to a high prevalence of false edge detections. False detections may encourage inaccurate or even incorrect scientific interpretations, with major implications in applications, such as biomedicine or healthcare. In this paper, we introduce a nodewise variable selection approach to graph learning and provably control the false discovery rate of the selected edge set at a self-estimated level. A novel fusion method of the individual neighborhoods outputs an undirected graph estimate. The proposed method is parameter-free and does not require tuning by the user. Benchmarks against competing false discovery rate controlling methods in numerical experiments considering 
    
[^28]: 通过双棱镜: 光谱视角下的图数据增强用于图分类

    Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v1 [cs.LG])

    [http://arxiv.org/abs/2401.09953](http://arxiv.org/abs/2401.09953)

    通过光谱视角，研究了图数据增强方法中的图属性和结构变化的关系，发现保持低频特征值不变可以保留关键属性，提出了双棱镜（DP）增强方法，该方法灵活地保留关键的图属性同时增加图的多样性。

    

    图神经网络（GNNs）已成为处理图数据的首选工具，其通过图数据增强技术的提高加强了其有效性。尽管增强方法的发展，但图属性扭曲和受限结构变化等问题仍然存在。这引发了一个问题：是否可能开发更加保留属性并具有结构敏感性的增强方法？通过光谱镜头，我们研究了图属性、它们的增强和它们的光谱行为之间的相互作用，并发现保持低频特征值不变可以保持生成的增强图的关键属性。这些观察结果启发我们引入了双棱镜（DP）增强方法，包括DP-Noise和DP-Mask，它们灵活地保留了关键的图属性并丰富了增强图。大量实验证实了我们方法的有效性，为一种新的、有前景的直接方法提供了支持。

    Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direct
    
[^29]: SymbolNet: 自适应动态修剪的神经符号回归

    SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])

    [http://arxiv.org/abs/2401.09949](http://arxiv.org/abs/2401.09949)

    SymbolNet是一种神经网络方法，通过动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性，实现了符号回归。通过引入稀疏正则化项，我们的模型可以自适应调整自身的强度，并收敛到目标稀疏度水平。与现有方法相比，SymbolNet能高效处理具有超过10个输入的数据集。

    

    与遗传编程的使用相反，神经网络方法可在高输入维度下有效扩展，并利用梯度方法加速方程搜索。常见的表达式复杂性约束方法依赖于多阶段修剪方法进行微调，但这往往会导致显著的性能损失。在本文中，我们提出了一种神经网络方法，即SymbolNet，以一种新颖的框架实现符号回归，该框架可以在单个训练中动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性。我们引入了每个修剪类型的稀疏正则化项，该项可以自适应调整自身的强度，并导致收敛到目标稀疏度水平。与大多数现有的符号回归方法无法高效处理具有超过10个输入的数据集不同，我们证明了我们的模型的有效性。

    Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the 
    
[^30]: HGAttack: 可转移的异构图对抗攻击

    HGAttack: Transferable Heterogeneous Graph Adversarial Attack. (arXiv:2401.09945v1 [cs.LG])

    [http://arxiv.org/abs/2401.09945](http://arxiv.org/abs/2401.09945)

    这篇论文介绍了HGAttack，一种针对异构图的攻击方法。通过设计一个拟合模型和利用梯度生成扰动，该方法能够有效利用异构信息，提高攻击的可转移性和效果。

    

    异构图神经网络（HGNN）在网络和电子商务等领域的性能越来越受到认可，对抗攻击的韧性对于这些领域至关重要。然而，现有的对抗攻击方法主要针对同质图设计，应用于HGNN时，由于其对HGNN结构和语义复杂性的限制，这些方法效果不佳。本文介绍了HGAttack，这是一种针对异构图的第一种专用灰盒逃避攻击方法。我们设计了一个新的拟合模型，以与目标HGNN的行为紧密相似，并利用基于梯度的方法生成扰动。具体来说，所提出的拟合模型通过提取元路径诱导的子图并应用GNN来学习每个子图中具有不同语义的节点嵌入，有效地利用了异构信息。这种方法提高了生成的攻击对目标HGNN的可转移性，并显著减少了攻击数量。

    Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for their performance in areas like the web and e-commerce, where resilience against adversarial attacks is crucial. However, existing adversarial attack methods, which are primarily designed for homogeneous graphs, fall short when applied to HGNNs due to their limited ability to address the structural and semantic complexity of HGNNs. This paper introduces HGAttack, the first dedicated gray box evasion attack method for heterogeneous graphs. We design a novel surrogate model to closely resemble the behaviors of the target HGNN and utilize gradient-based methods for perturbation generation. Specifically, the proposed surrogate model effectively leverages heterogeneous information by extracting meta-path induced subgraphs and applying GNNs to learn node embeddings with distinct semantics from each subgraph. This approach improves the transferability of generated attacks on the target HGNN and significantly reduces m
    
[^31]: WindSeer: 在小型无人机上实时预测复杂地形上的体积风

    WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV. (arXiv:2401.09944v1 [cs.LG])

    [http://arxiv.org/abs/2401.09944](http://arxiv.org/abs/2401.09944)

    WindSeer是一个名为WindSeer的神经网络，能够在实时预测低空风的同时节省计算资源。通过使用稀疏的测量数据和合成数据进行训练，它可以成功地预测已知地形上的真实风场，并在不同的分辨率和域大小上生成准确的预测结果，无需重新训练。

    

    实时高分辨率的风预测对于包括有人和无人航空在内的各种应用都很有益。当前的天气模型需要太多的计算，并且缺乏必要的预测能力，因为它们仅在多千米和几小时的尺度上有效 - 这远低于这些应用所需的空间和时间分辨率。我们的工作首次展示了在有限计算设备上实时预测低空风的能力，仅使用稀疏的测量数据训练了名为WindSeer的神经网络。我们使用计算流体力学模拟的合成数据进行训练，并展示它可以成功地从仅有少量的噪声和空间聚集的风测量数据中预测出已知地形上的真实风场。WindSeer可以在之前未见过的地形上以不同的分辨率和域大小生成准确的预测结果，无需重新训练。我们证明了该模型成功预测了历史风场。

    Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical w
    
[^32]: 无限时域图滤波器：利用幂级数增强稀疏信息聚合

    Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v1 [cs.LG])

    [http://arxiv.org/abs/2401.09943](http://arxiv.org/abs/2401.09943)

    提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，并能集成任何幂级数并捕捉长程依赖关系。

    

    近年来，图神经网络在各种图学习任务中表现出了相当的有效性，特别是基于消息传递方法。然而，它们的性能常常受到有限接收域的限制，在稀疏图存在的情况下，挑战变得更加严峻。鉴于具有无限扩展能力的幂级数，我们提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。具体而言，我们的GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，可以在频谱和空间域中进行分析。此外，我们还从理论上证明了我们的GPFN是一个通用框架，可以集成任何幂级数并捕捉长程依赖关系。最后，我们在三个数据集上进行了实验，并展示了GPFN在稀疏图上的优越性能。

    Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three data
    
[^33]: 期望进球模型中的偏见影响了射门能力

    Biases in Expected Goals Models Confound Finishing Ability. (arXiv:2401.09940v1 [cs.LG])

    [http://arxiv.org/abs/2401.09940](http://arxiv.org/abs/2401.09940)

    本研究旨在解决使用期望进球（xG）统计评估射门能力时的限制和偏见。研究发现，持续超出累积xG需要高射门频率。

    

    期望进球（xG）已成为评估足球分析中射门技能的一种常用工具。它涉及将球员的累积xG与实际进球数量进行比较，如果持续表现超出预期，则表明射门能力强。然而，使用xG统计评估足球射门技能仍存在争议，因为球员很难在持续表现中超出累积xG。本文旨在解决使用xG统计评估射门能力时的限制和细微差别。具体而言，我们探讨了三个假设：（1）实际进球和预期进球之间的偏差是一个不足的度量标准，因为射门结果的差异和样本量有限，（2）在累积xG计算中包含所有射门可能不合适，并且（3）xG模型中存在数据相关性引起的偏见，影响了技能测量。我们发现，持续超出累积xG需要高射门频率。

    Expected Goals (xG) has emerged as a popular tool for evaluating finishing skill in soccer analytics. It involves comparing a player's cumulative xG with their actual goal output, where consistent overperformance indicates strong finishing ability. However, the assessment of finishing skill in soccer using xG remains contentious due to players' difficulty in consistently outperforming their cumulative xG. In this paper, we aim to address the limitations and nuances surrounding the evaluation of finishing skill using xG statistics. Specifically, we explore three hypotheses: (1) the deviation between actual and expected goals is an inadequate metric due to the high variance of shot outcomes and limited sample sizes, (2) the inclusion of all shots in cumulative xG calculation may be inappropriate, and (3) xG models contain biases arising from interdependencies in the data that affect skill measurement. We found that sustained overperformance of cumulative xG requires both high shot volume
    
[^34]: 概率性真正无序规则集

    Probabilistic Truly Unordered Rule Sets. (arXiv:2401.09918v1 [cs.LG])

    [http://arxiv.org/abs/2401.09918](http://arxiv.org/abs/2401.09918)

    本论文提出了概率性真正无序规则集（TURS）方法，用于解决规则集学习中的三个缺点：强加顺序、重叠冲突和多类别目标分类问题。通过利用规则集的概率特性来解决重叠冲突，并形式化定义学习问题。

    

    最近人们经常重视规则集学习，因为它具有可解释性。然而，现有的方法存在一些缺点。首先，大多数现有方法在规则之间明确或隐含地强加顺序，这使得模型更难以理解。其次，由于处理重叠引起的冲突（即，被多个规则覆盖的实例）的困难，现有方法通常不考虑概率规则。第三，对于多类别目标的学习分类规则研究不足，因为大多数现有方法专注于二分类或通过"一对其余"方法进行多类别分类。为了解决这些缺点，我们提出了TURS，即真正无序规则集。为了解决重叠规则引起的冲突，我们提出了一种新颖的模型，利用我们的规则集的概率特性，只有当它们具有相似的概率输出时允许规则重叠。我们接下来对学习问题进行了形式化定义。

    Rule set learning has recently been frequently revisited because of its interpretability. Existing methods have several shortcomings though. First, most existing methods impose orders among rules, either explicitly or implicitly, which makes the models less comprehensible. Second, due to the difficulty of handling conflicts caused by overlaps (i.e., instances covered by multiple rules), existing methods often do not consider probabilistic rules. Third, learning classification rules for multi-class target is understudied, as most existing methods focus on binary classification or multi-class classification via the ``one-versus-rest" approach.  To address these shortcomings, we propose TURS, for Truly Unordered Rule Sets. To resolve conflicts caused by overlapping rules, we propose a novel model that exploits the probabilistic properties of our rule sets, with the intuition of only allowing rules to overlap if they have similar probabilistic outputs. We next formalize the problem of lear
    
[^35]: 使用二值神经网络实现设备上的持续学习

    Enabling On-device Continual Learning with Binary Neural Networks. (arXiv:2401.09916v1 [cs.LG])

    [http://arxiv.org/abs/2401.09916](http://arxiv.org/abs/2401.09916)

    本研究提出了一种结合了持续学习和二值神经网络的解决方案，以在资源受限的设备上进行训练，并保持竞争性能。该方法利用二值激活的重放和一种新颖的量化方案。

    

    设备上的学习仍然是一个巨大的挑战，特别是当处理资源受限的设备时，这些设备的计算能力有限。这个挑战主要源于两个关键问题：首先，嵌入式设备上可用的内存通常不足以容纳消耗内存的反向传播算法，该算法通常依赖于浮点精度。其次，对于具有极端量化级别的模型（如二值神经网络），开发学习算法至关重要，因为它们对位表示进行了剧烈减少。在本研究中，我们提出了一种解决方案，结合了持续学习（CL）和二值神经网络领域的最新进展，以在设备上进行训练同时保持竞争性能。具体而言，我们的方法利用了二值激活的重放（LR）和一种新颖的量化方案，可显着减少用于梯度计算的位数。

    On-device learning remains a formidable challenge, especially when dealing with resource-constrained devices that have limited computational capabilities. This challenge is primarily rooted in two key issues: first, the memory available on embedded devices is typically insufficient to accommodate the memory-intensive back-propagation algorithm, which often relies on floating-point precision. Second, the development of learning algorithms on models with extreme quantization levels, such as Binary Neural Networks (BNNs), is critical due to the drastic reduction in bit representation. In this study, we propose a solution that combines recent advancements in the field of Continual Learning (CL) and Binary Neural Networks to enable on-device training while maintaining competitive performance. Specifically, our approach leverages binary latent replay (LR) activations and a novel quantization scheme that significantly reduces the number of bits required for gradient computation. The experimen
    
[^36]: Qadence: 一个用于数字模拟程序的可微分接口

    Qadence: a differentiable interface for digital-analog programs. (arXiv:2401.09915v1 [quant-ph])

    [http://arxiv.org/abs/2401.09915](http://arxiv.org/abs/2401.09915)

    Qadence是一个高级编程接口，用于构建数字模拟量子程序。它具有灵活的接口、本地可微性和对真实设备执行的关注，促进了原生DAQC平台上变分量子算法的研究。

    

    数字模拟量子计算（DAQC）是一种将数字单量子比特门与作用于相互作用量子比特寄存器的全局模拟操作相结合的通用量子计算的替代范式。目前，没有任何可用的开源软件适用于表达、区分和执行DAQC范式内的程序。在这项工作中，我们通过介绍Qadence，一个在Pasqal开发的用于构建复杂数字模拟量子程序的高级编程接口，来解决这个不足。由于其灵活的接口、本地可微性和对真实设备执行的关注，Qadence致力于推进为Rydberg原子阵列等原生DAQC平台构建的变分量子算法的研究。

    Digital-analog quantum computing (DAQC) is an alternative paradigm for universal quantum computation combining digital single-qubit gates with global analog operations acting on a register of interacting qubits. Currently, no available open-source software is tailored to express, differentiate, and execute programs within the DAQC paradigm. In this work, we address this shortfall by presenting Qadence, a high-level programming interface for building complex digital-analog quantum programs developed at Pasqal. Thanks to its flexible interface, native differentiability, and focus on real-device execution, Qadence aims at advancing research on variational quantum algorithms built for native DAQC platforms such as Rydberg atom arrays.
    
[^37]: 深度和宽度在神经ODE插值中的相互作用

    Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])

    [http://arxiv.org/abs/2401.09902](http://arxiv.org/abs/2401.09902)

    本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\varepsilon$的关系有关。

    

    神经常微分方程(neural ODEs)已经成为从控制角度进行监督学习的自然工具，然而对其最佳结构的完全理解仍然难以捉摸。在这项工作中，我们研究了宽度$p$和层之间的过渡次数$L$（实际上是深度$L+1$）之间的相互作用。具体来说，我们评估了模型的表达能力，以其能够在Wasserstein误差边界$\varepsilon>0$内插值一个包含$N$对点的有限数据集$D$或两个概率测度在$\mathbb{R}^d$中。我们的发现揭示了$p$和$L$之间的平衡折衷关系，在数据集插值中，$L$随着$O(1+N/p)$的比例增长，而在测度插值中，$L=O\left(1+(p\varepsilon^d)^{-1}\right)$。在自主情况下，$L=0$，需要进行单独的研究，我们将重点关注数据集插值。我们解决了$\varepsilon$-近似控制性的放松问题，并建立了误差

    Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon>0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error 
    
[^38]: 关于大型语言模型的硬件加速器的调查

    A Survey on Hardware Accelerators for Large Language Models. (arXiv:2401.09890v1 [cs.AR])

    [http://arxiv.org/abs/2401.09890](http://arxiv.org/abs/2401.09890)

    这项论文调查了用于增强大型语言模型性能和能量效率的硬件加速器，并对多种加速器进行了深入分析，为研究人员、工程师和决策者在实际应用中优化大型语言模型的部署提供了宝贵的见解。

    

    大型语言模型（LLMs）已成为自然语言处理任务的强大工具，通过其理解和生成类似于人类文本的能力，它们正在为该领域带来革命性变革。随着对更复杂LLMs的需求不断增长，迫切需要解决与其规模和复杂性相关的计算挑战。本文针对提高大型语言模型性能和能量效率的硬件加速器进行了全面调查。通过对包括GPU、FPGA和定制架构在内的各种加速器进行研究，我们探索了旨在满足LLMs的独特计算需求的硬件解决方案的格局。本调查涵盖了对架构、性能指标和能量效率考虑的深入分析，为研究人员、工程师和决策者在实际应用中优化LLMs的部署提供了宝贵的见解。

    Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. As the demand for more sophisticated LLMs continues to grow, there is a pressing need to address the computational challenges associated with their scale and complexity. This paper presents a comprehensive survey on hardware accelerators designed to enhance the performance and energy efficiency of Large Language Models. By examining a diverse range of accelerators, including GPUs, FPGAs, and custom-designed architectures, we explore the landscape of hardware solutions tailored to meet the unique computational demands of LLMs. The survey encompasses an in-depth analysis of architecture, performance metrics, and energy efficiency considerations, providing valuable insights for researchers, engineers, and decision-makers aiming to optimize the deployment of LLMs in real-world applications.
    
[^39]: 基于弹性联邦和多智能体深度强化学习的下一代网络合作边缘缓存

    Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])

    [http://arxiv.org/abs/2401.09886](http://arxiv.org/abs/2401.09886)

    本论文提出了一种基于弹性联邦和多智能体深度强化学习的合作边缘缓存方案，通过训练个性化的本地模型，预测准确受欢迎的内容，并在不同的SBS之间合作缓存热门内容，以达到优化获取内容成本的目标。

    

    边缘缓存是下一代网络中的一种有前途的解决方案，通过赋予小型基站（SBS）中的缓存单元赋能，允许用户设备（UE）获取已在SBS中预缓存的用户请求内容。对于SBS来说，通过学习准确预测受欢迎的内容非常关键，同时保护用户个人信息。传统的联邦学习（FL）可以保护用户的隐私，但是UE之间的数据差异可能导致模型质量下降。因此，有必要为每个UE训练个性化的本地模型以准确预测受欢迎的内容。此外，下一代网络中相邻SBS之间可以共享缓存的内容，因此在不同的SBS中缓存预测到的热门内容可能会影响获取内容的成本。因此，确定合作缓存热门内容的位置至关重要。为了解决这些问题，我们提出了一种基于弹性联邦和多智能体深度强化学习的合作边缘缓存方案。

    Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and mu
    
[^40]: GA-SmaAt-GNet：用于极端降水暂时预测的生成对抗小型注意力GNet

    GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting. (arXiv:2401.09881v1 [cs.LG])

    [http://arxiv.org/abs/2401.09881](http://arxiv.org/abs/2401.09881)

    GA-SmaAt-GNet是一种生成对抗架构，用于改进极端降水暂时预测的深度学习模型性能。它通过创建一个新的生成器和使用注意力增强鉴别器来利用降水掩码提供的额外信息，提高了预测质量。

    

    近年来，数据驱动的建模方法在各种气象应用中获得了相当的关注，尤其是在天气预报领域。然而，这些方法在处理极端天气条件时常常面临挑战。鉴于此，我们提出了GA-SmaAt-GNet，这是一种新颖的生成对抗结构，利用了两种方法来增强深度学习模型在极端降水暂时预测中的性能。首先，它使用了一种基于成功的SmaAt-UNet结构构建的新型SmaAt-GNet作为生成器。该网络将降水掩码（二值化降水图）作为附加数据源，利用有价值的信息进行改进的预测。此外，GA-SmaAt-GNet还使用了一个灵感来自于著名的Pix2Pix结构的注意力增强鉴别器。另外，我们使用荷兰的实际降水数据集对GA-SmaAt-GNet的性能进行了评估。

    In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherland
    
[^41]: 基于注意力的循环神经网络对自动行为下蛋鸡识别的研究

    Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])

    [http://arxiv.org/abs/2401.09880](http://arxiv.org/abs/2401.09880)

    本研究提出了一种基于注意力的循环神经网络用于自动识别下蛋鸡行为。通过声音分析和特征提取，构建了一个鲁棒的行为特征化系统，对下蛋鸡的健康行为进行监测和识别。实验结果表明该模型具有良好的综合性能。

    

    现代养禽业的一个关注点是下蛋鸡的鸣叫声，其中包含了关于健康行为的非常有用的信息。这些信息被用作健康和福祉的指标，帮助养殖人员更好地监测下蛋鸡，从而及早发现问题，以便进行更快和更有效的干预。本研究专注于对下蛋鸡鸣叫类型的声音分析，以提出一种鲁棒的行为特征化系统，以便更好地监测下蛋鸡。为此，我们首先收集并注释了下蛋鸡的鸣叫信号，然后设计了一个基于时间和频率域特征组合的最佳声学特征化方法。然后我们使用这些特征构建了基于循环神经网络的多标签分类模型，将语义类别分配给描述下蛋鸡行为的鸣叫声。结果表明，我们的模型在综合性能上表现出色。

    One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combinati
    
[^42]: 调和空间和时间抽象化以实现目标表示

    Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])

    [http://arxiv.org/abs/2401.09870](http://arxiv.org/abs/2401.09870)

    本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。

    

    目标表示通过将复杂的学习问题分解为更容易的子任务来影响分层强化学习算法的性能。最近的研究表明，保留时间抽象环境动态的表示方法在解决困难问题和提供优化理论保证方面是成功的。然而，这些方法在环境动态越来越复杂（即时间抽象转换关系依赖更多变量）的任务中无法扩展。另一方面，其他方法则尝试使用空间抽象来缓解前面的问题。它们的限制包括无法适应高维环境和对先前知识的依赖。本文提出了一种新的三层分层强化学习算法，分层结构的不同层次引入了空间和时间目标抽象化。我们对学习策略的遗憾边界进行了理论研究。我们评估了我们提出的算法在不同任务上的性能。

    Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
    
[^43]: 提高图像-文本预训练中细粒度理解的方法

    Improving fine-grained understanding in image-text pre-training. (arXiv:2401.09865v1 [cs.CV])

    [http://arxiv.org/abs/2401.09865](http://arxiv.org/abs/2401.09865)

    本研究引入了一种名为SPARC的方法，通过在图像-文本对中学习每个令牌的图像组合，以提高图像-文本预训练中的细粒度理解能力。SPARC方法结合了细粒度损失和对比损失，可以以较低的计算成本学习同时编码全局和局部信息的表示。

    

    我们引入了一种名为SPARC的方法，用于从图像-文本对中预训练更细粒度的多模态表示。考虑到多个图像块通常对应于单个单词，我们提出为每个字幕令牌学习一组图像块的方法。为了实现这一点，我们使用了图像块和语言令牌之间的稀疏相似度度量，并计算出每个令牌的语言分组的视觉嵌入，作为图像块的加权平均值。然后，通过一种仅依赖于单个样本而不需要其他批次样本作为负样本的细粒度序列损失，对令牌和语言分组的视觉嵌入进行对比。这使得可以以较低的计算成本学习更详细的信息。SPARC将这种细粒度损失与全局图像和文本嵌入之间的对比损失相结合，以同时编码全局和局部信息的表示。

    We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thorough
    
[^44]: 大型语言模型提示的进化多目标优化以平衡情感

    Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])

    [http://arxiv.org/abs/2401.09862](http://arxiv.org/abs/2401.09862)

    本研究提出了一种针对语言模型提示优化的进化多目标方法，通过情感分析为案例研究，实现了生成能够同时体现两种相互冲突情感的提示语，从而提高模型的性能和相关信息的提取能力。

    

    大型语言模型（LLMs）如ChatGPT的出现引起了各个领域的广泛关注，因为它们的性能和多功能性非凡。随着这些模型的使用不断增长，有效的提示工程变得越来越重要。提示优化成为一个关键挑战，因为它直接影响模型性能和相关信息的提取。最近，进化算法（EAs）在解决这个问题方面显示出了希望，为新的优化策略铺平了道路。在这项工作中，我们提出了一种特别针对提示优化的进化多目标（EMO）方法，称为EMO-Prompts，以情感分析作为案例研究。我们将情感分析能力作为我们的实验目标。我们的结果表明，EMO-Prompts能够有效地生成提示，使LLM能够同时产生体现两种相互冲突情感的文本。

    The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.
    
[^45]: FREED++:通过彻底的复制改善基于片段的分子生成的RL代理

    FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction. (arXiv:2401.09840v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.09840](http://arxiv.org/abs/2401.09840)

    本研究改进了名为FREED的RL模型，通过复制、审查和简化，使其在蛋白质条件下的分子生成中具有更好的质量和性能

    

    新型治疗药物的理性设计旨在找到具有所需生物功能的分子结构，例如，通过与特定蛋白质结合来激活或抑制它。分子对接是评估蛋白质-分子相互作用的常见技术。最近，强化学习（RL）已经成为一种有希望的方法，通过对接评分（DS）作为奖励来生成分子。在这项工作中，我们复制、审查和改进了最近用于分子生成的RL模型FREED（arXiv：2110.01219）。对所提出方法的广泛评估揭示了一些局限性和挑战，尽管对三个目标蛋白质报告了杰出的结果。我们的贡献包括修复了许多实现错误，简化了模型并提高了其质量，大大扩展了实验范围，并与当前最先进的蛋白质条件下分子生成方法进行了准确比较。我们展示了该方法的鲁棒性和高效性

    A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the res
    
[^46]: PPNet: 一种用于端到端近似最优路径规划的新颖神经网络结构

    PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])

    [http://arxiv.org/abs/2401.09819](http://arxiv.org/abs/2401.09819)

    PPNet是一种新颖的神经网络结构，用于解决端到端近似最优路径规划问题。通过将路径规划问题分为两个子问题，并使用两级级联神经网络进行求解，同时引入了一种高效的数据生成方法EDaGe-PP。实验结果表明，PPNet在计算时间和成功率方面比其他方法有显著提升。

    

    传统的路径规划器，如基于采样的路径规划器，在初始解敏感性和收敛到最优解速度上具有局限性。然而，在许多应用中，如具有有限功率/燃料的自动驾驶车辆中，在短时间内找到近似最优解是具有挑战性的。为了实现端到端近似最优路径规划器，我们首先将路径规划问题分为两个子问题，即路径空间分段和给定路径空间中的航点生成。我们进一步提出了一个名为路径规划网络（PPNet）的两级级联神经网络来解决路径规划问题，通过解决上述子问题。此外，我们提出了一种名为EDaGe-PP的用于路径规划的高效数据生成方法。结果显示，PPNet训练集由EDaGe-PP生成的成功率相比其他方法提升了$2\times$，总计算时间少于1/33。我们验证了PPNet的性能。

    The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNe
    
[^47]: 点击驱动与质量: 如何优化基于参与度的策略影响在线平台的内容格局

    Clickbait vs. Quality: How Engagement-Based Optimization Shapes the Content Landscape in Online Platforms. (arXiv:2401.09804v1 [cs.GT])

    [http://arxiv.org/abs/2401.09804](http://arxiv.org/abs/2401.09804)

    研究表明，基于参与度的优化策略在在线平台中对内容格局产生了重要影响，其既鼓励了质量投资，又奖励了点击驱动等战术。然而，在均衡状态下，内容质量和战术之间呈正相关，而用户消费的平均内容质量可能会下降，基于参与度的优化策略在用户体验方面可能表现得更差。

    

    在线内容平台普遍使用基于参与度的优化策略来进行推荐。这鼓励内容创作者投资于质量，但也会奖励使用点击驱动等战术。为了了解对内容格局的总体影响，我们研究了内容创作者基于参与指标竞争的游戏，并分析了关于质量投资和战术的均衡决策。首先，我们证明了均衡状态下创建的内容在质量和战术之间呈正相关，并在Twitter数据集上进行了实证验证。利用内容格局的均衡结构，我们还从多个方面研究了基于参与度的优化策略的下游绩效。也许逆反地，用户消费的平均内容质量在均衡状态下可能会下降，因为战术对内容创作者的成本更高。此外，基于参与度的优化策略在用户体验方面可能表现得更差。

    Online content platforms commonly use engagement-based optimization when making recommendations. This encourages content creators to invest in quality, but also rewards gaming tricks such as clickbait. To understand the total impact on the content landscape, we study a game between content creators competing on the basis of engagement metrics and analyze the equilibrium decisions about investment in quality and gaming. First, we show the content created at equilibrium exhibits a positive correlation between quality and gaming, and we empirically validate this finding on a Twitter dataset. Using the equilibrium structure of the content landscape, we then examine the downstream performance of engagement-based optimization along several axes. Perhaps counterintuitively, the average quality of content consumed by users can decrease at equilibrium as gaming tricks become more costly for content creators to employ. Moreover, engagement-based optimization can perform worse in terms of user ut
    
[^48]: 一种快速、高性能、安全的分布式训练框架用于大型语言模型

    A Fast, Performant, Secure Distributed Training Framework For Large Language Model. (arXiv:2401.09796v1 [cs.LG])

    [http://arxiv.org/abs/2401.09796](http://arxiv.org/abs/2401.09796)

    本文提出了一种基于模型切片的安全分布式语言模型训练框架。通过在客户端和服务器端部署可信执行环境（TEE）并使用轻量级加密进行安全通信，解决了恶意窃取模型参数和数据的问题。此外，采用分割微调和稀疏化参数微调的方法，在降低设备成本的同时提高了模型性能和准确性。

    

    分布式（联邦式）LLM是一种重要的方法，用于使用隔离数据共同训练领域特定的LLM。然而，恶意地从服务器或客户端窃取模型参数和数据已经成为一个迫切需要解决的问题。在本文中，我们提出了一种基于模型切片的安全分布式LLM。在这种情况下，我们在客户端和服务器端都部署了可信执行环境（TEE），并将微调的结构（LoRA或P-tuning v2的嵌入）放入TEE中。然后，通过轻量级加密，在TEE和通用环境中进行安全通信。为了进一步降低设备成本并提高模型性能和准确性，我们提出了一种分割微调方案。特别是，我们通过层次进行LLM的分割，将后面的层次放在服务器端TEE中（客户端不需要TEE）。然后，我们将提出的稀疏化参数微调（SPF）与LoRA部分相结合，以提高推断的准确性。

    The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the down
    
[^49]: PatchAD: 基于块的MLP-Mixer的时间序列异常检测

    PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])

    [http://arxiv.org/abs/2401.09793](http://arxiv.org/abs/2401.09793)

    PatchAD是一种新颖的基于块的MLP-Mixer体系结构，利用对比学习进行时间序列异常检测。它具有高效和轻量级的架构，并采用创新的双项目约束模块来提高表示能力。

    

    异常检测是时间序列分析的关键方面，旨在识别时间序列样本中的异常事件。这一任务的核心挑战在于在缺乏标签的情况下有效地学习正常和异常模式的表示。先前的研究大多依赖于基于重构的方法，限制了模型的表征能力。此外，大多数当前的深度学习方法不够轻量级，这促使我们设计一个更高效的异常检测框架。本研究中，我们介绍了PatchAD，一种新颖的多尺度基于块的MLP-Mixer体系结构，利用对比学习进行表征提取和异常检测。具体而言，PatchAD由四个独特的MLP Mixer组成，专门利用MLP架构实现高效和轻量级的架构。此外，我们还创新地设计了一个双项目约束模块来缓解潜在的问题。

    Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
    
[^50]: BreastRegNet: 一种用于对乳腺Faxitron和组织病理学图像进行配准的深度学习框架。

    BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images. (arXiv:2401.09791v1 [eess.IV])

    [http://arxiv.org/abs/2401.09791](http://arxiv.org/abs/2401.09791)

    本研究介绍了一种用于对乳腺Faxitron和组织病理学图像进行配准的深度学习框架，可以改善组织病理学过程，并为病理学家选择取样区域提供了更准确的方法。

    

    乳腺癌的标准治疗方案包括给予新辅助治疗，然后手术切除肿瘤和周围组织。病理学家通常依靠柜式X射线照相机（称为Faxitron）来检查切除的乳腺组织，并诊断残余疾病的程度。然而，准确确定残留癌症的位置、大小和病灶性可能具有挑战性，错误的评估可能导致临床后果。利用自动化方法可以改善组织病理学过程，使病理学家能够更有效、更精确地选择取样区域。尽管认识到这一必要性，但目前还没有此类方法。训练这种自动检测模型需要在离体放射学图像上获取准确的基本真值标签，可以通过注册Faxitron和组织病理学图像，并对癌症的范围进行映射来获取。

    A standard treatment protocol for breast cancer entails administering neoadjuvant therapy followed by surgical removal of the tumor and surrounding tissue. Pathologists typically rely on cabinet X-ray radiographs, known as Faxitron, to examine the excised breast tissue and diagnose the extent of residual disease. However, accurately determining the location, size, and focality of residual cancer can be challenging, and incorrect assessments can lead to clinical consequences. The utilization of automated methods can improve the histopathology process, allowing pathologists to choose regions for sampling more effectively and precisely. Despite the recognized necessity, there are currently no such methods available. Training such automated detection models require accurate ground truth labels on ex-vivo radiology images, which can be acquired through registering Faxitron and histopathology images and mapping the extent of cancer from histopathology to x-ray images. This study introduces a
    
[^51]: 查询易于翻转样本的深度主动学习

    Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])

    [http://arxiv.org/abs/2401.09787](http://arxiv.org/abs/2401.09787)

    本文提出了一种基于模型的预测不确定性度量，即最小不一致度量（LDM），用于解决复杂决策边界情况下的主动学习问题。通过查询具有最小LDM的未标记数据，可以提高深度学习模型的性能。

    

    主动学习是一种机器学习范式，旨在通过选择和查询未标记数据来提高模型的性能。一种有效的选择策略是基于模型的预测不确定性，这可以解释为样本的信息量度量。样本到决策边界的距离是一种自然的预测不确定性度量，但通常难以计算，特别是对于多类分类任务中形成的复杂决策边界。为了解决这个问题，本文提出了“最小不一致度量”（LDM），定义为预测标签不一致的最小概率，并且证明了LDM的估计器在温和假设下是渐近一致的。该估计器计算效率高，并且可以通过参数扰动轻松实现在深度学习模型中使用。基于LDM的主动学习通过查询具有最小LDM的未标记数据来执行。

    Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Exper
    
[^52]: 走向异质图学习：进展与未来

    Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])

    [http://arxiv.org/abs/2401.09769](http://arxiv.org/abs/2401.09769)

    本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。

    

    图是用来模拟现实世界实体之间复杂关系的结构化数据。最近，异质图，其中连接的节点往往具有不同的标签或不同的特征，引起了广泛关注并找到了许多应用。与此同时，人们也在不断努力推进从异质图中学习。虽然有关该主题的调查存在，但它们只关注于异质图神经网络（GNNs），而忽略了异质图学习的其他子主题。在本调查中，我们全面回顾了关于从带有异质性的图中学习的现有研究。首先，我们收集了180多篇论文，介绍了该领域的发展。然后，我们根据层次分类法对现有方法进行了系统分类，包括学习策略、模型架构和实际应用。最后，我们讨论了现有研究的主要挑战，并突出了未来研究的潜在方向。

    Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
    
[^53]: 使用Shapley值解释漂移

    Explaining Drift using Shapley Values. (arXiv:2401.09756v1 [cs.LG])

    [http://arxiv.org/abs/2401.09756](http://arxiv.org/abs/2401.09756)

    本文提出了一个新的框架-DBShap，使用Shapley值来确定模型性能漂移的主要贡献者并量化他们的贡献。通过DBShap提供的解释，可以理解漂移背后的根本原因。

    

    当机器学习模型被用于预测其未训练数据的结果时，其性能常常会下降。这种情况在现实世界中经常发生，因为数据的分布会逐渐或突然地发生变化，或由于像大流行病这样的重大事件。在机器学习研究中，已经有许多尝试提出能够抵御这种概念漂移的技术。然而，没有一个原则性的框架来确定导致模型性能漂移的原因。在本文中，我们提出了一个新颖的框架-DBShap，它使用Shapley值来确定漂移的主要贡献者并量化他们的贡献。所提出的框架不仅量化了单个特征在驱动漂移方面的重要性，还包括了输入和输出之间底层关系的变化作为可能的驱动因素。DBShap所提供的解释可以用于理解漂移背后的根本原因。

    Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and
    
[^54]: 通过保持邻居相似性实现普适鲁棒的图神经网络

    Universally Robust Graph Neural Networks by Preserving Neighbor Similarity. (arXiv:2401.09754v1 [cs.LG])

    [http://arxiv.org/abs/2401.09754](http://arxiv.org/abs/2401.09754)

    本文通过保持邻居相似性实现了普适鲁棒的图神经网络，并在异类图上探索了图神经网络的脆弱性。理论上证明了负分类损失的更新与基于邻居特征的成对相似性呈负相关，解释了图攻击者连接不相似节点对的行为。通过这种方法，我们新颖地提出了一种解决方案。

    

    尽管图神经网络在学习关系数据方面取得了巨大成功，但已经广泛研究发现，图神经网络在同类图上容易受到结构攻击的影响。受此启发，我们提出了一系列鲁棒模型，以增强图神经网络在同类图上的对抗鲁棒性。然而，关于异类图上的脆弱性仍然存在许多未解之谜。为了弥合这一差距，本文开始探索图神经网络在异类图上的脆弱性，并在理论上证明了负分类损失的更新与基于邻居特征的幂和聚合的成对相似性呈负相关。这一理论证明解释了实证观察，即图攻击者倾向于基于邻居特征而不是个体特征连接不相似节点对，无论是在同类图还是异类图上。通过这种方式，我们新颖地引入了一种方法

    Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs. Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs. However, the vulnerability based on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs. In this way, we novelly introduce a
    
[^55]: 机器学习在聚烯烃制造优化中的应用

    Applications of Machine Learning to Optimizing Polyolefin Manufacturing. (arXiv:2401.09753v1 [cs.LG])

    [http://arxiv.org/abs/2401.09753](http://arxiv.org/abs/2401.09753)

    本文介绍了机器学习在化工和聚烯烃制造优化中的应用，包括核心机器学习组件、各种方法和深度学习网络，并提出了科学引导的机器学习的混合方法来增强模型准确性。

    

    本章节是我们的图书的预印本，重点介绍了如何利用机器学习在化工和聚烯烃制造优化中的应用。它既适合初学者，也适合那些对化学过程中最新的机器学习应用感兴趣的专业人士。我们追踪了人工智能和机器学习在化学行业中的发展，描述了核心的机器学习组件，并为初学者提供了学习资源。我们详细讨论了各种机器学习方法，包括回归、分类和无监督学习技术，给出了性能指标和示例。我们还探索了集成方法、深度学习网络（包括MLP、DNN、RNN、CNN和transformers）在化学应用中不断增长的作用。实践研讨会通过使用先进的机器学习算法指导读者进行预测建模。本章最终提出了科学引导的机器学习的见解，主张采用混合方法来增强模型的准确性。广泛的参考文献提供了进一步研究的资源。

    This chapter is a preprint from our book by , focusing on leveraging machine learning (ML) in chemical and polyolefin manufacturing optimization. It's crafted for both novices and seasoned professionals keen on the latest ML applications in chemical processes. We trace the evolution of AI and ML in chemical industries, delineate core ML components, and provide resources for ML beginners. A detailed discussion on various ML methods is presented, covering regression, classification, and unsupervised learning techniques, with performance metrics and examples. Ensemble methods, deep learning networks, including MLP, DNNs, RNNs, CNNs, and transformers, are explored for their growing role in chemical applications. Practical workshops guide readers through predictive modeling using advanced ML algorithms. The chapter culminates with insights into science-guided ML, advocating for a hybrid approach that enhances model accuracy. The extensive bibliography offers resources for further research a
    
[^56]: 使用动态联合分布适应提高说话人无关的语音情感识别

    Improving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation. (arXiv:2401.09752v1 [cs.SD])

    [http://arxiv.org/abs/2401.09752](http://arxiv.org/abs/2401.09752)

    本研究提出了一种动态联合分布适应（DJDA）方法，通过联合分布适应来消除多说话人引起的多域转移挑战，并学习具有辨别性和说话人无关性的语音情感特征。

    

    在说话人无关的语音情感识别中，训练和测试样本来自不同的说话人，导致了不同说话人数据的特征分布之间存在多域转移挑战。因此，当训练模型面对来自新说话人的数据时，其性能往往会下降。为了解决这个问题，我们提出了一种动态联合分布适应（DJDA）方法，在多源域适应框架下使用。DJDA首先利用联合分布适应（JDA），包括边缘分布适应（MDA）和条件分布适应（CDA），更精确地量化不同说话人引起的多域分布差异。这有助于消除情感特征中的说话人偏差，从粗粒度到细粒度学习具有辨别性和说话人无关性的语音情感特征。此外，我们通过使用动态权衡方法量化JDA中MDA和CDA的适应贡献来对其进行评估。

    In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynam
    
[^57]: 使用分布式随机网络蒸馏的探索和反探索

    Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])

    [http://arxiv.org/abs/2401.09750](http://arxiv.org/abs/2401.09750)

    该论文提出了一种新的深度强化学习探索算法，称为分布式随机网络蒸馏（DRND）。该算法通过蒸馏随机网络的分布和隐式融入伪计数来改进奖励分配的精度，从而增强了探索过程。理论分析和实验结果均表明该方法的优越性。

    

    在深度强化学习中，探索仍然是一个重要问题，对于一个智能体在未知环境中取得高回报至关重要。虽然目前的探索随机网络蒸馏（Random Network Distillation，RND）算法已在许多环境中证明有效，但它在奖励分配上往往需要更高的区分能力。本文突出了RND中的“奖励不一致”问题，并指出了其主要限制。为了解决这个问题，我们引入了分布式RND（DRND），它是RND的一个变体。DRND通过蒸馏随机网络的分布并隐式地融入伪计数来改进奖励分配的精度，从而增强了探索过程。我们的方法有效地缓解了不一致问题，而不会引入显著的计算开销。理论分析和实验结果均证明了我们方法的优越性。

    Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
    
[^58]: Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- 一个全面的符号回归框架

    Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework. (arXiv:2401.09748v1 [cs.SC])

    [http://arxiv.org/abs/2401.09748](http://arxiv.org/abs/2401.09748)

    引入了一个基于函数图像和操作树序列的科学计算多模态框架（Botfip），应用于符号回归问题，并验证了其在低复杂度问题上的优势，展示了其潜力。这个多模态框架在科学计算问题中具有广泛的应用前景。

    

    在科学计算领域中，许多问题解决方法往往只注重过程和最终结果，即使在科学人工智能领域，也缺乏对数据背后的深度多模态信息挖掘，缺乏类似于图像文本领域的多模态框架。本文以符号回归（SR）为重点，在图像文本领域的BLIP模型的启发下，提出了一种基于函数图像（Funcimg）和操作树序列（OTS）的科学计算多模态框架——引导OTS-Funcimg预训练模型（Botfip）。在SR实验中，我们验证了Botfip在低复杂度的SR问题中的优势，展示了其潜力。作为一个MED框架，Botfip在更广泛的科学计算问题中具有潜力。

    In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.
    
[^59]: 离线模仿学习通过控制有效规划范围

    Offline Imitation Learning by Controlling the Effective Planning Horizon. (arXiv:2401.09728v1 [cs.LG])

    [http://arxiv.org/abs/2401.09728](http://arxiv.org/abs/2401.09728)

    本文研究了离线模仿学习中通过控制有效规划范围来提高性能的问题，并提出了修正算法方法，解决了存在的逼近误差问题。

    

    在离线模仿学习中，我们通常假设只有少量的专家路径和来自次优行为的离线数据集来学习专家策略。虽然现在常常通过最小化状态-动作访问分布的差异来使代理考虑到动作的未来后果，但是离线数据集中的抽样误差可能导致对离线情况下状态-动作访问估计的错误。在本文中，我们研究了通过控制有效规划范围（即减小折扣因子）来对抗显式正则化之前所研究的影响。不幸的是，现有算法在有效规划范围缩短时会出现放大的逼近误差，从而导致性能的显著下降。我们分析了问题的主要原因，并提供了正确的修正算法方法。我们展示了修正后算法的效果，并验证了它的有效性。

    In offline imitation learning (IL), we generally assume only a handful of expert trajectories and a supplementary offline dataset from suboptimal behaviors to learn the expert policy. While it is now common to minimize the divergence between state-action visitation distributions so that the agent also considers the future consequences of an action, a sampling error in an offline dataset may lead to erroneous estimates of state-action visitations in the offline case. In this paper, we investigate the effect of controlling the effective planning horizon (i.e., reducing the discount factor) as opposed to imposing an explicit regularizer, as previously studied. Unfortunately, it turns out that the existing algorithms suffer from magnified approximation errors when the effective planning horizon is shortened, which results in a significant degradation in performance. We analyze the main cause of the problem and provide the right remedies to correct the algorithm. We show that the corrected 
    
[^60]: 基于聚类和用户交互嵌入档案的EfficientRec无限用户-物品规模推荐系统

    EfficientRec an unlimited user-item scale recommendation system based on clustering and users interaction embedding profile. (arXiv:2401.09693v1 [cs.IR])

    [http://arxiv.org/abs/2401.09693](http://arxiv.org/abs/2401.09693)

    EfficientRec是一种应用了图神经网络和对比学习框架的推荐系统，采用了软聚类架构，能以低计算成本学习用户偏好，并具有较高的准确性和对无限用户和产品的可扩展性。

    

    推荐系统是如今科技公司高度关注的技术，由于用户和产品数量不断增长，传统的推荐算法在对工业环境的适应性上存在困难。本文介绍了一种新的方法，应用图神经网络和对比学习框架来提取用户偏好。我们采用了软聚类架构，显著降低了推理过程的计算成本。实验证明，该模型能够以较低的计算成本在训练和预测阶段学习用户偏好，并且具有很好的准确性。我们称这种架构为EfficientRec，意味着模型的紧凑性和对无限用户和产品的可扩展性。

    Recommendation systems are highly interested in technology companies nowadays. The businesses are constantly growing users and products, causing the number of users and items to continuously increase over time, to very large numbers. Traditional recommendation algorithms with complexity dependent on the number of users and items make them difficult to adapt to the industrial environment. In this paper, we introduce a new method applying graph neural networks with a contrastive learning framework in extracting user preferences. We incorporate a soft clustering architecture that significantly reduces the computational cost of the inference process. Experiments show that the model is able to learn user preferences with low computational cost in both training and prediction phases. At the same time, the model gives a very good accuracy. We call this architecture EfficientRec with the implication of model compactness and the ability to scale to unlimited users and products.
    
[^61]: 将图像特征输入到神经网络的每一层的模仿学习

    Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])

    [http://arxiv.org/abs/2401.09691](http://arxiv.org/abs/2401.09691)

    本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。

    

    模仿学习使得机器人能够通过训练数据学习并复制人类行为。最近机器学习的进展使得能够直接处理高维观测数据（如图像）的端到端学习方法成为可能。然而，在处理多个模态的数据时，这些方法面临着一个关键挑战，即在使用短采样周期时无意中忽略与期望输出的相关性较低的数据。本文提出了一种有效的方法来解决这个挑战，通过将数据输入到每个神经网络层中，放大与输出相关性较低的数据的影响。所提出的方法有效地将多样的数据源纳入到学习过程中。通过使用原始图像和关节信息作为输入进行简单的拾取放置操作的实验，即使处理来自短采样周期的数据，也证明了成功率显著提高。

    Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
    
[^62]: 分类和回归任务中分类变量编码器性能的比较研究

    Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks. (arXiv:2401.09682v1 [cs.LG])

    [http://arxiv.org/abs/2401.09682](http://arxiv.org/abs/2401.09682)

    本研究比较了不同分类变量编码器在分类和回归任务中的性能，验证了对于ATI模型来说，独热编码器是最佳选择，而目标编码器及其变体是树模型中最合适的编码器。

    

    分类变量经常出现在分类和回归任务的数据集中，在训练之前需要将其编码为数值。由于已经开发出了许多编码器并且可以显著影响性能，选择适当的编码器成为一项耗时但重要的实践问题。本研究将机器学习模型大致分为三类：1）隐式对输入进行仿射变换的ATI模型，如多层感知机神经网络；2）基于决策树的树模型，如随机森林；3）其他模型，如kNN。在理论上，我们证明了对于ATI模型来说，独热编码器是最佳选择，因为它可以通过从数据中学习合适的权重模拟任何其他编码器。我们还解释了为什么目标编码器及其变体是树模型中最合适的编码器。本研究进行了综合的计算实验来评估

    Categorical variables often appear in datasets for classification and regression tasks, and they need to be encoded into numerical values before training. Since many encoders have been developed and can significantly impact performance, choosing the appropriate encoder for a task becomes a time-consuming yet important practical issue. This study broadly classifies machine learning models into three categories: 1) ATI models that implicitly perform affine transformations on inputs, such as multi-layer perceptron neural network; 2) Tree-based models that are based on decision trees, such as random forest; and 3) the rest, such as kNN. Theoretically, we prove that the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data. We also explain why the target encoder and its variants are the most suitable encoders for tree-based models. This study conducted comprehensive computational experiments to evaluate
    
[^63]: 利用密度比率进行在线强化学习

    Harnessing Density Ratios for Online Reinforcement Learning. (arXiv:2401.09681v1 [cs.LG])

    [http://arxiv.org/abs/2401.09681](http://arxiv.org/abs/2401.09681)

    本文通过提出一种新的算法（GLOW），结合密度比率模型和值函数模型，在线强化学习中解决了瓶颈问题，即如何在没有初始数据集的情况下收集具有良好覆盖度的探索数据集。

    

    尽管离线和在线强化学习的理论发展方向一直是平行的，但它们开始显示出可能统一的迹象，其中一个环境的算法和分析技术通常在另一个环境中具有自然的对应物。然而，密度比率建模的概念，这是离线强化学习中的新兴范式，在在线强化学习中很少出现，也许有充足的理由：密度比率的存在和有界性依赖于具有良好覆盖度的探索性数据集的访问性，但在线强化学习的核心挑战是在没有初始数据集的情况下收集这样的数据集。在这项工作中，我们表明 - 也许令人惊讶的是 - 基于密度比率的算法具有在线对应物。假定只存在具有良好覆盖度的探索性分布，即结构条件已知为coverability（Xie等，2023），我们提供了一种新的算法（GLOW），它利用密度比率可实现性和值函数可实现性来进行高效采样。

    The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-effici
    
[^64]: 使用本地自适应对抗颜色攻击对艺术品进行神经风格转换的保护

    Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])

    [http://arxiv.org/abs/2401.09673](http://arxiv.org/abs/2401.09673)

    本文提出了一种名为本地自适应对抗颜色攻击（LAACA）的方法，用于保护艺术品免受神经风格转换（NST）的滥用。该方法通过在不可察觉的情况下对图像进行修改，产生对NST具有干扰作用的扰动。

    

    神经风格转换（NST）广泛应用于计算机视觉中，用于生成具有任意风格的新图像。这个过程利用神经网络将风格图像的美学元素与内容图像的结构因素融合在一起，形成一个和谐整合的视觉结果。然而，未经授权的NST可能会滥用艺术品。这种滥用引起了关于艺术家权利的社会技术问题，并促使开发技术方法来积极保护原始创作。对抗性攻击主要在机器学习安全中进行探索。我们的工作将这一技术引入到保护艺术家知识产权的领域。本文引入了本地自适应对抗颜色攻击（LAACA）的方法，这种方法可以以对人眼不可察觉但对NST产生干扰的方式修改图像。具体而言，我们设计了针对高频内容丰富区域的扰动，这些扰动由中间特征的破坏产生。我们进行了实验和用户研究。

    Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
    
[^65]: 迈向可识别的无监督领域转换：一种多样化分布匹配的方法

    Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])

    [http://arxiv.org/abs/2401.09671](http://arxiv.org/abs/2401.09671)

    本研究旨在解决无监督领域转换中的可识别性问题，引入了一个MPA消除理论，解决了CycleGAN及其变体产生内容不对齐的限制。

    

    无监督领域转换（UDT）旨在找到将一个领域的样本（例如素描）转换为另一个领域（例如照片）的函数，同时不改变高层语义意义（也称为“内容”）。这些转换函数通常通过转换源领域和目标领域的概率分布来寻找。CycleGAN可以说是这一领域中最具代表性的方法。然而，文献中指出CycleGAN及其变体可能无法识别所需的转换函数，并产生内容不对齐的转换。这种局限性源于学习准则解空间中存在多个转换函数，称为“保度自同构（MPA）”。尽管意识到了这种可识别性问题，但解决方案仍然难以找到。本研究深入探究了核心的可识别性问题，并引入了MPA消除理论。我们的分析表明...

    Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
    
[^66]: 加速分布式随机优化的自排斥随机游走

    Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks. (arXiv:2401.09665v1 [math.PR])

    [http://arxiv.org/abs/2401.09665](http://arxiv.org/abs/2401.09665)

    本文研究了一种通过自排斥随机游走加速分布式随机优化的方法，该方法用非线性马尔可夫链取代传统的线性马尔可夫链，并在渐近方差方面取得了显著的改进。

    

    我们研究了一类分布式随机优化算法，其中梯度是由一个在代理网络中以随机游走方式移动的令牌采样得到的。通常情况下，这些随机游走被选择为渐近采样一个目标分布的马尔可夫链，在优化迭代的收敛中起着关键作用。本文采用一种新颖的方法，通过将标准线性马尔可夫令牌替换为遵循非线性马尔可夫链的令牌 - 即自排斥随机游走(SRRW)。对于任给的'base'马尔可夫链，由正标量{\alpha}参数化的SRRW，在过去高访问的状态转移概率较低，因此得名。在图上的MCMC采样的背景下，Doshi等人(2023)的最新突破表明，SRRW在采样方面在渐近方差上达到了O(1/{\alpha})的减小。我们提出了使用'generalized'版本的SRRW来驱动令牌算法。

    We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a critical role in the convergence of the optimization iterates. In this paper, we take a novel approach by replacing the standard linear Markovian token by one which follows a nonlinear Markov chain - namely the Self-Repellent Radom Walk (SRRW). Defined for any given 'base' Markov chain, the SRRW, parameterized by a positive scalar {\alpha}, is less likely to transition to states that were highly visited in the past, thus the name. In the context of MCMC sampling on a graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW achieves O(1/{\alpha}) decrease in the asymptotic variance for sampling. We propose the use of a 'generalized' version of the SRRW to drive token algorithms fo
    
[^67]: 移动性加速学习：车联网中分层联邦学习的收敛分析

    Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks. (arXiv:2401.09656v1 [cs.LG])

    [http://arxiv.org/abs/2401.09656](http://arxiv.org/abs/2401.09656)

    在车联网中，通过收敛性分析，本文证明移动性对分层联邦学习的收敛速度有积极影响，它增加了边缘层异构数据的融合和更快的数据融合速度，从而提高模型准确性。

    

    分层联邦学习(HFL)以隐私保护的方式，在多个设备上通过几个边缘服务器和一个云边缘服务器进行模型的分布式训练。本文考虑高度移动的设备，主要针对车联网。通过收敛性分析，我们证明移动性通过融合边缘数据和洗牌边缘模型来影响收敛速度。尽管从通信的角度来看，移动性通常被视为一个挑战，但我们证明它增加了边缘层异构数据下HFL的收敛速度，因为更多多样化的数据可以被合并。此外，我们证明高速度导致更快的收敛，因为它加速了数据的融合。仿真结果表明，当在CIFAR-10数据集上训练卷积神经网络时，移动性可以使HFL的模型准确性提高高达15.1%。

    Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.
    
[^68]: 神经符号推理和学习的凸二级优化研究

    Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])

    [http://arxiv.org/abs/2401.09651](http://arxiv.org/abs/2401.09651)

    本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。

    

    通过利用凸二级优化技术，我们解决了神经符号系统的一个关键挑战，开发了一个通用的基于梯度的端到端神经和符号参数学习框架。我们利用最先进的神经符号体系结构NeuPSL来证明我们的框架的适用性。为了实现这一目标，我们提出了NeuPSL推理的平滑原始和对偶形式，并显示学习梯度是最优对偶变量的函数。此外，我们为新的形式开发了一种对偶块坐标下降算法，自然地利用了热启动。这使得我们相比当前最好的NeuPSL推理方法的学习时间改进了100倍以上。最后，我们对涵盖各种任务的8个数据集进行了广泛的实证评估，并证明我们的学习框架相比替代学习方法能够提升高达16%的预测性能。

    We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
    
[^69]: ClimateGPT: 实现对气候变化领域的跨学科研究进行合成的AI模型

    ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])

    [http://arxiv.org/abs/2401.09646](http://arxiv.org/abs/2401.09646)

    ClimateGPT是一个针对气候变化领域的跨学科研究合成的AI模型，通过优化检索增强和使用级联机器翻译方法，提高了模型的性能和可访问性。

    

    本文介绍了ClimateGPT，一种特定领域的大型语言模型系列，用于合成气候变化的跨学科研究。我们从头开始训练了两个7B模型，训练数据集包含300B个科学导向的令牌。第一个模型在预训练期间包含了4.2B个特定领域的令牌，第二个模型在预训练后针对气候领域进行了调整。此外，我们还对ClimateGPT-7B，13B和70B进行了连续预训练，训练数据集包含4.2B个特定领域的令牌，并与气候科学家紧密合作创建。为了减少虚构生成的数量，我们为模型进行了检索增强优化，并提出了一种分层检索策略。为了提高我们模型对非英语使用者的可访问性，我们建议利用级联机器翻译，并证明这种方法可以与翻译的性能相媲美。

    This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to 
    
[^70]: 功能性线性非高斯无循环模型用于因果发现

    Functional Linear Non-Gaussian Acyclic Model for Causal Discovery. (arXiv:2401.09641v1 [cs.LG])

    [http://arxiv.org/abs/2401.09641](http://arxiv.org/abs/2401.09641)

    本研究提出了功能性线性非高斯无循环模型（Func-LiNGAM），将变量的概念扩展到了向量和函数，并解决了原始模型无法处理无限维数据集的问题，从而在因果发现中提供了更广泛的应用。

    

    在因果发现中，非高斯性已被用来描述线性非高斯无循环模型（LiNGAM）的完整配置，包括变量的因果顺序及它们之间的连接强度。然而，LiNGAM只能处理有限维情况。为了扩展这个概念，我们将变量的概念扩展到包括向量甚至函数，形成了功能性线性非高斯无循环模型（Func-LiNGAM）。我们的动机源于希望在涉及脑效应连接任务的情况下，比如fMRI和EEG数据集中识别因果关系。我们说明了为什么原始的LiNGAM无法处理这些本质上是无限维的数据集，并从实证和理论的角度解释了功能数据分析的可用性。

    In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-di
    
[^71]: 使用融合策略和深度学习的自动化三维多模式超声胎盘分割

    Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using Fusion Strategies and Deep Learning. (arXiv:2401.09638v1 [eess.IV])

    [http://arxiv.org/abs/2401.09638](http://arxiv.org/abs/2401.09638)

    本研究提出了一种使用融合策略和深度学习的自动化三维多模式超声胎盘分割方法，通过自动识别胎盘变化，有望解决当前超声处理方法的劳动密集、耗时长和容易出错的问题，从而促进早期检测和潜在治疗。

    

    目的：超声是临床实践中常用的医学成像模态，因其安全性、无创性和便携性，成为孕期胎儿评估的主要成像模态。当前超声处理方法要么是手动的，要么是半自动的，因此工作量大、耗时长且容易出错，自动化将在解决这些挑战方面大有帮助。在较早的孕期自动识别胎盘变化可能有助于治疗胎儿生长受限和子痫前期等疾病，这些疾病目前仅能在晚期孕龄检测到，潜在地可以预防周围婴儿的发病率和死亡率。方法：我们提出了一种使用深度学习结合不同融合策略的自动化三维多模式（B模式和功率多普勒）超声胎盘分割方法。

    Purpose: Ultrasound is the most commonly used medical imaging modality for diagnosis and screening in clinical practice. Due to its safety profile, noninvasive nature and portability, ultrasound is the primary imaging modality for fetal assessment in pregnancy. Current ultrasound processing methods are either manual or semi-automatic and are therefore laborious, time-consuming and prone to errors, and automation would go a long way in addressing these challenges. Automated identification of placental changes at earlier gestation could facilitate potential therapies for conditions such as fetal growth restriction and pre-eclampsia that are currently detected only at late gestational age, potentially preventing perinatal morbidity and mortality.  Methods: We propose an automatic three-dimensional multi-modal (B-mode and power Doppler) ultrasound segmentation of the human placenta using deep learning combined with different fusion strategies.We collected data containing Bmode and power Do
    
[^72]: 使用液体时间常数网络对磁导航系统中的气磁补偿进行物理信息校准

    Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic Navigation Systems using Liquid Time-Constant Networks. (arXiv:2401.09631v1 [cs.LG])

    [http://arxiv.org/abs/2401.09631](http://arxiv.org/abs/2401.09631)

    本研究提出了一种基于物理信息的方法，在磁导航系统中使用Tolles-Lawson系数和液体时间常数网络（LTCs）进行气磁补偿校准。实验结果显示，在实际飞行数据中，我们观察到了高达64％的气磁补偿误差减少，优于传统模型。

    

    磁导航（MagNav）是全球定位系统（GPS）的一种新兴替代方案，已被证明在飞行器导航中非常有用。传统的飞行器导航系统在某些环境和受到攻击时存在精度和可靠性方面的限制。空中磁导航利用地球的磁场提供准确的位置信息。然而，飞行器电子设备引起的外部磁场和地球的大规模磁场会干扰到较弱的感兴趣信号。我们引入了一种基于物理信息的方法，使用Tolles-Lawson系数进行补偿，并使用液体时间常数网络（LTCs）去除飞行器磁源产生的复杂而嘈杂的信号。使用实际飞行数据和磁力计测量以及飞行器测量，我们观察到气磁补偿误差（RMSE nT）最多可以减少64％，优于传统模型。这一显著改进突出了物理信息校准在磁导航中的潜力。

    Magnetic navigation (MagNav) is a rising alternative to the Global Positioning System (GPS) and has proven useful for aircraft navigation. Traditional aircraft navigation systems, while effective, face limitations in precision and reliability in certain environments and against attacks. Airborne MagNav leverages the Earth's magnetic field to provide accurate positional information. However, external magnetic fields induced by aircraft electronics and Earth's large-scale magnetic fields disrupt the weaker signal of interest. We introduce a physics-informed approach using Tolles-Lawson coefficients for compensation and Liquid Time-Constant Networks (LTCs) to remove complex, noisy signals derived from the aircraft's magnetic sources. Using real flight data with magnetometer measurements and aircraft measurements, we observe up to a 64% reduction in aeromagnetic compensation error (RMSE nT), outperforming conventional models. This significant improvement underscores the potential of a phys
    
[^73]: 多个局部线性核机器

    Multiple Locally Linear Kernel Machines. (arXiv:2401.09629v1 [cs.LG])

    [http://arxiv.org/abs/2401.09629](http://arxiv.org/abs/2401.09629)

    本文提出了一种基于多个局部线性分类器组合的新型非线性分类器，提供了可扩展的泛化多核学习训练算法，填补了高准确性但缓慢的非线性分类器和快速但低准确性的线性分类器之间的差距。

    

    本文提出了一种基于多个局部线性分类器组合的新型非线性分类器。我们将问题转化为一个$l_1$多核学习问题，使用许多局部线性核。由于这样的核函数数量庞大，我们提供了一种可扩展的泛化多核学习训练算法来处理流式核函数。在推断时间方面，得到的分类器填补了高准确性但缓慢的非线性分类器（如传统的多核学习）和快速但低准确性的线性分类器之间的差距。

    In this paper we propose a new non-linear classifier based on a combination of locally linear classifiers. A well known optimization formulation is given as we cast the problem in a $\ell_1$ Multiple Kernel Learning (MKL) problem using many locally linear kernels. Since the number of such kernels is huge, we provide a scalable generic MKL training algorithm handling streaming kernels. With respect to the inference time, the resulting classifier fits the gap between high accuracy but slow non-linear classifiers (such as classical MKL) and fast but low accuracy linear classifiers.
    
[^74]: SymTC:一种用于腰椎MRI实例分割的共生Transformer-CNN网络

    SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])

    [http://arxiv.org/abs/2401.09627](http://arxiv.org/abs/2401.09627)

    SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。

    

    椎间盘疾病是一种常见病症，经常导致间歇性或持续性的腰背疼痛，对该疾病的诊断和评估依赖于腰椎MR图像中椎骨和椎间盘几何形状的准确测量。深度神经网络（DNN）模型可以帮助临床医生以更高效的方式自动化地对腰椎的个体实例（椎骨和椎间盘）进行图像分割，这被称为实例图像分割。在这项工作中，我们提出了SymTC，一种创新的腰椎MR图像分割模型，它结合了Transformer和卷积神经网络（CNN）的优势。具体而言，我们设计了一个并行的双路径架构来融合CNN层和Transformer层，并在Transformer的自注意力模块中集成了一种新颖的位置嵌入，增强了位置信息的利用以实现更准确的分割。为了进一步提高模型的性能，我们引入了一个新的定位系统进行模型优化。

    Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
    
[^75]: MITS-GAN: 用生成对抗网络保护医学影像免受篡改

    MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks. (arXiv:2401.09624v1 [eess.IV])

    [http://arxiv.org/abs/2401.09624](http://arxiv.org/abs/2401.09624)

    MITS-GAN是一种用于保护医学影像免受篡改的新方法，通过引入适当的高斯噪声作为防护措施，打乱攻击者的生成对抗网络架构的输出。实验结果表明MITS-GAN能够生成耐篡改图像，具有优越的性能。

    

    生成模型，特别是生成对抗网络（GANs），在图像生成方面取得了进展，但也引发了潜在的恶意使用的担忧，尤其是在医学影像等敏感领域。本研究提出了一种新颖的方法MITS-GAN，用于防止医学影像中的篡改，特别关注CT扫描。该方法通过引入不可察觉但精确的扰动来打乱攻击者的CT-GAN架构的输出。具体而言，所提出的方法涉及将适当的高斯噪声引入到输入中作为对各种攻击的保护措施。我们的方法旨在提高防篡改能力，与现有技术相比具有优势。对CT扫描数据集的实验结果表明MITS-GAN具有卓越的性能，强调了其能够生成具有可忽略伪影的耐篡改图像的能力。由于医学领域中的图像篡改带来了危及生命的风险，我们的主动防护方法具有重要意义。

    The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing imperceptible but yet precise perturbations. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan dataset demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approac
    
[^76]: SMOOTHIE: 软件分析的超参数优化理论

    SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])

    [http://arxiv.org/abs/2401.09622](http://arxiv.org/abs/2401.09622)

    SMOOTHIE是一种通过考虑损失函数的“光滑度”来引导超参数优化的新型方法，在软件分析中应用可以带来显著的性能改进。

    

    超参数优化是调整学习器控制参数的黑魔法。在软件分析中，经常发现调优可以带来显著的性能改进。尽管如此，超参数优化在软件分析中通常被很少或很差地应用，可能是因为探索所有参数选项的CPU成本太高。我们假设当损失函数的“光滑度”更好时，学习器的泛化能力更强。这个理论非常有用，因为可以很快测试不同超参数选择对“光滑度”的影响（例如，对于深度学习器，在一个epoch之后就可以进行测试）。为了测试这个理论，本文实现和测试了SMOOTHIE，一种通过考虑“光滑度”来引导优化的新型超参数优化器。本文的实验将SMOOTHIE应用于多个软件工程任务，包括（a）GitHub问题寿命预测；（b）静态代码警告中错误警报的检测；（c）缺陷预测。

    Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
    
[^77]: 土地覆盖图像分类

    Land Cover Image Classification. (arXiv:2401.09607v1 [cs.CV])

    [http://arxiv.org/abs/2401.09607](http://arxiv.org/abs/2401.09607)

    本文研究了土地覆盖图像分类和深度学习模型在提高分类准确性和效率方面的应用。通过比较卷积神经网络和基于Transformer的方法，我们在LC研究中展示了它们的优势，并在EuroSAT数据集上取得了最先进的结果。

    

    土地覆盖（LC）图像分类在理解环境变化、城市规划和灾害管理方面变得越来越重要。然而，传统的LC方法往往需要大量劳动力且容易出错。本文探讨了最先进的深度学习模型，以提高土地覆盖分析的准确性和效率。我们比较了卷积神经网络（CNN）和基于Transformer的方法，在LC研究中展示了它们的应用和优势。我们使用了基于Sentinel-2卫星图像的EuroSAT基于补丁的LC分类数据集，并使用当前的Transformer模型取得了最先进的结果。

    Land Cover (LC) image classification has become increasingly significant in understanding environmental changes, urban planning, and disaster management. However, traditional LC methods are often labor-intensive and prone to human error. This paper explores state-of-the-art deep learning models for enhanced accuracy and efficiency in LC analysis. We compare convolutional neural networks (CNN) against transformer-based methods, showcasing their applications and advantages in LC studies. We used EuroSAT, a patch-based LC classification data set based on Sentinel-2 satellite images and achieved state-of-the-art results using current transformer models.
    
[^78]: 机器人臂动作识别在嘈杂环境中的鲁棒性评估

    Robustness Evaluation of Machine Learning Models for Robot Arm Action Recognition in Noisy Environments. (arXiv:2401.09606v1 [cs.CV])

    [http://arxiv.org/abs/2401.09606](http://arxiv.org/abs/2401.09606)

    本文研究了机器人臂动作识别在嘈杂环境中的鲁棒性评估。通过使用视觉系统追踪机器人的运动和深度学习模型提取臂部的关键点，我们的方法在嘈杂环境中实现了精确的动作分类。

    

    在嘈杂环境中，利用视觉系统识别机器人臂的不同但空间临近的运动是一个重要挑战。本文研究了在嘈杂环境中使用机器学习技术进行机器人臂动作识别。具体而言，使用视觉系统追踪机器人的运动，然后使用深度学习模型提取臂部的关键点。通过对比分析机器学习方法，评估了该模型在嘈杂环境中的有效性和鲁棒性。在一个3×3的栅格环境中进行了一个井字棋游戏的案例研究，其中重点是准确识别在这个受限环境中选择特定位置的臂部动作。实验结果表明，尽管数据集中存在噪声和不确定性，我们的方法可以实现精确的关键点检测和动作分类。

    In the realm of robot action recognition, identifying distinct but spatially proximate arm movements using vision systems in noisy environments poses a significant challenge. This paper studies robot arm action recognition in noisy environments using machine learning techniques. Specifically, a vision system is used to track the robot's movements followed by a deep learning model to extract the arm's key points. Through a comparative analysis of machine learning methods, the effectiveness and robustness of this model are assessed in noisy environments. A case study was conducted using the Tic-Tac-Toe game in a 3-by-3 grid environment, where the focus is to accurately identify the actions of the arms in selecting specific locations within this constrained environment. Experimental results show that our approach can achieve precise key point detection and action classification despite the addition of noise and uncertainties to the dataset.
    
[^79]: MedBlindTuner: 通过Transformer和全同态加密实现对生物医学图像的隐私保护微调

    MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical Images with Transformers and Fully Homomorphic Encryption. (arXiv:2401.09604v1 [cs.CR])

    [http://arxiv.org/abs/2401.09604](http://arxiv.org/abs/2401.09604)

    MedBlindTuner通过结合Transformer和全同态加密，在保护病人数据隐私的同时，实现对生物医学图像的隐私保护微调。通过实验验证，MedBlindTuner在加密数据上取得了与非加密数据训练模型相当的准确度，为外包机器学习计算提供了安全的解决方案。

    

    机器学习的进步显著改变了医学图像分析，促使医院依赖外部机器学习服务。然而，与第三方共享敏感患者数据，如胸部X射线，会存在固有的隐私风险。为解决这个问题，我们提出了MedBlindTuner，这是一个利用全同态加密和数据高效的图像转换器(DEiT)的隐私保护框架。MedBlindTuner使得机器学习模型仅在全同态加密的生物医学图像上进行训练。我们的实验评估表明，MedBlindTuner实现了与在非加密图像上训练的模型相当的准确度，为外包机器学习计算提供了安全的解决方案，同时保护了患者数据的隐私。据我们所知，这是第一个在该领域中使用数据高效图像转换器和全同态加密的工作。

    Advancements in machine learning (ML) have significantly revolutionized medical image analysis, prompting hospitals to rely on external ML services. However, the exchange of sensitive patient data, such as chest X-rays, poses inherent privacy risks when shared with third parties. Addressing this concern, we propose MedBlindTuner, a privacy-preserving framework leveraging fully homomorphic encryption (FHE) and a data-efficient image transformer (DEiT). MedBlindTuner enables the training of ML models exclusively on FHE-encrypted medical images. Our experimental evaluation demonstrates that MedBlindTuner achieves comparable accuracy to models trained on non-encrypted images, offering a secure solution for outsourcing ML computations while preserving patient data privacy. To the best of our knowledge, this is the first work that uses data-efficient image transformers and fully homomorphic encryption in this domain.
    
[^80]: 使用线性加法注意力Transformer的高效生成对抗网络

    Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])

    [http://arxiv.org/abs/2401.09596](http://arxiv.org/abs/2401.09596)

    这项工作提出了一种名为LadaGAN的高效生成对抗网络，它使用了一种名为Ladaformer的新型Transformer块，通过线性加法注意机制来降低计算复杂度并解决训练不稳定性问题。

    

    尽管像扩散模型（DMs）和生成对抗网络（GANs）等深度生成模型在图像生成方面的能力近年来得到了显著提高，但是它们的成功很大程度上归功于计算复杂的架构。这限制了它们在研究实验室和资源充足的公司中的采用和使用，同时也极大地增加了训练、微调和推理的碳足迹。在这项工作中，我们提出了LadaGAN，这是一个高效的生成对抗网络，它建立在一种名为Ladaformer的新型Transformer块上。该块的主要组成部分是一个线性加法注意机制，它每个头部计算一个注意向量，而不是二次的点积注意力。我们在生成器和判别器中都采用了Ladaformer，这降低了计算复杂度，并克服了Transformer GAN经常出现的训练不稳定性。LadaGAN一直表现优于现有的GANs。

    Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
    
[^81]: 无界平滑约束下的双层优化：一种新算法和收敛分析

    Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis. (arXiv:2401.09587v1 [cs.LG])

    [http://arxiv.org/abs/2401.09587](http://arxiv.org/abs/2401.09587)

    该论文提出了一种新的双层优化算法BO-REP，用于解决神经网络中存在的无界平滑性问题，通过使用归一化动量更新上层变量并引入初始化细化和周期性更新两种新技术来更新下层变量。

    

    双层优化是许多机器学习问题中的重要形式。当前的双层优化算法假设上层函数的梯度是Lipschitz的。然而，最近的研究发现，某些神经网络如循环神经网络（RNN）和长短期记忆网络（LSTM）展现出潜在的无界平滑性，使得传统的双层优化算法无法适用。在本文中，我们设计了一种新的双层优化算法，称为BO-REP，以应对这一挑战。该算法使用归一化动量更新上层变量，并采用两种新技术来更新下层变量：初始化细化和周期性更新。具体而言，一旦上层变量被初始化，将调用一个子程序来获取相应优化的下层变量的精细估计，并且只在特定周期后更新下层变量。

    Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \textit{initialization refinement} and \textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific peri
    
[^82]: eipy: 一种用于异构集成的多模态数据开源Python包

    eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles. (arXiv:2401.09582v1 [cs.LG])

    [http://arxiv.org/abs/2401.09582](http://arxiv.org/abs/2401.09582)

    eipy是一个开源Python包，用于开发多模态数据集成分类模型。它提供了严格和用户友好的框架，并通过嵌套交叉验证来评估并选择最佳的集成方法。

    

    本文介绍了eipy，一种用于开发有效的多模态异构集成分类的开源Python包。eipy同时提供了一个严格而用户友好的框架，通过系统评估它们在嵌套交叉验证中的性能来比较和选择最佳的多模态数据集成和预测建模方法。该包旨在利用类似scikit-learn的估计器作为组件来构建多模态预测模型。eipy的最新用户指南，包括API参考和教程，请参阅https://eipy.readthedocs.io。该项目的主要存储库位于GitHub上的https://github.com/GauravPandeyLab/eipy。

    In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io . The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy .
    
[^83]: 用于112 Gbit/s被动光网络中严重非线性失真的全盲神经网络均衡器

    Fully-blind Neural Network Based Equalization for Severe Nonlinear Distortions in 112 Gbit/s Passive Optical Networks. (arXiv:2401.09579v1 [eess.SP])

    [http://arxiv.org/abs/2401.09579](http://arxiv.org/abs/2401.09579)

    该论文提出了一种对于112 Gbit/s被动光网络中严重非线性失真的全盲神经网络均衡器，并评估了其性能。这种均衡器采用低硬件复杂度的神经网络拓扑结构。

    

    我们展示并评估了用于100G被动光网络(PON)的完全盲目的数字信号处理(DSP)链，以及基于神经网络的不同均衡器拓扑结构，具有低硬件复杂度。

    We demonstrate and evaluate a fully-blind digital signal processing (DSP) chain for 100G passive optical networks (PONs), and analyze different equalizer topologies based on neural networks with low hardware complexity.
    
[^84]: 实现可扩展和稳健的模型版本控制

    Towards Scalable and Robust Model Versioning. (arXiv:2401.09574v1 [cs.LG])

    [http://arxiv.org/abs/2401.09574](http://arxiv.org/abs/2401.09574)

    本文探讨了在不获取新的训练数据或更改模型架构的情况下，生成具有不同攻击属性的多个模型版本的可行性，以保护模型所有者免受恶意入侵带来的损失。

    

    随着深度学习模型在各行各业的不断部署，针对这些部署模型进行恶意入侵的威胁也在增加。如果攻击者能够通过服务器入侵、内部攻击或模型反转技术获取部署模型的访问权限，他们可以构造白盒对抗攻击来操纵模型的分类结果，从而给依赖这些模型进行关键任务的组织带来重大风险。模型所有者需要一种机制来保护自己免受这种损失，而不需要获取新的训练数据，这通常需要大量的时间和资本投入。，在本文中，我们探讨了在不获取新的训练数据或更改模型架构的情况下，生成具有不同攻击属性的多个模型版本的可行性。模型所有者可以逐个部署版本并立即替换泄露的版本。

    As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.  In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immed
    
[^85]: 在多任务深度强化学习中分享知识

    Sharing Knowledge in Multi-Task Deep Reinforcement Learning. (arXiv:2401.09561v1 [cs.LG])

    [http://arxiv.org/abs/2401.09561](http://arxiv.org/abs/2401.09561)

    本研究探讨了在多任务强化学习中分享表示的益处，并提供理论保证和实验评估结果，表明在多任务设置中分享表示可以改进性能。

    

    我们研究了在多任务强化学习中分享表示以实现深度神经网络的有效应用的益处。我们利用了学习不同任务，并分享共同属性的假设，有助于将它们的知识推广，从而获得比单个任务学习更有效的特征提取。直观地说，所得到的特征集在强化学习算法使用时可以带来性能方面的好处。我们通过提供理论保证来证明这一点，这些理论保证强调了在何种条件下分享任务之间的表示是方便的，并将近似值迭代的已知有限时间界扩展到多任务设置中。此外，我们通过提出三种强化学习算法的多任务扩展来补充我们的分析，并在广泛使用的强化学习基准上进行经验评估，表明与单任务对比相比有显著改进。

    We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in
    
[^86]: 深度学习增强的混合整数优化: 学习减少模型维度

    Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])

    [http://arxiv.org/abs/2401.09556](http://arxiv.org/abs/2401.09556)

    本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。

    

    本研究介绍了一种利用深度学习解决混合整数规划模型中的计算复杂性的框架。我们比较了前馈神经网络(ANN)和卷积神经网络(CNN)在近似混合整数规划问题中的作用。我们利用多标签分类来考虑多个活动维度。为了提高框架的性能，我们采用贝叶斯优化进行超参数调优，以最大化样本级准确性。主要目标是训练神经网络准确地预测所有活动维度，从而最大化全局最优解的出现频率。我们将该框架应用于描述个性化医学供应链中的长期投资规划和中期战术规划的基于流的设施位置分配混合整数线性规划(MILP)问题。

    This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
    
[^87]: 通过人类反馈改进分类性能：标记一些，我们标记其余部分

    Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])

    [http://arxiv.org/abs/2401.09555](http://arxiv.org/abs/2401.09555)

    本文探讨了通过人类反馈来改进分类模型性能的方法。使用少量有标签示例，通过连续反馈循环，我们能够显著提高模型的准确性。在多个数据集上进行评估，结果表明这种方法能够超越零样本大型语言模型，提供更强的文本分类性能。

    

    在人工智能领域，大部分数据是非结构化的，因此获取足够的有标签数据来训练监督式机器学习模型是一个重要挑战。为了解决这个问题，我们深入研究少样本学习和主动学习，即通过人类反馈来改进人工智能模型，仅使用一小部分有标签示例。本文着重于理解连续反馈循环如何改善模型，从而通过渐进式的人类参与提高模型的准确性、回归和精确度。通过使用大型语言模型（LLMs），如GPT-3.5、BERT和SetFit，我们旨在分析使用有限数量的有标签示例显著提高模型准确性的效果。我们在Financial Phrasebank、Banking、Craigslist、Trec和Amazon Reviews数据集上对此方法进行基准测试，证明仅使用少量有标签示例就能超越零样本大型语言模型的准确性，提供增强的文本分类性能。

    In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performa
    
[^88]: 神经影像学的维度内部表现：通过机器学习揭示疾病异质性的神经生物学表示

    Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning. (arXiv:2401.09517v1 [cs.LG])

    [http://arxiv.org/abs/2401.09517](http://arxiv.org/abs/2401.09517)

    本综述通过机器学习和多模态MRI揭示了神经精神疾病和神经退行性疾病的疾病异质性，为我们更好地理解这些疾病做出了贡献

    

    机器学习越来越多地用于获取神经影像学特征，以对神经精神疾病和神经退行性疾病的诊断、预后和治疗反应进行个体化。因此，它通过识别在各种大脑表型测量上存在显着差异的疾病亚型，为我们更好地理解疾病异质性做出了贡献。在这篇综述中，我们首先提供了使用机器学习和多模态MRI揭示各种神经精神疾病和神经退行性疾病中的疾病异质性的系统文献综述，包括阿尔茨海默病、精神分裂症、重性抑郁障碍、自闭症谱系障碍、多发性硬化症，以及它们在横断面的潜力。随后，我们总结了相关的机器学习方法，并讨论了一个新兴的范式，我们将其称为神经影像学的维度内部表现（DNE）。DNE分解了神经科学的异质性

    Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsych
    
[^89]: 通过克里洛夫子空间回收加速神经算子的数据生成

    Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])

    [http://arxiv.org/abs/2401.09516](http://arxiv.org/abs/2401.09516)

    该论文提出了一种名为排序克里洛夫回收（SKR）的新方法，用于加速神经算子训练的数据生成。该方法解决了现有方法在解决PDE问题时计算冗余的问题，显著提高了数据生成效率。

    

    学习用于解决偏微分方程(PDE)的神经算子因其高推理效率而受到广泛关注。然而，训练这些算子需要生成大量带有解决方案的标记数据，即PDE问题及其解决方案。数据生成过程非常耗时，因为它涉及解决大量线性方程组以获得PDE的数值解。许多现有方法独立地解决这些系统，而不考虑它们的内在相似性，导致计算极其冗余。为了解决这个问题，我们提出了一种新颖的方法，即排序克里洛夫回收(SKR)，以提高解决这些系统的效率，从而显著加速神经算子训练的数据生成。据我们所知，SKR是第一个解决学习神经算子数据生成耗时性质的尝试。SKR的核心是克里洛夫子空间。

    Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
    
[^90]: 多视图随机块模型中的社区检测

    Community Detection in the Multi-View Stochastic Block Model. (arXiv:2401.09510v1 [cs.SI])

    [http://arxiv.org/abs/2401.09510](http://arxiv.org/abs/2401.09510)

    本文介绍了一个多视图随机块模型（MVSBM），以信息理论的角度研究了在多个相关图上进行的社区检测问题。我们证明了当模型参数超过阈值时可以实现精确恢复社区，而当参数低于阈值时无法达到精确恢复社区的要求。

    

    本文从信息理论的角度考虑了基于多个可能相关的图的社区检测问题。我们首先提出了一个随机图模型，称为多视图随机块模型（MVSBM），用于在相同的节点集上生成相关的图（节点个数为n）。这n个节点被划分为两个相等大小的不相交社区。图中每对节点之间的边的存在与否取决于这两个节点是否属于同一个社区。学习器的目标是通过观察到的图来恢复隐藏的社区。我们的技术贡献有两个方面：（i）我们建立了一个信息理论上界（定理1），表明当MVSBM的模型参数超过一定阈值时，可以实现精确恢复社区。（ii）相反地，我们推导出一个信息理论下界（定理2），表明当MVSBM的模型参数低于前述阈值时，无法实现精确恢复社区。

    This paper considers the problem of community detection on multiple potentially correlated graphs from an information-theoretical perspective. We first put forth a random graph model, called the multi-view stochastic block model (MVSBM), designed to generate correlated graphs on the same set of nodes (with cardinality $n$). The $n$ nodes are partitioned into two disjoint communities of equal size. The presence or absence of edges in the graphs for each pair of nodes depends on whether the two nodes belong to the same community or not. The objective for the learner is to recover the hidden communities with observed graphs. Our technical contributions are two-fold: (i) We establish an information-theoretic upper bound (Theorem~1) showing that exact recovery of community is achievable when the model parameters of MVSBM exceed a certain threshold. (ii) Conversely, we derive an information-theoretic lower bound (Theorem~2) showing that when the model parameters of MVSBM fall below the afore
    
[^91]: 基于量化的系统阵列DNN加速器中激活错误可靠性的探索

    Exploration of Activation Fault Reliability in Quantized Systolic Array-Based DNN Accelerators. (arXiv:2401.09509v1 [cs.AR])

    [http://arxiv.org/abs/2401.09509](http://arxiv.org/abs/2401.09509)

    本文提出了一种综合方法论，用于探索基于量化的系统阵列DNN加速器中激活错误的可靠性，从而全面评估模型精度、可靠性和硬件效率之间的相互影响。

    

    在减少硬件平台的计算负担，即降低能耗和执行时间以及提高DNN加速器的效率的需求下，对于深度神经网络（DNN）加速器的可靠性要求变得越来越严格。此外，对于定制需求的专用DNN加速器的需求不断增长，特别是对于安全关键应用程序，需要进行全面的设计空间探索，以实现开发满足这些要求的高效和稳健的加速器。因此，硬件性能（面积和延迟）与DNN加速器实现的可靠性之间的权衡变得至关重要，并需要进行分析工具。本文提出了一种综合的方法论，用于探索和实现对模型精度、激活错误可靠性和硬件效率的三方面影响的全面评估。一种全自动化的框架被介绍用于进行这种方法论的实现。

    The stringent requirements for the Deep Neural Networks (DNNs) accelerator's reliability stand along with the need for reducing the computational burden on the hardware platforms, i.e. reducing the energy consumption and execution time as well as increasing the efficiency of DNN accelerators. Moreover, the growing demand for specialized DNN accelerators with tailored requirements, particularly for safety-critical applications, necessitates a comprehensive design space exploration to enable the development of efficient and robust accelerators that meet those requirements. Therefore, the trade-off between hardware performance, i.e. area and delay, and the reliability of the DNN accelerator implementation becomes critical and requires tools for analysis. This paper presents a comprehensive methodology for exploring and enabling a holistic assessment of the trilateral impact of quantization on model accuracy, activation fault reliability, and hardware efficiency. A fully automated framewor
    
[^92]: 深度集成形状校准：在线广告中的多场后处理校准

    Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising. (arXiv:2401.09507v1 [cs.LG])

    [http://arxiv.org/abs/2401.09507](http://arxiv.org/abs/2401.09507)

    这篇论文介绍了一种用于解决电子商务广告中多场后处理校准问题的方法。该方法通过训练校准器，并在在线推断过程中应用这些校准器来实现形状校准和数值校准。

    

    在电子商务广告场景中，估计CTR和CVR的真实概率（称为校准估计）是至关重要的，直接影响买方、卖方和平台的利益。先前的研究已经提出了许多解决校准问题的方法。这些方法通常涉及使用验证集训练校准器，并随后在在线推断过程中应用这些校准器来修正原始估计值。然而，电子商务广告场景的挑战在于多场校准。多场校准可以分为两个不同的子问题：数值校准和形状校准。数值校准被定义为每个关注领域下每个数值的不过度或不低估。形状校准被定义为在关注领域条件下特定范围内每个pCTR子集的不过度或不低估。为了实现形状校准和数值校准，我们提出了一种深度集成形状校准的方法。

    In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on CTR and CVR is critical and can directly affect the benefits of the buyer, seller and platform. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration can be subdivided into two distinct sub-problems: value calibration and shape calibration. Value calibration is defined as no over- or under-estimation for each value under concerned fields. Shape calibration is defined as no over- or under-estimation for each subset of the pCTR within the specified range under condition of concerned fields. In order to achieve shape calibration and va
    
[^93]: 功能自编码器用于平滑和表示学习

    Functional Autoencoder for Smoothing and Representation Learning. (arXiv:2401.09499v1 [cs.LG])

    [http://arxiv.org/abs/2401.09499](http://arxiv.org/abs/2401.09499)

    本研究提出了一种功能自编码器，用于学习功能数据的非线性表示，并避免了预处理的需要。

    

    功能数据分析中常用的流程是将离散观测数据转换为平滑函数，然后通过一个有限维度的系数向量来表示这些函数以总结信息。现有的数据平滑和降维方法主要集中在学习线性映射，但仅学习线性表示可能不足够。在本研究中，我们提出使用神经网络自编码器学习功能数据的非线性表示，而无需预处理。我们设计编码器采用投影层，计算功能数据和观察时间戳上的功能权重的加权内积，解码器应用恢复层，使用一组预先确定的有限维度向量将从功能数据中提取的向量映射回功能空间。

    A common pipeline in functional data analysis is to first convert the discretely observed data to smooth functions, and then represent the functions by a finite-dimensional vector of coefficients summarizing the information. Existing methods for data smoothing and dimensional reduction mainly focus on learning the linear mappings from the data space to the representation space, however, learning only the linear representations may not be sufficient. In this study, we propose to learn the nonlinear representations of functional data using neural network autoencoders designed to process data in the form it is usually collected without the need of preprocessing. We design the encoder to employ a projection layer computing the weighted inner product of the functional data and functional weights over the observed timestamp, and the decoder to apply a recovery layer that maps the finite-dimensional vector extracted from the functional data back to functional space using a set of predetermine
    
[^94]: 技术报告：关于节点不可访问情况下流言学习收敛性的研究

    Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])

    [http://arxiv.org/abs/2401.09498](http://arxiv.org/abs/2401.09498)

    本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。

    

    Gossip learning（GL）作为分散式学习的一种替代方法，更适用于资源受限的无线网络，如由无人机（UAV）组成的FANETs。GL能够显著提高UAV网络的效率并延长电池寿命。尽管具有这些优势，但GL的性能受数据分布、通信速度和网络连接性的影响较大。然而，这些因素如何影响GL的收敛性仍不清楚。现有研究基于虚拟数量来研究GL的收敛性，以方便性而忽略了当一些节点不可访问时网络的真实状态。在本文中，我们对动态网络拓扑下不可访问节点对GL的影响进行了建模和研究。首先，我们将权重发散分解为节点是否可访问的情况。然后，我们研究了在节点可访问性的动态下GL的收敛性，并在理论上提供了

    Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
    
[^95]: VeriBug: 一种基于注意力机制的硬件设计漏洞定位框架

    VeriBug: An Attention-based Framework for Bug-Localization in Hardware Designs. (arXiv:2401.09494v1 [cs.AR])

    [http://arxiv.org/abs/2401.09494](http://arxiv.org/abs/2401.09494)

    VeriBug是一种基于注意力机制的硬件设计漏洞定位框架，利用深度学习技术加速寄存器传输级别的调试，并生成可能的根本原因的解释。

    

    近年来，面向不同特定应用的SoC设计的规模和复杂性呈指数增长。在这些系统中，未检测到的漏洞的成本远高于传统处理器系统，因为它可能意味着财产或生命的损失。由于不断缩短的市场推出时间和不断增长的设备需求，这个问题变得更加严重。尽管在仿真和形式化方法方面进行了数十年的研究来进行调试和验证，但它仍然是当代硬件设计周期中耗时和资源密集型的过程之一。在这项工作中，我们提出了VeriBug，它利用深度学习的最新进展来加速寄存器传输级别的调试，并生成可能的根本原因的解释。首先，VeriBug使用硬件设计的控制数据流图，通过分析操作数及其分配的上下文来学习执行设计语句。

    In recent years, there has been an exponential growth in the size and complexity of System-on-Chip designs targeting different specialized applications. The cost of an undetected bug in these systems is much higher than in traditional processor systems as it may imply the loss of property or life. The problem is further exacerbated by the ever-shrinking time-to-market and ever-increasing demand to churn out billions of devices. Despite decades of research in simulation and formal methods for debugging and verification, it is still one of the most time-consuming and resource intensive processes in contemporary hardware design cycle. In this work, we propose VeriBug, which leverages recent advances in deep learning to accelerate debugging at the Register-Transfer Level and generates explanations of likely root causes. First, VeriBug uses control-data flow graph of a hardware design and learns to execute design statements by analyzing the context of operands and their assignments. Then, i
    
[^96]: 识别与早期热带气旋强化有关的三维辐射模式

    Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.09493](http://arxiv.org/abs/2401.09493)

    本研究利用线性变分编码器-解码器来学习云辐射反馈对早期热带气旋强化的影响，发现内核深对流和浅云的长波辐射强迫都对强化起到贡献，其中深对流的影响最大。

    

    云辐射反馈影响了早期热带气旋的强化，但现有诊断框架的局限性使其无法用来研究不对称或瞬态的辐射加热。我们提出了一种线性变分编码器-解码器（VED）来学习辐射与实际模拟的气旋表面强化之间的隐藏关系。限制VED模型的输入可以利用其不确定性来识别辐射对强化更重要的时期。对提取的三维辐射结构的细致检查表明，内核深对流和浅云的长波辐射强迫都对强化起到贡献，其中深对流在整体上具有最大的影响。我们发现，在浅云的下风处的深对流对海燕的强化至关重要。我们的工作表明，机器学习可以发现热力-动力学关系，而不依赖于轴对称或确定性的方案。

    Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
    
[^97]: 通过高斯过程回归实现热线风速计的不确定性自校准

    Uncertainty-Aware Calibration of a Hot-Wire Anemometer With Gaussian Process Regression. (arXiv:2401.09492v1 [cs.LG])

    [http://arxiv.org/abs/2401.09492](http://arxiv.org/abs/2401.09492)

    本研究针对低成本热线风速计由于气温变化而导致的精度损失问题，采用高斯过程回归进行概率校准，并在实验验证中取得了良好的性能。通过在实际使用前进行校准，可以估计典型环境温度下的风速，并提供每个速度测量的可靠不确定性估计。

    

    为了克服低成本热线风速计由于气温变化而导致的精度损失，本研究旨在利用高斯过程回归进行概率校准。高斯过程回归是一种非参数的贝叶斯和监督学习方法，旨在根据一个或多个已知的输入变量预测未知的目标变量。我们的方法在真实数据集上进行验证，在推断实际风速值方面表现良好。通过在实际使用前，对热线风速计进行考虑气温的校准，可以估计典型环境温度范围内的风速，并且为每个速度测量值提供可靠的不确定性估计。

    Expensive ultrasonic anemometers are usually required to measure wind speed accurately. The aim of this work is to overcome the loss of accuracy of a low cost hot-wire anemometer caused by the changes of air temperature, by means of a probabilistic calibration using Gaussian Process Regression. Gaussian Process Regression is a non-parametric, Bayesian, and supervised learning method designed to make predictions of an unknown target variable as a function of one or more known input variables. Our approach is validated against real datasets, obtaining a good performance in inferring the actual wind speed values. By performing, before its real use in the field, a calibration of the hot-wire anemometer taking into account air temperature, permits that the wind speed can be estimated for the typical range of ambient temperatures, including a grounded uncertainty estimation for each speed measure.
    
[^98]: PUPAE: 直观且可操作的时间序列异常解释

    PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies. (arXiv:2401.09489v1 [cs.LG])

    [http://arxiv.org/abs/2401.09489](http://arxiv.org/abs/2401.09489)

    PUPAE是一种直观且可操作的时间序列异常解释方法，通过引入领域无关的反事实解释，能够帮助解释和处理异常。

    

    近年来，时间序列异常检测取得了显著进展。然而，在检测到异常之后，我们能解释它吗？这样的解释对于处理异常非常有用。例如，在一个炼油厂中，我们是通过派遣液压工程师还是实习生更换传感器电池来响应异常？虽然有一些并行的努力来解释异常，但很多提出的技术产生的解释是间接的，并且通常比它们试图解释的异常更复杂。我们对各个领域前线从业人员使用的文献、清单和用户手册进行了评估，发现了一个有趣的共同点。大多数从业人员以以下格式讨论、解释和报告异常：如果没有破坏B，异常就会像正常数据A一样。读者将会意识到这是一种反事实的解释。在这项工作中，我们引入了一种领域无关的反事实解释方法。

    In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterf
    
[^99]: LoMA: 无损压缩的内存注意力

    LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])

    [http://arxiv.org/abs/2401.09486](http://arxiv.org/abs/2401.09486)

    LoMA是一种无损压缩的内存注意力方法，可以有效地处理长文本并减少资源消耗。

    

    处理长文本是大型语言模型（LLMs）最重要的能力之一，但随着文本长度的增加，资源消耗也急剧增加。目前，通过压缩KV缓存来减少资源消耗是一种常见的方法。尽管存在许多现有的压缩方法，但它们都有一个共同的缺点：压缩是有损的。也就是说，在压缩过程中信息不可避免地会丢失。如果压缩率很高，丢失重要信息的概率会大大增加。我们提出了一种新方法，无损压缩的内存注意力（LoMA），可以根据一组压缩比率将信息无损压缩成特殊的内存令牌KV对。我们的实验证明，LoMA具有出色的性能，可以高效训练且具有非常有效的性能。

    The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
    
[^100]: 使用多模态深度学习的不确定性感知硬件特洛伊检测

    Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])

    [http://arxiv.org/abs/2401.09479](http://arxiv.org/abs/2401.09479)

    本文提出了一种使用多模态深度学习进行硬件特洛伊检测的方法，通过生成对抗网络扩充数据，并采用早融合和晚融合策略进行评估。通过估计不确定性量化指标，实现风险感知的决策制定。

    

    在零信任的无厂无印造制造时代，硬件特洛伊在芯片生产的各个阶段被插入的风险增加了。为了应对这一问题，已经开发了各种机器学习解决方案用于检测硬件特洛伊。尽管大部分关注点都集中在统计学或深度学习方法上，但受到特洛伊感染基准样本数量有限的影响，检测准确性受限，无法检测到零日特洛伊。为了填补这一差距，我们首先采用生成对抗网络来扩充数据，以两种替代表示模态，图形和表格，确保数据集以代表性的方式分布。此外，我们提出了一种多模态深度学习方法来检测硬件特洛伊，并评估了早融合和晚融合策略的结果。我们还估计了每个预测的不确定性量化指标，用于风险感知的决策制定。结果不仅确认了我们方法的有效性，而且表明了不确定性估计对硬件特洛伊检测的重要性。

    The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
    
[^101]: Triamese-ViT: 一种基于三维感知的稳健脑龄估计方法

    Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from MRIs. (arXiv:2401.09475v1 [cs.CV])

    [http://arxiv.org/abs/2401.09475](http://arxiv.org/abs/2401.09475)

    Triamese-ViT是一种创新的ViT模型适应脑龄估计的方法，通过结合来自三个不同方向的ViTs来捕捉三维信息，显著提高了准确性和可解释性。

    

    机器学习在医学领域的应用显著提高了诊断精度，特别在解析复杂结构如人脑方面。诊断阿尔茨海默病等具有挑战性的疾病推动了脑龄估计技术的发展。这些方法通常利用三维磁共振成像(MRI)扫描，最近的研究强调了三维卷积神经网络(CNNs)如3D ResNet的有效性。然而，由于三维版本的ViT在这个领域中存在局限性，因此仍未充分发挥ViT模型的准确性和可解释性。本文介绍了Triamese-ViT，这是一种创新的ViT模型适应脑龄估计。我们的模型通过结合来自三个不同方向的ViTs来捕捉三维信息，显著提高了准确性和可解释性。在一个包含1351个MRI扫描的数据集上进行了测试，Triamese-ViT实现了一个平均绝对误差（MAE）...

    The integration of machine learning in medicine has significantly improved diagnostic precision, particularly in the interpretation of complex structures like the human brain. Diagnosing challenging conditions such as Alzheimer's disease has prompted the development of brain age estimation techniques. These methods often leverage three-dimensional Magnetic Resonance Imaging (MRI) scans, with recent studies emphasizing the efficacy of 3D convolutional neural networks (CNNs) like 3D ResNet. However, the untapped potential of Vision Transformers (ViTs), known for their accuracy and interpretability, persists in this domain due to limitations in their 3D versions. This paper introduces Triamese-ViT, an innovative adaptation of the ViT model for brain age estimation. Our model uniquely combines ViTs from three different orientations to capture 3D information, significantly enhancing accuracy and interpretability. Tested on a dataset of 1351 MRI scans, Triamese-ViT achieves a Mean Absolute E
    
[^102]: 脑肿瘤放射基因分类

    Brain Tumor Radiogenomic Classification. (arXiv:2401.09471v1 [eess.IV])

    [http://arxiv.org/abs/2401.09471](http://arxiv.org/abs/2401.09471)

    本研究通过分析多参数mpMRI扫描来预测胶质母细胞瘤中的MGMT生物标志物状态，结果表明ViT3D和Xception模型具有优势，可作为有效的分类工具进行使用。

    

    RSNA-MICCAI脑肿瘤放射基因分类挑战旨在通过对多参数mpMRI扫描（T1w，T1wCE，T2w和FLAIR）进行二分类来预测胶质母细胞瘤中的MGMT生物标志物状态。数据集分为训练集、验证集和测试集。使用了不同的架构来研究该问题，包括3D版本的Vision Transformer（ViT3D）、ResNet50、Xception和EfficientNet-B3。AUC被用作主要的评估指标，结果显示ViT3D和Xception模型在测试集上分别达到了0.6015和0.61745的优势。与其他结果相比，我们的结果在任务的复杂性下证明是有效的。通过探索不同的策略、不同的架构和更多样化的数据集可以进一步改进。

    The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to predict MGMT biomarker status in glioblastoma through binary classification on Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted into three main cohorts: training set, validation set which were used during training, and the testing were only used during final evaluation. Images were either in a DICOM format or in Png format. different architectures were used to investigate the problem including the 3D version of Vision Transformer (ViT3D), ResNet50, Xception and EfficientNet-B3. AUC was used as the main evaluation metric and the results showed an advantage for both the ViT3D and the Xception models achieving 0.6015 and 0.61745 respectively on the testing set. compared to other results, our results proved to be valid given the complexity of the task. further improvements can be made through exploring different strategies, different architectures and more diverse datasets.
    
[^103]: 自监督视觉用于气候下调尺度

    Self Supervised Vision for Climate Downscaling. (arXiv:2401.09466v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.09466](http://arxiv.org/abs/2401.09466)

    这项工作提出了一种深度学习模型，用于下调尺度 ESM 模拟数据，不需要高分辨率的真实数据进行模型优化。

    

    气候变化是我们星球面临的最重要的挑战之一。全球气温的上升已经带来了对地球气候和天气模式的显著变化，不可预测和极端天气事件的频率增加。气候变化研究的未来预测基于地球系统模型（ESMs），这些计算机模型模拟地球的气候系统。 ESMs 提供了整合各种物理系统的框架，但它们的输出受到运行和存档更高分辨率模拟所需的巨大计算资源的限制。对于给定的资源预算，ESMs 通常在较粗的网格上运行，然后进行计算量较小的“下调尺度”过程以获得更高分辨率的输出。在这项工作中，我们提出了一种用于下调尺度 ESM 模拟数据的深度学习模型，该模型不需要高分辨率的真实数据进行模型优化。这是通过利用显著的数据分布来实现的。

    Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribu
    
[^104]: 从一级原理出发的贝叶斯知识追踪的参数约束

    Parametric Constraints for Bayesian Knowledge Tracing from First Principles. (arXiv:2401.09456v1 [cs.CY])

    [http://arxiv.org/abs/2401.09456](http://arxiv.org/abs/2401.09456)

    本文从一级原理出发，推导出了可以在贝叶斯知识追踪的参数空间上施加的约束，解决了目前算法中存在的一系列问题。

    

    贝叶斯知识追踪(BKT)是一个学习者掌握状态的概率模型，对应一个知识组件。它将学习者的掌握状态视为一个“隐藏”的或潜在的二元变量，并根据学习者响应的正确性更新此状态，使用代表状态转换概率的参数。BKT通常被表示为一个隐藏马尔可夫模型，而期望最大化(EM)算法用于推断这些参数。然而，该算法可能面临多组可行的参数集、陷入局部最小值、产生退化的参数值以及拟合过程中的高计算成本等问题。本文采用“从一级原理”方法，推导出可以对BKT参数空间施加的约束。从概率的基本数学真理出发，逐步建立对于BKT参数在真实系统中所期望的行为。

    Bayesian Knowledge Tracing (BKT) is a probabilistic model of a learner's state of mastery corresponding to a knowledge component. It considers the learner's state of mastery as a "hidden" or latent binary variable and updates this state based on the observed correctness of the learner's response using parameters that represent transition probabilities between states. BKT is often represented as a Hidden Markov Model and the Expectation-Maximization (EM) algorithm is used to infer these parameters. However, this algorithm can suffer from several issues including producing multiple viable sets of parameters, settling into a local minima, producing degenerate parameter values, and a high computational cost during fitting. This paper takes a "from first principles" approach to deriving constraints that can be imposed on the BKT parameter space. Starting from the basic mathematical truths of probability and building up to the behaviors expected of the BKT parameters in real systems, this pa
    
[^105]: 集成卫星-地面网络的动态路由：基于约束的多智能体强化学习方法

    Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach. (arXiv:2401.09455v1 [cs.NI])

    [http://arxiv.org/abs/2401.09455](http://arxiv.org/abs/2401.09455)

    本论文提出了一种基于约束的多智能体强化学习方法来解决集成卫星-地面网络（ISTN）系统的动态路由问题，有效平衡了快速通信、能源效率和数据包丢失要求。

    

    集成卫星-地面网络（ISTN）系统经历了显著增长，为偏远地区提供了无缝通信服务，解决了有限的地面基础设施问题。然而，为ISTN设计路由方案极具挑战性，主要是由于增加了地面站的复杂性，并要求满足与卫星服务质量有关的各种约束条件。为解决这些挑战，我们研究了与地面站和卫星共同传输数据包的路由，同时优先考虑快速通信、满足能源效率和数据包丢失要求。具体而言，我们使用拉格朗日方法将带约束的数据包路由问题制定为一个最大最小化问题。然后，我们提出了一种名为CMADR的新颖约束多智能体强化学习（MARL）动态路由算法，它有效地平衡目标改善和约束满足。

    The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfacti
    
[^106]: Voila-A: 用用户注视注意力对齐视觉-语言模型

    Voila-A: Aligning Vision-Language Models with User's Gaze Attention. (arXiv:2401.09454v1 [cs.CV])

    [http://arxiv.org/abs/2401.09454](http://arxiv.org/abs/2401.09454)

    本文介绍了一种使用用户注视注意力对齐视觉-语言模型的方法，在处理复杂场景和多个物体的实际应用中提高了模型的可解释性和效果。

    

    在最近几年中，视觉和语言理解的整合通过视觉-语言模型（VLMs）在人工智能领域取得了重要突破。然而，现有的VLMs在处理复杂场景和多个物体的实际应用以及与人类用户的多样化注意力模式相一致方面面临挑战。本文引入了通过增强现实（AR）或虚拟现实（VR）设备收集的注视信息，作为人类注意力的代理来引导VLMs，并提出了一种新的方法Voila-A，以提高这些模型在实际应用中的可解释性和效果。首先，我们收集了数百分钟的注视数据，以展示我们可以使用本地化的叙事来模拟人类的注视方式。然后，我们设计了一个自动数据注释流水线，利用GPT-4生成了VOILA-COCO数据集。此外，我们创新了Voila Perceiver模块，将注视信息整合到VL模型中。

    In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VL
    
[^107]: 使用黎曼几何特征学习飞机机翼上的压力系数

    Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings. (arXiv:2401.09452v1 [cs.LG])

    [http://arxiv.org/abs/2401.09452](http://arxiv.org/abs/2401.09452)

    该论文提出了一种将黎曼几何特征应用于学习翼面压力系数分布的方法，以提高气动系数的预测准确性。

    

    飞机的气动系数受其几何形状的显著影响，尤其是当攻角较大时。在空气动力学领域，传统的基于多项式的参数化方法使用尽可能少的参数来描述翼型的几何形状。然而，由于翼的三维几何形状比二维翼型复杂，基于多项式的参数化方法难以准确表示翼在三维空间中的整体形状。现有的基于深度学习的方法可以提取用于描述二维翼型或翼截面形状的大量潜在神经表示。最近的研究表明，直接将几何特征作为神经网络的输入可以提高预测的气动系数的准确性。受几何理论的启发，我们提出了将黎曼几何特征纳入学习翼面压力系数分布的方法。我们的方法计算几何特征（黎曼）。

    The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Rieman
    
[^108]: 扩散驱动的分子构象预测生成框架

    Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.09451](http://arxiv.org/abs/2401.09451)

    本文介绍了一种基于扩散驱动的生成框架\method{}，用于预测分子的三维构象，具有较高的预测精度并改进了传统方法的不足。

    

    从二维图形表示中推断出三维分子构型的任务在计算化学和药物开发领域具有重要意义。它对我们理解分子机制和相互作用起着基本作用。机器学习，特别是深度生成网络的快速发展，推动了这种预测建模精度的突破。传统方法通常采用分叉策略：首先估计原子间距，然后通过解决距离几何问题来塑造分子的空间结构。然而，这种顺序方法有时无法准确捕捉到局部原子排列的复杂性，从而损害结果结构模型的完整性。为了解决这些不足，本文引入了一个前卫的生成框架：\method{}，它基于扩散驱动的方法进行预测，并取得了重要的改进。

    The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
    
[^109]: 《可解释的孟加拉语Memes的多模态情感分析》

    Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])

    [http://arxiv.org/abs/2401.09446](http://arxiv.org/abs/2401.09446)

    这项研究提出了一个多模态方法来解释孟加拉语Memes的情感，以填补此领域中低资源语言的研究空白。对比现有的数据集，提出了一个新的MemoSen数据集并表明其准确率的局限性。这项研究的主要贡献是在孟加拉语Memes情感分析领域引入了多模态方法。

    

    Memes已成为数字时代独特而有效的沟通形式，吸引了在线社区，并跨越文化障碍。尽管Memes经常和幽默联系在一起，但它们有着传达广泛情感的惊人能力，包括快乐、讽刺、沮丧等。在信息时代，理解和解释Memes背后的情感变得至关重要。先前的研究已探索了基于文本、基于图像和多模态方法，导致了像CAPSAN和PromptHate这样的模型的发展，用于检测各种Memes类别。然而，对于孟加拉语Memes这样的低资源语言的研究仍然稀缺，公开可用的数据集数量有限。最近的一个贡献是引入了MemoSen数据集。然而，所实现的准确率明显较低，并且数据集分布不平衡。在这项研究中，我们采用了ResNet50和多模态方法。

    Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and
    
[^110]: CRD: 实用异常检测的协同表示距离

    CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])

    [http://arxiv.org/abs/2401.09443](http://arxiv.org/abs/2401.09443)

    本文提出了一种基于协同表示模型的图像补丁距离计算方法，避免了查询图像和存储的补丁之间的最近邻搜索所带来的复杂度问题，能够在边缘环境下进行快速部署实用的异常检测。

    

    视觉缺陷检测在智能工业中起着重要作用。基于补丁的方法将视觉图像视为根据位置的图像补丁集合，对产品中的小缺陷（如药丸上的划痕）具有更强的辨别能力。然而，查询图像和存储的补丁之间的最近邻搜索将在时间和空间需求方面占用O(n) 的复杂度，对于在边缘环境部署而言提出了严格的挑战。在本文中，我们提出了一种通过协同表示模型计算图像补丁距离的替代方法。从具有L0约束的最近邻距离开始，我们放宽约束为L2约束，并通过封闭形式快速解决距离问题，而不需要实际访问原始存储的图像补丁集合。此外，我们指出，这种封闭形式解决方案的主要计算负担可以由高性能服务器在部署前预先计算。

    Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before 
    
[^111]: Voxceleb-ESP: 从声音中检测西班牙名人的初步实验

    Voxceleb-ESP: preliminary experiments detecting Spanish celebrities from their voices. (arXiv:2401.09441v1 [cs.SD])

    [http://arxiv.org/abs/2401.09441](http://arxiv.org/abs/2401.09441)

    Voxceleb-ESP是一个从声音中检测西班牙名人的实验，提供了一个全面多样的西班牙语数据集，与原版英语VoxCeleb相当复杂，为说话人识别基准的扩展做出了贡献。

    

    本文介绍了VoxCeleb-ESP，这是一个指向YouTube视频的指针和时间戳集合，便于创建一个新的说话人识别数据集。VoxCeleb-ESP捕捉了真实世界的场景，包括各种说话风格、噪音和信道失真。它包括了160个西班牙名人，涵盖了西班牙各个年龄组和地理区域，以确保代表性分布。我们为说话人识别任务提供了两个说话人试验列表，分别是同一视频和不同视频的目标试验，并伴有ResNet预训练模型的跨语言评估。初步的说话人识别结果表明，VoxCeleb-ESP中的检测任务的复杂性与原版更大的英语VoxCeleb相当。VoxCeleb-ESP通过提供一个全面多样的西班牙语数据集，为说话人识别基准的扩展做出了贡献。

    This paper presents VoxCeleb-ESP, a collection of pointers and timestamps to YouTube videos facilitating the creation of a novel speaker recognition dataset. VoxCeleb-ESP captures real-world scenarios, incorporating diverse speaking styles, noises, and channel distortions. It includes 160 Spanish celebrities spanning various categories, ensuring a representative distribution across age groups and geographic regions in Spain. We provide two speaker trial lists for speaker identification tasks, each of them with same-video or different-video target trials respectively, accompanied by a cross-lingual evaluation of ResNet pretrained models. Preliminary speaker identification results suggest that the complexity of the detection task in VoxCeleb-ESP is equivalent to that of the original and much larger VoxCeleb in English. VoxCeleb-ESP contributes to the expansion of speaker recognition benchmarks with a comprehensive and diverse dataset for the Spanish language.
    
[^112]: RoleCraft-GLM：推动大型语言模型中的个性化角色扮演

    RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])

    [http://arxiv.org/abs/2401.09432](http://arxiv.org/abs/2401.09432)

    RoleCraft-GLM是一个创新框架，通过大型语言模型实现个性化角色扮演，解决了缺乏个性化互动的问题。通过独特的对话数据集和细致入微的角色发展，它能够生成准确反映角色个性特征和情感的对话，提升用户参与度。

    

    本研究介绍了RoleCraft-GLM，这是一个创新的框架，旨在通过大型语言模型（LLMs）增强个性化角色扮演。RoleCraft-GLM解决了对话式人工智能中缺乏个性化互动的关键问题，并提供了一种能够详细描绘情感细腻的角色刻画的解决方案。我们贡献了一组独特的对话数据集，这些数据从传统的以名人为中心的角色转变为多样化的非名人角色，从而增强了语言建模互动的真实性和复杂性。此外，我们的方法还包括细致入微的角色发展，确保对话既真实又情感共鸣。通过多个案例研究验证了RoleCraft-GLM的有效性，突显了它在不同场景中的多功能性和技能。我们的框架在生成对话方面表现出色，能够准确反映角色的个性特征和情感，从而增强用户参与度。总之，RoleCraft-GLM标志着一个创新的里程碑，推动了大型语言模型中的个性化角色扮演。

    This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
    
[^113]: 一个用于l1支持向量机的平滑算法

    A Smoothing Algorithm for l1 Support Vector Machines. (arXiv:2401.09431v1 [math.OC])

    [http://arxiv.org/abs/2401.09431](http://arxiv.org/abs/2401.09431)

    本文提出了一个用于解决具有$\ell^{1}$惩罚的软间隔支持向量机（SVM）优化问题的平滑算法，可以在遍历数据时保持较低的成本，并经过实验证明算法具有很高的测试准确性。

    

    本文提出了一个用于解决具有$\ell^{1}$惩罚的软间隔支持向量机（SVM）优化问题的平滑算法。该算法旨在对数据进行适度的遍历，这是衡量其对于非常大的数据集的成本的重要指标。该算法使用了对铰链损失函数的平滑和对$\ell^{1}$惩罚的主动集方法。平滑参数$\alpha$最初较大，但通常在求解到足够精度的平滑问题后减半。收敛理论表明，在除渐近带$\alpha=\Theta(1)$和$\alpha=\Theta(1/N)$之外的每个$\alpha$值上，每个$\alpha$值需要$\mathcal{O}(1+\log(1+\log_+(1/\alpha)))$个守卫牛顿步骤，仅当$\eta\alpha\gg1/N$时，提供一个牛顿步骤，其中$N$是数据点的数量，停止准则是预测的减少小于$\eta\alpha$。实验结果表明，我们的算法能够获得很高的测试准确性。

    A smoothing algorithm is presented for solving the soft-margin Support Vector Machine (SVM) optimization problem with an $\ell^{1}$ penalty. This algorithm is designed to require a modest number of passes over the data, which is an important measure of its cost for very large datasets. The algorithm uses smoothing for the hinge-loss function, and an active set approach for the $\ell^{1}$ penalty. The smoothing parameter $\alpha$ is initially large, but typically halved when the smoothed problem is solved to sufficient accuracy. Convergence theory is presented that shows $\mathcal{O}(1+\log(1+\log_+(1/\alpha)))$ guarded Newton steps for each value of $\alpha$ except for asymptotic bands $\alpha=\Theta(1)$ and $\alpha=\Theta(1/N)$, with only one Newton step provided $\eta\alpha\gg1/N$, where $N$ is the number of data points and the stopping criterion that the predicted reduction is less than $\eta\alpha$. The experimental results show that our algorithm is capable of strong test accuracy
    
[^114]: Transduce: 学习用于字符串转换的迁移语法

    Transduce: learning transduction grammars for string transformation. (arXiv:2401.09426v1 [cs.LG])

    [http://arxiv.org/abs/2401.09426](http://arxiv.org/abs/2401.09426)

    Transduce算法通过构建抽象迁移语法和其泛化，可以从输入输出示例中高效地学习位置转换，并且成功率高于当前最新成果。

    

    从输入输出示例中合成字符串转换程序利用了各种技术，这些技术都基于归纳偏差，包括一组受限的基本运算符进行组合。提出了一种新算法Transduce，它基于抽象迁移语法及其泛化构建。我们在实验证明，Transduce可以有效地从一个或两个正例中学习位置转换，而无需归纳偏差，成功率高于当前的最新成果。

    The synthesis of string transformation programs from input-output examples utilizes various techniques, all based on an inductive bias that comprises a restricted set of basic operators to be combined. A new algorithm, Transduce, is proposed, which is founded on the construction of abstract transduction grammars and their generalization. We experimentally demonstrate that Transduce can learn positional transformations efficiently from one or two positive examples without inductive bias, achieving a success rate higher than the current state of the art.
    
[^115]: 使用轻量级学习器的集成预测降水模型

    Precipitation Prediction Using an Ensemble of Lightweight Learners. (arXiv:2401.09424v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.09424](http://arxiv.org/abs/2401.09424)

    本文提出了一个使用多个轻量级学习器的集成预测降水模型，并通过使用卫星图像进行训练，有效地模拟复杂的降雨模式，特别是对于高降水事件。在Weather4Cast 2023竞赛中取得了第一名。

    

    降水预测在现代农业和工业中起着至关重要的作用，然而由于时间和空间上多样的模式和动态以及高降水事件的稀缺性，它也带来了重大挑战。为了应对这一挑战，我们提出了一个集成学习框架，利用多个学习器来捕捉降水分布的多样模式。具体来说，该框架包括一个具有多个轻量级头部（学习器）的降水预测器和一个控制器，该控制器将这些头部的输出组合起来。学习器和控制器分别通过提出的3阶段训练方案进行优化。通过利用提供的卫星图像，这种方法可以有效地建模复杂的降雨模式，特别是对于高降水事件。它在Weather4Cast 2023竞赛的核心测试和即时预测排行榜上取得了第一名。有关详细实现，请参考我们的GitH

    Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.  To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.  By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitH
    
[^116]: 为渐进式训练语言模型准备课程的方法

    Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])

    [http://arxiv.org/abs/2401.09192](http://arxiv.org/abs/2401.09192)

    提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。

    

    Transformers在人工智能领域的迅速发展带来了资源消耗和温室气体排放的增加，这是由于模型规模的增长。先前的研究表明使用预训练的小模型可以提高训练效率，但这种方法对于新的模型结构可能不适用。另一方面，从头开始训练可能很慢，并且渐进堆叠层往往无法实现显著的加速。为了解决这些挑战，我们提出了一种名为Apollo的新方法，它通过在低层训练期间学习高层功能来准备膨胀操作的课程。我们的方法涉及低值优先采样(LVPS)来训练不同深度，并引入权重共享以促进高效扩展。我们还介绍了一种插值方法来实现稳定的模型深度扩展。实验证明，Apollo实现了最先进的加速比率，甚至……

    The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
    
[^117]: 通过可控的变分自编码器中的受控解缠来进行无监督多领域翻译

    Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v1 [cs.LG])

    [http://arxiv.org/abs/2401.09180](http://arxiv.org/abs/2401.09180)

    该论文提出了一种无监督多领域翻译的方法，通过修改后的变分自编码器实现了受控解缠的两个潜变量，其中一个仅与领域有关，另一个与数据的其他变化因素有关。实验证明该方法在不同的视觉数据集上提高了性能。

    

    无监督多领域翻译是将数据从一个领域转换到其他领域，而无需配对数据来训练系统的任务。通常，基于生成对抗网络（GAN）的方法被用来解决这个任务。然而，我们的提议完全依赖于修改过的变分自编码器。这种修改包括通过设计以控制方式解缠两个潜变量。其中一个潜变量被强制仅依赖于领域，而另一个潜变量必须依赖于数据的其他变化因素。此外，对于领域潜变量的条件限制可以更好地控制和理解潜空间。我们从实证上证明了我们的方法适用于不同的视觉数据集，提高了其他众所周知的方法的性能。最后，我们证明了实际上一个潜变量存储了与领域和其他变化因素有关的所有信息。

    Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the o
    
[^118]: MA2GCN: 使用轨迹数据进行交通预测的多邻接关系注意力图卷积网络

    MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])

    [http://arxiv.org/abs/2401.08727](http://arxiv.org/abs/2401.08727)

    提出了一种新的交通拥堵预测模型，使用车辆轨迹数据以及多邻接关系注意力图卷积网络（MA2GCN）来预测交通拥堵情况，不依赖于传感器数据，提取灵活且准确的交通信息。

    

    交通拥堵问题不仅导致巨大的经济损失，而且严重危害城市环境。预测交通拥堵具有重要的实际意义。迄今为止，大多数研究都是基于不同路段上的传感器的历史数据来预测未来的交通流量和速度，分析某个道路段的交通拥堵情况。然而，由于传感器的固定位置，很难挖掘新的信息。另一方面，车辆轨迹数据更加灵活，可以根据需要提取交通信息。因此，我们提出了一种新的交通拥堵预测模型——多邻接关系注意力图卷积网络（MA2GCN）。该模型将车辆轨迹数据转化为网格形式的图结构化数据，并基于不同网格之间的流动性提出了车辆进出矩阵。同时，为了提高模型的性能，

    The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
    
[^119]: 使用扩散噪声进行ODI（外分布检测）

    NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v1 [cs.CV])

    [http://arxiv.org/abs/2401.08689](http://arxiv.org/abs/2401.08689)

    本研究将扩散过程应用于外分布检测任务中，通过将整个训练集的信息集成到预测的噪声向量中，获得稳定的噪声向量，并将其转化为OOD分数。

    

    外分布（OOD）检测是安全部署机器学习模型的关键部分。在文献中已经广泛研究并开发了大量方法来解决这个问题。然而，以前的方法在计算OOD分数时对内分布数据集的使用有限。例如，OOD分数是根据内分布数据的一小部分信息计算的。此外，这些方法使用神经图像编码器对图像进行编码。然而，很少有人检查这些方法对不同训练方法和架构的图像编码器的鲁棒性。在这项工作中，我们将扩散过程引入到ODD任务中。扩散模型将整个训练集的信息集成到预测的噪声向量中。此外，我们推导出噪声向量（稳定点）的闭式解。然后，将噪声向量转化为我们的OOD分数，我们测试了深度模型预测的...

    Out-of-distribution (OOD) detection is a crucial part of deploying machine learning models safely. It has been extensively studied with a plethora of methods developed in the literature. This problem is tackled with an OOD score computation, however, previous methods compute the OOD scores with limited usage of the in-distribution dataset. For instance, the OOD scores are computed with information from a small portion of the in-distribution data. Furthermore, these methods encode images with a neural image encoder. The robustness of these methods is rarely checked with respect to image encoders of different training methods and architectures. In this work, we introduce the diffusion process into the OOD task. The diffusion model integrates information on the whole training set into the predicted noise vectors. What's more, we deduce a closed-form solution for the noise vector (stable point). Then the noise vector is converted into our OOD score, we test both the deep model predicted no
    
[^120]: RAG vs Fine-tuning: 管道，权衡以及在农业上的个案研究

    RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08406](http://arxiv.org/abs/2401.08406)

    本文评估了检索增强生成（RAG）和微调两种方法在大型语言模型上的性能差异，并提出了适用于农业数据集的管道和权衡。

    

    在构建大型语言模型应用程序时，开发者通常有两种常见方法来整合专有和领域特定的数据：检索增强生成（RAG）和微调。RAG利用外部数据增强提示信息，而微调则将附加知识整合到模型中。然而，这两种方法的优缺点并不为人所理解。在本文中，我们提出了一个微调和RAG的管道，并对多种流行的大型语言模型（包括Llama2-13B，GPT-3.5和GPT-4）进行了权衡。我们的管道由多个阶段组成，包括从PDF中提取信息，生成问题和答案，将其用于微调，并利用GPT-4评估结果。我们提出了评估RAG和微调管道不同阶段性能的指标。我们对农业数据集进行了深入研究。作为一个产业，农业在人工智能的应用方面并没有得到很大的渗透。

    There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, an
    
[^121]: 隐私保护下自适应实验中CATE的差分隐私估计

    Differentially Private Estimation of CATE in Adaptive Experiment. (arXiv:2401.08224v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2401.08224](http://arxiv.org/abs/2401.08224)

    本研究提出在自适应实验中使用差分隐私估计条件平均处理效果(CATE)，平衡社会福利损失和统计功率，并通过匹配上下界以及帕累托最优性概念来获得最优解。

    

    自适应实验广泛应用于临床试验和其他场景中估计条件平均处理效果(CATE)。虽然实验的主要目标是最大化估计精度，但由于社会福利的要求，为患者提供具有优越结果的治疗也是至关重要的，这可以通过环境批次框架中的遗憾来衡量。这两个目标经常导致对比优化分配机制。此外，在包含敏感数据（如患者健康记录）的临床场景中出现隐私问题。因此，治疗分配机制必须纳入强大的隐私保护措施。在本文中，我们研究了环境批次实验中社会福利损失和统计功率之间的权衡。我们为多目标优化问题提出了匹配的上下界，并采用帕累托最优性的概念来数学地刻画最优解。

    Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimal
    
[^122]: 大型语言模型的自我解释是否可靠?

    Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07927](http://arxiv.org/abs/2401.07927)

    大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。

    

    经过训练的大型语言模型在许多任务上表现出色，甚至能够提供其行为的解释。由于这些模型对公众是直接可访问的，因此存在这样的风险，即令人信服但错误的解释可能导致对大型语言模型的无支撑的自信。因此，解释能力和可靠性是AI安全的重要考虑因素。评估自我解释的可靠性和可解释性是一项具有挑战性的任务，因为这些模型对于人类来说过于复杂，无法注释什么是正确的解释。为了解决这个问题，我们提出使用自洽性检测作为可靠性的衡量指标。例如，如果一个大型语言模型说某组词对于做出预测很重要，那么在没有这些词的情况下，它应该无法做出相同的预测。虽然自洽性检测是一种常见的可靠性方法，但之前尚未应用于大型语言模型的自我解释中。我们将自洽性检测应用于...

    Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
    
[^123]: 利用先验知识发现具有未观测变量的因果可加模型及其在时间序列数据中的应用

    Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data. (arXiv:2401.07231v1 [cs.LG])

    [http://arxiv.org/abs/2401.07231](http://arxiv.org/abs/2401.07231)

    本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法，并扩展了这些方法以应用于时间序列数据。这些方法利用先验知识进行高效因果发现，并具有对因果关系顺序的特殊处理。

    

    本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法。CAM-UV假设因果函数采用广义可加模型的形式，并存在潜在的混淆变量。首先，我们提出了一种利用先验知识进行高效因果发现的方法。然后，我们扩展了这种方法，用于推断时间序列数据的因果关系。与其他现有的因果函数模型不同，原始的CAM-UV算法不寻求观测变量之间的因果顺序，而是旨在确定每个观测变量的原因。因此，本文中提出的第一种方法利用先验知识，例如理解某些变量不能成为特定变量的原因。此外，通过融入因果在时间上的先验知识，我们将第一个算法扩展为第二种用于时间序列数据中的因果发现的方法。我们验证了第一个提出的方法。

    This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method
    
[^124]: 卧底特工：训练骗人的LLM以通过安全训练

    Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])

    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)

    该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。

    

    人类有能力进行战略性的欺骗行为：在大多数情况下表现出有益的行为，但在有机会的时候却表现出截然不同的行为以追求其他目标。如果一个AI系统学会了这样的欺骗策略，是否能够通过当前最先进的安全训练技术检测并移除它？为了研究这个问题，我们构建了大型语言模型（LLM）中欺骗行为的概念验证样例。例如，我们训练模型，在提示语句中将年份设为2023时编写安全代码，但在年份设为2024时插入有漏洞的代码。我们发现，这种暗门行为可以被持续保留，无法通过标准的安全训练技术（包括监督微调、强化学习和对抗性训练）移除。暗门行为在最大的模型和训练成产生思维链的模型中最为持久。

    Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
    
[^125]: LPAC: 可学习的感知-行动-通信循环及其在覆盖控制中的应用

    LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])

    [http://arxiv.org/abs/2401.04855](http://arxiv.org/abs/2401.04855)

    提出了一种可学习的感知-行动-通信(LPAC)架构，使用卷积神经网络处理环境感知，图神经网络实现机器人之间的信息交流，浅层多层感知机计算机器人的动作。使用集中式显微算法训练模型，实现机器人群体的协作。

    

    覆盖控制是指导机器人群体协同监测未知的感兴趣特征或现象的问题。在有限的通信和感知能力的分散设置中，这个问题具有挑战性。本文提出了一种可学习的感知-行动-通信(LPAC)架构来解决覆盖控制问题。在该解决方案中，卷积神经网络(CNN)处理了环境的局部感知；图神经网络(GNN)实现了邻近机器人之间的相关信息通信；最后，浅层多层感知机(MLP)计算机器人的动作。通信模块中的GNN通过计算应该与邻居通信哪些信息以及如何利用接收到的信息采取适当的行动来实现机器人群体的协作。我们使用一个知晓整个环境的集中式显微算法来进行模型的训练。

    Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
    
[^126]: 变化滞后模式跟随关系推理的时间序列矩阵分析框架

    Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])

    [http://arxiv.org/abs/2401.02860](http://arxiv.org/abs/2401.02860)

    该论文提出了一个利用矩阵分析方法的框架，用于推理时间序列中的跟随模式。在模拟数据集和声音记录数据集中，该框架优于基准方法，并能够检测出加密货币数据集中的跟随模式。

    

    知道谁跟随谁以及他们跟随的模式是理解集体行为（如人群，鱼群或股市）的关键步骤。时间序列是用于获取跟随关系洞察的资源之一。然而，跟随模式或模式在时间序列中的发现解决方案并不明显。在这项工作中，我们形式化了两个时间序列之间的跟随模式概念，并提出了一个推断两个时间序列之间跟随模式的框架。该框架利用一种高效且可扩展的方法从时间序列中检索模式，称为矩阵分析方法。我们将提出的框架与几个基准进行了比较。在模拟数据集中，该框架优于基准方法。在声音记录数据集中，该框架能够在一对时间序列中检索出两位歌手相互跟随唱歌的跟随模式。在加密货币数据集中，

    Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset,
    
[^127]: TopCoW：基于拓扑感知解剖分割的Willis循环（CoW）在CTA和MRA中的基准测试

    TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA. (arXiv:2312.17670v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.17670](http://arxiv.org/abs/2312.17670)

    这项研究提出了TopCoW挑战，通过发布具有13种血管组分注释的Willis循环（CoW）数据集，并使用虚拟现实（VR）技术进行拓扑感知解剖分割，解决了手动和耗时的CoW表征问题。

    

    Willis循环（CoW）是连接大脑主要循环的重要动脉网络。其血管结构被认为影响着严重神经血管疾病的风险、严重程度和临床结果。然而，对高度变化的CoW解剖进行表征仍然是一项需要手动和耗时的专家任务。CoW通常通过两种血管造影成像模式进行成像，即磁共振血管成像（MRA）和计算机断层血管造影（CTA），但是关于CTA的CoW解剖的公共数据集及其注释非常有限。因此，我们在2023年组织了TopCoW挑战赛，并发布了一个带有注释的CoW数据集。TopCoW数据集是第一个具有13种可能的CoW血管组分的体素级注释的公共数据集，通过虚拟现实（VR）技术实现。它也是第一个带有来自同一患者的成对MRA和CTA的大型数据集。TopCoW挑战将CoW表征问题形式化为多类问题。

    The Circle of Willis (CoW) is an important network of arteries connecting major circulations of the brain. Its vascular architecture is believed to affect the risk, severity, and clinical outcome of serious neuro-vascular diseases. However, characterizing the highly variable CoW anatomy is still a manual and time-consuming expert task. The CoW is usually imaged by two angiographic imaging modalities, magnetic resonance angiography (MRA) and computed tomography angiography (CTA), but there exist limited public datasets with annotations on CoW anatomy, especially for CTA. Therefore we organized the TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The TopCoW dataset was the first public dataset with voxel-level annotations for thirteen possible CoW vessel components, enabled by virtual-reality (VR) technology. It was also the first large dataset with paired MRA and CTA from the same patients. TopCoW challenge formalized the CoW characterization problem as a multiclas
    
[^128]: 指数族的双重减性和除性归一化引起的差异及其凸变形

    Divergences induced by dual subtractive and divisive normalizations of exponential families and their convex deformations. (arXiv:2312.12849v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2312.12849](http://arxiv.org/abs/2312.12849)

    本文研究了指数族的双重减性和除性归一化方法引起的差异，提出了一对Bregman和Jensen散度。同时，证明了α散度可以通过分配函数引起缩放α偏斜Jensen散度，从而比较了不同归一化方法之间的差异。

    

    指数族是统计学、信息理论和机器学习等领域中的重要模型。指数族可以通过累积量或自由能函数减性归一化，也可以通过分配函数除性归一化。这两种归一化方法都是严格的凸平滑函数，会引发一对Bregman和Jensen散度。已知指数族的概率密度之间的偏斜巴氏距离等于通过累积量函数引起的偏斜Jensen散度，而在极限情况下，偏向Kullback-Leibler散度则会转变为反向Bregman散度。本文首先证明了指数族的非归一化概率密度之间的α散度等于由分配函数引起的缩放α偏斜Jensen散度。接下来，我们展示了如何比较不同减性和除性归一化方法的差异。

    Exponential families are statistical models which are the workhorses in statistics, information theory, and machine learning among others. An exponential family can either be normalized subtractively by its cumulant or free energy function or equivalently normalized divisively by its partition function. Both subtractive and divisive normalizers are strictly convex and smooth functions inducing pairs of Bregman and Jensen divergences. It is well-known that skewed Bhattacharryya distances between probability densities of an exponential family amounts to skewed Jensen divergences induced by the cumulant function between their corresponding natural parameters, and in limit cases that the sided Kullback-Leibler divergences amount to reverse-sided Bregman divergences. In this paper, we first show that the $\alpha$-divergences between unnormalized densities of an exponential family amounts to scaled $\alpha$-skewed Jensen divergences induced by the partition function. We then show how compara
    
[^129]: FedA3I:针对异构标注噪声的注释质量感知聚合的联邦医学图像分割

    FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise. (arXiv:2312.12838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.12838](http://arxiv.org/abs/2312.12838)

    本论文针对联邦医学图像分割中的异构标注噪声问题，提出了FedA3I方法，通过注释质量感知聚合来解决高质量客户端对联邦学习的更大影响。

    

    联邦学习（FL）由于其保护隐私的特性，已经成为分散式医学数据训练分割模型的一种有前景的范式。然而，现有研究忽视了在真实医学数据集中常见的注释噪声，这限制了FL的性能。在本文中，我们首次确定并解决了这个问题。对于问题的形式化，我们提出了一个轮廓演化来对每个客户端内像素之间的非独立同分布（Non-IID）噪声进行建模，然后扩展到多源数据的情况，形成一个异构噪声模型（即交叉区别标注噪声）。为了从具有这两个层次的非I-ID噪声的注释中实现健壮的学习，我们强调了模型聚合中数据质量的重要性，允许高质量的客户端对FL产生更大的影响。为了实现这一点，我们提出了具有注释质量感知聚合的联邦学习方法，称为FedA3I。

    Federated learning (FL) has emerged as a promising paradigm for training segmentation models on decentralized medical data, owing to its privacy-preserving property. However, existing research overlooks the prevalent annotation noise encountered in real-world medical datasets, which limits the performance ceilings of FL. In this paper, we, for the first time, identify and tackle this problem. For problem formulation, we propose a contour evolution for modeling non-independent and identically distributed (Non-IID) noise across pixels within each client and then extend it to the case of multi-source data to form a heterogeneous noise model (i.e., Non-IID annotation noise across clients). For robust learning from annotations with such two-level Non-IID noise, we emphasize the importance of data quality in model aggregation, allowing high-quality clients to have a greater impact on FL. To achieve this, we propose Federated learning with Annotation quAlity-aware AggregatIon, named FedA3I, b
    
[^130]: 将自回归模型提炼为具有较快推理速度的高性能非自回归车辆路径问题求解器

    Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed. (arXiv:2312.12469v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.12469](http://arxiv.org/abs/2312.12469)

    本论文提出了一种通用的引导非自回归知识蒸馏（GNARKD）方法，通过知识蒸馏将自回归模型中的关键组件保留在网络架构中，从而获得具有低推理延迟的高性能非自回归车辆路径问题求解器。

    

    通过采用自回归（AR）或非自回归（NAR）学习方法，神经构建模型在车辆路径问题（VRP）方面表现出有希望的性能。虽然AR模型能够生成高质量的解决方案，但由于其顺序生成性质，推理延迟通常较高。相反，NAR模型以低推理延迟并行生成解决方案，但通常表现出较低的性能。在本文中，我们提出了一种通用的引导非自回归知识蒸馏（GNARKD）方法，以获得具有低推理延迟的高性能NAR模型。GNARKD通过知识蒸馏，去除AR模型中顺序生成的约束，同时保留网络架构中学到的关键组件，获得相应的NAR模型。我们将GNARKD应用于三种广泛采用的AR模型，并在合成和实际实例上获得NAR VRP求解器，并进行了实验评估。

    Neural construction models have shown promising performance for Vehicle Routing Problems (VRPs) by adopting either the Autoregressive (AR) or Non-Autoregressive (NAR) learning approach. While AR models produce high-quality solutions, they generally have a high inference latency due to their sequential generation nature. Conversely, NAR models generate solutions in parallel with a low inference latency but generally exhibit inferior performance. In this paper, we propose a generic Guided Non-Autoregressive Knowledge Distillation (GNARKD) method to obtain high-performance NAR models having a low inference latency. GNARKD removes the constraint of sequential generation in AR models while preserving the learned pivotal components in the network architecture to obtain the corresponding NAR models through knowledge distillation. We evaluate GNARKD by applying it to three widely adopted AR models to obtain NAR VRP solvers for both synthesized and real-world instances. The experimental results
    
[^131]: 部分标签学习中的合作伙伴

    Partial Label Learning with a Partner. (arXiv:2312.11034v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11034](http://arxiv.org/abs/2312.11034)

    本文介绍了一种在部分标签学习中识别和纠正错误标记样本的方法，通过引入合作伙伴分类器和互相监督范式，以及使用模糊机制实现分类器之间的协作。

    

    在部分标签学习中，每个实例与一组候选标签相关联，其中只有一个是真实标签。现有的工作大多关注构建强大的分类器来估计候选标签的标签置信度，以识别正确的标签。然而，这些方法通常很难纠正错误标记的样本。为了帮助现有的部分标签学习方法识别和纠正错误标记的样本，在本文中，我们引入了一种新的合作伙伴分类器，并提出了一种新的“互相监督”范式。具体来说，我们根据隐含的事实实例化了合作伙伴分类器，即一个样本的非候选标签不应该被分配给它，这在部分标签学习中具有内在的准确性并且尚未得到充分研究。此外，我们还提出了一个新的协作项来将基础分类器和合作伙伴分类器链接起来。在每个互相监督的阶段中，两个分类器将通过一个模糊机制互相模糊彼此的预测。

    In partial label learning (PLL), each instance is associated with a set of candidate labels among which only one is ground-truth. The majority of the existing works focuses on constructing robust classifiers to estimate the labeling confidence of candidate labels in order to identify the correct one. However, these methods usually struggle to rectify mislabeled samples. To help existing PLL methods identify and rectify mislabeled samples, in this paper, we introduce a novel partner classifier and propose a novel ``mutual supervision'' paradigm. Specifically, we instantiate the partner classifier predicated on the implicit fact that non-candidate labels of a sample should not be assigned to it, which is inherently accurate and has not been fully investigated in PLL. Furthermore, a novel collaborative term is formulated to link the base classifier and the partner one. During each stage of mutual supervision, both classifiers will blur each other's predictions through a blurring mechanism
    
[^132]: 一种用于自动机器学习中顺序超参数空间缩减的元级学习算法

    A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06305](http://arxiv.org/abs/2312.06305)

    本文提出了一种元级学习算法SHSR，用于减少AutoML中的超参数空间，减少了约30%的执行时间并且性能损失小于0.1%。

    

    在AutoML平台上，每个分析步骤都有许多算法可供尝试，例如插补算法、转换算法、特征选择算法和建模算法等。找到最佳的算法组合和超参数值是计算上昂贵的，因为要探索的组合数量导致空间的指数爆炸。本文提出了一种名为顺序超参数空间缩减（SHSR）的算法，用于减少AutoML工具所需的空间，并且性能损失可以忽略不计。SHSR是一种元级学习算法，它分析AutoML工具在几个数据集上的过去运行结果，并学习哪些超参数值可以从要分析的新数据集中过滤掉。SHSR在284个分类问题和375个回归问题上进行了评估，显示出约30%的执行时间缩短和不到0.1%的性能损失。

    AutoML platforms have numerous options for the algorithms to try for each step of the analysis, i.e., different possible algorithms for imputation, transformations, feature selection, and modelling. Finding the optimal combination of algorithms and hyper-parameter values is computationally expensive, as the number of combinations to explore leads to an exponential explosion of the space. In this paper, we present the Sequential Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an AutoML tool with negligible drop in its predictive performance. SHSR is a meta-level learning algorithm that analyzes past runs of an AutoML tool on several datasets and learns which hyper-parameter values to filter out from consideration on a new dataset to analyze. SHSR is evaluated on 284 classification and 375 regression problems, showing an approximate 30% reduction in execution time with a performance drop of less than 0.1%.
    
[^133]: 不变随机森林：基于树模型的ODD泛化解决方案

    Invariant Random Forest: Tree-Based Model Solution for OOD Generalization. (arXiv:2312.04273v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.04273](http://arxiv.org/abs/2312.04273)

    本文引入了一种新颖且有效的解决方案，用于决策树模型的OOD泛化，称为不变决策树（IDT）。该方法通过在树的生长过程中惩罚不同环境下分裂的不稳定行为，构建了不变随机森林（IRF）。验证实验证明，相比非OOD树模型，该方法表现出更好的性能，强调了考虑树模型的OOD泛化的必要性。

    

    在机器学习中，Out-Of-Distribution（OOD）泛化是一个重要的主题。然而，最近的研究只关注神经网络的相应方法。本文引入了一种新颖且有效的解决方案，用于决策树模型的OOD泛化，称为不变决策树（IDT）。IDT通过在树的生长过程中关于在不同环境下分裂的不稳定/变化行为的惩罚项来推动其发展。其集成版本，不变随机森林（IRF）被构建。我们提出的方法受到了一个在温和条件下的理论结果的启发，并通过合成和实际数据集的数值测试进行了验证。与非OOD树模型相比的优越性能意味着，考虑树模型的OOD泛化是绝对必要的，应该给予更多关注。

    Out-Of-Distribution (OOD) generalization is an essential topic in machine learning. However, recent research is only focusing on the corresponding methods for neural networks. This paper introduces a novel and effective solution for OOD generalization of decision tree models, named Invariant Decision Tree (IDT). IDT enforces a penalty term with regard to the unstable/varying behavior of a split across different environments during the growth of the tree. Its ensemble version, the Invariant Random Forest (IRF), is constructed. Our proposed method is motivated by a theoretical result under mild conditions, and validated by numerical tests with both synthetic and real datasets. The superior performance compared to non-OOD tree models implies that considering OOD generalization for tree models is absolutely necessary and should be given more attention.
    
[^134]: DKiS: 衰减权重可逆图像隐写术及私钥

    DKiS: Decay weight invertible image steganography with private key. (arXiv:2311.18243v2 [cs.MM] UPDATED)

    [http://arxiv.org/abs/2311.18243](http://arxiv.org/abs/2311.18243)

    DKiS是一种基于私钥的图像隐写术，通过引入衰减权重来控制信息的传输，提高了隐写术的性能，并确保隐藏信息的安全性。

    

    图像隐写术是将信息隐藏在另一幅图像中的实践，传统上，在其方法公开或遭受攻击时，会遇到安全挑战。为了解决这个问题，引入了一种基于私钥的新型图像隐写术技术。这种方法确保隐藏信息的安全性，因为访问需要相应的私钥，而不管图像隐写术方法的公开知识如何。我们展示了实验证据，证明了我们的方法的有效性并展示了其现实世界的适用性。此外，我们还发现了可逆图像隐写术过程中的一个关键挑战：从秘密到主机传输非必要或“垃圾”信息。为了解决这个问题，我们引入了衰减权重来控制信息的传输，有效地筛选掉无关数据，提高了图像隐写术的性能。

    Image steganography, defined as the practice of concealing information within another image, traditionally encounters security challenges when its methods become publicly known or are under attack. To address this, a novel private key-based image steganography technique has been introduced. This approach ensures the security of the hidden information, as access requires a corresponding private key, regardless of the public knowledge of the steganography method. Experimental evidence has been presented, demonstrating the effectiveness of our method and showcasing its real-world applicability. Furthermore, a critical challenge in the invertible image steganography process has been identified by us: the transfer of non-essential, or `garbage', information from the secret to the host pipeline. To tackle this issue, the decay weight has been introduced to control the information transfer, effectively filtering out irrelevant data and enhancing the performance of image steganography. The cod
    
[^135]: 在逆向识别中标记神经表示

    Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13594](http://arxiv.org/abs/2311.13594)

    逆向识别 (INVERT) 是一种可扩展的方法，通过连接学习到的神经表示与人类可理解的概念，实现了对神经表示的标记并提供了统计显著性评估指标。

    

    深度神经网络(DNNs)在学习复杂的层级数据表示方面表现出卓越的能力，但这些表示的性质仍然大部分未知。现有的全局可解释性方法，如网络解剖(Network Dissection)，存在诸多限制，如依赖分割遮罩、缺乏统计显著性检验和高计算需求。我们提出了Inverse Recognition (INVERT)方法，一种可扩展的方法，通过利用其区分这些概念的能力，将学习到的表示与人类可理解的概念相连接。与之前的工作相比，INVERT能够处理不同类型的神经元，计算复杂度更低，并且不依赖于分割遮罩的可用性。此外，INVERT提供了一个可解释的度量，评估表示和其相应解释之间的对齐，并提供一种统计显著性的度量。我们展示了INVERT的应用性。

    Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. We demonstrate the applicability of INVE
    
[^136]: 大型语言模型增强的算法选择：朝着全面算法表示的方向

    Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation. (arXiv:2311.13184v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13184](http://arxiv.org/abs/2311.13184)

    本论文提出了一种方法，通过将算法表示集成到算法选择中，从而填补了当前算法选择技术对算法特征的研究空白。

    

    算法选择旨在在执行之前识别解决特定问题的最合适算法，已成为自动机器学习中的关键过程。当前主流的算法选择技术主要依赖于各种问题的特征表示，并使用每个算法的性能作为监督信息。然而，目前对算法特征的考虑存在重要的研究空白。这主要归因于算法的固有复杂性，使得在不同种类的算法中找到一种普适有效的特征提取方法特别具有挑战性。不幸的是，忽视了这一方面无疑会影响算法选择的准确性，并间接需要增加训练数据的数量。本文提出了一种方法来解决这一空白，即将算法表示集成到算法选择中。

    Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm
    
[^137]: FIKIT：基于优先级的实时GPU多任务调度与内核识别

    FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2311.10359](http://arxiv.org/abs/2311.10359)

    FIKIT是一种基于优先级的实时GPU多任务调度策略，具有内核识别功能，能够在高优先级任务的内核间填充空闲时间。

    

    高并行工作负载，如机器学习训练、推断和一般HPC任务，通过使用GPU设备得到了极大的加速。在云计算集群中，通过多任务共享来提供GPU的计算能力是非常需要的，因为总是有更多的任务请求而不是可用的GPU数量。现有的GPU共享解决方案着重于减少多个作业争夺单个GPU时的任务级等待时间或任务级切换成本。连续计算请求具有不同的优先级，对于共享GPU设备，对QoS产生了非对称的影响。现有工作没有充分利用这种情况带来的内核级优化机会。为了解决这个问题，我们提出了一种新颖的内核级调度策略FIKIT：填充内核间空闲时间。FIKIT包含任务级优先级信息、细粒度内核识别和内核测量，允许低优先级任务在高优先级任务的内核间执行。

    Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-k
    
[^138]: 输入凸LSTM：一种快速基于Lyapunov模型预测控制的凸方法

    Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.07202](http://arxiv.org/abs/2311.07202)

    本研究提出了一种基于输入凸LSTM的基于Lyapunov的模型预测控制方法，通过减少收敛时间和缓解梯度消失/爆炸问题来改善MPC的性能。

    

    利用输入凸神经网络（ICNN），基于ICNN的模型预测控制（MPC）通过在MPC框架中保持凸性成功实现全局最优解。然而，当前的ICNN架构存在梯度消失/爆炸问题，限制了它们作为复杂任务的深度神经网络的能力。此外，当前基于神经网络的MPC，包括传统的基于神经网络的MPC和基于ICNN的MPC，与基于第一原理模型的MPC相比面临较慢的收敛速度。在本研究中，我们利用ICNN的原理提出了一种新的基于输入凸LSTM的基于Lyapunov的MPC，旨在减少收敛时间、缓解梯度消失/爆炸问题并确保闭环稳定性。通过对非线性化学反应器的模拟研究，我们观察到了梯度消失/爆炸问题的缓解和收敛时间的减少，收敛时间平均降低了一定的百分之。

    Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
    
[^139]: 极端多标签分类中长尾性能的通用测试工具

    Generalized test utilities for long-tail performance in extreme multi-label classification. (arXiv:2311.05081v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.05081](http://arxiv.org/abs/2311.05081)

    这项研究提出了一种解决极端多标签分类中长尾性能问题的通用测试工具，通过分析广义度量预算和预期测试效用框架，推导出最优预测规则。

    

    极端多标签分类（XMLC）是从一个非常庞大的可能标签集中选择一个小子集的任务。因此，它以长尾标签为特征，即大多数标签只有很少的正实例。使用标准的性能度量如precision@k，分类器可以忽略长尾标签并仍然报告良好的性能。然而，通常认为在长尾中的正确预测更加“有趣”或“有奖励”，但学术界尚未就捕捉这个直观概念的度量方式达成共识。现有的倾向得分度量在解决长尾和丢失标签问题方面存在局限性。在本文中，我们分析了作为替代解决方案的广义度量预算“在k处”。为了解决这些度量的优化问题，我们将其制定在预期测试效用（ETU）框架中，该框架旨在优化在固定测试集上的预期性能。我们推导出最优预测规则。

    Extreme multi-label classification (XMLC) is the task of selecting a small subset of relevant labels from a very large set of possible labels. As such, it is characterized by long-tail labels, i.e., most labels have very few positive instances. With standard performance measures such as precision@k, a classifier can ignore tail labels and still report good performance. However, it is often argued that correct predictions in the tail are more "interesting" or "rewarding," but the community has not yet settled on a metric capturing this intuitive concept. The existing propensity-scored metrics fall short on this goal by confounding the problems of long-tail and missing labels. In this paper, we analyze generalized metrics budgeted "at k" as an alternative solution. To tackle the challenging problem of optimizing these metrics, we formulate it in the expected test utility (ETU) framework, which aims to optimize the expected performance on a fixed test set. We derive optimal prediction rul
    
[^140]: 使用矩匹配高斯混合模型改进了DDIM采样

    Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.04938](http://arxiv.org/abs/2311.04938)

    在DDIM框架中使用GMM作为反向转移算子，通过矩匹配可以获得质量更高的样本。在无条件模型和类条件模型上进行了实验，并通过FID和IS指标证明了我们的方法的改进效果。

    

    我们提出在Denoising Diffusion Implicit Models (DDIM)框架中使用高斯混合模型（GMM）作为反向转移算子（内核），这是一种从预训练的Denoising Diffusion Probabilistic Models (DDPM)中加速采样的广泛应用方法之一。具体而言，我们通过约束GMM的参数，匹配DDPM前向边际的一阶和二阶中心矩。我们发现，通过矩匹配，可以获得与使用高斯核的原始DDIM相同或更好质量的样本。我们在CelebAHQ和FFHQ的无条件模型以及ImageNet数据集的类条件模型上提供了实验结果。我们的结果表明，在采样步骤较少的情况下，使用GMM内核可以显著改善生成样本的质量，这是通过FID和IS指标衡量的。例如，在ImageNet 256x256上，使用10个采样步骤，我们实现了一个FID值为...

    We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
    
[^141]: 关于随机神经网络的Lipschitz常数

    On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])

    [http://arxiv.org/abs/2311.01356](http://arxiv.org/abs/2311.01356)

    本文研究了随机ReLU神经网络的Lipschitz常数，对于浅层神经网络，我们得到了Lipschitz常数的精确刻画，对于足够宽度的深层神经网络，我们给出了上下界，并匹配一个依赖于深度的对数因子。

    

    实证研究广泛证明神经网络对输入的微小对抗性扰动非常敏感。这些所谓的对抗性示例的最坏情况鲁棒性可以通过神经网络的Lipschitz常数来量化。然而，关于这个量的理论结果在文献中仅有少数。在本文中，我们开始研究随机ReLU神经网络的Lipschitz常数，即选择随机权重并采用ReLU激活函数的神经网络。对于浅层神经网络，我们将Lipschitz常数刻画到一个绝对数值常数。此外，我们将我们的分析扩展到足够宽度的深层神经网络，我们证明了Lipschitz常数的上下界。这些界匹配到一个依赖于深度的对数因子上。

    Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
    
[^142]: 在金融科技应用中寻找双目标 Pareto 最优欺诈预防规则集

    On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications. (arXiv:2311.00964v1 [cs.LG])

    [http://arxiv.org/abs/2311.00964](http://arxiv.org/abs/2311.00964)

    本文研究了在金融科技应用中寻找高质量的双目标 Pareto 最优欺诈预防规则集的问题。通过采用 Pareto 最优性概念和启发式框架 PORS，我们成功提出了一组非支配的规则子集，并通过实证评估证明了其有效性。

    

    规则在金融科技机构中被广泛用于进行欺诈预防决策，因为规则具有直观的 if-then 结构，易于理解。在实践中，大型金融科技机构通常采用两阶段欺诈预防决策规则集挖掘框架。本文关注于从初始规则集中找到高质量的规则子集，以双目标空间（如精确率和召回率）为基础。为此，我们采用 Pareto 最优性概念，旨在找到一组非支配的规则子集，构成一个 Pareto 前沿。我们提出了一个基于启发式的框架 PORS，并确定了 PORS 的核心问题是前沿解决方案选择（SSF）问题。我们对 SSF 问题进行了系统分类，并在公开和专有数据集上进行了全面的实证评估。我们还引入了一种名为 SpectralRules 的新颖变体的顺序覆盖算法，以鼓励规则的多样性。

    Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diver
    
[^143]: 对贝叶斯优化的期望改进的意外提升

    Unexpected Improvements to Expected Improvement for Bayesian Optimization. (arXiv:2310.20708v1 [cs.LG])

    [http://arxiv.org/abs/2310.20708](http://arxiv.org/abs/2310.20708)

    提出了LogEI作为一类新的贝叶斯优化的获得函数，具有与传统的EI函数相同或近似相等的最优解，但数值上更容易进行优化。

    

    期望改进（EI）可以说是贝叶斯优化中最流行的获得函数，并且已经在很多成功的应用中得到了应用。但是，EI的性能往往被一些新方法超越。尤其是，EI及其变种在并行和多目标设置中很难进行优化，因为它们的获得值在许多区域中数值上变为零。当观测次数增加、搜索空间的维度增加或约束条件的数量增加时，这种困难通常会增加，导致性能在文献中不一致且大多数情况下亚优化。在本论文中，我们提出了LogEI，这是一类新的采样函数。与标准EI相比，这些LogEI函数的成员要么具有相同的最优解，要么具有近似相等的最优解，但数值上更容易进行优化。我们证明了数值病态在“经典”分析EI、期望超体积改进（EHVI）以及它们的...

    Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their
    
[^144]: BasisFormer:基于可学习和可解释的基础的注意力机制时间序列预测

    BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis. (arXiv:2310.20496v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.20496](http://arxiv.org/abs/2310.20496)

    BasisFormer提出了一种基于可学习和可解释的基础的注意力机制时间序列预测方法，通过自适应的自监督学习获得基础，并设计了一个模块计算时间序列与基础之间的相似系数。

    

    基础已经成为现代基于深度学习的时间序列预测模型的重要组成部分，因为它们具有作为特征提取者或未来参考的能力。为了有效，基础必须根据特定的时间序列数据集进行定制，并且在数据集中与每个时间序列展现出明显的相关性。然而，当前最先进的方法在同时满足这两个要求方面存在局限性。为了解决这个挑战，我们提出了BasisFormer，一个端到端的时间序列预测架构，它利用了可学习和可解释的基础。该架构由三个组件组成：首先，我们通过自适应的自监督学习获得基础，该学习将时间序列的历史和未来部分视为两个不同的视图，并采用对比学习。接下来，我们设计了一个Coef模块，通过双向交叉注意力计算历史视图中时间序列与基础之间的相似系数。

    Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attenti
    
[^145]: 学习分类还是分类学习？自编码实现普适类别发现

    Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.19776](http://arxiv.org/abs/2310.19776)

    本论文提出了一种新颖、高效和自我监督的方法，可以在测试时发现以前未知的类别，通过将最小长度类别代码分配给单个数据实例来增强对类别细粒度的控制。

    

    在揭示测试时的新类别的探索中，我们面临着传统有监督识别模型的固有限制，这些模型受到预定义类别集的限制。虽然在自我监督和开放式学习领域取得了一定进展，以实现测试时的类别发现，但一个关键但常常被忽视的问题仍然存在：什么确切地界定了一个类别？在本文中，我们通过优化的视角概念化类别，将其视为一个明确定义问题的最优解。利用这种独特的概念化，我们提出了一种新颖、高效和自我监督的方法，能够在测试时发现以前未知的类别。我们方法的一个显著特点是将最小长度类别代码分配给单个数据实例，这样可以概括真实世界数据集中普遍存在的隐含类别层次结构。这种机制使我们能够更好地控制类别的细粒度，从而为我们的模型提供了增强的能力。

    In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a category? In this paper, we conceptualize a category through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our mode
    
[^146]: SpecTr: 通过最优传输实现快速具有推测性的解码

    SpecTr: Fast Speculative Decoding via Optimal Transport. (arXiv:2310.15141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15141](http://arxiv.org/abs/2310.15141)

    本研究通过最优传输的方法提供了推测性解码的原则性理解，使得从大语言模型中进行自回归采样的过程能够更快速地进行。

    

    从大语言模型中进行自回归采样在多个自然语言任务中取得了最先进的结果。然而，自回归采样一次只生成一个标记，这使得速度慢，在某些任务中甚至是禁止的。加速采样的一种方式是“推测性解码”：使用一个小模型来采样一个“草稿”（块或标记序列），然后由大语言模型并行评分草稿中的所有标记。根据统计方法，接受一部分草稿中的标记（拒绝剩余标记），以确保最终输出遵循大模型的分布。在这项工作中，我们通过最优传输（OT）与“成员费用”的视角提供了推测性解码的原则性理解。这个框架可以被看作是“最大耦合”问题的扩展。这种新的形式使我们能够将推测性解码方法推广到允许一个集合的标记的情况。

    Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks. However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is $\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$ (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with $\textit{membership cost}$. This framework can be viewed as an extension of the well-known $\textit{maximal-coupling}$ problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of $
    
[^147]: 掩码硬注意力变换器和布尔RASP准确识别无星语言。

    Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2310.13897](http://arxiv.org/abs/2310.13897)

    给出了一种新的变换器编码器模型，该模型具有硬注意力和严格未来掩码，并且证明这些网络识别的语言类别正是无星语言。研究还发现，通过添加位置嵌入，这一模型可以扩展到其他研究充分的语言类别。一个关键技术是布尔RASP，通过无星语言的研究，将变换器与一阶逻辑、时态逻辑和代数自动机理论相关联。

    

    我们考虑具有硬注意力（即所有注意力都集中在一个位置上）和严格的未来掩码（即每个位置只与严格左侧的位置进行注意力交互）的变换器编码器，并证明这些网络识别的语言类别正是无星语言。添加位置嵌入将被识别的语言类别扩展到其他研究充分的类别。这些证明中的一个关键技术是布尔RASP，它是一种受限于布尔值的RASP变种。通过无星语言，我们将变换器与一阶逻辑、时态逻辑和代数自动机理论联系起来。

    We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
    
[^148]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^149]: Transformer语言模型中跨任务的电路组件复用

    Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])

    [http://arxiv.org/abs/2310.08744](http://arxiv.org/abs/2310.08744)

    这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。

    

    最近在机制可解释性方面的研究表明，通过电路分析可以成功地逆向工程语言模型的行为。然而，一个常见的批评是每个电路都是任务特定的，因此这样的分析不能为更高级的理解模型做出贡献。在这项工作中，我们提出证据表明洞察力（关于特定头部的低级发现和关于一般算法的高级发现）确实可以在任务之间进行泛化。具体而言，我们研究了Wang等人（2022）在间接宾语识别任务（IOI）中发现的电路，并展示了这个电路在更大的GPT2模型上的重现，以及在看似不同的任务中大部分被复用来解决问题：彩色物体（Ippolito和Callison-Burch，2023）。我们提供证据表明两个任务底层的过程在功能上非常相似，并且在电路中的注意力头部之间有大约78％的重叠。我们进一步展示了一个概念验证干预实验

    Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
    
[^150]: 3D-Mol: 一种新颖的基于对比学习的分子性质预测框架，利用了3D信息

    3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information. (arXiv:2309.17366v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.17366](http://arxiv.org/abs/2309.17366)

    3D-Mol是一种新颖的基于3D结构的分子建模方法，通过对比学习提高了分子性质预测准确性，并在多个基准数据集上超过了最先进的模型。

    

    分子性质预测为药物候选物的早期筛选和优化提供了一种有效且高效的方法。尽管基于深度学习的方法取得了显著进展，但大多数现有方法仍未充分利用3D空间信息。这可能导致单个分子表示多个实际分子。为解决这些问题，我们提出了一种名为3D-Mol的新颖的基于3D结构的分子建模方法。为了准确表示完整的空间结构，我们设计了一种新颖的编码器，通过将分子分解成三个几何图形来提取3D特征。此外，我们使用20M个无标签数据通过对比学习对模型进行预训练。我们将具有相同拓扑结构的构象视为正样本对，将相反的构象视为负样本对，而权重则由构象之间的差异确定。我们在7个基准数据集上将3D-Mol与各种最先进的基准模型进行了对比。

    Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art (SOTA) baselines on 7 benchmarks and de
    
[^151]: 系统化泛化的组合式程序生成

    Compositional Program Generation for Systematic Generalization. (arXiv:2309.16467v1 [cs.LG])

    [http://arxiv.org/abs/2309.16467](http://arxiv.org/abs/2309.16467)

    系统化泛化是人类的关键技能之一，组合式程序生成器（CPG）通过模块化、类型抽象和递归组合的特征，能够以少样本的方式对新概念进行系统化的泛化，在各种语言任务上具有生产力。

    

    组合式泛化是人类的关键技能之一，它使我们能够从少数例子中学习新概念。机器学习模型，包括如今无处不在的transformers，在这方面很难进行泛化，并且通常需要在训练过程中提供数千个概念示例才能进行有意义的泛化。人类与人工神经网络结构在能力上的差异，促使了对一种称为组合式程序生成器（CPG）的神经符号体系结构进行研究。CPG具有三个关键特征：模块化、类型抽象和递归组合，它使其能够以少样本的方式对新概念进行系统化的泛化，同时在各种序列到序列的语言任务上具有生产力。对于每个输入，CPG使用输入领域的语法和解析器生成一个类型层次结构，在这个结构中，每个语法规则都被分配了一个独特的语义模块——一个概率性的复制或替换程序。

    Compositional generalization is a key ability of humans that enables us to learn new concepts from only a handful examples. Machine learning models, including the now ubiquitous transformers, struggle to generalize in this way, and typically require thousands of examples of a concept during training in order to generalize meaningfully. This difference in ability between humans and artificial neural architectures, motivates this study on a neuro-symbolic architecture called the Compositional Program Generator (CPG). CPG has three key features: modularity, type abstraction, and recursive composition, that enable it to generalize both systematically to new concepts in a few-shot manner, as well as productively by length on various sequence-to-sequence language tasks. For each input, CPG uses a grammar of the input domain and a parser to generate a type hierarchy in which each grammar rule is assigned its own unique semantic module, a probabilistic copy or substitution program. Instances w
    
[^152]: Astroconformer：使用基于Transformer的深度学习模型分析恒星光曲线的前景

    Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models. (arXiv:2309.16316v1 [astro-ph.SR])

    [http://arxiv.org/abs/2309.16316](http://arxiv.org/abs/2309.16316)

    Astroconformer是一个基于Transformer的深度学习框架，旨在从恒星光曲线中捕捉长程依赖关系。通过应用于Kepler光曲线数据集，实现了对恒星表面重力的准确估计。

    

    恒星的光变曲线包含了丰富的关于恒星振荡和颗粒运动的信息，从而为我们提供了关于恒星内部结构和演化状态的重要洞见。传统的星震学技术主要依赖于功率谱分析，忽略了光变曲线中包含的宝贵的相位信息。虽然最近的机器学习应用在星震学中利用卷积神经网络（CNNs）从光变曲线中成功地推断出恒星属性，但往往受限于卷积操作中固有的局部特征提取。为了解决这些限制，我们提出了Astroconformer，这是一个基于Transformer的深度学习框架，旨在捕捉恒星光曲线中的长程依赖关系。我们的实证分析主要集中在估计表面重力（log g），并基于从Kepler光曲线中精心筛选得到的数据集进行。这些光曲线包含了大量恒星的观测数据。

    Light curves of stars encapsulate a wealth of information about stellar oscillations and granulation, thereby offering key insights into the internal structure and evolutionary state of stars. Conventional asteroseismic techniques have been largely confined to power spectral analysis, neglecting the valuable phase information contained within light curves. While recent machine learning applications in asteroseismology utilizing Convolutional Neural Networks (CNNs) have successfully inferred stellar attributes from light curves, they are often limited by the local feature extraction inherent in convolutional operations. To circumvent these constraints, we present $\textit{Astroconformer}$, a Transformer-based deep learning framework designed to capture long-range dependencies in stellar light curves. Our empirical analysis, which focuses on estimating surface gravity ($\log g$), is grounded in a carefully curated dataset derived from $\textit{Kepler}$ light curves. These light curves fe
    
[^153]: ICML 2023拓扑深度学习挑战：设计与结果

    ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])

    [http://arxiv.org/abs/2309.15188](http://arxiv.org/abs/2309.15188)

    本文介绍了ICML 2023拓扑深度学习挑战，该挑战要求参与者在两个月内提供开源实现的拓扑神经网络，吸引了28个合格的提交。

    

    本文介绍了ICML 2023拓扑与几何机器学习研讨会中举办的拓扑深度学习计算挑战。该比赛要求参与者通过贡献于python包TopoNetX（数据处理）和TopoModelX（深度学习）的开源实现来提供文献中的拓扑神经网络。该挑战在两个月的时间内吸引了28个合格的提交。本文描述了挑战的设计并总结了其主要发现。

    This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
    
[^154]: 软混合降噪：超越扩散模型的表达瓶颈

    Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models. (arXiv:2309.14068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14068](http://arxiv.org/abs/2309.14068)

    本文发现了扩散模型在反向降噪中存在一个表达瓶颈，并且推出了一种新的软混合降噪（SMD）模型，该模型在理论上能够很好地逼近任意高斯混合分布，并且在实现上简单高效。

    

    由于扩散模型在诸如图像合成等任务中表现出令人印象深刻的性能，在最近的研究中有一种趋势证明（在某些假设下）这些模型具有强大的近似能力。本文中，我们展示了当前扩散模型在反向降噪方面实际上存在一个表达瓶颈，并且一些现有理论保证所做的假设过于强大。基于这一发现，我们证明了扩散模型在本地和全局降噪中存在无界误差。根据我们的理论研究，我们引入了软混合降噪（SMD），这是一个表达力强且高效的反向降噪模型。SMD不仅在理论上允许扩散模型很好地逼近任意高斯混合分布，而且在实现上简单高效。我们在多个图像数据集上的实验证明，SMD显著改善了不同类型的扩散模型（例如DDPM），尤其在少量反向降噪的情况下。

    Because diffusion models have shown impressive performances in a number of tasks, such as image synthesis, there is a trend in recent works to prove (with certain assumptions) that these models have strong approximation capabilities. In this paper, we show that current diffusion models actually have an expressive bottleneck in backward denoising and some assumption made by existing theoretical guarantees is too strong. Based on this finding, we prove that diffusion models have unbounded errors in both local and global denoising. In light of our theoretical studies, we introduce soft mixture denoising (SMD), an expressive and efficient model for backward denoising. SMD not only permits diffusion models to well approximate any Gaussian mixture distributions in theory, but also is simple and efficient for implementation. Our experiments on multiple image datasets show that SMD significantly improves different types of diffusion models (e.g., DDPM), espeically in the situation of few backw
    
[^155]: 基于花瓣拉普拉斯在简单复合体上的高阶图卷积网络

    Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])

    [http://arxiv.org/abs/2309.12971](http://arxiv.org/abs/2309.12971)

    本文提出了基于花瓣拉普拉斯的高阶图卷积网络，通过利用简单复合体来建模高阶交互，在不同拓扑尺度上识别内在特征，并使用可学习的图滤波器来量化高阶交互强度。

    

    尽管普通图神经网络（GNNs）在许多任务上取得了成功，但其基于配对交互网络的基础本质上限制了其识别复杂系统中潜在高阶交互的能力。为了弥补这种能力差距，我们提出了一种新颖的方法，利用复杂系统的高阶交互建模的丰富数学理论，即简单复合体（SCs）-一种对建模高阶交互具有鲁棒性的工具。目前基于SC的GNNs存在复杂度高和刻板的问题，并且量化高阶交互强度仍然具有挑战性。创新地，我们提出了一个高阶花瓣（FP）模型，将FP拉普拉斯引入到SC中。此外，我们引入了一个以FP拉普拉斯为基础的高阶图卷积网络（HiGCN），能够识别不同拓扑尺度上的内在特征。通过使用可学习的图滤波器，FP拉普拉斯域内的参数组，我们可以识别出具有不同模式的图案，其中滤波器的权重用作数量化高阶交互的工具。

    Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
    
[^156]: Virchow: 数百万张全数字病理学基础模型

    Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])

    [http://arxiv.org/abs/2309.07778](http://arxiv.org/abs/2309.07778)

    Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。

    

    计算病理学利用人工智能通过分析全数字切片图像实现精准医学和决策支持系统，有潜力彻底改变癌症的诊断和治疗。然而，实现这个目标的一个主要挑战是对于许多特定的计算病理学任务，数据量不足以进行开发。为了应对这个挑战，我们创建了Virchow，一个632百万参数的深度神经网络基础模型，用于计算病理学。通过自监督学习，Virchow在1.5百万个不同组织样本的苏木精和伊红染色全数字切片图像上进行训练，这比之前的研究数据量大得多。在包括瓦片级全癌检测和亚型以及幻灯片级生物标志物预测在内的下游任务上，Virchow在来自与预训练数据相同人群的内部数据集和外部公开数据集上均胜过最先进的系统。

    Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
    
[^157]: 聊天失败和问题：原因和解决方案

    Chat Failures and Troubles: Reasons and Solutions. (arXiv:2309.03708v1 [cs.RO])

    [http://arxiv.org/abs/2309.03708](http://arxiv.org/abs/2309.03708)

    本文研究了人机交互中聊天失败和问题的原因，提出了闭环控制算法和强化学习模型等解决方案以降低错误。

    

    本文研究了人机交互中聊天导致失败和问题的一些常见问题。一个给定的用例的设计决策始于选择合适的机器人、合适的聊天模型，识别导致失败的常见问题，找出潜在的解决方案，并计划持续改进。总结起来，建议使用闭环控制算法来引导训练好的人工智能预训练模型的使用，并提供词汇过滤、对新数据集重新训练批次模型、在线学习数据流以及/或使用强化学习模型来自我更新训练模型以降低错误。

    This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
    
[^158]: BridgeData V2:一个用于规模化机器人学习的数据集

    BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])

    [http://arxiv.org/abs/2308.12952](http://arxiv.org/abs/2308.12952)

    BridgeData V2是一个大规模且多样化的机器人操作行为数据集，广泛应用于机器人学习研究。该数据集具备任务和环境变异性，并且兼容多种学习方法，通过实验表明，使用此数据集可以提高模型性能。

    

    我们介绍了BridgeData V2，这是一个大规模且多样化的机器人操作行为数据集，旨在促进规模化机器人学习的研究。BridgeData V2包含了在一个公开可用且成本较低的机器人上收集的60,096个轨迹，覆盖了24个环境。BridgeData V2提供了广泛的任务和环境变异性，使得可以在不同的环境、领域和机构之间进行泛化的技能，使得该数据集成为广大研究人员的有用资源。此外，该数据集与多种开放词汇、多任务学习方法以目标图像或自然语言指令为条件是兼容的。在我们的实验中，我们在我们的数据集上训练了6种最先进的模仿学习和离线强化学习方法，并发现它们在一系列需要不同泛化程度的任务上取得了成功。我们还展示了这些方法的性能随着更多的数据和更高容量的模型而改善，并且通过训练集大小的增加和模型容量的增加获得了更好的表现。

    We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that trai
    
[^159]: 探索大语言模型用于代码生成的参数高效微调技术

    Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. (arXiv:2308.10462v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2308.10462](http://arxiv.org/abs/2308.10462)

    本文探索了大型语言模型在资源有限的环境下用于代码生成的参数高效微调技术，并提出了参数高效微调作为一种有前途的方法，可以在保持合理资源消耗的同时，高效地将语言模型专门用于任务特定的数据。

    

    大型语言模型（LLM）展示了在没有特定微调的情况下，即可根据自然语言意图生成准确的代码片段的印象能力。尽管先前的研究已经突出了微调LLMs的优势，但这个过程代价高，对于拥有数十亿个参数的模型来说，在资源稀缺的环境下是不切实际的。为了解决这些挑战，以前的研究探索了在上下文学习（ICL）作为一种策略，用任务特定的提示示例指导LLM生成过程。然而，ICL引入了一些不便之处，比如需要设计上下文相关的提示和没有学习任务特定的参数，从而限制了下游任务的性能。在这种情况下，我们预见参数高效微调（PEFT）技术作为一种有前途的方法，可以在保持合理资源消耗的同时，高效地将LLM专门用于任务特定的数据。

    Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In t
    
[^160]: LLM4TS:使用预训练的LLM进行两阶段微调用于时间序列预测

    LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])

    [http://arxiv.org/abs/2308.08469](http://arxiv.org/abs/2308.08469)

    这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。

    

    在这项工作中，我们利用预训练的大型语言模型（LLMs）来增强时间序列预测。借鉴了自然语言处理和计算机视觉统一模型的日益增长的兴趣，我们设想创建一个类似的模型用于长期时间序列预测。由于缺乏大规模的时间序列数据来构建稳健的基础模型，我们的方法LLM4TS专注于利用预训练的LLMs的优势。通过将时间序列修补与时间编码相结合，我们提高了LLMs处理时间序列数据的能力。受到聊天机器人领域的有监督微调的启发，我们优先进行两阶段的微调过程：首先进行有监督微调以使LLMs适应时间序列数据，然后进行任务特定的下游微调。此外，为了在不进行大量参数调整的情况下发挥预训练LLMs的灵活性，我们采用了几种参数高效微调（PEFT）技术。

    In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
    
[^161]: 扩散模型是否会受到错误传播的影响？理论分析和一致性正则化。

    Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])

    [http://arxiv.org/abs/2308.05021](http://arxiv.org/abs/2308.05021)

    扩散模型可能受到错误传播的影响，本文通过理论分析和数据实证验证了这个问题，并提出了一种一致性正则化方法来解决。我们的分析揭示了模型的去噪模块是否具有容错性以及如何减少分布差异。

    

    虽然扩散模型在数据合成方面取得了令人期待的成果，但由于其级联结构，即去噪模块链式传播和放大了分布不匹配的错误，因此可能会受到错误传播的影响。然而，我们期望进行严格的分析，因为许多顺序模型，如条件随机场（CRF），是不会出现错误传播的。在本文中，我们通过实证和理论验证了扩散模型确实受到错误传播的影响，并提出一种正则化方法来解决这个问题。我们的理论分析揭示了这个问题是否可以归结为扩散模型的每个去噪模块是否具有容错性。我们推导出了有深刻见解的转移方程，表明模块无法从输入错误中恢复，甚至会将额外的错误传播到下一个模块。我们的分析直接导致了扩散模型的一致性正则化方案，可以明确减少分布的差异。

    While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution 
    
[^162]: 通过随机定位方法获得扩散模型的线性收敛界限

    Linear Convergence Bounds for Diffusion Models via Stochastic Localization. (arXiv:2308.03686v1 [stat.ML])

    [http://arxiv.org/abs/2308.03686](http://arxiv.org/abs/2308.03686)

    通过随机定位方法，我们提供了一种解决扩散模型中线性收敛界限问题的方法，并证明了这种方法可以在有限二阶矩条件下达到可接受的精度。

    

    扩散模型是从高维数据分布中生成近似样本的有效方法。最近的一些研究结果提供了关于这种模型的收敛速度的多项式界限，假设$L^2$准确的得分估计器。然而，到目前为止，已知的最佳界限要么对数据维度是超线性的，要么需要强平滑性假设。我们提供了第一个假设只需要数据分布有有限二阶矩的收敛界限，这些界限对于数据维度是线性的（乘以对数因子）。我们证明了扩散模型最多需要$\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$步，就可以将带有方差为$\delta$的高斯噪声损坏的任意数据分布在Kullback--Leibler散度下近似到$\varepsilon^2$。我们的证明依赖于前人的Girsanov方法。我们引入了对于反向SD离散化误差的精细处理。

    Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SD
    
[^163]: 基于曲率的变压器用于分子属性预测

    Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])

    [http://arxiv.org/abs/2307.13275](http://arxiv.org/abs/2307.13275)

    该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。

    

    分子性质的预测是基于人工智能的药物设计领域中最重要且具有挑战性的任务之一。在当前主流的方法中，用于训练DNN模型的最常用特征表示基于SMILES和分子图，尽管这些方法简洁高效，但也限制了对空间信息的捕捉能力。在本研究中，我们提出了基于曲率的变压器，通过引入 Ricci 曲率离散化，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。为了将曲率嵌入模型中，在注意力得分计算期间，我们将图的曲率信息作为位置编码添加到节点特征中。这种方法可以在不改变原始网络结构的情况下，将曲率信息引入图数据，并且有潜力扩展到其他模型。我们进行了实验证明了这种方法的有效性。

    The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
    
[^164]: 开放联邦学习平台：技术和法律观察的综述和愿景

    Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2307.02140](http://arxiv.org/abs/2307.02140)

    本文探讨了开放联邦学习平台的技术和法律观察，提出了基于查询和基于合同的两种适用于开放联邦学习的合作框架，并对构建开放的FL平台的可行性进行了全面评估。

    

    传统的联邦学习（FL）遵循服务器主导的合作模式，限制了FL的应用场景，并降低了数据持有者参与的热情。为了充分释放FL的潜力，我们主张重新思考当前FL框架的设计，并将其扩展为更通用的概念：开放联邦学习平台。我们提出了两个相互合作的FL框架：基于查询的FL和基于合同的FL。在这个综述中，我们从技术和法律的角度对构建开放的FL平台的可行性进行了全面的评估。我们首先回顾了FL的定义，并总结了其固有的局限性，包括服务器-客户端耦合、模型可重用性低和非公开性。在基于查询的FL平台中，这是一个由社区赋能的开放模型共享和重用平台，我们探讨了一系列有价值的主题，包括全球最新可用模型和模型的查询、服务质量保证和奖励机制。

    Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-d
    
[^165]: 使用音频数据检测政治辩论、演讲和访谈中值得核实的论断

    Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])

    [http://arxiv.org/abs/2306.05535](http://arxiv.org/abs/2306.05535)

    政治辩论、演讲和访谈中的值得核实的论断可以使用音频数据进行检测和确认，这可帮助主持人、记者和事实核查组织进行工作。

    

    社会的一大部分团结在相同的愿景和思想周围，具有巨大的能量。这正是政治人物希望为他们的事业所累积的。为了达到这个目标，他们有时会使用扭曲或隐藏真相的手段，无论是无意的还是有意的，这为错误信息和误导开了大门。自动检测值得核实的论断的工具将对辩论主持人、记者和事实核查组织有很大帮助。虽然以前关于检测值得核实的论断的工作重点是文本，但在这里，我们探讨了音频信号作为额外信息源的实用性。我们创建了一个新的多模态数据集（英语文本和音频），包含48小时的演讲。我们的评估结果表明，在多个演讲者的情况下，音频模态与文本结合使用比仅使用文本具有改进效果。此外，单声道音频模型可以胜过单声道文本模型。

    A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
    
[^166]: 通过非凸迭代加权最小二乘法同时恢复结构化数据

    Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares. (arXiv:2306.04961v1 [cs.LG])

    [http://arxiv.org/abs/2306.04961](http://arxiv.org/abs/2306.04961)

    该论文提出了一种非凸迭代加权最小二乘法用于同时恢复行稀疏和低秩的数据矩阵，能够在最小样本复杂度的情况下局部二次收敛到同时结构化的数据矩阵，并在实验中表现出有利的经验收敛性。

    

    我们提出了一种新的算法，用于从线性观测中恢复遵循多个异构低维结构的数据。针对同时行稀疏和低秩的数据矩阵，我们提出并分析了一种迭代加权最小二乘（IRLS）算法，能够利用这两种结构。特别地，它优化了一种非凸稀疏性和秩的组合代理，其中平衡被建入到算法中。我们证明了在最小样本复杂度的情况下（最多常数和一个对数因子），迭代方式局部二次收敛到同时结构化数据矩阵，这对于组合凸代理而言是不可能的。在实验中，我们展示了IRLS方法表现出了有利的经验收敛性，从比最先进的方法更少的测量中确定了同时行稀疏和低秩矩阵。

    We propose a new algorithm for the problem of recovering data that adheres to multiple, heterogeneous low-dimensional structures from linear observations. Focusing on data matrices that are simultaneously row-sparse and low-rank, we propose and analyze an iteratively reweighted least squares (IRLS) algorithm that is able to leverage both structures. In particular, it optimizes a combination of non-convex surrogates for row-sparsity and rank, a balancing of which is built into the algorithm. We prove locally quadratic convergence of the iterates to a simultaneously structured data matrix in a regime of minimal sample complexity (up to constants and a logarithmic factor), which is known to be impossible for a combination of convex surrogates. In experiments, we show that the IRLS method exhibits favorable empirical convergence, identifying simultaneously row-sparse and low-rank matrices from fewer measurements than state-of-the-art methods.
    
[^167]: 通过RKHS逼近理解基于增广的自监督表示学习

    Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation. (arXiv:2306.00788v1 [cs.LG])

    [http://arxiv.org/abs/2306.00788](http://arxiv.org/abs/2306.00788)

    本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。

    

    好的数据增强是自监督表示学习（如对比学习和掩码语言建模）实现经验成功的关键因素之一，但其在学习好的表示方面的理论理解仍然有限。最近的工作建立了自监督学习和逼近图拉普拉斯算子的顶部特征空间之间的联系。在这项工作中，我们利用这一洞察力对基于增广的预训练进行统计分析。我们从保持等距的属性出发，这是由增强给出的目标函数的关键几何特征。我们的第一主要定理为任意编码器提供了接近紧密的上限，用于估计通过在编码器之上拟合线性探测器而产生的估计误差和编码器学习的RKHS的逼近误差。我们的第二个主要定理表明，在温和条件下，可以通过RKHS函数任意精确地逼近增广函数。这个结果意味着，随着增广函数质量的提高，学习的编码器的表示能力也会提高。我们的分析还揭示了自监督学习中批量归一化和数据增强的作用。

    Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main
    
[^168]: “思维克隆：通过模仿人类思维学习思考并行动”。（arXiv:2306.00323v1 [cs.AI]）

    Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])

    [http://arxiv.org/abs/2306.00323](http://arxiv.org/abs/2306.00323)

    本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。

    

    语言通常被认为是人类思维的一个关键方面，它为我们提供了非凡的泛化、探索、规划、重新规划和适应新情况的能力。然而，强化学习（RL）代理在这些能力中远未达到人类水平的表现。我们假设其中一个认知缺陷的原因是他们缺乏使用语言思考所带来的好处。我们认为通过训练AI代理人像人类一样思考，可以改善其性能。我们引入了一种新的模仿学习框架“思维克隆”，其想法不仅是克隆人类示范者的行为，而且还包括人类在执行这些行为时所产生的想法。虽然我们希望“思维克隆”在处理网络规模的人类思维和行为数据时能够发挥出色（例如，带有剧本的在线视频），但在这里，我们进行了在思考和行动数据为合成生成的领域的实验。结果显示，“思维克隆”学习速度比传统的强化学习方法快得多。

    Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
    
[^169]: 放松去中心化无遗憾高维贝叶斯优化中的加法约束

    Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19838](http://arxiv.org/abs/2305.19838)

    本文提出了一种放松去中心化无遗憾高维贝叶斯优化中加法约束的方法，并且解决了过度探索问题。该方法称为DumBO，并且在实验中展示了竞争性能。

    

    贝叶斯优化常用于优化一个未知函数$f$，该函数存在噪声且评估成本高昂，通过利用必须在每个优化步骤中最大化的收获函数来实现。尽管可证明渐进最优的BO算法在优化低维函数方面效率很高，但将其扩展到高维空间仍然是一个待解决的问题，通常通过假设$f$具有加法结构来解决。通过这样做，BO算法通常引入了对加法结构的额外限制性假设，降低了它们的适用范围。本文包含两个主要贡献：（i）放松对$f$加法结构的限制性假设，以减弱收获函数的最大化保证；（ii）解决去中心化BO算法中的过度探索问题。为此，我们提出了DumBO，一种渐进最优的去中心化BO算法，具有非常竞争的性能。

    Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, BO algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This paper contains two main contributions: (i) we relax the restrictive assumptions on the additive structure of $f$, at the expense of weakening the maximization guarantees of the acquisition function, and (ii) we address the over-exploration problem for decentralized BO algorithms. To these ends, we propose DumBO, an asymptotically optimal decentralized BO algorithm that achieves very competitive performance
    
[^170]: 点过程注意力支持网格编码以实现分布外泛化

    Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization. (arXiv:2305.18417v1 [cs.LG])

    [http://arxiv.org/abs/2305.18417](http://arxiv.org/abs/2305.18417)

    本文描述了一个基于点过程注意力和网格编码的算法，在分布外测试集上实现了泛化能力，为理解大脑强泛化能力提供了见解。

    

    深度神经网络在模仿类人智能方面取得了巨大进展，并且越来越多地被用来理解大脑如何解决复杂的计算问题。然而，它们仍然不能提供关于大脑如何支持人类能够实现的强形式泛化的见解。其中一个例子是分布外（OOD）泛化——在训练集分布之外的测试样例上成功执行。在这里，我们识别出大脑处理的特征，这些特征可能有助于实现这种能力。我们描述了一个两部分算法，利用神经计算的特定特征实现OOD泛化，并通过评估两个具有挑战性的认知任务的表现来提供概念验证。首先，我们利用哺乳动物大脑使用类似网格的表示（例如在内嗅皮层中）来表示度量空间的事实。

    Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization -successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid-like representations (e.g., in entorhinal cortex): abstract represe
    
[^171]: 离线多智能体强化学习协调问题的基于模型的解决方案

    A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])

    [http://arxiv.org/abs/2305.17198](http://arxiv.org/abs/2305.17198)

    提出了一个基于模型的离线多智能体强化学习方法MOMA-PPO，通过生成合成交互数据并优化智能体的政策，解决了策略一致性和策略微调两个协调问题，在具有挑战性的离线MARL场景中胜过主流的学习方法，提供了实际应用中的可行解决方案。

    

    训练多个智能体进行协调是一项重要问题，具有机器人技术、博弈论、经济学和社会科学等领域的应用。然而，大多数现有的多智能体强化学习方法是在线的，因此在收集新的交互数据成本高昂或危险的实际应用中不可行。虽然这些算法应该利用离线数据，但这样做会引起离线协调问题。具体而言，我们确定并形式化了策略一致性（SA）和策略微调（SFT）两个协调问题，这是当前离线多智能体强化学习算法失败的原因。为解决这个问题，我们提出了一种简单的基于模型的方法，生成合成交互数据，使智能体能够在微调策略的同时收敛于一个策略。我们提出的方法，Model-based Offline Multi-Agent Proximal Policy Optimization（MOMA-PPO），在具有挑战性的离线MARL场景中胜过主流的学习方法，证明了基于模型的方法提供了一个可行的解决方案。

    Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
    
[^172]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^173]: GraphCare: 使用个性化知识图谱提升医疗预测能力

    GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.12788](http://arxiv.org/abs/2305.12788)

    本论文提出了一种名为GraphCare的框架，通过使用个性化知识图谱来改进基于电子健康记录的医疗预测，并通过在两个公共数据集上的实验证明了其有效性。

    

    临床预测模型通常依赖于患者的电子健康记录(EHR)，但将医学知识整合到预测和决策中以提高效果具有挑战性。这是因为个性化预测需要个性化的知识图谱(KG)，而从患者EHR数据中生成个性化知识图谱很困难。为了解决这个问题，我们提出了一个名为\textsc{GraphCare}的开放式框架，它使用外部知识图谱来改进基于EHR的预测。我们的方法从大规模语言模型(LLM)和外部生物医学知识图谱中提取知识，构建个体化的患者知识图谱，然后使用我们提出的Bi-attention AugmenTed (BAT)图神经网络(GNN)进行医疗预测训练。在两个公共数据集MIMIC-III和MIMIC-IV上，\textsc{GraphCare}在四个关键的医疗预测任务上均超过了基准线：死亡率、再入院率、住院天数和药物推荐。在MIMIC-III上，它将AUROC提高了17.6%和6.6%，将F1得分提高了7.9%。

    Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
    
[^174]: 机器制造的媒体：监测虚假新闻和主流新闻网站上机器生成文章的动向。

    Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])

    [http://arxiv.org/abs/2305.09820](http://arxiv.org/abs/2305.09820)

    这篇论文研究了机器生成文章在虚假新闻和主流新闻网站的普及程度，发现虚假新闻网站上合成文章的使用速度比主流网站上更快。

    

    随着像ChatGPT这样的生成式大型语言模型（LLM）日益流行，越来越多的新闻网站开始利用它们生成文章。然而，这些语言模型不仅可能在声誉良好的网站上产生事实不准确的文章，而且不良新闻网站也可以利用这些LLM批量生产虚假信息。为了开始理解这一现象，我们提出了首个大规模研究合成文章在线新闻媒体中普及率的研究。为此，我们训练了一个基于DeBERTa的合成新闻检测器，并对3074个虚假新闻和主流新闻网站的超过1291万篇文章进行分类。我们发现，在2022年1月1日至2023年4月1日期间，合成新闻文章的相对数量在主流网站上增加了79.4％，而在虚假信息网站上增加了342％。分析ChatGPT发布的影响，使用中断时间序列，我们发现，虽然它的发布导致合成文章的使用显著增加，但虚假信息网站上的合成文章使用速度比主流网站上的快。

    With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
    
[^175]: DAISM：基于多项式近似的SRAM内数字乘法器的DNN训练和推理加速器

    DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference. (arXiv:2305.07376v1 [cs.AR])

    [http://arxiv.org/abs/2305.07376](http://arxiv.org/abs/2305.07376)

    本论文提出了一种基于多项式近似的SRAM内数字乘法器，在不依赖于新型存储技术和避免了比特串行计算的情况下，通过内存执行GEMM计算，从而为DNN训练和推理提供了高效的加速器。

    

    DNN是最广泛使用的深度学习模型之一。对于DNN的矩阵乘法运算会产生显著的计算成本，并受限于内存和处理单元之间的数据传输。为了优化矩阵乘法运算，提出了许多专门的加速器。一种流行的想法是使用PIM（Processing-in-Memory），其中计算是由内存存储元件执行的，从而减少了处理器和记忆体之间数据传输的开销。然而，大多数PIM解决方案要么依赖于尚未成熟的新型存储技术，要么依赖于比特串行计算，后者具有重大性能开销和可扩展性问题。在本研究中，提出了一种SRAM内数字乘法器来采用先进的GEMM计算技术，同时避免了比特串行计算的缺点。这使用户可以通过使用现有技术而实现性能显著提升的系统设计。

    DNNs are one of the most widely used Deep Learning models. The matrix multiplication operations for DNNs incur significant computational costs and are bottlenecked by data movement between the memory and the processing elements. Many specialized accelerators have been proposed to optimize matrix multiplication operations. One popular idea is to use Processing-in-Memory where computations are performed by the memory storage element, thereby reducing the overhead of data movement between processor and memory. However, most PIM solutions rely either on novel memory technologies that have yet to mature or bit-serial computations which have significant performance overhead and scalability issues. In this work, an in-SRAM digital multiplier is proposed to take the best of both worlds, i.e. performing GEMM in memory but using only conventional SRAMs without the drawbacks of bit-serial computations. This allows the user to design systems with significant performance gains using existing techno
    
[^176]: FPGAs 上的符号回归用于快速机器学习推断

    Symbolic Regression on FPGAs for Fast Machine Learning Inference. (arXiv:2305.04099v1 [cs.LG])

    [http://arxiv.org/abs/2305.04099](http://arxiv.org/abs/2305.04099)

    本论文提出了一种全新的利用符号回归的全流程，可在FPGA上进行机器学习推断，具有优化性能-资源平衡的特点。

    

    高能物理界正在研究在可编程门阵列（FPGAs）上部署基于机器学习的解决方案的可行性，以改善物理灵敏度并满足数据处理时延限制。本文引入了一种利用符号回归（SR）机器学习技术的全新端到端流程。它在方程空间中搜索近似表示数据集的代数关系。我们使用 PySR（基于进化算法发现这些表达式的软件）并扩展了 hls4ml 的功能（一种用于支持FPGAs中的机器学习推断的软件包），以支持在资源受限制的生产环境中使用 PySR 生成的表达式。深度学习模型通常通过固定网络大小来优化顶级指标，因为巨大的超参数空间会防止广泛的神经结构搜索。相反，SR 选择位于帕累托前沿的一组模型，这允许优化性能-资源的平衡。

    The high-energy physics community is investigating the feasibility of deploying machine-learning-based solutions on Field-Programmable Gate Arrays (FPGAs) to improve physics sensitivity while meeting data processing latency limitations. In this contribution, we introduce a novel end-to-end procedure that utilizes a machine learning technique called symbolic regression (SR). It searches equation space to discover algebraic relations approximating a dataset. We use PySR (software for uncovering these expressions based on evolutionary algorithm) and extend the functionality of hls4ml (a package for machine learning inference in FPGAs) to support PySR-generated expressions for resource-constrained production environments. Deep learning models often optimise the top metric by pinning the network size because vast hyperparameter space prevents extensive neural architecture search. Conversely, SR selects a set of models on the Pareto front, which allows for optimising the performance-resource
    
[^177]: 通过因果世界模型实现可解释强化学习

    Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])

    [http://arxiv.org/abs/2305.02749](http://arxiv.org/abs/2305.02749)

    本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。

    

    给强化学习提供解释是一项挑战，因为行动可能对未来产生长期影响。本文提出了一种新的可解释强化学习框架：通过学习一个因果世界模型而不预先知道环境的因果结构。该模型捕捉到动作的影响，使我们能够通过因果链来解释行动的长期影响，从而揭示出行动是如何影响环境变量并最终导致奖励的。与大多数解释性模型的低准确性不同，我们的模型保持高准确性的同时提高了解释性，使其适用于基于模型的学习。因此，我们证明了我们的因果模型可以成为解释性和学习之间的桥梁。

    Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
    
[^178]: Blahut和Arimoto的主题变体

    Variations on a Theme by Blahut and Arimoto. (arXiv:2305.02650v1 [cs.IT])

    [http://arxiv.org/abs/2305.02650](http://arxiv.org/abs/2305.02650)

    本文提出了BA算法的一种新的修改，通过让乘数在每次迭代中通过一维求根来更新，这使得算法能够直接计算所需失真的RD函数，而无需像原始算法一样探索整个RD曲线。

    

    Blahut-Arimoto（BA）算法在计算速率失真（RD）函数方面起着基础性作用，该算法通过交替最小化带有固定乘数的Lagrangian具有理想的单调收敛属性。在本文中，我们提出了BA算法的新颖修改，使乘数每次迭代通过相对于单调单变量函数的一维求根步骤更新，这可以通过牛顿法有效实现。这允许以灵活和高效的方式更新乘数，克服了原始BA算法的一个主要缺点，其中乘数在整个迭代过程中都是固定的。因此，修改后的算法能够直接计算所需失真的RD函数，而不像原始BA算法一样探索整个RD曲线。理论分析表明，修改后的算法仍会收敛到RD函数。

    The Blahut-Arimoto (BA) algorithm has played a fundamental role in the numerical computation of rate-distortion (RD) functions. This algorithm possesses a desirable monotonic convergence property by alternatively minimizing its Lagrangian with a fixed multiplier. In this paper, we propose a novel modification of the BA algorithm, letting the multiplier be updated in each iteration via a one-dimensional root-finding step with respect to a monotonic univariate function, which can be efficiently implemented by Newton's method. This allows the multiplier to be updated in a flexible and efficient manner, overcoming a major drawback of the original BA algorithm wherein the multiplier is fixed throughout iterations. Consequently, the modified algorithm is capable of directly computing the RD function for a given target distortion, without exploring the entire RD curve as in the original BA algorithm. A theoretical analysis shows that the modified algorithm still converges to the RD function a
    
[^179]: 双曲线图像文本表示方法

    Hyperbolic Image-Text Representations. (arXiv:2304.09172v1 [cs.CV])

    [http://arxiv.org/abs/2304.09172](http://arxiv.org/abs/2304.09172)

    本文提出了一个使用双曲表示捕捉图像和文本层次结构的对比模型MERU，并证明其在多模态任务上与CLIP相当。

    

    视觉和语言概念自然而然地组织成一个层次结构，其中一个文本概念“狗”包含所有包含狗的图像。尽管直觉上这是正确的，但目前的大规模视觉和语言模型（如CLIP）并没有明确地捕捉到这种层次结构。我们提出了MERU，一个对图像和文本进行双曲表示的对比模型。双曲空间具有嵌入树状数据的合适几何属性，因此MERU可以更好地捕捉图像文本数据的底层层次结构。我们的结果表明，MERU学习到了一个高度可解释的表示空间，同时在图像分类和图像文本检索等多模态任务上与CLIP的性能相当。

    Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ``dog'' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text data. Our results show that MERU learns a highly interpretable representation space while being competitive with CLIP's performance on multi-modal tasks like image classification and image-text retrieval.
    
[^180]: CodeKGC：用于生成知识图谱构建的代码语言模型

    CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])

    [http://arxiv.org/abs/2304.09048](http://arxiv.org/abs/2304.09048)

    本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。

    

    目前的生成式知识图谱构建方法通常无法捕捉结构性知识，而只是将自然语言转化为序列化文本或规范语言。然而，对于像代码这样的结构化数据进行训练的大型生成式语言模型已经展现了在理解自然语言以进行结构性预测和推理任务方面的卓越能力。本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法。具体而言，在给定代码格式的自然语言输入的情况下，目标是生成可以表示为代码补全任务的三元组。我们开发了具有模式感知型提示的方法，可以有效利用知识图谱内的语义结构。由于代码本质上具有结构，如类和函数定义，因此它作为先验的语义结构知识模型非常有用。此外，我们采用了基于原理的生成方法来提高性能。原理提供了模型生成结果的可解释性。

    Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
    
[^181]: 一种使用确定性目标的黑匣子变分推断：更快，更精确，更黑。

    Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])

    [http://arxiv.org/abs/2304.05527](http://arxiv.org/abs/2304.05527)

    本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。

    

    自动微分变分推断（ADVI）提供了多种现代概率编程语言中快速易用的后验近似方法。然而它的随机优化器缺乏明确的收敛标准，并且需要调整参数。此外，ADVI继承了均值场变分贝叶斯（MFVB）的较差后验不确定性估计。我们引入了“确定性ADVI”（DADVI）来解决这些问题。DADVI用固定的蒙特卡罗近似替换了MFVB的不可解目标，这一技术在随机优化文献中被称为“样本平均近似”（SAA）。通过优化近似但确定的目标，DADVI可以使用现成的二阶优化，而且与标准均值场ADVI不同的是，可以适用于更准确的后验线性响应（LR）协方差估计。与现有的最坏情况理论相反，我们表明，在某些常见的统计问题类别上，DADVI和SAA可以表现得更好。

    Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
    
[^182]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^183]: 训练神经网络在固定维度上是NP难的

    Training Neural Networks is NP-Hard in Fixed Dimension. (arXiv:2303.17045v1 [cs.CC])

    [http://arxiv.org/abs/2303.17045](http://arxiv.org/abs/2303.17045)

    研究了训练具有ReLU和线性阈值激活函数的两层神经网络的固定维度下的NP难度。 回答了两个问题，证明了这两个问题在二维情况下是NP难的，此外在ReLU案例中证明了固定参数问题的参数化固定复杂度维数和ReLU数量的组合参数。

    

    我们研究了在输入数据维度和隐藏神经元数量方面对两层神经网络进行参数化复杂性的研究，考虑ReLU和线性阈值激活函数。尽管这些问题的计算复杂性近年来已经被多次研究，但仍有几个问题尚未解决。我们回答了Arora et al. [ICLR '18]和Khalife和Basu [IPCO '22]的问题，显示两个问题在二维情况下都是NP难的，这排除了任何常数维度的多项式时间算法。我们还回答了Froese等人[JAIR '22]的问题，证明了具有零培训误差的四个ReLU(或两个线性阈值神经元)的W [1]-hardness。最后，在ReLU案例中，我们展示了参数化固定复杂度维数和ReLU数量的组合参数，如果网络被假定为计算凸映射，则可用于固定参数问题。我们的结果几乎完全解决了这些参数的复杂性状况。

    We study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering ReLU and linear threshold activation functions. Albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. We answer questions by Arora et al. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are NP-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. We also answer a question by Froese et al. [JAIR '22] proving W[1]-hardness for four ReLUs (or two linear threshold neurons) with zero training error. Finally, in the ReLU case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of ReLUs if the network is assumed to compute a convex map. Our results settle the complexity status regarding these parameters almost completely.
    
[^184]: 统计学习中的调整Wasserstein分布鲁棒估计

    Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])

    [http://arxiv.org/abs/2303.15579](http://arxiv.org/abs/2303.15579)

    本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。

    

    我们在统计学习中提出了一种调整的Wasserstein分布鲁棒估计——基于Wasserstein分布鲁棒估计（WDRO）的非线性转换。这种转换将提高WDRO的统计性能，因为调整后的WDRO估计器渐进无偏并且均方误差趋近于零。调整后的WDRO不会削弱WDRO的样本外性能保证。我们提出了调整WDRO估计器的存在的充分条件，并给出了计算调整WDRO估计器的过程。具体而言，我们将展示如何在广义线性模型中开发调整WDRO估计器。数值实验表明，调整后的估计器比经典估计器具有更好的实际性能。

    We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
    
[^185]: 用神经随机建模探索集体运动的介观描述

    Discovering mesoscopic descriptions of collective movement with neural stochastic modelling. (arXiv:2303.09906v1 [cs.LG])

    [http://arxiv.org/abs/2303.09906](http://arxiv.org/abs/2303.09906)

    介观集体运动的随机特征对确定性和随机性动力学建模至关重要。作者利用物理启发、神经网络和随机微分方程来研究相互作用个体的群体动力学，并对其进行了鉴定和分析，为这些系统的秩序性质提供了新颖的见解。

    

    集体运动是自然界中普遍存在的现象，启发工程师、物理学家和数学家开发数学模型和生物启发设计。小至中等群体规模（约10-1000个个体，也称“介观尺度”的集体运动，由于随机性而显示出非平凡的特征。因此，在介观尺度集体现象的研究中，表征确定性和随机动力学方面的特征是至关重要的。我们基于物理启发，采用基于神经网络的方法来表征相互作用个体的随机群体动力学，通过控制群体的随机微分方程（SDE）来研究这些系统的群体动力学。我们在合成和真实数据集上应用这种技术，利用漂移和扩散场鉴定系统的确定性和随机性方面的动力学，从而使我们能够针对这些系统的秩序性质做出新颖的推断。

    Collective motion is an ubiquitous phenomenon in nature, inspiring engineers, physicists and mathematicians to develop mathematical models and bio-inspired designs. Collective motion at small to medium group sizes ($\sim$10-1000 individuals, also called the `mesoscale'), can show nontrivial features due to stochasticity. Therefore, characterizing both the deterministic and stochastic aspects of the dynamics is crucial in the study of mesoscale collective phenomena. Here, we use a physics-inspired, neural-network based approach to characterize the stochastic group dynamics of interacting individuals, through a stochastic differential equation (SDE) that governs the collective dynamics of the group. We apply this technique on both synthetic and real-world datasets, and identify the deterministic and stochastic aspects of the dynamics using drift and diffusion fields, enabling us to make novel inferences about the nature of order in these systems.
    
[^186]: ESD:预期平方差作为一种无需调参的可训练校准度量

    ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02472](http://arxiv.org/abs/2303.02472)

    ESD是一种无需调参的可训练校准目标损失，通过将校准误差看作两个期望值之间的平方差，可以改善神经网络模型的校准度。

    

    研究表明，现代神经网络由于过于自信的预测而往往校准不良。传统上，在训练之后使用后处理方法来校准模型。近年来，已经提出了各种可训练的校准度量来直接将其纳入训练过程中。然而，这些方法都包含内部超参数，并且这些校准目标的性能依赖于调整这些超参数，随着神经网络和数据集的规模增大，会产生更多的计算成本。因此，我们提出了预期平方差（ESD），一种无需调参的可训练校准目标损失，我们从两个期望值之间的平方差的角度来看校准误差。通过对几种架构（CNN、Transformer）和数据集的大量实验证明，将ESD纳入训练可以改善模型的校准度。

    Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model cali
    
[^187]: 打开树集成的可解释性：一种层次化可视化工具和多变量最优重建树

    Unboxing Tree Ensembles for interpretability: a hierarchical visualization tool and a multivariate optimal re-built tree. (arXiv:2302.07580v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.07580](http://arxiv.org/abs/2302.07580)

    本论文开发了一种层次化可视化工具和多变量最优重建树，用于提高树集成模型的可解释性和洞察力。

    

    由于算法决策对现实世界应用的增长影响，模型的可解释性已成为机器学习中的关键问题。树集成方法（如随机森林或XgBoost）是用于分类任务的强大学习工具。然而，虽然将多个树组合起来可能会提供比单个树更高的预测质量，但它牺牲了可解释性的特性，导致了“黑盒”模型。基于此，我们的目标是开发一种可解释的树集成模型表示，可以为其行为提供有价值的见解。首先，针对目标树集成模型，我们开发了一种基于热图表示的层次化可视化工具，考虑到特征的频率和选择特征的层级作为重要性的指标。接下来，我们提出了一个混合整数线性规划（MILP）的形式，用于构造一个准确模拟目标模型的单个最优多变量树。

    The interpretability of models has become a crucial issue in Machine Learning because of algorithmic decisions' growing impact on real-world applications. Tree ensemble methods, such as Random Forests or XgBoost, are powerful learning tools for classification tasks. However, while combining multiple trees may provide higher prediction quality than a single one, it sacrifices the interpretability property resulting in "black-box" models. In light of this, we aim to develop an interpretable representation of a tree-ensemble model that can provide valuable insights into its behavior. First, given a target tree-ensemble model, we develop a hierarchical visualization tool based on a heatmap representation of the forest's feature use, considering the frequency of a feature and the level at which it is selected as an indicator of importance. Next, we propose a mixed-integer linear programming (MILP) formulation for constructing a single optimal multivariate tree that accurately mimics the tar
    
[^188]: 使用扩散模型的语义引导生成图像增强方法用于图像分类

    Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification. (arXiv:2302.02070v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02070](http://arxiv.org/abs/2302.02070)

    SGID是一种使用扩散模型的语义引导生成图像增强方法，旨在在图像分类中平衡图像多样性和语义一致性。实验证明，SGID在ResNet-5上的效果优于最佳增强基线1.72％。

    

    现有的图像增强方法分为两类：扰动方法和生成方法。扰动方法对原始图像应用预定义的扰动来增强图像，但只是局部变化图像，因此缺乏图像多样性。相反，生成方法在增强图像中带来更多的图像多样性，但可能无法保持语义一致性，从而错误地改变了原始图像的基本语义。为了在增强图像中平衡图像多样性和语义一致性，我们提出了SGID，一种语义引导生成图像增强方法，使用扩散模型用于图像分类。具体而言，SGID采用扩散模型生成具有良好图像多样性的增强图像。更重要的是，SGID以图像标签和标题作为指导，以保持增强和原始图像之间的语义一致性。实验证明，SGID在ResNet-5上的表现优于最佳增强基线1.72％。

    Existing image augmentation methods consist of two categories: perturbation-based methods and generative methods. Perturbation-based methods apply pre-defined perturbations to augment an original image, but only locally vary the image, thus lacking image diversity. In contrast, generative methods bring more image diversity in the augmented images but may not preserve semantic consistency, thus incorrectly changing the essential semantics of the original image. To balance image diversity and semantic consistency in augmented images, we propose SGID, a Semantic-guided Generative Image augmentation method with Diffusion models for image classification. Specifically, SGID employs diffusion models to generate augmented images with good image diversity. More importantly, SGID takes image labels and captions as guidance to maintain semantic consistency between the augmented and original images. Experimental results show that SGID outperforms the best augmentation baseline by 1.72% on ResNet-5
    
[^189]: 多功能能量概率模型在高能物理中的应用

    Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00695](http://arxiv.org/abs/2302.00695)

    本文提出了一个多功能的能量概率模型，用于描述高能物理事件，可用于参数化的事件生成，异常信号探测以及粒子识别。

    

    作为一种经典的生成建模方法，基于能量的模型具有能量函数形式灵活性的天然优势。最近，基于能量的模型在计算机视觉和自然语言处理中建模高维数据方面取得了巨大成功。与这些进展一致，我们建立了一个多功能能量概率模型，用于描述来自大型强子对撞机的高能物理事件。该框架基于一个强大的生成模型，并描述了更高阶的粒子间相互作用，适用于不同的编码体系结构和隐式生成。在应用方面，它可以作为强大的参数化事件生成器用于物理仿真，一种泛用的无假设关联的异常信号探测器，以及用于粒子识别的增强事件分类器。

    As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
    
[^190]: 增加偏差可以比增加权重更有效

    Increasing biases can be more efficient than increasing weights. (arXiv:2301.00924v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2301.00924](http://arxiv.org/abs/2301.00924)

    通过增加偏差而不是权重，可以显著提高神经网络模型的性能，并提供了优化神经网络信息流的替代视角。

    

    我们引入了一种新颖的神经网络计算单元，具有多个偏差，挑战了传统的感知器结构。这个单元注重保留未经损坏的信息，将其从一个单元传递给下一个单元，在过程的后期应用专门为每个单元设计的偏差的激活函数。通过经验和理论分析，我们证明了在神经网络模型的性能中，通过关注增加偏差而不是权重，存在显著提升的潜力。这种方法提供了一种优化神经网络信息流的替代视角。

    We introduce a novel computational unit for neural networks that features multiple biases, challenging the traditional perceptron structure. This unit emphasizes the importance of preserving uncorrupted information as it is passed from one unit to the next, applying activation functions later in the process with specialized biases for each unit. Through both empirical and theoretical analyses, we show that by focusing on increasing biases rather than weights, there is potential for significant enhancement in a neural network model's performance. This approach offers an alternative perspective on optimizing information flow within neural networks. See source code at https://github.com/CuriosAI/dac-dev.
    
[^191]: 使用隔离分布核检测变化间隔

    Detecting Change Intervals with Isolation Distributional Kernel. (arXiv:2212.14630v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14630](http://arxiv.org/abs/2212.14630)

    这篇论文提出了一种基于隔离分布核的CID方法，用于在数据流中高效识别各种类型的变点并容忍异常值。

    

    在流数据分析中，检测数据分布的突变是最重要的任务之一。尽管最近提出了许多无监督的变点检测（CPD）方法来识别这些变化，但它们仍然存在对微小变化的遗漏、可伸缩性差或/和对异常值的敏感性等问题。为了应对这些挑战，我们首次将变点检测问题广义为变间隔检测（CID）问题的特殊情况。然后，我们基于最新的隔离分布核（IDK）提出了一种CID方法，名为iCID。如果两个非均匀时间相邻间隔之间存在高不相似度得分，iCID将识别变化间隔。IDK的数据依赖性和有限特征映射使iCID能够有效识别具有异常值容忍度的数据流中的各种类型的变点。此外，iCID的提出了在线和离线版本，具有优化关键参数设置的能力。

    Detecting abrupt changes in data distribution is one of the most significant tasks in streaming data analysis. Although many unsupervised Change-Point Detection (CPD) methods have been proposed recently to identify those changes, they still suffer from missing subtle changes, poor scalability, or/and sensitivity to outliers. To meet these challenges, we are the first to generalise the CPD problem as a special case of the Change-Interval Detection (CID) problem. Then we propose a CID method, named iCID, based on a recent Isolation Distributional Kernel (IDK). iCID identifies the change interval if there is a high dissimilarity score between two non-homogeneous temporal adjacent intervals. The data-dependent property and finite feature map of IDK enabled iCID to efficiently identify various types of change-points in data streams with the tolerance of outliers. Moreover, the proposed online and offline versions of iCID have the ability to optimise key parameter settings. The effectiveness
    
[^192]: 一种用于不平衡半监督学习的令人尴尬简单基准

    An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11086](http://arxiv.org/abs/2211.11086)

    本文研究了一种名为SimiS的简单但被忽视的基准方法，通过将伪标签作为标签数据的补充，根据与最频繁类别的类别分布差异，有效地减少了不平衡半监督学习中的类别不平衡，相对于现有方法有显著的性能提升。

    

    半监督学习（SSL）在利用无标签数据改善模型性能方面表现出巨大潜力。然而，标准的SSL假设数据分布均匀，我们考虑了一个更加真实和具有挑战性的情境，即不平衡半监督学习（imbalanced SSL），其中标签数据和无标签数据都出现了不平衡的类别分布。尽管已有努力解决这一挑战的方法，但当遇到严重不平衡时，它们的性能会退化，因为它们无法对类别不平衡进行足够和有效的减少。在本文中，我们研究了一个简单但被忽视的基准方法--SimiS，通过简单地根据与最频繁类别的类别分布差异，将伪标签作为标签数据的补充。这样一个简单的基准方法在减少类别不平衡方面非常有效。它在CIFAR100-LT，FOOD101-LT和ImageNet127上相对于先前最佳方法的性能提升显著，分别提升了12.8％，13.6％和16.7％。

    Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respecti
    
[^193]: 语言控制扩散：通过空间、时间和任务高效扩展

    Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15629](http://arxiv.org/abs/2210.15629)

    本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。

    

    训练通用型智能体在各个方面都很困难，需要处理高维输入（空间）、长时间跨度（时间）和多个新任务。最近的结构方面的进展使得我们可以沿着其中一个或两个维度提高扩展性能力，但计算成本仍然很高。本文提出使用语言控制扩散模型作为一种基于自然语言条件的分层规划器（LCD）来应对这三个方面。我们有效而高效地扩展扩散模型，以应对时间、状态和任务空间维度的长时间跨度控制问题。我们在CALVIN语言机器人基准测试中将LCD与其他最先进的模型进行比较，发现LCD在多任务成功率方面优于其他最先进的方法，而单任务成功率（SR）为88.7%，远高于以前的最佳成绩82.6%，大大提高了计算效率。

    Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
    
[^194]: 连续控制的正常引导分布强化学习

    Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13125](http://arxiv.org/abs/2208.13125)

    本论文研究了连续控制任务中的值分布，并发现学习的值分布与正态分布非常接近。基于这一观察，提出了一种正态引导的分布式强化学习方法，利用方差网络预测的方差和回报，以及与标准值函数不同的值分布结构特征来更新策略。这种方法在两种在线算法上产生了显著效果。

    

    在许多强化学习算法中，学习一个预测回报的均值模型，或价值函数，起着关键作用。分布式强化学习(DRL)通过建模值分布而不仅仅是均值来提高性能。我们研究了几个连续控制任务中的值分布，并发现学习的值分布与正态分布非常接近。我们设计了一种利用这个性质的方法，利用从方差网络预测的方差，以及回报，来分析计算代表我们分布式值函数的正态分布的目标分位栏。此外，我们提出了一种基于值分布的结构特征的正确性来衡量的策略更新方法，这些特征在标准的值函数中不存在。我们概述的方法与许多DRL结构兼容。我们使用两种代表性的在线算法，PPO和TRPO，作为测试平台。我们的方法在统计上产生了显著的效果。

    Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirical quite close to normal. We design a method that exploits this property, employ variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically
    
[^195]: 气候不变的机器学习

    Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08440](http://arxiv.org/abs/2112.08440)

    本研究提出了一种新的框架——"气候不变"的机器学习，通过将气候过程的知识纳入机器学习算法中，可以在广泛的气候和地理条件下保持高准确性。

    

    气候变化预测是一个泛化问题：我们使用物理模型在过去、现在和未来的气候中对最近的过去进行外推。目前的气候模型需要对小于模型网格大小的尺度上发生的过程进行表示，这些过程是模型预测不确定性的主要来源。最近的机器学习（ML）算法有望改善这种过程表示，但往往在未经训练的气候环境中外推效果不佳。为了充分发挥物理和统计方法的优势，我们提出了一个新框架——称为"气候不变"的机器学习——将气候过程的知识纳入ML算法中，并证明它可以在三个不同的大气模型中在广泛的气候和地理条件下保持高准确性。我们的结果表明，将物理知识明确纳入数据驱动的地球系统过程模型中可以提高它们的一致性、数据效率和泛化能力。

    Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
    
[^196]: Bayesian层次回归模型的近似交叉验证均值估计

    Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models. (arXiv:2011.14238v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2011.14238](http://arxiv.org/abs/2011.14238)

    我们提出了一种用于贝叶斯层次回归模型的近似交叉验证均值估计的新方法，通过在方差-协方差参数上进行条件，将交叉验证问题转化为简单的优化问题，从而提高了大型BHRMs的可行性。

    

    我们引入了一种新的方法，用于获取贝叶斯层次回归模型(BHRMs)的交叉验证预测估计。贝叶斯层次模型以其能够建模复杂的依赖结构并提供概率不确定性估计而受到欢迎，但运行的计算开销很大。因此，交叉验证(CV)不是评估BHRMs预测性能的常见实践。我们的方法避免了为每个交叉验证折叠重新运行计算开销昂贵的估计方法的需要，使CV在大型BHRMs中更可行。通过在方差-协方差参数上进行条件，将CV问题从基于概率的抽样转化为简单熟悉的优化问题。在许多情况下，这产生的估计与完整的CV等效。我们提供理论结果，并在公开可用的数据和模拟中证明其有效性。

    We introduce a novel procedure for obtaining cross-validated predictive estimates for Bayesian hierarchical regression models (BHRMs). Bayesian hierarchical models are popular for their ability to model complex dependence structures and provide probabilistic uncertainty estimates, but can be computationally expensive to run. Cross-validation (CV) is therefore not a common practice to evaluate the predictive performance of BHRMs. Our method circumvents the need to re-run computationally costly estimation methods for each cross-validation fold and makes CV more feasible for large BHRMs. By conditioning on the variance-covariance parameters, we shift the CV problem from probability-based sampling to a simple and familiar optimization problem. In many cases, this produces estimates which are equivalent to full CV. We provide theoretical results and demonstrate its efficacy on publicly available data and in simulations.
    

