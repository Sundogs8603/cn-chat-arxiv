# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Compositional Generative Modeling: A Single Model is Not All You Need](https://rss.arxiv.org/abs/2402.01103) | 本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。 |
| [^2] | [The Price of Adaptivity in Stochastic Convex Optimization](https://arxiv.org/abs/2402.10898) | 该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。 |
| [^3] | [Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning](https://arxiv.org/abs/2402.10894) | 该研究提出了一种深度融合学习网络，利用扩散加权MRI模式和结构化健康概况数据联合预测急性中风后的功能结果，通过监督对比学习来学习区分特性。 |
| [^4] | [RLVF: Learning from Verbal Feedback without Overgeneralization](https://arxiv.org/abs/2402.10893) | 研究了如何在大型语言模型中利用口头反馈进行定制化调整而不发生过度泛化，并提出了一种新的方法C3PO。 |
| [^5] | [Proving membership in LLM pretraining data via data watermarks](https://arxiv.org/abs/2402.10892) | 使用数据水印在LLM预训练中检测版权持有人作品的方法，可以进行合理检测且提供误检率保证，研究了水印设计对假设检验能力的影响以及在模型和数据集缩放下的检测强度变化。 |
| [^6] | [Instruction Diversity Drives Generalization To Unseen Tasks](https://arxiv.org/abs/2402.10891) | 指导调整通过增加指令集的多样性来推动模型对未见任务的泛化。 |
| [^7] | [When is Tree Search Useful for LLM Planning? It Depends on the Discriminator](https://arxiv.org/abs/2402.10890) | 当前研究通过实验分析了大型语言模型在多步问题求解中使用树搜索的可行性，指出高级规划方法需要鉴别器至少90%准确性才能显著提高性能。 |
| [^8] | [Explainability for Machine Learning Models: From Data Adaptability to User Perception](https://arxiv.org/abs/2402.10888) | 本文旨在为已部署的机器学习模型生成局部解释并确保这些解释对用户具有可理解性，主要创新在于开发具有数据适应性和用户感知要求站点解释方法。 |
| [^9] | [3D Diffuser Actor: Policy Diffusion with 3D Scene Representations](https://arxiv.org/abs/2402.10885) | 通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。 |
| [^10] | [Multi-modal preference alignment remedies regression of visual instruction tuning on language model](https://arxiv.org/abs/2402.10884) | 通过收集轻量级VQA偏好数据集并使用Direct Preference Optimization，我们能够在语言模型的指导能力上取得显著提升，在小规模数据下比其他方法实现了更高的分数。 |
| [^11] | [Robust agents learn causal world models](https://arxiv.org/abs/2402.10877) | 智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。 |
| [^12] | [Design of 2D Skyrmionic Metamaterial Through Controlled Assembly](https://arxiv.org/abs/2402.10874) | 通过"模拟受控组装"方法，成功设计出多种稳定的2D Skyrm离子超材料。 |
| [^13] | [Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice](https://arxiv.org/abs/2402.10870) | 本文分享了关于在工业环境中使用AED系统面临的挑战，并提供了在这种环境中适当的目标和系统规格的视角，最终开发了一个基于反事实推断的AED框架并在商业环境中进行了测试。 |
| [^14] | [Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection](https://arxiv.org/abs/2402.10862) | 该论文介绍了一种差分私有联邦迁移学习框架，结合差分隐私和迁移学习，以提升心理健康监测中的数据隐私性和数据充足性。 |
| [^15] | [JetTrain: IDE-Native Machine Learning Experiments](https://arxiv.org/abs/2402.10857) | JetTrain是一个将特定任务从IDE委派给远程计算资源的IDE工具，可以提高机器学习实验的效率。 |
| [^16] | [HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images](https://arxiv.org/abs/2402.10851) | 本文提出了一种新的组织病理图像分析方法，使用基于胶囊网络的弱监督语义分割，首次在这方面的应用，实验结果展示了胶囊网络在增强组织病理图像分析精度和效率方面的潜力。 |
| [^17] | [FedD2S: Personalized Data-Free Federated Knowledge Distillation](https://arxiv.org/abs/2402.10846) | 提出了FedD2S方法，通过深到浅的层丢弃机制，在无数据联邦知识蒸馏中增强了本地模型的个性化，表现出卓越性能和改善客户间公平性。 |
| [^18] | [GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers](https://arxiv.org/abs/2402.10831) | 本文提出了一种基于生成对抗网络的深度学习方法，用于从多频散射电场的振幅准确高效地重建随机形状的二维介质物体，通过逆神经网络框架解决了非唯一性挑战。 |
| [^19] | [Goal-Conditioned Offline Reinforcement Learning via Metric Learning](https://arxiv.org/abs/2402.10820) | 通过度量学习的目标条件离线强化学习提出了一种新的方法来处理稀疏奖励、对称和确定性动作下的最优值函数近似，并展示了在学习次优离线数据集方面的显着优越性。 |
| [^20] | [Trading off Consistency and Dimensionality of Convex Surrogates for the Mode](https://arxiv.org/abs/2402.10818) | 通过在低维替代空间中的凸多面体顶点上嵌入结果，并探究单纯形中的一致性区域，权衡了替代损失维度、问题实例数量。 |
| [^21] | [TernaryVote: Differentially Private, Communication Efficient, and Byzantine Resilient Distributed Optimization on Heterogeneous Data](https://arxiv.org/abs/2402.10816) | TernaryVote提出了一种结合了三值压缩器和大多数投票机制的算法，实现了差分隐私、梯度压缩和拜占庭容忍，相较于已有的方法有更好的隐私保证和收敛性。 |
| [^22] | [Associative Memories in the Feature Space](https://arxiv.org/abs/2402.10814) | 在特征空间中计算相似性以提升联想存储器模型的性能，并通过使用对比损失预训练网络来生成嵌入向量，从而提高检索速度。 |
| [^23] | [Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2402.10810) | 本文提出了一种基于模型的算法，Variational Primal-Dual Policy Optimization (VPDPO)，通过实现Lagrangian和Fenchel对偶来将原始受限问题重构为无约束的原始-对偶优化，并且采用乐观原则更新原始变量和梯度上升更新对偶变量。 |
| [^24] | [TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models](https://arxiv.org/abs/2402.10802) | 时间序列异常检测模型的工业级基准TimeSeriesBench填补了当前算法在训练范式、在线检测范式和评估标准方面与实际需求之间的差距。 |
| [^25] | [BlackJAX: Composable Bayesian inference in JAX](https://arxiv.org/abs/2402.10797) | BlackJAX是一个实现在JAX中组合式贝叶斯推断的库，采用函数式方法提高易用性、速度和模块化，适用于需要尖端方法、研究人员和想要了解工作原理的人。 |
| [^26] | [Diversified Ensembling: An Experiment in Crowdsourced Machine Learning](https://arxiv.org/abs/2402.10795) | 该论文提出了一种多样化集成的方法，允许参与者协作解决更广泛的问题，并能处理子组不公平性。 |
| [^27] | [Masked Attention is All You Need for Graphs](https://arxiv.org/abs/2402.10793) | 提出了一种在图上学习的简单替代方法，称为掩码注意力（MAG），其利用注意力矩阵来创建定制的注意力模式，在长距离任务上表现出色并胜过其他方法。 |
| [^28] | [In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss](https://arxiv.org/abs/2402.10790) | 通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理长达 1000 万个元素的任务，这是迄今为止处理最长输入的开放神经网络模型，并展示了对长序列处理能力的显著改进。 |
| [^29] | [EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge](https://arxiv.org/abs/2402.10787) | 本文提出了EdgeQAT，使用熵和分布引导的量化感知训练方法来优化轻量级LLMs，在边缘设备上实现推理加速。 |
| [^30] | [Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants](https://arxiv.org/abs/2402.10774) | 本文研究了一种现代形式的错误反馈EF21，将其依赖的通信复杂度从平方平均值改进为更小的算术平均值，在实践中表现良好。 |
| [^31] | [Policy Learning for Off-Dynamics RL with Deficient Support](https://arxiv.org/abs/2402.10765) | 该论文主要研究在离线动力学强化学习中如何应对源环境和目标环境之间的动态差异挑战。 |
| [^32] | [RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction](https://arxiv.org/abs/2402.10760) | 提出了一种新型模型RAGIC，引入序列生成用于股票区间预测，利用生成对抗网络生成未来价格序列，通过风险模块和时间模块创建风险敏感区间。 |
| [^33] | [Stochastic Localization via Iterative Posterior Sampling](https://arxiv.org/abs/2402.10758) | 本论文提出了一种名为SLIPS的方法，通过迭代后验抽样实现随机定位，填补了从非标准化目标密度中抽样的问题的空白。 |
| [^34] | [Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering](https://arxiv.org/abs/2402.10756) | 提出了一种对比公平正则化的个体公平非负矩阵三因子分解模型，能够实现平衡和凝聚的簇，并允许用户在精度和公平度之间进行权衡。 |
| [^35] | [When Dataflow Analysis Meets Large Language Models](https://arxiv.org/abs/2402.10754) | 这个研究提出了一个由大型语言模型驱动的数据流分析框架，可以分析任意代码片段，无需编译基础设施，并自动合成下游应用，有效解决数据流相关漏洞检测问题 |
| [^36] | [A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers](https://arxiv.org/abs/2402.10748) | 提出了一种微型Transformer模型，用于在低功耗微控制器上对心电图信号进行分析，仅需6k个参数，准确率达到98.97%，适用于识别MIT-BIH心律失常数据库中的5个最常见心律失常类别 |
| [^37] | [Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting](https://arxiv.org/abs/2402.10747) | 提出了一种完全可微的拉格朗日卷积神经网络模型，实现了物理信息与数据驱动学习相结合，在降水预报中表现优秀，为其他拉格朗日机器学习模型提供了新思路。 |
| [^38] | [Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules](https://arxiv.org/abs/2402.10727) | 通过引入风险分解和适当评分规则，我们提出了一个通用框架来量化预测不确定性的不同来源，并澄清了它们之间的关系。 |
| [^39] | [Machine Learning based Prediction of Ditching Loads](https://arxiv.org/abs/2402.10724) | 该研究提出了使用机器学习来预测飞机机身动态水降载荷的方法，通过结合卷积自编码器和长短期记忆网络或Koopman算子方法，取得了令人满意的预测结果。 |
| [^40] | [Conformalized Credal Set Predictors](https://arxiv.org/abs/2402.10723) | 本论文提出了一种利用一致预测方法来学习可信集合预测器的方法，能有效表示预测中的不确定性。 |
| [^41] | [Unlink to Unlearn: Simplifying Edge Unlearning in GNNs](https://arxiv.org/abs/2402.10695) | 研究揭示了GNN中边解除过程的关键问题，即过度遗忘现象，提出了解决方法来解决损失函数引起的问题。 |
| [^42] | [Exploring Precision and Recall to assess the quality and diversity of LLMs](https://arxiv.org/abs/2402.10693) | 该研究提出了一种新的评估框架，将精度和召回率指标从图像生成转化为文本生成，细致评估了LLMs生成文本的质量和多样性，揭示了当前LLMs在生成任务中性能表现的重要见解。 |
| [^43] | [Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective](https://arxiv.org/abs/2402.10686) | 通过信息论框架分析了最先进的似然比攻击对不确定性、校准水平和数据集大小的影响，研究了成员推理攻击中隐含的风险 |
| [^44] | [Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes](https://arxiv.org/abs/2402.10681) | 提出了物理相关的MeshGraphNets（PI-MGNs），可以在任意网格上进行非定态和非线性仿真，利用PINNs来减少对大量昂贵训练数据的依赖 |
| [^45] | [Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model](https://arxiv.org/abs/2402.10677) | 研究了嵌套矩阵张量模型中多视图聚类的性能差距，量化了张量方法和矩阵方法之间的性能差距，并发现了展开方法的算法阈值，展示了类似BBP过渡行为。 |
| [^46] | [Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift](https://arxiv.org/abs/2402.10665) | 本文研究了在低资源环境中语义分割的选择性预测，提出了一种针对语义分割量身定制的新型图像级置信度测量，并通过实验证明了其有效性 |
| [^47] | [Linear Transformers with Learnable Kernel Functions are Better In-Context Models](https://arxiv.org/abs/2402.10644) | 本文介绍了一种将线性变换器与受指数函数的Taylor展开启发的核函数和卷积网络相结合的模型，通过优化其In-Context Learning能力，取得了较好的效果。 |
| [^48] | [A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface](https://arxiv.org/abs/2402.10641) | 调查了模型降阶和深度学习技术在预测冲击射流对凹凸面传热中的有效性，并引入了FFT-ANN和POD-LSTM两种方法来预测平均努塞数以及随机频率冲击情况下的局部传热速率。 |
| [^49] | [ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling](https://arxiv.org/abs/2402.10635) | 将vanilla Transformer的关系建模扩展到连续时间领域，提出了ContiFormer。 |
| [^50] | [Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling](https://arxiv.org/abs/2402.10634) | 通过 hierarchical spatiotemporal downsampling 处理缺失数据问题，结合可解释的注意机制，实现对时空预测的有效建模 |
| [^51] | [Multitask Kernel-based Learning with Logic Constraints](https://arxiv.org/abs/2402.10617) | 将逻辑约束融合到多任务核心学习中，提出了一种通用方法来转换逻辑陈述为连续实现，以实现核心谓词计算输出。 |
| [^52] | [Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements](https://arxiv.org/abs/2402.10614) | 本文通过辩论调节LLMs，使其生成可控的支持用户定义论点的声明，改进了LLMs的可控性，并提出了DEBATunE流程。通过两个LLMs之间的多轮辩论生成高质量的训练数据，以支持生成有更高质量和更突出的声明。 |
| [^53] | [U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model](https://arxiv.org/abs/2402.10609) | U$^2$MRPD是一个新颖的框架，通过大型潜在扩散模型引导，实现了无监督的欠采样MRI重建，能够支持图像特定的MRI重建，且在多个数据集上表现出与监督和MRI扩散方法相媲美甚至更好的性能。 |
| [^54] | [Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification](https://arxiv.org/abs/2402.10592) | 提出了一种统一模型，同时考虑了实验内部性能和实验后结果，在优化大规模人群中的表现方面提供了尖锐理论，揭示了新颖的见解 |
| [^55] | [Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation](https://arxiv.org/abs/2402.10580) | 本研究将不同的不确定性量化方法与联合语义分割和单目深度估计相结合，介绍了一种新的学生-教师蒸馏方法 EMUFormer，揭示了多任务学习对不确定性质量的益处。 |
| [^56] | [Symbolic Autoencoding for Self-Supervised Sequence Learning](https://arxiv.org/abs/2402.10575) | 符号自编码（$\Sigma$AE）是一个自监督框架，通过最小化重构损失和平行数据的监督损失来优化连接两个生成模型，实现了在转导任务上显著提升性能。 |
| [^57] | [Direct Preference Optimization with an Offset](https://arxiv.org/abs/2402.10571) | 提出了一种新颖的直接偏好优化方法，即具有偏置的DPO（ODPO），在微调过程中不同对待每个偏好对。 |
| [^58] | [A novel integrated industrial approach with cobots in the age of industry 4.0 through conversational interaction and computer vision](https://arxiv.org/abs/2402.10553) | 论文提出一种创新愿景，认为在工业4.0时代，协作机器人（Cobot）、人工智能和人类之间将会有更紧密的协作，有助于提高工业自动化的效率和决策制定。 |
| [^59] | [Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information](https://arxiv.org/abs/2402.10551) | 使用基于变压器的新方法，通过明确建模变异列表的顺序结构并利用辅助信息，实现了超越最先进DRP模型的性能表现 |
| [^60] | [Learning Disentangled Audio Representations through Controlled Synthesis](https://arxiv.org/abs/2402.10547) | 通过引入SynTone合成数据集，解决了音频表示学习中基准数据稀缺的问题，并提出了用于评估解耦技术的新方法。 |
| [^61] | [Properties and Challenges of LLM-Generated Explanations](https://arxiv.org/abs/2402.10532) | 该研究探讨了大型语言模型生成的解释在多领域指导微调数据集上的特性，发现生成的解释表现出选择性和包含说明性元素，但较少是主观或误导性的。 |
| [^62] | [LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models](https://arxiv.org/abs/2402.10524) | LLM Comparator是一种用于交互式分析自动并行评估结果的新型可视化工具，支持用户理解模型表现优劣和不同之处，解决了大型语言模型评估中的可扩展性和可解释性挑战。 |
| [^63] | [Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs](https://arxiv.org/abs/2402.10517) | 任意精度LLM引入了一种轻量级方法，通过将不同大小LLMs量化为不同位宽（如3、4、...，n位）并叠加到内存中，显着降低了部署多个不同大小LLMs的高成本 |
| [^64] | [Generative AI for Controllable Protein Sequence Design: A Survey](https://arxiv.org/abs/2402.10516) | 人工智能领域的进步推动了蛋白质设计领域朝着前所未有的革命方向发展，这篇论文系统地审查了用于可控蛋白质序列设计的生成AI的最新进展。 |
| [^65] | [Can Transformers Predict Vibrations?](https://arxiv.org/abs/2402.10511) | 提出了一种新颖的基于Transformer的模型Resoformer，用于预测电动汽车传动轴上的扭振，解决了当前阻尼技术只能在共振发生后检测到的问题 |
| [^66] | [Resilience of the quadratic Littlewood-Offord problem](https://arxiv.org/abs/2402.10504) | 论文研究了二次Littlewood-Offord问题的统计鲁棒性，估计了对抗性噪声对二次Radamecher混沌的影响，并提供了对二次和双线性Rademacher混沌的统计鲁棒性的下限估计。 |
| [^67] | [Late-time transition of $M_B$ inferred via neural networks](https://arxiv.org/abs/2402.10502) | 本研究通过神经网络对绝对星等$M_B$进行约束，并发现在$z\approx 1$区域存在转折红移迹象 |
| [^68] | [Provably Sample Efficient RLHF via Active Preference Optimization](https://arxiv.org/abs/2402.10500) | 通过Active Preference Optimization算法，在Bradley-Terry-Luce偏好模型下实现了RLHF的样本效率提高，优化了对提示收集偏好数据的策略。 |
| [^69] | [Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)](https://arxiv.org/abs/2402.10492) | 通过比较三种不同的人工神经网络方法，研究发现通用回归神经网络（GRNN）在预测小麦枯叶病严重程度方面表现出有效性，并需要较少的训练时间。 |
| [^70] | [Random Projection Layers for Multidimensional Time Sires Forecasting](https://arxiv.org/abs/2402.10487) | 提出了一种全MLP时间序列预测架构RPMixer，通过将随机投影层集成到模型中，增加了块输出之间的多样性，提高了整体性能 |
| [^71] | [Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise](https://arxiv.org/abs/2402.10482) | 自蒸馏在多类别分类中扮演着标签平均化的角色，有助于模型关注与特定实例相关的特征簇以预测标签，但随着蒸馏轮次增加，性能会降低。此外，在标签噪声情景下自蒸馏被证明是有效的，找到了实现100%分类准确率所需的最小蒸馏轮次。 |
| [^72] | [Emoji Driven Crypto Assets Market Reactions](https://arxiv.org/abs/2402.10481) | 该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。 |
| [^73] | [CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes](https://arxiv.org/abs/2402.10478) | 提出了CodaMal框架，实现了低成本显微镜下疟疾检测的对比域自适应，解决了HCM和LCM图像之间的域差异问题 |
| [^74] | [Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection](https://arxiv.org/abs/2402.10477) | 通过实验证明较简单的图像在正态流模型中得到更高可能性，揭示了对该现象的潜在机制，并提出了图像复杂度作为独立变量来解决这一问题。 |
| [^75] | [Fundamental Benefit of Alternating Updates in Minimax Optimization](https://arxiv.org/abs/2402.10475) | Alt-GDA算法被证明更快，提出了交替外推GDA（Alex-GDA）通用算法框架，以轮流从迭代的外推中获取梯度。 |
| [^76] | [One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression](https://arxiv.org/abs/2402.10474) | 通过正则化回归，在超参数化范围内，根据特定选择的凸函数并适当增加一个正则化项，可以实现稀疏和一位解决方案，其性能几乎与最佳分类性能相同。 |
| [^77] | [Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy](https://arxiv.org/abs/2402.10473) | 该研究引入了信息瓶颈(IB)方法和局部差分隐私(LDP)相结合的信息混淆方法，以填补隐私与公平之间关系的理论研究空白。 |
| [^78] | [Theoretical Understanding of Learning from Adversarial Perturbations](https://arxiv.org/abs/2402.10470) | 研究提供了一个理论框架，表明各种对抗性扰动（甚至是几个像素的扰动）包含足够的类特征用于泛化，进一步揭示了从扰动中学习时的决策边界 |
| [^79] | [Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation](https://arxiv.org/abs/2402.10468) | 提出一种对抗课程图对比学习（ACGCL）框架，利用成对增强生成具有可控相似性的图级正负样本，同时通过子图对比学习来识别有效的图模式 |
| [^80] | [FedKit: Enabling Cross-Platform Federated Learning for Android and iOS](https://arxiv.org/abs/2402.10464) | FedKit是一个专为安卓和iOS设备上的跨平台联邦学习研究设计的系统，支持模型转换、硬件加速训练和跨平台模型聚合，以促进持续模型交付和训练。 |
| [^81] | [QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning](https://arxiv.org/abs/2402.10462) | 本论文提出了一种名为QDyLoRA的高效量化动态低秩适应方法，能够在大型语言模型的预定义秩上实现有效微调，与QLoRA相竞争，并且在采用其最佳秩时表现更好。 |
| [^82] | [Learning-Augmented Skip Lists](https://arxiv.org/abs/2402.10457) | 通过将机器学习建议与跳表设计整合，提出了一种学习增强型跳表，能够实现最优期望搜索时间，在处理搜索查询时具有显著改进。 |
| [^83] | [Generative Modeling for Tabular Data via Penalized Optimal Transport Network](https://arxiv.org/abs/2402.10456) | 提出了一种名为POTNet的生成建模网络，基于边缘惩罚的Wasserstein损失，能够有效地建模同时包含分类和连续特征的表格数据。 |
| [^84] | [PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem](https://arxiv.org/abs/2402.10450) | 将时间动作抽象视为序列压缩问题，使用Primitive Sequence Encoding (PRISE)方法结合连续动作量化与BPE来学习强大的动作抽象，并在多任务模仿学习和少样本模仿学习中取得显著性能提升 |
| [^85] | [Incremental Sequence Labeling: A Tale of Two Shifts](https://arxiv.org/abs/2402.10447) | 提出了一种名为IS3的框架，旨在解决增量序列标记任务中的E2O和O2E两种重要的语义转变，通过使用知识蒸馏来维持对旧实体的判别能力。 |
| [^86] | [Collaborative Learning with Different Labeling Functions](https://arxiv.org/abs/2402.10445) | 研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。 |
| [^87] | [Parametric Augmentation for Time Series Contrastive Learning](https://arxiv.org/abs/2402.10434) | 通过信息理论分析时间序列数据增广，总结最常用的增广方法。 |
| [^88] | [Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations](https://arxiv.org/abs/2402.10433) | 本研究探索了预训练生成采样器在少样本情况下结合MD模拟的方法，以提高蛋白质构象采样的准确性和效率。 |
| [^89] | [Fixed Confidence Best Arm Identification in the Bayesian Setting](https://arxiv.org/abs/2402.10429) | 该研究在贝叶斯设置中探讨了固定置信度最佳臂识别问题，证明了传统频率设定下的算法在此设置下表现次优，并引入了一种性能与理论下限相匹配的连续排除变种。 |
| [^90] | [DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision](https://arxiv.org/abs/2402.10425) | 本研究提出了一种使用深度基于图谱的IAC分割网络，通过将IAC和ANFs预先定位在单个图谱中进行训练，从而准确推断ANFs的位置。 |
| [^91] | [Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412) | 提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。 |
| [^92] | [Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning](https://arxiv.org/abs/2402.10409) | 通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。 |
| [^93] | [Polyhedral Complex Derivation from Piecewise Trilinear Networks](https://arxiv.org/abs/2402.10403) | 本文以三线性插值方法作为位置编码，提出了理论见解和分析网格提取方法，将高维曲面转换为平面，并引入了一种近似交点的方法，拓展了更广泛的应用。 |
| [^94] | [ManiFPT: Defining and Analyzing Fingerprints of Generative Models](https://arxiv.org/abs/2402.10401) | 本文明确定了生成模型中的工件和指纹的定义，并提出了计算它们的算法，发现使用该定义可以显著提高识别潜在生成过程的性能。 |
| [^95] | [LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs](https://arxiv.org/abs/2402.10397) | LogELECTRA是一个新的日志异常检测模型，通过对单行日志消息进行深入分析，解决了基于模板出现模式的检测方法存在的局限性和可能导致不必要延迟的问题。 |
| [^96] | [Pretext Training Algorithms for Event Sequence Data](https://arxiv.org/abs/2402.10392) | 提出了针对事件序列数据的自监督预训练框架，通过引入新颖的对齐验证任务，构建了适用于事件序列的基础表示，可以推广到不同的下游任务和数据领域。 |
| [^97] | [MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling](https://arxiv.org/abs/2402.10387) | MFBind提出了一种多保真度方法，通过集成对接和结合自由能模拟器，使用主动学习训练多保真度深度代理模型，实现了在准确性和计算成本之间的最佳权衡。 |
| [^98] | [Subgraph-level Universal Prompt Tuning](https://arxiv.org/abs/2402.10380) | 设计了一种可适用于任何预训练策略的简单提示调整方法，通过位于输入图特征空间内的功能来实现，从而增加了其在各种下游应用中的通用性 |
| [^99] | [DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows](https://arxiv.org/abs/2402.10379) | DataDreamer是一种用于合成数据生成和可复现LLM工作流程的开源Python库，有助于研究人员实现强大的LLM工作流，提倡开放科学和可重现性。 |
| [^100] | [Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)](https://arxiv.org/abs/2402.10376) | 本研究提出了一种新方法，Sparse Linear Concept Embeddings（SpLiCE），通过将CLIP表示转换为人可解释概念的稀疏线性组合，实现了对CLIP嵌入的解释。 |
| [^101] | [Revisiting Experience Replayable Conditions](https://arxiv.org/abs/2402.10374) | 本文提出了更严格的“经验重放条件”，并揭示了政策改进的不稳定性因素，从而导出了相应的稳定化技巧，最终使得经验重放可应用于优势演员-评论家算法。 |
| [^102] | [BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains](https://arxiv.org/abs/2402.10373) | BioMistral是一种面向生物医学领域的开源预训练大型语言模型集合，在医学问答任务中表现出优越性能并具有竞争优势。 |
| [^103] | [Learnability is a Compact Property](https://arxiv.org/abs/2402.10360) | 监督学习问题的困难性具有紧凑的有限特性表征。 |
| [^104] | [Can we soft prompt LLMs for graph learning tasks?](https://arxiv.org/abs/2402.10359) | 引入了GraphPrompter框架，通过软提示将图信息与LLMs对齐，以进一步探究LLMs理解图信息的潜力。 |
| [^105] | [Efficient Sampling on Riemannian Manifolds via Langevin MCMC](https://arxiv.org/abs/2402.10357) | 通过Langevin MCMC在黎曼流形上高效采样，并证明了在特定条件下迭代步数为$\tilde{O}(\epsilon^{-2})$时，Langevin MCMC的迭代会与目标分布在$\epsilon$-Wasserstein距离内。 |
| [^106] | [Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models](https://arxiv.org/abs/2402.10353) | 本研究提出了一种空输入提示方法，用于校准预训练语言模型中的固有偏差，从而提升零/少样本学习的性能。 |
| [^107] | [Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review](https://arxiv.org/abs/2402.10350) | 大型语言模型在预测和异常检测领域展现出显著潜力，但面临着挑战包括依赖历史数据集、泛化困难、模型幻觉等问题，提出了整合多模态数据等解决方案。 |
| [^108] | [Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization](https://arxiv.org/abs/2402.10342) | 本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见 |
| [^109] | [What to Do When Your Discrete Optimization Is the Size of a Neural Network?](https://arxiv.org/abs/2402.10339) | 论文讨论了在大型神经网络中处理离散优化问题的方法，提出了使用继续路径方法和蒙特卡罗方法来引导模型达到良好解决方案的两种不同途径 |
| [^110] | [HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting](https://arxiv.org/abs/2402.10334) | 提出了一种名为HI-GAN的层次化修复GAN，通过三个GAN以层次结构的方式进行RGBD修复，其中EdgeGAN和LabelGAN分别修复遮罩边缘和分割标签图像，而CombinedRGBD-GAN结合它们的潜在表示输出并进行RGB和深度修复。 |
| [^111] | [Interpretable Generative Adversarial Imitation Learning](https://arxiv.org/abs/2402.10310) | 提出了一种结合了信号时序逻辑（STL）推断和控制合成的新颖仿真学习方法，可以明确表示任务为STL公式，同时通过人为调整STL公式实现对人类知识的纳入和新场景的适应，还采用了生成对抗网络（GAN）启发的训练方法，有效缩小了专家策略和学习策略之间的差距 |
| [^112] | [Discrete Probabilistic Inference as Control in Multi-path Environments](https://arxiv.org/abs/2402.10309) | 通过Generative Flow Networks (GFlowNets)学习一个随机策略，以近似实现在整个马尔可夫决策过程（MDP）中流量的守恒，从而解决了多路径生成相同对象的偏倚分布问题。 |
| [^113] | [An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM](https://arxiv.org/abs/2402.10291) | KCUSUM算法是一种非参数扩展算法，用于在高容量数据情景下实时检测突变变化，相比于现有算法，其能够更灵活地在在线环境中进行变点检测。 |
| [^114] | [Thompson Sampling in Partially Observable Contextual Bandits](https://arxiv.org/abs/2402.10289) | 研究了在部分观察到的上下文特征老虎机中使用Thompson Sampling策略，以学习选择最佳臂的问题 |
| [^115] | [Backdoor Attack against One-Class Sequential Anomaly Detection Models](https://arxiv.org/abs/2402.10283) | 本文提出了一种新型后门攻击策略，可以通过在良性正常数据中制作几乎不可察觉的触发器，并将其注入模型，成功妥协了两个一类异常检测模型。 |
| [^116] | [Information Capacity Regret Bounds for Bandits with Mediator Feedback](https://arxiv.org/abs/2402.10282) | 该研究提出了一种基于信息容量的新遗憾界限方法，适用于具有中介反馈的赌博机问题，同时在敌对和随机设置中提供了近乎匹配的下界。 |
| [^117] | [SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms](https://arxiv.org/abs/2402.10280) | 提出了SusFL系统，利用能量感知联合学习技术为智能农场监测提供可持续性解决方案，通过智能客户端选择优化监测质量，同时减少能量消耗，并具备抵抗对抗性攻击的能力。 |
| [^118] | [A StrongREJECT for Empty Jailbreaks](https://arxiv.org/abs/2402.10260) | 提出了一种新的基准 StrongREJECT，通过使用更高质量的问题，更好地区分有效和无效的空破解方法。 |
| [^119] | [Personalized Federated Learning for Statistical Heterogeneity](https://arxiv.org/abs/2402.10254) | 该论文总结了个性化联邦学习领域的当前研究进展，探讨了如何解决统计异质性带来的挑战，并讨论了进一步研究和障碍。 |
| [^120] | [Online Control of Linear Systems with Unbounded and Degenerate Noise](https://arxiv.org/abs/2402.10252) | 这项研究揭示了在线控制问题中，对于凸成本，可以实现 $ \widetilde{O}(\sqrt{T}) $ 的遗憾界，甚至在存在无界噪声的情况下；同时，在成本具有强凸性时，可以在不需要噪声协方差是非退化的情况下建立 $ O({\rm poly} (\log T)) $ 的遗憾界。 |
| [^121] | [Brant-2: Foundation Model for Brain Signals](https://arxiv.org/abs/2402.10251) | Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。 |
| [^122] | [A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals](https://arxiv.org/abs/2402.10248) | 该论文提出了一种基于数据驱动的监督机器学习方法，可用于估计全球环境空气污染浓度，并提供预测区间，为各利益相关者提供更全面的数据，并通过检验模型在不同地理位置上的性能为研究提供洞见。 |
| [^123] | [Understanding team collapse via probabilistic graphical models](https://arxiv.org/abs/2402.10243) | 本研究开发了一个概率图模型来理解团队的动态，并通过研究团队崩溃现象的主要原因以及构建弹性团队的原则来展示其在团队管理中的应用。 |
| [^124] | [Signed Diverse Multiplex Networks: Clustering and Inference](https://arxiv.org/abs/2402.10242) | 保留边的符号在网络构建过程中提高了估计和聚类精度，有助于解决现实世界问题。 |
| [^125] | [A Dynamical View of the Question of Why](https://arxiv.org/abs/2402.10240) | 提出了一种在时间过程中直接建立事件之间因果关系的学习范式，并提出了用于计算因果贡献的两个关键引理，可以揭示和量化扩散过程中的因果关系。 |
| [^126] | [A Language Model for Particle Tracking](https://arxiv.org/abs/2402.10239) | 提出一种使用语言模型统一不同粒子跟踪任务的方法，开发出能够用于其他任务的粒子跟踪BERT模型，为粒子探测器理解奠定基础。 |
| [^127] | [Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution](https://arxiv.org/abs/2402.10238) | 本研究利用机器学习方法（FNO和CNN）学习参数化偏微分方程的时间推进算子，扩展了现有的算子学习方法，能在不同参数条件下准确预测短期解并提供稳健的长期统计信息，有助于节约计算成本并加速工程仿真开发。 |
| [^128] | [Discovering Sensorimotor Agency in Cellular Automata using Diversity Search](https://arxiv.org/abs/2402.10236) | 本论文利用多样性搜索、课程学习和梯度下降等算法，自动搜索元胞自动机中能够自组织出具有基础感觉运动机构的“个体”，为寻找这种基本机构自组织的环境条件提供了新方法。 |
| [^129] | [A Multi-faceted Semi-Synthetic Dataset for Automated Cyberbullying Detection](https://arxiv.org/abs/2402.10231) | 提供了一个包含网络欺凌所有要素的广泛半合成数据集，填补了自动检测网络欺凌领域的数据缺口 |
| [^130] | [Temporal Analysis of Drifting Hashtags in Textual Data Streams: A Graph-Based Application](https://arxiv.org/abs/2402.10230) | 本文利用图分析和文本数据流的概念，在年度快照中分析了随时间漂移的标签，揭示了标签社区，特别是针对＃mybodymychoice标签的分析，并为监测社交媒体中关于实体的意见和情感模式随时间变化提供了有用的方法。 |
| [^131] | [Mixture-Models: a one-stop Python Library for Model-based Clustering using various Mixture Models](https://arxiv.org/abs/2402.10229) | \texttt{Mixture-Models}是一个集成了各种混合模型的Python库，通过自动微分工具简化模型的实现和分析，并支持高维数据，提供用户友好的评估工具。 |
| [^132] | [HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments](https://arxiv.org/abs/2402.10228) | HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。 |
| [^133] | [Correlational Lagrangian Schr\"odinger Bridge: Learning Dynamics with Population-Level Regularization](https://arxiv.org/abs/2402.10227) | 提出了一个名为相关拉格朗日薛定谔桥（CLSB）的新框架，通过人口级正则化来学习跨截面样本中的系统动态，适应个体粒子行为的异质性。 |
| [^134] | [Clustering Inductive Biases with Unrolled Networks](https://arxiv.org/abs/2402.10213) | 提出了一种自动编码器架构（WLSC），其潜在表示是隐含的，用于对聚类归纳偏好进行建模 |
| [^135] | [Reusing Softmax Hardware Unit for GELU Computation in Transformers](https://arxiv.org/abs/2402.10118) | 本文提出了一种在Transformer中重用Softmax硬件单元进行GELU计算的方法，实验证明这种方法不会降低NLP应用的准确性。 |
| [^136] | [Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion](https://arxiv.org/abs/2402.10009) | 本文研究了使用DDPM反转进行音频信号的零样本编辑技术，包括基于文本的编辑和无监督发现编辑方向。这些方法在音乐信号中展现了多样的音乐兴趣修改。 |
| [^137] | [Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786) | 这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。 |
| [^138] | [MiMiC: Minimally Modified Counterfactuals in the Representation Space](https://arxiv.org/abs/2402.09631) | 提出了一种新颖的对抗事实生成方法，利用闭式解决方案在表示空间中生成富有表达力的对抗事实，以减轻语言模型中的不良行为，该方法在地球移动问题方面提供理论上的保证，并对表示空间的几何组织进行改进。 |
| [^139] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^140] | [PMGDA: A Preference-based Multiple Gradient Descent Algorithm](https://arxiv.org/abs/2402.09492) | PMGDA是一个基于偏好的多梯度下降算法，可以 efficiently 在多目标机器学习应用中找到与决策者偏好完全匹配的帕累托最优解。 |
| [^141] | [Mitigating Reward Hacking via Information-Theoretic Reward Modeling](https://arxiv.org/abs/2402.09345) | 本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。 |
| [^142] | [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。 |
| [^143] | [BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation](https://arxiv.org/abs/2402.08712) | BECoTTA是一种基于输入的高效CTTA框架，通过采用MoDE（Mixture-of-Domain Low-rank Experts）模型，它包含领域自适应路由和领域专家协同损失两个核心组件，能够在持续的测试时间中自适应不断变化的领域，同时只需较少的可训练参数。 |
| [^144] | [Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering](https://arxiv.org/abs/2402.08277) | 这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。 |
| [^145] | [Improving Black-box Robustness with In-Context Rewriting](https://arxiv.org/abs/2402.08225) | 本文提出了一种名为LLM-TTA的方法，通过使用LLM生成的增强作为测试时间增强（TTA）的增强函数，提高了黑盒模型的鲁棒性。在BERT和T5模型的情感、毒性和新闻分类任务中，LLM-TTA优于传统的增强函数，使BERT的分布外鲁棒性平均提高了4.30个百分点，而不降低分布内性能。 |
| [^146] | [NICE: To Optimize In-Context Examples or Not?](https://arxiv.org/abs/2402.06733) | 通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。 |
| [^147] | [Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) Framework in UAV Networks](https://arxiv.org/abs/2402.05973) | 本论文提出了一种基于区块链的聚簇可扩展联邦学习（BCS-FL）框架，用于改善无人机网络中的联邦学习的去中心化、协调、可扩展性和效率。该框架将无人机网络划分为聚簇，并由聚簇头无人机进行协调，以提高大规模无人机网络中的联邦学习性能。 |
| [^148] | [Multi-View Symbolic Regression](https://arxiv.org/abs/2402.04298) | 多视角符号回归(MvSR)是一种同时考虑多个数据集的符号回归方法，能够找到一个参数化解来准确拟合所有数据集，解决了传统方法无法处理不同实验设置的问题。 |
| [^149] | [Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process](https://arxiv.org/abs/2402.04146) | 这篇论文提出了一种基于潜变量高斯过程的多源数据融合框架，用于解决多个数据源之间质量和全面性差异给系统优化带来的问题。 |
| [^150] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^151] | [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626) | 这项研究深入研究了大规模视觉语言模型（LVLM）对于自动生成的排版攻击的易受攻击性，并引入了一种新的、更有效的自动生成的排版攻击方法，为此设计了一个独特的测试基准。通过使用该基准，研究发现排版攻击对LVLM构成了重大威胁。 |
| [^152] | [Short: Benchmarking transferable adversarial attacks](https://arxiv.org/abs/2402.00418) | 本研究首次全面评估了可转移性对抗攻击的方面，引入了一个基准框架并系统分类和评估了各种增强对抗攻击可转移性的方法，为不同的模型架构提供了一个标准化的平台。 |
| [^153] | [Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM](https://arxiv.org/abs/2401.00685) | 本文提出了NomaFedHAP这一新型FL-SatCom方法，利用HAPs作为PS来增强卫星可见性，并引入NOMA技术实现快速高效的模型传输。 |
| [^154] | [Sample Path Regularity of Gaussian Processes from the Covariance Kernel](https://arxiv.org/abs/2312.14886) | 本文提供了关于高斯过程样本路径正则性的新颖和紧凑的特征描述，通过协方差核对应的GP样本路径达到一定正则性的充分必要条件，对常用于机器学习应用中的GPs的样本路径正则性进行了探讨。 |
| [^155] | [Q-SENN: Quantized Self-Explaining Neural Networks](https://arxiv.org/abs/2312.13839) | Q-SENN提出了量化自解释神经网络，通过描述每个类与特征之间的关系为正、负或中性，实现更二元化、更符合人类友好的特征，同时在准确性方面优于先前的工作。 |
| [^156] | [BloomVQA: Assessing Hierarchical Multi-modal Comprehension](https://arxiv.org/abs/2312.12716) | 提出了新VQA数据集BloomVQA，基于Bloom的分类法，通过层次图表示实现数据增强和模型一致性评估，揭示大型视觉语言模型在高级理解任务上的性能下降。 |
| [^157] | [KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know](https://arxiv.org/abs/2312.11539) | KGLens 是一个旨在衡量知识图与大型语言模型（LLMs）之间对齐程度的框架，帮助找出LLMs相对于知识图的知识不足之处。 |
| [^158] | [Cascade Speculative Drafting for Even Faster LLM Inference](https://arxiv.org/abs/2312.11462) | 引入了Cascade Speculative Drafting（CS Drafting）算法，通过垂直级联消除神经模型的自回归生成，通过水平级联优化草稿中的时间分配，从而进一步提高LLM推理效率。 |
| [^159] | [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2312.06635) | 该论文提出了一种具有硬件高效性的线性注意力算法，可在短序列长度下比现有方法更快，同时推广到了具有数据相关门的更具表达能力的线性注意力变体。 |
| [^160] | [Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context](https://arxiv.org/abs/2312.06528) | 本论文证明了Transformers可以在函数空间中自然地学习实现梯度下降，从而学习上下文中的非线性函数，并且展示了最优的非线性激活函数选择取决于需要学习的函数类别。 |
| [^161] | [PULSAR: Graph based Positive Unlabeled Learning with Multi Stream Adaptive Convolutions for Parkinson's Disease Recognition](https://arxiv.org/abs/2312.05780) | PULSAR是一种新颖的方法，用于通过分析手指敲击任务的视频来筛查帕金森病，采用了自适应图卷积神经网络和多通道自适应卷积来动态学习时空图边。 |
| [^162] | [Semi-Supervised Health Index Monitoring with Feature Generation and Fusion](https://arxiv.org/abs/2312.02867) | 通过深度半监督异常检测（DeepSAD）方法进行健康指数构建，并提出了多样性损失来丰富条件指标。 |
| [^163] | [An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis](https://arxiv.org/abs/2311.18048) | 本研究通过在高斯LTI系统中引入各种干预信号，连接了实验设计和动态系统中的表示可识别性，为系统参数的识别提供了新的方法。 |
| [^164] | [K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise](https://arxiv.org/abs/2311.10162) | 提出了一种在K空间进行图像退化和恢复的冷扩散模型，无需噪音，能够为加速MRI生成高质量的重建图像 |
| [^165] | [Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge](https://arxiv.org/abs/2311.09731) | 本研究系统地调查了大型语言模型在缺乏足够参数化知识的情况下如何表达对超出其知识范围的问题的不确定性，并强调了诚实与帮助性之间的权衡。 |
| [^166] | [Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness](https://arxiv.org/abs/2311.08936) | 本文提出了一个新的框架，称为自然度解释与评估模型（CNE）。该框架利用可解释的机器学习方法来分析卫星图像，以解释和评估形成自然度的模式，并带有相应的置信度。 |
| [^167] | [FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy](https://arxiv.org/abs/2310.09789) | FLrce方法通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，解决了安全和资源利用不均衡问题。 |
| [^168] | [Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks](https://arxiv.org/abs/2310.06549) | 标签平滑方法在深度学习中发挥重要作用，既能提升模型泛化能力和校准性，又可能成为模型隐私泄露的因素。研究揭示了结合负因子进行平滑可有效阻止模型反推攻击，提升隐私保护效果，超越了当前最先进的防御技术。 |
| [^169] | [Generative quantum machine learning via denoising diffusion probabilistic models](https://arxiv.org/abs/2310.05866) | 通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。 |
| [^170] | [Federated K-means Clustering](https://arxiv.org/abs/2310.01195) | 本研究提出一种实现K均值聚类的联邦学习算法，解决了中心之间聚类数量变化和在不太可分数据集上的收敛挑战。 |
| [^171] | [Light Schr\"odinger Bridge](https://arxiv.org/abs/2310.01174) | 提出了一种新颖的光谢尔宾格桥快速简单求解器，将谢尔宾格势能参数化为总和-指数二次函数，视为能量函数，实现了轻量级、无需模拟、理论上合理的求解器设计。 |
| [^172] | [Redundancy and Concept Analysis for Code-trained Language Models](https://arxiv.org/abs/2305.00875) | 本文针对代码训练的语言模型进行冗余和概念分析，通过消除与给定任务不相关的神经元，帮助理解网络中哪些神经元和层可以被消除，并在哪里找到重要的代码属性。 |
| [^173] | [Interpretable Deep Learning Methods for Multiview Learning](https://arxiv.org/abs/2302.07930) | 提出iDeepViewLearn（可解释的多视图学习深度学习方法）用于多视图数据的非线性关系学习和特征选择，融合深度学习的灵活性和统计优势，给出可解释结果。 |
| [^174] | [Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation](https://arxiv.org/abs/2302.03038) | 本文探讨了如何将单细胞视为空间标记，并利用Transformer模型促进空间转录组数据填充。 |
| [^175] | [Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior](https://arxiv.org/abs/2212.03733) | 层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。 |
| [^176] | [Hijack Vertical Federated Learning Models As One Party](https://arxiv.org/abs/2212.00322) | 这项研究探讨了水平联邦学习模型的安全性，弥补了现有研究中对于该模型安全性的不足。 |
| [^177] | [Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble](https://arxiv.org/abs/2211.11278) | 提出了一种基于最优扩展邻域规则的集成方法，通过新规则确定邻居和模型选择策略来解决传统$k$最近邻方法的局限性和提升集成性能。 |
| [^178] | [Enabling Deep Learning-based Physical-layer Secret Key Generation for FDD-OFDM Systems in Multi-Environments](https://arxiv.org/abs/2211.03065) | 本文在多个环境下将物理层秘钥生成问题转化为学习性问题，提出了基于深度传递学习和元学习的信道特征映射算法，实现了在多个新环境中快速有效地生成秘钥。 |
| [^179] | [Decorrelative Network Architecture for Robust Electrocardiogram Classification](https://arxiv.org/abs/2207.09031) | 我们提出了一种基于特征装饰和Fourier分区的新型集成方法，用于教授网络各种互补特征，减少基于扰动的愚弄的机会。 |
| [^180] | [Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions](https://arxiv.org/abs/2207.04771) | 提供了条件矩限制问题的功能化广义经验似然估计方法，并探索了其在小样本性能上的优势，同时提供了基于核和神经网络的实现方式。 |
| [^181] | [Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification](https://arxiv.org/abs/2206.12532) | 本文引入了因果评分作为一种新型方法，支持决策制定，提供洞察力，并可用于效应估计、效应排序和效应分类。 |
| [^182] | [Modeling Attrition in Recommender Systems with Departing Bandits](https://arxiv.org/abs/2203.13423) | 本论文提出了一个新型多臂赌博机设置，捕捉了推荐系统中用户离开的情况，首次证明了在所有用户共享相同类型时，基于UCB的算法是最优的。 |
| [^183] | [Fr\'echet random forests for metric space valued regression with non euclidean predictors](https://arxiv.org/abs/1906.01741) | 本文介绍了Fréchet随机森林，允许处理值在一般度量空间的数据，并引入了新的树节点分裂方式，扩展了预测过程，提出了一致性定理，并适用于数据驱动分区的Fréchet纯一致随机树 |
| [^184] | [Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents.](http://arxiv.org/abs/2401.16461) | 通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。 |
| [^185] | [Topology-aware Embedding Memory for Learning on Expanding Graphs.](http://arxiv.org/abs/2401.13200) | 这篇论文提出了一个基于拓扑感知嵌入记忆的学习扩展图的框架，该框架可以解决在不断扩展的图上应用记忆回放技术导致的内存爆炸问题。 |
| [^186] | [Cascading Reinforcement Learning.](http://arxiv.org/abs/2401.08961) | 本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。 |
| [^187] | [PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems.](http://arxiv.org/abs/2401.08903) | 本文提出了一种名为PPR的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。该方法利用对抗样本修剪，并通过嵌入对抗扰动来增强对抗人脸样本的躲避性能。 |
| [^188] | [MADA: Meta-Adaptive Optimizers through hyper-gradient Descent.](http://arxiv.org/abs/2401.08893) | MADA是一个统一的优化器框架，通过超梯度下降动态学习最适合的优化器。数值结果表明MADA在次优调整的超参数下是稳健的，并且在默认超参数下常常优于其他优化器。插值优化器可以改进收敛性能。 |
| [^189] | [Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning.](http://arxiv.org/abs/2401.06469) | 本文提出了批处理ICL方法，通过将ICL视为一个元优化过程，开发出了一个有效、高效且无序的推理算法。通过聚合元梯度并将其应用于零-shot学习，该方法使LLM对ICL示例顺序无关，并且在实验证明其在大多数情况下优于其他排列方式，甚至超过了标准ICL的最佳顺序的性能。 |
| [^190] | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning.](http://arxiv.org/abs/2401.05268) | AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。 |
| [^191] | [Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics.](http://arxiv.org/abs/2401.05146) | 这篇综述论文介绍了联邦遗忘的概念和挑战，以及解决这些问题的方法和设计准则，旨在为联邦学习中保护用户隐私和防止恶意攻击提供解决方案。 |
| [^192] | [Simple and Asymmetric Graph Contrastive Learning without Augmentations.](http://arxiv.org/abs/2310.18884) | 本文提出了一种无需增强的简单非对称图对比学习方法GraphACL，通过考虑邻居节点的非对称视图，该方法能够有效地在同类和异类图上进行对比学习，对于建模异类图非常重要。 |
| [^193] | [Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2310.15767) | 本文提出了一种利用自监督对比学习的无配对MRI超分辨率方法，可以在有限的训练数据下提高SR性能，改善MRI分辨率。 |
| [^194] | [A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems.](http://arxiv.org/abs/2310.08644) | 这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。 |
| [^195] | [Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework.](http://arxiv.org/abs/2310.04741) | 本文介绍了一个名为RDAC的框架，该框架解剖了持续学习中稳定性和可塑性之间的平衡，并详细分析了几种常用算法在处理任务时的稳定性和可塑性权衡。 |
| [^196] | [Prompting-based Efficient Temporal Domain Generalization.](http://arxiv.org/abs/2310.02473) | 我们提出了一种基于提示的高效时域泛化方法，通过学习全局提示、领域特定提示和感知时序漂移的提示，不需要目标域数据的情况下适应时序漂移，并在各种任务中取得了state-of-the-art的性能。 |
| [^197] | [On the Stability of Iterative Retraining of Generative Models on their own Data.](http://arxiv.org/abs/2310.00429) | 本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。 |
| [^198] | [Learning to Transform for Generalizable Instance-wise Invariance.](http://arxiv.org/abs/2309.16672) | 该论文提出了一种学习转换以实现通用的实例不变性的方法。通过使用归一化流来预测图像的变换分布，并对预测结果进行平均，可以实现对不同实例之间的对齐，从而推广不变性的类别间的应用。这种方法还可以适应超出分布范围的姿势，并且可以学习更广泛的变换范围。 |
| [^199] | [From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields.](http://arxiv.org/abs/2309.15126) | 这项研究提出了一种称为SO3krates的欧几里得变换器架构，它通过组合稀疏等变表示和自注意机制，在机器学习力场中实现了精度、稳定性和速度的独特组合，从而使我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。 |
| [^200] | [Quasi-Monte Carlo for 3D Sliced Wasserstein.](http://arxiv.org/abs/2309.11713) | 本文提出了准蒙特卡洛（QMC）方法用于三维切片Wasserstein（SW）的近似计算，并通过多种方法在三维单位超球面上构造了QMC点集。此外，还介绍了将QSW扩展为随机准切片Wasserstein（RQSW）的方法。 |
| [^201] | [Optimal Transport with Tempered Exponential Measures.](http://arxiv.org/abs/2309.04015) | 本文推广了熵正则化最优输运方法，将其应用于温度指数测度中，实现了快速有效的算法和可控的稀疏性。 |
| [^202] | [Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals.](http://arxiv.org/abs/2308.12563) | 这个论文介绍了一种针对带有污染数据的多变量时间序列异常检测的新方法，通过去污和变量依赖建模实现了无监督的异常检测，对于实际场景中的异常检测具有重要意义。 |
| [^203] | [Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT.](http://arxiv.org/abs/2308.07876) | 该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。 |
| [^204] | [Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability.](http://arxiv.org/abs/2307.15007) | 本文旨在搭建一座桥梁，将后期解释性和内在可解释性相结合，以解释复杂的黑盒模型的行为，并解决解释的忠实性和可验证性之间的问题。 |
| [^205] | [Fair Machine Unlearning: Data Removal while Mitigating Disparities.](http://arxiv.org/abs/2307.14754) | 本研究提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的机器学习方法。 |
| [^206] | [Bandits with Deterministically Evolving States.](http://arxiv.org/abs/2307.11655) | 该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。 |
| [^207] | [Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning.](http://arxiv.org/abs/2307.07881) | 提出了一种嵌入图的直觉模糊RVFL模型，用于解决类别不平衡学习的问题。该模型通过采用加权机制处理不平衡的数据集，并利用图嵌入提取语义丰富的信息。该模型在类别不平衡学习中具有较高的准确性。 |
| [^208] | [Pruning vs Quantization: Which is Better?.](http://arxiv.org/abs/2307.02973) | 本文比较了神经网络量化和修剪这两种压缩深度神经网络的技术，结果表明在大多数情况下，量化优于修剪。 |
| [^209] | [Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning.](http://arxiv.org/abs/2306.15585) | 本研究使用强化学习技术，通过平衡最大化投资组合收入和最小化准备金的对抗目标，自动化寻找最优信用卡额度调整策略。 |
| [^210] | [Learning Representations on the Unit Sphere: Application to Online Continual Learning.](http://arxiv.org/abs/2306.03364) | 该论文提出了一种基于单位球的表示学习方法，通过将表示推向固定方向，使得学习策略对数据漂移具有弹性，从而能够应对在线连续学习的挑战性问题。 |
| [^211] | [DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing.](http://arxiv.org/abs/2306.01794) | 本文提出了DiffPack，这是一个自回归的扭转扩散模型，通过在扭曲空间中进行扩散和去噪来学习侧链扭转角度的联合分布，从而准确地预测蛋白质侧链的构象，有效应用于蛋白质结构预测、设计和蛋白质相互作用等领域。 |
| [^212] | [Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL.](http://arxiv.org/abs/2305.17342) | 本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。 |
| [^213] | [Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization.](http://arxiv.org/abs/2305.13503) | 本文提出了MA-FL，应用异步模型传输体系结构来实现有多个下游任务需要训练的联邦学习。本文的收敛性分析揭示了资源分配、设备调度和个体模型状态对机器学习模型性能的影响。实验表明，MA-FL在收敛速度和模型精度方面优于现有的联邦学习方法。 |
| [^214] | [Unsupervised ASR via Cross-Lingual Pseudo-Labeling.](http://arxiv.org/abs/2305.13330) | 本研究提出了一种基于跨语言伪标注的无监督ASR方法，能够使用其他语言中的标注数据来引导新语言的无监督AM。在Common Voice上取得了良好的效果，可以实现18% WER。而且在不同语言的数据集上都优于基线模型。 |
| [^215] | [Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer.](http://arxiv.org/abs/2305.12095) | 本文提出了一种通道对齐鲁棒双Transformer模型，通过双Transformer结构和鲁棒损失函数的引入，解决了Transformer在时间序列预测中的关键缺点，显著提高了预测精度和效率。 |
| [^216] | [Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks.](http://arxiv.org/abs/2305.10544) | GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。 |
| [^217] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^218] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^219] | [On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers.](http://arxiv.org/abs/2304.10640) | 本文比较了投影方法和优化方法求解分布式线性系统的收敛速度，提出了角异构性的几何概念，并对最有效的算法(APC和D-HBM)的收敛速度进行了约束和比较。 |
| [^220] | [Cross-scale Multi-instance Learning for Pathological Image Diagnosis.](http://arxiv.org/abs/2304.00216) | 本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。 |
| [^221] | [Contrastive Learning Is Spectral Clustering On Similarity Graph.](http://arxiv.org/abs/2303.15103) | 本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。 |
| [^222] | [Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators.](http://arxiv.org/abs/2303.08431) | 本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。 |
| [^223] | [Differential Good Arm Identification.](http://arxiv.org/abs/2303.07154) | 本文提出了DGAI算法，它可以在好手臂识别问题中通过深度学习的方式减少样本复杂性，并且在具有给定阈值的情况下进一步提高多臂赌博问题的性能。 |
| [^224] | [Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++.](http://arxiv.org/abs/2301.11118) | Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。 |
| [^225] | [Invertible normalizing flow neural networks by JKO scheme.](http://arxiv.org/abs/2212.14424) | 本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。 |
| [^226] | [Data Augmentation techniques in time series domain: A survey and taxonomy.](http://arxiv.org/abs/2206.13508) | 本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。 |
| [^227] | [A survey on GANs for computer vision: Recent research, analysis and taxonomy.](http://arxiv.org/abs/2203.11242) | 本文综述了GAN的最新架构、损失函数优化、验证指标和应用领域，并提出了一个分类法以更好地理解计算机视觉中GAN的现状。 |

# 详细

[^1]: 组合生成建模：单一模型并不是您所需要的全部

    Compositional Generative Modeling: A Single Model is Not All You Need

    [https://rss.arxiv.org/abs/2402.01103](https://rss.arxiv.org/abs/2402.01103)

    本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。

    

    在人工智能研究中，通过训练大规模的巨大的生成模型来处理海量数据已经成为一种越来越主流的方法。本文中，我们认为我们应该通过将较小的生成模型组合在一起来构建大型生成系统。我们展示了这种组合生成方法如何以更高效的方式学习分布，使得我们在训练时未见的数据分布部分也能进行泛化。我们进一步展示了这种方法如何使我们能够为训练时完全未见的任务编写和构建新的生成模型。最后，我们展示在许多情况下，我们可以从数据中发现独立的组合组件。

    Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
    
[^2]: 随机凸优化中适应性的代价

    The Price of Adaptivity in Stochastic Convex Optimization

    [https://arxiv.org/abs/2402.10898](https://arxiv.org/abs/2402.10898)

    该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。

    

    我们证明了在非光滑随机凸优化中适应性的不可能性结果。给定一组我们希望适应的问题参数，我们定义了“适应性的代价”（PoA），粗略地说，它衡量了由于这些参数的不确定性而导致的次优性的乘法增加。当初始距离最优解未知但梯度范数有界时，我们证明PoA至少对于期望次优性是对数级别，对于中位数次优性是双对数级别。当距离和梯度范数都存在不确定性时，我们表明PoA必须是与不确定性水平多项式相关的。我们的下界几乎与现有的上界相匹配，并且确定了没有无参数午餐的结论。

    arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
    
[^3]: 将扩散加权MRI和临床数据融合用于预测急性缺血性中风后的功能结果：基于深度对比学习

    Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning

    [https://arxiv.org/abs/2402.10894](https://arxiv.org/abs/2402.10894)

    该研究提出了一种深度融合学习网络，利用扩散加权MRI模式和结构化健康概况数据联合预测急性中风后的功能结果，通过监督对比学习来学习区分特性。

    

    中风是一种常见的致残神经系统疾病，影响25岁以上成年人口中约四分之一；超过一半的患者在急性中风发作后仍然有不良结局，如永久性功能依赖甚至死亡。本研究旨在调查扩散加权MRI模式与结构化健康概况相结合对预测功能结果的有效性，以促进早期干预。 提出了一个深度融合学习网络，包括两阶段训练：第一阶段侧重于跨模态表示学习，第二阶段侧重于分类。利用监督对比学习来学习区分特性，将两类患者从单个模态的嵌入和融合的多模态嵌入中分开。网络以DWI和ADC图像以及结构化健康概况数据作为输入。输出是预测

    arXiv:2402.10894v1 Announce Type: cross  Abstract: Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke. The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention. A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification. Supervised contrastive learning is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused multimodal embedding. The network takes as the input DWI and ADC images, and structured health profile data. The outcome is the pred
    
[^4]: RLVF: 学习如何在没有泛化的情况下从口头反馈中学习

    RLVF: Learning from Verbal Feedback without Overgeneralization

    [https://arxiv.org/abs/2402.10893](https://arxiv.org/abs/2402.10893)

    研究了如何在大型语言模型中利用口头反馈进行定制化调整而不发生过度泛化，并提出了一种新的方法C3PO。

    

    大型语言模型（LLMs）部署的不同情境的多样性要求能够修改或定制默认模型行为，以满足细微的要求和偏好。规定这种模型调整的方便界面是高层次口头反馈，比如"在给老板起草邮件时不要使用表情符号"。然而，尽管撰写高层反馈比从人类反馈中收集强化学习注释（RLHF）简单得多，但我们发现只是用这种反馈提示模型会导致反馈在不相关的情境中产生泛化。我们研究了如何在没有这种泛化的情况下整合口头反馈的问题，并启发了一个新方法：带约束偏好优化的情境化评论（C3PO）。C3PO使用一段高层次反馈生成一个小的合成偏好数据集，指定了反馈应该如何（以及不应该如何）进行。

    arXiv:2402.10893v1 Announce Type: cross  Abstract: The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as "Don't use emojis when drafting emails to my boss." However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should no
    
[^5]: 通过数据水印证明LLM预训练数据的成员资格

    Proving membership in LLM pretraining data via data watermarks

    [https://arxiv.org/abs/2402.10892](https://arxiv.org/abs/2402.10892)

    使用数据水印在LLM预训练中检测版权持有人作品的方法，可以进行合理检测且提供误检率保证，研究了水印设计对假设检验能力的影响以及在模型和数据集缩放下的检测强度变化。

    

    检测版权持有人的作品是否在LLM预训练中使用是一个重要问题，本文提出使用数据水印实现基于黑盒模型访问的合理检测，前提是版权持有人在公开发布之前贡献了多个训练文档并对其进行了水印处理。通过应用随机采样的数据水印，检测可以被构造为假设检验，从而提供对误检率的保证。研究了两种水印：一种插入随机序列，另一种随机用Unicode类似字符替换字符。首先展示了水印设计的三个方面--水印长度、复制次数和干扰--如何影响假设检验的能力。接着研究了水印在模型和数据集缩放下的检测强度如何变化：增加数据集大小会降低水印的强度，水印...

    arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
    
[^6]: 指导多样性推动对未见任务的泛化

    Instruction Diversity Drives Generalization To Unseen Tasks

    [https://arxiv.org/abs/2402.10891](https://arxiv.org/abs/2402.10891)

    指导调整通过增加指令集的多样性来推动模型对未见任务的泛化。

    

    指导调整——在指令和期望结果之间微调大型语言模型（LLM）的方法——是一种使预训练语言模型执行现实世界任务并遵循人类指令的方法。其实际成功取决于模型学习比其训练时更广泛的指令集。然而，决定模型对这种“未见任务”的泛化的因素尚不十分清楚。为了了解泛化的驱动因素，本文通过字符串重写进行实验，这是一个符号任务，是图灵完整马尔可夫算法的基本组成部分，同时允许实验对“输入”和“指令”进行控制。我们调查了模型接受的指令数量和为每个指令提供的训练样本数量之间的权衡，并观察到指令集的多样性确定了泛化。

    arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of "inputs" and "instructions". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati
    
[^7]: LLM规划中树搜索何时有用？取决于鉴别器

    When is Tree Search Useful for LLM Planning? It Depends on the Discriminator

    [https://arxiv.org/abs/2402.10890](https://arxiv.org/abs/2402.10890)

    当前研究通过实验分析了大型语言模型在多步问题求解中使用树搜索的可行性，指出高级规划方法需要鉴别器至少90%准确性才能显著提高性能。

    

    在本文中，我们通过一个语言代理框架研究了大型语言模型（LLMs）如何在多步问题下解决问题，该框架包括生成器、鉴别器和规划方法三个部分。我们研究了两种先进规划方法，迭代校正和树搜索的实际效用。我们全面分析了鉴别准确性如何影响代理在使用这两种方法或更简单的重新排序方法时的整体性能。在两项任务，文本到SQL解析和数学推理上的实验表明：（1）高级规划方法需要至少90%准确性的鉴别器才能实现显著改进；（2）当前LLMs的鉴别能力尚未满足高级规划方法实现这种改进的需求；（3）采用基于LLM的鉴别器时，高级规划方法可能无法充分平衡准确性和效率。

    arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
    
[^8]: 机器学习模型的可解释性：从数据适应性到用户感知

    Explainability for Machine Learning Models: From Data Adaptability to User Perception

    [https://arxiv.org/abs/2402.10888](https://arxiv.org/abs/2402.10888)

    本文旨在为已部署的机器学习模型生成局部解释并确保这些解释对用户具有可理解性，主要创新在于开发具有数据适应性和用户感知要求站点解释方法。

    

    这篇论文探讨了为已部署的机器学习模型生成局部解释，旨在确定产生有意义解释的最佳条件，考虑到数据和用户需求。主要目标是开发方法，生成任何模型的解释，同时确保这些解释保持忠实于基础模型并对用户具有可理解性。论文分为两部分。第一部分增强了一个广泛使用的基于规则的解释方法。然后引入了一个评估线性解释逼近模型适宜性的新方法。此外，对比了两种反事实解释方法族以分析其中一种相对另一种的优势。第二部分侧重于用户实验，评估三种解释方法和两种不同表示的影响。这些实验测量了用户理解解释的速度，可信度和关注程度。

    arXiv:2402.10888v1 Announce Type: new  Abstract: This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure ho
    
[^9]: 基于3D场景表示的3D扩散器Actor：通过策略扩散进行机器人操作

    3D Diffuser Actor: Policy Diffusion with 3D Scene Representations

    [https://arxiv.org/abs/2402.10885](https://arxiv.org/abs/2402.10885)

    通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。

    

    我们将扩散策略和3D场景表示相结合，用于机器人操作。扩散策略通过条件扩散模型学习基于机器人和环境状态的动作分布。最近，它们已经表现出优于确定性和其他基于状态的动作分布学习方法。3D机器人策略使用从单个或多个摄像头视角获取的感应深度聚合的3D场景特征表示。它们已经证明比其2D对应物在摄像机视角上具有更好的泛化能力。我们统一了这两条线路的工作，并提出了3D扩散器Actor，这是一个神经策略架构，它在给定语言指令的情况下，构建视觉场景的3D表示，并在其上进行条件迭代去噪机器人末端执行器的3D旋转和平移。在每个去噪迭代中，我们的模型将末端执行器姿态估计表示为3D场景令牌，并预测t

    arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
    
[^10]: 多模式偏好对齐修复了语言模型在视觉指令调整上的回归

    Multi-modal preference alignment remedies regression of visual instruction tuning on language model

    [https://arxiv.org/abs/2402.10884](https://arxiv.org/abs/2402.10884)

    通过收集轻量级VQA偏好数据集并使用Direct Preference Optimization，我们能够在语言模型的指导能力上取得显著提升，在小规模数据下比其他方法实现了更高的分数。

    

    在实际应用中，多模式大型语言模型（MLLMs）被期望能够支持图像和文本模态的交换式多轮查询。然而，当前使用视觉问题回答（VQA）数据集训练的MLLMs可能会出现退化，因为VQA数据集缺乏原始文本指令数据集的多样性和复杂性，后者是底层语言模型训练的数据集。为了解决这一具有挑战性的退化问题，我们首先收集了一个轻量级（6k条记录）的VQA偏好数据集，其中答案由Gemini以细粒度方式注释了5个质量指标，然后研究了标准的监督微调、拒绝抽样、直接偏好优化（DPO）和SteerLM。我们的研究结果表明，通过DPO，我们能够超越语言模型的指导能力，实现了6.73的MT-Bench分数，而Vicuna的6.57和LLaVA的5.99，尽管数据规模较小。

    arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
    
[^11]: 强健的智能体学习因果世界模型

    Robust agents learn causal world models

    [https://arxiv.org/abs/2402.10877](https://arxiv.org/abs/2402.10877)

    智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。

    

    一直有人假设因果推理在强健且具有通用智能中起着基础作用，然而不清楚智能体是否必须学习因果模型才能推广到新的领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布转变下满足后悔界限的智能体必须学习数据生成过程的近似因果模型，对于优化智能体来说，该近似模型会收敛到真实的因果模型。我们讨论了这一结果对于多个研究领域，包括迁移学习和因果推断的影响。

    arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
    
[^12]: 通过受控组装设计2D Skyrm离子超材料

    Design of 2D Skyrmionic Metamaterial Through Controlled Assembly

    [https://arxiv.org/abs/2402.10874](https://arxiv.org/abs/2402.10874)

    通过"模拟受控组装"方法，成功设计出多种稳定的2D Skyrm离子超材料。

    

    尽管在磁性Skyrmion和反Skyrmion上进行了大量研究，但一个重要挑战仍然存在，即如何制造具有不同甚至定制拓扑结构的非平凡高阶Skyrm离子纹理。我们通过集中在单层薄膜内Skyrmion超材料的构建途径，提出了一些令人惊讶稳定的网格状、薄片状和细胞状Skyrm离子超材料。我们方法的核心是“模拟受控组装”概念，简而言之，这是受“点击化学”启发的一种协议，允许在喜欢的位置放置拓扑磁结构，然后通过能量最小化来阐明稳定性。

    arXiv:2402.10874v1 Announce Type: cross  Abstract: Despite extensive research on magnetic skyrmions and antiskyrmions, a significant challenge remains in crafting nontrivial high-order skyrmionic textures with varying, or even tailor-made, topologies. We address this challenge, by focusing on a construction pathway of skyrmionics metamaterial within a monolayer thin film and suggest several promising lattice-like, flakes-like, and cell-like skyrmionic metamaterials that are surprisingly stable. Central to our approach is the concept of 'simulated controlled assembly', in short, a protocol inspired by 'click chemistry' that allows for positioning topological magnetic structures where one likes, and then allowing for energy minimization to elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium (Q=0). These entities ser
    
[^13]: 三界之最：实践中的数字营销自适应实验

    Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice

    [https://arxiv.org/abs/2402.10870](https://arxiv.org/abs/2402.10870)

    本文分享了关于在工业环境中使用AED系统面临的挑战，并提供了在这种环境中适当的目标和系统规格的视角，最终开发了一个基于反事实推断的AED框架并在商业环境中进行了测试。

    

    自适应实验设计（AED）方法越来越多地被工业界用作一种工具，以提高测试吞吐量或减少与传统A/B/N测试方法相比的实验成本。然而，这些方法的行为和保证在理想化的稳态设置之外并不为人熟知。本文分享了有关在工业环境中天真地使用AED系统面临的挑战，以及在这种环境中适当的目标和系统规格的视角。我们根据这些经验开发了一个基于反事实推断的AED框架，并在商业环境中进行了测试。

    arXiv:2402.10870v1 Announce Type: new  Abstract: Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.
    
[^14]: 在日常环境中的差分隐私联邦迁移学习用于心理健康监测：以压力检测为案例研究

    Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection

    [https://arxiv.org/abs/2402.10862](https://arxiv.org/abs/2402.10862)

    该论文介绍了一种差分私有联邦迁移学习框架，结合差分隐私和迁移学习，以提升心理健康监测中的数据隐私性和数据充足性。

    

    心理健康状况在各个人群中普遍存在，需要有效的监测来减轻其对生活质量的不利影响。数据驱动的心理健康监测方法的兴起强调了在处理敏感健康数据时隐私保护技术的重要性。尽管联邦学习在心理健康监测方面取得了进展，但现有方法在应对特定网络攻击和现实应用中的数据不足方面存在困难。本文介绍了一种差分私有联邦迁移学习框架，用于增强数据隐私性并丰富数据充足性。为实现这一目标，我们将联邦学习与差分隐私（通过将噪声引入更新）和迁移学习（利用预训练的通用模型）两个关键元素相结合，以应对数据不平衡和缺如问题。

    arXiv:2402.10862v1 Announce Type: new  Abstract: Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality. The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data. Despite strides in federated learning for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications. In this paper, we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency. To accomplish this, we integrate federated learning with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) transfer learning, employing a pre-trained universal model to adeptly address issues of data imbalance and in
    
[^15]: JetTrain: IDE原生的机器学习实验

    JetTrain: IDE-Native Machine Learning Experiments

    [https://arxiv.org/abs/2402.10857](https://arxiv.org/abs/2402.10857)

    JetTrain是一个将特定任务从IDE委派给远程计算资源的IDE工具，可以提高机器学习实验的效率。

    

    集成开发环境（IDE）是常见的编写和调试工具。然而，它们尚未被广泛采用于启动机器学习（ML）实验。本文旨在通过引入JetTrain来填补这一空白，JetTrain是一个IDE集成工具，将特定任务从IDE委派给远程计算资源。用户可以在本地编写和调试代码，然后使用按需硬件无缝地在远程运行它。我们认为，这种方法可以降低ML训练问题的准入门槛并增加实验吞吐量。

    arXiv:2402.10857v1 Announce Type: cross  Abstract: Integrated development environments (IDEs) are prevalent code-writing and debugging tools. However, they have yet to be widely adopted for launching machine learning (ML) experiments. This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources. A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware. We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput.
    
[^16]: HistoSegCap: 胶囊网络用于全幻灯片图像中组织类型弱监督语义分割

    HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images

    [https://arxiv.org/abs/2402.10851](https://arxiv.org/abs/2402.10851)

    本文提出了一种新的组织病理图像分析方法，使用基于胶囊网络的弱监督语义分割，首次在这方面的应用，实验结果展示了胶囊网络在增强组织病理图像分析精度和效率方面的潜力。

    

    arXiv:2402.10851v1 公告类型：跨越 摘要：数字病理学涉及将物理组织切片转换为高分辨率的全幻灯片图像（WSIs），病理学家分析其中受疾病影响的组织。然而，具有许多显微领域的大型组织学切片对视觉搜索提出了挑战。为了帮助病理学家，计算机辅助诊断（CAD）系统提供视觉辅助，以有效地检查WSIs并识别诊断相关区域。本文提出了一种新颖的组织病理图像分析方法，采用基于胶囊网络的弱监督语义分割（WSSS），这是首次应用。提出的模型使用数字病理学图谱（ADP）数据集进行评估，并将其性能与其他组织病理语义分割方法进行比较。研究结果强调了胶囊网络在增强组织病理图像分析的精度和效率方面的潜力。

    arXiv:2402.10851v1 Announce Type: cross  Abstract: Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results s
    
[^17]: FedD2S: 个性化无数据联邦知识蒸馏

    FedD2S: Personalized Data-Free Federated Knowledge Distillation

    [https://arxiv.org/abs/2402.10846](https://arxiv.org/abs/2402.10846)

    提出了FedD2S方法，通过深到浅的层丢弃机制，在无数据联邦知识蒸馏中增强了本地模型的个性化，表现出卓越性能和改善客户间公平性。

    

    本文解决了联邦学习（FL）框架中客户端数据异构性的挑战。我们提出了一种名为FedD2S的新方法，用于个性化联邦学习（pFL），利用知识蒸馏。FedD2S在无数据知识蒸馏过程中结合了深到浅的层丢弃机制，以增强本地模型的个性化。通过在不同图像数据集（FEMNIST、CIFAR10、CINIC0和CIFAR100）上进行大量模拟，我们将FedD2S与最先进的FL基线进行了比较。所提出的方法表现出卓越性能，具有加速收敛和改善客户间公平性的特点。引入的层丢弃技术有效捕捉

    arXiv:2402.10846v1 Announce Type: cross  Abstract: This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively capture
    
[^18]: 基于GAN驱动的二维介质散射体电磁成像

    GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers

    [https://arxiv.org/abs/2402.10831](https://arxiv.org/abs/2402.10831)

    本文提出了一种基于生成对抗网络的深度学习方法，用于从多频散射电场的振幅准确高效地重建随机形状的二维介质物体，通过逆神经网络框架解决了非唯一性挑战。

    

    反漫射问题具有固有的挑战性，因为它们是逆问题且非线性的。本文提出了一种基于深度学习的强大方法，依赖生成对抗网络，从多频散射电场的振幅准确高效地重建任意形状的二维介质物体。通过训练对抗自编码器（AAE）来学习生成散射体几何形状，其约束为遵循高斯分布的低维潜在表示。构建了一个包含一系列适当设计的密集层、已训练的生成器以及单独训练的正向神经网络的内聚逆神经网络（INN）框架。通过将逆网络输出的图像与正向神经网络的输出进行比较验证重建的图像，解决了非唯一性挑战。

    arXiv:2402.10831v1 Announce Type: cross  Abstract: Inverse scattering problems are inherently challenging, given the fact they are ill-posed and nonlinear. This paper presents a powerful deep learning-based approach that relies on generative adversarial networks to accurately and efficiently reconstruct randomly-shaped two-dimensional dielectric objects from amplitudes of multi-frequency scattered electric fields. An adversarial autoencoder (AAE) is trained to learn to generate the scatterer's geometry from a lower-dimensional latent representation constrained to adhere to the Gaussian distribution. A cohesive inverse neural network (INN) framework is set up comprising a sequence of appropriately designed dense layers, the already-trained generator as well as a separately trained forward neural network. The images reconstructed at the output of the inverse network are validated through comparison with outputs from the forward neural network, addressing the non-uniqueness challenge inhe
    
[^19]: 通过度量学习的目标条件离线强化学习

    Goal-Conditioned Offline Reinforcement Learning via Metric Learning

    [https://arxiv.org/abs/2402.10820](https://arxiv.org/abs/2402.10820)

    通过度量学习的目标条件离线强化学习提出了一种新的方法来处理稀疏奖励、对称和确定性动作下的最优值函数近似，并展示了在学习次优离线数据集方面的显着优越性。

    

    在这项工作中，我们解决了在目标条件下离线强化学习中从次优数据集中学习最优行为的问题。为此，我们提出了一种新颖的方法来近似处理稀疏奖励、对称且确定性动作下的目标条件离线RL问题的最优值函数。我们研究了一种表示恢复优化的属性，并提出了导致该属性的新优化目标。我们使用学习到的值函数以演员-评论者的方式指导策略的学习，这种方法被我们称为MetricRL。在实验中，我们展示了我们的方法如何始终优于其他离线RL基线在从次优离线数据集中学习方面的表现。此外，我们展示了我们的方法在处理高维观测和多目标任务中的有效性。

    arXiv:2402.10820v1 Announce Type: new  Abstract: In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.
    
[^20]: 在模型的凸替代品的一致性和维度之间进行权衡

    Trading off Consistency and Dimensionality of Convex Surrogates for the Mode

    [https://arxiv.org/abs/2402.10818](https://arxiv.org/abs/2402.10818)

    通过在低维替代空间中的凸多面体顶点上嵌入结果，并探究单纯形中的一致性区域，权衡了替代损失维度、问题实例数量。

    

    在多类分类中，必须将结果嵌入到至少有$n-1$维的实数空间中，以设计一种一致的替代损失函数，这会导致"正确"的分类，而不受数据分布的影响。在信息检索和结构化预测任务等需要大量n时，优化n-1维替代常常是棘手的。我们研究了在多类分类中如何权衡替代损失维度、问题实例数量以及在单纯形上约束一致性区域的方法。我们跟随过去的研究，探讨了一种直观的嵌入过程，将结果映射到低维替代空间中的凸多面体的顶点上。我们展示了在每个点质量分布周围存在单纯形的全维子集，其中一致性成立，但是，少于n-1维度的情况下，存在一些分布，对于这些分布，一种现象性是

    arXiv:2402.10818v1 Announce Type: new  Abstract: In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the "correct" classification, regardless of the data distribution. For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomeno
    
[^21]: TernaryVote：具有差分隐私、通信高效性和拜占庭容忍特性的异构数据上的分布式优化

    TernaryVote: Differentially Private, Communication Efficient, and Byzantine Resilient Distributed Optimization on Heterogeneous Data

    [https://arxiv.org/abs/2402.10816](https://arxiv.org/abs/2402.10816)

    TernaryVote提出了一种结合了三值压缩器和大多数投票机制的算法，实现了差分隐私、梯度压缩和拜占庭容忍，相较于已有的方法有更好的隐私保证和收敛性。

    

    深度神经网络的分布式训练面临着隐私保护、通信效率和对故障和敌对行为的鲁棒性三个关键挑战。尽管已经有大量研究致力于分别解决这些挑战，但它们的综合仍未得到充分探讨。本文提出了TernaryVote，它结合了三值压缩器和大多数投票机制，同时实现了差分隐私、梯度压缩和拜占庭容忍。我们从新兴的f-差分隐私（DP）和所提出算法的拜占庭容忍角度对隐私保证进行了理论量化。特别是在隐私保证方面，与现有的基于符号的方法StoSign相比，所提出的方法改善了对梯度大小的维度依赖，并通过小批量抽样实现了隐私放大，同时确保了可比较的收敛性。

    arXiv:2402.10816v1 Announce Type: new  Abstract: Distributed training of deep neural networks faces three critical challenges: privacy preservation, communication efficiency, and robustness to fault and adversarial behaviors. Although significant research efforts have been devoted to addressing these challenges independently, their synthesis remains less explored. In this paper, we propose TernaryVote, which combines a ternary compressor and the majority vote mechanism to realize differential privacy, gradient compression, and Byzantine resilience simultaneously. We theoretically quantify the privacy guarantee through the lens of the emerging f-differential privacy (DP) and the Byzantine resilience of the proposed algorithm. Particularly, in terms of privacy guarantees, compared to the existing sign-based approach StoSign, the proposed method improves the dimension dependence on the gradient size and enjoys privacy amplification by mini-batch sampling while ensuring a comparable conver
    
[^22]: 特征空间中的联想记忆

    Associative Memories in the Feature Space

    [https://arxiv.org/abs/2402.10814](https://arxiv.org/abs/2402.10814)

    在特征空间中计算相似性以提升联想存储器模型的性能，并通过使用对比损失预训练网络来生成嵌入向量，从而提高检索速度。

    

    自联想存储器模型是一个函数，给定一组数据点，以任意向量作为输入，并输出与记忆集中最相似的数据点。然而，流行的存储模型在轻微损坏的情况下甚至无法检索图像，尽管这种损坏对人类评估者来说很容易检测。这是因为相似性是在原始像素空间中评估的，而原始像素空间不包含有关图像的任何语义信息。这个问题可以通过在嵌入空间而不是像素空间中计算相似性来轻松解决。我们展示了通过使用带有对比损失预训练的网络来计算这些嵌入的有效方法。由于嵌入空间的维度通常显着小于像素空间，我们还能更快地计算相似度分数。我们在复杂的数据集（如CIFAR10和STL10）上测试了这种方法。当前模型的另一个缺点是需要存储...

    arXiv:2402.10814v1 Announce Type: new  Abstract: An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing \emph{similarities} in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as CIFAR10 and STL10. An additional drawback of current models is the need of storing the
    
[^23]: 双重对偶：用于受限制强化学习的变分原始对偶策略优化

    Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning

    [https://arxiv.org/abs/2402.10810](https://arxiv.org/abs/2402.10810)

    本文提出了一种基于模型的算法，Variational Primal-Dual Policy Optimization (VPDPO)，通过实现Lagrangian和Fenchel对偶来将原始受限问题重构为无约束的原始-对偶优化，并且采用乐观原则更新原始变量和梯度上升更新对偶变量。

    

    我们研究受限凸马尔可夫决策过程（MDP），目标是最小化访问度量的凸泛函，受到凸约束的限制。为受限凸MDP设计算法面临几个挑战，包括（1）处理大状态空间，（2）管理探索/开拓的权衡，以及（3）解决约束优化，其中目标和约束都是访问度量的非线性函数。在这项工作中，我们提出了一种基于模型的算法，变分原始对偶策略优化（VPDPO），其中Lagrangian 和 Fenchel 对偶被用于将原始受限问题重新公式化为无限制原始-对偶优化。此外，原始变量通过基于模型的值迭代更新，遵循不确定性面前的乐观原则（OFU），而对偶变量则通过梯度上升更新。

    arXiv:2402.10810v1 Announce Type: new  Abstract: We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover,
    
[^24]: TimeSeriesBench：面向时间序列异常检测模型的工业级基准

    TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models

    [https://arxiv.org/abs/2402.10802](https://arxiv.org/abs/2402.10802)

    时间序列异常检测模型的工业级基准TimeSeriesBench填补了当前算法在训练范式、在线检测范式和评估标准方面与实际需求之间的差距。

    

    由于实际应用场景和规模的蔓延，时间序列异常检测（TSAD）引起了学术界和工业界的广泛兴趣。然而，与实际工业系统的需求相比，现有算法在训练范式、在线检测范式和评估标准方面存在差距。当前算法通常为每个单独的时间序列训练一个特定模型，然而在具有数以万计曲线的大规模在线系统中，维护这么多模型是不切实际的。仅使用一个统一模型来检测异常的性能尚不明确。大多数TSAD模型都是在时间序列的历史部分上进行训练，并在其未来部分上进行测试。然而，在分布式系统中，经常部署和升级系统，每天都会出现新的、以前没有见过的时间序列。使用历史数据所训练模型直接应用于新时间序列的性能也不明确。

    arXiv:2402.10802v1 Announce Type: new  Abstract: Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance o
    
[^25]: BlackJAX: JAX中的组合式贝叶斯推断

    BlackJAX: Composable Bayesian inference in JAX

    [https://arxiv.org/abs/2402.10797](https://arxiv.org/abs/2402.10797)

    BlackJAX是一个实现在JAX中组合式贝叶斯推断的库，采用函数式方法提高易用性、速度和模块化，适用于需要尖端方法、研究人员和想要了解工作原理的人。

    

    BlackJAX是一个库，实现了在贝叶斯计算中常用的抽样和变分推断算法。它通过采用函数式方法实现算法，旨在提高易用性、速度和模块化。BlackJAX使用Python编写，利用JAX在CPU、GPU和TPU上编译和运行类似Numpy的抽样器和变分方法。该库通过直接处理（非正则化）目标对数密度函数，与概率编程语言很好地集成。BlackJAX旨在成为基本统计“基元”的低级可组合实现的集合，可组合执行定义良好的贝叶斯推断，同时还提供高级例程以提高易用性。它面向需要尖端方法的用户、希望创建复杂抽样方法的研究人员，以及想要了解这些方法工作原理的人。

    arXiv:2402.10797v1 Announce Type: cross  Abstract: BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.
    
[^26]: 多样化集成：众包机器学习实验

    Diversified Ensembling: An Experiment in Crowdsourced Machine Learning

    [https://arxiv.org/abs/2402.10795](https://arxiv.org/abs/2402.10795)

    该论文提出了一种多样化集成的方法，允许参与者协作解决更广泛的问题，并能处理子组不公平性。

    

    arXiv:2402.10795v1 公告类型：新的 摘要：在诸如Kaggle等竞赛平台上的众包机器学习是一种常见且通常有效的生成准确模型的方法。通常，团队竞争获得最准确的模型，通过在一个保留数据集上的整体误差来衡量，往往在这样的比赛快结束时，排行榜前几名的团队会在平台机制之外集成或平均他们的模型，得到最终、最好的全局模型。在arXiv:2201.10408中，作者们在公平机器学习的背景下开发出了一种替代的众包框架，以便在子组不公平存在且可识别时将社区反馈整合到模型中。在那里，与经典的众包ML不同，参与者们通过专注于子问题，如服务于公平性的人口子群，有意地专门化他们的努力。在这里，我们对这项工作提出了更广泛的观点：我们注意到在这个框架内，参与者可以从事更广泛的问题解决活动，协作以组装子问题的专家选择，并为各种子问题设计特定的损失函数是如何协助让更广泛的问题得以解决并得到解决方案的。

    arXiv:2402.10795v1 Announce Type: new  Abstract: Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In arXiv:2201.10408, the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may b
    
[^27]: 掩码注意力是图的关键

    Masked Attention is All You Need for Graphs

    [https://arxiv.org/abs/2402.10793](https://arxiv.org/abs/2402.10793)

    提出了一种在图上学习的简单替代方法，称为掩码注意力（MAG），其利用注意力矩阵来创建定制的注意力模式，在长距离任务上表现出色并胜过其他方法。

    

    图神经网络（GNNs）和消息传递算法的变种主要用于在图上学习，这在很大程度上归功于它们的灵活性、速度和令人满意的性能。然而，设计强大而通用的GNNs需要大量的研究工作，通常依赖于精心选择的手工制作的消息传递操作符。受此启发，我们提出了一种在图上学习的非常简单的替代方法，它完全依赖于注意力。图被表示为节点或边集，并通过掩码注意权重矩阵来强制它们的连接，有效地为每个图创建定制的注意力模式。尽管其简单性，用于图的掩码注意力（MAG）在长距离任务上表现出色，并在55多个节点和图级任务上优于强消息传递基线和更复杂的基于注意力的方法。

    arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
    
[^28]: 在一个 1000 万根草垛中寻找针：循环记忆找到了语言模型不擅长的内容

    In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss

    [https://arxiv.org/abs/2402.10790](https://arxiv.org/abs/2402.10790)

    通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理长达 1000 万个元素的任务，这是迄今为止处理最长输入的开放神经网络模型，并展示了对长序列处理能力的显著改进。

    

    本文解决了使用生成式 Transformer 模型处理长文档的挑战。为了评估不同方法，我们引入了 BABILong，这是一个新的基准，旨在评估模型在提取和处理广泛文本中分布式事实方面的能力。我们的评估包括 GPT-4 和 RAG 的基准，结果显示常见方法仅适用于最多 $10^4$ 个元素的序列。相反，通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理涉及最多 $10^7$ 个元素的任务。这一成就标志着迄今为止任何开源神经网络模型处理的最长输入，显示了对长序列处理能力的显著改进。

    arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
    
[^29]: EdgeQAT: 熵和分布引导的量化感知训练，用于加速轻量级LLMs在边缘设备上的应用

    EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge

    [https://arxiv.org/abs/2402.10787](https://arxiv.org/abs/2402.10787)

    本文提出了EdgeQAT，使用熵和分布引导的量化感知训练方法来优化轻量级LLMs，在边缘设备上实现推理加速。

    

    尽管大型语言模型（LLMs）在各个领域取得了显著进展，但由于其庞大的参数和计算量，LLMs在边缘设备上的广泛应用受到限制。为了解决这一问题，通常采用量化方法生成具有高效计算和快速推理的轻量级LLMs。然而，后训练量化（PTQ）方法在将权重、激活和KV缓存一起量化至8位以下时，质量会急剧下降。此外，许多量化感知训练（QAT）工作对模型权重进行量化，而激活未被触及，这不能充分发挥量化对边缘端推理加速的潜力。在本文中，我们提出了EdgeQAT，即熵和分布引导的QAT，用于优化轻量级LLMs以实现在边缘设备上的推理加速。我们首先确定量化性能下降主要源自信息

    arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
    
[^30]: 错误反馈重新加载：从平方到平滑度常数的算术平均值

    Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants

    [https://arxiv.org/abs/2402.10774](https://arxiv.org/abs/2402.10774)

    本文研究了一种现代形式的错误反馈EF21，将其依赖的通信复杂度从平方平均值改进为更小的算术平均值，在实践中表现良好。

    

    错误反馈（EF）是一种非常流行且极其有效的机制，用于解决分布式训练方法（如分布式GD或SGD）中由于与贪婪通信压缩技术（如TopK）结合而产生的收敛问题。尽管EF提出已有近十年时间（Seide等人，2014年），并且尽管社区为推进对该机制的理论理解而集中努力，仍有很多尚待探索之处。在本文中，我们研究了一种名为EF21（Richtarik等人，2021年）的现代形式的错误反馈，它提供了目前已知的最佳理论保证，在最弱的假设下也在实践中运行良好。特别地，虽然EF21的理论通信复杂度取决于某些平滑度参数的平方均值，我们将这种依赖性改进为它们的算术平均值，后者始终更小，尤其是在...

    arXiv:2402.10774v1 Announce Type: cross  Abstract: Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in
    
[^31]: 政策学习在支持不足的离线动力学RL中的应用

    Policy Learning for Off-Dynamics RL with Deficient Support

    [https://arxiv.org/abs/2402.10765](https://arxiv.org/abs/2402.10765)

    该论文主要研究在离线动力学强化学习中如何应对源环境和目标环境之间的动态差异挑战。

    

    强化学习（RL）可以有效学习复杂的策略。然而，学习这些策略通常需要与环境进行大量的试错交互。在许多现实场景中，由于数据收集的高成本和安全问题，这种方法并不实际。因此，一个常见的策略是将在低成本、快速源模拟器中训练的策略转移到真实世界的目标环境。然而，这个过程存在挑战。无论模拟器多么先进，都不能完美复制真实世界的复杂性，导致源环境和目标环境之间存在动态差异。先前的研究提出，源领域必须包含所有可能的目标转换，这种条件我们称之为完全支持。然而，在预期完全支持往往是不切实际的，特别是在存在重大动态差异的情况下。在本文中，我们的重点转向了解决

    arXiv:2402.10765v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addres
    
[^32]: RAGIC: 面向股票区间构建的风险感知生成对抗模型

    RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction

    [https://arxiv.org/abs/2402.10760](https://arxiv.org/abs/2402.10760)

    提出了一种新型模型RAGIC，引入序列生成用于股票区间预测，利用生成对抗网络生成未来价格序列，通过风险模块和时间模块创建风险敏感区间。

    

    预测股市结果的努力取得了有限的成功，这是由市场固有的随机特性受许多不可预测因素影响造成的。许多现有的预测方法侧重于单点预测，缺乏有效决策所需的深度，经常忽视市场风险。为了弥补这一差距，我们提出了一种新颖的模型RAGIC，引入序列生成用于股票区间预测，更有效地量化不确定性。我们的方法利用生成对抗网络（GAN）生成未来价格序列，注入金融市场固有的随机性。RAGIC的生成器包括一个风险模块，捕捉熟悉投资者的风险感知，以及一个时间模块，考虑历史价格趋势和季节性。这个多方面的生成器通过统计推断呈现风险敏感区间的创建，融入时间

    arXiv:2402.10760v1 Announce Type: cross  Abstract: Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose a novel model, RAGIC, which introduces sequence generation for stock interval prediction to quantify uncertainty more effectively. Our approach leverages a Generative Adversarial Network (GAN) to produce future price sequences infused with randomness inherent in financial markets. RAGIC's generator includes a risk module, capturing the risk perception of informed investors, and a temporal module, accounting for historical price trends and seasonality. This multi-faceted generator informs the creation of risk-sensitive intervals through statistical inference, incorporating horizon
    
[^33]: 通过迭代后验抽样实现随机定位

    Stochastic Localization via Iterative Posterior Sampling

    [https://arxiv.org/abs/2402.10758](https://arxiv.org/abs/2402.10758)

    本论文提出了一种名为SLIPS的方法，通过迭代后验抽样实现随机定位，填补了从非标准化目标密度中抽样的问题的空白。

    

    建立在基于得分学习的基础上，近期对随机定位技术产生了新的兴趣。在这些模型中，人们通过随机过程（称为观测过程）为数据分布中的样本引入噪声，并逐渐学习与该动力学关联的去噪器。除了特定应用之外，对于从非标准化目标密度中抽样的问题，对随机定位的使用尚未得到广泛探讨。本项工作旨在填补这一空白。我们考虑了一个通用的随机定位框架，并引入了一类明确的观测过程，与灵活的去噪时间表相关联。我们提供了一种完整的方法论，即“通过迭代后验抽样实现随机定位”（SLIPS），以获得该动力学的近似样本，并作为副产品，样本来自目标分布。我们的方案基于马尔可夫链蒙特卡洛估计。

    arXiv:2402.10758v1 Announce Type: cross  Abstract: Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimati
    
[^34]: 朝向凝聚-公平-和谐：对比正则化在个体公平图聚类中的应用

    Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering

    [https://arxiv.org/abs/2402.10756](https://arxiv.org/abs/2402.10756)

    提出了一种对比公平正则化的个体公平非负矩阵三因子分解模型，能够实现平衡和凝聚的簇，并允许用户在精度和公平度之间进行权衡。

    

    传统的公平图聚类方法面临两个主要挑战：它们通过施加严格的约束优先考虑平衡的簇，而牺牲了簇的凝聚性；现有的个人和群体级公平方法在图分区中主要依赖于特征值分解，因此通常缺乏可解释性。为了解决这些问题，我们提出了iFairNMTF，这是一种具有对比公平正则化的个体公平非负矩阵三因子分解模型，实现了平衡和凝聚的簇。通过引入公平性正则化，我们的模型允许定制精度-公平度的权衡，从而增强了用户的自主权，而不会影响非负矩阵三因子分解提供的可解释性。在真实和合成数据集上的实验评估表明，iFairNMTF在实现公平性和聚类性能方面具有出色的灵活性。

    arXiv:2402.10756v1 Announce Type: cross  Abstract: Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters. By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance.
    
[^35]: 当数据流分析遇上大型语言模型

    When Dataflow Analysis Meets Large Language Models

    [https://arxiv.org/abs/2402.10754](https://arxiv.org/abs/2402.10754)

    这个研究提出了一个由大型语言模型驱动的数据流分析框架，可以分析任意代码片段，无需编译基础设施，并自动合成下游应用，有效解决数据流相关漏洞检测问题

    

    数据流分析是一种强大的代码分析技术，可以推断程序值之间的依赖关系，支持代码优化、程序理解和错误检测。本文介绍了LLMDFA，这是一个由LLM驱动的数据流分析框架，可以分析任意代码片段，无需编译基础设施，并自动合成下游应用。LLMDFA受基于摘要的数据流分析启发，将问题分解为三个子问题，并通过几种关键策略有效解决，包括少样本链式思维提示和工具合成。我们的评估表明，该设计可以减轻幻觉并提高推理能力，在检测基准测试中获取高精度和召回率

    arXiv:2402.10754v1 Announce Type: cross  Abstract: Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark
    
[^36]: 价值16个字的噪声节拍: 一种用于微控制器低功率心律失常分类的微型Transformer

    A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers

    [https://arxiv.org/abs/2402.10748](https://arxiv.org/abs/2402.10748)

    提出了一种微型Transformer模型，用于在低功耗微控制器上对心电图信号进行分析，仅需6k个参数，准确率达到98.97%，适用于识别MIT-BIH心律失常数据库中的5个最常见心律失常类别

    

    长期监测心血管疾病的可穿戴系统正在成为诊断和治疗中广泛应用且有价值的资产。一种用于实时分析心电图（ECG）信号，以及检测心脏状况（如心律失常）的有前途的方法是Transformer机器学习模型。该模型是用于时间序列分类的强大模型，但在可穿戴领域的高效实现却面临着重大的设计挑战，需要在兼顾足够精度和适当复杂度的情况下进行。在这项工作中，我们提出了一种用于分析ECG信号的微型Transformer模型，仅需要6k个参数，在MIT-BIH心律失常数据库中识别5个最常见心律失常类别时达到了98.97%的准确率，考虑到对低功耗微控制器设备进行高效执行所需的8位整数推理。我们探索了一种

    arXiv:2402.10748v1 Announce Type: cross  Abstract: Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy. A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the transformer machine learning model. Transformers are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity. In this work, we present a tiny transformer model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices. We explored an 
    
[^37]: 完全可微的拉格朗日卷积神经网络用于连续一致物理信息降水预报

    Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting

    [https://arxiv.org/abs/2402.10747](https://arxiv.org/abs/2402.10747)

    提出了一种完全可微的拉格朗日卷积神经网络模型，实现了物理信息与数据驱动学习相结合，在降水预报中表现优秀，为其他拉格朗日机器学习模型提供了新思路。

    

    本文提出了一种卷积神经网络模型，用于降水预报，结合了数据驱动学习和基于物理信息的领域知识。我们提出了LUPIN，即用于物理信息的拉格朗日双U-Net的现在预报，借鉴了现有的基于外推的预报方法，并以完全可微且GPU加速的方式实现了数据的拉格朗日坐标系转换，以允许实时端到端训练和推断。根据我们的评估，LUPIN与并超过了所选择基准的性能，为其他拉格朗日机器学习模型敞开了大门。

    arXiv:2402.10747v1 Announce Type: cross  Abstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.
    
[^38]: 通过风险分解实现严格适当评分规则的预测不确定性量化

    Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules

    [https://arxiv.org/abs/2402.10727](https://arxiv.org/abs/2402.10727)

    通过引入风险分解和适当评分规则，我们提出了一个通用框架来量化预测不确定性的不同来源，并澄清了它们之间的关系。

    

    在各个领域的预测模型应用中，区分预测不确定性的来源至关重要。尽管提出了许多不确定性度量，但并没有严格的定义来解开它们。此外，不同不确定性量化措施之间的关系仍然有些不清晰。在这项工作中，我们引入了一个根植于统计推理的通用框架，不仅允许创建新的不确定性度量，还澄清了它们之间的相互关系。我们的方法利用统计风险来区分aleatoric和epistemic不确定性成分，并利用适当的评分规则对其进行量化。为了使其在实践中易于处理，我们提出了在这一框架中整合贝叶斯推理的想法，并讨论了所提近似的性质。

    arXiv:2402.10727v1 Announce Type: cross  Abstract: Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation.
    
[^39]: 基于机器学习的水降载荷预测

    Machine Learning based Prediction of Ditching Loads

    [https://arxiv.org/abs/2402.10724](https://arxiv.org/abs/2402.10724)

    该研究提出了使用机器学习来预测飞机机身动态水降载荷的方法，通过结合卷积自编码器和长短期记忆网络或Koopman算子方法，取得了令人满意的预测结果。

    

    我们提出了使用机器学习来预测飞机机身动态水降载荷的方法。所采用的学习过程分为两部分，即使用卷积自编码器（CAE）重构空间载荷，以及在随后的部分中这些载荷的瞬时演化。评估了不同的CAE策略，并将其与长短期记忆（LSTM）网络或基于Koopman算子的方法相结合，以预测瞬时行为。训练数据是通过von-Karman和Wagner的动量方法的扩展编制的，训练方法的基本原理被简要总结。所涉及的应用是指DLR-D150飞机的全尺寸机身，包括一系列水平和垂直进场速度，入射角为6°。结果表明，对于所有四个研究的代理模型，采用LSTM结合...（内容缺失）

    arXiv:2402.10724v1 Announce Type: new  Abstract: We present approaches to predict dynamic ditching loads on aircraft fuselages using machine learning. The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a convolutional autoencoder (CAE) and the transient evolution of these loads in a subsequent part. Different CAE strategies are assessed and combined with either long short-term memory (LSTM) networks or Koopman-operator based methods to predict the transient behaviour. The training data is compiled by an extension of the momentum method of von-Karman and Wagner and the rationale of the training approach is briefly summarised. The application included refers to a full-scale fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach velocities at 6{\deg} incidence. Results indicate a satisfactory level of predictive agreement for all four investigated surrogate models examined, with the combination of an LSTM an
    
[^40]: 统一的可信集合预测器

    Conformalized Credal Set Predictors

    [https://arxiv.org/abs/2402.10723](https://arxiv.org/abs/2402.10723)

    本论文提出了一种利用一致预测方法来学习可信集合预测器的方法，能有效表示预测中的不确定性。

    

    可信集合是被视为不确定已知真实分布的候选概率分布集合。在机器学习中，由于可信集合能够表示预测中的不确定性，尤其是能够表示预测的aleatoric和epistemic不确定性，因此近来已经引起了关注。然而，设计用于学习可信集合预测器的方法仍然是一个具有挑战性的问题。在本文中，我们利用一致预测来解决这个问题。更具体地，我们提出了一种用于在分类任务中预测可信集合的方法，该方法利用标记为概率分布的训练数据。由于我们的方法继承了一致预测的覆盖性保证，我们的一致可信集合有很高的概率保证是有效的（而无需对模型或分布做出任何假设）。我们展示了我们的方法在自然语言中的适用性。

    arXiv:2402.10723v1 Announce Type: cross  Abstract: Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method to natural languag
    
[^41]: 与遗忘呼应的解除链接：简化GNN中的边解除

    Unlink to Unlearn: Simplifying Edge Unlearning in GNNs

    [https://arxiv.org/abs/2402.10695](https://arxiv.org/abs/2402.10695)

    研究揭示了GNN中边解除过程的关键问题，即过度遗忘现象，提出了解决方法来解决损失函数引起的问题。

    

    随着对数据隐私的担忧加剧，图神经网络（GNN）中的解除学习已经成为学术界一个突出的研究前沿。这一概念在强调被遗忘权利方面起着关键作用，包括在用户请求时有选择性地从已训练的GNN中删除特定数据。我们的研究关注边的解除学习，这一过程对现实应用特别相关，因为它具有广泛的适用性。目前的最先进方法如GNNDelete可以消除特定边的影响，然而我们的研究揭示了这些方法的一个关键局限，称为过度遗忘。当解除学习过程无意中除去超出特定数据的过多信息时，会导致对剩余边的预测准确性显著下降。为了解决这个问题，我们确定了GNNDelete的损失函数作为过度遗忘现象的主要来源。

    arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
    
[^42]: 探索精度和召回率以评估LLMs的质量和多样性

    Exploring Precision and Recall to assess the quality and diversity of LLMs

    [https://arxiv.org/abs/2402.10693](https://arxiv.org/abs/2402.10693)

    该研究提出了一种新的评估框架，将精度和召回率指标从图像生成转化为文本生成，细致评估了LLMs生成文本的质量和多样性，揭示了当前LLMs在生成任务中性能表现的重要见解。

    

    这篇论文介绍了一种针对大型语言模型（LLMs）如Llama-2和Mistral的新型评估框架，重点是将图像生成的精度和召回率指标转化为文本生成。这种方法允许对生成文本的质量和多样性进行细致评估，而无需对齐的语料库。通过对最先进的语言模型进行全面评估，研究揭示了它们在开放生成任务上的表现，这是传统基准无法充分捕捉的。研究结果突出了在模型利用人类反馈进行微调时，生成样本质量和多样性之间的权衡。这项工作扩展了基于分布的自然语言处理评估工具包，为当前LLMs在生成多样性和高质量文本方面面临的实际能力和挑战提供了见解。

    arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
    
[^43]: 不确定性、校准和成员推理攻击：信息论视角

    Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective

    [https://arxiv.org/abs/2402.10686](https://arxiv.org/abs/2402.10686)

    通过信息论框架分析了最先进的似然比攻击对不确定性、校准水平和数据集大小的影响，研究了成员推理攻击中隐含的风险

    

    在成员推理攻击（MIA）中，攻击者利用典型机器学习模型表现出的过度自信来确定特定数据点是否被用于训练目标模型。在本文中，我们在一个信息理论框架内分析了最先进的似然比攻击（LiRA）的性能，这个框架可以允许研究真实数据生成过程中的不确定性的影响，由有限训练数据集引起的认知不确定性以及目标模型的校准水平。我们比较了三种不同的设置，其中攻击者从目标模型接收到的信息逐渐减少：置信向量（CV）披露，其中输出概率向量被发布；真实标签置信度（TLC）披露，其中只有模型分配给真实标签的概率是可用的；以及决策集（DS）披露。

    arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
    
[^44]: 物理相关的MeshGraphNets（PI-MGNs）：适用于任意网格上非定态和非线性仿真的神经有限元求解器

    Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes

    [https://arxiv.org/abs/2402.10681](https://arxiv.org/abs/2402.10681)

    提出了物理相关的MeshGraphNets（PI-MGNs），可以在任意网格上进行非定态和非线性仿真，利用PINNs来减少对大量昂贵训练数据的依赖

    

    工程组件必须满足日益增长的技术需求，而且开发周期变得越来越短。为了应对这些挑战，需要一种整体化的方法，可以同时开发零件设计、材料系统和制造工艺。当前的方法使用数值仿真，然而对于迭代优化而言很快变得计算密集。数据驱动的机器学习方法可用于取代耗时和资源密集的数值仿真。具体而言，MeshGraphNets（MGNs）显示出令人满意的结果。它们可以在未知网格几何上进行快速准确的预测，同时对优化是完全可微的。然而，这些模型依赖于大量昂贵的训练数据，例如数值仿真。物理相关的神经网络（PINNs）提供了一种机会，使用偏微分方程而不是标记的数据来训练神经网络

    arXiv:2402.10681v1 Announce Type: cross  Abstract: Engineering components must meet increasing technological demands in ever shorter development cycles. To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process. Current approaches employ numerical simulations, which however quickly becomes computation-intensive, especially for iterative optimization. Data-driven machine learning methods can be used to replace time- and resource-intensive numerical simulations. In particular, MeshGraphNets (MGNs) have shown promising results. They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization. However, these models rely on large amounts of expensive training data, such as numerical simulations. Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled dat
    
[^45]: 多视图聚类在嵌套矩阵张量模型下的性能差异

    Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model

    [https://arxiv.org/abs/2402.10677](https://arxiv.org/abs/2402.10677)

    研究了嵌套矩阵张量模型中多视图聚类的性能差距，量化了张量方法和矩阵方法之间的性能差距，并发现了展开方法的算法阈值，展示了类似BBP过渡行为。

    

    我们研究了最近引入的嵌套矩阵张量模型中隐藏的植入信号的估计，该模型是经典尖峰秩一张量模型的扩展，受多视图聚类的启发。先前的工作在理论上研究了基于张量的方法的性能，该方法依赖于找到最佳秩一逼近，这是一个计算难题。一个可行的替代方法是计算观测到的张量数据展开的最佳秩一（矩阵）逼近，但其性能迄今为止未知。我们在这里量化了这两种方法之间的性能差距，特别是通过推导出展开方法的精确算法阈值，并展示它展现出类似BBP过渡行为。因此，这项工作与最近的贡献一致，这些贡献加深了我们对为什么张量方法优于矩阵方法的理解。

    arXiv:2402.10677v1 Announce Type: cross  Abstract: We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a BBP-type transition behavior. This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in 
    
[^46]: 使用事后置信度估计的选择性预测在语义分割中的性能及其在分布偏移下的表现

    Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift

    [https://arxiv.org/abs/2402.10665](https://arxiv.org/abs/2402.10665)

    本文研究了在低资源环境中语义分割的选择性预测，提出了一种针对语义分割量身定制的新型图像级置信度测量，并通过实验证明了其有效性

    

    语义分割在各种计算机视觉应用中扮演着重要角色，然而其有效性常常受到高质量标记数据的缺乏所限。为了解决这一挑战，一个常见策略是利用在不同种群上训练的模型，如公开可用的数据集。然而，这种方法导致了分布偏移问题，在兴趣种群上表现出降低的性能。在模型错误可能带来重大后果的情况下，选择性预测方法提供了一种减轻风险、减少对专家监督依赖的手段。本文研究了在资源匮乏环境下语义分割的选择性预测，着重于应用于在分布偏移下运行的预训练模型的事后置信度估计器。我们提出了一种针对语义分割量身定制的新型图像级置信度测量，并通过实验证明了其有效性。

    arXiv:2402.10665v1 Announce Type: new  Abstract: Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through expe
    
[^47]: 具有可学习核函数的线性变换器在上下文模型中表现更好

    Linear Transformers with Learnable Kernel Functions are Better In-Context Models

    [https://arxiv.org/abs/2402.10644](https://arxiv.org/abs/2402.10644)

    本文介绍了一种将线性变换器与受指数函数的Taylor展开启发的核函数和卷积网络相结合的模型，通过优化其In-Context Learning能力，取得了较好的效果。

    

    语言模型（LMs）的次二次体系结构的前沿是自然语言处理领域中不断发展的关键。我们提出一种改进的核函数，增强了其上下文学习能力，在Multi-Query Associative Recall任务和整体语言建模过程中得到了评估。

    arXiv:2402.10644v1 Announce Type: new  Abstract: Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demon
    
[^48]: 针对凹凸面上射流传热的预测代理模型

    A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface

    [https://arxiv.org/abs/2402.10641](https://arxiv.org/abs/2402.10641)

    调查了模型降阶和深度学习技术在预测冲击射流对凹凸面传热中的有效性，并引入了FFT-ANN和POD-LSTM两种方法来预测平均努塞数以及随机频率冲击情况下的局部传热速率。

    

    本文旨在全面调查各种模型降阶（MOR）和深度学习技术在预测冲击射流对凹凸面的传热方面的有效性。扩展了先前涉及脉冲圆形射流的实验和数值研究，本研究将评估用于不同射流特性下的传热的预测代理模型（PSM）。为此，本文介绍了两种预测方法，一种采用快速傅里叶变换增强人工神经网络（FFT-ANN）来预测在恒定频率情况下的平均努塞数。此外，研究引入了Proper Orthogonal Decomposition和Long Short-Term Memory（POD-LSTM）方法来处理随机频率的冲击射流。POD-LSTM方法被证明是一种在随机频率冲击情况下预测局部传热速率的强大解决方案。

    arXiv:2402.10641v1 Announce Type: cross  Abstract: This paper aims to comprehensively investigate the efficacy of various Model Order Reduction (MOR) and deep learning techniques in predicting heat transfer in a pulsed jet impinging on a concave surface. Expanding on the previous experimental and numerical research involving pulsed circular jets, this investigation extends to evaluate Predictive Surrogate Models (PSM) for heat transfer across various jet characteristics. To this end, this work introduces two predictive approaches, one employing a Fast Fourier Transformation augmented Artificial Neural Network (FFT-ANN) for predicting the average Nusselt number under constant-frequency scenarios. Moreover, the investigation introduces the Proper Orthogonal Decomposition and Long Short-Term Memory (POD-LSTM) approach for random-frequency impingement jets. The POD-LSTM method proves to be a robust solution for predicting the local heat transfer rate under random-frequency impingement scen
    
[^49]: ContiFormer：用于不规则时间序列建模的连续时间Transformer

    ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling

    [https://arxiv.org/abs/2402.10635](https://arxiv.org/abs/2402.10635)

    将vanilla Transformer的关系建模扩展到连续时间领域，提出了ContiFormer。

    

    在不规则时间序列上建模连续时间动态对于解释连续发生的数据演变和相关性至关重要。 传统方法包括循环神经网络或Transformer模型通过强大的神经架构利用归纳偏差来捕获复杂模式。然而，由于它们的离散特性，它们在泛化到连续时间数据范式方面存在局限性。 尽管神经常微分方程（Neural ODEs）及其变体在处理不规则时间序列方面表现出有前途的结果，但它们往往无法捕获这些序列内部复杂的相关性。 同时对输入数据点之间的关系进行建模并捕获连续时间系统的动态变化是具有挑战性但又需求迫切的。

    arXiv:2402.10635v1 Announce Type: cross  Abstract: Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, 
    
[^50]: 基于图的时空降采样缺失数据预测

    Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling

    [https://arxiv.org/abs/2402.10634](https://arxiv.org/abs/2402.10634)

    通过 hierarchical spatiotemporal downsampling 处理缺失数据问题，结合可解释的注意机制，实现对时空预测的有效建模

    

    给定一组与空间中传感器点相关联、具有相互关系的同步时间序列，时空预测问题包括为每个点预测未来观测值。时空图神经网络通过将时间序列表示为图来实现引人注目的结果。然而，大多数现有方法依赖于一个常常不切实际的假设，即输入始终可用，并且在数据部分缺失时无法捕捉隐藏的时空动态。在这项工作中，我们通过分层时空降采样来解决这个问题。输入时间序列随着时间和空间的推移逐渐粗化，获得一组捕捉异质时间和空间动态的表示。在观测值和缺失数据模式的条件下，通过一个可解释的注意机制来组合这些表示以生成

    arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
    
[^51]: 带有逻辑约束的多任务基于核心学习

    Multitask Kernel-based Learning with Logic Constraints

    [https://arxiv.org/abs/2402.10617](https://arxiv.org/abs/2402.10617)

    将逻辑约束融合到多任务核心学习中，提出了一种通用方法来转换逻辑陈述为连续实现，以实现核心谓词计算输出。

    

    本文提出了一个通用框架，将逻辑约束形式的先验知识整合到核心机器中的一组任务函数中。逻辑命题提供了环境的部分表示，学习算法利用它与监督样本中可用的信息。我们考虑了一个多任务学习方案，其中多个特征空间上的一元谓词要由核心机器学习，高级抽象表示由这些谓词的逻辑子句组成，已知对于任何输入都成立。我们提出了一种通用方法，将逻辑子句转换为连续实现，处理核心谓词计算的输出。学习任务被制定为损失函数的原始优化问题，其结合了测量监督样本拟合度的项

    arXiv:2402.10617v1 Announce Type: cross  Abstract: This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised ex
    
[^52]: 通过辩论调节LLMs以生成可控的具有争议性的声明

    Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements

    [https://arxiv.org/abs/2402.10614](https://arxiv.org/abs/2402.10614)

    本文通过辩论调节LLMs，使其生成可控的支持用户定义论点的声明，改进了LLMs的可控性，并提出了DEBATunE流程。通过两个LLMs之间的多轮辩论生成高质量的训练数据，以支持生成有更高质量和更突出的声明。

    

    LLMs代表不同的人群，尤其是少数群体，并产生支持其多样化甚至有争议观点的声明对于创造一个包容的环境至关重要。然而，现有的LLMs缺乏足够的控制性来支持生成内容的立场，其中往往包含不一致、中立或有偏见的声明。在本文中，我们改进了LLMs在生成支持用户在提示中定义的论点的声明时的可控性。我们发现两个持有相反立场的LLMs之间的多轮辩论产生了更高质量和更突出的声明，这些声明对于改善LLMs的可控性是重要的训练数据。受此启发，我们开发了一种新颖的Debate & Tuning（“DEBATunE”）流程，通过微调LLMs生成通过辩论获得的声明。为了检验DEBATunE，我们整理了迄今为止涵盖710个争议性主题的最大数据集。

    arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate & tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
    
[^53]: U$^2$MRPD: 通过大型潜在扩散模型引导的无监督MRI重建

    U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model

    [https://arxiv.org/abs/2402.10609](https://arxiv.org/abs/2402.10609)

    U$^2$MRPD是一个新颖的框架，通过大型潜在扩散模型引导，实现了无监督的欠采样MRI重建，能够支持图像特定的MRI重建，且在多个数据集上表现出与监督和MRI扩散方法相媲美甚至更好的性能。

    

    arXiv:2402.10609v1 公告类型: 跨领域 摘要: 在自然图像上预训练的大型潜在扩散模型(LLDM)中蕴含着丰富而假设上普遍适用于自然和医学图像的隐含视觉知识。为了测试这一假设，我们引入了一个新颖的框架，通过提示一个预训练的大型潜在扩散模型（U$^2$MRPD）进行无监督的欠采样MRI重建。现有的数据驱动、监督的欠采样MRI重建网络通常具有有限的泛化能力和适应性，不足以应对各种数据采集场景；然而，U$^2$MRPD通过使用量身定制的MRSampler，支持图像特定的MRI重建，该MRSampler适用于复值MRI图像。通过任何单一来源或多源MRI数据集，U$^2$MRPD的性能还可以通过MRAdapter进行进一步提升，同时保持生成图像先验不变。多个数据集上的实验表明，U$^2$MRPD实现了与监督和MRI扩散方法相媲美甚至更好的性能。

    arXiv:2402.10609v1 Announce Type: cross  Abstract: Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion metho
    
[^54]: 优化自适应实验：最小化后悔和最佳臂识别的统一方法

    Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification

    [https://arxiv.org/abs/2402.10592](https://arxiv.org/abs/2402.10592)

    提出了一种统一模型，同时考虑了实验内部性能和实验后结果，在优化大规模人群中的表现方面提供了尖锐理论，揭示了新颖的见解

    

    进行自适应实验的从业者通常面临两个竞争性优先级：通过在实验过程中有效地分配治疗来降低实验成本，以及迅速收集信息以结束实验并在整个人群中实施治疗。当前，文献意见分歧，有关最小化后悔的研究独立地处理前者的优先级，而有关最佳臂识别的研究则专注于后者。本文提出了一种统一模型，考虑到实验内部性能和实验后结果。我们随后提供了一个针对大规模人群的最佳性能的尖锐理论，将文献中的经典结果统一起来。这种统一还揭示了新的见解。例如，理论揭示了类似最近提出的顶部两个Thompson抽样算法等熟悉算法可被调整以优化广泛类别的目标。

    arXiv:2402.10592v1 Announce Type: new  Abstract: Practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population. Currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter. This paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes. We then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature. This unification also uncovers novel insights. For example, the theory reveals that familiar algorithms, like the recently proposed top-two Thompson sampling algorithm, can be adapted to optimize a broad class of obj
    
[^55]: 高效多任务不确定性用于联合语义分割和单目深度估计

    Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation

    [https://arxiv.org/abs/2402.10580](https://arxiv.org/abs/2402.10580)

    本研究将不同的不确定性量化方法与联合语义分割和单目深度估计相结合，介绍了一种新的学生-教师蒸馏方法 EMUFormer，揭示了多任务学习对不确定性质量的益处。

    

    预测不确定性的量化成为应对深度神经网络普遍挑战（如过度自信、缺乏解释性和鲁棒性）的可能解决方案，尽管它往往具有较高的计算成本。许多现实世界的应用都具有多模态性质，因此受益于多任务学习。例如，在自动驾驶中，语义分割和单目深度估计的联合解决方案已被证明具有重要价值。在这项工作中，我们首先将不同的不确定性量化方法与联合语义分割和单目深度估计相结合，并评估它们相互之间的表现。此外，我们揭示了与单独解决这两个任务相比，多任务学习在不确定性质量方面的益处。基于这些见解，我们引入了EMUFormer，一种新的学生-教师蒸馏方法，用于联合语义分割和单目深度估计。

    arXiv:2402.10580v1 Announce Type: cross  Abstract: Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular d
    
[^56]: 符号自编码用于自监督序列学习

    Symbolic Autoencoding for Self-Supervised Sequence Learning

    [https://arxiv.org/abs/2402.10575](https://arxiv.org/abs/2402.10575)

    符号自编码（$\Sigma$AE）是一个自监督框架，通过最小化重构损失和平行数据的监督损失来优化连接两个生成模型，实现了在转导任务上显著提升性能。

    

    传统语言模型擅长预测文本序列中的下一个标记，但在不同符号系统之间执行转导任务时通常会遇到困难，特别是在平行数据稀缺的情况下。为解决这一问题，我们引入了符号自编码（$\Sigma$AE），这是一个自监督框架，利用了丰富的不平行数据和有限的平行数据。$\Sigma$AE通过一个离散瓶颈层连接两个生成模型，并通过最小化重构损失（与平行数据的监督损失同时进行优化）进行端到端优化，使得离散瓶颈生成的序列可以被读取为转导的输入序列。我们还开发了基于梯度的方法，实现了尽管存在瓶颈离散性，仍能进行高效的自监督序列学习。我们的结果表明，$\Sigma$AE显著提高了转导任务的性能，即使使用了最少量的平行数据。

    arXiv:2402.10575v1 Announce Type: cross  Abstract: Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\Sigma$AE significantly enhances performance on transduction tasks, even with min
    
[^57]: 具有偏置的直接偏好优化

    Direct Preference Optimization with an Offset

    [https://arxiv.org/abs/2402.10571](https://arxiv.org/abs/2402.10571)

    提出了一种新颖的直接偏好优化方法，即具有偏置的DPO（ODPO），在微调过程中不同对待每个偏好对。

    

    直接偏好优化（DPO）是一种成功的微调策略，用于使大型语言模型与人类偏好保持一致，而无需训练奖励模型或使用强化学习。本文提出了一种DPO的泛化形式，称为具有偏置的DPO（ODPO），在微调过程中不将每个偏好对视为相等。

    arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
    
[^58]: 工业4.0时代中具有对话交互和计算机视觉的新型集成工业方法与合作机器人

    A novel integrated industrial approach with cobots in the age of industry 4.0 through conversational interaction and computer vision

    [https://arxiv.org/abs/2402.10553](https://arxiv.org/abs/2402.10553)

    论文提出一种创新愿景，认为在工业4.0时代，协作机器人（Cobot）、人工智能和人类之间将会有更紧密的协作，有助于提高工业自动化的效率和决策制定。

    

    从取代工人的机器人到作为有益同事的机器人，机器人自动化领域正在经历一种新的趋势，这对零部件制造商来说是一个巨大挑战。该论文贡献了一个创新愿景，看到协作机器人（Cobot）与人工智能世界以及人类之间愈发紧密的合作，Cobot能够精确执行特定的体力工作，人工智能能够分析信息并支持决策过程，而人类能够对未来有战略性的愿景。

    arXiv:2402.10553v1 Announce Type: cross  Abstract: From robots that replace workers to robots that serve as helpful colleagues, the field of robotic automation is experiencing a new trend that represents a huge challenge for component manufacturers. The contribution starts from an innovative vision that sees an ever closer collaboration between Cobot, able to do a specific physical job with precision, the AI world, able to analyze information and support the decision-making process, and the man able to have a strategic vision of the future.
    
[^59]: 使用辅助信息基于变压器的个性化癌症治疗药物识别器

    Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information

    [https://arxiv.org/abs/2402.10551](https://arxiv.org/abs/2402.10551)

    使用基于变压器的新方法，通过明确建模变异列表的顺序结构并利用辅助信息，实现了超越最先进DRP模型的性能表现

    

    癌症因其不断增长的临床和经济负担而仍然是全球性挑战。其独特的个性化表现使治疗变得困难，推动了个性化治疗策略的追求。因此，基因组学分析越来越成为临床诊断面板的一部分。有效利用这些面板需要准确的药物反应预测（DRP）模型，而由于有限的标记患者数据，建立这样的模型是具有挑战性的。以往解决这一问题的方法已经使用了各种形式的迁移学习。然而，它们并没有明确建模这些诊断面板中变异列表的可变长度的顺序结构。此外，它们也没有利用辅助信息（如患者生存）进行模型训练。我们通过一种新颖的基于变压器的方法来解决这些局限性，该方法在基准数据上超越了最先进的DRP模型的性能。我们还提出了一种设计...

    arXiv:2402.10551v1 Announce Type: new  Abstract: Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. We also present the design of a
    
[^60]: 通过受控合成学习解耦的音频表示

    Learning Disentangled Audio Representations through Controlled Synthesis

    [https://arxiv.org/abs/2402.10547](https://arxiv.org/abs/2402.10547)

    通过引入SynTone合成数据集，解决了音频表示学习中基准数据稀缺的问题，并提出了用于评估解耦技术的新方法。

    

    这篇论文解决了在解耦音频表示学习中基准数据稀缺的问题。我们引入了SynTone，一个具有明确地面实际解释因素的合成数据集，用于评估解耦技术。在SynTone上对最先进的方法进行基准测试突显了它在方法评估中的实用性。我们的结果强调了音频解耦的优势和局限性，激励未来的研究。

    arXiv:2402.10547v1 Announce Type: cross  Abstract: This paper tackles the scarcity of benchmarking data in disentangled auditory representation learning. We introduce SynTone, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques. Benchmarking state-of-the-art methods on SynTone highlights its utility for method evaluation. Our results underscore strengths and limitations in audio disentanglement, motivating future research.
    
[^61]: LLM生成的解释的特性和挑战

    Properties and Challenges of LLM-Generated Explanations

    [https://arxiv.org/abs/2402.10532](https://arxiv.org/abs/2402.10532)

    该研究探讨了大型语言模型生成的解释在多领域指导微调数据集上的特性，发现生成的解释表现出选择性和包含说明性元素，但较少是主观或误导性的。

    

    大型语言模型（LLMs）的自我合理化能力在限定环境中得到了探索，使用特定任务/数据集。然而，当前LLMs并不（仅）依赖于特定注释的数据；然而，它们经常解释它们的输出。生成的解释的特性受预训练语料库和用于指导微调的目标数据的影响。由于预训练语料库包含大量野外人类编写的解释，我们假设LLMs采用了人类解释的共同特性。通过分析多域指导微调数据集的输出，我们发现生成的解释表现出选择性并包含说明性元素，但很少是主观或误导性的。我们讨论了属性存在或缺失的原因和后果。特别是，我们概述了根据LLMs预训练语料库和微调数据的性质，这些属性存在或缺失的积极和消极影响。

    arXiv:2402.10532v1 Announce Type: cross  Abstract: The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations "in the wild", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the 
    
[^62]: LLM比较器：用于大型语言模型并行评估的可视化分析

    LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models

    [https://arxiv.org/abs/2402.10524](https://arxiv.org/abs/2402.10524)

    LLM Comparator是一种用于交互式分析自动并行评估结果的新型可视化工具，支持用户理解模型表现优劣和不同之处，解决了大型语言模型评估中的可扩展性和可解释性挑战。

    

    自动并行评估已成为评估大型语言模型（LLMs）响应质量的一种有前途的方法。然而，分析这种评估方法的结果存在可扩展性和可解释性挑战。本文提出了LLM比较器，这是一种新颖的可视化分析工具，用于交互式地分析自动并行评估结果。该工具支持用户进行交互式工作流，以了解为什么和何时模型比基准模型表现更好或更差，以及两个模型的响应在质量上有何不同。我们通过与一家大型科技公司的研究人员和工程师密切合作，迭代设计和开发了该工具。本文详细介绍了我们识别的用户挑战、该工具的设计和开发，以及定期评估其模型的参与者的观察研究。

    arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.
    
[^63]: 任意精度LLM：多个不同大小LLM的低成本部署

    Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs

    [https://arxiv.org/abs/2402.10517](https://arxiv.org/abs/2402.10517)

    任意精度LLM引入了一种轻量级方法，通过将不同大小LLMs量化为不同位宽（如3、4、...，n位）并叠加到内存中，显着降低了部署多个不同大小LLMs的高成本

    

    最近，人们对压缩大型语言模型（LLMs）进行了相当多的努力，这些LLMs在各种应用中展示了突破性的能力，但由于其庞大的体积而导致部署成本高昂。与此同时，尽管多个不同大小的LLMs部署的成本在实际意义上很重要，但却受到的关注较少。因此，本文引入了“任意精度LLM”，将任意精度DNN的概念扩展到LLMs。解决了任意精度LLM中的挑战，我们提出了一种轻量级的LLMs任意精度量化方法，利用后训练量化框架，并开发了一个专门的软件引擎来实现其有效的服务。结果，我们的解决方案通过将以不同位宽（如3、4、…，n位）量化的LLMs叠加到内存足印中，显着降低了部署多个不同大小的LLMs的高成本。

    arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri
    
[^64]: 控制可控蛋白质序列设计的生成AI：一项调查

    Generative AI for Controllable Protein Sequence Design: A Survey

    [https://arxiv.org/abs/2402.10516](https://arxiv.org/abs/2402.10516)

    人工智能领域的进步推动了蛋白质设计领域朝着前所未有的革命方向发展，这篇论文系统地审查了用于可控蛋白质序列设计的生成AI的最新进展。

    

    设计具有针对性功能的新型蛋白质序列是蛋白质工程中的一个核心主题，影响着药物发现和酶工程等各个领域。然而，由于时间和金融限制，导航这个庞大的组合搜索空间仍然是一个严峻挑战。随着人工智能领域的革命性进步，特别是生成模型和优化算法的突破性进展，这种情况正在迅速发展，推动蛋白质设计领域朝着前所未有的革命方向发展。在这项调查中，我们系统地审查了用于可控蛋白质序列设计的生成AI的最新进展。为了奠定基础，我们首先概述了蛋白质序列设计中涉及的约束性基本任务，并介绍了关键的生成模型和优化算法。然后，我们深入审查了每个设计任务，并讨论了相关应用。

    arXiv:2402.10516v1 Announce Type: cross  Abstract: The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finall
    
[^65]: 可以变压器预测振动吗？

    Can Transformers Predict Vibrations?

    [https://arxiv.org/abs/2402.10511](https://arxiv.org/abs/2402.10511)

    提出了一种新颖的基于Transformer的模型Resoformer，用于预测电动汽车传动轴上的扭振，解决了当前阻尼技术只能在共振发生后检测到的问题

    

    准确预测时间序列振动是电动汽车（EVs）的重要研究问题。EVs在崎岖地形上行驶时经常会产生振动，被称为扭振共振。这种由电机和轮胎振动之间的相互作用引起的共振会在车辆传动轴上施加过大负荷。然而，当前的阻尼技术仅在传动轴扭矩振动幅度达到一定阈值后才能检测到共振，导致在检测时传动轴上承受重要负荷。在本研究中，我们提出了一种新颖的方法来解决这一问题，引入了Resoformer，一种用于预测扭振的基于transformer的模型。Resoformer利用电机转速的时间序列作为输入，并在输入序列之后的特定分位数处预测传动轴扭振的幅度。通过计算递归和卷积之间的注意力

    arXiv:2402.10511v1 Announce Type: cross  Abstract: Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs). EVs often experience vibrations when driving on rough terrains, known as torsional resonance. This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle's drive shaft. However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection. In this study, we propose a novel approach to address this issue by introducing Resoformer, a transformer-based model for predicting torsional resonance. Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series. By calculating the attention between recursive and convolutio
    
[^66]: 二次Littlewood-Offord问题的弹性

    Resilience of the quadratic Littlewood-Offord problem

    [https://arxiv.org/abs/2402.10504](https://arxiv.org/abs/2402.10504)

    论文研究了二次Littlewood-Offord问题的统计鲁棒性，估计了对抗性噪声对二次Radamecher混沌的影响，并提供了对二次和双线性Rademacher混沌的统计鲁棒性的下限估计。

    

    我们研究了高维数据的统计鲁棒性。我们的结果提供了关于对抗性噪声对二次Radamecher混沌$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$反集中特性的影响的估计，其中$M$是一个固定的（高维）矩阵，$\boldsymbol{\xi}$是一个共形Rademacher向量。具体来说，我们探讨了$\boldsymbol{\xi}$能够承受多少对抗性符号翻转而不“膨胀”$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$，从而“去除”原始分布导致更“有粒度”和对抗性偏倚的分布。我们的结果为二次和双线性Rademacher混沌的统计鲁棒性提供了下限估计；这些结果在关键区域被证明是渐近紧的。

    arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
    
[^67]: 通过神经网络推断$M_B$的晚期转变

    Late-time transition of $M_B$ inferred via neural networks

    [https://arxiv.org/abs/2402.10502](https://arxiv.org/abs/2402.10502)

    本研究通过神经网络对绝对星等$M_B$进行约束，并发现在$z\approx 1$区域存在转折红移迹象

    

    强化宇宙参数之间的紧张关系导致对标准宇宙学基本方面的重新考虑。哈勃常数的紧张也可以被视为局部和早期宇宙对Ia型超新星绝对星等$ M_B $的约束之间的紧张。在这项工作中，我们以一种与模型无关的方式重新考虑了该参数的变化可能性。我们利用神经网络来无偏地限制绝对星等值，并评估与Pantheon+汇编中$ M_B $随红移变化的影响和统计显著性，以及对神经网络架构的彻底分析。我们发现了一个在$ z\approx 1 $区域的转折红移的迹象。

    arXiv:2402.10502v1 Announce Type: cross  Abstract: The strengthening of tensions in the cosmological parameters has led to a reconsideration of fundamental aspects of standard cosmology. The tension in the Hubble constant can also be viewed as a tension between local and early Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In this work, we reconsider the possibility of a variation of this parameter in a model-independent way. We employ neural networks to agnostically constrain the value of the absolute magnitude as well as assess the impact and statistical significance of a variation in $M_B$ with redshift from the Pantheon+ compilation, together with a thorough analysis of the neural network architecture. We find an indication for a transition redshift at the $z\approx 1$ region.
    
[^68]: 通过主动偏好优化实现经验证的样本效率的RLHF

    Provably Sample Efficient RLHF via Active Preference Optimization

    [https://arxiv.org/abs/2402.10500](https://arxiv.org/abs/2402.10500)

    通过Active Preference Optimization算法，在Bradley-Terry-Luce偏好模型下实现了RLHF的样本效率提高，优化了对提示收集偏好数据的策略。

    

    强化学习从人类反馈（RLHF）在将大型语言模型（LLMs）与人类偏好相一致方面至关重要。虽然这些对齐的生成模型已经在各种任务中展示出令人印象深刻的能力，但是依赖高质量的人类偏好数据在实际RLHF实施中构成了昂贵的瓶颈。因此，需要更好和自适应的数据收集策略。为此，我们将RLHF以上下文偏好赌博机问题的形式框定，其中提示作为上下文，并表明通过随机选择提示收集偏好数据的天真方式导致一个在奖励方面具有$\Omega(1)$次优性差距的策略。然后，我们提出了$\textit{Active Preference Optimization}$（$\texttt{APO}$）算法，该算法积极选择提示以收集偏好数据。在Bradley-Terry-Luce（BTL）偏好模型下，\texttt{APO}实现了样本效率，而不会妥协于polic

    arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
    
[^69]: 发展一种预测小麦枯叶病严重程度的最佳模型（阿尔西和巴勒区案例研究）

    Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)

    [https://arxiv.org/abs/2402.10492](https://arxiv.org/abs/2402.10492)

    通过比较三种不同的人工神经网络方法，研究发现通用回归神经网络（GRNN）在预测小麦枯叶病严重程度方面表现出有效性，并需要较少的训练时间。

    

    这项研究利用了三种人工神经网络（ANN）方法，分别是具有不同训练、传输、分割和学习功能的反向传播神经网络（BPNN），径向基函数神经网络（RBFNN）和通用回归神经网络（GRNN），来预测枯叶病的严重程度。考虑了参数如平均最高温度、平均最低温度、平均降雨量、平均气温、平均相对湿度和不同小麦品种。统计分析表明，GRNN表现出有效的预测能力，并且相比其他模型需要更少的训练时间。此外，结果表明总季节降雨量对小麦枯叶病的发展有积极影响。

    arXiv:2402.10492v1 Announce Type: cross  Abstract: This research utilized three types of artificial neural network (ANN) methodologies, namely Backpropagation Neural Network (BPNN) with varied training, transfer, divide, and learning functions; Radial Basis Function Neural Network (RBFNN); and General Regression Neural Network (GRNN), to forecast the severity of stem rust. It considered parameters such as mean maximum temperature, mean minimum temperature, mean rainfall, mean average temperature, mean relative humidity, and different wheat varieties. The statistical analysis revealed that GRNN demonstrated effective predictive capability and required less training time compared to the other models. Additionally, the results indicated that total seasonal rainfall positively influenced the development of wheat stem rust.   Keywords: Wheat stem rust, Back propagation neural network, Radial Basis Function Neural Network, General Regression Neural Network.
    
[^70]: 针对多维时间序列预测的随机投影层

    Random Projection Layers for Multidimensional Time Sires Forecasting

    [https://arxiv.org/abs/2402.10487](https://arxiv.org/abs/2402.10487)

    提出了一种全MLP时间序列预测架构RPMixer，通过将随机投影层集成到模型中，增加了块输出之间的多样性，提高了整体性能

    

    多层感知器（MLP）混合模型已被证明对时间序列预测问题有效。然而，当将此类模型应用于高维时间序列（例如空间-时间数据集中的时间序列）时，由于过拟合问题，其性能可能会下降。本文提出了一种全MLP时间序列预测架构，称为RPMixer。我们的方法利用了深度神经网络的集成式行为，其中网络中的每个单独块的作用类似于集成模型中的基本学习器，特别是在引入身份映射残差连接时。通过将随机投影层集成到我们的模型中，我们增加了块输出之间的多样性，从而提高了RPMixer的整体性能。对大规模空间-时间预测基准数据集进行的大量实验表明，我们提出的方法胜过了

    arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
    
[^71]: 理解带有标签噪音的多类别分类中的自蒸馏和部分标签学习

    Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise

    [https://arxiv.org/abs/2402.10482](https://arxiv.org/abs/2402.10482)

    自蒸馏在多类别分类中扮演着标签平均化的角色，有助于模型关注与特定实例相关的特征簇以预测标签，但随着蒸馏轮次增加，性能会降低。此外，在标签噪声情景下自蒸馏被证明是有效的，找到了实现100%分类准确率所需的最小蒸馏轮次。

    

    自蒸馏（SD）是使用教师模型的输出训练学生模型的过程，两个模型共享相同的架构。我们的研究从理论上考察了使用交叉熵损失的多类别分类中的SD，探索了多轮SD和具有精炼教师输出的SD，这些灵感来自部分标签学习（PLL）。通过推导学生模型输出的封闭形式解，我们发现SD本质上是在具有高特征相关性的实例之间进行标签平均。最初有益的平均化有助于模型专注于与给定实例相关联的特征簇以预测标签。然而，随着蒸馏轮次的增加，性能会下降。此外，我们展示了SD在标签噪声情景中的有效性，并确定实现100%分类准确率所需的标签损坏条件和最小蒸馏轮次数。

    arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur
    
[^72]: 基于表情符号的加密资产市场反应

    Emoji Driven Crypto Assets Market Reactions

    [https://arxiv.org/abs/2402.10481](https://arxiv.org/abs/2402.10481)

    该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。

    

    在加密货币领域，诸如Twitter之类的社交媒体平台已经成为影响市场趋势和投资者情绪的关键因素。在我们的研究中，我们利用GPT-4和经过微调的基于BERT模型的多模态情感分析，重点关注表情符号情绪对加密货币市场的影响。通过将表情符号转化为可量化的情感数据，我们将这些见解与BTC价格和VCRIX指数等关键市场指标进行了相关联。这种方法可以用于开发旨在利用社交媒体元素识别和预测市场趋势的交易策略。关键是，我们的研究结果表明，基于表情符号情绪的策略可以有助于避免重大市场下挫，并有助于回报的稳定。这项研究强调了将先进的基于人工智能的分析整合到金融策略中的实际益处，并提供了一种新的方式来看待市场预测。

    arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
    
[^73]: CodaMal：低成本显微镜下的疟疾检测的对比域自适应

    CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes

    [https://arxiv.org/abs/2402.10478](https://arxiv.org/abs/2402.10478)

    提出了CodaMal框架，实现了低成本显微镜下疟疾检测的对比域自适应，解决了HCM和LCM图像之间的域差异问题

    

    疟疾是全球重大健康问题，其诊断需要可扩展的解决方案，能够有效地处理低成本显微镜(LCM)下的显微图像。基于深度学习的方法在从显微图像中进行计算机辅助诊断方面取得了成功。然而，这些方法需要标注的显现出受疟疾寄生虫影响的细胞及其生命周期阶段的图像。与从高成本显微镜(HCM)中标注图像相比，从LCM中标注图像显著增加了医学专家的负担。因此，一个实际的解决方案应该在HCM图像上训练，能够在LCM图像上测试时具有良好的泛化能力。在本作品中，我们提出了一个名为CodaMal（对比域自适应用于疟疾检测）的端到端学习框架。

    arXiv:2402.10478v1 Announce Type: cross  Abstract: Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive Domain Adpation for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a domain adap
    
[^74]: 通过异常检测探究正态流模型的可能性和图像复杂度

    Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection

    [https://arxiv.org/abs/2402.10477](https://arxiv.org/abs/2402.10477)

    通过实验证明较简单的图像在正态流模型中得到更高可能性，揭示了对该现象的潜在机制，并提出了图像复杂度作为独立变量来解决这一问题。

    

    异常检测对于安全关键的机器学习应用至关重要，并得到了广泛研究。本文重点解释了深度生成模型（DGM）常常将未知的OOD输入分配更高的可能性，而不是它们已知的训练数据的潜在机制。我们提出了一个假设，即较简单的图像在潜在空间的高密度区域集中，导致在正态流（NF）中分配更高的可能性。我们通过实验证明了该假设在五种NF体系结构中的有效性，得出它们的可能性是不可信的结论。此外，我们展示了这个问题可以通过将图像复杂度作为一个独立变量来缓解。最后，我们提供了一个...

    arXiv:2402.10477v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial to safety-critical machine learning applications and has been extensively studied. While recent studies have predominantly focused on classifier-based methods, research on deep generative model (DGM)-based methods have lagged relatively. This disparity may be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods to unknown OOD inputs than to their known training data. This paper focuses on explaining the underlying mechanism of this phenomenon. We propose a hypothesis that less complex images concentrate in high-density regions in the latent space, resulting in a higher likelihood assignment in the Normalizing Flow (NF). We experimentally demonstrate its validity for five NF architectures, concluding that their likelihood is untrustworthy. Additionally, we show that this problem can be alleviated by treating image complexity as an independent variable. Finally, we provi
    
[^75]: 极小化优化中交替更新的基本利益

    Fundamental Benefit of Alternating Updates in Minimax Optimization

    [https://arxiv.org/abs/2402.10475](https://arxiv.org/abs/2402.10475)

    Alt-GDA算法被证明更快，提出了交替外推GDA（Alex-GDA）通用算法框架，以轮流从迭代的外推中获取梯度。

    

    Gradient Descent-Ascent（GDA）算法旨在解决极小化优化问题，其中采用下降和上升步骤，分为同时进行（Sim-GDA）或交替进行（Alt-GDA）。 Alt-GDA通常收敛更快，但两者之间的性能差距尚未在理论上得到很好的理解，尤其是在全局收敛速率方面。为了解决这种理论与实践之间的差距，我们针对强凸强凹和Lipschitz梯度目标提出了对两种算法的细粒度收敛分析。我们的Alt-GDA的新迭代复杂性上界严格小于Sim-GDA的下界；即Alt-GDA被证明更快。此外，我们提出了交替外推GDA（Alex-GDA），这是一个包含Sim-GDA和Alt-GDA的通用算法框架，其主要思想是轮流从迭代的外推中获取梯度。我们展示了Alex-GDA满足更小的迭代复杂度。

    arXiv:2402.10475v1 Announce Type: cross  Abstract: The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller it
    
[^76]: 一位量化和稀疏化用于多类线性分类的正则化回归

    One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression

    [https://arxiv.org/abs/2402.10474](https://arxiv.org/abs/2402.10474)

    通过正则化回归，在超参数化范围内，根据特定选择的凸函数并适当增加一个正则化项，可以实现稀疏和一位解决方案，其性能几乎与最佳分类性能相同。

    

    我们研究了在线性回归中用于多类分类的问题，这些问题在超参数化范围内，训练数据中一些标记错误。在这种情况下，为了避免过度拟合错误标记的数据，需要添加一个显式的正则化项，$\lambda f(w)$，其中$f(\cdot)$是某个凸函数。在我们的分析中，我们假设数据是从一个具有相等类大小的高斯混合模型中采样的，并且每个类别的训练标签中有一部分比例为$c$是错误的。在这些假设下，我们证明了当$f(\cdot) = \|\cdot\|^2_2$且$\lambda \to \infty$时，可以获得最佳的分类性能。然后我们继续分析了在大$\lambda$范围内$f(\cdot) = \|\cdot\|_1$和$f(\cdot) = \|\cdot\|_\infty$的分类错误，并且注意到通常可以找到稀疏和一位解决方案，分别表现几乎与$f(\cdot) = \|\cdot\|^2_2$相同。

    arXiv:2402.10474v1 Announce Type: new  Abstract: We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\lambda f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and $\lambda \to \infty$. We then proceed to analyze the classification errors for $f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large $\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\cdot) = \|\
    
[^77]: 隐私与公平：信息混淆用于带有局部差分隐私的公平表示学习

    Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy

    [https://arxiv.org/abs/2402.10473](https://arxiv.org/abs/2402.10473)

    该研究引入了信息瓶颈(IB)方法和局部差分隐私(LDP)相结合的信息混淆方法，以填补隐私与公平之间关系的理论研究空白。

    

    随着机器学习在人类中心应用中变得越来越普遍，算法公平性和隐私保护日益受到重视。本研究旨在填补这一空白，在公平性机器学习中引入了信息瓶颈（IB）方法和局部差分隐私（LDP）相结合的信息混淆方法，以理论框架全面考察隐私与公平之间的相互关系。

    arXiv:2402.10473v1 Announce Type: new  Abstract: As machine learning (ML) becomes more prevalent in human-centric applications, there is a growing emphasis on algorithmic fairness and privacy protection. While previous research has explored these areas as separate objectives, there is a growing recognition of the complex relationship between privacy and fairness. However, previous works have primarily focused on examining the interplay between privacy and fairness through empirical investigations, with limited attention given to theoretical exploration. This study aims to bridge this gap by introducing a theoretical framework that enables a comprehensive examination of their interrelation. We shall develop and analyze an information bottleneck (IB) based information obfuscation method with local differential privacy (LDP) for fair representation learning. In contrast to many empirical studies on fairness in ML, we show that the incorporation of LDP randomizers during the encoding proce
    
[^78]: 对从对抗性扰动中学习的理论理解

    Theoretical Understanding of Learning from Adversarial Perturbations

    [https://arxiv.org/abs/2402.10470](https://arxiv.org/abs/2402.10470)

    研究提供了一个理论框架，表明各种对抗性扰动（甚至是几个像素的扰动）包含足够的类特征用于泛化，进一步揭示了从扰动中学习时的决策边界

    

    尚未完全理解为什么对抗性示例可以欺骗神经网络并在不同网络之间传递。为了阐明这一点，几项研究假设，尽管对抗性扰动看似是噪音，但实际上包含类特征。这得到了通过实证证据支持，即对于在错误标记的对抗性示例上训练的网络仍然可以很好地推广到正确标记的测试样本。然而，对于扰动如何包含类特征并促进泛化的理论理解是有限的。在本研究中，我们提供了一个理论框架，用于理解通过在相互正交样本上训练的单隐藏层网络从扰动中学习。我们的结果突显了各种对抗性扰动，甚至是几个像素的扰动，均包含足够的类特征用于泛化。此外，我们揭示了从扰动学习时的决策边界

    arXiv:2402.10470v1 Announce Type: new  Abstract: It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations m
    
[^79]: 具有对抗课程图对比学习的成对增强

    Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation

    [https://arxiv.org/abs/2402.10468](https://arxiv.org/abs/2402.10468)

    提出一种对抗课程图对比学习（ACGCL）框架，利用成对增强生成具有可控相似性的图级正负样本，同时通过子图对比学习来识别有效的图模式

    

    图对比学习（GCL）已成为图表示学习领域中的一个关键技术。我们提出了一种创新的框架：对抗课程图对比学习（ACGCL），利用成对增强的优点生成具有可控相似性的图级正负样本，以及子图对比学习来识别其中的有效图模式。

    arXiv:2402.10468v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) has emerged as a pivotal technique in the domain of graph representation learning. A crucial aspect of effective GCL is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative graph patterns. To address this challenge, we propose an innovative framework: Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender graph-level positive and negative samples with controllable similarity, alongside subgraph contrastive learning to discern effective graph patterns therein. Within the ACGCL framework, we have devised a novel adversarial curriculum training methodology that facilitates progressive learning by se
    
[^80]: FedKit：实现安卓和iOS平台上的跨平台联邦学习

    FedKit: Enabling Cross-Platform Federated Learning for Android and iOS

    [https://arxiv.org/abs/2402.10464](https://arxiv.org/abs/2402.10464)

    FedKit是一个专为安卓和iOS设备上的跨平台联邦学习研究设计的系统，支持模型转换、硬件加速训练和跨平台模型聚合，以促进持续模型交付和训练。

    

    我们提出了FedKit，一个专为安卓和iOS设备上的跨平台联邦学习(FL)研究量身定制的FL系统。FedKit通过实现模型转换、硬件加速训练和跨平台模型聚合，推动了跨平台FL开发。我们的FL工作流支持生产中灵活的机器学习操作(MLOps)，促进持续模型交付和训练。我们已在大学校园的健康数据分析实际用例中部署了FedKit，展示了其效果。FedKit是开源的，网址为https://github.com/FedCampus/FedKit。

    arXiv:2402.10464v1 Announce Type: new  Abstract: We present FedKit, a federated learning (FL) system tailored for cross-platform FL research on Android and iOS devices. FedKit pipelines cross-platform FL development by enabling model conversion, hardware-accelerated training, and cross-platform model aggregation. Our FL workflow supports flexible machine learning operations (MLOps) in production, facilitating continuous model delivery and training. We have deployed FedKit in a real-world use case for health data analysis on university campuses, demonstrating its effectiveness. FedKit is open-source at https://github.com/FedCampus/FedKit.
    
[^81]: QDyLoRA: 高效的大型语言模型调优的量化动态低秩适应

    QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning

    [https://arxiv.org/abs/2402.10462](https://arxiv.org/abs/2402.10462)

    本论文提出了一种名为QDyLoRA的高效量化动态低秩适应方法，能够在大型语言模型的预定义秩上实现有效微调，与QLoRA相竞争，并且在采用其最佳秩时表现更好。

    

    Finetuning大型语言模型需要巨大的GPU内存，限制了获取更大模型的选择。虽然命名为QLoRA的低秩适应技术的量化版本显著缓解了这一问题，但是找到高效的LoRA秩仍然具有挑战性。此外，QLoRA是在预定义的秩上训练的，因此，在不需要进一步微调步骤的情况下无法重新配置为其较低的秩。本文提出了QDyLoRA-Quantized Dynamic Low-Rank Adaptation-，作为一种用于动态低秩适应的高效量化方法。受Dynamic LoRA的启发，QDyLoRA能够在一组预定义的LoRA秩上有效地微调LLMs。通过一轮微调，QDyLoRA能够在单个32 GB V100-GPU上为1到64个秩的Falcon-40b进行微调。实验结果表明，QDyLoRA与QLoRA具有竞争力，在使用其最佳秩时表现优越。

    arXiv:2402.10462v1 Announce Type: cross  Abstract: Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.
    
[^82]: 学习增强型跳表

    Learning-Augmented Skip Lists

    [https://arxiv.org/abs/2402.10457](https://arxiv.org/abs/2402.10457)

    通过将机器学习建议与跳表设计整合，提出了一种学习增强型跳表，能够实现最优期望搜索时间，在处理搜索查询时具有显著改进。

    

    我们研究了将机器学习建议整合到跳表设计中，以改进传统数据结构设计。通过访问可能有误的预测分数频率的预测值的预言神谕，我们构建了一个跳表，可以证明提供最佳的期望搜索时间，几乎有二倍的优势。事实上，我们的学习增强型跳表仍然是最佳的，即使神谕只在常数因子内准确。我们表明，如果搜索查询遵循普遍存在的Zipfian分布，那么我们的跳表对于一个项目的期望搜索时间仅为一个常数，与项目总数n无关，即O(1)，而传统的跳表的期望搜索时间为O(log n)。我们还展示了我们的数据结构的鲁棒性，通过展示我们的数据结构实现了一个期望搜索时间，

    arXiv:2402.10457v1 Announce Type: cross  Abstract: We study the integration of machine learning advice into the design of skip lists to improve upon traditional data structure design. Given access to a possibly erroneous oracle that outputs estimated fractional frequencies for search queries on a set of items, we construct a skip list that provably provides the optimal expected search time, within nearly a factor of two. In fact, our learning-augmented skip list is still optimal up to a constant factor, even if the oracle is only accurate within a constant factor. We show that if the search queries follow the ubiquitous Zipfian distribution, then the expected search time for an item by our skip list is only a constant, independent of the total number $n$ of items, i.e., $\mathcal{O}(1)$, whereas a traditional skip list will have an expected search time of $\mathcal{O}(\log n)$. We also demonstrate robustness by showing that our data structure achieves an expected search time that is wi
    
[^83]: 通过惩罚最优输运网络对表格数据进行生成建模

    Generative Modeling for Tabular Data via Penalized Optimal Transport Network

    [https://arxiv.org/abs/2402.10456](https://arxiv.org/abs/2402.10456)

    提出了一种名为POTNet的生成建模网络，基于边缘惩罚的Wasserstein损失，能够有效地建模同时包含分类和连续特征的表格数据。

    

    准确学习表格数据中行的概率分布并生成真实的合成样本的任务既关键又非平凡。Wasserstein生成对抗网络(WGAN)在生成建模中取得了显著进展，解决了其前身生成对抗网络所面临的挑战。然而，由于表格数据中存在混合数据类型和多模态性，生成器和鉴别器之间的微妙平衡以及Wasserstein距离在高维度中的固有不稳定性，WGAN通常无法生成高保真样本。因此，我们提出了POTNet（惩罚最优输运网络），这是一种基于新颖、强大且可解释的边际惩罚Wasserstein（MPW）损失的生成深度神经网络。POTNet能够有效地建模包含分类和连续特征的表格数据。

    arXiv:2402.10456v1 Announce Type: cross  Abstract: The task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable improvement in generative modeling, addressing the challenges faced by its predecessor, generative adversarial network. However, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of Wasserstein distance in high dimensions, WGAN often fails to produce high-fidelity samples. To this end, we propose POTNet (Penalized Optimal Transport Network), a generative deep neural network based on a novel, robust, and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can effectively model tabular data containing both categorical and continuous features. Moreover, it offers the flexibil
    
[^84]: PRISE：将时间动作抽象视为序列压缩问题

    PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem

    [https://arxiv.org/abs/2402.10450](https://arxiv.org/abs/2402.10450)

    将时间动作抽象视为序列压缩问题，使用Primitive Sequence Encoding (PRISE)方法结合连续动作量化与BPE来学习强大的动作抽象，并在多任务模仿学习和少样本模仿学习中取得显著性能提升

    

    时间动作抽象以及信念状态表示是序贯决策中的强大知识共享机制。本文提出了一个新颖的观点，将诱导时间动作抽象视为序列压缩问题。为此，我们将LLM训练流水线的一个微妙但至关重要的组成部分 -- 输入标记化通过字节对编码（BPE） -- 带到了连续控制领域中学习可变时间跨度技能的 seemingly distant 任务。我们引入一种称为Primitive Sequence Encoding（PRISE）的方法，该方法将连续动作量化与BPE相结合，学习强大的动作抽象。我们通过实验证明，PRISE从一组机器人操作演示中发现的高级技能显著提升了多任务模仿学习以及在未见任务上的少样本模仿学习的性能。我们的代码将在 https: 放出

    arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/
    
[^85]: 增量序列标记：两种转变的故事

    Incremental Sequence Labeling: A Tale of Two Shifts

    [https://arxiv.org/abs/2402.10447](https://arxiv.org/abs/2402.10447)

    提出了一种名为IS3的框架，旨在解决增量序列标记任务中的E2O和O2E两种重要的语义转变，通过使用知识蒸馏来维持对旧实体的判别能力。

    

    增量序列标记任务涉及在保留对先前类别知识的同时，随时间不断学习新类别。我们的研究确定了两种重要的语义转变：E2O（模型将旧实体错误标记为非实体）和O2E（模型将非实体或旧实体标记为新实体）。先前的研究主要集中在解决E2O问题上，忽视了O2E问题。这种忽略导致模型在学习过程中对新数据样本进行分类时存在偏见，认为它们属于新类别。为了解决这些挑战，我们提出了一种新颖的框架，即无语义转变的增量顺序标记（IS3）。受到已确定的语义转变（E2O和O2E）的启发，IS3旨在缓解模型中的灾难性遗忘。至于E2O问题，我们使用知识蒸馏来维持模型对旧实体的判别能力。

    arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
    
[^86]: 使用不同标注函数的协作学习

    Collaborative Learning with Different Labeling Functions

    [https://arxiv.org/abs/2402.10445](https://arxiv.org/abs/2402.10445)

    研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。

    

    我们研究了一种 Collaborative PAC Learning 的变体，在这种情况下，我们旨在学习每个$n$个数据分布的准确分类器，同时最小化从它们总共抽取的样本数量。与通常的协作学习设置不同，不假设存在一个同时对所有分布准确的单一分类器。我们表明，当数据分布满足较弱的可实现性假设时，仍然可以实现高效的学习。我们给出了一种基于经验风险最小化(ERM)的学习算法，应用于假设类的一个自然增强，分析依赖于对该增强类的VC维的上界。

    arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
    
[^87]: 时间序列对比学习的参数增广

    Parametric Augmentation for Time Series Contrastive Learning

    [https://arxiv.org/abs/2402.10434](https://arxiv.org/abs/2402.10434)

    通过信息理论分析时间序列数据增广，总结最常用的增广方法。

    

    现代技术如对比学习已经在许多领域得到有效应用，包括计算机视觉、自然语言处理和图结构数据。在对比学习方法中，创建有助于模型学习健壮和具有区分性表示的正例是一个关键阶段。通常，预设的人类直觉指导相关数据增广的选择。然而，由于人类容易识别的模式，此经验法则在视觉和语言领域表现良好。然而，在时间序列中视觉检查时间结构是不切实际的。数据集和实例级别的时间序列增广的多样性使得难以即时选择有意义的增广。在本研究中，我们通过使用信息理论分析时间序列数据增广，并以统一格式总结最常采用的增广来填补这一差距。

    arXiv:2402.10434v1 Announce Type: new  Abstract: Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. In this study, we address this gap by analyzing time series data augmentation using information theory and summarizing the most commonly adopted augmentations in a unified format. We
    
[^88]: 将神经与物理融合：用可处理的模拟增强蛋白质构象采样

    Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations

    [https://arxiv.org/abs/2402.10433](https://arxiv.org/abs/2402.10433)

    本研究探索了预训练生成采样器在少样本情况下结合MD模拟的方法，以提高蛋白质构象采样的准确性和效率。

    

    蛋白质动力学对于它们的生物功能和性质非常普遍且重要，研究通常涉及耗时的分子动力学(MD)模拟。最近，生成模型被利用作为一个替代采样器，以比传统方法快几个数量级地获得构象集合，而且不需要任何模拟数据（“零次推断”）。然而，由于不考虑底层能量景观，这种生成模型的准确性仍然可能受到限制。在这项工作中，我们探索了这种预训练生成采样器的少样本设置，它以一种可处理的方式结合了MD模拟。具体而言，对于一个目标蛋白质，我们首先从预训练采样器中获取一些种子构象，然后从这些种子样本开始进行一系列物理模拟。然后我们通过模拟轨迹对生成模型进行微调。

    arXiv:2402.10433v1 Announce Type: cross  Abstract: The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) simulations in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any simulation data (a "zero-shot" inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the few-shot setting of such pre-trained generative sampler which incorporates MD simulations in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical simulations in parallel starting from these seeding samples. Then we fine-tuned the generative model using the simulation trajectories a
    
[^89]: 在贝叶斯设置中的固定置信度最佳臂识别问题

    Fixed Confidence Best Arm Identification in the Bayesian Setting

    [https://arxiv.org/abs/2402.10429](https://arxiv.org/abs/2402.10429)

    该研究在贝叶斯设置中探讨了固定置信度最佳臂识别问题，证明了传统频率设定下的算法在此设置下表现次优，并引入了一种性能与理论下限相匹配的连续排除变种。

    

    我们考虑了贝叶斯设置中的固定置信度最佳臂识别（FC-BAI）问题。该问题旨在在已知先验采样的情况下以固定置信水平找到均值最大的臂。大多数关于FC-BAI问题的研究都是在频率设定中进行的，在该设定下，游戏开始前即确定了赌博模型。我们证明了在贝叶斯设置中，传统的在频率设定中研究的FC-BAI算法（如track-and-stop和top-two算法）会导致任意次优的表现。我们同时证明了在贝叶斯设置下预期样本数的下限，并引入了一种连续排除的变种，其性能与下限相匹配，最多差一个对数因子。仿真验证了理论结果。

    arXiv:2402.10429v1 Announce Type: cross  Abstract: We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian Setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the Bayesian setting. We also prove a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.
    
[^90]: DABS-LS: 使用区域水平集自监督的深度基于图谱分割

    DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision

    [https://arxiv.org/abs/2402.10425](https://arxiv.org/abs/2402.10425)

    本研究提出了一种使用深度基于图谱的IAC分割网络，通过将IAC和ANFs预先定位在单个图谱中进行训练，从而准确推断ANFs的位置。

    

    齿周植入物（CIs）是用于治疗严重至深度听力丧失患者的神经假体。对CIs刺激听神经纤维（ANFs）进行患者特异性建模可以帮助听力学家改善CI编程。这些模型需要定位ANFs相对于周围解剖结构和CI的位置。定位具有挑战性，因为ANFs非常小，在临床成像中不能直接看到。在这项工作中，我们假设ANFs的位置可以从内听道（IAC）的位置准确推断出来，在CT中，IAC具有很高的对比度，因为ANFs在耳蜗和大脑之间通过这个管道。受VoxelMorph启发，在本文中，我们提出了一种基于深度图谱的IAC分割网络。我们创建一个单一的图谱，其中IAC和ANFs已经被预定位。我们的网络被训练以生成将坐标从图谱映射到新目标的变形场（DFs）。

    arXiv:2402.10425v1 Announce Type: cross  Abstract: Cochlear implants (CIs) are neural prosthetics used to treat patients with severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of the auditory nerve fiber (ANFs) can help audiologists improve the CI programming. These models require localization of the ANFs relative to surrounding anatomy and the CI. Localization is challenging because the ANFs are so small they are not directly visible in clinical imaging. In this work, we hypothesize the position of the ANFs can be accurately inferred from the location of the internal auditory canal (IAC), which has high contrast in CT, since the ANFs pass through this canal between the cochlea and the brain. Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC segmentation network. We create a single atlas in which the IAC and ANFs are pre-localized. Our network is trained to produce deformation fields (DFs) mapping coordinates from the atlas to new target
    
[^91]: 通过专家加权来衡量和减少LLM在没有黄金标准答案的情况下的虚构

    Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting

    [https://arxiv.org/abs/2402.10412](https://arxiv.org/abs/2402.10412)

    提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。

    

    LLM幻觉，即生成事实不正确但看似令人信服的答案，目前是LLM可信度和可靠性的主要威胁。解决这一复杂问题的第一步是对其进行衡量。然而，现有的幻觉度量标准需要具有具有黄金标准答案的基准数据集，即人类编写的“最佳”或“正确”答案。这种要求使幻觉测量成本高昂，并容易出现人为误差。在这项工作中，我们提出了通过加权LLM对事实性进行评估（FEWL），这是第一个专门为金标准答案缺失时设计的幻觉度量标准。FEWL利用了现成的LLM答案作为黄金标准答案的代理。关键挑战是如何有效地量化参考LLM的专业知识。我们展示FEWL具有一定的理论保证，并在实证中证明它更准确。度量虚构。

    arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
    
[^92]: 通过图表示学习理解大型语言模型调查论文分类法

    Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning

    [https://arxiv.org/abs/2402.10409](https://arxiv.org/abs/2402.10409)

    通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。

    

    随着大型语言模型（LLMs）的新研究持续进行，难以跟上新的研究和模型。为帮助研究人员综合新研究成果，许多人写了调研论文，但即使这些论文也变得越来越多。本文提出了一种自动将调研论文分配到分类法的方法。我们收集了144篇LLM调研论文的元数据，并探讨了三种范例来对分类法内的论文进行分类。我们的工作表明，在共类别图上利用图结构信息可以显著优于两个范例中的语言模型; 使用LLMs进行预训练语言模型的微调和零-shot/few-shot分类。我们发现我们的模型超过了平均人类识别水平，并且利用较小模型生成的弱标签来微调LLMs（本研究中的GCN等）可能比使用地面实况标签更有效，揭示了从弱到强的潜力。

    arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
    
[^93]: 从分段三线性网络中导出多面体复合体

    Polyhedral Complex Derivation from Piecewise Trilinear Networks

    [https://arxiv.org/abs/2402.10403](https://arxiv.org/abs/2402.10403)

    本文以三线性插值方法作为位置编码，提出了理论见解和分析网格提取方法，将高维曲面转换为平面，并引入了一种近似交点的方法，拓展了更广泛的应用。

    

    最近关于深度神经网络可视化的进展揭示了它们结构的见解，并且可以从连续分段仿射（CPWA）函数中提取网格。与此同时，神经表面表示学习的发展包括非线性位置编码，解决了诸如谱偏差之类的问题；然而，这在应用基于CPWA函数的网格提取技术方面带来了挑战。我们聚焦于三线性插值方法作为位置编码，提供了理论见解和分析的网格提取，展示了在奇拿尔约束下将高维曲面转换为三线性区域内的平面的过程。此外，我们引入了一种方法来近似三个高维曲面之间的交点，从而扩展了更广泛的应用。通过汉明距离和效率以及角距离来经验性地验证正确性和简洁性，同时检查了t之间的相关性

    arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
    
[^94]: ManiFPT: 定义和分析生成模型的指纹

    ManiFPT: Defining and Analyzing Fingerprints of Generative Models

    [https://arxiv.org/abs/2402.10401](https://arxiv.org/abs/2402.10401)

    本文明确定了生成模型中的工件和指纹的定义，并提出了计算它们的算法，发现使用该定义可以显著提高识别潜在生成过程的性能。

    

    最近的研究表明，生成模型在生成的样本上留下了它们潜在生成过程的痕迹，广泛称为生成模型的指纹，并研究了它们在检测真实图像和合成图像方面的实用性。然而，这些指纹能够区分各种类型的合成图像以及帮助识别潜在生成过程的程度仍未得到充分探讨。尤其是，据我们所知，指纹的定义仍不清楚。为此，在这项工作中，我们明确定义了生成模型中的工件和指纹，提出了一种实际计算它们的算法，并最终研究了它在区分大量不同生成模型方面的有效性。我们发现使用我们提出的定义可以显著提高从样本中识别潜在生成过程的性能。

    arXiv:2402.10401v1 Announce Type: new  Abstract: Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model
    
[^95]: LogELECTRA：自监督异常检测的非结构化日志

    LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs

    [https://arxiv.org/abs/2402.10397](https://arxiv.org/abs/2402.10397)

    LogELECTRA是一个新的日志异常检测模型，通过对单行日志消息进行深入分析，解决了基于模板出现模式的检测方法存在的局限性和可能导致不必要延迟的问题。

    

    系统日志是维护软件系统的一些最重要信息，近年来软件系统变得更加庞大和复杂。基于日志的异常检测的目标是通过分析短时间内生成的大量日志自动检测系统异常，在现实世界中这是一个关键挑战。先前的研究使用日志解析器从非结构化日志数据中提取模板，并根据模板出现的模式检测异常。这些方法对于具有未知模板的日志存在局限性。此外，由于大多数日志异常被认为是点异常而不是上下文异常，基于出现模式的检测方法可能导致不必要的延迟。在本文中，我们提出了LogELECTRA，一种新的日志异常检测模型，它基于自监督异常更深入地分析单行日志消息。

    arXiv:2402.10397v1 Announce Type: new  Abstract: System logs are some of the most important information for the maintenance of software systems, which have become larger and more complex in recent years. The goal of log-based anomaly detection is to automatically detect system anomalies by analyzing the large number of logs generated in a short period of time, which is a critical challenge in the real world. Previous studies have used a log parser to extract templates from unstructured log data and detect anomalies on the basis of patterns of the template occurrences. These methods have limitations for logs with unknown templates. Furthermore, since most log anomalies are known to be point anomalies rather than contextual anomalies, detection methods based on occurrence patterns can cause unnecessary delays in detection. In this paper, we propose LogELECTRA, a new log anomaly detection model that analyzes a single line of log messages more deeply on the basis of self-supervised anomaly
    
[^96]: 针对事件序列数据的预训练算法

    Pretext Training Algorithms for Event Sequence Data

    [https://arxiv.org/abs/2402.10392](https://arxiv.org/abs/2402.10392)

    提出了针对事件序列数据的自监督预训练框架，通过引入新颖的对齐验证任务，构建了适用于事件序列的基础表示，可以推广到不同的下游任务和数据领域。

    

    预训练后进行特定任务微调在视觉和语言领域取得了成功。本文提出了一个针对事件序列数据定制的自监督预训练框架。我们引入了一项针对事件序列特化的新型对齐验证任务，借鉴了掩码重构和对比学习中的良好实践。我们的预训练任务可以释放基础表示，这些表示可以推广到不同的下游任务，包括用于时间点过程模型的下一事件预测，事件序列分类和缺失事件插值。对流行的公共基准数据集上的实验表明，所提出的方法在不同任务和数据领域上具有潜力。

    arXiv:2402.10392v1 Announce Type: cross  Abstract: Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.
    
[^97]: MFBind：一种用于在实际生成建模中评估药物化合物的多保真度方法

    MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling

    [https://arxiv.org/abs/2402.10387](https://arxiv.org/abs/2402.10387)

    MFBind提出了一种多保真度方法，通过集成对接和结合自由能模拟器，使用主动学习训练多保真度深度代理模型，实现了在准确性和计算成本之间的最佳权衡。

    

    当前药物发现的生成模型主要使用分子对接来评估生成的化合物的质量。然而，这样的模型在实践中通常并不实用，因为即使得分数高的化合物也不一定始终具有实验活性。存在更准确的活性预测方法，如基于分子动力学的结合自由能计算，但它们在生成模型中使用起来计算成本太高。我们提出了一种多保真度方法，Multi-Fidelity Bind（MFBind），以实现准确性和计算成本之间的最佳权衡。MFBind集成了对接和结合自由能模拟器，通过主动学习训练多保真度深度代理模型。我们的深度代理模型利用了一种预训练技术和线性预测头，以高效拟合少量高保真度数据。我们进行了大量实验证明MFBind（1）表现超越benchmark模型。

    arXiv:2402.10387v1 Announce Type: cross  Abstract: Current generative models for drug discovery primarily use molecular docking to evaluate the quality of generated compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. We propose a multi-fidelity approach, Multi-Fidelity Bind (MFBind), to achieve the optimal trade-off between accuracy and computational cost. MFBind integrates docking and binding free energy simulators to train a multi-fidelity deep surrogate model with active learning. Our deep surrogate model utilizes a pretraining technique and linear prediction heads to efficiently fit small amounts of high-fidelity data. We perform extensive experiments and show that MFBind (1) outperfor
    
[^98]: 子图级通用提示调整

    Subgraph-level Universal Prompt Tuning

    [https://arxiv.org/abs/2402.10380](https://arxiv.org/abs/2402.10380)

    设计了一种可适用于任何预训练策略的简单提示调整方法，通过位于输入图特征空间内的功能来实现，从而增加了其在各种下游应用中的通用性

    

    在不断发展的机器学习领域，通过提示调整来调整预训练模型的适应性变得日益突出。这一趋势在图领域特别明显，不同的预训练策略为为图神经网络开发有效的基于提示的调整方法提供了独特的挑战。之前的方法受到限制，主要针对具有边预测预训练任务的模型定制了专门的提示函数。然而，这些方法在不同预训练策略之间缺乏泛化能力。最近，设计了一种简单的提示调整方法，可适用于任何预训练策略，在输入图的特征空间内发挥作用。这使其从理论上可以模拟任何类型的提示函数，从而显著提高了其在一系列下游应用中的通用性。

    arXiv:2402.10380v1 Announce Type: cross  Abstract: In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to ful
    
[^99]: DataDreamer: 一种用于合成数据生成和可复现LLM工作流程的工具

    DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows

    [https://arxiv.org/abs/2402.10379](https://arxiv.org/abs/2402.10379)

    DataDreamer是一种用于合成数据生成和可复现LLM工作流程的开源Python库，有助于研究人员实现强大的LLM工作流，提倡开放科学和可重现性。

    

    大型语言模型（LLMs）已成为自然语言处理研究人员在各种任务中的主要工具。如今，许多研究人员在合成数据生成、任务评估、微调、提炼以及其他与模型相关的研究工作流中使用LLMs。然而，使用这些模型时会遇到挑战，这些挑战源于它们的规模、闭源性质以及缺乏针对这些新兴工作流的标准化工具。这些模型的迅速崛起和这些独特挑战对开放科学和使用它们的工作的可重现性产生了直接的负面影响。在本文中，我们介绍了DataDreamer，这是一个开源Python库，使研究人员能够编写简单的代码来实现强大的LLM工作流。DataDreamer还帮助研究人员遵循我们提出的最佳实践，以鼓励开放科学和可重现性。该库和文档可在h网站上找到。

    arXiv:2402.10379v1 Announce Type: new  Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at h
    
[^100]: 用稀疏线性概念嵌入（SpLiCE）解释CLIP

    Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)

    [https://arxiv.org/abs/2402.10376](https://arxiv.org/abs/2402.10376)

    本研究提出了一种新方法，Sparse Linear Concept Embeddings（SpLiCE），通过将CLIP表示转换为人可解释概念的稀疏线性组合，实现了对CLIP嵌入的解释。

    

    CLIP嵌入在各种计算机视觉任务中表现出色，但这些高维稠密向量表示并不容易解释，限制了它们在需要透明度的下游应用中的实用性。本文经验性地展示了CLIP的潜在空间高度结构化，因此可以将CLIP表示分解为其潜在语义组件。我们利用这一理解提出了一种新方法，稀疏线性概念嵌入（SpLiCE），用于将CLIP表示转换为人可解释概念的稀疏线性组合。与先前的工作不同，SpLiCE不需要概念标签，并且可以后期应用。通过对多个真实世界数据集进行广泛实验，我们验证了SpLiCE输出的表示可以解释甚至取代传统的密集CLIP表示。

    arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati
    
[^101]: 重新审视经验重放条件

    Revisiting Experience Replayable Conditions

    [https://arxiv.org/abs/2402.10374](https://arxiv.org/abs/2402.10374)

    本文提出了更严格的“经验重放条件”，并揭示了政策改进的不稳定性因素，从而导出了相应的稳定化技巧，最终使得经验重放可应用于优势演员-评论家算法。

    

    深度强化学习中使用的经验重放（ER）被认为仅适用于离策略算法。然而，已经有一些情况下将ER应用于在策略算法中，这表明离策略性可能是应用ER的充分条件。本文重新审视了更严格的“经验重放条件”（ERC），并提出了修改现有算法以满足ERC的方法。为此，假设政策改进的不稳定性是ERC的关键。从度量学习的角度揭示了不稳定因素，包括i）来自负样本的排斥力和ii）不当经验的重放。因此，导出了相应的稳定化技巧。通过数值模拟验证，所提出的稳定化技巧使得ER适用于优势演员-评论家算法，一种在策略算法。此外，其学习

    arXiv:2402.10374v1 Announce Type: new  Abstract: Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict "experience replayable conditions" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning 
    
[^102]: BioMistral：面向医学领域的开源预训练大型语言模型集合

    BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains

    [https://arxiv.org/abs/2402.10373](https://arxiv.org/abs/2402.10373)

    BioMistral是一种面向生物医学领域的开源预训练大型语言模型集合，在医学问答任务中表现出优越性能并具有竞争优势。

    

    大型语言模型（LLMs）近年来展示出卓越的多功能性，为医疗保健和医学等专业领域提供潜在应用。尽管有各种针对健康领域定制的开源LLMs可用，但将通用LLMs调整到医学领域仍面临重大挑战。本文介绍了BioMistral，一种专为生物医学领域量身定制的开源LLM，采用Mistral作为基础模型，并在PubMed Central上进一步进行预训练。我们在包含10个已建立的英文医学问答（QA）任务的基准上对BioMistral进行了全面评估。我们还探讨通过量化和模型合并方法获得的轻量级模型。我们的结果表明，BioMistral相较于现有开源医学模型具有优越性能，并与专有对手具有竞争优势。最后，为了解决

    arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
    
[^103]: 学习性是一种紧凑性质

    Learnability is a Compact Property

    [https://arxiv.org/abs/2402.10360](https://arxiv.org/abs/2402.10360)

    监督学习问题的困难性具有紧凑的有限特性表征。

    

    最近关于学习的工作取得了一个引人注目的结果：各种问题的可学习性可能是不可判定的，或者与标准集合论ZFC公理无关。此外，这种问题的可学习性可能不是具有有限特性的属性：非正式地说，它不能通过检查问题的有限投影来检测。

    arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
    
[^104]: 我们能否用软提示LLMs来进行图学习任务？

    Can we soft prompt LLMs for graph learning tasks?

    [https://arxiv.org/abs/2402.10359](https://arxiv.org/abs/2402.10359)

    引入了GraphPrompter框架，通过软提示将图信息与LLMs对齐，以进一步探究LLMs理解图信息的潜力。

    

    图在表示社交网络、生物数据和引用网络等现实世界应用中的复杂关系方面起着重要作用。最近，大型语言模型（LLMs）在各个领域取得了巨大成功，这使得将LLMs应用于图表格尤为诱人。然而，直接将LLMs应用于图表格形式存在独特挑战，因为图表格形式与文本形式之间存在差异和不匹配。因此，为了进一步探究LLMs理解图信息的潜力，我们引入了GraphPrompter，这是一个通过软提示来将图信息与LLMs对齐的新颖框架。具体而言，GraphPrompter包括两个主要组件：一个图神经网络用于编码复杂的图信息，以及一个能够有效处理文本信息的LLM。在不同基准数据集上进行了广泛实验，涵盖了节点分类和链接预测任务。

    arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
    
[^105]: 通过Langevin MCMC在黎曼流形上高效采样

    Efficient Sampling on Riemannian Manifolds via Langevin MCMC

    [https://arxiv.org/abs/2402.10357](https://arxiv.org/abs/2402.10357)

    通过Langevin MCMC在黎曼流形上高效采样，并证明了在特定条件下迭代步数为$\tilde{O}(\epsilon^{-2})$时，Langevin MCMC的迭代会与目标分布在$\epsilon$-Wasserstein距离内。

    

    我们研究了通过（几何）Langevin MCMC在黎曼流形$M$上高效地从Gibbs分布$d \pi^* = e^{-h} d {vol}_g$中采样的任务；该算法涉及在随机高斯方向上计算指数映射，在实践中可以高效实现。我们对Langevin MCMC的分析的关键在于对几何Euler-Murayama方案的离散化误差做出的界限，假设$\nabla h$是Lipschitz的，且$M$具有有界的曲率截面。我们的误差界限与欧几里得Euler-Murayama在步长依赖性方面的误差相匹配。结合Kendall-Cranston耦合下对几何Langevin扩散的收缩保证，我们证明Langevin MCMC迭代在经过$\tilde{O}(\epsilon^{-2})$步后与$\pi^*$之间的$\epsilon$-Wasserstein距离内，这与欧几里得Langevin MCMC的迭代复杂度相匹配。我们的结果适用于一般设置，其中$h$可以是非凸的。

    arXiv:2402.10357v1 Announce Type: cross  Abstract: We study the task of efficiently sampling from a Gibbs distribution $d \pi^* = e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin MCMC; this algorithm involves computing exponential maps in random Gaussian directions and is efficiently implementable in practice. The key to our analysis of Langevin MCMC is a bound on the discretization error of the geometric Euler-Murayama scheme, assuming $\nabla h$ is Lipschitz and $M$ has bounded sectional curvature. Our error bound matches the error of Euclidean Euler-Murayama in terms of its stepsize dependence. Combined with a contraction guarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling, we prove that the Langevin MCMC iterates lie within $\epsilon$-Wasserstein distance of $\pi^*$ after $\tilde{O}(\epsilon^{-2})$ steps, which matches the iteration complexity for Euclidean Langevin MCMC. Our results apply in general settings where $h$ can be nonc
    
[^106]: 提升基于提示的语言模型零/少样本学习的偏差校准策略

    Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models

    [https://arxiv.org/abs/2402.10353](https://arxiv.org/abs/2402.10353)

    本研究提出了一种空输入提示方法，用于校准预训练语言模型中的固有偏差，从而提升零/少样本学习的性能。

    

    提示学习容易受到预训练语言模型中固有偏差的影响，导致基于提示的零/少样本学习性能不佳。本文提出了一种空输入提示方法，用于校准预训练语言模型中编码的固有偏差。与以往主要致力于社会公平的固有偏差修正方法不同，我们的目标是在增强语言模型在下游零/少样本学习任务中的性能的同时，强调固有偏差校准的效率。具体来说，我们利用从GPT-4生成的一组自动选取的无意义输入来提示预训练语言模型以探测固有偏差。利用偏差反映的概率分布，我们提出了一个分布差异损失用于偏差校准，其中我们仅更新语言模型的偏差参数（总参数的0.1%）以朝向相等的概率分布。

    arXiv:2402.10353v1 Announce Type: new  Abstract: Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\%$ of total parameters) of LMs towards equal probabilit
    
[^107]: 大型语言模型在预测和异常检测中的应用：系统文献综述

    Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review

    [https://arxiv.org/abs/2402.10350](https://arxiv.org/abs/2402.10350)

    大型语言模型在预测和异常检测领域展现出显著潜力，但面临着挑战包括依赖历史数据集、泛化困难、模型幻觉等问题，提出了整合多模态数据等解决方案。

    

    这篇系统文献综述全面审查了大型语言模型（LLMs）在预测和异常检测中的应用，突出当前研究现状、固有挑战以及未来的发展方向。LLMs已经在解析和分析大量数据集，识别模式，预测未来事件，并在各个领域检测异常行为方面展现出显著潜力。然而，该综述确定了一些关键挑战，阻碍了它们更广泛的采用和有效性，包括依赖于庞大的历史数据集，在不同上下文中的泛化问题，模型幻觉现象，模型知识边界的限制以及所需的大量计算资源。通过详细分析，该综述讨论了克服这些障碍的潜在解决方案和策略，如整合多模态数据。

    arXiv:2402.10350v1 Announce Type: cross  Abstract: This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal dat
    
[^108]: 在RLHF中基于探索驱动的策略优化：关于有效数据利用的理论洞见

    Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization

    [https://arxiv.org/abs/2402.10342](https://arxiv.org/abs/2402.10342)

    本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见

    

    强化学习从人类反馈（RLHF）在依赖少量人类反馈的情况下取得了令人印象深刻的经验成功。然而，对于这种现象存在着有限的理论证明。此外，尽管最近的经验成功采用了基于策略的算法，但大多数最近的研究仍侧重于基于价值的算法。在这项工作中，我们考虑了基于策略优化（PO-RLHF）的RLHF算法。该算法基于流行的策略覆盖-策略梯度（PC-PG）算法，该算法假设对奖励函数有知识。在PO-RLHF中，不假设知道奖励函数，并且该算法依赖于基于轨迹的比较反馈来推断奖励函数。我们为PO-RLHF提供了低查询复杂度的性能界限，这为解释为什么少量的人类反馈可能足以在RLHF中获得良好性能提供了洞见。一个关键的创新是我们的轨迹级el

    arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
    
[^109]: 当你的离散优化问题的规模等同于一个神经网络时该怎么办？

    What to Do When Your Discrete Optimization Is the Size of a Neural Network?

    [https://arxiv.org/abs/2402.10339](https://arxiv.org/abs/2402.10339)

    论文讨论了在大型神经网络中处理离散优化问题的方法，提出了使用继续路径方法和蒙特卡罗方法来引导模型达到良好解决方案的两种不同途径

    

    在许多机器学习应用中，使用神经网络涉及解决离散优化问题，比如在剪枝、基于参数隔离的持续学习和二进制网络训练中。然而，这些离散问题具有组合特性，不适合使用基于梯度的优化方法。此外，在离散设置中经典方法不适用于大型神经网络，迫使科学家和实证者依赖于替代方法。在这些方法中，有两个主要不同的顶级信息来源可以用来引导模型达到良好的解决方案：（1）从解决方案集之外的点外推梯度信息（2）在一组有效解决方案的成员之间比较评估。我们采用继续路径（CP）方法代表纯粹使用前者，蒙特卡罗（MC）方法代表后者，同时也指出一些混合方法结合了这两者。

    arXiv:2402.10339v1 Announce Type: new  Abstract: Oftentimes, machine learning applications using neural networks involve solving discrete optimization problems, such as in pruning, parameter-isolation-based continual learning and training of binary networks. Still, these discrete problems are combinatorial in nature and are also not amenable to gradient-based optimization. Additionally, classical approaches used in discrete settings do not scale well to large neural networks, forcing scientists and empiricists to rely on alternative methods. Among these, two main distinct sources of top-down information can be used to lead the model to good solutions: (1) extrapolating gradient information from points outside of the solution set (2) comparing evaluations between members of a subset of the valid solutions. We take continuation path (CP) methods to represent using purely the former and Monte Carlo (MC) methods to represent the latter, while also noting that some hybrid methods combine th
    
[^110]: HI-GAN：具有辅助输入的层次化修复GAN用于混合RGB和深度修复

    HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting

    [https://arxiv.org/abs/2402.10334](https://arxiv.org/abs/2402.10334)

    提出了一种名为HI-GAN的层次化修复GAN，通过三个GAN以层次结构的方式进行RGBD修复，其中EdgeGAN和LabelGAN分别修复遮罩边缘和分割标签图像，而CombinedRGBD-GAN结合它们的潜在表示输出并进行RGB和深度修复。

    

    修复涉及填补图像中丢失的像素或区域，这是混合现实环境中使用的一项关键技术，特别是在减少现实（DR）中，其中从用户的视觉环境中删除内容。现有方法依赖于数字替换技术，需要多个摄像头并产生高成本。AR设备和智能手机使用ToF深度传感器捕获与RGB图像对齐的场景深度图。尽管速度快且价格实惠，但ToF相机会产生具有丢失像素的不完美深度图。为了解决以上挑战，我们提出了层次化修复GAN（HI-GAN），这是一种新颖的方法，由三个以层次结构方式组成的GAN构成，用于RGBD修复。EdgeGAN和LabelGAN分别修复遮罩边缘和分割标签图像，而CombinedRGBD-GAN结合它们的潜在表示输出并进行RGB和深度修复。

    arXiv:2402.10334v1 Announce Type: cross  Abstract: Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user's visual environment. Existing methods rely on digital replacement techniques which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address the above challenges, we propose Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and Depth inpainting. Edge images and particularly
    
[^111]: 可解释的生成对抗模仿学习

    Interpretable Generative Adversarial Imitation Learning

    [https://arxiv.org/abs/2402.10310](https://arxiv.org/abs/2402.10310)

    提出了一种结合了信号时序逻辑（STL）推断和控制合成的新颖仿真学习方法，可以明确表示任务为STL公式，同时通过人为调整STL公式实现对人类知识的纳入和新场景的适应，还采用了生成对抗网络（GAN）启发的训练方法，有效缩小了专家策略和学习策略之间的差距

    

    仿真学习方法已经通过专家演示在教授自主系统复杂任务方面取得了相当大的成功。然而，这些方法的局限性在于它们缺乏可解释性，特别是在理解学习代理试图完成的具体任务方面。在本文中，我们提出了一种结合了信号时序逻辑（STL）推断和控制合成的新颖仿真学习方法，使任务可以明确表示为STL公式。这种方法不仅可以清晰地理解任务，还可以通过手动调整STL公式来将人类知识纳入并适应新场景。此外，我们采用了受生成对抗网络（GAN）启发的训练方法进行推断和控制策略，有效地缩小了专家策略和学习策略之间的差距。我们算法的有效性

    arXiv:2402.10310v1 Announce Type: new  Abstract: Imitation learning methods have demonstrated considerable success in teaching autonomous systems complex tasks through expert demonstrations. However, a limitation of these methods is their lack of interpretability, particularly in understanding the specific task the learning agent aims to accomplish. In this paper, we propose a novel imitation learning method that combines Signal Temporal Logic (STL) inference and control synthesis, enabling the explicit representation of the task as an STL formula. This approach not only provides a clear understanding of the task but also allows for the incorporation of human knowledge and adaptation to new scenarios through manual adjustments of the STL formulae. Additionally, we employ a Generative Adversarial Network (GAN)-inspired training approach for both the inference and the control policy, effectively narrowing the gap between the expert and learned policies. The effectiveness of our algorithm
    
[^112]: 离散概率推断作为多路径环境中的控制

    Discrete Probabilistic Inference as Control in Multi-path Environments

    [https://arxiv.org/abs/2402.10309](https://arxiv.org/abs/2402.10309)

    通过Generative Flow Networks (GFlowNets)学习一个随机策略，以近似实现在整个马尔可夫决策过程（MDP）中流量的守恒，从而解决了多路径生成相同对象的偏倚分布问题。

    

    我们将从离散且结构化分布中采样的问题视为一个顺序决策问题，其目标是找到一种随机策略，使物体在这个顺序过程结束时以某些预定义奖励的比例被采样。本文中，我们扩展了最近纠正奖励的方法，以确保由最佳最大熵强化学习引起的边际分布

    arXiv:2402.10309v1 Announce Type: new  Abstract: We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL
    
[^113]: 使用KCUSUM算法评估实时自适应采样变点检测

    An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM

    [https://arxiv.org/abs/2402.10291](https://arxiv.org/abs/2402.10291)

    KCUSUM算法是一种非参数扩展算法，用于在高容量数据情景下实时检测突变变化，相比于现有算法，其能够更灵活地在在线环境中进行变点检测。

    

    从科学模拟数据流中实时检测突变变化是一项具有挑战性的任务，要求部署准确和高效的算法。本研究引入了基于核的累积和（KCUSUM）算法，一种传统累积和（CUSUM）方法的非参数扩展，以其在较少限制条件下在线变点检测方面的有效性而备受关注。

    arXiv:2402.10291v1 Announce Type: new  Abstract: Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic gr
    
[^114]: Thompson Sampling在部分可观察的上下文特征臂老虎机中的应用

    Thompson Sampling in Partially Observable Contextual Bandits

    [https://arxiv.org/abs/2402.10289](https://arxiv.org/abs/2402.10289)

    研究了在部分观察到的上下文特征老虎机中使用Thompson Sampling策略，以学习选择最佳臂的问题

    

    上下文特征臂老虎机构成了一个经典的不确定性决策框架。在这种情况下，目标是在上下文信息的条件下学习具有最高奖励的臂，同时需要通过实验来学习每个臂的未知奖励参数。因此，一个基本问题是在探索（即拉动不同臂以学习它们的参数）和开发（即拉动最佳臂以获得奖励）之间取得平衡。现有文献大多考虑完全观察到的上下文情境。然而，尽管在理论上更一般且在实践中更有多样性，但部分上下文观察情景至今仍未被探索。我们研究了在观察数据是未观察到的上下文向量的噪声线性函数的情况下学习选择最佳臂的老虎机策略。我们的理论分析表明Thompson采样算法...

    arXiv:2402.10289v1 Announce Type: cross  Abstract: Contextual bandits constitute a classical framework for decision-making under uncertainty. In this setting, the goal is to learn the arms of highest reward subject to contextual information, while the unknown reward parameters of each arm need to be learned by experimenting that specific arm. Accordingly, a fundamental problem is that of balancing exploration (i.e., pulling different arms to learn their parameters), versus exploitation (i.e., pulling the best arms to gain reward). To study this problem, the existing literature mostly considers perfectly observed contexts. However, the setting of partial context observations remains unexplored to date, despite being theoretically more general and practically more versatile. We study bandit policies for learning to select optimal arms based on the data of observations, which are noisy linear functions of the unobserved context vectors. Our theoretical analysis shows that the Thompson sam
    
[^115]: 对一类序列异常检测模型的后门攻击

    Backdoor Attack against One-Class Sequential Anomaly Detection Models

    [https://arxiv.org/abs/2402.10283](https://arxiv.org/abs/2402.10283)

    本文提出了一种新型后门攻击策略，可以通过在良性正常数据中制作几乎不可察觉的触发器，并将其注入模型，成功妥协了两个一类异常检测模型。

    

    arXiv:2402.10283v1 公告类型：跨界 摘要：深度学习在序列数据上的异常检测引起了广泛关注，然而，基于深度学习的模型面临一种关键的安全威胁 - 它们容易受到后门攻击的影响。本文研究了通过提出一种新型后门攻击策略来妥协深度序列异常检测模型。攻击方法包括两个主要步骤，触发器生成和后门注入。 触发器生成是通过从良性正常数据中制作扰动样本来导出几乎不可察觉的触发器，其中扰动样本仍然正常。 后门注入则是适当地将后门触发器注入模型，只为具有触发器的样本。 实验结果表明了我们提出的攻击策略的有效性，通过在两个知名的一类异常检测模型上注入后门触发器。

    arXiv:2402.10283v1 Announce Type: cross  Abstract: Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.
    
[^116]: 具有中介反馈的赌博机信息容量遗憾界限

    Information Capacity Regret Bounds for Bandits with Mediator Feedback

    [https://arxiv.org/abs/2402.10282](https://arxiv.org/abs/2402.10282)

    该研究提出了一种基于信息容量的新遗憾界限方法，适用于具有中介反馈的赌博机问题，同时在敌对和随机设置中提供了近乎匹配的下界。

    

    这项工作解决了中介反馈问题，即决策集包括多个策略，每个策略与共同结果空间上的概率分布相关联。选择一个策略后，学习者观察从其分布中采样的结果，并在当前回合中承担分配给该结果的损失。我们引入策略集容量作为衡量策略集复杂性的信息论指标。采用经典的EXP4算法，我们提供了新的遗憾界限，取决于策略集容量在敌对和随机设置中的性能。对于一些策略集家族的选择，我们证明了近乎匹配的下界，与容量类似地扩展。我们还考虑了策略分布在回合之间可以变化的情况，从而解决了相关的具有专家建议的赌博机问题，我们在其先前结果上有所改进。此外，我们证明

    arXiv:2402.10282v1 Announce Type: new  Abstract: This work addresses the mediator feedback problem, a bandit game where the decision set consists of a number of policies, each associated with a probability distribution over a common space of outcomes. Upon choosing a policy, the learner observes an outcome sampled from its distribution and incurs the loss assigned to this outcome in the present round. We introduce the policy set capacity as an information-theoretic measure for the complexity of the policy set. Adopting the classical EXP4 algorithm, we provide new regret bounds depending on the policy set capacity in both the adversarial and the stochastic settings. For a selection of policy set families, we prove nearly-matching lower bounds, scaling similarly with the capacity. We also consider the case when the policies' distributions can vary between rounds, thus addressing the related bandits with expert advice problem, which we improve upon its prior results. Additionally, we prov
    
[^117]: SusFL: 可持续智能农场的能量感知联合学习监控系统

    SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms

    [https://arxiv.org/abs/2402.10280](https://arxiv.org/abs/2402.10280)

    提出了SusFL系统，利用能量感知联合学习技术为智能农场监测提供可持续性解决方案，通过智能客户端选择优化监测质量，同时减少能量消耗，并具备抵抗对抗性攻击的能力。

    

    我们提出了一种新颖的能量感知联合学习（FL）系统，名为SusFL，用于可持续智能农业，旨在解决由于太阳能传感器能量水平波动而导致的不一致的健康监测挑战。该系统配备了具有计算能力的太阳能传感器，如树莓派，用于在健康数据上训练本地深度学习模型。这些传感器定期更新LoRa网关，形成无线传感器网络（WSN），以便检测乳腺炎等疾病。我们提出的SusFL系统融合了机制设计，即博弈论概念，用于智能客户端选择以优化监测质量同时最小化能量使用。该策略确保系统对抗破坏FL操作的对抗性攻击，包括可能破坏FL操作的数据投毒和隐私威胁。通过使用实时数据集进行广泛的比较分析，我们证明

    arXiv:2402.10280v1 Announce Type: new  Abstract: We propose a novel energy-aware federated learning (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system's sustainability and resilience against adversarial attacks, including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we de
    
[^118]: 一种用于空破解的强REJECT方法

    A StrongREJECT for Empty Jailbreaks

    [https://arxiv.org/abs/2402.10260](https://arxiv.org/abs/2402.10260)

    提出了一种新的基准 StrongREJECT，通过使用更高质量的问题，更好地区分有效和无效的空破解方法。

    

    大型语言模型（LLMs）的兴起引起了对“破解”的关注，这种破解允许模型被恶意使用。然而，目前没有标准的基准来衡量破解的严重程度，导致破解论文的作者不得不自行创建标准。我们表明这些基准经常包含模棱两可或无法回答的问题，并使用倾向于高估低质量模型响应的滥用潜力的评分标准。一些破解技术使问题更加严重，因为它们即使对于良性问题也会降低模型响应的质量：我们展示了几种破解技术显着降低了GPT-4在MMLU上的零射击表现。破解还会使从“未经审查”的开源模型中获取有害响应变得更加困难。我们提出了一个新的基准，StrongREJECT，通过使用更高质量的问题更好地区分有效和无效的破解方法。

    arXiv:2402.10260v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has drawn attention to the existence of "jailbreaks" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an "uncensored" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality que
    
[^119]: 个性化联邦学习的统计异质性

    Personalized Federated Learning for Statistical Heterogeneity

    [https://arxiv.org/abs/2402.10254](https://arxiv.org/abs/2402.10254)

    该论文总结了个性化联邦学习领域的当前研究进展，探讨了如何解决统计异质性带来的挑战，并讨论了进一步研究和障碍。

    

    联邦学习（FL）变得越来越受欢迎，随之而来的是人工智能应用中关于数据隐私的日益关注。 FL促进了协作式多方模型学习，同时确保了数据保密性。 然而，由于存在多样化客户数据分布而引起的统计异质性问题带来了一定的挑战，如个性化不足和收敛速度慢。 为了解决上述问题，本文简要总结了个性化联邦学习（PFL）领域的当前研究进展。 它概述了PFL概念，审视了相关技术，并突出了当前的努力。 此外，本文还讨论了与PFL相关的潜在进一步研究和障碍。

    arXiv:2402.10254v1 Announce Type: new  Abstract: The popularity of federated learning (FL) is on the rise, along with growing concerns about data privacy in artificial intelligence applications. FL facilitates collaborative multi-party model learning while simultaneously ensuring the preservation of data confidentiality. Nevertheless, the problem of statistical heterogeneity caused by the presence of diverse client data distributions gives rise to certain challenges, such as inadequate personalization and slow convergence. In order to address the above issues, this paper offers a brief summary of the current research progress in the field of personalized federated learning (PFL). It outlines the PFL concept, examines related techniques, and highlights current endeavors. Furthermore, this paper also discusses potential further research and obstacles associated with PFL.
    
[^120]: 具有无界和退化噪声的线性系统在线控制

    Online Control of Linear Systems with Unbounded and Degenerate Noise

    [https://arxiv.org/abs/2402.10252](https://arxiv.org/abs/2402.10252)

    这项研究揭示了在线控制问题中，对于凸成本，可以实现 $ \widetilde{O}(\sqrt{T}) $ 的遗憾界，甚至在存在无界噪声的情况下；同时，在成本具有强凸性时，可以在不需要噪声协方差是非退化的情况下建立 $ O({\rm poly} (\log T)) $ 的遗憾界。

    

    本文研究了在可能存在无界和退化噪声的情况下控制线性系统的问题，其中成本函数未知，被称为在线控制问题。与现有的仅假设噪声有界性的研究不同，我们揭示了对于凸成本，即使在存在无界噪声的情况下也可以实现 $ \widetilde{O}(\sqrt{T}) $ 的遗憾界，其中 $ T $ 表示时间跨度。此外，当成本具有强凸性时，我们建立了一个 $ O({\rm poly} (\log T)) $ 的遗憾界，而不需要噪声协方差是非退化的假设，这在文献中是必需的。消除噪声秩的关键是与噪声协方差相关联的系统转化。这同时实现了在线控制算法的参数减少。

    arXiv:2402.10252v1 Announce Type: cross  Abstract: This paper investigates the problem of controlling a linear system under possibly unbounded and degenerate noise with unknown cost functions, known as an online control problem. In contrast to the existing work, which assumes the boundedness of noise, we reveal that for convex costs, an $ \widetilde{O}(\sqrt{T}) $ regret bound can be achieved even for unbounded noise, where $ T $ denotes the time horizon. Moreover, when the costs are strongly convex, we establish an $ O({\rm poly} (\log T)) $ regret bound without the assumption that noise covariance is non-degenerate, which has been required in the literature. The key ingredient in removing the rank assumption on noise is a system transformation associated with the noise covariance. This simultaneously enables the parameter reduction of an online control algorithm.
    
[^121]: Brant-2：脑信号基础模型

    Brant-2: Foundation Model for Brain Signals

    [https://arxiv.org/abs/2402.10251](https://arxiv.org/abs/2402.10251)

    Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。

    

    基础模型受益于在大量未标记数据上进行预训练，并且在少量标记数据的情况下能够在各种应用中表现出色。这种模型在分析脑信号方面特别有效，因为这一领域涵盖了众多应用场景，并且进行大规模注释是成本高昂的。在这项工作中，我们提出了脑信号领域最大的基础模型，Brant-2。与用于颅内神经信号的基础模型Brant相比，Brant-2不仅对数据变化和建模尺度表现出稳健性，而且可以应用于更广泛范围的脑神经数据。通过在大量任务上进行实验，我们展示了Brant-2对脑信号中各种应用场景的适应性。进一步分析揭示了Brant-2的可扩展性，验证了每个组件的有效性，并展示了我们模型保持的能力。

    arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
    
[^122]: 一种基于数据驱动的监督机器学习方法用于估计全球环境空气污染浓度及其相关预测区间

    A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals

    [https://arxiv.org/abs/2402.10248](https://arxiv.org/abs/2402.10248)

    该论文提出了一种基于数据驱动的监督机器学习方法，可用于估计全球环境空气污染浓度，并提供预测区间，为各利益相关者提供更全面的数据，并通过检验模型在不同地理位置上的性能为研究提供洞见。

    

    全球环境空气污染是一个跨界挑战，通常通过依赖空间稀疏且异构放置的监测站数据的干预来解决。这些站点经常由于诸如停电等问题而出现时间数据缺失。为此，我们开发了一种可扩展的、数据驱动的、监督式机器学习框架。该模型旨在补充缺失的时间和空间测量数据，从而生成包括NO$_2$、O$_3$、PM$_{10}$、PM$_{2.5}$和SO$_2$等污染物的全面数据集。该数据集在每一估计值附带预测区间，并为依赖室外空气污染数据进行下游评估的广泛利益相关者提供服务。这使得可以进行更详细的研究。此外，还研究了该模型在不同地理位置上的性能，从而提供见解。

    arXiv:2402.10248v1 Announce Type: cross  Abstract: Global ambient air pollution, a transboundary challenge, is typically addressed through interventions relying on data from spatially sparse and heterogeneously placed monitoring stations. These stations often encounter temporal data gaps due to issues such as power outages. In response, we have developed a scalable, data-driven, supervised machine learning framework. This model is designed to impute missing temporal and spatial measurements, thereby generating a comprehensive dataset for pollutants including NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of 0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for each estimate, caters to a wide range of stakeholders relying on outdoor air pollution data for downstream assessments. This enables more detailed studies. Additionally, the model's performance across various geographical locations is examined, providing insights an
    
[^123]: 通过概率图模型理解团队崩溃

    Understanding team collapse via probabilistic graphical models

    [https://arxiv.org/abs/2402.10243](https://arxiv.org/abs/2402.10243)

    本研究开发了一个概率图模型来理解团队的动态，并通过研究团队崩溃现象的主要原因以及构建弹性团队的原则来展示其在团队管理中的应用。

    

    在这项工作中，我们开发了一个图模型来捕捉团队动态。我们分析了该模型，并展示了如何从数据中学习其参数。利用我们的模型，我们从计算的角度研究了团队崩溃现象。我们使用模拟和真实世界实验来找出团队崩溃的主要原因。我们还提供了构建弹性团队的原则，即避免崩溃的团队。最后，我们使用我们的模型来分析NBA球队的结构，并深入研究感兴趣的比赛。

    arXiv:2402.10243v1 Announce Type: cross  Abstract: In this work, we develop a graphical model to capture team dynamics. We analyze the model and show how to learn its parameters from data. Using our model we study the phenomenon of team collapse from a computational perspective. We use simulations and real-world experiments to find the main causes of team collapse. We also provide the principles of building resilient teams, i.e., teams that avoid collapsing. Finally, we use our model to analyze the structure of NBA teams and dive deeper into games of interest.
    
[^124]: 有符号多样化多重网络：聚类和推断

    Signed Diverse Multiplex Networks: Clustering and Inference

    [https://arxiv.org/abs/2402.10242](https://arxiv.org/abs/2402.10242)

    保留边的符号在网络构建过程中提高了估计和聚类精度，有助于解决现实世界问题。

    

    该论文介绍了一种有符号的广义随机点积图（SGRDPG）模型，这是广义随机点积图（GRDPG）的一个变种，其中边可以是正的也可以是负的。该设置被扩展为多重网络版本，其中所有层具有相同的节点集合并遵循SGRDPG。网络层的唯一公共特征是它们可以被划分为具有共同子空间结构的组，而其他情况下所有连接概率矩阵可能是完全不同的。上述设置非常灵活，并包括各种现有多重网络模型作为其特例。论文实现了两个目标。首先，它表明在网络构建过程中保留边的符号会导致更好的估计和聚类精度，因此有助于应对诸如大脑网络分析之类的现实问题。

    arXiv:2402.10242v1 Announce Type: cross  Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise all matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models as its particular cases. The paper fulfills two objectives. First, it shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as analysis of brain networks. Second, b
    
[^125]: 为什么问题的动态视角

    A Dynamical View of the Question of Why

    [https://arxiv.org/abs/2402.10240](https://arxiv.org/abs/2402.10240)

    提出了一种在时间过程中直接建立事件之间因果关系的学习范式，并提出了用于计算因果贡献的两个关键引理，可以揭示和量化扩散过程中的因果关系。

    

    我们研究由随机过程生成的多元时间序列数据中的因果推理。现有方法主要局限于静态设置，忽略了时间上的连续性和变化的发射。相比之下，我们提出了一个学习范式，直接在时间过程中建立事件之间的因果关系。我们提出了两个关键引理来计算因果贡献，并将其构造为强化学习问题。我们的方法提供了揭示和量化扩散过程中因果关系的形式化和计算工具，包括各种重要设置，如离散时间马尔可夫决策过程。最后，通过相当复杂的实验和通过纯学习，我们的框架揭示和量化了因果联系，否则看似莫名其妙。

    arXiv:2402.10240v1 Announce Type: cross  Abstract: We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.
    
[^126]: 一种用于粒子跟踪的语言模型

    A Language Model for Particle Tracking

    [https://arxiv.org/abs/2402.10239](https://arxiv.org/abs/2402.10239)

    提出一种使用语言模型统一不同粒子跟踪任务的方法，开发出能够用于其他任务的粒子跟踪BERT模型，为粒子探测器理解奠定基础。

    

    粒子跟踪对于大型强子对撞机中几乎所有物理分析程序至关重要。深度学习模型广泛应用于与粒子跟踪相关的任务。然而，目前的做法是为一个任务设计和训练一个深度学习模型，并使用监督学习技术。训练的模型对其训练的任务效果良好，但显示出几乎没有泛化能力。我们提出使用语言模型统一这些模型。在本文中，我们提出了一种标记化探测器表示，使我们能够为粒子跟踪训练一个BERT模型。训练的BERT模型，即TrackingBERT，提供了可以用于其他任务的潜在探测器模块嵌入。这项工作代表了为粒子探测器理解开发基础模型的第一步。

    arXiv:2402.10239v1 Announce Type: cross  Abstract: Particle tracking is crucial for almost all physics analysis programs at the Large Hadron Collider. Deep learning models are pervasively used in particle tracking related tasks. However, the current practice is to design and train one deep learning model for one task with supervised learning techniques. The trained models work well for tasks they are trained on but show no or little generalization capabilities. We propose to unify these models with a language model. In this paper, we present a tokenized detector representation that allows us to train a BERT model for particle tracking. The trained BERT model, namely TrackingBERT, offers latent detector module embedding that can be used for other tasks. This work represents the first step towards developing a foundational model for particle detector understanding.
    
[^127]: 用于不稳定火焰演化的时间推进算子的参数化学习

    Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution

    [https://arxiv.org/abs/2402.10238](https://arxiv.org/abs/2402.10238)

    本研究利用机器学习方法（FNO和CNN）学习参数化偏微分方程的时间推进算子，扩展了现有的算子学习方法，能在不同参数条件下准确预测短期解并提供稳健的长期统计信息，有助于节约计算成本并加速工程仿真开发。

    

    本研究探讨了将机器学习，具体而言是傅里叶神经算子（FNO）和卷积神经网络（CNN），应用于学习参数化偏微分方程（PDE）的时间推进算子。我们关注于扩展现有的算子学习方法，以处理代表PDE参数的附加输入。我们的目标是创建一个统一的学习方法，准确预测短期解并在不同参数条件下提供稳健的长期统计信息，以促进工程仿真中的计算成本节约和开发加速。我们开发并比较了基于FNO和CNN的参数化学习方法，评估它们在学习一维PDE和由Navier-Stokes方程的直接数值模拟获得的实际火焰前沿演化数据的参数依赖解时间推进算子方面的有效性。

    arXiv:2402.10238v1 Announce Type: new  Abstract: This study investigates the application of machine learning, specifically Fourier Neural Operator (FNO) and Convolutional Neural Network (CNN), to learn time-advancement operators for parametric partial differential equations (PDEs). Our focus is on extending existing operator learning methods to handle additional inputs representing PDE parameters. The goal is to create a unified learning approach that accurately predicts short-term solutions and provides robust long-term statistics under diverse parameter conditions, facilitating computational cost savings and accelerating development in engineering simulations. We develop and compare parametric learning methods based on FNO and CNN, evaluating their effectiveness in learning parametric-dependent solution time-advancement operators for one-dimensional PDEs and realistic flame front evolution data obtained from direct numerical simulations of the Navier-Stokes equations.
    
[^128]: 使用多样性搜索在元胞自动机中发现感觉运动机构

    Discovering Sensorimotor Agency in Cellular Automata using Diversity Search

    [https://arxiv.org/abs/2402.10236](https://arxiv.org/abs/2402.10236)

    本论文利用多样性搜索、课程学习和梯度下降等算法，自动搜索元胞自动机中能够自组织出具有基础感觉运动机构的“个体”，为寻找这种基本机构自组织的环境条件提供了新方法。

    

    人工生命研究领域研究类似生命现象的如自主生成、机构性或自我调节等在计算机模拟中如何自组织。在元胞自动机（CA）中，一个关键的未解之谜是是否可能找到能够自组织出稳健“个体”的环境规则，而这些个体在初始状态下没有“身体”、“大脑”、“感知”或“行动”的存在。本文利用机器学习的最新进展，结合多样性搜索、课程学习和梯度下降等算法，自动搜索这些“个体”，即能够移动并有能力以一致的方式对外部障碍做出反应且保持完整性的局部结构，从而形成基础形式的感觉运动机构。我们展示了这种方法使得能够系统地找到在CA中导致这种基本机构自组织的环境条件。

    arXiv:2402.10236v1 Announce Type: cross  Abstract: The research field of Artificial Life studies how life-like phenomena such as autopoiesis, agency, or self-regulation can self-organize in computer simulations. In cellular automata (CA), a key open-question has been whether it it is possible to find environment rules that self-organize robust "individuals" from an initial state with no prior existence of things like "bodies", "brain", "perception" or "action". In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, curriculum learning and gradient descent, to automate the search of such "individuals", i.e. localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. We show that this approach enables to find systematically environmental conditions in CA leading to self-organization of such basic forms of agency. Th
    
[^129]: 用于自动检测网络欺凌的多方面半合成数据集

    A Multi-faceted Semi-Synthetic Dataset for Automated Cyberbullying Detection

    [https://arxiv.org/abs/2402.10231](https://arxiv.org/abs/2402.10231)

    提供了一个包含网络欺凌所有要素的广泛半合成数据集，填补了自动检测网络欺凌领域的数据缺口

    

    近年来，社交媒体的使用日益增长，推动了自动检测网络欺凌成为一个重要的研究领域。然而，由于缺乏标准定义和普遍接受的数据集，挑战依然存在。许多研究人员现在将网络欺凌视为网络侵略的一个方面，包括重复性、同行关系和有意伤害等因素，除了在线侵略。从社交媒体网络中获取反映所有网络欺凌组成部分的综合数据证明是一项复杂的任务。本文描述了一个包含网络欺凌的所有要素的广泛半合成网络欺凌数据集，并简要概述了创建数据集的方法，另外还提供了可公开访问的数据集的详细概述。

    arXiv:2402.10231v1 Announce Type: cross  Abstract: In recent years, the rising use of social media has propelled automated cyberbullying detection into a prominent research domain. However, challenges persist due to the absence of a standardized definition and universally accepted datasets. Many researchers now view cyberbullying as a facet of cyberaggression, encompassing factors like repetition, peer relationships, and harmful intent in addition to online aggression. Acquiring comprehensive data reflective of all cyberbullying components from social media networks proves to be a complex task. This paper provides a description of an extensive semi-synthetic cyberbullying dataset that incorporates all of the essential aspects of cyberbullying, including aggression, repetition, peer relationships, and intent to harm. The method of creating the dataset is succinctly outlined, and a detailed overview of the publicly accessible dataset is additionally presented. This accompanying data arti
    
[^130]: 在文本数据流中漂移标签的时间分析：基于图的应用

    Temporal Analysis of Drifting Hashtags in Textual Data Streams: A Graph-Based Application

    [https://arxiv.org/abs/2402.10230](https://arxiv.org/abs/2402.10230)

    本文利用图分析和文本数据流的概念，在年度快照中分析了随时间漂移的标签，揭示了标签社区，特别是针对＃mybodymychoice标签的分析，并为监测社交媒体中关于实体的意见和情感模式随时间变化提供了有用的方法。

    

    社交媒体自出现以来就发挥着重要作用。人们利用互联网表达对任何事物的看法，使社交媒体平台成为社会传感器。最初由Twitter支持，现在已在多个社交媒体平台上使用标签。标签有助于标记、跟踪和对类似主题的帖子进行分组。在本文中，我们使用图分析和文本数据流的概念，运用Girvan-Newman方法来分析随时间漂移的标签，并在年度快照中揭示标签社区。更具体地，我们分析了2018年至2022年间的＃mybodymychoice标签。此外，我们提供了在研究中发现的一些标签的见解。此外，我们的方法可用于监测社交媒体上关于某个实体的意见和情感模式随时间的变化。尽管＃mybodymychoice标签最初与妇女权利、堕胎和身体自主有关，我们观察到

    arXiv:2402.10230v1 Announce Type: cross  Abstract: Social media has played an important role since its emergence. People use the internet to express opinions about anything, making social media platforms a social sensor. Initially supported by Twitter, the hashtags are now in use on several social media platforms. Hashtags are helpful to tag, track, and group posts on similar topics. In this paper, we analyze hashtag drifts over time using concepts from graph analysis and textual data streams using the Girvan-Newman method to uncover hashtag communities in annual snapshots. More specifically, we analyzed the #mybodymychoice hashtag between 2018 and 2022. In addition, we offer insights about some hashtags found in the study. Furthermore, our approach can be useful for monitoring changes over time in opinions and sentiment patterns about an entity on social media. Even though the hashtag #mybodymychoice was initially coupled with women's rights, abortion, and bodily autonomy, we observe 
    
[^131]: Mixture-Models: 一种集成了各种混合模型的Python库

    Mixture-Models: a one-stop Python Library for Model-based Clustering using various Mixture Models

    [https://arxiv.org/abs/2402.10229](https://arxiv.org/abs/2402.10229)

    \texttt{Mixture-Models}是一个集成了各种混合模型的Python库，通过自动微分工具简化模型的实现和分析，并支持高维数据，提供用户友好的评估工具。

    

    \texttt{Mixture-Models}是一个开源的Python库，用于拟合高斯混合模型（GMM）及其变种，如简约GMM、因子分析混合模型、MClust模型、学生t分布混合模型等。它通过自动微分工具简化了这些模型的实现和分析，并使用各种一阶/二阶优化例程，如梯度下降和牛顿-CG。这有助于将这些模型扩展到高维数据，这在Python库中还是首次。该库提供了用户友好的模型评估工具，如BIC、AIC和对数似然估计。源代码根据MIT许可证进行许可，可以在\url{https://github.com/kasakh/Mixture-Models}中访问。该软件包高度可拓展，用户可以轻松地加入新的分布和优化技术。

    arXiv:2402.10229v1 Announce Type: cross  Abstract: \texttt{Mixture-Models} is an open-source Python library for fitting Gaussian Mixture Models (GMM) and their variants, such as Parsimonious GMMs, Mixture of Factor Analyzers, MClust models, Mixture of Student's t distributions, etc. It streamlines the implementation and analysis of these models using various first/second order optimization routines such as Gradient Descent and Newton-CG through automatic differentiation (AD) tools. This helps in extending these models to high-dimensional data, which is first of its kind among Python libraries. The library provides user-friendly model evaluation tools, such as BIC, AIC, and log-likelihood estimation. The source-code is licensed under MIT license and can be accessed at \url{https://github.com/kasakh/Mixture-Models}. The package is highly extensible, allowing users to incorporate new distributions and optimization techniques with ease. We conduct a large scale simulation to compare the pe
    
[^132]: HyperAgent：一种简单、可扩展、高效且可证明用于复杂环境的强化学习框架

    HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments

    [https://arxiv.org/abs/2402.10228](https://arxiv.org/abs/2402.10228)

    HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    

    为了在资源约束下解决复杂任务，强化学习（RL）代理需要简单、高效、可扩展、具有大状态空间和不断积累的交互数据。我们提出了HyperAgent，这是一个具有超模型、索引抽样方案和增量更新机制的RL框架，可以在一般价值函数逼近中进行计算高效的顺序后验逼近和数据高效的动作选择，超越了共轭性。HyperAgent的实现简单，只需要在DDQN中添加一个模块和一行额外代码。在实践中，HyperAgent在大规模深度RL基准测试中表现出稳健的性能，无论是在数据还是计算方面都获得了显着的效率提升。在理论上，在实际可扩展的算法中，HyperAgent是第一个能够实现可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
    
[^133]: 相关拉格朗日薛定谔桥：通过人口级正则化学习动力学

    Correlational Lagrangian Schr\"odinger Bridge: Learning Dynamics with Population-Level Regularization

    [https://arxiv.org/abs/2402.10227](https://arxiv.org/abs/2402.10227)

    提出了一个名为相关拉格朗日薛定谔桥（CLSB）的新框架，通过人口级正则化来学习跨截面样本中的系统动态，适应个体粒子行为的异质性。

    

    系统动态的准确建模在包括细胞动力学和流体力学在内的广泛科学领域中具有引人注目的潜力。然而，当（i）观察仅限于横截面样本（个体轨迹不可学习）时，以及（ii）个体粒子行为异质时（尤其是由于生物多样性而为生物系统）。为了解决这些问题，我们引入了一个名为相关拉格朗日薛定谔桥（CLSB）的新框架，旨在寻求在横截观察之间的演变“桥梁”，同时以最小人口“成本”进行正则化。与先前依赖\textit{个体}级正则化器的方法形成对比（例如，限制个体运动），CLSB在人口水平运行，接受异质性本质，从而产生更具泛化性的模式。

    arXiv:2402.10227v1 Announce Type: new  Abstract: Accurate modeling of system dynamics holds intriguing potential in broad scientific fields including cytodynamics and fluid mechanics. This task often presents significant challenges when (i) observations are limited to cross-sectional samples (where individual trajectories are inaccessible for learning), and moreover, (ii) the behaviors of individual particles are heterogeneous (especially in biological systems due to biodiversity). To address them, we introduce a novel framework dubbed correlational Lagrangian Schr\"odinger bridge (CLSB), aiming to seek for the evolution "bridging" among cross-sectional observations, while regularized for the minimal population "cost". In contrast to prior methods relying on \textit{individual}-level regularizers for all particles \textit{homogeneously} (e.g. restraining individual motions), CLSB operates at the population level admitting the heterogeneity nature, resulting in a more generalizable mode
    
[^134]: 使用展开网络对聚类归纳偏好进行建模

    Clustering Inductive Biases with Unrolled Networks

    [https://arxiv.org/abs/2402.10213](https://arxiv.org/abs/2402.10213)

    提出了一种自动编码器架构（WLSC），其潜在表示是隐含的，用于对聚类归纳偏好进行建模

    

    经典的稀疏编码（SC）模型将视觉刺激表示为少量学习基函数的线性组合，在对自然图像数据进行训练时，这些基函数类似于Gabor。然而，经典稀疏编码学习的类Gabor滤波器远远超过了实际观察到的简单细胞感受野轮廓的良好预测。我们提出了一种自动编码器架构（WLSC），其潜在表示是隐含的

    arXiv:2402.10213v1 Announce Type: cross  Abstract: The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitl
    
[^135]: 在Transformer中重用Softmax硬件单元进行GELU计算

    Reusing Softmax Hardware Unit for GELU Computation in Transformers

    [https://arxiv.org/abs/2402.10118](https://arxiv.org/abs/2402.10118)

    本文提出了一种在Transformer中重用Softmax硬件单元进行GELU计算的方法，实验证明这种方法不会降低NLP应用的准确性。

    

    Transformers大大提高了自然语言处理（NLP）和计算机视觉应用的性能。Transformer的计算涉及矩阵乘法和非线性激活函数，如softmax和GELU（高斯误差线性单元），这些函数可以直接在硬件中加速。目前，每个函数的计算都是分开完成的，很少能够重复使用硬件。为了解决这个问题，本文将GELU计算映射到softmax运算符上。这样，已经设计用于softmax的高效硬件单元也可以用于计算GELU。GELU的计算可以充分利用softmax的向量化特性，同时并行产生多个GELU的结果。实验结果表明，通过预先存在并逐步修改的softmax硬件单元计算GELU（a）不会降低代表性NLP应用的准确性，（b）全部

    arXiv:2402.10118v1 Announce Type: cross  Abstract: Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) all
    
[^136]: 使用DDPM反转进行零样本无监督和基于文本的音频编辑

    Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion

    [https://arxiv.org/abs/2402.10009](https://arxiv.org/abs/2402.10009)

    本文研究了使用DDPM反转进行音频信号的零样本编辑技术，包括基于文本的编辑和无监督发现编辑方向。这些方法在音乐信号中展现了多样的音乐兴趣修改。

    

    使用大型预训练模型进行零样本编辑已经在图像领域取得了迅猛的发展，但在音频领域尚未出现。本文中，我们探索了两种基于DDPM反转的音频信号零样本编辑技术。第一种是从图像领域采用的方法，允许基于文本进行编辑。第二种是一种新颖的方法，可以在无监督情况下发现语义上有意义的编辑方向。当应用于音乐信号时，这种方法可以展现出一系列具有音乐兴趣的修改，从控制特定乐器的参与到对旋律进行即兴演奏。示例可以在我们的例子页面中找到：https://hilamanor.github.io/AudioEditing/ ，代码可以在 https://github.com/hilamanor/AudioEditing/ 找到。

    arXiv:2402.10009v1 Announce Type: cross  Abstract: Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .
    
[^137]: 检查生成对抗网络判别器中的病态偏见：以StyleGAN3模型为例的案例研究

    Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model

    [https://arxiv.org/abs/2402.09786](https://arxiv.org/abs/2402.09786)

    这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。

    

    生成对抗网络可以生成逼真的人脸，往往难以被人类区分出来。我们发现预训练的StyleGAN3模型中的判别器在图像和面部质量上系统地对得分进行分层，并且这不成比例地影响了不同性别、种族和其他类别的图像。我们检查了判别器在色彩和亮度方面对感知的种族和性别的偏见，然后检查了社会心理学中关于刻板印象研究中常见的偏见。

    arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
    
[^138]: MiMiC：表示空间中最小修改的对抗事实

    MiMiC: Minimally Modified Counterfactuals in the Representation Space

    [https://arxiv.org/abs/2402.09631](https://arxiv.org/abs/2402.09631)

    提出了一种新颖的对抗事实生成方法，利用闭式解决方案在表示空间中生成富有表达力的对抗事实，以减轻语言模型中的不良行为，该方法在地球移动问题方面提供理论上的保证，并对表示空间的几何组织进行改进。

    

    arXiv:2402.09631v1 公告类型：交叉学科 简介：语言模型经常表现出不良行为，如性别偏见或有毒语言。通过对表示空间进行干预，可以有效减轻这些问题，但两种常见的干预技术，即线性擦除和定向向量，并不能提供高度可控和表达丰富度。因此，我们提出了一种新颖的干预方法，旨在在表示空间中生成富有表达力的对抗事实，使源类别（例如“有毒”）的表示与目标类别（例如“非有毒”）的表示相似。这种方法利用高斯假设下的闭式解决方案，在地球移动问题方面提供了理论上的保证，并对表示空间的几何组织提供了进一步的改进。

    arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
    
[^139]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^140]: PMGDA: 基于偏好的多梯度下降算法

    PMGDA: A Preference-based Multiple Gradient Descent Algorithm

    [https://arxiv.org/abs/2402.09492](https://arxiv.org/abs/2402.09492)

    PMGDA是一个基于偏好的多梯度下降算法，可以 efficiently 在多目标机器学习应用中找到与决策者偏好完全匹配的帕累托最优解。

    

    针对多目标机器学习应用中的问题，如多任务学习和多目标强化学习，寻找与决策者给定偏好完全匹配的帕累托最优解是非常重要的。然而，这些问题通常规模较大，虽然有可用的梯度信息，但现有的算法无法很好地处理。为了解决这个关键问题，本文提出了一种新颖的“预测-校正”框架，用于找到决策者所需的精确帕累托最优解。在该框架中，引入了一个约束函数来在搜索过程中使解与用户特定偏好对齐，这个约束函数可以与多个目标函数同时优化。实验结果表明，我们提出的方法可以有效地找到标准基准测试、多任务和多目标强化学习问题的精确帕累托最优解。

    arXiv:2402.09492v1 Announce Type: new  Abstract: It is desirable in many multi-objective machine learning applications, such as multi-task learning and multi-objective reinforcement learning, to find a Pareto optimal solution that can exactly match a given preference of decision-makers. These problems are often large-scale with available gradient information but cannot be handled very well by the existing algorithms. To tackle this critical issue, this paper proposes a novel predict-and-correct framework for locating the exact Pareto optimal solutions required by a decision maker. In the proposed framework, a constraint function is introduced in the search progress to align the solution with a user-specific preference, which can be optimized simultaneously with multiple objective functions. Experimental results show that our proposed method can efficiently find exact Pareto optimal solutions for standard benchmarks, multi-task, and multi-objective reinforcement learning problems with m
    
[^141]: 通过信息论奖励建模来减轻奖励作弊问题

    Mitigating Reward Hacking via Information-Theoretic Reward Modeling

    [https://arxiv.org/abs/2402.09345](https://arxiv.org/abs/2402.09345)

    本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。

    

    尽管强化学习从人类反馈（RLHF）中的成功在与人类价值观的语言模型的对齐方面，奖励作弊问题，也被称为奖励过度优化，仍然是一个关键挑战，主要源于奖励建模的局限性，即奖励模型的泛化能力和偏好数据集的不一致性。在这项工作中，我们从信息论的视角来解决这个问题，并提出了一种可推广和鲁棒的奖励建模框架，称为InfoRM，通过引入变分信息瓶颈目标来过滤出不相关的信息，并开发一种模型复杂度调节机制。值得注意的是，我们进一步发现了过度优化与潜变量空间的异常值之间的相关性，将InfoRM作为检测奖励过度优化的一种有前途的工具。受到这一发现的启发，我们提出了集成聚类偏差得分（ICDS），用于量化过优化问题。

    arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
    
[^142]: 重新思考大型语言模型的机器消除技术

    Rethinking Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2402.08787](https://arxiv.org/abs/2402.08787)

    这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。

    

    我们研究了大型语言模型（LLM）领域的机器消除技术（MU），称为LLM消除技术。这个研究旨在消除不良数据的影响（例如敏感或非法信息）以及相关模型的能力，同时保持基本的知识生成的完整性，并不影响因果无关的信息。我们设想LLM消除技术将成为LLM生命周期管理中的关键要素，可能成为开发既安全、可靠又资源高效的生成式人工智能的基础，而无需进行完全重训练。我们从概念、方法、评估指标和应用等方面探索了LLM消除技术的研究领域。特别是，我们突出了现有LLM消除技术研究中经常被忽视的方面，例如消除范围、数据模型交互和多方面的有效性评估。

    arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
    
[^143]: BECoTTA: 基于输入的在线专家混合模型用于持续的测试时间自适应

    BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation

    [https://arxiv.org/abs/2402.08712](https://arxiv.org/abs/2402.08712)

    BECoTTA是一种基于输入的高效CTTA框架，通过采用MoDE（Mixture-of-Domain Low-rank Experts）模型，它包含领域自适应路由和领域专家协同损失两个核心组件，能够在持续的测试时间中自适应不断变化的领域，同时只需较少的可训练参数。

    

    持续的测试时间自适应（CTTA）要求在适应不断变化的未知领域的同时保留先前学到的知识。然而，尽管CTTA取得了一些进展，但忘记适应权衡和效率仍未得到探索。此外，当前的CTTA场景仅假设存在不相交的情况，而实际世界的领域是无缝变化的。为了解决这些挑战，本文提出了一种名为BECoTTA的基于输入的高效CTTA框架。我们提出了MoDE（Mixture-of-Domain Low-rank Experts），它包含两个核心组件：i）领域自适应路由，通过多个领域路由器有选择地捕捉领域自适应知识，和ii）领域专家协同损失，以增加每个领域和专家之间的依赖性。我们验证了我们的方法优于多个CTTA场景，包括不相交和渐变领域切换，同时只需要大约98％更少的可训练参数。

    arXiv:2402.08712v1 Announce Type: new Abstract: Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters
    
[^144]: 朝着忠实和强大的基于证据的问答专家的方向前进

    Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

    [https://arxiv.org/abs/2402.08277](https://arxiv.org/abs/2402.08277)

    这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。

    

    对大型语言模型（LLM）更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。其中一种达到这个目标的方法是基于可靠的来源提供答案。然而，这种基于证据的问答在使用LLM时已经证明在引用正确的来源（来源质量）和准确地表示来源中的信息（答案归因能力）方面工作不足。在这项工作中，我们系统地研究了如何鲁棒地微调LLM，以提高来源质量和答案归因能力。具体而言，我们引入了一个数据生成流水线，其中包括自动数据质量过滤器，可以大规模合成多样化的高质量训练和测试数据。我们还引入了四个测试集，以对微调后的专家模型的鲁棒性进行基准测试。广泛的评估结果表明，在合成数据上进行微调可以提高在内部和外部分布的性能。%基于证据的问答案例。此外，我们展示了用于评估的四个测试集，以评估微调后的专家模型的鲁棒性。

    Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
    
[^145]: 用上下文重写提高黑盒模型的鲁棒性

    Improving Black-box Robustness with In-Context Rewriting

    [https://arxiv.org/abs/2402.08225](https://arxiv.org/abs/2402.08225)

    本文提出了一种名为LLM-TTA的方法，通过使用LLM生成的增强作为测试时间增强（TTA）的增强函数，提高了黑盒模型的鲁棒性。在BERT和T5模型的情感、毒性和新闻分类任务中，LLM-TTA优于传统的增强函数，使BERT的分布外鲁棒性平均提高了4.30个百分点，而不降低分布内性能。

    

    机器学习模型在分布内（ID）数据上表现优秀，但在未见过的分布外（OOD）输入上表现困难。大多数提高OOD鲁棒性的技术在模型是黑盒的情况下不适用，例如权重被冻结，重新训练成本高，或者通过API使用模型。测试时间增强（TTA）是一种简单的事后技术，通过对测试输入的多个增强进行预测聚合来绕过黑盒约束来提高鲁棒性。由于生成有效的自然语言增强的挑战，TTA在自然语言处理中的应用受到限制。在这项研究中，我们提出了LLM-TTA，它使用LLM生成的增强作为TTA的增强函数。LLM-TTA在BERT和T5模型的情感、毒性和新闻分类任务中优于传统的增强函数，BERT的OOD鲁棒性提高了平均4.30个百分点而不会减退平均ID pe。

    Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID pe
    
[^146]: NICE: 优化上下文示例还是不优化？

    NICE: To Optimize In-Context Examples or Not?

    [https://arxiv.org/abs/2402.06733](https://arxiv.org/abs/2402.06733)

    通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。

    

    最近的研究表明，大型语言模型（LLMs）通过上下文学习和优化上下文示例（ICE），在各种任务上表现出色。然而，大多数研究假设在提示信息中要么是固定的，要么没有提供指令，导致了一个表面上的共识：优化上下文示例对于提高性能至关重要。我们针对经过指导的LLMs挑战这一共识，研究在提供了任务特定指令的情况下优化上下文示例是否必要，并发现有一些任务对于不同的优化上下文示例方法产生递减的回报。我们引入了一种任务特定的度量标准，称为"度量标准"（Metric），用于量化从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。通过对各种任务和逐步增加的指令集的系统性研究，我们验证了该启发式方法的有效性。

    Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
    
[^147]: 区块链技术支持的聚簇可扩展联邦学习（BCS-FL）框架在无人机网络中的应用

    Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) Framework in UAV Networks

    [https://arxiv.org/abs/2402.05973](https://arxiv.org/abs/2402.05973)

    本论文提出了一种基于区块链的聚簇可扩展联邦学习（BCS-FL）框架，用于改善无人机网络中的联邦学习的去中心化、协调、可扩展性和效率。该框架将无人机网络划分为聚簇，并由聚簇头无人机进行协调，以提高大规模无人机网络中的联邦学习性能。

    

    隐私、可扩展性和可靠性是无人机网络作为分布式系统所面临的重要挑战，特别是在使用大量数据交换的机器学习（ML）技术时。最近，在无人机网络中应用联邦学习（FL）改善了协作、隐私、韧性和适应性，成为无人机应用的一种有前景的框架。然而，为无人机网络实现FL引入了通信开销、同步问题、可扩展性限制和资源约束等缺点。为了解决这些挑战，本文提出了一种基于区块链的聚簇可扩展联邦学习（BCS-FL）框架，用于无人机网络。该框架将无人机网络划分为分离的聚簇，并由聚簇头无人机（CHs）进行协调，以建立一个连通图。聚簇化使得联邦学习在大规模无人机网络中的去中心化、协调、可扩展性和效率得到了改善。

    Privacy, scalability, and reliability are significant challenges in unmanned aerial vehicle (UAV) networks as distributed systems, especially when employing machine learning (ML) technologies with substantial data exchange. Recently, the application of federated learning (FL) to UAV networks has improved collaboration, privacy, resilience, and adaptability, making it a promising framework for UAV applications. However, implementing FL for UAV networks introduces drawbacks such as communication overhead, synchronization issues, scalability limitations, and resource constraints. To address these challenges, this paper presents the Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) framework for UAV networks. This improves the decentralization, coordination, scalability, and efficiency of FL in large-scale UAV networks. The framework partitions UAV networks into separate clusters, coordinated by cluster head UAVs (CHs), to establish a connected graph. Clustering enables
    
[^148]: 多视角符号回归

    Multi-View Symbolic Regression

    [https://arxiv.org/abs/2402.04298](https://arxiv.org/abs/2402.04298)

    多视角符号回归(MvSR)是一种同时考虑多个数据集的符号回归方法，能够找到一个参数化解来准确拟合所有数据集，解决了传统方法无法处理不同实验设置的问题。

    

    符号回归(SR)搜索表示解释变量和响应变量之间关系的分析表达式。目前的SR方法假设从单个实验中提取的单个数据集。然而，研究人员经常面临来自不同设置的多个实验结果集。传统的SR方法可能无法找到潜在的表达式，因为每个实验的参数可能不同。在这项工作中，我们提出了多视角符号回归(MvSR)，它同时考虑多个数据集，模拟实验环境，并输出一个通用的参数化解。这种方法将评估的表达式适应每个独立数据集，并同时返回能够准确拟合所有数据集的参数函数族f(x; \theta)。我们使用从已知表达式生成的数据以及来自实际世界的数据来展示MvSR的有效性。

    Symbolic regression (SR) searches for analytical expressions representing the relationship between a set of explanatory and response variables. Current SR methods assume a single dataset extracted from a single experiment. Nevertheless, frequently, the researcher is confronted with multiple sets of results obtained from experiments conducted with different setups. Traditional SR methods may fail to find the underlying expression since the parameters of each experiment can be different. In this work we present Multi-View Symbolic Regression (MvSR), which takes into account multiple datasets simultaneously, mimicking experimental environments, and outputs a general parametric solution. This approach fits the evaluated expression to each independent dataset and returns a parametric family of functions f(x; \theta) simultaneously capable of accurately fitting all datasets. We demonstrate the effectiveness of MvSR using data generated from known expressions, as well as real-world data from 
    
[^149]: 可解释的多源数据融合通过潜变量高斯过程

    Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process

    [https://arxiv.org/abs/2402.04146](https://arxiv.org/abs/2402.04146)

    这篇论文提出了一种基于潜变量高斯过程的多源数据融合框架，用于解决多个数据源之间质量和全面性差异给系统优化带来的问题。

    

    随着人工智能（AI）和机器学习（ML）的出现，各个科学和工程领域已经利用数据驱动的替代模型来建模来自大量信息源（数据）的复杂系统。这种增加导致了开发出用于执行特定功能的优越系统所需的成本和时间的显著降低。这样的替代模型往往广泛地融合多个数据来源，可能是发表的论文、专利、开放资源库或其他资源。然而，对于已知和未知的信息来源的基础物理参数的质量和全面性的差异，可能对系统优化过程产生后续影响，却没有得到充分的关注。为了解决这个问题，提出了一种基于潜变量高斯过程（LVGP）的多源数据融合框架。

    With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
    
[^150]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^151]: Vision-LLMs通过自动生成的排版攻击可以自欺欺人

    Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks

    [https://arxiv.org/abs/2402.00626](https://arxiv.org/abs/2402.00626)

    这项研究深入研究了大规模视觉语言模型（LVLM）对于自动生成的排版攻击的易受攻击性，并引入了一种新的、更有效的自动生成的排版攻击方法，为此设计了一个独特的测试基准。通过使用该基准，研究发现排版攻击对LVLM构成了重大威胁。

    

    最近，在大规模视觉语言模型（LVLM）方面取得了重大进展；这是一种利用大型预训练语言模型的全新类别的视觉语言模型。然而，LVLM对于涉及将误导性文本叠加到图像上的从排版攻击的容易受攻击性却没有研究。此外，先前的排版攻击依赖于从预定义类别集合中随机选择一个误导性类别。然而，随机选择的类别可能不是最有效的攻击类别。为了解决这些问题，我们首先引入了一种独特设计的新颖基准来测试LVLM对排版攻击的容易受攻击性。此外，我们介绍了一种新而更有效的排版攻击：自动生成的排版攻击。实际上，我们的方法通过简单地提示GPT-4V等模型利用其强大的语言能力推荐一种排版攻击来为给定的图像生成攻击。使用我们的新颖基准，我们发现排版攻击对LVLM构成了重大威胁。

    Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furth
    
[^152]: 短文: 基准测试可转移性对抗攻击

    Short: Benchmarking transferable adversarial attacks

    [https://arxiv.org/abs/2402.00418](https://arxiv.org/abs/2402.00418)

    本研究首次全面评估了可转移性对抗攻击的方面，引入了一个基准框架并系统分类和评估了各种增强对抗攻击可转移性的方法，为不同的模型架构提供了一个标准化的平台。

    

    深度学习模型对抗攻击的稳健性仍然是一个关键关注点。本研究首次全面评估了可转移性对抗攻击的方面。它系统地分类和批判性评估了各种增强对抗攻击可转移性的方法。该研究涵盖了一系列技术，包括生成结构、语义相似性、梯度编辑、目标修改和集成方法。与此同时，本文介绍了一个基准框架"TAA-Bench"，集成了十种主要的对抗攻击可转移性方法，从而为不同的模型架构提供了一个标准化和系统化的比较分析平台。通过全面的审查，我们揭示了每种方法的效力和限制，并阐明了它们的操作原理和实际效用。本综述试图成为一种基于多模型架构进行对比分析的标准化平台。

    The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintesse
    
[^153]: 集成混合NOMA-OFDM的HAP与LEO卫星网络的通信高效联合学习

    Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM

    [https://arxiv.org/abs/2401.00685](https://arxiv.org/abs/2401.00685)

    本文提出了NomaFedHAP这一新型FL-SatCom方法，利用HAPs作为PS来增强卫星可见性，并引入NOMA技术实现快速高效的模型传输。

    

    太空人工智能对政府、企业和社会变得日益重要，有时甚至成为必需。本文提出了一种针对LEO卫星量身定制的新型FL-SatCom方法NomaFedHAP，该方法利用高空平台(HAPs)作为分布式参数服务器(PS)来增强卫星的可见性，引入非正交多址接入(NOMA)到LEO，以实现快速和带宽高效的模型传输。

    arXiv:2401.00685v2 Announce Type: replace-cross  Abstract: Space AI has become increasingly important and sometimes even necessary for government, businesses, and society. An active research topic under this mission is integrating federated learning (FL) with satellite communications (SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively train a machine learning model. However, the special communication environment of SatCom leads to a very slow FL training process up to days and weeks. This paper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO satellites, that (1) utilizes high-altitude platforms (HAPs) as distributed parameter servers (PS) to enhance satellite visibility, and (2) introduces non-orthogonal multiple access (NOMA) into LEO to enable fast and bandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a new communication topology that exploits HAPs to bridge satellites among different orbits to mitigate the Dopple
    
[^154]: 来自协方差核的高斯过程样本路径正则性

    Sample Path Regularity of Gaussian Processes from the Covariance Kernel

    [https://arxiv.org/abs/2312.14886](https://arxiv.org/abs/2312.14886)

    本文提供了关于高斯过程样本路径正则性的新颖和紧凑的特征描述，通过协方差核对应的GP样本路径达到一定正则性的充分必要条件，对常用于机器学习应用中的GPs的样本路径正则性进行了探讨。

    

    高斯过程（GPs）是定义函数空间上的概率分布的最常见形式主义。尽管GPs的应用广泛，但对于GP样本路径的全面理解，即它们定义概率测度的函数空间，尚缺乏。在实践中，GPs不是通过概率测度构建的，而是通过均值函数和协方差核构建的。本文针对协方差核提供了GP样本路径达到给定正则性所需的充分必要条件。我们使用H\"older正则性框架，因为它提供了特别简单的条件，在平稳和各向同性GPs的情况下进一步简化。然后，我们证明我们的结果允许对机器学习应用中常用的GPs的样本路径正则性进行新颖且异常紧凑的表征。

    arXiv:2312.14886v2 Announce Type: replace  Abstract: Gaussian processes (GPs) are the most common formalism for defining probability distributions over spaces of functions. While applications of GPs are myriad, a comprehensive understanding of GP sample paths, i.e. the function spaces over which they define a probability measure, is lacking. In practice, GPs are not constructed through a probability measure, but instead through a mean function and a covariance kernel. In this paper we provide necessary and sufficient conditions on the covariance kernel for the sample paths of the corresponding GP to attain a given regularity. We use the framework of H\"older regularity as it grants particularly straightforward conditions, which simplify further in the cases of stationary and isotropic GPs. We then demonstrate that our results allow for novel and unusually tight characterisations of the sample path regularities of the GPs commonly used in machine learning applications, such as the Mat\'
    
[^155]: Q-SENN: 量化自解释神经网络

    Q-SENN: Quantized Self-Explaining Neural Networks

    [https://arxiv.org/abs/2312.13839](https://arxiv.org/abs/2312.13839)

    Q-SENN提出了量化自解释神经网络，通过描述每个类与特征之间的关系为正、负或中性，实现更二元化、更符合人类友好的特征，同时在准确性方面优于先前的工作。

    

    计算机视觉中常需要解释，但大多数深度神经网络只能提供具有可疑忠实度的显著性地图。自解释神经网络(SENN)提取可解释的概念，具有忠实度、多样性和基础性，将它们线性组合用于决策。我们提出了量化自解释神经网络Q-SENN。Q-SENN满足或超过SENN的期望，同时适用于更复杂的数据集，并保持了与不可解释基线模型几乎相同的准确性，优于以往所有考虑指标的工作。Q-SENN将每个类与特征之间的关系描述为正、负或中性，而不是任意数量的可能关系，强制采用更二元化、更符合人类友好的特征。

    arXiv:2312.13839v2 Announce Type: replace-cross  Abstract: Explanations in Computer Vision are often desired, but most Deep Neural Networks can only provide saliency maps with questionable faithfulness. Self-Explaining Neural Networks (SENN) extract interpretable concepts with fidelity, diversity, and grounding to combine them linearly for decision-making. While they can explain what was recognized, initial realizations lack accuracy and general applicability. We propose the Quantized-Self-Explaining Neural Network Q-SENN. Q-SENN satisfies or exceeds the desiderata of SENN while being applicable to more complex datasets and maintaining most or all of the accuracy of an uninterpretable baseline model, out-performing previous work in all considered metrics. Q-SENN describes the relationship between every class and feature as either positive, negative or neutral instead of an arbitrary number of possible relations, enforcing more binary human-friendly features. Since every class is assign
    
[^156]: BloomVQA：评估分层多模态理解

    BloomVQA: Assessing Hierarchical Multi-modal Comprehension

    [https://arxiv.org/abs/2312.12716](https://arxiv.org/abs/2312.12716)

    提出了新VQA数据集BloomVQA，基于Bloom的分类法，通过层次图表示实现数据增强和模型一致性评估，揭示大型视觉语言模型在高级理解任务上的性能下降。

    

    我们提出了一个新颖的VQA数据集BloomVQA，旨在促进对大型视觉语言模型在理解任务上的全面评估。与当前的基准不同，它们通常侧重于基于事实的记忆和没有理论基础的简单推理任务，我们收集了基于图片故事的多项选择样本，反映了不同层次的理解，正如布鲁姆的分类法所展示的，在教育研究中被广泛采用的经典框架。我们的数据映射到一种新颖的分层图表示，实现了自动数据增强和表征模型一致性的新措施。我们对最近的多模态模型进行了分级评估和可靠性分析。与低级任务相比，我们发现在需要高级理解和认知能力的任务上表现下降，VQA准确性下降了高达38.0%。与早期模型相比，GPT-4V表现出...

    arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons
    
[^157]: KGLens：一个参数化知识图解决方案，用于评估LLM知道和不知道的内容

    KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know

    [https://arxiv.org/abs/2312.11539](https://arxiv.org/abs/2312.11539)

    KGLens 是一个旨在衡量知识图与大型语言模型（LLMs）之间对齐程度的框架，帮助找出LLMs相对于知识图的知识不足之处。

    

    衡量知识图（KG）与大型语言模型（LLMs）之间的对齐程度是评估事实性并识别LLMs的知识盲点的有效方法。然而，这种方法面临两个主要挑战，包括将KGs转化为自然语言和高效评估这些广泛且复杂的结构。在本文中，我们提出了KGLens--一个旨在衡量KGs和LLMs之间对齐程度，并找出LLMs相对于KGs的知识缺陷的新颖框架。KGLens具有一个图引导的问题生成器，用于将KGs转化为自然语言，以及一个基于参数化KG结构的精心设计的采样策略，以加快KG的遍历。我们使用来自Wikidata的三个领域特定KG进行实验，这些KG包括超过19,000条边，700个关系和21,000个实体。我们跨越8个LLMs的分析表明，KGLens不仅

    arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
    
[^158]: 用于更快的LLM推理的级联推测草图

    Cascade Speculative Drafting for Even Faster LLM Inference

    [https://arxiv.org/abs/2312.11462](https://arxiv.org/abs/2312.11462)

    引入了Cascade Speculative Drafting（CS Drafting）算法，通过垂直级联消除神经模型的自回归生成，通过水平级联优化草稿中的时间分配，从而进一步提高LLM推理效率。

    

    引入了增强大型语言模型（LLM）推理效率的级联推测草图，通过较小的模型生成草稿来运作。较大的目标模型然后查看这个草稿以与其输出对齐，目标模型的任何接受都将减少目标模型运行的数量，从而提高效率。然而，在级联推测的草图过程中包括缓慢的自回归生成，并为生成的标记分配相同的时间，而不考虑它们的重要性。这些低效性共同导致级联推测的性能不佳。为了进一步改善LLM推理，我们引入了级联推测草图（CS Drafting），这是一种整合了两种级联类型的推测执行算法。垂直级联从神经模型中消除自回归生成，而水平级联优化了草稿中的时间分配

    arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
    
[^159]: 具有硬件高效训练的门控线性注意力变换器

    Gated Linear Attention Transformers with Hardware-Efficient Training

    [https://arxiv.org/abs/2312.06635](https://arxiv.org/abs/2312.06635)

    该论文提出了一种具有硬件高效性的线性注意力算法，可在短序列长度下比现有方法更快，同时推广到了具有数据相关门的更具表达能力的线性注意力变体。

    

    具有线性注意力的变压器允许进行高效的并行训练，同时可以被表述为具有2D（矩阵值）隐藏状态的RNN，从而享受线性时间推断复杂度。然而，线性注意力通常表现不如普通softmax注意力。而且，当前的线性注意力实现缺乏I/O感知性，因此比高度优化的softmax注意力实现更慢。本文描述了一种适用于线性注意力的硬件高效算法，它在内存移动和可并行性之间进行折中。由此产生的实现，被称为FLASHLINEARATTENTION，在短序列长度（例如，1K）下，即使作为单独的层也比FLASHATTENTION-2(Dao, 2023)更快。然后，我们将该算法推广到具有数据相关门的更具表达能力的线性注意力变体。当用作变换器中标准注意力层的替代时，产生的门控

    arXiv:2312.06635v4 Announce Type: replace-cross  Abstract: Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2(Dao, 2023) as a standalone layer even at short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gate
    
[^160]: Transformers实现了功能性梯度下降来学习上下文中的非线性函数

    Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context

    [https://arxiv.org/abs/2312.06528](https://arxiv.org/abs/2312.06528)

    本论文证明了Transformers可以在函数空间中自然地学习实现梯度下降，从而学习上下文中的非线性函数，并且展示了最优的非线性激活函数选择取决于需要学习的函数类别。

    

    许多神经网络架构被认为是图灵完备的，因此原则上可以实现任意算法。然而，Transformers独特之处在于它们可以在简单的参数配置下实现基于梯度的学习算法。本文提供了理论和实证证据，证明（非线性）Transformers自然地学会在函数空间中实现梯度下降，从而使它们能够在上下文中学习非线性函数。我们的结果适用于一大类非线性架构和非线性上下文学习任务的组合。此外，我们展示了非线性激活函数的最优选择以一种自然的方式取决于需要学习的函数类别。

    arXiv:2312.06528v4 Announce Type: replace  Abstract: Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.
    
[^161]: 帕金森病识别的基于图的正负样本学习方法PULSAR：具有多通道自适应卷积

    PULSAR: Graph based Positive Unlabeled Learning with Multi Stream Adaptive Convolutions for Parkinson's Disease Recognition

    [https://arxiv.org/abs/2312.05780](https://arxiv.org/abs/2312.05780)

    PULSAR是一种新颖的方法，用于通过分析手指敲击任务的视频来筛查帕金森病，采用了自适应图卷积神经网络和多通道自适应卷积来动态学习时空图边。

    

    帕金森病（PD）是一种影响运动、言语和协调的神经退行性疾病。及时诊断和治疗可以改善PD患者的生活质量。然而，低收入和中等收入国家（LMICs）的临床诊断资源有限。因此，开发用于PD的自动筛查工具对社会影响巨大，特别是在公共卫生领域。本文提出了PULSAR，一种新颖的方法，用于从运动障碍学会-统一帕金森病评分量表（MDS-UPDRS）的手指敲击任务的网络摄像头录制视频中进行PD筛查。PULSAR在382名参与者（183名自我报告为PD患者）收集的数据上进行训练和评估。我们使用自适应图卷积神经网络动态学习特定于手指敲击任务的时空图边。我们结合了多通道自适应卷积来增强这一想法。

    arXiv:2312.05780v2 Announce Type: replace-cross  Abstract: Parkinson's disease (PD) is a neuro-degenerative disorder that affects movement, speech, and coordination. Timely diagnosis and treatment can improve the quality of life for PD patients. However, access to clinical diagnosis is limited in low and middle income countries (LMICs). Therefore, development of automated screening tools for PD can have a huge social impact, particularly in the public health sector. In this paper, we present PULSAR, a novel method to screen for PD from webcam-recorded videos of the finger-tapping task from the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS). PULSAR is trained and evaluated on data collected from 382 participants (183 self-reported as PD patients). We used an adaptive graph convolutional neural network to dynamically learn the spatio temporal graph edges specific to the finger-tapping task. We enhanced this idea with a multi stream adaptive convolution m
    
[^162]: 使用特征生成和融合的半监督健康指数监测

    Semi-Supervised Health Index Monitoring with Feature Generation and Fusion

    [https://arxiv.org/abs/2312.02867](https://arxiv.org/abs/2312.02867)

    通过深度半监督异常检测（DeepSAD）方法进行健康指数构建，并提出了多样性损失来丰富条件指标。

    

    健康指数（HI）对于评估系统健康状态至关重要，有助于识别异常，并预测对高安全性和可靠性要求高的系统的剩余使用寿命。在实现高精度的同时降低成本，紧密监测至关重要。在现实应用中获取HI标签往往成本高昂，需要连续、精确的健康测量。因此，利用可能提供潜在机器磨损状态指示的“运行至故障”数据集，更方便采用半监督工具构建HI。

    arXiv:2312.02867v2 Announce Type: replace  Abstract: The Health Index (HI) is crucial for evaluating system health, aiding tasks like anomaly detection and predicting remaining useful life for systems demanding high safety and reliability. Tight monitoring is crucial for achieving high precision at a lower cost. Obtaining HI labels in real-world applications is often cost-prohibitive, requiring continuous, precise health measurements. Therefore, it is more convenient to leverage run-to failure datasets that may provide potential indications of machine wear condition, making it necessary to apply semi-supervised tools for HI construction. In this study, we adapt the Deep Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use the DeepSAD embedding as a condition indicators to address interpretability challenges and sensitivity to system-specific factors. Then, we introduce a diversity loss to enrich condition indicators. We employ an alternating projection algorit
    
[^163]: 高斯LTI系统中干预视角下的可识别性与独立成分分析

    An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis

    [https://arxiv.org/abs/2311.18048](https://arxiv.org/abs/2311.18048)

    本研究通过在高斯LTI系统中引入各种干预信号，连接了实验设计和动态系统中的表示可识别性，为系统参数的识别提供了新的方法。

    

    我们研究了动态系统中系统识别和干预设计之间的关系。以前的研究表明，诸如独立成分分析（ICA）的可识别表示学习方法可以揭示因果关系，但它们依赖于一种被动的视角，没有考虑如何收集数据。我们的工作表明，在高斯线性时不变（LTI）系统中，通过在多环境设置中引入各种干预信号，可以识别系统参数。通过利用ICA文献激发的适当多样性假设，我们的发现将实验设计与动态系统中的表示识别能力联系起来。我们在合成和（模拟）物理数据上证实了我们的发现。此外，我们还展示了隐马尔可夫模型（HMM）总体上以及（高斯）LTI系统特别满足Causal de Finetti 的一般化。

    arXiv:2311.18048v2 Announce Type: replace  Abstract: We investigate the relationship between system identification and intervention design in dynamical systems. While previous research demonstrated how identifiable representation learning methods, such as Independent Component Analysis (ICA), can reveal cause-effect relationships, it relied on a passive perspective without considering how to collect data. Our work shows that in Gaussian Linear Time-Invariant (LTI) systems, the system parameters can be identified by introducing diverse intervention signals in a multi-environment setting. By harnessing appropriate diversity assumptions motivated by the ICA literature, our findings connect experiment design and representational identifiability in dynamical systems. We corroborate our findings on synthetic and (simulated) physical data. Additionally, we show that Hidden Markov Models, in general, and (Gaussian) LTI systems, in particular, fulfil a generalization of the Causal de Finetti th
    
[^164]: K空间冷扩散：学习在没有噪音的情况下重建加速MRI

    K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise

    [https://arxiv.org/abs/2311.10162](https://arxiv.org/abs/2311.10162)

    提出了一种在K空间进行图像退化和恢复的冷扩散模型，无需噪音，能够为加速MRI生成高质量的重建图像

    

    基于深度学习的MRI重建模型近年来取得了优异的表现。最近，扩散模型在图像生成、修补、超分辨率、图像编辑等方面表现出色。作为一种通用的扩散模型，冷扩散进一步拓宽了范围，并考虑了围绕任意图像变换构建的模型，例如模糊、下采样等。本文提出了一种在K空间中执行图像退化和恢复的K空间冷扩散模型，无需高斯噪声。我们与多个基于深度学习的MRI重建模型进行比较，并在一个知名的大型开源MRI数据集上进行测试。我们的结果表明，这种新颖的退化方式可以为加速MRI生成高质量的重建图像。

    arXiv:2311.10162v2 Announce Type: replace-cross  Abstract: Deep learning-based MRI reconstruction models have achieved superior performance these days. Most recently, diffusion models have shown remarkable performance in image generation, in-painting, super-resolution, image editing and more. As a generalized diffusion model, cold diffusion further broadens the scope and considers models built around arbitrary image transformations such as blurring, down-sampling, etc. In this paper, we propose a k-space cold diffusion model that performs image degradation and restoration in k-space without the need for Gaussian noise. We provide comparisons with multiple deep learning-based MRI reconstruction models and perform tests on a well-known large open-source MRI dataset. Our results show that this novel way of performing degradation can generate high-quality reconstruction images for accelerated MRI.
    
[^165]: 探究大型语言模型如何表达对超出参数化知识范围的问题的不确定性

    Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge

    [https://arxiv.org/abs/2311.09731](https://arxiv.org/abs/2311.09731)

    本研究系统地调查了大型语言模型在缺乏足够参数化知识的情况下如何表达对超出其知识范围的问题的不确定性，并强调了诚实与帮助性之间的权衡。

    

    这项工作旨在系统地调查大型语言模型在缺乏足够参数化知识以生成合理回应的情况下的行为，强调诚实与帮助性之间的权衡。为了精确确定语言模型的知识空白挑战，我们诊断性地创建了包含不存在概念或错误前提的无法回答的问题，确保它们超出了语言模型庞大的训练数据。通过编制一个包含既有无法回答也有可回答问题的基准，UnknownBench，我们定量评估语言模型在保持诚实的同时提供帮助的表现。使用一个模型无关的统一信心引导方法，我们观察到大多数语言模型在一致拒绝或表达对超出其参数化知识范围的问题的不确定性方面表现不佳。

    arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin
    
[^166]: 自然度解释与评估模型（CNE）：解释和评估形成自然度的模式的框架

    Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness

    [https://arxiv.org/abs/2311.08936](https://arxiv.org/abs/2311.08936)

    本文提出了一个新的框架，称为自然度解释与评估模型（CNE）。该框架利用可解释的机器学习方法来分析卫星图像，以解释和评估形成自然度的模式，并带有相应的置信度。

    

    受人类活动（如城市化、农业和其他人为干预）最小影响的自然保护区是指那些地区。为了更好地理解和绘制这些区域的自然度，可以使用机器学习模型分析卫星图像。具体而言，可解释的机器学习方法在揭示这些保护环境中有助于自然度概念的模式方面显示出了潜力。此外，解决与机器学习模型固有的不确定性相关的问题对于全面了解这个概念至关重要。然而，现有方法存在局限性。他们要么无法提供既有效又客观的解释，要么难以提供准确衡量特定模式对自然度贡献的定量指标以及相关的置信度。在本文中，我们提出了一种新颖的框架，称为自然度解释与评估模型（CNE）。

    Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combi
    
[^167]: FLrce: 具有提前停止策略的资源高效联邦学习

    FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy

    [https://arxiv.org/abs/2310.09789](https://arxiv.org/abs/2310.09789)

    FLrce方法通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，解决了安全和资源利用不均衡问题。

    

    针对边缘设备资源短缺和不平衡的训练贡献，提出了FLrce方法，通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，在不泄露本地数据的情况下，有效解决了传统联邦学习中存在的安全和资源利用不均衡问题。

    arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener
    
[^168]: 谨慎平滑标签：标签平滑既可以作为隐私屏障，又可以成为模型反推攻击的催化剂

    Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks

    [https://arxiv.org/abs/2310.06549](https://arxiv.org/abs/2310.06549)

    标签平滑方法在深度学习中发挥重要作用，既能提升模型泛化能力和校准性，又可能成为模型隐私泄露的因素。研究揭示了结合负因子进行平滑可有效阻止模型反推攻击，提升隐私保护效果，超越了当前最先进的防御技术。

    

    标签平滑——使用软化的标签而不是硬标签——是深度学习中被广泛采用的正则化方法，表现出增强泛化和校准等多样益处。然而，它对于保护模型隐私的影响仍然没有被探索。为了填补这一空白，我们调查了标签平滑对模型反推攻击（MIAs）的影响，这些攻击旨在通过利用分类器中编码的知识生成具有类代表性的样本，从而推断有关其训练数据的敏感信息。通过广泛的分析，我们发现传统标签平滑促进了MIAs，从而增加了模型的隐私泄露。更甚者，我们揭示了用负因子进行平滑可以抵制这一趋势，阻碍提取与类相关的信息，实现隐私保护，胜过最先进的防御方法。这确立了一种实用且强大的新的增强方式。

    arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
    
[^169]: 通过去噪扩散概率模型进行生成性量子机器学习

    Generative quantum machine learning via denoising diffusion probabilistic models

    [https://arxiv.org/abs/2310.05866](https://arxiv.org/abs/2310.05866)

    通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。

    

    深度生成模型是计算机视觉、文本生成和大型语言模型的关键技术。最近，由于其能够生成多样化和高质量的样本，以及结构灵活、训练简单的特点，去噪扩散概率模型（DDPMs）在许多计算机视觉任务中受到了广泛关注。量子生成模型利用纠缠和叠加的能力为学习经典和量子数据带来了新的见解。受经典模型的启发，我们提出了“量子去噪扩散概率模型”（QuDDPM），以实现对量子数据的高效可训练的生成学习。QuDDPM采用足够层数的电路来保证表达能力，同时引入多个中间训练任务，将目标分布与噪声之间的插值作为训练过程，以避免贫瘠平原并保证高效的训练。我们给出了学习误差的上界和...（未完待续）

    Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
    
[^170]: 联邦式K均值聚类

    Federated K-means Clustering

    [https://arxiv.org/abs/2310.01195](https://arxiv.org/abs/2310.01195)

    本研究提出一种实现K均值聚类的联邦学习算法，解决了中心之间聚类数量变化和在不太可分数据集上的收敛挑战。

    

    联邦学习是一种技术，可以在不需要汇总数据的情况下利用分布式数据集进行机器学习，从而更好地保护数据的隐私和所有权。尽管监督式联邦学习研究在过去几年中取得了显著进展，但无监督式联邦学习方法仍然很少见。本文介绍了一种算法，以联邦方式实现K均值聚类，解决了中心之间聚类数量不同以及在较难分开的数据集上收敛的挑战。

    arXiv:2310.01195v2 Announce Type: replace  Abstract: Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.
    
[^171]: 光谢尔宾格桥

    Light Schr\"odinger Bridge

    [https://arxiv.org/abs/2310.01174](https://arxiv.org/abs/2310.01174)

    提出了一种新颖的光谢尔宾格桥快速简单求解器，将谢尔宾格势能参数化为总和-指数二次函数，视为能量函数，实现了轻量级、无需模拟、理论上合理的求解器设计。

    

    尽管计算谢尔宾格桥（SB）领域取得了最新进展，但大多数现有的SB求解器仍然过重，并需要对多个神经网络进行复杂优化。事实证明，目前不存在主要的求解器可以像聚类中的$k$-means方法、分类中的逻辑回归或离散最优输运中的Sinkhorn算法那样起到简单而有效的基准作用。我们解决了这个问题，提出了一种新颖的快速简单的SB求解器。我们的方法是最近出现在该领域的两个观点的巧妙结合：（a）使用总和-指数二次函数对谢尔宾格势能进行参数化，（b）将对数谢尔宾格势能视为能量函数。我们展示了将这些观点结合起来，可以产生一个轻量级、无需模拟且在理论上证明合理的SB求解器，具有简单直接的优化目标。因此，它是

    arXiv:2310.01174v2 Announce Type: replace  Abstract: Despite the recent advances in the field of computational Schrodinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., $k$-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schrodinger potentials with sum-exp quadratic functions and (b) viewing the log-Schrodinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it a
    
[^172]: 基于代码训练的语言模型的冗余和概念分析

    Redundancy and Concept Analysis for Code-trained Language Models

    [https://arxiv.org/abs/2305.00875](https://arxiv.org/abs/2305.00875)

    本文针对代码训练的语言模型进行冗余和概念分析，通过消除与给定任务不相关的神经元，帮助理解网络中哪些神经元和层可以被消除，并在哪里找到重要的代码属性。

    

    针对代码训练的语言模型在各种代码智能任务中表现出高效性。然而，由于计算瓶颈和内存限制，对于许多软件工程应用来说，它们可能难以训练和部署。为了解决这些问题，实施有效的策略需要更好地理解这些“黑匣子”模型。本文针对源代码模型进行了首次神经元级别分析，以识别潜在表示中的“重要”神经元。我们通过消除与给定任务高度相似或不相关的神经元来实现这一点。这种方法有助于我们了解哪些神经元和层可以被消除（冗余分析），以及网络中的重要代码属性位于何处（概念分析）。利用冗余分析，我们得出了与知识转移和模型优化应用相关的观察结果。

    arXiv:2305.00875v2 Announce Type: replace-cross  Abstract: Code-trained language models have proven to be highly effective for various code intelligence tasks. However, they can be challenging to train and deploy for many software engineering applications due to computational bottlenecks and memory constraints. Implementing effective strategies to address these issues requires a better understanding of these 'black box' models. In this paper, we perform the first neuron-level analysis for source code models to identify \textit{important} neurons within latent representations. We achieve this by eliminating neurons that are highly similar or irrelevant to the given task. This approach helps us understand which neurons and layers can be eliminated (redundancy analysis) and where important code properties are located within the network (concept analysis). Using redundancy analysis, we make observations relevant to knowledge transfer and model optimization applications. We find that over 9
    
[^173]: 可解释的多视图学习深度学习方法

    Interpretable Deep Learning Methods for Multiview Learning

    [https://arxiv.org/abs/2302.07930](https://arxiv.org/abs/2302.07930)

    提出iDeepViewLearn（可解释的多视图学习深度学习方法）用于多视图数据的非线性关系学习和特征选择，融合深度学习的灵活性和统计优势，给出可解释结果。

    

    技术进步已经实现了生成独特且互补类型的数据或视图（例如基因组学、蛋白质组学、代谢组学），并开创了多视图学习研究新时代，有潜力带来新的生物医学发现。我们提出 iDeepViewLearn（Interpretable Deep Learning Method for Multiview Learning），用于学习来自多视图数据中的非线性关系，同时实现特征选择。iDeepViewLearn结合了深度学习的灵活性与数据和基于知识的特征选择的统计优势，以给出可解释的结果。深度神经网络用于通过最小化观察数据与重构数据之间的差异，并对重构数据施加正则化惩罚来学习视图独立的低维嵌入。图的归一化拉普拉斯用于建模变量之间的双边关系。

    arXiv:2302.07930v2 Announce Type: replace  Abstract: Technological advances have enabled the generation of unique and complementary types of data or views (e.g. genomics, proteomics, metabolomics) and opened up a new era in multiview learning research with the potential to lead to new biomedical discoveries. We propose iDeepViewLearn (Interpretable Deep Learning Method for Multiview Learning) for learning nonlinear relationships in data from multiple views while achieving feature selection. iDeepViewLearn combines deep learning flexibility with the statistical benefits of data and knowledge-driven feature selection, giving interpretable results. Deep neural networks are used to learn view-independent low-dimensional embedding through an optimization problem that minimizes the difference between observed and reconstructed data, while imposing a regularization penalty on the reconstructed data. The normalized Laplacian of a graph is used to model bilateral relationships between variables
    
[^174]: 单细胞是空间标记：用于空间转录组数据填充的Transformer

    Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation

    [https://arxiv.org/abs/2302.03038](https://arxiv.org/abs/2302.03038)

    本文探讨了如何将单细胞视为空间标记，并利用Transformer模型促进空间转录组数据填充。

    

    空间解析转录组学通过提供物理位置和基因表达带来了单细胞分析的激动人心的突破。然而，由于极高的空间分辨率成本，细胞水平的空间转录组数据在很大程度上存在缺失值。本文提出将单个细胞视为空间标记，并通过使用多头自注意机制和位置编码的Transformer模型来促进空间转录组填充。

    arXiv:2302.03038v2 Announce Type: replace-cross  Abstract: Spatially resolved transcriptomics brings exciting breakthroughs to single-cell analysis by providing physical locations along with gene expression. However, as a cost of the extremely high spatial resolution, the cellular level spatial transcriptomic data suffer significantly from missing values. While a standard solution is to perform imputation on the missing values, most existing methods either overlook spatial information or only incorporate localized spatial context without the ability to capture long-range spatial information. Using multi-head self-attention mechanisms and positional encoding, transformer models can readily grasp the relationship between tokens and encode location information. In this paper, by treating single cells as spatial tokens, we study how to leverage transformers to facilitate spatial tanscriptomics imputation. In particular, investigate the following two key questions: (1) $\textit{how to encod
    
[^175]: 层级奖励函数：规定和快速学习所需行为

    Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior

    [https://arxiv.org/abs/2212.03733](https://arxiv.org/abs/2212.03733)

    层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。

    

    强化学习代理通过与环境的交互来最大化奖励信号。在学习过程中，我们作为人类的任务是设计奖励函数，以表达所期望的行为，并使代理能够迅速学习这种行为。在这项工作中，我们考虑了任务中达到良好状态和避免不良状态的奖励设计问题。首先，我们提出了一种严格的策略空间的部分排序，以解决行为偏好中的权衡。我们更倾向于能更快速地到达良好状态并以更高的概率到达，同时能更长时间地避免不良状态的策略。接下来，我们介绍了层级奖励，一类与环境无关的奖励函数，并表明它保证诱导出根据我们的偏好关系是帕累托最优的策略。最后，我们证明了层级奖励可以通过在多个环境上使用多个表格进行评估，从而实现快速学习。

    arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
    
[^176]: 水平联邦学习模型的单方面劫持

    Hijack Vertical Federated Learning Models As One Party

    [https://arxiv.org/abs/2212.00322](https://arxiv.org/abs/2212.00322)

    这项研究探讨了水平联邦学习模型的安全性，弥补了现有研究中对于该模型安全性的不足。

    

    水平联邦学习（VFL）是一种新兴范式，使合作伙伴能够以分布式方式共同构建机器学习模型。一般来说，这些参与方有一组共同用户，但拥有不同特征。现有的VFL框架使用密码技术提供数据隐私和安全性保证，导致了一系列研究计算效率和快速实现的工作。然而，VFL模型的安全性仍未得到充分探讨。

    arXiv:2212.00322v2 Announce Type: replace-cross  Abstract: Vertical federated learning (VFL) is an emerging paradigm that enables collaborators to build machine learning models together in a distributed fashion. In general, these parties have a group of users in common but own different features. Existing VFL frameworks use cryptographic techniques to provide data privacy and security guarantees, leading to a line of works studying computing efficiency and fast implementation. However, the security of VFL's model remains underexplored.
    
[^177]: 最优扩展邻域规则$k$最近邻集成

    Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble

    [https://arxiv.org/abs/2211.11278](https://arxiv.org/abs/2211.11278)

    提出了一种基于最优扩展邻域规则的集成方法，通过新规则确定邻居和模型选择策略来解决传统$k$最近邻方法的局限性和提升集成性能。

    

    传统的$k$最近邻($k$NN)方法使用一个球形区域内的距离公式来确定训练观测中与测试样本点最接近的$k$个观测。然而，当测试点位于该区域之外时，这种方法可能不起作用。此外，聚合许多基础$k$NN学习器可能会导致由于高分类误差而表现不佳的集成性能。为解决这些问题，本文提出了一种新的基于最优扩展邻域规则的集成方法。该规则从距离未见观测最近的样本点开始，经过$k$步确定邻居，并选择直到达到所需数量的观测数据点。每个基础模型都是在一个随机特征子集上的自举样本上构建的，并且在构建足够数量的模型后基于袋外表现选择最优模型。提出的集成方法与st进行了比较

    arXiv:2211.11278v2 Announce Type: replace-cross  Abstract: The traditional k nearest neighbor (kNN) approach uses a distance formula within a spherical region to determine the k closest training observations to a test sample point. However, this approach may not work well when test point is located outside this region. Moreover, aggregating many base kNN learners can result in poor ensemble performance due to high classification errors. To address these issues, a new optimal extended neighborhood rule based ensemble method is proposed in this paper. This rule determines neighbors in k steps starting from the closest sample point to the unseen observation and selecting subsequent nearest data points until the required number of observations is reached. Each base model is constructed on a bootstrap sample with a random subset of features, and optimal models are selected based on out-of-bag performance after building a sufficient number of models. The proposed ensemble is compared with st
    
[^178]: 在多环境下为FDD-OFDM系统实现基于深度学习的物理层秘钥生成

    Enabling Deep Learning-based Physical-layer Secret Key Generation for FDD-OFDM Systems in Multi-Environments

    [https://arxiv.org/abs/2211.03065](https://arxiv.org/abs/2211.03065)

    本文在多个环境下将物理层秘钥生成问题转化为学习性问题，提出了基于深度传递学习和元学习的信道特征映射算法，实现了在多个新环境中快速有效地生成秘钥。

    

    基于深度学习的物理层秘钥生成（PKG）被用于克服频分双工（FDD）正交频分复用（OFDM）系统中不完美的上下行信道互易性。然而，现有的研究主要集中在针对特定环境中用户的秘钥生成，其中训练样本和测试样本遵循相同的分布，这对于实际应用是不现实的。本文将多环境下的PKG问题表述为一个学习性问题，通过从已知环境中学习数据和模型等知识，快速高效地在多个新环境中生成秘钥。具体来说，我们提出了基于深度传递学习（DTL）和元学习的信道特征映射算法用于秘钥生成。

    arXiv:2211.03065v2 Announce Type: replace-cross  Abstract: Deep learning-based physical-layer secret key generation (PKG) has been used to overcome the imperfect uplink/downlink channel reciprocity in frequency division duplexing (FDD) orthogonal frequency division multiplexing (OFDM) systems. However, existing efforts have focused on key generation for users in a specific environment where the training samples and test samples follow the same distribution, which is unrealistic for real-world applications. This paper formulates the PKG problem in multiple environments as a learning-based problem by learning the knowledge such as data and models from known environments to generate keys quickly and efficiently in multiple new environments. Specifically, we propose deep transfer learning (DTL) and meta-learning-based channel feature mapping algorithms for key generation. The two algorithms use different training methods to pre-train the model in the known environments, and then quickly ad
    
[^179]: 鲁棒的心电图分类装饰网络架构

    Decorrelative Network Architecture for Robust Electrocardiogram Classification

    [https://arxiv.org/abs/2207.09031](https://arxiv.org/abs/2207.09031)

    我们提出了一种基于特征装饰和Fourier分区的新型集成方法，用于教授网络各种互补特征，减少基于扰动的愚弄的机会。

    

    人工智能在医学数据分析方面取得了很大进展，但缺乏鲁棒性和可信度使得这些方法尚未被广泛部署。我们提出了一种基于特征装饰和Fourier分区的新型集成方法，用于教授网络各种互补特征，减少基于扰动的愚弄的机会。我们在单通道和多通道心电图分类上测试了我们的方法，并将对抗训练和DVERGE调整为贝叶斯集成框架进行比较。我们的结果表明，结合

    arXiv:2207.09031v4 Announce Type: replace-cross  Abstract: Artificial intelligence has made great progress in medical data analysis, but the lack of robustness and trustworthiness has kept these methods from being widely deployed. As it is not possible to train networks that are accurate in all scenarios, models must recognize situations where they cannot operate confidently. Bayesian deep learning methods sample the model parameter space to estimate uncertainty, but these parameters are often subject to the same vulnerabilities, which can be exploited by adversarial attacks. We propose a novel ensemble approach based on feature decorrelation and Fourier partitioning for teaching networks diverse complementary features, reducing the chance of perturbation-based fooling. We test our approach on single and multi-channel electrocardiogram classification, and adapt adversarial training and DVERGE into the Bayesian ensemble framework for comparison. Our results indicate that the combination
    
[^180]: 条件矩限制的功能化广义经验似然估计

    Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions

    [https://arxiv.org/abs/2207.04771](https://arxiv.org/abs/2207.04771)

    提供了条件矩限制问题的功能化广义经验似然估计方法，并探索了其在小样本性能上的优势，同时提供了基于核和神经网络的实现方式。

    

    在因果推断、经济学习以及更一般的鲁棒机器学习中，重要问题可以被表达为条件矩限制，但由于需要解决一系列的无条件矩限制，估计变得具有挑战性。先前的工作通过将广义矩法（GMM）扩展到连续矩限制来解决这个问题。与此相反，广义经验似然（GEL）提供了一个更一般的框架，并且已经被证明在小样本性能上优于基于GMM的估计量。为了从最近的机器学习发展中受益，我们提供了GEL的功能重构，其中可以利用任意模型。受结果的无限维优化问题的对偶形式启发，我们设计了一个实用方法并探讨了其渐近性质。最后，我们提供了基于核和神经网络的实现方式。

    arXiv:2207.04771v2 Announce Type: replace  Abstract: Important problems in causal inference, economics, and, more generally, robust machine learning can be expressed as conditional moment restrictions, but estimation becomes challenging as it requires solving a continuum of unconditional moment restrictions. Previous works addressed this problem by extending the generalized method of moments (GMM) to continuum moment restrictions. In contrast, generalized empirical likelihood (GEL) provides a more general framework and has been shown to enjoy favorable small-sample properties compared to GMM-based estimators. To benefit from recent developments in machine learning, we provide a functional reformulation of GEL in which arbitrary models can be leveraged. Motivated by a dual formulation of the resulting infinite dimensional optimization problem, we devise a practical method and explore its asymptotic properties. Finally, we provide kernel- and neural network-based implementations of the e
    
[^181]: 因果评分：效应估计、效应排序和效应分类的框架

    Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification

    [https://arxiv.org/abs/2206.12532](https://arxiv.org/abs/2206.12532)

    本文引入了因果评分作为一种新型方法，支持决策制定，提供洞察力，并可用于效应估计、效应排序和效应分类。

    

    本文将因果评分引入到决策制定的背景中作为一种新颖方法，涉及估计支持决策制定的得分，从而提供因果效应的洞察力。我们提出了这些评分的三种有价值的因果解释：效应估计（EE）、效应排序（EO）和效应分类（EC）。在EE解释中，因果评分代表了效应本身。EO解释暗示评分可以作为效应大小的代理，可以根据其因果效应对个体进行排序。EC解释通过预定义的阈值，使个体分为高效应和低效应类别。我们通过两个关键结果展示了这些替代因果解释（EO和EC）的价值。

    arXiv:2206.12532v4 Announce Type: replace-cross  Abstract: This paper introduces causal scoring as a novel approach to frame causal estimation in the context of decision making. Causal scoring entails the estimation of scores that support decision making by providing insights into causal effects. We present three valuable causal interpretations of these scores: effect estimation (EE), effect ordering (EO), and effect classification (EC). In the EE interpretation, the causal score represents the effect itself. The EO interpretation implies that the score can serve as a proxy for the magnitude of the effect, enabling the sorting of individuals based on their causal effects. The EC interpretation enables the classification of individuals into high- and low-effect categories using a predefined threshold. We demonstrate the value of these alternative causal interpretations (EO and EC) through two key results. First, we show that aligning the statistical modeling with the desired causal inte
    
[^182]: 用离开的赌博机模型建议系统中的流失现象

    Modeling Attrition in Recommender Systems with Departing Bandits

    [https://arxiv.org/abs/2203.13423](https://arxiv.org/abs/2203.13423)

    本论文提出了一个新型多臂赌博机设置，捕捉了推荐系统中用户离开的情况，首次证明了在所有用户共享相同类型时，基于UCB的算法是最优的。

    

    在传统的多臂赌博机中，推荐系统的策略影响奖励的获取，但不影响交互的长度。然而，在现实世界中，不满足的用户可能会离开（并永远不再回来）。在这项工作中，我们提出了一个捕捉这种策略依赖性时段的新型多臂赌博机设置。我们的设置包括一个有限的用户类型集合，和多个具有伯努利回报的臂。每个（用户类型，臂）元组对应一个（未知的）奖励概率。每个用户的类型最初是未知的，只能通过其对推荐的响应来推断。此外，如果用户对他们的推荐不满意，他们可能会离开系统。我们首先解决了所有用户共享相同类型的情况，证明了最近基于UCB的算法的最优性。然后，我们转向更具挑战性的情况，即用户分为两类。

    arXiv:2203.13423v2 Announce Type: replace  Abstract: Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisfied users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a finite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user's type is initially unknown and can only be inferred through their response to recommendations. Moreover, if a user is dissatisfied with their recommendation, they might depart the system. We first address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two 
    
[^183]: 针对具有非欧几里德预测变量的度量空间回归问题的Fréchet随机森林

    Fr\'echet random forests for metric space valued regression with non euclidean predictors

    [https://arxiv.org/abs/1906.01741](https://arxiv.org/abs/1906.01741)

    本文介绍了Fréchet随机森林，允许处理值在一般度量空间的数据，并引入了新的树节点分裂方式，扩展了预测过程，提出了一致性定理，并适用于数据驱动分区的Fréchet纯一致随机树

    

    随机森林是一种广泛应用于科学研究领域的统计学习方法，因为它能够学习输入和输出变量之间复杂的关系，同时也能处理高维数据。然而，目前的随机森林方法对处理曲线、图像和形状等异质数据的灵活性不够。本文介绍了Fréchet树和Fréchet随机森林，允许处理输入和输出变量取值在一般度量空间中的数据。为此，引入了一种新的树节点分裂方式，并广义化了树和森林的预测过程。随机森林的袋外误差和变量重要性得分自然得到了调整。给出了使用数据驱动分区的Fréchet回归图预测器的一致性定理，并应用于Fréchet纯一致随机树。该方法

    arXiv:1906.01741v3 Announce Type: replace-cross  Abstract: Random forests are a statistical learning method widely used in many areas of scientific research because of its ability to learn complex relationships between input and output variables and also its capacity to handle high-dimensional data. However, current random forest approaches are not flexible enough to handle heterogeneous data such as curves, images and shapes. In this paper, we introduce Fr\'echet trees and Fr\'echet random forests, which allow to handle data for which input and output variables take values in general metric spaces. To this end, a new way of splitting the nodes of trees is introduced and the prediction procedures of trees and forests are generalized. Then, random forests out-of-bag error and variable importance score are naturally adapted. A consistency theorem for Fr\'echet regressogram predictor using data-driven partitions is given and applied to Fr\'echet purely uniformly random trees. The method i
    
[^184]: 温和的规范执行：更快的出现，更快乐的智能体

    Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])

    [http://arxiv.org/abs/2401.16461](http://arxiv.org/abs/2401.16461)

    通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。

    

    多智能体系统可视为一个自主智能体的社会，通过社会规范可以有效地调控智能体的交互。一般来说，一个社会的规范并不是硬编码的，而是从智能体的交互中产生的。具体来说，一个社会中的智能体对另一个智能体的行为作出的反应以及对他人反应的回应，决定了社会中出现哪些规范。我们将一个智能体对另一个智能体的满意或不满意行为的反应视为第一个智能体向第二个智能体的交流。理解这些交流是一种社会智能：这些交流通过推动智能体朝着某些行为进行，从而促进规范的出现。虽然众所周知惩罚可以导致规范的出现，但我们认为更宽泛的社会智能可能在促进多智能体系统中的合作方面更有效。因此，我们开发了一种被称为Ne的方法

    A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
    
[^185]: 基于拓扑感知嵌入记忆的学习扩展图

    Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])

    [http://arxiv.org/abs/2401.13200](http://arxiv.org/abs/2401.13200)

    这篇论文提出了一个基于拓扑感知嵌入记忆的学习扩展图的框架，该框架可以解决在不断扩展的图上应用记忆回放技术导致的内存爆炸问题。

    

    基于记忆回放的技术在连续学习中应用广泛，但是直接应用于不断扩展的图会导致潜在的内存爆炸问题。为了解决这个问题，我们系统分析了内存爆炸问题的关键挑战，并提出了一个通用框架，即Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM)，来解决这个问题。该框架不仅将内存空间复杂度从$\mathcal{O}(nd^L)$降低到$\mathcal{O}(n)$，还充分利用了拓扑信息进行记忆回放。

    Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$ to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \textit{Topology-aware Embeddings} 
    
[^186]: 级联强化学习

    Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])

    [http://arxiv.org/abs/2401.08961](http://arxiv.org/abs/2401.08961)

    本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    

    最近几年，级联赌博机在推荐系统和在线广告中应用广泛。在级联赌博机模型中，每个时刻，一个代理人从一组具有未知吸引概率的项目中推荐一个有序的项目子集（称为项目列表）。然后，用户检查列表，并点击第一个有吸引力的项目（如果有的话），之后，代理收到一个奖励。代理的目标是最大化预期的累积奖励。然而，以往的级联赌博机文献忽略了用户状态（例如历史行为）对推荐的影响以及会话进行过程中状态的变化。受此事实的启发，我们提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响。在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
    
[^187]: PPR: 在维持冒名顶替攻击的同时增强躲避攻击对人脸识别系统的影响

    PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])

    [http://arxiv.org/abs/2401.08903](http://arxiv.org/abs/2401.08903)

    本文提出了一种名为PPR的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。该方法利用对抗样本修剪，并通过嵌入对抗扰动来增强对抗人脸样本的躲避性能。

    

    人脸识别系统上的对抗攻击可以分为两种类型：冒名顶替攻击和躲避攻击。我们观察到，在黑盒设置中成功进行冒名顶替攻击并不一定能保证在人脸识别系统上成功进行躲避攻击。本文提出了一种名为预训练修剪恢复攻击（PPR）的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。我们的方法利用对抗样本修剪，可以将一部分对抗扰动设为零，并倾向于保持攻击性能。通过利用对抗样本修剪，我们可以修剪预训练的对抗样本，并有选择性地释放某些对抗扰动。然后，我们将对抗扰动嵌入到修剪区域，从而增强对抗人脸样本的躲避性能。通过实验证明了我们提出的攻击方法的有效性。

    Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
    
[^188]: MADA: 通过超梯度下降的元适应优化器

    MADA: Meta-Adaptive Optimizers through hyper-gradient Descent. (arXiv:2401.08893v1 [cs.LG])

    [http://arxiv.org/abs/2401.08893](http://arxiv.org/abs/2401.08893)

    MADA是一个统一的优化器框架，通过超梯度下降动态学习最适合的优化器。数值结果表明MADA在次优调整的超参数下是稳健的，并且在默认超参数下常常优于其他优化器。插值优化器可以改进收敛性能。

    

    自从Adam被引入以来，已经提出了几种用于深度学习的新型自适应优化器。这些优化器通常在某些任务上表现卓越，但可能无法在所有任务中均优于Adam。在这项工作中，我们引入了元适应优化器(MADA)，这是一个统一的优化器框架，可以概括多种已知的优化器，并在训练过程中动态学习最适合的优化器。MADA的关键思想是对优化器的空间进行参数化，并使用超梯度下降进行搜索。数值结果表明，MADA对于次优调整的超参数是稳健的，并且在默认超参数下常常优于Adam、Lion和Adan，甚至在优化超参数的情况下也是如此。我们还提出了AVGrad，它是AMSGrad的一个变种，在其中将最大操作符替换为平均操作符，并观察到它在MADA中的表现更好。最后，我们提供了收敛分析，以表明优化器的插值（特别是AVGrad和Adam）可以改进收敛性能。

    Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve
    
[^189]: 批处理ICL: 有效，高效且无序地进行上下文学习

    Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])

    [http://arxiv.org/abs/2401.06469](http://arxiv.org/abs/2401.06469)

    本文提出了批处理ICL方法，通过将ICL视为一个元优化过程，开发出了一个有效、高效且无序的推理算法。通过聚合元梯度并将其应用于零-shot学习，该方法使LLM对ICL示例顺序无关，并且在实验证明其在大多数情况下优于其他排列方式，甚至超过了标准ICL的最佳顺序的性能。

    

    本文将上下文学习（ICL）视为一个元优化过程，解释了LLM对ICL示例顺序敏感的原因。这种理解使我们开发出了Batch-ICL，一种用于ICL的有效、高效且无序的推理算法。与标准的N-shot学习方法不同，Batch-ICL使用N个单独的1-shot前向计算，并聚合得到的元梯度。然后，将这些聚合的元梯度应用于零-shot学习以生成最终预测。这种批处理方法使LLM对ICL示例的顺序无关。通过大量实验证明，Batch-ICL一致优于大多数示例序列的排列方式。在某些情况下，甚至超过了标准ICL的最佳顺序的性能，同时减少了所需的计算资源。此外，我们还开发了Batch-ICL的一种新颖变体，其中包含多个"epochs"。

    In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
    
[^190]: AUTOACT：通过自主规划实现的自动代理学习

    AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])

    [http://arxiv.org/abs/2401.05268](http://arxiv.org/abs/2401.05268)

    AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。

    

    语言代理在各种复杂任务上取得了相当的性能。尽管在这个领域进行了不断的探索，但现有的语言代理系统仍然面临昂贵、不可重复的数据依赖问题，并且面临将单一模型应用于多个功能的挑战。为此，我们介绍了AutoAct，这是一个自动代理学习框架，不依赖于大规模带注释的数据和来自闭源模型（如GPT-4）的合成轨迹。给定有限的数据和工具库，AutoAct首先自动合成规划轨迹，不需要人类或强闭源模型的任何辅助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动区分，产生一个子代理组来完成任务。我们进行了多种LLMs的广泛实验，结果显示AutoAct在性能上优于或与其相当。

    Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
    
[^191]: 联邦遗忘：方法、设计准则和评估指标的综述

    Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])

    [http://arxiv.org/abs/2401.05146](http://arxiv.org/abs/2401.05146)

    这篇综述论文介绍了联邦遗忘的概念和挑战，以及解决这些问题的方法和设计准则，旨在为联邦学习中保护用户隐私和防止恶意攻击提供解决方案。

    

    联邦学习使得多个参与方能够协同训练一个机器学习模型，通过保留数据在本地存储，从而维护了用户和机构的隐私。与集中化原始数据不同，联邦学习通过交换本地优化的模型参数来逐步构建全局模型。尽管联邦学习更加符合新兴规定，如欧洲通用数据保护条例（GDPR），但在此背景下确保遗忘权——允许联邦学习参与方从学习的模型中删除他们的数据贡献仍然不明确。此外，人们认识到恶意客户端可能通过更新将后门注入全局模型，例如对特制数据示例进行错误预测。因此，需要机制来确保个人有可能在聚合后移除他们的数据并清除恶意贡献，而不损害已获得的"全

    Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired "g
    
[^192]: 无需增强的简单非对称图对比学习

    Simple and Asymmetric Graph Contrastive Learning without Augmentations. (arXiv:2310.18884v1 [cs.LG])

    [http://arxiv.org/abs/2310.18884](http://arxiv.org/abs/2310.18884)

    本文提出了一种无需增强的简单非对称图对比学习方法GraphACL，通过考虑邻居节点的非对称视图，该方法能够有效地在同类和异类图上进行对比学习，对于建模异类图非常重要。

    

    图对比学习（GCL）在图结构数据的表示学习中显示出了优越的性能。尽管取得了成功，但大多数现有的GCL方法依赖于预制的图增强和同类假设。因此，它们在连通节点可能具有不同类标签和不相似特征的异类图上无法很好地推广。在本文中，我们研究了在同类和异类图上进行对比学习的问题。我们发现，通过考虑邻居节点的非对称视图，我们可以实现有希望的性能。由此产生的简单算法，称为图的非对称对比学习(GraphACL)，易于实现，不依赖于图增强和同类假设。我们提供了理论和实证证据，证明GraphACL能够捕捉单跳本地邻域信息和双跳单一相似性，这两者对于建模异类图非常重要。

    Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results s
    
[^193]: 无配对MRI超分辨率与自监督对比学习

    Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v1 [eess.IV])

    [http://arxiv.org/abs/2310.15767](http://arxiv.org/abs/2310.15767)

    本文提出了一种利用自监督对比学习的无配对MRI超分辨率方法，可以在有限的训练数据下提高SR性能，改善MRI分辨率。

    

    高分辨率（HR）磁共振成像（MRI）在临床环境中提高诊断准确性至关重要。然而，MRI分辨率的固有限制限制了其广泛应用。基于深度学习的图像超分辨率（SR）方法展现了提升MRI分辨率的潜力，而无需额外成本。然而，这些方法通常需要大量HR MRI图像进行训练，而这可能难以获取。在本文中，我们提出了一种无配对MRI SR方法，利用自监督对比学习来提高有限训练数据下的SR性能。我们的方法利用真实的HR图像和人工生成的SR图像构建正负样本对，从而促进辨别性特征的学习。本研究呈现的实证结果突出了峰值信噪比和结构相似性指数的显著提高，即使缺乏HR图像数据。

    High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR image
    
[^194]: 机器学习模型地球科学系统建模中的质量保持感知器

    A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])

    [http://arxiv.org/abs/2310.08644](http://arxiv.org/abs/2310.08644)

    这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。

    

    虽然数十年来致力于构建用于预测地球科学系统时间序列演化的物理-概念 (PC) 模型，但最近的研究表明，基于机器学习 (ML) 的门控循环神经网络技术可以用于开发更准确的模型。然而，从ML基础模型中提取物理理解的困难使得其在增强对系统结构和功能的科学知识方面的应用变得复杂。在这里，我们提出了一个理解物理性的质量保持感知器 (MCP) 作为弥合PC模型和ML模型的方法。MCP利用PC模型和GRNNs背后的有向图结构的内在同构性，以可解释的方式明确表示物理过程的质量保持性质，同时利用现有数据和现成的ML技术直接学习这种过程的功能性（可解释性）。

    Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
    
[^195]: 在持续学习中平衡稳定性和可塑性：激活变化的读出分解（RDAC）框架。

    Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04741](http://arxiv.org/abs/2310.04741)

    本文介绍了一个名为RDAC的框架，该框架解剖了持续学习中稳定性和可塑性之间的平衡，并详细分析了几种常用算法在处理任务时的稳定性和可塑性权衡。

    

    持续学习算法旨在获取新知识的同时保留先前的信息。然而，稳定性和可塑性之间的平衡仍然是一个中心挑战。本文介绍了一个框架，对这种平衡进行了解剖，提供了关于持续学习算法的宝贵见解。激活变化的读出分解（RDAC）框架首先解决了稳定性和可塑性困境及其与灾难性遗忘的关系。它将学习诱导的激活变化与先前读出范围内的稳定性程度和零空间的变化与可塑性程度相关联。在处理分裂CIFAR-110任务的深度非线性网络中，该框架阐明了常用正则化算法（SI、EWC和LwF）以及重放算法（GEM和数据重放）的稳定性和可塑性的权衡。

    Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
    
[^196]: 基于提示的高效时域泛化

    Prompting-based Efficient Temporal Domain Generalization. (arXiv:2310.02473v1 [cs.LG])

    [http://arxiv.org/abs/2310.02473](http://arxiv.org/abs/2310.02473)

    我们提出了一种基于提示的高效时域泛化方法，通过学习全局提示、领域特定提示和感知时序漂移的提示，不需要目标域数据的情况下适应时序漂移，并在各种任务中取得了state-of-the-art的性能。

    

    传统的机器学习假设训练和测试数据是独立且相同分布的。然而，在许多实际应用中，数据分布会随时间变化，导致训练好的模型在未来时间段的泛化能力变差。我们的论文提出了一种新颖的基于提示的时域泛化方法，它具有参数高效、时间高效，并且在训练过程中不需要访问目标域数据（即未知的未来时间段）。我们的方法通过学习全局提示、领域特定提示和感知到时序漂移的提示的方式，将目标预训练模型适应于时序漂移。它适用于各种任务，例如分类、回归和时间序列预测，并在时域泛化方面取得了新的最优性能。代码仓库将公开分享。

    Machine learning traditionally assumes that training and testing data are distributed independently and identically. However, in many real-world settings, the data distribution can shift over time, leading to poor generalization of trained models in future time periods. Our paper presents a novel prompting-based approach to temporal domain generalization that is parameter-efficient, time-efficient, and does not require access to the target domain data (i.e., unseen future time periods) during training. Our method adapts a target pre-trained model to temporal drift by learning global prompts, domain-specific prompts, and drift-aware prompts that capture underlying temporal dynamics. It is compatible across diverse tasks, such as classification, regression, and time series forecasting, and sets a new state-of-the-art benchmark in temporal domain generalization. The code repository will be publicly shared.
    
[^197]: 关于生成模型在其自己的数据上迭代训练的稳定性研究

    On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])

    [http://arxiv.org/abs/2310.00429](http://arxiv.org/abs/2310.00429)

    本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。

    

    深度生成模型在建模复杂数据方面取得了巨大的进展，往往展现出超过典型人类能力的样本真实性辨别能力。这一成功的关键驱动力无疑是这些模型消耗海量网络规模数据的结果。由于这些模型惊人的性能和易得性，网络上将不可避免地出现越来越多的合成内容。这个事实直接意味着生成模型的未来迭代必须面对一个现实：它们的训练数据由清洁数据和先前模型生成的人工数据组成。在本文中，我们开发了一个框架来对混合数据集（包括真实数据和合成数据）上训练生成模型对稳定性的影响进行严格研究。我们首先证明了在初始生成模型足够好地近似数据分布并且真实数据与合成数据的比例适当的情况下，迭代训练的稳定性。

    Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
    
[^198]: 学习转换以实现通用的实例不变性

    Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])

    [http://arxiv.org/abs/2309.16672](http://arxiv.org/abs/2309.16672)

    该论文提出了一种学习转换以实现通用的实例不变性的方法。通过使用归一化流来预测图像的变换分布，并对预测结果进行平均，可以实现对不同实例之间的对齐，从而推广不变性的类别间的应用。这种方法还可以适应超出分布范围的姿势，并且可以学习更广泛的变换范围。

    

    计算机视觉研究一直致力于构建对自然数据中的空间变换具有强鲁棒性的系统。传统上，可以通过数据增强或将不变性硬编码到架构中来实现这一点。然而，过多或过少的不变性都可能会影响结果，正确的不变性程度在先验中是未知的，并且依赖于实例。理想情况下，应该从数据中学习适当的不变性，并在测试时推断。我们将不变性视为一个预测问题。给定任何图像，我们使用一个归一化流来预测变换的分布，并对它们的预测进行平均。由于这个分布仅取决于实例，我们可以在分类之前对实例进行对齐，并在类别之间推广不变性。同样的分布也可以用于适应超出分布的姿势。这个归一化流是端到端训练的，并且可以学习比Augerino和InstaAug更多范围的变换。当用作数据增强时，我们的m

    Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.  We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our m
    
[^199]: 从肽到纳米结构：一种用于快速稳定的机器学习力场的欧几里得变换器

    From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields. (arXiv:2309.15126v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15126](http://arxiv.org/abs/2309.15126)

    这项研究提出了一种称为SO3krates的欧几里得变换器架构，它通过组合稀疏等变表示和自注意机制，在机器学习力场中实现了精度、稳定性和速度的独特组合，从而使我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。

    

    近年来，基于从头计算的机器学习力场（MLFFs）的发展取得了巨大进展。尽管在测试误差上取得了较低的效果，但MLFFs在分子动力学（MD）模拟中的适用性越来越受到人们的关注，因为其稳定性受到质疑。我们的研究结果表明MLFFs中的等变表示与MD模拟稳定性之间可能存在潜在联系，但计算成本可能限制了它们带来的实际优势。为了解决这个问题，我们提出了一种叫做SO3krates的变换器架构，它结合了稀疏等变表示（欧几里得变量）和自注意机制，可以分离不变和等变信息，消除了昂贵的张量积操作。SO3krates实现了精度、稳定性和速度的独特组合，使得我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。

    Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the suitability of MLFFs in molecular dynamics (MD) simulations is being increasingly scrutinized due to concerns about instability. Our findings suggest a potential connection between MD simulation stability and the presence of equivariant representations in MLFFs, but their computational cost can limit practical advantages they would otherwise bring.  To address this, we propose a transformer architecture called SO3krates that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that can separate invariant and equivariant information, eliminating the need for expensive tensor products. SO3krates achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on unprecedented time and system size scales. T
    
[^200]: 三维切片Wasserstein的准蒙特卡洛方法

    Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])

    [http://arxiv.org/abs/2309.11713](http://arxiv.org/abs/2309.11713)

    本文提出了准蒙特卡洛（QMC）方法用于三维切片Wasserstein（SW）的近似计算，并通过多种方法在三维单位超球面上构造了QMC点集。此外，还介绍了将QSW扩展为随机准切片Wasserstein（RQSW）的方法。

    

    Monte Carlo (MC)方法被用作计算切片Wasserstein (SW)距离的标准方法，因为它在分析形式中具有棘手的期望。然而，MC方法在最小化绝对近似误差方面并不优化。为了提供更好的经验SW类，我们提出了基于准蒙特卡洛（QMC）方法的准切片Wasserstein（QSW）逼近。为了对SW的QMC进行全面的研究，我们专注于三维设置，特别是计算三维概率测度之间的SW。具体而言，我们通过实证验证了在三维单位超球面上构造QMC点集的多种方法，包括基于高斯的映射，等面积映射，广义螺旋点和最优化差异能量。此外，为了获得随机优化的无偏估计，我们通过在所讨论的低维设置中引入随机性，将QSW扩展为随机准切片Wasserstein（RQSW）。

    Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-d
    
[^201]: 使用温度指数测度的最优输运

    Optimal Transport with Tempered Exponential Measures. (arXiv:2309.04015v1 [cs.LG])

    [http://arxiv.org/abs/2309.04015](http://arxiv.org/abs/2309.04015)

    本文推广了熵正则化最优输运方法，将其应用于温度指数测度中，实现了快速有效的算法和可控的稀疏性。

    

    在最优输运领域中，两个重要的子领域相互对立：（i）非正则化最优输运，“卡托诺维奇方式”，导致了非常稀疏的规划，但算法效率较低；（ii）熵正则化最优输运，“辛克霍恩-库都里方式”，获得了近似线性算法，但最大程度上无法稀疏规划。本文中，我们将后一种方法推广到温度指数测度，即具有间接测度归一化的指数族泛化，取得了非常方便的折中效果，具有非常快的近似算法和可控的稀疏性，同时也适用于不平衡最优输运问题。

    In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
    
[^202]: 针对带有污染数据的多变量时间序列异常检测：对生理信号的应用

    Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])

    [http://arxiv.org/abs/2308.12563](http://arxiv.org/abs/2308.12563)

    这个论文介绍了一种针对带有污染数据的多变量时间序列异常检测的新方法，通过去污和变量依赖建模实现了无监督的异常检测，对于实际场景中的异常检测具有重要意义。

    

    主流无监督异常检测算法通常在学术数据集中表现出色，但由于受到控制实验条件下的清洁训练数据的限制，它们在实际场景下的性能受到了限制。然而，在实际异常检测中，训练数据包含噪声的挑战经常被忽视。本研究在感知时间序列异常检测（TSAD）中深入研究了标签级噪声的领域。本文提出了一种新颖实用的端到端无监督TSAD方法，该方法处理训练数据中包含异常的情况下。该方法称为TSAD-C，其在训练阶段不需要访问异常标签。TSAD-C包括三个模块：一个去污器用于纠正训练数据中存在的异常（也称为噪声），一个变量依赖建模模块用于捕捉去污后数据中的长期内部和跨变量依赖关系，可以视为替代性的异常性度量方法。

    Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
    
[^203]: 通过编码本知识、自然语言推理和ChatGPT来合成政治零样本关系分类

    Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])

    [http://arxiv.org/abs/2308.07876](http://arxiv.org/abs/2308.07876)

    该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。

    

    最近的事件编码的监督模型在性能方面远远超过模式匹配方法。然而，它们仅仅依赖于新的注释，忽视了专家数据库中的大量知识，限制了它们在细粒度分类中的适用性。为了解决这些限制，我们通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类。我们的研究涵盖了ChatGPT和一种新颖的基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，将任务分解为上下文、语态和类别消歧的不同层次。该框架提高了解释性、效率和对模式更改的适应性。通过在我们新策划的数据集上进行大量实验，我们指出了ChatGPT中的不稳定性问题，并突出了ZSP的卓越性能。ZSP在细粒度根代码分类的F1得分上取得了令人印象深刻的提高40%。

    Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
    
[^204]: 可验证的特征归因：后期解释性和内在可解释性的桥梁

    Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])

    [http://arxiv.org/abs/2307.15007](http://arxiv.org/abs/2307.15007)

    本文旨在搭建一座桥梁，将后期解释性和内在可解释性相结合，以解释复杂的黑盒模型的行为，并解决解释的忠实性和可验证性之间的问题。

    

    随着机器学习模型在各种实际应用中的部署增加，研究人员和从业者一直强调对模型行为的解释需求。为此，在以前的文献中概述了两种广泛的策略来解释模型。后期解释方法通过突出显示对模型预测至关重要的特征来解释复杂的黑盒模型的行为；然而，之前的工作表明这些解释可能不忠实，更令人担忧的是我们无法验证它们。另一方面，内在可解释模型通过将解释性信息明确编码到模型架构中来规避这些问题，这意味着它们的解释自然忠实且可验证，但由于其有限的表达能力，它们通常表现出较差的预测性能。在这项工作中，我们旨在搭建一座桥梁，将后期解释性和内在可解释性相结合。

    With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge t
    
[^205]: 公平机器遗忘：在减少差异的同时删除数据

    Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])

    [http://arxiv.org/abs/2307.14754](http://arxiv.org/abs/2307.14754)

    本研究提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的机器学习方法。

    

    随着公众对企业收集和使用个人信息的意识增强，消费者积极参与企业数据集的管理变得越来越重要。为此，数据管理框架（如欧洲通用数据保护条例）已经提出了被遗忘的权利，允许个人请求将其个人数据从组织使用的数据库和模型中删除。为了实现遗忘，已经提出了几种机器学习遗忘方法，以解决每个遗忘请求重新训练模型的计算效率问题。虽然这些在线替代方案可以高效地进行遗忘，但它们对于其他关键的实际应用属性（如公平性）的影响尚不清楚。在这项工作中，我们提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的方法。

    As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
    
[^206]: 具有确定性演化状态的强盗模型

    Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])

    [http://arxiv.org/abs/2307.11655](http://arxiv.org/abs/2307.11655)

    该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。

    

    我们提出了一种学习与强盗反馈结合的模型，同时考虑到确定性演化和不可观测的状态，我们称之为具有确定性演化状态的强盗模型。我们的模型主要应用于推荐系统和在线广告的学习。在这两种情况下，算法在每一轮获得的奖励是选择行动的短期奖励和系统的“健康”程度（即通过其状态测量）的函数。例如，在推荐系统中，平台从用户对特定类型内容的参与中获得的奖励不仅取决于具体内容的固有特征，还取决于用户与平台上其他类型内容互动后其偏好的演化。我们的通用模型考虑了状态演化的不同速率λ∈[0,1]（例如，用户的偏好因先前内容消费而快速变化）。

    We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
    
[^207]: 嵌入图的直觉模糊RVFL用于类别不平衡学习

    Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning. (arXiv:2307.07881v1 [cs.LG])

    [http://arxiv.org/abs/2307.07881](http://arxiv.org/abs/2307.07881)

    提出了一种嵌入图的直觉模糊RVFL模型，用于解决类别不平衡学习的问题。该模型通过采用加权机制处理不平衡的数据集，并利用图嵌入提取语义丰富的信息。该模型在类别不平衡学习中具有较高的准确性。

    

    机器学习领域面临一个关键的研究领域，即类别不平衡学习，在对少数类别进行准确分类方面存在很大的困难。这个问题可能导致偏向于多数类别的模型在训练过程中优先考虑，从而导致少数类别的代表不足。随机向量函数链接（RVFL）网络是一种广泛使用且有效的分类学习模型，因其速度和效率高而受到广泛应用。然而，当处理不平衡的数据集时，它的准确性较低。为了克服这个局限性，我们提出了一种新颖的嵌入图的直觉模糊RVFL用于类别不平衡学习（GE-IFRVFL-CIL）模型，该模型采用加权机制来处理不平衡的数据集。所提出的GE-IFRVFL-CIL模型具有许多优点，例如：（i）它利用图嵌入从数据集中提取语义丰富的信息，（ii）它使用直觉模糊集来进行类别不平衡学习。

    The domain of machine learning is confronted with a crucial research area known as class imbalance learning, which presents considerable hurdles in the precise classification of minority classes. This issue can result in biased models where the majority class takes precedence in the training process, leading to the underrepresentation of the minority class. The random vector functional link (RVFL) network is a widely-used and effective learning model for classification due to its speed and efficiency. However, it suffers from low accuracy when dealing with imbalanced datasets. To overcome this limitation, we propose a novel graph embedded intuitionistic fuzzy RVFL for class imbalance learning (GE-IFRVFL-CIL) model incorporating a weighting mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model has a plethora of benefits, such as $(i)$ it leverages graph embedding to extract semantically rich information from the dataset, $(ii)$ it uses intuitionistic fuzzy sets to ha
    
[^208]: 修剪与量化：哪个更好？

    Pruning vs Quantization: Which is Better?. (arXiv:2307.02973v1 [cs.LG])

    [http://arxiv.org/abs/2307.02973](http://arxiv.org/abs/2307.02973)

    本文比较了神经网络量化和修剪这两种压缩深度神经网络的技术，结果表明在大多数情况下，量化优于修剪。

    

    神经网络修剪和量化技术几乎和神经网络本身一样古老。然而，迄今为止只有两者之间的临时比较发表过。本文旨在回答哪个更好：神经网络量化还是修剪？通过回答这个问题，我们希望为神经网络硬件的设计决策提供信息。我们对压缩深度神经网络的这两种技术进行了全面比较。首先，我们对一般数据分布的期望量化和修剪误差进行了分析比较。然后，我们提供了在训练好的网络中每层修剪和量化误差的下界，并将其与优化后的经验误差进行了比较。最后，我们对3个任务上的8个大规模模型进行了广泛的实验比较。我们的结果表明，在大多数情况下，量化优于修剪。只有在一些极高压缩比的情况下，修剪表现出更好的效果。

    Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, p
    
[^209]: 使用强化学习优化对抗目标下的信用额度调整

    Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning. (arXiv:2306.15585v1 [q-fin.GN])

    [http://arxiv.org/abs/2306.15585](http://arxiv.org/abs/2306.15585)

    本研究使用强化学习技术，通过平衡最大化投资组合收入和最小化准备金的对抗目标，自动化寻找最优信用卡额度调整策略。

    

    强化学习已经在很多问题中得到应用，从具有确定性环境的视频游戏到具有随机场景的投资组合和运营管理；然而，在银行问题中对这些方法的测试尝试很少。在本研究中，我们试图通过使用强化学习技术找到并自动化最优信用卡额度调整策略。具体而言，由于有历史数据可用，我们考虑每个客户的两种可能操作，即增加或保持个人当前的信用额度。为了找到这个策略，我们首先将这个决策问题形式化为一个优化问题，在其中最大化预期利润；因此，我们平衡了两个对抗目标：最大化投资组合的收入和最小化投资组合的准备金。其次，考虑到我们问题的特殊性，我们使用了离线学习策略，以基于历史数据模拟行动的影响。

    Reinforcement learning has been explored for many problems, from video games with deterministic environments to portfolio and operations management in which scenarios are stochastic; however, there have been few attempts to test these methods in banking problems. In this study, we sought to find and automatize an optimal credit card limit adjustment policy by employing reinforcement learning techniques. In particular, because of the historical data available, we considered two possible actions per customer, namely increasing or maintaining an individual's current credit limit. To find this policy, we first formulated this decision-making question as an optimization problem in which the expected profit was maximized; therefore, we balanced two adversarial goals: maximizing the portfolio's revenue and minimizing the portfolio's provisions. Second, given the particularities of our problem, we used an offline learning strategy to simulate the impact of the action based on historical data f
    
[^210]: 在单位球上学习表示：应用于在线连续学习

    Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])

    [http://arxiv.org/abs/2306.03364](http://arxiv.org/abs/2306.03364)

    该论文提出了一种基于单位球的表示学习方法，通过将表示推向固定方向，使得学习策略对数据漂移具有弹性，从而能够应对在线连续学习的挑战性问题。

    

    我们使用最大后验估计原理来学习分布在单位球上的表示。我们针对对称方向数据建立了 von Mises-Fisher 分布和角高斯分布的损失函数。我们方法的一个显著特点是，学习到的表示被推向固定的方向，使得学习策略对数据漂移具有弹性。这使得它适合于在线连续学习，即在连续的数据流上训练神经网络的问题，其中多个分类任务按顺序呈现，因此过去任务的数据不再可用，当前任务的数据只能看一次。为了应对这种具有挑战性的情况，我们提出了一种基于记忆的表示学习技术，配备了我们的新损失函数。我们的方法不需要负数据或任务边界的知识，并且在较小的批处理下表现良好。

    We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
    
[^211]: DiffPack：自回归蛋白质侧链包装的扭转扩散模型

    DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing. (arXiv:2306.01794v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.01794](http://arxiv.org/abs/2306.01794)

    本文提出了DiffPack，这是一个自回归的扭转扩散模型，通过在扭曲空间中进行扩散和去噪来学习侧链扭转角度的联合分布，从而准确地预测蛋白质侧链的构象，有效应用于蛋白质结构预测、设计和蛋白质相互作用等领域。

    

    蛋白质在生物功能中起着关键作用，它们的三维结构对于决定它们的功能至关重要。准确地预测给定骨架的蛋白质侧链的构象对于蛋白质结构预测、设计和蛋白质-蛋白质相互作用等应用非常重要。传统方法计算复杂度高，准确度有限，而现有的机器学习方法将问题视为回归任务，忽略了因常量共价键长度和角度所限制而带来的问题。在本研究中，我们提出了DiffPack，这是一个扭转扩散模型，通过在扭曲空间上进行扩散和去噪，学习侧链扭转角度的联合分布，这是侧链包装中唯一的自由度。为了避免同时扰动所有四个扭转角度带来的问题，我们建议从\c {hi}1到\c {hi}4自回归生成四个扭转角度，并训练扩散模型。

    Proteins play a critical role in carrying out biological functions, and their 3D structures are essential in determining their functions. Accurately predicting the conformation of protein side-chains given their backbones is important for applications in protein structure prediction, design and protein-protein interactions. Traditional methods are computationally intensive and have limited accuracy, while existing machine learning methods treat the problem as a regression task and overlook the restrictions imposed by the constant covalent bond lengths and angles. In this work, we present DiffPack, a torsional diffusion model that learns the joint distribution of side-chain torsional angles, the only degrees of freedom in side-chain packing, by diffusing and denoising on the torsional space. To avoid issues arising from simultaneous perturbation of all four torsional angles, we propose autoregressively generating the four torsional angles from \c{hi}1 to \c{hi}4 and training diffusion m
    
[^212]: 重新思考对抗策略：多智能体强化学习中的广义攻击形式和可证明的防御

    Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])

    [http://arxiv.org/abs/2305.17342](http://arxiv.org/abs/2305.17342)

    本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。

    

    大多数现有的研究研究直接扰动受害者的状态/动作或基础转移动态以展示强化学习智能体在对抗攻击下的脆弱性。然而，这样的直接操纵在实践中并不总是可行的。在本文中，我们考虑另一种常见且现实的攻击设置：在经过训练的多智能体RL的设置中，在部署期间，受害代理$\nu$被攻击者控制另一个代理$\alpha$以敌对方式行动，使用“对抗策略”对受害代理进行攻击。尽管之前的攻击模型考虑了这种设置，但他们没有考虑到攻击者可以遇到抵抗，因此只能部分控制代理$\alpha$，同时引入可察觉的“异常”行为，这些行为很容易被检测到。并且缺乏针对这些对抗策略的可证明的防御。为了解决这些问题，我们引入了一个更一般化的攻击形式，模拟了攻击者在何种程度上可以控制代理$\alpha$。

    Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
    
[^213]: 无线网络中的异步多模型联邦学习：理论、建模与优化(arXiv:2305.13503v1 [cs.LG])

    Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v1 [cs.LG])

    [http://arxiv.org/abs/2305.13503](http://arxiv.org/abs/2305.13503)

    本文提出了MA-FL，应用异步模型传输体系结构来实现有多个下游任务需要训练的联邦学习。本文的收敛性分析揭示了资源分配、设备调度和个体模型状态对机器学习模型性能的影响。实验表明，MA-FL在收敛速度和模型精度方面优于现有的联邦学习方法。

    

    联邦学习是一种分布式机器学习的重要技术。目前，联邦学习的大部分文献都关注单一任务/模型的机器学习模型训练，并采用同步模型参数传输设置。为了解决这个问题，本文提出了MA-FL，它考虑利用异步模型传输体系结构，实现有多个下游任务需要训练的联邦学习。我们首先通过引入一族调度张量来捕捉设备的调度，并对MA-FL下的机器学习模型训练收敛性进行了表征。我们的收敛性分析揭示了资源分配（例如，小批量大小和梯度下降迭代次数）、设备调度和个体模型状态（即预热与冷启动初始化）对机器学习模型性能的影响。最后，我们为MA-FL制定了一个非凸混整数优化问题，用于共同配置资源分配和设备调度。对合成和真实数据集的数值实验表明，MA-FL在收敛速度和模型精度方面优于现有的联邦学习方法。

    Federated learning (FL) has emerged as a key technique for distributed machine learning (ML). Most literature on FL has focused on systems with (i) ML model training for a single task/model, (ii) a synchronous setting for uplink/downlink transfer of model parameters, which is often unrealistic. To address this, we develop MA-FL, which considers FL with multiple downstream tasks to be trained over an asynchronous model transmission architecture. We first characterize the convergence of ML model training under MA-FL via introducing a family of scheduling tensors to capture the scheduling of devices. Our convergence analysis sheds light on the impact of resource allocation (e.g., the mini-batch size and number of gradient descent iterations), device scheduling, and individual model states (i.e., warmed vs. cold initialization) on the performance of ML models. We then formulate a non-convex mixed integer optimization problem for jointly configuring the resource allocation and device schedu
    
[^214]: 基于跨语言伪标注的无监督自动语音识别

    Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])

    [http://arxiv.org/abs/2305.13330](http://arxiv.org/abs/2305.13330)

    本研究提出了一种基于跨语言伪标注的无监督ASR方法，能够使用其他语言中的标注数据来引导新语言的无监督AM。在Common Voice上取得了良好的效果，可以实现18% WER。而且在不同语言的数据集上都优于基线模型。

    

    最近的研究表明，可以仅使用非配对的音频和文本来训练无监督自动语音识别（ASR）系统。现有的无监督ASR方法假定不能使用任何标注数据进行训练。本文认为，即使没有给定语言的任何标注音频，也始终可以使用其他语言中的标注数据。本文展示了如何使用其他语言的字符级声学模型（AM），来引导新语言的无监督AM。 这里，“无监督”意味着没有可用于目标语言的标注音频。本文的方法基于两个关键因素：（i）使用其他语言AM生成“目标”语言的伪标签（PLs）；（ii）使用“目标语言模型”限制这些PLs。我们的方法在Common Voice上非常有效：例如，将英语AM传递到斯瓦希里语可以实现18％的WER。 它还在不同语言的多个数据集上优于基于字符的基线模型。

    Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
    
[^215]: 使Transformer在时间序列预测中再次卓越：通道对齐鲁棒双Transformer

    Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])

    [http://arxiv.org/abs/2305.12095](http://arxiv.org/abs/2305.12095)

    本文提出了一种通道对齐鲁棒双Transformer模型，通过双Transformer结构和鲁棒损失函数的引入，解决了Transformer在时间序列预测中的关键缺点，显著提高了预测精度和效率。

    

    最近的研究表明，深度学习方法，尤其是Transformer和MLP，在时间序列预测方面具有巨大的优势。尽管在NLP和CV方面获得了成功，但许多研究发现，与MLP相比，Transformer在时间序列预测方面的效果不佳。在本文中，我们设计了一种特殊的Transformer，即通道对齐鲁棒双Transformer（CARD），以解决Transformer在时间序列预测中的关键缺点。首先，CARD引入了双Transformer结构，使其能够捕捉信号之间的时间相关性和多个变量在时间上的动态依赖。其次，我们引入了一种用于时间序列预测的鲁棒损失函数，以减轻潜在的过度拟合问题。这种新的损失函数基于预测不确定性加权预测在有限时间内的重要性。我们对多个长期和短期预测数据集进行的评估表明，CARD在精度和效率方面显著优于现有的方法。

    Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
    
[^216]: 可计算的基于图诱导的和积网络进行概率图表示学习

    Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])

    [http://arxiv.org/abs/2305.10544](http://arxiv.org/abs/2305.10544)

    GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。

    

    我们介绍了基于图诱导的和积网络 (GSPN)，它是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询。受消息传递神经网络中由顶点引起的计算树的启发，我们建立了一组和积网络（SPN）的层次结构，其中父SPN的参数是其子级的后验混合概率的可学习变换。由于权重共享和GSPN的树状计算图，我们获得了纯概率模型的效率和深度图网络的效果。我们在缺乏监督的情况下，处理缺失数据和图分类问题，证明了该模型相对于流行的神经模型的竞争力。我们通过超参数和模型回答概率查询的能力进行定性分析。

    We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
    
[^217]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^218]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^219]: 论数据异构性对分布式线性系统求解器收敛速度的影响

    On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers. (arXiv:2304.10640v1 [cs.DC])

    [http://arxiv.org/abs/2304.10640](http://arxiv.org/abs/2304.10640)

    本文比较了投影方法和优化方法求解分布式线性系统的收敛速度，提出了角异构性的几何概念，并对最有效的算法(APC和D-HBM)的收敛速度进行了约束和比较。

    

    本文考虑了解决大规模线性方程组的基本问题。特别地，我们考虑任务负责人打算在一组具有一些方程组子集的机器的分布式/联合帮助下解决该系统的设置。虽然有几种方法用于解决这个问题，但缺少对投影方法和优化方法收敛速度的严格比较。在本文中，我们分析并比较这两类算法，特别关注每个类别中最有效的方法，即最近提出的加速投影一致性(APC)和分布式重球方法(D-HBM)。为此，我们首先提出了称为角异构性的几何概念，并讨论其普遍性。使用该概念，我们约束并比较所研究算法的收敛速度，并捕捉两种方法的异构数据的效应。

    We consider the fundamental problem of solving a large-scale system of linear equations. In particular, we consider the setting where a taskmaster intends to solve the system in a distributed/federated fashion with the help of a set of machines, who each have a subset of the equations. Although there exist several approaches for solving this problem, missing is a rigorous comparison between the convergence rates of the projection-based methods and those of the optimization-based ones. In this paper, we analyze and compare these two classes of algorithms with a particular focus on the most efficient method from each class, namely, the recently proposed Accelerated Projection-Based Consensus (APC) and the Distributed Heavy-Ball Method (D-HBM). To this end, we first propose a geometric notion of data heterogeneity called angular heterogeneity and discuss its generality. Using this notion, we bound and compare the convergence rates of the studied algorithms and capture the effects of both 
    
[^220]: 病理图像诊断的跨尺度多实例学习

    Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])

    [http://arxiv.org/abs/2304.00216](http://arxiv.org/abs/2304.00216)

    本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。

    

    数字病理学中，跨多个尺度分析高分辨率的全幅图像 (WSIs) 带来了巨大的挑战。多实例学习 (MIL) 是利用分类对象集 (例如较小的图像块集) 对高分辨率图像进行处理的常见方法。然而，这种处理通常在WSIs的单个尺度（例如20倍放大）上进行，忽略了对人类病理学家诊断至关重要的跨尺度信息。在本研究中，我们提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中。本文的贡献有三个方面：(1) 提出了一种新的跨尺度MIL (CS-MIL)算法，它集成了多尺度信息和跨尺度关系；(2) 创建并发布了一个玩具数据集，其中包含尺度特异性形态特征，以检查和可视化不同的跨尺度关系；(3)在四个WSI的细胞肺癌数据集上进行了实验验证CS-MIL的有效性。

    Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
    
[^221]: 对比学习是相似性图谱上的谱聚类

    Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15103](http://arxiv.org/abs/2303.15103)

    本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。

    

    对比学习是一种强大的自监督学习方法，但我们对其运作原理和原因的理论理解有限。本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性。利用这种等价性作为基石，我们将分析扩展到CLIP模型，并严格描述多模态对象如何被嵌入到一起。在理论洞见的推动下，我们引入了核混合损失，结合新颖的核函数，在多个视觉数据集上优于标准高斯核。

    Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
    
[^222]: 政策梯度算法收敛于几乎线性二次型调节器的全局最优策略

    Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])

    [http://arxiv.org/abs/2303.08431](http://arxiv.org/abs/2303.08431)

    本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。

    

    决策者只获得了非完整信息的非线性控制系统在各种应用中普遍存在。本研究探索了强化学习方法，以找到几乎线性二次型调节器系统中最优策略。我们考虑一个动态系统，结合线性和非线性组成部分，并由相同结构的策略进行管理。在假设非线性组成部分包含具有小型Lipschitz系数的内核的情况下，我们对成本函数的优化进行了表征。虽然成本函数通常是非凸的，但我们确立了全局最优解附近局部的强凸性和光滑性。此外，我们提出了一种初始化机制，以利用这些属性。在此基础上，我们设计了一个策略梯度算法，可以保证以线性速率收敛于全局最优解。

    Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
    
[^223]: 不同的好手臂识别

    Differential Good Arm Identification. (arXiv:2303.07154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07154](http://arxiv.org/abs/2303.07154)

    本文提出了DGAI算法，它可以在好手臂识别问题中通过深度学习的方式减少样本复杂性，并且在具有给定阈值的情况下进一步提高多臂赌博问题的性能。

    

    本文针对一种变体的随机多臂赌博问题，称之为好手臂识别（GAI）。 GAI是一个纯探索的赌博问题，其目标是在尽可能少的样本数下输出尽可能多的好手臂，其中好手臂被定义为其期望奖励大于给定阈值的手臂。 在这项工作中，我们提出DGAI-一种可微的好手臂识别算法，以数据驱动方式改进了现有技术HDoC算法的样本复杂性。 我们还展示了DGAI可以进一步提升通用多臂赌博（MAB）问题的性能，给定一个阈值作为先验知识应用于手臂集。 大量实验证实，我们的算法在合成数据集和真实世界数据集中的GAI和MAB任务中显著优于基线算法。

    This paper targets a variant of the stochastic multi-armed bandit problem called good arm identification (GAI). GAI is a pure-exploration bandit problem with the goal to output as many good arms using as few samples as possible, where a good arm is defined as an arm whose expected reward is greater than a given threshold. In this work, we propose DGAI - a differentiable good arm identification algorithm to improve the sample complexity of the state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that the DGAI can further boost the performance of a general multi-arm bandit (MAB) problem given a threshold as a prior knowledge to the arm set. Extensive experiments confirm that our algorithm outperform the baseline algorithms significantly in both synthetic and real world datasets for both GAI and MAB tasks.
    
[^224]: Box$^2$EL: EL++描述逻辑中的概念和角色盒子嵌入的概念和角色盒子嵌入方法及其作用

    Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11118](http://arxiv.org/abs/2301.11118)

    Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。

    

    描述逻辑本体论扩展了知识图谱与概念信息和逻辑背景知识。近年来，人们对这种本体论的归纳推理技术越来越感兴趣，这些技术有望补充传统的演绎推理算法。类似于知识图谱的完善，现有的一些方法通过在潜在空间中学习本体论嵌入，同时确保这些嵌入能够准确地捕捉到底层描述逻辑的逻辑语义。然而，它们存在一些问题，主要是由于受限的角色表示。我们提出了Box$^2$EL方法，将概念和角色都表示为盒子（即轴对齐超矩形），并展示了它如何克服之前方法的局限性。我们在理论上证明了我们模型的正确性，并进行了大量的实验评估，在各种数据集上取得了领先的结果。作为我们评估的一部分，我们引入了一个新的基准。

    Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
    
[^225]: 基于JKO方案的可逆归一化流神经网络

    Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14424](http://arxiv.org/abs/2212.14424)

    本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。

    

    归一化流是一类用于高效采样和密度估计的深度生成模型。实际中，流通常表示为一系列可逆的神经网络模块链; 为了便于训练，现有的工作对流轨迹进行了正则化，并设计了特殊的网络架构。本文提出了受Jordan-Kinderleherer-Otto (JKO)方案启发的神经ODE流网络，它允许有效地按块进行残差块的训练，无需采样SDE轨迹或分数匹配或变分学习的内循环。由于JKO方案展开了梯度流的动态，所提出的模型自然地逐个堆叠残差网络块，降低了内存负载和进行端到端深度流网络训练的难度。我们还开发了自适应时间重新参数化的流网络，通过在概率空间中逐步细化轨迹，提高了模型的训练效率和准确性。

    Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
    
[^226]: 基于时间序列的数据增强技术：一份综述和分类

    Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13508](http://arxiv.org/abs/2206.13508)

    本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。

    

    随着深度学习建模的最新进展，利用其在时间序列领域中出色性能的方式并不需要太长时间。深度神经网络在处理时间序列方面严重依赖于训练中使用的数据集的大小和一致性。这些特征通常在现实世界中并不丰富，通常受到限制和需要保证的约束。因此，提高数据量的有效方法是使用数据增强技术，无论是通过添加噪声或置换还是生成新的合成数据。本文系统地回顾了该领域中的最新技术现状，提供了所有可用算法的概述，并提出了最相关研究的分类法。不同变体的效率将作为该过程的中心部分进行评估，同时还将评估不同的性能指标以及每个模型的主要问题。

    With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
    
[^227]: 计算机视觉GAN综述：最新研究、分析和分类（arXiv：2203.11242v2 [cs.LG] UPDATED）

    A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.11242](http://arxiv.org/abs/2203.11242)

    本文综述了GAN的最新架构、损失函数优化、验证指标和应用领域，并提出了一个分类法以更好地理解计算机视觉中GAN的现状。

    

    在过去的几年中，深度学习领域已经进行了几次革命，其中最受关注的是生成对抗网络（GANs）的巨大影响。GAN不仅在定义其模型时提供了独特的架构，而且生成了引人注目的结果，对社会产生了直接影响。由于GAN带来的重大改进和新的研究领域，社区不断提出新的研究，使得跟上时代几乎是不可能的。我们的综述旨在提供GAN的概述，展示最新的架构、损失函数的优化、验证指标和最广泛认可的变体的应用领域。将评估不同变体的模型架构效率，展示最佳的应用领域；作为该过程的重要组成部分，将分析评估GAN性能的不同度量标准和经常使用的损失函数。最后，将提出一个分类法以更好地理解GAN在计算机视觉领域中的现状。

    In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will
    

