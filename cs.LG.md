# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals.](http://arxiv.org/abs/2309.12312) | ForceSight是一个使用文本引导的移动操作系统，通过深度神经网络预测视觉力导向目标。在实验中，该系统展示了在未见环境中进行精确抓取、抽屉打开和物体交接等任务的能力，并取得了较高的成功率。 |
| [^2] | [LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent.](http://arxiv.org/abs/2309.12311) | LLM-Grounder是一种零样本、开放词汇的基于大型语言模型的3D视觉定位流程，通过利用语言模型分解查询并使用视觉定位工具识别物体，实现了在没有标记训练数据的情况下对新场景和文本查询的有效定位。在ScanRefer基准上取得了最先进的零样本定位准确性。 |
| [^3] | [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models.](http://arxiv.org/abs/2309.12307) | LongLoRA是一种高效的精细调整方法，可以在有限的计算成本下扩展预训练的大型语言模型的上下文大小。它通过稀疏的局部注意力实现精细调整，并使用移动短注意力有效实现上下文扩展，与传统方法具有相似的性能。 |
| [^4] | [Environment-biased Feature Ranking for Novelty Detection Robustness.](http://arxiv.org/abs/2309.12301) | 本文提出了一种环境偏向特征排序的方法，用于鲁棒性的新颖性检测。通过计算特征的环境之间分布方差进行评分，并通过去除高分特征来改善性能。这种方法在真实和合成基准数据上均能提高性能。 |
| [^5] | [See to Touch: Learning Tactile Dexterity through Visual Incentives.](http://arxiv.org/abs/2309.12300) | 本文提出了一种通过视觉激励优化触觉策略的框架，以增强多指机器人的触觉灵巧性，并在多个挑战性任务中取得了良好的效果。 |
| [^6] | [Learning to Drive Anywhere.](http://arxiv.org/abs/2309.12295) | 本文提出了一种能够学习适应不同地理位置和驾驶行为的模型，该模型通过引入基于地理位置的通道注意机制，在数据驱动的方式下高效地学习并灵活地建模不同地区之间的相似性和差异性。 |
| [^7] | [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A".](http://arxiv.org/abs/2309.12288) | LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。 |
| [^8] | [Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis.](http://arxiv.org/abs/2309.12283) | 本研究提出了一种基于扩散的多乐器音乐合成性能调节方法，通过对特定的演奏和录音环境进行条件处理，实现了更好的音色和风格引导控制。 |
| [^9] | [The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains.](http://arxiv.org/abs/2309.12279) | 本研究探讨特征模仿网络（FIN）在金融、语音和生理领域中的应用，发现FIN在比特币价格预测、语音情感识别和慢性颈痛检测方面能够显著改善性能，为深度学习架构提供了有希望的基础。 |
| [^10] | [Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports.](http://arxiv.org/abs/2309.12273) | 通过自适应的NLP模型选择和临床专家规则的分类器，该研究提出一种改进VTE识别的新方法，在放射学报告中准确识别VTE事件的准确性得到提高。 |
| [^11] | [Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications.](http://arxiv.org/abs/2309.12267) | 本文提出了一种名为估计平均聚合（EMA）的创新解决方案，旨在解决联邦学习中数据多样性和系统安全的挑战。EMA通过修剪均值处理恶意异常值，并揭示数据异质性，以确保训练模型适应不同的客户数据集。通过丰富的实验验证，EMA相对于其他方法表现出高准确性和曲线下面积（AUC），成为先进聚合技术的基准线。 |
| [^12] | [Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance.](http://arxiv.org/abs/2309.12259) | 本论文提出了一种灵活且鲁棒的软模型合并方法，用于增强神经网络性能。该方法通过学习门参数，利用硬具体分布乘以$l_0$范数的代理，实现了多个模型的快速合并，并增强了对具有极端值的恶意模型的鲁棒性。这种合并过程不仅提高了模型性能，还降低了计算成本。 |
| [^13] | [SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning.](http://arxiv.org/abs/2309.12253) | SALSA-CLRS是一种稀疏且可扩展的算法推理基准，优先考虑可扩展性和稀疏表示的利用。它通过引入适应于分布式算法和图神经网络的消息传递范式，解决了现有CLRS基准中内存需求高和运行时间难以扩展的问题。 |
| [^14] | [Parallelizing non-linear sequential models over the sequence length.](http://arxiv.org/abs/2309.12252) | 本论文提出了一种并行算法，能够加速GPU对于顺序模型的评估速度，提高了3个数量级，而不降低输出准确性。该算法适用于各种架构，并在长时间序列分类问题中发现了门控循环单元的有效性。 |
| [^15] | [SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References.](http://arxiv.org/abs/2309.12250) | 提出了一种新的问答系统评估指标SQuArE，使用多个参考答案进行句子级问答评估，实现了高度相关性的评估结果。 |
| [^16] | [Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images.](http://arxiv.org/abs/2309.12245) | 本论文研究了生成对抗网络中的模式塌陷问题对合成X射线图像多样性的影响。通过实验证明了将自适应输入图像归一化方法与深度模型结合的优势。 |
| [^17] | [Weakly-supervised Automated Audio Captioning via text only training.](http://arxiv.org/abs/2309.12242) | 通过仅使用文本数据和预训练的语言-音频对比模型（CLAP）来弱监督训练自动音频字幕生成模型，从而减少对配对音频和字幕数据的需求，并通过在训练和推断阶段采用策略来弥合音频和文本之间的差距。 |
| [^18] | [t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators.](http://arxiv.org/abs/2309.12237) | 该论文提出了t-EER这一新的无参数度量指标，用于联合评估在生物特征验证中与攻击检测同时操作的对策解决方案。这种联合评估方法相较于传统的分开评估方法具有更好的性能和实用性。 |
| [^19] | [Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing.](http://arxiv.org/abs/2309.12236) | 本文介绍了一种通过核平滑技术修正传统校准度量和可靠性图的方法，提出了一种新的平滑校准度量 SmoothECE，同时可视化该度量的可靠性图，并证明了其良好性质。 |
| [^20] | [Smooth Nash Equilibria: Algorithms and Complexity.](http://arxiv.org/abs/2309.12226) | 光滑纳什均衡是纳什均衡的一个松弛变种，可以通过实现与最佳光滑策略的偏离相同的效用来达到。我们定义了强和弱光滑纳什均衡的概念，并证明了它们在计算性质上优于传统的纳什均衡。 |
| [^21] | [Model-based Deep Learning for High-Dimensional Periodic Structures.](http://arxiv.org/abs/2309.12223) | 本研究提出了一种基于深度学习的代理模型，用于快速模拟高维频率选择性表面。通过引入物理洞察力，该模型可以在训练后使用减少的数据集准确预测某种结构的S参数。该模型适用于各种频率选择性表面，无论是基于孔隙还是任意几何形状的贴片。 |
| [^22] | [SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On.](http://arxiv.org/abs/2309.12218) | SR-PredictAO是一种基于会话的推荐系统，通过引入高能力预测器模块，解决了现有模型中低能力预测器模块的问题，可以在存在随机用户行为的情况下预测用户的下一个动作。 |
| [^23] | [A Multi-label Classification Approach to Increase Expressivity of EMG-based Gesture Recognition.](http://arxiv.org/abs/2309.12217) | 本论文提出了一种多标签分类方法，通过问题转换的方式有效提高基于肌电信号的手势识别系统的表达能力。通过生成合成数据，可以快速校准并识别组合手势，提高了性能。 |
| [^24] | [Regionally Additive Models: Explainable-by-design models minimizing feature interactions.](http://arxiv.org/abs/2309.12215) | 区域可加模型 (RAMs) 是一种设计可解释模型，通过在特征空间内识别子区域来最小化特征的交互。相比于广义可加模型(GAMs)，RAMs能拥有更丰富的模型表达能力，同时保持可解释性。 |
| [^25] | [SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices.](http://arxiv.org/abs/2309.12212) | 提出了一种基于绝热超导Josephson器件的随机二值神经网络加速框架SupeRBNN，通过软硬件协同优化，解决了绝热量子流参数元在二值神经网络加速中的挑战。 |
| [^26] | [Physics-informed State-space Neural Networks for Transport Phenomena.](http://arxiv.org/abs/2309.12211) | 这项研究引入了物理信息触发状态空间神经网络模型（PSMs），用于解决在化学、生物医学和电厂等传输现象中实现实时优化和容错性的问题。通过使用传感器数据训练深度神经网络，并使用物理模型进行传输系统建模，PSMs提供了比传统数据驱动模型更准确的方法，并具有多种应用场景。 |
| [^27] | [Boolformer: Symbolic Regression of Logic Functions with Transformers.](http://arxiv.org/abs/2309.12207) | Boolformer是第一个经过训练的Transformer架构，用于执行端到端的布尔函数符号回归。它可以预测复杂函数的简洁公式，并在提供不完整和有噪声观测时找到近似表达式。Boolformer在真实二分类数据集上展现出潜力作为可解释性替代方案，并在基因调控网络动力学建模任务中与最先进的遗传算法相比表现出竞争力。 |
| [^28] | [PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements.](http://arxiv.org/abs/2309.12204) | PrNet是一个用于改善Android原始GNSS测量定位的神经网络，通过校正伪距偏差来提高定位性能，实现了同时处理伪距偏差和噪声的混合管道。 |
| [^29] | [Empowering Precision Medicine: AI-Driven Schizophrenia Diagnosis via EEG Signals: A Comprehensive Review from 2002-2023.](http://arxiv.org/abs/2309.12202) | 本研究综述了基于脑电信号的人工智能技术在精神分裂症诊断中的应用，以解决脑电信号分析中的挑战。 |
| [^30] | [Electroencephalogram Sensor Data Compression Using An Asymmetrical Sparse Autoencoder With A Discrete Cosine Transform Layer.](http://arxiv.org/abs/2309.12201) | 本文提出了一种使用非对称稀疏自编码器和离散余弦变换层对脑电图传感器数据进行压缩的方法。实验结果表明，这种方法能有效地减少EEG信号的冗余，并在保持稀疏性的同时提高数据的准确性。 |
| [^31] | [A Variational Auto-Encoder Enabled Multi-Band Channel Prediction Scheme for Indoor Localization.](http://arxiv.org/abs/2309.12200) | 本文提出了一个基于变分自编码器的室内定位方案，通过预测信道状态信息值并将多频段信息拼接在一起，提高了室内指纹定位的准确性。 |
| [^32] | [Brain Tumor Detection Using Deep Learning Approaches.](http://arxiv.org/abs/2309.12193) | 本研究使用深度学习方法，特别是ResNet50，来改进脑肿瘤的检测和分类准确性，以探索自动化检测过程的可能性。 |
| [^33] | [Optimal Conditional Inference in Adaptive Experiments.](http://arxiv.org/abs/2309.12162) | 我们研究了在自适应实验中进行条件推断的问题，证明了在没有进一步限制的情况下，仅使用最后一批结果进行推断是最优的；当实验的自适应方面是位置不变的时，我们还发现了额外的信息；在停止时间、分配概率和目标参数仅依赖于数据的多面体事件集合的情况下，我们推导出了计算可行且最优的条件推断程序。 |
| [^34] | [Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval.](http://arxiv.org/abs/2309.12158) | 本文研究了音频-乐谱检索的当前发展情况，通过深度学习方法解决了鲁棒性和大规模应用的挑战。 |
| [^35] | [Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features.](http://arxiv.org/abs/2309.12140) | 本研究提出了一种方法，利用无标签的重复遍历来适应物体检测器到新的驾驶环境。通过结合重复的激光雷达扫描计算的统计数据，我们有效地引导自适应过程，并通过引入轻量级的回归头和自训练过程来增强检测模型。实验表明，该方法在现实世界数据集上取得了多达20个百分点的性能提升。 |
| [^36] | [Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems.](http://arxiv.org/abs/2309.12128) | 本研究通过探索连接理论和实践，提供了无监督神经网络在解决逆问题中的收敛和恢复性能保证。同时，我们还得出了对于两层具有平滑激活函数的深度逆先验网络的超参数化界限，该网络将从我们的保证中受益。 |
| [^37] | [Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval.](http://arxiv.org/abs/2309.12111) | 本文提出了一个使用循环模型的音频-乐谱检索方法，通过学习跨模态循环网络生成可以摘要对应音频和乐谱的更长段落的联合嵌入。相比于传统方法，该方法只需要弱对齐的音频-乐谱对，并且能够处理音频和乐谱之间的节奏变化导致的非线性。 |
| [^38] | [Bayesian sparsification for deep neural networks with Bayesian model reduction.](http://arxiv.org/abs/2309.12095) | 使用贝叶斯模型缩减作为一种更高效的替代方法来修剪模型权重，以提高深度神经网络的计算效率。 |
| [^39] | [Clustering-based Domain-Incremental Learning.](http://arxiv.org/abs/2309.12078) | 本文提出了一个基于聚类的方法来解决连续学习中的灾难性遗忘问题，该方法不需要提供任务更改的信息，并通过在线聚类方法有效地对抗灾难性遗忘。 |
| [^40] | [Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives.](http://arxiv.org/abs/2309.12067) | 本文提供了足球中动作识别、定位和时空定位的综述，重点介绍了多模态方法。通过利用深度学习技术和传统方法，这些方法整合了来自多个数据源的信息，并以多种方式表示一种来源，在提高模型准确性和稳健性方面具有潜力。 |
| [^41] | [An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM.](http://arxiv.org/abs/2309.12058) | 本研究提出了一个整合了词嵌入和深度学习技术的高效抗癌肽分类模型，并评估了Word2Vec和FastText作为词嵌入技术，以及CNN、LSTM、BiLSTM作为深度学习模型的性能。 |
| [^42] | [Uplift vs. predictive modeling: a theoretical analysis.](http://arxiv.org/abs/2309.12036) | 本文提供了提升建模与经典预测方法的理论分析，研究贡献包括新的利润度量公式，收敛性证明以及条件的模拟说明。 |
| [^43] | [Face Identity-Aware Disentanglement in StyleGAN.](http://arxiv.org/abs/2309.12033) | 本文通过引入PluGeN4Faces插件，利用对比损失来明确解缠脸部属性与人的身份，提出了一种解决面部图像修改中对身份冲突问题的方法。 |
| [^44] | [Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets.](http://arxiv.org/abs/2309.12032) | 该论文提出了一种人机协同的因果发现方法，通过使用生成流网按照基于评分函数的信念分布采样祖先图，并引入最佳实验设计与专家互动，以提供专家可验证的不确定性估计并迭代改进因果推断。 |
| [^45] | [Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting.](http://arxiv.org/abs/2309.12028) | 本文提出了一种名为动态超图结构学习(DyHSL)的模型，用于解决交通流量预测问题。该模型利用超图结构信息来建模复杂的交通网络，并且能够捕捉复杂的时空高阶交互作用。 |
| [^46] | [Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint.](http://arxiv.org/abs/2309.12025) | 本文提出了针对带有背包约束的非单调k-次模子模块化最大化问题的两个鲁棒逼近算法，大大改进了现有算法的查询复杂度，并分别提供了$1/19$和$1/5-\epsilon$的近似比率。 |
| [^47] | [Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption.](http://arxiv.org/abs/2309.12004) | 本文提出了一种针对立方卫星任务调度的层次化强化学习方法，通过整合任务优先级排序和能耗预测，实现了一个安全且容错的系统，并在多个立方卫星配置下优于MADDPG模型和传统随机调度。 |
| [^48] | [Identification of pneumonia on chest x-ray images through machine learning.](http://arxiv.org/abs/2309.11995) | 该研究开发了一种通过机器学习和迁移学习技术，能够在胸部X射线片上识别肺炎的软件，具有98％的敏感性和97.3％的特异性。 |
| [^49] | [Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization.](http://arxiv.org/abs/2309.11994) | 本文提出了一个框架，利用未评估的解决方案来提高代理辅助进化算法（SAEAs）的效率，通过代理模型识别高质量解决方案，并直接生成新的解决方案，无需评估。 |
| [^50] | [Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis.](http://arxiv.org/abs/2309.11987) | 通过用户研究，分析了后置解释方法中的可理解性和可预测性。发现当解释集中在模型决策边界附近的样本时，SHAP的可理解性显著降低。另外，发现反事实解释和错误分类可以显著提高用户对机器学习模型决策原理的理解。根据研究结果，提出了增强后置解释方法可理解性和可预测性的设计建议。 |
| [^51] | [Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling.](http://arxiv.org/abs/2309.11983) | 本论文将连接主义时间分类（CTC）与变分模型相结合，提出了两个版本的新型变分CTC，用于训练更具普适性的保序序列模型。这些方法允许直接优化模型对数似然的变分下界，并解决了计算上的挑战。 |
| [^52] | [Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT.](http://arxiv.org/abs/2309.11979) | 本论文通过构建自然语言处理模型BERT并对其进行fine-tune，实现了股市情绪的分类和基于该模型的回测分析。实验结果显示，fine-tuned模型相比原始模型和基准模型有不同程度的性能改进。 |
| [^53] | [Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions.](http://arxiv.org/abs/2309.11963) | 本研究介绍了一种新颖的层次分割聚类方法，通过使用随机分裂函数提高多类数据集的分类性能。该方法可以生成层次结构而无需显式信息，适用于缺乏层次先验知识的数据集。实验结果表明，该方法在一半以上的数据集中显著提升分类性能。 |
| [^54] | [A Study of Forward-Forward Algorithm for Self-Supervised Learning.](http://arxiv.org/abs/2309.11955) | 本文首次研究了自监督表示学习中的向前-向前算法和反向传播的性能，发现在自监督表示学习中，向前-向前算法与反向传播表现相当。 |
| [^55] | [On the Probability of Immunity.](http://arxiv.org/abs/2309.11942) | 本文研究了免疫的概率，提出了免疫的必要和充分条件，以及ε-有界免疫的条件。同时，借助随机对照试验估计受益概率，并得到比现有边界更紧密的概率边界。此外，介绍了间接免疫的概念，并提出了一种用于处理未测量混淆的免疫概率敏感性分析方法。 |
| [^56] | [A Machine Learning-oriented Survey on Tiny Machine Learning.](http://arxiv.org/abs/2309.11932) | 本综述旨在提供关于微型机器学习（TinyML）的学习算法的最新综述，重点关注其在人工智能领域的应用与潜在贡献。 |
| [^57] | [Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning.](http://arxiv.org/abs/2309.11930) | 本论文提出了两个方法，一个是自适应边界损失，通过调整边界来同步学习速度；另一个是伪标签对比聚类，通过聚集样本来增强新类别的发现。实验证明，该方法能够平衡已见和新类别，相比现有模型，在ImageNet数据集上提高了3%的平均准确率。 |
| [^58] | [Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes.](http://arxiv.org/abs/2309.11875) | 本文提出了一种基于物理信息的高斯过程模型，用于Timoshenko梁的刚度识别和响应估计。通过马尔可夫链蒙特卡洛方法进行贝叶斯优化，得到结构参数的随机模型。模型还可用于概率预测未观测到的响应，并提出了一种基于熵的传感器布置优化方法。 |
| [^59] | [Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization.](http://arxiv.org/abs/2309.11856) | 本论文提出了一种使用改进的方差最小化的分块量化策略，用于压缩图神经网络的激活，实现内存消耗的降低和运行时的加速。 |
| [^60] | [TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification.](http://arxiv.org/abs/2309.11845) | TMac是一个时态多模态图学习方法，用于声音事件分类。它通过图学习的方式对多模态数据中的时态信息进行建模，提高了声音事件分类的性能。 |
| [^61] | [A Comprehensive Review of Community Detection in Graphs.](http://arxiv.org/abs/2309.11798) | 本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。 |
| [^62] | [DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning.](http://arxiv.org/abs/2309.11782) | DimCL是一种在自监督学习中改进特征多样性的维度对比学习方法。实验证明DimCL的硬样本特性是成功的关键因素。将DimCL融入SSL框架可以提高性能。 |
| [^63] | [Dictionary Attack on IMU-based Gait Authentication.](http://arxiv.org/abs/2309.11766) | 这项研究提出了一种对使用惯性测量单元记录的步态模式进行认证的敌对模型。研究调查了是否可能构建一本IMUGait模式的字典，用于发动攻击或找到能够匹配目标IMUGait模式的模仿者。通过对错误率的进一步分析，挑战了基于IMUGait模式的认证系统是否最困难的观点。 |
| [^64] | [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation.](http://arxiv.org/abs/2309.11765) | 本论文提出了一种隐私保护下的上下文学习算法，通过生成具有差分隐私保证的合成少量示范，实现了有效的ICL。实验证明该算法在强隐私级别下能够取得竞争性能，为广泛应用领域的隐私保护下ICL开辟了新的可能性。 |
| [^65] | [SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.](http://arxiv.org/abs/2309.11758) | SAM-OCTA是一种将基础模型应用于OCTA图像分割任务的微调策略，在公开的OCTA-500数据集上实验表现最先进的性能指标，成功实现了局部血管分割和有效的动脉-静脉分割。 |
| [^66] | [How Robust is Google's Bard to Adversarial Image Attacks?.](http://arxiv.org/abs/2309.11751) | 本文研究了Google的Bard在对抗图像攻击方面的鲁棒性，并发现它可以被攻击以输出错误的图像描述。这一攻击还可以对其他多模态语言模型产生影响。研究还发现了Bard的两种防御机制。 |
| [^67] | [PIE: Simulating Disease Progression via Progressive Image Editing.](http://arxiv.org/abs/2309.11745) | PIE是一个新的渐进图像编辑框架，可以通过控制性地操纵图像特征来准确模拟个体患者的疾病进展。该方法利用文本到图像生成模型，实现了个性化的疾病进展模拟，并在验证实验中展现了优越性。 |
| [^68] | [Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations.](http://arxiv.org/abs/2309.11741) | 该论文提出了一种名为UGPIG的方法，通过应用剪枝用户图和意图图解决空间异质性和稀疏性数据的问题，以实现推荐适用的可持续发展模式。 |
| [^69] | [Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning.](http://arxiv.org/abs/2309.11722) | 本文介绍了一个联邦学习的数据共享博弈模型，并利用博弈论的方法设计了一个选核激励机制，以激励真实输入数据并促进稳定合作。 |
| [^70] | [A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification.](http://arxiv.org/abs/2309.11714) | 本文提出了一种动态领域适应深度学习网络，用于解决基于EEG的运动想象分类中的通道相关性和个体差异问题。 |
| [^71] | [Quasi-Monte Carlo for 3D Sliced Wasserstein.](http://arxiv.org/abs/2309.11713) | 本文提出了准蒙特卡洛（QMC）方法用于三维切片Wasserstein（SW）的近似计算，并通过多种方法在三维单位超球面上构造了QMC点集。此外，还介绍了将QSW扩展为随机准切片Wasserstein（RQSW）的方法。 |
| [^72] | [Meta OOD Learning for Continuously Adaptive OOD Detection.](http://arxiv.org/abs/2309.11705) | 该论文提出了一种基于元学习的持续适应性OOD检测模型，针对现实世界系统中ID和OOD分布持续变化的问题，能够在部署时动态快速地适应新到达的分布，并且在部署期间ID样本不足。 |
| [^73] | [Incentivized Communication for Federated Bandits.](http://arxiv.org/abs/2309.11702) | 本论文介绍了一个新的鼓励式通信问题，即针对自利的联邦赌臂机中，服务器通过提供激励来促使客户机分享数据，以提高学习效率和实际操作性。 |
| [^74] | [Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening.](http://arxiv.org/abs/2309.11687) | 本研究探究了在贝叶斯优化的主动学习框架中，预训练的Transformer语言模型和图神经网络在提高分子虚拟筛选样本效率方面的表现。 |
| [^75] | [Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework.](http://arxiv.org/abs/2309.11682) | 本文提出了一个基于随机分布鲁棒的公平性框架，解决了训练和测试数据分布不一致时公平模型表现不准确的问题，并且不需要知道因果图，也支持使用小批量数据。 |
| [^76] | [Federated Learning with Neural Graphical Models.](http://arxiv.org/abs/2309.11680) | 本研究提出了一种名为FedNGMs的联邦学习框架，利用概率神经图模型来处理多个客户端的数据，并在保持训练数据私密性的同时提升模型准确性。 |
| [^77] | [Popularity Degradation Bias in Local Music Recommendation.](http://arxiv.org/abs/2309.11671) | 本文研究了流行度衰减偏差对本地音乐推荐的影响，发现在推荐流行艺术家方面，权重相关矩阵分解和多项式变分自动编码器表现较好，但对于不太流行的艺术家来说，多项式变分自动编码器的相对性能更好。 |
| [^78] | [GLM Regression with Oblivious Corruptions.](http://arxiv.org/abs/2309.11657) | 这篇论文介绍了第一个在广义线性模型回归问题中处理加法无意识噪声的算法。算法的目标是通过样本访问来准确地恢复参数向量，使得模型的预测与真实值的误差尽可能小。 |
| [^79] | [Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks.](http://arxiv.org/abs/2309.11651) | 该论文提出了一种基于神经网络的计算方法，用于漂移控制高维RBMs。通过深度神经网络技术，该方法在测试问题上达到了较高的准确性。 |
| [^80] | [Orbital AI-based Autonomous Refuelling Solution.](http://arxiv.org/abs/2309.11648) | 本文介绍了一种基于人工智能的导航算法，旨在通过使用轨道上的可见波长摄像头作为主要传感器，减少对激光雷达的依赖，并大大降低成本。 |
| [^81] | [Potential and limitations of random Fourier features for dequantizing quantum machine learning.](http://arxiv.org/abs/2309.11647) | 本文研究了随机傅里叶特征在去量化量子机器学习中的潜力与局限性，并在回归问题上确立了其高效去量化的必要和充分条件，并提出了PQC架构设计建议和识别了潜在量子优势的必要结构。 |
| [^82] | [Early diagnosis of autism spectrum disorder using machine learning approaches.](http://arxiv.org/abs/2309.11646) | 本文利用机器学习方法旨在提供自闭症谱系障碍(ASD)的早期诊断，并通过研究不同算法寻找最显著的特征，自动化诊断过程。 |
| [^83] | [Latent Diffusion Models for Structural Component Design.](http://arxiv.org/abs/2309.11601) | 本文提出了一个潜在扩散模型的框架，用于生成符合加载条件的结构组件设计。与其他生成方法相比，该方法允许对现有设计进行编辑，并且具有近优性。实验结果证明了生成设计的结构性能和潜在候选设计的可变性。 |
| [^84] | [CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches.](http://arxiv.org/abs/2309.11587) | CATS是一种基于深度学习的地理人工智能方法，用于隐私保护轨迹数据的生成和发布。它采用K-匿名技术保障了分布级隐私，通过条件对抗训练和循环二部图匹配等方法实现了高质量轨迹数据的合成和重构。 |
| [^85] | [Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge.](http://arxiv.org/abs/2309.11575) | 本研究为Adversarial Nibbler挑战提供了一个大型的潜在对抗输入集合，并通过对提示和图像的分析揭示了当前生成图像模型中的系统性安全问题。 |
| [^86] | [BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model.](http://arxiv.org/abs/2309.11568) | BTLM-3B-8K是一个30亿参数的开源语言模型，相对于其他30亿和70亿参数模型，它在下游任务中表现出2-5.5%的性能提升，同时在长文本任务上也具有出色的表现。这种将70亿参数的模型压缩到30亿参数，并且性能几乎没有受到影响的方法具有重要意义。 |
| [^87] | [Hierarchical reinforcement learning with natural language subgoals.](http://arxiv.org/abs/2309.11564) | 本文介绍了一种层次强化学习的新方法，使用来自人类解决任务的数据来监督一组长程任务的目标空间，并使用自然语言来描述这个空间，该方法在克隆专家行为的代理和无监督子目标空间的层次强化学习中表现出色。 |
| [^88] | [EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian.](http://arxiv.org/abs/2309.11531) | 本文提出了一种名为EPTQ的增强后训练量化方法，该方法通过自适应加权层和无标签Hessian近似技术实现了最先进的结果。 |
| [^89] | [TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback.](http://arxiv.org/abs/2309.11527) | TrueLearn是一个Python库，用于构建个性化的信息推荐系统，并提供了丰富的文档和编码示例，可帮助开发人员和从业者使用。它采用了开放学习者的概念和人性化的用户表达方式，同时支持用户可视化和模型性能评估。 |
| [^90] | [Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems.](http://arxiv.org/abs/2309.11526) | 该论文介绍了一种基于似然的传感器校准方法，可以在物联网系统中实现专家支持的分布式学习算法。通过对模拟和实际测量数据的评估，证明了该方法的有效性和改进效果。 |
| [^91] | [Ad-load Balancing via Off-policy Learning in a Content Marketplace.](http://arxiv.org/abs/2309.11518) | 本文介绍了一个使用离线学习和强化学习反馈的方法来解决在线广告系统中的广告负载平衡问题，该方法能够适应用户偏好和上下文因素的变化，并最大化用户参与度和收入。 |
| [^92] | [Private Matrix Factorization with Public Item Features.](http://arxiv.org/abs/2309.11516) | 这项研究提出了一种使用公共项目特征进行私有矩阵分解的方法，以缓解差分隐私训练对推荐质量的影响，并展示了这种方法的简单性、易调整性和可扩展性。 |
| [^93] | [Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach.](http://arxiv.org/abs/2309.11515) | 这项工作提出了一种新颖的差分隐私顺序推荐框架，采用了噪声图神经网络方法，解决了现有差分隐私推荐系统在动态和依赖关系方面的局限性，同时也关注了敏感用户特征的隐私风险。 |
| [^94] | [Multidimensional well-being of US households at a fine spatial scale using fused household surveys: fusionACS.](http://arxiv.org/abs/2309.11512) | 该论文介绍了fusionACS项目，通过将多个美国家庭调查的数据进行融合，提供了一个能够分析家庭属性和福祉维度的综合微数据集，从而使研究者可以回答更加多样化的研究问题。 |
| [^95] | [Using causal inference to avoid fallouts in data-driven parametric analysis: a case study in the architecture, engineering, and construction industry.](http://arxiv.org/abs/2309.11509) | 本研究使用了一个建筑能源消耗案例研究，展示了在数据驱动模型中进行因果分析的必要性，发现数据驱动模型的准确性评估和领域知识筛选无法排除偏倚结果，因此在选择特征时需要仔细考虑因果关系，而因果分析的结果可以帮助设计和参数检查，避免认知偏差。 |
| [^96] | [AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting.](http://arxiv.org/abs/2309.11507) | AdBooster通过使用稳定扩散外景架构和生成模型，实现了个性化广告创作的创意优化，其有效性得到了实验证明。 |
| [^97] | [Fairness Vs. Personalization: Towards Equity in Epistemic Utility.](http://arxiv.org/abs/2309.11503) | 该论文研究了个性化推荐系统中公平性与个性化之间的困境，并提出了在认知效用背景下实现公正的公平的解决方案。 |
| [^98] | [Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning.](http://arxiv.org/abs/2309.11489) | Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。 |
| [^99] | [Self-supervised learning unveils change in urban housing from street-level images.](http://arxiv.org/abs/2309.11354) | 本论文使用自监督学习方法分析伦敦的城市变化，通过应用于1500万张街景图像，成功地识别出了住房供应的变化，并区分了主要和次要变化。 |
| [^100] | [Contrastive Pseudo Learning for Open-World DeepFake Attribution.](http://arxiv.org/abs/2309.11132) | 这项研究提出了一种对开放世界深度伪造归因任务的新框架，并引入了一个评估归因性能的新基准。该框架通过引入全局-局部投票模块和设计置信度-based的软伪标签策略来提高归因准确性，并缓解相似造成的伪噪声。 |
| [^101] | [Fake News BR: A Fake News Detection Platform for Brazilian Portuguese.](http://arxiv.org/abs/2309.11052) | 本研究提出了一个用于巴西葡萄牙语的假新闻检测平台，采用机器学习和自然语言处理技术，能够高效准确地识别假新闻，同时提供实时分析和验证新闻文章真实性的用户友好平台。 |
| [^102] | [What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples.](http://arxiv.org/abs/2309.10916) | 本文将图像处理领域中的对抗子空间技术应用于自然语言处理，提出了基于最近邻和影响函数的检测器，并通过使用影响函数揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系和任务差异。 |
| [^103] | [On the different regimes of Stochastic Gradient Descent.](http://arxiv.org/abs/2309.10688) | 这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。 |
| [^104] | [Learning End-to-End Channel Coding with Diffusion Models.](http://arxiv.org/abs/2309.10505) | 本文提出了使用扩散模型学习端到端信道编码的框架，并通过模拟实验证明了扩散模型能够准确学习信道分布从而实现接近最优的端到端符号误码率。 |
| [^105] | [Crowdotic: Transformer-based Occupancy Estimation for Hospital Waiting Rooms with Non-speech Audio and Differential Privacy.](http://arxiv.org/abs/2309.10280) | 本研究提出了一种基于非语音音频的人群分析方法，利用Transformer模型实现医院候诊室的占用预测，并且在准确性方面表现出色。这是首次提出使用非语音音频信号进行占用预测的方法，超过了其他基线方法。 |
| [^106] | [FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks.](http://arxiv.org/abs/2309.09517) | FedGKD是一种新颖的联邦图神经网络框架，通过利用客户端图数据集蒸馏方法提取更好的任务特征并引入感知全局协作结构的服务器端聚合机制，解决了联邦GNN系统中图异构性问题，提高了效率和准确性。 |
| [^107] | [Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems.](http://arxiv.org/abs/2309.08617) | Drifter是一个高效的在线特征监控系统，通过敏捷、响应和适应性的数据质量监控，实时分析、检测和解决推荐系统中的数据问题，使得实时推荐系统的可靠性和性能得到显著提升。 |
| [^108] | [Compositional Foundation Models for Hierarchical Planning.](http://arxiv.org/abs/2309.08587) | 本研究提出了一种基于组合式基础模型的层次规划方法，通过利用语言、视觉和动作数据的多个专家模型，解决了长期目标任务。通过符号计划、视频扩散和逆动力学模型的结合，实现了在新环境中做出有效决策的能力。 |
| [^109] | [Ensuring Toplogical Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes.](http://arxiv.org/abs/2309.08228) | 通过在高斯-勒让德节点上进行潜在空间正则化，我们的研究提出了一种新的无监督自编码器，能够确保在压缩过程中保持拓扑数据结构的完整性。 |
| [^110] | [Weakly supervised learning for pattern classification in serial femtosecond crystallography.](http://arxiv.org/abs/2309.04474) | 本文介绍了在串行飞秒晶体学中通过弱监督算法对衍射图进行分类的工作，旨在尽可能减少训练所需的标签数据集的规模。 |
| [^111] | [ConDA: Contrastive Domain Adaptation for AI-generated Text Detection.](http://arxiv.org/abs/2309.03992) | 创新点：提出了一种基于对比域适应的框架 ConDA，用于检测由大型语言模型生成的新闻文本。这种方法解决了获取标记训练数据的困难，通过利用未标记的目标数据进行无监督域适应。 |
| [^112] | [Instruction Tuning for Large Language Models: A Survey.](http://arxiv.org/abs/2308.10792) | 本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。 |
| [^113] | [ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations.](http://arxiv.org/abs/2308.10457) | ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。 |
| [^114] | [Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation.](http://arxiv.org/abs/2308.07929) | 本研究提出了一种快速自适应方法，利用Bradley-Terry偏好模型，通过很少的示例和最小的计算资源高效地微调大型多模态模型，使其更符合用户的偏好，并在多个领域中展示了该方法的能力。 |
| [^115] | [Bayesian Flow Networks.](http://arxiv.org/abs/2308.07037) | 本文介绍了贝叶斯流网络（BFNs），一种新的生成模型，它通过贝叶斯推断修改了一组独立分布的参数，并将其作为输入传递给神经网络来生成另一个相互依赖的分布。该方法不需要前向过程，适用于连续和离散数据，并具有优化数据压缩的功能。 |
| [^116] | [Multiclass Learnability Does Not Imply Sample Compression.](http://arxiv.org/abs/2308.06424) | 学习二元假设类具有样本压缩方案，而多类别假设类则不具备这个性质。 |
| [^117] | [Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting.](http://arxiv.org/abs/2307.15299) | 本研究使用差分进化算法选择Transformer神经网络模型的优化超参数，以提高负荷预测的准确性。 |
| [^118] | [Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization.](http://arxiv.org/abs/2307.02108) | 这篇论文提出了一种适用于情境赌博设置的新型计算效率高的赌博算法，具有简单和累积遗憾最小化的优势，并可自适应模型错误规范和连续臂设置。该算法利用"一致臂集"（CAS）来提供在每个情境下囊括情境特定的最佳臂的一组臂，跨越情境分布。这篇论文对简单和累积遗憾保证的研究提供了正面结果，同时也揭示了无法实现实例依赖性的简单遗憾保证的消极结果。 |
| [^119] | [A Constructive Approach to Function Realization by Neural Stochastic Differential Equations.](http://arxiv.org/abs/2307.00215) | 本文采用了一种构造性方法，通过限制系统动力学来刻画可以实现的函数类，从而避免了高复杂度的控制。实现方法包括神经随机微分方程、确定性动力系统和输出映射的级联连接。这些结果有助于提高函数逼近算法的实际可行性。 |
| [^120] | [$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces.](http://arxiv.org/abs/2306.17366) | 这项研究提出了一种$\lambda$-AC算法，通过学习连续状态空间中的潜在决策感知模型，实现了决策驱动的强化学习。通过理论和实证研究，确定了决策感知强化学习模型的必要组成部分，并展示了设计选择对算法性能的重要影响。 |
| [^121] | [HNO: Hyena Neural Operator for solving PDEs.](http://arxiv.org/abs/2306.16524) | 本研究使用了一种名为鬣狗的新型神经算子，它利用多层感知器参数化的长卷积滤波器来解决PDE问题。这种方法通过增强模型对输入上下文的理解，并为不同的PDE实例提供数据依赖权重，提供了一种有效的求解PDE的方式。 |
| [^122] | [Prodigy: An Expeditiously Adaptive Parameter-Free Learner.](http://arxiv.org/abs/2306.06101) | 本文提出了一种基于自适应算法(如Adagrad和Adam)的学习率估计方法Prodigy和Resetting，可以快速且正确地估计到达解决方案所需的距离D，从而提高了模型的收敛速度，并在多个数据集和模型上进行了测试，实验表明该方法优于D-Adaptation并可达到手动调整Adam的测试准确度值。 |
| [^123] | [Global universal approximation of functional input maps on weighted spaces.](http://arxiv.org/abs/2306.03303) | 本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。 |
| [^124] | [GrACE: Generation using Associated Code Edits.](http://arxiv.org/abs/2305.14129) | 本文研究了如何利用预训练大型语言模型来生成代码，并通过赋予模型先前相关的编辑知识，来解决代码多样性和开发人员意图难以捕捉的问题。实验表明，这种方法有效提高了模型的性能。 |
| [^125] | [Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2305.13706) | 这篇论文提出了一种基于单调性驱动的深度强化学习算法，用于处理在6G时代物联网系统中的大规模语义感知传输调度问题。数值结果显示所提出的算法相比基准算法可以大大减少训练时间并提高训练性能。 |
| [^126] | [ZeroFlow: Fast Zero Label Scene Flow via Distillation.](http://arxiv.org/abs/2305.10424) | ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。 |
| [^127] | [Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training.](http://arxiv.org/abs/2305.05237) | 本文提出一种名为SCPT的框架，利用对比学习进行空间预训练，并引入一个空间编码器模块，用于从未见数据中提取特征。该方法可以用于进行新道路的交通预测，无需重新训练模型。 |
| [^128] | [Persistent Homology of the Multiscale Clustering Filtration.](http://arxiv.org/abs/2305.04281) | 该论文引入了一种多尺度聚类过滤方法（MCF），用于描述不同尺度下的数据聚类，其中的持久同调可测量分区序列的层次关系和聚类分配冲突的出现和解决。 |
| [^129] | [CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis.](http://arxiv.org/abs/2304.12654) | CoDi 方法使用两个共同演化的对比扩散模型单独处理离散和连续变量并相互条件化，同时引入对比学习方法进行进一步的绑定，展现了在真实世界的表格数据集上的有效性。 |
| [^130] | [Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints.](http://arxiv.org/abs/2304.06104) | 提出了一种基于原始-对偶语境贝叶斯优化算法，可以实现对约束闭环控制系统的在线性能优化，同时满足所需的约束条件。 |
| [^131] | [Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning.](http://arxiv.org/abs/2304.03398) | 本文提出了一种通用方法，可以可靠地量化量子模型的不确定性，无论训练数据的数量、拍摄次数、ansatz、训练算法以及量子硬件噪声的存在如何。 |
| [^132] | [Cross-scale Multi-instance Learning for Pathological Image Diagnosis.](http://arxiv.org/abs/2304.00216) | 本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。 |
| [^133] | [A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models.](http://arxiv.org/abs/2303.13773) | 本研究提出基于GNN的纳米卫星任务调度方法，以更好地优化服务质量，解决ONTS问题的复杂性。 |
| [^134] | [Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks.](http://arxiv.org/abs/2303.10310) | 本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。 |
| [^135] | [Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks.](http://arxiv.org/abs/2303.01682) | Neural-BO是一种使用神经网络模型的黑盒优化算法，避免了高斯过程中的缩放和维数问题，具有高效收敛的特性。 |
| [^136] | [Subsampling Suffices for Adaptive Data Analysis.](http://arxiv.org/abs/2302.08661) | 子采样是自适应数据分析中的关键方法，仅需基于随机子样本和少量比特输出的查询，即可保证代表性和泛化性。 |
| [^137] | [Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies.](http://arxiv.org/abs/2301.13019) | 本文提出了一个解决方案，通过在离线数据集中识别专家行为，改善了机器人操作策略的行为克隆。在NeurIPS 2022竞赛中，我们发现最简单的离线学习算法，行为克隆，在专家数据集上表现出色，甚至超过了最先进的离线强化学习算法。然而，当应用于混合数据集时，行为克隆的性能下降，离线强化学习算法的表现也不理想。通过对混合数据集进行分析，我们发现其中包含大量未标记的专家数据。为了解决这个问题，我们提出了一种半监督学习的分类器，用于识别混合数据中的专家行为。 |
| [^138] | [Improving Behavioural Cloning with Positive Unlabeled Learning.](http://arxiv.org/abs/2301.11734) | 本文提出了一种通过迭代学习算法来识别机器人数据集中专家轨迹的新方法，并将该方法应用于行为克隆，表现出比竞争方法更高的准确性和最先进的性能。 |
| [^139] | [A Survey on Transformers in Reinforcement Learning.](http://arxiv.org/abs/2301.03044) | 这篇论文是一项调查研究，总结了在强化学习领域使用Transformers的动机、进展和未来前景。 |
| [^140] | [Decision-making and control with diffractive optical networks.](http://arxiv.org/abs/2212.11278) | 本文提出使用深度强化学习来实现具备决策和控制能力的衍射光学网络，以模拟人类的决策和控制能力。这种网络利用残差架构，并通过与环境的交互来找到最优的控制策略，具备高速和低功耗的特点。 |
| [^141] | [DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads.](http://arxiv.org/abs/2212.03414) | 该论文提出了一种名为DREAM的动态调度器，针对实时多模型机器学习工作负载中的各种动态行为设计。DREAM通过量化得分来驱动调度决策，考虑当前系统负载和其他推理任务。这项工作对于更好地利用底层硬件，提高系统效率具有重要意义。 |
| [^142] | [Grassmann Manifold Flows for Stable Shape Generation.](http://arxiv.org/abs/2211.02900) | 本文提出了一种利用Grassmann流形学习分布的方法，以生成稳定的形状。实验结果证明了该方法在生成高质量样本方面的优势。 |
| [^143] | [Multi-agent Deep Covering Skill Discovery.](http://arxiv.org/abs/2210.03269) | 这篇论文介绍了一种针对多智能体强化学习中合作选项发现的方法，通过最小化多个智能体联合状态空间的预期覆盖时间来构建多智能体选项，并提出了采用这些选项的新框架。 |
| [^144] | [Nonparametric and Regularized Dynamical Wasserstein Barycenters for Sequential Observations.](http://arxiv.org/abs/2210.01918) | 本研究考虑了逐步观测的概率模型，扩展了动态Wasserstein媒介中心模型，用于捕捉序列观测中的瞬态行为，并放宽了纯态的参数化。 |
| [^145] | [Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States.](http://arxiv.org/abs/2210.00997) | 本文提出了一种在线自协调且相对平滑的最小化算法，通过分析在线镜像下降算法在凸函数上的遗憾，改进了在线投资组合选择算法的性能，并在在线学习量子态问题中达到了与Soft-Bayes算法相当的效果。 |
| [^146] | [Analysis and Comparison of Classification Metrics.](http://arxiv.org/abs/2209.05355) | 本文回顾并比较了常用于度量分类系统表现的各种指标，发现期望成本指标具有更广泛的适用性和直观性，并可用于解决从连续得分生成分类决策的实践问题。 |
| [^147] | [Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions.](http://arxiv.org/abs/2208.03392) | 本文调查了联邦学习在医疗应用中的分类、当前趋势、挑战和未来研究方向。该调查强调了联邦学习在保护隐私和解决安全问题方面的重要性。 |
| [^148] | [Optimal Propagation for Graph Neural Networks.](http://arxiv.org/abs/2205.02998) | 本文提出了一种双层优化方法，通过学习个性化PageRank传播矩阵和下游半监督节点分类，来学习最优的图结构。该方法在实证评估中展现了优越的功效和鲁棒性。 |
| [^149] | [Class-wise Classifier Design Capable of Continual Learning using Adaptive Resonance Theory-based Topological Clustering.](http://arxiv.org/abs/2203.09879) | 本文提出了一种基于自适应共振理论的增长自组织聚类算法的监督分类算法，能够实现持续学习，并且在分类性能方面表现优于最先进的聚类算法。 |
| [^150] | [State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards.](http://arxiv.org/abs/2102.11941) | 本文提出了一种增强限制性强化学习的方法，通过增加状态的拉格朗日乘子并重新解释原始-对偶方法，可以解决传统方法无法得到最优策略的问题。 |

# 详细

[^1]: ForceSight: 使用文本引导的视觉力导向移动操作

    ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])

    [http://arxiv.org/abs/2309.12312](http://arxiv.org/abs/2309.12312)

    ForceSight是一个使用文本引导的移动操作系统，通过深度神经网络预测视觉力导向目标。在实验中，该系统展示了在未见环境中进行精确抓取、抽屉打开和物体交接等任务的能力，并取得了较高的成功率。

    

    我们提出了一个名为ForceSight的系统，它使用深度神经网络通过文本引导来预测视觉力导向的目标。给定一张RGBD图片和一个文本提示，ForceSight可以确定相机坐标系下的目标末端执行器位姿（运动目标）和相关的力量（力量目标）。这两个组成部分共同形成了一个视觉力导向目标。之前的研究已经表明，输出人可解释的运动目标的深度模型可以实现真实机器人的巧妙操作。力量在操作中至关重要，但在这些系统中通常被限制在较低层次的执行中。当应用于带有手臂和眼睛的移动操作装置的ForceSight时，在与训练数据差异显著的未见环境中，能够以81%的成功率完成诸如精确抓取、抽屉打开和物体交接等任务。在另一项独立实验中，ForceSight仅使用视觉伺服，不考虑力量信息，但依然显示出较高的操作成功率。

    We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force 
    
[^2]: LLM-Grounder: 利用大型语言模型作为代理的开放词汇3D视觉定位

    LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])

    [http://arxiv.org/abs/2309.12311](http://arxiv.org/abs/2309.12311)

    LLM-Grounder是一种零样本、开放词汇的基于大型语言模型的3D视觉定位流程，通过利用语言模型分解查询并使用视觉定位工具识别物体，实现了在没有标记训练数据的情况下对新场景和文本查询的有效定位。在ScanRefer基准上取得了最先进的零样本定位准确性。

    

    3D视觉定位是家用机器人的重要能力，可以使其在环境中导航、操作物体并根据环境回答问题。现有的方法通常依赖于大量标记的数据，或者在处理复杂语言查询时存在一定限制。我们提出了LLM-Grounder，一种新颖的零样本、开放词汇的基于大型语言模型的3D视觉定位流程。LLM-Grounder利用一个LLM将复杂的自然语言查询分解为语义成分，并使用诸如OpenScene或LERF之类的视觉定位工具来识别3D场景中的物体。然后，LLM评估所提出的物体之间的空间和常识关系，以做出最终的定位决策。我们的方法不需要任何标记的训练数据，并且可以推广到新的3D场景和任意文本查询。我们在ScanRefer基准上评估了LLM-Grounder，并展示了最先进的零样本定位准确性。我们的研究结果表明LLMs在3D视觉定位中的有效性。

    3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs si
    
[^3]: LongLoRA: 高效的长上下文大型语言模型的精细调整

    LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])

    [http://arxiv.org/abs/2309.12307](http://arxiv.org/abs/2309.12307)

    LongLoRA是一种高效的精细调整方法，可以在有限的计算成本下扩展预训练的大型语言模型的上下文大小。它通过稀疏的局部注意力实现精细调整，并使用移动短注意力有效实现上下文扩展，与传统方法具有相似的性能。

    

    我们提出了一种高效的精细调整方法——LongLoRA，可以在有限的计算成本下扩展预训练的大型语言模型(LLM)的上下文大小。通常，使用长上下文大小训练LLM的计算成本很高，需要大量的训练时间和GPU资源。本文中，我们在两个方面加快了LLM的上下文扩展。一方面，尽管推理过程中需要稠密的全局注意力，但模型的精细调整可以通过稀疏的局部注意力有效且高效地完成。所提出的移动短注意力有效地实现了上下文的扩展，在与使用传统注意力进行精细调整时具有相似的性能，同时可以在训练中只用两行代码实现，在推理中是可选的。另一方面，我们重新审视了参数效率问题。

    We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
    
[^4]: 环境偏向特征排序用于鲁棒性的新颖性检测

    Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])

    [http://arxiv.org/abs/2309.12301](http://arxiv.org/abs/2309.12301)

    本文提出了一种环境偏向特征排序的方法，用于鲁棒性的新颖性检测。通过计算特征的环境之间分布方差进行评分，并通过去除高分特征来改善性能。这种方法在真实和合成基准数据上均能提高性能。

    

    我们解决了鲁棒性新颖性检测的问题，在该问题中，我们旨在检测语义内容方面的新颖性，同时对其他无关因素的变化具有不变性。具体来说，我们在具有多个环境的设置中操作，确定与环境更相关而不是任务相关内容的特征集合。因此，我们提出了一种方法，该方法从预训练的嵌入和多环境设置开始，成功根据其环境关注度对特征进行排序。首先，我们基于环境之间的特征分布方差计算每个特征的得分。接下来，我们证明通过舍弃得分较高的特征，我们可以去除虚假的相关性，并在正态协方差和子种群转移的情况下提高整体性能，无论是对于真实的还是对于我们为此任务引入的合成基准数据。

    We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
    
[^5]: See to Touch: 通过视觉激励学习触觉灵巧性

    See to Touch: Learning Tactile Dexterity through Visual Incentives. (arXiv:2309.12300v1 [cs.RO])

    [http://arxiv.org/abs/2309.12300](http://arxiv.org/abs/2309.12300)

    本文提出了一种通过视觉激励优化触觉策略的框架，以增强多指机器人的触觉灵巧性，并在多个挑战性任务中取得了良好的效果。

    

    为了实现人类擅长的精确、富有接触、灵巧的操作，为多指机器人配备触觉传感至关重要。然而，仅依靠触觉传感无法提供足够的线索来推理物体的空间配置，从而限制了纠正错误和适应变化情况的能力。本文提出了一种新的框架——通过视觉激励实现触觉适应（TAVI），通过使用基于视觉奖励的优化触觉策略来增强基于触觉的灵巧性。首先，我们使用对比性目标来学习视觉表示。接下来，我们利用这些视觉表示通过基于最优传输匹配的方式构建奖励函数，其中参考一个人类示范。最后，我们利用在线强化学习来优化机器人上基于触觉的策略，以最大化视觉奖励。在六个具有挑战性的任务中，如插销拿取、卸下碗和翻转细长物体等，TAVI取得了不错的成绩。

    Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI ach
    
[^6]: 学习驾驶到任何地方

    Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])

    [http://arxiv.org/abs/2309.12295](http://arxiv.org/abs/2309.12295)

    本文提出了一种能够学习适应不同地理位置和驾驶行为的模型，该模型通过引入基于地理位置的通道注意机制，在数据驱动的方式下高效地学习并灵活地建模不同地区之间的相似性和差异性。

    

    人类驾驶员可以无缝地适应不同地理位置的驾驶决策，包括不同的道路条件和交通规则，例如左驾驶和右驾驶。然而，现有的自动驾驶模型只能在限定的操作领域内部署，不能考虑不同地理位置之间的驾驶行为差异和模型的可扩展性。本文提出了AnyD，一种单一的具有地理感知的条件性模仿学习（CIL）模型，能够高效地从具有动态环境、交通和社会特征的异构和全球分布的数据中进行学习。我们的关键见解是引入一个高容量的基于地理位置的通道注意机制，可以在数据驱动的方式下有效地适应本地细微差异并灵活地建模不同地区之间的相似性。通过优化对比性模仿目标，我们提出的方法可以高效地适应固有的不平衡数据分布和地理位置差异。

    Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and loca
    
[^7]: 翻转诅咒: 在大型语言模型中训练的"A是B"无法学习"B是A"

    The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])

    [http://arxiv.org/abs/2309.12288](http://arxiv.org/abs/2309.12288)

    LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。

    

    我们揭示了自回归大型语言模型（LLM）在泛化上的令人惊讶的失败。如果一个模型是基于"A是B"形式的句子进行训练，它不会自动推广到相反的方向"B是A"。这就是翻转诅咒。例如，如果一个模型是基于"Olaf Scholz是德国第九任总理"进行训练的，它不会自动能够回答问题"谁是德国第九任总理？"。此外，正确答案（"Olaf Scholz"）的可能性不会比随机名字更高。因此，模型在逻辑推断上存在基本失败，并且不会推广到它们训练集中的普遍模式（即如果出现"A是B"，则"B是A"更可能出现）。我们通过在虚构的陈述（如"Uriah Hawthorne是'Abyssal Melodies'的作曲家"）上对GPT-3和Llama-1进行微调，并展示它们无法正确回答"谁创作了'Abyssal Melodies'?"来提供翻转诅咒的证据。

    We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
    
[^8]: 基于扩散的多乐器音乐合成的性能调节

    Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis. (arXiv:2309.12283v1 [cs.SD])

    [http://arxiv.org/abs/2309.12283](http://arxiv.org/abs/2309.12283)

    本研究提出了一种基于扩散的多乐器音乐合成性能调节方法，通过对特定的演奏和录音环境进行条件处理，实现了更好的音色和风格引导控制。

    

    从符号音乐表示生成多乐器音乐是音乐信息检索（MIR）中的一个重要任务。在这个背景下，一个集中但仍然很大程度上未解决的问题是在生成过程中以音乐和声学为基础的控制。作为本工作的主要贡献，我们提出通过对特定的演奏和录音环境进行条件处理，以增强多乐器合成的控制，从而更好地引导音色和风格。在现有先进的基于扩散的音乐生成模型基础上，我们引入了性能调节-一种简单的工具，指示生成模型用特定演奏中特定乐器的风格和音色合成音乐。我们的原型使用具有多样化仪器的非策划表演进行评估，并在保留了新颖音色和风格控制的同时实现了最先进的FAD逼真度得分。我们的项目页面包括样本和演示。

    Generating multi-instrument music from symbolic music representations is an important task in Music Information Retrieval (MIR). A central but still largely unsolved problem in this context is musically and acoustically informed control in the generation process. As the main contribution of this work, we propose enhancing control of multi-instrument synthesis by conditioning a generative model on a specific performance and recording environment, thus allowing for better guidance of timbre and style. Building on state-of-the-art diffusion-based music generative models, we introduce performance conditioning - a simple tool indicating the generative model to synthesize music with style and timbre of specific instruments taken from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. Our project page, including samples and demonstrations, is
    
[^9]: 特征模仿的广泛影响：金融、语音和生理领域中的神经增强

    The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains. (arXiv:2309.12279v1 [cs.LG])

    [http://arxiv.org/abs/2309.12279](http://arxiv.org/abs/2309.12279)

    本研究探讨特征模仿网络（FIN）在金融、语音和生理领域中的应用，发现FIN在比特币价格预测、语音情感识别和慢性颈痛检测方面能够显著改善性能，为深度学习架构提供了有希望的基础。

    

    神经网络权重的初始化在确定它们的性能方面起到关键作用。特征模仿网络（FIN）通过将权重初始化为近似特定的闭合统计特征，提供了一种新的策略，为深度学习架构奠定了有希望的基础。虽然FIN的适用性主要在生物医学领域进行了测试，但本研究将其扩展到了其他时间序列数据集。本研究进行了三个不同的实验，以测试模仿Tsallis熵以提高性能的适用性：比特币价格预测，语音情感识别和慢性颈痛检测。在比特币价格预测中，嵌入有FIN的模型将均方根误差减少了约1000与基准相比。在语音情感识别任务中，FIN增强模型的分类准确率提高了3％以上。最后，在CNP检测实验中，改进约为7％。

    Initialization of neural network weights plays a pivotal role in determining their performance. Feature Imitating Networks (FINs) offer a novel strategy by initializing weights to approximate specific closed-form statistical features, setting a promising foundation for deep learning architectures. While the applicability of FINs has been chiefly tested in biomedical domains, this study extends its exploration into other time series datasets. Three different experiments are conducted in this study to test the applicability of imitating Tsallis entropy for performance enhancement: Bitcoin price prediction, speech emotion recognition, and chronic neck pain detection. For the Bitcoin price prediction, models embedded with FINs reduced the root mean square error by around 1000 compared to the baseline. In the speech emotion recognition task, the FIN-augmented model increased classification accuracy by over 3 percent. Lastly, in the CNP detection experiment, an improvement of about 7 percent
    
[^10]: 通过自适应的NLP模型选择和基于临床专家规则的分类器改进VTE的识别

    Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])

    [http://arxiv.org/abs/2309.12273](http://arxiv.org/abs/2309.12273)

    通过自适应的NLP模型选择和临床专家规则的分类器，该研究提出一种改进VTE识别的新方法，在放射学报告中准确识别VTE事件的准确性得到提高。

    

    快速准确地识别静脉血栓栓塞（VTE），包括深静脉血栓（DVT）和肺栓塞（PE），对于有效治疗非常重要。利用自然语言处理（NLP）在放射学报告中，自动化方法已经在从回顾性数据集中识别VTE事件或帮助临床专家识别放射学报告中的VTE事件方面展示了有希望的进展。然而，由于标记有限的医学文本数据、放射学报告的复杂性和异质性以及数据不平衡，有效训练深度学习（DL）和NLP模型存在挑战。本研究提出了DL方法的新的组合方法，结合数据增强、自适应预训练的NLP模型选择和临床专家NLP基于规则的分类器，以提高非结构化（自由文本）放射学报告中VTE识别的准确性。我们的实验结果证明了该模型的有效性。

    Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
    
[^11]: 使基于四分位数的估计平均梯度聚合成为联邦图像分类的基准线的技术

    Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications. (arXiv:2309.12267v1 [cs.CR])

    [http://arxiv.org/abs/2309.12267](http://arxiv.org/abs/2309.12267)

    本文提出了一种名为估计平均聚合（EMA）的创新解决方案，旨在解决联邦学习中数据多样性和系统安全的挑战。EMA通过修剪均值处理恶意异常值，并揭示数据异质性，以确保训练模型适应不同的客户数据集。通过丰富的实验验证，EMA相对于其他方法表现出高准确性和曲线下面积（AUC），成为先进聚合技术的基准线。

    

    联邦学习（FL）通过实现分散协作、保护敏感数据并提高模型性能，彻底改变了我们训练深度神经网络的方式。然而，FL面临两个关键挑战：个体客户的数据多样性以及FL系统易受安全漏洞影响。本文引入了一种名为估计平均聚合（EMA）的创新解决方案，不仅解决了这些挑战，而且作为FL系统中先进聚合技术的基准线。EMA的重要性在于其双重作用：通过修剪均值有效处理恶意异常值，揭示数据异质性以确保训练模型适应各种客户数据集。通过大量实验，EMA始终相对于其他方法表现出高准确性和曲线下面积（AUC），确立自身地位。

    Federated Learning (FL) has revolutionized how we train deep neural networks by enabling decentralized collaboration while safeguarding sensitive data and improving model performance. However, FL faces two crucial challenges: the diverse nature of data held by individual clients and the vulnerability of the FL system to security breaches. This paper introduces an innovative solution named Estimated Mean Aggregation (EMA) that not only addresses these challenges but also provides a fundamental reference point as a $\mathsf{baseline}$ for advanced aggregation techniques in FL systems. EMA's significance lies in its dual role: enhancing model security by effectively handling malicious outliers through trimmed means and uncovering data heterogeneity to ensure that trained models are adaptable across various client datasets. Through a wealth of experiments, EMA consistently demonstrates high accuracy and area under the curve (AUC) compared to alternative methods, establishing itself as a ro
    
[^12]: 柔性合并：一种灵活且鲁棒的软模型合并方法，用于增强神经网络性能

    Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance. (arXiv:2309.12259v1 [cs.LG])

    [http://arxiv.org/abs/2309.12259](http://arxiv.org/abs/2309.12259)

    本论文提出了一种灵活且鲁棒的软模型合并方法，用于增强神经网络性能。该方法通过学习门参数，利用硬具体分布乘以$l_0$范数的代理，实现了多个模型的快速合并，并增强了对具有极端值的恶意模型的鲁棒性。这种合并过程不仅提高了模型性能，还降低了计算成本。

    

    随机梯度下降（SGD）是深度学习中广泛使用的优化算法，由于问题的非凸性，它通常只能收敛到局部最优解，利用这些局部最优解改进模型性能仍然是一个具有挑战性的任务。鉴于神经网络的固有复杂性，简单的算术平均会导致不理想的结果。本文提出了一种“软合并”方法，通过学习门参数，利用硬具体分布乘以$l_0$范数的代理，实现了多个模型的快速合并，简化了神经网络特定部分的合并，并增强了对具有极端值的恶意模型的鲁棒性。这个合并过程不仅通过收敛到更好的局部最优解来提高模型性能，还最小化了计算成本，提供了高效且明确的方法。

    Stochastic Gradient Descent (SGD), a widely used optimization algorithm in deep learning, is often limited to converging to local optima due to the non-convex nature of the problem. Leveraging these local optima to improve model performance remains a challenging task. Given the inherent complexity of neural networks, the simple arithmetic averaging of the obtained local optima models in undesirable results. This paper proposes a {\em soft merging} method that facilitates rapid merging of multiple models, simplifies the merging of specific parts of neural networks, and enhances robustness against malicious models with extreme values. This is achieved by learning gate parameters through a surrogate of the $l_0$ norm using hard concrete distribution without modifying the model weights of the given local optima models. This merging process not only enhances the model performance by converging to a better local optimum, but also minimizes computational costs, offering an efficient and expli
    
[^13]: SALSA-CLRS:一种稀疏且可扩展的算法推理基准

    SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])

    [http://arxiv.org/abs/2309.12253](http://arxiv.org/abs/2309.12253)

    SALSA-CLRS是一种稀疏且可扩展的算法推理基准，优先考虑可扩展性和稀疏表示的利用。它通过引入适应于分布式算法和图神经网络的消息传递范式，解决了现有CLRS基准中内存需求高和运行时间难以扩展的问题。

    

    我们介绍了CLRS算法学习基准的扩展，优先考虑可扩展性和稀疏表示的利用。CLRS中的许多算法需要全局存储器或信息交换，在其执行模型中镜像表达为基于底层问题构建完全连接（而非稀疏）图的操作。尽管CLRS的目标是评估学习算法在更大实例上的泛化能力，但现有的执行模型由于其要求高的内存需求和运行时间而成为一个重要限制（难以扩展）。然而，许多重要的算法并不需要完全连接的图；这些主要分布式算法与图神经网络采用的消息传递范式密切相关。因此，我们提出了SALSA-CLRS，一个专门考虑可扩展性和稀疏性的CLRS基准的扩展。我们的方法包括从原始CLRS基准中改编的算法，并引入了新的问题。

    We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new probl
    
[^14]: 在序列长度上并行化非线性顺序模型

    Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])

    [http://arxiv.org/abs/2309.12252](http://arxiv.org/abs/2309.12252)

    本论文提出了一种并行算法，能够加速GPU对于顺序模型的评估速度，提高了3个数量级，而不降低输出准确性。该算法适用于各种架构，并在长时间序列分类问题中发现了门控循环单元的有效性。

    

    顺序模型，例如循环神经网络和神经常微分方程，在训练过程中一直由于其本质上的顺序特性而存在训练缓慢的问题。多年来这个瓶颈一直存在，因为很多人认为顺序模型无法并行化。我们通过并行算法挑战了这个长期以来的信念，加速了GPU对于顺序模型的评估速度，速度提高了3个数量级，而不牺牲输出准确性。该算法不需要顺序模型架构中的任何特殊结构，适用于各种架构。使用我们的方法，训练顺序模型可以比常规的顺序方法快10倍以上，而训练结果没有明显差异。借助这种加速训练，我们在一个包含17k个时间样本的长时间序列分类问题中发现了门控循环单元的有效性。通过克服训练瓶颈，我们的工作使得顺序模型的训练更加高效。

    Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work 
    
[^15]: SQUARE: 使用多个正负参考答案自动评估问答系统

    SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])

    [http://arxiv.org/abs/2309.12250](http://arxiv.org/abs/2309.12250)

    提出了一种新的问答系统评估指标SQuArE，使用多个参考答案进行句子级问答评估，实现了高度相关性的评估结果。

    

    问答系统的评估是非常具有挑战性和昂贵的，最可靠的方法是通过人工标注问题的答案的正确性。最近的研究表明，基于Transformer LM编码器的相似性度量在问答评估中具有良好的迁移性，但它们的使用受限于单个正确参考答案。我们提出了一种新的评估指标：SQuArE（句子级问答评估），使用多个参考答案（组合多个正确和错误的参考答案）进行句子形式的问答评估。我们在句子级提取式（答案选择）和生成式（GenQA）问答系统上评估了SQuArE，在多个学术和工业数据集上，结果表明它优于先前的基准，并与人工标注具有最高的相关性。

    Evaluation of QA systems is very challenging and expensive, with the most reliable approach being human annotations of correctness of answers for questions. Recent works (AVA, BEM) have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation, but they are limited by the usage of a single correct reference answer. We propose a new evaluation metric: SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference answers (combining multiple correct and incorrect references) for sentence-form QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and generative (GenQA) QA systems, across multiple academic and industrial datasets, and show that it outperforms previous baselines and obtains the highest correlation with human annotations.
    
[^16]: 适应性输入图像归一化方法解决基于GAN的X射线图像的模式塌陷问题

    Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])

    [http://arxiv.org/abs/2309.12245](http://arxiv.org/abs/2309.12245)

    本论文研究了生成对抗网络中的模式塌陷问题对合成X射线图像多样性的影响。通过实验证明了将自适应输入图像归一化方法与深度模型结合的优势。

    

    由于疾病的罕见性，生物医学图像数据集可能存在不平衡。生成对抗网络通过生成合成图像来扩充数据集，起到了解决这种不平衡的关键作用。生成合成图像需要包含多样化的特征，以准确表示训练图像中存在的特征分布。此外，合成图像中缺乏多样性的特征会降低机器学习分类器的性能。模式塌陷问题影响生成对抗网络生成多样化图像的能力，并分为类内和类间两种类型。本文研究了这两种模式塌陷问题，并评估了它们对合成X射线图像多样性的影响。本研究将自适应输入图像归一化方法与深度模型相结合，通过实验证明了其在解决模式塌陷问题上的优势。

    Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep
    
[^17]: 弱监督自动音频字幕生成的纯文本训练方法

    Weakly-supervised Automated Audio Captioning via text only training. (arXiv:2309.12242v1 [cs.SD])

    [http://arxiv.org/abs/2309.12242](http://arxiv.org/abs/2309.12242)

    通过仅使用文本数据和预训练的语言-音频对比模型（CLAP）来弱监督训练自动音频字幕生成模型，从而减少对配对音频和字幕数据的需求，并通过在训练和推断阶段采用策略来弥合音频和文本之间的差距。

    

    近年来，配对的音频和字幕数据集在自动生成音频片段描述方面取得了显著的成功，即自动音频字幕生成（AAC）。然而，收集足够数量的配对音频和字幕数据是一项费时费力的工作。受到对比性语言-音频预训练（CLAP）的最新进展的启发，我们提出了一种弱监督的方法，只使用文本数据和预训练的CLAP模型来训练AAC模型，减少了对配对目标数据的需求。我们的方法利用CLAP中音频和文本嵌入之间的相似性。在训练过程中，我们学习从CLAP文本嵌入中重建文本，而在推断过程中，我们使用音频嵌入进行解码。为了减小音频和文本嵌入之间的模态差距，我们在训练和推断阶段采用了策略来弥合差距。我们在Clotho和AudioCaps数据集上评估了我们的方法，证明了其能够...

    In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to 
    
[^18]: t-EER: 无参数的联合评估对策和生物特征比较器

    t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators. (arXiv:2309.12237v1 [cs.CR])

    [http://arxiv.org/abs/2309.12237](http://arxiv.org/abs/2309.12237)

    该论文提出了t-EER这一新的无参数度量指标，用于联合评估在生物特征验证中与攻击检测同时操作的对策解决方案。这种联合评估方法相较于传统的分开评估方法具有更好的性能和实用性。

    

    提供攻击（伪冒）检测（PAD）通常与生物特征验证同时操作以提高在面对伪冒攻击时的可靠性。尽管这两个子系统同时作用于可靠的生物特征验证的单一任务，但它们解决不同的检测任务，因此通常被分开评估。证据表明，这种方法是次优的。我们引入了一种新的指标，用于在与生物特征验证同时操作的情况下联合评估PAD解决方案。与最近提出的串联检测成本函数相反，新的串联等误差率（t-EER）是无参数的。然而，两个分类器的组合导致了一组工作点，在这些工作点上，误报和漏报率是相等的，也取决于攻击的普遍程度。因此，我们引入了“并发”的t-EER，这是一个与攻击的普遍程度无关的独特工作点。利用模态（甚至应用）的一致性，我们显示了t-EER的实用性和性能优势。

    Presentation attack (spoofing) detection (PAD) typically operates alongside biometric verification to improve reliablity in the face of spoofing attacks. Even though the two sub-systems operate in tandem to solve the single task of reliable biometric verification, they address different detection tasks and are hence typically evaluated separately. Evidence shows that this approach is suboptimal. We introduce a new metric for the joint evaluation of PAD solutions operating in situ with biometric verification. In contrast to the tandem detection cost function proposed recently, the new tandem equal error rate (t-EER) is parameter free. The combination of two classifiers nonetheless leads to a \emph{set} of operating points at which false alarm and miss rates are equal and also dependent upon the prevalence of attacks. We therefore introduce the \emph{concurrent} t-EER, a unique operating point which is invariable to the prevalence of attacks. Using both modality (and even application) ag
    
[^19]: 平滑的ECE: 通过核平滑实现的可靠性图。

    Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing. (arXiv:2309.12236v1 [cs.LG])

    [http://arxiv.org/abs/2309.12236](http://arxiv.org/abs/2309.12236)

    本文介绍了一种通过核平滑技术修正传统校准度量和可靠性图的方法，提出了一种新的平滑校准度量 SmoothECE，同时可视化该度量的可靠性图，并证明了其良好性质。

    

    校准度量和可靠性图是衡量和解释概率预测器校准性的两个基本工具。校准度量用于量化校准偏差的程度，可靠性图可视化该校准偏差的结构。然而，常见的校准度量和可靠性图构建方法-分箱和ECE -- 都存在已知的缺陷（如不连续性）。我们展示了一个简单的改进可以修复这两种构建方法：首先使用RBF核平滑观测结果，然后计算平滑函数的期望校准误差（ECE）。我们证明在小心选择带宽的情况下，该方法产生的校准度量是良好行为的（由B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a定义的一致校准度量）。我们将这个度量称为SmoothECE。此外，从这个平滑函数得到的可靠性图可以直观地表示SmoothECE，就像分箱方法一样。

    Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures -binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned
    
[^20]: 光滑纳什均衡：算法和复杂性

    Smooth Nash Equilibria: Algorithms and Complexity. (arXiv:2309.12226v1 [cs.GT])

    [http://arxiv.org/abs/2309.12226](http://arxiv.org/abs/2309.12226)

    光滑纳什均衡是纳什均衡的一个松弛变种，可以通过实现与最佳光滑策略的偏离相同的效用来达到。我们定义了强和弱光滑纳什均衡的概念，并证明了它们在计算性质上优于传统的纳什均衡。

    

    纳什均衡的一个基本缺点是其计算复杂性：在正则形式的博弈中，近似纳什均衡是PPAD难的。在本文中，受到平滑分析思想的启发，我们引入了一个被称为$\sigma$-光滑纳什均衡的松弛变种，其中$\sigma$是光滑性参数。在$\sigma$-光滑纳什均衡中，玩家们只需要实现至少与他们最佳$\sigma$-光滑策略的偏离相同的效用，而这个$\sigma$-光滑策略是不会对任何固定动作产生过多质量（根据$\sigma$参数化）。我们区分了两种$\sigma$-光滑纳什均衡的变种：强$\sigma$-光滑纳什均衡，在这种情况下，玩家们需要在均衡中采用$\sigma$-光滑策略进行游戏；弱$\sigma$-光滑纳什均衡中，没有这样的要求。我们证明了无论是弱$\sigma$-光滑纳什均衡还是强$\sigma$-光滑纳什均衡，都比纳什均衡具有更好的计算性质。

    A fundamental shortcoming of the concept of Nash equilibrium is its computational intractability: approximating Nash equilibria in normal-form games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis, we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash equilibrium, players only need to achieve utility at least as high as their best deviation to a $\sigma$-smooth strategy, which is a distribution that does not put too much mass (as parametrized by $\sigma$) on any fixed action. We distinguish two variants of $\sigma$-smooth Nash equilibria: strong $\sigma$-smooth Nash equilibria, in which players are required to play $\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth Nash equilibria, where there is no such requirement.  We show that both weak and strong $\sigma$-smooth Nash equilibria have superior computational properties to Nash equilibri
    
[^21]: 基于模型的深度学习用于高维周期结构

    Model-based Deep Learning for High-Dimensional Periodic Structures. (arXiv:2309.12223v1 [eess.SP])

    [http://arxiv.org/abs/2309.12223](http://arxiv.org/abs/2309.12223)

    本研究提出了一种基于深度学习的代理模型，用于快速模拟高维频率选择性表面。通过引入物理洞察力，该模型可以在训练后使用减少的数据集准确预测某种结构的S参数。该模型适用于各种频率选择性表面，无论是基于孔隙还是任意几何形状的贴片。

    

    本文提出了一种用于快速模拟高维频率选择性表面的深度学习代理模型。我们考虑由多个连接的屏幕堆叠组成的单元格，其设计需要对许多几何自由度进行控制。通过将物理洞察力引入模型，经过训练使用减少的数据集，它可以对特定结构的S参数进行精确预测。所提出的模型非常多功能，并且可以与任何类型的频率选择性表面一起使用，无论是基于孔隙还是任意几何形状的贴片。在这里，我们给出了频率选择性表面由具有矩形孔隙的屏幕组成的数值示例，显示出预测性能与使用全波模拟器获得的性能之间的极好一致性。

    This work presents a deep learning surrogate model for the fast simulation of high-dimensional frequency selective surfaces. We consider unit-cells which are built as multiple concatenated stacks of screens and their design requires the control over many geometrical degrees of freedom. Thanks to the introduction of physical insight into the model, it can produce accurate predictions of the S-parameters of a certain structure after training with a reduced dataset.The proposed model is highly versatile and it can be used with any kind of frequency selective surface, based on either perforations or patches of any arbitrary geometry. Numeric examples are presented here for the case of frequency selective surfaces composed of screens with rectangular perforations, showing an excellent agreement between the predicted performance and such obtained with a full-wave simulator.
    
[^22]: SR-PredictAO: 具有高能力预测器附加件的基于会话的推荐系统

    SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On. (arXiv:2309.12218v1 [cs.IR])

    [http://arxiv.org/abs/2309.12218](http://arxiv.org/abs/2309.12218)

    SR-PredictAO是一种基于会话的推荐系统，通过引入高能力预测器模块，解决了现有模型中低能力预测器模块的问题，可以在存在随机用户行为的情况下预测用户的下一个动作。

    

    基于会话的推荐系统旨在通过仅基于单个会话中的信息来预测用户的下一个项目点击，即使在存在某些随机用户行为的情况下，这是一个复杂的问题。这个复杂的问题需要一个高能力的预测用户下一个动作的模型。大多数（如果不是全部）现有模型遵循编码器-预测器范式，在这个范式中所有的研究都集中在如何广泛优化编码器模块，但它们忽视了如何优化预测器模块。在本文中，我们发现了现有模型中低能力预测器模块存在的关键问题。受此启发，我们提出了一种新颖的框架称为\emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO)。在这个框架中，我们提出了一个高能力的预测器模块，可以减轻随机用户行为对预测的影响。值得一提的是，

    Session-based recommendation, aiming at making the prediction of the user's next item click based on the information in a single session only even in the presence of some random user's behavior, is a complex problem. This complex problem requires a high-capability model of predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm where all studies focus on how to optimize the encoder module extensively in the paradigm but they ignore how to optimize the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module which could alleviate the effect of random user's behavior for prediction. It is worth mentioning that t
    
[^23]: 增强基于肌电信号的手势识别的表达能力的多标签分类方法

    A Multi-label Classification Approach to Increase Expressivity of EMG-based Gesture Recognition. (arXiv:2309.12217v1 [eess.SP])

    [http://arxiv.org/abs/2309.12217](http://arxiv.org/abs/2309.12217)

    本论文提出了一种多标签分类方法，通过问题转换的方式有效提高基于肌电信号的手势识别系统的表达能力。通过生成合成数据，可以快速校准并识别组合手势，提高了性能。

    

    目标：本研究旨在有效提高基于肌电信号的手势识别系统的表达能力。方法：我们采用问题转换的方法，将动作分为两个生物力学独立的组成部分——一组手腕方向和一组手指修饰器。为了保持快速校准时间，我们仅使用单个手势训练每个组件的模型，并通过生成合成数据来外推到组合手势的完整产品空间。我们收集了一个有高置信度标签的监督式数据集，其中受试者在持有游戏手柄的同时进行组合手势，并进行实验分析所提出方法的模型架构、分类器算法和合成数据生成策略对性能的影响。主要结果：我们发现，在并行模型架构和非线性分类器的问题转换方法中，使用合成数据生成策略可以显著提高性能。

    Objective: The objective of the study is to efficiently increase the expressivity of surface electromyography-based (sEMG) gesture recognition systems. Approach: We use a problem transformation approach, in which actions were subset into two biomechanically independent components - a set of wrist directions and a set of finger modifiers. To maintain fast calibration time, we train models for each component using only individual gestures, and extrapolate to the full product space of combination gestures by generating synthetic data. We collected a supervised dataset with high-confidence ground truth labels in which subjects performed combination gestures while holding a joystick, and conducted experiments to analyze the impact of model architectures, classifier algorithms, and synthetic data generation strategies on the performance of the proposed approach. Main Results: We found that a problem transformation approach using a parallel model architecture in combination with a non-linear 
    
[^24]: 区域可加模型: 最小化特征交互的设计可解释模型

    Regionally Additive Models: Explainable-by-design models minimizing feature interactions. (arXiv:2309.12215v1 [cs.LG])

    [http://arxiv.org/abs/2309.12215](http://arxiv.org/abs/2309.12215)

    区域可加模型 (RAMs) 是一种设计可解释模型，通过在特征空间内识别子区域来最小化特征的交互。相比于广义可加模型(GAMs)，RAMs能拥有更丰富的模型表达能力，同时保持可解释性。

    

    广义可加模型 (GAMs) 是在各种应用中广泛使用的设计可解释模型。 GAMs假设输出可以表示为一组单变量函数的和，称为组件。然而，在输出依赖于多个特征同时的机器学习问题中，这种假设不成立。在这些情况下，GAMs无法捕捉到底层函数的交互项，导致准确性不佳。为了(部分)解决这个问题，我们提出了区域可加模型 (RAMs)，一种新颖的设计可解释模型。RAMs识别特征空间内的子区域，在这些子区域中最小化了特征的交互。在这些区域内，把输出表示为一组单变量函数 (组件) 相对于把输出表示为一个特征的单变量函数更加准确。因此，RAMs相比于GAMs拥有更丰富的模型表达能力，同时保持可解释性。RAM框架由三个步骤组成。

    Generalized Additive Models (GAMs) are widely used explainable-by-design models in various applications. GAMs assume that the output can be represented as a sum of univariate functions, referred to as components. However, this assumption fails in ML problems where the output depends on multiple features simultaneously. In these cases, GAMs fail to capture the interaction terms of the underlying function, leading to subpar accuracy. To (partially) address this issue, we propose Regionally Additive Models (RAMs), a novel class of explainable-by-design models. RAMs identify subregions within the feature space where interactions are minimized. Within these regions, it is more accurate to express the output as a sum of univariate functions (components). Consequently, RAMs fit one component per subregion of each feature instead of one component per feature. This approach yields a more expressive model compared to GAMs while retaining interpretability. The RAM framework consists of three step
    
[^25]: SupeRBNN: 使用绝热超导Josephson器件的随机二值神经网络

    SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices. (arXiv:2309.12212v1 [cs.ET])

    [http://arxiv.org/abs/2309.12212](http://arxiv.org/abs/2309.12212)

    提出了一种基于绝热超导Josephson器件的随机二值神经网络加速框架SupeRBNN，通过软硬件协同优化，解决了绝热量子流参数元在二值神经网络加速中的挑战。

    

    绝热量子流参数元(AQFP)是一种具有极高能量效率的超导逻辑。通过利用电流的不同极性来表示逻辑“0”和“1”，AQFP器件成为二值神经网络(BNN)计算的优秀载体。尽管最近的研究在开发基于AQFP的BNN加速器方面取得了初步进展，但仍存在一些关键挑战，阻碍了该设计成为综合解决方案。在本文中，我们提出了SupeRBNN，一种基于AQFP的随机二值神经网络加速框架，通过软硬件协同优化最终使AQFP器件成为BNN加速的可行解决方案。具体来说，我们研究了AQFP器件的随机行为，并分析了交叉开关尺寸对电流衰减的影响，随后将电流振幅转化为适合用于BNN计算的值。为了解决积累问题并提高整体硬件性能，我们提出了解决方法。

    Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with extremely high energy efficiency. By employing the distinct polarity of current to denote logic `0' and `1', AQFP devices serve as excellent carriers for binary neural network (BNN) computations. Although recent research has made initial strides toward developing an AQFP-based BNN accelerator, several critical challenges remain, preventing the design from being a comprehensive solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN acceleration framework that leverages software-hardware co-optimization to eventually make the AQFP devices a feasible solution for BNN acceleration. Specifically, we investigate the randomized behavior of the AQFP devices and analyze the impact of crossbar size on current attenuation, subsequently formulating the current amplitude into the values suitable for use in BNN computation. To tackle the accumulation problem and improve overall hardware performance, we propo
    
[^26]: 物理信息触发状态空间神经网络用于传输现象

    Physics-informed State-space Neural Networks for Transport Phenomena. (arXiv:2309.12211v1 [cs.LG])

    [http://arxiv.org/abs/2309.12211](http://arxiv.org/abs/2309.12211)

    这项研究引入了物理信息触发状态空间神经网络模型（PSMs），用于解决在化学、生物医学和电厂等传输现象中实现实时优化和容错性的问题。通过使用传感器数据训练深度神经网络，并使用物理模型进行传输系统建模，PSMs提供了比传统数据驱动模型更准确的方法，并具有多种应用场景。

    

    本研究介绍了物理信息触发状态空间神经网络模型（PSMs），这是一种在自主系统中实现实时优化、灵活性和容错性的新颖解决方案，特别适用于化学、生物医学和电厂等以传输为主导的系统。传统的数据驱动方法由于缺乏像质量守恒这样的物理约束而有所不足。PSMs通过使用传感器数据训练深度神经网络，并使用部分微分方程对物理信息进行建模，从而得到具有物理约束的可迭代前向动力学模型。通过两个仿真实验 - 加热通道和冷却系统回路，我们证明PSMs比纯数据驱动模型提供更准确的方法。除了准确性之外，PSMs还具有多种令人信服的用例。本文展示了其中的两个：通过顺序更新的状态空间表示创建非线性监控控制器

    This work introduces Physics-informed State-space neural network Models (PSMs), a novel solution to achieving real-time optimization, flexibility, and fault tolerance in autonomous systems, particularly in transport-dominated systems such as chemical, biomedical, and power plants. Traditional data-driven methods fall short due to a lack of physical constraints like mass conservation; PSMs address this issue by training deep neural networks with sensor data and physics-informing using components' Partial Differential Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable forward dynamics model. Through two in silico experiments - a heated channel and a cooling system loop - we demonstrate that PSMs offer a more accurate approach than purely data-driven models.  Beyond accuracy, there are several compelling use cases for PSMs. In this work, we showcase two: the creation of a nonlinear supervisory controller through a sequentially updated state-space representatio
    
[^27]: Boolformer: 用Transformer进行逻辑函数的符号回归

    Boolformer: Symbolic Regression of Logic Functions with Transformers. (arXiv:2309.12207v1 [cs.LG])

    [http://arxiv.org/abs/2309.12207](http://arxiv.org/abs/2309.12207)

    Boolformer是第一个经过训练的Transformer架构，用于执行端到端的布尔函数符号回归。它可以预测复杂函数的简洁公式，并在提供不完整和有噪声观测时找到近似表达式。Boolformer在真实二分类数据集上展现出潜力作为可解释性替代方案，并在基因调控网络动力学建模任务中与最先进的遗传算法相比表现出竞争力。

    

    在这项工作中，我们介绍了Boolformer，这是第一个经过训练的Transformer架构，用于执行端到端的布尔函数符号回归。首先，我们展示了当提供干净的真值表时，它可以预测复杂函数的简洁公式。然后，我们展示了它在提供不完整和有噪声观测时找到近似表达式的能力。我们在广泛的真实二分类数据集上评估了Boolformer，证明了它作为传统机器学习方法的可解释性替代品的潜力。最后，我们将其应用于建模基因调控网络动力学的常见任务。使用最近的基准测试，我们展示了Boolformer与最先进的遗传算法相比，速度提高了几个数量级。我们的代码和模型公开可用。

    In this work, we introduce Boolformer, the first Transformer architecture trained to perform end-to-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions which were not seen during training, when provided a clean truth table. Then, we demonstrate its ability to find approximate expressions when provided incomplete and noisy observations. We evaluate the Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modelling the dynamics of gene regulatory networks. Using a recent benchmark, we show that Boolformer is competitive with state-of-the art genetic algorithms with a speedup of several orders of magnitude. Our code and models are available publicly.
    
[^28]: PrNet：一个用于改善Android原始GNSS测量定位的神经网络，用于校正伪距偏差

    PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements. (arXiv:2309.12204v1 [cs.LG])

    [http://arxiv.org/abs/2309.12204](http://arxiv.org/abs/2309.12204)

    PrNet是一个用于改善Android原始GNSS测量定位的神经网络，通过校正伪距偏差来提高定位性能，实现了同时处理伪距偏差和噪声的混合管道。

    

    我们提出了一个神经网络来减轻伪距偏差，以提高使用从安卓智能手机收集的数据的定位性能。我们使用一个实用的基于卫星的多层感知器（MLP）表示伪距偏差，其输入是从Android原始全球导航卫星系统（GNSS）测量中得出的与卫星-接收器-环境相关的六个特征。为了监督训练过程，我们仔细计算伪距偏差的目标值，并利用地理真实位置和平滑技术优化了一个包含智能手机时钟偏差估计残差的损失函数。在推理过程中，我们使用基于模型的定位引擎校正神经网络的伪距计算位置。因此，这个混合管道可以处理伪距偏差和噪声。我们在一个开放数据集上评估了该框架，并考虑了四个应用场景来研究指纹定位和交叉跟踪定位。

    We present a neural network for mitigating pseudorange bias to improve localization performance with data collected from Android smartphones. We represent pseudorange bias using a pragmatic satellite-wise Multiple Layer Perceptron (MLP), the inputs of which are six satellite-receiver-context-related features derived from Android raw Global Navigation Satellite System (GNSS) measurements. To supervise the training process, we carefully calculate the target values of pseudorange bias using location ground truth and smoothing techniques and optimize a loss function containing the estimation residuals of smartphone clock bias. During the inference process, we employ model-based localization engines to compute locations with pseudoranges corrected by the neural network. Consequently, this hybrid pipeline can attend to both pseudorange bias and noise. We evaluate the framework on an open dataset and consider four application scenarios for investigating fingerprinting and cross-trace localiza
    
[^29]: 提升精准医学：基于脑电信号的人工智能驱动精神分裂症诊断：2002-2023年完整综述

    Empowering Precision Medicine: AI-Driven Schizophrenia Diagnosis via EEG Signals: A Comprehensive Review from 2002-2023. (arXiv:2309.12202v1 [eess.SP])

    [http://arxiv.org/abs/2309.12202](http://arxiv.org/abs/2309.12202)

    本研究综述了基于脑电信号的人工智能技术在精神分裂症诊断中的应用，以解决脑电信号分析中的挑战。

    

    精神分裂症（SZ）是一种常见的精神障碍，其特征为认知、情绪和行为改变。SZ的症状包括幻觉、错觉、妄想、缺乏动力和注意力难度。诊断SZ需要使用各种工具，包括临床访谈、体格检查、心理评估、《精神障碍诊断与统计手册》（DSM）和神经影像技术。脑电图（EEG）记录是一种重要的功能性神经影像模态，可以提供有关SZ期间脑功能的宝贵见解。然而，脑电信号分析对神经学家和科学家来说存在艺术品、长期记录和多通道使用的挑战。为了解决这些挑战，研究人员引入了人工智能（AI）技术，包括传统的机器学习（ML）和深度学习（DL）方法，以辅助SZ诊断。本研究对这些方法进行了综述。

    Schizophrenia (SZ) is a prevalent mental disorder characterized by cognitive, emotional, and behavioral changes. Symptoms of SZ include hallucinations, illusions, delusions, lack of motivation, and difficulties in concentration. Diagnosing SZ involves employing various tools, including clinical interviews, physical examinations, psychological evaluations, the Diagnostic and Statistical Manual of Mental Disorders (DSM), and neuroimaging techniques. Electroencephalography (EEG) recording is a significant functional neuroimaging modality that provides valuable insights into brain function during SZ. However, EEG signal analysis poses challenges for neurologists and scientists due to the presence of artifacts, long-term recordings, and the utilization of multiple channels. To address these challenges, researchers have introduced artificial intelligence (AI) techniques, encompassing conventional machine learning (ML) and deep learning (DL) methods, to aid in SZ diagnosis. This study reviews
    
[^30]: 使用具有离散余弦变换层的非对称稀疏自编码器对脑电图传感器数据进行压缩

    Electroencephalogram Sensor Data Compression Using An Asymmetrical Sparse Autoencoder With A Discrete Cosine Transform Layer. (arXiv:2309.12201v1 [eess.SP])

    [http://arxiv.org/abs/2309.12201](http://arxiv.org/abs/2309.12201)

    本文提出了一种使用非对称稀疏自编码器和离散余弦变换层对脑电图传感器数据进行压缩的方法。实验结果表明，这种方法能有效地减少EEG信号的冗余，并在保持稀疏性的同时提高数据的准确性。

    

    脑电图（EEG）数据压缩对于无线记录应用来说是必要的，以减少需要传输的数据量。本文提出了一种使用离散余弦变换（DCT）层的非对称稀疏自编码器来压缩EEG信号的方法。自编码器的编码器模块采用全连接线性层和DCT层的组合，使用硬阈值非线性降低冗余数据。此外，DCT层包括可训练的硬阈值参数和缩放层，可强调或减弱单个DCT系数。最后，一对一卷积层生成潜空间。在潜空间中，采用稀疏惩罚型成本函数使特征图尽可能稀疏。潜空间数据被传输到接收端。自编码器的解码器模块使用逆DCT和两个全连接线性层来提高数据的准确性。

    Electroencephalogram (EEG) data compression is necessary for wireless recording applications to reduce the amount of data that needs to be transmitted. In this paper, an asymmetrical sparse autoencoder with a discrete cosine transform (DCT) layer is proposed to compress EEG signals. The encoder module of the autoencoder has a combination of a fully connected linear layer and the DCT layer to reduce redundant data using hard-thresholding nonlinearity. Furthermore, the DCT layer includes trainable hard-thresholding parameters and scaling layers to give emphasis or de-emphasis on individual DCT coefficients. Finally, the one-by-one convolutional layer generates the latent space. The sparsity penalty-based cost function is employed to keep the feature map as sparse as possible in the latent space. The latent space data is transmitted to the receiver. The decoder module of the autoencoder is designed using the inverse DCT and two fully connected linear layers to improve the accuracy of data
    
[^31]: 一个基于变分自编码器的室内定位的多频段信道预测方案

    A Variational Auto-Encoder Enabled Multi-Band Channel Prediction Scheme for Indoor Localization. (arXiv:2309.12200v1 [eess.SP])

    [http://arxiv.org/abs/2309.12200](http://arxiv.org/abs/2309.12200)

    本文提出了一个基于变分自编码器的室内定位方案，通过预测信道状态信息值并将多频段信息拼接在一起，提高了室内指纹定位的准确性。

    

    室内定位在虚拟/增强现实和智能家居等前沿技术中的需求越来越大。传统的基于模型的定位在计算负担方面存在显著问题，所以指纹定位引起了越来越多的关注，它需要在建立指纹数据库后降低计算成本。然而，室内定位的准确性受到复杂室内环境带来的多径信号折射的限制。在本文中，我们提出了一种方案，通过从另一个发送通道预测信道状态信息（CSI）值，并将多频段信息拼接在一起，以提高室内指纹定位的准确性。我们在COST 2100模拟数据和从办公场景收集的实时正交频分复用（OFDM）WiFi数据上测试了我们的方案。

    Indoor localization is getting increasing demands for various cutting-edged technologies, like Virtual/Augmented reality and smart home. Traditional model-based localization suffers from significant computational overhead, so fingerprint localization is getting increasing attention, which needs lower computation cost after the fingerprint database is built. However, the accuracy of indoor localization is limited by the complicated indoor environment which brings the multipath signal refraction. In this paper, we provided a scheme to improve the accuracy of indoor fingerprint localization from the frequency domain by predicting the channel state information (CSI) values from another transmitting channel and spliced the multi-band information together to get more precise localization results. We tested our proposed scheme on COST 2100 simulation data and real time orthogonal frequency division multiplexing (OFDM) WiFi data collected from an office scenario.
    
[^32]: 使用深度学习方法进行脑肿瘤检测

    Brain Tumor Detection Using Deep Learning Approaches. (arXiv:2309.12193v1 [eess.IV])

    [http://arxiv.org/abs/2309.12193](http://arxiv.org/abs/2309.12193)

    本研究使用深度学习方法，特别是ResNet50，来改进脑肿瘤的检测和分类准确性，以探索自动化检测过程的可能性。

    

    脑肿瘤是一些异常细胞的聚集体，可以发展成肿块或团簇。由于它们有潜在的浸润其他组织的可能性，对患者构成风险。主要的成像技术MRI可以准确识别脑肿瘤。大量的训练数据和模型构建的改进使得深度学习方法在计算机视觉应用中得以快速发展，为监督学习提供了更好的近似。对于这些方法的需求是这个扩展的主要驱动因素。深度学习方法已经显示出在利用磁共振成像（MRI）改进脑肿瘤检测和分类的准确性方面是有希望的。本文介绍了使用深度学习技术，特别是ResNet50，进行脑肿瘤识别的研究。因此，本研究探讨了使用深度学习技术自动化检测过程的可能性。

    Brain tumors are collections of abnormal cells that can develop into masses or clusters. Because they have the potential to infiltrate other tissues, they pose a risk to the patient. The main imaging technique used, MRI, may be able to identify a brain tumor with accuracy. The fast development of Deep Learning methods for use in computer vision applications has been facilitated by a vast amount of training data and improvements in model construction that offer better approximations in a supervised setting. The need for these approaches has been the main driver of this expansion. Deep learning methods have shown promise in improving the precision of brain tumor detection and classification using magnetic resonance imaging (MRI). The study on the use of deep learning techniques, especially ResNet50, for brain tumor identification is presented in this abstract. As a result, this study investigates the possibility of automating the detection procedure using deep learning techniques. In thi
    
[^33]: 自适应实验中的最优条件推断

    Optimal Conditional Inference in Adaptive Experiments. (arXiv:2309.12162v1 [stat.ME])

    [http://arxiv.org/abs/2309.12162](http://arxiv.org/abs/2309.12162)

    我们研究了在自适应实验中进行条件推断的问题，证明了在没有进一步限制的情况下，仅使用最后一批结果进行推断是最优的；当实验的自适应方面是位置不变的时，我们还发现了额外的信息；在停止时间、分配概率和目标参数仅依赖于数据的多面体事件集合的情况下，我们推导出了计算可行且最优的条件推断程序。

    

    我们研究了批量赌徒实验，并考虑了在实现停止时间、分配概率和目标参数的条件下进行推断的问题，其中所有这些可能都是根据实验的最后一批信息进行自适应选择的。在没有对实验进行进一步限制的情况下，我们证明仅使用最后一批结果进行推断是最优的。当实验的自适应方面被认为是位置不变的，即当我们将所有批次-臂的平均值都向一个常数移动时，我们证明数据中还存在额外的信息，可以通过一个额外的批次-臂均值的线性函数来捕捉。在更严格的情况下，停止时间、分配概率和目标参数被认为仅依赖于数据通过一个多面体事件的集合，我们推导出了计算可行且最优的条件推断程序。

    We study batched bandit experiments and consider the problem of inference conditional on the realized stopping time, assignment probabilities, and target parameter, where all of these may be chosen adaptively using information up to the last batch of the experiment. Absent further restrictions on the experiment, we show that inference using only the results of the last batch is optimal. When the adaptive aspects of the experiment are known to be location-invariant, in the sense that they are unchanged when we shift all batch-arm means by a constant, we show that there is additional information in the data, captured by one additional linear function of the batch-arm means. In the more restrictive case where the stopping time, assignment probabilities, and target parameter are known to depend on the data only through a collection of polyhedral events, we derive computationally tractable and optimal conditional inference procedures.
    
[^34]: 迈向鲁棒和真正大规模的音频-乐谱检索

    Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval. (arXiv:2309.12158v1 [cs.SD])

    [http://arxiv.org/abs/2309.12158](http://arxiv.org/abs/2309.12158)

    本文研究了音频-乐谱检索的当前发展情况，通过深度学习方法解决了鲁棒性和大规模应用的挑战。

    

    多模态音乐信息检索的一系列应用集中在解决将大量乐谱图像与相应的音频记录连接起来的问题，即识别引用相同音乐内容的音频和乐谱摘录。最近的一种典型方法是使用跨模态深度学习架构来学习将两种不同模态（音频和乐谱图像）连接起来的联合嵌入空间。尽管在过去几年中在这一领域取得了稳定的进展，但仍有一些开放问题阻碍了这种方法的大规模应用。在本文中，我们试图通过深度学习方法对当前在音频-乐谱检索方面的发展进行深入研究。我们首先确定了在实际场景中实现鲁棒和大规模跨模态音乐检索的一系列主要挑战。然后，我们强调了迄今为止我们已经采取的一些步骤来解决其中一些挑战。

    A range of applications of multi-modal music information retrieval is centred around the problem of connecting large collections of sheet music (images) to corresponding audio recordings, that is, identifying pairs of audio and score excerpts that refer to the same musical content. One of the typical and most recent approaches to this task employs cross-modal deep learning architectures to learn joint embedding spaces that link the two distinct modalities - audio and sheet music images. While there has been steady improvement on this front over the past years, a number of open problems still prevent large-scale employment of this methodology. In this article we attempt to provide an insightful examination of the current developments on audio-sheet music retrieval via deep learning methods. We first identify a set of main challenges on the road towards robust and large-scale cross-modal music retrieval in real scenarios. We then highlight the steps we have taken so far to address some o
    
[^35]: 基于过去遍历特征的无监督领域自适应用于自动驾驶

    Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features. (arXiv:2309.12140v1 [cs.CV])

    [http://arxiv.org/abs/2309.12140](http://arxiv.org/abs/2309.12140)

    本研究提出了一种方法，利用无标签的重复遍历来适应物体检测器到新的驾驶环境。通过结合重复的激光雷达扫描计算的统计数据，我们有效地引导自适应过程，并通过引入轻量级的回归头和自训练过程来增强检测模型。实验表明，该方法在现实世界数据集上取得了多达20个百分点的性能提升。

    

    现如今，针对自动驾驶汽车的三维物体检测系统的快速发展极大提高了准确性。然而，这些系统往往难以泛化到多样化的驾驶环境中，这可能导致在检测交通参与者时发生安全关键性的失败。为了解决这个问题，我们提出了一种方法，利用无标签的多个位置重复遍历来适应物体检测器到新的驾驶环境。通过结合重复的激光雷达扫描计算的统计数据，我们有效地引导自适应过程。我们的方法通过使用空间量化的历史特征来增强基于激光雷达的检测模型，并引入了一个轻量级的回归头来利用统计数据进行特征正则化。此外，我们利用统计数据进行了一种新颖的自训练过程来稳定训练。该框架适用于任何检测器模型，并在现实世界数据集上进行的实验表明，取得了多达20个百分点的性能提升。

    The rapid development of 3D object detection systems for self-driving cars has significantly improved accuracy. However, these systems struggle to generalize across diverse driving environments, which can lead to safety-critical failures in detecting traffic participants. To address this, we propose a method that utilizes unlabeled repeated traversals of multiple locations to adapt object detectors to new driving environments. By incorporating statistics computed from repeated LiDAR scans, we guide the adaptation process effectively. Our approach enhances LiDAR-based detection models using spatial quantized historical features and introduces a lightweight regression head to leverage the statistics for feature regularization. Additionally, we leverage the statistics for a novel self-training process to stabilize the training. The framework is detector model-agnostic and experiments on real-world datasets demonstrate significant improvements, achieving up to a 20-point performance gain, 
    
[^36]: 无监督神经网络在逆问题中的收敛和恢复性能保证

    Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])

    [http://arxiv.org/abs/2309.12128](http://arxiv.org/abs/2309.12128)

    本研究通过探索连接理论和实践，提供了无监督神经网络在解决逆问题中的收敛和恢复性能保证。同时，我们还得出了对于两层具有平滑激活函数的深度逆先验网络的超参数化界限，该网络将从我们的保证中受益。

    

    近年来，神经网络已成为解决逆问题的主要方法。虽然已经有很多这样的方法被提出来经验性地解决逆问题，但我们仍然缺乏对这些方法的明确理论保证。另一方面，许多研究已经证明，通过过参数化来控制神经切向核，神经网络可以在更通用的设置下收敛到最优解。在这项工作中，我们探索如何连接这两个领域，并为无监督前馈多层神经网络解决逆问题的训练过程提供确定性的收敛和恢复性能保证。我们还推导出超参数化界限，在这些界限下，具有平滑激活函数的两层深度逆先验网络将受益于我们的保证。

    Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
    
[^37]: 使用循环模型的音频-乐谱检索的段落摘要

    Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval. (arXiv:2309.12111v1 [cs.SD])

    [http://arxiv.org/abs/2309.12111](http://arxiv.org/abs/2309.12111)

    本文提出了一个使用循环模型的音频-乐谱检索方法，通过学习跨模态循环网络生成可以摘要对应音频和乐谱的更长段落的联合嵌入。相比于传统方法，该方法只需要弱对齐的音频-乐谱对，并且能够处理音频和乐谱之间的节奏变化导致的非线性。

    

    跨模态音乐检索的许多应用与将乐谱图像与音频录音连接在一起有关。目前的典型方法是通过深度神经网络学习一个关联短固定大小的音频和乐谱片段的联合嵌入空间，通过适当的相似性结构。然而，这种策略带来的两个挑战是训练网络需要强对齐的数据，并且由于局部和全局速度差异而造成的音频和乐谱片段之间的音乐内容差异。在本文中，我们通过设计一个跨模态循环网络来解决这两个缺点，该网络学习可以摘要对应音频和乐谱的更长段落的联合嵌入。我们方法的好处是它只需要弱对齐的音频-乐谱对，以及循环网络能够处理音频和乐谱之间的节奏变化导致的非线性。

    Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet m
    
[^38]: 具有贝叶斯模型缩减的贝叶斯稀疏性深度神经网络

    Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])

    [http://arxiv.org/abs/2309.12095](http://arxiv.org/abs/2309.12095)

    使用贝叶斯模型缩减作为一种更高效的替代方法来修剪模型权重，以提高深度神经网络的计算效率。

    

    深度学习的巨大能力常常受到其模型复杂性的限制，因此对于有效的稀疏技术的需求不断增加。贝叶斯稀疏性对于深度学习而言是一种关键方法，可以促进在各种深度学习应用中设计既具有计算效率又具有竞争性能的模型。目前，贝叶斯稀疏化深度神经网络的最新技术是将结构收缩先验应用于模型权重，并结合基于黑盒随机变分推断的近似推断方案。然而，与标准的深度学习点估计相比，完整生成模型的模型反演在计算方面非常耗费时间。在这种情况下，我们提倡使用贝叶斯模型缩减（BMR）作为模型权重修剪的更高效替代方法。作为决策率的推广，BMR允许对模型权重进行事后消除

    Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimina
    
[^39]: 基于聚类的领域增量学习

    Clustering-based Domain-Incremental Learning. (arXiv:2309.12078v1 [cs.LG])

    [http://arxiv.org/abs/2309.12078](http://arxiv.org/abs/2309.12078)

    本文提出了一个基于聚类的方法来解决连续学习中的灾难性遗忘问题，该方法不需要提供任务更改的信息，并通过在线聚类方法有效地对抗灾难性遗忘。

    

    本文考虑在连续学习场景中学习多个任务的问题，其中来自不同任务的数据以流式呈现给学习器。在这种设置下的一个关键挑战是所谓的“灾难性遗忘问题”，即在“旧任务”上进行后续训练时学习器在“新任务”上的性能下降。现有的连续学习方法，如平均梯度情节性记忆（A-GEM）和正交梯度下降（OGD），通过最小化当前任务的损失而不增加先前任务的损失来解决灾难性遗忘问题。然而，这些方法假设学习器知道任务何时改变，这在实践中是不现实的。本文通过在动态更新的有限样本或梯度池中使用在线聚类方法来消除为算法提供有关任务更改的信息的需求。从而成功地对抗了其中一个困难的灾难性遗忘情况。

    We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming fashion. A key challenge in this setting is the so-called "catastrophic forgetting problem", in which the performance of the learner in an "old task" decreases when subsequently trained on a "new task". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume the learner knows when the task changes, which is unrealistic in practice. In this paper, we alleviate the need to provide the algorithm with information about task changes by using an online clustering-based approach on a dynamically updated finite pool of samples or gradients. We thereby successfully counteract catastrophic forgetting in one of the har
    
[^40]: 足球中动作识别、定位和时空定位的调查--当前趋势和研究展望

    Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])

    [http://arxiv.org/abs/2309.12067](http://arxiv.org/abs/2309.12067)

    本文提供了足球中动作识别、定位和时空定位的综述，重点介绍了多模态方法。通过利用深度学习技术和传统方法，这些方法整合了来自多个数据源的信息，并以多种方式表示一种来源，在提高模型准确性和稳健性方面具有潜力。

    

    由于足球比赛的复杂性和动态性以及球员之间的互动，对足球中的动作场景进行理解是一项具有挑战性的任务。本文将此任务分为动作识别、定位和时空动作定位，并重点介绍所使用的模态和多模态方法。我们探索了公开可用的数据源和用于评估模型性能的度量标准。本文回顾了利用深度学习技术和传统方法的最新研究方法。我们关注整合来自多个来源（如视频和音频数据）的信息的多模态方法，以及以多种方式表示一种来源的方法。讨论了方法的优势和局限性，以及它们改进模型准确性和稳健性的潜力。最后，本文强调了该领域的一些开放性研究问题和未来方向。

    Action scene understanding in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of 
    
[^41]: 一个高效整合词嵌入和深度学习技术的抗癌肽分类方法：FastText+BiLSTM

    An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM. (arXiv:2309.12058v1 [cs.LG])

    [http://arxiv.org/abs/2309.12058](http://arxiv.org/abs/2309.12058)

    本研究提出了一个整合了词嵌入和深度学习技术的高效抗癌肽分类模型，并评估了Word2Vec和FastText作为词嵌入技术，以及CNN、LSTM、BiLSTM作为深度学习模型的性能。

    

    抗癌肽（ACP）是一类具备抗肿瘤特性的肽。使用ACP在癌症预防中可以作为传统癌症治疗的替代品，因为它们具有更高的选择性和安全性。最近科学的进展引起了对基于肽的治疗的兴趣，它们能够高效地治疗目标细胞而不对正常细胞产生负面影响。然而，随着肽序列的数量不断增加，开发可靠和精确的预测模型变得具有挑战性。本研究旨在通过整合词嵌入和深度学习模型来提出一个高效的抗癌肽分类模型。首先，评估了Word2Vec和FastText作为提取肽序列的词嵌入技术。然后，将词嵌入模型的输出输入到深度学习方法CNN、LSTM、BiLSTM中。

    Anticancer peptides (ACPs) are a group of peptides that exhibite antineoplastic properties. The utilization of ACPs in cancer prevention can present a viable substitute for conventional cancer therapeutics, as they possess a higher degree of selectivity and safety. Recent scientific advancements generate an interest in peptide-based therapies which offer the advantage of efficiently treating intended cells without negatively impacting normal cells. However, as the number of peptide sequences continues to increase rapidly, developing a reliable and precise prediction model becomes a challenging task. In this work, our motivation is to advance an efficient model for categorizing anticancer peptides employing the consolidation of word embedding and deep learning models. First, Word2Vec and FastText are evaluated as word embedding techniques for the purpose of extracting peptide sequences. Then, the output of word embedding models are fed into deep learning approaches CNN, LSTM, BiLSTM. To
    
[^42]: 提供举报与预测模型的理论分析

    Uplift vs. predictive modeling: a theoretical analysis. (arXiv:2309.12036v1 [cs.LG])

    [http://arxiv.org/abs/2309.12036](http://arxiv.org/abs/2309.12036)

    本文提供了提升建模与经典预测方法的理论分析，研究贡献包括新的利润度量公式，收敛性证明以及条件的模拟说明。

    

    尽管机器学习技术在决策制定中越来越受欢迎，但因果定向策略相对于纯粹的机器学习方法的增值很少在文献中进行量化。这些策略对于营销、电信、医疗保健和金融等各个领域的从业者来说至关重要。本文从坚实的理论基础开始，并重点介绍影响提升和预测方法性能的参数。本文侧重于二元结果和二元操作的情况，并通过理论分析比较了提升建模与经典预测方法。本文的主要研究贡献包括对利润度量的新形式化公式，提升曲线收敛到利润度量的形式化证明，以及通过模拟对条件进行说明。

    Despite the growing popularity of machine-learning techniques in decision-making, the added value of causal-oriented strategies with respect to pure machine-learning approaches has rarely been quantified in the literature. These strategies are crucial for practitioners in various domains, such as marketing, telecommunications, health care and finance. This paper presents a comprehensive treatment of the subject, starting from firm theoretical foundations and highlighting the parameters that influence the performance of the uplift and predictive approaches. The focus of the paper is on a binary outcome case and a binary action, and the paper presents a theoretical analysis of uplift modeling, comparing it with the classical predictive approach. The main research contributions of the paper include a new formulation of the measure of profit, a formal proof of the convergence of the uplift curve to the measure of profit ,and an illustration, through simulations, of the conditions under whi
    
[^43]: StyleGAN中的脸部身份感知解缠论

    Face Identity-Aware Disentanglement in StyleGAN. (arXiv:2309.12033v1 [cs.CV])

    [http://arxiv.org/abs/2309.12033](http://arxiv.org/abs/2309.12033)

    本文通过引入PluGeN4Faces插件，利用对比损失来明确解缠脸部属性与人的身份，提出了一种解决面部图像修改中对身份冲突问题的方法。

    

    有条件的生成对抗网络经常用于操纵面部图像的属性，如表情、发型、姿势或年龄。尽管最先进的模型成功地修改了请求的属性，但同时也修改了图像的其他重要特征，如一个人的身份。本文致力于通过引入PluGeN4Faces（一种StyleGAN插件），从人的身份中明确解缠面部属性，来解决这个问题。我们的关键思想是对从电影帧中检索到的图像进行训练，该图像中的同一人以不同的姿势和属性出现。通过应用对比损失的一种类型，我们鼓励模型将同一人的图像分组到潜在空间的相似区域。我们的实验证明，PluGeN4Faces所执行的面部属性修改对图像的其他特征的侵入性明显小于现有的最先进模型。

    Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models.
    
[^44]: 人机协同下使用祖先GFlowNets进行潜在混淆的因果发现

    Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])

    [http://arxiv.org/abs/2309.12032](http://arxiv.org/abs/2309.12032)

    该论文提出了一种人机协同的因果发现方法，通过使用生成流网按照基于评分函数的信念分布采样祖先图，并引入最佳实验设计与专家互动，以提供专家可验证的不确定性估计并迭代改进因果推断。

    

    结构学习是因果推断的关键。值得注意的是，当数据稀缺时，因果发现（CD）算法很脆弱，可能推断出与专家知识相矛盾的不准确因果关系，尤其是考虑到潜在混淆因素时更是如此。为了加重这个问题，大多数CD方法并不提供不确定性估计，这使得用户难以解释结果和改进推断过程。令人惊讶的是，尽管CD是一个以人为中心的事务，但没有任何研究专注于构建既能输出专家可验证的不确定性估计又能与专家进行交互迭代改进的方法。为了解决这些问题，我们首先提出使用生成流网，根据基于评分函数（如贝叶斯信息准则）的信念分布，按比例对（因果）祖先图进行采样。然后，我们利用候选图的多样性并引入最佳实验设计，以迭代性地探索实验来与专家互动。

    Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expe
    
[^45]: 动态超图结构学习用于交通流量预测

    Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting. (arXiv:2309.12028v1 [cs.LG])

    [http://arxiv.org/abs/2309.12028](http://arxiv.org/abs/2309.12028)

    本文提出了一种名为动态超图结构学习(DyHSL)的模型，用于解决交通流量预测问题。该模型利用超图结构信息来建模复杂的交通网络，并且能够捕捉复杂的时空高阶交互作用。

    

    本文研究了交通流量预测问题，旨在基于过去的道路网络和交通状况来预测未来的交通条件。通常通过使用时空图神经网络（GNN）来建模交通数据中的复杂时空相关性来解决这个问题。然而，这些方法的性能仍然不令人满意，因为当涉及到复杂的交通网络时，GNNs的表示能力通常有限。图形本质上无法捕捉非配对关系。更糟糕的是，现有的方法都遵循信息传递的范式，线性地聚合邻域信息，无法捕捉复杂的时空高阶交互作用。为了解决这些问题，本文提出了一种名为动态超图结构学习(DyHSL)的新模型用于交通流量预测。为了学习非配对关系，我们的DyHSL提取超图结构信息来进行建模。

    This paper studies the problem of traffic flow forecasting, which aims to predict future traffic conditions on the basis of road networks and traffic conditions in the past. The problem is typically solved by modeling complex spatio-temporal correlations in traffic data using spatio-temporal graph neural networks (GNNs). However, the performance of these methods is still far from satisfactory since GNNs usually have limited representation capacity when it comes to complex traffic networks. Graphs, by nature, fall short in capturing non-pairwise relations. Even worse, existing methods follow the paradigm of message passing that aggregates neighborhood information linearly, which fails to capture complicated spatio-temporal high-order interactions. To tackle these issues, in this paper, we propose a novel model named Dynamic Hypergraph Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise relationships, our DyHSL extracts hypergraph structural information to model
    
[^46]: 针对带有背包约束的非单调k-次模子模块化最大化问题的鲁棒逼近算法

    Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint. (arXiv:2309.12025v1 [cs.DS])

    [http://arxiv.org/abs/2309.12025](http://arxiv.org/abs/2309.12025)

    本文提出了针对带有背包约束的非单调k-次模子模块化最大化问题的两个鲁棒逼近算法，大大改进了现有算法的查询复杂度，并分别提供了$1/19$和$1/5-\epsilon$的近似比率。

    

    在机器学习中的许多应用中，例如数据汇总、信息传播等，非单调k-次模子模块化最大化问题（$\kSMK$）在底层集合大小$n$上被提出。然而，现有算法面临如何克服非单调情况以及在数据大小大的情况下如何快速返回一个好解的问题。本文引入了两个确定性逼近算法，竞争性地改进了现有算法的查询复杂度。我们的第一个算法$\LAA$在$O(nk)$的查询复杂度内返回了$1/19$的近似比。第二个算法$\RLA$在$O(nk)$的查询中改进了近似比率到$1/5-\epsilon$，其中$\epsilon$是一个输入参数。我们的算法是第一个在非单调目标函数中仅使用$O(nk)$查询复杂度提供恒定近似比率的算法。因此，它们需要更少的查询次数。

    The problem of non-monotone $k$-submodular maximization under a knapsack constraint ($\kSMK$) over the ground set size $n$ has been raised in many applications in machine learning, such as data summarization, information propagation, etc. However, existing algorithms for the problem are facing questioning of how to overcome the non-monotone case and how to fast return a good solution in case of the big size of data. This paper introduces two deterministic approximation algorithms for the problem that competitively improve the query complexity of existing algorithms.  Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within $O(nk)$ query complexity. The second one, $\RLA$, improves the approximation ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input parameter.  Our algorithms are the first ones that provide constant approximation ratios within only $O(nk)$ query complexity for the non-monotone objective. They, therefore, need fewer the number of
    
[^47]: 基于能耗的立方卫星任务调度的安全层次化强化学习

    Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption. (arXiv:2309.12004v1 [cs.LG])

    [http://arxiv.org/abs/2309.12004](http://arxiv.org/abs/2309.12004)

    本文提出了一种针对立方卫星任务调度的层次化强化学习方法，通过整合任务优先级排序和能耗预测，实现了一个安全且容错的系统，并在多个立方卫星配置下优于MADDPG模型和传统随机调度。

    

    本文提出了一种针对低地球轨道（LEO）中立方卫星任务调度进行优化的层次化强化学习方法。该方法包括用于全局任务分配的高级策略和用于实时调整的低级策略作为安全机制，整合了基于相似度注意力编码器（SABE）进行任务优先级排序以及用于能耗预测的多层感知器（MLP）估计器。该机制的整合为立方卫星任务调度创建了一个安全且容错的系统。仿真结果证明了层次化强化学习具有更好的收敛性和任务成功率，优于MADDPG模型和传统随机调度在多个立方卫星配置下的表现。

    This paper presents a Hierarchical Reinforcement Learning methodology tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO). Incorporating a high-level policy for global task distribution and a low-level policy for real-time adaptations as a safety mechanism, our approach integrates the Similarity Attention-based Encoder (SABE) for task prioritization and an MLP estimator for energy consumption forecasting. Integrating this mechanism creates a safe and fault-tolerant system for CubeSat task scheduling. Simulation results validate the Hierarchical Reinforcement Learning superior convergence and task success rate, outperforming both the MADDPG model and traditional random scheduling across multiple CubeSat configurations.
    
[^48]: 通过机器学习在胸部X光图像上识别肺炎

    Identification of pneumonia on chest x-ray images through machine learning. (arXiv:2309.11995v1 [eess.IV])

    [http://arxiv.org/abs/2309.11995](http://arxiv.org/abs/2309.11995)

    该研究开发了一种通过机器学习和迁移学习技术，能够在胸部X射线片上识别肺炎的软件，具有98％的敏感性和97.3％的特异性。

    

    肺炎是全球婴儿死亡的主要感染原因。早期识别可以改变患者的预后，可以利用成像检查来帮助诊断确认。尽早执行和解读检查对于良好的治疗至关重要，对于这种病理学，最常见的检查是胸部X线。本研究的目标是开发一个能够在胸部X射线片中识别肺炎是否存在的软件。该软件基于使用迁移学习技术的机器学习的计算模型开发而成。在训练过程中，从中国的一家医院收集了儿童胸部X射线图像的在线数据库。经过训练后，该模型暴露于新的图像，并在识别该病理学方面取得了相关结果，在用于测试的样本中达到了98％的敏感性和97.3％的特异性。可以得出结论，开发能够在胸部X射线片上识别肺炎的软件是可行的。

    Pneumonia is the leading infectious cause of infant death in the world. When identified early, it is possible to alter the prognosis of the patient, one could use imaging exams to help in the diagnostic confirmation. Performing and interpreting the exams as soon as possible is vital for a good treatment, with the most common exam for this pathology being chest X-ray. The objective of this study was to develop a software that identify the presence or absence of pneumonia in chest radiographs. The software was developed as a computational model based on machine learning using transfer learning technique. For the training process, images were collected from a database available online with children's chest X-rays images taken at a hospital in China. After training, the model was then exposed to new images, achieving relevant results on identifying such pathology, reaching 98% sensitivity and 97.3% specificity for the sample used for testing. It can be concluded that it is possible to deve
    
[^49]: 用未评估的解决方案增强SAEAs：昂贵优化问题的关系模型案例研究

    Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization. (arXiv:2309.11994v1 [cs.NE])

    [http://arxiv.org/abs/2309.11994](http://arxiv.org/abs/2309.11994)

    本文提出了一个框架，利用未评估的解决方案来提高代理辅助进化算法（SAEAs）的效率，通过代理模型识别高质量解决方案，并直接生成新的解决方案，无需评估。

    

    基于代理的进化算法（SAEAs）在解决昂贵优化问题（EOPs）方面具有重要意义。通过开发高效的模型辅助选择方法，人们已经做出了大量努力来提高SAEAs的效能。然而，生成高质量的解决方案是选择的先决条件。在SAEAs的每一代中仅评估有限数量的解决方案的基本范式减少了相邻种群的方差，从而影响了后代解决方案的质量。这是一个经常遇到的问题，但还没有得到广泛关注。本文提出了一个框架，利用未评估的解决方案来提高SAEAs的效率。代理模型被用来识别高质量解决方案，直接生成新解决方案，无需评估。为了确保可靠的选择，我们引入了两个定制的关系模型，用于选择最佳解决方案和未评估种群。

    Surrogate-assisted evolutionary algorithms (SAEAs) hold significant importance in resolving expensive optimization problems~(EOPs). Extensive efforts have been devoted to improving the efficacy of SAEAs through the development of proficient model-assisted selection methods. However, generating high-quality solutions is a prerequisite for selection. The fundamental paradigm of evaluating a limited number of solutions in each generation within SAEAs reduces the variance of adjacent populations, thus impacting the quality of offspring solutions. This is a frequently encountered issue, yet it has not gained widespread attention. This paper presents a framework using unevaluated solutions to enhance the efficiency of SAEAs. The surrogate model is employed to identify high-quality solutions for direct generation of new solutions without evaluation. To ensure dependable selection, we have introduced two tailored relation models for the selection of the optimal solution and the unevaluated pop
    
[^50]: 后置解释方法中的可预测性和可理解性：一项以用户为中心的分析

    Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis. (arXiv:2309.11987v1 [cs.LG])

    [http://arxiv.org/abs/2309.11987](http://arxiv.org/abs/2309.11987)

    通过用户研究，分析了后置解释方法中的可理解性和可预测性。发现当解释集中在模型决策边界附近的样本时，SHAP的可理解性显著降低。另外，发现反事实解释和错误分类可以显著提高用户对机器学习模型决策原理的理解。根据研究结果，提出了增强后置解释方法可理解性和可预测性的设计建议。

    

    后置解释方法旨在澄清黑盒机器学习模型的预测结果。然而，用户对提供的解释有多好理解以及这些解释是否增强了用户对模型行为的预测能力仍然不清楚。我们通过进行用户研究来解决这个问题，评估了两种广泛使用的工具（LIME和SHAP）的可理解性和可预测性。此外，我们还研究了反事实解释和错误分类对用户理解和预测模型行为能力的影响。我们发现，当为接近模型决策边界的样本提供解释时，SHAP的可理解性显著降低。此外，我们还发现，反事实解释和错误分类可以显著增加用户对机器学习模型决策原理的理解。根据我们的研究结果，我们还提出了未来后置解释方法的设计建议，以增强其可理解性和可预测性。

    Post-hoc explainability methods aim to clarify predictions of black-box machine learning models. However, it is still largely unclear how well users comprehend the provided explanations and whether these increase the users ability to predict the model behavior. We approach this question by conducting a user study to evaluate comprehensibility and predictability in two widely used tools: LIME and SHAP. Moreover, we investigate the effect of counterfactual explanations and misclassifications on users ability to understand and predict the model behavior. We find that the comprehensibility of SHAP is significantly reduced when explanations are provided for samples near a model's decision boundary. Furthermore, we find that counterfactual explanations and misclassifications can significantly increase the users understanding of how a machine learning model is making decisions. Based on our findings, we also derive design recommendations for future post-hoc explainability methods with increas
    
[^51]: 变分连接主义时间分类用于保序序列建模

    Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v1 [cs.LG])

    [http://arxiv.org/abs/2309.11983](http://arxiv.org/abs/2309.11983)

    本论文将连接主义时间分类（CTC）与变分模型相结合，提出了两个版本的新型变分CTC，用于训练更具普适性的保序序列模型。这些方法允许直接优化模型对数似然的变分下界，并解决了计算上的挑战。

    

    连接主义时间分类（CTC）常被用于保序序列建模任务，比如语音识别，其中保持输入和目标序列的顺序是必要的。然而，CTC仅应用于确定性序列模型，其中潜在空间是不连续且稀疏的，这使得它们在处理数据的变异性方面比变分模型能力更弱。在本文中，我们将CTC与变分模型相结合，并导出了可以用于训练更具普适性的序列模型的损失函数，以保持顺序。具体而言，我们根据两个合理的假设导出了两个版本的新型变分CTC，第一个假设是每个时间步的变分潜在变量在条件下是独立的；第二个假设是这些潜在变量是马尔可夫的。我们展示了这两个损失函数都允许直接优化模型对数似然的变分下界，并且展示了计算上的一些挑战和解决方案。

    Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally t
    
[^52]: 股市情绪分类与基于Fine-tuned BERT的回测

    Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT. (arXiv:2309.11979v1 [q-fin.CP])

    [http://arxiv.org/abs/2309.11979](http://arxiv.org/abs/2309.11979)

    本论文通过构建自然语言处理模型BERT并对其进行fine-tune，实现了股市情绪的分类和基于该模型的回测分析。实验结果显示，fine-tuned模型相比原始模型和基准模型有不同程度的性能改进。

    

    随着大数据和计算设备的快速发展，基于实时信息获取的低延迟自动交易平台成为股票交易市场的主要组成部分，因此量化交易的主题得到了广泛关注。对于非强有效的交易市场来说，人类情绪和期望总是主导市场趋势和交易决策。因此，本文从情绪理论出发，以东方财富为例，从其对应的股吧爬取用户评论标题数据并进行数据清洗。随后，构建了自然语言处理模型BERT，并使用现有的带有标注数据集对BERT模型进行fine-tune。实验结果表明，与原始模型和基准模型相比，fine-tuned模型有不同程度的性能改进。随后，在以上模型的基础上，对爬取的用户评论数据进行了情绪极性标注。

    With the rapid development of big data and computing devices, low-latency automatic trading platforms based on real-time information acquisition have become the main components of the stock trading market, so the topic of quantitative trading has received widespread attention. And for non-strongly efficient trading markets, human emotions and expectations always dominate market trends and trading decisions. Therefore, this paper starts from the theory of emotion, taking East Money as an example, crawling user comment titles data from its corresponding stock bar and performing data cleaning. Subsequently, a natural language processing model BERT was constructed, and the BERT model was fine-tuned using existing annotated data sets. The experimental results show that the fine-tuned model has different degrees of performance improvement compared to the original model and the baseline model. Subsequently, based on the above model, the user comment data crawled is labeled with emotional pola
    
[^53]: 使用随机分裂函数生成层次结构以改善时间序列分类

    Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions. (arXiv:2309.11963v1 [cs.LG])

    [http://arxiv.org/abs/2309.11963](http://arxiv.org/abs/2309.11963)

    本研究介绍了一种新颖的层次分割聚类方法，通过使用随机分裂函数提高多类数据集的分类性能。该方法可以生成层次结构而无需显式信息，适用于缺乏层次先验知识的数据集。实验结果表明，该方法在一半以上的数据集中显著提升分类性能。

    

    本研究引入了一种新颖的使用随机分裂函数（SSFs）的层次分割聚类方法，通过层次分类（HC）提高多类数据集的分类性能。该方法具有在不需要显式信息的情况下生成层次的独特能力，适用于缺乏层次先验知识的数据集。通过根据分类器的可辨别性将类别系统地划分为两个子集，该方法构建了层次类别的二叉树表示。该方法在46个多类时间序列数据集上使用流行的分类器（svm和rocket）和SSFs（potr、srtr和lsoo）进行评估。结果表明，在使用rocket和svm作为分类器时，该方法在近一半和三分之一的数据集中显著提高了分类性能。本研究还探讨了数据集特征与HC之间的关系。

    This study introduces a novel hierarchical divisive clustering approach with stochastic splitting functions (SSFs) to enhance classification performance in multi-class datasets through hierarchical classification (HC). The method has the unique capability of generating hierarchy without requiring explicit information, making it suitable for datasets lacking prior knowledge of hierarchy. By systematically dividing classes into two subsets based on their discriminability according to the classifier, the proposed approach constructs a binary tree representation of hierarchical classes. The approach is evaluated on 46 multi-class time series datasets using popular classifiers (svm and rocket) and SSFs (potr, srtr, and lsoo). The results reveal that the approach significantly improves classification performance in approximately half and a third of the datasets when using rocket and svm as the classifier, respectively. The study also explores the relationship between dataset features and HC 
    
[^54]: 自监督学习的向前-向前算法研究

    A Study of Forward-Forward Algorithm for Self-Supervised Learning. (arXiv:2309.11955v1 [cs.CV])

    [http://arxiv.org/abs/2309.11955](http://arxiv.org/abs/2309.11955)

    本文首次研究了自监督表示学习中的向前-向前算法和反向传播的性能，发现在自监督表示学习中，向前-向前算法与反向传播表现相当。

    

    在过去的几年中，自监督表示学习取得了显著的进展，其中一些最新方法能够在没有标签的情况下学习出有用的图像表示。这些方法使用了反向传播作为训练的事实标准。最近，Geoffrey Hinton提出了向前-向前算法作为一种替代的训练方法。它利用了两次向前传递和每层都有一个单独的损失函数来训练网络，从而避免了反向传播。在这项研究中，我们首次研究了向前-向前算法与反向传播在自监督表示学习中的性能，并对学习到的表示空间提供了一些见解。我们的基准测试使用了四个标准数据集，分别是MNIST、F-MNIST、SVHN和CIFAR-10，以及三种常用的自监督表示学习技术，即旋转、翻转和拼图。我们的主要发现是，在自监督表示学习中，向前-向前算法与反向传播表现相当。

    Self-supervised representation learning has seen remarkable progress in the last few years, with some of the recent methods being able to learn useful image representations without labels. These methods are trained using backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the forward-forward algorithm as an alternative training method. It utilizes two forward passes and a separate loss function for each layer to train the network without backpropagation.  In this study, for the first time, we study the performance of forward-forward vs. backpropagation for self-supervised representation learning and provide insights into the learned representation spaces. Our benchmark employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and three commonly used self-supervised representation learning techniques, namely rotation, flip and jigsaw.  Our main finding is that while the forward-forward algorithm performs comparably to backpropagation during (self-)
    
[^55]: 关于免疫的概率的研究

    On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])

    [http://arxiv.org/abs/2309.11942](http://arxiv.org/abs/2309.11942)

    本文研究了免疫的概率，提出了免疫的必要和充分条件，以及ε-有界免疫的条件。同时，借助随机对照试验估计受益概率，并得到比现有边界更紧密的概率边界。此外，介绍了间接免疫的概念，并提出了一种用于处理未测量混淆的免疫概率敏感性分析方法。

    

    本文致力于研究免疫的概率，即无论暴露与否，效果都会发生。我们导出了免疫的必要和充分条件以及ε-有界免疫的条件，前者允许我们从随机对照试验中估计受益的概率（即只有在暴露的情况下效果才会发生），后者允许我们得到比现有的边界更紧密的受益概率边界。我们还引入了间接免疫的概念（通过介质），并对其进行了前述分析。最后，我们提出了一种用于在未测量混淆情况下进行免疫概率敏感性分析的方法。

    This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
    
[^56]: 基于机器学习的微型机器学习综述

    A Machine Learning-oriented Survey on Tiny Machine Learning. (arXiv:2309.11932v1 [cs.LG])

    [http://arxiv.org/abs/2309.11932](http://arxiv.org/abs/2309.11932)

    本综述旨在提供关于微型机器学习（TinyML）的学习算法的最新综述，重点关注其在人工智能领域的应用与潜在贡献。

    

    微型机器学习（TinyML）的出现通过推动资源受限的物联网硬件设备与基于学习的软件架构的联合设计，积极革命了人工智能领域。TinyML在第四和第五次工业革命中发挥着重要作用，帮助社会、经济和个人应用有效的人工智能融入计算技术（如智慧城市、汽车和医疗机器人）。由于其多学科性质，TinyML领域已经从许多不同的角度进行了研究：本综述希望提供一个最新的综述，重点关注在基于TinyML的解决方案中的所有学习算法。本综述基于系统综述和元分析的首选报告项（PRISMA）方法流程，实现了系统和完整的文献综述。具体而言，首先我们将研究实现TinyML基础解决方案的三种不同工作流程。

    The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly we will examine the three different workflows for implementing a TinyML-bas
    
[^57]: 弥合差距：针对开放世界半监督学习的学习速度同步

    Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning. (arXiv:2309.11930v1 [cs.LG])

    [http://arxiv.org/abs/2309.11930](http://arxiv.org/abs/2309.11930)

    本论文提出了两个方法，一个是自适应边界损失，通过调整边界来同步学习速度；另一个是伪标签对比聚类，通过聚集样本来增强新类别的发现。实验证明，该方法能够平衡已见和新类别，相比现有模型，在ImageNet数据集上提高了3%的平均准确率。

    

    在开放世界半监督学习中，一个机器学习模型被要求从无标签数据中发现新的类别，同时在有标签数据中保持对已见类别的表现。其中的核心挑战是已见和新类别之间存在巨大的学习差距，因为由于准确的监督信息，模型学习已见类别的速度更快。为了解决这个问题，我们引入以下两个方法：1）基于估计类别分布的自适应边界损失，鼓励对已见类别样本使用较大的负边界，以同步学习速度；2）伪标签对比聚类，将可能来自同一类别的样本在输出空间中聚集在一起，增强新类别发现。我们在多个数据集上进行了广泛的评估，结果表明现有模型仍然阻碍新类别的学习，而我们的方法明显平衡了已见和新类别，与ImageNet数据集相比，平均准确率提高了3%。

    In open-world semi-supervised learning, a machine learning model is tasked with uncovering novel categories from unlabeled data while maintaining performance on seen categories from labeled data. The central challenge is the substantial learning gap between seen and novel categories, as the model learns the former faster due to accurate supervisory information. To address this, we introduce 1) an adaptive margin loss based on estimated class distribution, which encourages a large negative margin for samples in seen classes, to synchronize learning paces, and 2) pseudo-label contrastive clustering, which pulls together samples which are likely from the same class in the output space, to enhance novel class discovery. Our extensive evaluations on multiple datasets demonstrate that existing models still hinder novel class learning, whereas our approach strikingly balances both seen and novel classes, achieving a remarkable 3% average accuracy increase on the ImageNet dataset compared to t
    
[^58]: 基于物理信息高斯过程的Timoshenko梁的随机刚度识别和响应估计

    Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes. (arXiv:2309.11875v1 [cs.LG])

    [http://arxiv.org/abs/2309.11875](http://arxiv.org/abs/2309.11875)

    本文提出了一种基于物理信息的高斯过程模型，用于Timoshenko梁的刚度识别和响应估计。通过马尔可夫链蒙特卡洛方法进行贝叶斯优化，得到结构参数的随机模型。模型还可用于概率预测未观测到的响应，并提出了一种基于熵的传感器布置优化方法。

    

    使用结构健康监测数据训练的机器学习模型已成为系统识别的强大工具。本文提出了一种基于物理信息的高斯过程模型，用于Timoshenko梁元素。该模型是一个多输出高斯过程，其协方差和交叉协方差核根据挠度、转动、应变、弯矩、剪力和施加载荷的微分方程解析地推导而来。通过马尔可夫链蒙特卡洛方法在贝叶斯格式下进行刚度识别，最大化后验模型，得到结构参数的随机模型。优化后的高斯过程模型进一步用于对未观测到的响应进行概率预测。此外，还提出了一种基于熵的物理信息传感器布置优化方法，利用异质传感器位置信息和嵌入高斯过程模型的结构边界条件。结果表明，所提出的方法能有效识别刚度并预测未观测到的响应。

    Machine learning models trained with structural health monitoring data have become a powerful tool for system identification. This paper presents a physics-informed Gaussian process (GP) model for Timoshenko beam elements. The model is constructed as a multi-output GP with covariance and cross-covariance kernels analytically derived based on the differential equations for deflections, rotations, strains, bending moments, shear forces and applied loads. Stiffness identification is performed in a Bayesian format by maximising a posterior model through a Markov chain Monte Carlo method, yielding a stochastic model for the structural parameters. The optimised GP model is further employed for probabilistic predictions of unobserved responses. Additionally, an entropy-based method for physics-informed sensor placement optimisation is presented, exploiting heterogeneous sensor position information and structural boundary conditions built into the GP model. Results demonstrate that the propose
    
[^59]: 使用改进的方差最小化的分块量化对图神经网络进行激活压缩

    Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v1 [stat.ML])

    [http://arxiv.org/abs/2309.11856](http://arxiv.org/abs/2309.11856)

    本论文提出了一种使用改进的方差最小化的分块量化策略，用于压缩图神经网络的激活，实现内存消耗的降低和运行时的加速。

    

    已经研究了大规模图神经网络（GNNs）的高效训练，重点是减少其内存消耗。Liu等人（2022年）提出了极限激活压缩（EXACT），通过将中间激活图的量化降至INT2精度，实现了内存消耗的剧烈减少。他们在实现大幅减少GPU内存消耗的同时，表现几乎没有降低。在这项工作中，我们通过使用中间激活图的分块量化，对EXACT策略进行了改进。我们实验分析了不同的块大小，并展示了进一步的内存消耗降低（>15%）和每个epoch的运行时加速（约5%），即使进行了极其大的量化程度，也能获得与原始EXACT相似的性能权衡。此外，我们对EXACT中关于中间激活图分布的假设进行了纠正（假设为u

    Efficient training of large-scale graph neural networks (GNNs) has been studied with a specific focus on reducing their memory consumption. Work by Liu et al. (2022) proposed extreme activation compression (EXACT) which demonstrated drastic reduction in memory consumption by performing quantization of the intermediate activation maps down to using INT2 precision. They showed little to no reduction in performance while achieving large reductions in GPU memory consumption. In this work, we present an improvement to the EXACT strategy by using block-wise quantization of the intermediate activation maps. We experimentally analyze different block sizes and show further reduction in memory consumption (>15%), and runtime speedup per epoch (about 5%) even when performing extreme extents of quantization with similar performance trade-offs as with the original EXACT. Further, we present a correction to the assumptions on the distribution of intermediate activation maps in EXACT (assumed to be u
    
[^60]: TMac：用于声音事件分类的时态多模态图学习

    TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification. (arXiv:2309.11845v1 [cs.SD])

    [http://arxiv.org/abs/2309.11845](http://arxiv.org/abs/2309.11845)

    TMac是一个时态多模态图学习方法，用于声音事件分类。它通过图学习的方式对多模态数据中的时态信息进行建模，提高了声音事件分类的性能。

    

    在这个数字时代，音频视觉数据随处可见，这对于对它们开发的深度学习模型提出了更高的要求。有效处理多模态数据的信息是更好的音频视觉模型的关键。我们观察到这些音频视觉数据自然具有时间属性，例如视频中每一帧的时间信息。更具体地说，这些数据根据音频和视觉线索自然形成多模态，并且严格按照时间顺序进行。这表明，在多模态声音事件建模中，时态信息对于内部和跨模态都很重要。然而，现有的方法独立地处理每个模态特征，仅简单地将它们融合在一起，忽视了时态关系的挖掘，从而导致次优的性能。出于这个动机，我们提出了一种用于声音事件分类的时态多模态图学习方法，称为TMac，通过图学习对这种时态信息进行建模。

    Audiovisual data is everywhere in this digital age, which raises higher requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual modal. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning
    
[^61]: 图中社区检测的综合评述

    A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])

    [http://arxiv.org/abs/2309.11798](http://arxiv.org/abs/2309.11798)

    本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。

    

    复杂网络研究显著促进了我们对真实世界图的社区结构的理解，这是一个具有挑战性的问题，在社会学、生物学和计算机科学中具有应用价值。尽管跨学科科学家社区的努力，但尚未找到一个令人满意的解决方案。本综述详细介绍了图中社区检测的主题，这对于理解复杂系统的组织和功能起着关键的作用。首先，我们介绍社区结构的概念，它指的是将顶点划分为具有强内部连接和较弱连接的集群。然后，我们对各种社区检测方法进行了彻底的阐述，包括我们设计的一种新方法。此外，我们还探讨了社区检测在各种网络中的真实应用。

    The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
    
[^62]: DimCL: 用于改进自监督学习的维度对比学习

    DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning. (arXiv:2309.11782v1 [cs.CV])

    [http://arxiv.org/abs/2309.11782](http://arxiv.org/abs/2309.11782)

    DimCL是一种在自监督学习中改进特征多样性的维度对比学习方法。实验证明DimCL的硬样本特性是成功的关键因素。将DimCL融入SSL框架可以提高性能。

    

    自监督学习（SSL）取得了显著的成功，其中对比学习（CL）起到了关键作用。然而，最近发展起来的新的非CL框架已经取得了可比甚至更好的性能，并且有很大的改进潜力，这促使研究人员进一步提升这些框架。将CL融入非CL框架被认为是有益的，但经验证据表明没有明显的改进。基于此，本文提出了一种在维度方向上执行CL而不是以传统的批次方向进行对比学习的策略，命名为Dimensional Contrastive Learning（DimCL）。DimCL旨在增强特征的多样性，并且可以作为先前的SSL框架的正则化器。结果发现DimCL是有效的，并且硬样本对其成功起到了关键作用。广泛的实验结果表明将DimCL融入SSL框架可以提高性能。

    Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key role. However, the recent development of new non-CL frameworks has achieved comparable or better performance with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance 
    
[^63]: 基于惯性测量单元(IMU)的步态认证的字典攻击

    Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v1 [cs.CR])

    [http://arxiv.org/abs/2309.11766](http://arxiv.org/abs/2309.11766)

    这项研究提出了一种对使用惯性测量单元记录的步态模式进行认证的敌对模型。研究调查了是否可能构建一本IMUGait模式的字典，用于发动攻击或找到能够匹配目标IMUGait模式的模仿者。通过对错误率的进一步分析，挑战了基于IMUGait模式的认证系统是否最困难的观点。

    

    我们提出了一种新的对使用智能手机内置的惯性测量单元(IMU)记录的步态模式进行认证的敌对模型。攻击思路受到字典攻击知识(PIN或密码)基础认证系统的概念启发，并以此命名。具体而言，本研究调查是否可能构建一本IMUGait模式的字典，然后使用它发动攻击，或找到一个能够主动复制与目标IMUGait模式匹配的模仿者。九个身体和人口统计学多样化的个体以不同水平的四个预定义可控和可调节的步态因素(速度、步长、步宽和大腿抬起)行走，产生了178种独特的IMUGait模式。每个模式都会攻击各种用户认证模型。对错误率的深入分析(攻击前后)挑战了基于IMUGait模式的认证系统是最困难的这种信仰。

    We present a novel adversarial model for authentication systems that use gait patterns recorded by the inertial measurement unit (IMU) built into smartphones. The attack idea is inspired by and named after the concept of a dictionary attack on knowledge (PIN or password) based authentication systems. In particular, this work investigates whether it is possible to build a dictionary of IMUGait patterns and use it to launch an attack or find an imitator who can actively reproduce IMUGait patterns that match the target's IMUGait pattern. Nine physically and demographically diverse individuals walked at various levels of four predefined controllable and adaptable gait factors (speed, step length, step width, and thigh-lift), producing 178 unique IMUGait patterns. Each pattern attacked a wide variety of user authentication models. The deeper analysis of error rates (before and after the attack) challenges the belief that authentication systems based on IMUGait patterns are the most difficul
    
[^64]: 隐私保护下的上下文学习与差分隐私弱监督生成

    Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v1 [cs.LG])

    [http://arxiv.org/abs/2309.11765](http://arxiv.org/abs/2309.11765)

    本论文提出了一种隐私保护下的上下文学习算法，通过生成具有差分隐私保证的合成少量示范，实现了有效的ICL。实验证明该算法在强隐私级别下能够取得竞争性能，为广泛应用领域的隐私保护下ICL开辟了新的可能性。

    

    我们研究了使用大型语言模型（LLM）在私有数据集上进行上下文学习（ICL）的问题。这种情景会带来隐私风险，因为LLM可能泄漏或复述在提示中展示的私有示例。我们提出了一种新算法，可以从私有数据集中生成具有形式差分隐私保证的合成少量示范，并在实证上证明它能够实现有效的ICL。我们在标准基准测试上进行了大量实验，并将我们的算法与非私有ICL和零样本解决方案进行了比较。我们的结果表明，我们的算法可以在强隐私级别下达到竞争性能。这些结果为具有隐私保护的ICL在广泛应用领域打开了新的可能性。

    We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.
    
[^65]: SAM-OCTA：一种将基础模型应用于OCTA图像分割任务的微调策略

    SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks. (arXiv:2309.11758v1 [cs.CV])

    [http://arxiv.org/abs/2309.11758](http://arxiv.org/abs/2309.11758)

    SAM-OCTA是一种将基础模型应用于OCTA图像分割任务的微调策略，在公开的OCTA-500数据集上实验表现最先进的性能指标，成功实现了局部血管分割和有效的动脉-静脉分割。

    

    在光学相干断层扫描血管成像(OCTA)图像分析中，分割特定目标是必要的。现有方法通常在有限样本（大约几百个）的监督数据集上进行训练，这可能导致过拟合。为了解决这个问题，采用低秩适应技术进行基础模型的微调，并提出相应的提示点生成策略，以处理OCTA数据集上的各种分割任务。该方法被命名为SAM-OCTA，并在公开可用的OCTA-500数据集上进行了实验。该方法在实现最先进的性能指标的同时，实现了局部血管分割以及有效的动脉-静脉分割，这在先前的工作中尚未得到很好的解决。代码可在https://github.com/ShellRedia/SAM-OCTA找到。

    In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.
    
[^66]: Google的Bard在对抗图像攻击方面有多强大？

    How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])

    [http://arxiv.org/abs/2309.11751](http://arxiv.org/abs/2309.11751)

    本文研究了Google的Bard在对抗图像攻击方面的鲁棒性，并发现它可以被攻击以输出错误的图像描述。这一攻击还可以对其他多模态语言模型产生影响。研究还发现了Bard的两种防御机制。

    

    结合文本和其他模态（尤其是视觉）的多模态大型语言模型（MLLM）在各种多模态任务中取得了前所未有的性能。然而，由于视觉模型的未解决的对抗鲁棒性问题，引入视觉输入可能使MLLM面临更严重的安全风险和安全风险。在这项工作中，我们研究了Google的Bard的对抗鲁棒性，它是一个竞争性的聊天机器人，最近发布了其多模态能力，以更好地了解商业MLLM的漏洞。通过攻击白盒子代理视觉编码器或MLLM，生成的对抗性示例可以使Bard以22％的成功率仅基于可转移性输出错误的图像描述。我们还表明，对抗性示例还可以攻击其他MLLM，例如，对Bing Chat的成功攻击率为26％，对ERNIE bot的成功攻击率为86％。此外，我们确定了Bard的两种防御机制，包括面部检测。

    Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection
    
[^67]: PIE: 通过渐进图像编辑模拟疾病进展

    PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v1 [eess.IV])

    [http://arxiv.org/abs/2309.11745](http://arxiv.org/abs/2309.11745)

    PIE是一个新的渐进图像编辑框架，可以通过控制性地操纵图像特征来准确模拟个体患者的疾病进展。该方法利用文本到图像生成模型，实现了个性化的疾病进展模拟，并在验证实验中展现了优越性。

    

    疾病进展模拟是一项重要的研究领域，对临床诊断、预后和治疗具有重要意义。该领域面临的主要挑战之一是缺乏对个体患者进行连续的医学成像监测的数据。为了解决这个问题，我们提出了一种名为渐进图像编辑（PIE）的新框架，可以控制性地操纵与疾病相关的图像特征，实现精确和逼真的疾病进展模拟。具体地，我们利用了最近在文本到图像生成模型方面的进展，准确地模拟疾病进展并针对每个患者进行个性化处理。我们理论上分析了我们框架中的迭代优化过程，将其视为具有指数衰减学习率的梯度下降。为了验证我们的框架，我们在三个医学成像领域进行了实验。我们的结果证明了PIE相对于稳定扩散漫游和基于风格的特征生成方法的优越性。

    Disease progression simulation is a crucial area of research that has significant implications for clinical diagnosis, prognosis, and treatment. One major challenge in this field is the lack of continuous medical imaging monitoring of individual patients over time. To address this issue, we develop a novel framework termed Progressive Image Editing (PIE) that enables controlled manipulation of disease-related image features, facilitating precise and realistic disease progression simulation. Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient. We theoretically analyze the iterative refining process in our framework as a gradient descent with an exponentially decayed learning rate. To validate our framework, we conduct experiments in three medical imaging domains. Our results demonstrate the superiority of PIE over existing methods such as Stable Diffusion Walk and Style-Based Mani
    
[^68]: 揭示最佳可持续发展目标路径：利用图剪枝和意图图进行有效推荐的创新方法

    Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations. (arXiv:2309.11741v1 [cs.IR])

    [http://arxiv.org/abs/2309.11741](http://arxiv.org/abs/2309.11741)

    该论文提出了一种名为UGPIG的方法，通过应用剪枝用户图和意图图解决空间异质性和稀疏性数据的问题，以实现推荐适用的可持续发展模式。

    

    在特定地区促进生态、经济、社会和资源可持续性的重要手段是推荐适当的发展路径，也称为生态文明模式（即可持续发展模式）。然而，当前计算机科学领域的推荐算法在充分解决与环境相关的空间异质性和区域历史互动数据的稀疏性方面存在不足，限制了它们在推荐可持续发展模式方面的有效性。为了克服这些挑战，本文提出了一种名为“剪枝用户图和意图图（UGPIG）”的方法。

    The recommendation of appropriate development pathways, also known as ecological civilization patterns for achieving Sustainable Development Goals (namely, sustainable development patterns), are of utmost importance for promoting ecological, economic, social, and resource sustainability in a specific region. To achieve this, the recommendation process must carefully consider the region's natural, environmental, resource, and economic characteristics. However, current recommendation algorithms in the field of computer science fall short in adequately addressing the spatial heterogeneity related to environment and sparsity of regional historical interaction data, which limits their effectiveness in recommending sustainable development patterns. To overcome these challenges, this paper proposes a method called User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the high-density linking capability of the pruned User Graph to address the issue of spatial heterogeneity neg
    
[^69]: 高效的选核激励机制用于联邦学习中的数据共享

    Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning. (arXiv:2309.11722v1 [cs.GT])

    [http://arxiv.org/abs/2309.11722](http://arxiv.org/abs/2309.11722)

    本文介绍了一个联邦学习的数据共享博弈模型，并利用博弈论的方法设计了一个选核激励机制，以激励真实输入数据并促进稳定合作。

    

    联邦学习是一种分布式机器学习系统，利用参与者的数据训练一个改进的全局模型。在联邦学习中，参与者合作训练一个全局模型，并且他们将获得全局模型和支付。理性的参与者试图最大化自己的个体效用，除非他们基于数据质量获得令人满意的支付，否则他们将不会真实地输入高质量的数据。此外，联邦学习受益于参与者的合作贡献。因此，如何建立一个既激励真实输入数据又促进稳定合作的激励机制已成为一个重要的问题。在本文中，我们引入了一个联邦学习的数据共享博弈模型，并运用博弈论的方法来设计一个选核激励机制，利用合作博弈中的一个流行概念，即核心。

    Federated learning is a distributed machine learning system that uses participants' data to train an improved global model. In federated learning, participants cooperatively train a global model, and they will receive the global model and payments. Rational participants try to maximize their individual utility, and they will not input their high-quality data truthfully unless they are provided with satisfactory payments based on their data quality. Furthermore, federated learning benefits from the cooperative contributions of participants. Accordingly, how to establish an incentive mechanism that both incentivizes inputting data truthfully and promotes stable cooperation has become an important issue to consider. In this paper, we introduce a data sharing game model for federated learning and employ game-theoretic approaches to design a core-selecting incentive mechanism by utilizing a popular concept in cooperative games, the core. In federated learning, the core can be empty, resulti
    
[^70]: 一种用于基于EEG的运动想象分类的动态领域适应深度学习网络

    A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification. (arXiv:2309.11714v1 [eess.SP])

    [http://arxiv.org/abs/2309.11714](http://arxiv.org/abs/2309.11714)

    本文提出了一种动态领域适应深度学习网络，用于解决基于EEG的运动想象分类中的通道相关性和个体差异问题。

    

    电脑脑电图（EEG）相邻通道之间存在相关性，如何表示这种相关性是当前研究的一个问题。此外，由于EEG信号在不同个体之间存在差异，这种差异导致新主体需要花费大量校准时间用于基于EEG的运动想象脑机接口。为了解决上述问题，我们提出了一种基于动态领域适应的深度学习网络（DADL-Net）。首先，将EEG数据映射到三维几何空间，并通过3D卷积模块学习其时空特征，然后使用空间通道注意机制强化特征，最后的卷积模块可以进一步学习特征的时空信息。最后，为了考虑个体间和跨会话的差异，我们采用动态领域自适应策略，通过引入最大化距离来减小特征之间的差异。

    There is a correlation between adjacent channels of electroencephalogram (EEG), and how to represent this correlation is an issue that is currently being explored. In addition, due to inter-individual differences in EEG signals, this discrepancy results in new subjects need spend a amount of calibration time for EEG-based motor imagery brain-computer interface. In order to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep Learning Network (DADL-Net). First, the EEG data is mapped to the three-dimensional geometric space and its temporal-spatial features are learned through the 3D convolution module, and then the spatial-channel attention mechanism is used to strengthen the features, and the final convolution module can further learn the spatial-temporal information of the features. Finally, to account for inter-subject and cross-sessions differences, we employ a dynamic domain-adaptive strategy, the distance between features is reduced by introducing a Maximum
    
[^71]: 三维切片Wasserstein的准蒙特卡洛方法

    Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])

    [http://arxiv.org/abs/2309.11713](http://arxiv.org/abs/2309.11713)

    本文提出了准蒙特卡洛（QMC）方法用于三维切片Wasserstein（SW）的近似计算，并通过多种方法在三维单位超球面上构造了QMC点集。此外，还介绍了将QSW扩展为随机准切片Wasserstein（RQSW）的方法。

    

    Monte Carlo (MC)方法被用作计算切片Wasserstein (SW)距离的标准方法，因为它在分析形式中具有棘手的期望。然而，MC方法在最小化绝对近似误差方面并不优化。为了提供更好的经验SW类，我们提出了基于准蒙特卡洛（QMC）方法的准切片Wasserstein（QSW）逼近。为了对SW的QMC进行全面的研究，我们专注于三维设置，特别是计算三维概率测度之间的SW。具体而言，我们通过实证验证了在三维单位超球面上构造QMC点集的多种方法，包括基于高斯的映射，等面积映射，广义螺旋点和最优化差异能量。此外，为了获得随机优化的无偏估计，我们通过在所讨论的低维设置中引入随机性，将QSW扩展为随机准切片Wasserstein（RQSW）。

    Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-d
    
[^72]: 基于元学习的持续适应性OOD检测

    Meta OOD Learning for Continuously Adaptive OOD Detection. (arXiv:2309.11705v1 [cs.LG])

    [http://arxiv.org/abs/2309.11705](http://arxiv.org/abs/2309.11705)

    该论文提出了一种基于元学习的持续适应性OOD检测模型，针对现实世界系统中ID和OOD分布持续变化的问题，能够在部署时动态快速地适应新到达的分布，并且在部署期间ID样本不足。

    

    识别和警报不应被测试或用于进行预测的OOD样本是现代深度学习应用的关键。目前的OOD检测方法在从静态分布中提取的ID和OOD样本上取得了显著进展。然而，当应用于现实世界系统时，这可能是不切实际的，因为ID和OOD分布往往会随着时间的推移发生持续的变化和转移。因此，为了在现实世界系统中有效应用，需要开发能够适应这些动态和演化分布的OOD检测方法是至关重要的。本文提出了一种新颖而更逼真的设置，称为持续适应性OOD检测（CAOOD），旨在开发一种OOD检测模型，能够在部署时动态快速地适应新到达的分布，并且在部署期间ID样本不足。为了解决CAOOD，我们开发了一种基于元学习的方法。

    Out-of-distribution (OOD) detection is crucial to modern deep learning applications by identifying and alerting about the OOD samples that should not be tested or used for making predictions. Current OOD detection methods have made significant progress when in-distribution (ID) and OOD samples are drawn from static distributions. However, this can be unrealistic when applied to real-world systems which often undergo continuous variations and shifts in ID and OOD distributions over time. Therefore, for an effective application in real-world systems, the development of OOD detection methods that can adapt to these dynamic and evolving distributions is essential. In this paper, we propose a novel and more realistic setting called continuously adaptive out-of-distribution (CAOOD) detection which targets on developing an OOD detection model that enables dynamic and quick adaptation to a new arriving distribution, with insufficient ID samples during deployment time. To address CAOOD, we deve
    
[^73]: 鼓励式沟通的联邦赌臂机

    Incentivized Communication for Federated Bandits. (arXiv:2309.11702v1 [cs.LG])

    [http://arxiv.org/abs/2309.11702](http://arxiv.org/abs/2309.11702)

    本论文介绍了一个新的鼓励式通信问题，即针对自利的联邦赌臂机中，服务器通过提供激励来促使客户机分享数据，以提高学习效率和实际操作性。

    

    大多数现有的关于联邦赌臂机的工作都默认所有客户机都愿意在需要的时候将其数据无私地与服务器共享以获取集体利益。尽管这种假设在性能和通信效率上有令人信服的理论保证，但在实践中往往过于理想化，并且经常被违背，特别是当算法在自利的客户机上运行时，这些客户机不愿意在没有明确的好处的情况下分享数据。忽视这种自利行为可能会显著影响联邦赌臂机学习的效率甚至实际可操作性。鉴于此，我们旨在通过正式引入一种鼓励式通信问题来为这个未被充分开发的研究领域带来新的见解，服务器通过提供激励来激励客户机分享数据。在不失一般性的情况下，我们将这个赌臂机问题实例化为上下文线性设置，并提出了第一个解决方案。

    Most existing works on federated bandits take it for granted that all clients are altruistic about sharing their data with the server for the collective good whenever needed. Despite their compelling theoretical guarantee on performance and communication efficiency, this assumption is overly idealistic and oftentimes violated in practice, especially when the algorithm is operated over self-interested clients, who are reluctant to share data without explicit benefits. Negligence of such self-interested behaviors can significantly affect the learning efficiency and even the practical operability of federated bandit learning. In light of this, we aim to spark new insights into this under-explored research area by formally introducing an incentivized communication problem for federated bandits, where the server shall motivate clients to share data by providing incentives. Without loss of generality, we instantiate this bandit problem with the contextual linear setting and propose the first
    
[^74]: 大规模预训练改善了基于主动学习的分子虚拟筛选的样本效率

    Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening. (arXiv:2309.11687v1 [cs.LG])

    [http://arxiv.org/abs/2309.11687](http://arxiv.org/abs/2309.11687)

    本研究探究了在贝叶斯优化的主动学习框架中，预训练的Transformer语言模型和图神经网络在提高分子虚拟筛选样本效率方面的表现。

    

    针对大规模化合物库进行虚拟筛选以寻找潜在的命中候选物在药物发现中是最早的步骤之一。随着商业可得化合物库的规模以指数级增长，使用传统工具如对接进行暴力虚拟筛选在时间和计算资源方面变得不可行。最近，主动学习和贝叶斯优化已被证明是缩小搜索空间的有效方法。这些方法的一个重要组成部分是使用小型库子集进行训练的替代机器学习模型，用于预测化合物的所需特性。准确的模型可以通过仅虚拟筛选整个库的一小部分来实现高样本效率，发现最有前途的化合物。本研究中，我们在贝叶斯优化的主动学习框架中研究了预训练的基于Transformer的语言模型和图神经网络的性能。

    Virtual screening of large compound libraries to identify potential hit candidates is one of the earliest steps in drug discovery. As the size of commercially available compound collections grows exponentially to the scale of billions, brute-force virtual screening using traditional tools such as docking becomes infeasible in terms of time and computational resources. Active learning and Bayesian optimization has recently been proven as effective methods of narrowing down the search space. An essential component in those methods is a surrogate machine learning model that is trained with a small subset of the library to predict the desired properties of compounds. Accurate model can achieve high sample efficiency by finding the most promising compounds with only a fraction of the whole library being virtually screened. In this study, we examined the performance of pretrained transformer-based language model and graph neural network in Bayesian optimization active learning framework. The
    
[^75]: Dr. FERMI：一种基于随机分布鲁棒的公平经验风险最小化框架

    Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])

    [http://arxiv.org/abs/2309.11682](http://arxiv.org/abs/2309.11682)

    本文提出了一个基于随机分布鲁棒的公平性框架，解决了训练和测试数据分布不一致时公平模型表现不准确的问题，并且不需要知道因果图，也支持使用小批量数据。

    

    虽然最近几年已经广泛研究了训练公平机器学习模型的方法，但大多数方法都依赖于训练和测试数据具有相似的分布的假设。在分布发生变化的情况下，公平模型可能在测试数据上表现不公平。为了解决这个问题，已经提出了一些针对分布变化的公平学习方法。然而，大多数现有的解决方案都基于具有描述不同特征交互的因果图的假设。此外，现有的算法需要完全访问数据，不能在使用小批量（随机/批量实现）时使用。本文提出了第一个具有收敛保证的随机分布鲁棒公平性框架，不需要对因果图有任何知识。具体而言，我们将在分布发生变化的情况下的公平推断问题制定为$L_p$-范的分布鲁棒优化问题。

    While training fair machine learning models has been studied extensively in recent years, most developed methods rely on the assumption that the training and test data have similar distributions. In the presence of distribution shifts, fair models may behave unfairly on test data. There have been some developments for fair learning robust to distribution shifts to address this shortcoming. However, most proposed solutions are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. More specifically, we formulate the fair inference in the presence of the distribution shift as a distributionally robust optimization problem under $L_p$ n
    
[^76]: 具有神经图模型的联邦学习

    Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])

    [http://arxiv.org/abs/2309.11680](http://arxiv.org/abs/2309.11680)

    本研究提出了一种名为FedNGMs的联邦学习框架，利用概率神经图模型来处理多个客户端的数据，并在保持训练数据私密性的同时提升模型准确性。

    

    联邦学习（FL）解决了在多个客户端保留对数据的独占控制的同时，基于专有数据创建模型的需求。近期提出的神经图模型（NGMs）是概率图模型，利用神经网络的表达能力学习输入特征之间的复杂非线性依赖关系。它们学会捕捉底层的数据分布，并具有高效的推理和采样算法。我们开发了一个FL框架，它维护一个全局的NGM模型，从本地NGM模型中学习到平均信息，同时保持训练数据在客户端的环境中。我们的设计FedNGMs避免了神经元匹配框架（如联邦匹配平均）中模型参数爆炸的缺点和不足。我们的全局模型大小在整个过程中保持不变。

    Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
    
[^77]: 本文研究了本地音乐推荐中的流行度衰减偏差问题

    Popularity Degradation Bias in Local Music Recommendation. (arXiv:2309.11671v1 [cs.IR])

    [http://arxiv.org/abs/2309.11671](http://arxiv.org/abs/2309.11671)

    本文研究了流行度衰减偏差对本地音乐推荐的影响，发现在推荐流行艺术家方面，权重相关矩阵分解和多项式变分自动编码器表现较好，但对于不太流行的艺术家来说，多项式变分自动编码器的相对性能更好。

    

    本文研究了流行度衰减偏差对本地音乐推荐的影响。具体来说，我们检验了两种最佳推荐算法，即权重相关矩阵分解（WRMF）和多项式变分自动编码器（Mult-VAE）在推荐艺术家时的准确性与艺术家流行度的关系。我们发现，这两个算法在推荐流行艺术家方面具有更好的性能，因此表现出流行度衰减偏差。虽然这两个算法在推荐流行艺术家方面的表现水平相似，但对于不太流行的艺术家来说，Mult-VAE相对表现更好。这表明在本地（长尾）音乐艺术家推荐中，应该优先选择这个算法。

    In this paper, we study the effect of popularity degradation bias in the context of local music recommendations. Specifically, we examine how accurate two top-performing recommendation algorithms, Weight Relevance Matrix Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at recommending artists as a function of artist popularity. We find that both algorithms improve recommendation performance for more popular artists and, as such, exhibit popularity degradation bias. While both algorithms produce a similar level of performance for more popular artists, Mult-VAE shows better relative performance for less popular artists. This suggests that this algorithm should be preferred for local (long-tail) music artist recommendation.
    
[^78]: GLM回归与无意识数据损坏

    GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])

    [http://arxiv.org/abs/2309.11657](http://arxiv.org/abs/2309.11657)

    这篇论文介绍了第一个在广义线性模型回归问题中处理加法无意识噪声的算法。算法的目标是通过样本访问来准确地恢复参数向量，使得模型的预测与真实值的误差尽可能小。

    

    我们展示了在广义线性模型（GLMs）的回归问题中，存在加法无意识噪声的第一个算法。我们假设我们有样本访问到的例子$(x, y)$，其中$y$是$g(w^* \cdot x)$的带噪声测量值。特别地，噪声标签的形式为$y = g(w^* \cdot x) + \xi + \epsilon$，其中$\xi$是与$x$独立抽取的无意识噪声满足$\Pr[\xi = 0] \geq o(1)$，而$\epsilon \sim \mathcal N(0, \sigma^2)$。我们的目标是准确地恢复一个参数向量$w$，使得函数$g(w \cdot x)$与真实值$g(w^* \cdot x)$相比具有任意小的误差，而不是与噪声测量$y$相比。我们提出了一个算法，解决了最一般的与分布无关的情况，其中解可能甚至不可识别。我们的算法返回一个准确的估计，如果它是可识别的，否则

    We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.  We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise
    
[^79]: 高维RBM的漂移控制：基于神经网络的计算方法

    Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks. (arXiv:2309.11651v1 [eess.SY])

    [http://arxiv.org/abs/2309.11651](http://arxiv.org/abs/2309.11651)

    该论文提出了一种基于神经网络的计算方法，用于漂移控制高维RBMs。通过深度神经网络技术，该方法在测试问题上达到了较高的准确性。

    

    受排队理论应用的启发，我们考虑了一个状态空间为d维正半轴的随机控制问题。控制过程Z按照一个反射布朗运动演化，其协方差矩阵是外生指定的，反射方向是从正半轴边界表面反射。系统管理员根据Z的历史选择每个时间点t上的漂移向量θ(t)，而时间点t上的成本率取决于Z(t)和θ(t)。在我们的初始问题表述中，目标是在无限规划时间范围内最小化期望贴现成本，之后我们处理相应的人均控制问题。借鉴韩海亮等人（国家科学院学报，2018, 8505-8510）的早期工作，我们开发并展示了一种基于深度神经网络技术的基于模拟的计算方法。到目前为止，我们研究的测试问题中，我们的方法的精度在一个小数范围内准确。

    Motivated by applications in queueing theory, we consider a stochastic control problem whose state space is the $d$-dimensional positive orthant. The controlled process $Z$ evolves as a reflected Brownian motion whose covariance matrix is exogenously specified, as are its directions of reflection from the orthant's boundary surfaces. A system manager chooses a drift vector $\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem formulation, the objective is to minimize expected discounted cost over an infinite planning horizon, after which we treat the corresponding ergodic control problem. Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction o
    
[^80]: 基于轨道人工智能的自主加油解决方案

    Orbital AI-based Autonomous Refuelling Solution. (arXiv:2309.11648v1 [cs.CV])

    [http://arxiv.org/abs/2309.11648](http://arxiv.org/abs/2309.11648)

    本文介绍了一种基于人工智能的导航算法，旨在通过使用轨道上的可见波长摄像头作为主要传感器，减少对激光雷达的依赖，并大大降低成本。

    

    由于其小型尺寸和低成本的功率、质量和体积，摄像头正迅速成为太空交会的选择机载传感器。然而，在对接方面，它们通常起到次要作用，而主要工作由激光雷达等主动传感器完成。本文介绍了一种提出的基于人工智能的导航算法的开发，旨在使船载可见波长摄像头作为对接和轨道服务的主要传感器成熟起来，减少对激光雷达的依赖并大大降低成本。具体来说，利用人工智能使得相对导航解决方案能够扩展到多种情况，例如目标或照明条件，在传统图像处理方法中，这些情况都必须进行个案处理。在合成生成的数据上对多个卷积神经网络(CNN)骨干架构进行了基准测试。

    Cameras are rapidly becoming the choice for on-board sensors towards space rendezvous due to their small form factor and inexpensive power, mass, and volume costs. When it comes to docking, however, they typically serve a secondary role, whereas the main work is done by active sensors such as lidar. This paper documents the development of a proposed AI-based (artificial intelligence) navigation algorithm intending to mature the use of on-board visible wavelength cameras as a main sensor for docking and on-orbit servicing (OOS), reducing the dependency on lidar and greatly reducing costs. Specifically, the use of AI enables the expansion of the relative navigation solution towards multiple classes of scenarios, e.g., in terms of targets or illumination conditions, which would otherwise have to be crafted on a case-by-case manner using classical image processing methods. Multiple convolutional neural network (CNN) backbone architectures are benchmarked on synthetically generated data of 
    
[^81]: 随机傅里叶特征在去量化量子机器学习中的潜力与局限性

    Potential and limitations of random Fourier features for dequantizing quantum machine learning. (arXiv:2309.11647v1 [quant-ph])

    [http://arxiv.org/abs/2309.11647](http://arxiv.org/abs/2309.11647)

    本文研究了随机傅里叶特征在去量化量子机器学习中的潜力与局限性，并在回归问题上确立了其高效去量化的必要和充分条件，并提出了PQC架构设计建议和识别了潜在量子优势的必要结构。

    

    量子机器学习是近期量子设备最广泛探索的应用之一。目前主要关注的是变分量子机器学习，其中参数化量子电路被用作学习模型。这些参数化量子电路模型具有丰富的结构，因此可能通过随机傅里叶特征进行高效的去量化。本文在回归问题上确立了随机傅里叶特征在变分量子机器学习中提供高效去量化的必要和充分条件。利用这些结果，我们提出了具体的参数化量子电路架构设计建议，以及识别了在回归问题中取得潜在量子优势的必要结构。

    Quantum machine learning is arguably one of the most explored applications of near-term quantum devices. Much focus has been put on notions of variational quantum machine learning where parameterized quantum circuits (PQCs) are used as learning models. These PQC models have a rich structure which suggests that they might be amenable to efficient dequantization via random Fourier features (RFF). In this work, we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization.
    
[^82]: 早期诊断自闭症谱系障碍的机器学习方法

    Early diagnosis of autism spectrum disorder using machine learning approaches. (arXiv:2309.11646v1 [cs.LG])

    [http://arxiv.org/abs/2309.11646](http://arxiv.org/abs/2309.11646)

    本文利用机器学习方法旨在提供自闭症谱系障碍(ASD)的早期诊断，并通过研究不同算法寻找最显著的特征，自动化诊断过程。

    

    自闭症谱系障碍(ASD)是一种神经系统疾病，表现为社交互动困难、语言沟通困难和重复行为。这些困难的严重程度各不相同，被诊断为ASD的人面临着独特的挑战。尽早识别和处理ASD可以促进该疾病的改善。近年来，机器学习驱动的智能诊断作为传统临床方法的补充出现，旨在解决传统方法耗时且昂贵的潜在缺点。本文利用不同的机器学习算法寻找ASD的最显著特征并自动化诊断过程。我们研究了六种分类模型，以找到最适合识别ASD的模型，同时还研究了五种流行的聚类方法，以对这些ASD数据集进行有意义的洞察。

    Autistic Spectrum Disorder (ASD) is a neurological disease characterized by difficulties with social interaction, communication, and repetitive activities. The severity of these difficulties varies, and those with this diagnosis face unique challenges. While its primary origin lies in genetics, identifying and addressing it early can contribute to the enhancement of the condition. In recent years, machine learning-driven intelligent diagnosis has emerged as a supplement to conventional clinical approaches, aiming to address the potential drawbacks of time-consuming and costly traditional methods. In this work, we utilize different machine learning algorithms to find the most significant traits responsible for ASD and to automate the diagnostic process. We study six classification models to see which model works best to identify ASD and also study five popular clustering methods to get a meaningful insight of these ASD datasets. To find the best classifier for these binary datasets, we 
    
[^83]: 结构组件设计的潜在扩散模型

    Latent Diffusion Models for Structural Component Design. (arXiv:2309.11601v1 [cs.LG])

    [http://arxiv.org/abs/2309.11601](http://arxiv.org/abs/2309.11601)

    本文提出了一个潜在扩散模型的框架，用于生成符合加载条件的结构组件设计。与其他生成方法相比，该方法允许对现有设计进行编辑，并且具有近优性。实验结果证明了生成设计的结构性能和潜在候选设计的可变性。

    

    最近生成模型，尤其是扩散模型的进展，已经在生成模型领域带来了革命性变化，实现了符合用户需求的高质量图像生成。本文提出了一个框架，用于生成结构组件的设计。具体而言，我们采用潜在扩散模型生成满足一组具体加载条件的组件潜在设计。相比于其他生成方法如生成对抗网络（GAN），我们的方法的一个明显优势是可以编辑现有设计。我们使用使用SIMP算法得到的结构拓扑优化几何数据集训练模型，因此我们的框架生成的设计具有固有的近优性。我们的工作提供了支持生成设计结构性能和潜在候选设计的可变性的定量结果。此外，我们提供了框架的可扩展性证据。

    Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches, such as generative adversarial networks (GANs), is that it permits the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability 
    
[^84]: CATS: 基于深度学习方法的条件对抗轨迹合成，用于隐私保护轨迹数据发布

    CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches. (arXiv:2309.11587v1 [cs.LG])

    [http://arxiv.org/abs/2309.11587](http://arxiv.org/abs/2309.11587)

    CATS是一种基于深度学习的地理人工智能方法，用于隐私保护轨迹数据的生成和发布。它采用K-匿名技术保障了分布级隐私，通过条件对抗训练和循环二部图匹配等方法实现了高质量轨迹数据的合成和重构。

    

    随着无处不在的定位感知设备和移动互联网的普及，我们能够从用户那里收集大规模的个体级轨迹数据集。这些轨迹大数据为人类移动性研究带来了新的机遇，但也引发了关于位置隐私的公众关切。在这项工作中，我们提出了一种基于深度学习的地理人工智能方法论框架，名为 Conditional Adversarial Trajectory Synthesis (CATS)，用于隐私保护轨迹数据的生成和发布。CATS 将 K-匿名应用于人类移动性的时空分布，提供了强大的分布级隐私保障。通过利用条件对抗训练技术、基于注意力机制的轨迹全局上下文学习，以及相邻轨迹点的循环二部图匹配，CATS 能够从条件采样位置中重构轨迹拓扑，并生成高质量的个体轨迹数据。

    The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-
    
[^85]: 从安全基准中提炼对抗性提示：对对抗性Nibbler挑战报告

    Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])

    [http://arxiv.org/abs/2309.11575](http://arxiv.org/abs/2309.11575)

    本研究为Adversarial Nibbler挑战提供了一个大型的潜在对抗输入集合，并通过对提示和图像的分析揭示了当前生成图像模型中的系统性安全问题。

    

    最近，基于文本的图像生成模型取得了惊人的图像质量和对齐结果。因此，它们被应用于越来越多的应用程序中。由于这些模型高度依赖于从网络随机爬取的数十亿个数据集，它们也会产生不安全的内容。作为对Adversarial Nibbler挑战的贡献，我们从现有的安全基准中提炼了一组超过1000个潜在的对抗输入。我们对收集到的提示和相应的图像进行分析，展示了输入过滤器的脆弱性，并进一步揭示了当前生成图像模型中的系统性安全问题。

    Text-conditioned image generation models have recently achieved astonishing image quality and alignment results. Consequently, they are employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. As a contribution to the Adversarial Nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
    
[^86]: BTLM-3B-8K: 一个3B参数模型中使用7B参数性能的研究

    BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])

    [http://arxiv.org/abs/2309.11568](http://arxiv.org/abs/2309.11568)

    BTLM-3B-8K是一个30亿参数的开源语言模型，相对于其他30亿和70亿参数模型，它在下游任务中表现出2-5.5%的性能提升，同时在长文本任务上也具有出色的表现。这种将70亿参数的模型压缩到30亿参数，并且性能几乎没有受到影响的方法具有重要意义。

    

    我们介绍了Bittensor语言模型, 名为"BTLM-3B-8K", 这是一个新的、拥有30亿参数的开源语言模型. BTLM-3B-8K在SlimPajama数据集上进行了训练，训练数据为627B个token，采用了2048和8192的混合上下文长度. BTLM-3B-8K在下游任务中的表现比所有现有的30亿参数模型提高了2-5.5% ，甚至与一些70亿参数模型相媲美. 另外，BTLM-3B-8K在长文本上的表现也很好，在长度为8192的任务上超过了MPT-7B-8K和XGen-7B-8K. 我们在清理和去重的SlimPajama数据集上训练了模型，对µP超参数和调度进行了调优，使用了ALiBi位置嵌入和SwiGLU非线性. 在Hugging Face上，最受欢迎的模型是70亿参数，这表明用户更倾向于质量大小比为70亿参数的模型. 将70亿参数模型压缩为30亿参数，性能几乎没有影响，这是一个重要的里程碑.

    We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.  On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone
    
[^87]: 带有自然语言子目标的层次强化学习

    Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])

    [http://arxiv.org/abs/2309.11564](http://arxiv.org/abs/2309.11564)

    本文介绍了一种层次强化学习的新方法，使用来自人类解决任务的数据来监督一组长程任务的目标空间，并使用自然语言来描述这个空间，该方法在克隆专家行为的代理和无监督子目标空间的层次强化学习中表现出色。

    

    层次强化学习一直是一种实现长序列动作目标导向行为的有吸引力的方法。然而，在现实或开放环境中实现层次强化学习是具有挑战性的。主要的挑战之一是找到适合实例化层次的子目标空间。我们提出了一种新方法，利用人类解决这些任务的数据，对3D躯体环境中一组长程任务的目标空间进行软监督。特别是，我们使用非约束的自然语言来参数化这个空间。这有两个优点：首先，可以从天真的人类参与者那里轻松生成这些数据；其次，它足够灵活，能够表示人类相关任务中的一大范围子目标。我们的方法在这些任务上优于克隆专家行为的代理和没有这种受监督子目标空间的从头开始的层次强化学习方法。我们的工作提出了一种将人类专家监督与这种受益相结合的新方法。

    Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefi
    
[^88]: EPTQ:通过无标签Hessian增强的后训练量化

    EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. (arXiv:2309.11531v1 [cs.CV])

    [http://arxiv.org/abs/2309.11531](http://arxiv.org/abs/2309.11531)

    本文提出了一种名为EPTQ的增强后训练量化方法，该方法通过自适应加权层和无标签Hessian近似技术实现了最先进的结果。

    

    深度神经网络的量化已成为将这些网络嵌入到最终用户设备上的关键要素。然而，当前的量化方法通常会导致准确性严重下降。本文提出了一种名为EPTQ的增强后训练量化方法。该方法基于知识蒸馏，并采用自适应加权层的方式。此外，我们提出了一种新的无标签Hessian近似技术，名为Label-Free Hessian。这种技术消除了计算Hessian所需的标记数据集的要求。自适应知识蒸馏利用Label-Free Hessian技术，在进行优化时更加关注模型的敏感部分。通过使用EPTQ，我们在各种模型、任务和数据集上实现了最先进的结果，包括ImageNet分类、COCO目标检测和用于语义分割的Pascal-VOC数据集。

    Quantization of deep neural networks (DNN) has become a key element in the efforts of embedding such networks on end-user devices. However, current quantization methods usually suffer from costly accuracy degradation. In this paper, we propose a new method for Enhanced Post Training Quantization named EPTQ. The method is based on knowledge distillation with an adaptive weighting of layers. In addition, we introduce a new label-free technique for approximating the Hessian trace of the task loss, named Label-Free Hessian. This technique removes the requirement of a labeled dataset for computing the Hessian. The adaptive knowledge distillation uses the Label-Free Hessian technique to give greater attention to the sensitive parts of the model while performing the optimization. Empirically, by employing EPTQ we achieve state-of-the-art results on a wide variety of models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.
    
[^89]: TrueLearn: 一种用于个性化信息推荐的Python库（带有（隐式）反馈）

    TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])

    [http://arxiv.org/abs/2309.11527](http://arxiv.org/abs/2309.11527)

    TrueLearn是一个Python库，用于构建个性化的信息推荐系统，并提供了丰富的文档和编码示例，可帮助开发人员和从业者使用。它采用了开放学习者的概念和人性化的用户表达方式，同时支持用户可视化和模型性能评估。

    

    本文介绍了TrueLearn Python库，其中包含一组在线学习贝叶斯模型，用于构建教育（或更一般地说，信息）推荐系统。这组模型是根据“开放学习者”的概念设计的，使用直观的用户表达。为了可解释性和让用户有控制感，TrueLearn库还包含不同的表示形式，以帮助最终用户可视化学习者模型，这可能有助于将来用户与自己的模型进行交互。与该库一起，我们还提供了一个先前公开发布的隐式反馈教育数据集和评估指标，以衡量模型的性能。丰富的文档和编码示例使该库对机器学习开发人员和教育数据挖掘和学习分析从业者都非常易于使用。该库和带有示例的支持文档可在https：//获得。

    This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https:/
    
[^90]: 基于似然的物联网系统中专家支持的分布式学习算法中传感器校准的研究

    Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])

    [http://arxiv.org/abs/2309.11526](http://arxiv.org/abs/2309.11526)

    该论文介绍了一种基于似然的传感器校准方法，可以在物联网系统中实现专家支持的分布式学习算法。通过对模拟和实际测量数据的评估，证明了该方法的有效性和改进效果。

    

    传感器技术领域中的一个重要任务是将一个传感器的测量结果高效地适应到另一个具有相同设计的传感器。一种想法是使用不同系统之间的仿射变换估计，这可以通过专家的知识进行改进。本文介绍了Glacier Research在1973年发表的改进解决方案，并展示了该解决方案可以用于传感器的软件校准、基于专家的适应和联邦学习方法。我们通过模拟和实际测量数据对我们的研究进行了评估，实验中使用了一个具有8个相同传感器的多传感器板。结果表明，无论是模拟还是实验数据，都得到了改进。

    An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
    
[^91]: 在内容市场中的离线学习下的广告负载平衡

    Ad-load Balancing via Off-policy Learning in a Content Marketplace. (arXiv:2309.11518v1 [cs.IR])

    [http://arxiv.org/abs/2309.11518](http://arxiv.org/abs/2309.11518)

    本文介绍了一个使用离线学习和强化学习反馈的方法来解决在线广告系统中的广告负载平衡问题，该方法能够适应用户偏好和上下文因素的变化，并最大化用户参与度和收入。

    

    广告负载平衡是在线广告系统中的一个关键挑战，尤其在社交媒体平台的背景下，目标是在保持用户体验的同时最大化用户参与度和收入。传统的广告负载平衡方法依赖于静态分配策略，无法适应用户偏好和上下文因素的变化。本文提出了一种利用离线学习和依据记录的强化学习反馈的方法。我们首先进行了广告负载平衡问题的分析，强调了用户满意度和广告收入之间的冲突目标。我们强调了由于用户异质性和用户在会话中的位置的依赖性而引起的细微差别。基于这个分析，我们定义了在特定的内容获取中确定最优广告负载的问题。

    Ad-load balancing is a critical challenge in online advertising systems, particularly in the context of social media platforms, where the goal is to maximize user engagement and revenue while maintaining a satisfactory user experience. This requires the optimization of conflicting objectives, such as user satisfaction and ads revenue. Traditional approaches to ad-load balancing rely on static allocation policies, which fail to adapt to changing user preferences and contextual factors. In this paper, we present an approach that leverages off-policy learning and evaluation from logged bandit feedback. We start by presenting a motivating analysis of the ad-load balancing problem, highlighting the conflicting objectives between user satisfaction and ads revenue. We emphasize the nuances that arise due to user heterogeneity and the dependence on the user's position within a session. Based on this analysis, we define the problem as determining the optimal ad-load for a particular feed fetch.
    
[^92]: 具有公共项目特征的私有矩阵分解

    Private Matrix Factorization with Public Item Features. (arXiv:2309.11516v1 [cs.IR])

    [http://arxiv.org/abs/2309.11516](http://arxiv.org/abs/2309.11516)

    这项研究提出了一种使用公共项目特征进行私有矩阵分解的方法，以缓解差分隐私训练对推荐质量的影响，并展示了这种方法的简单性、易调整性和可扩展性。

    

    我们考虑在具有公共项目特征的情况下训练私有推荐模型的问题。使用差分隐私（DP）训练可以提供强大的隐私保障，但会导致推荐质量下降。我们展示出在训练过程中加入公共项目特征可以帮助缓解推荐质量下降的问题。我们提出了一种基于集体矩阵分解（CMF）的通用方法，通过同时对两个矩阵进行分解：用户反馈矩阵（代表敏感数据）和一个编码公开可用（非敏感）项目信息的项目特征矩阵。这种方法在概念上简单，易于调整，而且具有高度的可扩展性。它可以应用于不同类型的公共项目数据，包括：（1）分类项目特征；（2）从公共来源学习的项目间相似性；以及（3）公开可用的用户反馈。此外，这些数据模态可以共同利用，以充分利用公共数据。

    We consider the problem of training private recommendation models with access to public item features. Training with Differential Privacy (DP) offers strong privacy guarantees, at the expense of loss in recommendation quality. We show that incorporating public item features during training can help mitigate this loss in quality. We propose a general approach based on collective matrix factorization (CMF), that works by simultaneously factorizing two matrices: the user feedback matrix (representing sensitive data) and an item feature matrix that encodes publicly available (non-sensitive) item information.  The method is conceptually simple, easy to tune, and highly scalable. It can be applied to different types of public item data, including: (1) categorical item features; (2) item-item similarities learned from public sources; and (3) publicly available user feedback. Furthermore, these data modalities can be collectively utilized to fully leverage public data.  Evaluating our method o
    
[^93]: 运用噪声图神经网络方法实现差分隐私的顺序推荐研究

    Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])

    [http://arxiv.org/abs/2309.11515](http://arxiv.org/abs/2309.11515)

    这项工作提出了一种新颖的差分隐私顺序推荐框架，采用了噪声图神经网络方法，解决了现有差分隐私推荐系统在动态和依赖关系方面的局限性，同时也关注了敏感用户特征的隐私风险。

    

    随着各种在线平台中高调的隐私泄露事件频繁发生，用户对隐私越来越关注。推荐系统作为在线平台提供个性化服务的核心组件，其隐私保护引起了极大的关注。作为隐私保护的黄金标准，差分隐私已被广泛应用于推荐系统中的隐私保护。然而，现有的差分隐私推荐系统只考虑静态和独立的用户交互，因此无法应用于具有动态和依赖关系的顺序推荐。同时，对于敏感用户特征的隐私风险关注较少，大多数只保护用户的反馈数据。在这项工作中，我们提出了一个新颖的差分隐私顺序推荐框架，采用了噪声图神经网络方法（称为DIPSGNN）来解决这些局限性。

    With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, 
    
[^94]: 使用融合的家庭调查在精细空间尺度上评估美国家庭的多维福祉：fusionACS.

    Multidimensional well-being of US households at a fine spatial scale using fused household surveys: fusionACS. (arXiv:2309.11512v1 [stat.AP])

    [http://arxiv.org/abs/2309.11512](http://arxiv.org/abs/2309.11512)

    该论文介绍了fusionACS项目，通过将多个美国家庭调查的数据进行融合，提供了一个能够分析家庭属性和福祉维度的综合微数据集，从而使研究者可以回答更加多样化的研究问题。

    

    社会科学常常依赖于家庭和个体的调查。美国政府经常进行数十个这样的调查。然而，它们是独立的、不相关的样本，并且有专门的问题，限制了只能通过一个调查来回答研究问题。fusionACS项目旨在通过统计学上将“供体”调查的变量与美国人社区调查 (ACS) 的微观数据进行“融合”，从而整合多个美国家庭调查的数据。这产生了一个综合的家庭属性和福祉维度的微数据集，可以用于分析研究问题，这是目前无法实现的。所呈现的数据包括对2015年住宅能源消费调查(RECS)、2017年全国家庭交通调查(NHTS)、2019年美国住房调查(AHS)和2015-2019年消费者支出调查-访谈(CEI)的选择供体变量进行融合到ACS的结果。

    Social science often relies on surveys of households and individuals. Dozens of such surveys are regularly administered by the U.S. government. However, they field independent, unconnected samples with specialized questions, limiting research questions to those that can be answered by a single survey. The fusionACS project seeks to integrate data from multiple U.S. household surveys by statistically "fusing" variables from "donor" surveys onto American Community Survey (ACS) microdata. This results in an integrated microdataset of household attributes and well-being dimensions that can be analyzed to address research questions in ways that are not currently possible. The presented data comprise the fusion onto the ACS of select donor variables from the Residential Energy Consumption Survey (RECS) of 2015, the National Household Transportation Survey (NHTS) of 2017, the American Housing Survey (AHS) of 2019, and the Consumer Expenditure Survey - Interview (CEI) for the years 2015-2019. 
    
[^95]: 使用因果推断避免数据驱动参数分析的问题：在建筑、工程和施工行业的一个案例研究

    Using causal inference to avoid fallouts in data-driven parametric analysis: a case study in the architecture, engineering, and construction industry. (arXiv:2309.11509v1 [cs.CE])

    [http://arxiv.org/abs/2309.11509](http://arxiv.org/abs/2309.11509)

    本研究使用了一个建筑能源消耗案例研究，展示了在数据驱动模型中进行因果分析的必要性，发现数据驱动模型的准确性评估和领域知识筛选无法排除偏倚结果，因此在选择特征时需要仔细考虑因果关系，而因果分析的结果可以帮助设计和参数检查，避免认知偏差。

    

    实际实施中的决策过程受到对数据驱动模型越来越多的依赖的影响。我们研究了数据驱动方法、经验领域知识和第一原理模拟之间的协同模式。我们展示了在没有因果分析的情况下使用数据驱动模型可能导致偏倚结果的潜在风险。通过对一栋建筑的多个设计方案对能源消耗的影响进行案例研究，我们证明了在数据驱动建模过程中进行因果分析的必要性。我们得出以下结论：(a) 数据驱动模型的准确性评估或领域知识筛选可能无法排除偏倚和虚假结果；(b) 数据驱动模型的特征选择应该仔细考虑因果关系，尤其是协变量；(c) 因果分析的结果可以作为第一原理模拟设计和参数检查的辅助工具，以避免认知偏差。我们证明了因果分析的好处。

    The decision-making process in real-world implementations has been affected by a growing reliance on data-driven models. We investigated the synergetic pattern between the data-driven methods, empirical domain knowledge, and first-principles simulations. We showed the potential risk of biased results when using data-driven models without causal analysis. Using a case study assessing the implication of several design solutions on the energy consumption of a building, we proved the necessity of causal analysis during the data-driven modeling process. We concluded that: (a) Data-driven models' accuracy assessment or domain knowledge screening may not rule out biased and spurious results; (b) Data-driven models' feature selection should involve careful consideration of causal relationships, especially colliders; (c) Causal analysis results can be used as an aid to first-principles simulation design and parameter checking to avoid cognitive biases. We proved the benefits of causal analysis 
    
[^96]: AdBooster：使用稳定扩散外景生成的个性化广告创意生成

    AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting. (arXiv:2309.11507v1 [cs.IR])

    [http://arxiv.org/abs/2309.11507](http://arxiv.org/abs/2309.11507)

    AdBooster通过使用稳定扩散外景架构和生成模型，实现了个性化广告创作的创意优化，其有效性得到了实验证明。

    

    在数字广告中，最佳推荐项目（推荐项）的选择和最佳创意展示（创意优化）传统上被认为是不同的学科。然而，两者对用户满意度都有显著贡献，这是基于我们的假设，即它们依赖于项目的相关性和展示方式，尤其是在视觉创意的情况下。为此，我们引入了“生成创意优化（GCO）”的任务，该任务提出使用生成模型进行创意生成，这些模型可以结合用户的兴趣，以及基于稳定扩散外景架构的“AdBooster”，这是一个用于个性化广告创意的模型。该模型在微调和生成时都可以独特地结合用户的兴趣。为了进一步提高AdBooster的性能，我们还引入了一个自动数据增强流程。通过在模拟数据上进行实验，我们验证了AdBooster在生成更多精彩创意方面的有效性。

    In digital advertising, the selection of the optimal item (recommendation) and its best creative presentation (creative optimization) have traditionally been considered separate disciplines. However, both contribute significantly to user satisfaction, underpinning our assumption that it relies on both an item's relevance and its presentation, particularly in the case of visual creatives. In response, we introduce the task of {\itshape Generative Creative Optimization (GCO)}, which proposes the use of generative models for creative generation that incorporate user interests, and {\itshape AdBooster}, a model for personalized ad creatives based on the Stable Diffusion outpainting architecture. This model uniquely incorporates user interests both during fine-tuning and at generation time. To further improve AdBooster's performance, we also introduce an automated data augmentation pipeline. Through our experiments on simulated data, we validate AdBooster's effectiveness in generating more 
    
[^97]: 公平性与个性化：迈向认知效用的公正

    Fairness Vs. Personalization: Towards Equity in Epistemic Utility. (arXiv:2309.11503v1 [cs.IR])

    [http://arxiv.org/abs/2309.11503](http://arxiv.org/abs/2309.11503)

    该论文研究了个性化推荐系统中公平性与个性化之间的困境，并提出了在认知效用背景下实现公正的公平的解决方案。

    

    个性化推荐系统的应用范围正在迅速扩展，涵盖社交媒体、在线购物、搜索引擎结果等领域。这些系统提供了一种更高效的方式来浏览大量可用的物品。然而，随着发展，人们对算法系统可能存在并延续偏见的潜力有了更多认识，存在个性化领域中的不公平风险。在本研究中，我们阐明了个性化与传统公平实现之间固有的紧张关系。作为一种替代方案，我们提出了在认知效用背景下实现公正的公平。我们提供了目标和实际实现之间的映射，并详细介绍了关键利益相关者的政策建议，以在个性化系统中实现公平。

    The applications of personalized recommender systems are rapidly expanding: encompassing social media, online shopping, search engine results, and more. These systems offer a more efficient way to navigate the vast array of items available. However, alongside this growth, there has been increased recognition of the potential for algorithmic systems to exhibit and perpetuate biases, risking unfairness in personalized domains. In this work, we explicate the inherent tension between personalization and conventional implementations of fairness. As an alternative, we propose equity to achieve fairness in the context of epistemic utility. We provide a mapping between goals and practical implementations and detail policy recommendations across key stakeholders to forge a path towards achieving fairness in personalized systems.
    
[^98]: Text2Reward：针对强化学习的自动生成密集奖励函数的自动化框架

    Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])

    [http://arxiv.org/abs/2309.11489](http://arxiv.org/abs/2309.11489)

    Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。

    

    设计奖励函数是强化学习中长期以来的挑战；它需要专业知识或领域数据，导致开发成本高。为了解决这个问题，我们引入了Text2Reward，一个无需数据的框架，可基于大型语言模型（LLM）自动生成密集奖励函数。给定自然语言描述的目标，Text2Reward生成作为环境紧凑表示的可执行程序的密集奖励函数。与逆强化学习和最近使用LLM编写稀疏奖励代码的工作不同，Text2Reward生成可解释的、自由形式的密集奖励代码，可涵盖各种任务，利用现有软件包，并允许通过人类反馈进行迭代改进。我们在两个机器人操作基准（ManiSkill2，MetaWorld）和两个MuJoCo的运动环境上评估了Text2Reward。在17个操作任务中的13个任务中，使用生成的奖励代码训练的政策实现了类似或更好的性能。

    Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
    
[^99]: 自监督学习揭示了街景图像中城市住房的变化

    Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v1 [cs.CV])

    [http://arxiv.org/abs/2309.11354](http://arxiv.org/abs/2309.11354)

    本论文使用自监督学习方法分析伦敦的城市变化，通过应用于1500万张街景图像，成功地识别出了住房供应的变化，并区分了主要和次要变化。

    

    全球各地的城市都面临着可负担和体面住房严重短缺的问题。尽管这对政策至关重要，但我们在有效监测和追踪城市住房进展方面的能力有限。应用于街景图像的基于深度学习的计算机视觉方法在测量社会经济和环境不平等方面取得了成功，但由于时变标签通常不可用，它们并没有充分利用时间图像来跟踪城市变化。我们使用自监督方法在2008年至2021年之间使用1500万张伦敦街景图像来测量变化。我们对Barlow Twins的新颖改进Street2Vec，在不需要手动注释的情况下，嵌入了城市结构，并对季节性和日常变化具有不变性。它优于通用嵌入，成功地从街景图像中识别了伦敦住房供应的点级变化，并区分了主要和次要变化。这种能力可以为城市规划提供及时信息。

    Cities around the world face a critical shortage of affordable and decent housing. Despite its critical importance for policy, our ability to effectively monitor and track progress in urban housing is limited. Deep learning-based computer vision methods applied to street-level images have been successful in the measurement of socioeconomic and environmental inequalities but did not fully utilize temporal images to track urban change as time-varying labels are often unavailable. We used self-supervised methods to measure change in London using 15 million street images taken between 2008 and 2021. Our novel adaptation of Barlow Twins, Street2Vec, embeds urban structure while being invariant to seasonal and daily changes without manual annotations. It outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change. This capability can provide timely information for urban plann
    
[^100]: 对开放世界深度伪造归因的对比伪学习

    Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2309.11132](http://arxiv.org/abs/2309.11132)

    这项研究提出了一种对开放世界深度伪造归因任务的新框架，并引入了一个评估归因性能的新基准。该框架通过引入全局-局部投票模块和设计置信度-based的软伪标签策略来提高归因准确性，并缓解相似造成的伪噪声。

    

    由于生成技术的快速发展，为伪造面部进行归因的挑战引起了广泛关注。尽管最近的许多研究已经在GAN生成的面部方面迈出了重要的一步，但与身份交换或表情转移相关的更具威胁性的攻击仍然被忽视。而在开放世界的未标记面部中隐藏的伪造痕迹仍然没有得到充分的探索。为了推动相关的前沿研究，我们引入了一个名为Open-World DeepFake Attribution (OW-DFA)的新基准，旨在评估在开放世界场景下对各种类型伪造面部的归因性能。与此同时，我们提出了一种名为对比伪学习(Contrastive Pseudo Learning, CPL)的新框架，用于OW-DFA任务，通过1)引入全局-局部投票模块来引导不同操纵区域的伪造面部特征对齐，2)设计一种基于置信度的软伪标签策略来缓解由相似造成的伪噪声。

    The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or expression transferring are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces still remain under-explored. To push the related frontier research, we introduce a new benchmark called Open-World DeepFake Attribution (OW-DFA), which aims to evaluate attribution performance against various types of fake faces under open-world scenarios. Meanwhile, we propose a novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task through 1) introducing a Global-Local Voting module to guide the feature alignment of forged faces with different manipulated regions, 2) designing a Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused by simi
    
[^101]: Fake News BR: 一种用于巴西葡萄牙语的假新闻检测平台

    Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])

    [http://arxiv.org/abs/2309.11052](http://arxiv.org/abs/2309.11052)

    本研究提出了一个用于巴西葡萄牙语的假新闻检测平台，采用机器学习和自然语言处理技术，能够高效准确地识别假新闻，同时提供实时分析和验证新闻文章真实性的用户友好平台。

    

    由于假新闻传播误导公众舆论的潜力，其传播已成为近期关注的一个重要问题。本文对巴西葡萄牙语中的假新闻检测进行了全面的研究，重点关注新闻类型。我们提出了一种基于机器学习的方法，利用自然语言处理技术，包括TF-IDF和Word2Vec，从文本数据中提取特征。我们评估了各种分类算法的性能，如逻辑回归、支持向量机、随机森林、AdaBoost和LightGBM，使用包含真实和假新闻文章的数据集。所提出的方法在准确率和F1得分上都取得了高水平，证明了其识别假新闻的有效性。此外，我们开发了一个用户友好的网站平台FAKENEWSBR.COM，以便验证新闻文章的真实性。我们的平台提供实时分析，允许用户检查新闻文章的真实性。

    The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. In this paper, we present a comprehensive study on the detection of fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves a high level of accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we develop a user-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to 
    
[^102]: 通过学习表示和影响函数，我们能从对抗样本中获得什么信息

    What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])

    [http://arxiv.org/abs/2309.10916](http://arxiv.org/abs/2309.10916)

    本文将图像处理领域中的对抗子空间技术应用于自然语言处理，提出了基于最近邻和影响函数的检测器，并通过使用影响函数揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系和任务差异。

    

    对抗样本是通过微小扰动来欺骗深度神经网络的，起初在图像处理领域进行研究，最近在自然语言处理领域也开始关注。尽管在自然语言处理中检测对抗样本的方法主要依赖于输入扰动的搜索，但图像处理领域已经发展出一系列技术来表征学习表示中的对抗子空间。本文将这两种方法应用于自然语言处理，一种基于最近邻和影响函数，一种基于马氏距离。特别是前者相比几个强基准产生了最先进的检测器；此外，对影响函数的新颖使用揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系，并展示了它们根据不同自然语言处理任务的差异。

    Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
    
[^103]: 关于随机梯度下降的不同模式

    On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])

    [http://arxiv.org/abs/2309.10688](http://arxiv.org/abs/2309.10688)

    这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。

    

    现代深度网络通过随机梯度下降（SGD）进行训练，其关键参数是每个步骤考虑的数据量或批量大小B以及步长或学习率η。对于小的B和大的η，SGD对应于参数的随机演化，其噪声幅度由“温度”T=η/B控制。然而当批量大小B≥B^*足够大时，这种描述被观察到失效，或者在温度足够小时简化为梯度下降（GD）。理解这些交叉发生的位置仍然是一个中心挑战。在这里，我们解决了这些问题，在一个教师-学生感知器分类模型中，我们展示了我们的关键预测仍适用于深度网络。具体来说，我们在B-η平面上获得了一个相位图，将三个动态阶段分开：（i）受温度控制的噪声主导的SGD，（ii）大步骤主导的SGD和

    Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
    
[^104]: 使用扩散模型学习端到端信道编码

    Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])

    [http://arxiv.org/abs/2309.10505](http://arxiv.org/abs/2309.10505)

    本文提出了使用扩散模型学习端到端信道编码的框架，并通过模拟实验证明了扩散模型能够准确学习信道分布从而实现接近最优的端到端符号误码率。

    

    通过扩散模型近似信道分布，本文提出了一个基于扩散模型的端到端信道编码框架，并提出了一种高效的训练算法。通过与各种信道模型的模拟实验，验证了扩散模型精确学习信道分布的能力，从而实现了接近最优的端到端符号误码率（SER）。

    The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
    
[^105]: Crowdotic：基于Transformer的医院候诊室非语音音频与差分隐私的占用预测

    Crowdotic: Transformer-based Occupancy Estimation for Hospital Waiting Rooms with Non-speech Audio and Differential Privacy. (arXiv:2309.10280v1 [cs.SD])

    [http://arxiv.org/abs/2309.10280](http://arxiv.org/abs/2309.10280)

    本研究提出了一种基于非语音音频的人群分析方法，利用Transformer模型实现医院候诊室的占用预测，并且在准确性方面表现出色。这是首次提出使用非语音音频信号进行占用预测的方法，超过了其他基线方法。

    

    隐私保护的人群密度分析在各种场景中应用广泛，在提高智能建筑运营管理的同时，保持了不同空间隐私的期望。我们提出了一种基于非语音音频的人群分析方法，利用了Transformer模型。我们的结果表明，仅通过非语音音频就可以实现这种分析，而且准确性十分出色。据我们所知，这是第一次提出使用非语音音频信号来预测占用情况。截至目前，我们不知道有其他类似的方法。为了实现这一目标，我们在一家大型医院的候诊室中部署了基于传感器的平台，并获得了IRB批准，在数个月的时间里捕获了非语音音频和热像图以用于模型的训练和评估。我们提出的基于非语音的方法在性能上超过了基于热像摄像头的模型和其他所有基线方法。

    Privacy-preserving crowd density analysis finds application across a wide range of scenarios, substantially enhancing smart building operation and management while upholding privacy expectations in various spaces. We propose a non-speech audio-based approach for crowd analytics, leveraging a transformer-based model. Our results demonstrate that non-speech audio alone can be used to conduct such analysis with remarkable accuracy. To the best of our knowledge, this is the first time when non-speech audio signals are proposed for predicting occupancy. As far as we know, there has been no other similar approach of its kind prior to this. To accomplish this, we deployed our sensor-based platform in the waiting room of a large hospital with IRB approval over a period of several months to capture non-speech audio and thermal images for the training and evaluation of our models. The proposed non-speech-based approach outperformed the thermal camera-based model and all other baselines. In addit
    
[^106]: FedGKD:在联邦图神经网络中释放协作的力量

    FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09517](http://arxiv.org/abs/2309.09517)

    FedGKD是一种新颖的联邦图神经网络框架，通过利用客户端图数据集蒸馏方法提取更好的任务特征并引入感知全局协作结构的服务器端聚合机制，解决了联邦GNN系统中图异构性问题，提高了效率和准确性。

    

    最近几年来，由于联邦图神经网络（GNN）能够在数据隔离场景下执行与图相关的任务并保护数据隐私，联邦训练已经变得流行起来。然而，联邦GNN系统中的图异构性问题仍然存在挑战。现有的框架通过使用不同的统计量来表示局部任务，并通过简单的聚合机制将它们联系起来来解决这个问题。然而，这些方法在两个方面都效率有限：任务相关性量化的质量低和利用协作结构的无效性。为了解决这些问题，我们提出了FedGKD，一种新颖的联邦GNN框架，它利用一种新颖的客户端图数据集蒸馏方法提取更好地描述任务相关性的任务特征，并引入一个新颖的服务器端聚合机制，该机制能够感知到全局的协作结构。我们在六个真实世界的数据集上进行了大量实验证明了FedGKD框架的有效性。

    Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
    
[^107]: Drifter: 大规模推荐系统中高效的在线特征监控以提高数据完整性

    Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v1 [cs.IR])

    [http://arxiv.org/abs/2309.08617](http://arxiv.org/abs/2309.08617)

    Drifter是一个高效的在线特征监控系统，通过敏捷、响应和适应性的数据质量监控，实时分析、检测和解决推荐系统中的数据问题，使得实时推荐系统的可靠性和性能得到显著提升。

    

    实际生产系统通常面临在大规模、动态流中维护数据质量的问题。我们介绍了Drifter，这是一个用于推荐系统中在线特征监控和验证的高效且轻量级的系统。Drifter通过提供敏捷、响应和适应性的数据质量监控，能够实时进行根本原因分析、漂移检测以及对有问题的生产事件的洞察。Drifter集成了最先进的稀疏数据在线特征排名和异常检测方法，具有高度可扩展性和资源效率性，每分钟处理数百万个实例仅需要两个线程和少于一GB的RAM。在真实数据集上的评估证明了Drifter在警报和缓解数据质量问题方面的有效性，大大提高了实时实况推荐系统的可靠性和性能。

    Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
    
[^108]: 基于组合式基础模型的层次规划

    Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v1 [cs.LG])

    [http://arxiv.org/abs/2309.08587](http://arxiv.org/abs/2309.08587)

    本研究提出了一种基于组合式基础模型的层次规划方法，通过利用语言、视觉和动作数据的多个专家模型，解决了长期目标任务。通过符号计划、视频扩散和逆动力学模型的结合，实现了在新环境中做出有效决策的能力。

    

    在新环境中做出有效决策需要进行跨空间和时间尺度的层次推理。本文提出了一种基于组合式基础模型的层次规划方法，利用多个专家模型分别对语言、视觉和动作数据进行训练，共同解决长期目标任务。我们利用一个大型语言模型构建在环境中扎根的符号计划，并通过大型视频扩散模型来实现。生成的视频计划通过逆动力学模型与视觉-动作控制相结合。为了在此层次结构中进行有效推理，我们通过迭代改进强制保持模型的一致性。

    To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustr
    
[^109]: 在高斯－勒让德节点上由于潜在空间正则化而压缩中保持拓扑数据结构完整性的确保

    Ensuring Toplogical Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes. (arXiv:2309.08228v1 [cs.LG])

    [http://arxiv.org/abs/2309.08228](http://arxiv.org/abs/2309.08228)

    通过在高斯-勒让德节点上进行潜在空间正则化，我们的研究提出了一种新的无监督自编码器，能够确保在压缩过程中保持拓扑数据结构的完整性。

    

    我们为一般的无监督自编码器制定了一个数据无关的潜在空间正则化约束。该正则化基于在勒让德节点上对自编码器的雅可比矩阵进行采样，这些节点是高斯-勒让德积分的中心。重新审视这个经典问题能够证明，经过正则化的自编码器能够将初始数据流形一对一地重新嵌入到其潜在表示中。实验证明，之前提出的正则化策略（如收缩自编码）在简单示例中已经导致了拓扑缺陷，而基于卷积的（变分）自编码器也是如此。相比之下，通过我们的贡献，标准的多层感知器神经网络在正则化的情况下已经确保了拓扑完整性。这个观察结果适用于经典的FashionMNIST数据集以及MRI脑部扫描的真实世界编码问题，这表明在各个领域中，对于复杂的高维数据，可靠的低维表示已得以确保。

    We formulate a data independent latent space regularisation constraint for general unsupervised autoencoders. The regularisation rests on sampling the autoencoder Jacobian in Legendre nodes, being the centre of the Gauss-Legendre quadrature. Revisiting this classic enables to prove that regularised autoencoders ensure a one-to-one re-embedding of the initial data manifold to its latent representation. Demonstrations show that prior proposed regularisation strategies, such as contractive autoencoding, cause topological defects already for simple examples, and so do convolutional based (variational) autoencoders. In contrast, topological preservation is ensured already by standard multilayer perceptron neural networks when being regularised due to our contribution. This observation extends through the classic FashionMNIST dataset up to real world encoding problems for MRI brain scans, suggesting that, across disciplines, reliable low dimensional representations of complex high-dimensiona
    
[^110]: 弱监督学习在串行飞秒晶体学中的模式分类中的应用

    Weakly supervised learning for pattern classification in serial femtosecond crystallography. (arXiv:2309.04474v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04474](http://arxiv.org/abs/2309.04474)

    本文介绍了在串行飞秒晶体学中通过弱监督算法对衍射图进行分类的工作，旨在尽可能减少训练所需的标签数据集的规模。

    

    X射线自由电子激光设施上的串行飞秒晶体学为晶体结构的确定开辟了新的时代。然而，这些实验的数据处理面临前所未有的挑战，因为确定高分辨率结构所需的衍射图的总数巨大。机器学习方法在处理如此大量的数据方面可能起到重要作用。卷积神经网络在模式分类领域取得了巨大成功，然而，训练这些网络需要带有标签的大规模数据集。对标签数据集的强依赖严重限制了网络的应用，因为注释大量衍射图非常昂贵。本文介绍了我们在弱监督算法下对衍射图进行分类的工作，旨在尽可能减少训练所需的标签数据集的规模。

    Serial femtosecond crystallography at X-ray free electron laser facilities opens a new era for the determination of crystal structure. However, the data processing of those experiments is facing unprecedented challenge, because the total number of diffraction patterns needed to determinate a high-resolution structure is huge. Machine learning methods are very likely to play important roles in dealing with such a large volume of data. Convolutional neural networks have made a great success in the field of pattern classification, however, training of the networks need very large datasets with labels. Th is heavy dependence on labeled datasets will seriously restrict the application of networks, because it is very costly to annotate a large number of diffraction patterns. In this article we present our job on the classification of diffraction pattern by weakly supervised algorithms, with the aim of reducing as much as possible the size of the labeled dataset required for training. Our res
    
[^111]: ConDA: 基于对比域适应的AI生成文本检测

    ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])

    [http://arxiv.org/abs/2309.03992](http://arxiv.org/abs/2309.03992)

    创新点：提出了一种基于对比域适应的框架 ConDA，用于检测由大型语言模型生成的新闻文本。这种方法解决了获取标记训练数据的困难，通过利用未标记的目标数据进行无监督域适应。

    

    大型语言模型（LLMs）越来越多地被用于各种用途的文本生成，包括新闻报道。鉴于这些LLMs可能被恶意使用来大规模生成虚假信息，构建有效的检测AI生成文本的工具显得尤为重要。由于新的LLMs不断被开发，获取用于监督式检测器的标记训练数据成为一个瓶颈。然而，可能存在大量未标记的文本数据，没有关于其生成器的信息。在这项工作中，我们解决了此数据问题，即检测AI生成的新闻文本，并将问题框架化为无监督域适应任务。这里的域是不同的文本生成器，即LLMs，我们假设只能访问标记的源数据和未标记的目标数据。我们开发了一个名为ConDA的对比域适应框架，将标准的域适应技术与表示能力相结合。

    Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
    
[^112]: 大型语言模型的指令调优：一项调研

    Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10792](http://arxiv.org/abs/2308.10792)

    本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。

    

    本文调查了指令调优（IT）这一快速发展的领域中的研究工作，这是一种增强大型语言模型（LLM）能力和可控性的关键技术。指令调优是指以监督方式在包含“指令-输出”对的数据集上进一步训练LLM，这将LLM的下一个词预测目标与用户希望LLM遵守人类指令的目标之间的差距。本文对IT的常规方法、IT数据集的构建、IT模型的训练以及应用于不同模态、领域和应用的情况进行了系统的文献综述，并对影响IT结果的各个方面进行了分析（例如，指令输出的生成、指令数据集的大小等）。我们还回顾了IT的潜在问题以及针对其的批评，以及指出当前不足的努力。

    This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
    
[^113]: ALI-DPFL: 具有自适应本地迭代的差分隐私联邦学习

    ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10457](http://arxiv.org/abs/2308.10457)

    ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。

    

    联邦学习是一种分布式机器学习技术，通过共享训练参数而不是原始数据，允许多个设备或组织之间进行模型训练。然而，攻击者仍然可以通过对这些训练参数的推理攻击（例如差分攻击）来推断个体信息。因此，差分隐私被广泛应用于联邦学习中以防止此类攻击。我们在资源受限的场景中考虑差分隐私联邦学习，其中既有隐私预算受限，又有通信轮次受限。通过理论分析收敛性，我们可以找到在任意两个顺序全局更新之间的客户机之间的最佳差分隐私本地迭代次数。基于此，我们设计了一种具有自适应本地迭代的差分隐私联邦学习算法（ALI-DPFL）。我们在FashionMNIST和CIFAR10数据集上对我们的算法进行实验，并展示了显著更好的性能。

    Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
    
[^114]: 在文本到图像分类和生成中，使用Bradley-Terry偏好模型进行快速自适应

    Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation. (arXiv:2308.07929v1 [cs.CV])

    [http://arxiv.org/abs/2308.07929](http://arxiv.org/abs/2308.07929)

    本研究提出了一种快速自适应方法，利用Bradley-Terry偏好模型，通过很少的示例和最小的计算资源高效地微调大型多模态模型，使其更符合用户的偏好，并在多个领域中展示了该方法的能力。

    

    最近，大型多模态模型，如CLIP和Stable Diffusion在基础理论和应用方面取得了巨大成功。然而，随着这些模型的参数大小和计算要求增加，用户为特定任务或偏好个性化它们变得更具挑战性。在这项工作中，我们解决了将之前的模型适应到特定人类偏好集合的问题，将检索或生成的图像与用户的偏好对齐。我们利用Bradley-Terry偏好模型开发了一种快速自适应方法，通过很少的示例和最小的计算资源高效地微调原始模型。通过与多模态文本和图像理解相关的不同领域的实验证据，我们提供了这个框架的能力。

    Recently, large multimodal models, such as CLIP and Stable Diffusion have experimented tremendous successes in both foundations and applications. However, as these models increase in parameter size and computational requirements, it becomes more challenging for users to personalize them for specific tasks or preferences. In this work, we address the problem of adapting the previous models towards sets of particular human preferences, aligning the retrieved or generated images with the preferences of the user. We leverage the Bradley-Terry preference model to develop a fast adaptation method that efficiently fine-tunes the original model, with few examples and with minimal computing resources. Extensive evidence of the capabilities of this framework is provided through experiments in different domains related to multimodal text and image understanding, including preference prediction as a reward model, and generation tasks.
    
[^115]: 贝叶斯流网络

    Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.07037](http://arxiv.org/abs/2308.07037)

    本文介绍了贝叶斯流网络（BFNs），一种新的生成模型，它通过贝叶斯推断修改了一组独立分布的参数，并将其作为输入传递给神经网络来生成另一个相互依赖的分布。该方法不需要前向过程，适用于连续和离散数据，并具有优化数据压缩的功能。

    

    本文介绍了贝叶斯流网络（BFNs），一种新的生成模型。在BFNs中，独立分布的参数会在嘈杂的数据样本的影响下通过贝叶斯推断进行修改，然后作为输入传递给神经网络，该神经网络输出一个相互依赖的分布。从简单的先验开始，通过迭代更新这两个分布可以得到一个类似于扩散模型反向过程的生成过程；不过，这个过程在概念上更简单，无需前向过程。对于连续、离散化和离散数据，推导出了离散和连续时间的损失函数，以及样本生成过程。值得注意的是，对于离散数据，网络的输入位于概率单纯形上，因此本质上是可微分的，为基于梯度的样本引导和在语言建模等离散领域进行少量步骤生成铺平了道路。损失函数直接优化了数据压缩，并且不放置限制。

    This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
    
[^116]: 学习能力与样本压缩并不相同的多类别问题

    Multiclass Learnability Does Not Imply Sample Compression. (arXiv:2308.06424v1 [cs.LG])

    [http://arxiv.org/abs/2308.06424](http://arxiv.org/abs/2308.06424)

    学习二元假设类具有样本压缩方案，而多类别假设类则不具备这个性质。

    

    如果一个假设类能够通过只保留一个小的子样本推断出整个样本的标签，那么它就具有样本压缩方案。学习二元假设类（必须具有有限的VC维度）都可以通过VC维度的一个有限函数大小的样本压缩方案实现。然而，对于多类别假设类来说，DS维度是相对应的，我们发现学习多类别假设类（必须具有有限的DS维度）并不能通过一个DS维度的有限函数大小的样本压缩方案实现。

    A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
    
[^117]: 基于差分进化算法的Transformer神经网络模型用于负荷预测的超参数选择

    Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])

    [http://arxiv.org/abs/2307.15299](http://arxiv.org/abs/2307.15299)

    本研究使用差分进化算法选择Transformer神经网络模型的优化超参数，以提高负荷预测的准确性。

    

    精确的负荷预测在众多领域都起着重要作用，但准确捕捉动力系统的复杂动态仍然是传统统计模型面临的挑战。因此，时间序列模型（ARIMA）和深度学习模型（ANN，LSTM，GRU等）经常被使用，并且通常能够取得更好的成功率。本文分析了最近开发的Transformer-based神经网络模型在负荷预测中的效果。Transformer模型有望改进负荷预测，因为它们能够通过其Attention机制学习到长期依赖关系。我们运用了几种元启发式算法，如差分进化，以寻找Transformer-based神经网络的最优超参数，以产生精确的预测。差分进化为非可微分、多目标或约束优化问题提供了可扩展、强健和全局的解决方案。我们的工作比较了所提出的基于Transformer的神经网络与其他模型在负荷预测上的性能。

    Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
    
[^118]: 比例响应：用于简单和累积遗憾最小化的情境赌博算法

    Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v1 [cs.LG])

    [http://arxiv.org/abs/2307.02108](http://arxiv.org/abs/2307.02108)

    这篇论文提出了一种适用于情境赌博设置的新型计算效率高的赌博算法，具有简单和累积遗憾最小化的优势，并可自适应模型错误规范和连续臂设置。该算法利用"一致臂集"（CAS）来提供在每个情境下囊括情境特定的最佳臂的一组臂，跨越情境分布。这篇论文对简单和累积遗憾保证的研究提供了正面结果，同时也揭示了无法实现实例依赖性的简单遗憾保证的消极结果。

    

    在医疗保健和电子商务等领域，简单遗憾最小化是学习最佳治疗分配策略的关键问题。然而，情境赌博设置中的简单遗憾最小化问题仍未充分研究。我们提出了一种新的计算效率高的赌博算法族，针对随机情境赌博设置，在累积遗憾最小化（具有近乎最优的极小极大保证）和简单遗憾最小化（具有SOTA保证）方面具有灵活性。此外，我们的算法对模型错误规范进行自适应，并扩展到连续臂设置。这些优势来自于构建和依赖于“一致臂集”（CAS），CAS在每个情境下提供一组臂，这些臂以一定的概率囊括了情境特定的最佳臂，跨越了情境分布。我们关于简单和累积遗憾保证的积极结果与一个消极结果形成对比，后者表明一个算法无法实现实例依赖性的简单遗憾保证。

    Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-de
    
[^119]: 一种基于神经随机微分方程的函数实现的构造性方法

    A Constructive Approach to Function Realization by Neural Stochastic Differential Equations. (arXiv:2307.00215v1 [math.OC])

    [http://arxiv.org/abs/2307.00215](http://arxiv.org/abs/2307.00215)

    本文采用了一种构造性方法，通过限制系统动力学来刻画可以实现的函数类，从而避免了高复杂度的控制。实现方法包括神经随机微分方程、确定性动力系统和输出映射的级联连接。这些结果有助于提高函数逼近算法的实际可行性。

    

    通过神经动力系统进行函数逼近的问题通常采用自上而下的方法：用给定结构的复杂模型可以将任何连续函数逼近到任意精度。然而，这可能导致在应用中不实际的高复杂度控制。本文采用相反的构造性方法：我们对系统动力学施加各种结构限制，从而刻画了可以通过这种系统实现的函数类。系统实现为神经随机微分方程（Neural SDE）、确定性动力系统和一个输出映射的级联连接。采用概率和几何（李论）方法来刻画这些系统实现的函数类。

    The problem of function approximation by neural dynamical systems has typically been approached in a top-down manner: Any continuous function can be approximated to an arbitrary accuracy by a sufficiently complex model with a given architecture. This can lead to high-complexity controls which are impractical in applications. In this paper, we take the opposite, constructive approach: We impose various structural restrictions on system dynamics and consequently characterize the class of functions that can be realized by such a system. The systems are implemented as a cascade interconnection of a neural stochastic differential equation (Neural SDE), a deterministic dynamical system, and a readout map. Both probabilistic and geometric (Lie-theoretic) methods are used to characterize the classes of functions realized by such systems.
    
[^120]: $\lambda$-AC：学习连续状态空间强化学习中的潜在决策感知模型

    $\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])

    [http://arxiv.org/abs/2306.17366](http://arxiv.org/abs/2306.17366)

    这项研究提出了一种$\lambda$-AC算法，通过学习连续状态空间中的潜在决策感知模型，实现了决策驱动的强化学习。通过理论和实证研究，确定了决策感知强化学习模型的必要组成部分，并展示了设计选择对算法性能的重要影响。

    

    决策感知模型学习的思想，在模型驱动的强化学习中变得越来越重要，即模型在决策制定时应该是准确的。尽管已经建立了一些有希望的理论结果，但是在连续控制问题中，利用决策感知损失的算法的实际性能仍然不足。本文研究了决策感知强化学习模型所需的必要组成部分，并展示了能够实现良好算法性能的设计选择。为此，我们对该领域的重要算法思想进行了理论和实证研究。我们强调，在MuZero系列工作中所建立的经验性设计决策对于相关算法的良好性能至关重要，并展示了在随机环境中，不同的价值感知算法实例之间行为差异。在这些见解的基础上，我们提出了潜在模型驱动决策的算法，称为$\lambda$-AC。

    The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
    
[^121]: HNO：用于解决PDE的鬣狗神经算子

    HNO: Hyena Neural Operator for solving PDEs. (arXiv:2306.16524v1 [cs.LG])

    [http://arxiv.org/abs/2306.16524](http://arxiv.org/abs/2306.16524)

    本研究使用了一种名为鬣狗的新型神经算子，它利用多层感知器参数化的长卷积滤波器来解决PDE问题。这种方法通过增强模型对输入上下文的理解，并为不同的PDE实例提供数据依赖权重，提供了一种有效的求解PDE的方式。

    

    数值求解偏微分方程（PDE）通常需要精细离散化以解析必要的时空尺度，这可能会耗费大量计算资源。深度学习的最新进展提供了一种新方法来解决PDE，该方法涉及使用神经算子。神经算子是一种神经网络架构，可以学习函数空间之间的映射，并能够基于数据解决偏微分方程。本研究利用了一种称为鬣狗（Hyena）的新型神经算子，该算子采用由多层感知器参数化的长卷积滤波器。鬣狗算子是一种具有次线性复杂性的操作，它使用状态空间模型来参数化具有全局感受野的长卷积。这种机制增强了模型对输入上下文的理解，并能够为不同的PDE实例提供数据依赖权重。为了衡量各个层在解决PDE中的有效性，我们进行实验评估。

    Numerically solving partial differential equations (PDEs) typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving PDEs that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and state space model to parameterize long convolution that enjoys global receptive field. This mechanism enhances the model's comprehension of the input's context and enables data-dependent weight for different PDE instances. To measure how effective the layers are in solving PDEs, we conduct experime
    
[^122]: Prodigy: 一种快速自适应零参数学习算法

    Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])

    [http://arxiv.org/abs/2306.06101](http://arxiv.org/abs/2306.06101)

    本文提出了一种基于自适应算法(如Adagrad和Adam)的学习率估计方法Prodigy和Resetting，可以快速且正确地估计到达解决方案所需的距离D，从而提高了模型的收敛速度，并在多个数据集和模型上进行了测试，实验表明该方法优于D-Adaptation并可达到手动调整Adam的测试准确度值。

    

    本文研究自适应算法(如Adagrad和Adam)中的学习率估计问题，描述了两种技术Prodigy和Resetting，可以证明地估计到达解决方案所需的距离D，以便最优设置学习率。我们的技术是基于学习率自由的D-Adaptation方法的修改，并通过$O(\sqrt{\log(D/d_0)})$的因子提高了D-Adaptation的收敛速度，其中$d_0$是$D$的初始估计值。我们在12个常见的逻辑回归基准数据集、在CIFAR10上训练的VGG11和ResNet-50、在Imagenet上训练的ViT、在IWSLT14上训练的LSTM、在Criteo数据集上训练的DLRM、在Knee MRI数据集上的VarNet，以及在BookWiki上训练的RoBERTa和GPT transformer上测试了我们的方法。我们的实验结果表明，我们的方法始终优于D-Adaptation，并达到手动调整Adam的测试准确度值。

    We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
    
[^123]: 带权重空间上功能性输入映射的全局普适逼近

    Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])

    [http://arxiv.org/abs/2306.03303](http://arxiv.org/abs/2306.03303)

    本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。

    

    我们引入了所谓的功能性输入神经网络，定义在可能是无限维带权重空间上，其值也在可能是无限维的输出空间中。为此，我们使用一个加性族作为隐藏层映射，以及一个非线性激活函数应用于每个隐藏层。依靠带权重空间上的Stone-Weierstrass定理，我们可以证明连续函数的推广的全局普适逼近结果，超越了常规紧集逼近。这特别适用于通过功能性输入神经网络逼近（非先见之明的）路径空间函数。作为带权Stone-Weierstrass定理的进一步应用，我们证明了线性函数签名的全局普适逼近结果。我们还在这个设置中引入了高斯过程回归的观点，并展示了签名内核的再生核希尔伯特空间是某些高斯过程的Cameron-Martin空间。

    We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
    
[^124]: GrACE：使用相关代码编辑生成代码

    GrACE: Generation using Associated Code Edits. (arXiv:2305.14129v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.14129](http://arxiv.org/abs/2305.14129)

    本文研究了如何利用预训练大型语言模型来生成代码，并通过赋予模型先前相关的编辑知识，来解决代码多样性和开发人员意图难以捕捉的问题。实验表明，这种方法有效提高了模型的性能。

    

    开发人员会花费大量时间编辑代码，其原因包括修复错误或添加新功能。设计有效的代码编辑预测方法一直是一个活跃而具有挑战性的研究领域，因为代码编辑的多样性和捕捉开发人员意图的难度。在本文中，我们通过赋予预训练大型语言模型（LLMs）先前相关的编辑知识，来解决这些挑战。LLMs的生成能力有助于解决代码更改的多样性，而将代码生成的条件设定为先前编辑有助于捕捉潜在的开发人员意图。我们使用两个数据集评估了两种知名的LLMs，Codex和CodeT5，分别进行零样本和微调设置。在我们的实验中，先前编辑的知识显著提高了LLMs的性能，并使其在前1个建议中生成29％和54％更正确的编辑代码，相对于当前最先进的符号化方法。

    Developers expend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) of code with the knowledge of prior, relevant edits. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, Codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, the knowledge of prior edits boosts the performance of the LLMs significantly and enables them to generate 29% and 54% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic
    
[^125]: 语义感知的传输调度：一种基于单调性驱动的深度强化学习方法

    Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v1 [cs.LG])

    [http://arxiv.org/abs/2305.13706](http://arxiv.org/abs/2305.13706)

    这篇论文提出了一种基于单调性驱动的深度强化学习算法，用于处理在6G时代物联网系统中的大规模语义感知传输调度问题。数值结果显示所提出的算法相比基准算法可以大大减少训练时间并提高训练性能。

    

    在6G时代的物联网系统中，需要语义传输来连接分布式设备，以保证应用层性能，不仅仅是集中于通信层性能。语义在这里是信息传输有用性的衡量。大规模系统的语义感知传输调度常常涉及庞大的决策空间，现有算法无法有效地获得最优策略。本文首先研究最优语义感知调度策略的基本属性，然后根据理论指导原则开发了先进的深度强化学习算法。我们的数值结果显示，相比基准算法，所提出的算法可以大大减少训练时间并提高训练性能。

    For cyber-physical systems in the 6G era, semantic communications connecting distributed devices for dynamic control and remote state estimation are required to guarantee application-level performance, not merely focus on communication-centric performance. Semantics here is a measure of the usefulness of information transmissions. Semantic-aware transmission scheduling of a large system often involves a large decision-making space, and the optimal policy cannot be obtained by existing algorithms effectively. In this paper, we first investigate the fundamental properties of the optimal semantic-aware scheduling policy and then develop advanced deep reinforcement learning (DRL) algorithms by leveraging the theoretical guidelines. Our numerical results show that the proposed algorithms can substantially reduce training time and enhance training performance compared to benchmark algorithms.
    
[^126]: ZeroFlow: 通过蒸馏实现快速零标签场景流

    ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])

    [http://arxiv.org/abs/2305.10424](http://arxiv.org/abs/2305.10424)

    ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。

    

    场景流估计是描述连续点云之间的三维运动场的任务。最先进的方法使用强大的先验知识和测试时优化技术，但对于大规模点云需要数十秒的时间，使其无法作为实时应用程序（如开放世界目标检测）的计算机视觉基元使用。前向传递方法相对快速，对于大规模点云的运行时间在数十至数百毫秒之间，但需要昂贵的人力监督。为了解决这两个限制，我们提出了一种简单的蒸馏框架 Scene Flow via Distillation，使用无标签优化方法来生成伪标签以监督前向传递模型。我们实现了这个框架中的 ZeroFlow，使用零人工标签，在大规模点云上实时生成场景流估计结果，同时质量竞争状态下的最先进方法。值得注意的是，在测试时 ZeroFlow

    Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
    
[^127]: 利用空间对比预训练进行未见训练数据的新道路交通预测

    Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])

    [http://arxiv.org/abs/2305.05237](http://arxiv.org/abs/2305.05237)

    本文提出一种名为SCPT的框架，利用对比学习进行空间预训练，并引入一个空间编码器模块，用于从未见数据中提取特征。该方法可以用于进行新道路的交通预测，无需重新训练模型。

    

    随着时间推移会不断建设新的道路，但是之前的深度预测模型对于新道路（未见数据）的泛化能力很少被探索。本文引入了一个被称为时空（ST）分割的新设置，以评估模型对未见数据的泛化能力。在这个设置中，模型训练时使用一部分的道路数据，但测试时使用未见数据的道路。我们还提出了一种新的框架，称之为空间对比预训练（SCPT），其中引入了一个空间编码器模块来提取推理时未见道路的潜在特征。这个空间编码器是使用对比学习预训练的。在推理时，空间编码器仅需要新道路的两天交通数据，而不需要任何重新训练。我们还展示了空间编码器的输出可以有效地用于推断未见道路上的潜在节点嵌入。

    New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
    
[^128]: 多尺度聚类过滤中的持久同调

    Persistent Homology of the Multiscale Clustering Filtration. (arXiv:2305.04281v2 [math.AT] UPDATED)

    [http://arxiv.org/abs/2305.04281](http://arxiv.org/abs/2305.04281)

    该论文引入了一种多尺度聚类过滤方法（MCF），用于描述不同尺度下的数据聚类，其中的持久同调可测量分区序列的层次关系和聚类分配冲突的出现和解决。

    

    在许多数据聚类应用中，不仅希望找到一种单一的分区方式，还希望找到描述不同尺度或粗糙层次下的数据的一系列分区方式。因此，一个自然的问题是分析和比较支撑这种多尺度数据描述的（不一定是层次性的）分区序列。在这里，我们引入了一种抽象单纯复形的过滤，称为多尺度聚类过滤（MCF），它编码了跨尺度的任意模式的聚类分配，并证明了MCF产生稳定的持久图。然后，我们展示了MCF的零维持久同调测量了分区序列中的层次关系程度，而高维持久同调则跟踪了分区序列中聚类分配冲突的出现和解决。为了拓宽MCF的理论基础，我们还提供了一个等价的构造方法。

    In many applications in data clustering, it is desirable to find not just a single partition into clusters but a sequence of partitions describing the data at different scales, or levels of coarseness. A natural problem then is to analyse and compare the (not necessarily hierarchical) sequences of partitions that underpin such multiscale descriptions of data. Here, we introduce a filtration of abstract simplicial complexes, denoted the Multiscale Clustering Filtration (MCF), which encodes arbitrary patterns of cluster assignments across scales, and we prove that the MCF produces stable persistence diagrams. We then show that the zero-dimensional persistent homology of the MCF measures the degree of hierarchy in the sequence of partitions, and that the higher-dimensional persistent homology tracks the emergence and resolution of conflicts between cluster assignments across the sequence of partitions. To broaden the theoretical foundations of the MCF, we also provide an equivalent constr
    
[^129]: CoDi: 混合类型表格生成的共同演化对比扩散模型

    CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])

    [http://arxiv.org/abs/2304.12654](http://arxiv.org/abs/2304.12654)

    CoDi 方法使用两个共同演化的对比扩散模型单独处理离散和连续变量并相互条件化，同时引入对比学习方法进行进一步的绑定，展现了在真实世界的表格数据集上的有效性。

    

    随着越来越多的注意力被放在表格数据上，将综合表格应用于各种任务的尝试已经向各种场景扩展。由于生成建模的最新进展，通过表格数据综合模型生成的虚假数据变得复杂而真实。但是，建模表格数据的离散变量（列）仍然存在困难。在本研究中，我们提出通过两个对比扩散模型单独处理连续和离散变量（但相互条件化）。两个扩散模型通过彼此读取条件在训练中共同演化。此外，为了进一步绑定扩散模型，我们引入了一个负采样的对比学习方法。在11个真实世界的表格数据集和8个基准方法的实验中，我们证明了所提出的方法 CoDi 的有效性。

    With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
    
[^130]: 基于原始-对偶语境贝叶斯优化的带时间平均约束的控制系统在线优化

    Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])

    [http://arxiv.org/abs/2304.06104](http://arxiv.org/abs/2304.06104)

    提出了一种基于原始-对偶语境贝叶斯优化算法，可以实现对约束闭环控制系统的在线性能优化，同时满足所需的约束条件。

    

    本文研究带有外生时间变化上下文干扰的未知黑盒函数的约束闭环控制系统在线性能优化问题。提出了一种原始-对偶语境贝叶斯优化算法，在满足一定正则条件下，实现了对动态最优解的亚线性累积遗憾。此外，该算法可以实现零时间平均约束违规，确保了约束函数的平均值满足所需的约束条件。该方法应用于高斯过程的采样实例和连续搅拌槽反应器参数调节问题。仿真结果表明，该方法同时提供接近最优的性能和平均保持约束可行性，这与当前的最先进方法形成对比，后者要么遭受大量累积遗憾，要么存在严重约束违规问题。

    This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
    
[^131]: 量子相容预测用于量子机器学习中的可靠不确定性量化

    Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning. (arXiv:2304.03398v1 [quant-ph])

    [http://arxiv.org/abs/2304.03398](http://arxiv.org/abs/2304.03398)

    本文提出了一种通用方法，可以可靠地量化量子模型的不确定性，无论训练数据的数量、拍摄次数、ansatz、训练算法以及量子硬件噪声的存在如何。

    

    量子机器学习是在当前的噪声中间规模量子(NISQ)计算机时代中优化量子算法的有前途的编程范式。量子机器学习中的一个基本挑战是泛化性能，因为设计者的目标是在测试条件下获得良好的性能，但只能访问有限的训练数据。现有的泛化分析虽然能够识别重要的一般趋势和规模定律，但不能用于为量子模型所作出的决策分配可靠和有信息量的“误差条”。在本文中，我们提出了一种通用方法，可以可靠地量化量子模型的不确定性，无论训练数据的数量、拍摄次数、ansatz、训练算法以及量子硬件噪声的存在如何，在概率性相容预测的基础上构建方法，可以将预先训练的量子模型的任意可能小的拍摄次数转换为一组预测。

    Quantum machine learning is a promising programming paradigm for the optimization of quantum algorithms in the current era of noisy intermediate scale quantum (NISQ) computers. A fundamental challenge in quantum machine learning is generalization, as the designer targets performance under testing conditions, while having access only to limited training data. Existing generalization analyses, while identifying important general trends and scaling laws, cannot be used to assign reliable and informative "error bars" to the decisions made by quantum models. In this article, we propose a general methodology that can reliably quantify the uncertainty of quantum models, irrespective of the amount of training data, of the number of shots, of the ansatz, of the training algorithm, and of the presence of quantum hardware noise. The approach, which builds on probabilistic conformal prediction, turns an arbitrary, possibly small, number of shots from a pre-trained quantum model into a set predicti
    
[^132]: 病理图像诊断的跨尺度多实例学习

    Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])

    [http://arxiv.org/abs/2304.00216](http://arxiv.org/abs/2304.00216)

    本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。

    

    数字病理学中，跨多个尺度分析高分辨率的全幅图像 (WSIs) 带来了巨大的挑战。多实例学习 (MIL) 是利用分类对象集 (例如较小的图像块集) 对高分辨率图像进行处理的常见方法。然而，这种处理通常在WSIs的单个尺度（例如20倍放大）上进行，忽略了对人类病理学家诊断至关重要的跨尺度信息。在本研究中，我们提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中。本文的贡献有三个方面：(1) 提出了一种新的跨尺度MIL (CS-MIL)算法，它集成了多尺度信息和跨尺度关系；(2) 创建并发布了一个玩具数据集，其中包含尺度特异性形态特征，以检查和可视化不同的跨尺度关系；(3)在四个WSI的细胞肺癌数据集上进行了实验验证CS-MIL的有效性。

    Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
    
[^133]: 基于图神经网络的纳米卫星任务调度方法：学习混合整数模型的洞见

    A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])

    [http://arxiv.org/abs/2303.13773](http://arxiv.org/abs/2303.13773)

    本研究提出基于GNN的纳米卫星任务调度方法，以更好地优化服务质量，解决ONTS问题的复杂性。

    

    本研究探讨如何利用图神经网络（GNN）更有效地调度纳米卫星任务。在离线纳米卫星任务调度（ONTS）问题中，目标是找到在轨道上执行任务的最佳安排，同时考虑服务质量（QoS）方面的考虑因素，如优先级，最小和最大激活事件，执行时间框架，周期和执行窗口，以及卫星电力资源和能量收集和管理的复杂性的约束。ONTS问题已经使用传统的数学公式和精确方法进行了处理，但是它们在问题的挑战性案例中的适用性有限。本研究考察了在这种情况下使用GNN的方法，该方法已经成功应用于许多优化问题，包括旅行商问题，调度问题和设施放置问题。在本文中，我们将ONTS问题的MILP实例完全表示成二分图网络结构来应用GNN。

    This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
    
[^134]: 伪监督度量：在无监督跨域分类框架中评估无监督图像翻译模型

    Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])

    [http://arxiv.org/abs/2303.10310](http://arxiv.org/abs/2303.10310)

    本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。

    

    图像分类的准确性和高效性取决于访问大型标记数据集并在模型训练的相同领域上测试数据。当处理来自不同领域的新数据时，分类变得更加具有挑战性，因为收集大型标记数据集并从头训练新分类器耗时、昂贵，有时是不可行或不可能的。跨域分类框架通过利用无监督图像对图像 (UI2I) 翻译模型将输入图像从未标记的域转换为标记域来处理这个数据域漂移问题。这些无监督模型的问题在于它们是无监督的。由于缺少注释，无法使用传统的监督度量来评估这些翻译模型以选择最佳的检查点模型。在本文中，我们介绍了一种新的方法，称为伪监督度量，专门用于评估无监督跨域分类框架中 UI2I 翻译模型的性能。我们通过对几个基准数据集的实验证明了我们的方法的有效性。

    The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
    
[^135]: Neural-BO: 使用深度神经网络的黑盒优化算法

    Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks. (arXiv:2303.01682v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01682](http://arxiv.org/abs/2303.01682)

    Neural-BO是一种使用神经网络模型的黑盒优化算法，避免了高斯过程中的缩放和维数问题，具有高效收敛的特性。

    

    贝叶斯优化（BO）是一种对黑盒函数进行全局优化的有效方法，当函数评估代价高时。之前的大部分工作使用高斯过程来模拟黑盒函数，然而，高斯过程中使用的核函数导致了两个问题：一是基于核函数的方法在数据点数量较大时缩放困难，二是核方法在复杂结构高维数据上通常效果不佳，因为维数灾难。因此，我们提出了一种新颖的黑盒优化算法，其中黑盒函数使用神经网络进行建模。我们的算法不需要贝叶斯神经网络来估计预测不确定性，因此计算上更加有利。我们使用NTK理论的进展分析了我们算法的理论行为，展示了其收敛的高效性。我们在合成和真实世界的优化任务上进行了实验，证明了我们算法的

    Bayesian Optimization (BO) is an effective approach for global optimization of black-box functions when function evaluations are expensive. Most prior works use Gaussian processes to model the black-box function, however, the use of kernels in Gaussian processes leads to two problems: first, the kernel-based methods scale poorly with the number of data points and second, kernel methods are usually not effective on complex structured high dimensional data due to curse of dimensionality. Therefore, we propose a novel black-box optimization algorithm where the black-box function is modeled using a neural network. Our algorithm does not need a Bayesian neural network to estimate predictive uncertainty and is therefore computationally favorable. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory showing its efficient convergence. We perform experiments with both synthetic and real-world optimization tasks and show that our algorithm is
    
[^136]: 自适应数据分析中的子采样足够

    Subsampling Suffices for Adaptive Data Analysis. (arXiv:2302.08661v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08661](http://arxiv.org/abs/2302.08661)

    子采样是自适应数据分析中的关键方法，仅需基于随机子样本和少量比特输出的查询，即可保证代表性和泛化性。

    

    确保对数据集的分析代表整个样本总体是统计学中的核心问题之一。大多数经典技术假设数据集与分析师的查询无关，并在多次、自适应选择的查询中失效。这个“自适应数据分析”问题在Dwork等人（STOC，2015）和Hardt和Ullman（FOCS，2014）的开创性工作中得到了形式化。我们确定了一个非常简单的假设集，使得即使在自适应选择的情况下，查询仍然具有代表性：唯一的要求是每个查询采用随机子样本作为输入并输出少量比特。这个结果表明，子采样中固有的噪音足以保证查询的响应具有泛化性。这种基于子采样的框架的简单性使其能够模拟之前研究所未涵盖的各种实际情境。

    Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst's query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of \emph{adaptive data analysis} was formalized in the seminal works of Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014).  We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: The only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work.  In addition to its simplicity, we demo
    
[^137]: 在离线训练数据集中识别专家行为改善了机器人操作策略的行为克隆

    Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies. (arXiv:2301.13019v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.13019](http://arxiv.org/abs/2301.13019)

    本文提出了一个解决方案，通过在离线数据集中识别专家行为，改善了机器人操作策略的行为克隆。在NeurIPS 2022竞赛中，我们发现最简单的离线学习算法，行为克隆，在专家数据集上表现出色，甚至超过了最先进的离线强化学习算法。然而，当应用于混合数据集时，行为克隆的性能下降，离线强化学习算法的表现也不理想。通过对混合数据集进行分析，我们发现其中包含大量未标记的专家数据。为了解决这个问题，我们提出了一种半监督学习的分类器，用于识别混合数据中的专家行为。

    

    本文介绍了我们在NeurIPS 2022竞赛中提出的解决方案，该竞赛旨在通过从预先收集的离线数据中进行学习来解决机器人灵巧操作任务。我们为每个任务提供了两种类型的数据集：专家数据集和不同技能水平的混合数据集。虽然在专家数据集上训练的最简单的离线策略学习算法（行为克隆）表现出色，甚至超过了最先进的离线强化学习算法，但它在应用于混合数据集时性能下降，离线强化学习算法的表现也令人不满意。经过检查混合数据集，我们发现其中包含大量未标记的专家数据。为了解决这个问题，我们提出了一种基于半监督学习的分类器来识别混合数据中的专家行为。

    This paper presents our solution for the Real Robot Challenge (RRC) III, a competition featured in the NeurIPS 2022 Competition Track, aimed at addressing dexterous robotic manipulation tasks through learning from pre-collected offline data. Participants were provided with two types of datasets for each task: expert and mixed datasets with varying skill levels. While the simplest offline policy learning algorithm, Behavioral Cloning (BC), performed remarkably well when trained on expert datasets, it outperformed even the most advanced offline reinforcement learning (RL) algorithms. However, BC's performance deteriorated when applied to mixed datasets, and the performance of offline RL algorithms was also unsatisfactory. Upon examining the mixed datasets, we observed that they contained a significant amount of expert data, although this data was unlabeled. To address this issue, we proposed a semi-supervised learning-based classifier to identify the underlying expert behavior within mix
    
[^138]: 通过正未标记学习改进行为克隆

    Improving Behavioural Cloning with Positive Unlabeled Learning. (arXiv:2301.11734v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11734](http://arxiv.org/abs/2301.11734)

    本文提出了一种通过迭代学习算法来识别机器人数据集中专家轨迹的新方法，并将该方法应用于行为克隆，表现出比竞争方法更高的准确性和最先进的性能。

    

    从预先记录的数据集中离线学习控制策略是解决实际问题的一种有前途的方法。然而，现有的数据集通常具有混合质量，其中高质量示范（即高质量的轨迹）的数量有限。因此，我们提出了一种新颖的迭代学习算法，在给定最小数量的正样本的情况下，用于识别无标记混合质量机器人数据集中的专家轨迹，并在准确性上超越现有算法。我们表明，将行为克隆应用于结果过滤后的数据集优于多个竞争的离线强化学习和模仿学习基线。我们在一系列模拟运动任务和一个真实机器人系统上进行实验；在这些实验中，我们的方法展示了最先进的性能。

    Learning control policies offline from pre-recorded datasets is a promising avenue for solving challenging real-world problems. However, available datasets are typically of mixed quality, with a limited number of the trajectories that we would consider as positive examples; i.e., high-quality demonstrations. Therefore, we propose a novel iterative learning algorithm for identifying expert trajectories in unlabeled mixed-quality robotics datasets given a minimal set of positive examples, surpassing existing algorithms in terms of accuracy. We show that applying behavioral cloning to the resulting filtered dataset outperforms several competitive offline reinforcement learning and imitation learning baselines. We perform experiments on a range of simulated locomotion tasks and on two challenging manipulation tasks on a real robotic system; in these experiments, our method showcases state-of-the-art performance. Our website: \url{https://sites.google.com/view/offline-policy-learning-pubc}.
    
[^139]: 关于强化学习中Transformers的调查

    A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03044](http://arxiv.org/abs/2301.03044)

    这篇论文是一项调查研究，总结了在强化学习领域使用Transformers的动机、进展和未来前景。

    

    Transformer已被认为是自然语言处理（NLP）和计算机视觉（CV）领域中的主导神经架构，主要应用于监督学习任务。最近，在强化学习（RL）领域中也出现了类似的使用Transformers的潮流，但面临着RL的特殊设计选择和挑战。然而，Transformers在RL中的发展尚未被充分揭示。在本文中，我们系统地回顾了在RL中使用Transformers的动机和进展，提供了一个现有工作的分类体系，讨论了每个子领域，并总结了未来的前景。

    Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
    
[^140]: 使用衍射光学网络的决策和控制

    Decision-making and control with diffractive optical networks. (arXiv:2212.11278v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11278](http://arxiv.org/abs/2212.11278)

    本文提出使用深度强化学习来实现具备决策和控制能力的衍射光学网络，以模拟人类的决策和控制能力。这种网络利用残差架构，并通过与环境的交互来找到最优的控制策略，具备高速和低功耗的特点。

    

    人工智能的终极目标是模拟人脑，从高维感知输入中直接进行决策和控制。衍射光学网络提供了一个有希望的解决方案，可以实现高速和低功耗的人工智能。大多数报道的衍射光学网络都集中在单个或多个任务上，这些任务不涉及环境交互，比如物体识别和图像分类。相反，我们目前还没有开发出能够进行决策和控制的网络。在这里，我们提出使用深度强化学习来实现模拟人类级别的决策和控制能力的衍射光学网络。这些网络利用残差架构的优势，通过与环境的交互找到最优的控制策略，并且可以通过现有的光学设备轻松实现。这种方法展现了卓越的性能。

    The ultimate goal of artificial intelligence is to mimic the human brain to perform decision-making and control directly from high-dimensional sensory input. Diffractive optical networks provide a promising solution for implementing artificial intelligence with high-speed and low-power consumption. Most of the reported diffractive optical networks focus on single or multiple tasks that do not involve environmental interaction, such as object recognition and image classification. In contrast, the networks capable of performing decision-making and control have not yet been developed to our knowledge. Here, we propose using deep reinforcement learning to implement diffractive optical networks that imitate human-level decision-making and control capability. Such networks taking advantage of a residual architecture, allow for finding optimal control policies through interaction with the environment and can be readily implemented with existing optical devices. The superior performance of the
    
[^141]: DREAM: 一种动态调度器，用于动态实时多模型机器学习工作负载

    DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads. (arXiv:2212.03414v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2212.03414](http://arxiv.org/abs/2212.03414)

    该论文提出了一种名为DREAM的动态调度器，针对实时多模型机器学习工作负载中的各种动态行为设计。DREAM通过量化得分来驱动调度决策，考虑当前系统负载和其他推理任务。这项工作对于更好地利用底层硬件，提高系统效率具有重要意义。

    

    新兴的实时多模型机器学习（RTMM）工作负载，如AR/VR和无人机控制，涉及到不同粒度的动态行为；任务、模型以及模型内的层次。这些动态行为给ML系统中的系统软件带来了新的挑战，因为整体系统负载不完全可预测，不像传统的机器学习工作负载。此外，RTMM工作负载需要实时处理，涉及到高度异构的模型，并且目标是资源受限的设备。在这种情况下，开发一种有效的调度器对于更好地利用底层硬件来说更加重要，考虑到RTMM工作负载的独特特性。因此，我们提出了一种新的调度器，名为DREAM，它能够有效处理RTMM工作负载中的各种动态性，针对多加速器系统进行调度。DREAM量化了RTMM工作负载的独特需求，并利用这些量化得分来驱动调度决策，考虑当前系统负载和其他推理任务。

    Emerging real-time multi-model ML (RTMM) workloads such as AR/VR and drone control involve dynamic behaviors in various granularity; task, model, and layers within a model. Such dynamic behaviors introduce new challenges to the system software in an ML system since the overall system load is not completely predictable, unlike traditional ML workloads. In addition, RTMM workloads require real-time processing, involve highly heterogeneous models, and target resource-constrained devices. Under such circumstances, developing an effective scheduler gains more importance to better utilize underlying hardware considering the unique characteristics of RTMM workloads. Therefore, we propose a new scheduler, DREAM, which effectively handles various dynamicity in RTMM workloads targeting multi-accelerator systems. DREAM quantifies the unique requirements for RTMM workloads and utilizes the quantified scores to drive scheduling decisions, considering the current system load and other inference jobs
    
[^142]: Grassmann流形流用于稳定形状生成

    Grassmann Manifold Flows for Stable Shape Generation. (arXiv:2211.02900v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02900](http://arxiv.org/abs/2211.02900)

    本文提出了一种利用Grassmann流形学习分布的方法，以生成稳定的形状。实验结果证明了该方法在生成高质量样本方面的优势。

    

    最近，机器学习的研究集中在利用特定流形中的对称性作为归纳偏差的方法上。Grassmann流形提供了处理以形状空间表示的基本形状的能力，实现了稳定的形状分析。本文提出了一种新的方法，通过连续的归一化流在Grassmann流形上建立学习分布的理论基础，明确的目标是生成稳定的形状。我们的方法通过在Grassmann流形内学习和生成，有效地消除了旋转和翻转等外部变换的影响，从而实现更稳健的生成，以适应对象的基本形状信息。实验结果表明，所提出的方法能够通过捕捉数据结构生成高质量的样本。此外，所提出的方法在t方面显著优于最先进的方法。

    Recently, studies on machine learning have focused on methods that use symmetry implicit in a specific manifold as an inductive bias. Grassmann manifolds provide the ability to handle fundamental shapes represented as shape spaces, enabling stable shape analysis. In this paper, we present a novel approach in which we establish the theoretical foundations for learning distributions on the Grassmann manifold via continuous normalization flows, with the explicit goal of generating stable shapes. Our approach facilitates more robust generation by effectively eliminating the influence of extraneous transformations, such as rotations and inversions, through learning and generating within a Grassmann manifolds designed to accommodate the essential shape information of the object. The experimental results indicated that the proposed method can generate high-quality samples by capturing the data structure. Furthermore, the proposed method significantly outperformed state-of-the-art methods in t
    
[^143]: 多智能体深度覆盖技能发现

    Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03269](http://arxiv.org/abs/2210.03269)

    这篇论文介绍了一种针对多智能体强化学习中合作选项发现的方法，通过最小化多个智能体联合状态空间的预期覆盖时间来构建多智能体选项，并提出了采用这些选项的新框架。

    

    在强化学习中使用技能（即选项）可以大大加快探索速度，尤其是当只有稀疏的奖励信号可用时。虽然已经提出了针对个体智能体的选项发现方法，在多智能体强化学习环境中，尚未考虑如何发现协作选项，以协调多个智能体的行为并鼓励它们访问联合状态空间中未开发的区域。因此，我们提出了多智能体深度覆盖选项发现，通过最小化多个智能体联合状态空间的预期覆盖时间来构建多智能体选项。此外，我们提出了一种新颖的框架来在多智能体强化学习过程中采用多智能体选项。实际上，多智能体任务通常可以分为一些子任务，每个子任务可以由一个子团体的智能体完成。因此，我们的算法框架首先利用注意力机制来找到协作智能体子团体。

    The use of skills (a.k.a., options) can greatly accelerate exploration in reinforcement learning, especially when only sparse reward signals are available. While option discovery methods have been proposed for individual agents, in multi-agent reinforcement learning settings, discovering collaborative options that can coordinate the behavior of multiple agents and encourage them to visit the under-explored regions of their joint state space has not been considered. In this case, we propose Multi-agent Deep Covering Option Discovery, which constructs the multi-agent options through minimizing the expected cover time of the multiple agents' joint state space. Also, we propose a novel framework to adopt the multi-agent options in the MARL process. In practice, a multi-agent task can usually be divided into some sub-tasks, each of which can be completed by a sub-group of the agents. Therefore, our algorithm framework first leverages an attention mechanism to find collaborative agent sub-gr
    
[^144]: 非参数化和正则化的动态Wasserstein媒介中心对于序列观测

    Nonparametric and Regularized Dynamical Wasserstein Barycenters for Sequential Observations. (arXiv:2210.01918v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01918](http://arxiv.org/abs/2210.01918)

    本研究考虑了逐步观测的概率模型，扩展了动态Wasserstein媒介中心模型，用于捕捉序列观测中的瞬态行为，并放宽了纯态的参数化。

    

    我们考虑了逐步观测的概率模型，其中有限数量的状态之间存在渐进转换。我们特别关注人类活动分析等应用，其中观测到的加速度计时间序列包含表示不同活动的片段，我们称之为纯态，以及由这些纯态之间持续转换特征的时期。为了捕捉这种瞬态行为，我们扩展了程式等人在2021年提出的动态Wasserstein媒介中心(DWB)模型[1]，将每个纯态与一个数据生成分布相关联，并将这些分布的连续转换建模为具有动态演化权重的Wasserstein媒介中心。针对在一元情况下可以通过闭合形式计算Wasserstein距离和媒介中心的情况，我们具体放宽了纯态的参数化。我们强调与确定唯一性相关的问题。

    We consider probabilistic models for sequential observations which exhibit gradual transitions among a finite number of states. We are particularly motivated by applications such as human activity analysis where observed accelerometer time series contains segments representing distinct activities, which we call pure states, as well as periods characterized by continuous transition among these pure states. To capture this transitory behavior, the dynamical Wasserstein barycenter (DWB) model of Cheng et al. in 2021 [1] associates with each pure state a data-generating distribution and models the continuous transitions among these states as a Wasserstein barycenter of these distributions with dynamically evolving weights. Focusing on the univariate case where Wasserstein distances and barycenters can be computed in closed form, we extend [1] specifically relaxing the parameterization of the pure states as Gaussian distributions. We highlight issues related to the uniqueness in identifying
    
[^145]: 在线自协调且相对平滑的最小化问题及其在在线投资组合选择和学习量子态中的应用

    Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. (arXiv:2210.00997v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.00997](http://arxiv.org/abs/2210.00997)

    本文提出了一种在线自协调且相对平滑的最小化算法，通过分析在线镜像下降算法在凸函数上的遗憾，改进了在线投资组合选择算法的性能，并在在线学习量子态问题中达到了与Soft-Bayes算法相当的效果。

    

    本文考虑一类在线凸优化问题，其中损失函数是自协调障碍函数，在某个凸函数h的相对平滑，可能不是Lipschitz的。我们分析了在线镜像下降算法在h上的遗憾，并基于结果以统一的方式证明了以下结论。对于在线投资组合选择问题，当T>4d/logd时，改进了Helmbold等人提出的指数化梯度算法的遗憾界为O(T^{2/3} d^{1/3})，原有界是O(T^{3/4} d^{1/2})。对于在线投资组合选择问题，使用对数障碍的在线镜像下降算法的遗憾界为O(sqrt(Td))，与Orseau等人的Soft-Bayes算法具有相同的遗憾界，除去对数因子。对于使用对数损失的在线学习量子态问题，使用对数障碍的在线镜像下降算法的遗憾界是...

    Consider an online convex optimization problem where the loss functions are self-concordant barriers, smooth relative to a convex function $h$, and possibly non-Lipschitz. We analyze the regret of online mirror descent with $h$. Then, based on the result, we prove the following in a unified manner. Denote by $T$ the time horizon and $d$ the parameter dimension. 1. For online portfolio selection, the regret of $\widetilde{\text{EG}}$, a variant of exponentiated gradient due to Helmbold et al., is $\tilde{O} ( T^{2/3} d^{1/3} )$ when $T > 4 d / \log d$. This improves on the original $\tilde{O} ( T^{3/4} d^{1/2} )$ regret bound for $\widetilde{\text{EG}}$. 2. For online portfolio selection, the regret of online mirror descent with the logarithmic barrier is $\tilde{O}(\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due to Orseau et al. up to logarithmic terms. 3. For online learning quantum states with the logarithmic loss, the regret of online mirror descent with the log
    
[^146]: 分类指标的分析与比较

    Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05355](http://arxiv.org/abs/2209.05355)

    本文回顾并比较了常用于度量分类系统表现的各种指标，发现期望成本指标具有更广泛的适用性和直观性，并可用于解决从连续得分生成分类决策的实践问题。

    

    在机器学习领域，常用各种性能指标来评估分类系统的表现。本文介绍了一些最常用的用于衡量硬决策质量的标准和平衡准确率、标准和平衡错误率、F-beta分数和Matthews相关系数（MCC）等指标。我们回顾了这些和其他指标的定义，并将它们与期望成本（EC）进行比较，后者是每个统计学习课程中都介绍但在机器学习文献中很少使用的指标。我们表明标准和平衡错误率都是EC的特殊情况，进一步展示了EC与F分数和MCC的关系，并认为EC指标优于传统指标，因其更具有优雅性、通用性和直观性，且基于统计学的基本原理。本文中介绍的指标均用于度量硬决策的质量。然而，大多数现代分类系统输出连续得分，而有一个重要的实践问题是如何从这些连续得分中生成分类决策。

    A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
    
[^147]: 用于医疗应用的联邦学习：分类、当前趋势、挑战和未来研究方向

    Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions. (arXiv:2208.03392v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03392](http://arxiv.org/abs/2208.03392)

    本文调查了联邦学习在医疗应用中的分类、当前趋势、挑战和未来研究方向。该调查强调了联邦学习在保护隐私和解决安全问题方面的重要性。

    

    随着物联网、人工智能和机器学习/深度学习算法的出现，基于数据驱动的医学应用领域已成为设计强大且可扩展的诊断和预测模型的有望途径。因此，数据驱动的医学应用领域受到了学术界和工业界的广泛关注，在改善医疗服务质量方面取得了显著进展。尽管取得了这些进展，但人们普遍面临着采用基于人工智能的医疗应用的巨大挑战，包括满足安全、隐私和服务质量标准的艰巨任务。联邦学习的最新发展使得在分布式环境中训练复杂的机器学习模型成为可能，并已成为一个活跃的研究领域，特别是在分散的边缘网络中处理医学数据以保护隐私和解决安全问题。为此，本调查论文重点介绍了当前和未来的研究方向。

    With the advent of the IoT, AI and ML/DL algorithms, the landscape of data-driven medical applications has emerged as a promising avenue for designing robust and scalable diagnostic and prognostic models from medical data. Consequently, the realm of data-driven medical applications has garnered significant attention spanning academia and industry, ushering in marked enhancements in healthcare delivery quality. Despite these strides, the adoption of AI-driven medical applications remains hindered by formidable challenges, including the arduous task of meeting security, privacy, and quality of service (QoS) standards. Recent developments in federated learning have made it possible to train complex machine-learned models in a distributed manner and has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and address security concerns. To this end, this survey paper highlights the current and future
    
[^148]: 图神经网络的最优传播策略

    Optimal Propagation for Graph Neural Networks. (arXiv:2205.02998v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02998](http://arxiv.org/abs/2205.02998)

    本文提出了一种双层优化方法，通过学习个性化PageRank传播矩阵和下游半监督节点分类，来学习最优的图结构。该方法在实证评估中展现了优越的功效和鲁棒性。

    

    图神经网络通过使用固定的图数据作为输入，在各种实际应用中取得了巨大的成功。然而，由于信息稀缺、噪声、对抗性攻击或图拓扑、特征和真实标签分布之间的差异，初始输入图可能在特定下游任务上并不是最优的。在本文中，我们提出了一种双层优化方法，通过直接学习个性化PageRank传播矩阵以及下游半监督节点分类的方法来学习最优图结构。我们还探索了一种低秩逼近模型，进一步减少时间复杂度。实证评估表明，所提出的模型在所有基准方法上具有优越的功效和鲁棒性。

    Graph Neural Networks (GNNs) have achieved tremendous success in a variety of real-world applications by relying on the fixed graph data as input. However, the initial input graph might not be optimal in terms of specific downstream tasks, because of information scarcity, noise, adversarial attacks, or discrepancies between the distribution in graph topology, features, and groundtruth labels. In this paper, we propose a bi-level optimization approach for learning the optimal graph structure via directly learning the Personalized PageRank propagation matrix as well as the downstream semi-supervised node classification simultaneously. We also explore a low-rank approximation model for further reducing the time complexity. Empirical evaluations show the superior efficacy and robustness of the proposed model over all baseline methods.
    
[^149]: 使用基于自适应共振理论的拓扑聚类能力的类别分类器设计

    Class-wise Classifier Design Capable of Continual Learning using Adaptive Resonance Theory-based Topological Clustering. (arXiv:2203.09879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09879](http://arxiv.org/abs/2203.09879)

    本文提出了一种基于自适应共振理论的增长自组织聚类算法的监督分类算法，能够实现持续学习，并且在分类性能方面表现优于最先进的聚类算法。

    

    本文提出了一种基于自适应共振理论（ART）的增长自组织聚类算法，实现了能够进行持续学习的监督分类算法。该算法独立地将ART聚类算法应用于每个训练数据类别，生成分类器。当给定来自新类别的额外训练数据集时，将在不同的学习空间中定义一个新的ART聚类。由于上述特性，该算法实现了持续学习能力。仿真实验表明，与能够进行持续学习的最先进的基于聚类的分类算法相比，所提出的算法具有更优秀的分类性能。

    This paper proposes a supervised classification algorithm capable of continual learning by utilizing an Adaptive Resonance Theory (ART)-based growing self-organizing clustering algorithm. The ART-based clustering algorithm is theoretically capable of continual learning, and the proposed algorithm independently applies it to each class of training data for generating classifiers. Whenever an additional training data set from a new class is given, a new ART-based clustering will be defined in a different learning space. Thanks to the above-mentioned features, the proposed algorithm realizes continual learning capability. Simulation experiments showed that the proposed algorithm has superior classification performance compared with state-of-the-art clustering-based classification algorithms capable of continual learning.
    
[^150]: 增强限制性强化学习：克服学习奖励的局限性

    State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards. (arXiv:2102.11941v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.11941](http://arxiv.org/abs/2102.11941)

    本文提出了一种增强限制性强化学习的方法，通过增加状态的拉格朗日乘子并重新解释原始-对偶方法，可以解决传统方法无法得到最优策略的问题。

    

    传统的限制性强化学习引入了多个奖励，这些奖励必须分别累积到给定的阈值。在这类问题中，我们展示了一个简单的例子，其中无法通过任何加权线性组合的奖励来诱导出理想的最优策略。因此，存在一些限制性强化学习问题，常规化和经典的原始-对偶方法都无法得到最优策略。本文通过增加拉格朗日乘子，并将原始-对偶方法重新解释为驱动乘子演化的动力学部分，来解决这个问题。这种方法提供了一个系统的状态增强程序，能够保证解决带有约束的强化学习问题。因此，正如我们通过一个例子所示，尽管之前的方法可能无法找到最优策略，但在执行增强策略时运行对偶动力学可以从中获得一种可以证明采样动作的算法。

    A common formulation of constrained reinforcement learning involves multiple rewards that must individually accumulate to given thresholds. In this class of problems, we show a simple example in which the desired optimal policy cannot be induced by any weighted linear combination of rewards. Hence, there exist constrained reinforcement learning problems for which neither regularized nor classical primal-dual methods yield optimal policies. This work addresses this shortcoming by augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the portion of the dynamics that drives the multipliers evolution. This approach provides a systematic state augmentation procedure that is guaranteed to solve reinforcement learning problems with constraints. Thus, as we illustrate by an example, while previous methods can fail at finding optimal policies, running the dual dynamics while executing the augmented policy yields an algorithm that provably samples actions from 
    

