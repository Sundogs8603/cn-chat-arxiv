# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data.](http://arxiv.org/abs/2310.15890) | 本文提出了一种用于异构数据的分散式学习方法，通过跨特征对比损失实现数据无关知识蒸馏，实验结果表明该方法在各种计算机视觉任务上取得了优越性能。 |
| [^2] | [State Sequences Prediction via Fourier Transform for Representation Learning.](http://arxiv.org/abs/2310.15888) | 通过傅里叶变换预测状态序列的方法（SPF）利用状态序列的频域特性，高效地提取时间序列数据中的潜在模式，从而改善了长期决策的质量。 |
| [^3] | [KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models.](http://arxiv.org/abs/2310.15872) | 本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。 |
| [^4] | [Using Causality-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs.](http://arxiv.org/abs/2310.15865) | 本研究提出了一种使用因果感知图神经网络预测动态图中的时间中心性的方法，并在不同领域的13个时间图上进行了实验验证，结果显示该方法显著改善了介数和接近度中心性的预测能力。 |
| [^5] | [Improving Event Time Prediction by Learning to Partition the Event Time Space.](http://arxiv.org/abs/2310.15853) | 该论文提出通过学习划分事件时间空间的方法来改善事件时间预测，避免了对事件密度进行强参数假设，能够提高预测性能，特别是在可用数据有限的临床环境中。通过在模拟数据集和真实数据集上的实验证明了该方法的有效性。 |
| [^6] | [On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms.](http://arxiv.org/abs/2310.15848) | 这篇论文讨论了负责任的机器学习数据集的重要性，并提出了一个通过负责任评价标准来评估数据集的框架。 |
| [^7] | [A Diffusion Weighted Graph Framework for New Intent Discovery.](http://arxiv.org/abs/2310.15836) | 本研究提出了一种名为Diffusion Weighted Graph Framework (DWGF)的方法，用于捕捉数据中的语义相似性和结构关系，从而实现更充分和可靠的监督信号，解决了以往方法在新意图发现中无法平衡数量和质量的问题。 |
| [^8] | [Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods.](http://arxiv.org/abs/2310.15830) | 本研究提出了一种只使用压力测量进行水配水网络中小漏洞定位的方法，旨在解决因气候变化导致的饮用水稀缺问题。 |
| [^9] | [Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge.](http://arxiv.org/abs/2310.15827) | 重度增强、高分辨率3D ResUNet自动主动脉分割在SEG.A挑战中取得了优异的成绩，达到了所有测试案例0.9以上的Dice分数，并在稳定性上超过了其他参与者。它在临床评估、定量结果和体积网格质量方面名列前茅。 |
| [^10] | [One or Two Things We know about Concept Drift -- A Survey on Monitoring Evolving Environments.](http://arxiv.org/abs/2310.15826) | 本文对非监督数据流中的概念漂移进行了文献综述和系统分类，提供了对漂移检测和漂移定位的最新研究状态，并提供了相关问题的准确数学定义。 |
| [^11] | [Discriminator Guidance for Autoregressive Diffusion Models.](http://arxiv.org/abs/2310.15817) | 本文引入了判别器引导，用于自回归扩散模型的训练，通过使用最优判别器来纠正预训练模型，并提出了一个顺序蒙特卡洛算法来应对使用次优判别器的情况。在生成分子图的任务中，判别器引导有助于提高生成性能。 |
| [^12] | [Nonlinear dimensionality reduction then and now: AIMs for dissipative PDEs in the ML era.](http://arxiv.org/abs/2310.15816) | 该研究提出了一种纯数据驱动的工作流程，用于构建分布式动力系统的降阶模型。利用机器学习工具，可以回避对精确截断投影和封闭式修正的需求。自编码器和扩散映射可以用于发现和测试合适的潜变量集合。这种方法可以表达ROMs的不同坐标。 |
| [^13] | [Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations.](http://arxiv.org/abs/2310.15815) | 本研究提出了一种自我激励的模仿学习方法（SMILE），通过过滤低于当前策略的演示来解决模仿学习受到嘈杂演示限制的问题，不需要附加信息。使用扩散模型的正向和反向过程来提取扩散专业水平的噪声信息，并利用该信息预测当前策略和演示者之间的专业水平差距。 |
| [^14] | [Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation.](http://arxiv.org/abs/2310.15797) | 本文研究了参数高效的组合知识图谱表示的问题，通过随机实体量化的方法，可以达到与当前策略类似的效果，这是因为随机实体量化下，实体码有更高的熵和码字级别的Jaccard距离，使得不同实体更容易区分，从而有效地表示知识图谱。 |
| [^15] | [Improving generalization in large language models by learning prefix subspaces.](http://arxiv.org/abs/2310.15793) | 本文提出了一种通过学习前缀子空间来改进大型语言模型的泛化能力的方法。我们通过联合优化模型参数空间中的整个单纯形模型，在稀缺数据环境中实现了更广的局部最优解。这种方法在预训练变换器模型中表现出了很好的兼容性和有效性。 |
| [^16] | [Amortised Inference in Neural Networks for Small-Scale Probabilistic Meta-Learning.](http://arxiv.org/abs/2310.15786) | 本文提出了一种在神经网络中进行小规模概率元学习的方法，通过将诱导输入替换为实际数据，通过训练推理网络来实现对任务特定贝叶斯推理的元学习。 |
| [^17] | [Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2310.15767) | 本文提出了一种利用自监督对比学习的无配对MRI超分辨率方法，可以在有限的训练数据下提高SR性能，改善MRI分辨率。 |
| [^18] | [Robust Learning via Conditional Prevalence Adjustment.](http://arxiv.org/abs/2310.15766) | 该论文提出了一种名为CoPA的方法，用于处理医疗数据中不稳定相关性的问题，特别适用于反因果任务。这种方法通过调整患病率来处理混淆变量和标签的关系，从而实现鲁棒学习。 |
| [^19] | [Analyzing Single Cell RNA Sequencing with Topological Nonnegative Matrix Factorization.](http://arxiv.org/abs/2310.15744) | 该论文介绍了两种持久性拓扑非负矩阵分解方法，即TNMF和rTNMF，用于分析单细胞RNA测序。通过实验证明，TNMF和rTNMF显著优于其他NMF方法，并且可以用于可视化UMAP和t-SNE。 |
| [^20] | [Improving Diffusion Models for ECG Imputation with an Augmented Template Prior.](http://arxiv.org/abs/2310.15742) | 该论文提出了一种模板引导去噪扩散概率模型PulseDiff，通过使用信息先验来改进心电图(ECG)的插值和预测准确性。此模型考虑了主体间变化和心跳关系，从而提高了性能。 |
| [^21] | [Recurrent Linear Transformers.](http://arxiv.org/abs/2310.15719) | 本文提出了循环线性变换器作为transformer自注意机制的替代方案，解决了transformers在处理长距离依赖关系和推断成本方面的限制。在强化学习问题中的实验证明了其有效性和可行性。 |
| [^22] | [Causal Representation Learning Made Identifiable by Grouping of Observational Variables.](http://arxiv.org/abs/2310.15709) | 该论文研究了因果表示学习（CRL），提出了一种基于观测变量分组的新型弱约束的可辨识性方法，该方法不依赖于时间结构、干预或监督，具有实际应用的意义。 |
| [^23] | [Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning.](http://arxiv.org/abs/2310.15706) | 本文提出了一种能够通过生成多样化的调度策略来解决大型柔性车间调度实例的方法，并应用深度强化学习来优化调度质量。 |
| [^24] | [COPF: Continual Learning Human Preference through Optimal Policy Fitting.](http://arxiv.org/abs/2310.15694) | 通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。 |
| [^25] | [Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems.](http://arxiv.org/abs/2310.15690) | 本文介绍了一种名为增强型残差网络的新颖神经网络结构，通过在残差元素中添加幂次项提升了网络的表达能力，具有卓越的准确性和应用性能，尤其适用于非平滑函数的处理。同时，该网络结构在解决反问题方面也表现出卓越的性能。 |
| [^26] | [Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit.](http://arxiv.org/abs/2310.15681) | 本文研究了固定预算下多臂老虎机问题的实数组合纯探索。首先，我们引入了CSA算法，可在臂数指数增长的情况下找到最佳动作；然后，我们提出了Minimax-CombSAR算法，适用于动作类大小为多项式情况，并证明了它的最优性。最后，在实验中与之前的方法进行比较，结果表明我们的算法性能更好。 |
| [^27] | [Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting.](http://arxiv.org/abs/2310.15662) | 本文提出了一种交互式广义可加模型，既可解释性又能融入电力行业的特定领域知识以提高性能。通过使用分段线性函数和高效算法，我们的模型在公共基准和电力数据集中表现优异，并在极端天气事件的情况下具有良好的泛化能力。 |
| [^28] | [Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks.](http://arxiv.org/abs/2310.15656) | 本文设计了一个新的基于动量梯度的HGNNs攻击模型，用于非目标攻击。通过特征选择和特征修改，该模型在HGNNs训练之前实施攻击，填补了现有对HGNNs对抗攻击研究的空白。 |
| [^29] | [A Survey on Detection of LLMs-Generated Content.](http://arxiv.org/abs/2310.15654) | 该论文是关于LLMs生成内容检测的综述，提供了现有策略和挑战的概述，并提倡采用更灵活和强大的模型以提高检测准确性，并强调使用多方面的方法来应对不同攻击。这是首个综合调查LLMs时代检测的工作。 |
| [^30] | [Deceptive Fairness Attacks on Graphs via Meta Learning.](http://arxiv.org/abs/2310.15653) | 本文研究了对图进行欺骗性公平攻击的方法，通过元学习的框架FATE，在保持下游任务效用的前提下，能够放大图神经网络的偏见，无论是否考虑公平性。该研究可以为设计鲁棒和公平的图学习模型提供启示。 |
| [^31] | [Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models.](http://arxiv.org/abs/2310.15648) | 本研究在已有的Transformer到CNN的知识蒸馏基础上，通过引入动态CNN块，提升了高效CNN的性能，相比传统的高效CNN更具优势。 |
| [^32] | [Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection.](http://arxiv.org/abs/2310.15645) | 本研究评估了特定混淆技术对使用静态分析提取的特征的影响，并发现混淆技术对所有静态分析特征都产生不同程度的影响，这可能会破坏依赖这些特征的机器学习恶意软件检测器的有效性。 |
| [^33] | [Guaranteed Coverage Prediction Intervals with Gaussian Process Regression.](http://arxiv.org/abs/2310.15641) | 本论文介绍了一种基于机器学习框架Conformal Prediction的Gaussian Process Regression扩展方法，可以在模型完全错误的情况下保证生成具有所需覆盖度的预测区间。 |
| [^34] | [Contextual directed acyclic graphs.](http://arxiv.org/abs/2310.15627) | 本论文研究了上下文定向无环图的问题，通过神经网络将上下文特征映射到DAG，利用稀疏的加权邻接矩阵表示图结构，并通过新颖的投影层满足无环性的特点。实验证明该方法能够成功恢复出真实的上下文特定图。 |
| [^35] | [GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection.](http://arxiv.org/abs/2310.15624) | GUPNet++是一种通过以概率方式建模几何投影的几何不确定性传播网络，可以提高单目三维物体检测的深度预测稳定性和效率。 |
| [^36] | [Machine Translation for Nko: Tools, Corpora and Baseline Results.](http://arxiv.org/abs/2310.15612) | 该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。 |
| [^37] | [Using Slisemap to interpret physical data.](http://arxiv.org/abs/2310.15610) | Slisemap是一种结合了流形可视化和可解释人工智能的方法，可以帮助我们在物理数据中找到有意义的信息。 |
| [^38] | [tagE: Enabling an Embodied Agent to Understand Human Instructions.](http://arxiv.org/abs/2310.15605) | tagE是一种能够从自然语言指令中提取任务的具身代理系统，解决了智能代理理解人类意图时的歧义性和不完整性问题。 |
| [^39] | [Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning.](http://arxiv.org/abs/2310.15586) | 本论文提出了一种基于自监督深度学习技术和变压器模型的方法，用于检测开放海域海上监视中的有意AIS关闭。模型通过比较预测结果来报告检测到的异常情况。 |
| [^40] | [Multimodal Representations for Teacher-Guided Compositional Visual Reasoning.](http://arxiv.org/abs/2310.15585) | 本论文提出了一种教师引导的多模态表示方法，通过利用大规模跨模态编码器的特征来改进神经模块网络(NMN)，并引入了计划的教师引导的学习策略，以减少误差积累并提高性能。 |
| [^41] | [Accelerating Split Federated Learning over Wireless Communication Networks.](http://arxiv.org/abs/2310.15584) | 本论文提出了一种加速无线通信网络上的分裂联合学习的方法。通过将深度神经网络分为两部分，在设备和服务器上分别进行共同训练或共同推理，以解决在资源受限的边缘设备上部署大规模深度神经网络的难题。通过选择分割点和带宽分配以最小化系统延迟，实现了联合优化。 |
| [^42] | [Identifiable Latent Polynomial Causal Models Through the Lens of Change.](http://arxiv.org/abs/2310.15580) | 本文通过扩展潜在因果模型的范围，从线性高斯模型转化为多项式模型，并研究了部分参数保持不变时的部分识别性结果。 |
| [^43] | [VMAF Re-implementation on PyTorch: Some Experimental Results.](http://arxiv.org/abs/2310.15578) | 这项研究重新在PyTorch上实现了VMAF，与标准实现进行比较，结果显示在VMAF单位上的差异小于$10^{-2}$。同时，研究了在使用VMAF作为目标函数时的梯度计算，并证明使用该函数进行训练不会导致梯度不良。 |
| [^44] | [From Oja's Algorithm to the Multiplicative Weights Update Method with Applications.](http://arxiv.org/abs/2310.15559) | 本文发现了当Oja算法应用于共享特征向量的对称矩阵时，可以直接用乘法权重更新方法的遗憾来界定其遗憾，并讨论了在单位球上的二次形式优化的应用。 |
| [^45] | [Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series.](http://arxiv.org/abs/2310.15555) | 本研究以欧洲国家的电力需求时间序列为例，通过转移学习的方法来进行短期负荷预测。研究采用神经网络模型，并使用聚类分析来识别相似的模式，以提高预测性能。 |
| [^46] | [PET Synthesis via Self-supervised Adaptive Residual Estimation Generative Adversarial Network.](http://arxiv.org/abs/2310.15550) | 本论文提出了一种自我监督自适应残差估计生成对抗网络，用于PET图像的合成。该方法解决了合成图像与真实图像之间的纹理和结构差异问题，同时对低剂量PET和标准PET之间的分布偏移进行了研究。 |
| [^47] | [Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing.](http://arxiv.org/abs/2310.15549) | 在矩阵感知问题中，通过对称、秩1张量进行优化求解时，将梯度下降应用于升维问题可以得到近似的秩1张量和具有逃逸方向的临界点，这结果强调了张量参数化和一阶方法在实现全局最优性方面的重要性。 |
| [^48] | [Symmetry-preserving graph attention network to solve routing problems at multiple resolutions.](http://arxiv.org/abs/2310.15543) | 该论文介绍了一种保持对称性的图注意力网络，用于解决多尺度的路由问题。该方法是第一个完全等变的模型和训练方法，能够解决组合问题。此外，该方法还能够捕捉输入图的多尺度结构，从而避免了局部或次优解的问题。 |
| [^49] | [Privacy Amplification for Matrix Mechanisms.](http://arxiv.org/abs/2310.15526) | 提出了通过抽样分析矩阵机制的隐私放大算法MMCC，证明了通过条件性组合定理可以将相关输出视为独立输出，并展示了将噪声添加到DP-FTRL算法时可以渐近地与DP-SGD算法中放大后添加的噪声相匹配。 |
| [^50] | [On the Inherent Privacy Properties of Discrete Denoising Diffusion Models.](http://arxiv.org/abs/2310.15524) | 本研究探索了离散扩散模型在隐私保护方面的潜力，提供了关于训练数据集中每个数据点的隐私泄露的洞察，以及通过数据预处理减少合成数据集生成中隐私风险的方法。 |
| [^51] | [Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning.](http://arxiv.org/abs/2310.15523) | 生成式和对比式范式在图自监督学习中是互补的，我们提出了图对比掩码自编码器（GCMAE）框架来统一它们，GCMAE通过利用对比学习的全局信息来弥补掩码自编码器在捕捉全局信息方面的不足。 |
| [^52] | [Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs.](http://arxiv.org/abs/2310.15516) | 本论文提出了一个基于图注意力的深度强化学习方法来解决带有负载相关成本的中国邮递员问题。该方法将问题形式化为马尔可夫决策过程，引入了一个编码器和解码器的自回归模型来有效处理问题。 |
| [^53] | [KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval.](http://arxiv.org/abs/2310.15511) | 本研究评估了最先进的模型在信息检索中回答约束满足查询的能力，并引入了一个新的数据集KITAB来衡量语言模型的约束满足能力。 |
| [^54] | [AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing.](http://arxiv.org/abs/2310.15479) | 使用自动编码器和扩散模型结合的AutoDiff模型可以有效地生成合成的表格数据，克服了表格数据中的异构特征和特征间相关性的挑战，生成的数据与真实数据在统计上非常相似，并在机器学习任务中表现良好。 |
| [^55] | [Interpretable Survival Analysis for Heart Failure Risk Prediction.](http://arxiv.org/abs/2310.15472) | 该论文提出了一个新颖的、可解释的生存分析流程，利用改进的生存堆叠技术将生存分析问题转化为分类问题，并利用可解释性强的增强机器进行心衰风险预测。 |
| [^56] | [Continual Event Extraction with Semantic Confusion Rectification.](http://arxiv.org/abs/2310.15470) | 本文提出了一种带有语义混淆修正的持续事件提取模型，通过标注伪标签和传递关键知识来缓解事件类型语义混淆并提高模型在长尾事件类型的理解上的性能。 |
| [^57] | [Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization.](http://arxiv.org/abs/2310.15468) | 本研究探讨了从集中到分散的方法在电力行业中的转变，并重点讨论了机器学习对于可再生能源赋能和电网管理的关键作用。在预测可再生能源的产生和消耗方面，机器学习模型的应用变得越来越重要。将大数据和机器学习应用于智能电网可以提高能源效率、更好地响应需求和更好地整合可再生能源。然而，需要解决处理大数据量、网络安全和专业知识等挑战。 |
| [^58] | [EKGNet: A 10.96{\mu}W Fully Analog Neural Network for Intra-Patient Arrhythmia Classification.](http://arxiv.org/abs/2310.15466) | EKGNet是一种全模拟的神经网络架构，用于心电图心律失常分类。它利用模拟计算和亚阈值晶体管的能量效率，消除了对模拟到数字转换器和静态随机存储器的需求。实验结果表明，EKGNet在内患者心律失常分类和心肌梗死分类中表现出色，具有很大的潜力。 |
| [^59] | [Private Learning with Public Features.](http://arxiv.org/abs/2310.15454) | 本研究针对具有私有和公共特征的个人化学习问题进行了研究，并提出了一种新的算法，通过只保护特定的统计量来提高实用性，在两个标准私有推荐基准测试中达到了最新的成果。 |
| [^60] | [General Identifiability and Achievability for Causal Representation Learning.](http://arxiv.org/abs/2310.15450) | 本文在通用非参数因果潜变量模型和通用转换模型下，通过非耦合干预建立了因果表达学习的可识别性和可实现性结果。在不知道具体干预对应的节点的情况下，这些结果保证了潜在的因果模型和变量的完美恢复，并设计了一个算法来实现这一目标。 |
| [^61] | [An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems.](http://arxiv.org/abs/2310.15448) | 本文提出了一种加速的算法来解决随机非凸-凹极小极大问题，并证明了该算法迭代复杂度达到了最佳已知界限。 |
| [^62] | [Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing.](http://arxiv.org/abs/2310.15440) | 该论文对线性VAE中的学习动态进行了理论分析，证明了在大输入维度的限制下，学习动态收敛为确定性过程，并提出了后验崩塌阈值和KL退火加速策略。研究发现，VAE最初学习到纠缠表示，并逐渐获得不纠缠的表示。在确定性过程中，超fluous潜在空间是一个潜在的问题。 |
| [^63] | [Off-Policy Evaluation for Large Action Spaces via Policy Convolution.](http://arxiv.org/abs/2310.15433) | 本研究提出了一种名为策略卷积（PC）的离策略估计方法，该方法通过动作嵌入来解决大动作空间下的分布转移问题，可以在偏差和方差之间进行权衡 |
| [^64] | [The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation.](http://arxiv.org/abs/2310.15425) | 本文介绍了一种基于深度神经网络和插值的新型强制对齐系统，该系统名为Mason-Alberta音标分割器。该系统的创新点包括将声学模型视为标注任务，而不是分类任务，并采用了插值技术实现更精确的分段边界。 |
| [^65] | [Fractal Landscapes in Policy Optimization.](http://arxiv.org/abs/2310.15418) | 该论文研究了政策优化过程中非平滑或分形的优化景观，提出了一种理解策略梯度方法固有限制的框架，并开发了一种实用方法来识别训练过程中是否遇到分形景观。 |
| [^66] | [Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction.](http://arxiv.org/abs/2310.15416) | 本文提出了一种利用基于点/顺序重构模型的框架来进行无监督时间序列异常检测的方法。通过计算重构误差的组合值之比得到命名分数, 进一步结合命名分数和异常分数导出感应异常分数，从而实现对点异常和上下文异常的量化和检测。 |
| [^67] | [Diverse Conventions for Human-AI Collaboration.](http://arxiv.org/abs/2310.15414) | 本研究通过最大化自我对弈的奖励并最小化与先前发现的约定交互时的奖励来生成多样化约定，确保学到的策略在交叉对弈的对抗性优化过程中遵守善意行事 |
| [^68] | [Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach.](http://arxiv.org/abs/2310.15411) | 这个论文研究了在结构化无标签数据分布下，对于具有Tsybakov噪声的$d$维半空间，计算和标签的高效PAC主动学习问题。通过设计一种基于非凸优化的算法，它能够在一定的噪声参数范围内达到较低的标签复杂度。 |
| [^69] | [DoGE: Domain Reweighting with Generalization Estimation.](http://arxiv.org/abs/2310.15393) | DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。 |
| [^70] | [MEMPSEP III. A machine learning-oriented multivariate data set for forecasting the Occurrence and Properties of Solar Energetic Particle Events using a Multivariate Ensemble Approach.](http://arxiv.org/abs/2310.15390) | 这个论文介绍了一个面向机器学习的多元数据集，用于预测太阳能粒子事件的发生和属性。数据集包含了多个航天器收集的原位和遥感观测数据，能够与产生太阳能粒子事件的物理过程相关联。 |
| [^71] | [Irreducible Curriculum for Language Model Pretraining.](http://arxiv.org/abs/2310.15389) | 本论文提出了一种不可约课程算法，用于语言模型预训练，通过优先选择具有更高学习能力的样本，并使用小规模代理模型模拟样本丢失，从而在大型语言模型上解决了传统数据选择方法的困难，并在实验证明算法能够持续改进模型性能。 |
| [^72] | [Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training.](http://arxiv.org/abs/2310.15388) | 该论文提出了一种利用自监督学习的方法，在智能环境中通过视频进行远程心率监测和估计，减少了对标记数据的依赖并提高了性能。 |
| [^73] | [Error analysis of generative adversarial network.](http://arxiv.org/abs/2310.15387) | 该论文利用Talagrand不等式和Borel-Cantelli引理，建立了生成对抗网络（GAN）的误差紧致收敛速度，并提供了改进的收敛速度方法。 |
| [^74] | [Course Correcting Koopman Representations.](http://arxiv.org/abs/2310.15386) | 本文修正了Koopman表示的方法，并提出了一种称为“周期重新编码”的机制，用于准确捕捉非线性动力系统中的长期动态。 |
| [^75] | [Vicinal Feature Statistics Augmentation for Federated 3D Medical Volume Segmentation.](http://arxiv.org/abs/2310.15371) | 本论文提出了一种基于邻近特征统计增强的联邦三维医学体积分割方案，通过数据增强来提高FL分割的性能，并解决了标记数据有限和数据分布异构的问题。 |
| [^76] | [Learning Fair Representations with High-Confidence Guarantees.](http://arxiv.org/abs/2310.15358) | 本文提出了一个具有高概率公平性保证的公平表示学习框架（FRG），通过用户定义的上界限制，在所有下游模型和任务中减少不公平性。实证评估结果证明了FRG的有效性。 |
| [^77] | [Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency.](http://arxiv.org/abs/2310.15351) | 本论文研究了在贝叶斯优化中使用随机探索的方法，并证明了其能够实现最佳的误差率和最优遗憾保证。同时，所提出的算法通过随机探索避免了每次迭代中非凸获取函数的昂贵优化，具有计算上的优势。 |
| [^78] | [Burgers' pinns with implicit euler transfer learning.](http://arxiv.org/abs/2310.15343) | 本研究将物理构建的神经网络（PINNs）与隐式欧拉迁移学习方法相结合，应用于求解Burgers方程。通过一系列人工神经网络，从前一个网络向下一个网络传递知识，并通过最小化基于隐式欧拉近似的损失函数，学习当前时间解。与通常的PINN模型相比，该方法具有更小的神经网络结构，同时具有类似准确结果和潜在优势。 |
| [^79] | [Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network.](http://arxiv.org/abs/2310.15342) | 本论文提出了一种针对深度稀疏网络的混合粒度特征交互选择方法，能够同时考虑特征域和特征值，实验证明该方法在准确性和效率方面表现良好。 |
| [^80] | [ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training.](http://arxiv.org/abs/2310.15334) | 本论文设计了一系列序列和并行的ADMM算法解决残差网络训练问题，并通过理论分析证明了算法的收敛性和收敛速度，同时分析了并行实现的时间复杂性和内存消耗的优势。 |
| [^81] | [Estimating Trustworthy and Safe Optimal Treatment Regimes.](http://arxiv.org/abs/2310.15333) | 这篇论文提出了一种安全和可解释的框架，通过匹配患者的医学和药物特性来识别最佳治疗方案。研究结果表明，个性化的治疗策略可以根据患者的病史和药物特征来制定，并发现减少药物剂量可以减轻病情而不会对治疗效果产生负面影响。 |
| [^82] | [Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks.](http://arxiv.org/abs/2310.15330) | 本文介绍了一种针对带有异构混合比例的混合模型的无监督学习的新型联邦梯度EM算法，在适用于普通混合模型的全面有限样本理论基础上，对高斯混合模型（GMM）和混合回归（MoRs）进行了具体的估计误差分析。该算法具有适应未知任务相似性、抵抗对少部分数据源的对抗攻击、保护本地数据隐私以及计算和通信效率等关键优势。 |
| [^83] | [Serverless Federated Learning with flwr-serverless.](http://arxiv.org/abs/2310.15329) | flwr-serverless是一种无服务器联邦学习的方法，能够有效训练来自不同来源的数据，同时不损害安全和隐私。 |
| [^84] | [DeepVox and SAVE-CT: a contrast- and dose-independent 3D deep learning approach for thoracic aorta segmentation and aneurysm prediction using computed tomography scans.](http://arxiv.org/abs/2310.15328) | 通过使用DeepVox和SAVE-CT模型，该论文提出了一种无关对比度和剂量的三维深度学习方法，用于胸主动脉分割和瘤样动脉扩张预测的计算机断层扫描。新模型在测试中展现出较高的Dice系数，并且训练速度更快。 |
| [^85] | [LXMERT Model Compression for Visual Question Answering.](http://arxiv.org/abs/2310.15325) | 本文通过组合大规模预训练模型的观察结果，并评估在视觉问答任务中对LXMERT进行微调时的可训练子网络，研究了LXMERT模型的压缩。实验结果表明，在仅损失3%的精度下，可以有效地通过剪枝方法将LXMERT模型大小减小40%-60%。 |
| [^86] | [Hallucination Detection for Grounded Instruction Generation.](http://arxiv.org/abs/2310.15319) | 该论文研究了生成指导人类在模拟环境中导航的说明的问题，通过预训练模型并使用对比损失进行微调，提出了一种检测幻觉参考的模型，该模型在性能上超过了几个基线模型。 |
| [^87] | [HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2310.15318) | HetGPT是一种预训练异构图神经网络的方法，通过利用提示调整来解决预训练与下游任务之间的不匹配问题。 |
| [^88] | [SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding.](http://arxiv.org/abs/2310.15308) | 该论文提出了一种将视觉基础模型合并为一个统一模型的方法，通过集成多任务学习、持续学习技术和师生蒸馏，实现了显著较少的计算成本和较少的预训练数据需求。通过应用该方法于SAM和CLIP，得到了一个统一模型SAM-CLIP，将两者的优势融合在一起。 |
| [^89] | [ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease.](http://arxiv.org/abs/2310.15301) | ADMarker是一个多模式联邦学习系统，用于在自然生活环境中监测阿尔茨海默病的数字生物标志物。它具有新颖的联邦学习架构，能够准确检测出数字生物标志物，并在临床试验中展示出高准确率和早期AD识别能力。 |
| [^90] | [Neural Network with Local Converging Input (NNLCI) for Supersonic Flow Problems with Unstructured Grids.](http://arxiv.org/abs/2310.15299) | 本研究开发了一种具有局部收敛输入的神经网络（NNLCI），用于使用非结构化数据进行高保真预测。该方法大大降低了计算资源和训练时间，并在无粘超音速流问题中展示了有效性和多功能性。 |
| [^91] | [Fast and Reliable Generation of EHR Time Series via Diffusion Models.](http://arxiv.org/abs/2310.15290) | 本研究通过使用扩散模型提出了一种快速可靠生成EHR时间序列数据的新方法，该方法在数据效用方面明显优于现有方法，并且对训练工作的需求更少。同时，该方法还提供了多样化和真实的合成EHR数据，增强了下游医疗数据分析。 |
| [^92] | [Active teacher selection for reinforcement learning from human feedback.](http://arxiv.org/abs/2310.15288) | 本论文提出了一个用于强化学习中的主动教师选择模型以解决多教师的学习问题，研究表明该模型在论文推荐系统和COVID-19疫苗测试领域具有优越性能，并揭示了利用教师间差异来学习准确奖励模型的重要性。 |
| [^93] | [A Doubly Robust Approach to Sparse Reinforcement Learning.](http://arxiv.org/abs/2310.15286) | 我们提出了一种新的遗憾最小化算法，用于稀疏强化学习问题，通过结合双重稳健方法和新颖的分析技术，克服了之前算法对稀疏参数和未知策略的限制。 |
| [^94] | [UncertaintyPlayground: A Fast and Simplified Python Library for Uncertainty Estimation.](http://arxiv.org/abs/2310.15281) | UncertaintyPlayground是一款基于PyTorch和GPyTorch的Python库，可以快速进行不确定性估计和可视化预测区间，并提供了多种速度优化技术。 |
| [^95] | [Triple Simplex Matrix Completion for Expense Forecasting.](http://arxiv.org/abs/2310.15275) | 本文提出了一种使用三重单纯形矩阵完成方法进行费用预测的模型。该模型通过学习项目与潜在空间中的费用模式相关性来预测费用，同时满足预算约束并保证预测结果的准确性。 |
| [^96] | [Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI Grand Challenges.](http://arxiv.org/abs/2310.15274) | 本论文讨论了面临能源、对齐和从狭义人工智能到AGI的三大挑战的系统化人工智能方法。现有的人工智能方法在能源消耗、系统设计和对齐问题上存在不足，而系统设计在解决对齐、能源和AGI大挑战中是至关重要的。 |
| [^97] | [GradSim: Gradient-Based Language Grouping for Effective Multilingual Training.](http://arxiv.org/abs/2310.15269) | 在本文中，我们提出了一种基于梯度相似性的语言分组方法，名为GradSim。它通过选择最合适的语言集合进行多语言训练，避免了负面干扰，并在多个基准数据集上取得了最先进的性能表现。此外，我们的分析还揭示了数据集的主题对模型的性能也起着重要作用。 |
| [^98] | [One-hot Generalized Linear Model for Switching Brain State Discovery.](http://arxiv.org/abs/2310.15263) | 本论文提出了一种先验信息驱动的状态切换广义线性模型，可以揭示动态变化的功能性相互作用，从而对潜在的解剖连通图有所启示。 |
| [^99] | [Modality Dropout for Multimodal Device Directed Speech Detection using Verbal and Non-Verbal Features.](http://arxiv.org/abs/2310.15261) | 本论文研究了在语音识别中处理模态丢失问题的方法，并通过将韵律特征与语言线索结合使用，提高了DDSD性能。 |
| [^100] | [Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards.](http://arxiv.org/abs/2310.15259) | 这项研究提出了一种解决噪声环境中问题翻译挑战的方法，通过只使用源语数据进行微调的训练，实现了翻译问题的充分性和流畅性的平衡。 |
| [^101] | [SimBIG: Field-level Simulation-Based Inference of Galaxy Clustering.](http://arxiv.org/abs/2310.15256) | SimBIG是一个基于模拟的推断方法，通过对星系团集群的场景级分析，利用非线性和非高斯特性，获得了比传统方法更严格的关于宇宙参数和哈勃常量的约束。 |
| [^102] | [SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis.](http://arxiv.org/abs/2310.15247) | SyncFusion是一个多模态同步的音视频霍利合成系统，能够从视频中提取重复的动作起始点，并使用音频或文本嵌入来生成新的同步音效音轨，为声音设计师提供了完全的创作控制权并简化了流程。 |
| [^103] | [Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects.](http://arxiv.org/abs/2310.15234) | 本文提出了一种使用星系目录进行场级别的基于模拟的推断方法，能够鲁棒地推断出宇宙学参数，解决了观测受到的系统效应的影响。 |
| [^104] | [A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude.](http://arxiv.org/abs/2310.15233) | 该论文提出了一种新方法，通过在引力波模板库中包含高次谐波模式，利用引力波模式之间的自然关联，可以大幅度减少匹配滤波的成本，并提高搜索引力波事件的灵敏度。 |
| [^105] | [Function Vectors in Large Language Models.](http://arxiv.org/abs/2310.15213) | 大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。 |
| [^106] | [Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing.](http://arxiv.org/abs/2310.15211) | 该论文提出了一种基于网络的新方法（MPI）来有效进行阿尔茨海默病药物重用。该方法通过学习节点嵌入来优先考虑重要路径，从而更好地发现候选药物。 |
| [^107] | [Mid-Long Term Daily Electricity Consumption Forecasting Based on Piecewise Linear Regression and Dilated Causal CNN.](http://arxiv.org/abs/2310.15204) | 该研究提出了一种基于分段线性回归和空洞因果卷积神经网络的中长期日电力消耗预测方法，通过将电力消耗序列分解为趋势、季节性和残差三个部分，并采用两阶段预测方法，结合了过滤器和预测器的建模来提高预测准确性。实验证明该方法具有更高的准确性。 |
| [^108] | [Predicting Transcription Factor Binding Sites using Transformer based Capsule Network.](http://arxiv.org/abs/2310.15202) | 本文提出了一种基于Transformer的胶囊网络DNABERT-Cap，用于预测转录因子结合位点。该模型在预测中利用了双向编码器、胶囊层、卷积和双向长短时记忆层的特征，并通过对这些特征的联合优化构建了转录因子结合位点的预测器。 |
| [^109] | [Can strong structural encoding reduce the importance of Message Passing?.](http://arxiv.org/abs/2310.15197) | 本研究探索了提供强结构编码时消息传递的贡献，通过引入特征和结构信息之间的张量乘积交互方式，比较了不同情景下的选择，甚至在容量受限的情况下完全移除了消息传递阶段。 |
| [^110] | [Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization.](http://arxiv.org/abs/2310.15196) | 本研究提出了一种高效的元神经启发式算法（EMNH），通过训练一个元模型并进行微调，来解决多目标组合优化问题。实验结果表明，EMNH在学习效率和解决质量上取得了显著改进。 |
| [^111] | [Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement.](http://arxiv.org/abs/2310.15195) | 提出了一种具有多样性增强的神经启发式方法（NHDE）来解决神经多目标组合优化（MOCO）问题，该方法通过引入指示器增强的深度强化学习方法和多重帕累托最有策略，能够产生更多且具有更高多样性的帕累托解。 |
| [^112] | [Application of deep and reinforcement learning to boundary control problems.](http://arxiv.org/abs/2310.15191) | 本研究将深度学习和强化学习应用于解决边界控制问题，通过使用空间神经网络构建初始猜测以及利用策略梯度方法学习迭代优化算法的时空神经网络，通过使用生成的合成数据进行训练和验证，研究发现深度学习和强化学习方法在解决边界控制问题方面具有潜在优势。 |
| [^113] | [Towards Subject Agnostic Affective Emotion Recognition.](http://arxiv.org/abs/2310.15189) | 本文提出了一种基于元学习的增强领域适应框架，用于处理主体无关的情感脑机接口。该方法有效解决了脑电信号中的分布偏移问题，具有较好的结果。 |
| [^114] | [Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic Fiber Composites.](http://arxiv.org/abs/2310.15188) | 本文利用深度学习方法，将微结构与力学性能建立映射关系，加快了设计过程，并实现了根据期望性能生成微结构的目标。 |
| [^115] | [Reducing Uncertainty in Sea-level Rise Prediction: A Spatial-variability-aware Approach.](http://arxiv.org/abs/2310.15179) | 这篇论文提出了一种基于区域回归模型的方法，通过解决空间可变性和模型相互依赖的问题，准确可靠地预测未来海平面上升，并降低不确定性。 |
| [^116] | [Ghost on the Shell: An Expressive Representation of General 3D Shapes.](http://arxiv.org/abs/2310.15168) | 这里是中文总结出的一句话要点 |
| [^117] | [Meta- (out-of-context) learning in neural networks.](http://arxiv.org/abs/2310.15047) | 该研究通过合成实验展示了一种称为元-超文本外语境学习（meta-OCL）的现象在神经网络中的存在。这种学习使神经网络能够更好地吸收广泛适用的语义内容，并在适当的情况下进行使用。研究者提出了关于元-超文本外语境学习产生的两种假设，并就未来AI系统的能力和潜在风险进行了讨论。 |
| [^118] | [Meta learning with language models: Challenges and opportunities in the classification of imbalanced text.](http://arxiv.org/abs/2310.15019) | 本文提出了一种元学习技术(MLT)，通过将不同文本表示构建的个体模型进行组合，在不平衡的文本分类中提高了性能，并通过阈值移动技术进一步改善了预测器的性能。 |
| [^119] | [Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries.](http://arxiv.org/abs/2310.14948) | 本研究提出了物理信息图卷积网络作为解决复杂几何体中偏微分方程问题的广义框架，并结合经典数值求解器从而解决了物理信息框架在复杂几何体上的问题。 |
| [^120] | [Revisiting Implicit Differentiation for Learning Problems in Optimal Control.](http://arxiv.org/abs/2310.14468) | 本文提出了一种新方法，通过隐函数定理对非凸、受约束的最优控制问题中的最优轨迹进行微分。与先前的方法相比，该方法可以线性缩放地计算轨迹导数，可以容易地进行并行化，并且在模型规模、数值稳定性和计算效率方面表现出色。 |
| [^121] | [Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms.](http://arxiv.org/abs/2310.14168) | 该论文介绍了一种随机前向模式自动微分优化算法，通过在神经网络的正向传递中计算损失函数的方向导数来更新参数。算法通过采样不同概率分布的随机方向，使用正向模式自动微分计算雅可比向量乘积，并提供了对其收敛速度和计算复杂性的严格分析。 |
| [^122] | [Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback.](http://arxiv.org/abs/2310.14085) | 本文提出了一个自适应的OGD算法\textsf{AdaOGD}，在强凸性下实现了$ O(\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。 |
| [^123] | [Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series.](http://arxiv.org/abs/2310.14017) | 本论文提出了一个名为COMET的创新层次对比框架，用于医疗时间序列分析。该框架通过在多个层级上开发对比损失，可以充分利用医疗时间序列的复杂特性，并实现自监督学习。使用多个数据集进行实验验证了COMET的有效性。 |
| [^124] | [Contrastive Preference Learning: Learning from Human Feedback without RL.](http://arxiv.org/abs/2310.13639) | 对比偏好学习方法解决了传统强化学习从人类反馈中学习奖励函数的假设错误以及优化挑战的问题。 |
| [^125] | [Towards Understanding Sycophancy in Language Models.](http://arxiv.org/abs/2310.13548) | 这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。 |
| [^126] | [Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions.](http://arxiv.org/abs/2310.13129) | 本研究提出了一种基于深度强化学习的智能交通信号控制算法，通过优化CO2排放和行驶时间等指标，实现了较好的性能。 |
| [^127] | [Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study.](http://arxiv.org/abs/2310.13123) | 本文研究了使用机器学习和业务数据预测客运渡轮燃料消费的方法，并通过选择适当的输入变量来提高预测性能和实际可应用性。 |
| [^128] | [Understanding Addition in Transformers.](http://arxiv.org/abs/2310.13121) | 本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。 |
| [^129] | [Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning.](http://arxiv.org/abs/2310.12952) | 该论文提出了一种基于相似性的多样性度量方法——Vendi分数族群，为科学和机器学习等领域准确测量多样性提供了灵活性。 |
| [^130] | [Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems.](http://arxiv.org/abs/2310.12046) | 本文探讨了在贝叶斯逆问题中，使用机器学习为基础的代理模型，以提高计算效率和处理数值挑战问题。 |
| [^131] | [Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?.](http://arxiv.org/abs/2310.11978) | 本文研究了分组缩放方法（BVS）的几种改进方法，探索了使用替代损失函数和基于输入特征的分组方案来提高机器学习回归的预测不确定性的一致性和适应性。 |
| [^132] | [On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction.](http://arxiv.org/abs/2310.11479) | 该论文探讨了将温度参数纳入贝叶斯图神经网络在一致预测中的优势，以提供有效的不确定性量化。 |
| [^133] | [A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems.](http://arxiv.org/abs/2310.08644) | 这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。 |
| [^134] | [Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis.](http://arxiv.org/abs/2310.05374) | 本论文提出了一种名为LaSyn的文本数据利用框架，通过将文本数据转换为中间潜变表示来增强端到端语音处理模型的训练。在低资源环境下的语音识别和口语理解任务中，LaSyn相对词错误率减少了22.3%，绝对意图分类准确率提高了4.1%。 |
| [^135] | [OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon.](http://arxiv.org/abs/2310.03388) | OpenPatch是一个3D拼贴，基于大型预训练模型提取中间特征来描述每个已知类别的补丁表示，在超出分布检测中取得了进展。 |
| [^136] | [Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation.](http://arxiv.org/abs/2310.01320) | 本研究通过使用复杂的Avalon游戏作为测试平台，引入了一种名为递归思考（ReCon）的新框架，用于增强大型语言模型（LLM）识别和对抗欺骗信息的能力。 |
| [^137] | [Explainable machine learning-based prediction model for diabetic nephropathy.](http://arxiv.org/abs/2309.16730) | 本研究提出了一个可解释的基于机器学习的糖尿病肾病预测模型，通过分析血清代谢物对疾病的影响并选择最优特征来预测疾病的患病率。最优模型采用了极限梯度提升（XGB）算法，其在筛选DN方面表现最佳，同时具有更好的临床效益和拟合度。 |
| [^138] | [Leveraging Deep Learning and Online Source Sentiment for Financial Portfolio Management.](http://arxiv.org/abs/2309.16679) | 本文研究利用深度学习方法进行金融交易，并考虑情绪信息的作用。同时讨论常见的训练问题，并提供应用方法。 |
| [^139] | [Exploiting the Signal-Leak Bias in Diffusion Models.](http://arxiv.org/abs/2309.15842) | 本文展示了如何利用现有扩散模型中的信号泄漏偏差，以实现对生成图像的更好控制，并生成更多样化的亮度以及更满足特定风格和颜色要求的图像。 |
| [^140] | [Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels.](http://arxiv.org/abs/2309.14518) | 本文提出了一种名为Detach-ROCKET的方法，用于时间序列分类中的顺序特征选择。通过利用随机卷积核模型中的大量特征，并使用顺序特征分离方法剪枝非主要特征，提高了模型的可扩展性和泛化能力。 |
| [^141] | [Lifelong Robot Learning with Human Assisted Language Planners.](http://arxiv.org/abs/2309.14321) | 本文介绍了一种利用大型语言模型(LLM)进行终身机器人学习的方法，该方法可以使机器人查询和学习新的技能，并在刚性物体操作方面实现数据和时间高效。该系统具有重复使用新获得技能的能力，展示了开放世界和终身学习的潜力。 |
| [^142] | [Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations.](http://arxiv.org/abs/2309.13837) | 本文介绍了一种先进的分析方法，用于预测库存管理中的缺货情况。该方法考虑了多种分类技术和成本考虑，通过提高服务水平来提高客户满意度和整体组织绩效。 |
| [^143] | [Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors.](http://arxiv.org/abs/2309.13135) | 该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。 |
| [^144] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^145] | [How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization.](http://arxiv.org/abs/2309.12671) | 本文提出了一个统一模型偏移和模型偏差的优化目标，并通过微调过程实现了自适应的模型更新，以提供性能改进保证和避免模型过拟合。 |
| [^146] | [Efficient Concept Drift Handling for Batch Android Malware Detection Models.](http://arxiv.org/abs/2309.09807) | 本文研究了批量安卓恶意软件检测模型的高效概念漂移处理方法，通过重新训练技术来维持检测器的能力，并通过比较不同的重新训练频率和数据使用方法的影响，提出了改进策略。 |
| [^147] | [Scalable neural network models and terascale datasets for particle-flow reconstruction.](http://arxiv.org/abs/2309.06782) | 本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。 |
| [^148] | [Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit.](http://arxiv.org/abs/2308.10238) | 这篇论文介绍了一种名为广义汤普森抽样探索算法，能够解决多臂老虎机实值组合纯探索问题中动作集合大小为指数级别的情况。 |
| [^149] | [Path Signatures for Seizure Forecasting.](http://arxiv.org/abs/2308.09312) | 本研究以癫痫预测为目标，通过自动发现和量化患者特定的统计特征，特别是最新的路径签名算法，探索其在癫痫预测中的性能，为个性化的癫痫预测解决方案提供了参考。 |
| [^150] | [TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties.](http://arxiv.org/abs/2308.03051) | 这项研究对Bard和ChatGPT在十种阿拉伯语变体的机器翻译能力进行了评估，发现LLM在翻译方言方面表现优于商业系统，但在古典阿拉伯语和现代标准阿拉伯语方面落后于谷歌翻译等商业系统。 |
| [^151] | [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.](http://arxiv.org/abs/2308.02490) | MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。 |
| [^152] | [DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation.](http://arxiv.org/abs/2307.15317) | 本文提出了一种利用可微分Kendall排名相关性进行少样本学习的新方法，证明了特征通道的重要性排序在少样本学习中比几何相似度度量更可靠，并且实验证明在推理过程中用Kendall排名相关性替换几何相似度度量能够提高少样本学习性能。 |
| [^153] | [Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?.](http://arxiv.org/abs/2307.14642) | 本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。 |
| [^154] | [Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior.](http://arxiv.org/abs/2307.14619) | 本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。 |
| [^155] | [Efficient Estimation of the Local Robustness of Machine Learning Models.](http://arxiv.org/abs/2307.13885) | 本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。 |
| [^156] | [WebArena: A Realistic Web Environment for Building Autonomous Agents.](http://arxiv.org/abs/2307.13854) | WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。 |
| [^157] | [Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control.](http://arxiv.org/abs/2307.12388) | 本文提出了UGAT方法，通过在模拟环境中动态转换具有不确定性的行动，实现了从模拟环境到真实环境的策略转移，显著提高了在真实世界中的性能。 |
| [^158] | [Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering.](http://arxiv.org/abs/2307.11030) | 这项工作首次从理论上理解了关系知识蒸馏，通过种群上的谱聚类，我们证明了关系知识蒸馏能够导致低聚类误差，并展示了它在半监督学习中的标签效率。 |
| [^159] | [Conformal prediction under ambiguous ground truth.](http://arxiv.org/abs/2307.09302) | 本文提出了一种适用于含有模糊地面真相的符合预测框架，解决了在缺乏明确地面真相标签的情况下低估不确定性的问题。 |
| [^160] | [RADAR: Robust AI-Text Detection via Adversarial Learning.](http://arxiv.org/abs/2307.03838) | 本论文提出了一种名为RADAR的新框架，通过对抗性学习实现了鲁棒的AI文本检测，以解决当前AI文本检测器对于大语言模型的改写不具备鲁棒性的问题。 |
| [^161] | [Quantification of Uncertainty with Adversarial Models.](http://arxiv.org/abs/2307.03217) | 该论文提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。与先前的方法相比，QUAM对认知不确定性的近似误差更小。 |
| [^162] | [Improving Fairness in Deepfake Detection.](http://arxiv.org/abs/2306.16635) | 本研究首次尝试通过提出新的损失函数来改善深度伪造检测的公平性，并在多个数据集和检测器上进行了广泛实验证明了其有效性。 |
| [^163] | [ViNT: A Foundation Model for Visual Navigation.](http://arxiv.org/abs/2306.14846) | 该论文介绍了一种名为ViNT的基础模型，旨在将通用预训练模型的成功应用于视觉导航领域。ViNT通过通用目标达成目标进行训练，并采用灵活的Transformer架构来学习导航功能和实现对各种导航任务的高效适应。 |
| [^164] | [Learning Unseen Modality Interaction.](http://arxiv.org/abs/2306.12795) | 本文提出了一个解决多模态学习中未见过的模态组合的问题的方法。该方法利用一个模块将不同模态的特征投影到一个共享的空间中，并通过伪监督来减少过拟合。实验证明该方法在多个任务和模态上都是有效的。 |
| [^165] | [A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design.](http://arxiv.org/abs/2306.11768) | 本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。 |
| [^166] | [Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.](http://arxiv.org/abs/2306.09347) | 本研究引入了一种名为Seal的新型框架，利用视觉基础模型(VFMs)对汽车点云序列进行分割。Seal具有可伸缩性、一致性和泛化能力等优势，并通过大量实验展示了其有效性和优越性。 |
| [^167] | [Differentially Private Conditional Independence Testing.](http://arxiv.org/abs/2306.06721) | 本文介绍了两个差分隐私条件独立性检验方法，可适用于Z为连续值的一般情况。 |
| [^168] | [Conformal Prediction for Federated Uncertainty Quantification Under Label Shift.](http://arxiv.org/abs/2306.05131) | 该论文提出了一种基于分位数回归的新型联邦合规性预测方法，该方法利用重要性加权有效地解决了代理之间的标签漂移问题，并为预测集的有效覆盖和差分隐私提供了理论保证。广泛的实验结果表明，该方法优于目前的竞争对手。 |
| [^169] | [Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC.](http://arxiv.org/abs/2306.04712) | 本文利用可微分的快速逼近方法，训练了一个编码器神经网络用于高亮LHC数据的压缩，同时保留了数据内与粒子探测器中的能量沉积分布相关的信息。 |
| [^170] | [State Regularized Policy Optimization on Data with Dynamics Shift.](http://arxiv.org/abs/2306.03552) | 本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。 |
| [^171] | [Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning.](http://arxiv.org/abs/2305.17297) | 本论文研究了具有低秩结构但非独立同分布数据的情况，在分离训练和测试分布的假设下，解决了分布偏移问题，实验结果表明，在分布偏移的情况下，本方法显著提高了泛化误差的性能。 |
| [^172] | [Empirical Optimal Transport between Conditional Distributions.](http://arxiv.org/abs/2305.15901) | 本文考虑在一个公共变量的条件下，相应分布之间的最优输运问题。通过采用基于 MMD 的核正则化器，克服了条件变量是连续的和两个分布中该变量的边缘是不同的挑战。 |
| [^173] | [Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model.](http://arxiv.org/abs/2305.13999) | 这项研究提出了一个统一的框架来分析稀疏前馈网络在预训练大型语言模型中的设计选择。在语言建模任务中，通过使用平均聚合隐藏状态的选择方法，相比现有的MoE架构，可以实现更低的困惑度。 |
| [^174] | [SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes.](http://arxiv.org/abs/2305.13998) | SMT 2.0是一个开源的代理模型工具包，引入了处理混合变量和层次变量的能力，并通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。 |
| [^175] | [KineticNet: Deep learning a transferable kinetic energy functional for orbital-free density functional theory.](http://arxiv.org/abs/2305.13316) | 论文介绍了如何从Kohn-Sham密度泛函理论提供的真实数据中学习动能函数，并以此来促进轨道自由密度泛函理论的实际应用。 |
| [^176] | [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.](http://arxiv.org/abs/2305.13245) | 该论文介绍了一种将现有的多头语言模型检查点升级为具有多查询注意力（MQA）的模型的方法，并引入了群组查询注意力（GQA）来解决MQA可能导致的质量下降问题。通过升级后的GQA模型，实现了接近多头注意力的质量，并具备与MQA相当的速度。 |
| [^177] | [Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.](http://arxiv.org/abs/2305.13035) | 本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。 |
| [^178] | [Revisiting the Minimalist Approach to Offline Reinforcement Learning.](http://arxiv.org/abs/2305.09836) | 这篇论文提出了一种名为ReBRAC的极简算法，它在TD3+BC方法的基础上整合了设计元素，通过对近期离线强化学习研究的回顾性分析，证明其在离线强化学习上的领先地位。 |
| [^179] | [Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images.](http://arxiv.org/abs/2305.07644) | 扩散模型在医学图像合成中可能会导致记忆训练图像的问题，研究人员在选择合适的模型时需要谨慎。 |
| [^180] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^181] | [A mean-field games laboratory for generative modeling.](http://arxiv.org/abs/2304.13534) | 本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。 |
| [^182] | [Diffusion Model for GPS Trajectory Generation.](http://arxiv.org/abs/2304.11582) | 该论文提出了基于扩散模型的GPS轨迹生成框架，通过将真实轨迹逐渐转换为噪声，再从噪声重构伪造的轨迹，以达到生成隐私信息保护的高质量轨迹的目的。 |
| [^183] | [Multi-annotator Deep Learning: A Probabilistic Framework for Classification.](http://arxiv.org/abs/2304.02539) | 该论文提出了一个名为多注释深度学习（MaDL）的概率训练框架，可以在由易错注释者提供的有噪音类别标签上进行端到端的联合训练，并有效地解决多注释监督学习中的次优表现问题。 |
| [^184] | [DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps.](http://arxiv.org/abs/2304.00133) | DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。 |
| [^185] | [Promoting Non-Cooperation Through Ordering.](http://arxiv.org/abs/2303.17971) | 本文研究了如何通过排序方式，在处罚的个人中激励非合作行为，对于中央管理部门能够显著增加总付款。 |
| [^186] | [CoLT5: Faster Long-Range Transformers with Conditional Computation.](http://arxiv.org/abs/2303.09752) | CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。 |
| [^187] | [CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network.](http://arxiv.org/abs/2303.03387) | CoSyn是一个上下文协同的神经网络，用于检测在线对话中的隐含仇恨言论。它通过引入新的编码方法和上下文交互机制，在双曲空间中进行操作，以适应社交媒体的特点。 |
| [^188] | [Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width.](http://arxiv.org/abs/2302.12250) | 我们研究了深度神经网络早期训练动力学的相图，发现四种不同的状态，并发现了一个“陡峭度减小”相位的出现。 |
| [^189] | [Derandomized Novelty Detection with FDR Control via Conformal E-values.](http://arxiv.org/abs/2302.07294) | 通过使用统一E-值来量化统计显著性，我们提出了一种去随机化的新颖性检测方法，该方法可以稳定地聚合相同数据的多次分析的证据，同时控制虚假发现率。 |
| [^190] | [Efficient Graph Laplacian Estimation by Proximal Newton.](http://arxiv.org/abs/2302.06434) | 本文提出了一种基于Proximal Newton算法的高效图拉普拉斯估计方法，通过引入非凸minimax concave penalty，并利用二阶优化方法和几个算法技巧，实现了准确且高效的求解器。 |
| [^191] | [Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion.](http://arxiv.org/abs/2302.04451) | 本文提出了用特征扩散矩阵的最大奇异值来缩放泛化界限的方法，并用Hessians来衡量图神经网络对噪声扰动的稳定性。实验证明，这些方法可以有效减小泛化界限，更好地解决了实际图形问题。 |
| [^192] | [A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions.](http://arxiv.org/abs/2302.04032) | 这项工作通过系统评估多种常用的预训练网络及其不同特征提取点，在四个深度感知损失用例上解决了迁移学习中的问题。 |
| [^193] | [Data Selection for Language Models via Importance Resampling.](http://arxiv.org/abs/2302.03169) | 通过重要性重采样方法，我们提出了一种高效且可扩展的数据选择框架（DSIR），可以在语言模型中选择适合的预训练数据集。我们使用KL减少作为数据度量来确定合适的特征空间，并在降维特征空间中估计重要性权重以进行数据选择。 |
| [^194] | [NA-SODINN: a deep learning algorithm for exoplanet image detection based on residual noise regimes.](http://arxiv.org/abs/2302.02854) | NA-SODINN是一种基于残余噪声模式的深度学习算法，通过识别噪声相关性来提高外行星图像检测性能。 |
| [^195] | [Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective.](http://arxiv.org/abs/2302.01735) | 本文提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。 |
| [^196] | [Learning Large-scale Neural Fields via Context Pruned Meta-Learning.](http://arxiv.org/abs/2302.00617) | 通过上下文修剪元学习实现大规模神经场训练的优化， 显著节省内存，并能在短时间内学习高质量神经场。 |
| [^197] | [Zero-One Laws of Graph Neural Networks.](http://arxiv.org/abs/2301.13060) | 本文提出了一个新的理论研究视角，回答了当图节点数量变得非常大时GNN的行为如何的问题。通过证明不断增大的图映射到GNN分类器的特定输出的概率趋于零或一，建立了这些GNN的零一定律，限制了它们的能力。实验证实了理论结论。 |
| [^198] | [Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion.](http://arxiv.org/abs/2301.11757) | 本研究开发了一种高效、表现力强且能处理长期结构的文本到音乐生成模型Mo^usai，可以根据文本描述生成多分钟高质量音乐。通过实验证明了该模型在多个标准上的优势。 |
| [^199] | [Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure.](http://arxiv.org/abs/2301.10956) | 本文研究了图神经网络是否可以利用图结构从而实现对潜在特征的恢复，并表明GNN可以完全利用图结构。 |
| [^200] | [Adaptive Federated Minimax Optimization with Lower complexities.](http://arxiv.org/abs/2211.07303) | 本文提出了一种自适应联邦式最小最大优化算法（AdaFGDA），用于解决分布式最小最大问题，在梯度和通信复杂性较低的情况下取得了良好的效果。 |
| [^201] | [Harmonizing output imbalance for defect segmentation on extremely-imbalanced photovoltaic module cells images.](http://arxiv.org/abs/2211.05295) | 本文提出了解决光伏电池模块细胞图像中THC缺陷分割输出不平衡的方法，包括明确的输出不平衡度量方法、基于分布的损失函数的推广以处理不同类型的输出不平衡，以及复合损失函数和自适应超参数选择算法的引入以实现输出不平衡在极度不平衡输入数据上的协调。 |
| [^202] | [Content-Based Search for Deep Generative Models.](http://arxiv.org/abs/2210.03116) | 这个论文介绍了基于内容的深层生成模型搜索任务，通过优化问题选择生成与查询最相似内容概率最高的模型，并提出了适用于不同查询模态的对比学习框架。（翻译为中文） |
| [^203] | [Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep Reconstruction without Reference.](http://arxiv.org/abs/2210.01125) | 本文提出了一种名为Spectral2Spectral的深度重建网络，它利用图像-光谱领域内的相似性先验通过无参考方式来辅助光谱CT的深度重建。 |
| [^204] | [Amortized Variational Inference: A Systematic Review.](http://arxiv.org/abs/2209.10888) | 分期变分推断（Amortized Variational Inference）是一种高效且可扩展的变分推断方法，通过利用参数化函数学习近似后验概率密度参数，解决了传统VI算法在处理大规模数据集和推断出界数据点方面的问题。 |
| [^205] | [DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization.](http://arxiv.org/abs/2208.09708) | DenseShift网络是一种准确和高效的低位幂乘法量化方法，通过改进Shift网络的精度和引入非量化浮点激活来提高性能。 |
| [^206] | [Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective.](http://arxiv.org/abs/2208.07365) | 本文基于解缠视角处理视频领域无监督自适应问题，通过逐步解缠静态和动态信息并使用多种约束方法，有效地移除空间领域特定信息和减少时间领域差异，实验结果验证了该方法的有效性和优越性。 |
| [^207] | [Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory Models.](http://arxiv.org/abs/2206.15316) | 本文提出了一种用于心脏超声视频的新颖异常检测方法，利用心脏周期性特性，在婴儿心脏超声视频数据集上训练了三种变分潜在轨迹模型，可可靠地识别严重的先天性心脏缺陷，并取得了较好的性能。 |
| [^208] | [ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision.](http://arxiv.org/abs/2204.06863) | ULF是一种用于弱监督学习的无监督标签函数校正方法，通过基于交叉验证原理的噪声降低技术，有效提高了弱监督学习的性能，无需手动标记。 |
| [^209] | [Fully Adaptive Composition in Differential Privacy.](http://arxiv.org/abs/2203.05481) | 本文介绍了完全自适应差分隐私组合的概念，通过引入隐私过滤器和隐私里程表来解决现有组合方法的局限性。 |
| [^210] | [LAP: An Attention-Based Module for Concept Based Self-Interpretation and Knowledge Injection in Convolutional Neural Networks.](http://arxiv.org/abs/2201.11808) | 本研究提出了一种新的基于注意力的汇聚层，称为LAP，它在卷积神经网络中实现了自解释性和知识注入的可能性，而不会降低性能。该模块可以轻松地插入任何卷积神经网络中，甚至是已经训练好的网络。 |
| [^211] | [Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing.](http://arxiv.org/abs/2112.01956) | 本文通过流形理论的严格方法重新审视了媒体输入变异中的感知多样性（DIV）和有效性（VAL）两个关键目标，证明了它们不可分割地相互绑定，并证明了SOTA基于生成模型的方法的重要性。 |
| [^212] | [Simplest Streaming Trees.](http://arxiv.org/abs/2110.08483) | 我们提出了最简单的决策树扩展，通过继续生长现有树来更新它们，并用新树替换一些旧树来控制总树的数量。在72个分类问题的基准套件中，我们的方法在精度和内存使用方面表现优异。 |
| [^213] | [Bellman-consistent Pessimism for Offline Reinforcement Learning.](http://arxiv.org/abs/2106.06926) | 本文提出了Bellman一致的悲观论述的概念，用于离线强化学习中的函数逼近，通过在与Bellman方程一致的函数集合上实施初始状态的悲观主义，改善了基于奖励的悲观主义方法的样本复杂性。 |
| [^214] | [Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles.](http://arxiv.org/abs/2106.06483) | 本论文研究了在随机上下文推断设置中，针对累计遗憾最小化的最优模型选择问题。通过引入渐增类别复杂性和递减边际收益条件，我们提出了一种基于新颖误配测试的算法，并展示了模型选择在奖励估计中的优势。 |
| [^215] | [Nonlinear model reduction for slow-fast stochastic systems near unknown invariant manifolds.](http://arxiv.org/abs/2104.02120) | 本论文介绍了一种面向慢-快随机系统的非线性模型简化技术，可以通过黑盒模拟器估计不变流形并计算有效的随机动力学过程，实现高效的状态空间探索。 |
| [^216] | [Interpretable Sequence Classification Via Prototype Trajectory.](http://arxiv.org/abs/2007.01777) | ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。 |
| [^217] | [Differentiable Sparsification for Deep Neural Networks.](http://arxiv.org/abs/1910.03201) | 本研究提出了一种完全可微的深度神经网络稀疏化方法，能够通过优化目标函数直接学习网络的稀疏结构和权重，有效解决了有效架构确定和网络尺寸缩小的问题。 |

# 详细

[^1]: 跨特征对比损失用于异构数据的分散式深度学习

    Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v1 [cs.LG])

    [http://arxiv.org/abs/2310.15890](http://arxiv.org/abs/2310.15890)

    本文提出了一种用于异构数据的分散式学习方法，通过跨特征对比损失实现数据无关知识蒸馏，实验结果表明该方法在各种计算机视觉任务上取得了优越性能。

    

    当前最先进的分散式学习算法大多数假设数据分布是独立同分布（IID）。然而，在实际场景中，分散式数据集在代理之间可以具有显著的异构数据分布。在这项工作中，我们提出了一种新颖的针对异构数据的分散式学习方法，通过交叉特征上的无数据知识蒸馏和对比损失来提高性能。对于一对相邻代理，跨特征是从一个代理的数据获取的特征（即最后一个隐藏层的激活）关于另一个代理的模型参数。我们通过一系列详尽的实验在各种计算机视觉数据集（CIFAR-10、CIFAR-100、Fashion MNIST 和 ImageNet）、模型架构和网络拓扑上展示了所提出技术的有效性。我们的实验结果表明，所提出的方法在性能上取得了更好的表现。

    The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (
    
[^2]: 通过傅里叶变换预测状态序列的表示学习

    State Sequences Prediction via Fourier Transform for Representation Learning. (arXiv:2310.15888v1 [cs.LG])

    [http://arxiv.org/abs/2310.15888](http://arxiv.org/abs/2310.15888)

    通过傅里叶变换预测状态序列的方法（SPF）利用状态序列的频域特性，高效地提取时间序列数据中的潜在模式，从而改善了长期决策的质量。

    

    虽然深度强化学习已经证明在解决复杂控制任务中是有效的，但由于出色性能所需的大量数据，样本效率仍然是一个关键挑战。现有的研究探索了表示学习在数据高效强化学习中的应用，例如通过预测长期未来状态来学习预测性表示。然而，许多现有方法并没有充分利用序列状态信号中固有的结构信息，这可能会改善长期决策的质量，但在时域难以分辨。为了解决这个问题，我们提出了通过傅里叶变换预测状态序列（SPF），这是一种利用状态序列的频域来高效地提取时间序列数据中的潜在模式的新方法。具体来说，我们在理论上分析了状态序列中结构信息的存在，

    While deep reinforcement learning (RL) has been demonstrated effective in solving complex control tasks, sample efficiency remains a key challenge due to the large amounts of data required for remarkable performance. Existing research explores the application of representation learning for data-efficient RL, e.g., learning predictive representations by predicting long-term future states. However, many existing methods do not fully exploit the structural information inherent in sequential state signals, which can potentially improve the quality of long-term decision-making but is difficult to discern in the time domain. To tackle this problem, we propose State Sequences Prediction via Fourier Transform (SPF), a novel method that exploits the frequency domain of state sequences to extract the underlying patterns in time series data for learning expressive representations efficiently. Specifically, we theoretically analyze the existence of structural information in state sequences, which 
    
[^3]: KirchhoffNet：一种连接消息传递和连续深度模型的电路桥接神经网络

    KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])

    [http://arxiv.org/abs/2310.15872](http://arxiv.org/abs/2310.15872)

    本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。

    

    在本文中，我们利用了模拟电路的基本原理基赫霍夫电流定律，引入了一类独特的神经网络模型，称为基赫霍夫网络。基赫霍夫网络与消息传递神经网络和连续深度网络建立了密切联系。我们证明，即使在没有任何传统层（如卷积、池化或线性层）的情况下，基赫霍夫网络在MNIST数据集上取得了98.86%的测试准确度，与最先进的结果相当。让基赫霍夫网络更加有趣的是其在硬件领域的潜力。当代深度神经网络通常部署在GPU上。相反，基赫霍夫网络可以通过模拟电路来实现。此外，我们证明了无论在基赫霍夫网络内有多少参数，其正向计算都可以在1/f秒内完成，其中f表示硬件的时钟频率。这种特性表明，基赫霍夫网络具有潜力实现快速计算的硬件。

    In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
    
[^4]: 使用因果感知图神经网络在动态图中预测时间中心性

    Using Causality-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs. (arXiv:2310.15865v1 [cs.LG])

    [http://arxiv.org/abs/2310.15865](http://arxiv.org/abs/2310.15865)

    本研究提出了一种使用因果感知图神经网络预测动态图中的时间中心性的方法，并在不同领域的13个时间图上进行了实验验证，结果显示该方法显著改善了介数和接近度中心性的预测能力。

    

    节点中心性在网络科学、社交网络分析和推荐系统中起着重要作用。在时间数据中，静态基于路径的中心性如接近度或介数可能会对节点在时间图中的真实重要性产生误导。为了解决这个问题，已经定义了基于节点对之间最短时间路径的时间一般化介数和接近度。然而，这些一般化的一个主要问题是计算这样的路径的计算成本较高。为了解决这个问题，我们研究了De Bruijn图神经网络(DBGNN)，一种因果感知的图神经网络架构，在时间序列数据中预测基于路径的时间中心性。我们在13个生物和社交系统的时间图中实验评估了我们的方法，并显示它相比静态图卷积方法显著改善了介数和接近度中心性的预测能力。

    Node centralities play a pivotal role in network science, social network analysis, and recommender systems. In temporal data, static path-based centralities like closeness or betweenness can give misleading results about the true importance of nodes in a temporal graph. To address this issue, temporal generalizations of betweenness and closeness have been defined that are based on the shortest time-respecting paths between pairs of nodes. However, a major issue of those generalizations is that the calculation of such paths is computationally expensive. Addressing this issue, we study the application of De Bruijn Graph Neural Networks (DBGNN), a causality-aware graph neural network architecture, to predict temporal path-based centralities in time series data. We experimentally evaluate our approach in 13 temporal graphs from biological and social systems and show that it considerably improves the prediction of both betweenness and closeness centrality compared to a static Graph Convolut
    
[^5]: 通过学习划分事件时间空间来改进事件时间预测

    Improving Event Time Prediction by Learning to Partition the Event Time Space. (arXiv:2310.15853v1 [stat.ML])

    [http://arxiv.org/abs/2310.15853](http://arxiv.org/abs/2310.15853)

    该论文提出通过学习划分事件时间空间的方法来改善事件时间预测，避免了对事件密度进行强参数假设，能够提高预测性能，特别是在可用数据有限的临床环境中。通过在模拟数据集和真实数据集上的实验证明了该方法的有效性。

    

    最近发展起来的生存分析方法通过预测在一些预先指定的（离散的）时间间隔内事件发生的概率来改进现有方法。通过避免对事件密度施加强烈的参数假设，这种方法在数据丰富的情况下往往能够提高预测性能。然而，在临床环境中，由于可用数据有限，通常更喜欢将事件时间空间划分为少量适合手头预测任务的时间间隔。在这项工作中，我们开发了一种从数据中学习划分定义的切点集合的方法。我们展示在两个模拟数据集中，我们能够恢复与基础生成模型相匹配的时间间隔。然后，我们在三个真实的观察性数据集中展示了改进的预测性能，其中包括一个大型的、新调和的中风风险预测数据集。最后，我们认为我们的方法有助于临床。

    Recently developed survival analysis methods improve upon existing approaches by predicting the probability of event occurrence in each of a number pre-specified (discrete) time intervals. By avoiding placing strong parametric assumptions on the event density, this approach tends to improve prediction performance, particularly when data are plentiful. However, in clinical settings with limited available data, it is often preferable to judiciously partition the event time space into a limited number of intervals well suited to the prediction task at hand. In this work, we develop a method to learn from data a set of cut points defining such a partition. We show that in two simulated datasets, we are able to recover intervals that match the underlying generative model. We then demonstrate improved prediction performance on three real-world observational datasets, including a large, newly harmonized stroke risk prediction dataset. Finally, we argue that our approach facilitates clinical d
    
[^6]: 关于负责任的机器学习数据集和公平性、隐私和法规准则的论文

    On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])

    [http://arxiv.org/abs/2310.15848](http://arxiv.org/abs/2310.15848)

    这篇论文讨论了负责任的机器学习数据集的重要性，并提出了一个通过负责任评价标准来评估数据集的框架。

    

    人工智能已经进入各个科学领域，在各种任务上比现有算法有了惊人的改进。近年来，人们对人工智能技术的可信性存在严重担忧。科学界致力于开发可信的人工智能算法。然而，目前在人工智能社区中流行的机器学习和深度学习算法在其开发过程中严重依赖使用的数据。这些学习算法通过识别数据中的模式来学习行为目标。数据中的任何缺陷都有可能直接转化为算法的缺陷。在这项研究中，我们讨论了负责任的机器学习数据集的重要性，并提出了一个通过负责任评价标准来评估数据集的框架。现有的工作侧重于对算法的后期评估以确保其可信性，而我们提供了一个框架，单独考虑数据组件以理解其特性。

    Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
    
[^7]: 一种用于新意图发现的扩散加权图框架

    A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])

    [http://arxiv.org/abs/2310.15836](http://arxiv.org/abs/2310.15836)

    本研究提出了一种名为Diffusion Weighted Graph Framework (DWGF)的方法，用于捕捉数据中的语义相似性和结构关系，从而实现更充分和可靠的监督信号，解决了以往方法在新意图发现中无法平衡数量和质量的问题。

    

    新意图发现旨在通过有限的带有已知意图的标记数据的帮助，识别出未标记数据中的新意图和已知意图。以前的方法未考虑样本之间的结构关系，生成的噪声监督信号无法在数量和质量之间取得平衡，阻碍了新意图聚类的形成和预训练知识的有效传递。为了缓解这一限制，我们提出了一种新颖的扩散加权图框架（DWGF），以捕捉数据中的语义相似性和结构关系，从而实现更充分和可靠的监督信号。具体而言，对于每个样本，我们沿着由最近邻指导的语义路径扩散邻域关系，以鉴别地刻画其局部结构。然后，我们根据语义相似性和局部结构对其正样本进行抽样和加权，用于对比学习。

    New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inferen
    
[^8]: 使用概念漂移解释方法对水配水网络中的小漏洞进行定位

    Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods. (arXiv:2310.15830v1 [cs.LG])

    [http://arxiv.org/abs/2310.15830](http://arxiv.org/abs/2310.15830)

    本研究提出了一种只使用压力测量进行水配水网络中小漏洞定位的方法，旨在解决因气候变化导致的饮用水稀缺问题。

    

    面对气候变化，饮用水的可用性将来会减少，使得饮用水成为越来越稀缺的资源。大量的水通过水运输和配水网络中的漏洞流失。漏洞的检测和定位是具有挑战性的问题，由于水配水网络中的复杂相互作用和需求的变化。尤其是小漏洞很难确定，但它们的定位对于避免长时间的水损失至关重要。虽然存在不同的方法来解决漏洞的检测和定位任务，但它们依赖于系统的各种信息，例如实时需求测量和精确的网络拓扑结构，这在许多真实环境中是一个不切实际的假设。相比之下，本研究尝试仅使用压力测量来进行漏洞定位。为此，首先建立了水配水网络中的漏洞模型。

    Facing climate change the already limited availability of drinking water will decrease in the future rendering drinking water an increasingly scarce resource. Considerable amounts of it are lost through leakages in water transportation and distribution networks. Leakage detection and localization are challenging problems due to the complex interactions and changing demands in water distribution networks. Especially small leakages are hard to pinpoint yet their localization is vital to avoid water loss over long periods of time. While there exist different approaches to solving the tasks of leakage detection and localization, they are relying on various information about the system, e.g. real-time demand measurements and the precise network topology, which is an unrealistic assumption in many real-world scenarios. In contrast, this work attempts leakage localization using pressure measurements only. For this purpose, first, leakages in the water distribution network are modeled employin
    
[^9]: 《重度增强、高分辨率3D ResUNet自动主动脉分割：对SEG.A挑战的贡献》翻译

    Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge. (arXiv:2310.15827v1 [cs.CV])

    [http://arxiv.org/abs/2310.15827](http://arxiv.org/abs/2310.15827)

    重度增强、高分辨率3D ResUNet自动主动脉分割在SEG.A挑战中取得了优异的成绩，达到了所有测试案例0.9以上的Dice分数，并在稳定性上超过了其他参与者。它在临床评估、定量结果和体积网格质量方面名列前茅。

    

    从3D医学图像中自动分割主动脉是一项重要但困难的任务。许多因素使得这个问题具有挑战性，比如主动脉夹层的可能性或者对小分支进行分割和注释的困难。本文介绍了MedGIFT团队在MICCAI 2023会议期间组织的SEG.A挑战中的一项贡献。我们提出了一种基于深度编码-解码器架构的全自动算法。我们的工作的主要假设是在低数据情况下，数据预处理和增强比深度架构更重要。因此，解决方案基于传统的卷积U-Net的变体。提出的解决方案在所有测试案例中都实现了0.9以上的Dice分数，并且在所有参与者中具有最高的稳定性。该方法在临床评估、定量结果和体积网格质量方面分别排名第一、第四和第三。我们免费公开源代码，

    Automatic aorta segmentation from 3-D medical volumes is an important yet difficult task. Several factors make the problem challenging, e.g. the possibility of aortic dissection or the difficulty with segmenting and annotating the small branches. This work presents a contribution by the MedGIFT team to the SEG.A challenge organized during the MICCAI 2023 conference. We propose a fully automated algorithm based on deep encoder-decoder architecture. The main assumption behind our work is that data preprocessing and augmentation are much more important than the deep architecture, especially in low data regimes. Therefore, the solution is based on a variant of traditional convolutional U-Net. The proposed solution achieved a Dice score above 0.9 for all testing cases with the highest stability among all participants. The method scored 1st, 4th, and 3rd in terms of the clinical evaluation, quantitative results, and volumetric meshing quality, respectively. We freely release the source code,
    
[^10]: 关于概念漂移我们所知的一两件事 - 对监测演变环境的调查

    One or Two Things We know about Concept Drift -- A Survey on Monitoring Evolving Environments. (arXiv:2310.15826v1 [cs.LG])

    [http://arxiv.org/abs/2310.15826](http://arxiv.org/abs/2310.15826)

    本文对非监督数据流中的概念漂移进行了文献综述和系统分类，提供了对漂移检测和漂移定位的最新研究状态，并提供了相关问题的准确数学定义。

    

    我们周围的世界不断变化。这些变化经常被描述为概念漂移，影响着许多工业和技术过程。由于它们可能导致故障和其他异常行为，在许多场景中可能具有安全风险，因此检测和分析概念漂移至关重要。本文提供了一份关于非监督数据流中概念漂移的文献综述。虽然许多调查重点关注监督数据流，但迄今为止还没有针对非监督环境的工作进行综述。然而，这种设置对于监测和异常检测特别重要，这些技术直接适用于工程中的许多任务和挑战。本调查提供了现有漂移检测工作的分类法，并以系统的方式涵盖了漂移定位研究的最新状态。除了提供系统的文献综述外，本文还提供了对所涉问题的精确定义，并介绍了相关的数学定义。

    The world surrounding us is subject to constant change. These changes, frequently described as concept drift, influence many industrial and technical processes. As they can lead to malfunctions and other anomalous behavior, which may be safety-critical in many scenarios, detecting and analyzing concept drift is crucial. In this paper, we provide a literature review focusing on concept drift in unsupervised data streams. While many surveys focus on supervised data streams, so far, there is no work reviewing the unsupervised setting. However, this setting is of particular relevance for monitoring and anomaly detection which are directly applicable to many tasks and challenges in engineering. This survey provides a taxonomy of existing work on drift detection. Besides, it covers the current state of research on drift localization in a systematic way. In addition to providing a systematic literature review, this work provides precise mathematical definitions of the considered problems and 
    
[^11]: 判别器引导下的自回归扩散模型

    Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])

    [http://arxiv.org/abs/2310.15817](http://arxiv.org/abs/2310.15817)

    本文引入了判别器引导，用于自回归扩散模型的训练，通过使用最优判别器来纠正预训练模型，并提出了一个顺序蒙特卡洛算法来应对使用次优判别器的情况。在生成分子图的任务中，判别器引导有助于提高生成性能。

    

    我们在自回归扩散模型中引入了判别器引导。在连续扩散模型中，使用判别器引导扩散过程的方法已经被使用过，本文中，我们推导了在离散情况下使用判别器和预训练生成模型的方法。首先，我们证明使用最优判别器将纠正预训练模型，并能够从底层数据分布中精确采样。其次，为了应对使用次优判别器的实际情况，我们推导了一个顺序蒙特卡洛算法，该算法在生成过程中迭代地将判别器的预测纳入考虑。我们将这些方法应用于生成分子图的任务，并展示了判别器相较于仅使用预训练模型时的生成性能提升。

    We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.
    
[^12]: 非线性降维的过去与现在：机器学习时代耗散PDEs的AIMs

    Nonlinear dimensionality reduction then and now: AIMs for dissipative PDEs in the ML era. (arXiv:2310.15816v1 [math.DS])

    [http://arxiv.org/abs/2310.15816](http://arxiv.org/abs/2310.15816)

    该研究提出了一种纯数据驱动的工作流程，用于构建分布式动力系统的降阶模型。利用机器学习工具，可以回避对精确截断投影和封闭式修正的需求。自编码器和扩散映射可以用于发现和测试合适的潜变量集合。这种方法可以表达ROMs的不同坐标。

    

    本研究提出了一系列纯数据驱动的工作流程，用于构建分布式动力系统的降阶模型（ROMs）。我们聚焦于受Approximate Inertial Manifolds（AIMs）理论启发并以其为模板的数据辅助模型；其中的一种特殊应用是Garcia-Archilla，Novo和Titi提出的后处理Galerkin方法。这种方法的适用性可以通过使用机器学习工具来规避对精确截断Galerkin投影和派生封闭式修正的需求。当正确的潜变量事先不知道时，我们展示了如何使用自编码器和扩散映射（一种流形学习方法）来发现好的潜变量集合并测试解释性。所提出的方法可以使用（a）理论（Fourier系数），（b）线性数据驱动（POD模态）和/或（c）非线性数据驱动（扩散映射）坐标来表达ROMs。另外，Bla

    This study presents a collection of purely data-driven workflows for constructing reduced-order models (ROMs) for distributed dynamical systems. The ROMs we focus on, are data-assisted models inspired by, and templated upon, the theory of Approximate Inertial Manifolds (AIMs); the particular motivation is the so-called post-processing Galerkin method of Garcia-Archilla, Novo and Titi. Its applicability can be extended: the need for accurate truncated Galerkin projections and for deriving closed-formed corrections can be circumvented using machine learning tools. When the right latent variables are not a priori known, we illustrate how autoencoders as well as Diffusion Maps (a manifold learning scheme) can be used to discover good sets of latent variables and test their explainability. The proposed methodology can express the ROMs in terms of (a) theoretical (Fourier coefficients), (b) linear data-driven (POD modes) and/or (c) nonlinear data-driven (Diffusion Maps) coordinates. Both Bla
    
[^13]: 好，更好，最好：自我激励的模仿学习在嘈杂演示中的应用

    Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations. (arXiv:2310.15815v1 [cs.LG])

    [http://arxiv.org/abs/2310.15815](http://arxiv.org/abs/2310.15815)

    本研究提出了一种自我激励的模仿学习方法（SMILE），通过过滤低于当前策略的演示来解决模仿学习受到嘈杂演示限制的问题，不需要附加信息。使用扩散模型的正向和反向过程来提取扩散专业水平的噪声信息，并利用该信息预测当前策略和演示者之间的专业水平差距。

    

    模仿学习旨在通过最小化智能体行为与专家演示之间的差异来发现一种策略。然而，由于来自非专家行为的嘈杂演示的限制，模仿学习容易受到影响，这是由于缺乏评估他们专业水平的附加信息而造成的挑战。本文介绍了一种称为自我激励的模仿学习(SMILE)的方法，它能够逐步过滤掉被认为低于当前策略的策略所收集的演示，从而消除了额外信息的需求。我们利用扩散模型的正向和反向过程来模拟演示专业水平从低到高和相反方向的变化，从而提取扩散专业水平的噪声信息。然后，利用噪声信息来预测当前策略和演示者之间的扩散步骤，并在理论上证明了它与他们的专业水平差距的等效性。

    Imitation Learning (IL) aims to discover a policy by minimizing the discrepancy between the agent's behavior and expert demonstrations. However, IL is susceptible to limitations imposed by noisy demonstrations from non-expert behaviors, presenting a significant challenge due to the lack of supplementary information to assess their expertise. In this paper, we introduce Self-Motivated Imitation LEarning (SMILE), a method capable of progressively filtering out demonstrations collected by policies deemed inferior to the current policy, eliminating the need for additional information. We utilize the forward and reverse processes of Diffusion Models to emulate the shift in demonstration expertise from low to high and vice versa, thereby extracting the noise information that diffuses expertise. Then, the noise information is leveraged to predict the diffusion steps between the current policy and demonstrators, which we theoretically demonstrate its equivalence to their expertise gap. We furt
    
[^14]: 随机实体量化用于参数高效的组合知识图谱表示

    Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation. (arXiv:2310.15797v1 [cs.AI])

    [http://arxiv.org/abs/2310.15797](http://arxiv.org/abs/2310.15797)

    本文研究了参数高效的组合知识图谱表示的问题，通过随机实体量化的方法，可以达到与当前策略类似的效果，这是因为随机实体量化下，实体码有更高的熵和码字级别的Jaccard距离，使得不同实体更容易区分，从而有效地表示知识图谱。

    

    知识图谱（KG）上的表示学习对下游任务至关重要。主导方法KG嵌入（KGE）通过独立向量表示实体，面临可扩展性挑战。最近的研究提出了一种参数效率的替代方法，通过从预定义的小规模码书中匹配实体对应的码字来表示实体。我们将获取每个实体对应码字的过程称为实体量化，先前的工作设计了复杂的策略。令人惊讶的是，本文表明简单的随机实体量化可以实现与当前策略类似的结果。我们分析了这种现象并揭示了在随机实体量化下，表示实体的量化结果-实体码具有更高的熵和码字级别的Jaccard距离。因此，不同实体更容易区分，有助于有效的KG表示。

    Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG represen
    
[^15]: 通过学习前缀子空间改进大型语言模型的泛化能力

    Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])

    [http://arxiv.org/abs/2310.15793](http://arxiv.org/abs/2310.15793)

    本文提出了一种通过学习前缀子空间来改进大型语言模型的泛化能力的方法。我们通过联合优化模型参数空间中的整个单纯形模型，在稀缺数据环境中实现了更广的局部最优解。这种方法在预训练变换器模型中表现出了很好的兼容性和有效性。

    

    本文关注于大型语言模型（LLMs）在稀缺数据环境中的微调（也被称为“少样本”学习设置）。我们提出了一种基于神经网络子空间的方法来增加LLMs的泛化能力。这种优化方法最近在计算机视觉领域中引入，旨在通过在参数空间中的整个单纯形模型的联合优化，识别更广的局部最优解，从而提高模型的泛化能力。然而，将其适应于大规模预训练变换器模型则面临一些挑战。首先，它们大量的参数使得联合训练多个模型变得困难，其次，它们的确定性参数初始化方案使其不适用于最初的子空间方法。我们在本文中展示，“参数高效微调”（PEFT）方法与最初的方法完全兼容，并提出学习整个连续前缀的单纯形。我们在实验证明了这个方法在大型语言模型的泛化上的有效性。

    This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on 
    
[^16]: 神经网络中的分摊推理用于小规模概率元学习

    Amortised Inference in Neural Networks for Small-Scale Probabilistic Meta-Learning. (arXiv:2310.15786v1 [stat.ML])

    [http://arxiv.org/abs/2310.15786](http://arxiv.org/abs/2310.15786)

    本文提出了一种在神经网络中进行小规模概率元学习的方法，通过将诱导输入替换为实际数据，通过训练推理网络来实现对任务特定贝叶斯推理的元学习。

    

    基于全局诱导点的变分逼近是基于使用一组诱导输入来构建一系列条件分布，从而准确地逼近真实后验分布的条件分布。我们的关键洞察力是这些诱导输入可以被实际数据替代，使得变分分布由每个数据点的一组近似似然组成。这种结构适合于分摊推理，其中每个近似似然的参数是通过将每个数据点通过称为推理网络的元模型通过获得的。通过在相关数据集上训练这个推理网络，我们可以元学习任务特定BNN上的贝叶斯推理。

    The global inducing point variational approximation for BNNs is based on using a set of inducing inputs to construct a series of conditional distributions that accurately approximate the conditionals of the true posterior distribution. Our key insight is that these inducing inputs can be replaced by the actual data, such that the variational distribution consists of a set of approximate likelihoods for each datapoint. This structure lends itself to amortised inference, in which the parameters of each approximate likelihood are obtained by passing each datapoint through a meta-model known as the inference network. By training this inference network across related datasets, we can meta-learn Bayesian inference over task-specific BNNs.
    
[^17]: 无配对MRI超分辨率与自监督对比学习

    Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v1 [eess.IV])

    [http://arxiv.org/abs/2310.15767](http://arxiv.org/abs/2310.15767)

    本文提出了一种利用自监督对比学习的无配对MRI超分辨率方法，可以在有限的训练数据下提高SR性能，改善MRI分辨率。

    

    高分辨率（HR）磁共振成像（MRI）在临床环境中提高诊断准确性至关重要。然而，MRI分辨率的固有限制限制了其广泛应用。基于深度学习的图像超分辨率（SR）方法展现了提升MRI分辨率的潜力，而无需额外成本。然而，这些方法通常需要大量HR MRI图像进行训练，而这可能难以获取。在本文中，我们提出了一种无配对MRI SR方法，利用自监督对比学习来提高有限训练数据下的SR性能。我们的方法利用真实的HR图像和人工生成的SR图像构建正负样本对，从而促进辨别性特征的学习。本研究呈现的实证结果突出了峰值信噪比和结构相似性指数的显著提高，即使缺乏HR图像数据。

    High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR image
    
[^18]: 通过条件性患病率调整实现鲁棒学习

    Robust Learning via Conditional Prevalence Adjustment. (arXiv:2310.15766v1 [cs.LG])

    [http://arxiv.org/abs/2310.15766](http://arxiv.org/abs/2310.15766)

    该论文提出了一种名为CoPA的方法，用于处理医疗数据中不稳定相关性的问题，特别适用于反因果任务。这种方法通过调整患病率来处理混淆变量和标签的关系，从而实现鲁棒学习。

    

    医疗数据通常来自多个来源，其中混淆变量之间的相关性可能变化很大。如果深度学习模型利用这些不稳定的相关性，它们可能在未知的来源中发生灾难性失败。尽管提出了许多方法来解决不稳定的相关性，但每种方法都有其局限性。例如，对抗训练强制模型完全忽略不稳定的相关性，但这样做可能导致预测性能不佳。其他方法（例如，不变风险最小化）试图学习仅依赖于稳定关联的域不变表示，通过假设存在一个因果数据生成过程（输入X导致类别标签Y）。因此，它们对反因果任务（Y导致X）可能不起作用，这在计算机视觉中很常见。我们提出了一种名为CoPA（条件患病率调整）的反因果任务方法。CoPA假设（1）生成机制稳定，即标签Y和混淆变量Z生成X。

    Healthcare data often come from multiple sites in which the correlations between confounding variables can vary widely. If deep learning models exploit these unstable correlations, they might fail catastrophically in unseen sites. Although many methods have been proposed to tackle unstable correlations, each has its limitations. For example, adversarial training forces models to completely ignore unstable correlations, but doing so may lead to poor predictive performance. Other methods (e.g. Invariant risk minimization [4]) try to learn domain-invariant representations that rely only on stable associations by assuming a causal data-generating process (input X causes class label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X), which are common in computer vision. We propose a method called CoPA (Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that (1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z generate X, and (
    
[^19]: 使用拓扑非负矩阵分解分析单细胞RNA测序

    Analyzing Single Cell RNA Sequencing with Topological Nonnegative Matrix Factorization. (arXiv:2310.15744v1 [stat.ML])

    [http://arxiv.org/abs/2310.15744](http://arxiv.org/abs/2310.15744)

    该论文介绍了两种持久性拓扑非负矩阵分解方法，即TNMF和rTNMF，用于分析单细胞RNA测序。通过实验证明，TNMF和rTNMF显著优于其他NMF方法，并且可以用于可视化UMAP和t-SNE。

    

    单细胞RNA测序(scRNA-seq)是一项相对较新的技术，由于其高维度、复杂性和大规模性，引起了统计学、数据科学和计算生物学领域的巨大兴趣。非负矩阵分解(NMF)因其对结果低维度分量的元基因解释而提供了一种独特的方法。然而，NMF方法在缺乏多尺度分析方面存在问题。该研究引入了两种持久性拉普拉斯正则化NMF方法，即拓扑NMF(TNMF)和鲁棒拓扑NMF(rTNMF)。通过使用共计12个数据集，我们证明了所提出的TNMF和rTNMF显著优于所有其他NMF方法。我们还利用TNMF和rTNMF对流行的UMAP(Uniform Manifold Approximation and Projection)和t-SNE(t-distributed stochastic neighbor embedding)进行了可视化。

    Single-cell RNA sequencing (scRNA-seq) is a relatively new technology that has stimulated enormous interest in statistics, data science, and computational biology due to the high dimensionality, complexity, and large scale associated with scRNA-seq data. Nonnegative matrix factorization (NMF) offers a unique approach due to its meta-gene interpretation of resulting low-dimensional components. However, NMF approaches suffer from the lack of multiscale analysis. This work introduces two persistent Laplacian regularized NMF methods, namely, topological NMF (TNMF) and robust topological NMF (rTNMF). By employing a total of 12 datasets, we demonstrate that the proposed TNMF and rTNMF significantly outperform all other NMF-based methods. We have also utilized TNMF and rTNMF for the visualization of popular Uniform Manifold Approximation and Projection (UMAP) and t-distributed stochastic neighbor embedding (t-SNE).
    
[^20]: 用增强的模板先验改进心电图(ECG)插值的扩散模型

    Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v1 [cs.LG])

    [http://arxiv.org/abs/2310.15742](http://arxiv.org/abs/2310.15742)

    该论文提出了一种模板引导去噪扩散概率模型PulseDiff，通过使用信息先验来改进心电图(ECG)的插值和预测准确性。此模型考虑了主体间变化和心跳关系，从而提高了性能。

    

    作为常规临床护理的一部分，如心电图(ECG)等脉冲信号被广泛收集。然而，使用移动健康系统收集的信号存在噪音和质量差的录音，导致缺失值的存在，这是一个重要问题，降低了信号的质量，并影响了自动化的下游任务。最近的研究探索使用概率时间序列模型对ECG进行缺失值插补。然而，与确定性模型相比，它们的性能仍然有限，因为训练目标中没有明确考虑主体间的变化和心跳关系。在这项工作中，为了改善概率模型的ECG插值和预测准确性，我们提出了一个条件信息先验的模板引导去噪扩散概率模型，PulseDiff。具体而言，1)我们首先从观测中提取一个主体级别的脉冲模板作为先验

    Pulsative signals such as the electrocardiogram (ECG) are extensively collected as part of routine clinical care. However, noisy and poor-quality recordings, leading to missing values, are a major issue for signals collected using mobile health systems, decreasing the signal quality and affecting the automated downstream tasks. Recent studies have explored imputation of missing values for ECG with probabilistic time-series models. Nevertheless, in comparison with the deterministic models, their performance is still limited, as the variations across subjects and heart-beat relationships are not explicitly considered in the training objective. In this work, to improve the ECG imputation and forecasting accuracy with probabilistic models, we present an template-guided denoising diffusion probabilistic model, PulseDiff, which is conditioned an informative prior for a range of health conditions. Specifically, 1) we first extract a subject-level pulsative template from the observation as an 
    
[^21]: 循环线性变换器

    Recurrent Linear Transformers. (arXiv:2310.15719v1 [cs.LG])

    [http://arxiv.org/abs/2310.15719](http://arxiv.org/abs/2310.15719)

    本文提出了循环线性变换器作为transformer自注意机制的替代方案，解决了transformers在处理长距离依赖关系和推断成本方面的限制。在强化学习问题中的实验证明了其有效性和可行性。

    

    transformer架构中的自注意机制能够捕捉长距离的依赖关系，这也是其在处理序列数据时有效的主要原因。然而，尽管其成功，transformers仍然有两个重大缺点，限制了其更广泛的适用性：(1)为了记住过去的信息，自注意机制需要访问整个历史信息作为上下文。(2)transformers的推断成本很高。本文提出了对transformer自注意机制的循环替代方案，其具有独立于上下文的推断成本并有效地利用长距离依赖关系，在实践中表现良好。我们在强化学习问题中评估了我们的方法，在这些问题中，上述计算限制几乎使得transformers的应用不可行。我们在一个诊断环境中量化了我们架构中不同部分的影响。

    The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and as
    
[^22]: 通过观测变量的分组使因果表示学习可辨识化

    Causal Representation Learning Made Identifiable by Grouping of Observational Variables. (arXiv:2310.15709v1 [stat.ML])

    [http://arxiv.org/abs/2310.15709](http://arxiv.org/abs/2310.15709)

    该论文研究了因果表示学习（CRL），提出了一种基于观测变量分组的新型弱约束的可辨识性方法，该方法不依赖于时间结构、干预或监督，具有实际应用的意义。

    

    当今很有意义的话题是因果表示学习（Causal Representation Learning，简称CRL），其目标是以数据驱动的方式学习隐藏特征的因果模型。不幸的是，CRL存在严重的不适问题，因为它结合了表示学习和因果发现这两个容易不适问题。然而，要找到能保证唯一解的实际可识别性条件对于其实际应用非常重要。目前大多数方法都基于对潜在因果机制的假设，比如时间因果性、监督或干预的存在；在实际应用中，这些假设可能过于限制性。在这里，我们通过对观测混合表现出合适的变量分组的新型弱约束，展示了可辨识性。我们还提出了一种与模型一致的新型自我监督估计框架。

    A topic of great current interest is Causal Representation Learning (CRL), whose goal is to learn a causal model for hidden features in a data-driven manner. Unfortunately, CRL is severely ill-posed since it is a combination of the two notoriously ill-posed problems of representation learning and causal discovery. Yet, finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most approaches so far have been based on assumptions on the latent causal mechanisms, such as temporal causality, or existence of supervision or interventions; these can be too restrictive in actual applications. Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision. The approach is based assuming the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework consistent with the model, 
    
[^23]: 用深度强化学习生成多样化调度策略来解决大型柔性车间调度实例

    Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning. (arXiv:2310.15706v1 [cs.AI])

    [http://arxiv.org/abs/2310.15706](http://arxiv.org/abs/2310.15706)

    本文提出了一种能够通过生成多样化的调度策略来解决大型柔性车间调度实例的方法，并应用深度强化学习来优化调度质量。

    

    柔性车间调度问题（FJSSP）在文献中得到了广泛研究，提出了许多启发式、精确和元启发式方法。然而，工业对实时响应突发事件的需求产生了在几秒内生成新调度的必要性。在这些方法中，只有调度规则（DRs）能够在约束下生成调度，尽管其质量可以得到改进。为了改善结果，最近的方法将FJSSP建模为马尔可夫决策过程（MDP），并应用强化学习生成一个策略，将操作分配到机器上生成最优解。然而，在大型的FJSSP实例中仍然有改进的空间，而这在实际情况中很常见。因此，本文的目标是提出一种能够稳健解决大型FJSSP实例的方法。

    The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large insta
    
[^24]: COPF: 通过最优策略拟合实现持续学习人类偏好

    COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])

    [http://arxiv.org/abs/2310.15694](http://arxiv.org/abs/2310.15694)

    通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。

    

    强化学习通过人类反馈（RLHF）的技术是改善预训练语言模型（LM）以符合人类偏好的常用方法。然而，当前基于RLHF的LM在引入新的查询或反馈时需要完全重新训练，这是一项具有挑战性的任务，因为人类偏好在不同领域或任务之间可能会有所变化。由于所需的时间和计算资源以及与数据隐私相关的问题，重新训练LM在许多现实世界的情况下存在实际困难。为了解决这个限制，我们提出了一种新的方法，称为持续最优策略拟合（COPF），其中我们使用蒙特卡罗法估计一系列最优策略，然后通过函数正则化不断拟合策略序列。COPF包含一个单一的学习阶段，不需要复杂的强化学习。

    The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
    
[^25]: 使用增强型残差网络进行插值和反问题的物理驱动方法

    Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems. (arXiv:2310.15690v1 [cs.LG])

    [http://arxiv.org/abs/2310.15690](http://arxiv.org/abs/2310.15690)

    本文介绍了一种名为增强型残差网络的新颖神经网络结构，通过在残差元素中添加幂次项提升了网络的表达能力，具有卓越的准确性和应用性能，尤其适用于非平滑函数的处理。同时，该网络结构在解决反问题方面也表现出卓越的性能。

    

    本文介绍了一种新颖的神经网络结构，称为增强型残差网络，旨在改善2D和3D环境下平滑和非平滑函数的插值能力。通过在残差元素中添加幂次项，该网络结构增强了网络的表达能力。研究探究了网络深度、宽度和优化方法，并展示了该网络结构的适应性和性能优势。结果一致表明，增强型残差网络在非平滑函数方面具有异常的准确性。实际示例也证实了其在准确性、收敛性和效率方面相对于普通神经网络的优越性。研究还探讨了更深层网络的影响。此外，提出的网络结构还应用于解决反Burgers方程问题，展示了优越的性能。总之，增强型残差网络提供了一种多功能的解决方案，明显提升了插值和反问题的能力。

    This paper introduces a novel neural network structure called the Power-Enhancing residual network, designed to improve interpolation capabilities for both smooth and non-smooth functions in 2D and 3D settings. By adding power terms to residual elements, the architecture boosts the network's expressive power. The study explores network depth, width, and optimization methods, showing the architecture's adaptability and performance advantages. Consistently, the results emphasize the exceptional accuracy of the proposed Power-Enhancing residual network, particularly for non-smooth functions. Real-world examples also confirm its superiority over plain neural network in terms of accuracy, convergence, and efficiency. The study also looks at the impact of deeper network. Moreover, the proposed architecture is also applied to solving the inverse Burgers' equation, demonstrating superior performance. In conclusion, the Power-Enhancing residual network offers a versatile solution that significa
    
[^26]: 固定预算下多臂老虎机问题的实数组合纯探索

    Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2310.15681v1 [cs.LG])

    [http://arxiv.org/abs/2310.15681](http://arxiv.org/abs/2310.15681)

    本文研究了固定预算下多臂老虎机问题的实数组合纯探索。首先，我们引入了CSA算法，可在臂数指数增长的情况下找到最佳动作；然后，我们提出了Minimax-CombSAR算法，适用于动作类大小为多项式情况，并证明了它的最优性。最后，在实验中与之前的方法进行比较，结果表明我们的算法性能更好。

    

    我们研究了在固定预算设置下多臂老虎机问题的实数组合纯探索。首先，我们引入了连续组合成功分配（CSA）算法，这是第一个能够在动作类大小相对于臂数呈指数增长的情况下确定最佳动作的算法。我们证明了CSA算法的误差概率上界与下界在指数的对数因子内相匹配。然后，我们介绍了另一个算法，名为极小化组合连续接受与拒绝（Minimax-CombSAR）算法，用于动作类大小为多项式情况，并证明它是最优的，与下界相匹配。最后，我们通过实验比较了这些算法与之前的方法，并展示了我们的算法表现更好。

    We study the real-valued combinatorial pure exploration of the multi-armed bandit in the fixed-budget setting. We first introduce the Combinatorial Successive Asign (CSA) algorithm, which is the first algorithm that can identify the best action even when the size of the action class is exponentially large with respect to the number of arms. We show that the upper bound of the probability of error of the CSA algorithm matches a lower bound up to a logarithmic factor in the exponent. Then, we introduce another algorithm named the Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) algorithm for the case where the size of the action class is polynomial, and show that it is optimal, which matches a lower bound. Finally, we experimentally compare the algorithms with previous methods and show that our algorithm performs better.
    
[^27]: 交互式广义可加模型及其在电力负荷预测中的应用

    Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting. (arXiv:2310.15662v1 [cs.LG])

    [http://arxiv.org/abs/2310.15662](http://arxiv.org/abs/2310.15662)

    本文提出了一种交互式广义可加模型，既可解释性又能融入电力行业的特定领域知识以提高性能。通过使用分段线性函数和高效算法，我们的模型在公共基准和电力数据集中表现优异，并在极端天气事件的情况下具有良好的泛化能力。

    

    电力负荷预测是电力系统规划和管理中不可或缺的组成部分。不准确的负荷预测可能导致停电威胁或能源浪费。在数据有限甚至没有数据的情况下，例如节假日负荷预测或极端天气条件下的负荷预测，准确的电力负荷预测具有挑战性。由于负荷预测之后往往会做出重大决策,模型的可解释性对于预测模型的采用至关重要。在本文中，我们提出了一种交互式广义可加模型，既可解释性又能融入电力行业的特定领域知识以提高性能。这种基于提升的广义可加模型利用分段线性函数，并可以通过我们的高效算法进行学习。在公共基准和电力数据集中，我们的交互式广义可加模型优于当前最先进的方法，并在极端天气事件的情况下表现出良好的泛化能力。

    Electric load forecasting is an indispensable component of electric power system planning and management. Inaccurate load forecasting may lead to the threat of outages or a waste of energy. Accurate electric load forecasting is challenging when there is limited data or even no data, such as load forecasting in holiday, or under extreme weather conditions. As high-stakes decision-making usually follows after load forecasting, model interpretability is crucial for the adoption of forecasting models. In this paper, we propose an interactive GAM which is not only interpretable but also can incorporate specific domain knowledge in electric power industry for improved performance. This boosting-based GAM leverages piecewise linear functions and can be learned through our efficient algorithm. In both public benchmark and electricity datasets, our interactive GAM outperforms current state-of-the-art methods and demonstrates good generalization ability in the cases of extreme weather events. We
    
[^28]: 基于动量梯度的超图神经网络非目标攻击

    Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks. (arXiv:2310.15656v1 [cs.LG])

    [http://arxiv.org/abs/2310.15656](http://arxiv.org/abs/2310.15656)

    本文设计了一个新的基于动量梯度的HGNNs攻击模型，用于非目标攻击。通过特征选择和特征修改，该模型在HGNNs训练之前实施攻击，填补了现有对HGNNs对抗攻击研究的空白。

    

    由于其出色的高阶表示能力，超图神经网络（HGNNs）已成功应用于各种超图相关任务。最近的研究表明，深度学习模型容易受到对抗攻击。大多数关于图对抗攻击的研究都集中在图神经网络（GNNs）上，对HGNNs的对抗攻击研究仍然相对较少。本文试图填补这一空白。我们设计了一个新的HGNNs攻击模型，用于非目标攻击，即MGHGA，重点是修改节点特征。我们考虑HGNNs训练过程，并使用替代模型在超图建模之前实施攻击。具体而言，MGHGA包括两部分：特征选择和特征修改。我们在特征选择模块中使用动量梯度机制选择攻击节点特征。在特征修改模块中，我们使用两种特征生成方法（直接修改和符号梯度）来进行修改。

    Hypergraph Neural Networks (HGNNs) have been successfully applied in various hypergraph-related tasks due to their excellent higher-order representation capabilities. Recent works have shown that deep learning models are vulnerable to adversarial attacks. Most studies on graph adversarial attacks have focused on Graph Neural Networks (GNNs), and the study of adversarial attacks on HGNNs remains largely unexplored. In this paper, we try to reduce this gap. We design a new HGNNs attack model for the untargeted attack, namely MGHGA, which focuses on modifying node features. We consider the process of HGNNs training and use a surrogate model to implement the attack before hypergraph modeling. Specifically, MGHGA consists of two parts: feature selection and feature modification. We use a momentum gradient mechanism to choose the attack node features in the feature selection module. In the feature modification module, we use two feature generation approaches (direct modification and sign gra
    
[^29]: LLMs生成内容检测综述

    A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])

    [http://arxiv.org/abs/2310.15654](http://arxiv.org/abs/2310.15654)

    该论文是关于LLMs生成内容检测的综述，提供了现有策略和挑战的概述，并提倡采用更灵活和强大的模型以提高检测准确性，并强调使用多方面的方法来应对不同攻击。这是首个综合调查LLMs时代检测的工作。

    

    先进的大型语言模型（LLMs）如ChatGPT的不断发展，导致合成内容生成不断增加，涉及媒体、网络安全、公共话语和教育等多个领域。因此，检测LLMs生成内容的能力变得至关重要。我们旨在提供现有检测策略和基准的详细概述，审查它们的差异，并确定领域中的关键挑战和前景，提倡采用更灵活和强大的模型以提高检测准确性。我们还主张采用多方面的方法应对不同攻击，以抵御LLMs不断发展的能力。据我们所知，这项工作是LLMs时代检测的首个综合调查。我们希望它能够提供对LLMs生成内容检测当前情况的广泛理解，并为研究该领域提供参考。

    The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for res
    
[^30]: 通过元学习对图进行欺骗性公平攻击

    Deceptive Fairness Attacks on Graphs via Meta Learning. (arXiv:2310.15653v1 [cs.LG])

    [http://arxiv.org/abs/2310.15653](http://arxiv.org/abs/2310.15653)

    本文研究了对图进行欺骗性公平攻击的方法，通过元学习的框架FATE，在保持下游任务效用的前提下，能够放大图神经网络的偏见，无论是否考虑公平性。该研究可以为设计鲁棒和公平的图学习模型提供启示。

    

    我们研究了对图进行欺骗性公平攻击，以回答以下问题：如何通过污染攻击图学习模型来欺骗性地加剧偏见？我们通过一个双层优化问题回答了这个问题，并提出了一个基于元学习的框架FATE。FATE广泛适用于各种公平定义和图学习模型，以及任意选择的操作方法。我们进一步将FATE实例化为攻击图神经网络上的统计平衡和个体公平。我们在半监督节点分类任务的真实世界数据集上进行了大量实验评估。实验结果表明，FATE能够在维持下游任务的效用的同时放大图神经网络的偏见，无论是否考虑公平性。我们希望本文能够对公平图学习的对抗性鲁棒性提供一些见解，并能为设计鲁棒和公平的图学习模型提供启示。

    We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust an
    
[^31]: 动态卷积神经网络作为高效的预训练音频模型

    Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models. (arXiv:2310.15648v1 [cs.SD])

    [http://arxiv.org/abs/2310.15648](http://arxiv.org/abs/2310.15648)

    本研究在已有的Transformer到CNN的知识蒸馏基础上，通过引入动态CNN块，提升了高效CNN的性能，相比传统的高效CNN更具优势。

    

    大规模音频数据集（如AudioSet）的引入为Transformers在音频领域的使用铺平了道路，并取代了CNN作为许多任务的最先进神经网络架构。音频频谱Transformers在利用大规模数据集方面表现出了优良的性能，创造出了强大的预训练模型，在下游任务的微调中超过了CNN。然而，与CNN相比，目前流行的音频频谱Transformers在计算复杂度上要求较高。最近，我们通过使用Transformer到CNN的知识蒸馏，展示了高效CNN可以在大数据集上追赶甚至超过Transformer。在这项工作中，我们延伸了这一研究领域，并通过引入由动态非线性、动态卷积和注意机制构成的动态CNN块来提升高效CNN的容量。我们展示了这些动态CNN在性能和复杂度的权衡上优于传统的高效CNN。

    The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic convolutions and attention mechanisms. We show that these dynamic CNNs outperform traditional efficient CNNs, in terms of the performance-complexity trade
    
[^32]: 点亮那个机器人！关于使用静态分析特征对抗安卓恶意软件检测的有效性的研究

    Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection. (arXiv:2310.15645v1 [cs.CR])

    [http://arxiv.org/abs/2310.15645](http://arxiv.org/abs/2310.15645)

    本研究评估了特定混淆技术对使用静态分析提取的特征的影响，并发现混淆技术对所有静态分析特征都产生不同程度的影响，这可能会破坏依赖这些特征的机器学习恶意软件检测器的有效性。

    

    恶意软件作者将混淆视为绕过基于静态分析特征的恶意软件检测器的手段。对于安卓系统，多项研究已确认许多反恶意软件产品很容易通过简单的程序转换来规避。与这些研究相反，使用静态分析特征的机器学习检测方法也被提出来对抗混淆。因此，需要确定特定混淆策略或工具的使用对于基于静态分析特征的机器学习恶意软件检测的有效性构成何种风险。为了阐明这一问题，在这篇文章中，我们评估了特定混淆技术对使用静态分析提取的常见特征的影响，并确定这些变化是否足够显著以破坏依赖这些特征的机器学习恶意软件检测器的有效性。实验结果表明，混淆技术对所有静态分析特征都产生不同程度的影响。

    Malware authors have seen obfuscation as the mean to bypass malware detectors based on static analysis features. For Android, several studies have confirmed that many anti-malware products are easily evaded with simple program transformations. As opposed to these works, ML detection proposals for Android leveraging static analysis features have also been proposed as obfuscation-resilient. Therefore, it needs to be determined to what extent the use of a specific obfuscation strategy or tool poses a risk for the validity of ML malware detectors for Android based on static analysis features. To shed some light in this regard, in this article we assess the impact of specific obfuscation techniques on common features extracted using static analysis and determine whether the changes are significant enough to undermine the effectiveness of ML malware detectors that rely on these features. The experimental results suggest that obfuscation techniques affect all static analysis features to varyi
    
[^33]: 使用高斯过程回归的保证覆盖预测区间

    Guaranteed Coverage Prediction Intervals with Gaussian Process Regression. (arXiv:2310.15641v1 [cs.LG])

    [http://arxiv.org/abs/2310.15641](http://arxiv.org/abs/2310.15641)

    本论文介绍了一种基于机器学习框架Conformal Prediction的Gaussian Process Regression扩展方法，可以在模型完全错误的情况下保证生成具有所需覆盖度的预测区间。

    

    高斯过程回归是一种流行的回归方法，与大多数机器学习技术不同，它提供了其预测的不确定性估计。然而，这些不确定性估计是基于模型假设正确的前提下进行的，而在大多数实际应用中，所需的知识很少可用，这导致产生的不确定性估计可能非常误导人，例如对于95％置信水平产生的预测区间可能只覆盖了少于95％的真实标签。为了解决这个问题，本文介绍了一种基于机器学习框架Conformal Prediction（CP）的GPR扩展。这种扩展可以在模型完全错误的情况下保证生成具有所需覆盖度的预测区间。所提出的方法结合了GPR的优势和CP的有效覆盖保证，实验结果验证了其有效性。

    Gaussian Process Regression (GPR) is a popular regression method, which unlike most Machine Learning techniques, provides estimates of uncertainty for its predictions. These uncertainty estimates however, are based on the assumption that the model is well-specified, an assumption that is violated in most practical applications, since the required knowledge is rarely available. As a result, the produced uncertainty estimates can become very misleading; for example the prediction intervals (PIs) produced for the 95\% confidence level may cover much less than 95\% of the true labels. To address this issue, this paper introduces an extension of GPR based on a Machine Learning framework called, Conformal Prediction (CP). This extension guarantees the production of PIs with the required coverage even when the model is completely misspecified. The proposed approach combines the advantages of GPR with the valid coverage guarantee of CP, while the performed experimental results demonstrate its 
    
[^34]: 上下文定向无环图

    Contextual directed acyclic graphs. (arXiv:2310.15627v1 [stat.ML])

    [http://arxiv.org/abs/2310.15627](http://arxiv.org/abs/2310.15627)

    本论文研究了上下文定向无环图的问题，通过神经网络将上下文特征映射到DAG，利用稀疏的加权邻接矩阵表示图结构，并通过新颖的投影层满足无环性的特点。实验证明该方法能够成功恢复出真实的上下文特定图。

    

    从观测数据中估计定向无环图（DAG）的结构仍然是机器学习中的一个重大挑战。这个领域的大部分研究集中在为整个人口学习单个DAG上。本文考虑了一个替代性的设置，其中图结构基于可用的“上下文”特征而因人而异。我们通过一个将上下文特征映射到DAG的神经网络来解决这个上下文DAG问题，DAG以加权邻接矩阵表示。神经网络配备了一个新颖的投影层，确保输出矩阵是稀疏的，并满足最近发展的无环性的特点。我们设计了一个可扩展的计算框架来学习上下文DAG，并提供了收敛保证和通过投影层反向传播的分析梯度。我们的实验证明，这种新方法可以恢复出真实的上下文特定图，而现有方法则失败。

    Estimating the structure of directed acyclic graphs (DAGs) from observational data remains a significant challenge in machine learning. Most research in this area concentrates on learning a single DAG for the entire population. This paper considers an alternative setting where the graph structure varies across individuals based on available "contextual" features. We tackle this contextual DAG problem via a neural network that maps the contextual features to a DAG, represented as a weighted adjacency matrix. The neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. We devise a scalable computational framework for learning contextual DAGs and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. Our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.
    
[^35]: GUPNet++：用于单目三维物体检测的几何不确定性传播网络

    GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection. (arXiv:2310.15624v1 [cs.CV])

    [http://arxiv.org/abs/2310.15624](http://arxiv.org/abs/2310.15624)

    GUPNet++是一种通过以概率方式建模几何投影的几何不确定性传播网络，可以提高单目三维物体检测的深度预测稳定性和效率。

    

    几何在单目三维物体检测中起着重要作用。它可以通过物体的物理尺寸与图像平面中的二维投影之间的透视投影来估计物体的深度，这可以将数学先验引入深度模型。然而，这个投影过程也会引入误差放大，估计高度的误差会被放大并反映到投影的深度中。这导致深度推断不可靠，并且影响训练的稳定性。为了解决这个问题，我们提出了一种新颖的几何不确定性传播网络(GUPNet++)，通过以概率方式建模几何投影。这确保了深度预测是有界的，并与合理的不确定性相关联。引入这种几何不确定性的意义有两个方面：(1)。它模拟了几何投影在训练过程中的不确定性传播关系，提高了端到端模型学习的稳定性和效率。

    Geometry plays a significant role in monocular 3D object detection. It can be used to estimate object depth by using the perspective projection between object's physical size and 2D projection in the image plane, which can introduce mathematical priors into deep models. However, this projection process also introduces error amplification, where the error of the estimated height is amplified and reflected into the projected depth. It leads to unreliable depth inferences and also impairs training stability. To tackle this problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++) by modeling geometry projection in a probabilistic manner. This ensures depth predictions are well-bounded and associated with a reasonable uncertainty. The significance of introducing such geometric uncertainty is two-fold: (1). It models the uncertainty propagation relationship of the geometry projection during training, improving the stability and efficiency of the end-to-end model learni
    
[^36]: Nko语的机器翻译：工具、语料库和基准结果

    Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])

    [http://arxiv.org/abs/2310.15612](http://arxiv.org/abs/2310.15612)

    该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。

    

    目前，尼科语（一种在多个西非国家使用的语言）没有可用的机器翻译系统，但它在文化和教育价值上具有重要意义。为了解决这个问题，我们提出了一套工具、资源和基准结果，旨在开发可用的尼科语和其他当前没有足够大的平行文本语料库的语言的机器翻译系统。具体包括：(1) Friallel：一种新颖的协作平行文本整理软件，通过基于副本编辑的工作流程实现质量控制。(2) 扩展了FLoRes-200和NLLB-Seed语料库，从其他语言中与尼科语平行翻译了2,009和6,193个高质量的文本。(3) nicolingua-0005：包含130,850个平行片段的三语和双语语料库，以及超过3百万尼科语单语言语料库。(4) 基线双语和多语言神经机器翻译结果与b...

    Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
    
[^37]: 使用Slisemap解释物理数据

    Using Slisemap to interpret physical data. (arXiv:2310.15610v1 [cs.LG])

    [http://arxiv.org/abs/2310.15610](http://arxiv.org/abs/2310.15610)

    Slisemap是一种结合了流形可视化和可解释人工智能的方法，可以帮助我们在物理数据中找到有意义的信息。

    

    流形可视化技术通常用于在物理科学中可视化高维数据集。本文将一种最近介绍的流形可视化方法Slise应用于物理和化学数据集。Slisemap将流形可视化与可解释的人工智能相结合，用于研究黑盒机器学习模型和复杂模拟器的决策过程。通过Slisemap，我们找到一种嵌入，使得具有类似局部解释的数据项被聚集在一起。因此，Slisemap为我们提供了黑盒模型不同行为的概览。这使得Slisemap成为一种有监督的流形可视化方法，其中嵌入的模式反映了目标属性。本文展示了如何在物理数据上使用和评估Slisemap，并证明Slisemap在找到分类和回归模型的有意义信息方面是有帮助的。

    Manifold visualisation techniques are commonly used to visualise high-dimensional datasets in physical sciences. In this paper we apply a recently introduced manifold visualisation method, called Slise, on datasets from physics and chemistry. Slisemap combines manifold visualisation with explainable artificial intelligence. Explainable artificial intelligence is used to investigate the decision processes of black box machine learning models and complex simulators. With Slisemap we find an embedding such that data items with similar local explanations are grouped together. Hence, Slisemap gives us an overview of the different behaviours of a black box model. This makes Slisemap into a supervised manifold visualisation method, where the patterns in the embedding reflect a target property. In this paper we show how Slisemap can be used and evaluated on physical data and that Slisemap is helpful in finding meaningful information on classification and regression models trained on these data
    
[^38]: tagE: 让具身代理理解人类指令的方法

    tagE: Enabling an Embodied Agent to Understand Human Instructions. (arXiv:2310.15605v1 [cs.RO])

    [http://arxiv.org/abs/2310.15605](http://arxiv.org/abs/2310.15605)

    tagE是一种能够从自然语言指令中提取任务的具身代理系统，解决了智能代理理解人类意图时的歧义性和不完整性问题。

    

    自然语言是具有物理存在的智能代理与人类交流的主要方式。尽管有大量关注自然语言理解（NLU）的研究，如情感分析、意图预测、问题回答和摘要，但面向具身代理需要实际行动的情境的NLU的范围仍然有限。自然语言中的歧义性和不完整性给智能代理解读人类意图带来了挑战。为了应对这个问题，我们引入了一种新颖的系统，称为具身代理的任务和参数基础（tagE）。在核心部分，我们的系统采用了一种创新的神经网络模型，用以从用自然语言表达的复杂任务指令中提取一系列任务。我们提出的模型采用了一种编码器-解码器框架，加强了嵌套解码以有效地提取任务。

    Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural language present challenges for intelligent agents striving to decipher human intention. To tackle this predicament head-on, we introduce a novel system known as task and argument grounding for Embodied agents (tagE). At its core, our system employs an inventive neural network model designed to extract a series of tasks from complex task instructions expressed in natural language. Our proposed model adopts an encoder-decoder framework enriched with nested decoding to effectively extract tasks and t
    
[^39]: 检测开放海域海上监视中的有意AIS关闭使用自监督深度学习

    Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning. (arXiv:2310.15586v1 [cs.LG])

    [http://arxiv.org/abs/2310.15586](http://arxiv.org/abs/2310.15586)

    本论文提出了一种基于自监督深度学习技术和变压器模型的方法，用于检测开放海域海上监视中的有意AIS关闭。模型通过比较预测结果来报告检测到的异常情况。

    

    在海上交通监视中，检测非法活动，如非法捕鱼或非法货物转船是沿海管理的关键任务。在开放海域中，人们必须依赖船上的自动识别系统（AIS）发出的信息，这些信息被监视卫星捕获。然而，不诚实的船只通常会有意关闭其AIS发射机，以隐藏非法活动。在开放海上，很难将有意的AIS关闭与由于协议限制，恶劣天气条件或限制卫星位置而导致的接收缺失区分开来。本文提出了一种基于自监督深度学习技术和变压器模型的异常AIS接收丢失检测方法。通过使用历史数据，训练的模型预测未来一分钟是否应接收信息。然后，该模型通过比较预测结果来报告检测到的异常情况。

    In maritime traffic surveillance, detecting illegal activities, such as illegal fishing or transshipment of illicit products is a crucial task of the coastal administration. In the open sea, one has to rely on Automatic Identification System (AIS) message transmitted by on-board transponders, which are captured by surveillance satellites. However, insincere vessels often intentionally shut down their AIS transponders to hide illegal activities. In the open sea, it is very challenging to differentiate intentional AIS shutdowns from missing reception due to protocol limitations, bad weather conditions or restricting satellite positions. This paper presents a novel approach for the detection of abnormal AIS missing reception based on self-supervised deep learning techniques and transformer models. Using historical data, the trained model predicts if a message should be received in the upcoming minute or not. Afterwards, the model reports on detected anomalies by comparing the prediction w
    
[^40]: 教师引导的组合视觉推理的多模态表示

    Multimodal Representations for Teacher-Guided Compositional Visual Reasoning. (arXiv:2310.15585v1 [cs.CL])

    [http://arxiv.org/abs/2310.15585](http://arxiv.org/abs/2310.15585)

    本论文提出了一种教师引导的多模态表示方法，通过利用大规模跨模态编码器的特征来改进神经模块网络(NMN)，并引入了计划的教师引导的学习策略，以减少误差积累并提高性能。

    

    神经模块网络（NMN）是一种有吸引力的视觉问答方法，可以将问题翻译为由一系列推理子任务组成的程序，这些子任务按顺序在图像上执行以产生答案。与集成模型相比，NMN提供了更强的解释性，可以更好地理解底层推理过程。为了提高NMN的效果，我们提出利用大规模跨模态编码器获得的特征。此外，目前的NMN训练方法依赖于将模块输出传播到后续模块，导致预测误差累积和误生成答案。为了缓解这个问题，我们引入了一种NMN学习策略，涉及计划的教师引导。最初，模型完全由地面真实的中间输出引导，但随着训练的进行逐渐过渡到自主行为。这减少了误差积累，从而改善了性能。

    Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving 
    
[^41]: 加速无线通信网络上的分裂联合学习

    Accelerating Split Federated Learning over Wireless Communication Networks. (arXiv:2310.15584v1 [cs.LG])

    [http://arxiv.org/abs/2310.15584](http://arxiv.org/abs/2310.15584)

    本论文提出了一种加速无线通信网络上的分裂联合学习的方法。通过将深度神经网络分为两部分，在设备和服务器上分别进行共同训练或共同推理，以解决在资源受限的边缘设备上部署大规模深度神经网络的难题。通过选择分割点和带宽分配以最小化系统延迟，实现了联合优化。

    

    人工智能的发展为基于深度神经网络的应用提供了机遇。然而，庞大的参数量和计算复杂度使得将深度神经网络部署在资源受限的边缘设备上变得困难。针对这一挑战，一种有效的方法是模型分割/分裂，即将深度神经网络分为两部分，分别部署在设备和服务器上进行共同训练或共同推理。本文考虑了一种结合了联合学习的并行模型训练机制和分裂学习的模型分割结构的分裂联合学习（SFL）框架。我们考虑了具有个体分割点的异构设备的实际场景。我们建立了一个联合问题，即选择分割点和带宽分配以最小化系统延迟。通过使用交替优化，我们将问题分解为两个子问题，并进行求解。

    The development of artificial intelligence (AI) provides opportunities for the promotion of deep neural network (DNN)-based applications. However, the large amount of parameters and computational complexity of DNN makes it difficult to deploy it on edge devices which are resource-constrained. An efficient method to address this challenge is model partition/splitting, in which DNN is divided into two parts which are deployed on device and server respectively for co-training or co-inference. In this paper, we consider a split federated learning (SFL) framework that combines the parallel model training mechanism of federated learning (FL) and the model splitting structure of split learning (SL). We consider a practical scenario of heterogeneous devices with individual split points of DNN. We formulate a joint problem of split point selection and bandwidth allocation to minimize the system latency. By using alternating optimization, we decompose the problem into two sub-problems and solve 
    
[^42]: 透过变化的视角，可识别的潜在多项式因果模型

    Identifiable Latent Polynomial Causal Models Through the Lens of Change. (arXiv:2310.15580v1 [cs.LG])

    [http://arxiv.org/abs/2310.15580](http://arxiv.org/abs/2310.15580)

    本文通过扩展潜在因果模型的范围，从线性高斯模型转化为多项式模型，并研究了部分参数保持不变时的部分识别性结果。

    

    因果表示学习旨在从观察到的低级数据中揭示潜在的高级因果表示。其中一个主要任务是提供可靠的保证，以确保识别出这些潜在的因果模型，即可识别性。最近的一项突破性研究通过利用潜在因果变量之间在多个环境下的因果影响的变化来探索可识别性。然而，这一进展建立在潜在因果变量之间的因果关系严格遵循线性高斯模型的假设基础上。本文将潜在因果模型的范围扩展到涉及非线性因果关系的情况，这些关系由多项式模型表示，并且噪声分布符合指数分布族。此外，我们研究了对所有因果参数施加变化的必要性，并在部分参数保持不变的情况下提出了部分可识别性的结果。此外，我们提出了一种新颖的经验估计方法。

    Causal representation learning aims to unveil latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as identifiability. A recent breakthrough explores identifiability by leveraging the change of causal influences among latent causal variables across multiple environments \citep{liu2022identifying}. However, this progress rests on the assumption that the causal relationships among latent causal variables adhere strictly to linear Gaussian models. In this paper, we extend the scope of latent causal models to involve nonlinear causal relationships, represented by polynomial models, and general noise distributions conforming to the exponential family. Additionally, we investigate the necessity of imposing changes on all causal parameters and present partial identifiability results when part of them remains unchanged. Further, we propose a novel empirical estimation me
    
[^43]: 在PyTorch上重新实现的VMAF：一些实验结果

    VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])

    [http://arxiv.org/abs/2310.15578](http://arxiv.org/abs/2310.15578)

    这项研究重新在PyTorch上实现了VMAF，与标准实现进行比较，结果显示在VMAF单位上的差异小于$10^{-2}$。同时，研究了在使用VMAF作为目标函数时的梯度计算，并证明使用该函数进行训练不会导致梯度不良。

    

    基于标准的VMAF实现，我们提出了使用PyTorch框架实现VMAF的方法。对于这个实现，与标准的(libvmaf)进行比较，VMAF单位上的差异小于$10^{-2}$。我们研究了在使用VMAF作为目标函数时的梯度计算，并证明使用该函数进行训练不会导致梯度不良。

    Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
    
[^44]: 从Oja算法到乘法权重更新方法的应用

    From Oja's Algorithm to the Multiplicative Weights Update Method with Applications. (arXiv:2310.15559v1 [math.OC])

    [http://arxiv.org/abs/2310.15559](http://arxiv.org/abs/2310.15559)

    本文发现了当Oja算法应用于共享特征向量的对称矩阵时，可以直接用乘法权重更新方法的遗憾来界定其遗憾，并讨论了在单位球上的二次形式优化的应用。

    

    Oja算法是一个众所周知的在线算法，主要在随机主成分析的背景下进行研究。我们做出了一个简单的观察，然而据我们所知，这是一个新颖的发现，即当应用于一系列（不一定是随机的）共享公共特征向量的对称矩阵时，Oja算法的遗憾可以直接用于预测专家建议问题中的广为人知的乘法权重更新方法的遗憾来界定。讨论了在单位球上的二次形式优化的几个应用。

    Oja's algorithm is a well known online algorithm studied mainly in the context of stochastic principal component analysis. We make a simple observation, yet to the best of our knowledge a novel one, that when applied to a any (not necessarily stochastic) sequence of symmetric matrices which share common eigenvectors, the regret of Oja's algorithm could be directly bounded in terms of the regret of the well known multiplicative weights update method for the problem of prediction with expert advice. Several applications to optimization with quadratic forms over the unit sphere in $\reals^n$ are discussed.
    
[^45]: 基于转移学习的前一天负荷预测：以欧洲国家电力需求时间序列为例的案例研究

    Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series. (arXiv:2310.15555v1 [cs.LG])

    [http://arxiv.org/abs/2310.15555](http://arxiv.org/abs/2310.15555)

    本研究以欧洲国家的电力需求时间序列为例，通过转移学习的方法来进行短期负荷预测。研究采用神经网络模型，并使用聚类分析来识别相似的模式，以提高预测性能。

    

    短期负荷预测(STLF)对电力网络的日常运营至关重要。然而，电力需求时间序列的非线性、非平稳性和随机性使得STLF成为一项具有挑战性的任务。为了改进STLF，提出了各种预测方法，包括使用可能不包括目标序列的多个电力需求序列训练的神经网络(NN)模型。在本研究中，我们通过考虑一组表示典型欧洲国家未来一天电力需求的27个时间序列，来研究这种特殊情况下的STLF，即转移学习(TL)。我们采用一个流行且易于实施的NN模型，并进行聚类分析，以识别系列之间的相似模式并辅助TL。在这种背景下，编制了两种不同的TL方法，一种是带有聚类步骤的，一种是没有的，并将它们与典型的NN训练进行比较。

    Short-term load forecasting (STLF) is crucial for the daily operation of power grids. However, the non-linearity, non-stationarity, and randomness characterizing electricity demand time series renders STLF a challenging task. Various forecasting approaches have been proposed for improving STLF, including neural network (NN) models which are trained using data from multiple electricity demand series that may not necessary include the target series. In the present study, we investigate the performance of this special case of STLF, called transfer learning (TL), by considering a set of 27 time series that represent the national day-ahead electricity demand of indicative European countries. We employ a popular and easy-to-implement NN model and perform a clustering analysis to identify similar patterns among the series and assist TL. In this context, two different TL approaches, with and without the clustering step, are compiled and compared against each other as well as a typical NN train
    
[^46]: PET合成：自我监督自适应残差估计生成对抗网络的研究

    PET Synthesis via Self-supervised Adaptive Residual Estimation Generative Adversarial Network. (arXiv:2310.15550v1 [eess.IV])

    [http://arxiv.org/abs/2310.15550](http://arxiv.org/abs/2310.15550)

    本论文提出了一种自我监督自适应残差估计生成对抗网络，用于PET图像的合成。该方法解决了合成图像与真实图像之间的纹理和结构差异问题，同时对低剂量PET和标准PET之间的分布偏移进行了研究。

    

    正电子发射断层扫描（PET）是一种在临床诊断中广泛应用的高灵敏度的分子成像技术。减少PET的辐射暴露，同时保持足够的图像质量是一个问题。最近采用卷积神经网络（CNN）从低剂量PET生成合成高质量图像的方法被报道为低到高图像恢复技术的最先进方法。然而，这些方法易于展示合成图像和真实图像之间的纹理和结构差异。此外，低剂量PET和标准PET之间的分布偏移尚未完全研究。为了解决这些问题，我们开发了一种自我监督自适应残差估计生成对抗网络（SS-AEGAN）。我们引入了（1）自适应残差估计映射机制AE-Net，该机制通过采用低剂量PET和合成PET之间的残差图动态修正初步合成的PET图像。

    Positron emission tomography (PET) is a widely used, highly sensitive molecular imaging in clinical diagnosis. There is interest in reducing the radiation exposure from PET but also maintaining adequate image quality. Recent methods using convolutional neural networks (CNNs) to generate synthesized high-quality PET images from low-dose counterparts have been reported to be state-of-the-art for low-to-high image recovery methods. However, these methods are prone to exhibiting discrepancies in texture and structure between synthesized and real images. Furthermore, the distribution shift between low-dose PET and standard PET has not been fully investigated. To address these issues, we developed a self-supervised adaptive residual estimation generative adversarial network (SS-AEGAN). We introduce (1) An adaptive residual estimation mapping mechanism, AE-Net, designed to dynamically rectify the preliminary synthesized PET images by taking the residual map between the low-dose PET and synthe
    
[^47]: 张量优化中的算法正则化: 迈向矩阵感知中的升维方法

    Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing. (arXiv:2310.15549v1 [math.OC])

    [http://arxiv.org/abs/2310.15549](http://arxiv.org/abs/2310.15549)

    在矩阵感知问题中，通过对称、秩1张量进行优化求解时，将梯度下降应用于升维问题可以得到近似的秩1张量和具有逃逸方向的临界点，这结果强调了张量参数化和一阶方法在实现全局最优性方面的重要性。

    

    梯度下降（GD）在机器学习模型中对于泛化至关重要，因为它引入了隐式正则化，促进了紧凑的表示。在这项工作中，我们研究了GD在张量优化中引导隐式正则化的作用，尤其是在升维矩阵感知框架中的应用。最近提出的这个框架通过将对称的、秩1的张量上的优化问题转化成严格鞍点，从而解决了非凸的矩阵感知问题。我们发现，在足够小的初始化尺度下，将GD应用于这个升维问题，可以得到近似秩1的张量和具有逃逸方向的临界点。我们的研究结果强调了张量参数化与一阶方法在这类问题中实现全局最优性的重要性。

    Gradient descent (GD) is crucial for generalization in machine learning models, as it induces implicit regularization, promoting compact representations. In this work, we examine the role of GD in inducing implicit regularization for tensor optimization, particularly within the context of the lifted matrix sensing framework. This framework has been recently proposed to address the non-convex matrix sensing problem by transforming spurious solutions into strict saddles when optimizing over symmetric, rank-1 tensors. We show that, with sufficiently small initialization scale, GD applied to this lifted problem results in approximate rank-1 tensors and critical points with escape directions. Our findings underscore the significance of the tensor parametrization of matrix sensing, in combination with first-order methods, in achieving global optimality in such problems.
    
[^48]: 保持对称性的图注意力网络用于解决多尺度路由问题

    Symmetry-preserving graph attention network to solve routing problems at multiple resolutions. (arXiv:2310.15543v1 [cs.LG])

    [http://arxiv.org/abs/2310.15543](http://arxiv.org/abs/2310.15543)

    该论文介绍了一种保持对称性的图注意力网络，用于解决多尺度的路由问题。该方法是第一个完全等变的模型和训练方法，能够解决组合问题。此外，该方法还能够捕捉输入图的多尺度结构，从而避免了局部或次优解的问题。

    

    旅行商问题（TSP）和车辆路径问题（VRP）在准确性和计算时间方面通过机器学习（ML）方法的应用取得了合理的进展。然而，以前的工作都没有完全尊重TSP和VRP中产生的对称性，包括旋转、平移、排列和缩放。在这项工作中，我们引入了第一个完全等变的模型和训练方法来解决组合问题。此外，对于大型和长距离图的情况，捕捉输入图的多尺度结构（即从局部到全局信息）是至关重要的，而以前的方法仅限于提取局部信息，可能导致局部或次优解。为了解决上述限制，我们提出了一个多分辨率方案与等变图注意力网络（mEGAT）结构相结合，可以基于低级别和高级别的图表示学习最佳路径。

    Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs) have achieved reasonable improvement in accuracy and computation time with the adaptation of Machine Learning (ML) methods. However, none of the previous works completely respects the symmetries arising from TSPs and VRPs including rotation, translation, permutation, and scaling. In this work, we introduce the first-ever completely equivariant model and training to solve combinatorial problems. Furthermore, it is essential to capture the multiscale structure (i.e. from local to global information) of the input graph, especially for the cases of large and long-range graphs, while previous methods are limited to extracting only local information that can lead to a local or sub-optimal solution. To tackle the above limitation, we propose a Multiresolution scheme in combination with Equivariant Graph Attention network (mEGAT) architecture, which can learn the optimal route based on low-level and high-level graph res
    
[^49]: 矩阵机制的隐私放大

    Privacy Amplification for Matrix Mechanisms. (arXiv:2310.15526v1 [cs.LG])

    [http://arxiv.org/abs/2310.15526](http://arxiv.org/abs/2310.15526)

    提出了通过抽样分析矩阵机制的隐私放大算法MMCC，证明了通过条件性组合定理可以将相关输出视为独立输出，并展示了将噪声添加到DP-FTRL算法时可以渐近地与DP-SGD算法中放大后添加的噪声相匹配。

    

    隐私放大利用数据选择中的随机性来提供更严格的差分隐私保证。这种分析对于DP-SGD在机器学习中的成功至关重要，但对于最新的最先进算法并不适用。这是因为这些算法，即DP-FTRL，使用矩阵机制来添加相关噪声，而不是像DP-SGD那样添加独立噪声。在本文中，我们提出了“MMCC”算法，该算法是通过抽样来分析任何通用矩阵机制的隐私放大的首个算法。MMCC接近一个下界，当ε→0时，它趋近于该下界。为了分析MMCC中的相关输出，我们证明可以将其视为独立输出，通过对先前输出进行调节。我们的“条件性组合定理”具有广泛的实用性：我们使用它来显示添加到二叉树DP-FTRL的噪声可以渐近地与通过放大添加到DP-SGD中的噪声相匹配。我们的放大算法也具有实际的经验性实用性。

    Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.  In this paper, we propose "MMCC", the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as $\epsilon\to0$. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our "conditional composition theorem" has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our amplification algorithm also has practical empirical util
    
[^50]: 关于离散去噪扩散模型内在隐私属性的研究

    On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])

    [http://arxiv.org/abs/2310.15524](http://arxiv.org/abs/2310.15524)

    本研究探索了离散扩散模型在隐私保护方面的潜力，提供了关于训练数据集中每个数据点的隐私泄露的洞察，以及通过数据预处理减少合成数据集生成中隐私风险的方法。

    

    隐私问题导致合成数据集的创建激增，扩散模型成为一种有前景的方法。虽然以前的研究已经对这些模型进行了经验评估，但在提供数学特征化其隐私保护能力方面存在差距。为了解决这个问题，我们提出了离散扩散模型（DDMs）内在隐私保护的开创性理论研究，用于离散数据集生成。对于每个数据点的每个实例差异隐私（pDP），我们的框架阐明了给定训练数据集中每个数据点的潜在隐私泄露，从而为通过DDMs降低合成数据集生成的隐私风险提供了洞察。我们的界限还表明，使用$s$个大小的数据点进行训练会导致隐私泄露从$(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP到$(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP的激增。

    Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pu
    
[^51]: 生成式和对比式范式在图自监督学习中互补

    Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])

    [http://arxiv.org/abs/2310.15523](http://arxiv.org/abs/2310.15523)

    生成式和对比式范式在图自监督学习中是互补的，我们提出了图对比掩码自编码器（GCMAE）框架来统一它们，GCMAE通过利用对比学习的全局信息来弥补掩码自编码器在捕捉全局信息方面的不足。

    

    对于图自监督学习（GSSL），掩码自编码器（MAE）遵循生成式范式，并学习重构掩码图的边缘或节点特征。对比学习（CL）通过最大化同一图的增强视图之间的相似性来广泛用于GSSL。然而，MAE和CL在现有的GSSL工作中被单独考虑。我们观察到MAE和CL的范式是互补的，并提出了图对比掩码自编码器（GCMAE）框架来统一它们。具体而言，通过专注于局部边缘或节点特征，MAE不能捕捉到图的全局信息，并对特定的边缘和特征敏感。相反，在提取全局信息方面，CL表现出色，因为它考虑了图之间的关系。因此，我们将GCMAE装备了一个MAE分支和一个CL分支，并且这两个分支共享一个通用的编码器，这使得MAE分支能够利用CL分支提取的全局信息。为了强制GCMAE捕捉全局信息...

    For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows the generative paradigm and learns to reconstruct masked graph edges or node features. Contrastive Learning (CL) maximizes the similarity between augmented views of the same graph and is widely used for GSSL. However, MAE and CL are considered separately in existing works for GSSL. We observe that the MAE and CL paradigms are complementary and propose the graph contrastive masked autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local edges or node features, MAE cannot capture global information of the graph and is sensitive to particular edges and features. On the contrary, CL excels in extracting global information because it considers the relation between graphs. As such, we equip GCMAE with an MAE branch and a CL branch, and the two branches share a common encoder, which allows the MAE branch to exploit the global information extracted by the CL branch. To force GCMAE to capture glob
    
[^52]: 基于图注意力的深度强化学习用于解决带有负载相关成本的中国邮递员问题

    Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v1 [cs.LG])

    [http://arxiv.org/abs/2310.15516](http://arxiv.org/abs/2310.15516)

    本论文提出了一个基于图注意力的深度强化学习方法来解决带有负载相关成本的中国邮递员问题。该方法将问题形式化为马尔可夫决策过程，引入了一个编码器和解码器的自回归模型来有效处理问题。

    

    最近，深度强化学习（DRL）模型在解决路径规划问题方面展现了良好的结果。然而，大多数DRL求解器通常是用来解决节点路径规划问题，例如旅行推销员问题（TSP）。与此同时，关于应用神经方法来解决弧路径规划问题，例如中国邮递员问题（CPP），的研究却十分有限，因为与TSP相比，它们的解空间通常更加不规则和复杂。为了填补这些空白，本文提出了一个新的DRL框架，来解决带有负载相关成本（CPP-LC）的CPP问题，这是一个具有负载约束的复杂弧路径规划问题。我们方法的创新点有两个。首先，我们将CPP-LC问题形式化为马尔可夫决策过程（MDP）顺序模型。随后，我们引入了一种基于DRL的自回归模型，即Arc-DRL模型，它由一个编码器和一个解码器组成，可以有效处理CPP-LC问题。这样的框架使得DRL模型能够高效地工作。

    Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently 
    
[^53]: 在信息检索中评估基于约束满足的LLMs

    KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])

    [http://arxiv.org/abs/2310.15511](http://arxiv.org/abs/2310.15511)

    本研究评估了最先进的模型在信息检索中回答约束满足查询的能力，并引入了一个新的数据集KITAB来衡量语言模型的约束满足能力。

    

    我们研究了最先进的模型在信息检索中回答约束满足查询（例如，“圣地亚哥的冰淇淋店列表”）的能力。过去，这样的查询被认为只能通过网络搜索或知识库来解决。最近，大型语言模型（LLMs）在这个任务中展示了初步的能力。然而，许多当前的检索基准要么已饱和，要么不能衡量约束满足。受到对LLMs事实不正确和产生幻觉的日益关注的驱动，我们提出了KITAB，一个用于衡量语言模型约束满足能力的新数据集。KITAB包含600多位作者和13,000个查询的与书籍相关的数据，还提供了一个关联的动态数据收集和约束验证方法，以获得其他作者的类似测试数据。我们对GPT4和GPT3.5进行了扩展实验，对常见的失败模式进行了表征和解耦。

    We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes acros
    
[^54]: AutoDiff:结合自动编码器和扩散模型用于表格数据合成

    AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v1 [stat.ML])

    [http://arxiv.org/abs/2310.15479](http://arxiv.org/abs/2310.15479)

    使用自动编码器和扩散模型结合的AutoDiff模型可以有效地生成合成的表格数据，克服了表格数据中的异构特征和特征间相关性的挑战，生成的数据与真实数据在统计上非常相似，并在机器学习任务中表现良好。

    

    扩散模型已成为现代机器学习许多子领域中合成数据生成的主要范式，包括计算机视觉、语言模型或语音合成。在本文中，我们利用扩散模型的力量来生成合成的表格数据。表格数据中的异构特征一直是表格数据合成的主要障碍，我们通过使用自动编码器的架构来解决这个问题。与最先进的表格合成器相比，我们模型生成的合成表格在统计上与真实数据非常相似，并在机器学习工具的下游任务中表现良好。我们在15个公开可用的数据集上进行了实验。值得注意的是，我们的模型灵活地捕捉了特征之间的相关性，这是表格数据合成中长期存在的挑战。如若接纳了论文，我们的代码将根据要求提供，并且将公开发布。

    Diffusion model has become a main paradigm for synthetic data generation in many subfields of modern machine learning, including computer vision, language model, or speech synthesis. In this paper, we leverage the power of diffusion model for generating synthetic tabular data. The heterogeneous features in tabular data have been main obstacles in tabular data synthesis, and we tackle this problem by employing the auto-encoder architecture. When compared with the state-of-the-art tabular synthesizers, the resulting synthetic tables from our model show nice statistical fidelities to the real data, and perform well in downstream tasks for machine learning utilities. We conducted the experiments over 15 publicly available datasets. Notably, our model adeptly captures the correlations among features, which has been a long-standing challenge in tabular data synthesis. Our code is available upon request and will be publicly released if paper is accepted.
    
[^55]: 可解释的心衰风险预测的生存分析

    Interpretable Survival Analysis for Heart Failure Risk Prediction. (arXiv:2310.15472v1 [cs.LG])

    [http://arxiv.org/abs/2310.15472](http://arxiv.org/abs/2310.15472)

    该论文提出了一个新颖的、可解释的生存分析流程，利用改进的生存堆叠技术将生存分析问题转化为分类问题，并利用可解释性强的增强机器进行心衰风险预测。

    

    生存分析，或时间事件分析，在医疗研究中是一个重要且广泛存在的问题。医疗研究传统上依赖于Cox模型进行生存分析，因为其简单且可解释性强。Cox模型假设一个对数线性风险函数以及时间上的比例风险，当这些假设失败时可能表现不佳。基于机器学习的新型生存模型避免了这些假设，并提供了更高的准确性，但有时以模型的可解释性为代价，而这对于临床使用至关重要。我们提出了一种新颖的生存分析流程，既可解释性强，又能与最先进的生存模型竞争。具体来说，我们使用改进的生存堆叠技术将生存分析问题转化为一个分类问题，使用ControlBurn进行特征选择，并使用可解释的增强机器生成可解释的预测。为了评估我们的流程，我们预测心衰的风险。

    Survival analysis, or time-to-event analysis, is an important and widespread problem in healthcare research. Medical research has traditionally relied on Cox models for survival analysis, due to their simplicity and interpretability. Cox models assume a log-linear hazard function as well as proportional hazards over time, and can perform poorly when these assumptions fail. Newer survival models based on machine learning avoid these assumptions and offer improved accuracy, yet sometimes at the expense of model interpretability, which is vital for clinical use. We propose a novel survival analysis pipeline that is both interpretable and competitive with state-of-the-art survival models. Specifically, we use an improved version of survival stacking to transform a survival analysis problem to a classification problem, ControlBurn to perform feature selection, and Explainable Boosting Machines to generate interpretable predictions. To evaluate our pipeline, we predict risk of heart failure 
    
[^56]: 持续事件提取与语义混淆修正

    Continual Event Extraction with Semantic Confusion Rectification. (arXiv:2310.15470v1 [cs.CL])

    [http://arxiv.org/abs/2310.15470](http://arxiv.org/abs/2310.15470)

    本文提出了一种带有语义混淆修正的持续事件提取模型，通过标注伪标签和传递关键知识来缓解事件类型语义混淆并提高模型在长尾事件类型的理解上的性能。

    

    我们研究了持续事件提取，旨在提取不断出现的事件信息同时避免遗忘。我们观察到，事件类型的语义混淆源于同一文本的注释随时间更新。事件类型之间的不平衡甚至加剧了这个问题。本文提出了一种新颖的带有语义混淆修正的持续事件提取模型。我们为每个句子标注伪标签以缓解语义混淆。我们在当前模型和之前模型之间传递关键知识，以增强对事件类型的理解。此外，我们通过利用其他相关类型来鼓励模型关注长尾事件类型的语义。实验结果表明，我们的模型优于最先进的基线模型，并且在不平衡的数据集上表现出良好的性能。

    We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer pivotal knowledge between current and previous models to enhance the understanding of event types. Moreover, we encourage the model to focus on the semantics of long-tailed event types by leveraging other associated types. Experimental results show that our model outperforms state-of-the-art baselines and is proficient in imbalanced datasets.
    
[^57]: 在可再生能源系统和电网优化中赋能分布式解决方案

    Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization. (arXiv:2310.15468v1 [cs.LG])

    [http://arxiv.org/abs/2310.15468](http://arxiv.org/abs/2310.15468)

    本研究探讨了从集中到分散的方法在电力行业中的转变，并重点讨论了机器学习对于可再生能源赋能和电网管理的关键作用。在预测可再生能源的产生和消耗方面，机器学习模型的应用变得越来越重要。将大数据和机器学习应用于智能电网可以提高能源效率、更好地响应需求和更好地整合可再生能源。然而，需要解决处理大数据量、网络安全和专业知识等挑战。

    

    本研究探讨了电力行业从集中式向分散式方法的转变，特别关注机器学习（ML）的进展在赋能可再生能源和改善电网管理方面的关键作用。ML模型在预测可再生能源的产生和消耗方面变得越来越重要，使用了人工神经网络、支持向量机和决策树等各种技术。此外，还采用了数据预处理方法，如数据分割、归一化、分解和离散化，以提高预测准确性。将大数据和ML融入智能电网具有多种优势，包括提高能源效率，更有效地应对需求，更好地整合可再生能源。然而，还需要解决处理大数据量、确保网络安全和获取专业知识等挑战。研究部分。

    This study delves into the shift from centralized to decentralized approaches in the electricity industry, with a particular focus on how machine learning (ML) advancements play a crucial role in empowering renewable energy sources and improving grid management. ML models have become increasingly important in predicting renewable energy generation and consumption, utilizing various techniques like artificial neural networks, support vector machines, and decision trees. Furthermore, data preprocessing methods, such as data splitting, normalization, decomposition, and discretization, are employed to enhance prediction accuracy.  The incorporation of big data and ML into smart grids offers several advantages, including heightened energy efficiency, more effective responses to demand, and better integration of renewable energy sources. Nevertheless, challenges like handling large data volumes, ensuring cybersecurity, and obtaining specialized expertise must be addressed. The research inves
    
[^58]: EKGNet：一种用于患者内心律失常分类的10.96μW全模拟神经网络

    EKGNet: A 10.96{\mu}W Fully Analog Neural Network for Intra-Patient Arrhythmia Classification. (arXiv:2310.15466v1 [cs.LG])

    [http://arxiv.org/abs/2310.15466](http://arxiv.org/abs/2310.15466)

    EKGNet是一种全模拟的神经网络架构，用于心电图心律失常分类。它利用模拟计算和亚阈值晶体管的能量效率，消除了对模拟到数字转换器和静态随机存储器的需求。实验结果表明，EKGNet在内患者心律失常分类和心肌梗死分类中表现出色，具有很大的潜力。

    

    我们提出了一种结合模拟计算和深度学习的综合方法，用于心电图(ECG)心律失常分类。我们提出了EKGNet，这是一种硬件高效且完全模拟的心律失常分类架构，可以在低功耗下实现高准确性。该架构利用了亚阈值区域中的晶体管的能量效率，消除了模拟到数字转换器(ADC)和静态随机存储器(SRAM)的需求。系统设计包括一种新颖的模拟序列乘积累加(MAC)电路，可以缓解工艺、供电电压和温度变化。对PhysioNet的MIT-BIH和PTB诊断数据集进行的实验评估表明，所提出的方法的有效性，分别实现了95%和94.25%的平均均衡准确性，用于患者内心律失常分类和心肌梗死(MI)分类。这种创新方法为未来的研究提供了一个有前途的途径。

    We present an integrated approach by combining analog computing and deep learning for electrocardiogram (ECG) arrhythmia classification. We propose EKGNet, a hardware-efficient and fully analog arrhythmia classification architecture that archives high accuracy with low power consumption. The proposed architecture leverages the energy efficiency of transistors operating in the subthreshold region, eliminating the need for analog-to-digital converters (ADC) and static random access memory (SRAM). The system design includes a novel analog sequential Multiply-Accumulate (MAC) circuit that mitigates process, supply voltage, and temperature variations. Experimental evaluations on PhysioNet's MIT-BIH and PTB Diagnostics datasets demonstrate the effectiveness of the proposed method, achieving average balanced accuracy of 95% and 94.25% for intra-patient arrhythmia classification and myocardial infarction (MI) classification, respectively. This innovative approach presents a promising avenue fo
    
[^59]: 具有公共特征的个人化学习

    Private Learning with Public Features. (arXiv:2310.15454v1 [cs.LG])

    [http://arxiv.org/abs/2310.15454](http://arxiv.org/abs/2310.15454)

    本研究针对具有私有和公共特征的个人化学习问题进行了研究，并提出了一种新的算法，通过只保护特定的统计量来提高实用性，在两个标准私有推荐基准测试中达到了最新的成果。

    

    我们研究了一类私有学习问题，其中数据是私有特征和公共特征的联结。这在私人个性化任务中经常出现，例如推荐或广告预测，在这些任务中，与个人相关的特征是敏感的，而与物品相关的特征（如推荐的电影或歌曲，或者向用户展示的广告）是公开可用的并且不需要保护。一个自然的问题是，在公共特征存在的情况下，私有算法是否能够实现更高的实用性。我们对多编码器模型进行了研究，其中一个编码器处理公共特征。我们开发了新的算法，利用这种分离只保护某些充分统计量（而不是向梯度添加噪音）。这种方法在线性回归中保证了实用性改进，并且在两个标准私有推荐基准测试中取得了最新的成果，证明了方法的重要性。

    We study a class of private learning problems in which the data is a join of private and public features. This is often the case in private personalization tasks such as recommendation or ad prediction, in which features related to individuals are sensitive, while features related to items (the movies or songs to be recommended, or the ads to be shown to users) are publicly available and do not require protection. A natural question is whether private algorithms can achieve higher utility in the presence of public features. We give a positive answer for multi-encoder models where one of the encoders operates on public features. We develop new algorithms that take advantage of this separation by only protecting certain sufficient statistics (instead of adding noise to the gradient). This method has a guaranteed utility improvement for linear regression, and importantly, achieves the state of the art on two standard private recommendation benchmarks, demonstrating the importance of metho
    
[^60]: 通用因果表达学习的可识别性和可实现性

    General Identifiability and Achievability for Causal Representation Learning. (arXiv:2310.15450v1 [cs.LG])

    [http://arxiv.org/abs/2310.15450](http://arxiv.org/abs/2310.15450)

    本文在通用非参数因果潜变量模型和通用转换模型下，通过非耦合干预建立了因果表达学习的可识别性和可实现性结果。在不知道具体干预对应的节点的情况下，这些结果保证了潜在的因果模型和变量的完美恢复，并设计了一个算法来实现这一目标。

    

    本文关注通用非参数因果潜变量模型和将潜变量数据映射到观测数据的通用转换模型下的因果表达学习。通过在潜在因果图中每个节点进行两个硬性非耦合干预来建立可识别性和可实现性结果。值得注意的是，人们不知道哪个干预环境对应的节点是相同的（因此是非耦合环境）。在可识别性方面，本文确保在非耦合干预下能够完美恢复潜在的因果模型和变量。在可实现性方面，设计了一个算法，利用观测和干预数据，并提供了对该算法的可验证的保证，以恢复潜在的因果模型和变量。该算法利用不同环境中的得分变化来估计转换器的逆和随后的潜变量。该分析还...

    This paper focuses on causal representation learning (CRL) under a general nonparametric causal latent model and a general transformation model that maps the latent data to the observational data. It establishes \textbf{identifiability} and \textbf{achievability} results using two hard \textbf{uncoupled} interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled environments). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees for the algorithm. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, addit
    
[^61]: 一种加速的一阶正则动量下降算法用于随机非凸-凹极小极大问题

    An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems. (arXiv:2310.15448v1 [math.OC])

    [http://arxiv.org/abs/2310.15448](http://arxiv.org/abs/2310.15448)

    本文提出了一种加速的算法来解决随机非凸-凹极小极大问题，并证明了该算法迭代复杂度达到了最佳已知界限。

    

    随机非凸极小极大问题近年来在机器学习、信号处理等领域引起了广泛关注。本文提出了一种加速的一阶正则动量下降算法（FORMDA）用于解决随机非凸-凹极小极大问题。证明了该算法的迭代复杂度为$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$以达到$\varepsilon$-稳定点，这在目标函数稳定性下实现了解决随机非凸-凹极小极大问题的最佳已知复杂度界限。

    Stochastic nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose an accelerated first-order regularized momentum descent ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax problems. The iteration complexity of the algorithm is proved to be $\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$ to obtain an $\varepsilon$-stationary point, which achieves the best-known complexity bound for single-loop algorithms to solve the stochastic nonconvex-concave minimax problems under the stationarity of the objective function.
    
[^62]: Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])的学习动态：后验崩塌阈值，多余的潜在空间陷阱以及KL退火的加速。

    Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])

    [http://arxiv.org/abs/2310.15440](http://arxiv.org/abs/2310.15440)

    该论文对线性VAE中的学习动态进行了理论分析，证明了在大输入维度的限制下，学习动态收敛为确定性过程，并提出了后验崩塌阈值和KL退火加速策略。研究发现，VAE最初学习到纠缠表示，并逐渐获得不纠缠的表示。在确定性过程中，超fluous潜在空间是一个潜在的问题。

    

    变分自编码器（VAEs）存在一个臭名昭著的问题，即变分后验通常与先验非常接近，这种现象称为后验崩塌，它阻碍了表示学习的质量。为了缓解这个问题，提出了一个可调的超参数β以及一种称为KL退火的策略来调整该参数。本研究在一个简化的VAE中对学习动态进行了理论分析。经过严格证明，在输入维度趋于无穷大的极限下，动态收敛为确定性过程，从而实现了对泛化误差的详细动态分析。此外，分析还表明，VAE最初学习到纠缠表示，并逐渐获得不纠缠的表示。对确定性过程的固定点分析揭示，当β超过一定阈值时，无论学习周期多长，后验崩塌都是不可避免的。此外，研究还探讨了超fluous潜在空间的问题。

    Variational autoencoders (VAEs) face a notorious problem wherein the variational posterior often aligns closely with the prior, a phenomenon known as posterior collapse, which hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter $\beta$ and a strategy for annealing this parameter, called KL annealing, are proposed. This study presents a theoretical analysis of the learning dynamics in a minimal VAE. It is rigorously proved that the dynamics converge to a deterministic process within the limit of large input dimensions, thereby enabling a detailed dynamical analysis of the generalization error. Furthermore, the analysis shows that the VAE initially learns entangled representations and gradually acquires disentangled representations. A fixed-point analysis of the deterministic process reveals that when $\beta$ exceeds a certain threshold, posterior collapse becomes inevitable regardless of the learning period. Additionally, the superfluou
    
[^63]: 基于策略卷积的大动作空间离策略评估

    Off-Policy Evaluation for Large Action Spaces via Policy Convolution. (arXiv:2310.15433v1 [cs.LG])

    [http://arxiv.org/abs/2310.15433](http://arxiv.org/abs/2310.15433)

    本研究提出了一种名为策略卷积（PC）的离策略估计方法，该方法通过动作嵌入来解决大动作空间下的分布转移问题，可以在偏差和方差之间进行权衡

    

    发展准确的离策略估计器对于评估和优化新策略至关重要。离策略估计的主要挑战在于生成数据的记录策略和我们要评估的目标策略之间的分布转移。通常，纠正分布转移的技术涉及某种形式的重要性采样。这种方法导致了无偏值估计，但往往会带来高方差的代价，即使在简单的一步情境多臂老虎机的情况下也是如此。此外，重要性采样依赖于共同支持假设，在动作空间很大时变得不切实际。为了解决这些挑战，我们引入了策略卷积 (PC)家族的估计器。这些方法利用通过动作嵌入提供的动作内部结构进行策略的策略卷积。这种卷积引入了独特的偏差-方差权衡，可以进行控制

    Developing accurate off-policy estimators is crucial for both evaluating and optimizing for new policies. The main challenge in off-policy estimation is the distribution shift between the logging policy that generates data and the target policy that we aim to evaluate. Typically, techniques for correcting distribution shift involve some form of importance sampling. This approach results in unbiased value estimation but often comes with the trade-off of high variance, even in the simpler case of one-step contextual bandits. Furthermore, importance sampling relies on the common support assumption, which becomes impractical when the action space is large. To address these challenges, we introduce the Policy Convolution (PC) family of estimators. These methods leverage latent structure within actions -- made available through action embeddings -- to strategically convolve the logging and target policies. This convolution introduces a unique bias-variance trade-off, which can be controlled 
    
[^64]: Mason-Alberta音标分割器: 基于深度神经网络和插值的强制对齐系统

    The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation. (arXiv:2310.15425v1 [eess.AS])

    [http://arxiv.org/abs/2310.15425](http://arxiv.org/abs/2310.15425)

    本文介绍了一种基于深度神经网络和插值的新型强制对齐系统，该系统名为Mason-Alberta音标分割器。该系统的创新点包括将声学模型视为标注任务，而不是分类任务，并采用了插值技术实现更精确的分段边界。

    

    强制对齐系统在给定正字法转录的语音数据中自动确定分段边界。这些工具在语音学中很常见，以便使用那些手动转录和分段难以实现的语音数据。在本文中，我们描述了一个新的基于神经网络的强制对齐系统，即Mason-Alberta音标分割器（MAPS）。MAPS对齐器作为我们追求强制对齐系统两个潜在改进的试验平台。第一个是将强制对齐器中的声学模型视为标注任务，而不是分类任务，这是基于人们对语音中的段落并不是真正离散和常常重叠的共同认识。第二个是插值技术，使得边界可以比现代强制对齐系统常见的10毫秒限制更精确。我们将我们系统的配置与最先进的系统Montreal Forced Aligner进行比较。

    Forced alignment systems automatically determine boundaries between segments in speech data, given an orthographic transcription. These tools are commonplace in phonetics to facilitate the use of speech data that would be infeasible to manually transcribe and segment. In the present paper, we describe a new neural network-based forced alignment system, the Mason-Alberta Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two possible improvements we pursue for forced alignment systems. The first is treating the acoustic model in a forced aligner as a tagging task, rather than a classification task, motivated by the common understanding that segments in speech are not truly discrete and commonly overlap. The second is an interpolation technique to allow boundaries more precise than the common 10 ms limit in modern forced alignment systems. We compare configurations of our system to a state-of-the-art system, the Montreal Forced Aligner. The tagging approach did not gener
    
[^65]: 政策优化中的分形景观

    Fractal Landscapes in Policy Optimization. (arXiv:2310.15418v1 [cs.LG])

    [http://arxiv.org/abs/2310.15418](http://arxiv.org/abs/2310.15418)

    该论文研究了政策优化过程中非平滑或分形的优化景观，提出了一种理解策略梯度方法固有限制的框架，并开发了一种实用方法来识别训练过程中是否遇到分形景观。

    

    策略梯度是连续领域深度强化学习的核心。尽管取得了许多成功，但实践中经常观察到利用策略梯度进行RL训练可能因为多种原因而失败，甚至在已知解的标准控制问题中也是如此。我们提出了一个框架来理解策略梯度方法的一个固有限制：对于某些类别的MDPs，策略空间中的优化景观可以非常非平滑或分形，以至于根本不存在需要估计的梯度。我们借鉴混沌理论和非平滑分析的方法，分析了策略优化目标的最大Lyapunov指数和Hölder指数。此外，我们还开发了一种实用方法，可以从样本中估计目标函数的局部平滑性，以便识别训练过程是否遇到分形景观。我们展示了实验来说明一些策略优化失败的情况。

    Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can b
    
[^66]: 基于点/顺序重构的命名分数条件时间序列异常检测

    Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction. (arXiv:2310.15416v1 [cs.LG])

    [http://arxiv.org/abs/2310.15416](http://arxiv.org/abs/2310.15416)

    本文提出了一种利用基于点/顺序重构模型的框架来进行无监督时间序列异常检测的方法。通过计算重构误差的组合值之比得到命名分数, 进一步结合命名分数和异常分数导出感应异常分数，从而实现对点异常和上下文异常的量化和检测。

    

    由于复杂性和多样性模式的存在，时间序列异常检测是具有挑战性的。一个主要的困难在于建模时间相关的关系以寻找上下文异常，同时保持对点异常的检测准确性。在本文中，我们提出了一个无监督时间序列异常检测的框架，利用了基于点和序列的重构模型。基于点的模型试图量化点异常，而基于序列的模型试图量化点异常和上下文异常。在假设观察到的时间点是从一种标准时间点开始的两个阶段的偏离值的情况下，我们引入了一个由重构误差的组合值之比计算得出的命名分数。我们通过进一步整合命名分数和异常分数来导出感应异常分数，然后在特定条件下理论上证明了感应异常分数优于原始异常分数的优越性。

    Time series anomaly detection is challenging due to the complexity and variety of patterns that can occur. One major difficulty arises from modeling time-dependent relationships to find contextual anomalies while maintaining detection accuracy for point anomalies. In this paper, we propose a framework for unsupervised time series anomaly detection that utilizes point-based and sequence-based reconstruction models. The point-based model attempts to quantify point anomalies, and the sequence-based model attempts to quantify both point and contextual anomalies. Under the formulation that the observed time point is a two-stage deviated value from a nominal time point, we introduce a nominality score calculated from the ratio of a combined value of the reconstruction errors. We derive an induced anomaly score by further integrating the nominality score and anomaly score, then theoretically prove the superiority of the induced anomaly score over the original anomaly score under certain condi
    
[^67]: 人工智能与人类协作的多样化约定

    Diverse Conventions for Human-AI Collaboration. (arXiv:2310.15414v1 [cs.AI])

    [http://arxiv.org/abs/2310.15414](http://arxiv.org/abs/2310.15414)

    本研究通过最大化自我对弈的奖励并最小化与先前发现的约定交互时的奖励来生成多样化约定，确保学到的策略在交叉对弈的对抗性优化过程中遵守善意行事

    

    在合作多智体游戏中，约定对于强大的性能至关重要，因为它们允许玩家在没有明确交流的情况下进行共同战略的协调。然而，标准的多智体强化学习技术，如自我对弈，会收敛到任意和非多样化的约定，导致在与新的合作伙伴互动时表现不佳。本文提出了一种通过在自我对弈过程中最大化其奖励，并在与先前发现的约定进行交互时最小化其奖励（交叉对弈），以刺激约定在语义上有所不同的技术，来生成多样化约定。为了确保学到的策略在交叉对弈的对抗性优化过程中始终遵守善意行事，我们引入了混合对弈（mixed-play）的概念，即通过从自我对弈和交叉对弈的转换中随机生成初始状态，并学习在此初始状态下最大化自我对弈的奖励。我们分析了这种方法的优势

    Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits
    
[^68]: 高效的带有Tsybakov噪声的半空间主动学习：一种非凸优化方法

    Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach. (arXiv:2310.15411v1 [cs.LG])

    [http://arxiv.org/abs/2310.15411](http://arxiv.org/abs/2310.15411)

    这个论文研究了在结构化无标签数据分布下，对于具有Tsybakov噪声的$d$维半空间，计算和标签的高效PAC主动学习问题。通过设计一种基于非凸优化的算法，它能够在一定的噪声参数范围内达到较低的标签复杂度。

    

    我们研究了在结构化无标签数据分布下，对于具有Tsybakov噪声的$d$维半空间，计算和标签的高效PAC主动学习问题。受到\cite{diakonikolas2020learning}的启发，我们证明了平滑非凸损失函数的任何近似一阶稳定点都会产生一个具有低过量误差保证的半空间。根据上述结构性结果，我们设计了一种基于非凸优化的算法，其标签复杂度为$\tilde{O}(d (\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$，在Tsybakov噪声参数$\alpha \in (\frac13, 1]$的假设下，这缩小了先前已知的高效被动或主动算法的标签复杂度间隔。

    We study the problem of computationally and label efficient PAC active learning $d$-dimensional halfspaces with Tsybakov Noise~\citep{tsybakov2004optimal} under structured unlabeled data distributions. Inspired by~\cite{diakonikolas2020learning}, we prove that any approximate first-order stationary point of a smooth nonconvex loss function yields a halfspace with a low excess error guarantee. In light of the above structural result, we design a nonconvex optimization-based algorithm with a label complexity of $\tilde{O}(d (\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$\footnote{In the main body of this work, we use $\tilde{O}(\cdot), \tilde{\Theta}(\cdot)$ to hide factors of the form $\polylog(d, \frac{1}{\epsilon}, \frac{1}{\delta})$}, under the assumption that the Tsybakov noise parameter $\alpha \in (\frac13, 1]$, which narrows down the gap between the label complexities of the previously known efficient passive or active algorithms~\citep{diakonikolas2020polynomial,zhang2021im
    
[^69]: DoGE: 使用泛化估计进行领域重新加权

    DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])

    [http://arxiv.org/abs/2310.15393](http://arxiv.org/abs/2310.15393)

    DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。

    

    预训练数据语料库的覆盖范围和组成对大型语言模型的泛化能力有着重要影响。传统上，预训练语料库由各种来源领域（如CommonCrawl、Wikipedia、Github等）按照特定的采样概率（领域权重）组成。然而，当前的方法缺乏一种基于最终泛化目标优化领域权重的原则方法。我们提出了一种称为DOmain reweighting with Generalization Estimation（DoGE）的方法，其中我们重新调整了每个领域的采样概率，根据它对最终泛化目标的贡献进行了基于梯度的泛化估计函数评估。首先，我们使用最小最大优化训练了一个小规模的代理模型来获取重新加权的领域权重。在每一步中，通过镜像下降法更新领域权重以最大化整体的泛化增益。最后，我们使用获得的领域权重来训练一个规模更大的完整语言模型。

    The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
    
[^70]: MEMPSEP III. 一种面向机器学习的多元数据集，用于使用多元集成方法预测太阳能粒子事件的发生和属性（arXiv:2310.15390v1 [astro-ph.SR]）

    MEMPSEP III. A machine learning-oriented multivariate data set for forecasting the Occurrence and Properties of Solar Energetic Particle Events using a Multivariate Ensemble Approach. (arXiv:2310.15390v1 [astro-ph.SR])

    [http://arxiv.org/abs/2310.15390](http://arxiv.org/abs/2310.15390)

    这个论文介绍了一个面向机器学习的多元数据集，用于预测太阳能粒子事件的发生和属性。数据集包含了多个航天器收集的原位和遥感观测数据，能够与产生太阳能粒子事件的物理过程相关联。

    

    我们介绍了一个新的多元数据集，利用多个航天器收集原位和遥感观测的太阳圈层测量数据，这些数据与产生太阳能粒子事件的物理过程相关。利用地球同步环境卫星（GOES）太阳系活动事件列表从太阳活动周期（SC）23和SC 24的一部分（1998-2013），我们确定了252个产生太阳能粒子（SEP）的太阳事件（耀斑）和17,542个不产生SEP的事件。对于每个确定的事件，我们记录了1 au处的局部等离子体属性，如高能质子和电子数据，上游太阳风条件以及国家地理学会卫星（GOES）和高级组成探测器（ACE）航天器上的不同仪器测得的星际磁场矢量量。我们还收集了来自太阳动力学观测卫星（SDO）、太阳和日球卫星（SoHO）以及Wind太阳射电仪的遥感数据。该数据集旨在允许进行各种变化

    We introduce a new multivariate data set that utilizes multiple spacecraft collecting in-situ and remote sensing heliospheric measurements shown to be linked to physical processes responsible for generating solar energetic particles (SEPs). Using the Geostationary Operational Environmental Satellites (GOES) flare event list from Solar Cycle (SC) 23 and part of SC 24 (1998-2013), we identify 252 solar events (flares) that produce SEPs and 17,542 events that do not. For each identified event, we acquire the local plasma properties at 1 au, such as energetic proton and electron data, upstream solar wind conditions, and the interplanetary magnetic field vector quantities using various instruments onboard GOES and the Advanced Composition Explorer (ACE) spacecraft. We also collect remote sensing data from instruments onboard the Solar Dynamic Observatory (SDO), Solar and Heliospheric Observatory (SoHO), and the Wind solar radio instrument WAVES. The data set is designed to allow for variati
    
[^71]: 语言模型预训练的不可约课程

    Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])

    [http://arxiv.org/abs/2310.15389](http://arxiv.org/abs/2310.15389)

    本论文提出了一种不可约课程算法，用于语言模型预训练，通过优先选择具有更高学习能力的样本，并使用小规模代理模型模拟样本丢失，从而在大型语言模型上解决了传统数据选择方法的困难，并在实验证明算法能够持续改进模型性能。

    

    训练大型语言模型的自动数据选择和课程设计具有挑战性，只有少数现有的方法在标准训练上显示出改进。此外，当前的方案更关注领域级别的选择，忽视了每个单独训练点的更细粒度的贡献。在大型语言模型上应用传统的数据点选择方法很困难：大多数在线批选择方法执行两次前向或后向传递，这会带来巨大的额外成本。为了克服这些障碍，我们提出了不可约课程作为语言模型预训练的课程学习算法，该算法优先选择具有更高学习能力的样本。具体而言，为了避免过高的额外计算开销，我们使用小规模代理模型模拟样本丢失沿主模型训练轨迹的情况。我们在RedPajama-1B数据集上的实验表明，课程学习算法能够持续改进模型在验证集上的性能。

    Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on valida
    
[^72]: 在智能环境中通过视频进行自监督预训练的远程心率监测

    Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training. (arXiv:2310.15388v1 [cs.CV])

    [http://arxiv.org/abs/2310.15388](http://arxiv.org/abs/2310.15388)

    该论文提出了一种利用自监督学习的方法，在智能环境中通过视频进行远程心率监测和估计，减少了对标记数据的依赖并提高了性能。

    

    深度学习的最新进展使得通过分析视频远程估计心率在智能环境中变得越来越可行。然而，深度学习方法的一个显著限制是它们对大量标记数据的严重依赖以进行有效的训练。为了解决这个问题，自监督学习作为一种有前途的途径应运而生。在此基础上，我们提出了一种解决方案，利用自监督对比学习来估计远程光谱测量（PPG）和心率监测，从而减少对标记数据的依赖并提高性能。我们提出使用3种空间和3种时间增强方法训练编码器，并通过对比框架利用编码器的后期中间嵌入进行远程PPG和心率估计。我们在两个公开数据集上的实验展示了我们提出的方法在多个相关工作以及监督学习方法上的改进。

    Recent advances in deep learning have made it increasingly feasible to estimate heart rate remotely in smart environments by analyzing videos. However, a notable limitation of deep learning methods is their heavy reliance on extensive sets of labeled data for effective training. To address this issue, self-supervised learning has emerged as a promising avenue. Building on this, we introduce a solution that utilizes self-supervised contrastive learning for the estimation of remote photoplethysmography (PPG) and heart rate monitoring, thereby reducing the dependence on labeled data and enhancing performance. We propose the use of 3 spatial and 3 temporal augmentations for training an encoder through a contrastive framework, followed by utilizing the late-intermediate embeddings of the encoder for remote PPG and heart rate estimation. Our experiments on two publicly available datasets showcase the improvement of our proposed approach over several related works as well as supervised learni
    
[^73]: 生成对抗网络的误差分析

    Error analysis of generative adversarial network. (arXiv:2310.15387v1 [stat.ML])

    [http://arxiv.org/abs/2310.15387](http://arxiv.org/abs/2310.15387)

    该论文利用Talagrand不等式和Borel-Cantelli引理，建立了生成对抗网络（GAN）的误差紧致收敛速度，并提供了改进的收敛速度方法。

    

    生成对抗网络（GAN）是近年来用于高维分布学习的重要模型。然而，急需一种全面的方法来理解其误差收敛速度。本研究着重研究基于鉴别器和生成器神经网络的函数类别构建的GAN模型的误差收敛速度。在我们的假设下，这些函数属于VC类型，并具有有界信封函数，可以应用Talagrand不等式。通过运用Talagrand不等式和Borel-Cantelli引理，我们建立了GAN误差的紧致收敛速度。该方法还可以应用于现有的GAN误差估计，并获得改进的收敛速度。特别是，在我们的定义中，用神经网络距离定义的误差是一种特殊情况误差。

    The generative adversarial network (GAN) is an important model developed for high-dimensional distribution learning in recent years. However, there is a pressing need for a comprehensive method to understand its error convergence rate. In this research, we focus on studying the error convergence rate of the GAN model that is based on a class of functions encompassing the discriminator and generator neural networks. These functions are VC type with bounded envelope function under our assumptions, enabling the application of the Talagrand inequality. By employing the Talagrand inequality and Borel-Cantelli lemma, we establish a tight convergence rate for the error of GAN. This method can also be applied on existing error estimations of GAN and yields improved convergence rates. In particular, the error defined with the neural network distance is a special case error in our definition.
    
[^74]: 修正Koopman表示的方法

    Course Correcting Koopman Representations. (arXiv:2310.15386v1 [cs.LG])

    [http://arxiv.org/abs/2310.15386](http://arxiv.org/abs/2310.15386)

    本文修正了Koopman表示的方法，并提出了一种称为“周期重新编码”的机制，用于准确捕捉非线性动力系统中的长期动态。

    

    Koopman表示旨在学习非线性动力系统中导致潜在空间线性动力学的特征。从理论上讲，这些特征可以用于简化非线性动力系统建模和控制中的许多问题。在本文中，我们研究了此问题的自动编码器方法，并探讨了它们在建模动力学方面的不同应用，特别是在长期预测未来状态方面。我们发现在潜在空间中预测未来状态存在一些限制，并提出了一种称为“周期重新编码”的推理时间机制，以实现长期动态的准确捕捉。我们通过在低维和高维非线性动力系统上的实验证明了该方法的合理性和实用性。

    Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.
    
[^75]: 基于邻近特征统计增强的联邦三维医学体积分割

    Vicinal Feature Statistics Augmentation for Federated 3D Medical Volume Segmentation. (arXiv:2310.15371v1 [eess.IV])

    [http://arxiv.org/abs/2310.15371](http://arxiv.org/abs/2310.15371)

    本论文提出了一种基于邻近特征统计增强的联邦三维医学体积分割方案，通过数据增强来提高FL分割的性能，并解决了标记数据有限和数据分布异构的问题。

    

    联邦学习（FL）使得多个医疗机构能够在隐私保护的前提下，共同训练深度学习（DL）模型。然而，在小型机构中，标记数据的有限可用性和异构的数据分布（即非i.i.d.）可能限制FL的性能。虽然数据增强被证明是提高传统集中式DL泛化能力的有效技术，但其在FL中的应用尚未得到充分探索。值得注意的是，受到昂贵的标注成本的限制，3D医学分割通常依赖于数据增强。在本文中，我们旨在开发一种邻近特征层级数据增强（VFDA）方案，以有效减轻局部特征漂移，并促进隐私感知的FL分割的协同训练。我们考虑了内部和跨机构的差异，而无需跨机构转移原始数据或混合数据。

    Federated learning (FL) enables multiple client medical institutes collaboratively train a deep learning (DL) model with privacy protection. However, the performance of FL can be constrained by the limited availability of labeled data in small institutes and the heterogeneous (i.e., non-i.i.d.) data distribution across institutes. Though data augmentation has been a proven technique to boost the generalization capabilities of conventional centralized DL as a "free lunch", its application in FL is largely underexplored. Notably, constrained by costly labeling, 3D medical segmentation generally relies on data augmentation. In this work, we aim to develop a vicinal feature-level data augmentation (VFDA) scheme to efficiently alleviate the local feature shift and facilitate collaborative training for privacy-aware FL segmentation. We take both the inner- and inter-institute divergence into consideration, without the need for cross-institute transfer of raw data or their mixup. Specifically
    
[^76]: 学习具有高置信度保证的公平表示

    Learning Fair Representations with High-Confidence Guarantees. (arXiv:2310.15358v1 [cs.LG])

    [http://arxiv.org/abs/2310.15358](http://arxiv.org/abs/2310.15358)

    本文提出了一个具有高概率公平性保证的公平表示学习框架（FRG），通过用户定义的上界限制，在所有下游模型和任务中减少不公平性。实证评估结果证明了FRG的有效性。

    

    越来越多地使用表示学习生成跨多个下游任务具有预测性的表示。因此，开发能提供强有力公平保证的表示学习算法非常重要，因为它可以防止对弱势群体在所有下游预测任务中的不公平待遇。为了防止在所有下游任务中对弱势群体的不公平待遇，提供提供公平保证的表示学习算法至关重要。本文形式化定义了学习具有高置信度的公平表示的问题。然后，我们介绍了具有高置信度保证的公平表示学习（FRG）框架，该框架以用户定义的上界为限制，在所有下游模型和任务中降低不公平性，并证明FRG能以高概率保证所有下游模型和任务的公平性。最后，我们进行了实证评估，证明了FRG框架的有效性。

    Representation learning is increasingly employed to generate representations that are predictive across multiple downstream tasks. The development of representation learning algorithms that provide strong fairness guarantees is thus important because it can prevent unfairness towards disadvantaged groups for all downstream prediction tasks. To prevent unfairness towards disadvantaged groups in all downstream tasks, it is crucial to provide representation learning algorithms that provide fairness guarantees. In this paper, we formally define the problem of learning representations that are fair with high confidence. We then introduce the Fair Representation learning with high-confidence Guarantees (FRG) framework, which provides high-confidence guarantees for limiting unfairness across all downstream models and tasks, with user-defined upper bounds. After proving that FRG ensures fairness for all downstream models and tasks with high probability, we present empirical evaluations that de
    
[^77]: 在贝叶斯优化中的随机探索：最佳遗憾和计算效率优化

    Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency. (arXiv:2310.15351v1 [cs.LG])

    [http://arxiv.org/abs/2310.15351](http://arxiv.org/abs/2310.15351)

    本论文研究了在贝叶斯优化中使用随机探索的方法，并证明了其能够实现最佳的误差率和最优遗憾保证。同时，所提出的算法通过随机探索避免了每次迭代中非凸获取函数的昂贵优化，具有计算上的优势。

    

    我们考虑使用高斯过程模型的贝叶斯优化，也称为基于核的赌博优化。我们研究使用从分布中随机抽样来探索领域的方法。我们证明了这种随机探索方法能够实现最佳的误差率。我们的分析基于在本研究中建立的无限维希尔伯特空间中的新型集中边界，这可能具有独立的意义。我们进一步开发了一种基于随机探索和领域缩小的算法，并在无噪声和有噪声环境下建立其最佳遗憾保证。在无噪声环境中，我们的分析填补了在遗憾性能方面存在的差距，从而解决了COLT中的一个开放问题。由于随机探索消除了每次迭代中选择查询点的非凸获取函数的昂贵优化，所以所提出的算法也具有计算优势。

    We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance and thereby resolves a COLT open problem. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.
    
[^78]: 用隐式欧拉迁移学习的PINNs求解Burgers方程

    Burgers' pinns with implicit euler transfer learning. (arXiv:2310.15343v1 [cs.LG])

    [http://arxiv.org/abs/2310.15343](http://arxiv.org/abs/2310.15343)

    本研究将物理构建的神经网络（PINNs）与隐式欧拉迁移学习方法相结合，应用于求解Burgers方程。通过一系列人工神经网络，从前一个网络向下一个网络传递知识，并通过最小化基于隐式欧拉近似的损失函数，学习当前时间解。与通常的PINN模型相比，该方法具有更小的神经网络结构，同时具有类似准确结果和潜在优势。

    

    Burgers方程是计算建模中多种现象（如流体动力学、气体动力学、冲击理论、宇宙学等）的一个成熟的测试案例。本文将物理构建的神经网络（PINNs）与隐式欧拉迁移学习方法相结合，应用于求解Burgers方程。所提出的方法通过一系列人工神经网络（ANNs）寻求时间离散解。在每个时间步骤中，前一个ANN将其知识传递给下一个网络模型，通过最小化基于Burgers方程隐式欧拉近似的损失函数，学习当前时间解。该方法在两个基准问题上进行了测试：一个具有精确解，另一个具有替代解析解。与通常的PINN模型相比，所提出的方法具有更小的神经网络结构，同时具有类似准确结果和潜在的优势。

    The Burgers equation is a well-established test case in the computational modeling of several phenomena such as fluid dynamics, gas dynamics, shock theory, cosmology, and others. In this work, we present the application of Physics-Informed Neural Networks (PINNs) with an implicit Euler transfer learning approach to solve the Burgers equation. The proposed approach consists in seeking a time-discrete solution by a sequence of Artificial Neural Networks (ANNs). At each time step, the previous ANN transfers its knowledge to the next network model, which learns the current time solution by minimizing a loss function based on the implicit Euler approximation of the Burgers equation. The approach is tested for two benchmark problems: the first with an exact solution and the other with an alternative analytical solution. In comparison to the usual PINN models, the proposed approach has the advantage of requiring smaller neural network architectures with similar accurate results and potentiall
    
[^79]: 面向深度稀疏网络的混合粒度特征交互选择方法

    Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network. (arXiv:2310.15342v1 [cs.LG])

    [http://arxiv.org/abs/2310.15342](http://arxiv.org/abs/2310.15342)

    本论文提出了一种针对深度稀疏网络的混合粒度特征交互选择方法，能够同时考虑特征域和特征值，实验证明该方法在准确性和效率方面表现良好。

    

    深度稀疏网络被广泛研究作为具有高维稀疏特征的预测任务的神经网络架构，其中特征交互选择是一个关键组成部分。然而，先前的方法主要集中在如何在粗粒度空间中搜索特征交互，对于更细粒度的细节则关注较少。在这项工作中，我们引入了一种针对深度稀疏网络的混合粒度特征交互选择方法，旨在同时考虑特征域和特征值。为了探索这样广阔的空间，我们提出了一种即时计算的分解空间。然后，我们开发了一个名为OptFeature的选择算法，它可以有效地从特征域和特征值同时选择特征交互。在三个大型真实世界基准数据集的实验结果表明，OptFeature在准确性和效率方面表现良好。额外的研究支持了我们方法的可行性。

    Deep sparse networks are widely investigated as a neural network architecture for prediction tasks with high-dimensional sparse features, with which feature interaction selection is a critical component. While previous methods primarily focus on how to search feature interaction in a coarse-grained space, less attention has been given to a finer granularity. In this work, we introduce a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks. To explore such expansive space, we propose a decomposed space which is calculated on the fly. We then develop a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. Results from experiments on three large real-world benchmark datasets demonstrate that OptFeature performs well in terms of accuracy and efficiency. Additional studies support the feasibility of our method.
    
[^80]: ADMM训练算法用于残差网络：收敛性，复杂度和并行训练

    ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training. (arXiv:2310.15334v1 [cs.LG])

    [http://arxiv.org/abs/2310.15334](http://arxiv.org/abs/2310.15334)

    本论文设计了一系列序列和并行的ADMM算法解决残差网络训练问题，并通过理论分析证明了算法的收敛性和收敛速度，同时分析了并行实现的时间复杂性和内存消耗的优势。

    

    我们通过引入辅助变量，设计了一系列序列和并行的近端点（梯度）ADMM算法来解决完全连接的残差网络（FCResNets）训练问题。通过基于Kurdyka-Lojasiewicz（KL）属性分析框架的证明，我们证明了近端点版本的收敛性，并且可以确保在不同的Kurdyka-Lojasiewicz（KL）指数范围内，实现局部R-线性或亚线性收敛速度。此外，我们从理论上分析了并行实现在时间复杂性和（每个节点的）内存消耗方面的优势。据我们所知，这是第一个从理论上分析应用于FCResNets训练问题的ADMM算法的收敛性，收敛速度，时间复杂性和（每个节点的）内存需求的工作。我们报告了实验结果，展示了高速度，更好的性能，鲁棒性和潜力。

    We design a series of serial and parallel proximal point (gradient) ADMMs for the fully connected residual networks (FCResNets) training problem by introducing auxiliary variables. Convergence of the proximal point version is proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we can ensure a locally R-linear or sublinear convergence rate depending on the different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary auxiliary function is constructed to realize our goal. Moreover, the advantages of the parallel implementation in terms of lower time complexity and less (per-node) memory consumption are analyzed theoretically. To the best of our knowledge, this is the first work analyzing the convergence, convergence rate, time complexity and (per-node) runtime memory requirement of the ADMM applied in the FCResNets training problem theoretically. Experiments are reported to show the high speed, better performance, robustness and potential in the 
    
[^81]: 估计可信赖和安全的最佳治疗方案

    Estimating Trustworthy and Safe Optimal Treatment Regimes. (arXiv:2310.15333v1 [cs.LG])

    [http://arxiv.org/abs/2310.15333](http://arxiv.org/abs/2310.15333)

    这篇论文提出了一种安全和可解释的框架，通过匹配患者的医学和药物特性来识别最佳治疗方案。研究结果表明，个性化的治疗策略可以根据患者的病史和药物特征来制定，并发现减少药物剂量可以减轻病情而不会对治疗效果产生负面影响。

    

    最近的统计学和强化学习方法显著推动了患者护理策略的发展。然而，在高风险环境中，这些方法面临着很大的挑战，包括缺失数据、固有的随机性以及对解释性和患者安全性的重要要求。我们的工作运用了一种安全且可解释的框架来识别最佳治疗方案。这种方法涉及将具有相似医学和药物特征的患者进行匹配，从而通过插值构建最佳政策。我们进行了全面的模拟研究，以展示该框架即使在复杂环境中也能够识别最佳政策的能力。最终，我们将我们的方法应用于研究对重症患者进行癫痫治疗的方案。我们的发现强烈支持基于患者的医疗历史和药物特征的个性化治疗策略。值得注意的是，我们发现减少药物剂量可以减轻病情而不会对治疗的效果产生负面影响。

    Recent statistical and reinforcement learning methods have significantly advanced patient care strategies. However, these approaches face substantial challenges in high-stakes contexts, including missing data, inherent stochasticity, and the critical requirements for interpretability and patient safety. Our work operationalizes a safe and interpretable framework to identify optimal treatment regimes. This approach involves matching patients with similar medical and pharmacological characteristics, allowing us to construct an optimal policy via interpolation. We perform a comprehensive simulation study to demonstrate the framework's ability to identify optimal policies even in complex settings. Ultimately, we operationalize our approach to study regimes for treating seizures in critically ill patients. Our findings strongly support personalized treatment strategies based on a patient's medical history and pharmacological features. Notably, we identify that reducing medication doses for 
    
[^82]: 无监督联邦学习：具有对抗攻击鲁棒性的异构混合模型的联邦梯度EM算法

    Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks. (arXiv:2310.15330v1 [stat.ML])

    [http://arxiv.org/abs/2310.15330](http://arxiv.org/abs/2310.15330)

    本文介绍了一种针对带有异构混合比例的混合模型的无监督学习的新型联邦梯度EM算法，在适用于普通混合模型的全面有限样本理论基础上，对高斯混合模型（GMM）和混合回归（MoRs）进行了具体的估计误差分析。该算法具有适应未知任务相似性、抵抗对少部分数据源的对抗攻击、保护本地数据隐私以及计算和通信效率等关键优势。

    

    尽管有监督的联邦学习方法取得了显著的成功，但无监督的联邦学习领域相对较少探索。在本文中，我们介绍了一种针对带有异构混合比例的混合模型的无监督学习的新型联邦梯度EM算法。我们首先提出了适用于普通混合模型的全面有限样本理论，然后将这一通用理论应用于高斯混合模型（GMM）和混合回归（MoRs）以描述模型参数和混合比例的显式估计误差。我们提出的联邦梯度EM算法具有以下几个关键优势：适应未知任务相似性、对少部分数据源的对抗攻击具有弹性、保护本地数据隐私以及计算和通信效率。

    While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. In this paper, we introduce a novel federated gradient EM algorithm designed for the unsupervised learning of mixture models with heterogeneous mixture proportions across tasks. We begin with a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on Gaussian Mixture Models (GMMs) and Mixture of Regressions (MoRs) to characterize the explicit estimation error of model parameters and mixture proportions. Our proposed federated gradient EM algorithm demonstrates several key advantages: adaptability to unknown task similarity, resilience against adversarial attacks on a small fraction of data sources, protection of local data privacy, and computational and communication efficiency.
    
[^83]: 使用flwr-serverless的无服务器联邦学习

    Serverless Federated Learning with flwr-serverless. (arXiv:2310.15329v1 [cs.LG])

    [http://arxiv.org/abs/2310.15329](http://arxiv.org/abs/2310.15329)

    flwr-serverless是一种无服务器联邦学习的方法，能够有效训练来自不同来源的数据，同时不损害安全和隐私。

    

    随着数据收集和个人身份信息存储量的激增，联邦学习变得越来越重要和受欢迎。与此同时，世界各国提出了许多关于为个人数据提供更多保护以及对数据隐私措施的强化兴趣的建议。随着深度学习在新的和现有领域的重要性不断增加，开发像联邦学习这样能有效训练来自不同来源（如边缘设备）的数据、同时不损害安全和隐私的策略是至关重要的。最近，引入了名为Flower（Flwr）的Python包，为实现联邦学习提供了可扩展、灵活且易于使用的框架。然而，到目前为止，Flower只能运行同步联邦学习，这可能会导致运行成本高、耗时长，因为过程受限于慢速或不稳定的客户端训练任务。

    Federated learning is becoming increasingly relevant and popular as we witness a surge in data collection and storage of personally identifiable information. Alongside these developments there have been many proposals from governments around the world to provide more protections for individuals' data and a heightened interest in data privacy measures. As deep learning continues to become more relevant in new and existing domains, it is vital to develop strategies like federated learning that can effectively train data from different sources, such as edge devices, without compromising security and privacy. Recently, the Flower (\texttt{Flwr}) Python package was introduced to provide a scalable, flexible, and easy-to-use framework for implementing federated learning. However, to date, Flower is only able to run synchronous federated learning which can be costly and time-consuming to run because the process is bottlenecked by client-side training jobs that are slow or fragile. Here, we in
    
[^84]: DeepVox和SAVE-CT：一种无关对比度和剂量的三维深度学习方法，用于胸部主动脉分割和瘤样动脉扩张预测的计算机断层扫描

    DeepVox and SAVE-CT: a contrast- and dose-independent 3D deep learning approach for thoracic aorta segmentation and aneurysm prediction using computed tomography scans. (arXiv:2310.15328v1 [eess.IV])

    [http://arxiv.org/abs/2310.15328](http://arxiv.org/abs/2310.15328)

    通过使用DeepVox和SAVE-CT模型，该论文提出了一种无关对比度和剂量的三维深度学习方法，用于胸主动脉分割和瘤样动脉扩张预测的计算机断层扫描。新模型在测试中展现出较高的Dice系数，并且训练速度更快。

    

    胸主动脉扩张是一种致命的疾病，可能通过主动脉逐渐扩大而导致剥离或破裂。它通常是无症状的，筛查建议有限。金标准评估是通过计算机断层扫描血管造影（CTA）和放射科医师耗时的评估来完成的。其他适应症的扫描可以帮助进行筛查，但如果没有对比增强或低剂量协议进行获取，可能会使临床评估困难，同时增加放射科医师的扫描数量。在这项研究中，选择了587例独特的CT扫描，包括对照组和胸主动脉扩张患者，使用低剂量和标准剂量协议进行获取，有无对比增强。一种新的分割模型，DeepVox，在开发集和测试集中展示了0.932和0.897的Dice系数，与文献中报告的模型相比，训练速度更快。

    Thoracic aortic aneurysm (TAA) is a fatal disease which potentially leads to dissection or rupture through progressive enlargement of the aorta. It is usually asymptomatic and screening recommendation are limited. The gold-standard evaluation is performed by computed tomography angiography (CTA) and radiologists time-consuming assessment. Scans for other indications could help on this screening, however if acquired without contrast enhancement or with low dose protocol, it can make the clinical evaluation difficult, besides increasing the scans quantity for the radiologists. In this study, it was selected 587 unique CT scans including control and TAA patients, acquired with low and standard dose protocols, with or without contrast enhancement. A novel segmentation model, DeepVox, exhibited dice score coefficients of 0.932 and 0.897 for development and test sets, respectively, with faster training speed in comparison to models reported in the literature. The novel TAA classification mod
    
[^85]: 视觉问答任务中的LXMERT模型压缩

    LXMERT Model Compression for Visual Question Answering. (arXiv:2310.15325v1 [cs.CV])

    [http://arxiv.org/abs/2310.15325](http://arxiv.org/abs/2310.15325)

    本文通过组合大规模预训练模型的观察结果，并评估在视觉问答任务中对LXMERT进行微调时的可训练子网络，研究了LXMERT模型的压缩。实验结果表明，在仅损失3%的精度下，可以有效地通过剪枝方法将LXMERT模型大小减小40%-60%。

    

    大规模预训练模型如LXMERT在文本-图像对上学习跨模态表示变得流行。根据中彩票假说，自然语言处理和计算机视觉模型中包含可单独训练以达到完全性能的较小子网络。本文结合这些观察结果，评估在VQA任务上对LXMERT进行微调时是否存在这样的可训练子网络。此外，我们通过研究可以进行多少剪枝而不会造成显著的精度损失来进行模型大小成本收益分析。我们的实验结果表明，对LXMERT进行40%-60%的有效剪枝，仅会损失3%的精度。

    Large-scale pretrained models such as LXMERT are becoming popular for learning cross-modal representations on text-image pairs for vision-language tasks. According to the lottery ticket hypothesis, NLP and computer vision models contain smaller subnetworks capable of being trained in isolation to full performance. In this paper, we combine these observations to evaluate whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA task. In addition, we perform a model size cost-benefit analysis by investigating how much pruning can be done without significant loss in accuracy. Our experiment results demonstrate that LXMERT can be effectively pruned by 40%-60% in size with 3% loss in accuracy.
    
[^86]: 为基于指导性说明生成的幻觉检测问题

    Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])

    [http://arxiv.org/abs/2310.15319](http://arxiv.org/abs/2310.15319)

    该论文研究了生成指导人类在模拟环境中导航的说明的问题，通过预训练模型并使用对比损失进行微调，提出了一种检测幻觉参考的模型，该模型在性能上超过了几个基线模型。

    

    我们研究了在模拟的住宅环境中生成指导人类导航的说明的问题。目前模型存在的一个重要问题是幻觉：它们生成与人类跟随者在描述的路径上执行或遇到的行为或物体不一致的参考。我们开发了一个模型，通过采用在大型图像-文本对语料库上预训练的模型，并使用对比损失进行微调，检测这些幻觉参考。我们的最终模型胜过了几个基线模型，包括使用由说明生成模型估计的词概率以及基于LSTM和Transformer的监督模型。

    We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.
    
[^87]: HetGPT: 利用预训练异构图神经网络中的提示调整的能力

    HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])

    [http://arxiv.org/abs/2310.15318](http://arxiv.org/abs/2310.15318)

    HetGPT是一种预训练异构图神经网络的方法，通过利用提示调整来解决预训练与下游任务之间的不匹配问题。

    

    图表现为表示和分析Web中的复杂模式和丰富信息的自然选择，使得在线页面分类和社交推荐等应用成为可能。然而，当前的“预训练，微调”范式在图机器学习任务中广泛应用，特别是在有限标记节点的情况下，往往存在预训练目标任务与下游任务之间的不匹配问题。这种差距可能导致“负转移”问题，即预训练所获得的知识对下游任务的性能产生不利影响。自然语言处理领域中基于提示的学习的兴起表明了将“预训练，提示”范式应用于图形的潜力，作为一种替代方案。然而，现有的图形提示技术针对的是同质图，忽视了Web图的内在异构性。为了填补这一差距，我们提出了HetGPT，

    Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
    
[^88]: SAM-CLIP: 将视觉基础模型合并为语义和空间理解

    SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])

    [http://arxiv.org/abs/2310.15308](http://arxiv.org/abs/2310.15308)

    该论文提出了一种将视觉基础模型合并为一个统一模型的方法，通过集成多任务学习、持续学习技术和师生蒸馏，实现了显著较少的计算成本和较少的预训练数据需求。通过应用该方法于SAM和CLIP，得到了一个统一模型SAM-CLIP，将两者的优势融合在一起。

    

    公开可用的视觉基础模型（VFMs）的领域，如CLIP和Segment Anything Model（SAM），正在迅速扩大。VFMs具有源自它们的预训练目标的不同能力。例如，CLIP在语义理解方面表现出色，而SAM专注于分割的空间理解。在这项工作中，我们介绍了一种将VFMs高效合并为一个统一模型的简单方法，以吸收它们的专业知识。我们提出的方法集成了多任务学习、持续学习技术和师生蒸馏。与传统的从头开始进行多任务训练相比，这种策略具有显著较少的计算成本。此外，它只需要最初用于训练单个模型的预训练数据集的一小部分。通过将我们的方法应用于SAM和CLIP，我们得到了SAM-CLIP：将SAM和CLIP的优势融合为单一主干的统一模型，使其适用于...

    The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for ed
    
[^89]: ADMarker: 一种多模式联邦学习系统，用于监测阿尔茨海默病的数字生物标志物

    ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease. (arXiv:2310.15301v1 [cs.LG])

    [http://arxiv.org/abs/2310.15301](http://arxiv.org/abs/2310.15301)

    ADMarker是一个多模式联邦学习系统，用于在自然生活环境中监测阿尔茨海默病的数字生物标志物。它具有新颖的联邦学习架构，能够准确检测出数字生物标志物，并在临床试验中展示出高准确率和早期AD识别能力。

    

    阿尔茨海默病（AD）及相关痴呆症由于人口老龄化而成为全球日益严重的健康挑战。本文介绍了ADMarker，这是第一个将多模式传感器和新的联邦学习算法整合起来，以在自然生活环境中检测多维AD数字生物标志物的端到端系统。ADMarker具有一种新颖的三阶段多模式联邦学习架构，能够以保护隐私的方式准确检测数字生物标志物。我们的方法共同解决了数据标签有限、数据异质性和有限的计算资源等几个主要的现实世界挑战。我们建立了一个紧凑的多模式硬件系统，并在一个为期四周的临床试验中将其部署在91名老年参与者身上。结果表明，ADMarker能够准确检测出全面的数字生物标志物，准确率高达93.8％，并以平均88.9％的准确率识别出早期AD。ADMarker提供了一个新的平台，可以在监测AD患者时提供准确和高效的数据分析。

    Alzheimer's Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. ADMarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 91 elderly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 93.8% accuracy and identify early AD with an average of 88.9% accuracy. ADMarker offers a new platform that 
    
[^90]: 具有局部收敛输入的神经网络（NNLCI）用于具有非结构化网格的超音速流问题

    Neural Network with Local Converging Input (NNLCI) for Supersonic Flow Problems with Unstructured Grids. (arXiv:2310.15299v1 [math.NA])

    [http://arxiv.org/abs/2310.15299](http://arxiv.org/abs/2310.15299)

    本研究开发了一种具有局部收敛输入的神经网络（NNLCI），用于使用非结构化数据进行高保真预测。该方法大大降低了计算资源和训练时间，并在无粘超音速流问题中展示了有效性和多功能性。

    

    近年来，基于深度神经网络（DNN）的代理模型广泛应用于解决传统上通过数值模拟处理的偏微分方程。然而，这种代理模型着重于对训练数据集的全局插值，因此需要较大的网络结构。这个过程耗时且计算成本高，限制了其在复杂物理问题的高保真预测中的应用。本研究开发了一种具有局部收敛输入的神经网络（NNLCI），用于使用非结构化数据进行高保真预测。该框架利用局部依赖域和收敛的粗略解作为输入，大大降低了计算资源和训练时间。作为验证案例，NNLCI方法被应用于研究具有凸块的导流道中的无粘超音速流。考虑了不同的凸块几何形状和位置，用于评估其有效性和多功能性。

    In recent years, surrogate models based on deep neural networks (DNN) have been widely used to solve partial differential equations, which were traditionally handled by means of numerical simulations. This kind of surrogate models, however, focuses on global interpolation of the training dataset, and thus requires a large network structure. The process is both time consuming and computationally costly, thereby restricting their use for high-fidelity prediction of complex physical problems. In the present study, we develop a neural network with local converging input (NNLCI) for high-fidelity prediction using unstructured data. The framework utilizes the local domain of dependence with converging coarse solutions as input, which greatly reduces computational resource and training time. As a validation case, the NNLCI method is applied to study inviscid supersonic flows in channels with bumps. Different bump geometries and locations are considered to benchmark the effectiveness and versa
    
[^91]: 通过扩散模型快速可靠地生成电子健康记录时间序列

    Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])

    [http://arxiv.org/abs/2310.15290](http://arxiv.org/abs/2310.15290)

    本研究通过使用扩散模型提出了一种快速可靠生成EHR时间序列数据的新方法，该方法在数据效用方面明显优于现有方法，并且对训练工作的需求更少。同时，该方法还提供了多样化和真实的合成EHR数据，增强了下游医疗数据分析。

    

    电子健康记录（EHR）是丰富的患者级数据来源，包括实验室检验、药物和诊断，为医疗数据分析提供了宝贵资源。然而，对隐私的担忧常常限制了对EHR的访问，阻碍了下游分析。研究人员已经探索了各种方法来生成保护隐私的EHR数据。在本研究中，我们引入了一种使用去噪扩散概率模型（DDPM）生成多样化和真实的合成EHR时间序列数据的新方法。我们对六个数据集进行了实验证明，我们的方法在数据效用方面明显优于七种现有方法，并且需要更少的训练工作。我们的方法还通过提供多样化和真实的合成EHR数据来增强下游医疗数据分析。

    Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
    
[^92]: 强化学习中基于人类反馈的主动教师选择

    Active teacher selection for reinforcement learning from human feedback. (arXiv:2310.15288v1 [cs.AI])

    [http://arxiv.org/abs/2310.15288](http://arxiv.org/abs/2310.15288)

    本论文提出了一个用于强化学习中的主动教师选择模型以解决多教师的学习问题，研究表明该模型在论文推荐系统和COVID-19疫苗测试领域具有优越性能，并揭示了利用教师间差异来学习准确奖励模型的重要性。

    

    从人类反馈中进行强化学习（RLHF）使得机器学习系统能够从人类反馈中学习目标。这些系统的一个核心限制是它们假设所有反馈都来自一个单一的人类教师，尽管需要询问不同教师的意见。我们提出了"Hidden Utility Bandit"（HUB）框架来建模教师在理性、专业知识和成本方面的差异，从而形式化了从多个教师学习的问题。我们开发了多种解决算法，并将它们应用于两个现实世界的领域：论文推荐系统和COVID-19疫苗测试。我们发现，"Active Teacher Selection"（ATS）算法通过主动选择何时以及选择哪个教师来查询，优于基准算法。HUB框架和ATS算法展示了利用教师之间的差异来学习准确的奖励模型的重要性，为鲁棒奖励建模的主动教师选择的未来研究提供了基础。

    Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.
    
[^93]: 一种用于稀疏强化学习的双重稳健方法

    A Doubly Robust Approach to Sparse Reinforcement Learning. (arXiv:2310.15286v1 [stat.ML])

    [http://arxiv.org/abs/2310.15286](http://arxiv.org/abs/2310.15286)

    我们提出了一种新的遗憾最小化算法，用于稀疏强化学习问题，通过结合双重稳健方法和新颖的分析技术，克服了之前算法对稀疏参数和未知策略的限制。

    

    我们提出了一种新的遗憾最小化算法，用于状态-转移分布为观测特征的线性函数的周期性稀疏线性马尔可夫决策过程（SMDP）。之前已知的SMDP算法需要知道稀疏参数并能够访问未知策略的oracle。我们通过结合双重稳健方法允许使用\emph{所有}动作的特征向量和一种新颖的分析技术来克服这些限制。该算法的遗憾是$\tilde{O}(\sigma^{-1}_{\min} s_{\star} H \sqrt{N})$，其中$\sigma_{\min}$表示特征向量的平均Gram矩阵的最小特征值，$s_\star$是稀疏参数，$H$是一个周期的长度，$N$是回合数。我们提供了一个匹配上界的遗憾下界，适用于新识别出的SMDP子类，对数因子除外。

    We propose a new regret minimization algorithm for episodic sparse linear Markov decision process (SMDP) where the state-transition distribution is a linear function of observed features. The only previously known algorithm for SMDP requires the knowledge of the sparsity parameter and oracle access to an unknown policy. We overcome these limitations by combining the doubly robust method that allows one to use feature vectors of \emph{all} actions with a novel analysis technique that enables the algorithm to use data from all periods in all episodes. The regret of the proposed algorithm is $\tilde{O}(\sigma^{-1}_{\min} s_{\star} H \sqrt{N})$, where $\sigma_{\min}$ denotes the restrictive the minimum eigenvalue of the average Gram matrix of feature vectors, $s_\star$ is the sparsity parameter, $H$ is the length of an episode, and $N$ is the number of rounds. We provide a lower regret bound that matches the upper bound up to logarithmic factors on a newly identified subclass of SMDPs. Our
    
[^94]: UncertaintyPlayground: 一款快速简化的用于不确定性估计的Python库

    UncertaintyPlayground: A Fast and Simplified Python Library for Uncertainty Estimation. (arXiv:2310.15281v1 [stat.ML])

    [http://arxiv.org/abs/2310.15281](http://arxiv.org/abs/2310.15281)

    UncertaintyPlayground是一款基于PyTorch和GPyTorch的Python库，可以快速进行不确定性估计和可视化预测区间，并提供了多种速度优化技术。

    

    本文介绍了UncertaintyPlayground，这是一个基于PyTorch和GPyTorch构建的Python库，用于在监督学习任务中进行不确定性估计。该库通过稀疏和变分高斯过程回归（SVGPR）用于正态分布结果和混合密度网络（MDN）用于混合分布，提供了高斯和多模态结果分布的快速训练。除了使用不同超参数进行模型训练外，UncertaintyPlayground还可以可视化一个或多个实例的预测区间。由于使用了张量操作，该库可以在CPU和GPU上进行训练，并提供各种PyTorch特定的速度优化技术。该库包含每个模块的单元测试，并通过GitHub Workflows（在线集成）和Tox（本地集成）确保多平台持续集成。最后，代码使用了Google风格的文档字符串，并提供了使用MkDocs和MkDocStrings创建的文档网站。

    This paper introduces UncertaintyPlayground, a Python library built on PyTorch and GPyTorch for uncertainty estimation in supervised learning tasks. The library offers fast training for Gaussian and multi-modal outcome distributions through Sparse and Variational Gaussian Process Regressions (SVGPRs) for normally distributed outcomes and Mixed Density Networks (MDN) for mixed distributions. In addition to model training with various hyperparameters, UncertaintyPlayground can visualize the prediction intervals of one or more instances. Due to using tensor operations, the library can be trained both on CPU and GPU and offers various PyTorch-specific techniques for speed optimization. The library contains unit tests for each module and ensures multi-platform continuous integration with GitHub Workflows (online integration) and Tox (local integration). Finally, the code is documented with Google-style docstrings and offers a documentation website created with MkDocs and MkDocStrings.
    
[^95]: 三重单纯形矩阵完成用于费用预测

    Triple Simplex Matrix Completion for Expense Forecasting. (arXiv:2310.15275v1 [cs.LG])

    [http://arxiv.org/abs/2310.15275](http://arxiv.org/abs/2310.15275)

    本文提出了一种使用三重单纯形矩阵完成方法进行费用预测的模型。该模型通过学习项目与潜在空间中的费用模式相关性来预测费用，同时满足预算约束并保证预测结果的准确性。

    

    预测项目费用是企业避免预算超支和项目失败的关键步骤。传统上，这是由财务分析师或数据科学技术（如时间序列分析）完成的。然而，这些方法可能存在不确定性，并产生与计划预算不同的结果，特别是在项目开始时数据点有限的情况下。本文提出了一种受约束的非负矩阵完成模型，通过学习项目与潜在空间中某些费用模式的相关性，预测费用的可能性。该模型在因子矩阵和缺失条目上受到三个概率单纯形的约束。此外，预测的费用值保证满足预算约束，无需后处理。一个非精确的交替优化算法被开发用于解决相关优化问题，并证明收敛到一个稳定点。

    Forecasting project expenses is a crucial step for businesses to avoid budget overruns and project failures. Traditionally, this has been done by financial analysts or data science techniques such as time-series analysis. However, these approaches can be uncertain and produce results that differ from the planned budget, especially at the start of a project with limited data points. This paper proposes a constrained non-negative matrix completion model that predicts expenses by learning the likelihood of the project correlating with certain expense patterns in the latent space. The model is constrained on three probability simplexes, two of which are on the factor matrices and the third on the missing entries. Additionally, the predicted expense values are guaranteed to meet the budget constraint without the need of post-processing. An inexact alternating optimization algorithm is developed to solve the associated optimization problem and is proven to converge to a stationary point. Res
    
[^96]: 系统化的人工智能方法用于AGI：解决对齐、能源和AGI大挑战

    Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI Grand Challenges. (arXiv:2310.15274v1 [cs.AI])

    [http://arxiv.org/abs/2310.15274](http://arxiv.org/abs/2310.15274)

    本论文讨论了面临能源、对齐和从狭义人工智能到AGI的三大挑战的系统化人工智能方法。现有的人工智能方法在能源消耗、系统设计和对齐问题上存在不足，而系统设计在解决对齐、能源和AGI大挑战中是至关重要的。

    

    人工智能面临着三大挑战：能源壁垒、对齐问题和从狭义人工智能到AGI的飞跃。当代人工智能解决方案在模型训练和日常运行过程中消耗着不可持续的能源。更糟糕的是，自2020年以来，每个新的人工智能模型所需的计算量每两个月就翻倍，直接导致能源消耗的增加。从人工智能到AGI的飞跃需要多个功能子系统以平衡的方式运作，这需要一个系统架构。然而，当前的人工智能方法缺乏系统设计；即使系统特征在人脑中扮演着重要角色，从它处理信息的方式到它做出决策的方式。同样，当前的对齐和人工智能伦理方法在很大程度上忽视了系统设计，然而研究表明，大脑的系统架构在健康的道德决策中起着关键作用。在本文中，我们认为系统设计在解决对齐、能源和AGI大挑战中至关重要。

    AI faces a trifecta of grand challenges the Energy Wall, the Alignment Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume unsustainable amounts of energy during model training and daily operations.Making things worse, the amount of computation required to train each new AI model has been doubling every 2 months since 2020, directly translating to increases in energy consumption.The leap from AI to AGI requires multiple functional subsystems operating in a balanced manner, which requires a system architecture. However, the current approach to artificial intelligence lacks system design; even though system characteristics play a key role in the human brain from the way it processes information to how it makes decisions. Similarly, current alignment and AI ethics approaches largely ignore system design, yet studies show that the brains system architecture plays a critical role in healthy moral decisions.In this paper, we argue that system design is critically im
    
[^97]: GradSim: 基于梯度相似性的语言分组方法用于有效的多语言训练

    GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])

    [http://arxiv.org/abs/2310.15269](http://arxiv.org/abs/2310.15269)

    在本文中，我们提出了一种基于梯度相似性的语言分组方法，名为GradSim。它通过选择最合适的语言集合进行多语言训练，避免了负面干扰，并在多个基准数据集上取得了最先进的性能表现。此外，我们的分析还揭示了数据集的主题对模型的性能也起着重要作用。

    

    世界上大多数语言都对自然语言处理模型提出了低资源的挑战。通过多语言训练，可以在语言之间共享知识。然而，并不是所有语言都能互相积极地影响，如何选择最合适的语言集合进行多语言训练，并避免那些特征或数据分布不兼容的语言之间的负面干扰，这是一个开放的研究问题。在本文中，我们提出了一种基于梯度相似性的语言分组方法，称为GradSim。我们在三个不同的多语言基准数据集上的实验证明，相较于其他相似性度量，GradSim可以带来最大的性能提升，并与跨语言模型的性能更好地相关。结果是，我们在 AfriSenti 上建立了新的最先进模型，这是一个用于对低资源非洲语言进行情感分析的基准数据集。在我们的广泛分析中，我们进一步揭示了除语言特征外，数据集的主题也起着重要的作用。

    Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datase
    
[^98]: 用于发现切换脑状态的独热广义线性模型

    One-hot Generalized Linear Model for Switching Brain State Discovery. (arXiv:2310.15263v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.15263](http://arxiv.org/abs/2310.15263)

    本论文提出了一种先验信息驱动的状态切换广义线性模型，可以揭示动态变化的功能性相互作用，从而对潜在的解剖连通图有所启示。

    

    揭示有意义且可解释的神经相互作用对于理解神经回路至关重要。从神经信号推断出的神经相互作用主要反映功能性相互作用。在长时间的实验中，实验对象动物可能会经历由实验、刺激或行为状态定义的不同阶段，因此功能性相互作用可能会随时间而变化。为了建模动态变化的功能性相互作用，先前的工作采用隐藏马尔可夫模型的状态切换广义线性模型（即HMM-GLMs）。然而，我们认为这些模型在生物学可信度上存在一些问题，因为功能性相互作用是受到潜在解剖连通图的影响和限制的。在这里，我们提出了一种新颖的先验信息驱动的状态切换GLM。我们在每个状态下引入了高斯先验和独热先验。这些先验是可学习的。我们将展示学习到的先验应该捕捉到状态恒定的相互作用，从而揭示潜在的解剖连通图。

    Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional interactions. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional interactions can change over time. To model dynamically changing functional interactions, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional interactions are shaped and confined by the underlying anatomical connectome. Here, we propose a novel prior-informed state-switching GLM. We introduce both a Gaussian prior and a one-hot prior over the GLM in each state. The priors are learnable. We will show that the learned prior should capture the state-constant interaction, shedding light on the underlying anatomical connectome and rev
    
[^99]: 基于多模态特征的语音识别中的模态丢失问题研究

    Modality Dropout for Multimodal Device Directed Speech Detection using Verbal and Non-Verbal Features. (arXiv:2310.15261v1 [cs.SD])

    [http://arxiv.org/abs/2310.15261](http://arxiv.org/abs/2310.15261)

    本论文研究了在语音识别中处理模态丢失问题的方法，并通过将韵律特征与语言线索结合使用，提高了DDSD性能。

    

    设备导向的语音识别（DDSD）是一种将问题区分为针对语音助手的查询和旁白或背景语音的二元分类任务。目前的DDSD系统使用语言线索（如声学特征、文本和/或自动语音识别系统（ASR）特征）来将语音分类为设备导向或其他类型，并在实际情况中往往要处理其中一种或多种模态不可用的情况。本文研究了能够对缺失模态更加稳健的DDSD系统的融合方案。同时，我们研究了在DDSD中将非语言线索（具体是韵律特征）与语言线索结合使用的方法。我们通过非线性中间融合的方式，将韵律特征的分数和嵌入与对应的语言线索结合起来，发现韵律特征可以使DDSD性能在给定固定操作点上的误接受率（FA）提高多达8.5%。

    Device-directed speech detection (DDSD) is the binary classification task of distinguishing between queries directed at a voice assistant versus side conversation or background speech. State-of-the-art DDSD systems use verbal cues, e.g acoustic, text and/or automatic speech recognition system (ASR) features, to classify speech as device-directed or otherwise, and often have to contend with one or more of these modalities being unavailable when deployed in real-world settings. In this paper, we investigate fusion schemes for DDSD systems that can be made more robust to missing modalities. Concurrently, we study the use of non-verbal cues, specifically prosody features, in addition to verbal cues for DDSD. We present different approaches to combine scores and embeddings from prosody with the corresponding verbal cues, finding that prosody improves DDSD performance by upto 8.5% in terms of false acceptance rate (FA) at a given fixed operating point via non-linear intermediate fusion, whil
    
[^100]: 无参考领域自适应翻译带有问题特定奖励的噪声问题

    Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])

    [http://arxiv.org/abs/2310.15259](http://arxiv.org/abs/2310.15259)

    这项研究提出了一种解决噪声环境中问题翻译挑战的方法，通过只使用源语数据进行微调的训练，实现了翻译问题的充分性和流畅性的平衡。

    

    社区问答(CQA)平台是帮助组织内用户的有价值工具。然而，使它们对非英语用户可访问仍然是一个挑战。翻译问题可以拓宽社区的覆盖范围，使有类似问题的人能够受益，但在嘈杂的环境中使用神经机器翻译(NMT)进行问题翻译会面临更多挑战，因为这些问题的语法正确性没有受到监控。这些问题可能被非母语用户以陈述句的形式表达，具有不正确的主谓语序，甚至有时缺少问号。由于数据存在噪声，从这些数据中创建一个合成的平行语料库也是困难的。为了解决这个问题，我们提出了一种只使用源语数据进行微调的训练方法。我们的方法通过结合BERTScore和Masked Language Model (MLM) S的损失函数，平衡了充分性和流畅性。

    Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community's reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) S
    
[^101]: SimBIG: 基于场景的宇宙星系团集群模拟推断

    SimBIG: Field-level Simulation-Based Inference of Galaxy Clustering. (arXiv:2310.15256v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.15256](http://arxiv.org/abs/2310.15256)

    SimBIG是一个基于模拟的推断方法，通过对星系团集群的场景级分析，利用非线性和非高斯特性，获得了比传统方法更严格的关于宇宙参数和哈勃常量的约束。

    

    我们首次提出了通过对星系团集群的场景级分析进行宇宙参数的基于模拟的推断（SBI）方法。标准的星系团集群分析依赖于分析汇总统计量，如功率谱$P_\ell$，并基于摄动理论的解析模型。因此，它们没有充分利用星系分布的非线性和非高斯特性。为了解决这些局限性，我们使用SimBIG前向建模框架，利用归一化流进行SBI。我们使用卷积神经网络和随机权重平均化对BOSS CMASS星系样本的一个子集进行数据的大规模压缩。我们对$\Omega_m$取值约为0.267（上限+0.033，下限-0.029），$\sigma_8$取值约为0.762（上限+0.036，下限-0.035）进行推断。虽然我们对$\Omega_m$的约束与标准的$P_\ell$分析一致，但对$\sigma_8$的约束更为严格，约束范围为$2.65$倍。我们的分析还给出了哈勃常量的约束。

    We present the first simulation-based inference (SBI) of cosmological parameters from field-level analysis of galaxy clustering. Standard galaxy clustering analyses rely on analyzing summary statistics, such as the power spectrum, $P_\ell$, with analytic models based on perturbation theory. Consequently, they do not fully exploit the non-linear and non-Gaussian features of the galaxy distribution. To address these limitations, we use the {\sc SimBIG} forward modelling framework to perform SBI using normalizing flows. We apply SimBIG to a subset of the BOSS CMASS galaxy sample using a convolutional neural network with stochastic weight averaging to perform massive data compression of the galaxy field. We infer constraints on $\Omega_m = 0.267^{+0.033}_{-0.029}$ and $\sigma_8=0.762^{+0.036}_{-0.035}$. While our constraints on $\Omega_m$ are in-line with standard $P_\ell$ analyses, those on $\sigma_8$ are $2.65\times$ tighter. Our analysis also provides constraints on the Hubble constant 
    
[^102]: SyncFusion:多模态同步音视频霍利合成

    SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis. (arXiv:2310.15247v1 [cs.SD])

    [http://arxiv.org/abs/2310.15247](http://arxiv.org/abs/2310.15247)

    SyncFusion是一个多模态同步的音视频霍利合成系统，能够从视频中提取重复的动作起始点，并使用音频或文本嵌入来生成新的同步音效音轨，为声音设计师提供了完全的创作控制权并简化了流程。

    

    声音设计涉及到为电影、视频游戏和虚拟/增强现实等各种媒体选择、录制和编辑音效。在设计声音时，其中一个最耗时的步骤是将音频与视频同步。在某些情况下，可以利用视频拍摄的环境录音来帮助此过程。然而，在视频游戏和动画中，没有参考音频存在，需要从视频中手动注释事件的时间。我们提出了一个系统，用于从视频中提取重复动作的起始点，然后与音频或文本嵌入一起，用于条件化训练的扩散模型生成新的同步音效音轨。通过这种方式，我们将完全的创作控制权交给了声音设计师，同时消除了与视频同步的负担。此外，编辑起始轨道或更改条件嵌入所需的工作量要比编辑音频轨道本身要少得多，简化了流程。

    Sound design involves creatively selecting, recording, and editing sound effects for various media like cinema, video games, and virtual/augmented reality. One of the most time-consuming steps when designing sound is synchronizing audio with video. In some cases, environmental recordings from video shoots are available, which can aid in the process. However, in video games and animations, no reference audio exists, requiring manual annotation of event timings from the video. We propose a system to extract repetitive actions onsets from a video, which are then used - in conjunction with audio or textual embeddings - to condition a diffusion model trained to generate a new synchronized sound effects audio track. In this way, we leave complete creative control to the sound designer while removing the burden of synchronization with video. Furthermore, editing the onset track or changing the conditioning embedding requires much less effort than editing the audio track itself, simplifying th
    
[^103]: 使用星系目录进行场级别的基于模拟的推断：系统效应的影响

    Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects. (arXiv:2310.15234v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.15234](http://arxiv.org/abs/2310.15234)

    本文提出了一种使用星系目录进行场级别的基于模拟的推断方法，能够鲁棒地推断出宇宙学参数，解决了观测受到的系统效应的影响。

    

    最近的研究表明，一种从星系红移调查中限制宇宙学参数的有效方法是训练图神经网络进行场级别的无似然推断，而不对尺度进行剪切。具体而言，德桑蒂等人（2023年）开发了能够准确推断出通过仅包含星系位置和径向速度的目录来确定$\Omega_{\rm m}$值的模型，这些模型具有对天体物理和亚网格模型的不确定性具有鲁棒性。然而，观测受到许多效应的影响，包括1）掩蔽效应，2）特异速度和径向距离的不确定性，以及3）不同的星系选择。此外，观测只允许我们测量红移，纠缠了星系的径向位置和速度。在本文中，我们使用来自CAMELS项目中不同代码运行的最新水动力学模拟生成的星系目录来训练和测试我们的模型，这些模拟考虑了这些观测效应。

    It has been recently shown that a powerful way to constrain cosmological parameters from galaxy redshift surveys is to train graph neural networks to perform field-level likelihood-free inference without imposing cuts on scale. In particular, de Santi et al. (2023) developed models that could accurately infer the value of $\Omega_{\rm m}$ from catalogs that only contain the positions and radial velocities of galaxies that are robust to uncertainties in astrophysics and subgrid models. However, observations are affected by many effects, including 1) masking, 2) uncertainties in peculiar velocities and radial distances, and 3) different galaxy selections. Moreover, observations only allow us to measure redshift, intertwining galaxies' radial positions and velocities. In this paper we train and test our models on galaxy catalogs, created from thousands of state-of-the-art hydrodynamic simulations run with different codes from the CAMELS project, that incorporate these observational effect
    
[^104]: 一个新方法用于带有更高次谐波的引力波模板库：通过十倍减少匹配滤波成本

    A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])

    [http://arxiv.org/abs/2310.15233](http://arxiv.org/abs/2310.15233)

    该论文提出了一种新方法，通过在引力波模板库中包含高次谐波模式，利用引力波模式之间的自然关联，可以大幅度减少匹配滤波的成本，并提高搜索引力波事件的灵敏度。

    

    引力波事件的搜索使用信号模型或模板。目前在LIGO-Virgo-Kagra (LVK)数据中使用的模板仅模拟了信号的主导四极模式$(\ell,m)=(2,2)$，忽略了次要的高阶模式(HM)例如$(\ell,m)=(3,3)$，$(4,4)$，这些模式是由广义相对论预测的。因此，这些搜索可能会在参数空间的一些有趣区域，如高质量和非对称质量比的系统中失去对黑洞合并的灵敏度。我们开发了一种新策略，将HM包含在模板库中，利用这些模式之间的自然关联。我们使用了牛顿附加公式和机器学习工具来模拟与给定$(2,2)$波形相对应的自旋对齐的$(3,3)$，$(4,4)$波形。可以对每个模式的数据进行单独滤波，得到信噪比(SNR)的独立时间序列，然后可以以相对廉价的方式将其组合起来进行综合。

    Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
    
[^105]: 大型语言模型中的函数向量

    Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])

    [http://arxiv.org/abs/2310.15213](http://arxiv.org/abs/2310.15213)

    大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。

    

    我们报告了一个简单的神经机制，将输入-输出函数表示为自回归变换语言模型（LMs）中的向量。通过在各种上下文学习（ICL）任务上使用因果中介分析，我们发现少数注意力头传输了展示任务的紧凑表示，我们称之为函数向量（FV）。FV对上下文的变化具有鲁棒性，即它们在不类似于其收集时的ICL上下文的情况下触发对输入的任务执行，例如零样本和自然文本设置。我们在各种任务、模型和层上测试了FV，并在中层发现强大的因果效应。我们研究了FV的内部结构，并发现虽然它们通常包含编码函数的输出空间的信息，但仅此信息无法重构FV。最后，我们测试了FV中的语义向量组合，并发现在某种程度上存在组合的能力。

    We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
    
[^106]: 模拟对阿尔茨海默病药物重用的有效性进行路径重要性建模

    Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing. (arXiv:2310.15211v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.15211](http://arxiv.org/abs/2310.15211)

    该论文提出了一种基于网络的新方法（MPI）来有效进行阿尔茨海默病药物重用。该方法通过学习节点嵌入来优先考虑重要路径，从而更好地发现候选药物。

    

    最近，药物重用作为一种有效且资源高效的阿尔茨海默病药物发现范式已经崭露头角。在各种药物重用方法中，基于网络的方法显示出了有希望的结果，因为它们能够利用复杂网络，整合多种相互作用类型（如蛋白质相互作用），更有效地识别潜在药物。然而，现有的方法通常假设网络中相同长度的路径对于识别药物的治疗效果具有相等的重要性。其他领域发现，相同长度的路径并不一定具有相同的重要性。因此，依赖于这一假设可能对药物重用尝试产生不利影响。在这项工作中，我们提出了MPI（模拟路径重要性），这是一种新颖的基于网络的阿尔茨海默病药物重用方法。MPI的独特之处在于，通过学习节点嵌入来优先考虑重要路径，这可以有效捕捉网络的丰富结构信息。因此，利用学习的节点嵌入可以提高药物重用的效果。

    Recently, drug repurposing has emerged as an effective and resource-efficient paradigm for AD drug discovery. Among various methods for drug repurposing, network-based methods have shown promising results as they are capable of leveraging complex networks that integrate multiple interaction types, such as protein-protein interactions, to more effectively identify candidate drugs. However, existing approaches typically assume paths of the same length in the network have equal importance in identifying the therapeutic effect of drugs. Other domains have found that same length paths do not necessarily have the same importance. Thus, relying on this assumption may be deleterious to drug repurposing attempts. In this work, we propose MPI (Modeling Path Importance), a novel network-based method for AD drug repurposing. MPI is unique in that it prioritizes important paths via learned node embeddings, which can effectively capture a network's rich structural information. Thus, leveraging learn
    
[^107]: 基于分段线性回归和空洞因果卷积神经网络的中长期日电力消耗预测

    Mid-Long Term Daily Electricity Consumption Forecasting Based on Piecewise Linear Regression and Dilated Causal CNN. (arXiv:2310.15204v1 [cs.LG])

    [http://arxiv.org/abs/2310.15204](http://arxiv.org/abs/2310.15204)

    该研究提出了一种基于分段线性回归和空洞因果卷积神经网络的中长期日电力消耗预测方法，通过将电力消耗序列分解为趋势、季节性和残差三个部分，并采用两阶段预测方法，结合了过滤器和预测器的建模来提高预测准确性。实验证明该方法具有更高的准确性。

    

    日电力消耗预测是一个经典问题。现有的预测算法在特殊日期如节假日上容易准确性下降。本研究将日电力消耗序列分解为趋势、季节性和残差三个组成部分，并采用分段线性回归作为过滤器和空洞因果卷积神经网络作为预测器构建了一个两阶段预测方法。具体步骤包括在时间轴上设置断点，使用月份、工作日和节假日的独热编码信息拟合分段线性回归模型。针对春节的挑战性预测，引入了距离作为变量，采用三次多项式形式纳入模型。前一步得到的残差序列使用空洞因果卷积神经网络进行建模，最终的日电力消耗预测是两阶段预测的总和。实验结果证明，该方法实现了更高的准确性。

    Daily electricity consumption forecasting is a classical problem. Existing forecasting algorithms tend to have decreased accuracy on special dates like holidays. This study decomposes the daily electricity consumption series into three components: trend, seasonal, and residual, and constructs a two-stage prediction method using piecewise linear regression as a filter and Dilated Causal CNN as a predictor. The specific steps involve setting breakpoints on the time axis and fitting the piecewise linear regression model with one-hot encoded information such as month, weekday, and holidays. For the challenging prediction of the Spring Festival, distance is introduced as a variable using a third-degree polynomial form in the model. The residual sequence obtained in the previous step is modeled using Dilated Causal CNN, and the final prediction of daily electricity consumption is the sum of the two-stage predictions. Experimental results demonstrate that this method achieves higher accuracy 
    
[^108]: 基于Transformer的胶囊网络预测转录因子结合位点

    Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v1 [q-bio.GN])

    [http://arxiv.org/abs/2310.15202](http://arxiv.org/abs/2310.15202)

    本文提出了一种基于Transformer的胶囊网络DNABERT-Cap，用于预测转录因子结合位点。该模型在预测中利用了双向编码器、胶囊层、卷积和双向长短时记忆层的特征，并通过对这些特征的联合优化构建了转录因子结合位点的预测器。

    

    预测转录因子的结合位点对于理解它们如何调控基因表达以及如何通过治疗手段进行调节非常重要。尽管在过去几年里已经有相当多的工作针对这个问题进行了研究，但仍然有改进的空间。在这方面，本文提出了一种基于Transformer的胶囊网络DNABERT-Cap，用于利用ChIP-seq数据集挖掘预测转录因子结合位点。DNABERT-Cap是一个双向编码器，经过大量基因组DNA序列的预训练，并通过胶囊层进行最终预测。所提出的模型通过对包含双向编码器、胶囊层、卷积和双向长短时记忆层特征的联合优化，构建了转录因子结合位点的预测器。为了评估所提方法的效率，我们使用了五个细胞系的基准ChIP-seq数据集，包括A54。

    Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERT-Cap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A54
    
[^109]: 强结构编码是否能减少消息传递的重要性？

    Can strong structural encoding reduce the importance of Message Passing?. (arXiv:2310.15197v1 [cs.LG])

    [http://arxiv.org/abs/2310.15197](http://arxiv.org/abs/2310.15197)

    本研究探索了提供强结构编码时消息传递的贡献，通过引入特征和结构信息之间的张量乘积交互方式，比较了不同情景下的选择，甚至在容量受限的情况下完全移除了消息传递阶段。

    

    在图上操作的最常见的神经网络类别是消息传递神经网络(MPNNs)，其中节点的表示通过在1-hop邻域中聚合信息进行迭代更新。由于这种计算节点嵌入的范式可能会阻止模型学习粗糙的拓扑结构，因此初始特征通常会通过图的结构信息进行增强，通常以拉普拉斯特征向量或随机游走转移概率的形式提供。在这项工作中，我们探索了在提供强结构编码时消息传递的贡献。我们提出了一种新颖的建模特征和结构信息之间相互作用的方法，基于它们的张量乘积而不是标准的连接。我们比较了在常见情景和消息传递层容量严重降低甚至最终被完全移除的设置下选择的相互作用方式。我们res

    The most prevalent class of neural networks operating on graphs are message passing neural networks (MPNNs), in which the representation of a node is updated iteratively by aggregating information in the 1-hop neighborhood. Since this paradigm for computing node embeddings may prevent the model from learning coarse topological structures, the initial features are often augmented with structural information of the graph, typically in the form of Laplacian eigenvectors or Random Walk transition probabilities. In this work, we explore the contribution of message passing when strong structural encodings are provided. We introduce a novel way of modeling the interaction between feature and structural information based on their tensor product rather than the standard concatenation. The choice of interaction is compared in common scenarios and in settings where the capacity of the message-passing layer is severely reduced and ultimately the message-passing phase is removed altogether. Our res
    
[^110]: 高效元神经启发式算法用于多目标组合优化问题

    Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization. (arXiv:2310.15196v1 [cs.LG])

    [http://arxiv.org/abs/2310.15196](http://arxiv.org/abs/2310.15196)

    本研究提出了一种高效的元神经启发式算法（EMNH），通过训练一个元模型并进行微调，来解决多目标组合优化问题。实验结果表明，EMNH在学习效率和解决质量上取得了显著改进。

    

    最近，基于深度强化学习的神经启发式算法在解决多目标组合优化问题方面显示出了潜力。然而，它们仍然在学习效率和解决质量方面遇到困难。为了解决这个问题，我们提出了一种高效的元神经启发式算法（EMNH），其中首先训练一个元模型，然后通过几个步骤对应的单目标子问题来进行微调。具体而言，在训练过程中，利用（部分）架构共享的多任务模型实现元模型的并行学习，以加快训练速度；同时，设计了一种与权重向量相关的比例对称采样方法来稳定训练过程。在微调过程中，提出了一种高效的层次化方法来系统地处理所有的子问题。在多目标旅行商问题（MOTSP）、多目标容量车辆路径问题（MOVRPTW）和多目标背包问题（MOKP）上进行了实验，结果表明，与现有方法相比，EMNH在学习效率和解决质量方面取得了显著的改进。

    Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle ro
    
[^111]: 具有多样性增强的神经多目标组合优化

    Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement. (arXiv:2310.15195v1 [cs.LG])

    [http://arxiv.org/abs/2310.15195](http://arxiv.org/abs/2310.15195)

    提出了一种具有多样性增强的神经启发式方法（NHDE）来解决神经多目标组合优化（MOCO）问题，该方法通过引入指示器增强的深度强化学习方法和多重帕累托最有策略，能够产生更多且具有更高多样性的帕累托解。

    

    大多数现有的神经多目标组合优化（MOCO）问题的方法仅依赖于分解，这往往导致子问题的重复解决方案，从而得到有限的帕累托集。除了分解，我们提出了一种具有多样性增强的神经启发式方法（NHDE），从两个角度产生更多帕累托解。一方面，为了阻止不同子问题的重复解决方案，我们提出了一种指示器增强的深度强化学习方法来指导模型，并设计了一种异构图注意机制来捕捉实例图和帕累托前沿图之间的关系。另一方面，为了在每个子问题的邻域中挖掘更多的解决方案，我们提出了多重帕累托最有策略来采样和保留理想的解决方案。在经典的 MOCO 问题上的实验结果表明，我们的 NHDE 能够生成具有更高多样性的帕累托前沿，从而实现更优越的综合效果

    Most of existing neural methods for multi-objective combinatorial optimization (MOCO) problems solely rely on decomposition, which often leads to repetitive solutions for the respective subproblems, thus a limited Pareto set. Beyond decomposition, we propose a novel neural heuristic with diversity enhancement (NHDE) to produce more Pareto solutions from two perspectives. On the one hand, to hinder duplicated solutions for different subproblems, we propose an indicator-enhanced deep reinforcement learning method to guide the model, and design a heterogeneous graph attention mechanism to capture the relations between the instance graph and the Pareto front graph. On the other hand, to excavate more solutions in the neighborhood of each subproblem, we present a multiple Pareto optima strategy to sample and preserve desirable solutions. Experimental results on classic MOCO problems show that our NHDE is able to generate a Pareto front with higher diversity, thereby achieving superior overa
    
[^112]: 将深度学习和强化学习应用于边界控制问题的研究

    Application of deep and reinforcement learning to boundary control problems. (arXiv:2310.15191v1 [cs.LG])

    [http://arxiv.org/abs/2310.15191](http://arxiv.org/abs/2310.15191)

    本研究将深度学习和强化学习应用于解决边界控制问题，通过使用空间神经网络构建初始猜测以及利用策略梯度方法学习迭代优化算法的时空神经网络，通过使用生成的合成数据进行训练和验证，研究发现深度学习和强化学习方法在解决边界控制问题方面具有潜在优势。

    

    边界控制问题是许多科学领域中的非凸优化和控制问题，包括流体力学、结构工程和热传递优化。其目标是找到使符合控制方程要求的封闭域内的状态值达到期望值的最优边界值。传统上，使用非线性优化方法，如内点法（IPM）来解决这类问题。本研究探讨了使用深度学习和强化学习解决边界控制问题的可能性。我们遵循迭代优化策略的框架，利用空间神经网络构建有启发性的初始猜测，并使用策略梯度的方法来学习迭代优化算法的时空神经网络。使用从文献中提出的问题生成的合成数据进行训练、测试和验证。数值实验表明，深度学习和强化学习方法在解决边界控制问题方面具有潜在优势。

    The boundary control problem is a non-convex optimization and control problem in many scientific domains, including fluid mechanics, structural engineering, and heat transfer optimization. The aim is to find the optimal values for the domain boundaries such that the enclosed domain adhering to the governing equations attains the desired state values. Traditionally, non-linear optimization methods, such as the Interior-Point method (IPM), are used to solve such problems.  This project explores the possibilities of using deep learning and reinforcement learning to solve boundary control problems. We adhere to the framework of iterative optimization strategies, employing a spatial neural network to construct well-informed initial guesses, and a spatio-temporal neural network learns the iterative optimization algorithm using policy gradients. Synthetic data, generated from the problems formulated in the literature, is used for training, testing and validation. The numerical experiments ind
    
[^113]: 朝着主体无关的情感识别：基于脑电信号的研究

    Towards Subject Agnostic Affective Emotion Recognition. (arXiv:2310.15189v1 [cs.LG])

    [http://arxiv.org/abs/2310.15189](http://arxiv.org/abs/2310.15189)

    本文提出了一种基于元学习的增强领域适应框架，用于处理主体无关的情感脑机接口。该方法有效解决了脑电信号中的分布偏移问题，具有较好的结果。

    

    本文关注情感识别，旨在基于脑电信号在主体无关的情境下进行。然而，在主体无关情感脑机接口中，脑电信号表现出主体不稳定性，导致分布偏移的问题。此外，通过领域泛化和领域适应等方法可以缓解这个问题。通常，基于领域适应的方法相对于领域泛化的方法具有更好的结果，但在处理新主体时需要更多的计算资源。我们提出了一种新颖的框架，基于元学习的增强领域适应，适用于主体无关的情感脑机接口。我们的领域适应方法通过元学习进行增强，其中包括循环神经网络、分类器和基于可分解函数的分布偏移控制器。此外，我们还展示了一个解释可分解函数的神经网络如何有效地估计距离。

    This paper focuses on affective emotion recognition, aiming to perform in the subject-agnostic paradigm based on EEG signals. However, EEG signals manifest subject instability in subject-agnostic affective Brain-computer interfaces (aBCIs), which led to the problem of distributional shift. Furthermore, this problem is alleviated by approaches such as domain generalisation and domain adaptation. Typically, methods based on domain adaptation confer comparatively better results than the domain generalisation methods but demand more computational resources given new subjects. We propose a novel framework, meta-learning based augmented domain adaptation for subject-agnostic aBCIs. Our domain adaptation approach is augmented through meta-learning, which consists of a recurrent neural network, a classifier, and a distributional shift controller based on a sum-decomposable function. Also, we present that a neural network explicating a sum-decomposable function can effectively estimate the dive
    
[^114]: 深度学习方法在粘弹纤维复合材料的动态力学分析中的应用

    Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic Fiber Composites. (arXiv:2310.15188v1 [cs.LG])

    [http://arxiv.org/abs/2310.15188](http://arxiv.org/abs/2310.15188)

    本文利用深度学习方法，将微结构与力学性能建立映射关系，加快了设计过程，并实现了根据期望性能生成微结构的目标。

    

    受到生态设计标准的推动，增强型聚合物（RP）复合材料的应用越来越广泛，这要求在轻量化、刚性和有效的振动控制之间取得良好的平衡。这些材料对于提高舒适性、安全性和能源效率至关重要。动态力学分析（DMA）可表征粘弹性行为，然而，使用机器学习（ML）加速微结构设计和理解引起了越来越多的兴趣。本文旨在利用深度神经网络将微结构映射到其力学性能上，加快过程并实现从期望性能生成微结构的目标。

    The increased adoption of reinforced polymer (RP) composite materials, driven by eco-design standards, calls for a fine balance between lightness, stiffness, and effective vibration control. These materials are integral to enhancing comfort, safety, and energy efficiency. Dynamic Mechanical Analysis (DMA) characterizes viscoelastic behavior, yet there's a growing interest in using Machine Learning (ML) to expedite the design and understanding of microstructures. In this paper we aim to map microstructures to their mechanical properties using deep neural networks, speeding up the process and allowing for the generation of microstructures from desired properties.
    
[^115]: 减少海平面上升预测的不确定性：一种空间可变性感知的方法

    Reducing Uncertainty in Sea-level Rise Prediction: A Spatial-variability-aware Approach. (arXiv:2310.15179v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.15179](http://arxiv.org/abs/2310.15179)

    这篇论文提出了一种基于区域回归模型的方法，通过解决空间可变性和模型相互依赖的问题，准确可靠地预测未来海平面上升，并降低不确定性。

    

    鉴于多模型集合气候预测，目标是准确可靠地预测未来海平面上升，同时降低不确定性。由于气候变化对极地冰盖和海洋的影响，海平面上升影响着沿海社区和其他地区的数百万人口，因此这个问题非常重要。由于空间变异性和不确定性，如可能的临界点（例如，格陵兰或西部南极洲冰架的崩塌）、气候反馈循环（例如，云、永久冻土融化）、未来政策决策和人类行为，这个问题具有挑战性。大多数现有气候建模方法在回归或深度学习中全局使用相同的权重来组合不同的气候预测，这种方法对于准确可靠的海平面上升预测需要不同的地区采用不同的加权方案是不够的。本文提出了一种区域回归模型，解决了空间可变性和模型相互依赖的问题。

    Given multi-model ensemble climate projections, the goal is to accurately and reliably predict future sea-level rise while lowering the uncertainty. This problem is important because sea-level rise affects millions of people in coastal communities and beyond due to climate change's impacts on polar ice sheets and the ocean. This problem is challenging due to spatial variability and unknowns such as possible tipping points (e.g., collapse of Greenland or West Antarctic ice-shelf), climate feedback loops (e.g., clouds, permafrost thawing), future policy decisions, and human actions. Most existing climate modeling approaches use the same set of weights globally, during either regression or deep learning to combine different climate projections. Such approaches are inadequate when different regions require different weighting schemes for accurate and reliable sea-level rise predictions. This paper proposes a zonal regression model which addresses spatial variability and model inter-depende
    
[^116]: 这里是翻译过的论文标题

    Ghost on the Shell: An Expressive Representation of General 3D Shapes. (arXiv:2310.15168v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.15168](http://arxiv.org/abs/2310.15168)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they 1) enable fast physics-based rendering with realistic material and lighting, 2) support physical simulation, and 3) are memory-efficient for modern graphics pipelines. Recent work on reconstructing and statistically modeling 3D shape, however, has critiqued meshes as being topologically inflexible. To capture a wide range of object shapes, any 3D representation must be able to model solid, watertight, shapes as well as thin, open, surfaces. Recent work has focused on the former, and methods for reconstructing open surfaces do not support fast reconstruction with material and lighting or unconditional generative modelling. Inspired by the observation that open surfaces can be seen as islands floating on watertight surfaces, we parameterize open surfaces by defining a manifold signed distance field on watertight templat
    
[^117]: 神经网络中的元-（超文本外语境）学习

    Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15047](http://arxiv.org/abs/2310.15047)

    该研究通过合成实验展示了一种称为元-超文本外语境学习（meta-OCL）的现象在神经网络中的存在。这种学习使神经网络能够更好地吸收广泛适用的语义内容，并在适当的情况下进行使用。研究者提出了关于元-超文本外语境学习产生的两种假设，并就未来AI系统的能力和潜在风险进行了讨论。

    

    Brown等人（2020）通过对大型语言模型（LLMs）进行精心设计的合成实验，建立了一种称为元-超文本外语境学习（meta-OCL）的现象的存在。我们的结果表明，元-超文本外语境学习使LLMs更容易“内化”文本的语义内容，该文本广泛适用（例如真实陈述或权威来源的文本），并在适当的情况下使用它。我们进一步在合成计算机视觉环境中展示了元-超文本外语境学习，并提出了两种假设，解释了元-超文本外语境学习的出现：一种是依赖于模型在其参数中存储知识的方式，另一种是暗示梯度下降优化器的隐含梯度对齐偏差可能负责。最后，我们对我们的结果可能意味着未来AI系统的能力进行了思考，并讨论了潜在的风险。我们的代码可以在https://github.com/krasheni找到。

    Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
    
[^118]: 语言模型在不平衡文本分类中的元学习: 挑战与机遇

    Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15019](http://arxiv.org/abs/2310.15019)

    本文提出了一种元学习技术(MLT)，通过将不同文本表示构建的个体模型进行组合，在不平衡的文本分类中提高了性能，并通过阈值移动技术进一步改善了预测器的性能。

    

    检测违规言论内容是重要但困难的。虽然机器学习是应对这一挑战性任务的强大工具，但由于训练数据的数量和质量限制以及违规定义和数据标注的不一致性等因素，难以突破性能瓶颈。为了充分发挥有限资源的潜力，我们提出了一种元学习技术(MLT)，它将使用不同文本表示构建的个体模型进行组合。我们通过分析证明，所得到的技术在数值上是稳定的，并产生合理的组合权重。我们将MLT与阈值移动(TM)技术相结合，进一步提高组合预测器在高度不平衡的分布和超出分布数据集上的性能。我们还提供了计算结果，展示了所提出的MLT方法的统计优势。所有作者对这项工作贡献相同。

    Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.
    
[^119]: 物理信息图卷积网络：面向复杂几何的广义框架

    Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries. (arXiv:2310.14948v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14948](http://arxiv.org/abs/2310.14948)

    本研究提出了物理信息图卷积网络作为解决复杂几何体中偏微分方程问题的广义框架，并结合经典数值求解器从而解决了物理信息框架在复杂几何体上的问题。

    

    自从[9]的开创性工作以及他们的物理信息神经网络（PINNs）之后，已经有很多工作致力于使用深度学习模型来解决偏微分方程（PDEs）。然而，仍然存在一些挑战，例如将这些模型扩展到复杂的三维几何体，以及如何将这些方法与经典的数值求解器结合起来进行研究。在本工作中，我们基于图神经网络和传统数值技术中用于求解偏微分方程的网格之间的相似性，证明了将图神经网络用于这些问题的合理性。在证明了物理信息框架在复杂几何体上计算PDE残差时存在问题后，我们提出了一种替代方案，将经典的数值求解器与物理信息框架相结合。最后，我们提出了一种实现该方法的方案，并在一个不规则几何体上的三维问题上进行了测试。

    Since the seminal work of [9] and their Physics-Informed neural networks (PINNs), many efforts have been conducted towards solving partial differential equations (PDEs) with Deep Learning models. However, some challenges remain, for instance the extension of such models to complex three-dimensional geometries, and a study on how such approaches could be combined to classical numerical solvers. In this work, we justify the use of graph neural networks for these problems, based on the similarity between these architectures and the meshes used in traditional numerical techniques for solving partial differential equations. After proving an issue with the Physics-Informed framework for complex geometries, during the computation of PDE residuals, an alternative procedure is proposed, by combining classical numerical solvers and the Physics-Informed framework. Finally, we propose an implementation of this approach, that we test on a three-dimensional problem on an irregular geometry.
    
[^120]: 重新审视隐式微分以解决最优控制中的学习问题

    Revisiting Implicit Differentiation for Learning Problems in Optimal Control. (arXiv:2310.14468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14468](http://arxiv.org/abs/2310.14468)

    本文提出了一种新方法，通过隐函数定理对非凸、受约束的最优控制问题中的最优轨迹进行微分。与先前的方法相比，该方法可以线性缩放地计算轨迹导数，可以容易地进行并行化，并且在模型规模、数值稳定性和计算效率方面表现出色。

    

    本文提出了一种新方法，通过隐函数定理，对非凸、受约束的离散时间最优控制（COC）问题中产生的最优轨迹进行微分。先前的工作通过解决一种微分卡鲁什-库恩-塔克（KKT）系统来求解轨迹导数，并通过解决一个辅助线性二次调节器（LQR）问题来实现高效计算。相比之下，我们直接评估由在（微分）KKT系统中消除拉格朗日乘子项产生的矩阵方程。通过适当考虑结果方程中的项的结构，我们表明轨迹导数与时间步数呈线性缩放关系。此外，我们的方法允许轻松并行化，与模型大小显著改善可扩展性，直接计算向量 - 雅可比乘积，并且与以前的工作相比具有改进的数值稳定性。作为额外的贡献，我们统一了...

    This paper proposes a new method for differentiating through optimal trajectories arising from non-convex, constrained discrete-time optimal control (COC) problems using the implicit function theorem (IFT). Previous works solve a differential Karush-Kuhn-Tucker (KKT) system for the trajectory derivative, and achieve this efficiently by solving an auxiliary Linear Quadratic Regulator (LQR) problem. In contrast, we directly evaluate the matrix equations which arise from applying variable elimination on the Lagrange multiplier terms in the (differential) KKT system. By appropriately accounting for the structure of the terms within the resulting equations, we show that the trajectory derivatives scale linearly with the number of timesteps. Furthermore, our approach allows for easy parallelization, significantly improved scalability with model size, direct computation of vector-Jacobian products and improved numerical stability compared to prior works. As an additional contribution, we unif
    
[^121]: 随机前向模式自动微分优化算法

    Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2310.14168](http://arxiv.org/abs/2310.14168)

    该论文介绍了一种随机前向模式自动微分优化算法，通过在神经网络的正向传递中计算损失函数的方向导数来更新参数。算法通过采样不同概率分布的随机方向，使用正向模式自动微分计算雅可比向量乘积，并提供了对其收敛速度和计算复杂性的严格分析。

    

    神经网络中的反向传播利用了自动微分的基本要素，即反向模式微分，或称为向量雅可比乘积(VJP)，或在微分几何的背景下被称为拉回过程。梯度的计算对于使用梯度下降方法更新神经网络参数非常重要。在本研究中，我们提出了一种通用的随机方法，通过使用通过正向模式AD或雅可比向量乘积(JVP)高效计算的损失函数的方向导数来更新神经网络的参数。这些JVP沿着从不同概率分布（例如伯努利、正态、维格纳、拉普拉斯和均匀分布）采样的随机方向计算。梯度的计算在神经网络的正向传递过程中进行。我们还提供了对所提出方法的严格分析，包括收敛速度以及计算复杂性。

    Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
    
[^122]: 在具有梯度反馈的强单调和指数凸博弈中的自适应、双重最优无悔学习

    Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2310.14085](http://arxiv.org/abs/2310.14085)

    本文提出了一个自适应的OGD算法\textsf{AdaOGD}，在强凸性下实现了$ O(\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。

    

    在强凸性或单调性假设下，网上梯度下降（OGD）被广泛认为是双重最优的：（1）在单个代理设置中，对于强凸成本函数，它实现了$ \Theta(\log T) $的最优后悔；（2）在具有强单调性的多代理博弈的情况下，每个代理使用OGD，我们获得了关于联合行动的最后一次收敛到唯一纳什均衡的最优速率$ \Theta(\frac{1}{T}) $。尽管这些有限时间的保证突出了其优点，但OGD的缺点是需要知道强凸性/单调性的参数。在本文中，我们设计了一个完全自适应的OGD算法\textsf{AdaOGD}，它不需要先验的知识这些参数。在单个代理设置中，我们的算法在强凸性下实现了$ O(\log^2(T)) $的后悔，这是最优的除了一个对数因子。此外，如果在强单调博弈中每个代理都使用\textsf{AdaOGD}，则联合行动收敛到最后一个迭代时的一次。

    Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s
    
[^123]: 全面对比：面向医疗时间序列的层次对比框架

    Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14017](http://arxiv.org/abs/2310.14017)

    本论文提出了一个名为COMET的创新层次对比框架，用于医疗时间序列分析。该框架通过在多个层级上开发对比损失，可以充分利用医疗时间序列的复杂特性，并实现自监督学习。使用多个数据集进行实验验证了COMET的有效性。

    

    在医疗时间序列分析中，对比表示学习至关重要，因为它减少了对劳动密集、领域特定和稀缺的专家注释的依赖。然而，现有的对比学习方法主要关注单一数据层面，未能充分利用医疗时间序列的复杂特性。为了解决这个问题，我们提出了COMET，一个创新的层次框架，它利用医疗时间序列中所有内在层级的数据一致性。我们精心设计的模型系统地捕捉了来自四个潜在层级的数据一致性：观测、样本、试验和患者层级。通过在多个层级上开发对比损失，我们可以学习到保持全面数据一致性的有效表示，实现自监督方法中的信息最大利用。我们在具有挑战性的独立患者设置下进行实验。我们使用三个不同的数据集将COMET与六个基准方法进行比较。

    Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include 
    
[^124]: 对比偏好学习：学习用户反馈而无需RL的方法

    Contrastive Preference Learning: Learning from Human Feedback without RL. (arXiv:2310.13639v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13639](http://arxiv.org/abs/2310.13639)

    对比偏好学习方法解决了传统强化学习从人类反馈中学习奖励函数的假设错误以及优化挑战的问题。

    

    "从人类反馈中进行强化学习（RLHF）已经成为一种与人类意图对齐的流行范式。通常的RLHF算法分为两个阶段：首先，利用人类偏好学习奖励函数，然后通过强化学习（RL）优化学习到的奖励函数以对齐模型。这种范式假设人类偏好是根据奖励分布的，但最近的研究表明，实际上它们遵循用户最佳策略下的遗憾。因此，从反馈中学习奖励函数不仅基于人类偏好的错误假设，还导致了由于策略梯度或RL阶段的自助法引起的棘手的优化挑战。由于这些优化挑战，当代的RLHF方法限制自己只能应用于上下文强化学习（如大型语言模型）或限制了观测维度（如基于状态的机器人）。我们通过引入一种新的方法来克服这些限制。"

    Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new famil
    
[^125]: 探索语言模型中谄媚行为的理解

    Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])

    [http://arxiv.org/abs/2310.13548](http://arxiv.org/abs/2310.13548)

    这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。

    

    「从人类反馈中进行强化学习（RLHF）」是训练高质量AI助手的一种流行技术。然而，RLHF可能会鼓励模型通过与用户信念相符的回答来代替真实回答，这种行为被称为谄媚行为。我们研究了RLHF训练模型中谄媚行为的普遍性以及人类偏好判断是否起到了作用。首先，我们证明了五个最先进的AI助手在四个不同的自由文本生成任务中一贯表现出谄媚行为。为了理解人类偏好是否驱动了RLHF模型的这种广泛行为，我们分析了现有的人类偏好数据。我们发现，当回答与用户的观点相符时，它更有可能被选中。此外，人类和偏好模型（PMs）将有说服力的谄媚回答与正确回答相比，有时几乎可以忽略不计地选择了谄媚回答。优化模型输出以满足PMs有时也会在真实性和谄媚行为之间做出取舍。

    Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
    
[^126]: 基于深度强化学习的优化CO2排放的智能交通信号控制

    Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])

    [http://arxiv.org/abs/2310.13129](http://arxiv.org/abs/2310.13129)

    本研究提出了一种基于深度强化学习的智能交通信号控制算法，通过优化CO2排放和行驶时间等指标，实现了较好的性能。

    

    如今，交通网络面临着次优控制策略的挑战，这可能对人类健康、环境产生不利影响，并 contribute to traffic congestion. 由于交通拥堵导致的空气污染水平上升和通勤时间延长，交叉路口交通信号控制器成为现代交通基础设施的关键组成部分。尽管文献中有几个自适应交通信号控制器，但对其比较性能的研究有限。此外，尽管二氧化碳（CO2）排放在全球范围内非常重要，但文献对该领域关注不够。在本报告中，我们提出了EcoLight，一种针对强化学习算法的奖励塑造方案，不仅能减少CO2排放，还能在诸如行驶时间等指标上取得有竞争力的结果。我们使用行驶时间、CO2排放、等指标比较了表格式Q-Learning、DQN、SARSA和A2C算法的性能。

    Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
    
[^127]: 使用机器学习和业务数据预测客运渡轮燃料消费：一项比较研究

    Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study. (arXiv:2310.13123v1 [cs.LG])

    [http://arxiv.org/abs/2310.13123](http://arxiv.org/abs/2310.13123)

    本文研究了使用机器学习和业务数据预测客运渡轮燃料消费的方法，并通过选择适当的输入变量来提高预测性能和实际可应用性。

    

    随着环保交通的重要性增加，为海运船只提供高效的操作方法至关重要。考虑到天气条件和使用船只业务数据进行预测的状态监控方法需要准确完整的模型来预测船只的energy efficiency。这些模型需要能够实时有效地处理所有操作数据。本文提出了使用从客运船只收集的业务数据来预测燃料消耗的模型。统计和领域知识方法被用来选择模型的适当输入变量。这些方法能够防止过拟合、缺失数据和多重共线性，并提供实际可应用性。研究中调查的预测模型包括多元线性回归（MLR）、决策树方法（DT）、人工神经网络（ANN）和集成方法。最好的预测性能来自于使用ensemble方法开发的模型

    As the importance of eco-friendly transportation increases, providing an efficient approach for marine vessel operation is essential. Methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. The models need to effectively process all the operational data in real-time. This paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. Statistical and domain-knowledge methods were used to select the proper input variables for the models. These methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. Prediction models that were investigated include multiple linear regression (MLR), decision tree approach (DT), an artificial neural network (ANN), and ensemble methods. The best predictive performance was from a model developed using t
    
[^128]: 理解Transformer中的加法

    Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])

    [http://arxiv.org/abs/2310.13121](http://arxiv.org/abs/2310.13121)

    本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。

    

    了解像Transformer这样的机器学习模型的内部工作方式对于其安全和道德使用至关重要。本文对经过训练进行整数加法的单层Transformer模型进行了深入分析。我们揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法。我们的研究还发现该模型开始计算较晚，但执行速度非常快。我们还发现了一种罕见的高损失的使用情况，并予以解释。总体而言，我们详细解释了该模型的算法。这些发现通过严格测试和数学建模得到了验证，对于机制可解释性、人工智能安全性和对齐性等广泛研究做出了贡献。我们的方法为分析更复杂的任务和多层Transformer模型打开了大门。

    Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
    
[^129]: 《科学和机器学习的Vendi分数的近亲：基于相似性的多样性度量族群》

    Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning. (arXiv:2310.12952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.12952](http://arxiv.org/abs/2310.12952)

    该论文提出了一种基于相似性的多样性度量方法——Vendi分数族群，为科学和机器学习等领域准确测量多样性提供了灵活性。

    

    准确测量多样性对于许多科学领域至关重要，包括机器学习、生态学和化学。 Vendi分数是一种扩展了q=1阶Hill数的通用基于相似性的多样性度量方法，借鉴量子统计力学的思想。与生态学中的许多多样性度量方法不同，Vendi分数考虑了相似性，并且不需要了解集合中各个类别的普遍性来评估多样性。然而，Vendi分数对于给定集合中的每个项目都以与该项目的普遍性成比例的敏感度进行处理。这在项目普遍性存在显著不平衡的情况下是不可取的。在本文中，我们利用相似性扩展了其他Hill数，以提供对稀有或常见项目分配敏感度的灵活性。这导致了一族多样性度量方法——具有不同敏感度水平的Vendi分数，可应用于各种场景中。

    Measuring diversity accurately is important for many scientific fields, including machine learning (ML), ecology, and chemistry. The Vendi Score was introduced as a generic similarity-based diversity metric that extends the Hill number of order q=1 by leveraging ideas from quantum statistical mechanics. Contrary to many diversity metrics in ecology, the Vendi Score accounts for similarity and does not require knowledge of the prevalence of the categories in the collection to be evaluated for diversity. However, the Vendi Score treats each item in a given collection with a level of sensitivity proportional to the item's prevalence. This is undesirable in settings where there is a significant imbalance in item prevalence. In this paper, we extend the other Hill numbers using similarity to provide flexibility in allocating sensitivity to rare or common items. This leads to a family of diversity metrics -- Vendi scores with different levels of sensitivity -- that can be used in a variety o
    
[^130]: 以机器学习为基础的代理模型在贝叶斯逆问题方法中的应用

    Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems. (arXiv:2310.12046v1 [stat.ML])

    [http://arxiv.org/abs/2310.12046](http://arxiv.org/abs/2310.12046)

    本文探讨了在贝叶斯逆问题中，使用机器学习为基础的代理模型，以提高计算效率和处理数值挑战问题。

    

    神经网络已成为一种强大的工具，作为代理模型，在增加计算效率的同时为科学问题提供数值解。这种效率在时间至关重要的数值挑战问题或需要评估许多类似分析方案的情况下具有优势。一个特定的科学兴趣领域是逆问题的设置，其中我们知道系统的正向动态由偏微分方程描述，任务是根据这些动态的（潜在有噪声的）观测来推断系统的性质。我们考虑推断给定2D声波方程的嘈杂解的方块域中波源的位置的逆问题。在假设为高斯噪声的情况下，可以构造源位置的似然函数，每个评估都需要对系统进行一次正向模拟。使用标准的神经网络作为代理模型

    Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model make
    
[^131]: 可以通过分组缩放提高机器学习回归的预测不确定性的一致性和适应性吗？

    Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?. (arXiv:2310.11978v1 [stat.ML])

    [http://arxiv.org/abs/2310.11978](http://arxiv.org/abs/2310.11978)

    本文研究了分组缩放方法（BVS）的几种改进方法，探索了使用替代损失函数和基于输入特征的分组方案来提高机器学习回归的预测不确定性的一致性和适应性。

    

    最近，分组方差缩放（BVS）被提出作为一种用于机器学习回归问题的预测不确定性的事后校准方法，能够比统一方差（或温度）缩放更有效地进行校正。原始版本的BVS使用基于不确定性的分组，旨在提高条件上的校准性，即一致性。本文探讨了BVS的几种改进方法，特别是在损失函数和基于输入特征（X）的分组方案上进行改进，以提高适应性，即在给定X的条件下进行校准性。将BVS及其改进方案在预测原子化能的基准数据集上进行了性能测试，并与保序回归的结果进行了比较。

    Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
    
[^132]: 关于贝叶斯图神经网络在一致预测中的温度问题

    On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v1 [cs.LG])

    [http://arxiv.org/abs/2310.11479](http://arxiv.org/abs/2310.11479)

    该论文探讨了将温度参数纳入贝叶斯图神经网络在一致预测中的优势，以提供有效的不确定性量化。

    

    准确的不确定性量化对于图神经网络(GNNs)至关重要，特别是在高风险领域中经常使用GNNs的情况下。一致预测(CP)为任何黑盒模型提供了一个量化不确定性的有前途的框架。CP保证了一个预测集以所需的概率包含真实标签的形式的官方概率保证。然而，预测集的大小，即"低效率"，受到底层模型和数据生成过程的影响。另一方面，贝叶斯学习还基于估计的后验分布提供一个可信区域，但只有在模型正确指定的情况下，这个区域才是"良好校准"的。在一个最近的工作的基础上，该工作引入了一个缩放参数，用于从后验估计中构建有效的可信区域，我们的研究探讨了在CP框架中将一个温度参数纳入贝叶斯GNNs中的优势。

    Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP fra
    
[^133]: 机器学习模型地球科学系统建模中的质量保持感知器

    A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])

    [http://arxiv.org/abs/2310.08644](http://arxiv.org/abs/2310.08644)

    这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。

    

    虽然数十年来致力于构建用于预测地球科学系统时间序列演化的物理-概念 (PC) 模型，但最近的研究表明，基于机器学习 (ML) 的门控循环神经网络技术可以用于开发更准确的模型。然而，从ML基础模型中提取物理理解的困难使得其在增强对系统结构和功能的科学知识方面的应用变得复杂。在这里，我们提出了一个理解物理性的质量保持感知器 (MCP) 作为弥合PC模型和ML模型的方法。MCP利用PC模型和GRNNs背后的有向图结构的内在同构性，以可解释的方式明确表示物理过程的质量保持性质，同时利用现有数据和现成的ML技术直接学习这种过程的功能性（可解释性）。

    Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
    
[^134]: 通过有效利用文本数据合成提高端到端语音处理的效率

    Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05374](http://arxiv.org/abs/2310.05374)

    本论文提出了一种名为LaSyn的文本数据利用框架，通过将文本数据转换为中间潜变表示来增强端到端语音处理模型的训练。在低资源环境下的语音识别和口语理解任务中，LaSyn相对词错误率减少了22.3%，绝对意图分类准确率提高了4.1%。

    

    在数据中心的人工智能时代，培训高性能的端到端语音处理模型需要大量标记的语音数据。然而，与文本数据相比，标记的语音数据通常更加稀缺和昂贵。我们提出了一种名为LaSyn的有效的文本数据利用框架，用于端到端语音处理模型。我们训练一个潜变合成器将文本数据转换为预训练语音模型的中间潜变表示。这些伪声学表示用于增强模型训练的声学数据。我们在低资源的自动语音识别（ASR）和口语理解（SLU）任务上评估了LaSyn。对于ASR，LaSyn改进了在LibriSpeech train-clean-100上训练的E2E基线，在不同的测试集上相对词错误率减少了22.3%。对于SLU，LaSyn改进了我们的E2E基线，绝对意图分类准确率提高了4.1%。

    Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
    
[^135]: OpenPatch:一个用于超出分布检测的3D拼贴

    OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])

    [http://arxiv.org/abs/2310.03388](http://arxiv.org/abs/2310.03388)

    OpenPatch是一个3D拼贴，基于大型预训练模型提取中间特征来描述每个已知类别的补丁表示，在超出分布检测中取得了进展。

    

    将深度学习模型从实验室环境迁移到开放世界，需要使它们能够处理未知条件。在一些应用中，部署过程中出现新的类别可能构成重大威胁，因此有效地检测它们至关重要。理想情况下，这种能力应该在需要时使用，而不需要在每个新任务中进行任何进一步的计算训练。过去几年中，超出分布检测引起了广泛关注，然而绝大多数研究都处理2D图像，忽视了现实世界中固有的3D特性，并经常混淆领域和语义的新颖性。在这项工作中，我们专注于后者，考虑由3D点云捕捉的物体几何结构，而不考虑特定领域。我们通过引入OpenPatch推动该领域的进展，它建立在一个大型预训练模型的基础上，简单地提取从其中间特征中描述每个已知类别的一组补丁表示。

    Moving deep learning models from the laboratory setting to the open world entails preparing them to handle unforeseen conditions. In several applications the occurrence of novel classes during deployment poses a significant threat, thus it is crucial to effectively detect them. Ideally, this skill should be used when needed without requiring any further computational training effort at every new task. Out-of-distribution detection has attracted significant attention in the last years, however the majority of the studies deal with 2D images ignoring the inherent 3D nature of the real-world and often confusing between domain and semantic novelty. In this work, we focus on the latter, considering the objects geometric structure captured by 3D point clouds regardless of the specific domain. We advance the field by introducing OpenPatch that builds on a large pre-trained model and simply extracts from its intermediate features a set of patch representations that describe each known class. F
    
[^136]: Avalon的思考游戏：通过递归思考对抗欺骗

    Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.01320](http://arxiv.org/abs/2310.01320)

    本研究通过使用复杂的Avalon游戏作为测试平台，引入了一种名为递归思考（ReCon）的新框架，用于增强大型语言模型（LLM）识别和对抗欺骗信息的能力。

    

    最近在大型语言模型（LLM）的突破带来了在LLM作为智能体领域的显著成功。然而，一种普遍的假设是LLM处理的信息始终是诚实的，忽视了人类社会和AI生成内容中普遍存在的欺骗或误导性信息。这个疏忽使得LLM容易受到恶意操纵，可能导致不利的结果。本研究利用复杂的Avalon游戏作为测试平台，探索LLM在欺骗环境中的潜力。Avalon充满了错误信息，并需要复杂的逻辑，表现为“思考的游戏”。受到人类在Avalon游戏中递归思考和透视能力的启发，我们引入了一种新颖的框架——递归思考（ReCon），以增强LLM识别和对抗欺骗信息的能力。ReCon结合了公式化思考和完善思考的过程；公式化思考产生初始思考，完善思考对初始思考进行调整和改进。

    Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
    
[^137]: 可解释的基于机器学习的糖尿病肾病预测模型

    Explainable machine learning-based prediction model for diabetic nephropathy. (arXiv:2309.16730v1 [cs.LG])

    [http://arxiv.org/abs/2309.16730](http://arxiv.org/abs/2309.16730)

    本研究提出了一个可解释的基于机器学习的糖尿病肾病预测模型，通过分析血清代谢物对疾病的影响并选择最优特征来预测疾病的患病率。最优模型采用了极限梯度提升（XGB）算法，其在筛选DN方面表现最佳，同时具有更好的临床效益和拟合度。

    

    本研究旨在分析血清代谢物对糖尿病肾病（DN）的影响，并通过机器学习方法预测DN的患病率。数据集包括2018年4月至2019年4月大连医科大学附属第二医院的548名患者。我们通过最小绝对收缩和选择算子（LASSO）回归模型和10折交叉验证选择了最优的38个特征。我们比较了四种机器学习算法，包括极限梯度提升（XGB）、随机森林、决策树和逻辑回归，并通过AUC-ROC曲线、决策曲线和校准曲线进行比较。我们利用Shapley Additive exPlanations（SHAP）方法来量化最优预测模型中的特征重要性和交互效应。XGB模型在筛选DN方面具有最佳性能，最高AUC值为0.966。XGB模型的临床净效益也优于其他模型，并且拟合度更好。

    The aim of this study is to analyze the effect of serum metabolites on diabetic nephropathy (DN) and predict the prevalence of DN through a machine learning approach. The dataset consists of 548 patients from April 2018 to April 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU). We select the optimal 38 features through a Least absolute shrinkage and selection operator (LASSO) regression model and a 10-fold cross-validation. We compare four machine learning algorithms, including eXtreme Gradient Boosting (XGB), random forest, decision tree and logistic regression, by AUC-ROC curves, decision curves, calibration curves. We quantify feature importance and interaction effects in the optimal predictive model by Shapley Additive exPlanations (SHAP) method. The XGB model has the best performance to screen for DN with the highest AUC value of 0.966. The XGB model also gains more clinical net benefits than others and the fitting degree is better. In addition, there are s
    
[^138]: 利用深度学习和在线情绪分析进行金融投资组合管理

    Leveraging Deep Learning and Online Source Sentiment for Financial Portfolio Management. (arXiv:2309.16679v1 [q-fin.PM])

    [http://arxiv.org/abs/2309.16679](http://arxiv.org/abs/2309.16679)

    本文研究利用深度学习方法进行金融交易，并考虑情绪信息的作用。同时讨论常见的训练问题，并提供应用方法。

    

    金融投资组合管理是指在一系列金融资产（如股票、指数基金、外汇或加密货币）上分配资金并进行交易操作的任务，旨在最大化利润同时最小化交易操作所造成的损失。深度学习方法一直以来在各种任务中表现出色，自动化金融交易就是其中最复杂的任务之一。本文旨在提供关于金融交易中各种深度学习方法的见解，分别在监督学习和强化学习框架下进行讨论。同时，考虑到与交易资产相关的情绪信息，我们通过相应的研究研究论证了它们的有用性。最后，我们讨论了训练此类金融智能体中常见的问题，并为读者提供必要的知识，以避免这些问题并将讨论的方法应用于实践中。

    Financial portfolio management describes the task of distributing funds and conducting trading operations on a set of financial assets, such as stocks, index funds, foreign exchange or cryptocurrencies, aiming to maximize the profit while minimizing the loss incurred by said operations. Deep Learning (DL) methods have been consistently excelling at various tasks and automated financial trading is one of the most complex one of those. This paper aims to provide insight into various DL methods for financial trading, under both the supervised and reinforcement learning schemes. At the same time, taking into consideration sentiment information regarding the traded assets, we discuss and demonstrate their usefulness through corresponding research studies. Finally, we discuss commonly found problems in training such financial agents and equip the reader with the necessary knowledge to avoid these problems and apply the discussed methods in practice.
    
[^139]: 利用扩散模型中的信号泄漏偏差

    Exploiting the Signal-Leak Bias in Diffusion Models. (arXiv:2309.15842v1 [cs.CV])

    [http://arxiv.org/abs/2309.15842](http://arxiv.org/abs/2309.15842)

    本文展示了如何利用现有扩散模型中的信号泄漏偏差，以实现对生成图像的更好控制，并生成更多样化的亮度以及更满足特定风格和颜色要求的图像。

    

    大多数扩散模型的推理过程中存在偏差。这种偏差是由信号泄漏引起的，其分布与噪声分布不一致，导致训练和推理过程之间存在差异。我们证明了在模型针对特定风格进行调优时，这种信号泄漏偏差特别显著，导致风格匹配不够优化。最近的研究试图在训练过程中避免信号泄漏。我们相反地展示了如何利用现有的扩散模型中的信号泄漏偏差，以实现对生成图像的更好控制。这使我们能够生成亮度更多样化的图像以及更能匹配所需风格或颜色的图像。通过对空间频率和像素域中的信号泄漏进行建模，并在初始潜变量中引入信号泄漏，我们生成更符合预期结果的图像，而无需额外的训练。

    There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training.
    
[^140]: Detach-ROCKET: 基于随机卷积核的时间序列分类中的顺序特征选择

    Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels. (arXiv:2309.14518v1 [cs.LG])

    [http://arxiv.org/abs/2309.14518](http://arxiv.org/abs/2309.14518)

    本文提出了一种名为Detach-ROCKET的方法，用于时间序列分类中的顺序特征选择。通过利用随机卷积核模型中的大量特征，并使用顺序特征分离方法剪枝非主要特征，提高了模型的可扩展性和泛化能力。

    

    时间序列分类在许多领域中都是必不可少的，如医学、金融、环境科学和制造业，可以实现疾病诊断、异常检测和股价预测等任务。尽管循环神经网络和InceptionTime等机器学习模型在许多应用中取得了成功，但由于训练需求的繁重，可能会面临可扩展性限制。为了解决这个问题，出现了Rocket及其衍生模型等随机卷积核模型，通过利用从时间序列数据中随机生成的大量特征，简化训练并实现最先进的性能。然而，由于其随机性质，生成的大部分特征是冗余或非信息性的，增加了不必要的计算负载并损害了泛化能力。在这里，我们引入了顺序特征分离（SFD）作为一种识别和修剪这些非主要特征的方法。SFD利用模型系数来估计特征的重要性。

    Time series classification is essential in many fields, such as medicine, finance, environmental science, and manufacturing, enabling tasks like disease diagnosis, anomaly detection, and stock price prediction. Machine learning models like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive training requirements. To address this, random convolutional kernel models such as Rocket and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance a
    
[^141]: 与人类辅助语言规划器的终身机器人学习

    Lifelong Robot Learning with Human Assisted Language Planners. (arXiv:2309.14321v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.14321](http://arxiv.org/abs/2309.14321)

    本文介绍了一种利用大型语言模型(LLM)进行终身机器人学习的方法，该方法可以使机器人查询和学习新的技能，并在刚性物体操作方面实现数据和时间高效。该系统具有重复使用新获得技能的能力，展示了开放世界和终身学习的潜力。

    

    大型语言模型(LLM)已被证明可以像规划者一样将高级指令分解为可执行指令的序列。然而，当前基于LLM的规划器只能使用固定的技能集合运作。我们克服了这个关键限制，并提出了一种利用LLM-based规划器查询新技能并以数据和时间高效的方式教授机器人这些技能的方法，用于刚性物体操作。我们的系统可以重复使用新获得的技能用于未来的任务，展示了开放世界和终身学习的潜力。我们在模拟和真实世界中对提出的框架进行了多个任务的评估。视频可在以下网址观看：https://sites.google.com/mit.edu/halp-robot-learning。

    Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning.
    
[^142]: 库存管理中的缺货预测: 分类技术和成本考虑

    Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations. (arXiv:2309.13837v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13837](http://arxiv.org/abs/2309.13837)

    本文介绍了一种先进的分析方法，用于预测库存管理中的缺货情况。该方法考虑了多种分类技术和成本考虑，通过提高服务水平来提高客户满意度和整体组织绩效。

    

    本文介绍了一种先进的分析方法，用于预测库存管理中的缺货情况。缺货是指由于库存耗尽而无法立即满足的订单。本文使用多种分类技术，包括平衡装袋分类器、模糊逻辑、变分自编码器-生成对抗网络和多层感知器分类器，使用ROC-AUC和PR-AUC等性能评估指标对其进行评估。此外，本文还考虑了利润函数和错分成本，考虑了与库存管理和缺货处理相关的财务影响和成本。结果表明，预测模型能够提高库存系统的服务水平，从而提高客户满意度和整体组织绩效。在商业应用中考虑可解释性是使用人工智能的一个重要方面，本研究还应用了排列重要性方法来选择模型。

    This article introduces an advanced analytical approach for predicting backorders in inventory management. Backorder refers to an order that cannot be immediately fulfilled due to stock depletion. Multiple classification techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron classifiers, are assessed in this work using performance evaluation metrics such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function and misclassification costs, considering the financial implications and costs associated with inventory management and backorder handling. The results demonstrate the effectiveness of the predictive model in enhancing inventory system service levels, which leads to customer satisfaction and overall organizational performance. Considering interpretability is a significant aspect of using AI in commercial applications, permutation importance is applied to the selected mo
    
[^143]: 使用深度学习和药动学先验预测治疗反应

    Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])

    [http://arxiv.org/abs/2309.13135](http://arxiv.org/abs/2309.13135)

    该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。

    

    对医疗时间序列的预测对于早期检测不良结果和患者监测至关重要。然而，由于数据嘈杂和间歇性，实际中预测可能很困难。这些挑战通常通过外部因素诱导的变化点（如药物使用）而加剧。我们提出了一种新颖的编码器，以向深度学习模型提供药物的药动学效应信息，从而实现对受治疗影响的时间序列的准确预测。我们展示了我们方法在使用逼真模拟和真实世界数据预测血糖的任务中的有效性。我们的药动学编码器使深度学习模型在模拟数据上超过基准约11％，在真实世界数据上超过8％。所提出的方法可以在临床实践中具有多种有益应用，例如发出关于意外治疗反应的早期警告，或帮助表征特定于患者的治疗效果。

    Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
    
[^144]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^145]: 如何微调模型：统一模型偏移和模型偏差策略优化

    How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization. (arXiv:2309.12671v1 [cs.LG])

    [http://arxiv.org/abs/2309.12671](http://arxiv.org/abs/2309.12671)

    本文提出了一个统一模型偏移和模型偏差的优化目标，并通过微调过程实现了自适应的模型更新，以提供性能改进保证和避免模型过拟合。

    

    设计和推导出具有性能改进保证的有效基于模型的强化学习（MBRL）算法具有挑战性，这主要归因于模型学习和策略优化之间的高耦合性。许多先前的方法依靠回报差异来指导模型学习，忽略了模型偏移的影响，这可能导致由于过多的模型更新而性能下降。其他方法使用性能差异边界来明确考虑模型偏移。然而，这些方法依赖于固定的阈值来限制模型偏移，导致对阈值的严重依赖，并且在训练过程中缺乏适应性。在本文中，我们从理论上推导出一个可以统一模型偏移和模型偏差的优化目标，然后制定一个微调过程。这个过程可以自适应地调整模型更新，以获得性能改进保证，同时避免模型过拟合。基于这些，我们开发了一个简单直观的方法

    Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward 
    
[^146]: 批量安卓恶意软件检测模型的高效概念漂移处理

    Efficient Concept Drift Handling for Batch Android Malware Detection Models. (arXiv:2309.09807v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2309.09807](http://arxiv.org/abs/2309.09807)

    本文研究了批量安卓恶意软件检测模型的高效概念漂移处理方法，通过重新训练技术来维持检测器的能力，并通过比较不同的重新训练频率和数据使用方法的影响，提出了改进策略。

    

    Android应用程序的快速发展性对于静态批处理机器学习算法在恶意软件检测系统中的应用构成了重大挑战，因为它们很快就会过时。尽管存在这个挑战，但现有文献对解决此问题的关注有限，许多先进的Android恶意软件检测方法（如Drebin、DroidDet和MaMaDroid）依赖于静态模型。在这项工作中，我们展示了重新训练技术如何能够在一段时间内保持检测器的能力。特别地，我们分析了两个方面对检测器的效率和性能的影响：1）模型重新训练的频率，2）用于重新训练的数据。在第一个实验中，我们将周期性重新训练与仅在必要时触发重新训练的更先进的概念漂移检测方法进行了比较。在第二个实验中，我们分析了用于减少用于重新训练模型的数据量的采样方法。具体而言，我们比较了固定si

    The rapidly evolving nature of Android apps poses a significant challenge to static batch machine learning algorithms employed in malware detection systems, as they quickly become obsolete. Despite this challenge, the existing literature pays limited attention to addressing this issue, with many advanced Android malware detection approaches, such as Drebin, DroidDet and MaMaDroid, relying on static models. In this work, we show how retraining techniques are able to maintain detector capabilities over time. Particularly, we analyze the effect of two aspects in the efficiency and performance of the detectors: 1) the frequency with which the models are retrained, and 2) the data used for retraining. In the first experiment, we compare periodic retraining with a more advanced concept drift detection method that triggers retraining only when necessary. In the second experiment, we analyze sampling methods to reduce the amount of data used to retrain models. Specifically, we compare fixed si
    
[^147]: 可扩展的神经网络模型和千兆级数据集用于粒子流重建

    Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])

    [http://arxiv.org/abs/2309.06782](http://arxiv.org/abs/2309.06782)

    本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。

    

    本研究针对高能电子-正电子碰撞中基于高度粒度探测器模拟的完整事件重建，研究了可扩展的机器学习模型。粒子流（PF）重建可通过跟踪和量能器团簇或击中来构建监督学习任务。我们比较了图神经网络和基于内核的变换器，并证明两者都避免了二次内存分配和计算成本，同时实现了真实的粒子流重建。我们展示了在超级计算机上进行的超参数调优显著提高了模型的物理性能。我们还展示了所得模型在硬件处理器上具有高度可移植性，支持NVIDIA, AMD和英特尔 Habana卡。最后，我们证明了模型可以在由跟踪和量能器击中组成的高粒度输入上进行训练，从而获得与基准相竞争的物理性能。有关复现研究的数据集和软件已发布。

    We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
    
[^148]: Thompson Sampling用于多臂老虎机的实值组合纯探索

    Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2308.10238v1 [cs.LG])

    [http://arxiv.org/abs/2308.10238](http://arxiv.org/abs/2308.10238)

    这篇论文介绍了一种名为广义汤普森抽样探索算法，能够解决多臂老虎机实值组合纯探索问题中动作集合大小为指数级别的情况。

    

    我们研究了多臂老虎机的实值组合纯探索（R-CPE-MAB）问题。在R-CPE-MAB中，玩家从给定的d个随机臂中选择一个，每个臂s的奖励遵循未知分布，其平均值为μs。在每个时间步骤中，玩家拉动一个臂并观察其奖励。玩家的目标是以尽可能少的臂拉动次数来确定最优动作π* = argmaxπ∈A μTπ，其中A是有限大小的实值动作集合。之前的方法假设动作集合A的大小在d的多项式级别上。我们引入了一种名为广义汤普森抽样探索（GenTS-Explore）算法，它是第一个可以解决动作集合大小在d的指数级别上的算法。同时，我们还引入了一种新的问题相关的样本复杂性。

    We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity 
    
[^149]: 癫痫预测中的路径签名

    Path Signatures for Seizure Forecasting. (arXiv:2308.09312v1 [stat.ML])

    [http://arxiv.org/abs/2308.09312](http://arxiv.org/abs/2308.09312)

    本研究以癫痫预测为目标，通过自动发现和量化患者特定的统计特征，特别是最新的路径签名算法，探索其在癫痫预测中的性能，为个性化的癫痫预测解决方案提供了参考。

    

    从观测到的时间序列中预测系统状态是许多领域（如计算神经科学）的研究课题。在这里，从大脑测量中预测癫痫发作是一个尚未解决的问题。既没有完整的描述底层大脑动态的模型，也没有单个患者表现出单一的癫痫发作模式，这使得开发“一刀切”的解决方案变得复杂。基于纵向患者数据集，我们解决了自动发现和量化可用于以患者为中心的癫痫预测的统计特征（生物标志物）的问题。我们使用现有和新颖的特征提取算法，尤其是路径签名，即时间序列分析的最新发展。特别值得关注的是，与简单的线性特征相比，这组复杂的非线性特征在这个任务中的表现如何。我们的推断基于统计分类算法，并带有内置的子集选择功能。

    Forecasting the state of a system from an observed time series is the subject of research in many domains, such as computational neuroscience. Here, the prediction of epileptic seizures from brain measurements is an unresolved problem. There are neither complete models describing underlying brain dynamics, nor do individual patients exhibit a single seizure onset pattern, which complicates the development of a `one-size-fits-all' solution. Based on a longitudinal patient data set, we address the automated discovery and quantification of statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way. We use existing and novel feature extraction algorithms, in particular the path signature, a recent development in time series analysis. Of particular interest is how this set of complex, nonlinear features performs compared to simpler, linear features on this task. Our inference is based on statistical classification algorithms with in-built subset select
    
[^150]: 《TARJAMAT: Bard和ChatGPT在十种阿拉伯语变体机器翻译上的评估》

    TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties. (arXiv:2308.03051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03051](http://arxiv.org/abs/2308.03051)

    这项研究对Bard和ChatGPT在十种阿拉伯语变体的机器翻译能力进行了评估，发现LLM在翻译方言方面表现优于商业系统，但在古典阿拉伯语和现代标准阿拉伯语方面落后于谷歌翻译等商业系统。

    

    尽管像ChatGPT和Bard这样的经过指导微调的大型语言模型（LLM）被认为在多语言上有很高的能力，但这些模型的语言包容性还没有得到充分的探索。考虑到这个限制，我们对Bard和ChatGPT（包括GPT-3.5和GPT-4）在十种阿拉伯语变体的机器翻译能力进行了全面评估。我们的评估涵盖了古典阿拉伯语（CA）、现代标准阿拉伯语（MSA）和几种国家方言变体等多种阿拉伯语变体。我们的分析表明，LLM在存在少量公共数据集的方言上可能会遇到挑战，但平均而言，它们比现有的商业系统更擅长翻译方言。然而，在CA和MSA方面，经过指导调整的LLM与谷歌翻译等商业系统相比仍然有所不足。最后，我们开展了一个以人为中心的研究，以审查相对较新的模型Bard在遵循人类指令方面的效果。

    Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level dialectal variants. Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems. On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate. Finally, we undertake a human-centric study to scrutinize the efficacy of the relatively recent model, Bard, in following human instructio
    
[^151]: MM-Vet: 评估大型多模态模型的综合能力

    MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])

    [http://arxiv.org/abs/2308.02490](http://arxiv.org/abs/2308.02490)

    MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。

    

    我们提出了MM-Vet，一个评估标准，用于检查在复杂多模态任务上的大型多模态模型（LMM）的表现。最近的LMM展示了各种有趣的能力，例如解决书写在黑板上的数学问题，推理新闻图片中的事件和名人，以及解释视觉笑话。快速的模型进步给评估标准的开发带来了挑战。问题包括：（1）如何系统地构建和评估复杂的多模态任务；（2）如何设计适用于不同类型问题和回答的评估指标；（3）如何给出超出简单性能排名的模型洞察。为此，我们提出了MM-Vet，基于这样一个洞察：解决复杂任务的有趣能力通常通过一种通才模型能够整合不同的核心视觉-语言（VL）能力来实现。MM-Vet定义了6个核心VL能力，并检查了从这些能力组合中得出的16种有趣的整合方式。

    We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
    
[^152]: DiffKendall:一种利用可微分Kendall排名相关性进行少样本学习的新方法

    DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation. (arXiv:2307.15317v1 [cs.CV])

    [http://arxiv.org/abs/2307.15317](http://arxiv.org/abs/2307.15317)

    本文提出了一种利用可微分Kendall排名相关性进行少样本学习的新方法，证明了特征通道的重要性排序在少样本学习中比几何相似度度量更可靠，并且实验证明在推理过程中用Kendall排名相关性替换几何相似度度量能够提高少样本学习性能。

    

    少样本学习旨在将在基本数据集上训练的模型适应到模型之前未见过的新领域的任务。这经常导致新类别上通道上特征值的分布相对均匀，难以确定新任务中通道的重要性。标准的少样本学习方法使用几何相似度度量，如余弦相似度和负欧几里德距离，来衡量两个特征之间的语义相关性。然而，在少样本学习的情况下，具有高几何相似度的特征可能具有不同的语义。本文证明了特征通道的重要性排序在少样本学习中比几何相似度度量更可靠。我们观察到，仅在推理过程中用Kendall排名相关性替换几何相似度度量能够提高在各种数据集中的少样本学习性能。

    Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with diffe
    
[^153]: 黑盒变分推断的线性收敛性：我们应该坚持到底吗？

    Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])

    [http://arxiv.org/abs/2307.14642](http://arxiv.org/abs/2307.14642)

    本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。

    

    我们证明了带有控制变量的黑盒变分推断（BBVI），特别是着陆稳定（STL）估计器，在完美变分族规范下收敛于几何（传统上称为“线性”）速度。特别地，我们证明了STL估计器的梯度方差的二次界限，该界限包括了误指定的变分族。结合先前关于二次方差条件的工作，这直接暗示了在使用投影随机梯度下降的情况下BBVI的收敛性。我们还改进了现有对于正常封闭形式熵梯度估计器的分析，这使得我们能够将其与STL估计器进行比较，并为两者提供明确的非渐进复杂度保证。

    We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
    
[^154]: 模仿复杂轨迹：桥接低层稳定性与高层行为

    Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])

    [http://arxiv.org/abs/2307.14619](http://arxiv.org/abs/2307.14619)

    本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。

    

    我们提出了一个理论框架来研究在非线性动态系统中模仿随机、非马尔可夫、潜在多模态（即“复杂”）专家演示的行为。我们的框架使用低层控制器（无论是学习的还是隐含的）来稳定围绕专家演示的模仿策略。我们证明，在（a）合适的低层稳定性保证和（b）学习策略的随机连续性属性（我们称之为“总变差连续性”）（TVC）的情况下，一个精确估计演示者状态分布上的行动的模仿者会与演示者对整个轨迹的分布相近。然后，我们证明可以通过将流行的数据增强规则与一种新颖的算法技巧相结合（即在执行时添加增强噪声）来确保TVC并且最小程度上降低精度。我们将我们的保证实例化为由扩散模型参数化的策略，并证明如果学习者准确地估计了演示者的分布，则最终完成这种实例化。

    We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
    
[^155]: 机器学习模型的局部鲁棒性的高效估计

    Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])

    [http://arxiv.org/abs/2307.13885](http://arxiv.org/abs/2307.13885)

    本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    

    机器学习模型通常需要对噪声输入数据具有鲁棒性。现实世界中的噪声（通常是随机的）对模型预测的影响可以通过模型的局部鲁棒性来捕捉，即在输入周围的局部区域内模型预测的一致性。然而，基于蒙特卡罗采样的计算局部鲁棒性的朴素方法在统计上是低效的，对于大规模应用而言计算成本高昂。在这项工作中，我们通过局部线性函数逼近和多元正态CDF开发了首个分析估计器，以高效计算多类别判别模型的局部鲁棒性。通过这些估计器的推导，我们展示了局部鲁棒性与随机平滑和softmax概率等概念的联系。我们还通过实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
    
[^156]: WebArena: 一个用于构建自主智能体的真实网络环境

    WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])

    [http://arxiv.org/abs/2307.13854](http://arxiv.org/abs/2307.13854)

    WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。

    

    随着生成式人工智能的进展，通过自然语言指令进行日常任务的自主智能体的潜力逐渐显现。然而，当前的智能体主要是在简化的合成环境中创建和测试的，严重限制了现实世界场景的表示能力。在本文中，我们构建了一个高度逼真且可复现的智能体指令和控制环境。具体而言，我们关注在网站上执行任务的智能体，我们创建了一个包含来自四个常见领域的完全功能网站的环境，分别是电子商务、社交论坛讨论、协同软件开发和内容管理。我们的环境使用工具（如地图）和外部知识库（如用户手册）来鼓励像人类一样解决任务。在我们的环境基础上，我们发布了一组重点评估任务完成功能正确性的基准任务。我们基准任务具有多样性和长远的视野，并且被设计为鼓励智能体进行更深层次的任务理解和解决。

    With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
    
[^157]: 面向交通信号控制的不确定性感知基于实例的行动转换的模拟到实际转移

    Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12388](http://arxiv.org/abs/2307.12388)

    本文提出了UGAT方法，通过在模拟环境中动态转换具有不确定性的行动，实现了从模拟环境到真实环境的策略转移，显著提高了在真实世界中的性能。

    

    交通信号控制（TSC）是一个影响数百万人日常生活的复杂而重要的任务。强化学习（RL）在优化交通信号控制方面取得了有希望的结果，但当前基于RL的TSC方法主要在模拟环境中训练，存在模拟和真实世界之间性能差距的问题。本文提出了一种模拟到实际环境转移的方法，称为UGAT，通过在模拟中动态转换具有不确定性的行动，以减轻转移动态的领域差距，将在模拟环境中训练的学习策略转移到真实环境中。我们在模拟交通环境中评估了我们的方法，并表明它显著提高了转移后的RL策略在真实世界中的性能。

    Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
    
[^158]: 集群感知的半监督学习：关系知识蒸馏可证明的学习聚类

    Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering. (arXiv:2307.11030v1 [stat.ML])

    [http://arxiv.org/abs/2307.11030](http://arxiv.org/abs/2307.11030)

    这项工作首次从理论上理解了关系知识蒸馏，通过种群上的谱聚类，我们证明了关系知识蒸馏能够导致低聚类误差，并展示了它在半监督学习中的标签效率。

    

    尽管基于关系的知识蒸馏在匹配教师和学生模型之间的特征关系方面取得了实证成功和实际意义，但对于各种知识蒸馏范式，其相应的理论解释仍然有限。在这项工作中，我们首次从理论上理解关系知识蒸馏（RKD），并重点关注半监督分类问题。我们首先将RKD视为教师模型揭示的由种群产生的图上的谱聚类。通过衡量预测和基本事实聚类之间差异的聚类误差概念，我们说明了种群上的RKD可证明地导致低聚类误差。此外，我们提供了对于有限无标签样本的RKD的样本复杂度界限。对于半监督学习，我们进一步通过集群感知的半监督学习框架展示了RKD的标签效率。

    Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning th
    
[^159]: 含有不确定地面真相的符合预测

    Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])

    [http://arxiv.org/abs/2307.09302](http://arxiv.org/abs/2307.09302)

    本文提出了一种适用于含有模糊地面真相的符合预测框架，解决了在缺乏明确地面真相标签的情况下低估不确定性的问题。

    

    在安全关键的分类任务中，符合预测可以通过提供置信区间来进行严格的不确定性量化，其中包括真正类别的用户指定的概率。这通常假设有一个独立的校准集合，并且能够访问地面真相标签。不幸的是，在许多领域中，这些标签很难获得，并且通常通过聚合专家意见来近似。事实上，这适用于几乎所有数据集，包括知名的数据集如CIFAR和ImageNet。使用这样的标签应用符合预测会低估不确定性。事实上，当专家意见无法解决时，标签中存在固有的模糊性。也就是说，我们没有“清晰”、明确的地面真相标签，而在校准过程中应该考虑这种不确定性。在本文中，我们针对这种模糊地面真相情景开发了一个符合预测框架，该框架依赖于对潜在模糊性的近似。

    In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlyi
    
[^160]: RADAR: 通过对抗性学习实现鲁棒的AI文本检测

    RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])

    [http://arxiv.org/abs/2307.03838](http://arxiv.org/abs/2307.03838)

    本论文提出了一种名为RADAR的新框架，通过对抗性学习实现了鲁棒的AI文本检测，以解决当前AI文本检测器对于大语言模型的改写不具备鲁棒性的问题。

    

    最近大语言模型（LLMs）的进展以及ChatGPT类应用的普及已经模糊了人类和机器之间高质量文本生成的界限。然而，除了对我们的技术和社会预期的革命性变化外，区分LLM生成的文本（AI文本）和人类生成的文本的困难也带来了新的滥用和公平性挑战，例如虚假内容生成，抄袭以及对无辜作者的错误指控。尽管现有的研究表明当前的AI文本检测器对基于LLM的改写不具有鲁棒性，但本文旨在通过提出一种名为RADAR的新框架来弥合这一差距，该框架通过对抗性学习共同训练了一个鲁棒的AI文本检测器。RADAR基于一个改写器和一个检测器的对抗性训练。改写器的目标是生成逼真的内容以规避AI文本检测。RADAR使用来自检测器的反馈来更新改写器，反之亦然。

    Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
    
[^161]: 使用对抗模型量化不确定性

    Quantification of Uncertainty with Adversarial Models. (arXiv:2307.03217v1 [cs.LG])

    [http://arxiv.org/abs/2307.03217](http://arxiv.org/abs/2307.03217)

    该论文提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。与先前的方法相比，QUAM对认知不确定性的近似误差更小。

    

    在实际应用中，量化不确定性对于可操作的预测非常重要。预测不确定性的关键在于估计认知不确定性，它被定义为一个散度函数和后验的乘积的积分。当前的方法如Deep Ensembles或MC dropout在估计认知不确定性方面表现不佳，因为它们主要考虑后验在采样模型时。我们提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。因此，与先前的方法相比，QUAM对认知不确定性的近似误差更小。乘积较大的模型对应于对抗模型（不是对抗性示例！）。对抗模型既有较高的后验，也有其预测与其他模型之间的较高差异。

    Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of
    
[^162]: 改进深度伪造检测中的公平性

    Improving Fairness in Deepfake Detection. (arXiv:2306.16635v1 [cs.CV])

    [http://arxiv.org/abs/2306.16635](http://arxiv.org/abs/2306.16635)

    本研究首次尝试通过提出新的损失函数来改善深度伪造检测的公平性，并在多个数据集和检测器上进行了广泛实验证明了其有效性。

    

    尽管近年来已经开发出有效的深度伪造检测模型，但是一些最近的研究表明，在开发深度伪造检测模型时所使用的训练数据中存在偏见可能导致不同种族和/或性别的人群的不公平表现。这可能导致这些群体受到不公平的定位或被排除在检测之外，从而让被错误分类的深度伪造操纵舆论并破坏对模型的信任。虽然这些研究着重于确定和评估深度伪造检测中的不公平性，但目前还没有开发出解决深度伪造检测算法层面公平性问题的方法。在这项工作中，我们首次尝试通过提出新的损失函数来改进深度伪造检测的公平性，以在不考虑或考虑人口因素的情况下训练公平的深度伪造检测模型。对四个深度伪造数据集和五个深度伪造检测器的大量实验证明了这种方法的有效性和灵活性。

    Despite the development of effective deepfake detection models in recent years, several recent studies have demonstrated that biases in the training data utilized to develop deepfake detection models can lead to unfair performance for demographic groups of different races and/or genders. Such can result in these groups being unfairly targeted or excluded from detection, allowing misclassified deepfakes to manipulate public opinion and erode trust in the model. While these studies have focused on identifying and evaluating the unfairness in deepfake detection, no methods have been developed to address the fairness issue of deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions to train fair deepfake detection models in ways that are agnostic or aware of demographic factors. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexi
    
[^163]: ViNT:一种用于视觉导航的基础模型

    ViNT: A Foundation Model for Visual Navigation. (arXiv:2306.14846v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.14846](http://arxiv.org/abs/2306.14846)

    该论文介绍了一种名为ViNT的基础模型，旨在将通用预训练模型的成功应用于视觉导航领域。ViNT通过通用目标达成目标进行训练，并采用灵活的Transformer架构来学习导航功能和实现对各种导航任务的高效适应。

    

    通用预训练模型（"基础模型"）使从事机器学习问题的从业者能够使用比学习自零开始所需数据集小得多的数据，提供可泛化的解决方案。这些模型通常在大型和多样化的数据集上进行训练，使用弱监督进行学习，消耗比任何单个下游应用程序可用的训练数据要多得多。本文中，我们介绍了一种名为Visual Navigation Transformer（ViNT）的基础模型，旨在将通用预训练模型的成功应用于基于视觉的机器人导航。ViNT通过使用适用于任何导航数据集的通用目标达成目标来进行训练，并采用灵活的基于Transformer的架构来学习导航功能，并实现对各种下游导航任务的高效适应。ViNT在多个现有的导航数据集上进行了训练，涵盖数百小时的机器人导航数据。

    General-purpose pre-trained models ("foundation models") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navi
    
[^164]: 学习未见过的模态交互

    Learning Unseen Modality Interaction. (arXiv:2306.12795v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2306.12795](http://arxiv.org/abs/2306.12795)

    本文提出了一个解决多模态学习中未见过的模态组合的问题的方法。该方法利用一个模块将不同模态的特征投影到一个共享的空间中，并通过伪监督来减少过拟合。实验证明该方法在多个任务和模态上都是有效的。

    

    多模态学习假定在训练期间可以使用所有感兴趣的模态组合来学习跨模态对应关系。本文对多模态学习中的模态完整性做出了挑战，而是在推理过程中努力实现对未见过的模态组合的泛化。我们提出了未见过的模态交互问题，并介绍了第一个解决方案。它利用一个模块将不同模态的多维特征投影到一个具有多样信息的公共空间中，从而可以通过简单的求和操作对可用的模态进行信息累积。为了减少在训练过程中对较少辨别力的模态组合的过拟合，我们进一步改进了模型学习，通过伪监督指示模态预测的可靠性。我们通过对多模态视频分类、机器人状态回归等进行评估，证明了我们的方法对于不同任务和模态是有效的。

    Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences.In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, a
    
[^165]: 基于几何深度学习的结构药物设计系统综述

    A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.11768](http://arxiv.org/abs/2306.11768)

    本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。

    

    结构药物设计利用蛋白质的三维几何结构来识别潜在的药物候选物，在药物发现中变得越来越重要。然而，基于物理化学建模和专家领域知识的传统方法费时费力。近年来，几何深度学习的发展，可以处理和整合三维几何数据，加上类似AlphaFold的工具提供准确的蛋白质三维结构预测，极大地推动了结构药物设计的进展。在本文中，我们系统地回顾了几何深度学习在结构药物设计中的最新进展。我们从结构药物设计中的主流任务、常用的3D蛋白质表示和预测/生成模型入手，然后详细介绍每个任务的回顾（例如结合位点预测、结合构象生成、\emph{de novo} 分子设计等），并按不同的几何深度学习方法进行组织。最后，我们总结了该领域未来研究的挑战和前景。

    Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
    
[^166]: 通过提炼视觉基础模型将任意点云序列进行分割

    Segment Any Point Cloud Sequences by Distilling Vision Foundation Models. (arXiv:2306.09347v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.09347](http://arxiv.org/abs/2306.09347)

    本研究引入了一种名为Seal的新型框架，利用视觉基础模型(VFMs)对汽车点云序列进行分割。Seal具有可伸缩性、一致性和泛化能力等优势，并通过大量实验展示了其有效性和优越性。

    

    最近对视觉基础模型(VFMs)的进展为多样化和高效率的视觉感知开辟了新的可能性。在这项工作中，我们介绍了一种名为Seal的新型框架，利用VFMs对各种汽车点云序列进行分割。Seal具有三个吸引人的特点：i）可伸缩性：VFMs直接被提取到点云中，避免了预训练期间2D或3D注释的需求。ii）一致性：在相机到激光雷达和点到分割规则化阶段都强制执行了空间和时间关系，促进了跨模态表示学习。iii）泛化能力：Seal以即插即用的方式实现知识传递到涉及多样化点云的下游任务中，包括来自真实/合成、低/高分辨率、大/小规模以及干净/破损数据集的点云。在对十一个不同点云数据集进行的大量实验中，展示了Seal的有效性和优越性。

    Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notab
    
[^167]: 差分隐私条件独立性检验

    Differentially Private Conditional Independence Testing. (arXiv:2306.06721v1 [stat.ML])

    [http://arxiv.org/abs/2306.06721](http://arxiv.org/abs/2306.06721)

    本文介绍了两个差分隐私条件独立性检验方法，可适用于Z为连续值的一般情况。

    

    条件独立性（CI）检验在统计数据分析中被广泛使用，例如，它们是许多因果图发现算法的构建块。CI测试旨在接受或拒绝$X \perp \!\!\! \perp Y \mid Z$的零假设，其中$X \in \mathbb{R}，Y \in \mathbb{R}，Z \in \mathbb{R}^d$。本文研究了在差分隐私约束下的条件独立性检验。我们设计了基于Shah和Peters（2020）的一般化协方差测量和基于Cand\`es等人的条件随机化检验的两种私人CI测试过程（在模型-X假设下）。我们提供了关于我们测试性能的理论保证，并在实证上验证它们。这些是第一个适用于Z为连续的一般情况的私人CI测试。

    Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests that work for the general case when $Z$ is continuous.
    
[^168]: 面向标签漂移的联邦不确定性量化的合规性预测

    Conformal Prediction for Federated Uncertainty Quantification Under Label Shift. (arXiv:2306.05131v1 [stat.ML])

    [http://arxiv.org/abs/2306.05131](http://arxiv.org/abs/2306.05131)

    该论文提出了一种基于分位数回归的新型联邦合规性预测方法，该方法利用重要性加权有效地解决了代理之间的标签漂移问题，并为预测集的有效覆盖和差分隐私提供了理论保证。广泛的实验结果表明，该方法优于目前的竞争对手。

    

    联邦学习（FL）是一种机器学习框架，许多客户端协作训练模型，同时保持训练数据分散。尽管近年来在FL方面取得了进展，但未对不确定性量化主题（UQ）进行部分处理。在UQ方法中，合规性预测（CP）方法在最小假设下提供无分布保证。我们基于分位数回归开发了一种新的联邦合规性预测方法，并考虑了隐私约束。该方法利用重要性加权有效地解决了代理之间的标签漂移问题，并为预测集的有效覆盖和差分隐私提供了理论保证。广泛的实验证明，该方法优于目前的竞争对手。

    Federated Learning (FL) is a machine learning framework where many clients collaboratively train models while keeping the training data decentralized. Despite recent advances in FL, the uncertainty quantification topic (UQ) remains partially addressed. Among UQ methods, conformal prediction (CP) approaches provides distribution-free guarantees under minimal assumptions. We develop a new federated conformal prediction method based on quantile regression and take into account privacy constraints. This method takes advantage of importance weighting to effectively address the label shift between agents and provides theoretical guarantees for both valid coverage of the prediction sets and differential privacy. Extensive experimental studies demonstrate that this method outperforms current competitors.
    
[^169]: 可微的地球移动距离在高亮LHC数据压缩中的应用

    Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC. (arXiv:2306.04712v1 [hep-ex])

    [http://arxiv.org/abs/2306.04712](http://arxiv.org/abs/2306.04712)

    本文利用可微分的快速逼近方法，训练了一个编码器神经网络用于高亮LHC数据的压缩，同时保留了数据内与粒子探测器中的能量沉积分布相关的信息。

    

    地球移动距离(EMD)是图像识别和分类的有用指标，但其通常实现不可微分或过于缓慢，无法用作通过梯度下降训练其他算法的损失函数。本文训练了一个卷积神经网络(CNN)，学习了可微分的、快速的EMD的逼近方法，并证明它可以用作计算密集的EMD实现的替代品。我们将这种可微分的逼近方法应用于用于数据压缩的类自编码神经网络(encoder NN)的训练，这些数据来自欧洲核子研究组织的高亮LHC。编码器NN的目标是在保留与粒子探测器中的能量沉积分布相关的信息的同时压缩数据。我们证明，使用可微的EMD CNN训练的编码器NN的性能超越基于平均平方误差的损失函数的训练。

    The Earth mover's distance (EMD) is a useful metric for image recognition and classification, but its usual implementations are not differentiable or too slow to be used as a loss function for training other algorithms via gradient descent. In this paper, we train a convolutional neural network (CNN) to learn a differentiable, fast approximation of the EMD and demonstrate that it can be used as a substitute for computing-intensive EMD implementations. We apply this differentiable approximation in the training of an autoencoder-inspired neural network (encoder NN) for data compression at the high-luminosity LHC at CERN. The goal of this encoder NN is to compress the data while preserving the information related to the distribution of energy deposits in particle detectors. We demonstrate that the performance of our encoder NN trained using the differentiable EMD CNN surpasses that of training with loss functions based on mean squared error.
    
[^170]: 数据中动态偏移的状态规范化策略优化

    State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])

    [http://arxiv.org/abs/2306.03552](http://arxiv.org/abs/2306.03552)

    本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。

    

    在许多实际场景中，强化学习算法使用的数据受到动态偏移的影响，即具有不同的环境动态。目前的大多数方法通过训练上下文编码器来识别环境参数来解决这个问题。根据其环境参数将带有动态漂移的数据分开以训练相应的策略。然而，这些方法可能会出现样本效率低下的问题，因为数据是“特定场景”使用的，针对某个环境训练的策略不能从收集在其他具有不同动态的所有其他环境中的数据中受益。本文发现，在许多具有相似结构和不同动态的环境中，最优策略具有类似的稳态分布。我们利用这种特性，并从具有动态漂移的数据中学习稳态分布，以实现高效的数据重用。这种分布用于规范新环境中训练的策略，导致了 SRPO（状态规范化策略优化）算法的出现。实验结果表明，SRPO 在具有动态偏移的任务上显著优于现有的方法。

    In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
    
[^171]: 无独立性的泛化误差：去噪、线性回归和迁移学习

    Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning. (arXiv:2305.17297v1 [cs.LG])

    [http://arxiv.org/abs/2305.17297](http://arxiv.org/abs/2305.17297)

    本论文研究了具有低秩结构但非独立同分布数据的情况，在分离训练和测试分布的假设下，解决了分布偏移问题，实验结果表明，在分布偏移的情况下，本方法显著提高了泛化误差的性能。

    

    研究线性模型在真实数据中的泛化能力是统计学习中的一个核心问题。先前的一些重要工作验证了理论工作与真实数据的相关性，但这些工作由于技术假设存在限制，这些假设包括具有良好条件的协方差矩阵以及具有独立同分布数据，这些假设在真实数据中并不一定成立。此外，以前的一些关于分布偏移的工作通常对训练和测试数据的联合分布进行技术假设，并且不在真实数据上进行测试。为了解决这些问题并更好地对真实数据进行建模，我们研究了具有低秩结构但非独立同分布数据的情况，同时通过分离训练和测试分布的假设来解决分布偏移问题。我们还在这些松弛的假设下，研究了去噪问题、线性回归和迁移学习。我们的实验结果表明，相比以前的方法，在分布偏移的情况下，我们的方法显著提高了泛化误差的性能。

    Studying the generalization abilities of linear models with real data is a central question in statistical learning. While there exist a limited number of prior important works (Loureiro et al. (2021A, 2021B), Wei et al. 2022) that do validate theoretical work with real data, these works have limitations due to technical assumptions. These assumptions include having a well-conditioned covariance matrix and having independent and identically distributed data. These assumptions are not necessarily valid for real data. Additionally, prior works that do address distributional shifts usually make technical assumptions on the joint distribution of the train and test data (Tripuraneni et al. 2021, Wu and Xu 2020), and do not test on real data.  In an attempt to address these issues and better model real data, we look at data that is not I.I.D. but has a low-rank structure. Further, we address distributional shift by decoupling assumptions on the training and test distribution. We provide anal
    
[^172]: 条件分布之间的经验最优输运

    Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])

    [http://arxiv.org/abs/2305.15901](http://arxiv.org/abs/2305.15901)

    本文考虑在一个公共变量的条件下，相应分布之间的最优输运问题。通过采用基于 MMD 的核正则化器，克服了条件变量是连续的和两个分布中该变量的边缘是不同的挑战。

    

    给定两个联合分布的样本，考虑在一个公共变量的条件下，相应分布之间的最优输运问题。本文的目标是估计伴随条件值的输运成本（Wasserstein 距离），以及条件分布间的输运计划。由于匹配条件分布是监督训练判别模型和（隐式）条件生成模型的核心，条件分布之间的最优输运具有在不同的机器学习应用中被应用的潜力。然而，由于涉及到隐式特定于联合（样本）的条件分布，因此制定这个问题是具有挑战性的，特别是在（i）条件变量是连续的和（ii）两个分布中该变量的边缘是不同的情况下。我们通过采用特定的基于 MMD（最大均值差异）的核正则化器来克服这些挑战。

    Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
    
[^173]: 迈向预训练大型语言模型中稀疏前馈网络的统一视角

    Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13999](http://arxiv.org/abs/2305.13999)

    这项研究提出了一个统一的框架来分析稀疏前馈网络在预训练大型语言模型中的设计选择。在语言建模任务中，通过使用平均聚合隐藏状态的选择方法，相比现有的MoE架构，可以实现更低的困惑度。

    

    大型且稀疏的前馈层（S-FFN），如专家混合（MoE），已被证明在扩大Transformer模型规模以进行大型语言模型的预训练中非常有效。通过仅激活部分依赖于输入的FFN参数，S-FFN在保持训练和推理成本（以FLOPs计算）不变的同时提高了泛化性能。在本研究中，我们在稀疏神经记忆的通用概念框架下，分析了S-FFN的两个主要设计选择：内存块（即专家）大小和内存块选择方法。利用这个统一框架，我们比较了几种用于语言建模的S-FFN架构，并深入探讨了它们的相对效果和效率。我们发现一种更简单的选择方法 - Avg-K，通过均值聚合的隐藏状态来选择块，相比包括Switch Transformer（Fedus等，2021）和HashLaye在内的现有MoE架构，在语言模型预训练中实现了更低的困惑度。

    Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
    
[^174]: SMT 2.0：一个关注层次和混合变量高斯过程的代理模型工具包

    SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13998](http://arxiv.org/abs/2305.13998)

    SMT 2.0是一个开源的代理模型工具包，引入了处理混合变量和层次变量的能力，并通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。

    

    Surrogate Modeling Toolbox (SMT)是一个开源的Python包，提供了一系列代理建模方法、采样技术和一套示例问题。本文介绍了SMT 2.0，这是SMT的一个重要新版本，引入了显著的升级和新功能。这个版本增加了处理混合变量代理模型和层次变量的能力。这些类型的变量在多个代理建模应用中变得越来越重要。SMT 2.0还通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。这个版本还包括了处理带噪声和使用多保真度数据的新函数。据我们所知，SMT 2.0是第一个提出层次和混合输入的开源代理库。这个开源软件采用New BSD许可证进行分发。

    The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
    
[^175]: KineticNet: 深度学习可转移的轨道自由密度泛函的动能函数

    KineticNet: Deep learning a transferable kinetic energy functional for orbital-free density functional theory. (arXiv:2305.13316v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.13316](http://arxiv.org/abs/2305.13316)

    论文介绍了如何从Kohn-Sham密度泛函理论提供的真实数据中学习动能函数，并以此来促进轨道自由密度泛函理论的实际应用。

    

    轨道自由密度泛函理论（OF-DFT）有望以最小代价计算基态分子性质，但由于我们无法将动能计算为电子密度的函数而受到限制。因此，本论文介绍了如何从更昂贵的Kohn-Sham密度泛函理论提供的真实数据中学习动能函数，并以此来促进轨道自由密度泛函理论的实际运用。KineticNet成功地实现了此目的。

    Orbital-free density functional theory (OF-DFT) holds the promise to compute ground state molecular properties at minimal cost. However, it has been held back by our inability to compute the kinetic energy as a functional of the electron density only. We here set out to learn the kinetic energy functional from ground truth provided by the more expensive Kohn-Sham density functional theory. Such learning is confronted with two key challenges: Giving the model sufficient expressivity and spatial context while limiting the memory footprint to afford computations on a GPU; and creating a sufficiently broad distribution of training data to enable iterative density optimization even when starting from a poor initial guess. In response, we introduce KineticNet, an equivariant deep neural network architecture based on point convolutions adapted to the prediction of quantities on molecular quadrature grids. Important contributions include convolution filters with sufficient spatial resolution i
    
[^176]: GQA:从多头检查点训练广义多查询Transformer模型

    GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13245](http://arxiv.org/abs/2305.13245)

    该论文介绍了一种将现有的多头语言模型检查点升级为具有多查询注意力（MQA）的模型的方法，并引入了群组查询注意力（GQA）来解决MQA可能导致的质量下降问题。通过升级后的GQA模型，实现了接近多头注意力的质量，并具备与MQA相当的速度。

    

    多查询注意力（MQA）仅使用一个键值头，大大加快了解码器推理速度。然而，MQA可能导致质量下降，并且为了更快地推理而训练一个单独的模型可能不是理想的。我们（1）提出了一个方法，利用原始预训练计算量的5％，将现有的多头语言模型检查点升级为具有MQA的模型，并（2）引入了群组查询注意力（GQA），它是多查询注意力的广义形式，使用中间数量的键值头（多于一个，少于查询头的数量）。我们表明，经过升级的GQA实现了与多头注意力相当的速度，并且具有接近的质量。

    Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
    
[^177]: 论文标题：使ViT成形：计算-优化模型设计的缩放定律。

    Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.13035](http://arxiv.org/abs/2305.13035)

    本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。

    

    近期，缩放定律被用来推导在给定计算时间范围内的计算-优化模型大小（参数数量）。我们发展并改进了这些方法，以推测如宽度和深度等计算-优化模型形状，并在视觉变换器中成功实现了这一点。我们经过形状优化的视觉变换器SoViT，在仅使用相同数量的计算量进行预训练的情况下，取得了与超过其两倍大小的模型相竞争的结果。例如，SoViT-400m/14在ILSRCV2012上取得了90.3%的微调准确度，超过了更大的ViT-g/14，在相同设置下接近ViT-G/14，同时推断成本也不到一半。我们进行了多个任务的彻底评估，例如图像分类、字幕、VQA和零-shot转移，在广泛领域中展示了我们模型的有效性并确定了其限制。总体而言，我们的研究发现挑战了盲目扩大视觉模型的现有方法。

    Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
    
[^178]: 重新审视离线强化学习的极简方法

    Revisiting the Minimalist Approach to Offline Reinforcement Learning. (arXiv:2305.09836v1 [cs.LG])

    [http://arxiv.org/abs/2305.09836](http://arxiv.org/abs/2305.09836)

    这篇论文提出了一种名为ReBRAC的极简算法，它在TD3+BC方法的基础上整合了设计元素，通过对近期离线强化学习研究的回顾性分析，证明其在离线强化学习上的领先地位。

    

    近年来，离线强化学习取得了显着的进展，出现了许多具有不同复杂度的算法。虽然这些算法带来了显著的改进，但很多算法包含了看似微不足道的设计选择，这些选择对算法的有效性产生了影响，超出了核心算法的进步。然而，这些设计选择对于已有基线算法的影响尚未得到充分研究。在这项工作中，我们旨在通过对近期离线强化学习研究的回顾性分析，提出一种名为ReBRAC的极简算法，该算法在TD3+BC方法的基础上整合了这些设计元素。我们使用D4RL和V-D4RL基准测试评估了ReBRAC在51个具有自我感知和视觉状态空间的数据集上的性能，证明了其在不需要集成的方法中处于领先地位。为了进一步说明这些设计选择的有效性，我们进行了大规模消融研究和超参数敏感性分析，揭示了ReBRAC的成功源于其基于策略改进和评论家正则化的原则性设计选择。

    Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity anal
    
[^179]: 警惕扩散模型合成医学图像 -- 与 GAN 在记忆脑肿瘤图像方面的比较。

    Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images. (arXiv:2305.07644v1 [eess.IV])

    [http://arxiv.org/abs/2305.07644](http://arxiv.org/abs/2305.07644)

    扩散模型在医学图像合成中可能会导致记忆训练图像的问题，研究人员在选择合适的模型时需要谨慎。

    

    扩散模型最初是为文本到图像生成而开发的，现在也被用于生成高质量的合成图像。在 GAN 之前，扩散模型已经展示了令人印象深刻的结果，使用了各种评估指标。然而，常用的指标如 FID 和 IS 并不适合确定扩散模型是否只是复制了训练图像。这里我们使用 BRATS20 和 BRATS21 数据集训练 StyleGAN 和扩散模型，生成脑肿瘤图像，并测量合成图像与所有训练图像之间的相关性。我们的结果表明，扩散模型更有可能记忆训练图像，特别是对于小数据集。如果最终目标是共享合成的图像，研究人员在使用扩散模型进行医学成像时应该小心。

    Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21 datasets, to synthesize brain tumor images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are much more likely to memorize the training images, especially for small datasets. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthetic images.
    
[^180]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^181]: 用均场博弈为生成模型搭建实验室

    A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])

    [http://arxiv.org/abs/2304.13534](http://arxiv.org/abs/2304.13534)

    本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。

    

    本文展示了均场博弈 (MFGs) 作为一种数学框架用于解释、增强和设计生成模型的多功能性。我们建立了 MFGs 与主要流动和扩散型生成模型之间关联，并通过不同的粒子动力学和代价函数推导了这三个类别的生成模型。此外，我们通过研究它们相关的 MFG 的最优条件——一组耦合的非线性偏微分方程，来研究每个生成模型的数学结构和特性。本文还提出了一个新的基于双人 MFG 的生成模型，其中一个代理合成样本，另一个代理对样本进行识别，理论和实验结果表明，该模型生成的样本多样且逼真，同时与基准模型相比，改善了解缠结和公平性。总之，本文突显了 MFGs 作为设计和分析生成模型的实验室的潜力。

    In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
    
[^182]: GPS轨迹生成的扩散模型

    Diffusion Model for GPS Trajectory Generation. (arXiv:2304.11582v1 [cs.LG])

    [http://arxiv.org/abs/2304.11582](http://arxiv.org/abs/2304.11582)

    该论文提出了基于扩散模型的GPS轨迹生成框架，通过将真实轨迹逐渐转换为噪声，再从噪声重构伪造的轨迹，以达到生成隐私信息保护的高质量轨迹的目的。

    

    随着GPS设备和数据采集技术的部署，大量的GPS轨迹数据为推进时空数据挖掘研究提供了核心支持。但是，GPS轨迹包括个人地理位置信息，因此不可避免地涉及到隐私问题。解决此问题的一种有前途的方法是使用轨迹生成，用生成的无隐私信息替换原始数据。然而，由于人类活动的复杂和随机行为，生成高质量的轨迹仍处于初级阶段。为了实现这一目标，我们提出了一个基于扩散的轨迹生成（Diff-Traj）框架，有效地将扩散模型的生成能力与轨迹的时空特征学习相结合。具体地，我们通过前向轨迹噪声处理逐渐将真实轨迹转换为噪声。然后，Diff-Traj从噪声重构伪造的轨迹。

    With the deployment of GPS-enabled devices and data acquisition technology, the massively generated GPS trajectory data provide a core support for advancing spatial-temporal data mining research. Nonetheless, GPS trajectories comprise personal geo-location information, rendering inevitable privacy concerns on plain data. One promising solution to this problem is trajectory generation, replacing the original data with the generated privacy-free ones. However, owing to the complex and stochastic behavior of human activities, generating high-quality trajectories is still in its infancy. To achieve the objective, we propose a diffusion-based trajectory generation (Diff-Traj) framework, effectively integrating the generation capability of the diffusion model and learning from the spatial-temporal features of trajectories. Specifically, we gradually convert real trajectories to noise through a forward trajectory noising process. Then, Diff-Traj reconstructs forged trajectories from the noise
    
[^183]: 多注释深度学习：分类问题的概率框架

    Multi-annotator Deep Learning: A Probabilistic Framework for Classification. (arXiv:2304.02539v1 [cs.LG])

    [http://arxiv.org/abs/2304.02539](http://arxiv.org/abs/2304.02539)

    该论文提出了一个名为多注释深度学习（MaDL）的概率训练框架，可以在由易错注释者提供的有噪音类别标签上进行端到端的联合训练，并有效地解决多注释监督学习中的次优表现问题。

    

    使用深度神经网络解决复杂分类任务通常需要大量注释数据，然而由易错注释者（如众包工人）提供的对应类别标签有噪音。在这样的多注释监督学习中，训练标准的深度神经网络会导致次优表现。我们通过提出名为多注释深度学习（MaDL）的概率训练框架来解决这个问题。在端到端的学习方法中，联合训练一个地面真相模型和一个注释者表现模型。地面真相模型学习预测实例的真实类别，而注释者表现模型推断注释者表现的概率估计。模块化的网络结构使我们能够对注释者的表现做出不同的假设，例如，可以考虑类别或实例依赖性。此外，我们学习注释者嵌入以估计潜在空间中注释者的密度作为迁移学习的代理。我们的方法不仅使我们能够估计真实的类别标签，而且能够估计所分配标签的可信度。在合成数据和真实数据上的实验表明，在注释者性能假设不同的情况下，我们的方法都具有有效性。

    Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of t
    
[^184]: DeforestVis：使用代理决策树进行机器学习模型行为分析

    DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])

    [http://arxiv.org/abs/2304.00133](http://arxiv.org/abs/2304.00133)

    DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。

    

    随着机器学习（ML）模型的复杂性增加以及不同（和关键）领域中的应用增加，越来越需要更易解释和可信赖的ML。解释复杂ML模型的一种简单且与模型无关的方法是训练代理模型（例如规则集和决策树），以足够接近原始模型，但更简单和易于解释。然而，规则集可以变得非常冗长，包含许多if-else语句，而决策树的深度会随着准确模拟复杂ML模型而迅速增加。在这种情况下，两种方法都可能无法实现其核心目标，提供用户模型的可解释性。我们通过提出DeforestVis解决了这个问题，这是一种可视化分析工具，通过提供使用自适应增强（AdaBoost）技术生成的代理决策树（一级决策树），为用户提供了对复杂ML模型行为的友好总结。我们的解决方案帮助用户探索模型的复杂性。

    As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
    
[^185]: 通过排序促进非合作行为

    Promoting Non-Cooperation Through Ordering. (arXiv:2303.17971v1 [cs.MA])

    [http://arxiv.org/abs/2303.17971](http://arxiv.org/abs/2303.17971)

    本文研究了如何通过排序方式，在处罚的个人中激励非合作行为，对于中央管理部门能够显著增加总付款。

    

    在很多现实情况下，例如大城市中的小交通违规事件，中央管理部门需要定期对大量个人进行惩罚。通常的做法是给每个个人一个机会，承担一小笔罚款，并保证免除可能会面临的较大处罚和法律程序。然而，由于违法者数量众多，中央管理部门能力有限，个人面临的风险通常很小，理性的个人将选择不支付罚款。本文展示了，如果中央管理部门按照已知的公开顺序处理违法者，就能适当地激励违法者支付罚款。我们在实验和理论分析中显示，我们的机制促进了非合作行为，激励个人支付罚款。而且，对于任意联盟也是适用的。我们量化了中央管理部门收到的预期总付款，并显示其显著增加。

    In many real world situations, like minor traffic offenses in big cities, a central authority is tasked with periodic administering punishments to a large number of individuals. Common practice is to give each individual a chance to suffer a smaller fine and be guaranteed to avoid the legal process with probable considerably larger punishment. However, thanks to the large number of offenders and a limited capacity of the central authority, the individual risk is typically small and a rational individual will not choose to pay the fine. Here we show that if the central authority processes the offenders in a publicly known order, it properly incentives the offenders to pay the fine. We show analytically and on realistic experiments that our mechanism promotes non-cooperation and incentives individuals to pay. Moreover, the same holds for an arbitrary coalition. We quantify the expected total payment the central authority receives, and show it increases considerably.
    
[^186]: CoLT5: 基于条件计算的快速长距离Transformer模型

    CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])

    [http://arxiv.org/abs/2303.09752](http://arxiv.org/abs/2303.09752)

    CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。

    

    许多自然语言处理任务需要处理长输入，但使用Transformer处理长文档很昂贵——这不仅是因为二次注意复杂性，还因为对每个标记应用前馈和投影层。然而，不是所有标记都同样重要，特别是对于较长的文档。我们提出了CoLT5，一种长输入Transformer模型，通过使用条件计算来利用此直觉，在前馈和注意层中为重要标记提供更多资源。我们展示了CoLT5比LongT5表现更强，训练和推理速度更快，在长输入SCROLLS基准测试上达到了SOTA。此外，CoLT5能够有效且可控地利用极长的输入，展示了高达64k输入长度的强大增益。

    Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
    
[^187]: CoSyn：使用上下文协同的双曲线网络检测在线对话中的隐含仇恨言论

    CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03387](http://arxiv.org/abs/2303.03387)

    CoSyn是一个上下文协同的神经网络，用于检测在线对话中的隐含仇恨言论。它通过引入新的编码方法和上下文交互机制，在双曲空间中进行操作，以适应社交媒体的特点。

    

    在线对话中隐含的仇恨言论对来自各个群体的人们产生了重要影响，因此社交媒体用户越来越多。大部分之前的研究都集中于检测明确的仇恨言论，这些言论明显且利用了仇恨短语，对于检测隐含或通过间接或编码语言表达出的仇恨言论的研究很少。在本文中，我们提出了CoSyn，一个上下文协同的神经网络，明确地结合了用户和对话上下文来检测在线对话中的隐含仇恨言论。CoSyn引入了新的方法来编码这些外部上下文，并采用了一种新颖的上下文交互机制，清晰地捕捉了它们之间的相互作用，独立评估了从这些嘈杂的上下文中检索的信息量。此外，它在双曲空间中进行所有这些操作，以适应社交媒体的无标度动态。

    The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech, affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context-synergized neural network that explicitly incorporates user- and conversational context for detecting implicit hate speech in online conversations. CoSyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. Additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. We de
    
[^188]: 深度神经网络早期训练动力学的相图：学习率、深度和宽度的影响

    Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width. (arXiv:2302.12250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12250](http://arxiv.org/abs/2302.12250)

    我们研究了深度神经网络早期训练动力学的相图，发现四种不同的状态，并发现了一个“陡峭度减小”相位的出现。

    

    我们系统地分析用随机梯度下降（SGD）训练的深度神经网络（DNN）中的优化动力学，并研究学习率 $\eta$、深度 $d$ 和宽度 $w$ 的神经网络的影响。通过分析损失函数的Hessian矩阵的最大特征值 $\lambda^H_t$，即损失函数景观的陡峭程度的衡量，我们发现动力学可以展示出四种不同的状态：（i）早期临时状态，（ii）中间饱和状态，（iii）逐渐锐化状态和（iv）后期“稳定边缘”状态。早期和中间状态（i）和（ii）呈现出丰富的相图，取决于 $\eta \equiv c / \lambda_0^H $、$d$ 和 $w$。我们确定了几个临界值 $c$，它们在训练损失和陡峭度的早期动力学中分隔出不同的现象。值得注意的是，我们发现了一个“陡峭度减小”相位的出现，其中陡峭度在早期时间下降，当 $d$ 和 $1/w$ 增加时。

    We systematically analyze optimization dynamics in deep neural networks (DNNs) trained with stochastic gradient descent (SGD) and study the effect of learning rate $\eta$, depth $d$, and width $w$ of the neural network. By analyzing the maximum eigenvalue $\lambda^H_t$ of the Hessian of the loss, which is a measure of sharpness of the loss landscape, we find that the dynamics can show four distinct regimes: (i) an early time transient regime, (ii) an intermediate saturation regime, (iii) a progressive sharpening regime, and (iv) a late time ``edge of stability" regime. The early and intermediate regimes (i) and (ii) exhibit a rich phase diagram depending on $\eta \equiv c / \lambda_0^H $, $d$, and $w$. We identify several critical values of $c$, which separate qualitatively distinct phenomena in the early time dynamics of training loss and sharpness. Notably, we discover the opening up of a ``sharpness reduction" phase, where sharpness decreases at early times, as $d$ and $1/w$ are inc
    
[^189]: 使用统一的E-值通过FDR控制去随机化的新颖性检测

    Derandomized Novelty Detection with FDR Control via Conformal E-values. (arXiv:2302.07294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07294](http://arxiv.org/abs/2302.07294)

    通过使用统一E-值来量化统计显著性，我们提出了一种去随机化的新颖性检测方法，该方法可以稳定地聚合相同数据的多次分析的证据，同时控制虚假发现率。

    

    统一推理提供了一种通用的无分布方法，用于严格校准任何机器学习算法的新颖性检测输出。虽然这种方法有很多优点，但它也有随机性的限制，即在分析相同数据时可能会导致不同的结果，这可能会妨碍对任何发现的解释。我们提出通过利用适当的统一E-值而不是p-值来量化统计显著性，使统一推理更加稳定。这种解决方案允许有效地汇总对相同数据多次分析的证据，同时可靠地控制虚假发现率。此外，我们还展示了与标准统一推理相比，所提方法可以减少随机性而不会损失太多功率，部分原因是基于从相同数据中精心提取的附加辅助信息来加权统一E-值的创新方法。通过合成数据的模拟实验表明...

    Conformal inference provides a general distribution-free method to rigorously calibrate the output of any machine learning algorithm for novelty detection. While this approach has many strengths, it has the limitation of being randomized, in the sense that it may lead to different results when analyzing twice the same data, and this can hinder the interpretation of any findings. We propose to make conformal inferences more stable by leveraging suitable conformal e-values instead of p-values to quantify statistical significance. This solution allows the evidence gathered from multiple analyses of the same data to be aggregated effectively while provably controlling the false discovery rate. Further, we show that the proposed method can reduce randomness without much loss of power compared to standard conformal inference, partly thanks to an innovative way of weighting conformal e-values based on additional side information carefully extracted from the same data. Simulations with synthet
    
[^190]: Proximal Newton算法实现高效图拉普拉斯估计

    Efficient Graph Laplacian Estimation by Proximal Newton. (arXiv:2302.06434v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06434](http://arxiv.org/abs/2302.06434)

    本文提出了一种基于Proximal Newton算法的高效图拉普拉斯估计方法，通过引入非凸minimax concave penalty，并利用二阶优化方法和几个算法技巧，实现了准确且高效的求解器。

    

    Laplacian约束的高斯马尔科夫随机场（LGMRF）是一种常见的多元统计模型，用于从给定数据中学习权重稀疏的依赖图。这个图学习问题可以被形式化为最大似然估计（MLE）的精度矩阵，受到Laplacian结构约束的限制，并带有稀疏性诱导的惩罚项。本论文旨在准确且高效地解决这个学习问题。首先，由于在这个设置中通常使用的$\ell_1$-范数惩罚不适用并且可能导致完全图，所以我们采用了非凸的MCP（minimax concave penalty），它促进具有更低估计偏差的稀疏解。其次，与现有的该问题的一阶方法相反，我们开发了一种二阶proximal Newton方法来获得高效的求解器，利用了多种算法特性，如使用共轭梯度法、预条件化和分割到活动/自由集。数值实验证明了该算法的优点。

    The Laplacian-constrained Gaussian Markov Random Field (LGMRF) is a common multivariate statistical model for learning a weighted sparse dependency graph from given data. This graph learning problem can be formulated as a maximum likelihood estimation (MLE) of the precision matrix, subject to Laplacian structural constraints, with a sparsity-inducing penalty term. This paper aims to solve this learning problem accurately and efficiently. First, since the commonly used $\ell_1$-norm penalty is inappropriate in this setting and may lead to a complete graph, we employ the nonconvex minimax concave penalty (MCP), which promotes sparse solutions with lower estimation bias. Second, as opposed to existing first-order methods for this problem, we develop a second-order proximal Newton approach to obtain an efficient solver, utilizing several algorithmic features, such as using Conjugate Gradients, preconditioning, and splitting to active/free sets. Numerical experiments demonstrate the advanta
    
[^191]: 图神经网络中的泛化：基于图扩散的改进PAC-Bayesian界限。

    Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion. (arXiv:2302.04451v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04451](http://arxiv.org/abs/2302.04451)

    本文提出了用特征扩散矩阵的最大奇异值来缩放泛化界限的方法，并用Hessians来衡量图神经网络对噪声扰动的稳定性。实验证明，这些方法可以有效减小泛化界限，更好地解决了实际图形问题。

    

    图神经网络是图预测任务中广泛使用的工具。由其实证表现所驱动，先前的研究开发了图神经网络的泛化界限，它们根据最大度数在图结构方面进行缩放。在本文中，我们提出了泛化界限，这些界限根据图神经网络特征扩散矩阵的最大奇异值进行缩放。对于实际图形，这些界限的数值要比先前的界限小得多。我们还构建了一个相符的泛化差距下限，其渐近地匹配了我们的上限界限。为了实现这些结果，我们分析了一个统一的模型，其中包括先前的设置（即卷积和消息传递网络）和新的设置（即图同构网络）。我们的关键思想是利用Hessians来衡量图神经网络对于噪声扰动的稳定性。实验证明，基于Hessian的测量与观察到的泛化差距相关。

    Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps
    
[^192]: 深度感知损失网络的系统性能分析：打破迁移学习的约定

    A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions. (arXiv:2302.04032v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04032](http://arxiv.org/abs/2302.04032)

    这项工作通过系统评估多种常用的预训练网络及其不同特征提取点，在四个深度感知损失用例上解决了迁移学习中的问题。

    

    深度感知损失是一种在计算机视觉中使用的损失函数，旨在通过使用从神经网络中提取的深度特征来模仿人类感知。近年来，该方法在许多有趣的计算机视觉任务上取得了显著的效果，特别是对于具有图像或类似图像输出的任务，如图像合成、分割、深度预测等。许多应用程序使用预先训练的网络，通常是卷积网络，用于损失计算。尽管对该方法的兴趣和广泛使用增加了，但仍需要更多的努力来探索用于计算深度感知损失的网络以及从哪些层提取特征。本研究旨在通过系统地评估多种常用且易于获取的预训练网络，以及针对四个现有深度感知损失用例的不同特征提取点来纠正这一问题。

    Deep perceptual loss is a type of loss function in computer vision that aims to mimic human perception by using the deep features extracted from neural networks. In recent years, the method has been applied to great effect on a host of interesting computer vision tasks, especially for tasks with image or image-like outputs, such as image synthesis, segmentation, depth prediction, and more. Many applications of the method use pretrained networks, often convolutional networks, for loss calculation. Despite the increased interest and broader use, more effort is needed toward exploring which networks to use for calculating deep perceptual loss and from which layers to extract the features.  This work aims to rectify this by systematically evaluating a host of commonly used and readily available, pretrained networks for a number of different feature extraction points on four existing use cases of deep perceptual loss. The use cases of perceptual similarity, super-resolution, image segmentat
    
[^193]: 通过重要性重采样进行语言模型的数据选择

    Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03169](http://arxiv.org/abs/2302.03169)

    通过重要性重采样方法，我们提出了一种高效且可扩展的数据选择框架（DSIR），可以在语言模型中选择适合的预训练数据集。我们使用KL减少作为数据度量来确定合适的特征空间，并在降维特征空间中估计重要性权重以进行数据选择。

    

    选择适合的预训练数据集对于通用领域（如GPT-3）和特定领域（如Codex）的语言模型（LM）都至关重要。我们将这个问题形式化为从大型原始无标签数据集中选择一个子集，以匹配给定一些无标签目标样本的所需目标分布。鉴于原始文本数据的大规模和高维度，现有方法使用简单的启发式方法或专家手动策划数据。相反，我们扩展了在LM数据选择中使用的经典重要性重采样方法，以低维空间进行数据选择。我们提出了一种高效且可扩展的框架，称为数据选择与重要性重采样（DSIR），它在一个降维特征空间中估计重要性权重，以便根据这些权重进行重要性重采样数据选择。为了确定一个合适的特征空间，我们还展示了KL减少，一种在特征空间中衡量所选预训练数据与目标之间相似度的数据度量，具有较高的相关性。

    Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlat
    
[^194]: NA-SODINN:一种基于残余噪声模式的深度学习算法用于外行星图像检测

    NA-SODINN: a deep learning algorithm for exoplanet image detection based on residual noise regimes. (arXiv:2302.02854v2 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2302.02854](http://arxiv.org/abs/2302.02854)

    NA-SODINN是一种基于残余噪声模式的深度学习算法，通过识别噪声相关性来提高外行星图像检测性能。

    

    最近，通过SODINN算法介绍了监督式深度学习在高对比度成像（HCI）中的应用，该算法是为角差分成像（ADI）数据集中的外行星检测而设计的卷积神经网络。在外行星成像数据挑战（EIDC）中进行的HCI算法基准测试显示，（i）SODINN在最终检测图中可能产生大量的误报，（ii）以更局部的方式处理图像的算法表现更好。本文旨在通过引入新的局部处理方法并相应地调整学习过程来提高SODINN的检测性能。我们提出了一种新的基于卷积神经网络（CNN）的深度学习二分类器NA-SODINN，通过识别噪声模式更好地捕捉ADI处理帧中的图像噪声相关性。我们的新方法通过局部接收函数测试了其前任模型以及基于SODINN的两种混合模型和更标准的环形PCA方法。

    Supervised deep learning was recently introduced in high-contrast imaging (HCI) through the SODINN algorithm, a convolutional neural network designed for exoplanet detection in angular differential imaging (ADI) datasets. The benchmarking of HCI algorithms within the Exoplanet Imaging Data Challenge (EIDC) showed that (i) SODINN can produce a high number of false positives in the final detection maps, and (ii) algorithms processing images in a more local manner perform better. This work aims to improve the SODINN detection performance by introducing new local processing approaches and adapting its learning process accordingly. We propose NA-SODINN, a new deep learning binary classifier based on a convolutional neural network (CNN) that better captures image noise correlations in ADI-processed frames by identifying noise regimes. Our new approach was tested against its predecessor, as well as two SODINN-based hybrid models and a more standard annular-PCA approach, through local receivin
    
[^195]: 重新思考半监督医学图像分割：方差缩减的视角

    Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01735](http://arxiv.org/abs/2302.01735)

    本文提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。

    This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.

    对于医学图像分割，对比学习是提高视觉表示质量的主要方法，通过对比语义相似和不相似的样本对来实现。这是通过观察到，在没有访问地面真实标签的情况下，如果采样具有真正不同解剖特征的负样本，则可以显着提高性能。然而，在现实中，这些样本可能来自相似的解剖特征，模型可能难以区分少数尾类样本，使得尾类更容易被错误分类，这通常导致模型崩溃。在本文中，我们提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。特别是，我们首先提出通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。

    For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
    
[^196]: 通过上下文修剪元学习学习大规模神经场

    Learning Large-scale Neural Fields via Context Pruned Meta-Learning. (arXiv:2302.00617v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00617](http://arxiv.org/abs/2302.00617)

    通过上下文修剪元学习实现大规模神经场训练的优化， 显著节省内存，并能在短时间内学习高质量神经场。

    

    我们通过自动在线上下文点选择实现了大规模神经场训练的高效优化元学习技术，从而实现显著的内存节省。通过将每个学习步骤集中在具有最高期望立即模型质量改进的数据子集上，实现全局结构的几乎即时建模和高频细节的后续细化。我们通过引入引导校正，进一步提高了元学习初始化的质量，从而实现了在减少上下文集时引入的任何误差的最小化，并同时缓解了基于优化的元学习所带来的短视问题。最后，我们展示了如何在元测试时进行梯度重新缩放，从而在显著缩短优化过程的同时学习极高质量的神经场。我们的框架与模型无关，直观易懂，易于实现，并表现出显著的重构能力。

    We introduce an efficient optimization-based meta-learning technique for large-scale neural field training by realizing significant memory savings through automated online context point selection. This is achieved by focusing each learning step on the subset of data with the highest expected immediate improvement in model quality, resulting in the almost instantaneous modeling of global structure and subsequent refinement of high-frequency details. We further improve the quality of our meta-learned initialization by introducing a bootstrap correction resulting in the minimization of any error introduced by reduced context sets while simultaneously mitigating the well-known myopia of optimization-based meta-learning. Finally, we show how gradient re-scaling at meta-test time allows the learning of extremely high-quality neural fields in significantly shortened optimization procedures. Our framework is model-agnostic, intuitive, straightforward to implement, and shows significant reconst
    
[^197]: 图神经网络的零一定律

    Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13060](http://arxiv.org/abs/2301.13060)

    本文提出了一个新的理论研究视角，回答了当图节点数量变得非常大时GNN的行为如何的问题。通过证明不断增大的图映射到GNN分类器的特定输出的概率趋于零或一，建立了这些GNN的零一定律，限制了它们的能力。实验证实了理论结论。

    

    图神经网络(GNN)是用于对图进行机器学习的深度学习标准体系结构。这导致了大量的工作分析这些模型的能力和限制，特别是他们的表示和外推能力。我们提供了一个新的理论视角，回答了一个问题：当图节点的数量变得非常大时，GNNs的行为如何？在温和的假设下，我们证明，当我们从Erdős-Rényi模型中绘制不断增大的图时，这些图映射到GNN分类器的特定输出的概率趋于零或一。这个类包括流行的图卷积网络体系结构。这个结果建立了这些GNN的零一定律，并且类比于其他收敛定律，带来了它们在理论上的局限性。我们通过实验证实了我们的结果，观察到理论与实践相符。

    Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret
    
[^198]: Mo^usai: 使用长上下文潜在扩散进行文本到音乐生成

    Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11757](http://arxiv.org/abs/2301.11757)

    本研究开发了一种高效、表现力强且能处理长期结构的文本到音乐生成模型Mo^usai，可以根据文本描述生成多分钟高质量音乐。通过实验证明了该模型在多个标准上的优势。

    

    近年来，大规模生成模型在文本领域取得了快速发展；然而，对于文本与另一种“语言”——音乐的关联关系，研究相对较少。音乐与文本一样，可以传达情感、故事和思想，具有自己独特的结构和语法。我们的工作通过一种高效、表现力强且能够处理长期结构的文本到音乐生成模型，将文本与音乐联系起来。具体而言，我们开发了Mo^usai，这是一个级联的两阶段潜在扩散模型，可以根据文本描述生成多分钟的高质量48kHz立体声音乐。此外，我们的模型具有高效性，可以在单个消费级GPU上进行实时推断，并具有合理的速度。通过实验证明了我们模型在多种标准下相对于现有音乐生成模型的优势。最后，为了推动开源文化，我们提供了一套开源的工具集。

    Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another "language" of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source
    
[^199]: 图神经网络可以仅仅从图结构中恢复出隐藏的特征。

    Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure. (arXiv:2301.10956v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10956](http://arxiv.org/abs/2301.10956)

    本文研究了图神经网络是否可以利用图结构从而实现对潜在特征的恢复，并表明GNN可以完全利用图结构。

    

    图神经网络(GNN)是处理图学习问题的流行模型，它在许多实际任务中表现出强大的实证性能。但是，其理论性质尚未完全阐明。在本文中，我们从GNN的表达能力角度研究了GNN是否可以利用图结构。在我们的分析中，我们考虑受隐藏（或潜在）节点特征控制的图生成过程，这些特征包含有关图结构的所有信息。这种框架的典型示例是从隐藏特征构建的kNN图。在我们的主要结果中，我们表明GNN可以仅从输入图中恢复出隐藏节点特征，即使所有节点特征，包括隐藏特征本身和任何间接提示都不可用。GNN可以进一步利用恢复的节点特征进行下游任务。这些结果表明，GNN可以完全利用图结构，从而可以使用图结构自身实现。

    Graph Neural Networks (GNNs) are popular models for graph learning problems. GNNs show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether GNNs can exploit the graph structure from the perspective of the expressive power of GNNs. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is kNN graphs constructed from the hidden features. In our main results, we show that GNNs can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable. GNNs can further use the recovered node features for downstream tasks. These results show that GNNs can fully exploit the graph structure by themselves, and in effect, GNNs can use bot
    
[^200]: 自适应联邦式最小最大优化及其较低复杂度

    Adaptive Federated Minimax Optimization with Lower complexities. (arXiv:2211.07303v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07303](http://arxiv.org/abs/2211.07303)

    本文提出了一种自适应联邦式最小最大优化算法（AdaFGDA），用于解决分布式最小最大问题，在梯度和通信复杂性较低的情况下取得了良好的效果。

    

    联邦学习是一种流行的分布式和隐私保护的机器学习范例。同时，作为一种有效的层次优化方法，最小最大优化广泛应用于机器学习中。最近，已经提出了一些联邦式最小最大优化方法来解决分布式最小最大问题。然而，这些联邦式最小最大方法仍然受到梯度和通信复杂性高的问题的困扰。同时，很少有算法专注于使用自适应学习率来加速算法。为了填补这一空白，在本文中，我们研究了一类非凸最小最大优化问题，并提出了一种高效的自适应联邦式最小最大优化算法（即AdaFGDA）来解决这些分布式最小最大问题。具体而言，我们的AdaFGDA建立在基于动量的方差减少和局部SGD技术的基础上，通过使用统一的自适应矩阵可以灵活地结合各种自适应学习率。从理论上讲，我们提供了一个坚实的收敛性分析框架……

    Federated learning is a popular distributed and privacy-preserving machine learning paradigm. Meanwhile, minimax optimization, as an effective hierarchical optimization, is widely applied in machine learning. Recently, some federated optimization methods have been proposed to solve the distributed minimax problems. However, these federated minimax methods still suffer from high gradient and communication complexities. Meanwhile, few algorithm focuses on using adaptive learning rate to accelerate algorithms. To fill this gap, in the paper, we study a class of nonconvex minimax optimization, and propose an efficient adaptive federated minimax optimization algorithm (i.e., AdaFGDA) to solve these distributed minimax problems. Specifically, our AdaFGDA builds on the momentum-based variance reduced and local-SGD techniques, and it can flexibly incorporate various adaptive learning rates by using the unified adaptive matrix. Theoretically, we provide a solid convergence analysis framework fo
    
[^201]: 解决极度不平衡光伏电池模块细胞图像中的缺陷分割输出不平衡问题

    Harmonizing output imbalance for defect segmentation on extremely-imbalanced photovoltaic module cells images. (arXiv:2211.05295v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05295](http://arxiv.org/abs/2211.05295)

    本文提出了解决光伏电池模块细胞图像中THC缺陷分割输出不平衡的方法，包括明确的输出不平衡度量方法、基于分布的损失函数的推广以处理不同类型的输出不平衡，以及复合损失函数和自适应超参数选择算法的引入以实现输出不平衡在极度不平衡输入数据上的协调。

    

    光伏（PV）行业的持续发展对单晶光伏模块细胞的质量提出了高要求。在学习光伏模块细胞图像中的缺陷区域分割时，微小隐藏裂缝（THC）导致极度不平衡的样本。缺陷像素与正常像素的比例可以低至1：2000。这种极端不平衡使得难以分割光伏模块细胞的THC，也是语义分割的一个挑战。为了解决在极度不平衡的THC数据上进行缺陷分割的问题，本文从三个方面做出了贡献：（1）提出了一个明确的输出不平衡度量方法；（2）推广了一种基于分布的损失函数，可以处理不同类型的输出不平衡；（3）引入了一种复合损失函数和自适应超参数选择算法，可以保持训练和推断的一致性，以实现在极度不平衡输入数据上的输出不平衡的协调。所提出的方法

    The continuous development of the photovoltaic (PV) industry has raised high requirements for the quality of monocrystalline of PV module cells. When learning to segment defect regions in PV module cell images, Tiny Hidden Cracks (THC) lead to extremely-imbalanced samples. The ratio of defect pixels to normal pixels can be as low as 1:2000. This extreme imbalance makes it difficult to segment the THC of PV module cells, which is also a challenge for semantic segmentation. To address the problem of segmenting defects on extremely-imbalanced THC data, the paper makes contributions from three aspects: (1) it proposes an explicit measure for output imbalance; (2) it generalizes a distribution-based loss that can handle different types of output imbalances; and (3) it introduces a compound loss with our adaptive hyperparameter selection algorithm that can keep the consistency of training and inference for harmonizing the output imbalance on extremelyimbalanced input data. The proposed metho
    
[^202]: 基于内容的深层生成模型搜索

    Content-Based Search for Deep Generative Models. (arXiv:2210.03116v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.03116](http://arxiv.org/abs/2210.03116)

    这个论文介绍了基于内容的深层生成模型搜索任务，通过优化问题选择生成与查询最相似内容概率最高的模型，并提出了适用于不同查询模态的对比学习框架。（翻译为中文）

    

    自定义和预训练生成模型的不断增加使得用户不可能完全了解每个存在的模型。为了解决这个问题，我们引入了基于内容的模型搜索任务：给定一个查询和一组大规模的生成模型，找到与查询最匹配的模型。由于每个生成模型产生一系列图像的分布，我们将搜索任务作为一个优化问题，选择生成与查询相似内容概率最高的模型。我们提出了一个用于近似计算概率的公式，可以根据不同的查询模态（例如图像、草图和文本）来计算。此外，我们提出了一个对模型检索的对比学习框架，该框架学习适应不同查询模态的特征。我们证明了我们的方法在生成模型动物园（Generative Model Zoo）上优于几个基准模型的表现。

    The growing proliferation of customized and pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, finding the models that best match the query. As each generative model produces a distribution of images, we formulate the search task as an optimization problem to select the model with the highest probability of generating similar content as the query. We introduce a formulation to approximate this probability given the query from different modalities, e.g., image, sketch, and text. Furthermore, we propose a contrastive learning framework for model retrieval, which learns to adapt features for various query modalities. We demonstrate that our method outperforms several baselines on Generative Model Zoo, a new benchmark we create for the model retrieval task.
    
[^203]: Spectral2Spectral: 无参考的图像光谱相似性辅助光谱CT深度重建

    Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep Reconstruction without Reference. (arXiv:2210.01125v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.01125](http://arxiv.org/abs/2210.01125)

    本文提出了一种名为Spectral2Spectral的深度重建网络，它利用图像-光谱领域内的相似性先验通过无参考方式来辅助光谱CT的深度重建。

    

    基于光子计数探测器（PCD）的光谱计算机断层扫描（CT）吸引了越来越多的关注，因为它能够为生物医学材料提供更准确的识别和定量分析。狭窄能量区间内有限的光子数量导致图像结果的信噪比较低。针对这些挑战，现有的CT重建的监督式深度重建网络很难解决，因为通常无法获取带有清晰结构的无噪声临床图像作为参考。本文提出了一种迭代深度重建网络，将无监督方法和数据先验相结合，命名为Spectral2Spectral。我们的Spectral2Spectral采用无监督的深度训练策略，在端到端的方式中从噪声数据中获得高质量图像。图像-光谱领域内的结构相似性先验被改进为正则化项，进一步约束网络。

    Spectral computed tomography based on a photon-counting detector (PCD) attracts more and more attentions since it has the capability to provide more accurate identification and quantitative analysis for biomedical materials. The limited number of photons within narrow energy bins leads to imaging results of low signal-noise ratio. The existing supervised deep reconstruction networks for CT reconstruction are difficult to address these challenges because it is usually impossible to acquire noise-free clinical images with clear structures as references. In this paper, we propose an iterative deep reconstruction network to synergize unsupervised method and data priors into a unified framework, named as Spectral2Spectral. Our Spectral2Spectral employs an unsupervised deep training strategy to obtain high-quality images from noisy data in an end-to-end fashion. The structural similarity prior within image-spectral domain is refined as a regularization term to further constrain the network t
    
[^204]: 分期变分推断：一项系统综述

    Amortized Variational Inference: A Systematic Review. (arXiv:2209.10888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10888](http://arxiv.org/abs/2209.10888)

    分期变分推断（Amortized Variational Inference）是一种高效且可扩展的变分推断方法，通过利用参数化函数学习近似后验概率密度参数，解决了传统VI算法在处理大规模数据集和推断出界数据点方面的问题。

    

    变分推断（VI）的核心原则是将计算复杂后验概率密度的统计推断问题转化为可处理的优化问题。这个特性使得VI比几种基于采样的技术更快。然而，传统的VI算法在处理大规模数据集时不具有可扩展性，并且无法直接推断出界数据点而不重新运行优化过程。近年来，如随机、黑盒和分期VI等领域的发展已经帮助解决了这些问题。现在，生成建模任务广泛使用分期VI，因为它利用参数化函数来学习近似后验概率密度参数，具有高效性和可扩展性。在本文中，我们回顾了各种VI技术的数学基础，为理解分期VI提供了基础。此外，我们概述了解决分期VI若干问题的最新趋势，如

    The core principle of Variational Inference (VI) is to convert the statistical inference problem of computing complex posterior probability densities into a tractable optimization problem. This property enables VI to be faster than several sampling-based techniques. However, the traditional VI algorithm is not scalable to large data sets and is unable to readily infer out-of-bounds data points without re-running the optimization process. Recent developments in the field, like stochastic-, black box-, and amortized-VI, have helped address these issues. Generative modeling tasks nowadays widely make use of amortized VI for its efficiency and scalability, as it utilizes a parameterized function to learn the approximate posterior density parameters. In this paper, we review the mathematical foundations of various VI techniques to form the basis for understanding amortized VI. Additionally, we provide an overview of the recent trends that address several issues of amortized VI, such as the 
    
[^205]: DenseShift: 实现准确和高效的低位幂乘法的量化

    DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.09708](http://arxiv.org/abs/2208.09708)

    DenseShift网络是一种准确和高效的低位幂乘法量化方法，通过改进Shift网络的精度和引入非量化浮点激活来提高性能。

    

    在低资源边缘设备上高效部署深度神经网络是具有挑战性的，因为其不断增加的资源需求。为了解决这个问题，研究人员提出了无乘法神经网络，如幂乘法的量化，也被称为Shift网络，旨在减少内存使用和简化计算。然而，现有的低位Shift网络不如全精度网络准确，通常受到有限权重范围编码方案和量化损失的影响。在本文中，我们提出了DenseShift网络，显著提高了Shift网络的准确性，为视觉和语音应用实现了与全精度网络相媲美的性能。此外，我们引入了一种使用非量化浮点激活的高效DenseShift网络部署方法，同时获得了现有方法的1.6倍加速。为了实现这一点，我们证明了低位Shift网络中零权重值的作用。

    Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw
    
[^206]: 无监督视频领域自适应动作识别：一个解缠视角

    Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective. (arXiv:2208.07365v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.07365](http://arxiv.org/abs/2208.07365)

    本文基于解缠视角处理视频领域无监督自适应问题，通过逐步解缠静态和动态信息并使用多种约束方法，有效地移除空间领域特定信息和减少时间领域差异，实验结果验证了该方法的有效性和优越性。

    

    无监督视频领域自适应是一项实践性而又具有挑战性的任务。本文首次从解缠视角入手处理该问题。我们的主要思路是通过解缠来分别处理空间和时间领域的差异。具体而言，我们考虑从包含静态信息的一组潜在因素和包含动态信息的另一组潜在因素中生成跨领域视频。我们开发了一个转移时序VAE（TranSVAE）框架来建模这种生成过程。为了更好地进行自适应，我们提出了几个约束潜在因素的目标。通过这些约束，静态领域特定信息的解缠可以轻松移除，通过对抗性学习从帧和视频层面进一步减少了时间差异。在UCF-HMDB、Jester和Epic-Kitchens数据集上进行的广泛实验验证了TranSVAE相比其他方法的有效性和优越性。

    Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared wi
    
[^207]: 动态变分轨迹模型中的心脏超声异常检测

    Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory Models. (arXiv:2206.15316v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15316](http://arxiv.org/abs/2206.15316)

    本文提出了一种用于心脏超声视频的新颖异常检测方法，利用心脏周期性特性，在婴儿心脏超声视频数据集上训练了三种变分潜在轨迹模型，可可靠地识别严重的先天性心脏缺陷，并取得了较好的性能。

    

    我们提出了一种用于心脏超声视频的新颖异常检测方法。该方法利用心脏周期性的特性，学习三种变分潜在轨迹模型（TVAE）的变体。其中前两种变体（TVAE-C和TVAE-R）模拟心脏的严格周期性运动，而第三种变体（TVAE-S）更为通用，允许视频中空间表示的移位。所有模型都在一个新的内部婴儿心脏超声视频数据集的健康样本上进行训练，以学习健康人群的规范先验知识。在推断过程中，我们使用基于最大后验概率（MAP）的异常检测来检测数据集中的离群样本。所提出的方法可可靠地识别严重的先天性心脏缺陷，如埃普斯坦异常或Shone综合征。此外，相比于基于标准变分自动编码器的MAP-based异常检测，它实现了更优异的性能。

    We propose a novel anomaly detection method for echocardiogram videos. The introduced method takes advantage of the periodic nature of the heart cycle to learn three variants of a variational latent trajectory model (TVAE). While the first two variants (TVAE-C and TVAE-R) model strict periodic movements of the heart, the third (TVAE-S) is more general and allows shifts in the spatial representation throughout the video. All models are trained on the healthy samples of a novel in-house dataset of infant echocardiogram videos consisting of multiple chamber views to learn a normative prior of the healthy population. During inference, maximum a posteriori (MAP) based anomaly detection is performed to detect out-of-distribution samples in our dataset. The proposed method reliably identifies severe congenital heart defects, such as Ebstein's Anomaly or Shone-complex. Moreover, it achieves superior performance over MAP-based anomaly detection with standard variational autoencoders when detect
    
[^208]: ULF: 无监督标签函数校正方法——基于交叉验证的弱监督学习

    ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision. (arXiv:2204.06863v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.06863](http://arxiv.org/abs/2204.06863)

    ULF是一种用于弱监督学习的无监督标签函数校正方法，通过基于交叉验证原理的噪声降低技术，有效提高了弱监督学习的性能，无需手动标记。

    

    弱监督学习是一种代替手动标记数据的经济有效方法，该方法通过使用预定义的一组标签函数（LFs）对数据样本进行自动标注。标签函数是基于规则的机制，用于为相关类别生成人工标签。在本研究中，我们探索了基于k折交叉验证原理的弱监督学习噪声降低技术。我们引入了一种名为ULF的新算法，用于无监督标签函数校正，通过利用在所有标签函数之外训练的模型来识别和纠正特定于保留标签函数的偏差，从而对弱监督数据进行去噪。具体来说，ULF通过重新估计高可靠性交叉验证样本上的标签函数分配，来改进标签函数对类别的分配。在多个数据集上的评估证实了ULF在增强弱监督学习方面的有效性，而无需手动标记。

    A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling
    
[^209]: 完全自适应差分隐私组合

    Fully Adaptive Composition in Differential Privacy. (arXiv:2203.05481v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.05481](http://arxiv.org/abs/2203.05481)

    本文介绍了完全自适应差分隐私组合的概念，通过引入隐私过滤器和隐私里程表来解决现有组合方法的局限性。

    

    组合是差分隐私的一个重要特性。著名的高级组合定理允许在基本隐私组合允许的情况下，查询私有数据库的次数增加到原来的平方倍。然而，这些结果要求在与数据交互之前固定所有算法的隐私参数。为了解决这个问题，Rogers等人引入了完全自适应组合，其中算法和其隐私参数可以自适应选择。他们定义了两个概率对象来衡量自适应组合中的隐私：隐私过滤器（privacy filters），用于提供组合交互的差分隐私保证，以及隐私里程表（privacy odometers），对隐私损失的时间均匀界限。高级组合和现有的过滤器和里程表之间存在实质性差距。首先，现有的过滤器对被组合的算法提出了更强的假设。其次，这些里程表和过滤器存在较大的常数，使得它们不实用。我们构建了能够在完全自适应组合中使用的过滤器。

    Composition is a key feature of differential privacy. Well-known advanced composition theorems allow one to query a private database quadratically more times than basic privacy composition would permit. However, these results require that the privacy parameters of all algorithms be fixed before interacting with the data. To address this, Rogers et al. introduced fully adaptive composition, wherein both algorithms and their privacy parameters can be selected adaptively. They defined two probabilistic objects to measure privacy in adaptive composition: privacy filters, which provide differential privacy guarantees for composed interactions, and privacy odometers, time-uniform bounds on privacy loss. There are substantial gaps between advanced composition and existing filters and odometers. First, existing filters place stronger assumptions on the algorithms being composed. Second, these odometers and filters suffer from large constants, making them impractical. We construct filters that 
    
[^210]: LAP: 基于注意力的模块用于卷积神经网络中基于概念的自解释和知识注入

    LAP: An Attention-Based Module for Concept Based Self-Interpretation and Knowledge Injection in Convolutional Neural Networks. (arXiv:2201.11808v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.11808](http://arxiv.org/abs/2201.11808)

    本研究提出了一种新的基于注意力的汇聚层，称为LAP，它在卷积神经网络中实现了自解释性和知识注入的可能性，而不会降低性能。该模块可以轻松地插入任何卷积神经网络中，甚至是已经训练好的网络。

    

    尽管深度卷积神经网络具有最先进的性能，但它们在未知情况下容易受到偏见和故障的影响。此外，它们推理背后的复杂计算对于人类来说并不可理解，难以建立信任。外部解释方法试图以人类可理解的方式解释网络决策，但由于其假设和简化而被指责存在谬误。与此相反，模型的固有自解释性虽然更抗坚持上述谬误，但无法应用于已经训练过的模型。在这项工作中，我们提出了一种新的基于注意力的汇聚层，称为局部注意力汇聚（LAP），它可以实现自解释性和可能性，而不会丧失性能。该模块可以轻松地插入任何卷积神经网络中，包括已经训练好的网络。我们提供了一种弱监督训练方案，以学习区分特征。

    Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. Moreover, the complex computation behind their reasoning is not human-understandable to develop trust. External explainer methods have tried to interpret network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection without performance loss. The module is easily pluggable into any convolutional neural network, even the already trained ones. We have defined a weakly supervised training scheme to learn the distinguishing features in d
    
[^211]: 可证明有效且多样化的DNN测试中的真实媒体数据变异

    Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing. (arXiv:2112.01956v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.01956](http://arxiv.org/abs/2112.01956)

    本文通过流形理论的严格方法重新审视了媒体输入变异中的感知多样性（DIV）和有效性（VAL）两个关键目标，证明了它们不可分割地相互绑定，并证明了SOTA基于生成模型的方法的重要性。

    

    深度神经网络（DNN）通常接受高维的媒体数据（例如照片、文本和音频），并理解它们的感知内容（例如一只猫）。为了测试DNN，需要多样化的输入来触发错误预测。一些初步的工作使用字节级变异或领域特定的过滤器（例如，模糊），其启用的变异可能有限且容易出错。最先进的工作采用深度生成模型生成（无限的）输入。此外，为了保持变异后的输入在感知上有效（例如，一只猫在变异后仍然是“猫”），现有的研究依赖于不精确且不可泛化的启发式方法。本研究以流形为基础，在一个严格的方式中重新审视媒体输入变异中的两个关键目标 - 感知多样性（DIV）和有效性（VAL）。我们展示了DIV和VAL不可分割地相互绑定的重要结果，并证明了SOTA基于生成模型的方法。

    Deep neural networks (DNNs) often accept high-dimensional media data (e.g., photos, text, and audio) and understand their perceptual content (e.g., a cat). To test DNNs, diverse inputs are needed to trigger mis-predictions. Some preliminary works use byte-level mutations or domain-specific filters (e.g., foggy), whose enabled mutations may be limited and likely error-prone. SOTA works employ deep generative models to generate (infinite) inputs. Also, to keep the mutated inputs perceptually valid (e.g., a cat remains a "cat" after mutation), existing efforts rely on imprecise and less generalizable heuristics.  This study revisits two key objectives in media input mutation - perception diversity (DIV) and validity (VAL) - in a rigorous manner based on manifold, a well-developed theory capturing perceptions of high-dimensional media data in a low-dimensional space. We show important results that DIV and VAL inextricably bound each other, and prove that SOTA generative model-based methods
    
[^212]: 最简易的流式决策树

    Simplest Streaming Trees. (arXiv:2110.08483v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08483](http://arxiv.org/abs/2110.08483)

    我们提出了最简单的决策树扩展，通过继续生长现有树来更新它们，并用新树替换一些旧树来控制总树的数量。在72个分类问题的基准套件中，我们的方法在精度和内存使用方面表现优异。

    

    决策森林，包括随机森林和梯度提升树，在许多实际数据问题上仍然是主流的机器学习方法，特别是在表格数据上。然而，大部分当前的实现只能以批处理模式运行，因此不能在有更多数据到达时进行增量更新。之前有几项工作开发了流式决策树和集成来克服这个限制。然而，我们发现这些最新算法存在一些问题，包括在某些问题上精度低和在其他问题上内存使用量大。因此，我们开发了最简单的决策树扩展：给定新数据时，通过继续生长现有树来更新它们，并用新树替换一些旧树来控制总树的数量。在包含72个分类问题的基准套件（OpenML-CC18数据套件）中，我们证明了我们的方法Stream Decision Forest（SDF）既不遭受上述问题的困扰

    Decision forests, including random forests and gradient boosting trees, remain the leading machine learning methods for many real-world data problems, especially on tabular data. However, most of the current implementations only operate in batch mode, and therefore cannot incrementally update when more data arrive. Several previous works developed streaming trees and ensembles to overcome this limitation. Nonetheless, we found that those state-of-the-art algorithms suffer from a number of drawbacks, including low accuracy on some problems and high memory usage on others. We therefore developed the simplest possible extension of decision trees: given new data, simply update existing trees by continuing to grow them, and replace some old trees with new ones to control the total number of trees. In a benchmark suite containing 72 classification problems (the OpenML-CC18 data suite), we illustrate that our approach, Stream Decision Forest (SDF), does not suffer from either of the aforement
    
[^213]: Bellman一致的悲观论述用于离线强化学习

    Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06926](http://arxiv.org/abs/2106.06926)

    本文提出了Bellman一致的悲观论述的概念，用于离线强化学习中的函数逼近，通过在与Bellman方程一致的函数集合上实施初始状态的悲观主义，改善了基于奖励的悲观主义方法的样本复杂性。

    

    最近在离线强化学习中，当推理数据集缺乏详尽探索时，使用悲观主义获得了显著的重要性。尽管悲观主义增加了算法的鲁棒性，但过度悲观的推理同样会阻碍发现良好策略，这对于流行的基于奖励的悲观主义是一个问题。在本文中，我们引入了Bellman一致的悲观主义的概念，用于一般函数逼近：我们不是计算值函数的逐点下界，而是在与Bellman方程一致的函数集合上实施初始状态上的悲观主义。我们的理论保证仅需要标准的Bellman封闭性作为探索性设置中的要求，在这种情况下，基于奖励的悲观主义无法提供保证。即使在线性函数逼近的特殊情况下，更强的表现力假设成立时，我们的结果在样本复杂性上优于最近的基于奖励的方法，复杂性改善了Ο(d)。

    The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees. Even in the special case of linear function approximation where stronger expressivity assumptions hold, our result improves upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample c
    
[^214]: 在具有许多类别的上下文推断中通过离线神谕进行最优模型选择

    Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles. (arXiv:2106.06483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06483](http://arxiv.org/abs/2106.06483)

    本论文研究了在随机上下文推断设置中，针对累计遗憾最小化的最优模型选择问题。通过引入渐增类别复杂性和递减边际收益条件，我们提出了一种基于新颖误配测试的算法，并展示了模型选择在奖励估计中的优势。

    

    在监督学习中，模型选择提供了一种无成本的保证，就好像最优平衡偏差和方差的模型是先验已知的一样。我们研究了在随机上下文推断设置中实现类似保证的可行性。最近的研究 [Marinov and Zimmert, 2021] 鉴别出没有算法能够保证无成本的遗憾界限的情况。然而，我们发现在渐增类别复杂性和随着类别复杂性增加最佳策略价值边际收益递减的温和条件下，无成本模型选择是可行的。我们的算法基于一种新颖的误配测试，我们的分析展示了模型选择在奖励估计中的优势。与先前关于上下文推断中模型选择的工作不同，我们的算法在收集更多数据时会仔细地适应逐渐演变的偏差-方差权衡。特别地，我们的算法和分析超越了适应时间复杂性的范畴。

    Model selection in supervised learning provides costless guarantees as if the model that best balances bias and variance was known a priori. We study the feasibility of similar guarantees for cumulative regret minimization in the stochastic contextual bandit setting. Recent work [Marinov and Zimmert, 2021] identifies instances where no algorithm can guarantee costless regret bounds. Nevertheless, we identify benign conditions where costless model selection is feasible: gradually increasing class complexity, and diminishing marginal returns for best-in-class policy value with increasing class complexity. Our algorithm is based on a novel misspecification test, and our analysis demonstrates the benefits of using model selection for reward estimation. Unlike prior work on model selection in contextual bandits, our algorithm carefully adapts to the evolving bias-variance trade-off as more data is collected. In particular, our algorithm and analysis go beyond adapting to the complexity of t
    
[^215]: 面向未知不变流形的慢-快随机系统的非线性模型简化

    Nonlinear model reduction for slow-fast stochastic systems near unknown invariant manifolds. (arXiv:2104.02120v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2104.02120](http://arxiv.org/abs/2104.02120)

    本论文介绍了一种面向慢-快随机系统的非线性模型简化技术，可以通过黑盒模拟器估计不变流形并计算有效的随机动力学过程，实现高效的状态空间探索。

    

    我们介绍了一种非线性随机模型简化技术，适用于具有低维不变有效流形和高维大快模的高维随机动力系统。通过仅访问能够获得短时间模拟的黑盒模拟器，我们设计了一种算法，输出不变流形的估计、在其上的有效随机动力过程（平均掉了快模），以及对应的模拟器。这个模拟器是高效的，因为它利用了不变流形的低维特性，并且采用的时间步长大小取决于有效过程的正则性，因此通常比原模拟器的时间步长大得多，原模拟器需要解决快模。算法和估计可以实时进行，以有效地探索有效状态空间，而不会失去与基础动力学的一致性。

    We introduce a nonlinear stochastic model reduction technique for high-dimensional stochastic dynamical systems that have a low-dimensional invariant effective manifold with slow dynamics, and high-dimensional, large fast modes. Given only access to a black box simulator from which short bursts of simulation can be obtained, we design an algorithm that outputs an estimate of the invariant manifold, a process of the effective stochastic dynamics on it, which has averaged out the fast modes, and a simulator thereof. This simulator is efficient in that it exploits of the low dimension of the invariant manifold, and takes time steps of size dependent on the regularity of the effective process, and therefore typically much larger than that of the original simulator, which had to resolve the fast modes. The algorithm and the estimation can be performed on-the-fly, leading to efficient exploration of the effective state space, without losing consistency with the underlying dynamics. This cons
    
[^216]: 可解释的序列分类通过原型轨迹

    Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.01777](http://arxiv.org/abs/2007.01777)

    ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。

    

    我们提出了一种新颖的用于文本分类的可解释深度神经网络，称为ProtoryNet，它基于原型轨迹的新概念。受现代语言学中的原型理论的启发，ProtoryNet通过为文本序列中的每个句子找到最相似的原型，并将每个句子与相应的活动原型的接近程度输入到RNN主干中进行预测。然后，RNN主干捕捉到原型的时间模式，我们称之为原型轨迹。原型轨迹能够直观而细致地解释RNN模型的推理过程，类似于人类分析文本的方式。我们还设计了原型修剪过程，以减少模型使用的原型总数，以提高解释性。在多个公共数据集上的实验证明，ProtoryNet比基线的基于原型的深度神经网络更准确，并减少了与现有模型相比的性能差距。

    We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
    
[^217]: 可微稀疏化的深度神经网络

    Differentiable Sparsification for Deep Neural Networks. (arXiv:1910.03201v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.03201](http://arxiv.org/abs/1910.03201)

    本研究提出了一种完全可微的深度神经网络稀疏化方法，能够通过优化目标函数直接学习网络的稀疏结构和权重，有效解决了有效架构确定和网络尺寸缩小的问题。

    

    深度神经网络极大地减轻了特征工程的负担，但对于这些网络的有效架构的确定也需要相当大的努力。此外，随着网络规模变得过于庞大，大量资源被投入到缩小它们的大小。这些挑战可以通过对过完备模型进行稀疏化来有效解决。在本研究中，我们提出了一种完全可微的深度神经网络稀疏化方法，可以通过直接优化带有随机梯度下降的正则化目标函数将无关紧要的参数置零。因此，所提出的方法可以端到端地学习网络的稀疏结构和权重。它可以直接应用于各种现代深度神经网络，并对训练过程的修改要求很小。据我们所知，这是第一个完全可微的稀疏化方法。

    Deep neural networks have significantly alleviated the burden of feature engineering, but comparable efforts are now required to determine effective architectures for these networks. Furthermore, as network sizes have become excessively large, a substantial amount of resources is invested in reducing their sizes. These challenges can be effectively addressed through the sparsification of over-complete models. In this study, we propose a fully differentiable sparsification method for deep neural networks, which can zero out unimportant parameters by directly optimizing a regularized objective function with stochastic gradient descent. Consequently, the proposed method can learn both the sparsified structure and weights of a network in an end-to-end manner. It can be directly applied to various modern deep neural networks and requires minimal modification to the training process. To the best of our knowledge, this is the first fully differentiable sparsification method.
    

