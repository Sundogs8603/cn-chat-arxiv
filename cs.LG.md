# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment](https://arxiv.org/abs/2404.00044) | 本文提出了UAlign，一种无模板化的图到序列的逆合成预测方法，通过结合图神经网络和Transformer，利用分子的固有图结构，并引入一种简单有效的SMILES对齐技术来促进未改变结构的复用。 |
| [^2] | [Non-negative Contrastive Learning](https://arxiv.org/abs/2403.12459) | 非负对比学习(NCL)是对非负矩阵分解(NMF)的重新演绎，通过对特征施加非负约束来获得可解释的特征，保留了NMF的可解释属性，从而得到比标准对比学习(CL)更稀疏和解耦的表示 |
| [^3] | [Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions](https://arxiv.org/abs/2403.12007) | 本研究旨在定义通过数字行为变化干预支持癌症患者提高生活质量的有效参与方式，发现医生处方显著增加患者对移动数字行为变化干预的持续参与，同时指出每周参与一次已足以维持福祉，但内在动机可能需要更高水平的参与。 |
| [^4] | [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | 通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。 |
| [^5] | [Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization](https://arxiv.org/abs/2403.02882) | 通过在SUMO中随机化规则微观交通流的行为，利用深度强化学习算法训练自动驾驶车辆的决策策略，提高其在更真实交通场景中的性能。 |
| [^6] | [Mixed-Strategy Nash Equilibrium for Crowd Navigation](https://arxiv.org/abs/2403.01537) | 通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。 |
| [^7] | [Improving Socratic Question Generation using Data Augmentation and Preference Optimization](https://arxiv.org/abs/2403.00199) | 通过数据增强和偏好优化，改进了苏格拉底提问生成方法，减轻教师繁重的工作量，防止生成无效问题。 |
| [^8] | [Using Graph Neural Networks to Predict Local Culture](https://arxiv.org/abs/2402.17905) | 本研究提出了使用图神经网络（GNN）方法，结合多个信息源来更好地预测社区属性，特别是预测当地文化，结果显示了这一方法在考虑结构相连性方面的潜力。 |
| [^9] | [Measuring Vision-Language STEM Skills of Neural Models](https://arxiv.org/abs/2402.17205) | 该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。 |
| [^10] | [BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation](https://arxiv.org/abs/2402.16880) | 该论文提出了一种名为BESA的新型大型语言模型修剪技术，通过应用分块重构损失，与传统的逐层修剪技术不同，BESA具有优势 |
| [^11] | [A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks](https://arxiv.org/abs/2402.15984) | 通过使用傅里叶表达式导出尖峰变换，实现了对各种现代神经网络的描述和分析。 |
| [^12] | [Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics](https://arxiv.org/abs/2402.11515) | 该研究专注于优化深度强化学习在流体力学中主动流控制中的并行设置，通过拆解DRL框架、进行扩展性基准测试、提出混合并行化配置并优化多环境DRL训练中的I/O操作，提出了有效的并行化策略。 |
| [^13] | [How to validate average calibration for machine learning regression tasks ?](https://arxiv.org/abs/2402.10043) | 本文提出了两种验证机器学习回归任务平均校准性的方法，将校准误差与平均绝对误差之间的差值和将平均平方z-分数与1进行比较。研究发现，前者对不确定性分布敏感，而后者在该方面提供了最可靠的方法。 |
| [^14] | [Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction](https://arxiv.org/abs/2402.08174) | 本论文提出了一种使用地标和聚类的层级位置嵌入方法用于链接预测任务。通过选择具有高度中心度的节点作为地标和进行图聚类，本方法有效地将位置信息嵌入到图中，提高了链接预测的准确性和性能。 |
| [^15] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^16] | [Task-conditioned adaptation of visual features in multi-task policy learning](https://arxiv.org/abs/2402.07739) | 本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。 |
| [^17] | [Succint Interaction-Aware Explanations](https://arxiv.org/abs/2402.05566) | 本文提出了一种结合了SHAP和NSHAP的方法，通过将特征分成显著交互的部分，构建了一种简明、易解释的加性解释，并通过统计检验剪枝次优解，提高了解释的运行效率。 |
| [^18] | [Quantum Normalizing Flows for Anomaly Detection](https://arxiv.org/abs/2402.02866) | 本文介绍了一种将任意分布计算为预定义分布的量子标准化流方法，该方法在异常检测方面具有与传统方法相竞争的性能，并且可以在量子计算机上执行。 |
| [^19] | [Event-Based Contrastive Learning for Medical Time Series](https://arxiv.org/abs/2312.10308) | 本论文介绍了一种基于事件的对比学习方法（EBCL），用于学习医学时间序列中关键事件前后的数据编码。研究发现，相对于其他预训练方法，EBCL能够产生性能更好的模型，对于心力衰竭队列的关键下游任务具有更好的微调性能，并能有效地将具有相似风险的患者进行聚类。 |
| [^20] | [FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning](https://arxiv.org/abs/2312.09006) | FedSSA是一种基于语义相似度的聚合方法，用于高效模型异构个性化联邦学习。它通过异构 feature extractor 和同质 classification header 将每个客户端的模型拆分，并通过语义相似度进行头部参数聚合实现本地到全局的知识传输。此外，通过自适应参数稳定策略实现了全局到本地的知识传输。 |
| [^21] | [RefinedFields: Radiance Fields Refinement for Unconstrained Scenes](https://arxiv.org/abs/2312.00639) | RefinedFields是第一种利用预训练模型改善无约束场景建模的方法。通过优化指导和交替训练过程，该方法能够从真实世界图像的先验条件中提取更丰富的细节，并在新视角合成任务中优于以往的方法。 |
| [^22] | [Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?.](http://arxiv.org/abs/2401.16669) | 人工智能技术在天气预报领域的快速发展代表了一个重大突破，它克服了传统模型的局限性，有潜力引领天气预报的第二次革命。 |
| [^23] | [Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness.](http://arxiv.org/abs/2401.15127) | 本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。 |
| [^24] | [End-To-End Set-Based Training for Neural Network Verification.](http://arxiv.org/abs/2401.14961) | 本论文提出了一种端到端基于集合的训练方法，用于训练鲁棒性神经网络进行形式化验证，并证明该方法能够简化验证过程并有效训练出易于验证的神经网络。 |
| [^25] | [Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation.](http://arxiv.org/abs/2401.09943) | 提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，并能集成任何幂级数并捕捉长程依赖关系。 |
| [^26] | [Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization.](http://arxiv.org/abs/2311.00944) | 本论文提出了一种新算法，FESS-GDA，利用平滑技术进行联邦极小极大优化。通过解决不同类型的联邦极小极大问题，我们证明了FESS-GDA的收敛性，并展示了其在实际联邦学习任务中的实际效果。 |
| [^27] | [High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise.](http://arxiv.org/abs/2310.18784) | 本研究探讨了一类非线性随机梯度下降方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，即使噪声是重尾的，结果证明了对失败概率的对数依赖。这些结果适用于剪切、归一化和量化等任何具有有界输出的非线性函数。 |
| [^28] | [Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks.](http://arxiv.org/abs/2310.12079) | 最近的研究发现，对于具有形状激活函数的神经网络，其缩放极限可以由微分方程描述。然而，关于未经形状处理的神经网络的信息尚不明确。本文研究了两种未经形状处理的网络，发现它们也可以由类似的微分方程来描述，并给出了它们的一些特征。 |
| [^29] | [RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization.](http://arxiv.org/abs/2310.07983) | RandCom是一种去中心化的随机通信跳跃方法，能够在分布式优化中通过概率性本地更新减少通信开销，并在不同的设置中实现线性加速。 |
| [^30] | [ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting.](http://arxiv.org/abs/2310.07446) | ProbTS是一个统一的工具包，用于协同和比较定制神经架构和深度生成模型在时间序列预测中的方法，揭示了它们的特点、优势和需要进一步研究的领域。 |
| [^31] | [Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts.](http://arxiv.org/abs/2310.05898) | Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。 |
| [^32] | [High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning.](http://arxiv.org/abs/2310.03624) | 本论文介绍了一种使用神经领域进行机器人自建模和运动规划的方法。通过利用2D图像和相机姿态进行学习，无需深度图像或几何知识，实现了对高自由度物体的建模。在7自由度机器人测试中，所学的自建模与真实模型的差距仅为2%。 |
| [^33] | [AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval.](http://arxiv.org/abs/2310.01880) | AutoCast++是一个零-shot基于排名的上下文检索系统，用于从广泛的新闻文档集合中进行事件预测。 |
| [^34] | [Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words.](http://arxiv.org/abs/2309.16108) | 本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。 |
| [^35] | [Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals.](http://arxiv.org/abs/2309.05927) | 本研究提出了一种名为$\texttt{bio}$FAME的频率感知掩码自编码器，用于多模态生物信号的预训练。其通过在频率空间中对生物信号进行表示参数化，利用固定大小的傅里叶变换运算符进行全局令牌混合，并通过频率维持预训练策略保持每个输入通道中的频率成分。 |
| [^36] | [A Lightweight and Transferable Design for Robust LEGO Manipulation.](http://arxiv.org/abs/2309.02354) | 本文介绍了一种轻量化且可传递的设计，用于解决机器人乐高操纵中的复杂性和精确性要求。通过硬件软件协同设计和进化策略优化，实现了高效可靠的乐高操纵，并展示了该设计的普适性和可传递性。 |
| [^37] | [Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds.](http://arxiv.org/abs/2309.00380) | 本文提出了一种用于多模态数据的深度潜变量模型，并开发了更灵活的编码特征聚合方案，能够紧密地下界数据对数似然。 |
| [^38] | [Explicability and Inexplicability in the Interpretation of Quantum Neural Networks.](http://arxiv.org/abs/2308.11098) | 本文探索了量子神经网络的可解释性，引入了不可解释性带的概念，为理解如何构建负责任且可追究的量子人工智能模型迈出了一步。 |
| [^39] | [Interpretable Graph Neural Networks for Tabular Data.](http://arxiv.org/abs/2308.08945) | 本论文提出了一种称为IGNNet的方法，可以在处理表格数据时使用图神经网络，该方法能够产生可解释的模型，从原始输入特征精确计算预测结果，并且在性能上与最先进的机器学习算法性能相当。 |
| [^40] | [Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning.](http://arxiv.org/abs/2307.02620) | 本文研究了在观测代价敏感强化学习中，强化学习代理在每个时间步不需要昂贵的测量，提出了一种新的方法DMSOA，并在多个环境中进行了评估，结果表明DMSOA能够以更少的决策步骤和测量次数学到更好的策略。 |
| [^41] | [Noise Stability Optimization for Flat Minima with Optimal Convergence Rates.](http://arxiv.org/abs/2306.08553) | 本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。 |
| [^42] | [Conditional Diffusion Models for Semantic 3D Medical Image Synthesis.](http://arxiv.org/abs/2305.18453) | 这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。 |
| [^43] | [Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values.](http://arxiv.org/abs/2305.07877) | 本研究开发了一种基于血液检查数值的病毒与细菌机器学习模型，用于准确识别感染类型。该模型在CRP水平10-40 mg/L范围内表现出更好的区分细菌和病毒感染的准确性，证明了多种血液参数对于诊断决策的重要性。 |
| [^44] | [Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness.](http://arxiv.org/abs/2304.09779) | 研究发现，通过平等化受保护子群体之间的预测分布来实现组公平和视相似个体同等对待实现个人公正是不兼容的。 并提出了一种构建连续概率函数的解决方法，来实现组和个人公平。 |
| [^45] | [Approximation of group explainers with coalition structure using Monte Carlo sampling on the product space of coalitions and features.](http://arxiv.org/abs/2303.10216) | 本文研究了使用蒙特卡罗采样算法逼近联盟结构群体解释器的复杂问题，提出一种快速、易于实现且与特定模型无关的新型算法，并提供了严密的统计分析及误差界的证明。 |
| [^46] | [One-shot domain adaptation in video-based assessment of surgical skills.](http://arxiv.org/abs/2301.00812) | 本文提出了一种元学习模型A-VBANet，可以通过一次性学习提供领域不可知的手术技能分类，成功地适应了模拟任务和腹腔镜胆囊切除术，为基于视频的手术技能评估提供了领域不可知程序。 |

# 详细

[^1]: UAlign: 无模板化的非监督式SMILES对齐推动无模板化逆合成预测的极限

    UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment

    [https://arxiv.org/abs/2404.00044](https://arxiv.org/abs/2404.00044)

    本文提出了UAlign，一种无模板化的图到序列的逆合成预测方法，通过结合图神经网络和Transformer，利用分子的固有图结构，并引入一种简单有效的SMILES对齐技术来促进未改变结构的复用。

    

    逆合成规划在有机化工行业中，特别是在制药领域，面临着巨大挑战。单步逆合成预测是规划过程中至关重要的一步，近年来由于科学人工智能的进步，这一步骤引起了人们的浓厚兴趣。近年来已经提出了各种基于深度学习的方法来解决这一问题，其中包括不同程度的额外化学知识依赖。本文介绍了UAlign，这是一种基于图到序列的无模板化逆合成预测管线。通过结合图神经网络和Transformer，我们的方法能够更有效地利用分子的固有图结构。基于分子结构在化学反应过程中保持不变的事实，我们提出了一种简单而有效的SMILES对齐技术，以促进未改变结构的复用以生成反应物。大量实验...

    arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
    
[^2]: 非负对比学习

    Non-negative Contrastive Learning

    [https://arxiv.org/abs/2403.12459](https://arxiv.org/abs/2403.12459)

    非负对比学习(NCL)是对非负矩阵分解(NMF)的重新演绎，通过对特征施加非负约束来获得可解释的特征，保留了NMF的可解释属性，从而得到比标准对比学习(CL)更稀疏和解耦的表示

    

    深度表示在以黑盒方式转移到下游任务时表现出了良好的性能。然而，它们固有的不可解释性仍然是一个重大挑战，因为这些特征通常对人类理解而言是不透明的。在本文中，我们提出了非负对比学习（NCL），这是对非负矩阵分解（NMF）的复兴，旨在得出可解释的特征。NCL的力量在于强制将非负约束应用于特征，这让人想起NMF能够提取与样本集群紧密对齐的特征的能力。NCL不仅在数学上与NMF目标很好地对齐，而且保留了NMF的可解释属性，使得与标准对比学习（CL）相比，得到了更加稀疏和解耦的表示。从理论上，我们为NCL的可识别性和下游泛化性能提供了保证。从经验上看，我们展示了这些

    arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
    
[^3]: 确定通过移动数字行为变化干预提高癌症患者福祉的有效参与方式

    Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions

    [https://arxiv.org/abs/2403.12007](https://arxiv.org/abs/2403.12007)

    本研究旨在定义通过数字行为变化干预支持癌症患者提高生活质量的有效参与方式，发现医生处方显著增加患者对移动数字行为变化干预的持续参与，同时指出每周参与一次已足以维持福祉，但内在动机可能需要更高水平的参与。

    

    数字行为变化干预（DBCI）正在支持新健康行为的发展。评估它们的有效性对于改进它们和理解成功因素至关重要。然而，特别是在受伦理限制的小规模研究中，开发者的全面指导仍然有限。本研究基于CAPABLE项目，旨在定义通过DBCI支持癌症患者提高生活质量的有效参与方式。我们确定了衡量参与度的指标，探讨了患者和临床医生对DBCI的兴趣，并提出了在这种背景下评估DBCI影响的假设。我们的研究结果表明，医生的处方显着增加了患者对移动DBCI的持续参与。此外，尽管每周一次参与DBCI就足以维持福祉，但从外在动机向内在动机的转变可能需要更高水平的参与。

    arXiv:2403.12007v1 Announce Type: cross  Abstract: Digital Behavior Change Interventions (DBCIs) are supporting development of new health behaviors. Evaluating their effectiveness is crucial for their improvement and understanding of success factors. However, comprehensive guidance for developers, particularly in small-scale studies with ethical constraints, is limited. Building on the CAPABLE project, this study aims to define effective engagement with DBCIs for supporting cancer patients in enhancing their quality of life. We identify metrics for measuring engagement, explore the interest of both patients and clinicians in DBCIs, and propose hypotheses for assessing the impact of DBCIs in such contexts. Our findings suggest that clinician prescriptions significantly increase sustained engagement with mobile DBCIs. In addition, while one weekly engagement with a DBCI is sufficient to maintain well-being, transitioning from extrinsic to intrinsic motivation may require a higher level o
    
[^4]: MM1：多模式LLM预训练的方法、分析与见解

    MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training

    [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)

    通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。

    

    在这项工作中，我们讨论了构建高性能的多模式大型语言模型（MLLMs）。具体来说，我们研究了各种架构组件和数据选择的重要性。通过对图像编码器、视觉语言连接器和各种预训练数据选择进行仔细和全面的消融实验，我们确定了几个关键的设计经验。例如，我们展示了对大规模多模式预训练使用仔细混合的图像标题、交替图像文本和仅文本数据对于在多个基准测试中实现最新潮（SOTA）的少样本结果至关重要，与其他已发表的预训练结果相比。此外，我们表明图像编码器连同图像分辨率和图像标记计数具有重要影响，而视觉语言连接器设计相对重要性较小。通过扩大所提出的方法，我们构建了MM1，一个多模式模型系列。

    arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
    
[^5]: 通过以交通流随机化方式进行强化学习的自动驾驶车辆决策与控制

    Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization

    [https://arxiv.org/abs/2403.02882](https://arxiv.org/abs/2403.02882)

    通过在SUMO中随机化规则微观交通流的行为，利用深度强化学习算法训练自动驾驶车辆的决策策略，提高其在更真实交通场景中的性能。

    

    当前大多数关于自动驾驶车辆决策和控制任务基于强化学习的研究是在模拟环境中进行的。这些研究的训练和测试是在基于规则的微观交通流下进行的，很少考虑将它们迁移到真实或接近真实的环境中以测试它们的性能。本研究提出了一种方法，通过随机化SUMO中基于规则的微观交通流的车辆跟驰模型和换道模型的某些参数来随机化周围车辆的驾驶风格和行为。我们使用深度强化学习算法在公路和合并场景中的领域随机化规则微观交通流下训练策略，然后分别在基于规则的微观交通流和.

    arXiv:2403.02882v1 Announce Type: cross  Abstract: Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and h
    
[^6]: 混合策略纳什均衡用于人群导航

    Mixed-Strategy Nash Equilibrium for Crowd Navigation

    [https://arxiv.org/abs/2403.01537](https://arxiv.org/abs/2403.01537)

    通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。

    

    我们解决了针对人群导航找到混合策略纳什均衡的问题。混合策略纳什均衡为机器人提供了一个严谨的模型，使其能够预测人群中不确定但合作的人类行为，但计算成本通常太高，无法进行可扩展和实时的决策制定。在这里，我们证明了一个简单的迭代贝叶斯更新方案收敛于混合策略社交导航游戏的纳什均衡。此外，我们提出了一个基于数据驱动的框架，通过将代理策略初始化为从人类数据集学习的高斯过程，来构建该游戏。基于所提出的混合策略纳什均衡模型，我们开发了一个基于采样的人群导航框架，可以集成到现有导航方法中，并可在笔记本电脑 CPU 上实时运行。我们通过模拟环境和真实世界的非结构化环境中人类数据集对我们的框架进行了评估。

    arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
    
[^7]: 利用数据增强和偏好优化改进苏格拉底提问生成

    Improving Socratic Question Generation using Data Augmentation and Preference Optimization

    [https://arxiv.org/abs/2403.00199](https://arxiv.org/abs/2403.00199)

    通过数据增强和偏好优化，改进了苏格拉底提问生成方法，减轻教师繁重的工作量，防止生成无效问题。

    

    苏格拉底方法是一种引导学生独立解决问题而不直接揭示问题解决方案的方法。本文提出一种通过数据增强和偏好优化改进苏格拉底提问生成的方法，用于增强巨大语言模型自动生成苏格拉底问题，以减轻教师的繁重工作量。研究表明，现有涉及提示这些巨大语言模型的方法有时会产生无效的输出，例如直接揭示问题解决方案或提供无关或过早的问题。为了解决这一问题，本研究首先提出一种数据增强方法，以丰富现有的苏格拉底提问数据集；其次，提出一种方法来优化开源巨大语言模型，例如LLama 2，以更倾向于地面真值问题。

    arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
    
[^8]: 使用图神经网络预测当地文化

    Using Graph Neural Networks to Predict Local Culture

    [https://arxiv.org/abs/2402.17905](https://arxiv.org/abs/2402.17905)

    本研究提出了使用图神经网络（GNN）方法，结合多个信息源来更好地预测社区属性，特别是预测当地文化，结果显示了这一方法在考虑结构相连性方面的潜力。

    

    城市研究长期以来一直认识到社区是动态和关联的。然而，缺乏数据、方法论和计算处理能力阻碍了对社区关系动态进行正式定量分析。为了在这个问题上取得进展，本研究提出了一种图神经网络（GNN）方法，允许结合和评估关于社区内部特征、它们的过去特征以及在它们之间流动的群体的多个信息源，潜在地为预测模型提供更大的表达能力。通过探索 Yelp 的公开大规模数据集，我们展示了我们的方法在考虑结构相连性方面对预测社区属性（特别是预测当地文化）的潜力。从实质和方法论角度来看，结果是令人鼓舞的。从实质上讲，我们发现无论是当地区域信息（例如区域人口统计信息）

    arXiv:2402.17905v1 Announce Type: new  Abstract: Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or g
    
[^9]: 测量神经模型的视觉语言STEM技能

    Measuring Vision-Language STEM Skills of Neural Models

    [https://arxiv.org/abs/2402.17205](https://arxiv.org/abs/2402.17205)

    该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。

    

    我们引入了一个新挑战，用于测试神经模型的STEM技能。现实世界中的问题通常需要结合STEM（科学、技术、工程和数学）知识来解决。与现有数据集不同，我们的数据集需要理解STEM的多模式视觉语言信息。我们的数据集是挑战性问题中最大、最全面的数据集之一。它包括448项技能和1,073,146个跨越所有STEM科目的问题。与通常侧重于检验专家水平能力的现有数据集不同，我们的数据集包括基础技能和根据K-12课程设计的问题。我们还将最先进的基础模型，如CLIP和GPT-3.5-Turbo，添加到我们的基准中。结果显示，最近的模型进展只有助于掌握数据集中非常有限数量的低年级技能（三年级中的2.5%）。事实上，这些模型仍远没有完全掌握学前教育阶段的技能。

    arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
    
[^10]: BESA: 使用分块参数高效稀疏分配修剪大型语言模型

    BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation

    [https://arxiv.org/abs/2402.16880](https://arxiv.org/abs/2402.16880)

    该论文提出了一种名为BESA的新型大型语言模型修剪技术，通过应用分块重构损失，与传统的逐层修剪技术不同，BESA具有优势

    

    大型语言模型（LLMs）在文本摘要、文本问答等各种任务中表现出色。尽管它们的性能令人印象深刻，但由于大量参数造成的计算占用可能是禁锢的。现有解决方案（如SparseGPT和Wanda）尝试通过权重修剪缓解此问题。然而，它们的逐层方法会导致模型输出显著扰动，并需要细致的超参数调整，如修剪速率，这可能会对整体模型性能产生不利影响。为解决此问题，本文引入了一种新颖的LLM修剪技术，称为分块参数高效稀疏分配（BESA），通过应用分块重构损失。与典型的逐层修剪技术相比，BESA具有两个独特的特点：i）它定位于整体修剪误差相对于每个

    arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
    
[^11]: 用统一的傅里叶切片方法导出一种适用于多种深度-2神经网络的尖峰变换

    A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks

    [https://arxiv.org/abs/2402.15984](https://arxiv.org/abs/2402.15984)

    通过使用傅里叶表达式导出尖峰变换，实现了对各种现代神经网络的描述和分析。

    

    研究神经网络参数时，研究参数分布比研究每个神经元的参数更容易。尖峰变换是一个伪逆算子，将给定函数 $f$ 映射到参数分布 $\gamma$，使得网络 $\mathtt{NN}[\gamma]$ 能够重现 $f$，即 $\mathtt{NN}[\gamma]=f$。在欧氏空间上的深度-2全连接网络中，已发现了尖峰变换的闭合形式表达式，因此我们可以描述参数的分布。然而，对于多种现代神经网络架构，尚不知道闭合形式表达式。本文介绍了一种使用傅里叶表达式的系统方法，用于推导各种现代网络的尖峰变换，例如有限域 $\mathbb{F}_p$ 上的网络、抽象希尔伯特空间 $\mathcal{H}$ 上的群卷积网络，以及非紧致对称的全连接网络。

    arXiv:2402.15984v1 Announce Type: new  Abstract: To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$ reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\mathcal{H}$, fully-connected networks on noncompact symm
    
[^12]: 深度强化学习在流体力学中主动流控制中的最佳并行化策略

    Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics

    [https://arxiv.org/abs/2402.11515](https://arxiv.org/abs/2402.11515)

    该研究专注于优化深度强化学习在流体力学中主动流控制中的并行设置，通过拆解DRL框架、进行扩展性基准测试、提出混合并行化配置并优化多环境DRL训练中的I/O操作，提出了有效的并行化策略。

    

    深度强化学习（DRL）已被证明是处理高动态和非线性主动流控制（AFC）问题的一种有前途的方法。然而，与训练DRL模型相关的计算成本构成了重要的性能瓶颈。为了应对这一挑战并在高性能计算架构上实现有效的扩展，本研究侧重于优化并行设置中的基于DRL的算法。我们验证了用于AFC问题的现有最先进的DRL框架，并讨论了其效率瓶颈。随后，通过拆解整体框架，并为各个组件进行广泛的可扩展性基准测试，我们研究了各种混合并行化配置，并提出了有效的并行化策略。此外，我们优化了多环境DRL训练中的输入/输出（I/O）操作，以解决与数据移动相关的关键开销。

    arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
    
[^13]: 如何验证机器学习回归任务的平均校准性？

    How to validate average calibration for machine learning regression tasks ?

    [https://arxiv.org/abs/2402.10043](https://arxiv.org/abs/2402.10043)

    本文提出了两种验证机器学习回归任务平均校准性的方法，将校准误差与平均绝对误差之间的差值和将平均平方z-分数与1进行比较。研究发现，前者对不确定性分布敏感，而后者在该方面提供了最可靠的方法。

    

    机器学习回归任务的平均校准性可以通过两种方式进行测试。一种方式是将校准误差（CE）估计为平均绝对误差（MSE）与平均方差（MV）或平均平方不确定性之间的差值。另一种方式是将平均平方z-分数或缩放误差（ZMS）与1进行比较。两种方法可能得出不同的结论，正如来自最近的机器学习不确定性量化文献中的数据集集合所示。研究表明，CE对不确定性分布非常敏感，特别是对于离群不确定性的存在，因此无法可靠地用于校准测试。相比之下，ZMS统计量不具有这种敏感性问题，在这种情况下提供了最可靠的方法。文章还讨论了对条件校准验证的影响。

    arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
    
[^14]: 使用地标和聚类的层级位置嵌入图形用于链接预测

    Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction

    [https://arxiv.org/abs/2402.08174](https://arxiv.org/abs/2402.08174)

    本论文提出了一种使用地标和聚类的层级位置嵌入方法用于链接预测任务。通过选择具有高度中心度的节点作为地标和进行图聚类，本方法有效地将位置信息嵌入到图中，提高了链接预测的准确性和性能。

    

    学习图中节点的位置信息对于链接预测任务非常重要。我们提出了使用代表性节点（称为地标）来表示位置信息的方法。我们选择少量具有高度中心度的节点作为地标，它们作为节点位置的参考点。我们证明了这种选择策略对于众所周知的随机图模型是合理的，并推导出涉及地标的平均路径长度的闭合形式上界。在幂律图的模型中，我们证明了地标为节点之间距离提供了渐近完全准确的信息。我们将理论洞察力应用于实际网络，并提出了具有地标和聚类的层级位置嵌入（HPLC）方法。HPLC将地标选择和图聚类相结合，其中图被分割为连通密集的聚类，选择具有最高度中心度的节点作为地标。HPLC利用了基于地标的节点位置信息的层级性。

    Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
    
[^15]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^16]: 多任务策略学习中基于任务条件的视觉特征自适应

    Task-conditioned adaptation of visual features in multi-task policy learning

    [https://arxiv.org/abs/2402.07739](https://arxiv.org/abs/2402.07739)

    本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。

    

    成功地解决各种任务是自主代理的核心能力，这需要灵活地调整底层的决策策略，并且如我们在这项工作中所提出的，还需要调整底层的感知模块。一个类比的论证是人类的视觉系统，它使用自上而下的信号来专注于当前任务。类似地，在这项工作中，我们在多任务策略学习的上下文中，通过特定的下游任务来调整预先训练的大视觉模型。我们引入了基于任务条件的适配器，在不需要微调任何预先训练权重的情况下，与通过行为克隆训练的单一策略结合使用，能够解决多个任务。我们在策略和视觉适配器上根据任务嵌入进行条件化，如果任务是已知的，则可以在推理过程中选择任务嵌入，否则可以从一组示例演示中进行推断。为此，我们提出了一种新的基于优化的估计器。我们在...（摘要未完成）

    Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
    
[^17]: 简明考虑交互的解释

    Succint Interaction-Aware Explanations

    [https://arxiv.org/abs/2402.05566](https://arxiv.org/abs/2402.05566)

    本文提出了一种结合了SHAP和NSHAP的方法，通过将特征分成显著交互的部分，构建了一种简明、易解释的加性解释，并通过统计检验剪枝次优解，提高了解释的运行效率。

    

    SHAP是一种流行的解释黑箱模型的方法，通过揭示各个特征的重要性来进行解释。由于忽略了特征之间的交互作用，SHAP的解释可能会令人困惑甚至误导。另一方面，NSHAP报告了所有特征子集的加性重要性。虽然这包含了所有相互作用的特征集，但也导致了一个指数级大小的难以解释的解释。在本文中，我们提出了将这两个方法的优点结合起来的方法，将特征分成显著交互的部分，并使用这些部分构成简明、易解释的加性解释。我们提出了一个标准来衡量这种分区对模型行为的代表性，折衷于得到的解释的复杂性。为了高效地从超指数数量中找到最佳分区，我们展示了如何使用统计检验来剪枝次优解，不仅提高了运行时间，还有助于解释。

    SHAP is a popular approach to explain black-box models by revealing the importance of individual features. As it ignores feature interactions, SHAP explanations can be confusing up to misleading. NSHAP, on the other hand, reports the additive importance for all subsets of features. While this does include all interacting sets of features, it also leads to an exponentially sized, difficult to interpret explanation. In this paper, we propose to combine the best of these two worlds, by partitioning the features into parts that significantly interact, and use these parts to compose a succinct, interpretable, additive explanation. We derive a criterion by which to measure the representativeness of such a partition for a models behavior, traded off against the complexity of the resulting explanation. To efficiently find the best partition out of super-exponentially many, we show how to prune sub-optimal solutions using a statistical test, which not only improves runtime but also helps to det
    
[^18]: 量子标准化流在异常检测中的应用

    Quantum Normalizing Flows for Anomaly Detection

    [https://arxiv.org/abs/2402.02866](https://arxiv.org/abs/2402.02866)

    本文介绍了一种将任意分布计算为预定义分布的量子标准化流方法，该方法在异常检测方面具有与传统方法相竞争的性能，并且可以在量子计算机上执行。

    

    标准化流将任意分布计算为预定义（例如正态）分布的双射映射。一旦学习到这样的映射，它可以用于解决不同的任务，例如异常检测。在本研究中，我们引入了用于量子架构的标准化流，描述了如何建模和优化这样的流，并在示例数据集上评估了我们的方法。我们提出的模型在异常检测方面表现出与传统方法（如孤立森林、局部离群因子（LOF）或单类支持向量机）相竞争的性能，同时可以在量子计算机上完全执行。

    A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution. Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned. In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets. Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer.
    
[^19]: 基于事件的对比学习用于医学时间序列

    Event-Based Contrastive Learning for Medical Time Series

    [https://arxiv.org/abs/2312.10308](https://arxiv.org/abs/2312.10308)

    本论文介绍了一种基于事件的对比学习方法（EBCL），用于学习医学时间序列中关键事件前后的数据编码。研究发现，相对于其他预训练方法，EBCL能够产生性能更好的模型，对于心力衰竭队列的关键下游任务具有更好的微调性能，并能有效地将具有相似风险的患者进行聚类。

    

    在临床实践中，我们经常需要确定某个关键医学事件后患者是否处于不良结果的高风险状态，例如心力衰竭入院后的短期死亡风险。这个任务由于长期医学数据的复杂性、变异性和异质性而具有挑战性，特别是对于像心力衰竭这样的慢性疾病患者。在本文中，我们引入了基于事件的对比学习（EBCL）方法，用于学习不同类型患者数据的嵌入表示，以保留关键索引事件前后的时间信息。我们证明，相对于其他预训练方法，EBCL产生的模型在心力衰竭队列的关键下游任务（包括30天再入院、1年死亡率和1周住院天数）的微调性能更好。我们的研究还揭示了EBCL预训练单独能够有效地将具有相似死亡率和再入院风险的患者进行聚类，从而提供了新的见解。

    In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event; for example, the short-term risk of death after an admission for heart failure. This task is challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL), a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL produces models that yield better fine-tuning performance on critical downstream tasks for a heart failure cohort, including 30-day readmission, 1-year mortality, and 1-week length of stay, relative to other pretraining methods. Our findings also reveal that EBCL pretraining alone can effectively cluster patients with similar mortality and readmission risks, offering 
    
[^20]: 基于语义相似度的聚合的FedSSA: 用于高效模型异构个性化联邦学习

    FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning

    [https://arxiv.org/abs/2312.09006](https://arxiv.org/abs/2312.09006)

    FedSSA是一种基于语义相似度的聚合方法，用于高效模型异构个性化联邦学习。它通过异构 feature extractor 和同质 classification header 将每个客户端的模型拆分，并通过语义相似度进行头部参数聚合实现本地到全局的知识传输。此外，通过自适应参数稳定策略实现了全局到本地的知识传输。

    

    联邦学习（FL）是一种保护隐私的协作机器学习范式。传统的FL要求所有数据所有者（即FL客户端）训练相同的本地模型。这种设计并不适用于涉及数据和/或系统异构的场景。模型异构个性化FL（MHPFL）已经出现来解决这个挑战。现有的MHPFL方法通常依赖于具有相同学习任务性质的公共数据集，或者会产生高计算和通信成本。为了解决这些限制，我们提出了一种名为Federated Semantic Similarity Aggregation（FedSSA）的方法，该方法将每个客户端的模型分为异构（结构不同）特征提取器和同质（结构相同）分类头部。它通过基于语义相似度的头部参数聚合实现了本地到全局的知识传输。此外，通过自适应参数稳定策略实现了全局到本地的知识传输。

    Federated learning (FL) is a privacy-preserving collaboratively machine learning paradigm. Traditional FL requires all data owners (a.k.a. FL clients) to train the same local model. This design is not well-suited for scenarios involving data and/or system heterogeneity. Model-Heterogeneous Personalized FL (MHPFL) has emerged to address this challenge. Existing MHPFL approaches often rely on having a public dataset with the same nature of the learning task, or incur high computation and communication costs. To address these limitations, we propose the Federated Semantic Similarity Aggregation (FedSSA) approach, which splits each client's model into a heterogeneous (structure-different) feature extractor and a homogeneous (structure-same) classification header. It performs local-to-global knowledge transfer via semantic similarity-based header parameter aggregation. In addition, global-to-local knowledge transfer is achieved via an adaptive parameter stabilization strategy which fuses th
    
[^21]: RefinedFields: 对无约束场景的辐射场细化

    RefinedFields: Radiance Fields Refinement for Unconstrained Scenes

    [https://arxiv.org/abs/2312.00639](https://arxiv.org/abs/2312.00639)

    RefinedFields是第一种利用预训练模型改善无约束场景建模的方法。通过优化指导和交替训练过程，该方法能够从真实世界图像的先验条件中提取更丰富的细节，并在新视角合成任务中优于以往的方法。

    

    从无约束的图像中建模大场景被证明是计算机视觉中的一个重大挑战。现有方法处理野外场景建模是在封闭的环境中，没有对从真实世界图像获得的先验条件进行约束。我们提出了RefinedFields，这是我们所知的第一种利用预训练模型来改善野外场景建模的方法。我们使用预训练网络通过优化指导使用交替训练过程来细化K-Planes表示。我们进行了大量实验证实我们方法在合成数据和真实旅游照片集上的优点。RefinedFields增强了渲染场景的细节，优于以往在野外进行新视角合成任务的工作。我们的项目页面可以在https://refinedfields.github.io找到。

    Modeling large scenes from unconstrained images has proven to be a major challenge in computer vision. Existing methods tackling in-the-wild scene modeling operate in closed-world settings, where no conditioning on priors acquired from real-world images is present. We propose RefinedFields, which is, to the best of our knowledge, the first method leveraging pre-trained models to improve in-the-wild scene modeling. We employ pre-trained networks to refine K-Planes representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and outperforms previous work on the task of novel view synthesis in the wild. Our project page can be found at https://refinedfields.github.io .
    
[^22]: 人工智能是否为天气预报带来了第二次革命？

    Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?. (arXiv:2401.16669v1 [cs.LG])

    [http://arxiv.org/abs/2401.16669](http://arxiv.org/abs/2401.16669)

    人工智能技术在天气预报领域的快速发展代表了一个重大突破，它克服了传统模型的局限性，有潜力引领天气预报的第二次革命。

    

    人工智能技术的快速发展，特别是近年来，导致了几种大参数人工智能天气预报模型的出现。这些模型代表了一个重大突破，克服了传统数值天气预报模型的局限性，并表明了天气预报可能迎来第二次革命的潜力。本研究探讨了这些先进人工智能预报模型的演变，并在确定的共同点的基础上，提出了它们的发展的“三大规则”。我们讨论了人工智能在革命数值天气预报中的潜力，并简要概述了潜在的原因。此外，我们还探讨了大型人工智能天气预报模型未来发展前景的关键领域，将整个数值预报过程进行整合。通过将大型人工智能模型与其他信息综合，给出了一个应用实例。

    The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with 
    
[^23]: 评估用于基于OSINT的网络威胁意识的LLM聊天机器人

    Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])

    [http://arxiv.org/abs/2401.15127](http://arxiv.org/abs/2401.15127)

    本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。

    

    在快速发展的网络安全领域中，关于新兴威胁的知识共享至关重要，并构成了网络威胁情报的基础。在这个背景下，大型语言模型在网络安全领域越来越重要，提供了广泛的机遇。本研究探讨了ChatGPT、GPT4all、Dolly、Stanford Alpaca、Alpaca-LoRA和Falcon等聊天机器人在识别开源情报中与网络安全相关的文本方面的能力。我们评估了现有聊天机器人模型在自然语言处理任务中的能力。我们考虑了二分类和命名实体识别作为任务。本研究分析了从Twitter收集的经过充分验证的数据，该数据来源于以往的研究工作。在网络安全的二分类问题方面，商业模型Chatbot GPT-4实现了可接受的F1分数0.94，而开源模型GPT4all实现了F1分数0.90。然而，就网络安全实体识别而言，

    Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
    
[^24]: 神经网络验证的端到端基于集合的训练方法

    End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])

    [http://arxiv.org/abs/2401.14961](http://arxiv.org/abs/2401.14961)

    本论文提出了一种端到端基于集合的训练方法，用于训练鲁棒性神经网络进行形式化验证，并证明该方法能够简化验证过程并有效训练出易于验证的神经网络。

    

    神经网络容易受到对抗性攻击，即微小的输入扰动可能导致神经网络输出产生重大变化。安全关键环境需要对输入扰动具有鲁棒性的神经网络。然而，训练和形式化验证鲁棒性神经网络是具有挑战性的。我们首次采用端到端基于集合的训练方法来解决这个挑战，该训练方法能够训练出可进行形式化验证的鲁棒性神经网络。我们的训练方法能够大大简化已训练神经网络的后续形式化鲁棒性验证过程。相比于以往的研究主要关注增强神经网络训练的对抗性攻击，我们的方法利用基于集合的计算来训练整个扰动输入集合上的神经网络。此外，我们证明我们的基于集合的训练方法可以有效训练出易于验证的鲁棒性神经网络。

    Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
    
[^25]: 无限时域图滤波器：利用幂级数增强稀疏信息聚合

    Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v1 [cs.LG])

    [http://arxiv.org/abs/2401.09943](http://arxiv.org/abs/2401.09943)

    提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，并能集成任何幂级数并捕捉长程依赖关系。

    

    近年来，图神经网络在各种图学习任务中表现出了相当的有效性，特别是基于消息传递方法。然而，它们的性能常常受到有限接收域的限制，在稀疏图存在的情况下，挑战变得更加严峻。鉴于具有无限扩展能力的幂级数，我们提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。具体而言，我们的GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，可以在频谱和空间域中进行分析。此外，我们还从理论上证明了我们的GPFN是一个通用框架，可以集成任何幂级数并捕捉长程依赖关系。最后，我们在三个数据集上进行了实验，并展示了GPFN在稀疏图上的优越性能。

    Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three data
    
[^26]: 基于随机平滑梯度上升下降法的联邦极小极大优化研究

    Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization. (arXiv:2311.00944v1 [stat.ML])

    [http://arxiv.org/abs/2311.00944](http://arxiv.org/abs/2311.00944)

    本论文提出了一种新算法，FESS-GDA，利用平滑技术进行联邦极小极大优化。通过解决不同类型的联邦极小极大问题，我们证明了FESS-GDA的收敛性，并展示了其在实际联邦学习任务中的实际效果。

    

    近年来，由于其在各种机器学习任务中的广泛应用，联邦极小极大优化引起了越来越多的关注。虽然在集中非凸极小极大优化中，平滑交替梯度上升下降（Smoothed-AGDA）已经证明了其成功之处，但平滑技术在联邦设置中的作用和是否有所帮助尚未被探究。在本文中，我们提出了一种新算法，称为联邦随机平滑梯度上升下降（FESS-GDA），该算法利用平滑技术进行联邦极小极大优化。我们证明了FESS-GDA可以统一解决几类联邦极小极大问题，并为这些设置提供了新的或更好的收敛结果分析。我们展示了FESS-GDA在实际联邦学习任务中，如生成对抗网络（GANs）的训练和公平分类中的实际效率。

    In recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. While Smoothed Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. In this paper, we propose a new algorithm termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes the smoothing technique for federated minimax optimization. We prove that FESS-GDA can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. We showcase the practical efficiency of FESS-GDA in practical federated learning tasks of training generative adversarial networks (GANs) and fair classification.
    
[^27]: 高概率收敛边界下的非线性随机梯度下降在重尾噪声下的研究

    High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v1 [cs.LG])

    [http://arxiv.org/abs/2310.18784](http://arxiv.org/abs/2310.18784)

    本研究探讨了一类非线性随机梯度下降方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，即使噪声是重尾的，结果证明了对失败概率的对数依赖。这些结果适用于剪切、归一化和量化等任何具有有界输出的非线性函数。

    

    最近几个研究工作研究了随机梯度下降（SGD）及其剪切变体的高概率收敛。与普通的SGD相比，剪切SGD在实际中更加稳定，并且在理论上有对数依赖于失败概率的额外好处。然而，其他实际非线性SGD变体（如符号SGD、量化SGD和归一化SGD）的收敛性理解要少得多，这些方法实现了改进的通信效率或加速收敛。在本工作中，我们研究了一类广义非线性SGD方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，即使噪声是重尾的，我们证明了对失败概率的对数依赖。与剪切SGD的结果相比，我们的结果更为一般，适用于具有有界输出的任何非线性函数，如剪切、归一化和量化。

    Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantizati
    
[^28]: 形状和非形状神经网络的微分方程缩放极限

    Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks. (arXiv:2310.12079v1 [stat.ML])

    [http://arxiv.org/abs/2310.12079](http://arxiv.org/abs/2310.12079)

    最近的研究发现，对于具有形状激活函数的神经网络，其缩放极限可以由微分方程描述。然而，关于未经形状处理的神经网络的信息尚不明确。本文研究了两种未经形状处理的网络，发现它们也可以由类似的微分方程来描述，并给出了它们的一些特征。

    

    最近对具有形状激活函数（即随着网络规模增大而缩放的激活函数）的神经网络进行的分析表明，它们具有由微分方程描述的缩放极限。然而，这些结果不预先告诉我们关于“普通”非形状网络的任何信息，其中激活函数在网络规模增大时保持不变。在本文中，我们针对两种类型的非形状网络找到了类似的基于微分方程的渐近特征描述。首先，我们证明以下两种架构在初始化时会收敛到相同的无限深度和宽度极限：（i）带有残差分支上的 $d^{-1/2}$ 因子的全连接 ResNet，其中 $d$ 是网络的深度；（ii）带有深度 $d \ll$ 宽度 $n$ 和形状 ReLU 激活函数 (activation) 的多层感知机 (MLP)，以 $d^{-1/2}$ 的速率。其次，对于初始化的非形状 MLP，我们推导了层间相关性的一阶渐近修正。特别地，如果 $\rho_\ell$ 是第 $\ell$ 层的相关性，则...

    Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.  Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.  Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer
    
[^29]: RandCom：去中心化随机通信跳跃方法用于分布式随机优化

    RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])

    [http://arxiv.org/abs/2310.07983](http://arxiv.org/abs/2310.07983)

    RandCom是一种去中心化的随机通信跳跃方法，能够在分布式优化中通过概率性本地更新减少通信开销，并在不同的设置中实现线性加速。

    

    具有随机通信跳过的分布式优化方法因其在加速通信复杂性方面具有的优势而受到越来越多的关注。然而，现有的研究主要集中在强凸确定性设置的集中式通信协议上。在本研究中，我们提出了一种名为RandCom的分布式优化方法，它采用了概率性的本地更新。我们分析了RandCom在随机非凸、凸和强凸设置中的性能，并证明了它能够通过通信概率来渐近地减少通信开销。此外，我们证明当节点数量增加时，RandCom能够实现线性加速。在随机强凸设置中，我们进一步证明了RandCom可以通过独立于网络的步长实现线性加速。此外，我们将RandCom应用于联邦学习，并提供了关于实现线性加速的潜力的积极结果。

    Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
    
[^30]: ProbTS：一种用于探索深度时间序列预测的统一工具包。

    ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])

    [http://arxiv.org/abs/2310.07446](http://arxiv.org/abs/2310.07446)

    ProbTS是一个统一的工具包，用于协同和比较定制神经架构和深度生成模型在时间序列预测中的方法，揭示了它们的特点、优势和需要进一步研究的领域。

    

    时间序列预测在各个领域的各种应用中起着关键作用。随着深度学习的发展，这个领域分化成了两个显著的分支：一个专注于为时间序列定制特定的神经架构，另一个利用先进的深度生成模型进行概率预测。虽然这两个分支都取得了显著的进展，但它们在数据情景、方法论焦点和解码方案上的差异提出了深入而未被探索的研究问题。为了填补这一知识鸿沟，我们引入了ProbTS，这是一个创新的工具包，旨在协同和比较这两个不同的分支。ProbTS具备统一的数据模块、模块化的模型模块和全面的评估器模块，使我们能够重新审视和基准测试这两个分支的领先方法。通过ProbTS的审查，突显了它们各自的特点、相对优势和劣势，以及需要进一步研究的领域。

    Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
    
[^31]: 狮子秘密地解决受限制优化问题：正如李雅普诺夫所预测的。

    Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])

    [http://arxiv.org/abs/2310.05898](http://arxiv.org/abs/2310.05898)

    Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。

    

    通过程序搜索发现的新优化器Lion（进化的符号动量）在训练大型AI模型方面显示出有希望的结果。它在训练效果上与AdamW相当或更好，并具有更高的内存效率。正如我们可以从随机搜索程序的结果中期待的，Lion集成了几个现有算法的元素，包括符号动量、独立的权重衰减、Polak和Nesterov动量，但又不属于任何现有的理论基础优化器类别。因此，尽管Lion作为广泛任务的通用优化器表现良好，但其理论基础仍然不明确。这种缺乏理论的明确性限制了进一步增强和扩展Lion的可能性。本文旨在揭开Lion的神秘面纱。基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数$f(x)$的同时强制执行边界约束。

    Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
    
[^32]: 用于机器人自建模和运动规划的高自由度动态神经领域

    High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])

    [http://arxiv.org/abs/2310.03624](http://arxiv.org/abs/2310.03624)

    本论文介绍了一种使用神经领域进行机器人自建模和运动规划的方法。通过利用2D图像和相机姿态进行学习，无需深度图像或几何知识，实现了对高自由度物体的建模。在7自由度机器人测试中，所学的自建模与真实模型的差距仅为2%。

    

    机器人自建模是机器人物理形态的任务无关表示，可用于在没有经典几何运动学模型的情况下进行运动规划任务。特别是当后者很难工程化或机器人的运动学出现意外变化时，无人自建模是真正自主控制机器人的必要功能。在这项工作中，我们利用神经领域来使机器人能够以神经隐式查询模型的形式自建模其运动学，该模型仅通过带有相机姿态和配置的2D图像进行学习。这比现有方法具有更广泛的适用性，因为它不依赖深度图像或几何知识。为此，我们提出了一种基于编码器的新型动态物体中心场景的神经密度场架构，该架构可以适应高自由度（DOFs）的条件。在一个7自由度的机器人测试设置中，学得的自建模与真实模型之间的Chamfer-L2距离仅为2%。

    A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of
    
[^33]: AutoCast++：利用零-shot基于排名的上下文检索增强世界事件预测

    AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval. (arXiv:2310.01880v1 [cs.LG])

    [http://arxiv.org/abs/2310.01880](http://arxiv.org/abs/2310.01880)

    AutoCast++是一个零-shot基于排名的上下文检索系统，用于从广泛的新闻文档集合中进行事件预测。

    

    基于机器的实时事件预测因其在决策过程中能提供知情决策而受到关注。然而，传统的预测方法主要依赖于结构化数据，如时间序列，而最近语言模型的突破使得可以使用非结构化文本进行预测。本文介绍了AutoCast++，它是一个零-shot基于排名的上下文检索系统，旨在针对广泛的新闻文档集合进行事件预测。我们的方法首先根据零-shot的问题-段落相关性对文章进行重新排名，从而找到与语义相关的新闻。之后，所选的文章将进行零-shot处理用于事件预测。

    Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. In particular, (Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles for answering forecasting queries. Nevertheless, existing methods still trail behind human performance. The cornerstone of accurate forecasting, we argue, lies in identifying a concise, yet rich subset of news snippets from a vast corpus. With this motivation, we introduce AutoCast++, a zero-shot ranking-based context retrieval system, tailored to sift through expansive news document collections for event forecasting. Our approach first re-ranks articles based on zero-shot question-passage relevance, honing in on semantically pertinent news. Following this, the chosen articles are subjected to zero-shot 
    
[^34]: 频道视觉Transformer：一张图值C x 16 x 16个词

    Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])

    [http://arxiv.org/abs/2309.16108](http://arxiv.org/abs/2309.16108)

    本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。

    

    视觉Transformer在现代计算机视觉领域中已经成为一种强大的架构。然而，它在某些图像领域的应用，如显微镜和卫星成像，面临着独特的挑战。在这些领域中，图像通常包含多个通道，每个通道都携带着语义上不同和独立的信息。此外，模型必须对输入通道的稀疏性表现出鲁棒性，在训练或测试过程中可能没有密集可用的通道。在本文中，我们提出了对ViT架构的修改，增强了对输入通道之间的推理，并引入了分层通道采样(HCS)作为一种附加的正则化技术，以确保在测试过程中仅出现部分通道时的鲁棒性。我们提出的模型ChannelViT独立地构建补丁令牌并利用可学习的通道嵌入将其添加到补丁令牌中，类似于位置嵌入。我们进行了评估

    Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
    
[^35]: 针对生物信号的频率感知掩码自编码器的多模态预训练

    Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals. (arXiv:2309.05927v1 [cs.LG])

    [http://arxiv.org/abs/2309.05927](http://arxiv.org/abs/2309.05927)

    本研究提出了一种名为$\texttt{bio}$FAME的频率感知掩码自编码器，用于多模态生物信号的预训练。其通过在频率空间中对生物信号进行表示参数化，利用固定大小的傅里叶变换运算符进行全局令牌混合，并通过频率维持预训练策略保持每个输入通道中的频率成分。

    

    利用来自生物信号的多模态信息对人们的身心状态进行综合建模非常重要。然而，多模态生物信号通常在预训练和推断数据集之间存在重大的分布偏移，这源于任务规范的变化或者模态组合的差异。为了在潜在分布偏移的情况下实现有效的预训练，我们提出了一种频率感知的掩码自编码器（$\texttt{bio}$FAME），该自编码器学习在频率空间中对生物信号的表示进行参数化。$\texttt{bio}$FAME包含一个频率感知变压器，利用基于傅里叶变换的固定大小的运算符进行全局令牌混合，与输入的长度和采样率无关。为了保持每个输入通道中的频率成分，我们还采用了一种频率维持预训练策略，在潜空间中执行掩码自编码。最终的架构有效地捕获不同任务间的频率特征和模态组合的变化。

    Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectivel
    
[^36]: 一种轻量化且可传递的设计用于稳健的乐高操纵

    A Lightweight and Transferable Design for Robust LEGO Manipulation. (arXiv:2309.02354v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.02354](http://arxiv.org/abs/2309.02354)

    本文介绍了一种轻量化且可传递的设计，用于解决机器人乐高操纵中的复杂性和精确性要求。通过硬件软件协同设计和进化策略优化，实现了高效可靠的乐高操纵，并展示了该设计的普适性和可传递性。

    

    乐高是一个用于原型化像素化对象的众所周知的平台。然而，机器人乐高原型化（即操纵乐高积木）由于紧密的连接和精确性要求而具有挑战性。本文研究了安全高效的机器人乐高操纵。具体而言，本文通过硬件软件协同设计减少了操纵的复杂性。设计了一个末端工具（EOAT），它减少了问题维度，使大型工业机器人能够轻松操纵乐高积木。此外，本文使用进化策略安全地优化机器人运动，用于乐高操纵。实验表明，EOAT在操纵乐高积木方面表现可靠，而学习框架可以有效且安全地将操纵性能提高到100%的成功率。所设计的协同设计已经部署到多台机器人（FANUC LR-mate 200id/7L和Yaskawa GP4）上，以展示其普适性和可传递性。最后，我们展示了本研究的创新和贡献。

    LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the p
    
[^37]: 用排序不变的编码器和更紧的变分边界学习多模态生成模型

    Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds. (arXiv:2309.00380v1 [stat.ML])

    [http://arxiv.org/abs/2309.00380](http://arxiv.org/abs/2309.00380)

    本文提出了一种用于多模态数据的深度潜变量模型，并开发了更灵活的编码特征聚合方案，能够紧密地下界数据对数似然。

    

    设计用于多模态数据的深度潜变量模型一直是机器学习研究中的一个重要主题。多模态变分自编码器 (VAE) 是一种常用的生成模型类别，它学习能够共同解释多种模态的潜在表示。各种客观函数已被提出用于这样的模型，往往以多模态数据对数似然的下界以及信息论方面的考虑为动机。为了对不同模态子集进行编码，我们经常使用并展示了产品型专家 (PoE) 或者混合型专家 (MoE) 聚合方案，这些方案在生成质量或者多模态一致性等方面具有不同的权衡。在本研究中，我们考虑了一个能够紧密地下界数据对数似然的变分边界。我们通过将不同模态的编码特征组合起来，开发了更灵活的聚合方案，这些方案推广了 PoE 或者 MoE 方法。

    Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modali
    
[^38]: 量子神经网络解释性与不可解释性的探索

    Explicability and Inexplicability in the Interpretation of Quantum Neural Networks. (arXiv:2308.11098v1 [quant-ph])

    [http://arxiv.org/abs/2308.11098](http://arxiv.org/abs/2308.11098)

    本文探索了量子神经网络的可解释性，引入了不可解释性带的概念，为理解如何构建负责任且可追究的量子人工智能模型迈出了一步。

    

    人工智能(AI)方法的可解释性，特别是深度神经网络的可解释性，引起了广泛关注，因为AI支持的系统往往具有不可解释的行为。解释这种模型的可解释性是构建可信系统的关键组成部分。许多方法用于解决这个问题，但它们不能明显地推广到量子环境中。在这里，我们使用量子和经典神经网络的局部模型无关解释性指标来探索量子神经网络的可解释性。我们引入了不可解释性带的概念，表示在该区域内的数据样本没有解释，很可能是不可避免的随机量子测量的受害者。我们将此视为理解如何构建负责任且可追究的量子人工智能模型的一步。

    Interpretability of artificial intelligence (AI) methods, particularly deep neural networks, is of great interest due to the widespread use of AI-backed systems, which often have unexplainable behavior. The interpretability of such models is a crucial component of building trusted systems. Many methods exist to approach this problem, but they do not obviously generalize to the quantum setting. Here we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures of quantum and classical neural networks. We introduce the concept of the band of inexplicability, representing the interpretable region in which data samples have no explanation, likely victims of inherently random quantum measurements. We see this as a step toward understanding how to build responsible and accountable quantum AI models.
    
[^39]: 可解释的基于图神经网络的表格数据处理方法

    Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])

    [http://arxiv.org/abs/2308.08945](http://arxiv.org/abs/2308.08945)

    本论文提出了一种称为IGNNet的方法，可以在处理表格数据时使用图神经网络，该方法能够产生可解释的模型，从原始输入特征精确计算预测结果，并且在性能上与最先进的机器学习算法性能相当。

    

    在现实世界的应用中，表格格式的数据经常出现。图神经网络（GNNs）近期被扩展以有效处理此类数据，通过表示学习捕捉特征之间的相互作用。然而，这些方法本质上产生了黑盒模型，以深度神经网络的形式存在，使得用户无法理解模型预测的逻辑。我们提出了一种称为IGNNet（基于图神经网络的可解释表格数据处理方法）的方法，它限制学习算法以产生可解释的模型，该模型展示了如何从原始输入特征准确计算预测结果。通过大规模实证研究，我们展示了IGNNet与面向表格数据的最先进机器学习算法（包括XGBoost，Random Forests和TabNet）性能相当。同时，结果显示从IGNNet获得的解释与真实情况一致。

    Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
    
[^40]: 在观测代价敏感强化学习中的动态观测策略

    Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])

    [http://arxiv.org/abs/2307.02620](http://arxiv.org/abs/2307.02620)

    本文研究了在观测代价敏感强化学习中，强化学习代理在每个时间步不需要昂贵的测量，提出了一种新的方法DMSOA，并在多个环境中进行了评估，结果表明DMSOA能够以更少的决策步骤和测量次数学到更好的策略。

    

    强化学习已被证明可以学习复杂任务的高级控制策略，包括游戏、机器人、供暖与制冷系统和文本生成。然而，强化学习中的动作-感知循环通常假设在每个时间步都可以获得对环境状态的测量，且不产生成本。然而，在深海和行星机器人探索、材料设计和医学等应用中，测量或者近似环境状态可能会产生高昂的成本。本文调查了近来不断增长的文献，采取了RL代理可能不需要或者不想在每个时间步进行昂贵测量的观点。在这个背景下，我们提出了Deep Dynamic Multi-Step Observationless Agent (DMSOA)，并将其与文献进行对比，并在OpenAI gym和Atari Pong环境中进行了实证评估。我们的结果显示，DMSOA能够以更少的决策步骤和测量次数学到更好的策略。

    Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
    
[^41]: 噪声稳定优化对于具有最优收敛率的平坦极小值的影响

    Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])

    [http://arxiv.org/abs/2306.08553](http://arxiv.org/abs/2306.08553)

    本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。

    

    本文研究通过加入加权扰动来找到平坦的极小值。给定一个非凸函数$f:\mathbb{R}^d\rightarrow \mathbb{R}$和一个$d$维分布$\mathcal{P}$，我们扰动$f$的权重，并定义$F(W)=\mathbb{E}[f({W+U})]$，其中$U$是一个从$\mathcal{P}$中随机抽取的样本。这个过程通过$f$的海森矩阵的迹来诱导正则化，以适应于小的、各向同性的高斯扰动。因此，加权扰动的函数偏向于带有低海森矩阵迹的极小值。本文提出了一种类似于SGD的算法，在计算梯度之前注入随机噪声，同时利用$\mathcal{P}$的对称性来减少方差。我们还提供了严格的分析，证明了...

    We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing
    
[^42]: 基于条件扩散模型的语义化三维医学图像合成

    Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])

    [http://arxiv.org/abs/2305.18453](http://arxiv.org/abs/2305.18453)

    这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。

    

    本文提出了Med-DDPM，它是一种创新的解决方案，使用扩散模型进行语义化的三维医学图像合成，解决了医学成像中数据稀缺、采集方法不一致和隐私问题等普遍存在的问题。实验结果表明，扩散模型在稳定性和性能方面都超过了生成对抗网络（GAN），能够生成高质量、逼真的三维医学图像。Med-DDPM的独特特点在于使用语义条件进行三维图像合成的扩散模型。通过控制像素级掩码标签的生成过程，它便于创建逼真的医学图像。经验证明，Med-DDPM在精度、稳定性和多样性等指标上优于GAN技术。此外，Med-DDPM在增强分割模型的准确性方面也优于传统的增强技术和GAN合成图像。它解决了医学图像合成中的难点。

    This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
    
[^43]: 基于例行血液检查数值的机器学习模型鉴别病毒和细菌感染。

    Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values. (arXiv:2305.07877v1 [cs.LG])

    [http://arxiv.org/abs/2305.07877](http://arxiv.org/abs/2305.07877)

    本研究开发了一种基于血液检查数值的病毒与细菌机器学习模型，用于准确识别感染类型。该模型在CRP水平10-40 mg/L范围内表现出更好的区分细菌和病毒感染的准确性，证明了多种血液参数对于诊断决策的重要性。

    

    随着抗生素耐药性日益威胁，正确区分细菌和病毒感染以进行正确的抗生素使用变得越来越重要。本研究开发了一种基于16个例行血液检查结果、C-反应蛋白水平、生物性别和年龄的病毒与细菌机器学习模型，用于区分这些感染类型。使用单个医疗中心的44,120个案例数据集，"病毒 vs. 细菌"模型表现出令人瞩目的82.2%的准确率，0.129的Brier得分和0.91的ROC曲线下面积，超越了传统CRP决策规则模型的性能。该模型在CRP范围为10-40 mg/L时表现出显著的改进准确性，这个范围内仅靠CRP无法为细菌和病毒感染进行区分的诊断价值有限。这些发现强调了在诊断决策中考虑多种血液参数的重要性，并建议病毒 vs. 细菌模型使得应用于临床决策成为可能。

    The growing threat of antibiotic resistance necessitates accurate differentiation between bacterial and viral infections for proper antibiotic administration. In this study, a Virus vs. Bacteria machine learning model was developed to discern between these infection types using 16 routine blood test results, C-reactive protein levels, biological sex, and age. With a dataset of 44,120 cases from a single medical center, the Virus vs. Bacteria model demonstrated remarkable accuracy of 82.2%, a Brier score of 0.129, and an area under the ROC curve of 0.91, surpassing the performance of traditional CRP decision rule models. The model demonstrates substantially improved accuracy within the CRP range of 10 40 mg/L, an interval in which CRP alone offers limited diagnostic value for distinguishing between bacterial and viral infections. These findings underscore the importance of considering multiple blood parameters for diagnostic decision-making and suggest that the Virus vs. Bacteria model 
    
[^44]: 平等攸关不等于平等个人几率: 用于组和个人公平的后处理方法

    Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness. (arXiv:2304.09779v1 [cs.LG])

    [http://arxiv.org/abs/2304.09779](http://arxiv.org/abs/2304.09779)

    研究发现，通过平等化受保护子群体之间的预测分布来实现组公平和视相似个体同等对待实现个人公正是不兼容的。 并提出了一种构建连续概率函数的解决方法，来实现组和个人公平。

    

    组公平通过平衡受保护子群体之间的预测分布来实现；个人公平要求将相似的个体视为同等对待。然而，当评分模型通过不连续的概率函数进行校准时，这两个目标是不兼容的，其中个体可能会随机分配由固定概率确定的结果。这个过程可能会使来自同一受保护组的两个相似个体的分类几率差别明显不同，这是个人公平的明显违反。为每个受保护子群体分配唯一的几率也可能会阻止一个子群体的成员接到另一个子群体有正面结果的平等机会，我们认为这是另一种称为个人几率的不公平类型。我们通过构建受群体阈值约束的连续概率函数来解决所有这些问题。我们的解决方案保留了模型的预测能力。

    Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive powe
    
[^45]: 利用蒙特卡罗采样的联盟结构群体解释器逼近

    Approximation of group explainers with coalition structure using Monte Carlo sampling on the product space of coalitions and features. (arXiv:2303.10216v1 [cs.LG])

    [http://arxiv.org/abs/2303.10216](http://arxiv.org/abs/2303.10216)

    本文研究了使用蒙特卡罗采样算法逼近联盟结构群体解释器的复杂问题，提出一种快速、易于实现且与特定模型无关的新型算法，并提供了严密的统计分析及误差界的证明。

    

    近年来，许多机器学习（ML）解释技术都采用了合作博弈理论的思想。这些游戏理论解释器由于复杂度高，在实际环境下无法精确计算。本文采用适当的样本空间估计期望值，设计了一种新型蒙特卡罗采样算法, 来以依赖于背景数据集大小的线性复杂度估计特定机器学习模型和预测向量的边际博弈的联盟值，这种方法快速、易于执行且与其他已知的许多更复杂和特定于模型的估计技术具有类似的统计准确性。我们提供了严密的证明。

    In recent years, many Machine Learning (ML) explanation techniques have been designed using ideas from cooperative game theory. These game-theoretic explainers suffer from high complexity, hindering their exact computation in practical settings. In our work, we focus on a wide class of linear game values, as well as coalitional values, for the marginal game based on a given ML model and predictor vector. By viewing these explainers as expectations over appropriate sample spaces, we design a novel Monte Carlo sampling algorithm that estimates them at a reduced complexity that depends linearly on the size of the background dataset. We set up a rigorous framework for the statistical analysis and obtain error bounds for our sampling methods. The advantage of this approach is that it is fast, easily implementable, and model-agnostic. Furthermore, it has similar statistical accuracy as other known estimation techniques that are more complex and model-specific. We provide rigorous proofs of s
    
[^46]: 一次性领域自适应在基于视频的手术技能评估中的应用

    One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00812](http://arxiv.org/abs/2301.00812)

    本文提出了一种元学习模型A-VBANet，可以通过一次性学习提供领域不可知的手术技能分类，成功地适应了模拟任务和腹腔镜胆囊切除术，为基于视频的手术技能评估提供了领域不可知程序。

    This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.

    深度学习已经实现了手术技能的自动和客观评估。然而，深度学习模型需要大量数据，并且受限于其训练领域。这阻止了它们过渡到数据有限的新任务。因此，领域自适应对于在现实生活中实现深度学习至关重要。在这里，我们提出了一种元学习模型A-VBANet，它可以通过一次性学习提供领域不可知的手术技能分类。我们在五个腹腔镜和机器人手术模拟器上开发了A-VBANet。此外，我们在腹腔镜胆囊切除术的手术室视频上进行了测试。我们的模型成功地适应了模拟任务，准确率高达99.5%（一次性）和99.9%（少量样本），在腹腔镜胆囊切除术中的准确率为89.7%。我们首次提供了基于视频的手术技能评估的领域不可知程序。这种方法的一个重要影响是它允许使用来自手术模拟器的数据来评估手术表现。

    Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
    

