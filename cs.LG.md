# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning.](http://arxiv.org/abs/2307.01849) | 本论文提出了一种名为Crossway Diffusion的方法，通过使用自监督学习目标增强了基于扩散的视觉机器人策略学习，实验证明了其在各种机器人任务中的有效性和优势。 |
| [^2] | [Empirical Sample Complexity of Neural Network Mixed State Reconstruction.](http://arxiv.org/abs/2307.01840) | 该论文通过数值研究混合态重构技术的性能，展示了在不同混合度下不同神经量子态编码的效率，并提出了设计更高效编码的需求。 |
| [^3] | [DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation.](http://arxiv.org/abs/2307.01831) | DiT-3D是一种针对3D形状生成的新型扩散Transformer，通过在纯Transformer上进行去噪处理，结合3D位置和补丁嵌入来聚合体素化点云的输入，并引入了3D窗口注意力以降低计算成本。 |
| [^4] | [Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses.](http://arxiv.org/abs/2307.01827) | 这项研究探讨了神经网络内部对训练数据的记忆过程，并在多个方向上扩展了已有的研究。研究发现，使用权重衰减可以增加重建能力，同时还分析了神经元数量对网络易受重建方案影响的影响。 |
| [^5] | [Structural Balance and Random Walks on Complex Networks with Complex Weights.](http://arxiv.org/abs/2307.01813) | 这篇论文研究了复杂网络中的结构平衡和随机游走，介绍了用于复数权重网络的结构平衡的分类方法，并研究了其动态属性。通过分析复数权重网络上的随机游走，发现当网络是结构平衡的时候，可以实现局部一致性，而当网络严格不平衡时，可以实现全局一致性。 |
| [^6] | [Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators.](http://arxiv.org/abs/2307.01804) | 本论文介绍了一种使用傅立叶神经算子捕捉增材制造过程中局部温度演变的数据驱动模型，并提出使用R^2指标来评估模型的一种方法。 |
| [^7] | [Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI.](http://arxiv.org/abs/2307.01798) | 本文提出了一种基于边缘感知的多任务网络用于在多模态非对比度MRI上对肝肿瘤进行定量分割和不确定性预测。通过设计边缘感知特征聚合模块与多任务学习，该网络能够有效融合多模态信息并提供准确的边界信息，从而提高诊断准确性。 |
| [^8] | [GHOST: A Graph Neural Network Accelerator using Silicon Photonics.](http://arxiv.org/abs/2307.01782) | GHOST是第一个使用硅光子学的图神经网络加速器，高效地处理和加速GNN，克服了传统加速器的缺点，使得它可以应用于各种GNN模型和架构的推理。 |
| [^9] | [FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices.](http://arxiv.org/abs/2307.01780) | FedHIL是一种面向移动设备的鲁棒室内定位的异构鲁棒联邦学习框架，通过结合室内定位和联邦学习来提高在设备异构环境中的定位精度。 |
| [^10] | [Shapley Sets: Feature Attribution via Recursive Function Decomposition.](http://arxiv.org/abs/2307.01777) | Shapley Sets是一种通过递归函数分解的方法进行特征归因，它避免了Shapley值特征归因中的特征相互作用问题，并且对于具有复杂依赖结构的数据类型特别有优势。 |
| [^11] | [Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics.](http://arxiv.org/abs/2307.01770) | 本文提出了一种快速计算最优输运的方法，通过切片Wasserstein广义测地线进行近似，得到了一个基于一维最优投影的代理距离min-SWGG，并提供了相关的传输计划。这种方法具有较低的计算复杂度，适用于优化算法。 |
| [^12] | [Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana.](http://arxiv.org/abs/2307.01767) | 本文研究通过加纳全生命周期作物疾病识别案例，展示了本地化数据工作作为数据中心机器学习的前提条件的重要性，为公共领域任务如农业生产力和食品安全提供了有用的解决方案。 |
| [^13] | [Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification.](http://arxiv.org/abs/2307.01759) | 提出了一种用于自闭症谱系障碍分类的多atlas增强transformer框架，利用静息态功能磁共振成像数据进行建模，采用自我监督预训练提高了分类性能。 |
| [^14] | [Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies.](http://arxiv.org/abs/2307.01753) | 本研究利用DESI成像调查的亮红星系的角聚类信息限制了局部原初非高斯性参数fNL，发现在假设宇宙规律性关系的情况下，fNL为47^{+14(+29)}_{-11(-22)}，使用更积极的处理方法后，最大似然值略微偏离fNL≈5。 |
| [^15] | [SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection.](http://arxiv.org/abs/2307.01750) | 本文提供了针对单领域广义目标检测的新框架SRCD，通过学习和维护自增广义跨领域样本的语义结构，提高模型的泛化能力。 |
| [^16] | [RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network.](http://arxiv.org/abs/2307.01725) | RRCNN通过利用深度学习的技术，包括卷积神经网络、残差结构和非线性激活函数，以创新的方式计算信号的局部平均，并提出一种新的非平稳信号分解方法。 |
| [^17] | [MOPO-LSI: A User Guide.](http://arxiv.org/abs/2307.01719) | MOPO-LSI是一款开源的多目标投资组合优化库，为可持续投资提供用户指南，并介绍了版本1.0的问题设置、工作流程和超参数。 |
| [^18] | [On the Constrained Time-Series Generation Problem.](http://arxiv.org/abs/2307.01717) | 这篇论文研究了约束时间序列生成问题。在实际应用中，合成时间序列被广泛用于增强历史时间序列数据集，提高机器学习算法的性能，放大稀有事件的发生，以及创建反事实情景。然而，现有的方法在满足约束方面存在问题，需要重新训练且计算代价高，或者在复杂约束条件下不切实际。 |
| [^19] | [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework.](http://arxiv.org/abs/2307.01715) | 本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。 |
| [^20] | [Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning.](http://arxiv.org/abs/2307.01708) | 本文研究了风险敏感强化学习中的分布模型等效性问题。我们提出了两种新的模型等价性概念，并展示了如何将这些概念应用于增强任何基于模型的风险敏感算法。 |
| [^21] | [Online Learning and Solving Infinite Games with an ERM Oracle.](http://arxiv.org/abs/2307.01689) | 这项工作提出了一种仅依赖ERM预言机调用的在线学习算法，该算法在可实现情况下具有有限的遗憾，并在不可知情况下具有亚线性增长的遗憾。同时，还提供了类似的结果用于非参数博弈环境中的学习算法，即仅依赖最佳响应预言机的学习算法，并收敛到近似极小-极大均衡点。 |
| [^22] | [Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services.](http://arxiv.org/abs/2307.01684) | 本文提出了一种分布式实时GNN推断框架Fograph，该框架利用了靠近物联网数据源的多个雾节点的资源，通过引入异构感知执行规划和GNN特定的压缩技术，以最大限度地发挥雾计算的架构优势。 |
| [^23] | [Learning Discrete Weights and Activations Using the Local Reparameterization Trick.](http://arxiv.org/abs/2307.01683) | 本论文研究了使用局部重新参数化技巧学习离散权重和激活的方法，通过二值化网络来降低计算复杂度，提高神经网络推理效率。 |
| [^24] | [Training Energy-Based Models with Diffusion Contrastive Divergences.](http://arxiv.org/abs/2307.01668) | 本文提出了一种使用扩散对比发散（DCD）训练能量模型（EBM）的方法，相较于传统的对比发散（CD），DCD在计算效率上更高，并且不受非可忽略梯度项的限制。 |
| [^25] | [Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks.](http://arxiv.org/abs/2307.01649) | 本文研究了卷积残差网络在非参数分类任务中的性能。研究表明，通过使用权重衰减的ConvResNeXts，可以隐含地实现对模块的稀疏性，从而使网络能够适应低维流形的平滑性和结构，并高效地学习函数。 |
| [^26] | [SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation.](http://arxiv.org/abs/2307.01646) | 本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。 |
| [^27] | [Heuristic Algorithms for the Approximation of Mutual Coherence.](http://arxiv.org/abs/2307.01639) | 本研究提出了近似互连性的启发式算法，用于加速计算互连性指标。算法通过建模确认值的分布并估计其模型参数，然后利用分布的期望值来近似计算互连性。 |
| [^28] | [HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2307.01636) | 本文介绍了HAGNN框架，它是一种用于处理异构图的混合聚合方法。HAGNN同时利用元路径邻居和直接连接邻居进行节点聚合，将整个聚合过程分为基于元路径的内类型聚合和无元路径的跨类型聚合。这种方法能够全面利用异构图中的类型语义信息。 |
| [^29] | [Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network.](http://arxiv.org/abs/2307.01622) | 本研究提出了一种基于循环趋势预测神经网络的嵌入式调度预测算法，在智能家居环境中实现了高效的住宅需求控制，并同时预测可再生能源发电。该算法具有鲁棒性，能够应对预测误差。 |
| [^30] | [SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2307.01616) | 本文介绍了SageFormer，一种面向多变量时间序列预测的系列感知图增强Transformer模型，通过图结构有效捕捉和建模序列之间的依赖关系，在表示不同序列中的时间模式和减少序列间冗余信息等方面取得了优越性能。 |
| [^31] | [Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction.](http://arxiv.org/abs/2307.01610) | 本文提出了一种防御技术HAMP，可以在不需要额外数据的情况下，通过强制模型进行不太自信的预测，达到强大的成员隐私保护和高准确性的目标。 |
| [^32] | [Prototypes as Explanation for Time Series Anomaly Detection.](http://arxiv.org/abs/2307.01601) | 本文提出了ProtoAD，利用原型作为解释来解释时间序列的异常检测。该方法提供了透明度和直观理解，为黑盒模型的安全关键应用打开了新的突破口。 |
| [^33] | [A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management.](http://arxiv.org/abs/2307.01599) | 提出了一种名为CryptoRLPM的基于强化学习的系统，利用链上数据进行加密货币投资组合管理，背测结果显示其在三个投资组合上的表现优于所有基准。 |
| [^34] | [Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework.](http://arxiv.org/abs/2307.01597) | 本文提出了Seq2Peak框架，针对高峰小时序列预测任务，该框架通过解决高度非平稳性和性能评估问题，成功缩小了在常规时间序列预测模型中观察到的性能差距。 |
| [^35] | [Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising.](http://arxiv.org/abs/2307.01593) | 这篇论文提出了一个跨元素组合选择框架CECS，用于解决显示广告中多元素创意选择的问题，通过采用跨元素交互的方式进行编码，将创意组合问题转化为多个创意元素级联选择问题。 |
| [^36] | [Learning Lie Group Symmetry Transformations with Neural Networks.](http://arxiv.org/abs/2307.01583) | 本研究通过神经网络探索和表征数据集中的未知李群对称变换，展示了该方法在模型选择和数据分析等领域的实用性。 |
| [^37] | [IAdet: Simplest human-in-the-loop object detection.](http://arxiv.org/abs/2307.01582) | 本文提出了一种最简单的人机交互目标检测方法，其中的关键创新是智能注释（IA）策略和开源的IAdet工具。对于PASCAL VOC数据集，IAdet工具能够减少数据库标注时间，并提供免费训练好的模型，为强大的人机交互目标检测系统提供了改进的可能。 |
| [^38] | [Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation.](http://arxiv.org/abs/2307.01578) | 本文研究了人机协作注释中的二元问题，提出了从最优通用解决方案到实际高效方法的一系列解决方案。我们将问题构建为在给定预测器的情况下用最少的是/否问题来完全注释一个二元分类数据集。通过编码理论和启发式算法，我们提供了一个计算可行且高效的替代方案。 |
| [^39] | [Approximate information for efficient exploration-exploitation strategies.](http://arxiv.org/abs/2307.01563) | 本文提出了一种称为AIM的算法，用于解决决策中的探索-利用困境，特别针对多臂赌博机问题。AIM算法利用近似熵梯度来选择每个时间点要拉动的手臂，与Infomax和Thompson抽样相比，在性能上能够匹配，同时具有更好的计算速度、确定性和可计算性。经实证评估表明，AIM算法符合Lai-Robbins渐进界，对于不同的先验具有鲁棒性。 |
| [^40] | [Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones.](http://arxiv.org/abs/2307.01559) | 本文提出了一种新颖的分布式边缘-雾执行方案，通过在纳米无人机上冗余执行随机子网络来验证雾计算。与完全在机载上运行的最先进视觉姿态估计网络相比，采用分布式方式执行的更大网络显著提高了性能，并且在攻击情况下能够快速检测到。 |
| [^41] | [Scalable variable selection for two-view learning tasks with projection operators.](http://arxiv.org/abs/2307.01558) | 本文提出了一种可扩展的变量选择方法，适用于两视图学习任务或向量值监督学习问题，能够处理规模极大的选择任务，并利用投影算子以及核函数进行相关性衡量和非线性相关模型的处理。 |
| [^42] | [Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation.](http://arxiv.org/abs/2307.01524) | 该论文提出了使用学习压缩表示进行语义分割的方法，以减少解压缩操作的延迟开销，并在实验证明了这种方法的有效性。 |
| [^43] | [Deep Attention Q-Network for Personalized Treatment Recommendation.](http://arxiv.org/abs/2307.01519) | 本研究提出了Deep Attention Q-Network，利用Transformer架构在深度强化学习框架内，个性化推荐治疗方案，通过高效整合过去的病患观察信息，解决了仅依赖当前观察信息的限制，从而提高了治疗效果。 |
| [^44] | [SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT.](http://arxiv.org/abs/2307.01514) | 这篇论文提出了一种名为SelfFed的自监督联邦学习框架，用于解决IoMT中的数据异质性和标签匮乏问题。该框架包括预训练和微调两个阶段，通过分散训练和增强建模来克服数据异质性和标签稀缺问题。 |
| [^45] | [Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction.](http://arxiv.org/abs/2307.01507) | 本论文提出了一种新的关注关系的子图嵌入与对比学习方法RaSECo，用于预测多关系药物相互作用。该方法通过构建不同的药物图和采用对比学习机制，能够解决现有方法中对于新药物过拟合的问题。 |
| [^46] | [All in One: Multi-task Prompting for Graph Neural Networks.](http://arxiv.org/abs/2307.01504) | 本文提出了一种新颖的图模型的多任务提示方法，通过统一图提示和语言提示的格式，填补了预训练模型与各种图任务之间的差距。 |
| [^47] | [Accelerated stochastic approximation with state-dependent noise.](http://arxiv.org/abs/2307.01497) | 该论文研究了一类具有状态相关噪声的随机平滑凸优化问题。通过引入两种非欧几里得加速随机逼近算法，实现了在精度、问题参数和小批量大小方面的最优性。 |
| [^48] | [Review of Deep Learning-based Malware Detection for Android and Windows System.](http://arxiv.org/abs/2307.01494) | 本文综述了基于深度学习的安卓和Windows系统恶意软件检测技术，这些技术在检测不同恶意软件家族上均取得了完美的准确性。 |
| [^49] | [Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series.](http://arxiv.org/abs/2307.01482) | 提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。 |
| [^50] | [Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2307.01472) | DOM2是一种离线多智能体强化学习模型，通过扩散策略的改进，提高了算法在性能、泛化能力和数据效率方面的表现。DOM2在多智能体粒子和多智能体MuJoCo环境中优于现有算法，并在移位环境中具有更好的泛化能力。此外，DOM2还展现了卓越的数据效率，只使用较少的数据即可达到最先进的性能水平。 |
| [^51] | [A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding.](http://arxiv.org/abs/2307.01470) | 本文综述了驾驶员凝视估计的基本知识、估计方法以及在实际驾驶场景中的应用。通过讨论不同的数据收集方法和算法技术，对驾驶员的凝视行为进行了理解。 |
| [^52] | [Causal Reinforcement Learning: A Survey.](http://arxiv.org/abs/2307.01452) | 这项综述总结了因果强化学习的研究文献，强调因果关系的重要作用，它能够形式化知识并实现有效的知识传递。 |
| [^53] | [A Double Machine Learning Approach to Combining Experimental and Observational Data.](http://arxiv.org/abs/2307.01449) | 这种双机器学习方法将实验和观测研究结合起来，能够测试假设的违反情况并一致估计处理效应。它提供了半参数高效的处理效应估计器。这种方法在实际环境中是可行的。 |
| [^54] | [On Conditional and Compositional Language Model Differentiable Prompting.](http://arxiv.org/abs/2307.01446) | 本论文研究了条件和组合的可微提示方法，提出了Prompt Production System（PRopS）模型，通过将任务说明或输入元数据转化为连续的提示，使预训练语言模型（PLM）能够生成任务特定的输出。该模型利用了神经网络结构和离散规则的学习，适用于组合式迁移学习和少样本学习。实证和理论分析表明，PRopS在PLM适应中始终优于其他技术，并且通常改进了完全微调的方法。 |
| [^55] | [Learning to Branch in Combinatorial Optimization with Graph Pointer Networks.](http://arxiv.org/abs/2307.01434) | 本文提出了一种使用图指针网络学习分支策略的方法，通过提取图特征和全局特征来表示求解器状态，并结合了图神经网络和指针机制，通过模仿经典的强分支专家规则来训练模型。实验证明该方法在解决组合优化问题上明显优于其他方法。 |
| [^56] | [Generative Flow Networks: a Markov Chain Perspective.](http://arxiv.org/abs/2307.01422) | 通过使用马尔可夫链，本文提出了生成流网络的新视角，对于具有明确组合结构的样本，通过将采样视为序贯决策问题，能够消除马尔可夫链蒙特卡洛方法在高度多模式分布中收敛缓慢的问题。 |
| [^57] | [Free energy of Bayesian Convolutional Neural Network with Skip Connection.](http://arxiv.org/abs/2307.01417) | 本文研究了具有跳连接的贝叶斯卷积神经网络的自由能，揭示了其不依赖于过度参数化，并且具有类似的泛化误差性质。 |
| [^58] | [Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors.](http://arxiv.org/abs/2307.01408) | 多预测器融合算法（MPF）通过将学习型和规则型的预测器以概率组合，提高了轨迹预测器的性能，并在多个指标上表现出最佳的一致性能。 |
| [^59] | [Learning to Communicate using Contrastive Learning.](http://arxiv.org/abs/2307.01403) | 本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。 |
| [^60] | [Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II -- Clustering Extremely High-Dimensional Grid-Based Data.](http://arxiv.org/abs/2307.01400) | 这项研究致力于解决构建喷气与高爆炸物相互作用计算机模拟的准确空间输出替代模型的挑战。通过将输出数据聚类并为每个聚类构建独立的替代模型，可以提高精度。当空间域由数百万个栅格点表示时，数据的聚类变得更加困难。 |
| [^61] | [In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes.](http://arxiv.org/abs/2307.01394) | 本论文通过从高性能计算的角度出发，对面向高性能数据框的并行处理模式进行了深入分析，并指出当前最广泛使用的串行数据框在处理中等规模的数据集时存在性能限制，提出改进的空间。 |
| [^62] | [Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I -- Analysis with a Small Sample Size.](http://arxiv.org/abs/2307.01393) | 本研究通过一个二维问题探讨了如何构建高质量的替代模型，以解决模拟复杂现象时所面临的挑战。该方法可以应用于与高爆炸物相互作用的喷流问题。 |
| [^63] | [Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives.](http://arxiv.org/abs/2307.01390) | 欺诈检测是数据经济的关键防御机制，但机器学习在面对欺诈活动和对抗性攻击时面临挑战。对抗性机器学习在特定领域具有巨大潜力，但在将其推广到其他领域和应用方面还有待进一步研究。 |
| [^64] | [Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer's Disease Progression via Counterfactual Inference.](http://arxiv.org/abs/2307.01389) | 通过逆向推断方法揭示淀粉样蛋白β积累与阿尔茨海默病之间的因果关系，有助于早期诊断和治疗。 |
| [^65] | [Systematic Bias in Sample Inference and its Effect on Machine Learning.](http://arxiv.org/abs/2307.01384) | 机器学习模型在样本推断中存在系统偏差，尤其是在少数群体中，导致目标特征欠预测。这种欠预测模式是小样本统计推断的可预测结果。 |
| [^66] | [Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation.](http://arxiv.org/abs/2307.01381) | 本文提出了一种隐式内存变换器，通过新的左上下文方法隐式保留记忆，从而实现了计算高效的同时语音翻译。在MuST-C数据集上的实验表明，该方法提供了显著的速度提升和少量的性能损失。 |
| [^67] | [Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models.](http://arxiv.org/abs/2307.01379) | 本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。 |
| [^68] | [Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation.](http://arxiv.org/abs/2307.01377) | 该论文提出了一种名为"可转移上下文"的简单而有效的方案，用于解决同时语音翻译中的训练-推理上下文不匹配问题。通过保持一致的段落和上下文大小，即使存在部分填充的段落，该方案在流式任务的分段Transformer中也是广泛适用的。实验证明，应用于Augmented Memory Transformer后可以提高BLEU得分。 |
| [^69] | [Adaptive Principal Component Regression with Applications to Panel Data.](http://arxiv.org/abs/2307.01357) | 本文提出了自适应主成分回归方法，并在面板数据中的应用中获得了均匀有限样本保证。该方法可以用于面板数据中的实验设计，特别是当干预方案是自适应分配的情况。 |
| [^70] | [Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator.](http://arxiv.org/abs/2307.01354) | 通过学习Buckley-Leverett PDE的流函数空间与解空间之间的映射，使用Physics-Informed DeepONets (PI-DeepONets)在没有配对输入-输出观测的情况下，通过软约束的底层物理定律来加速多孔介质中流体流动和传输的数值模拟。a |
| [^71] | [Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols.](http://arxiv.org/abs/2307.01346) | Patch-CNN是一种数据有效的深度学习方法，可以从仅六个方向的扩散加权图像中精确估计扩散张量，克服了训练数据需求大和无法估计纤维方向的限制。 |
| [^72] | [Robust Uncertainty Estimation for Classification of Maritime Objects.](http://arxiv.org/abs/2307.01325) | 本论文研究了在航海领域中使用不确定性估计的方法，通过使用蒙特卡洛Dropout获得的类内不确定性和异常检测的结合，对航海对象进行了鲁棒的分类。实验证明了这种方法相比其他方法在性能上的改进。 |
| [^73] | [Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly.](http://arxiv.org/abs/2307.01317) | 本文提出了一种基于密度的可行性学习方法，使用归一化流进行自省式机器人装配。该方法只需要可行的示例来训练，可以更好地检测不可行的装配方案。 |
| [^74] | [Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2307.01316) | 本文介绍了一种名为DRL with Symbolic Logics (DRLSL)的新颖神经符号无模型深度强化学习方法，旨在实现在真实环境中安全学习自主驾驶策略。该方法结合了深度强化学习和符号逻辑驱动的推理，允许通过与物理环境的实时交互来学习自主驾驶策略并确保安全性。 |
| [^75] | [A numerical algorithm for attaining the Chebyshev bound in optimal learning.](http://arxiv.org/abs/2307.01304) | 本文提出了一个数值算法，用于从有限数据点中解决优化学习中的Chebyshev边界问题。算法能够计算出假设空间的Chebyshev半径和Chebyshev中心，从而实现了从数据中最优恢复函数的目标。算法基于有针对性抽样的近似最优解，具有独立的兴趣，并通过示例展示了算法的有效性。 |
| [^76] | [Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems.](http://arxiv.org/abs/2307.01292) | 本论文研究了模型服务系统的安全性，通过引入一个查询高效的指纹算法，使得攻击者能够一致地触发任何想要的模型，从而增强了对模型提取攻击的鲁棒性和准确性。 |
| [^77] | [Fighting the disagreement in Explainable Machine Learning with consensus.](http://arxiv.org/abs/2307.01288) | 这项研究评估了六种共识函数用于解释五个机器学习模型，并发现了在解释模型方面存在着分歧问题，对于解决这个问题尚需进一步研究。 |
| [^78] | [Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction.](http://arxiv.org/abs/2307.01238) | 本研究提出了一种用于餐后血糖预测的新方法，通过结构化语法演化学习差分方程，并结合聚类分析提供可解释性强的预测模型。 |
| [^79] | [Dynamical Graph Echo State Networks with Snapshot Merging for Dissemination Process Classification.](http://arxiv.org/abs/2307.01237) | 本研究提出了一种将动态图回声状态网络(DynGESN)与称为快照合并的新型数据增强策略相结合的模型，用于处理传播过程分类(DPC)任务。该模型通过快照合并和多个储层编码器提取时空特征，并采用逻辑回归进行分类。实验结果显示该方法在多个基准数据集上取得了显著的改进。 |
| [^80] | [Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch.](http://arxiv.org/abs/2307.01236) | Rockmate是一个高效、快速、自动和通用的PyTorch重新材料化工具，通过检测计算和数据依赖关系的结构，将模型重写为复杂块的序列，以实现快速和高效的训练，并与Checkmate和Rotor的性能相当。 |
| [^81] | [Internet of Things Fault Detection and Classification via Multitask Learning.](http://arxiv.org/abs/2307.01234) | 本文通过多任务学习实现了物联网故障检测和分类系统，并在真实数据上展示了优越的性能，相较于现有技术，在特异性、精确度、召回率和F1值等方面均有显著改进。 |
| [^82] | [RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations.](http://arxiv.org/abs/2307.01233) | RobustL2S是一个模块化框架，利用自监督表示将唇语转换为语音。该方法通过解耦语音内容并对其进行高效地训练，实现了在相关数据集上的最先进性能。 |
| [^83] | [Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data.](http://arxiv.org/abs/2307.01232) | 本论文提出了一种使用有噪声数据进行手术工具检测的鲁棒模型的系统方法，创新之处在于智能主动学习策略和学生-教师模型组装策略。 |
| [^84] | [A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms.](http://arxiv.org/abs/2307.01231) | 本研究重新评估了(深度)学习匹配算法的基准数据集，发现其中大多数数据集都属于相对简单的分类任务。 |
| [^85] | [Large Language and Text-to-3D Models for Engineering Design Optimization.](http://arxiv.org/abs/2307.01230) | 本文研究了深度文本到三维模型在工程设计优化中的潜力和挑战，提出并实现了一个自动化的进化设计优化框架。 |
| [^86] | [EmoGen: Eliminating Subjective Bias in Emotional Music Generation.](http://arxiv.org/abs/2307.01229) | EmoGen是一种消除情感音乐生成中主观偏差的系统，通过利用与情感相关的音乐属性作为桥梁，将生成分为情感到属性的映射以及属性到音乐的生成两个阶段，并在学习过程中消除主观偏差，实现生成具有普遍情感的音乐。 |
| [^87] | [ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting.](http://arxiv.org/abs/2307.01227) | ESGCN是一种用于交通流量预测的边缘压缩注意图卷积网络，通过建模时空动态和引入边缘特征和边缘注意机制来提高预测的准确性。 |
| [^88] | [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport.](http://arxiv.org/abs/2307.01226) | vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。 |
| [^89] | [Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT).](http://arxiv.org/abs/2307.01225) | 通过提出的解释性和透明性驱动的检测与转换（IT-DT）框架，我们在检测和转换文本对抗示例方面注重解释性和透明性。这个框架利用了注意力图、集成梯度和模型反馈等技术，在检测阶段有助于识别对对抗性分类有贡献的显著特征和扰动词语，并在转换阶段使用预训练的嵌入和模型反馈来生成扰动词语的最佳替代，以将对抗性示例转换为正常示例。 |
| [^90] | [FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy.](http://arxiv.org/abs/2307.01217) | 提出了一种名为FedCP的个性化联邦学习方法，通过生成条件策略分离特征中的全局信息和个性化信息，并分别进行处理。实验证明该方法在计算机视觉和自然语言处理领域的性能超过了十一种最先进的方法，最高可提高6.69%。 |
| [^91] | [Of Spiky SVDs and Music Recommendation.](http://arxiv.org/abs/2307.01212) | 本文研究了音乐推荐中的一个有趣现象：嵌入空间中的尖峰形成。通过提出度量标准并进行数学证明，我们揭示了尖峰形成与不同内部流行度的项目社区相关。最后，我们通过一个实际用例，探讨了在添加数据的情况下音乐嵌入的相似项目如何随时间变化。 |
| [^92] | [Multi-Dialectal Representation Learning of Sinitic Phonology.](http://arxiv.org/abs/2307.01209) | 该论文提出了一种在汉字音韵学中获取多方言表示的方法，通过构建知识图谱和应用无监督聚类技术，这些表示可以捕捉输入方言的音位对比并展示古老的原始语言特征的潜力。 |
| [^93] | [Recommender Systems for Online and Mobile Social Networks: A survey.](http://arxiv.org/abs/2307.01207) | 本文调研了针对在线和移动社交网络设计和实施的推荐系统，强调了社交上下文信息如何改善推荐任务，并描述了这些系统在完全分布环境下的优点和缺点。 |
| [^94] | [Confidence Ranking for CTR Prediction.](http://arxiv.org/abs/2307.01206) | 本文提出了一种名为“置信度排序”的新型框架，在CTR预测任务中引入了置信度排序损失，可以在公共和工业数据集上胜过所有基线方法。 |
| [^95] | [Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach.](http://arxiv.org/abs/2307.01204) | 本文提出了一种基于关系匿名漫步引导的神经过程方法，用于知识图谱上的少样本归纳链接预测。该方法利用未见实体周围的子图来获取语义并归纳地预测链接，在捕捉一般的归纳模式的同时，还能快速适应新的实体并估计预测的不确定性。 |
| [^96] | [Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT.](http://arxiv.org/abs/2307.01202) | 本研究采用ChatGPT技术，以OpenAI的最先进的文本嵌入为基础，通过深度学习预测模型实现了对专利创新成功和估值的准确预测，为专利估值提供了革命性的改进。此外，通过预测接受率构建的多空投资组合实现了显著的异常收益率。 |
| [^97] | [Improving Language Plasticity via Pretraining with Active Forgetting.](http://arxiv.org/abs/2307.01163) | 本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。 |
| [^98] | [AVSegFormer: Audio-Visual Segmentation with Transformer.](http://arxiv.org/abs/2307.01146) | AVSegFormer是一种基于Transformer的音视频分割框架，通过引入音频查询和可学习查询来选择性地关注视觉特征，还使用音频-视觉混合器动态调整视觉特征，并通过中间掩模损失增强解码器的监督。实验证明该方法的有效性。 |
| [^99] | [Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data.](http://arxiv.org/abs/2307.01088) | 本文在大规模数据集和模型上首次对分布转移和长尾类别分布下的合规预测方法进行了实证评估。研究发现，这些方法在分布转移和长尾设置下的性能大大下降，对于在现实世界和安全关键应用中的部署具有重要的局限性。 |
| [^100] | [CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery.](http://arxiv.org/abs/2307.00859) | CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。 |
| [^101] | [SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption.](http://arxiv.org/abs/2307.00677) | 本文提出了一种基于密度的聚类算法，能够检测到高密度区域中的结构，具有先前算法所不具备的能力。 |
| [^102] | [Active Sensing with Predictive Coding and Uncertainty Minimization.](http://arxiv.org/abs/2307.00668) | 本论文提出了一种基于预测编码和不确定性最小化的主动感知方法，能够在各种任务中实现有效的探索和学习。 |
| [^103] | [Sparsity-aware generalization theory for deep neural networks.](http://arxiv.org/abs/2307.00426) | 本文研究了深度神经网络的泛化能力，提出了一种基于稀疏感知的分析方法。通过考虑隐藏层激活的稀疏程度，我们展示了稀疏和泛化之间的权衡，而且结果对模型的稀疏程度没有强烈的假设，并在特定情况下的数值实验中得到了非空的界限。 |
| [^104] | [Counterfactual Collaborative Reasoning.](http://arxiv.org/abs/2307.00165) | 本文提出了反事实协同推理（CCR）方法，通过整合反事实推理和逻辑推理来提高机器学习模型的准确性和可解释性。通过利用反事实推理生成困难的反事实训练样本进行数据增强，CCR在推荐系统中展示了如何缓解数据稀缺、提高准确性和增强透明度。 |
| [^105] | [Distance Functions and Normalization Under Stream Scenarios.](http://arxiv.org/abs/2307.00106) | 论文研究了在流场景中的数据规范化问题，比较了八种距离函数的准确度，并发现在没有预先了解数据流信息的情况下，使用原始数据流和Canberra距离的组合效果较好。 |
| [^106] | [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing.](http://arxiv.org/abs/2306.17010) | milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。 |
| [^107] | [SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores.](http://arxiv.org/abs/2306.16688) | SRL是一个可扩展，高效，可扩展的分布式强化学习系统，通过一种新的抽象框架统一了各种实际强化学习训练，并实现了精细优化。 |
| [^108] | [Continuous-Time q-learning for McKean-Vlasov Control Problems.](http://arxiv.org/abs/2306.16208) | 本文研究了连续时间q-learning在熵正则化强化学习框架下用于McKean-Vlasov控制问题，并揭示了两种不同的q函数的存在及其积分表示。 |
| [^109] | [Elucidating Interfacial Dynamics of Ti-Al Systems Using Molecular Dynamics Simulation and Markov State Modeling.](http://arxiv.org/abs/2306.14568) | 本研究通过分子动力学模拟和马尔可夫状态建模方法揭示了Ti-Al系统中的界面动力学，特别关注了TiAl3晶界在实验热处理条件下Ti和Al原子的行为。分子动力学模拟揭示了热处理早期阶段Al原子通过TiAl3晶界朝Ti表面扩散是主要过程。马尔可夫状态建模则可以定位并确认关键步骤。 |
| [^110] | [Phase Unwrapping of Color Doppler Echocardiography using Deep Learning.](http://arxiv.org/abs/2306.13695) | 本文提出了一种基于展开型原始-对偶网络的新方法，用于纠正彩色多普勒心脏超声图像中的相位包裹伪影，与其他最新分割技术相比，该方法在性能上表现突出。 |
| [^111] | [Recent Developments in Recommender Systems: A Survey.](http://arxiv.org/abs/2306.12680) | 本篇综述全面总结了推荐系统领域的最新进展和趋势，包括推荐系统分类，知识推荐系统，鲁棒性，数据偏见和公平性问题，以及评估度量。该研究还提供了未来研究的新方向。 |
| [^112] | [Task-Robust Pre-Training for Worst-Case Downstream Adaptation.](http://arxiv.org/abs/2306.12070) | 本文提出了一种任务鲁棒的预训练方法，将上游任务分成几个代表性任务并应用极小极大损失进行预训练，以保证模型能够在下游任务中具有均匀良好的性能。 |
| [^113] | [Segment Anything Model (SAM) for Radiation Oncology.](http://arxiv.org/abs/2306.11730) | Segment Anything Model (SAM) 在临床放射治疗中的性能得到评估，并展示了在自动分割中的稳健泛化能力。它可以实现针对不同部位的不同器官的精确描绘。 |
| [^114] | [Convergence and concentration properties of constant step-size SGD through Markov chains.](http://arxiv.org/abs/2306.11497) | 本文通过马尔科夫链研究了常步长随机梯度下降的性质，证明了迭代收敛于一个不变分布，并获得了高置信度边界。 |
| [^115] | [Learning Models of Adversarial Agent Behavior under Partial Observability.](http://arxiv.org/abs/2306.11168) | 本论文提出了一种名为GrAMMI的方法，用于在部分可观测性下建模对抗对手代理的行为。通过最大化互信息作为辅助目标来预测对抗对手的当前和未来状态，GrAMMI在两个大规模追逐-逃避领域中表现出了显著优越性能。 |
| [^116] | [Recurrent Memory Decision Transformer.](http://arxiv.org/abs/2306.09459) | 本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。 |
| [^117] | [Neural Mixed Effects for Nonlinear Personalized Predictions.](http://arxiv.org/abs/2306.08149) | 本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。 |
| [^118] | [DRCFS: Doubly Robust Causal Feature Selection.](http://arxiv.org/abs/2306.07024) | DRCFS是一个双重稳健因果特征选择方法，可以在非线性和高维环境中识别因果特征，优于现有方法。 |
| [^119] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^120] | [GPT-FL: Generative Pre-trained Model-Assisted Federated Learning.](http://arxiv.org/abs/2306.02210) | GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。 |
| [^121] | [Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust.](http://arxiv.org/abs/2305.20030) | 本文提出了一种名为树轮数字水印的技术，可以稳定地指纹扩散模型的输出，与现有的在采样后对图像进行修改的方法不同，数字水印微妙地影响整个采样过程，从而产生模型指纹，对人类不可见。 |
| [^122] | [Discovering New Interpretable Conservation Laws as Sparse Invariants.](http://arxiv.org/abs/2305.19525) | 这篇论文介绍了一种名为Sparse Invariant Detector（SID）的算法，它能够自动发现微分方程中的保守律。该算法可以重新发现已知的保守律，甚至发现新的保守律，并且已发现的保守律具有稳健性和可解释性。 |
| [^123] | [Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN).](http://arxiv.org/abs/2305.12052) | 本研究使用深度神经网络优化水动力模型，成功加速并提升了洪水深度和速度的预测能力。 |
| [^124] | [Addressing Heterophily in Node Classification with Graph Echo State Networks.](http://arxiv.org/abs/2305.08233) | 这篇论文介绍了使用图形回声状态网络(GESN)解决异质性图上节点分类的挑战。GESN是一种储备计算模型，通过递归计算节点嵌入来处理在异质性图上节点分类的问题。 |
| [^125] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^126] | [Conditional and Residual Methods in Scalable Coding for Humans and Machines.](http://arxiv.org/abs/2305.02562) | 该论文提出了适用于人类和机器的可扩展编码中的条件编码和残差编码方法，并应用于图像重建，取得了类似的性能表现。 |
| [^127] | [String Diagrams with Factorized Densities.](http://arxiv.org/abs/2305.02506) | 本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。 |
| [^128] | [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.](http://arxiv.org/abs/2305.02301) | 本研究提出了Distilling Step-by-Step机制，通过提取LLM基础信息为小型模型提供额外的监督训练，从而使它们胜过更大的LLM模型，并需更少的训练数据。 |
| [^129] | [Towards Automated Circuit Discovery for Mechanistic Interpretability.](http://arxiv.org/abs/2304.14997) | 该论文提出了一种新算法 ACDC，可以自动识别网络中的重要单元，从而实现机制可解释性自动电路发现。 |
| [^130] | [An Introduction to Transformers.](http://arxiv.org/abs/2304.10557) | Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。 |
| [^131] | [Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms.](http://arxiv.org/abs/2304.09872) | 本文提出了一种基于深度函数的偏序集合描述性分析框架，并引入了偏序版本的单纯深度，用于比较基于多维性能度量的机器学习算法。实验证明此方法与现有基准方法不同，为分类器比较提供了新的视角。 |
| [^132] | [Active Cost-aware Labeling of Streaming Data.](http://arxiv.org/abs/2304.06808) | 本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。 |
| [^133] | [DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps.](http://arxiv.org/abs/2304.00133) | DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。 |
| [^134] | [Random Inverse Problems Over Graphs: Decentralized Online Learning.](http://arxiv.org/abs/2303.11789) | 本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。 |
| [^135] | [FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification.](http://arxiv.org/abs/2303.08325) | 本文提出了FairAdaBN，将批归一化适应敏感属性，可以将其简单而有效地应用到原本不了解公平性的多个分类主干中，能够有效地减少不公平性，并实现模型性能和公平性之间的平衡。 |
| [^136] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^137] | [Graph Neural Network contextual embedding for Deep Learning on Tabular Data.](http://arxiv.org/abs/2303.06455) | 本文介绍了一种基于图神经网络的深度学习模型，使用交互网络进行上下文嵌入，用于表格数据的处理。该模型在公共数据集上的表现优于最近的深度学习基准调查，并且与提升树解决方案相比也取得了竞争性的结果。 |
| [^138] | [Causal Dependence Plots.](http://arxiv.org/abs/2303.04209) | 本论文提出了因果依赖图（CDPs）来解释人工智能或机器学习模型的因果依赖关系。CDPs与传统方法不同，可以模块化地结合因果学习或敏感度分析方法。这些图表可以成为可解释机器学习工具包中的强大工具，并对相关应用做出贡献。 |
| [^139] | [Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension.](http://arxiv.org/abs/2302.13849) | 本研究证明了在线学习中，在学习一个类别时，最优期望错误边界等于其随机化的Littlestone维度。在不可知的情况下，最优错误边界与最佳函数的错误次数之间存在特定关系。此外，该研究还解决了一个开放问题，并将其应用于预测问题。 |
| [^140] | [Can we avoid Double Descent in Deep Neural Networks?.](http://arxiv.org/abs/2302.13259) | 这项研究表明，通过适当调整学习问题的条件，可以避免双下降现象，特别是在复杂情况下使用适当的正则化。这对于寻找深度学习模型的最优大小具有重要意义。 |
| [^141] | [Exploring Local Norms in Exp-concave Statistical Learning.](http://arxiv.org/abs/2302.10726) | 这篇论文研究了在 exp-concave 统计学习中使用经验风险最小化的问题，提出了适用于广泛类别的有界 exp-concave 损失的过量风险界，维度和样本大小对结果有影响，并且基于统一几何假设和本地规范的概念。 |
| [^142] | [FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy.](http://arxiv.org/abs/2302.10429) | 本文提出了FedSpeed方法，通过应用prox校正项和合并额外梯度上升步骤的扰动，从而减少偏差和过拟合的影响，提高了联邦学习的性能。 |
| [^143] | [Pseudo Contrastive Learning for Graph-based Semi-supervised Learning.](http://arxiv.org/abs/2302.09532) | 本论文提出了一种基于伪对比学习的半监督图神经网络方法，通过生成可靠的负样本对来改进伪标签的质量。 |
| [^144] | [Minimizing Dynamic Regret on Geodesic Metric Spaces.](http://arxiv.org/abs/2302.08652) | 本文研究了在测地度量空间上最小化动态遗憾的顺序决策问题，并提出了乐观的在线学习算法，为该领域的研究做出了首次贡献。 |
| [^145] | [LEVER: Learning to Verify Language-to-Code Generation with Execution.](http://arxiv.org/abs/2302.08468) | 提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。 |
| [^146] | [The Expressive Power of Tuning Only the Normalization Layers.](http://arxiv.org/abs/2302.07937) | 本研究发现，仅调整神经网络的归一化层参数就可以达到高准确性，甚至可以重建比原网络小O(根号宽度)倍的目标网络。 |
| [^147] | [Deep Anomaly Detection under Labeling Budget Constraints.](http://arxiv.org/abs/2302.07832) | 本文针对标签预算约束下的深度异常检测问题提出了一种数据标记策略和半监督学习框架，在多种数据集上取得了最先进的性能。 |
| [^148] | [Beyond In-Domain Scenarios: Robust Density-Aware Calibration.](http://arxiv.org/abs/2302.05118) | 这个论文提出了一种鲁棒的密度感知校准方法，通过利用隐藏层的信息，该方法能够提供在领域转移和领域外情景下可靠的不确定性估计，并在保持出色领域内预测不确定性估计的同时提高校准性能。 |
| [^149] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^150] | [Generative models for two-ground-truth partitions in networks.](http://arxiv.org/abs/2302.02787) | 本研究提出了一种生成模型，称为随机交叉块模型（SCBM），可以在网络的中尺度结构中构建两个不同的划分。通过评估随机块模型的性能，展示了该模型的使用案例。 |
| [^151] | [ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts.](http://arxiv.org/abs/2301.12040) | ProtST是一个多模态学习框架，利用蛋白质的文本描述来增强蛋白质序列的预训练和理解，通过设计多种类型的任务，该框架能够获得不同粒度的蛋白质属性信息，并保持原始表征。 |
| [^152] | [EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval.](http://arxiv.org/abs/2301.12005) | 这项研究提出了一种新的几何知识蒸馏方法，通过利用查询和文档的相对几何关系以及嵌入匹配任务来对齐大型教师模型和学生模型的表示，从而提高信息检索的效果。 |
| [^153] | [SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces.](http://arxiv.org/abs/2301.11832) | SOBER算法是一种在离散和混合空间上进行高并行贝叶斯优化的方法，能够进行可扩展和多样化的批量全局优化和积分，且优于11个竞争基线方法。 |
| [^154] | [Fine-tuning Neural-Operator architectures for training and generalization.](http://arxiv.org/abs/2301.11509) | 本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。 |
| [^155] | [Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions.](http://arxiv.org/abs/2301.06535) | 案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。 |
| [^156] | [Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment.](http://arxiv.org/abs/2212.10549) | 本研究提出了一种跨模态注意力一致性正则化方法，用于视觉-语言关系对齐。通过鼓励语言注意力与视觉注意力的一致性来实现关系级别的对齐，从而提高视觉-语言模型在组合概括性基准测试中的性能。 |
| [^157] | [Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training.](http://arxiv.org/abs/2212.10503) | 使用迷你模型适应的方法，通过构建浅层迷你模型以及高效训练新的语言特定嵌入向量，实现了将预训练模型扩展到新语言上的快速跨语言传输。 |
| [^158] | [Exploring Randomly Wired Neural Networks for Climate Model Emulation.](http://arxiv.org/abs/2212.03369) | 本研究探索了随机连接的神经网络在气候模型仿真中的应用。通过使用ClimateBench数据集，我们发现随机连接的神经网络在拥有100万和1000万参数的模型上具有与传统网络相当的性能，且能耗更低。 |
| [^159] | [Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling.](http://arxiv.org/abs/2212.02090) | 本文提出了一种通过公平干预和纠正采样的方法来解决条件生成中的伪因果关系。实验证明，该方法在各种数据集上都能有效地解决这个问题。 |
| [^160] | [Sentiment analysis and opinion mining on E-commerce site.](http://arxiv.org/abs/2211.15536) | 本论文研究了电子商务网站上的情感分析和意见挖掘，提出了解决情感极性分类挑战的广泛技术，并进行了句子级分类和评论级分类。 |
| [^161] | [TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation.](http://arxiv.org/abs/2211.09325) | 本论文提出了一种名为TAX-Pose的系统，在机器人操作中实现了任务特定跨姿势的估计。通过学习对象之间的对应关系，这种系统能够在给定操作任务的情况下准确估计两个对象之间的跨姿势，并利用估计结果指导下游的运动规划。 |
| [^162] | [Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment.](http://arxiv.org/abs/2211.08416) | 本论文提出了Sirius框架，通过任务分工实现人机协作，部分自主的机器人负责决策工作，人类操作员在需要时进行干预。这种人机团队可以确保复杂任务的安全部署。同时，引入了一种新的学习算法，通过重新加权训练样本来改进策略性能。 |
| [^163] | [Age Prediction Performance Varies Across Deep, Superficial, and Cerebellar White Matter Connections.](http://arxiv.org/abs/2211.07398) | 本研究通过研究深部、表浅部和小脑白质连接，提出了一种利用深度学习的年龄预测模型，在人类连接组计划的数据集上取得了优于其他方法的结果。研究发现，深部白质对于年龄预测尤为重要。 |
| [^164] | [DriftRec: Adapting diffusion models to blind JPEG restoration.](http://arxiv.org/abs/2211.06757) | DriftRec是一种将扩散模型应用于盲JPEG恢复的方法，通过优雅地修改扩散模型的正向随机微分方程，DriftRec能够在高压缩水平下恢复干净图像的分布，避免生成模糊图像，并且不需要关于损坏操作的先验知识，具有广泛的适用性。 |
| [^165] | [On the Informativeness of Supervision Signals.](http://arxiv.org/abs/2211.01407) | 本文使用信息论比较了常用的监督信号对表示学习性能的贡献，并为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。 |
| [^166] | [FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs.](http://arxiv.org/abs/2210.16940) | 本论文提出了一个通用框架，用于在神经ODE中训练和证明鲁棒前不变性，应用于鲁棒连续控制和图像分类，具有非虚假保证。 |
| [^167] | [Deep Subspace Encoders for Nonlinear System Identification.](http://arxiv.org/abs/2210.14816) | 本文提出了一种使用截断预测损失和子空间编码器进行状态估计的方法，用于解决非线性系统识别中的噪声处理、模型一致性和可靠估计等问题。 |
| [^168] | [SignReLU neural network and its approximation ability.](http://arxiv.org/abs/2210.10264) | 本文研究了一种名为SignReLU的不同激活函数对深度神经网络逼近能力的影响，结果表明SignReLU网络在逼近性能方面优于有理数和ReLU网络。 |
| [^169] | [Exclusive Supermask Subnetwork Training for Continual Learning.](http://arxiv.org/abs/2210.10209) | 本研究提出了一种连续学习方法ExSSNeT，通过独占超掩码子网络训练和KNN-based知识传递，解决了固定权重限制和知识积累问题。 |
| [^170] | [Improving Adversarial Robustness by Contrastive Guided Diffusion Process.](http://arxiv.org/abs/2210.09643) | 该论文提出了一种通过对比引导扩散过程改善对抗鲁棒性的方法，分析了合成数据的优化条件，并强调了提高生成数据可区分性对于改善鲁棒性的重要性。 |
| [^171] | [UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image.](http://arxiv.org/abs/2210.09477) | 本文提出了一种新的图像编辑方法UniTune，通过在单个图像上微调扩散模型来将图像生成模型转换为图像编辑模型。UniTune能够实现基于文本的图像编辑，避免了使用编辑掩码和丢失编辑部分细节的问题。 |
| [^172] | [(1,1)-Cluster Editing is Polynomial-time Solvable.](http://arxiv.org/abs/2210.07722) | 本文解决了Abu-Khzam（2017）关于(1,1)-集群编辑问题多项式时间可解性的猜想，并提供了相应的约简方法和算法。 |
| [^173] | [IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces.](http://arxiv.org/abs/2210.05098) | 这项研究介绍了IsoVec，一种控制词向量空间相对同构性的方法，通过在Skip-gram损失函数中加入全局同构度度量，提高了训练后词向量空间的同构性，进而改善了跨语言映射效果。 |
| [^174] | [EraseNet: A Recurrent Residual Network for Supervised Document Cleaning.](http://arxiv.org/abs/2210.00708) | 本文介绍了一种名为EraseNet的循环残差网络，用于监督文档清洗。该方法利用全卷积自编码器架构来清洗脏乱文档，并能够恢复具有缺陷的文档以提高光学字符识别系统的性能。 |
| [^175] | [Universal Prompt Tuning for Graph Neural Networks.](http://arxiv.org/abs/2209.15240) | 本文介绍了一种名为Graph Prompt Feature（GPF）的新方法，可通用地调整预先训练过的图神经网络模型，操作于输入的特征空间，能够对应任何形式的Prompt函数。 |
| [^176] | [Metrics to guide development of machine learning algorithms for malaria diagnosis.](http://arxiv.org/abs/2209.06947) | 该论文讨论了在机器学习算法开发疟疾诊断中指导的度量标准，包括了理解临床需求和任务相关度量标准两个关键因素，为解决当前ML工作忽视临床需求的问题提供了解决方案。 |
| [^177] | [PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning.](http://arxiv.org/abs/2209.06606) | PlaStIL是一种无需内存的增量学习方法，通过在最早的增量状态中使用冻结的特征提取器来保证稳定性，同时使用部分微调模型来引入可塑性，以找到可塑性和稳定性之间的平衡。 |
| [^178] | [A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism.](http://arxiv.org/abs/2209.06356) | 本论文提出了一种在离散动作空间中构建同态映射的新方法，通过使用环境动力学的部分模型来推断相同状态的状态动作对，从而减小状态-动作空间的大小。 |
| [^179] | [Statistical Comparisons of Classifiers by Generalized Stochastic Dominance.](http://arxiv.org/abs/2209.01857) | 这篇论文通过采用决策理论的最新发展，提出了一种基于广义随机优势的分类器比较框架，该框架通过解决易处理的线性规划问题进行操作，并通过适应的两样本观察随机化测试进行统计测试。 |
| [^180] | [On establishing learning separations between classical and quantum machine learning with classical data.](http://arxiv.org/abs/2208.06339) | 本文讨论了在经典数据上找到量子学习算法能够比任何经典学习算法更快学习的问题，并研究如何识别这样的学习问题。研究揭示了定义的微妙变化可以导致概念上显着不同的任务，这可能会导致分离或无分离。此外，通过研究已有的具有证明的量子加速学习问题，提炼出了一组更通用和充分的条件，以展示经典和量子学习者之间的差异。 |
| [^181] | [Softmax-free Linear Transformers.](http://arxiv.org/abs/2207.03341) | 这项研究提出了无softmax的线性变换器(SOFT)，用高斯核函数来逼近自注意机制，以改善视觉识别领域中现有方法的局限性。 |
| [^182] | [Privacy Amplification via Shuffled Check-Ins.](http://arxiv.org/abs/2206.03151) | 本论文研究了一种称为随机签到的分布式计算协议，该协议通过差分隐私实现了严格的隐私保护，并通过隐私放大提高了现有工作的隐私计算。同时，引入了一种数值方法评估分布式环境下通用的洗牌机制，实验证明了该方法的有效性。 |
| [^183] | [Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?.](http://arxiv.org/abs/2205.10652) | 这项研究发现简单的多层感知器（MLP）模型在知识图谱补全任务上能够与消息传递神经网络（MPNNs）相媲美，暗示消息传递可能不像之前认为的那样关键。评分函数和损失函数设计对于模型性能有更大影响。 |
| [^184] | [Collaborative Learning for Cyberattack Detection in Blockchain Networks.](http://arxiv.org/abs/2203.11076) | 本文旨在研究入侵攻击并为区块链网络开发一种新颖的网络攻击检测框架。通过实验室合成的网络攻击数据集，提出了一种协作学习模型，允许区块链节点共享学习知识以检测攻击，并增强整个区块链网络的安全性。 |
| [^185] | [Meta-Learning for Simple Regret Minimization.](http://arxiv.org/abs/2202.12888) | 本论文提出了用于在赌博机中进行简单遗憾最小化的元学习框架，并提出了首个贝叶斯和频率派元学习算法。贝叶斯算法具有先验分布并且具有较小的元简单遗憾，而频率派算法更通用且可以在更多的设置中进行分析。通过将算法应用于不同的赌博机问题，我们验证了理论的有效性。 |
| [^186] | [Continual Learning Beyond a Single Model.](http://arxiv.org/abs/2202.09826) | 本论文研究了超越单一模型的持续学习。通过使用集成模型，能够改善持续性能，但随着模型数量增加，计算成本也会增加。为了解决这个问题，提出了一种计算成本较低的算法，能够在运行时间上与单一模型相当，并享有集成的性能优势。 |
| [^187] | [SAITS: Self-Attention-based Imputation for Time Series.](http://arxiv.org/abs/2202.08516) | SAITS是一种基于自注意力机制的多元时间序列缺失值插值方法，通过两个对角线掩码自注意力块的加权组合，能够明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。 |
| [^188] | [Compositionality as Lexical Symmetry.](http://arxiv.org/abs/2201.12926) | 本文将组合性定义为对数据分布的对称性约束，而不是模型，通过自动发现数据转换并应用于训练数据，提高模型的组合归纳偏置。 |
| [^189] | [A Non-Classical Parameterization for Density Estimation Using Sample Moments.](http://arxiv.org/abs/2201.04786) | 本文提出了一种使用样本矩方法进行非传统参数化密度估计的方法，通过平方Hellinger距离进行参数化，并通过凸优化得到唯一解。通过幂矩估计提出了估计器的统计性质和渐近误差上界，模拟结果验证了该方法的性能。 |
| [^190] | [Learning Interpretable Models Through Multi-Objective Neural Architecture Search.](http://arxiv.org/abs/2112.08645) | 本研究提出了一种多目标分布式神经架构搜索框架，旨在优化深度神经网络的任务性能和可解释性。利用非支配排序遗传算法（NSGA-II）和可解释人工智能（XAI）技术，奖励那些可以被领域专家更好理解的架构。 |
| [^191] | [Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation.](http://arxiv.org/abs/2112.01021) | 本文提出了一种无需明确偏见标签或无偏样本的新方法，通过生成偏见转化，使用图像翻译模型将一个偏见模式转换成另一个，同时保留任务相关信息，以实现对比去偏执。 |
| [^192] | [The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration.](http://arxiv.org/abs/2111.15430) | 这篇论文研究了深度神经网络的校准问题，发现传统的交叉熵损失会导致过度自信的预测。通过引入最大熵预测的损失函数，可以提高网络的校准性能。 |
| [^193] | [Tractability from overparametrization: The example of the negative perceptron.](http://arxiv.org/abs/2110.15824) | 本研究考虑了负感知机问题，通过两个随机模型研究了在大数据条件下最大负间隔的上下界。 |
| [^194] | [On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods.](http://arxiv.org/abs/2109.10933) | 本研究证明了在特定选择的Θ和ν下，范数测试和内积/正交性测试在随机梯度下降方法的收敛速度方面是等价的，同时指出在最理想情况下，内积/正交性测试可以像范数测试一样廉价。 |
| [^195] | [The Curse of Passive Data Collection in Batch Reinforcement Learning.](http://arxiv.org/abs/2106.09973) | 本文研究了批量强化学习中被动数据采集的代价问题，并发现与主动数据采集相比，被动采集的样本复杂性呈指数级增加。 |
| [^196] | [Fixed-Budget Best-Arm Identification in Structured Bandits.](http://arxiv.org/abs/2106.04763) | 本论文提出了一个通用易处理的算法，用于解决固定预算下的结构化赌博机最佳臂识别问题。通过从联合广义模型中基于均值奖励估计逐步消除次优臂，该算法具有在线性模型和广义线性模型中竞争力的错误保证，并且在GLM中是第一个具有固定预算BAI分析的实用算法。 |
| [^197] | [Broadcasted Residual Learning for Efficient Keyword Spotting.](http://arxiv.org/abs/2106.04140) | 本研究提出了一种广播残差学习方法，用于高效关键词检测。通过配置大部分残差函数为1D时域卷积，并使用广播残差连接将时域输出扩展到频率-时域维度，该方法能够以小模型尺寸和计算负载实现高准确性。此外，还提出了基于广播残差学习的广播残差网络（BC-ResNet），并介绍了如何根据目标设备的资源来扩展模型。 |
| [^198] | [SCEI: A Smart-Contract Driven Edge Intelligence Framework for IoT Systems.](http://arxiv.org/abs/2103.07050) | 本文提出了一种基于区块链和联邦学习的智能合约驱动的边缘智能框架，用于解决联邦学习在处理非iid数据集和信任问题上的挑战。创新的智能合约允许边缘设备达成一致，确定个性化模型的最佳权重。 |
| [^199] | [Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation.](http://arxiv.org/abs/2011.14956) | 本文研究了处理噪声标签的新方法，特别针对医学组织病理学图像分析中的困难情况。通过一步式绳索多目标学习，该方法克服了标签中存在的复杂噪声和评估策略不明确的问题。 |
| [^200] | [Ensemble Knowledge Distillation for CTR Prediction.](http://arxiv.org/abs/2011.04106) | 本论文针对CTR预测提出了一种集成知识蒸馏的模型训练策略，该策略能够通过教师模型将知识传输给学生模型，并取得了显著的准确性提升。 |
| [^201] | [Transfer Learning in Deep Reinforcement Learning: A Survey.](http://arxiv.org/abs/2009.07888) | 这篇综述调查了深度强化学习领域中的迁移学习方法的最新进展，并提供了一个对这些方法进行分类的框架。分析了它们的目标、方法学、兼容的强化学习背景以及实际应用，并探讨了迁移学习与其他相关主题之间的联系。 |
| [^202] | [Cross-Shape Attention for Part Segmentation of 3D Point Clouds.](http://arxiv.org/abs/2003.09053) | 本文提出了一种新方法，在形状集合中通过交叉形状注意力机制来实现三维点云分割，通过评估点之间的交互程度和介导特征传播来提升结果精度和一致性。 |
| [^203] | [Deep Learning for Genomics: A Concise Overview.](http://arxiv.org/abs/1802.00810) | 深度学习在基因组学领域面临独特的挑战，但通过深入了解任务需求，并与适当的深度架构相匹配，深度学习在基因组学中取得了成功。 |
| [^204] | [Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN.](http://arxiv.org/abs/1705.03387) | 本论文提出了一种使用生成对抗网络来防御对抗扰动的新方法，通过交替训练分类器和生成器网络，生成器网络生成对抗扰动以欺骗分类器网络，同时分类器网络被训练以正确分类原始和对抗图像，这一过程使分类器网络对对抗扰动更加鲁棒，同时还能有效地降低网络的过拟合问题。 |

# 详细

[^1]: 交叉扩散：通过自监督学习改进基于扩散的视觉机器人策略

    Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])

    [http://arxiv.org/abs/2307.01849](http://arxiv.org/abs/2307.01849)

    本论文提出了一种名为Crossway Diffusion的方法，通过使用自监督学习目标增强了基于扩散的视觉机器人策略学习，实验证明了其在各种机器人任务中的有效性和优势。

    

    序列建模方法在机器人模仿学习中显示出有希望的结果。最近，扩散模型已经被采用于行为克隆中，并从其在建模复杂数据分布方面的特异能力中获益。在这项工作中，我们提出了一种名为Crossway Diffusion的方法，通过使用额外的自监督学习（SSL）目标来增强基于扩散的视觉机器人策略学习。标准的基于扩散的策略从随机噪声中生成动作序列，条件是视觉观测和其他低维状态。我们进一步扩展了这一方法，引入了一个新的解码器，从反向扩散过程的中间表示中重构原始图像像素（和其他状态信息），并使用SSL损失联合训练模型。我们的实验证明了Crossway Diffusion在各种模拟和真实世界机器人任务中的有效性，验证了其相对于标准基于扩散的策略的优势。

    Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate tha
    
[^2]: 神经网络混合状态重构的经验样本复杂度

    Empirical Sample Complexity of Neural Network Mixed State Reconstruction. (arXiv:2307.01840v1 [quant-ph])

    [http://arxiv.org/abs/2307.01840](http://arxiv.org/abs/2307.01840)

    该论文通过数值研究混合态重构技术的性能，展示了在不同混合度下不同神经量子态编码的效率，并提出了设计更高效编码的需求。

    

    使用神经量子态进行量子状态重构被提出作为减少实际应用中的量子击穿复杂性的可行工具，并且在主要关注无噪声情况的数值实验中展示了其优势。在这项工作中，我们在混合态：有限温度伊辛模型上进行了数字上的性能研究。我们展示了如何通过应用方差减少技术系统地减少算法的量子资源需求。然后，我们比较了两种主要的神经量子态编码，即神经密度算符和正算符值测量表示，并展示了它们在目标态混合程度变化时的不同性能。我们发现在不同混合度范围内，某些编码更加高效，并指出设计更加高效编码的需求，无论是从计算复杂性还是从性能角度。

    Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques. Then, we compare the two leading neural quantum state encodings of the state, namely, the Neural Density Operator and the positive operator-valued measurement representation, and illustrate their different performance as the mixedness of the target state varies. We find that certain encodings are more efficient in different regimes of mixedness and point out the need for designing more efficient encodings in terms of both cla
    
[^3]: DiT-3D: 探索用于3D形状生成的纯扩散Transformer

    DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])

    [http://arxiv.org/abs/2307.01831](http://arxiv.org/abs/2307.01831)

    DiT-3D是一种针对3D形状生成的新型扩散Transformer，通过在纯Transformer上进行去噪处理，结合3D位置和补丁嵌入来聚合体素化点云的输入，并引入了3D窗口注意力以降低计算成本。

    

    最近的扩散Transformer（例如DiT）已经在生成高质量的2D图像方面展示了强大的效果。但是，是否Transformer架构在3D形状生成方面同样有效仍然有待确定，因为先前的3D扩散方法大部分采用了U-Net架构。为了弥合这一差距，我们提出了一种新颖的用于3D形状生成的扩散Transformer，称为DiT-3D，它可以直接在用于体素化点云的纯Transformer上进行去噪处理。与现有的U-Net方法相比，我们的DiT-3D在模型尺寸上具有更好的可扩展性，并且产生的生成结果质量更高。具体来说，DiT-3D采用了DiT的设计理念，但通过融合3D位置和补丁嵌入来自适应地聚合来自体素化点云的输入。为了减少3D形状生成中自注意力的计算成本，我们在Transformer块中引入了3D窗口注意力，以增加3D令牌数量。

    Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful effectiveness in generating high-quality 2D images. However, it is still being determined whether the Transformer architecture performs equally well in 3D shape generation, as previous 3D diffusion methods mostly adopted the U-Net architecture. To bridge this gap, we propose a novel Diffusion Transformer for 3D shape generation, namely DiT-3D, which can directly operate the denoising process on voxelized point clouds using plain Transformers. Compared to existing U-Net approaches, our DiT-3D is more scalable in model size and produces much higher quality generations. Specifically, the DiT-3D adopts the design philosophy of DiT but modifies it by incorporating 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. To reduce the computational cost of self-attention in 3D shape generation, we incorporate 3D window attention into Transformer blocks, as the increased 3D token le
    
[^4]: 解构数据重建：多类别、权重衰减和通用损失

    Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])

    [http://arxiv.org/abs/2307.01827](http://arxiv.org/abs/2307.01827)

    这项研究探讨了神经网络内部对训练数据的记忆过程，并在多个方向上扩展了已有的研究。研究发现，使用权重衰减可以增加重建能力，同时还分析了神经元数量对网络易受重建方案影响的影响。

    

    训练数据的记忆是一个活跃的研究领域，然而我们对神经网络内部运作的理解还处于初级阶段。最近，Haim等人提出了一种方案，可以从多层感知器二元分类器中重建训练样本，有效地证明了这样的网络参数中编码了大部分训练样本。在本研究中，我们在多个方向上扩展了他们的发现，包括从多类别和卷积神经网络中进行重建。我们推导出了一种更通用的重建方案，可适用于更广泛的损失函数，如回归损失。此外，我们研究了影响网络易受此类重建方案影响的各种因素。有趣的是，我们观察到在训练过程中使用权重衰减会增加重建能力，无论是在数量还是质量方面。此外，我们还检验了神经元数量相对于训练数量的影响。

    Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training
    
[^5]: 复杂网络中的结构平衡和随机游走与复数权重的研究

    Structural Balance and Random Walks on Complex Networks with Complex Weights. (arXiv:2307.01813v1 [cs.SI])

    [http://arxiv.org/abs/2307.01813](http://arxiv.org/abs/2307.01813)

    这篇论文研究了复杂网络中的结构平衡和随机游走，介绍了用于复数权重网络的结构平衡的分类方法，并研究了其动态属性。通过分析复数权重网络上的随机游走，发现当网络是结构平衡的时候，可以实现局部一致性，而当网络严格不平衡时，可以实现全局一致性。

    

    复数在很多情况下定义了实体之间的关系。量子物理中的哈密顿矩阵的非对角项就是一个典型的例子。近年来，人们对在边的权重是复数时拓展网络科学工具的兴趣不断增加。在这里，我们专注于权重矩阵是厄米矩阵的情况，这在许多应用中是一个合理的假设，并研究了复数权重网络的结构和动态属性。基于有符号图的概念，我们引入了一种基于结构平衡的复数权重网络分类，并展示了每种类型中的共享谱属性。然后，我们将结果应用于对复数权重网络上的随机游走动力学的特征化，当图是结构平衡的时候，局部一致性可以渐近地实现，而当图严格不平衡时，全局一致性将被实现。最后，我们探索了p值....（原文未完成）

    Complex numbers define the relationship between entities in many situations. A canonical example would be the off-diagonal terms in a Hamiltonian matrix in quantum physics. Recent years have seen an increasing interest to extend the tools of network science when the weight of edges are complex numbers. Here, we focus on the case when the weight matrix is Hermitian, a reasonable assumption in many applications, and investigate both structural and dynamical properties of the complex-weighted networks. Building on concepts from signed graphs, we introduce a classification of complex-weighted networks based on the notion of structural balance, and illustrate the shared spectral properties within each type. We then apply the results to characterise the dynamics of random walks on complex-weighted networks, where local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. Finally, we explore p
    
[^6]: 通过傅立叶神经算子捕捉增材制造过程中的局部温度演变

    Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators. (arXiv:2307.01804v1 [cs.LG])

    [http://arxiv.org/abs/2307.01804](http://arxiv.org/abs/2307.01804)

    本论文介绍了一种使用傅立叶神经算子捕捉增材制造过程中局部温度演变的数据驱动模型，并提出使用R^2指标来评估模型的一种方法。

    

    高保真、数据驱动的模型能够快速模拟增材制造过程中的热行为，对于提高增材制造技术在零件设计、工艺规划、监控和控制方面的性能至关重要。然而，零件几何形状的复杂性使得当前的模型很难在广泛的几何形状范围内保持高精度。此外，许多模型报告了整个领域（零件）的低均方差（MSE）。然而，在每个时间步骤中，除了近期沉积的热影响区域外，领域的大部分区域的温度变化并不显著。因此，基于MSE的模型保真度测量可能被高估。本文提出了一种使用傅立叶神经算子捕捉增材制造过程中局部温度演变的数据驱动模型。此外，作者提出使用R^2指标来评估模型，该指标提供了一个

    High-fidelity, data-driven models that can quickly simulate thermal behavior during additive manufacturing (AM) are crucial for improving the performance of AM technologies in multiple areas, such as part design, process planning, monitoring, and control. However, the complexities of part geometries make it challenging for current models to maintain high accuracy across a wide range of geometries. Additionally, many models report a low mean square error (MSE) across the entire domain (part). However, in each time step, most areas of the domain do not experience significant changes in temperature, except for the heat-affected zones near recent depositions. Therefore, the MSE-based fidelity measurement of the models may be overestimated.  This paper presents a data-driven model that uses Fourier Neural Operator to capture the local temperature evolution during the additive manufacturing process. In addition, the authors propose to evaluate the model using the $R^2$ metric, which provides
    
[^7]: 基于边缘感知的多任务网络用于集成多模态非对比度MRI上肝肿瘤的定量分割和不确定性预测

    Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI. (arXiv:2307.01798v1 [eess.IV])

    [http://arxiv.org/abs/2307.01798](http://arxiv.org/abs/2307.01798)

    本文提出了一种基于边缘感知的多任务网络用于在多模态非对比度MRI上对肝肿瘤进行定量分割和不确定性预测。通过设计边缘感知特征聚合模块与多任务学习，该网络能够有效融合多模态信息并提供准确的边界信息，从而提高诊断准确性。

    

    在多模态非对比度磁共振成像（NCMRI）上同时进行肝肿瘤的多指标定量、分割和不确定性估计对于准确诊断至关重要。然而，现有的方法缺乏有效的多模态NCMRI融合机制和准确的边界信息捕捉机制，使得这些任务具有挑战性。为了解决这些问题，本文提出了一个统一的框架，即基于边缘感知的多任务网络（EaMtNet），用于在多模态NCMRI上关联肝肿瘤的多指标定量、分割和不确定性。EaMtNet采用两个平行的CNN编码器和Sobel滤波器来提取局部特征和边缘图像。新设计的边缘感知特征聚合模块（EaFA）用于特征融合和选择，通过捕捉特征和边缘图像之间的长程依赖使网络具有边缘感知能力。多任务学习利用预测差异来估算不确定性并改善模型性能。

    Simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods lack an effective mechanism for multi-modality NCMRI fusion and accurate boundary information capture, making these tasks challenging. To address these issues, this paper proposes a unified framework, namely edge-aware multi-task network (EaMtNet), to associate multi-index quantification, segmentation, and uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs two parallel CNN encoders and the Sobel filters to extract local features and edge maps, respectively. The newly designed edge-aware feature aggregation module (EaFA) is used for feature fusion and selection, making the network edge-aware by capturing long-range dependency between feature and edge maps. Multi-tasking leverages prediction discrepancy to estimate uncertainty and improve s
    
[^8]: GHOST:一种使用硅光子学的图神经网络加速器

    GHOST: A Graph Neural Network Accelerator using Silicon Photonics. (arXiv:2307.01782v1 [cs.AR])

    [http://arxiv.org/abs/2307.01782](http://arxiv.org/abs/2307.01782)

    GHOST是第一个使用硅光子学的图神经网络加速器，高效地处理和加速GNN，克服了传统加速器的缺点，使得它可以应用于各种GNN模型和架构的推理。

    

    图神经网络（GNN）已经成为一种强大的建模和学习图结构数据的方法。GNN的能力已经在推荐系统、社交网络分析、药物发现和机器人领域等多个领域得到了巨大的益处。然而，加速和高效处理GNN需要一种超越传统人工神经网络加速器的独特方法，这是由于GNN具有巨大的计算和存储需求。CMOS平台缩放的减速也推动了对替代实现基体的寻求。在本文中，我们提出了GHOST，这是第一个用于GNN的硅光子硬件加速器。GHOST高效地减轻了与顶点和边操作相关的成本。它在光学领域中分别实现了运行GNN所涉及的三个主要阶段，使其可以用于各种广泛使用的GNN模型和架构的推理。

    Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, 
    
[^9]: FedHIL: 面向移动设备的鲁棒室内定位的异构鲁棒联邦学习

    FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices. (arXiv:2307.01780v1 [cs.LG])

    [http://arxiv.org/abs/2307.01780](http://arxiv.org/abs/2307.01780)

    FedHIL是一种面向移动设备的鲁棒室内定位的异构鲁棒联邦学习框架，通过结合室内定位和联邦学习来提高在设备异构环境中的定位精度。

    

    室内定位在紧急响应、仓库管理和增强现实等应用中起着至关重要的作用。通过在移动设备上部署基于机器学习的室内定位框架，用户可以在各种室内和地下环境中进行定位。然而，由于移动设备的硬件和软件堆栈的异构性，精确的室内定位可能会面临挑战，这可能导致位置估计不一致和不准确。传统的机器学习模型也严重依赖于初始训练数据，这使其在室内环境动态变化时易受性能下降的影响。为了解决设备异构性和缺乏适应性带来的挑战，我们提出了一种新颖的嵌入式机器学习框架——FedHIL。我们的框架将室内定位和联邦学习相结合，以在设备异构环境中提高室内定位的精度。

    Indoor localization plays a vital role in applications such as emergency response, warehouse management, and augmented reality experiences. By deploying machine learning (ML) based indoor localization frameworks on their mobile devices, users can localize themselves in a variety of indoor and subterranean environments. However, achieving accurate indoor localization can be challenging due to heterogeneity in the hardware and software stacks of mobile devices, which can result in inconsistent and inaccurate location estimates. Traditional ML models also heavily rely on initial training data, making them vulnerable to degradation in performance with dynamic changes across indoor environments. To address the challenges due to device heterogeneity and lack of adaptivity, we propose a novel embedded ML framework called FedHIL. Our framework combines indoor localization and federated learning (FL) to improve indoor localization accuracy in device-heterogeneous environments while also preserv
    
[^10]: Shapley Sets: 通过递归函数分解进行特征归因

    Shapley Sets: Feature Attribution via Recursive Function Decomposition. (arXiv:2307.01777v1 [cs.LG])

    [http://arxiv.org/abs/2307.01777](http://arxiv.org/abs/2307.01777)

    Shapley Sets是一种通过递归函数分解的方法进行特征归因，它避免了Shapley值特征归因中的特征相互作用问题，并且对于具有复杂依赖结构的数据类型特别有优势。

    

    尽管Shapley值特征归因被广泛使用，但由于模型和数据中的特征相互作用，它们可能会引导误解。我们提出了一种替代归因方法，即Shapley Sets，它将价值授予特征组合。Shapley Sets使用具有对数线性复杂度的递归函数分解算法将底层模型分解为不可分的变量组。 Shapley Sets为每个不可分变量组分配其对于特定预测的组合值。我们表明，Shapley Sets等效于对变换后的特征集进行Shapley值计算，因此具有公平性的同样公理。 Shapley Sets对值函数不敏感，并且理论上和实验上展示了Shapley Sets如何避免与基于Shapley值的替代方法相关联的陷阱，并在具有复杂依赖结构的数据类型中具有特殊优势。

    Despite their ubiquitous use, Shapley value feature attributions can be misleading due to feature interaction in both model and data. We propose an alternative attribution approach, Shapley Sets, which awards value to sets of features. Shapley Sets decomposes the underlying model into non-separable variable groups using a recursive function decomposition algorithm with log linear complexity in the number of variables. Shapley Sets attributes to each non-separable variable group their combined value for a particular prediction. We show that Shapley Sets is equivalent to the Shapley value over the transformed feature set and thus benefits from the same axioms of fairness. Shapley Sets is value function agnostic and we show theoretically and experimentally how Shapley Sets avoids pitfalls associated with Shapley value based alternatives and are particularly advantageous for data types with complex dependency structure.
    
[^11]: 快速通过切片Wasserstein广义测地线实现最优输运

    Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics. (arXiv:2307.01770v1 [stat.ML])

    [http://arxiv.org/abs/2307.01770](http://arxiv.org/abs/2307.01770)

    本文提出了一种快速计算最优输运的方法，通过切片Wasserstein广义测地线进行近似，得到了一个基于一维最优投影的代理距离min-SWGG，并提供了相关的传输计划。这种方法具有较低的计算复杂度，适用于优化算法。

    

    Wassserstein距离和相关的最优输运计划在许多涉及概率度量的应用中被证明是有用的。在本文中，我们提出了一个新的平方Wasserstein距离的代理，称为min-SWGG，它基于两个输入分布的一维最优投影引导的运输映射。我们在min-SWGG和Wasserstein广义测地线之间建立了联系，其中枢纽测度在一条直线上得到支持。我们特别提供了一个新的闭合形式的精确Wasserstein距离，在其中一个分布支持在一条直线上的特殊情况下，使我们能够推导出一种适用于梯度下降优化的快速计算方案。我们表明min-SWGG是WD的上界，并且它具有与Sliced-Wasserstein类似的复杂度，同时提供了一个相关的输运计划。我们还研究了一些理论性质，如距离性、弱收敛、计算和拓扑性质等。

    Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and top
    
[^12]: 作为数据中心机器学习的前提条件的本地化数据工作：加纳全生命周期作物疾病识别案例研究

    Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana. (arXiv:2307.01767v1 [cs.LG])

    [http://arxiv.org/abs/2307.01767](http://arxiv.org/abs/2307.01767)

    本文研究通过加纳全生命周期作物疾病识别案例，展示了本地化数据工作作为数据中心机器学习的前提条件的重要性，为公共领域任务如农业生产力和食品安全提供了有用的解决方案。

    

    加纳人工智能腰果疾病识别项目展示了在农业生产力和食品安全等公共任务中，有效的本地化数据工作作为实现有用的、针对本地需求的数据中心解决方案的先决条件的重要性。利用无人机收集的数据和机器学习技术，确定作物的压力因素，并共同开发数据、模型和最终应用程序，通过桌面应用程序向当地农民提供。

    The Ghana Cashew Disease Identification with Artificial Intelligence (CADI AI) project demonstrates the importance of sound data work as a precondition for the delivery of useful, localized datacentric solutions for public good tasks such as agricultural productivity and food security. Drone collected data and machine learning are utilized to determine crop stressors. Data, model and the final app are developed jointly and made available to local farmers via a desktop application.
    
[^13]: 只需预训练：一种用于自闭症谱系障碍分类的多atlas增强transformer框架

    Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification. (arXiv:2307.01759v1 [cs.CV])

    [http://arxiv.org/abs/2307.01759](http://arxiv.org/abs/2307.01759)

    提出了一种用于自闭症谱系障碍分类的多atlas增强transformer框架，利用静息态功能磁共振成像数据进行建模，采用自我监督预训练提高了分类性能。

    

    自闭症谱系障碍（ASD）是一种常见的精神疾病，表现为非典型的认知、情绪和社交模式。及时准确的诊断对ASD患者的有效干预和改善结果至关重要。在本研究中，我们提出了一种新颖的多atlas增强transformer框架（METAFormer）用于ASD分类。我们的框架利用了ABIDE I数据集中的静息态功能磁共振成像数据，包括406名ASD患者和476名典型对照（TC）受试者。METAFormer采用了多atlas方法，其中来自AAL、CC200和DOS160图谱的展平连接矩阵作为变压器编码器的输入。值得注意的是，我们证明了自我监督预训练，包括从输入中重建掩码值，可以在不需要额外或独立的训练数据的情况下显著提高分类性能。通过分层交叉验证，我们评估了提出的框架并展示了

    Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show
    
[^14]: 来自DESI亮红星系大尺度聚类的局部原初非高斯性翻译标题

    Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies. (arXiv:2307.01753v1 [astro-ph.CO])

    [http://arxiv.org/abs/2307.01753](http://arxiv.org/abs/2307.01753)

    本研究利用DESI成像调查的亮红星系的角聚类信息限制了局部原初非高斯性参数fNL，发现在假设宇宙规律性关系的情况下，fNL为47^{+14(+29)}_{-11(-22)}，使用更积极的处理方法后，最大似然值略微偏离fNL≈5。

    

    我们使用来自Dark Energy Spectroscopic Instrument (DESI)成像调查的亮红星系的角聚类信息来限制局部原初非高斯性参数fNL。我们的样本包括超过1200万个目标，覆盖了14000平方度的天空区域，红移范围为0.2 < z < 1.35。我们确定了银河消光、调查深度和观测条件是系统误差的主要来源，并采用线性回归和人工神经网络来减轻大尺度上的非宇宙学过度聚类。我们的方法经过了包含和不包含fNL和系统误差的对数正态模拟的测试，结果显示了神经网络处理在减小剩余系统误差方面的卓越性能。在假设宇宙规律性关系的情况下，我们发现fNL的68\%（95\%）置信区间为fNL = 47^{+14(+29)}_{-11(-22)}。通过更积极的处理方法，包括对所有成像图集进行回归，我们的最大似然值略微偏离fNL≈5。

    We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range 0.2< z < 1.35. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against log-normal simulations with and without fNL and systematics, showing superior performance of the neural network treatment in reducing remaining systematics. Assuming the universality relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence. With a more aggressive treatment, including regression against the full set of imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 5
    
[^15]: SRCD：多个复合领域的语义推理用于单领域广义目标检测

    SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v1 [cs.CV])

    [http://arxiv.org/abs/2307.01750](http://arxiv.org/abs/2307.01750)

    本文提供了针对单领域广义目标检测的新框架SRCD，通过学习和维护自增广义跨领域样本的语义结构，提高模型的泛化能力。

    

    本文提供了一个针对单领域广义目标检测（即Single-DGOD）的新框架，其中我们对学习和维护自增广义跨领域样本的语义结构以增强模型的泛化能力感兴趣。与在多个源领域上训练的DGOD不同，Single-DGOD要仅使用一个单一源领域很难很好地泛化到多个目标领域。现有方法大多采用了与DGOD类似的处理方式，通过解耦合或压缩语义空间来学习域不变特征。然而，存在两个潜在的限制：1）由于极度稀缺的单领域数据，伪属性-标签相关性；2）通常忽略语义结构信息，即我们发现示例中的实例级语义关系的相似性对模型的泛化至关重要。在本文中，我们引入了用于Single-DGOD的复合领域的语义推理（SRCD）。

    This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. 
    
[^16]: RRCNN:一种基于循环残差卷积神经网络的新型信号分解方法

    RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network. (arXiv:2307.01725v1 [cs.LG])

    [http://arxiv.org/abs/2307.01725](http://arxiv.org/abs/2307.01725)

    RRCNN通过利用深度学习的技术，包括卷积神经网络、残差结构和非线性激活函数，以创新的方式计算信号的局部平均，并提出一种新的非平稳信号分解方法。

    

    非平稳信号的分解是信号时频分析领域中一个重要且具有挑战性的任务。在过去的二十年中，许多信号分解方法，如1998年由黄宏祥等人首创的经验模态分解法，已被不同的研究团队提出。然而，它们仍然存在一些限制。例如，它们通常容易产生边界和模式混合效应，并且对噪声不太鲁棒。受深度学习在图像处理和自然语言处理等领域的成功应用的启发，鉴于文献中缺乏直接使用深度学习技术将非平稳信号分解为简单振荡分量的工作，我们使用卷积神经网络、残差结构和非线性激活函数以创新的方式计算信号的局部平均，并研究一种新的非平稳信号分解方法。

    The decomposition of non-stationary signals is an important and challenging task in the field of signal time-frequency analysis. In the recent two decades, many signal decomposition methods led by the empirical mode decomposition, which was pioneered by Huang et al. in 1998, have been proposed by different research groups. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning in fields like image processing and natural language processing, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the fram
    
[^17]: MOPO-LSI：用户指南

    MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.01719](http://arxiv.org/abs/2307.01719)

    MOPO-LSI是一款开源的多目标投资组合优化库，为可持续投资提供用户指南，并介绍了版本1.0的问题设置、工作流程和超参数。

    

    MOPO-LSI是一种开源的可持续投资多目标投资组合优化库。本文提供了MOPO-LSI版本1.0的用户指南，包括问题设置、工作流程和配置中的超参数。

    MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
    
[^18]: 关于约束时间序列生成问题的研究

    On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])

    [http://arxiv.org/abs/2307.01717](http://arxiv.org/abs/2307.01717)

    这篇论文研究了约束时间序列生成问题。在实际应用中，合成时间序列被广泛用于增强历史时间序列数据集，提高机器学习算法的性能，放大稀有事件的发生，以及创建反事实情景。然而，现有的方法在满足约束方面存在问题，需要重新训练且计算代价高，或者在复杂约束条件下不切实际。

    

    合成时间序列经常在实际应用中用于增加历史时间序列数据集，以提高机器学习算法的性能，放大稀有事件的发生，并创建由时间序列描述的反事实情景。分布相似性（我们称之为真实性）以及满足一定数值约束是反事实时间序列场景生成请求中常见的要求。例如，美联储发布了给定约束时间序列的合成市场压力情景，供金融机构评估其在假设性衰退中的表现。现有的生成约束时间序列的方法通常通过对损失函数进行惩罚来强制满足约束，并拒绝不符合约束的样本。然而，如果我们改变约束条件，这些方法需要重新训练，而拒绝抽样可能在计算上是昂贵的，或者在复杂约束条件下是不切实际的。

    Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints.
    
[^19]: 符合目标：使用通用的插入式框架在CTC模型中优化所需属性

    Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])

    [http://arxiv.org/abs/2307.01715](http://arxiv.org/abs/2307.01715)

    本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。

    

    连接主义时间分类（CTC）是训练监督序列到序列模型广泛使用的准则。它通过将完美对齐（产生基本事实）的边际化来学习输入和输出序列之间的关系，称为对其，以代价不完美对齐。这种对完美和不完美对齐的二元区分无法捕捉到在其他实际应用中具有重要意义的其他关键对齐属性。在这里，我们提出了$\textit{Align With Purpose}$，这是一个用于增强CTC条件下训练模型中所需属性的$\textbf{通用插入式框架}$。我们通过使用额外的损失项来补充CTC来优先考虑符合所需属性的对齐。我们的方法不需要干预CTC损失函数，能够轻松优化各种属性，并且可以区分完美和不完美的对齐。

    Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
    
[^20]: 风险敏感强化学习的分布模型等效性

    Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])

    [http://arxiv.org/abs/2307.01708](http://arxiv.org/abs/2307.01708)

    本文研究了风险敏感强化学习中的分布模型等效性问题。我们提出了两种新的模型等价性概念，并展示了如何将这些概念应用于增强任何基于模型的风险敏感算法。

    

    我们考虑学习用于风险敏感强化学习的模型的问题。我们在理论上证明了适当的价值等价性，这是一种学习模型的方法，可以用于在风险中性的情况下进行最优规划，但在风险敏感的情况下无法进行最优规划。我们利用分布式强化学习引入了两种新的模型等价性概念，其中一个是通用的，可以用于针对任何风险度量进行规划，但是计算复杂；另一个是实际的变体，允许选择可以进行最优规划的风险度量。我们展示了如何使用我们的框架来增强任何基于模型的风险敏感算法，并提供了表格和大规模实验来展示其能力。

    We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
    
[^21]: 在线学习和使用ERM预言机解决无穷博弈问题

    Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])

    [http://arxiv.org/abs/2307.01689](http://arxiv.org/abs/2307.01689)

    这项工作提出了一种仅依赖ERM预言机调用的在线学习算法，该算法在可实现情况下具有有限的遗憾，并在不可知情况下具有亚线性增长的遗憾。同时，还提供了类似的结果用于非参数博弈环境中的学习算法，即仅依赖最佳响应预言机的学习算法，并收敛到近似极小-极大均衡点。

    

    在基于在线学习的情况下，ERM足以达到接近最优泛化误差的目标，但在在线学习环境下并非如此，通常的概念类算法依赖计算效率较低的预言机，如标准最优算法(SOA)。在这项工作中，我们提出了一种仅依赖ERM预言机调用的在线二分类算法，并证明在可实现的情况下具有有限的遗憾(regret)，在不可知的情况下具有亚线性增长的遗憾。我们通过底层概念类的Littlestone和阈值维度来限制遗憾。我们获得了类似的结果用于非参数博弈，其中ERM预言机可以被理解为最佳响应预言机，根据其他玩家的游戏历史找到一个玩家的最佳响应。在这种情况下，我们提供了仅依赖最佳响应预言机的学习算法，并收敛到两人零和博弈的近似极小-极大均衡点。

    While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
    
[^22]: 使用分布式雾服务器为智能物联网服务提供图神经网络支持

    Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services. (arXiv:2307.01684v1 [cs.DC])

    [http://arxiv.org/abs/2307.01684](http://arxiv.org/abs/2307.01684)

    本文提出了一种分布式实时GNN推断框架Fograph，该框架利用了靠近物联网数据源的多个雾节点的资源，通过引入异构感知执行规划和GNN特定的压缩技术，以最大限度地发挥雾计算的架构优势。

    

    由于其在图结构上提取潜在表示能力的突出优势，图神经网络（GNN）在各种应用中引起了越来越多的关注。为了为物联网驱动的智能应用提供基于GNN的服务，传统的模型服务范例通常通过完全上传地理分布的输入数据到远程数据中心来依赖云。然而，我们的实证测量显示这种基于云的服务存在显著的通信开销，并且强调了应用新兴的雾计算的潜在巨大潜力。为了最大限度地发挥雾计算带来的架构优势，本文提出了一种新颖的分布式实时GNN推断框架Fograph，该框架利用了靠近物联网数据源的多个雾节点的多样性和动态资源。通过引入异构感知执行规划和GNN特定的压缩技术，Fograph的设计与GNN服务的独特特征相契合。

    Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN servin
    
[^23]: 使用局部重新参数化技巧学习离散权重和激活

    Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])

    [http://arxiv.org/abs/2307.01683](http://arxiv.org/abs/2307.01683)

    本论文研究了使用局部重新参数化技巧学习离散权重和激活的方法，通过二值化网络来降低计算复杂度，提高神经网络推理效率。

    

    在计算机视觉和机器学习中，一个关键的挑战是降低神经网络推理的计算和内存需求。一个常见的解决方案是使用二值化。通过对网络权重和激活进行二值化，可以通过用更快的位运算替代计算复杂的浮点操作来显著降低计算复杂度。这导致了一个更高效的神经网络推理，可以部署在资源有限的设备上。在这项工作中，我们扩展了之前使用局部重新参数化技巧训练具有离散权重的网络的方法，以允许离散激活。原始方法优化了离散权重的分布，并使用中心极限定理将预激活近似为连续的高斯分布。在这里，我们展示了概率建模也可以有效地训练具有离散激活的网络。

    In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as w
    
[^24]: 使用扩散对比发散训练能量模型

    Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])

    [http://arxiv.org/abs/2307.01668](http://arxiv.org/abs/2307.01668)

    本文提出了一种使用扩散对比发散（DCD）训练能量模型（EBM）的方法，相较于传统的对比发散（CD），DCD在计算效率上更高，并且不受非可忽略梯度项的限制。

    

    能量模型（EBM）广泛应用于生成建模。传统的对比发散（CD）训练目标需要使用马尔可夫链蒙特卡罗方法（MCMCs）从EBM中采样，这导致了计算负担和CD有效性之间的不可调和的折衷。MCMCs的收敛需要大量计算资源，而短期运行的MCMC会引入难以处理的额外参数梯度项。本文提出了扩散对比发散（DCD）系列的一般解释，将CD视为DCD的一种特殊情况，并通过在CD中使用不同于Langevin动力学的EBM参数自由扩散过程，提出了一种更有效的发散方法。我们展示了DCD比CD更加计算高效，并且不受非可忽略梯度项的限制。我们进行了大量实验，包括合成数据和实际应用场景的验证。

    Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data
    
[^25]: 低维流形上过参数化卷积残差网络的非参数分类

    Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])

    [http://arxiv.org/abs/2307.01649](http://arxiv.org/abs/2307.01649)

    本文研究了卷积残差网络在非参数分类任务中的性能。研究表明，通过使用权重衰减的ConvResNeXts，可以隐含地实现对模块的稀疏性，从而使网络能够适应低维流形的平滑性和结构，并高效地学习函数。

    

    卷积残差神经网络(ConvResNets)虽然过参数化，但在实践中能够获得显著的预测性能，这不能被常规智慧很好地解释。为了弥合这一差距，我们从非参数分类的角度研究了使用权重衰减训练的ConvResNeXts（覆盖ConvResNets作为一种特殊情况）的性能。我们的分析允许ConvResNeXts中有无限多的构建模块，并显示权重衰减隐含地强制这些模块的稀疏性。具体而言，我们考虑在低维流形上的平滑目标函数，然后证明ConvResNeXts可以适应函数的平滑性和低维结构，并且能够高效地学习函数而不受维度诅咒的困扰。我们的发现部分证明了过参数化的ConvResNeXts相对于常规机器学习模型的优势。

    Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
    
[^26]: SwinGNN:重新思考在图生成的扩散模型中的置换不变性

    SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])

    [http://arxiv.org/abs/2307.01646](http://arxiv.org/abs/2307.01646)

    本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。

    

    基于置换等变网络的扩散模型可以学习图数据的置换不变分布。然而，相对于非不变模型，我们发现这些不变模型遇到了更大的学习挑战，因为1）它们的目标分布更具模态性；2）它们的最优一步去噪得分是具有更多成分的高斯混合物的得分函数。受到这个分析的启发，我们提出了一种非不变的扩散模型，称为“SwinGNN”，它采用了一种高效的边到边的2-WL消息传递网络，并利用SwinTransformers中的移动窗口自注意力。此外，通过系统性的实验和剖析，我们确定了几种关键的训练和采样技术，显著提高了图生成样本的质量。最后，我们引入了一种简单的后处理技巧，即随机置换生成的图，可以证明将任何图转换成图形统计量。

    Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
    
[^27]: 近似互连性的启发式算法

    Heuristic Algorithms for the Approximation of Mutual Coherence. (arXiv:2307.01639v1 [cs.AI])

    [http://arxiv.org/abs/2307.01639](http://arxiv.org/abs/2307.01639)

    本研究提出了近似互连性的启发式算法，用于加速计算互连性指标。算法通过建模确认值的分布并估计其模型参数，然后利用分布的期望值来近似计算互连性。

    

    互连性是衡量两种观点相似性的指标。尽管该概念来自哲学，但它在广泛的技术中至关重要，例如Wahl-O-Mat系统。在德国，该系统帮助选民找到最符合他们政治偏好的候选人。由于要迭代遍历观点的所有子集，准确计算互连性非常耗时。而且，对于每个子集，必须解决一个SAT模型计数问题的实例，这在计算机科学中被认为是一个硬问题。本研究是加速这种计算的第一项研究。我们将所谓的确认值的分布建模为三个高斯混合，并提出了有效的启发式算法来估计其模型参数。然后，通过分布的期望值来近似互连性。其中一些算法是全多项式时间的，其他算法只需要解决少量的SA模型实例。

    Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SA
    
[^28]: HAGNN: 混合聚合用于异构图神经网络

    HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks. (arXiv:2307.01636v1 [cs.LG])

    [http://arxiv.org/abs/2307.01636](http://arxiv.org/abs/2307.01636)

    本文介绍了HAGNN框架，它是一种用于处理异构图的混合聚合方法。HAGNN同时利用元路径邻居和直接连接邻居进行节点聚合，将整个聚合过程分为基于元路径的内类型聚合和无元路径的跨类型聚合。这种方法能够全面利用异构图中的类型语义信息。

    

    异构图神经网络在处理异构图方面取得了成功。在现有异构图神经网络中，元路径起着重要作用。然而，最近的研究指出，简单的无元路径的同质图模型也可以取得相近的结果，这对元路径的必要性提出了质疑。本文首先介绍了基于元路径和无元路径模型之间的内在差异，即如何选择节点聚合的邻居。然后，我们提出了一种新颖的框架来全面利用异构图中丰富的类型语义信息，即HAGNN（混合聚合用于异构图神经网络）。HAGNN的核心是同时利用元路径邻居和直接连接邻居进行节点聚合。HAGNN将整个聚合过程分为两个阶段：基于元路径的内类型聚合和无元路径的跨类型聚合。

    Heterogeneous graph neural networks (GNNs) have been successful in handling heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an essential role. However, recent work pointed out that simple homogeneous graph model without meta-path can also achieve comparable results, which calls into question the necessity of meta-path. In this paper, we first present the intrinsic difference about meta-path-based and meta-path-free models, i.e., how to select neighbors for node aggregation. Then, we propose a novel framework to utilize the rich type semantic information in heterogeneous graphs comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The core of HAGNN is to leverage the meta-path neighbors and the directly connected neighbors simultaneously for node aggregations. HAGNN divides the overall aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation. During the intra-type aggregation phase, w
    
[^29]: 在智能家居环境中基于循环趋势预测神经网络的可再生能源管理

    Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network. (arXiv:2307.01622v1 [cs.LG])

    [http://arxiv.org/abs/2307.01622](http://arxiv.org/abs/2307.01622)

    本研究提出了一种基于循环趋势预测神经网络的嵌入式调度预测算法，在智能家居环境中实现了高效的住宅需求控制，并同时预测可再生能源发电。该算法具有鲁棒性，能够应对预测误差。

    

    智能家居能源管理系统能够帮助配电网络更加高效和可靠地运行，并有效地整合分布式可再生能源。这些系统依赖于强大的预测、优化和控制/调度算法，能够处理需求和可再生能源发电的不确定性。本文提出了一种先进的机器学习算法，称为基于循环趋势预测神经网络的嵌入式调度预测（rTPNN-FES），用于提供高效的住宅需求控制。rTPNN-FES是一种新颖的神经网络架构，可以同时预测可再生能源发电并安排家电的使用时间。通过其嵌入式结构，rTPNN-FES消除了使用单独算法进行预测和调度的需要，并生成对预测误差具有鲁棒性的调度安排。本文还评估了该算法在物联网智能家居中的性能。评估结果显示，rTPNN-FES能够有效应对预测误差。

    Smart home energy management systems help the distribution grid operate more efficiently and reliably, and enable effective penetration of distributed renewable energy sources. These systems rely on robust forecasting, optimization, and control/scheduling algorithms that can handle the uncertain nature of demand and renewable generation. This paper proposes an advanced ML algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), to provide efficient residential demand control. rTPNN-FES is a novel neural network architecture that simultaneously forecasts renewable energy generation and schedules household appliances. By its embedded structure, rTPNN-FES eliminates the utilization of separate algorithms for forecasting and scheduling and generates a schedule that is robust against forecasting errors. This paper also evaluates the performance of the proposed algorithm for an IoT-enabled smart home. The evaluation results reveal that rTPNN
    
[^30]: SageFormer：面向多变量时间序列预测的系列感知图增强Transformer

    SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])

    [http://arxiv.org/abs/2307.01616](http://arxiv.org/abs/2307.01616)

    本文介绍了SageFormer，一种面向多变量时间序列预测的系列感知图增强Transformer模型，通过图结构有效捕捉和建模序列之间的依赖关系，在表示不同序列中的时间模式和减少序列间冗余信息等方面取得了优越性能。

    

    多变量时间序列预测在各个领域起着至关重要的作用。虽然近期深度学习方法，特别是Transformer，展示了很大的潜力，但在解决跨序列依赖性的重要性问题上仍存在差距。本文介绍了SageFormer，一种系列感知图增强Transformer模型，旨在使用图结构有效捕捉和建模序列之间的依赖关系。SageFormer解决了两个关键挑战：有效地表示不同序列中的时间模式以及减少序列之间的冗余信息。重要的是，所提议的系列感知框架可以无缝集成到现有的基于Transformer的模型中，增强了模型对跨序列依赖性的建模能力。通过对真实世界和合成数据集进行广泛的实验证明，SageFormer相比先前的最先进方法展示出了优越的性能。

    Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
    
[^31]: 过度自信是一件危险的事情：通过强制不太自信的预测来缓解成员推断攻击

    Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])

    [http://arxiv.org/abs/2307.01610](http://arxiv.org/abs/2307.01610)

    本文提出了一种防御技术HAMP，可以在不需要额外数据的情况下，通过强制模型进行不太自信的预测，达到强大的成员隐私保护和高准确性的目标。

    

    机器学习（ML）模型容易受到成员推断攻击（MIAs）的威胁，这些攻击确定给定的输入是否被用于训练目标模型。尽管有很多努力来缓解MIAs，但它们往往会受到有限的隐私保护、大幅降低准确性和/或需要难以获得的额外数据的困扰。本文提出了一种防御技术HAMP，可以在不需要额外数据的情况下实现强大的成员隐私和高准确性。为了缓解不同形式的MIAs，我们观察到它们可以统一，因为它们都利用了ML模型在通过不同的代理预测训练样本时的过度自信。这促使我们设计了一种通过模型强制进行不太自信预测的方法，从而迫使模型在训练样本和测试样本上表现类似。HAMP包括一个新颖的训练框架，使用高熵软标签和基于熵的正则化器来约束模型的预测，同时实现高准确性和成员隐私保护。

    Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving h
    
[^32]: 用原型解释时间序列异常检测

    Prototypes as Explanation for Time Series Anomaly Detection. (arXiv:2307.01601v1 [cs.LG])

    [http://arxiv.org/abs/2307.01601](http://arxiv.org/abs/2307.01601)

    本文提出了ProtoAD，利用原型作为解释来解释时间序列的异常检测。该方法提供了透明度和直观理解，为黑盒模型的安全关键应用打开了新的突破口。

    

    在许多大数据应用中，检测偏离一定规律重复模式的异常模式在时间序列中是至关重要的。然而，缺乏标签、时间序列数据的动态性以及意想不到的异常行为使得检测过程充满挑战。尽管最近的深度异常检测方法取得了成功，但这些黑盒模型中的神秘机制成为安全关键应用的新挑战。模型的透明度和预测可靠性的缺失阻碍了这些领域进一步突破。本文提出ProtoAD，利用原型作为基于示例的解释来解释异常检测过程中的正常模式状态。在不显著影响检测性能的情况下，原型照亮了深黑盒模型，并为领域专家和利益相关者提供了直观的理解。我们将广泛使用的原型学习方法在分类问题中扩展到异常检测。通过可视化bo

    Detecting abnormal patterns that deviate from a certain regular repeating pattern in time series is essential in many big data applications. However, the lack of labels, the dynamic nature of time series data, and unforeseeable abnormal behaviors make the detection process challenging. Despite the success of recent deep anomaly detection approaches, the mystical mechanisms in such black-box models have become a new challenge in safety-critical applications. The lack of model transparency and prediction reliability hinders further breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as the example-based explanation for the state of regular patterns during anomaly detection. Without significant impact on the detection performance, prototypes shed light on the deep black-box models and provide intuitive understanding for domain experts and stakeholders. We extend the widely used prototype learning in classification problems into anomaly detection. By visualizing bo
    
[^33]: 一种利用链上数据进行加密货币投资组合管理的可扩展强化学习系统

    A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management. (arXiv:2307.01599v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.01599](http://arxiv.org/abs/2307.01599)

    提出了一种名为CryptoRLPM的基于强化学习的系统，利用链上数据进行加密货币投资组合管理，背测结果显示其在三个投资组合上的表现优于所有基准。

    

    类似于公司基本面的区块链网络的链上数据（指标）提供了对网络的重要和全面的见解。尽管具有信息性质，但链上数据尚未在基于强化学习的加密货币投资组合管理系统中得到利用。一个有趣的课题是，与基准相比，链上数据的利用程度能够提高强化学习系统的回报表现。因此，在本研究中，我们提出了CryptoRLPM，这是一种新颖的基于强化学习的系统，将链上数据纳入端到端的加密货币投资组合管理。CryptoRLPM由五个单元组成，从信息理解到交易订单执行。在CryptoRLPM中，链上数据经过测试和指定，以解决指标无效性的问题。此外，CryptoRLPM的可扩展性使得可以随时更改投资组合中的加密货币。在三个投资组合上的回测结果表明，CryptoRLPM的表现优于所有基准。

    On-chain data (metrics) of blockchain networks, akin to company fundamentals, provide crucial and comprehensive insights into the networks. Despite their informative nature, on-chain data have not been utilized in reinforcement learning (RL)-based systems for cryptocurrency (crypto) portfolio management (PM). An intriguing subject is the extent to which the utilization of on-chain data can enhance an RL-based system's return performance compared to baselines. Therefore, in this study, we propose CryptoRLPM, a novel RL-based system incorporating on-chain data for end-to-end crypto PM. CryptoRLPM consists of five units, spanning from information comprehension to trading order execution. In CryptoRLPM, the on-chain data are tested and specified for each crypto to solve the issue of ineffectiveness of metrics. Moreover, the scalable nature of CryptoRLPM allows changes in the portfolios' cryptos at any time. Backtesting results on three portfolios indicate that CryptoRLPM outperforms all th
    
[^34]: 在高峰小时序列预测中缩小性能差距: Seq2Peak框架

    Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])

    [http://arxiv.org/abs/2307.01597](http://arxiv.org/abs/2307.01597)

    本文提出了Seq2Peak框架，针对高峰小时序列预测任务，该框架通过解决高度非平稳性和性能评估问题，成功缩小了在常规时间序列预测模型中观察到的性能差距。

    

    高峰小时序列预测（PHSF）是各个领域中一个重要但未被充分探索的任务。虽然最先进的深度学习模型在常规时间序列预测（TSF）中表现出色，但在PHSF中却难以达到可比较的结果。这可能归因于高峰小时序列中高度非平稳性的挑战，使得直接预测比标准的TSF更加困难。此外，手动从常规预测结果中提取最大值会导致性能不佳，因为模型会最小化平均差。为了解决这些问题，本文提出了Seq2Peak，一个专为PHSF任务而设计的新颖框架，以弥合在TSF模型中观察到的性能差距。Seq2Peak具有两个关键组件：CyclicNorm流程来减轻非平稳性问题，以及一个简单而有效的可训练参数自由峰值小时解码器，采用混合损失函数来利用原始序列和高峰小时序列作为监督信号。

    Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as superv
    
[^35]: 显示广告中多元素创意的跨元素组合选择

    Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising. (arXiv:2307.01593v1 [cs.IR])

    [http://arxiv.org/abs/2307.01593](http://arxiv.org/abs/2307.01593)

    这篇论文提出了一个跨元素组合选择框架CECS，用于解决显示广告中多元素创意选择的问题，通过采用跨元素交互的方式进行编码，将创意组合问题转化为多个创意元素级联选择问题。

    

    广告创意的有效性很大程度上受其视觉外观的影响。广告平台可以通过组合广告创意中的不同元素来生成具有不同外观的广告创意。然而，随着广告创意元素数量的增加，从无数可能性中选择合适的组合变得具有挑战性。行业的主流方法是独立选择各个创意元素，这经常忽视了建模过程中创意元素之间相互作用的重要性。作为回应，本文提出了一个用于多个创意元素的跨元素组合选择框架，称为CECS。在编码器过程中，采用了跨元素交互，根据当前候选创意动态调整单个创意元素的表达。在解码器过程中，将创意组合问题转化为多个创意元素级联选择问题。

    The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. 
    
[^36]: 用神经网络学习李群对称变换

    Learning Lie Group Symmetry Transformations with Neural Networks. (arXiv:2307.01583v1 [cs.LG])

    [http://arxiv.org/abs/2307.01583](http://arxiv.org/abs/2307.01583)

    本研究通过神经网络探索和表征数据集中的未知李群对称变换，展示了该方法在模型选择和数据分析等领域的实用性。

    

    在数据集中检测和量化对称性的问题对于模型选择、生成建模和数据分析等方面都是有用的。

    The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.
    
[^37]: IAdet：最简单的人机交互目标检测

    IAdet: Simplest human-in-the-loop object detection. (arXiv:2307.01582v1 [cs.CV])

    [http://arxiv.org/abs/2307.01582](http://arxiv.org/abs/2307.01582)

    本文提出了一种最简单的人机交互目标检测方法，其中的关键创新是智能注释（IA）策略和开源的IAdet工具。对于PASCAL VOC数据集，IAdet工具能够减少数据库标注时间，并提供免费训练好的模型，为强大的人机交互目标检测系统提供了改进的可能。

    

    本文提出了一种在标注数据的同时训练模型的策略，称为智能注释（IA）。IA包括三个模块：（1）辅助数据标注，（2）背景模型训练和（3）主动选择下一个数据点。在这个框架下，我们开源了特定于单类目标检测的IAdet工具。此外，我们设计了一种自动评估这种人机交互系统的方法。对于PASCAL VOC数据集，IAdet工具在减少数据库标注时间的同时提供了一个训练好的模型。这些结果是基于一个故意设计非常简单的IAdet系统。因此，IAdet易于改进，为强大的人机交互目标检测系统铺平了道路。

    This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
    
[^38]: 人机协作注释的最优高效二元问题研究

    Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])

    [http://arxiv.org/abs/2307.01578](http://arxiv.org/abs/2307.01578)

    本文研究了人机协作注释中的二元问题，提出了从最优通用解决方案到实际高效方法的一系列解决方案。我们将问题构建为在给定预测器的情况下用最少的是/否问题来完全注释一个二元分类数据集。通过编码理论和启发式算法，我们提供了一个计算可行且高效的替代方案。

    

    尽管数据注释对解释能力、人工智能解决方案的研究和开发至关重要，但大多数研究如主动学习或少样本学习都集中在样本效率问题上。本文研究了另一个被忽视的问题，即在给定预测器的情况下如何获得带注释的数据。针对简单的二元分类设置，我们提出了从最优通用解决方案到实际高效方法的一系列解决方案。问题被构建为给定预测器情况下拥有最少的是/否问题来完全注释一个二元分类数据集。对于一般二元问题，解决方案可以在编码理论中找到，其中最优的提问策略由可能的标签编码的霍夫曼编码给出。然而，即使对于较小的数据集大小，这种方法的计算复杂度也是难以处理的。我们提出了一种基于多种启发式和前瞻性最小化的替代实际解决方案。

    Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimizati
    
[^39]: 用于高效探索-利用策略的近似信息方法

    Approximate information for efficient exploration-exploitation strategies. (arXiv:2307.01563v1 [stat.ML])

    [http://arxiv.org/abs/2307.01563](http://arxiv.org/abs/2307.01563)

    本文提出了一种称为AIM的算法，用于解决决策中的探索-利用困境，特别针对多臂赌博机问题。AIM算法利用近似熵梯度来选择每个时间点要拉动的手臂，与Infomax和Thompson抽样相比，在性能上能够匹配，同时具有更好的计算速度、确定性和可计算性。经实证评估表明，AIM算法符合Lai-Robbins渐进界，对于不同的先验具有鲁棒性。

    

    本文针对决策中潜在的探索-利用困境，重点研究多臂赌博机问题。该问题涉及一个代理决定是否利用当前的知识以获取即时收益，还是探索新的途径以获得潜在的长期回报。我们引入了一种新颖的算法，即近似信息最大化（AIM），它利用熵梯度的解析近似来选择每个时间点要拉动的手臂。AIM在性能上与Infomax和Thompson抽样相匹配，同时提供了增强的计算速度、确定性和可计算性。对AIM的实证评估表明其符合Lai-Robbins渐进界，并展示了它对一系列先验的鲁棒性。其表达式可调节，可以在不同场景下进行具体优化。

    This paper addresses the exploration-exploitation dilemma inherent in decision-making, focusing on multi-armed bandit problems. The problems involve an agent deciding whether to exploit current knowledge for immediate gains or explore new avenues for potential long-term rewards. We here introduce a novel algorithm, approximate information maximization (AIM), which employs an analytical approximation of the entropy gradient to choose which arm to pull at each point in time. AIM matches the performance of Infomax and Thompson sampling while also offering enhanced computational speed, determinism, and tractability. Empirical evaluation of AIM indicates its compliance with the Lai-Robbins asymptotic bound and demonstrates its robustness for a range of priors. Its expression is tunable, which allows for specific optimization in various settings.
    
[^40]: 大型无人机层级云计算安全的深度学习分布式智能

    Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones. (arXiv:2307.01559v1 [cs.RO])

    [http://arxiv.org/abs/2307.01559](http://arxiv.org/abs/2307.01559)

    本文提出了一种新颖的分布式边缘-雾执行方案，通过在纳米无人机上冗余执行随机子网络来验证雾计算。与完全在机载上运行的最先进视觉姿态估计网络相比，采用分布式方式执行的更大网络显著提高了性能，并且在攻击情况下能够快速检测到。

    

    手掌大小的纳米无人机是一种吸引人的边缘节点类别，但它们的计算资源有限，无法在机载上运行大型深度学习模型。采用边缘-雾计算范式，我们可以将部分计算卸载到雾中，然而，如果不能信任雾节点或通信链路，这将带来安全问题。为了解决这个问题，我们提出了一种新颖的分布式边缘-雾执行方案，通过在纳米无人机上冗余执行随机子网络来验证雾计算。与完全在机载上运行的最先进视觉姿态估计网络相比，以分布式方式执行的更大网络的$R^2$得分提高了+0.19；在攻击的情况下，我们的方法能够在2秒内以95%的概率检测到。

    Palm-sized nano-drones are an appealing class of edge nodes, but their limited computational resources prevent running large deep-learning models onboard. Adopting an edge-fog computational paradigm, we can offload part of the computation to the fog; however, this poses security concerns if the fog node, or the communication link, can not be trusted. To tackle this concern, we propose a novel distributed edge-fog execution scheme that validates fog computation by redundantly executing a random subnetwork aboard our nano-drone. Compared to a State-of-the-Art visual pose estimation network that entirely runs onboard, a larger network executed in a distributed way improves the $R^2$ score by +0.19; in case of attack, our approach detects it within 2s with 95% probability.
    
[^41]: 可扩展的投影算子用于两视图学习任务的变量选择方法

    Scalable variable selection for two-view learning tasks with projection operators. (arXiv:2307.01558v1 [cs.LG])

    [http://arxiv.org/abs/2307.01558](http://arxiv.org/abs/2307.01558)

    本文提出了一种可扩展的变量选择方法，适用于两视图学习任务或向量值监督学习问题，能够处理规模极大的选择任务，并利用投影算子以及核函数进行相关性衡量和非线性相关模型的处理。

    

    本文提出了一种针对两视图设置或向量值监督学习问题的新型变量选择方法。我们的框架能够处理规模极大的选择任务，样本数甚至可以达到百万级。简言之，我们的方法通过选择与输出变量高度相关但与先前选择的变量无关的变量来进行变量选择。为了衡量相关性，我们的方法使用了投影算子及其代数的概念。通过投影算子，输入和输出变量集之间的关系、相关性也可以通过核函数来表达，从而可以利用非线性相关模型。通过实验证明了我们方法的可扩展性以及所选择特征的相关性。

    In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued mea
    
[^42]: 利用图像的学习压缩表示来进行语义分割的利用

    Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])

    [http://arxiv.org/abs/2307.01524](http://arxiv.org/abs/2307.01524)

    该论文提出了使用学习压缩表示进行语义分割的方法，以减少解压缩操作的延迟开销，并在实验证明了这种方法的有效性。

    

    自动驾驶车辆和先进驾驶辅助系统（ADAS）有可能彻底改变我们出行的方式。许多这样的车辆目前依赖于分割和目标检测算法来检测和跟踪周围的物体。从车辆收集的数据通常被发送到云服务器以便于对这些算法进行持续/终身学习。考虑到带宽限制，数据在发送到服务器之前被压缩，通常会在训练和分析时进行解压缩。在这项工作中，我们提出使用基于学习的压缩编解码器来减少标准流水线中解压缩操作所产生的时延开销。我们证明了学习的压缩表示还可以用于执行语义分割等任务，以获取图像。我们在Cityscapes数据集上进行了实验证实所提出的流水线，在其中实现了一个压缩因子。

    Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression facto
    
[^43]: 个性化治疗推荐的深度注意力Q网络

    Deep Attention Q-Network for Personalized Treatment Recommendation. (arXiv:2307.01519v1 [cs.LG])

    [http://arxiv.org/abs/2307.01519](http://arxiv.org/abs/2307.01519)

    本研究提出了Deep Attention Q-Network，利用Transformer架构在深度强化学习框架内，个性化推荐治疗方案，通过高效整合过去的病患观察信息，解决了仅依赖当前观察信息的限制，从而提高了治疗效果。

    

    个性化治疗对于每个病患至关重要，但是在达到最佳的医疗效果方面也面临挑战。最近强化学习的进展为个性化治疗推荐提供了有希望的方法；但是，这些方法仅依靠当前病患的观察信息（生命体征、人口统计数据）作为病患的状态，可能无法准确地代表病患的真实健康状况。这种限制妨碍了策略学习和评估，最终限制了治疗效果。在本研究中，我们提出了深度注意力Q网络来进行个性化治疗推荐，利用Transformer架构在深度强化学习框架内，高效地整合了所有过去的病患观察信息。我们在实际的败血症和急性低血压病人群中评估了该模型，证明了其优于现有模型的优越性。我们的模型源代码可以在https://github.com/stevenmsm/RL-ICU-DAQN获得。

    Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
    
[^44]: SelfFed: 自监督的联邦学习用于IoMT中的数据异质性和标签匮乏问题

    SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])

    [http://arxiv.org/abs/2307.01514](http://arxiv.org/abs/2307.01514)

    这篇论文提出了一种名为SelfFed的自监督联邦学习框架，用于解决IoMT中的数据异质性和标签匮乏问题。该框架包括预训练和微调两个阶段，通过分散训练和增强建模来克服数据异质性和标签稀缺问题。

    

    基于自监督学习的联邦学习范式在行业和研究领域中引起了很大的兴趣，因为它可以协作学习未标记但孤立的数据。然而，自监督的联邦学习策略在标签稀缺和数据异质性（即数据分布不同）方面存在性能下降的问题。在本文中，我们提出了适用于医疗物联网（IoMT）的SelfFed框架。我们的SelfFed框架分为两个阶段。第一个阶段是预训练范式，使用基于Swin Transformer的编码器以分散的方式进行增强建模。SelfFed框架的第一个阶段有助于克服数据异质性问题。第二个阶段是微调范式，引入对比网络和一种在有限标记数据上进行训练的新型聚合策略，用于目标任务的分散训练。这个微调阶段克服了标签稀缺问题。

    Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem
    
[^45]: 关注关系的子图嵌入与对比学习在药物相互作用预测中的应用

    Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction. (arXiv:2307.01507v1 [cs.LG])

    [http://arxiv.org/abs/2307.01507](http://arxiv.org/abs/2307.01507)

    本论文提出了一种新的关注关系的子图嵌入与对比学习方法RaSECo，用于预测多关系药物相互作用。该方法通过构建不同的药物图和采用对比学习机制，能够解决现有方法中对于新药物过拟合的问题。

    

    关注关系的子图嵌入对于预测多关系药物相互作用（DDI）非常有前景。然而，现有方法中大部分都局限于学习现有药物的子图嵌入，导致在涉及新药物的测试DDIs中出现严重过拟合问题。为了缓解这个问题，我们提出了一种基于关注关系的子图嵌入与对比学习的新型DDI预测方法，即RaSECo。RaSECo构建了两个异构药物图：多关系DDI图和基于多属性的药物相似度（DDS）图。这两个图分别用于学习和传播药物的子图嵌入，从而确保所有药物，包括新药物，能够聚合有效的子图嵌入。此外，我们采用了交叉视图对比机制来增强药物对（DP）的嵌入。

    Relation-aware subgraph embedding is promising for predicting multi-relational drug-drug interactions (DDIs). Typically, most existing methods begin by constructing a multi-relational DDI graph and then learning relation-aware subgraph embeddings (RaSEs) of drugs from the DDI graph. However, most existing approaches are usually limited in learning RaSEs of new drugs, leading to serious over-fitting when the test DDIs involve such drugs. To alleviate this issue, We propose a novel DDI prediction method based on relation-aware subgraph embedding with co-contrastive learning, RaSECo. RaSECo constructs two heterogeneous drug graphs: a multi-relational DDI graph and a multi-attributes-based drug-drug similarity (DDS) graph. The two graphs are used respectively for learning and propagating the RaSEs of drugs, thereby ensuring that all drugs, including new ones, can aggregate effective RaSEs. Additionally, we employ a cross-view contrastive mechanism to enhance drug-pair (DP) embedding. RaSEC
    
[^46]: 一体化：图神经网络的多任务提示

    All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])

    [http://arxiv.org/abs/2307.01504](http://arxiv.org/abs/2307.01504)

    本文提出了一种新颖的图模型的多任务提示方法，通过统一图提示和语言提示的格式，填补了预训练模型与各种图任务之间的差距。

    

    最近，“预训练和微调”已成为许多图任务的标准工作流程，因为它可以利用通用的图知识来缓解每个应用中缺乏图注释的问题。然而，节点级、边级和图级的图任务差异很大，导致预训练预文本通常与这些多任务不兼容。这种差距甚至可能导致对特定应用的“负迁移”，从而导致结果不佳。受自然语言处理（NLP）中提示学习的启发，该方法在各种NLP任务中利用先前知识已经显示出较大的有效性，我们研究了填补预训练模型和各种图任务之间差距的提示主题。在本文中，我们提出了一种新颖的用于图模型的多任务提示方法。具体来说，我们首先通过提示令牌、令牌结构和插入模式统一图提示和语言提示的格式。

    Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In
    
[^47]: 具有状态相关噪声的加速随机逼近

    Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])

    [http://arxiv.org/abs/2307.01497](http://arxiv.org/abs/2307.01497)

    该论文研究了一类具有状态相关噪声的随机平滑凸优化问题。通过引入两种非欧几里得加速随机逼近算法，实现了在精度、问题参数和小批量大小方面的最优性。

    

    我们考虑具有一般噪声假设的随机平滑凸优化问题的一类问题，在这些问题中，随机梯度观测的噪声的方差与算法产生的近似解的"亚最优性" 相关。这类问题在多种应用中自然而然地出现，特别是在统计学中的广义线性回归问题中。然而，据我们所知，现有的解决这类问题的随机逼近算法在精度、问题参数和小批量大小的依赖性方面都未达到最优。我们讨论了两种非欧几里得加速随机逼近算法——随机加速梯度下降（SAGD）和随机梯度外推（SGE）——它们具有一种特殊的对偶关系

    We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
    
[^48]: 基于深度学习的安卓和Windows系统恶意软件检测综述

    Review of Deep Learning-based Malware Detection for Android and Windows System. (arXiv:2307.01494v1 [cs.LG])

    [http://arxiv.org/abs/2307.01494](http://arxiv.org/abs/2307.01494)

    本文综述了基于深度学习的安卓和Windows系统恶意软件检测技术，这些技术在检测不同恶意软件家族上均取得了完美的准确性。

    

    分辨恶意软件的行为和威胁级别对于制定防御策略至关重要。为此，各种反恶意软件系统已经开发出来用于区分不同的恶意软件。然而，大部分最近的恶意软件都具备人工智能能力，并且可以使用不同的混淆技术欺骗传统的反恶意软件系统。因此，只有基于人工智能的反恶意软件系统对抗这些技术更加强大，并且能够检测到恶意活动所涉及的恶意软件文件中的不同特征。本研究回顾了两种基于人工智能的技术，分别用于在Windows和安卓操作系统中检测恶意软件。这两种技术在检测各种恶意软件家族时均实现了完美的准确性。

    Differentiating malware is important to determine their behaviors and level of threat; as well as to devise defensive strategy against them. In response, various anti-malware systems have been developed to distinguish between different malwares. However, most of the recent malware families are Artificial Intelligence (AI) enable and can deceive traditional anti-malware systems using different obfuscation techniques. Therefore, only AI-enabled anti-malware system is robust against these techniques and can detect different features in the malware files that aid in malicious activities. In this study we review two AI-enabled techniques for detecting malware in Windows and Android operating system, respectively. Both the techniques achieved perfect accuracy in detecting various malware families.
    
[^49]: Nexus sine qua non：基于节点识别的神经网络连接的时空预测多变量时间序列

    Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])

    [http://arxiv.org/abs/2307.01482](http://arxiv.org/abs/2307.01482)

    提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。

    

    建模和预测多变量时间序列不仅有助于从业者的决策，还加深我们对底层动态系统的科学理解。时空图神经网络（STGNNs）已经成为强大的预测器，并成为学习时空表示的事实标准模型。然而，现有的STGNNs的架构往往通过堆叠一系列复杂的层次而变得复杂。设计的模型可能多余或难以理解，这给复杂性和可扩展性带来了巨大挑战。这些问题促使我们重新审视现代STGNNs的设计，并确定对强大和高效的神经预测器有所贡献的核心原则。在这里，我们提出了一个紧凑的预测模型，完全由密集编码器-解码器和消息传递层来定义，基于节点识别，没有任何复杂的顺序模块，例如TCNs，RNNs和Transformers。通过实证重新评估该模型的性能，我们证明了该模型的有效性和高效性。

    Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
    
[^50]: 超越保守主义：离线多智能体强化学习中的扩散策略

    Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning. (arXiv:2307.01472v1 [cs.AI])

    [http://arxiv.org/abs/2307.01472](http://arxiv.org/abs/2307.01472)

    DOM2是一种离线多智能体强化学习模型，通过扩散策略的改进，提高了算法在性能、泛化能力和数据效率方面的表现。DOM2在多智能体粒子和多智能体MuJoCo环境中优于现有算法，并在移位环境中具有更好的泛化能力。此外，DOM2还展现了卓越的数据效率，只使用较少的数据即可达到最先进的性能水平。

    

    我们提出了一种新颖的离线多智能体模型（DOM2），用于离线多智能体强化学习（MARL）。与现有算法在策略设计中主要依赖保守主义不同，DOM2基于扩散增强了策略的表达能力和多样性。具体而言，我们将扩散模型纳入策略网络，并提出了一种基于轨迹的数据增强方案进行训练。这些关键因素使我们的算法在环境变化方面更加稳健，并在性能、泛化能力和数据效率方面取得了显著的改进。我们广泛的实验结果表明，DOM2在多智能体粒子和多智能体MuJoCo环境中优于现有的最先进方法，并且由于其高表达能力和多样性，在移位环境中具有更好的泛化能力。此外，DOM2表现出卓越的数据效率，在与现有算法相比只使用$20+$倍少的数据下，就能达到最先进的性能水平。

    We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algori
    
[^51]: 对驾驶员凝视估计和在凝视行为理解中的应用的综述

    A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding. (arXiv:2307.01470v1 [cs.CV])

    [http://arxiv.org/abs/2307.01470](http://arxiv.org/abs/2307.01470)

    本文综述了驾驶员凝视估计的基本知识、估计方法以及在实际驾驶场景中的应用。通过讨论不同的数据收集方法和算法技术，对驾驶员的凝视行为进行了理解。

    

    驾驶员的凝视在驾驶员注意力检测、视觉分心检测、凝视行为理解和构建驾驶员辅助系统等凝视基于应用中起着重要的作用。本研究的主要目标是对驾驶员的凝视基础知识、驾驶员凝视估计方法及其在现实驾驶场景中的应用进行总结。首先我们讨论与驾驶员凝视相关的基础知识，包括头戴式和远程设置的凝视估计以及每种数据收集方法所使用的术语。接下来，我们列举了现有的基准驾驶员凝视数据集，重点介绍了数据收集方法和所使用的设备。然后，我们讨论了用于驾驶员凝视估计的算法，主要涉及传统的机器学习和深度学习技术。接着，估计的驾驶员凝视被用于理解在驾驶过程中的凝视行为。

    Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuver
    
[^52]: 因果强化学习：一项综述

    Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])

    [http://arxiv.org/abs/2307.01452](http://arxiv.org/abs/2307.01452)

    这项综述总结了因果强化学习的研究文献，强调因果关系的重要作用，它能够形式化知识并实现有效的知识传递。

    

    强化学习是在不确定性条件下解决序列决策问题的一种重要范式。尽管近几十年来取得了许多显著的成就，但将强化学习方法应用于现实世界仍然具有挑战性。其中一个主要障碍是强化学习代理缺乏对世界的基本理解，因此必须通过大量的试错交互学习。他们可能在解释自己的决策以及推广所获得的知识方面面临挑战。然而，因果关系具有明显的优势，它可以以系统化的方式形式化知识，并利用不变性进行有效的知识传递。这导致了因果强化学习的出现，它是强化学习的一个子领域，旨在通过将因果关系纳入学习过程来增强现有算法。在这篇综述中，我们全面回顾了有关因果强化学习的文献。

    Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcemen
    
[^53]: 将实验数据与观测数据结合的双机器学习方法

    A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])

    [http://arxiv.org/abs/2307.01449](http://arxiv.org/abs/2307.01449)

    这种双机器学习方法将实验和观测研究结合起来，能够测试假设的违反情况并一致估计处理效应。它提供了半参数高效的处理效应估计器。这种方法在实际环境中是可行的。

    

    实验和观测研究通常由于无法测试的假设而缺乏有效性。我们提出了一种双机器学习方法，将实验和观测研究结合起来，使从业人员能够测试假设违反情况并一致估计处理效应。我们的框架在较轻的假设下测试外部效度和可忽视性的违反情况。当只有一个假设被违反时，我们提供半参数高效的处理效应估计器。然而，我们的无免费午餐定理强调了准确识别违反的假设对一致的处理效应估计的必要性。我们通过三个实际案例研究展示了我们方法的适用性，并突出了其在实际环境中的相关性。

    Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
    
[^54]: 关于条件和组合语言模型可微提示的论文

    On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])

    [http://arxiv.org/abs/2307.01446](http://arxiv.org/abs/2307.01446)

    本论文研究了条件和组合的可微提示方法，提出了Prompt Production System（PRopS）模型，通过将任务说明或输入元数据转化为连续的提示，使预训练语言模型（PLM）能够生成任务特定的输出。该模型利用了神经网络结构和离散规则的学习，适用于组合式迁移学习和少样本学习。实证和理论分析表明，PRopS在PLM适应中始终优于其他技术，并且通常改进了完全微调的方法。

    

    提示已被证明是一种有效的方法，用于使预训练语言模型（PLM）在下游任务中表现出色。提示可以由人工设计的词序列或学习得到的连续嵌入来表示。在这项工作中，我们研究了条件和组合的可微提示。我们提出了一个新模型，Prompt Production System（PRopS），它学习将任务说明或输入元数据转化为连续的提示，从而激发PLM产生任务特定的输出。我们的模型使用基于我们对于产品系统的神经形式化的模块化网络结构，这使得模型能够学习离散规则——神经函数，这些函数学习专门将特定的提示输入模式转化为特定的输出，使其适用于组合式迁移学习和少样本学习。我们进行了广泛的实证和理论分析，并展示了PRopS始终超越其他PLM适应技术，并且通常改进了完全微调的方法。

    Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine
    
[^55]: 使用图指针网络学习组合优化中的分支策略

    Learning to Branch in Combinatorial Optimization with Graph Pointer Networks. (arXiv:2307.01434v1 [cs.LG])

    [http://arxiv.org/abs/2307.01434](http://arxiv.org/abs/2307.01434)

    本文提出了一种使用图指针网络学习分支策略的方法，通过提取图特征和全局特征来表示求解器状态，并结合了图神经网络和指针机制，通过模仿经典的强分支专家规则来训练模型。实验证明该方法在解决组合优化问题上明显优于其他方法。

    

    分支定界是解决组合优化问题的常见方法。本文提出了一种图指针网络模型，用于学习分支定界中的变量选择策略。我们提取了图特征、全局特征和历史特征来表示求解器状态。所提出的模型将图神经网络和指针机制相结合，可以有效地将求解器状态映射到分支变量决策上。该模型通过设计的前k个KL散度损失函数训练，以模仿经典的强分支专家规则。一系列基准问题的实验证明，该方法明显优于广泛使用的专家设计的分支规则。我们的方法在所有测试实例上在求解速度和搜索树大小方面也优于最先进的基于机器学习的分支定界方法。此外，该模型可以推广到未见过的实例和可扩展的情况。

    Branch-and-bound is a typical way to solve combinatorial optimization problems. This paper proposes a graph pointer network model for learning the variable selection policy in the branch-and-bound. We extract the graph features, global features and historical features to represent the solver state. The proposed model, which combines the graph neural network and the pointer mechanism, can effectively map from the solver state to the branching variable decisions. The model is trained to imitate the classic strong branching expert rule by a designed top-k Kullback-Leibler divergence loss function. Experiments on a series of benchmark problems demonstrate that the proposed approach significantly outperforms the widely used expert-designed branching rules. Our approach also outperforms the state-of-the-art machine-learning-based branch-and-bound methods in terms of solving speed and search tree size on all the test instances. In addition, the model can generalize to unseen instances and sca
    
[^56]: 生成流网络: 基于马尔可夫链的视角

    Generative Flow Networks: a Markov Chain Perspective. (arXiv:2307.01422v1 [cs.LG])

    [http://arxiv.org/abs/2307.01422](http://arxiv.org/abs/2307.01422)

    通过使用马尔可夫链，本文提出了生成流网络的新视角，对于具有明确组合结构的样本，通过将采样视为序贯决策问题，能够消除马尔可夫链蒙特卡洛方法在高度多模式分布中收敛缓慢的问题。

    

    尽管马尔可夫链蒙特卡洛方法（MCMC）为从概率分布中采样提供了一个一般性框架，但当目标分布是高度多模式时，它们往往收敛缓慢。最近，生成流网络（GFlowNets）被提出作为一种替代框架，通过将采样视为序贯决策问题，可以减轻这个问题，当样本具有明确的组合结构时。尽管最初是从流网络的角度引入的，但最近GFlowNets的进一步发展越来越多地受到马尔可夫链文献的启发，完全绕过了流的需求。在本文中，我们形式化了这种联系，并提供了一种使用马尔可夫链的新视角来解释GFlowNets的统一视图，无论状态空间的性质如何，都可以视为是经常换手的马尔可夫链。将GFlowNets定位到与MCMC方法相同的理论框架下，也允许...

    While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allo
    
[^57]: 具有跳连接的贝叶斯卷积神经网络的自由能研究

    Free energy of Bayesian Convolutional Neural Network with Skip Connection. (arXiv:2307.01417v1 [cs.LG])

    [http://arxiv.org/abs/2307.01417](http://arxiv.org/abs/2307.01417)

    本文研究了具有跳连接的贝叶斯卷积神经网络的自由能，揭示了其不依赖于过度参数化，并且具有类似的泛化误差性质。

    

    自从Residual Network(ResNet)的成功之后，许多卷积神经网络(CNNs)的架构都采用了跳连接。虽然跳连接的CNN的泛化性能已在集成学习框架下得到解释，但参数数量的依赖性尚未揭示。本文中，我们展示了在贝叶斯学习中，有跳连接和无跳连接的卷积神经网络的贝叶斯自由能。具有跳连接的贝叶斯CNN的自由能上界不依赖于过度参数化，而贝叶斯CNN的泛化误差也具有相似的性质。

    Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters have not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.
    
[^58]: 多预测器融合：结合基于学习和规则的轨迹预测器

    Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors. (arXiv:2307.01408v1 [cs.RO])

    [http://arxiv.org/abs/2307.01408](http://arxiv.org/abs/2307.01408)

    多预测器融合算法（MPF）通过将学习型和规则型的预测器以概率组合，提高了轨迹预测器的性能，并在多个指标上表现出最佳的一致性能。

    

    轨迹预测模块是实现自动驾驶车辆（AV）安全高效规划的关键驱动因素，尤其在高度互动交通场景下。最近，基于学习的轨迹预测器通过能够从数据中学习其他参与者的多模态行为的能力，取得了相当大的成功，提供了最先进的性能。本文提出了一种称为多预测器融合（MPF）的算法，通过融合满足基于逻辑规则的运动规划器来增强学习型预测器的性能。MPF通过将来自独立预测器的轨迹按照信念分布混合，概率地组合了基于学习和规则的预测器。实验结果表明，MPF在各种指标上优于两个独立预测器，并提供了最一致的性能。

    Trajectory prediction modules are key enablers for safe and efficient planning of autonomous vehicles (AVs), particularly in highly interactive traffic scenarios. Recently, learning-based trajectory predictors have experienced considerable success in providing state-of-the-art performance due to their ability to learn multimodal behaviors of other agents from data. In this paper, we present an algorithm called multi-predictor fusion (MPF) that augments the performance of learning-based predictors by imbuing them with motion planners that are tasked with satisfying logic-based rules. MPF probabilistically combines learning- and rule-based predictors by mixing trajectories from both standalone predictors in accordance with a belief distribution that reflects the online performance of each predictor. In our results, we show that MPF outperforms the two standalone predictors on various metrics and delivers the most consistent performance.
    
[^59]: 学习使用对比学习进行通信

    Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])

    [http://arxiv.org/abs/2307.01403](http://arxiv.org/abs/2307.01403)

    本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。

    

    通信是多智能体强化学习中协调的有力工具。但在分散的环境中诱导一个有效的共同语言是一个困难的挑战。在这项工作中，我们引入了一个替代视角，即将智能体之间发送的通信消息视为环境状态的不完整视图。通过检查发送和接收的消息之间的关系，我们提出使用对比学习来最大化给定轨迹的消息之间的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作。使用定性指标和表示探测，我们展示了我们的方法诱导了更对称的通信并从环境中捕获了全局状态信息。总体而言，我们展示了对比学习的力量以及利用消息作为编码实现有效通信的重要性。

    Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
    
[^60]: 与高爆炸物相互作用的喷气的时空替代物：第二部分--对高维栅格数据进行聚类

    Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II -- Clustering Extremely High-Dimensional Grid-Based Data. (arXiv:2307.01400v1 [cs.LG])

    [http://arxiv.org/abs/2307.01400](http://arxiv.org/abs/2307.01400)

    这项研究致力于解决构建喷气与高爆炸物相互作用计算机模拟的准确空间输出替代模型的挑战。通过将输出数据聚类并为每个聚类构建独立的替代模型，可以提高精度。当空间域由数百万个栅格点表示时，数据的聚类变得更加困难。

    

    构建一个准确的计算机模拟时空输出的替代模型是一项具有挑战性的任务。改进替代模型准确性的一种简单方法是基于相似性将输出进行聚类，并为每个聚类构建一个独立的替代模型。当每个时间步骤的输出具有中等大小时，这种聚类相对简单。然而，当空间域由数以百万计的栅格点表示时，数据的聚类变得更加困难。在本报告中，我们考虑了与高爆炸物相互作用的喷流的输出数据。这些数据在不同大小的空间域中可用，在空间坐标上变化，并以每个模拟时间步骤在多个文件中分布输出。我们首先描述了在聚类之前如何将这些数据统一为一致的格式。从数据的随机投影中借鉴了随机投影的想法

    Building an accurate surrogate model for the spatio-temporal outputs of a computer simulation is a challenging task. A simple approach to improve the accuracy of the surrogate is to cluster the outputs based on similarity and build a separate surrogate model for each cluster. This clustering is relatively straightforward when the output at each time step is of moderate size. However, when the spatial domain is represented by a large number of grid points, numbering in the millions, the clustering of the data becomes more challenging. In this report, we consider output data from simulations of a jet interacting with high explosives. These data are available on spatial domains of different sizes, at grid points that vary in their spatial coordinates, and in a format that distributes the output across multiple files at each time step of the simulation. We first describe how we bring these data into a consistent format prior to clustering. Borrowing the idea of random projections from data
    
[^61]: 面向高性能数据框的并行处理模式的深入分析

    In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes. (arXiv:2307.01394v1 [cs.DC])

    [http://arxiv.org/abs/2307.01394](http://arxiv.org/abs/2307.01394)

    本论文通过从高性能计算的角度出发，对面向高性能数据框的并行处理模式进行了深入分析，并指出当前最广泛使用的串行数据框在处理中等规模的数据集时存在性能限制，提出改进的空间。

    

    在过去的十年中，由于大数据革命，数据科学领域在研究和工业界都得到了巨大的扩展。人工智能（AI）和机器学习（ML）给数据工程应用带来了更多的复杂性，这些应用现在被集成到数据处理管道中以处理大量的数据。通常，在这些管道中花费大量时间进行数据预处理，因此提高其效率直接影响整体管道的性能。近年来，社区已经接受了数据框作为事实上的数据表示和操作的数据结构的概念。然而，目前最广泛使用的串行数据框（R、pandas）在处理中等规模的数据集时存在性能限制。我们相信从高性能计算的角度出发，还有很大的提升空间来解决这个问题。

    The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we p
    
[^62]: 与高爆炸物相互作用的喷流的时空替代模型：第一部分 - 小样本量分析

    Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I -- Analysis with a Small Sample Size. (arXiv:2307.01393v1 [cs.LG])

    [http://arxiv.org/abs/2307.01393](http://arxiv.org/abs/2307.01393)

    本研究通过一个二维问题探讨了如何构建高质量的替代模型，以解决模拟复杂现象时所面临的挑战。该方法可以应用于与高爆炸物相互作用的喷流问题。

    

    计算机模拟，特别是对于复杂的现象，可能需要高性能计算资源。通常，为了理解一个现象，需要运行多个模拟，每个模拟都有不同的模拟输入参数。然后，使用这些数据创建一个插值器或替代模型，将模拟输出与相应的输入相关联。当输入和输出是标量时，简单的机器学习模型就足够了。然而，当模拟输出是以二维或三维空间维度为位置的向量值，并且通常具有时间组件时，创建替代模型就更具挑战性。在本报告中，我们使用一个二维的喷流与高爆炸物相互作用的问题，来理解如何构建高质量的替代模型。我们的数据集特点独特 - 每个模拟产生的向量值输出在超过两百万个空间位置可用；每个模拟运行的时间相对较少。

    Computer simulations, especially of complex phenomena, can be expensive, requiring high-performance computing resources. Often, to understand a phenomenon, multiple simulations are run, each with a different set of simulation input parameters. These data are then used to create an interpolant, or surrogate, relating the simulation outputs to the corresponding inputs. When the inputs and outputs are scalars, a simple machine learning model can suffice. However, when the simulation outputs are vector valued, available at locations in two or three spatial dimensions, often with a temporal component, creating a surrogate is more challenging. In this report, we use a two-dimensional problem of a jet interacting with high explosives to understand how we can build high-quality surrogates. The characteristics of our data set are unique - the vector-valued outputs from each simulation are available at over two million spatial locations; each simulation is run for a relatively small number of ti
    
[^63]: 真实世界中对抗学习在欺诈检测中的挑战与展望

    Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives. (arXiv:2307.01390v1 [cs.LG])

    [http://arxiv.org/abs/2307.01390](http://arxiv.org/abs/2307.01390)

    欺诈检测是数据经济的关键防御机制，但机器学习在面对欺诈活动和对抗性攻击时面临挑战。对抗性机器学习在特定领域具有巨大潜力，但在将其推广到其他领域和应用方面还有待进一步研究。

    

    数据经济依赖于数据驱动的系统，而复杂的机器学习应用则受到诈骗活动和对抗性攻击的威胁，从而威胁到其安全和可信度。对抗性机器学习的研究兴趣在过去的十年中显著增长，揭示了有效攻击如何严重影响学习应用。尽管对抗性机器学习的早期结果表明了该方法在特定领域（如图像处理）的巨大潜力，但在如何将对抗性技术推广到其他领域和应用方面的研究文献和实践中仍存在差距。欺诈检测是数据经济的关键防御机制，也对机器学习提出了几个挑战。在这项工作中，我们描述了对欺诈检测系统的攻击与其他系统的不同之处。

    Data economy relies on data-driven systems and complex machine learning applications are fueled by them. Unfortunately, however, machine learning models are exposed to fraudulent activities and adversarial attacks, which threaten their security and trustworthiness. In the last decade or so, the research interest on adversarial machine learning has grown significantly, revealing how learning applications could be severely impacted by effective attacks. Although early results of adversarial machine learning indicate the huge potential of the approach to specific domains such as image processing, still there is a gap in both the research literature and practice regarding how to generalize adversarial techniques in other domains and applications. Fraud detection is a critical defense mechanism for data economy, as it is for other applications as well, which poses several challenges for machine learning. In this work, we describe how attacks against fraud detection systems differ from other
    
[^64]: 通过逆向推断识别淀粉样蛋白β积累与阿尔茨海默病进展之间的因果关系

    Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer's Disease Progression via Counterfactual Inference. (arXiv:2307.01389v1 [cs.LG])

    [http://arxiv.org/abs/2307.01389](http://arxiv.org/abs/2307.01389)

    通过逆向推断方法揭示淀粉样蛋白β积累与阿尔茨海默病之间的因果关系，有助于早期诊断和治疗。

    

    阿尔茨海默病（AD）是一种神经退行性疾病，始于淀粉样蛋白β沉积，随后引发神经元丢失和结构、功能以及认知的恶化。通过18F-florbetapir (AV45) 正电子发射断层摄影（PET）成像测量的淀粉样蛋白β在大脑中的积累已被广泛应用于AD的早期诊断。然而，淀粉样蛋白β积累与AD病理生理之间的关系仍不清楚，因此需要因果推断方法来揭示淀粉样蛋白β水平如何影响AD的发展。在本文中，我们提出了一种用于估计连续治疗水平下个体治疗效果的图形变系数神经网络（GVCNet），使用了图卷积神经网络。我们强调了因果推断方法，包括GVCNet，在测量淀粉样蛋白β积累与AD病理生理之间的区域因果连接方面的潜力，这可能成为早期诊断和治疗的强大工具。

    Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning with amyloidosis, followed by neuronal loss and deterioration in structure, function, and cognition. The accumulation of amyloid-beta in the brain, measured through 18F-florbetapir (AV45) positron emission tomography (PET) imaging, has been widely used for early diagnosis of AD. However, the relationship between amyloid-beta accumulation and AD pathophysiology remains unclear, and causal inference approaches are needed to uncover how amyloid-beta levels can impact AD development. In this paper, we propose a graph varying coefficient neural network (GVCNet) for estimating the individual treatment effect with continuous treatment levels using a graph convolutional neural network. We highlight the potential of causal inference approaches, including GVCNet, for measuring the regional causal connections between amyloid-beta accumulation and AD pathophysiology, which may serve as a robust tool for early diagnosis and 
    
[^65]: 样本推断中的系统偏差及其对机器学习的影响

    Systematic Bias in Sample Inference and its Effect on Machine Learning. (arXiv:2307.01384v1 [cs.LG])

    [http://arxiv.org/abs/2307.01384](http://arxiv.org/abs/2307.01384)

    机器学习模型在样本推断中存在系统偏差，尤其是在少数群体中，导致目标特征欠预测。这种欠预测模式是小样本统计推断的可预测结果。

    

    在机器学习模型中，常观察到的一个模式是目标特征的欠预测，对于给定类别的成员，模型预测的目标率通常低于训练集中该类别成员的实际目标率。这种欠预测在少数群体的成员中通常更明显；例如，在“adult”数据集中，无论是男性还是女性的收入水平都被欠预测，但对于女性（该数据集中的少数群体），欠预测程度明显更高。我们提出，这种针对少数群体的欠预测模式是对小样本统计推断的可预测结果。当对一个新个体进行分类时，机器学习模型进行的推断不是基于整个训练集，而是基于一个与新个体在某种程度上相似的子集，这些子集的大小通常遵循幂律分布，大多数都很小（并且这些子集必须是必然小的）。

    A commonly observed pattern in machine learning models is an underprediction of the target feature, with the model's predicted target rate for members of a given category typically being lower than the actual target rate for members of that category in the training set. This underprediction is usually larger for members of minority groups; while income level is underpredicted for both men and women in the 'adult' dataset, for example, the degree of underprediction is significantly higher for women (a minority in that dataset). We propose that this pattern of underprediction for minorities arises as a predictable consequence of statistical inference on small samples. When presented with a new individual for classification, an ML model performs inference not on the entire training set, but on a subset that is in some way similar to the new individual, with sizes of these subsets typically following a power law distribution so that most are small (and with these subsets being necessarily 
    
[^66]: 隐式内存变换器用于计算高效的同时语音翻译

    Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])

    [http://arxiv.org/abs/2307.01381](http://arxiv.org/abs/2307.01381)

    本文提出了一种隐式内存变换器，通过新的左上下文方法隐式保留记忆，从而实现了计算高效的同时语音翻译。在MuST-C数据集上的实验表明，该方法提供了显著的速度提升和少量的性能损失。

    

    同时语音翻译是一项困难的人类交流任务，即在进行语音输入的同时生成翻译。对于这样的流式任务，使用块处理将输入序列分割成片段的Transformer在降低成本的同时实现了最先进的性能。当前的方法允许信息在片段之间传播，包括左上下文和存储器库，但它们既是不充分的表示又是不必要的计算开销。在本文中，我们提出了一种隐式内存变换器，通过一种新的左上下文方法隐式保留记忆，从而消除了需要用存储器库显式表示记忆的需求。我们通过前一个片段的注意力输出生成左上下文，并将其包含在当前片段的注意力计算的键和值中。对MuST-C数据集的实验证明，隐式内存变换器提供了显著的速度提升和少量的性能损失。

    Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedu
    
[^67]: 将关注点转移到相关性上: 探索大型语言模型的不确定性估计

    Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])

    [http://arxiv.org/abs/2307.01379](http://arxiv.org/abs/2307.01379)

    本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。

    

    虽然大型语言模型（LLMs）在自然语言生成方面表现出了巨大的潜力，但是对于模型生成的不确定性的特征化仍然具有挑战性，即用户何时可以信任模型的输出。我们的研究基于一些启发性的事实，即在自回归的LLMs中，令牌在反映生成的含义方面是不平等的，即一些令牌比其他令牌更相关（或更具代表性），然而在估计不确定性时所有的令牌被等值对待。这是由于语言冗余，其中大部分情况下，只需要几个关键词就足以传达一个长句的含义。我们将这些不平等称为生成的不平等，并研究它们如何影响不确定性的估计。我们的结果揭示，相当数量的令牌和包含有限语义的句子，在估计不确定性时被同等或甚至更加重视。为了解决由生成的不平等引起的这些偏差，我们提出了共同转移关注点来更好地估计不确定性。

    Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
    
[^68]: 可转移上下文：解决同时语音翻译中的训练-推理上下文不匹配问题

    Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])

    [http://arxiv.org/abs/2307.01377](http://arxiv.org/abs/2307.01377)

    该论文提出了一种名为"可转移上下文"的简单而有效的方案，用于解决同时语音翻译中的训练-推理上下文不匹配问题。通过保持一致的段落和上下文大小，即使存在部分填充的段落，该方案在流式任务的分段Transformer中也是广泛适用的。实验证明，应用于Augmented Memory Transformer后可以提高BLEU得分。

    

    在同时语音翻译中，使用分段处理的Transformer模型已经成为一种有效的架构。然而，这种模型在训练和推理环境之间创建了上下文不匹配的问题，阻碍了潜在的翻译准确性。我们通过提出可转移上下文来解决这个问题，这是一种简单而有效的方案，可以确保在训练和推理过程中始终维持一致的段落和上下文大小，即使由于流式翻译的性质导致部分填充的段落存在。可转移上下文在流式任务的分段Transformer中也是广泛适用的。我们在MUST-C数据集的英德、英法和英西语言对上进行的实验表明，将该方案应用于Augmented Memory Transformer（一种用于同时语音翻译的最先进模型）可以平均提高2.09、1.83和1.95个BLEU分数。

    Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value f
    
[^69]: 自适应主成分回归在面板数据中的应用

    Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])

    [http://arxiv.org/abs/2307.01357](http://arxiv.org/abs/2307.01357)

    本文提出了自适应主成分回归方法，并在面板数据中的应用中获得了均匀有限样本保证。该方法可以用于面板数据中的实验设计，特别是当干预方案是自适应分配的情况。

    

    主成分回归(PCR)是一种流行的固定设计误差变量回归技术，它是线性回归的推广，观测的协变量受到随机噪声的污染。我们在数据收集时提供了在线（正则化）PCR的第一次均匀有限样本保证。由于分析固定设计中PCR的证明技术无法很容易地扩展到在线设置，我们的结果依赖于将现代鞅浓度的工具适应到误差变量设置中。作为我们界限的应用，我们在面板数据设置中提供了实验设计框架，当干预被自适应地分配时。我们的框架可以被认为是合成控制和合成干预框架的泛化，其中数据是通过自适应干预分配策略收集的。

    Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
    
[^70]: 通过流函数算子学习多相介质中的传输通用解决方案

    Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator. (arXiv:2307.01354v1 [physics.comp-ph])

    [http://arxiv.org/abs/2307.01354](http://arxiv.org/abs/2307.01354)

    通过学习Buckley-Leverett PDE的流函数空间与解空间之间的映射，使用Physics-Informed DeepONets (PI-DeepONets)在没有配对输入-输出观测的情况下，通过软约束的底层物理定律来加速多孔介质中流体流动和传输的数值模拟。a

    

    传统的用于模拟多孔介质中流体流动和传输的数值方案可能计算代价高昂。科学计算中机器学习的进展具有潜力帮助加快许多科学和工程领域中的模拟时间。DeepONet是最近出现的一种强大工具，可以通过学习偏微分方程(PDE)的算子(函数空间之间的映射)来加速求解。在这项工作中，我们学习了Buckley-Leverett PDE的流函数空间与解空间(饱和度)之间的映射。我们使用物理信息DeepONets (PI-DeepONets)来实现这种映射，除了一组给定的初始条件或边界条件之外，不需要任何成对的输入-输出观测，因此消除了昂贵的数据生成过程。通过在模型训练过程中利用软约束的底层物理定律，类似于物理信息神经网络(PINNs)，以及一个u

    Traditional numerical schemes for simulating fluid flow and transport in porous media can be computationally expensive. Advances in machine learning for scientific computing have the potential to help speed up the simulation time in many scientific and engineering fields. DeepONet has recently emerged as a powerful tool for accelerating the solution of partial differential equations (PDEs) by learning operators (mapping between function spaces) of PDEs. In this work, we learn the mapping between the space of flux functions of the Buckley-Leverett PDE and the space of solutions (saturations). We use Physics-Informed DeepONets (PI-DeepONets) to achieve this mapping without any paired input-output observations, except for a set of given initial or boundary conditions; ergo, eliminating the expensive data generation process. By leveraging the underlying physical laws via soft penalty constraints during model training, in a manner similar to Physics-Informed Neural Networks (PINNs), and a u
    
[^71]: Patch-CNN: 从最少扩散协议获得高保真度扩散张量估计的数据有效深度学习方法

    Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols. (arXiv:2307.01346v1 [cs.CV])

    [http://arxiv.org/abs/2307.01346](http://arxiv.org/abs/2307.01346)

    Patch-CNN是一种数据有效的深度学习方法，可以从仅六个方向的扩散加权图像中精确估计扩散张量，克服了训练数据需求大和无法估计纤维方向的限制。

    

    我们提出了一种新的方法，Patch-CNN，用于从仅六个方向的扩散加权图像(DWI)中估计扩散张量(DT)。最近提出了基于深度学习的方法来估计dMRI参数，可以使用基于体素的全连接神经网络(FCN)或基于图像的卷积神经网络(CNN)。在压力甚大的临床环境中，时间的压力限制了成像方向的数量，现有的方法要么需要大量的训练图像体积(基于图像的CNN)，要么无法估计纤维方向(基于体素的FCN)，无法进行束迹估计。为了克服这些限制，我们提出了Patch-CNN，它是一个带有最小(非基于体素的)卷积核(3×3×3)的神经网络。与基于体素的FCN相比，这样做的优势在于可以利用局部解剖信息。与基于图像的CNN相比，最小的卷积核大大降低了训练数据的需求。

    We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from only six-direction diffusion weighted images (DWI). Deep learning-based methods have been recently proposed for dMRI parameter estimation, using either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN). In the acute clinical context -- where pressure of time limits the number of imaged directions to a minimum -- existing approaches either require an infeasible number of training images volumes (image-wise CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for tractogram estimation. To overcome these limitations, we propose Patch-CNN, a neural network with a minimal (non-voxel-wise) convolutional kernel (3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of allowing the network to leverage local anatomical information. Compared with image-wise CNNs, the minimal kernel vastly reduces training data demand. Evaluate
    
[^72]: 对航海对象分类的鲁棒性不确定性估计

    Robust Uncertainty Estimation for Classification of Maritime Objects. (arXiv:2307.01325v1 [cs.LG])

    [http://arxiv.org/abs/2307.01325](http://arxiv.org/abs/2307.01325)

    本论文研究了在航海领域中使用不确定性估计的方法，通过使用蒙特卡洛Dropout获得的类内不确定性和异常检测的结合，对航海对象进行了鲁棒的分类。实验证明了这种方法相比其他方法在性能上的改进。

    

    我们探讨了在航海领域中使用不确定性估计的方法，并展示了在玩具数据集（CIFAR10）上的功效，并在我们自己的数据集SHIPS上验证了其有效性。我们提出了一种方法，通过使用蒙特卡洛Dropout获得的类内不确定性和异常检测领域的最新发现相结合，以获得更全面的不确定性度量。我们探讨了引入的不确定性度量与其在CIFAR10和实际应用环境中的工作表现之间的关系。我们的工作在没有离群数据训练的模型上将FPR95提高了8％，相比于目前最高性能的工作。相比于常规实现的Wide ResNet，我们的性能提高了77％。我们发布了SHIPS数据集，并通过与基线相比将FPR95提高了44.2％来展示我们方法的有效性。我们的方法适用于各种模型，易于实现，并且通常不需要重新训练模型。

    We explore the use of uncertainty estimation in the maritime domain, showing the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset, SHIPS. We present a method joining the intra-class uncertainty achieved using Monte Carlo Dropout, with recent discoveries in the field of outlier detection, to gain more holistic uncertainty measures. We explore the relationship between the introduced uncertainty measures and examine how well they work on CIFAR10 and in a real-life setting. Our work improves the FPR95 by 8% compared to the current highest-performing work when the models are trained without out-of-distribution data. We increase the performance by 77% compared to a vanilla implementation of the Wide ResNet. We release the SHIPS dataset and show the effectiveness of our method by improving the FPR95 by 44.2% with respect to the baseline. Our approach is model agnostic, easy to implement, and often does not require model retraining.
    
[^73]: 基于密度的可行性学习与归一化流在自省式机器人装配中的应用

    Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly. (arXiv:2307.01317v1 [cs.RO])

    [http://arxiv.org/abs/2307.01317](http://arxiv.org/abs/2307.01317)

    本文提出了一种基于密度的可行性学习方法，使用归一化流进行自省式机器人装配。该方法只需要可行的示例来训练，可以更好地检测不可行的装配方案。

    

    机器学习模型在机器人装配序列规划中需要对预测的解决方案进行自省，即判断其可行性，以避免潜在的效率降低。之前的研究在训练时需要可行和不可行的示例。然而，当需要重新训练以适应新的产品变体时，收集足够的不可行示例是困难的。在这项工作中，我们提出了一种只需要可行示例的基于密度的可行性学习方法。具体地，我们将可行性学习问题形式化为使用归一化流进行分布区分，归一化流是用于估计复杂概率分布的强大生成模型。实验结果表明，所提出的方法在机器人装配案例中表现优于其他单分类基线模型，能够检测出不可行的装配方案。我们进一步研究了该方法的内部工作机理，并展示了一个大规模的记忆机制。

    Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP) need to be introspective on the predicted solutions, i.e. whether they are feasible or not, to circumvent potential efficiency degradation. Previous works need both feasible and infeasible examples during training. However, the infeasible ones are hard to collect sufficiently when re-training is required for swift adaptation to new product variants. In this work, we propose a density-based feasibility learning method that requires only feasible examples. Concretely, we formulate the feasibility learning problem as Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are powerful generative models for estimating complex probability distributions. Empirically, the proposed method is demonstrated on robotic assembly use cases and outperforms other single-class baselines in detecting infeasible assemblies. We further investigate the internal working mechanism of our method and show that a large memo
    
[^74]: 用神经符号深度强化学习方法实现安全自主驾驶策略的研究

    Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])

    [http://arxiv.org/abs/2307.01316](http://arxiv.org/abs/2307.01316)

    本文介绍了一种名为DRL with Symbolic Logics (DRLSL)的新颖神经符号无模型深度强化学习方法，旨在实现在真实环境中安全学习自主驾驶策略。该方法结合了深度强化学习和符号逻辑驱动的推理，允许通过与物理环境的实时交互来学习自主驾驶策略并确保安全性。

    

    自主驾驶中的动态驾驶环境和多样化道路使用者的存在给决策造成了巨大的挑战。深度强化学习(DRL)已成为解决这一问题的一种流行方法。然而，由于安全问题的限制，现有的DRL解决方案的应用主要局限于模拟环境，阻碍了它们在现实世界中的部署。为了克服这一局限，本文引入了一种新颖的神经符号无模型深度强化学习方法，称为带有符号逻辑的DRL(DRLSL)，它将DRL(从经验中学习)和符号一阶逻辑知识驱动的推理相结合，以实现在实际环境下安全学习自主驾驶的实时交互。这种创新的方法提供了一种通过积极与物理环境互动来学习自主驾驶政策并确保安全性的方式。我们使用高维度数据实现了自主驾驶的DRLSL框架。

    The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
    
[^75]: 一个求解优化学习中Chebyshev边界的数值算法

    A numerical algorithm for attaining the Chebyshev bound in optimal learning. (arXiv:2307.01304v1 [math.OC])

    [http://arxiv.org/abs/2307.01304](http://arxiv.org/abs/2307.01304)

    本文提出了一个数值算法，用于从有限数据点中解决优化学习中的Chebyshev边界问题。算法能够计算出假设空间的Chebyshev半径和Chebyshev中心，从而实现了从数据中最优恢复函数的目标。算法基于有针对性抽样的近似最优解，具有独立的兴趣，并通过示例展示了算法的有效性。

    

    给定Banach空间中的一个紧致子集，Chebyshev中心问题是找到一个最小的包围子集的球。本文在有限数据点的最优学习环境中建立了一个可计算的数值算法，用于解决Chebyshev中心问题。对于一个假设空间，它被实现为一个紧致但不一定是凸集的有限维子空间的子集，在此算法中计算了假设空间的Chebyshev半径和Chebyshev中心，从而解决了从数据中最优恢复函数的问题。该算法本身基于最近通过有针对性抽样实现的函数近似问题的近似最优解的结果，并且具有独立的兴趣。为了说明算法的有效性，本文还包括了几个数值计算Chebyshev中心的示例。

    Given a compact subset of a Banach space, the Chebyshev center problem consists of finding a minimal circumscribing ball containing the set. In this article we establish a numerically tractable algorithm for solving the Chebyshev center problem in the context of optimal learning from a finite set of data points. For a hypothesis space realized as a compact but not necessarily convex subset of a finite-dimensional subspace of some underlying Banach space, this algorithm computes the Chebyshev radius and the Chebyshev center of the hypothesis space, thereby solving the problem of optimal recovery of functions from data. The algorithm itself is based on, and significantly extends, recent results for near-optimal solutions of convex semi-infinite problems by means of targeted sampling, and it is of independent interest. Several examples of numerical computations of Chebyshev centers are included in order to illustrate the effectiveness of the algorithm.
    
[^76]: Pareto-安全的机器学习（PSML）：指纹和保护推断服务系统。

    Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])

    [http://arxiv.org/abs/2307.01292](http://arxiv.org/abs/2307.01292)

    本论文研究了模型服务系统的安全性，通过引入一个查询高效的指纹算法，使得攻击者能够一致地触发任何想要的模型，从而增强了对模型提取攻击的鲁棒性和准确性。

    

    随着大型基础模型的出现，模型服务系统越来越受欢迎。在这样的系统中，用户将查询发送到服务器，并指定所需的性能指标（例如准确性、延迟等）。服务器在后端维护一组模型（模型库），并根据指定的指标提供查询服务。本文研究了这些系统的安全性，特别是对模型提取攻击的鲁棒性。现有的黑盒攻击不能直接应用于提取受害模型，因为模型隐藏在推理服务接口背后的模型库中，攻击者无法确定使用的是哪个模型。需要一个中间步骤来确保每个输入查询都能得到受害模型的输出。为此，我们提出了一种查询高效的指纹算法，使攻击者能够一致地触发任何想要的模型。我们证明，通过使用我们的指纹算法，模型提取可以具有保真度和准确性。

    With the emergence of large foundational models, model-serving systems are becoming popular. In such a system, users send the queries to the server and specify the desired performance metrics (e.g., accuracy, latency, etc.). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks cannot be directly applied to extract a victim model, as models hide among the model zoo behind the inference serving interface, and attackers cannot identify which model is being used. An intermediate step is required to ensure that every input query gets the output from the victim model. To this end, we propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accu
    
[^77]: 用共识方式解决可解释机器学习中的分歧问题

    Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])

    [http://arxiv.org/abs/2307.01288](http://arxiv.org/abs/2307.01288)

    这项研究评估了六种共识函数用于解释五个机器学习模型，并发现了在解释模型方面存在着分歧问题，对于解决这个问题尚需进一步研究。

    

    机器学习模型的价值通常通过其预测的准确性来评估。然而，在某些科学领域中，模型的内部工作方式与其准确性同等重要。为了理解机器学习模型的内部工作原理，解释性算法是首选。然而，尽管有多种算法可供选择，它们在解释模型方面经常存在分歧，导致相互矛盾的解释结果。为了应对这个问题，在模型被解释之后可以应用共识函数。然而，问题并没有完全解决，因为最终结果将取决于选择的共识函数和其他因素。本文评估了六种共识函数用于解释五个机器学习模型。这些模型先前在四个已知内部规则的合成数据集上进行了训练。然后，使用与模型无关的局部和全局可解释性算法对模型进行了解释。最后，进行共识处理。

    Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was c
    
[^78]: 用结构化语法演化学习差分方程用于餐后血糖预测

    Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction. (arXiv:2307.01238v1 [cs.LG])

    [http://arxiv.org/abs/2307.01238](http://arxiv.org/abs/2307.01238)

    本研究提出了一种用于餐后血糖预测的新方法，通过结构化语法演化学习差分方程，并结合聚类分析提供可解释性强的预测模型。

    

    糖尿病患者必须密切监测他们的血糖水平，尤其是进餐后。血糖调节需要正确组合的食物摄入和胰岛素注射。血糖预测对于避免治疗糖尿病患者餐后并发症非常重要。虽然传统的方法，如人工神经网络，显示出很高的准确率，但由于其缺乏可解释性，有时不适用于医生开发个性化治疗。在本研究中，我们提出了一种强调可解释性的新型血糖预测方法：可解释稀疏识别通过语法演化。结合先前的聚类阶段，我们的方法提供了有限差分方程来预测餐后两小时内的血糖水平。我们将数据集分为四小时的段，根据进餐前两小时的血糖值进行聚类。预测模型被训练。

    People with diabetes must carefully monitor their blood glucose levels, especially after eating. Blood glucose regulation requires a proper combination of food intake and insulin boluses. Glucose prediction is vital to avoid dangerous post-meal complications in treating individuals with diabetes. Although traditional methods, such as artificial neural networks, have shown high accuracy rates, sometimes they are not suitable for developing personalised treatments by physicians due to their lack of interpretability. In this study, we propose a novel glucose prediction method emphasising interpretability: Interpretable Sparse Identification by Grammatical Evolution. Combined with a previous clustering stage, our approach provides finite difference equations to predict postprandial glucose levels up to two hours after meals. We divide the dataset into four-hour segments and perform clustering based on blood glucose values for the twohour window before the meal. Prediction models are traine
    
[^79]: 动态图回声状态网络与快照合并用于传播过程分类

    Dynamical Graph Echo State Networks with Snapshot Merging for Dissemination Process Classification. (arXiv:2307.01237v1 [cs.LG])

    [http://arxiv.org/abs/2307.01237](http://arxiv.org/abs/2307.01237)

    本研究提出了一种将动态图回声状态网络(DynGESN)与称为快照合并的新型数据增强策略相结合的模型，用于处理传播过程分类(DPC)任务。该模型通过快照合并和多个储层编码器提取时空特征，并采用逻辑回归进行分类。实验结果显示该方法在多个基准数据集上取得了显著的改进。

    

    传播过程分类(DPC)是一种常用的时间图分类应用。DPC的目标是对由离散时间序列图表示的社区中的信息或疫情传播模式进行分类。最近，提出了一种基于储层计算的模型称为动态图回声状态网络(DynGESN)，该模型在处理具有较高有效性和较低计算成本的时间图方面表现出色。在本研究中，我们提出了一个将称为快照合并的新型数据增强策略与DynGESN相结合用于处理DPC任务的模型。在我们的模型中，快照合并策略旨在通过合并相邻的快照来形成新的快照，然后设置多个储层编码器以从合并的快照中捕获时空特征。在此之后，采用逻辑回归将求和池化的嵌入解码为分类结果。实验结果表明，我们的模型在几个基准数据集上取得了显著的改进。

    The Dissemination Process Classification (DPC) is a popular application of temporal graph classification. The aim of DPC is to classify different spreading patterns of information or pestilence within a community represented by discrete-time temporal graphs. Recently, a reservoir computing-based model named Dynamical Graph Echo State Network (DynGESN) has been proposed for processing temporal graphs with relatively high effectiveness and low computational costs. In this study, we propose a novel model which combines a novel data augmentation strategy called snapshot merging with the DynGESN for dealing with DPC tasks. In our model, the snapshot merging strategy is designed for forming new snapshots by merging neighboring snapshots over time, and then multiple reservoir encoders are set for capturing spatiotemporal features from merged snapshots. After those, the logistic regression is adopted for decoding the sum-pooled embeddings into the classification results. Experimental results o
    
[^80]: Rockmate: 一个高效、快速、自动和通用的PyTorch重新材料化工具

    Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch. (arXiv:2307.01236v1 [cs.LG])

    [http://arxiv.org/abs/2307.01236](http://arxiv.org/abs/2307.01236)

    Rockmate是一个高效、快速、自动和通用的PyTorch重新材料化工具，通过检测计算和数据依赖关系的结构，将模型重写为复杂块的序列，以实现快速和高效的训练，并与Checkmate和Rotor的性能相当。

    

    我们提出了Rockmate来控制训练PyTorch深度学习模型时的内存需求。Rockmate是一个自动工具，从模型代码开始，使用预定义的激活内存量生成一个等效模型，以少量重新计算为代价。Rockmate自动检测计算和数据依赖关系的结构，并将初始模型重写为复杂块的序列。我们证明这样的结构很普遍，在许多文献的模型中都可以找到（基于Transformer的模型，ResNet，RegNets等）。这种结构使我们能够以快速和高效的方式解决问题，通过在单个块的级别上使用Checkmate的改编（在整个模型上速度太慢但通用），并在序列本身的级别上使用Rotor的改编（速度快但仅限于顺序模型）。我们通过对许多模型的实验证明，Rockmate与Rotor一样快，与Checkmate一样高效，并且在许多情况下可以

    We propose Rockmate to control the memory requirements when training PyTorch DNN models. Rockmate is an automatic tool that starts from the model code and generates an equivalent model, using a predefined amount of memory for activations, at the cost of a few re-computations. Rockmate automatically detects the structure of computational and data dependencies and rewrites the initial model as a sequence of complex blocks. We show that such a structure is widespread and can be found in many models in the literature (Transformer based models, ResNet, RegNets,...). This structure allows us to solve the problem in a fast and efficient way, using an adaptation of Checkmate (too slow on the whole model but general) at the level of individual blocks and an adaptation of Rotor (fast but limited to sequential models) at the level of the sequence itself. We show through experiments on many models that Rockmate is as fast as Rotor and as efficient as Checkmate, and that it allows in many cases to 
    
[^81]: 通过多任务学习实现物联网故障检测和分类

    Internet of Things Fault Detection and Classification via Multitask Learning. (arXiv:2307.01234v1 [cs.LG])

    [http://arxiv.org/abs/2307.01234](http://arxiv.org/abs/2307.01234)

    本文通过多任务学习实现了物联网故障检测和分类系统，并在真实数据上展示了优越的性能，相较于现有技术，在特异性、精确度、召回率和F1值等方面均有显著改进。

    

    本文针对实际工业物联网应用开发了一个全面的故障检测和分类系统进行了深入研究。研究解决了数据收集、标注、算法开发和部署方面的挑战。通过使用一个真实的工业物联网系统，我们对11个预定义的故障类别进行了三个阶段的数据收集模拟。我们提出了SMTCNN用于工业物联网中的故障检测和分类，评估其在真实数据上的性能。与现有技术相比，SMTCNN在特异性（3.5%）方面取得了卓越的效果，并且在精确度、召回率和F1值方面显示出显著改进。

    This paper presents a comprehensive investigation into developing a fault detection and classification system for real-world IIoT applications. The study addresses challenges in data collection, annotation, algorithm development, and deployment. Using a real-world IIoT system, three phases of data collection simulate 11 predefined fault categories. We propose SMTCNN for fault detection and category classification in IIoT, evaluating its performance on real-world data. SMTCNN achieves superior specificity (3.5%) and shows significant improvements in precision, recall, and F1 measures compared to existing techniques.
    
[^82]: RobustL2S: 利用自监督表示的个别讲话者唇语合成的鲁棒性

    RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations. (arXiv:2307.01233v1 [cs.SD])

    [http://arxiv.org/abs/2307.01233](http://arxiv.org/abs/2307.01233)

    RobustL2S是一个模块化框架，利用自监督表示将唇语转换为语音。该方法通过解耦语音内容并对其进行高效地训练，实现了在相关数据集上的最先进性能。

    

    在个别讲话者唇语合成方面取得了显著进展，旨在从无声的说话人面部视频中生成语音。目前最先进的方法主要使用非自回归的序列到序列架构，直接从唇部表示预测mel频谱图或音频波形。我们假设直接mel预测会因训练/模型效率受到语音内容与环境信息和讲话者特征的纠缠而受到限制。为此，我们提出了RobustL2S，一个用于唇语合成的模块化框架。首先，非自回归的序列到序列模型将自监督的视觉特征映射到解耦的语音内容表示。然后，一个声码器将语音特征转换为原始波形。广泛的评估证实了我们的设置的有效性，在无约束的Lip2Wav数据集和约束的GRID和TCD-TIMIT数据集上实现了最先进的性能。

    Significant progress has been made in speaker dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. Current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. We hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. To this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. First, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. A vocoder then converts the speech features into raw waveforms. Extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. Spee
    
[^83]: 无噪声数据中内镜视频的鲁棒手术工具检测

    Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data. (arXiv:2307.01232v1 [eess.IV])

    [http://arxiv.org/abs/2307.01232](http://arxiv.org/abs/2307.01232)

    本论文提出了一种使用有噪声数据进行手术工具检测的鲁棒模型的系统方法，创新之处在于智能主动学习策略和学生-教师模型组装策略。

    

    在过去几年里，外科数据科学引起了机器学习（ML）社区的广泛关注。各种研究已经证明了新兴的ML技术在分析手术数据方面的有效性，特别是手术记录，用于数字化临床和非临床功能，如术前计划、上下文感知决策和手术技能评估。然而，这个领域仍处于初级阶段，缺乏代表性和有注释的数据集，以用于训练中间ML任务中的鲁棒模型。此外，现有数据集存在标签不准确的问题，阻碍了可靠模型的开发。在本文中，我们提出了一种系统的方法，利用有噪声的数据开发用于手术工具检测的鲁棒模型。我们的方法引入了两个关键创新：(1)智能主动学习策略，用于最小数据集的识别和人工专家的标签修正；(2)用于学生-教师模型组装的策略。

    Over the past few years, surgical data science has attracted substantial interest from the machine learning (ML) community. Various studies have demonstrated the efficacy of emerging ML techniques in analysing surgical data, particularly recordings of procedures, for digitizing clinical and non-clinical functions like preoperative planning, context-aware decision-making, and operating skill assessment. However, this field is still in its infancy and lacks representative, well-annotated datasets for training robust models in intermediate ML tasks. Also, existing datasets suffer from inaccurate labels, hindering the development of reliable models. In this paper, we propose a systematic methodology for developing robust models for surgical tool detection using noisy data. Our methodology introduces two key innovations: (1) an intelligent active learning strategy for minimal dataset identification and label correction by human experts; and (2) an assembling strategy for a student-teacher m
    
[^84]: 对(深度)学习匹配算法的基准数据集的关键重新评估

    A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])

    [http://arxiv.org/abs/2307.01231](http://arxiv.org/abs/2307.01231)

    本研究重新评估了(深度)学习匹配算法的基准数据集，发现其中大多数数据集都属于相对简单的分类任务。

    

    实体解析(ER)是识别在一个或多个数据库中指向相同实体的记录的过程。多年来，已经开发了许多技术来解决ER挑战，近年来，机器学习和深度学习方法在匹配阶段受到了重视。然而，在文献中尚未对实验评估中常用的学习匹配算法的基准数据集的质量进行检查。为了弥补这个空白，我们提出了四种不同的方法来评估13个已建立数据集的难度和适用性：两种理论方法，涉及新的线性度量和现有的复杂度度量，以及两种实际方法：最佳非线性和线性匹配器之间的差异，以及最佳学习匹配器和完美预测器之间的差异。我们的分析表明，大多数流行数据集都提出了相当简单的分类任务。

    Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
    
[^85]: 大规模语言和文本到三维模型用于工程设计优化

    Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])

    [http://arxiv.org/abs/2307.01230](http://arxiv.org/abs/2307.01230)

    本文研究了深度文本到三维模型在工程设计优化中的潜力和挑战，提出并实现了一个自动化的进化设计优化框架。

    

    当前生成式人工智能在学习大规模神经网络模型方面取得的进展，具有从文本提示生成论文、图像、音乐甚至三维资产的能力，为多学科提供了机会。本文研究了深度文本到三维模型在工程领域的潜力，重点关注在计算模拟设计优化中整合和交互三维资产的机会和挑战。与传统的基于数值表示的三维几何设计优化不同，自然语言要求对变异算子有不同的解释，同时也可以减轻和激发人类用户的交互。在这里，我们提出并实现了一个完全自动的进化设计优化框架，使用了最近推出的Shap-E模型。

    The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently pu
    
[^86]: EmoGen: 消除情感音乐生成中的主观偏差

    EmoGen: Eliminating Subjective Bias in Emotional Music Generation. (arXiv:2307.01229v1 [cs.SD])

    [http://arxiv.org/abs/2307.01229](http://arxiv.org/abs/2307.01229)

    EmoGen是一种消除情感音乐生成中主观偏差的系统，通过利用与情感相关的音乐属性作为桥梁，将生成分为情感到属性的映射以及属性到音乐的生成两个阶段，并在学习过程中消除主观偏差，实现生成具有普遍情感的音乐。

    

    音乐用于传达情感，因此在自动生成音乐时生成情感音乐非常重要。之前关于情感音乐生成的工作直接使用标注的情感标签作为控制信号，但存在主观偏差：不同的人可能会在同样的音乐上标注不同的情感，同一个人在不同情境下也可能感受到不同的情感。因此，直接将情感标签映射到音乐序列中会混淆学习过程，并阻碍模型生成具有普遍情感的音乐。本文提出了EmoGen，一种情感音乐生成系统，它利用一组与情感相关的音乐属性作为情感和音乐之间的桥梁，并将生成分为两个阶段：基于监督聚类的情感到属性映射以及基于自监督学习的属性到音乐生成。这两个阶段都是有益的：在第一个阶段，聚类周围的属性值有助于消除主观偏差，第二个阶段则实现了音乐的生成。

    Music is used to convey emotions, and thus generating emotional music is important in automatic music generation. Previous work on emotional music generation directly uses annotated emotion labels as control signals, which suffers from subjective bias: different people may annotate different emotions on the same music, and one person may feel different emotions under different situations. Therefore, directly mapping emotion labels to music sequences in an end-to-end way would confuse the learning process and hinder the model from generating music with general emotions. In this paper, we propose EmoGen, an emotional music generation system that leverages a set of emotion-related music attributes as the bridge between emotion and music, and divides the generation into two stages: emotion-to-attribute mapping with supervised clustering, and attribute-to-music generation with self-supervised learning. Both stages are beneficial: in the first stage, the attribute values around the clusterin
    
[^87]: ESGCN: 边缘压缩注意图卷积网络用于交通流量预测

    ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])

    [http://arxiv.org/abs/2307.01227](http://arxiv.org/abs/2307.01227)

    ESGCN是一种用于交通流量预测的边缘压缩注意图卷积网络，通过建模时空动态和引入边缘特征和边缘注意机制来提高预测的准确性。

    

    交通流量预测是一个极具挑战性的任务，由于交通流的动态时空依赖关系。为了应对这个问题，我们着重于建模时空动态并提出了一种名为Edge Squeeze Graph Convolutional Network (ESGCN)的网络来预测多个地区的交通流量。ESGCN由两个模块组成：W模块和ES模块。W模块是一个完全以节点为基础的卷积网络。它分别对每个交通区域的时间序列进行编码，并在不同尺度上分解时间序列以捕捉细粒度和粗粒度的特征。ES模块使用图卷积网络(GCN)建模时空动态，并利用时序特征生成自适应邻接矩阵(AAM)。为了提高AAM的准确性，我们引入了三个关键概念。1）使用边缘特征直接捕捉区域之间的时空流动表示。2）将边缘注意机制应用于GCN，从边缘特征中提取AAM。

    Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
    
[^88]: vONTSS：基于vMF和最优传输的半监督神经主题建模

    vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])

    [http://arxiv.org/abs/2307.01226](http://arxiv.org/abs/2307.01226)

    vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。

    

    最近，受变分自编码器启发的神经主题模型（NTM）引起了很多研究兴趣，然而，由于整合人类知识的挑战，这些方法在实际应用中受到了限制。本研究提出了一种半监督神经主题建模方法vONTSS，该方法利用基于von Mises-Fisher（vMF）的变分自编码器和最优传输。在半监督设置中，当提供每个主题的少量关键词时，vONTSS生成潜在主题并优化主题-关键词质量和主题分类。实验证明，vONTSS在分类准确率和多样性方面优于现有的半监督主题建模方法。vONTSS还支持无监督主题建模。定量和定性实验证明，vONTSS在无监督设置下在多个方面优于最近的NTM：vONTSS在基准数据集上发现高度聚类和连贯的主题。它也比现有-手法快得多。

    Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
    
[^89]: 解释性和透明性驱动的文本对抗示例的检测与转换（IT-DT）

    Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])

    [http://arxiv.org/abs/2307.01225](http://arxiv.org/abs/2307.01225)

    通过提出的解释性和透明性驱动的检测与转换（IT-DT）框架，我们在检测和转换文本对抗示例方面注重解释性和透明性。这个框架利用了注意力图、集成梯度和模型反馈等技术，在检测阶段有助于识别对对抗性分类有贡献的显著特征和扰动词语，并在转换阶段使用预训练的嵌入和模型反馈来生成扰动词语的最佳替代，以将对抗性示例转换为正常示例。

    

    基于Transformer的文本分类器如BERT、Roberta、T5和GPT-3在自然语言处理方面展示了令人印象深刻的性能。然而，它们对于对抗性示例的脆弱性提出了安全风险。现有的防御方法缺乏解释性，很难理解对抗性分类并识别模型的漏洞。为了解决这个问题，我们提出了解释性和透明性驱动的检测与转换（IT-DT）框架。它专注于在检测和转换文本对抗示例时的解释性和透明性。IT-DT利用注意力图、集成梯度和模型反馈等技术进行解释性检测。这有助于识别对对抗性分类有贡献的显著特征和扰动词语。在转换阶段，IT-DT利用预训练的嵌入和模型反馈来生成扰动词语的最佳替代。通过找到合适的替换，我们的目标是将对抗性示例转换为正常示例。

    Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
    
[^90]: FedCP:通过条件策略对个性化联邦学习中的特征信息进行分离

    FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])

    [http://arxiv.org/abs/2307.01217](http://arxiv.org/abs/2307.01217)

    提出了一种名为FedCP的个性化联邦学习方法，通过生成条件策略分离特征中的全局信息和个性化信息，并分别进行处理。实验证明该方法在计算机视觉和自然语言处理领域的性能超过了十一种最先进的方法，最高可提高6.69%。

    

    最近，个性化联邦学习（pFL）在隐私保护、协作学习以及解决客户端之间的统计异质性等方面引起了越来越多的关注，例如医院、移动智能手机等。大多数现有的pFL方法侧重于利用客户端级模型参数中的全局信息和个性化信息，但忽略了数据是这两种信息的源头。为了解决这个问题，我们提出了联邦条件策略（FedCP）方法，该方法为每个样本生成一个条件策略，以分离其特征中的全局信息和个性化信息，然后分别通过全局头和个性化头进行处理。与现有的pFL方法相比，FedCP更加细粒度地考虑个性化的样本特定方式。在计算机视觉和自然语言处理领域进行的大量实验表明，FedCP在性能上超过了十一种最先进的方法，最高可提高6.69%。

    Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
    
[^91]: 关于尖锐的奇异值分解和音乐推荐

    Of Spiky SVDs and Music Recommendation. (arXiv:2307.01212v1 [cs.IR])

    [http://arxiv.org/abs/2307.01212](http://arxiv.org/abs/2307.01212)

    本文研究了音乐推荐中的一个有趣现象：嵌入空间中的尖峰形成。通过提出度量标准并进行数学证明，我们揭示了尖峰形成与不同内部流行度的项目社区相关。最后，我们通过一个实际用例，探讨了在添加数据的情况下音乐嵌入的相似项目如何随时间变化。

    

    截断奇异值分解是音乐推荐中广泛使用的方法，用于直接检索类似物品或为下游任务嵌入音乐项目。本文研究了在许多推荐数据集中自然发生的一种有趣效应：嵌入空间中的尖峰形成。我们首先提出了一种衡量这种尖峰组织强度的度量标准，然后在数学上证明其起源与内部流行度各异的项目社区有关。凭借这种新获得的理论理解，我们最后以一种工业用例开放了该主题，用于估计在添加数据的情况下音乐嵌入的前k个相似项目将如何随时间变化。

    The truncated singular value decomposition is a widely used methodology in music recommendation for direct similar-item retrieval or embedding musical items for downstream tasks. This paper investigates a curious effect that we show naturally occurring on many recommendation datasets: spiking formations in the embedding space. We first propose a metric to quantify this spiking organization's strength, then mathematically prove its origin tied to underlying communities of items of varying internal popularity. With this new-found theoretical understanding, we finally open the topic with an industrial use case of estimating how music embeddings' top-k similar items will change over time under the addition of data.
    
[^92]: 汉字音系的多方言表示学习

    Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])

    [http://arxiv.org/abs/2307.01209](http://arxiv.org/abs/2307.01209)

    该论文提出了一种在汉字音韵学中获取多方言表示的方法，通过构建知识图谱和应用无监督聚类技术，这些表示可以捕捉输入方言的音位对比并展示古老的原始语言特征的潜力。

    

    机器学习技术在语言和音韵等象征系统的表示和推理方面表现出了自己的能力。在汉字历史音韵学中，可以受益于机器学习的重要任务包括方言比较和原始语言系统的重建。本文提出了一种获取汉字音节的多方言表示的方法，通过从结构化音韵数据构建知识图谱，然后应用知识库学习中的BoxE技术。我们应用无监督的聚类技术对所得到的表示进行观察，发现这些表示捕捉到输入方言的音位对比。此外，我们训练了分类器来进行无法观察的中古汉语标签的推理，展示了这些表示揭示古老的原始语言特征的潜力。这些表示可以用于对碎片化的汉字音韵进行补全。

    Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data, then applying the BoxE technique from knowledge base learning. We applied unsupervised clustering techniques to the obtained representations to observe that the representations capture phonemic contrast from the input dialects. Furthermore, we trained classifiers to perform inference of unobserved Middle Chinese labels, showing the representations' potential for indicating archaic, proto-language features. The representations can be used for performing completion of fragmented Sinitic phonological 
    
[^93]: 在线和移动社交网络的推荐系统：一项调研

    Recommender Systems for Online and Mobile Social Networks: A survey. (arXiv:2307.01207v1 [cs.IR])

    [http://arxiv.org/abs/2307.01207](http://arxiv.org/abs/2307.01207)

    本文调研了针对在线和移动社交网络设计和实施的推荐系统，强调了社交上下文信息如何改善推荐任务，并描述了这些系统在完全分布环境下的优点和缺点。

    

    推荐系统（RS）目前在在线服务中扮演着基础工具的角色，尤其是随着在线社交网络（OSN）的出现。在这种情况下，用户会产生大量的内容，很容易被无用信息淹没。同时，社交媒体也是表征内容和用户兴趣的重要信息源。RS可以利用这些信息来进一步个性化推荐并改进推荐过程。本文介绍了为在线和移动社交网络设计和实施的推荐系统的调研，重点描述了社交上下文信息如何改善推荐任务，以及标准算法在完全分布环境（如机会网络）中如何进行改进和优化。我们通过算法、目标领域、评估指标和性能评估来描述这些系统的优点和缺点。最后，我们提出了一些开放问题。

    Recommender Systems (RS) currently represent a fundamental tool in online services, especially with the advent of Online Social Networks (OSN). In this case, users generate huge amounts of contents and they can be quickly overloaded by useless information. At the same time, social media represent an important source of information to characterize contents and users' interests. RS can exploit this information to further personalize suggestions and improve the recommendation process. In this paper we present a survey of Recommender Systems designed and implemented for Online and Mobile Social Networks, highlighting how the use of social context information improves the recommendation task, and how standard algorithms must be enhanced and optimized to run in a fully distributed environment, as opportunistic networks. We describe advantages and drawbacks of these systems in terms of algorithms, target domains, evaluation metrics and performance evaluations. Eventually, we present some open
    
[^94]: CTR预测的置信度排序

    Confidence Ranking for CTR Prediction. (arXiv:2307.01206v1 [cs.IR])

    [http://arxiv.org/abs/2307.01206](http://arxiv.org/abs/2307.01206)

    本文提出了一种名为“置信度排序”的新型框架，在CTR预测任务中引入了置信度排序损失，可以在公共和工业数据集上胜过所有基线方法。

    

    在大规模现实世界的机器学习应用中，模型演进和数据的持续可用性是两个常见现象，例如广告和推荐系统。为了适应这种情况，实际系统通常会使用所有可用数据进行重新训练，并使用最近可用的数据进行在线学习，以期定期更新模型以提高性能。本文提出了一种名为“置信度排序”的新型框架，将优化目标设计为具有两个不同模型的排序函数。我们的置信度排序损失允许直接优化不同凸替代函数（例如AUC和准确度）的logits输出，具体取决于目标任务和数据集。通过我们提出的方法，在公共和工业数据集的CTR预测任务中，我们的实验表明引入置信度排序损失可以胜过所有基线方法。该框架已在京东广告系统中部署，以提供服务。

    Model evolution and constant availability of data are two common phenomena in large-scale real-world machine learning applications, e.g. ads and recommendation systems. To adapt, the real-world system typically retrain with all available data and online learn with recently available data to update the models periodically with the goal of better serving performance. In this paper, we propose a novel framework, named Confidence Ranking, which designs the optimization objective as a ranking function with two different models. Our confidence ranking loss allows direct optimization of the logits output for different convex surrogate functions of metrics, e.g. AUC and Accuracy depending on the target task and dataset. Armed with our proposed methods, our experiments show that the introduction of confidence ranking loss can outperform all baselines on the CTR prediction tasks of public and industrial datasets. This framework has been deployed in the advertisement system of JD.com to serve the
    
[^95]: 面向知识图谱的少样本归纳链接预测：一种基于关系匿名漫步引导的神经过程方法

    Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach. (arXiv:2307.01204v1 [cs.AI])

    [http://arxiv.org/abs/2307.01204](http://arxiv.org/abs/2307.01204)

    本文提出了一种基于关系匿名漫步引导的神经过程方法，用于知识图谱上的少样本归纳链接预测。该方法利用未见实体周围的子图来获取语义并归纳地预测链接，在捕捉一般的归纳模式的同时，还能快速适应新的实体并估计预测的不确定性。

    

    知识图谱上的少样本归纳链接预测旨在用少样本的链接来预测未见实体的缺失链接。以往的方法仅限于传导场景，即在知识图谱中存在实体，因此无法处理未见实体。因此，近期的归纳方法利用未见实体周围的子图来获取语义并归纳地预测链接。然而，在少样本场景下，子图通常是稀疏的，无法提供有意义的归纳模式。本文提出了一种新颖的基于关系匿名漫步引导的神经过程，用于知识图谱上的少样本归纳链接预测，称为RawNP。具体而言，我们开发了一种基于神经过程的方法，对链接预测函数建模为灵活的分布。这使得模型能够快速适应新的实体并在进行预测时估计不确定性。为了捕捉一般的归纳模式，我们提出了一种关系匿名漫步方法。

    Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict missing links for unseen entities with few-shot links observed. Previous methods are limited to transductive scenarios, where entities exist in the knowledge graphs, so they are unable to handle unseen entities. Therefore, recent inductive methods utilize the sub-graphs around unseen entities to obtain the semantics and predict links inductively. However, in the few-shot setting, the sub-graphs are often sparse and cannot provide meaningful inductive patterns. In this paper, we propose a novel relational anonymous walk-guided neural process for few-shot inductive link prediction on knowledge graphs, denoted as RawNP. Specifically, we develop a neural process-based method to model a flexible distribution over link prediction functions. This enables the model to quickly adapt to new entities and estimate the uncertainty when making predictions. To capture general inductive patterns, we present a relational anony
    
[^96]: 预测性专利学：使用ChatGPT技术预测创新成功和估值

    Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])

    [http://arxiv.org/abs/2307.01202](http://arxiv.org/abs/2307.01202)

    本研究采用ChatGPT技术，以OpenAI的最先进的文本嵌入为基础，通过深度学习预测模型实现了对专利创新成功和估值的准确预测，为专利估值提供了革命性的改进。此外，通过预测接受率构建的多空投资组合实现了显著的异常收益率。

    

    传统方法对于创新的分析在广泛的结构变量方面存在根本性的局限性。本文通过开创性的ChatGPT技术采用LLM方法对专利进行分析，突破了边界。OpenAI的最先进的文本嵌入能够访问关于每个发明的质量和影响的复杂信息，用于驱动深度学习预测模型。细致的嵌入使预测专利价值的R-squared提高了24％，并明确地将最差和最佳应用程序分离开来。这些模型通过预测接受率构建的多空投资组合每年实现显著的异常收益率高达3.3%，同时也证明了市场无法及时整合有关应用程序的信息。这些模型为革命性改变科根、帕帕尼科洛、塞鲁和斯托夫曼（2017）对专利估值的更正提供了机会。

    Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutioniz
    
[^97]: 通过主动遗忘在预训练中提高语言可塑性

    Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.01163](http://arxiv.org/abs/2307.01163)

    本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。

    

    预训练语言模型(PLMs)是自然语言处理中的主要模型。尽管它们在下游任务的性能令人印象深刻，但将PLMs应用于新语言可能很困难，这是使它们的能力普遍可访问的壁垒。先前的研究表明，通过为新语言学习新的嵌入层可以解决此问题，但这样做既浪费数据又浪费计算资源。我们建议在预训练期间使用主动遗忘机制，作为快速适应新语言的PLMs的简单方法。具体而言，通过在预训练期间的每K次更新时重置嵌入层，我们鼓励PLM在有限次更新内提高学习新嵌入的能力，类似于元学习的效果。使用RoBERTa进行的实验证明，使用我们的遗忘机制预训练的模型不仅在语言适应过程中显示出更快的收敛速度，而且在低数据的情况下也优于标准模型。

    Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
    
[^98]: AVSegFormer: 基于Transformer的音视频分割

    AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01146](http://arxiv.org/abs/2307.01146)

    AVSegFormer是一种基于Transformer的音视频分割框架，通过引入音频查询和可学习查询来选择性地关注视觉特征，还使用音频-视觉混合器动态调整视觉特征，并通过中间掩模损失增强解码器的监督。实验证明该方法的有效性。

    

    音频与视觉的结合长期以来一直是多模态领域的一个研究课题。最近，引入了一项新的音频-视觉分割（AVS）任务，旨在定位和分割给定视频中的有声对象。这个任务首次要求在像素级别对音频驱动的场景进行理解，存在着重大挑战。在本文中，我们提出了AVSegFormer，这是一种利用Transformer架构进行AVS任务的新框架。具体来说，我们在变压器解码器中引入了音频查询和可学习查询，使网络能够有选择地关注感兴趣的视觉特征。此外，我们还设计了一个音频-视觉混合器，通过增强相关的空间通道和抑制无关的空间通道来动态调整视觉特征。此外，我们设计了一个中间掩模损失，以增强解码器的监督，鼓励网络产生更准确的中间预测。大量实验证明了我们方法的有效性。

    The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha
    
[^99]: 在分布转移和长尾数据下，对现代视觉架构进行合规预测的经验证实

    Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data. (arXiv:2307.01088v1 [cs.LG])

    [http://arxiv.org/abs/2307.01088](http://arxiv.org/abs/2307.01088)

    本文在大规模数据集和模型上首次对分布转移和长尾类别分布下的合规预测方法进行了实证评估。研究发现，这些方法在分布转移和长尾设置下的性能大大下降，对于在现实世界和安全关键应用中的部署具有重要的局限性。

    

    合规预测已经成为一种可靠地为深度学习模型提供不确定性估计和安全保证的方法。然而，它的性能已知在分布转移和长尾类别分布下会下降，而这在现实世界的应用中经常存在。在本文中，我们对这些情况下的几种事后和基于训练的合规预测方法进行了性能表征，并首次在大规模数据集和模型上进行了实证评估。我们发现在许多合规方法和神经网络家族中，性能在分布转移下违反安全保证时大大下降。同样，在长尾设置中，我们发现许多类别的保证经常被违反。了解这些方法的局限性对于在现实世界和安全关键应用中部署是必要的。

    Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.
    
[^100]: CardiGraphormer: 揭示自监督学习在颠覆药物发现中的力量

    CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])

    [http://arxiv.org/abs/2307.00859](http://arxiv.org/abs/2307.00859)

    CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。

    

    在广阔的药物发现领域中，已知药物约有15,000种，但只有大约4,200种得到了批准，化学空间的组合性质提供了一项艰巨的挑战。尽管人工智能成为了有力的伙伴，传统的人工智能框架仍面临重大障碍。本文介绍了CardiGraphormer，这是一种划时代的方法，通过结合自监督学习（SSL）、图神经网络（GNN）和保持基数注意力，从而颠覆药物发现。CardiGraphormer是Graphormer和保持基数注意力的新颖组合，利用SSL学习有效的分子表示，并利用GNN提取分子指纹，提高了预测性能和可解释性，并减少了计算时间。它在处理分子结构等复杂数据方面表现出色，并能执行与节点、节点对、子图或整个图结构相关的任务。

    In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
    
[^101]: SDC-HSDD-NDSA: 使用层次次级导向差异和归一化密度自适应的结构检测聚类算法

    SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00677](http://arxiv.org/abs/2307.00677)

    本文提出了一种基于密度的聚类算法，能够检测到高密度区域中的结构，具有先前算法所不具备的能力。

    

    基于密度的聚类算法是最受欢迎的聚类算法之一，因为它能够识别任意形状的聚类，只要不同的高密度聚类之间有低密度区域分隔。然而，通过低密度区域将聚类分隔开的要求并不是微不足道的，因为高密度区域可能具有不同的结构，应该被聚类到不同的组中。这种情况说明了我们已知的所有先前基于密度的聚类算法的主要缺陷--无法检测高密度聚类中的结构。因此，本文旨在提供一种基于密度的聚类方案，既具有先前方法的能力，又能够检测到高密度区域中未被低密度区分开的结构。该算法采用层次次级导向差异、层次化、归一化密度以及自适应系数，因此被称为结构检测聚类算法。

    Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
    
[^102]: 使用预测编码和不确定性最小化的主动感知

    Active Sensing with Predictive Coding and Uncertainty Minimization. (arXiv:2307.00668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00668](http://arxiv.org/abs/2307.00668)

    本论文提出了一种基于预测编码和不确定性最小化的主动感知方法，能够在各种任务中实现有效的探索和学习。

    

    我们提出了一种基于预测编码和不确定性最小化的全流程的主动探索方法。该方法可以在任何任务无关和内在驱动的情境下应用于探索。我们首先在迷宫导航任务中展示了我们的方法，并展示了我们的模型能够发现基础的转换分布并重建环境的空间特征。其次，我们将我们的模型应用于更复杂的主动视觉任务中，其中一个代理必须主动采样其视觉环境以获取信息。我们展示了我们的模型能够构建无监督表示，使其能够主动采样和高效分类感知场景。我们进一步展示，使用这些表示作为下游分类的输入相比其他基线方法具有更高的数据效率和学习速度，同时保持更低的参数复杂度。

    We present an end-to-end procedure for embodied exploration based on two biologically inspired computations: predictive coding and uncertainty minimization. The procedure can be applied to any exploration setting in a task-independent and intrinsically driven manner. We first demonstrate our approach in a maze navigation task and show that our model is capable of discovering the underlying transition distribution and reconstructing the spatial features of the environment. Second, we apply our model to the more complex task of active vision, where an agent must actively sample its visual environment to gather information. We show that our model is able to build unsupervised representations that allow it to actively sample and efficiently categorize sensory scenes. We further show that using these representations as input for downstream classification leads to superior data efficiency and learning speed compared to other baselines, while also maintaining lower parameter complexity. Final
    
[^103]: 基于稀疏感知的深度神经网络泛化理论

    Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00426](http://arxiv.org/abs/2307.00426)

    本文研究了深度神经网络的泛化能力，提出了一种基于稀疏感知的分析方法。通过考虑隐藏层激活的稀疏程度，我们展示了稀疏和泛化之间的权衡，而且结果对模型的稀疏程度没有强烈的假设，并在特定情况下的数值实验中得到了非空的界限。

    

    深度人工神经网络取得了令人惊讶的泛化能力，但其具体机制尚不清楚。本文提出了一种新的方法来分析前向深度ReLU网络的泛化能力，利用隐藏层激活的稀疏程度。通过构建一个考虑每个输入样本减小有效模型大小的框架，我们能够展示出稀疏和泛化之间的根本权衡。重要的是，我们的结果对模型实现的稀疏程度没有强烈的假设，并且改进了最近的基于范数的方法。我们通过数值实例展示了结果，即使在过参数化的模型中，在特定情况下与数据相关的先验结合时也能得到非空界限。

    Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.
    
[^104]: 反事实协同推理

    Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR])

    [http://arxiv.org/abs/2307.00165](http://arxiv.org/abs/2307.00165)

    本文提出了反事实协同推理（CCR）方法，通过整合反事实推理和逻辑推理来提高机器学习模型的准确性和可解释性。通过利用反事实推理生成困难的反事实训练样本进行数据增强，CCR在推荐系统中展示了如何缓解数据稀缺、提高准确性和增强透明度。

    

    因果推理和逻辑推理是人类智能的两种重要推理能力。然而，在机器智能背景下，它们的关系还未得到广泛探索。本文探讨了如何共同建模这两种推理能力，以提高机器学习模型的准确性和可解释性。具体而言，通过整合反事实推理和（神经）逻辑推理两种重要的推理能力，我们提出了反事实协同推理（CCR），它通过进行反事实逻辑推理来改进性能。特别是，我们以推荐系统为例，展示了CCR如何缓解数据稀缺、提高准确性和增强透明度。从技术上讲，我们利用反事实推理来生成“困难”的反事实训练样本进行数据增强，这与原始的训练样本一起可以提升模型性能。

    Causal reasoning and logical reasoning are two important types of reasoning abilities for human intelligence. However, their relationship has not been extensively explored under machine intelligence context. In this paper, we explore how the two reasoning abilities can be jointly modeled to enhance both accuracy and explainability of machine learning models. More specifically, by integrating two important types of reasoning ability -- counterfactual reasoning and (neural) logical reasoning -- we propose Counterfactual Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to improve the performance. In particular, we use recommender system as an example to show how CCR alleviate data scarcity, improve accuracy and enhance transparency. Technically, we leverage counterfactual reasoning to generate "difficult" counterfactual training examples for data augmentation, which -together with the original training examples -- can enhance the model performance. Since the 
    
[^105]: 距离函数在流场景下的规范化研究

    Distance Functions and Normalization Under Stream Scenarios. (arXiv:2307.00106v1 [cs.LG])

    [http://arxiv.org/abs/2307.00106](http://arxiv.org/abs/2307.00106)

    论文研究了在流场景中的数据规范化问题，比较了八种距离函数的准确度，并发现在没有预先了解数据流信息的情况下，使用原始数据流和Canberra距离的组合效果较好。

    

    数据规范化是建模分类系统时的重要任务。在处理数据流时，由于可能无法预先了解特征的属性（如最小/最大值），且这些属性可能会随时间变化，因此数据规范化变得尤为具有挑战性。我们比较了八种著名的距离函数在没有规范化的数据流中生成的准确度，以及考虑到接收到的第一批数据的统计信息和考虑到上一批数据的规范化。我们认为将全流程视为规范化的实验协议是不现实的，并会导致偏倚和糟糕的结果。我们的结果表明，当事先不了解数据流的信息时，使用原始数据流而不应用规范化，以及使用Canberra距离，可能是一个好的组合。

    Data normalization is an essential task when modeling a classification system. When dealing with data streams, data normalization becomes especially challenging since we may not know in advance the properties of the features, such as their minimum/maximum values, and these properties may change over time. We compare the accuracies generated by eight well-known distance functions in data streams without normalization, normalized considering the statistics of the first batch of data received, and considering the previous batch received. We argue that experimental protocols for streams that consider the full stream as normalized are unrealistic and can lead to biased and poor results. Our results indicate that using the original data stream without applying normalization, and the Canberra distance, can be a good combination when no information about the data stream is known beforehand.
    
[^106]: milliFlow：用于人体运动感知的毫米波雷达点云场景流估计

    milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])

    [http://arxiv.org/abs/2306.17010](http://arxiv.org/abs/2306.17010)

    milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。

    

    随着普适计算时代的到来，人体运动感知在智能系统中起着关键作用，用于决策、用户交互和个性化服务。在传统方法中，人体跟踪、姿势估计、手势识别和活动识别等方面进行了大量研究，这些方法主要基于摄像机。然而，摄像机的侵入性特点限制了它们在智能家居应用中的使用。为了解决这个问题，毫米波雷达由于其保护隐私的特点而受到欢迎。在这项工作中，我们提出了一种新颖的深度学习方法milliFlow，用于对毫米波雷达点云进行场景流估计，作为中间层的特征，直接受益于下游的人体运动感知任务。实验结果表明，我们的方法具有优越的性能，平均3D端点误差为4.6cm，明显超过竞争方法。此外，通过结合...

    Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
    
[^107]: SRL: 将分布式强化学习扩展到一万多个核心

    SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])

    [http://arxiv.org/abs/2306.16688](http://arxiv.org/abs/2306.16688)

    SRL是一个可扩展，高效，可扩展的分布式强化学习系统，通过一种新的抽象框架统一了各种实际强化学习训练，并实现了精细优化。

    

    强化学习（RL）任务的不断复杂化要求分布式RL系统可以高效地生成和处理大量数据以训练智能Agent。然而，现有的开源库存在各种限制，阻碍了它们在需要大规模训练的挑战性场景中的实际应用。虽然OpenAI和DeepMind的工业系统已经成功实现了大规模RL训练，但是它们的系统架构和实现细节对社区来说仍然不公开。在本文中，我们提出了RL训练数据流的新抽象，将各种应用中的实际RL训练统一成一个通用框架，并实现了精细优化。根据这个抽象，我们开发了一个可扩展、高效、可扩展的分布式RL系统，名为"ReaLly Scalable RL（SRL）"。

    The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
    
[^108]: 连续时间q-learning用于McKean-Vlasov控制问题

    Continuous-Time q-learning for McKean-Vlasov Control Problems. (arXiv:2306.16208v1 [cs.LG])

    [http://arxiv.org/abs/2306.16208](http://arxiv.org/abs/2306.16208)

    本文研究了连续时间q-learning在熵正则化强化学习框架下用于McKean-Vlasov控制问题，并揭示了两种不同的q函数的存在及其积分表示。

    

    本文研究了q-learning，在熵正则化强化学习框架下，用于连续时间的McKean-Vlasov控制问题。与Jia和Zhou（2022c）的单个代理控制问题不同，代理之间的均场相互作用使得q函数的定义更加复杂，我们揭示了自然产生两种不同q函数的情况：（i）被称为集成q函数（用$q$表示），作为Gu、Guo、Wei和Xu（2023）引入的集成Q函数的一阶近似，可以通过涉及测试策略的弱鞅条件进行学习；（ii）作为策略改进迭代中所使用的实质q函数（用$q_e$表示）。我们证明了这两个q函数在所有测试策略下通过积分表示相关联。基于集成q函数的弱鞅条件和我们提出的搜索方法，我们设计了算法来学习两个q函数以解决Mckean-Vlasov控制问题。

    This paper studies the q-learning, recently coined as the continuous-time counterpart of Q-learning by Jia and Zhou (2022c), for continuous time Mckean-Vlasov control problems in the setting of entropy-regularized reinforcement learning. In contrast to the single agent's control problem in Jia and Zhou (2022c), the mean-field interaction of agents render the definition of q-function more subtle, for which we reveal that two distinct q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as the first-order approximation of the integrated Q-function introduced in Gu, Guo, Wei and Xu (2023) that can be learnt by a weak martingale condition involving test policies; and (ii) the essential q-function (denoted by $q_e$) that is employed in the policy improvement iterations. We show that two q-functions are related via an integral representation under all test policies. Based on the weak martingale condition of the integrated q-function and our proposed searching method of
    
[^109]: 用分子动力学模拟和马尔可夫状态建模揭示Ti-Al系统的界面动力学

    Elucidating Interfacial Dynamics of Ti-Al Systems Using Molecular Dynamics Simulation and Markov State Modeling. (arXiv:2306.14568v2 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2306.14568](http://arxiv.org/abs/2306.14568)

    本研究通过分子动力学模拟和马尔可夫状态建模方法揭示了Ti-Al系统中的界面动力学，特别关注了TiAl3晶界在实验热处理条件下Ti和Al原子的行为。分子动力学模拟揭示了热处理早期阶段Al原子通过TiAl3晶界朝Ti表面扩散是主要过程。马尔可夫状态建模则可以定位并确认关键步骤。

    

    由于其优异的机械和化学性能，Ti-Al基材料在汽车、航空航天和国防等众多工程领域引起了极大的兴趣。由于其低密度、高强度以及抗腐蚀和抗氧化性，这些金属间化合物合金和复合金属-金属复合材料已经找到了各种应用。本研究深入研究了Ti-Al系统的界面动力学，特别关注于TiAl3晶界在实验热处理条件下Ti和Al原子的行为。通过分子动力学和马尔可夫状态模型分析的组合，我们对形成TiAl3所涉及的动力学过程进行了详细研究。分子动力学模拟表明，在热处理的早期阶段，Al原子通过TiAl3晶界朝Ti表面扩散是主要过程。马尔可夫状态建模则可以定位并确认过程中的关键步骤。

    Due to their remarkable mechanical and chemical properties, Ti-Al based materials are attracting considerable interest in numerous fields of engineering, such as automotive, aerospace, and defense. With their low density, high strength, and resistance to corrosion and oxidation, these intermetallic alloys and compound metal-metallic composites have found diverse applications. The present study delves into the interfacial dynamics of these Ti-Al systems, particularly focusing on the behavior of Ti and Al atoms in the presence of TiAl$_3$ grain boundaries under experimental heat treatment conditions. Using a combination of Molecular Dynamics and Markov State Model analyses, we scrutinize the kinetic processes involved in the formation of TiAl$_3$. The Molecular Dynamics simulation indicates that at the early stage of heat treatment, the predominating process is the diffusion of Al atoms towards the Ti surface through the TiAl$_3$ grain boundaries. The Markov State Modeling identifies thr
    
[^110]: 基于深度学习的彩色多普勒心脏超声相位展开技术研究

    Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v1 [eess.IV])

    [http://arxiv.org/abs/2306.13695](http://arxiv.org/abs/2306.13695)

    本文提出了一种基于展开型原始-对偶网络的新方法，用于纠正彩色多普勒心脏超声图像中的相位包裹伪影，与其他最新分割技术相比，该方法在性能上表现突出。

    

    彩色多普勒心脏超声是一种广泛使用的非侵入性成像技术，可以提供关于心脏血流的实时信息。在左心室长轴视图中，彩色多普勒容易出现相位包裹现象，特别是在心脏收缩和舒张期。当基于彩色多普勒的定量方法时，必须纠正这种包裹伪影。我们开发了一个基于展开型原始-对偶网络的方法来解包(去伪影)彩色多普勒心脏超声图像，将其有效性与基于nnU-Net和Transformer模型的两种最新分割方法进行了比较。我们在自有数据集上对每种方法进行了训练和评估，并发现nnU-Net方法提供了最佳的去伪影结果，其次是展开型原始-对偶方法和基于Transformer的技术。值得注意的是，展开型原始-对偶网络拥有显著更少的可训练参数，但性能仍能与其他方法相媲美。

    Color Doppler echocardiography is a widely used non-invasive imaging modality that provides real-time information about the intracardiac blood flow. In an apical long-axis view of the left ventricle, color Doppler is subject to phase wrapping, or aliasing, especially during cardiac filling and ejection. When setting up quantitative methods based on color Doppler, it is necessary to correct this wrapping artifact. We developed an unfolded primal-dual network to unwrap (dealias) color Doppler echocardiographic images and compared its effectiveness against two state-of-the-art segmentation approaches based on nnU-Net and transformer models. We trained and evaluated the performance of each method on an in-house dataset and found that the nnU-Net-based method provided the best dealiased results, followed by the primal-dual approach and the transformer-based technique. Noteworthy, the primal-dual network, which had significantly fewer trainable parameters, performed competitively with respec
    
[^111]: 推荐系统的最新发展：综述

    Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR])

    [http://arxiv.org/abs/2306.12680](http://arxiv.org/abs/2306.12680)

    本篇综述全面总结了推荐系统领域的最新进展和趋势，包括推荐系统分类，知识推荐系统，鲁棒性，数据偏见和公平性问题，以及评估度量。该研究还提供了未来研究的新方向。

    

    这篇技术综述全面总结了推荐系统领域的最新进展。本研究的目的是提供领域内现状的概述，并强调推荐系统发展的最新趋势。该研究首先全面总结了主要推荐系统分类方法，包括个性化和群组推荐系统，然后深入探讨了基于知识的推荐系统类别。此外，该综述分析了推荐系统中的鲁棒性、数据偏见和公平性问题，并总结了评估度量用于评估这些系统的性能。最后，研究提供了有关推荐系统发展的最新趋势的见解，并强调了未来研究的新方向。

    In this technical survey, we comprehensively summarize the latest advancements in the field of recommender systems. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. The study starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems, and then delves into the category of knowledge-based recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, the study provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.
    
[^112]: 对最坏情况下游任务适应性的任务鲁棒预训练

    Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])

    [http://arxiv.org/abs/2306.12070](http://arxiv.org/abs/2306.12070)

    本文提出了一种任务鲁棒的预训练方法，将上游任务分成几个代表性任务并应用极小极大损失进行预训练，以保证模型能够在下游任务中具有均匀良好的性能。

    

    预训练在转移到下游任务时取得了显着的成功。在机器学习中，我们关心模型不仅具有良好的性能，而且在合理的条件变化下的行为。当预训练基础模型时，同样的哲学也适用。然而，基础模型可能并不会在一系列相关下游任务中均匀地表现良好。本文考虑预训练一个模型，保证其在下游任务中具有均匀良好的性能，我们称此目标为下游任务鲁棒性。我们的方法首先将上游任务分成几个代表性任务，并应用简单的minimax loss 进行预训练，然后设计了一个高效的算法来解决极小极大问题，并表明我们的方法优于先前的基线。

    Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim
    
[^113]: 用于放射肿瘤治疗的Segment Anything Model (SAM)

    Segment Anything Model (SAM) for Radiation Oncology. (arXiv:2306.11730v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2306.11730](http://arxiv.org/abs/2306.11730)

    Segment Anything Model (SAM) 在临床放射治疗中的性能得到评估，并展示了在自动分割中的稳健泛化能力。它可以实现针对不同部位的不同器官的精确描绘。

    

    在这项研究中，我们评估了Segment Anything Model (SAM) 在临床放射治疗中的性能。我们的结果表明，SAM的“segment anything”模式可以在大多数风险器官（OARs）上实现临床可接受的分割结果，Dice分数高于0.7。SAM的“box prompt”模式进一步提高了Dice分数0.1至0.5个单位。考虑到器官的大小和边界的清晰度，SAM在大小适中且边界清晰的器官上表现更好，但在边界不清晰的小器官上表现较差。由于SAM是一种在自然图像上进行预训练的模型，可以以临床可接受的精度处理从医学图像中的OARs的描绘，这些结果突出了SAM在放射治疗的自动分割中具有一致准确性的稳健泛化能力。换句话说，SAM可以通过通用的自动分割模型实现对不同部位的不同OARs的描绘。

    In this study, we evaluate the performance of the Segment Anything Model (SAM) in clinical radiotherapy. Our results indicate that SAM's 'segment anything' mode can achieve clinically acceptable segmentation results in most organs-at-risk (OARs) with Dice scores higher than 0.7. SAM's 'box prompt' mode further improves the Dice scores by 0.1 to 0.5. Considering the size of the organ and the clarity of its boundary, SAM displays better performance for large organs with clear boundaries but performs worse for smaller organs with unclear boundaries. Given that SAM, a model pre-trained purely on natural images, can handle the delineation of OARs from medical images with clinically acceptable accuracy, these results highlight SAM's robust generalization capabilities with consistent accuracy in automatic segmentation for radiotherapy. In other words, SAM can achieve delineation of different OARs at different sites using a generic automatic segmentation model. SAM's generalization capabilitie
    
[^114]: 基于马尔科夫链的常步长SGD的收敛和集中性质

    Convergence and concentration properties of constant step-size SGD through Markov chains. (arXiv:2306.11497v1 [stat.ML])

    [http://arxiv.org/abs/2306.11497](http://arxiv.org/abs/2306.11497)

    本文通过马尔科夫链研究了常步长随机梯度下降的性质，证明了迭代收敛于一个不变分布，并获得了高置信度边界。

    

    本文考虑使用常步长随机梯度下降（SGD）优化平滑且强凸的目标，并通过马尔科夫链研究其性质。我们证明，对于具有轻微受控方差的无偏梯度估计，迭代以总变差距离收敛于一个不变分布。我们还在与以前工作相比梯度噪声分布的放宽假设下，在Wasserstein-2距离下建立了这种收敛性。由于极限分布的不变性质，我们的分析表明，当这些对于梯度成立时，后者继承了亚高斯或亚指数浓度特性。这允许推导出对于最终估计的高置信度边界。最后，在这种条件下，在线性情况下，对于Polyak-Ruppert序列的尾部，我们获得了一个无维度偏差限制。所有结果均为非渐近性质，并讨论了其后果。

    We consider the optimization of a smooth and strongly convex objective using constant step-size stochastic gradient descent (SGD) and study its properties through the prism of Markov chains. We show that, for unbiased gradient estimates with mildly controlled variance, the iteration converges to an invariant distribution in total variation distance. We also establish this convergence in Wasserstein-2 distance under a relaxed assumption on the gradient noise distribution compared to previous work. Thanks to the invariance property of the limit distribution, our analysis shows that the latter inherits sub-Gaussian or sub-exponential concentration properties when these hold true for the gradient. This allows the derivation of high-confidence bounds for the final estimate. Finally, under such conditions in the linear case, we obtain a dimension-free deviation bound for the Polyak-Ruppert average of a tail sequence. All our results are non-asymptotic and their consequences are discussed thr
    
[^115]: 在部分可观测性下学习对抗代理行为模型

    Learning Models of Adversarial Agent Behavior under Partial Observability. (arXiv:2306.11168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11168](http://arxiv.org/abs/2306.11168)

    本论文提出了一种名为GrAMMI的方法，用于在部分可观测性下建模对抗对手代理的行为。通过最大化互信息作为辅助目标来预测对抗对手的当前和未来状态，GrAMMI在两个大规模追逐-逃避领域中表现出了显著优越性能。

    

    在许多现实场景中，比如职业体育、视频游戏设计和毒品截获中，对手建模和跟踪的需求越来越多。本文提出了一种基于图的对抗建模方法，名为GrAMMI，用于建模对抗对手代理的行为。GrAMMI是一种新颖的基于图神经网络(GNN)的方法，使用互信息最大化作为辅助目标来预测部分可观测的对抗对手的当前和未来状态。为了评估GrAMMI，我们设计了两个大规模的追逐-逃避领域，灵感来自于真实世界的场景，在这些领域中，一个异构代理团队的任务是追踪和截获一个对抗对手代理，而对抗对手代理必须在同时达到自己目标的情况下逃避被发现。通过互信息的形式化，GrAMMI在两个领域中均优于所有基线，并实现了未来对抗的平均对数似然值提高了31.68%。

    The need for opponent modeling and tracking arises in several real-world scenarios, such as professional sports, video game design, and drug-trafficking interdiction. In this work, we present Graph based Adversarial Modeling with Mutal Information (GrAMMI) for modeling the behavior of an adversarial opponent agent. GrAMMI is a novel graph neural network (GNN) based approach that uses mutual information maximization as an auxiliary objective to predict the current and future states of an adversarial opponent with partial observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion domains inspired by real-world scenarios, where a team of heterogeneous agents is tasked with tracking and interdicting a single adversarial agent, and the adversarial agent must evade detection while achieving its own objectives. With the mutual information formulation, GrAMMI outperforms all baselines in both domains and achieves 31.68% higher log-likelihood on average for future adversarial
    
[^116]: 循环记忆决策变压器

    Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])

    [http://arxiv.org/abs/2306.09459](http://arxiv.org/abs/2306.09459)

    本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。

    

    变革性模型最初是为自然语言问题而开发的，最近在离线强化学习任务中得到广泛应用。这是因为代理的历史可以表示为序列，并且整个任务可以缩减为序列建模任务。然而，变压器操作的二次复杂性限制了上下文的潜在增加。因此，为了在自然语言中处理长序列，使用了不同版本的记忆机制。在本文中，我们提出了循环记忆决策变压器（RMDT），这是一种在强化学习问题中使用循环记忆机制的模型。我们在Atari游戏和MoJoCo控制问题上进行了彻底的实验，并表明我们提出的模型在Atari游戏上显着优于没有循环记忆机制的对应模型。我们还仔细研究了记忆对所提出的模型绩效的影响。这些发现为开发更高效和更有效的处理长序列的强化学习模型提供了启示。

    Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
    
[^117]: 非线性个性化预测的神经混合效应

    Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])

    [http://arxiv.org/abs/2306.08149](http://arxiv.org/abs/2306.08149)

    本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。

    

    个性化预测是一种机器学习方法，根据过去标记观测预测一个人未来的观测值，通常用于连续任务，例如预测日常情绪评分。在进行个性化预测时，模型可以结合两种趋势：（a）跨人共享的趋势，即个人通用趋势，例如周末更开心，和（b）每个人独特的趋势，即个人特定的趋势，例如每周有一次压力大的会议。混合效应模型是一种流行的统计模型，用于通过组合个人通用和个人特定参数来研究这两种趋势。尽管现在线性混合效应模型通过将其与神经网络整合而变得越来越流行，但这种整合目前仅限于线性个人特定参数：排除非线性个人特定趋势。在本文中，我们提出了神经混合效应（NME）模型，以优化非线性个人特定参数。

    Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
    
[^118]: DRCFS：双重稳健因果特征选择

    DRCFS: Doubly Robust Causal Feature Selection. (arXiv:2306.07024v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07024](http://arxiv.org/abs/2306.07024)

    DRCFS是一个双重稳健因果特征选择方法，可以在非线性和高维环境中识别因果特征，优于现有方法。

    

    在许多科学领域中，了解对于特定目标变量非常相关的复杂系统特征是非常重要的。现有的方法通常局限于线性设置，有时缺乏保证，并且在大多数情况下无法扩展到需要处理的问题，特别是图像问题。我们提出了DRCFS，一种双重稳健特征选择方法，可以在非线性和高维设置中识别因果特征。我们提供了理论保证，阐明了我们的假设必要条件，并在广泛的模拟和半合成数据集上进行了大量实验。DRCFS在选择具有鲁棒性特征的技巧非常非线性和高维问题上明显优于现有的最先进方法。

    Knowing the features of a complex system that are highly relevant to a particular target variable is of fundamental interest in many areas of science. Existing approaches are often limited to linear settings, sometimes lack guarantees, and in most cases, do not scale to the problem at hand, in particular to images. We propose DRCFS, a doubly robust feature selection method for identifying the causal features even in nonlinear and high dimensional settings. We provide theoretical guarantees, illustrate necessary conditions for our assumptions, and perform extensive experiments across a wide range of simulated and semi-synthetic datasets. DRCFS significantly outperforms existing state-of-the-art methods, selecting robust features even in challenging highly non-linear and high-dimensional problems.
    
[^119]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^120]: GPT-FL: 生成预训练模型辅助的联邦学习

    GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02210](http://arxiv.org/abs/2306.02210)

    GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。

    

    在这项工作中，我们提出了GPT-FL，一种生成预训练模型辅助的联邦学习（FL）框架。GPT-FL利用生成预训练模型生成多样化的合成数据。这些生成的数据用于在服务器上训练下游模型，然后在标准FL框架下使用私有客户端数据进行微调。我们展示了GPT-FL在模型测试准确性、通信效率和客户端采样效率方面始终优于最先进的FL方法。通过全面的消融分析，我们发现在FL训练过程中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，这提高了收敛速度，并对观察到的GPT-FL的显著准确性提升做出了贡献。此外，无论目标数据是否在预训练生成模型的领域内或外，GPT-FL始终实现了显著的性能提升。

    In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai
    
[^121]: 树轮数字水印：一种不可见且健壮的扩散图像指纹技术

    Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. (arXiv:2305.20030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20030](http://arxiv.org/abs/2305.20030)

    本文提出了一种名为树轮数字水印的技术，可以稳定地指纹扩散模型的输出，与现有的在采样后对图像进行修改的方法不同，数字水印微妙地影响整个采样过程，从而产生模型指纹，对人类不可见。

    

    生成模型输出的数字水印是追踪版权和防止人工智能生成内容潜在威胁的关键技术。本文引入一种新颖的技术，称为树轮数字水印，可以稳定地指纹扩散模型的输出。与现有的在采样后对图像进行修改的方法不同，树轮数字水印微妙地影响整个采样过程，从而产生模型指纹，对人类不可见。数字水印将图像生成所使用的初始噪声向量中嵌入一个模式。这些模式在傅里叶空间中结构化，因此它们对卷积、裁剪、膨胀、翻转和旋转具有不变性。图像生成后，通过反演扩散过程来检测水印信号，以检索嵌入信号的噪声向量。我们展示了这种技术可以轻松应用于任意扩散模型，包括文本条件稳定扩散。

    Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diff
    
[^122]: 发现新的可解释保守律作为稀疏不变量

    Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v1 [math.DS])

    [http://arxiv.org/abs/2305.19525](http://arxiv.org/abs/2305.19525)

    这篇论文介绍了一种名为Sparse Invariant Detector（SID）的算法，它能够自动发现微分方程中的保守律。该算法可以重新发现已知的保守律，甚至发现新的保守律，并且已发现的保守律具有稳健性和可解释性。

    

    发现给定动力系统的保守律是重要但具有挑战性的任务。在理论设置（已知微分方程和基函数）中，我们提出了Sparse Invariant Detector（SID），这是一种从微分方程中自动发现保守律的算法。其算法简单性确保了已发现保守数量的稳健性和可解释性。我们展示了SID能够在各种系统中重新发现已知保守律，甚至发现新的保守律。在流体力学和大气化学的两个例子中，SID分别发现了14个和3个守恒量，而这些领域专家先前只知道12个和2个。

    Discovering conservation laws for a given dynamical system is important but challenging. In a theorist setup (differential equations and basis functions are both known), we propose the Sparse Invariant Detector (SID), an algorithm that auto-discovers conservation laws from differential equations. Its algorithmic simplicity allows robustness and interpretability of the discovered conserved quantities. We show that SID is able to rediscover known and even discover new conservation laws in a variety of systems. For two examples in fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved quantities, respectively, where only 12 and 2 were previously known to domain experts.
    
[^123]: 深度学习在洪水预测中的应用：DL Hydro-FRAN

    Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN). (arXiv:2305.12052v1 [cs.LG])

    [http://arxiv.org/abs/2305.12052](http://arxiv.org/abs/2305.12052)

    本研究使用深度神经网络优化水动力模型，成功加速并提升了洪水深度和速度的预测能力。

    

    水动力洪水建模提高了暴雨事件的水文和水力预测能力。然而，高分辨率水动力学所需的计算密集型数值解通常会阻碍在近实时洪水预测中的应用。本研究探讨了多种深度神经网络（DNN）模型用于优化洪水模型的可行性，并使用2D HEC-RAS水动力模型在低海拔、高分辨率城市环境中模拟了几次暴雨事件。将这些模拟数据用于DNN训练集，预测洪水深度和速度。与水动力洪水模型相比，DNN模型的预测具有良好的一致性，在研究区域内细胞洪水深度中，中位数RMSE约为2毫米。同时，DNN模型预测的计算时间明显优于传统方法，预测时间比传统方法快34.2到72.4倍。

    Hydrodynamic flood modeling improves hydrologic and hydraulic prediction of storm events. However, the computationally intensive numerical solutions required for high-resolution hydrodynamics have historically prevented their implementation in near-real-time flood forecasting. This study examines whether several Deep Neural Network (DNN) architectures are suitable for optimizing hydrodynamic flood models. Several pluvial flooding events were simulated in a low-relief high-resolution urban environment using a 2D HEC-RAS hydrodynamic model. These simulations were assembled into a training set for the DNNs, which were then used to forecast flooding depths and velocities. The DNNs' forecasts were compared to the hydrodynamic flood models, and showed good agreement, with a median RMSE of around 2 mm for cell flooding depths in the study area. The DNNs also improved forecast computation time significantly, with the DNNs providing forecasts between 34.2 and 72.4 times faster than conventional
    
[^124]: 使用图形回声状态网络解决节点分类中的异质性问题

    Addressing Heterophily in Node Classification with Graph Echo State Networks. (arXiv:2305.08233v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.08233](http://arxiv.org/abs/2305.08233)

    这篇论文介绍了使用图形回声状态网络(GESN)解决异质性图上节点分类的挑战。GESN是一种储备计算模型，通过递归计算节点嵌入来处理在异质性图上节点分类的问题。

    

    本文针对图上的节点分类任务，通过完全训练的深度消息传递模型来学习节点表示的层次结构，通过多次聚合节点附近的邻居来实现。在展示高比例内类边缘的图形上有效，但在异质性相反的情况下，即属于同一类的节点通常相隔较远，这种方法面临挑战。在具有高度异质性的图形中，卷积模型计算的基于紧密邻居的平滑表示不再有效。目前，在消息传递模型中提出了体系结构变化以减少过度平滑或重新布线输入图以改善长程消息传递。本文通过使用图形回声状态网络(GESN)来解决异质性图形的挑战以进行节点分类。GESN是一种图形的储备计算模型，在该模型中，节点嵌入是通过未经训练的消息传递函数递归计算的。

    Node classification tasks on graphs are addressed via fully-trained deep message-passing models that learn a hierarchy of node representations via multiple aggregations of a node's neighbourhood. While effective on graphs that exhibit a high ratio of intra-class edges, this approach poses challenges in the opposite case, i.e. heterophily, where nodes belonging to the same class are usually further apart. In graphs with a high degree of heterophily, the smoothed representations based on close neighbours computed by convolutional models are no longer effective. So far, architectural variations in message-passing models to reduce excessive smoothing or rewiring the input graph to improve longer-range message passing have been proposed. In this paper, we address the challenges of heterophilic graphs with Graph Echo State Network (GESN) for node classification. GESN is a reservoir computing model for graphs, where node embeddings are recursively computed by an untrained message-passing func
    
[^125]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^126]: 可扩展编码中的条件编码和残差编码方法，适用于人类和机器

    Conditional and Residual Methods in Scalable Coding for Humans and Machines. (arXiv:2305.02562v1 [eess.IV])

    [http://arxiv.org/abs/2305.02562](http://arxiv.org/abs/2305.02562)

    该论文提出了适用于人类和机器的可扩展编码中的条件编码和残差编码方法，并应用于图像重建，取得了类似的性能表现。

    

    我们提出了适用于人类和机器的可扩展编码中的条件编码和残差编码方法。我们的重点是利用计算机视觉任务中可用的信息来优化重构任务的速率失真性能。我们包括了两种方法的信息分析，提供基准，并提出了一种熵模型，适用于条件编码，具备增加的建模能力和与之前的工作类似的可操作性。我们将这些方法应用于图像重建，其中一种实验使用在Cityscapes数据集上创建的语义分割表示，另一种使用在COCO数据集上创建的目标检测表示。在两个实验中，我们获得了条件和残差方法之间的类似性能，得到的速率失真曲线包含在我们的基线内。

    We present methods for conditional and residual coding in the context of scalable coding for humans and machines. Our focus is on optimizing the rate-distortion performance of the reconstruction task using the information available in the computer vision task. We include an information analysis of both approaches to provide baselines and also propose an entropy model suitable for conditional coding with increased modelling capacity and similar tractability as previous work. We apply these methods to image reconstruction, using, in one instance, representations created for semantic segmentation on the Cityscapes dataset, and in another instance, representations created for object detection on the COCO dataset. In both experiments, we obtain similar performance between the conditional and residual methods, with the resulting rate-distortion curves contained within our baselines.
    
[^127]: 带有分解密度的字符串图表

    String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])

    [http://arxiv.org/abs/2305.02506](http://arxiv.org/abs/2305.02506)

    本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。

    

    有关概率编程和因果模型的研究越来越多地强调了需要在扩展定向图模型的模型类之间进行组合推理的必要性。概率编程和因果模型都定义了一组随机变量上的联合概率密度，并且展示了可以用于推理因果关系和条件独立性的稀疏结构。本文基于最近有关概率映射的马尔可夫范畴的工作，定义了一个范畴，其态射将分别由每个样本空间分解的联合密度与从样本到返回值的确定性映射组合。这是迈向最近的范畴论概率测度描述和通常在概率编程和因果推断中使用的分解密度的操作定义之间的缩小差距的一步。

    A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
    
[^128]: Distilling Step-by-Step！使用更少的训练数据和更小的模型尺寸胜过更大的语言模型

    Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])

    [http://arxiv.org/abs/2305.02301](http://arxiv.org/abs/2305.02301)

    本研究提出了Distilling Step-by-Step机制，通过提取LLM基础信息为小型模型提供额外的监督训练，从而使它们胜过更大的LLM模型，并需更少的训练数据。

    

    部署大型语言模型（LLM）面临内存效率低和计算密集度高的问题，研究人员通过微调或精炼使用LLM生成的标签来训练较小的任务特定模型。但是，要想达到LLM相当的性能，这需要大量的训练数据。我们引入了Distilling Step-by-Step，这是一种新的机制， (a)训练较小的模型比LLM表现更好，(b)并通过利用微调或精炼所需的更少的训练数据来实现。我们的方法在多任务训练框架中提取LLM基础，并作为额外的监督来训练小型模型。在四个NLP基准测试中，我们提出了三个发现：第一，与微调和精炼相比，我们的机制使用较少的标记/未标记训练示例取得更好的性能。第二，与LLM相比，即使使用更小的模型，我们也实现了更好的性能。

    Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
    
[^129]: 实现机制可解释性自动电路发现

    Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])

    [http://arxiv.org/abs/2304.14997](http://arxiv.org/abs/2304.14997)

    该论文提出了一种新算法 ACDC，可以自动识别网络中的重要单元，从而实现机制可解释性自动电路发现。

    

    最近的机制可解释性研究倒推了变形金刚模型的非平凡行为。这些发现需要大量的工作和研究者的直觉，这使得应用相同的方法来了解当前模型所展示的复杂行为变得困难。然而，这些发现的核心工作流程非常相似。研究人员创建一个数据集和度量，诱发所需的模型行为，将网络分为适当的抽象单元，替换这些单元的激活以确定哪些参与了行为，然后解释这些单元实施的功能。通过改变数据集、度量和待研究的单元，研究人员可以理解每个神经网络区域的功能和它们组成的电路。本文提出了一种新算法，自动电路发现（ACDC），以自动识别网络中的重要单元。

    Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
    
[^130]: Transformer介绍

    An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])

    [http://arxiv.org/abs/2304.10557](http://arxiv.org/abs/2304.10557)

    Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。

    

    Transformer是一种可以学习序列或数据集表示的神经网络组件。Transformer在自然语言处理、计算机视觉和时空建模方面取得了重大进展。虽然有很多Transformer的介绍，但大多数都缺少对其架构的精确数学描述，其设计选择的直觉也常常缺失。此外，随着研究路径的曲折，Transformer部件的解释可能是异质的。在这篇论文中，我们旨在提供一个数学精确、直观、简洁的Transformer架构描述。

    The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
    
[^131]: 基于深度函数的偏序集合的描述性分析和机器学习算法

    Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms. (arXiv:2304.09872v1 [cs.LG])

    [http://arxiv.org/abs/2304.09872](http://arxiv.org/abs/2304.09872)

    本文提出了一种基于深度函数的偏序集合描述性分析框架，并引入了偏序版本的单纯深度，用于比较基于多维性能度量的机器学习算法。实验证明此方法与现有基准方法不同，为分类器比较提供了新的视角。

    

    我们提出了一个框架，基于深度函数对偏序集合进行描述性分析。尽管深度函数在线性和度量空间中进行了大量研究，但是对于偏序等非标准数据类型的深度函数的讨论却很少。我们介绍了著名的单纯深度的偏序版本-无并通用深度（ufg depth）。此外，我们利用我们的 ufg depth 来比较基于多维性能度量的机器学习算法。具体地，我们分析不同分类器在标准基准数据集样本上的表现分布。我们的结果有希望地证明了我们的方法与现有基准方法有很大不同，因此为分类器比较的激烈辩论增加了新的视角。

    We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. Our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.
    
[^132]: 流式数据主动计费标注

    Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v1 [cs.LG])

    [http://arxiv.org/abs/2304.06808](http://arxiv.org/abs/2304.06808)

    本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。

    

    我们研究了主动标记流数据问题，其中主动学习者面临一系列数据点，并必须通过昂贵的实验精心选择哪些点进行标记，此类问题常常出现在医疗和天文学等领域。我们首先研究的是数据输入属于$K$个离散分布之一的情况，并通过损失函数形式化描述此问题，该损失函数捕捉了标记成本和预测误差。当标记成本为$B$时，我们的算法通过选择标记点，仅在不确定性大于时间和成本相关阈值时进行，可以在$T$轮之后实现$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。我们还提供了更细致的上界，证明了在到达模式更有利时，算法可以适应到达模式，并实现更好的性能。我们还补充了两个上界的匹配下界。接下来，我们研究了在流数据具有不确定性分布的情况下标记流数据的问题，并提供与前面情况类似的结果。

    We study actively labeling streaming data, where an active learner is faced with a stream of data points and must carefully choose which of these points to label via an expensive experiment. Such problems frequently arise in applications such as healthcare and astronomy. We first study a setting when the data's inputs belong to one of $K$ discrete distributions and formalize this problem via a loss that captures the labeling cost and the prediction error. When the labeling cost is $B$, our algorithm, which chooses to label a point if the uncertainty is larger than a time and cost dependent threshold, achieves a worst-case upper bound of $O(B^{\frac{1}{3}} K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuanced upper bound which demonstrates that the algorithm can adapt to the arrival pattern, and achieves better performance when the arrival pattern is more favorable. We complement both upper bounds with matching lower bounds. We next study this pr
    
[^133]: DeforestVis：使用代理决策树进行机器学习模型行为分析

    DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])

    [http://arxiv.org/abs/2304.00133](http://arxiv.org/abs/2304.00133)

    DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。

    

    随着机器学习（ML）模型的复杂性增加以及不同（和关键）领域中的应用增加，越来越需要更易解释和可信赖的ML。解释复杂ML模型的一种简单且与模型无关的方法是训练代理模型（例如规则集和决策树），以足够接近原始模型，但更简单和易于解释。然而，规则集可以变得非常冗长，包含许多if-else语句，而决策树的深度会随着准确模拟复杂ML模型而迅速增加。在这种情况下，两种方法都可能无法实现其核心目标，提供用户模型的可解释性。我们通过提出DeforestVis解决了这个问题，这是一种可视化分析工具，通过提供使用自适应增强（AdaBoost）技术生成的代理决策树（一级决策树），为用户提供了对复杂ML模型行为的友好总结。我们的解决方案帮助用户探索模型的复杂性。

    As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
    
[^134]: 图上随机逆问题：分布式在线学习

    Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])

    [http://arxiv.org/abs/2303.11789](http://arxiv.org/abs/2303.11789)

    本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。

    

    我们建立了一个随机逆问题的框架，该问题具有实时的图上观测，并提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来。我们将算法收敛性转化为带有L2有界鞅差分项的希尔伯特空间中随机时变差分方程的渐近稳定性，并发展了L2-渐近稳定性理论。结果表明，如果网络图是连通的，并且正向算子序列满足无限维度时空励磁条件，则所有节点的估计均为均方和几乎必然的强一致的。通过将RKHS中的分布式学习问题等效地转化为图上随机逆问题，我们提出了一种基于无中心节点的RKHS分布式在线学习算法。

    We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
    
[^135]: FairAdaBN：自适应批归一化减少不公平性及其在皮肤病分类中的应用

    FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification. (arXiv:2303.08325v1 [cs.LG])

    [http://arxiv.org/abs/2303.08325](http://arxiv.org/abs/2303.08325)

    本文提出了FairAdaBN，将批归一化适应敏感属性，可以将其简单而有效地应用到原本不了解公平性的多个分类主干中，能够有效地减少不公平性，并实现模型性能和公平性之间的平衡。

    

    深度学习正在医学研究和应用中变得越来越普遍，同时涉及敏感信息，甚至包括关键的诊断决策。研究人员观察到不同人口属性子组之间的显著性能差异，称为模型不公平性，并致力于精心设计优雅的体系结构以解决不公平性问题，这带来了沉重的训练负担、较差的泛化能力，并揭示了模型性能和公平性之间的权衡。为了解决这些问题，我们提出了FairAdaBN，通过使批归一化适应敏感属性，可以将其简单而有效地应用到原本不了解公平性的多个分类主干中。另外，我们提出了一种新的损失函数，通过限制小批量子组之间的统计平衡，鼓励模型以相当公平的方式收敛。为了评估模型性能和公平性之间的权衡，我们在HAM10000数据集上进行了实验，该数据集是一个大规模的开放获取皮肤病数据库，用于分类七种常见的皮肤病病变。我们的结果表明，FairAdaBN可以有效地减少不公平性，并实现模型性能和公平性之间的平衡，开销可忽略不计。

    Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions. Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is called model unfairness, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we propose FairAdaBN by making batch normalization adaptive to sensitive attribute. This simple but effective design can be adopted to several classification backbones that are originally unaware of fairness. Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness. In order to evaluate the trade-off between model performanc
    
[^136]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^137]: 基于图神经网络的上下文嵌入在表格数据深度学习中的应用

    Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])

    [http://arxiv.org/abs/2303.06455](http://arxiv.org/abs/2303.06455)

    本文介绍了一种基于图神经网络的深度学习模型，使用交互网络进行上下文嵌入，用于表格数据的处理。该模型在公共数据集上的表现优于最近的深度学习基准调查，并且与提升树解决方案相比也取得了竞争性的结果。

    This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.

    所有行业都试图利用现有的大数据进行基于人工智能的应用，这些数据通常以所谓的表格形式存在，其中每个记录由许多异构的连续和分类列组成，也称为特征。深度学习在自然语言处理等与人类技能相关的领域中已经取得了重大突破，但其在表格数据上的应用更具挑战性。更经典的机器学习模型，如基于树的集成模型通常表现更好。本文介绍了一种新颖的深度学习模型，它使用图神经网络（GNN），更具体地说是交互网络（IN），进行上下文嵌入。其结果优于最近发布的基于五个公共数据集的深度学习基准调查，与提升树解决方案相比也取得了竞争性的结果。

    All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
    
[^138]: 因果依赖图

    Causal Dependence Plots. (arXiv:2303.04209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04209](http://arxiv.org/abs/2303.04209)

    本论文提出了因果依赖图（CDPs）来解释人工智能或机器学习模型的因果依赖关系。CDPs与传统方法不同，可以模块化地结合因果学习或敏感度分析方法。这些图表可以成为可解释机器学习工具包中的强大工具，并对相关应用做出贡献。

    

    解释人工智能或机器学习模型的重要性越来越大。为了明智地使用这些数据驱动的系统，我们必须了解它们如何与世界互动，包括它们在数据输入上的因果依赖关系。在这项工作中，我们开发了因果依赖图 (CDPs)，用于可视化一个变量（结果）如何随另一个变量（预测器）的变化而变化，以及其他预测器变量的因果变化。关键是，CDPs与基于保持其他预测器恒定或假设它们独立的标准方法不同。CDPs利用辅助因果模型，因为因果结论需要因果假设。通过模拟和真实数据实验，我们展示了CDPs可以与因果学习或敏感性分析方法以模块化的方式结合使用。由于人们经常在输入-输出依赖性方面进行因果思考，CDPs可以成为xAI或可解释机器学习工具包中强有力的工具，并对应用有所贡献。

    Explaining artificial intelligence or machine learning models is increasingly important. To use such data-driven systems wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how one variable--an outcome--depends on changes in another variable--a predictor--$\textit{along with any consequent causal changes in other predictor variables}$. Crucially, CDPs differ from standard methods based on holding other predictors constant or assuming they are independent. CDPs make use of an auxiliary causal model because causal conclusions require causal assumptions. With simulations and real data experiments, we show CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to appl
    
[^139]: 使用专家建议和随机化的Littlestone维度进行最优预测

    Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension. (arXiv:2302.13849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13849](http://arxiv.org/abs/2302.13849)

    本研究证明了在线学习中，在学习一个类别时，最优期望错误边界等于其随机化的Littlestone维度。在不可知的情况下，最优错误边界与最佳函数的错误次数之间存在特定关系。此外，该研究还解决了一个开放问题，并将其应用于预测问题。

    

    在在线学习中，经典的结果表明使用确定性学习器的最优错误边界可以通过Littlestone维度来实现（Littlestone '88）。我们证明了随机学习器的类似结果：我们证明了在学习一个类别 $\mathcal{H}$时，最优期望错误边界等于其随机化的Littlestone维度，即存在一个由 $\mathcal{H}$ 打碎的树，其平均深度为 $2d$，而 $d$ 是最大的维度。此外，我们进一步研究了在不可知的情况下，最优错误边界与 $\mathcal{H}$ 中最佳函数的错误次数 $k$ 之间的关系。我们证明了具有Littlestone维度 $d$ 的类别学习的最优随机化错误边界是 $k + \Theta (\sqrt{k d} + d )$。这也意味着确定性学习的最优错误边界是 $2k + O (\sqrt{k d} + d )$，从而解决了Auer和Long ['99]研究的一个开放问题。作为我们理论的一个应用，我们重新审视了经典问题的预测

    A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + O (\sqrt{k d} + d )$, thus resolving an open question which was studied by Auer and Long ['99].  As an application of our theory, we revisit the classical problem of prediction 
    
[^140]: 是否可以在深度神经网络中避免双下降？

    Can we avoid Double Descent in Deep Neural Networks?. (arXiv:2302.13259v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13259](http://arxiv.org/abs/2302.13259)

    这项研究表明，通过适当调整学习问题的条件，可以避免双下降现象，特别是在复杂情况下使用适当的正则化。这对于寻找深度学习模型的最优大小具有重要意义。

    

    寻找深度学习模型的最优大小非常重要且具有广泛影响，尤其在节能方案中。最近，一个意外的现象，“双下降”，引起了深度学习界的关注。随着模型大小的增加，性能首先变差，然后恢复提升。这对于维持高泛化的最优模型大小提出了严重的问题：模型需要足够的超参数化，但添加过多的参数会浪费训练资源。是否可能以高效的方式找到最佳折衷方案？我们的工作表明，通过适当调整学习问题的条件，可能可以避免双下降现象，但最终答案仍待确定。我们经验地观察到，在复杂情况下，通过适当的正则化有望避开双下降，简单的$\ell_2$正则化已经对此有积极的贡献。

    Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the ``double descent'', has caught the attention of the deep learning community. As the model's size grows, the performance gets first worse, and then goes back to improving. It raises serious questions about the optimal model's size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple $\ell_2$ regularization is already positively contributing to such a perspective.
    
[^141]: 在 exp-concave 统计学习中探索本地规范

    Exploring Local Norms in Exp-concave Statistical Learning. (arXiv:2302.10726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10726](http://arxiv.org/abs/2302.10726)

    这篇论文研究了在 exp-concave 统计学习中使用经验风险最小化的问题，提出了适用于广泛类别的有界 exp-concave 损失的过量风险界，维度和样本大小对结果有影响，并且基于统一几何假设和本地规范的概念。

    

    我们考虑使用经验风险最小化在一个凸类中处理带有 exp-concave 损失的随机凸优化问题。回答了一些之前研究中提出的问题，我们给出了一个对于一类广泛的有界 exp-concave 损失的 $O(d/n + \log(1/\delta)/n)$ 过量风险界，其中 $d$ 是凸参考集的维度，$n$ 是样本大小，$\delta$ 是置信水平。我们的结果基于对损失梯度的统一几何假设和本地规范的概念。

    We consider the problem of stochastic convex optimization with exp-concave losses using Empirical Risk Minimization in a convex class. Answering a question raised in several prior works, we provide a $O( d / n + \log( 1 / \delta) / n )$ excess risk bound valid for a wide class of bounded exp-concave losses, where $d$ is the dimension of the convex reference set, $n$ is the sample size, and $\delta$ is the confidence level. Our result is based on a unified geometric assumption on the gradient of losses and the notion of local norms.
    
[^142]: FedSpeed: 更大的本地间隔，更少的通信轮次，更高的泛化准确性。

    FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy. (arXiv:2302.10429v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10429](http://arxiv.org/abs/2302.10429)

    本文提出了FedSpeed方法，通过应用prox校正项和合并额外梯度上升步骤的扰动，从而减少偏差和过拟合的影响，提高了联邦学习的性能。

    

    联邦学习是一种新兴的分布式机器学习框架，通过大量具有数据隐私保护的本地设备联合训练全局模型。其性能受到本地不一致最优引入的非消失偏差和本地过拟合引起的不稳定客户漂移的影响。本文提出了一种新颖而实用的方法FedSpeed，以减轻这些问题带来的负面影响。具体地说，FedSpeed在当前本地更新上应用了prox校正项，以有效减少prox项引入的偏差，prox项是一种必要的正则化器，用于保持强烈的本地一致性。此外，FedSpeed将纯随机梯度与邻域中额外梯度上升步骤计算的扰动相结合，从而减轻本地过拟合的问题。我们的理论分析表明，收敛速度与通信轮次$T$和本地间隔$K$有关

    Federated learning is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems. Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ w
    
[^143]: 基于伪对比学习的基于图的半监督学习

    Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09532](http://arxiv.org/abs/2302.09532)

    本论文提出了一种基于伪对比学习的半监督图神经网络方法，通过生成可靠的负样本对来改进伪标签的质量。

    

    伪标签是一种用于改进半监督图神经网络(GNNs)性能的技术，通过根据自信的预测生成附加的伪标签。然而，由于分类目标对给定标签的敏感性，生成的伪标签质量一直是一个长期存在的问题。为了避免不可靠的分类监督“一个节点属于特定类”，我们更喜欢容错性对比监督“两个节点不属于同一类”。因此，生成高质量伪标签问题转化为一个放松的版本，即识别可靠的负样本对。为了实现这一点，我们提出了一种通用的GNNs框架，称之为伪对比学习(PCL)。它将目标为相同类的正伪标签和负伪标签的两个节点分开。为了将拓扑知识纳入学习中，我们设计了一种拓扑加权对比学习方法

    Pseudo Labeling is a technique used to improve the performance of semi-supervised Graph Neural Networks (GNNs) by generating additional pseudo-labels based on confident predictions. However, the quality of generated pseudo-labels has been a longstanding concern due to the sensitivity of the classification objective with respect to the given labels. To avoid the untrustworthy classification supervision indicating ``a node belongs to a specific class,'' we favor the fault-tolerant contrasting supervision demonstrating ``two nodes do not belong to the same class.'' Thus, the problem of generating high-quality pseudo-labels is then transformed into a relaxed version, i.e., identifying reliable negative pairs. To achieve this, we propose a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It separates two nodes whose positive and negative pseudo-labels target the same class. To incorporate topological knowledge into learning, we devise a topologically weighted contrastiv
    
[^144]: 在测地度量空间上最小化动态遗憾

    Minimizing Dynamic Regret on Geodesic Metric Spaces. (arXiv:2302.08652v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08652](http://arxiv.org/abs/2302.08652)

    本文研究了在测地度量空间上最小化动态遗憾的顺序决策问题，并提出了乐观的在线学习算法，为该领域的研究做出了首次贡献。

    

    本文探讨了在完备的黎曼流形上最小化一般动态遗憾的顺序决策问题。最近，离线优化在这样一个领域，也被称为测地度量空间，引起了相当大的关注。在线设置却引起了较少的关注，并且尚未确定在测地度量空间中是否可以复制欧几里德设定中的结论（例如曲率）所需的结果。本文展示了如何在具有非正曲率的流形上找到乐观的遗憾界限，只要允许不适当的学习，并提出了一系列的自适应无遗憾算法。据我们所知，这是第一篇考虑一般动态遗憾并开发可应用于测地度量空间的“乐观”在线学习算法的工作。

    In this paper, we consider the sequential decision problem where the goal is to minimize the general dynamic regret on a complete Riemannian manifold. The task of offline optimization on such a domain, also known as a geodesic metric space, has recently received significant attention. The online setting has received significantly less attention, and it has remained an open question whether the body of results that hold in the Euclidean setting can be transplanted into the land of Riemannian manifolds where new challenges (e.g., curvature) come into play. In this paper, we show how to get optimistic regret bound on manifolds with non-positive curvature whenever improper learning is allowed and propose an array of adaptive no-regret algorithms. To the best of our knowledge, this is the first work that considers general dynamic regret and develops "optimistic" online learning algorithms which can be employed on geodesic metric spaces.
    
[^145]: LEVER: 使用执行进行语言到代码生成的学习验证

    LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08468](http://arxiv.org/abs/2302.08468)

    提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。

    

    训练在代码上的大型语言模型（code LLMs）的出现，已经在语言到代码生成方面取得了显著进展。此领域的最新方法将LLM解码与使用测试用例或基于执行结果的启发式方法的样本修剪和重新排序相结合。然而，对于许多现实世界的语言到代码应用来说，获取测试用例是具有挑战性的，而启发式方法不能很好地捕捉执行结果的语义特征，比如数据类型和值范围，这往往表明程序的正确性。在这项工作中，我们提出了LEVER，一种通过学习使用执行结果来验证生成的程序，从而改进语言到代码生成的简单方法。具体地说，我们训练验证器根据自然语言输入、程序本身和执行结果来确定从LLM中抽样的程序是否正确。通过将验证分数与LLM生成分数相结合，对抽样的程序进行重新排序。

    The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
    
[^146]: 仅调整归一化层的表达能力

    The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07937](http://arxiv.org/abs/2302.07937)

    本研究发现，仅调整神经网络的归一化层参数就可以达到高准确性，甚至可以重建比原网络小O(根号宽度)倍的目标网络。

    

    特征归一化转换，如批量归一化和层归一化，已成为当今先进深度神经网络不可或缺的组成部分。关于微调大型预训练模型的最近研究表明，仅调整这些仿射变换的参数就可以在下游任务中获得高准确性。这些研究结果引发了对调整冻结网络的归一化层的表达能力的问题。本文首次探讨这个问题，并显示对于随机ReLU网络，仅微调其归一化层可以重建任何大小为O(根号宽度)倍小的目标网络。我们证明，即使在随机稀疏网络中，在足够超参数化的情况下，这个结论也成立，与先前的实证工作一致。

    Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work.
    
[^147]: 标签预算约束下的深度异常检测

    Deep Anomaly Detection under Labeling Budget Constraints. (arXiv:2302.07832v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07832](http://arxiv.org/abs/2302.07832)

    本文针对标签预算约束下的深度异常检测问题提出了一种数据标记策略和半监督学习框架，在多种数据集上取得了最先进的性能。

    

    在各种场景下，选择信息丰富的数据点进行专家反馈可以显著提高异常检测的性能，例如医学诊断或欺诈检测。在本文中，我们确定了一组理论条件，使得从标记的查询到未标记的数据中异常分数能够推广。基于这些结果，我们提出了一种在标签预算约束下具有最优数据覆盖的数据标记策略。此外，我们还提出了一种新的半监督异常检测学习框架。在图像、表格和视频数据集上进行了大量实验证明，我们的方法在标签预算约束下具有最先进的半监督异常检测性能。

    Selecting informative data points for expert feedback can significantly improve the performance of anomaly detection (AD) in various contexts, such as medical diagnostics or fraud detection. In this paper, we determine a set of theoretical conditions under which anomaly scores generalize from labeled queries to unlabeled data. Motivated by these results, we propose a data labeling strategy with optimal data coverage under labeling budget constraints. In addition, we propose a new learning framework for semi-supervised AD. Extensive experiments on image, tabular, and video data sets show that our approach results in state-of-the-art semi-supervised AD performance under labeling budget constraints.
    
[^148]: 超越领域情景：鲁棒的密度感知校准

    Beyond In-Domain Scenarios: Robust Density-Aware Calibration. (arXiv:2302.05118v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05118](http://arxiv.org/abs/2302.05118)

    这个论文提出了一种鲁棒的密度感知校准方法，通过利用隐藏层的信息，该方法能够提供在领域转移和领域外情景下可靠的不确定性估计，并在保持出色领域内预测不确定性估计的同时提高校准性能。

    

    随着深度神经网络在安全关键应用中的部署越来越多，校准深度学习模型以获得具有不确定性意识的预测变得至关重要。尽管现有的事后校准方法在领域内的测试数据集上取得了令人印象深刻的结果，但它们在领域转移和领域外（OOD）情景下无法产生可靠的不确定性估计。我们旨在通过提出基于k近邻的DAC（Density-Aware Calibration）方法来弥补这个差距。与现有的事后方法不同，我们利用分类器的隐藏层作为与不确定性相关信息的来源，并研究它们的重要性。我们表明DAC是一个通用的方法，可以方便地与最先进的事后方法结合使用。DAC提高了在领域转移和OOD情景下校准性能的稳健性，同时保持了出色的领域内预测不确定性估计。我们证明了DAC能够一直引导着一致

    Calibrating deep learning models to yield uncertainty-aware predictions is crucial as deep neural networks get increasingly deployed in safety-critical applications. While existing post-hoc calibration methods achieve impressive results on in-domain test datasets, they are limited by their inability to yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD) scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN). In contrast to existing post-hoc methods, we utilize hidden layers of classifiers as a source for uncertainty-related information and study their importance. We show that DAC is a generic method that can readily be combined with state-of-the-art post-hoc methods. DAC boosts the robustness of calibration performance in domain-shift and OOD, while maintaining excellent in-domain predictive uncertainty estimates. We demonstrate that DAC leads to consistently b
    
[^149]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^150]: 网络中的两种真实划分的生成模型

    Generative models for two-ground-truth partitions in networks. (arXiv:2302.02787v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2302.02787](http://arxiv.org/abs/2302.02787)

    本研究提出了一种生成模型，称为随机交叉块模型（SCBM），可以在网络的中尺度结构中构建两个不同的划分。通过评估随机块模型的性能，展示了该模型的使用案例。

    

    提出了一种称为随机交叉块模型（SCBM）的生成模型，可以在单个基准网络的中尺度结构中同时构建两个不同的划分。通过评估随机块模型的性能，展示了基准模型的使用案例。

    A myriad of approaches have been proposed to characterise the mesoscale structure of networks - most often as a partition based on patterns variously called communities, blocks, or clusters. Clearly, distinct methods designed to detect different types of patterns may provide a variety of answers to the network's mesoscale structure. Yet, even multiple runs of a given method can sometimes yield diverse and conflicting results, producing entire landscapes of partitions which potentially include multiple (locally optimal) mesoscale explanations of the network. Such ambiguity motivates a closer look at the ability of these methods to find multiple qualitatively different 'ground truth' partitions in a network. Here, we propose the stochastic cross-block model (SCBM), a generative model which allows for two distinct partitions to be built into the mesoscale structure of a single benchmark network. We demonstrate a use case of the benchmark model by appraising the power of stochastic block m
    
[^151]: ProtST: 蛋白质序列与生物医学文本的多模态学习

    ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts. (arXiv:2301.12040v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.12040](http://arxiv.org/abs/2301.12040)

    ProtST是一个多模态学习框架，利用蛋白质的文本描述来增强蛋白质序列的预训练和理解，通过设计多种类型的任务，该框架能够获得不同粒度的蛋白质属性信息，并保持原始表征。

    

    当前的蛋白质语言模型（PLMs）主要基于蛋白质的序列学习蛋白质的表征，从而很好地捕捉到共进化信息，但它们无法明确获得蛋白质的功能，这是蛋白质表征学习的最终目标。幸运的是，对于许多蛋白质来说，它们的文本属性描述是可用的，其中也描述了它们的各种功能。受到这个事实的启发，我们首先构建了ProtDescribe数据集，通过蛋白质序列的文本描述来增强它们的功能和其他重要属性。基于这个数据集，我们提出了ProtST框架，通过生物医学文本来增强蛋白质序列的预训练和理解。在预训练过程中，我们设计了三种类型的任务，即单模态掩码预测、多模态表示对齐和多模态掩码预测，以增强PLM具有不同粒度的蛋白质属性信息，并同时保持PLM的原始表征。

    Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original represen
    
[^152]: EmbedDistill: 一种用于信息检索的几何知识蒸馏模型

    EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval. (arXiv:2301.12005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12005](http://arxiv.org/abs/2301.12005)

    这项研究提出了一种新的几何知识蒸馏方法，通过利用查询和文档的相对几何关系以及嵌入匹配任务来对齐大型教师模型和学生模型的表示，从而提高信息检索的效果。

    

    大型神经模型（如Transformers）在信息检索（IR）方面取得了最先进的性能。本文旨在改进蒸馏方法，以便在实践中实现这些模型的资源高效部署。受到我们对IR模型的师生泛化差距的理论分析的启发，我们提出了一种新的蒸馏方法，利用大型教师模型学到的查询和文档之间的相对几何关系。与现有的基于教师分数的蒸馏方法不同，我们的方法采用嵌入匹配任务，以提供更强的信号来对齐教师和学生模型的表示。此外，它还利用查询生成来探索数据流形，以减少训练数据稀缺情况下学生和教师之间的差异。此外，我们的分析还推动了学生模型的新型非对称架构，实现了更好的嵌入对齐。

    Large neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without i
    
[^153]: SOBER：离散和混合空间上高并行贝叶斯优化和贝叶斯积分

    SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces. (arXiv:2301.11832v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11832](http://arxiv.org/abs/2301.11832)

    SOBER算法是一种在离散和混合空间上进行高并行贝叶斯优化的方法，能够进行可扩展和多样化的批量全局优化和积分，且优于11个竞争基线方法。

    

    批处理贝叶斯优化和贝叶斯积分已被证明是在需并行查询昂贵的目标函数时执行优化和积分的高效方法。然而，当前的方法不适用于大批量操作。我们提出了一种新算法——SOBER，它允许在离散和混合空间上使用任意采集函数和内核进行可扩展和多样化的批量全局优化和积分。我们的方法的关键在于将全局优化的批量选择重新定义为积分问题，并将采集函数的最大化（非凸）松弛为内核重组（凸），从而有效地解决了两个任务。我们展示SOBER优于11个竞争基线方法。

    Batch Bayesian optimisation and Bayesian quadrature have been shown to be sample-efficient methods of performing optimisation and quadrature where expensive-to-evaluate objective functions can be queried in parallel. However, current methods do not scale to large batch sizes -- a frequent desideratum in practice (e.g. drug discovery or simulation-based inference). We present a novel algorithm, SOBER, which permits scalable and diversified batch global optimisation and quadrature with arbitrary acquisition functions and kernels over discrete and mixed spaces. The key to our approach is to reformulate batch selection for global optimisation as a quadrature problem, which relaxes acquisition function maximisation (non-convex) to kernel recombination (convex). Bridging global optimisation and quadrature can efficiently solve both tasks by balancing the merits of exploitative Bayesian optimisation and explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive baselines o
    
[^154]: 细调神经算符结构以提高训练和泛化能力

    Fine-tuning Neural-Operator architectures for training and generalization. (arXiv:2301.11509v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11509](http://arxiv.org/abs/2301.11509)

    本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。

    

    本篇论文全面分析了神经算符（NOs）及其衍生结构的泛化特性。通过对测试损失的经验评估、基于复杂性的泛化界限的分析以及对损失景观可视化的定性评估，我们研究了旨在提高NOs泛化能力的修改。受Transformer的成功启发，我们提出了${\textit{s}}{\text{NO}}+\varepsilon$，该方法引入了一个核积分算符来代替自关注机制。我们的结果显示，伴随着损失景观可视化的定性变化，性能显著提高了，适用于各种数据集和初始化。我们猜测，Transformer的布局使优化算法能够找到更好的极小值，并且随机深度可以提高泛化性能。由于训练动态的严格分析是深度学习最突出的未解决问题之一，因此我们还推出了一个新的训练课程，重点是逐渐增加模型容量，从而显著提高了泛化能力。

    This work provides a comprehensive analysis of the generalization properties of Neural Operators (NOs) and their derived architectures. Through empirical evaluation of the test loss, analysis of the complexity-based generalization bounds, and qualitative assessments of the visualization of the loss landscape, we investigate modifications aimed at enhancing the generalization capabilities of NOs. Inspired by the success of Transformers, we propose ${\textit{s}}{\text{NO}}+\varepsilon$, which introduces a kernel integral operator in lieu of self-Attention. Our results reveal significantly improved performance across datasets and initializations, accompanied by qualitative changes in the visualization of the loss landscape. We conjecture that the layout of Transformers enables the optimization algorithm to find better minima, and stochastic depth, improve the generalization performance. As a rigorous analysis of training dynamics is one of the most prominent unsolved problems in deep lear
    
[^155]: 案例基础神经网络：具有时间变化的高阶交互的生存分析

    Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06535](http://arxiv.org/abs/2301.06535)

    案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。

    

    神经网络基于生存分析方法可以模拟数据驱动的协变量交互。虽然这些方法可以比回归方法提供更好的预测性能，但并不是所有的方法都可以模拟时间变化的交互和复杂的基线风险。为了解决这个问题，我们提出了一种称为案例基础神经网络（CBNNs）的新方法，它将案例基础抽样框架与灵活的神经网络结构相结合。通过使用一种新颖的抽样方案和数据增强来自然地考虑到截尾，我们构建了一个可以接受时间输入的前馈神经网络。CBNNs通过预测在给定时刻事件发生的概率来估计危险函数。我们通过模拟和三个案例研究使用两个时间依赖指标比较CBNNs与回归和神经网络基于生存分析方法的性能。首先，我们通过涉及复杂基线风险和时间变化交互的模拟来评估所有方法，其中包括CBNNs。

    Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
    
[^156]: 跨模态注意力一致性正则化用于视觉-语言关系对齐

    Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10549](http://arxiv.org/abs/2212.10549)

    本研究提出了一种跨模态注意力一致性正则化方法，用于视觉-语言关系对齐。通过鼓励语言注意力与视觉注意力的一致性来实现关系级别的对齐，从而提高视觉-语言模型在组合概括性基准测试中的性能。

    

    尽管近年来联合视觉-语言模型的规模不断扩大，但这些模型在诸如Winoground等组合概括性基准测试中仍然存在困难。我们发现，当前视觉-语言模型缺乏的关键组成部分是关系级别的对齐：即能够将文本中的定向语义关系（例如，“草坪中的杯子”）与图像中的空间关系（例如，杯子相对于草坪的位置）进行匹配。为了解决这个问题，我们展示了通过鼓励从“杯子”到“草坪”（捕捉语义关系“在”）的定向语言注意力与从杯子到草坪的定向视觉注意力相匹配来实现关系对齐。通过跨模态注意力软性地识别标记及其对应的对象。我们证明了这种软性关系对齐的概念等同于在由跨模态提供的“基底变换”下实施视觉和语言注意力矩阵之间的一致性。

    Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., "mug in grass") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal a
    
[^157]: 迷你模型适应：通过对齐的浅层训练高效地将预训练模型扩展到新语言上

    Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10503](http://arxiv.org/abs/2212.10503)

    使用迷你模型适应的方法，通过构建浅层迷你模型以及高效训练新的语言特定嵌入向量，实现了将预训练模型扩展到新语言上的快速跨语言传输。

    

    先前的研究表明，通过学习一组新的嵌入向量，并保持transformer主体部分冻结，可以将预训练的遮蔽语言模型（MLM）扩展到新语言。尽管只学习了一小部分参数，但这种方法在计算效率上并不高，因为训练新的嵌入向量需要对整个模型进行完整的前向和反向传播。我们提出了迷你模型适应，一种计算高效的替代方案，通过从大型模型的一小部分参数中构建一个浅层迷你模型。然后可以在迷你模型上高效地训练新的语言特定嵌入向量，并将其插入到对齐的大型模型中进行快速的跨语言传输。我们探索了两种学习迷你模型的方法：MiniJoint，它使用一个具有中间层次上辅助MLM头的单个transformer同时预训练主模型和迷你模型。MiniPost则从常规预训练模型开始，通过提取和冻结几层来构建迷你模型，并学习语言特定的嵌入向量。

    Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a 
    
[^158]: 探索随机连接的神经网络在气候模型仿真中的应用

    Exploring Randomly Wired Neural Networks for Climate Model Emulation. (arXiv:2212.03369v3 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2212.03369](http://arxiv.org/abs/2212.03369)

    本研究探索了随机连接的神经网络在气候模型仿真中的应用。通过使用ClimateBench数据集，我们发现随机连接的神经网络在拥有100万和1000万参数的模型上具有与传统网络相当的性能，且能耗更低。

    

    探索不同人类活动排放情景的气候影响对于制定气候变化减缓和适应的决策至关重要。先进的地球系统模型可以提供对这些影响的详细洞见，但每个情景的计算成本很高。这种巨大的计算负担引发了对开发廉价的机器学习模型进行气候模型仿真任务的兴趣。在本文中，我们探讨了随机连接的神经网络在这一任务中的有效性。我们描述了如何构建这些网络，并将它们与传统的前馈神经网络进行比较，使用ClimateBench数据集进行评估。具体而言，我们将多层感知机、卷积神经网络和卷积长短期记忆网络中串行连接的稠密层替换为随机连接的稠密层，并评估在拥有100万和1000万参数的模型上的性能影响。我们发现，随机连接的神经网络在能耗较低的情况下可以获得与传统网络相当的模型性能。

    Exploring the climate impacts of various anthropogenic emissions scenarios is key to making informed decisions for climate change mitigation and adaptation. State-of-the-art Earth system models can provide detailed insight into these impacts, but have a large associated computational cost on a per-scenario basis. This large computational burden has driven recent interest in developing cheap machine learning models for the task of climate model emulation. In this manuscript, we explore the efficacy of randomly wired neural networks for this task. We describe how they can be constructed and compare them to their standard feedforward counterparts using the ClimateBench dataset. Specifically, we replace the serially connected dense layers in multilayer perceptrons, convolutional neural networks, and convolutional long short-term memory networks with randomly wired dense layers and assess the impact on model performance for models with 1 million and 10 million parameters. We find that model
    
[^159]: 通过公平干预和纠正采样来打破条件生成的伪因果关系

    Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling. (arXiv:2212.02090v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02090](http://arxiv.org/abs/2212.02090)

    本文提出了一种通过公平干预和纠正采样的方法来解决条件生成中的伪因果关系。实验证明，该方法在各种数据集上都能有效地解决这个问题。

    

    为了捕捉样本和标签之间的关系，条件生成模型经常从训练数据集中继承了伪相关性。这可能导致标签条件分布在另一个潜在属性上不平衡。为了减轻这个问题，我们提出了一种通用的两步策略。 （a）公平干预（FI）：强调在训练数据集中由于伪相关性难以生成的少数样本。（b）纠正采样（CS）：显式过滤生成的样本，并确保它们遵循所需的潜在属性分布。我们设计的公平干预可以适用于不同程度的对伪属性的监督，包括无监督、弱监督和半监督场景。我们的实验结果证明，FICS可以有效地解决各种数据集上的条件生成的伪因果关系。

    To capture the relationship between samples and labels, conditional generative models often inherit spurious correlations from the training dataset. This can result in label-conditional distributions that are imbalanced with respect to another latent attribute. To mitigate this issue, which we call spurious causality of conditional generation, we propose a general two-step strategy. (a) Fairness Intervention (FI): emphasize the minority samples that are hard to generate due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): explicitly filter the generated samples and ensure that they follow the desired latent attribute distribution. We have designed the fairness intervention to work for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results demonstrate that FICS can effectively resolve spurious causality of conditional generation across various datasets.
    
[^160]: 电子商务网站上的情感分析和意见挖掘

    Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15536](http://arxiv.org/abs/2211.15536)

    本论文研究了电子商务网站上的情感分析和意见挖掘，提出了解决情感极性分类挑战的广泛技术，并进行了句子级分类和评论级分类。

    

    情感分析或意见挖掘有助于说明自然语言处理（NLP）中的短语。情感分析是近年来最重要的主题。本研究的目标是解决情感分析中的情感极性分类挑战。我们提出了一种广泛的技术来对情感对立进行分类，并提供了详细的过程解释。通过分析的结果，进行了句子级分类和评论级分类。最后，我们讨论了未来情感分析研究的计划。

    Sentiment analysis or opinion mining help to illustrate the phrase NLP (Natural Language Processing). Sentiment analysis has been the most significant topic in recent years. The goal of this study is to solve the sentiment polarity classification challenges in sentiment analysis. A broad technique for categorizing sentiment opposition is presented, along with comprehensive process explanations. With the results of the analysis, both sentence-level classification and review-level categorization are conducted. Finally, we discuss our plans for future sentiment analysis research.
    
[^161]: TAX-Pose：机器人操作的任务特定跨姿势估计

    TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2211.09325](http://arxiv.org/abs/2211.09325)

    本论文提出了一种名为TAX-Pose的系统，在机器人操作中实现了任务特定跨姿势的估计。通过学习对象之间的对应关系，这种系统能够在给定操作任务的情况下准确估计两个对象之间的跨姿势，并利用估计结果指导下游的运动规划。

    

    我们如何赋予机器人有效地操作未知物体的能力，并基于示范转移相关技能？端到端学习方法通常无法泛化到新的物体或未见过的配置。相反，我们关注交互对象相关部分的任务特定姿势关系。我们推测这种关系是一种可以转移到同一类别新物体的操作任务的可泛化概念；例如，平底锅相对于烤箱的姿势关系或者杯子相对于杯架的姿势关系。我们称这种任务特定姿势关系为“跨姿势”，并提供了该概念的数学定义。我们提出了一个基于视觉的系统，使用学习的对象间对应关系来学习估计给定操作任务的两个对象之间的跨姿势。然后，估计的跨姿势用于引导下游的运动规划器将对象操纵到所需的姿势。

    How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
    
[^162]: 在工作中学习的机器人：人为环境下的自主学习和部署期间的学习

    Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment. (arXiv:2211.08416v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.08416](http://arxiv.org/abs/2211.08416)

    本论文提出了Sirius框架，通过任务分工实现人机协作，部分自主的机器人负责决策工作，人类操作员在需要时进行干预。这种人机团队可以确保复杂任务的安全部署。同时，引入了一种新的学习算法，通过重新加权训练样本来改进策略性能。

    

    随着计算能力的快速增长和深度学习的最新进展，我们在研究环境中见证了新型机器人能力的令人印象深刻的展示。然而，这些学习系统表现出脆弱的泛化能力，并且在实际任务中需要大量训练数据。为了利用最先进的机器人学习模型的能力，同时接受它们的不完美性，我们提出了Sirius，一个为人类和机器人通过分工合作而设计的原则性框架。在这个框架中，部分自主的机器人负责处理大部分决策工作，在这些工作中它们能可靠地工作；与此同时，人类操作员监控这个过程，并在挑战性情况下进行干预。这样的人机团队确保了复杂任务的安全部署。此外，我们引入了一种新的学习算法来改进从任务执行中收集的数据对策略的性能的影响。核心思想是使用近似人类行为的样本对训练样本进行重新加权。

    With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy's performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human
    
[^163]: 年龄预测表现在深部、表浅部和小脑白质连接中有所不同

    Age Prediction Performance Varies Across Deep, Superficial, and Cerebellar White Matter Connections. (arXiv:2211.07398v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.07398](http://arxiv.org/abs/2211.07398)

    本研究通过研究深部、表浅部和小脑白质连接，提出了一种利用深度学习的年龄预测模型，在人类连接组计划的数据集上取得了优于其他方法的结果。研究发现，深部白质对于年龄预测尤为重要。

    

    人脑的白质在人类寿命期间经历发育和退行性过程。为了研究白质解剖区域与年龄之间的关系，我们研究了将扩散磁共振成像轨迹图细分为深部、表浅部和小脑白质的纤维簇。我们提出了一种基于深度学习的年龄预测模型，利用了大卷积核和倒置瓶颈。我们利用新型的离散多方面混合数据增强和一种基于先验知识的损失函数来改善性能，鼓励年龄预测在预期范围内。我们研究了来自人类连接组计划的965名健康年轻成年人（22-37岁）的数据集。实验结果表明，所提出的模型的平均绝对误差为2.59岁，优于其他方法。我们发现在这个队列中，深部白质对于年龄预测最有信息，而表浅部白质对于年龄预测不太有信息。

    The brain's white matter (WM) undergoes developmental and degenerative processes during the human lifespan. To investigate the relationship between WM anatomical regions and age, we study diffusion magnetic resonance imaging tractography that is finely parcellated into fiber clusters in the deep, superficial, and cerebellar WM. We propose a deep-learning-based age prediction model that leverages large convolutional kernels and inverted bottlenecks. We improve performance using novel discrete multi-faceted mix data augmentation and a novel prior-knowledge-based loss function that encourages age predictions in the expected range. We study a dataset of 965 healthy young adults (22-37 years) derived from the Human Connectome Project (HCP). Experimental results demonstrate that the proposed model achieves a mean absolute error of 2.59 years and outperforms compared methods. We find that the deep WM is the most informative for age prediction in this cohort, while the superficial WM is the le
    
[^164]: DriftRec: 将扩散模型应用于盲JPEG恢复

    DriftRec: Adapting diffusion models to blind JPEG restoration. (arXiv:2211.06757v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.06757](http://arxiv.org/abs/2211.06757)

    DriftRec是一种将扩散模型应用于盲JPEG恢复的方法，通过优雅地修改扩散模型的正向随机微分方程，DriftRec能够在高压缩水平下恢复干净图像的分布，避免生成模糊图像，并且不需要关于损坏操作的先验知识，具有广泛的适用性。

    

    本研究利用扩散模型的高保真生成能力，在高压缩水平下解决盲JPEG恢复问题。我们提出了对扩散模型正向随机微分方程的优雅修改，使其适应此恢复任务，并将我们的方法命名为DriftRec。通过将DriftRec与具有相同网络架构的$L_2$回归基线以及两种最先进的JPEG恢复技术进行比较，我们表明我们的方法可以避免其他方法生成模糊图像的倾向，并显著更加真实地恢复了干净图像的分布。为此，只需要一个干净/损坏图像对的数据集，而无需关于损坏操作的任何知识，使得它在其他恢复任务中具有更广泛的适用性。与其他有条件和无条件的扩散模型不同，我们利用了干净图像和损坏图像的分布彼此更接近的观念。

    In this work, we utilize the high-fidelity generation abilities of diffusion models to solve blind JPEG restoration at high compression levels. We propose an elegant modification of the forward stochastic differential equation of diffusion models to adapt them to this restoration task and name our method DriftRec. Comparing DriftRec against an $L_2$ regression baseline with the same network architecture and two state-of-the-art techniques for JPEG restoration, we show that our approach can escape the tendency of other methods to generate blurry images, and recovers the distribution of clean images significantly more faithfully. For this, only a dataset of clean/corrupted image pairs and no knowledge about the corruption operation is required, enabling wider applicability to other restoration tasks. In contrast to other conditional and unconditional diffusion models, we utilize the idea that the distributions of clean and corrupted images are much closer to each other than each is to th
    
[^165]: 关于监督信号的信息量

    On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01407](http://arxiv.org/abs/2211.01407)

    本文使用信息论比较了常用的监督信号对表示学习性能的贡献，并为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。

    

    监督学习通常侧重于从人类标注的训练示例中学习可转移的表示。虽然丰富的注释（如软标签）比稀疏的注释（如硬标签）提供更多信息，但它们的收集成本也更高。我们使用信息论比较了许多常用的监督信号对于表示学习性能的贡献，以及它们的能力如何受到标签数、类别、维度和噪声等因素的影响。我们的框架为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。

    Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati
    
[^166]: FI-ODE: 神经ODE中的可证明鲁棒前不变性

    FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs. (arXiv:2210.16940v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16940](http://arxiv.org/abs/2210.16940)

    本论文提出了一个通用框架，用于在神经ODE中训练和证明鲁棒前不变性，应用于鲁棒连续控制和图像分类，具有非虚假保证。

    

    前不变性是控制理论中长期研究的性质，用于证明动态系统在所有时间内保持在一些预定状态集合内，并且具有鲁棒性保证（例如，在扰动下保持证书有效）。我们提出了一个通用框架，用于训练和可证明证实神经ODE中的鲁棒前不变性。我们在两个场景中应用了这个框架：鲁棒连续控制中的可证明安全性，以及图像分类中的可证明的对抗鲁棒性。据我们所知，这是第一个具有非虚假保证的训练NODE策略的实例。

    Forward invariance is a long-studied property in control theory that is used to certify that a dynamical system stays within some pre-specified set of states for all time, and also admits robustness guarantees (e.g., the certificate holds under perturbations). We propose a general framework for training and provably certifying robust forward invariance in Neural ODEs. We apply this framework in two settings: certified safety in robust continuous control, and certified adversarial robustness for image classification. To our knowledge, this is the first instance of training NODE policies with such non-vacuous certified guarantees.
    
[^167]: 深度子空间编码器用于非线性系统识别

    Deep Subspace Encoders for Nonlinear System Identification. (arXiv:2210.14816v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.14816](http://arxiv.org/abs/2210.14816)

    本文提出了一种使用截断预测损失和子空间编码器进行状态估计的方法，用于解决非线性系统识别中的噪声处理、模型一致性和可靠估计等问题。

    

    使用人工神经网络（ANN）进行非线性系统识别已被证明是一种有希望的方法，但尽管最近的研究努力，许多实际和理论问题仍然存在。具体而言，噪声处理和模型、一致性和可靠估计在最小化预测误差下是最严重的问题。后者伴随着许多实际挑战，例如数据样本数量的计算成本爆炸和优化过程中的不稳定性。在本文中，我们旨在通过提出一种方法来克服这些问题，该方法使用截断预测损失和子空间编码器进行状态估计。通过从时间序列中选择多个截断子段并计算平均预测损失来计算截断预测损失。为了获得一个计算效率高且最小化截断预测损失的估计方法，我们采用了一个子空间编码器来表示。

    Using Artificial Neural Networks (ANN) for nonlinear system identification has proven to be a promising approach, but despite of all recent research efforts, many practical and theoretical problems still remain open. Specifically, noise handling and models, issues of consistency and reliable estimation under minimisation of the prediction error are the most severe problems. The latter comes with numerous practical challenges such as explosion of the computational cost in terms of the number of data samples and the occurrence of instabilities during optimization. In this paper, we aim to overcome these issues by proposing a method which uses a truncated prediction loss and a subspace encoder for state estimation. The truncated prediction loss is computed by selecting multiple truncated subsections from the time series and computing the average prediction loss. To obtain a computationally efficient estimation method that minimizes the truncated prediction loss, a subspace encoder represe
    
[^168]: SignReLU神经网络及其逼近能力

    SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10264](http://arxiv.org/abs/2210.10264)

    本文研究了一种名为SignReLU的不同激活函数对深度神经网络逼近能力的影响，结果表明SignReLU网络在逼近性能方面优于有理数和ReLU网络。

    

    近年来，深度神经网络（DNNs）在科学和技术的各个领域引起了重大关注。激活函数定义了DNN中神经元如何处理输入信号。它们对于学习非线性变换和在连续神经元层之间执行多样化的计算至关重要。近年来，研究者们通过研究DNN的逼近能力来解释其强大和成功。本文通过使用一种名为SignReLU的不同激活函数，探索了DNN的逼近能力。我们的理论结果表明，SignReLU网络在逼近性能方面优于有理数和ReLU网络。数值实验比较了SignReLU与现有的激活函数（如ReLU、Leaky ReLU和ELU），结果显示了SignReLU的竞争实际性能。

    Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.
    
[^169]: 连续学习的独占超掩码子网络训练

    Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.10209](http://arxiv.org/abs/2210.10209)

    本研究提出了一种连续学习方法ExSSNeT，通过独占超掩码子网络训练和KNN-based知识传递，解决了固定权重限制和知识积累问题。

    

    连续学习方法关注在避免灾难性遗忘的同时随着时间累积知识。最近，Wortsman等人提出了一种连续学习方法SupSup，该方法使用一个随机初始化的固定基础网络，并为每个新任务找到一个超掩码，以选择性地保留或移除每个权重以产生一个子网络。他们通过不更新网络权重来避免遗忘。虽然没有遗忘，但SupSup的性能不佳，因为固定权重限制了其表征能力。此外，在学习新任务时，模型内部没有知识的积累或传递。因此，我们提出了ExSSNeT（独占超掩码子网络训练），它进行了独有且不重叠的子网络权重训练，避免了后续任务对共享权重的冲突更新，从而提高性能的同时仍然防止遗忘。此外，我们提出了一种基于KNN的知识传递（KKT）方法。

    Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT)
    
[^170]: 通过对比引导扩散过程改善对抗鲁棒性

    Improving Adversarial Robustness by Contrastive Guided Diffusion Process. (arXiv:2210.09643v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09643](http://arxiv.org/abs/2210.09643)

    该论文提出了一种通过对比引导扩散过程改善对抗鲁棒性的方法，分析了合成数据的优化条件，并强调了提高生成数据可区分性对于改善鲁棒性的重要性。

    

    合成数据生成已成为提高分类任务中对抗鲁棒性的新兴工具，因为鲁棒学习需要比标准分类任务更多的训练样本。在各种深度生成模型中，扩散模型已被证明能产生高质量的合成图像，并在改善对抗鲁棒性方面取得了良好的性能。然而，与其他生成模型相比，扩散型方法在数据生成方面通常较慢。近期虽然提出了不同的加速技术，但研究如何提高生成数据在下游任务中的样本效率也非常重要。在本文中，我们首先分析了合成分布达到非平凡鲁棒准确性的最优条件。我们表明增强生成数据之间的可区分性对于提高对抗鲁棒性至关重要。因此，我们提出了一种对比引导扩散过程来改善样本效率的方法。

    Synthetic data generation has become an emerging tool to help improve the adversarial robustness in classification tasks since robust learning requires a significantly larger amount of training samples compared with standard classification tasks. Among various deep generative models, the diffusion model has been shown to produce high-quality synthetic images and has achieved good performance in improving the adversarial robustness. However, diffusion-type methods are typically slow in data generation as compared with other generative models. Although different acceleration techniques have been proposed recently, it is also of great importance to study how to improve the sample efficiency of generated data for the downstream task. In this paper, we first analyze the optimality condition of synthetic distribution for achieving non-trivial robust accuracy. We show that enhancing the distinguishability among the generated data is critical for improving adversarial robustness. Thus, we prop
    
[^171]: UniTune: 通过在单个图像上微调扩散模型进行文本驱动的图像编辑

    UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image. (arXiv:2210.09477v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09477](http://arxiv.org/abs/2210.09477)

    本文提出了一种新的图像编辑方法UniTune，通过在单个图像上微调扩散模型来将图像生成模型转换为图像编辑模型。UniTune能够实现基于文本的图像编辑，避免了使用编辑掩码和丢失编辑部分细节的问题。

    

    最近，基于文本的图像生成方法取得了令人瞩目的成果，使普通用户能够通过提供文本描述生成高质量的图像。然而，对于编辑现有图像来说，类似的能力仍然无法实现。基于文本驱动的图像编辑方法通常需要编辑掩码，对于需要进行显著视觉变化的编辑有困难，并且无法轻松保留编辑部分的特定细节。在本文中，我们观察到图像生成模型可以通过在单个图像上进行微调来转换为图像编辑模型。我们还展示了在采样前使用对基础图像添加噪声的初始随机采样器，并在采样后插值相关细节，进一步提高了编辑操作的质量。结合这些观察结果，我们提出了一种新颖的图像编辑方法UniTune。UniTune接收任意图像和文本编辑描述作为输入，并执行编辑操作。

    Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit wh
    
[^172]: (1,1)-集群编辑问题是多项式时间可解的

    (1,1)-Cluster Editing is Polynomial-time Solvable. (arXiv:2210.07722v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2210.07722](http://arxiv.org/abs/2210.07722)

    本文解决了Abu-Khzam（2017）关于(1,1)-集群编辑问题多项式时间可解性的猜想，并提供了相应的约简方法和算法。

    

    如果一个图H是一个团图，那么H是一个顶点不交并的团的联合。在2017年，Abu-Khzam引入了(a,d)-集群编辑问题，其中给定一个图G和顶点权重，我们需要决定是否可以通过删除不超过每个顶点v上的d^*(v)条边，并添加不超过每个顶点v上的a^*(v)条边，将G变成一个集群图。之前的研究已经给出了(a,d)-集群编辑问题在P或NP完全性之间的复杂性分界线，唯一未解决的情况是当a=d=1。Abu-Khzam（2017）猜测(1,1)-集群编辑问题是多项式时间可解的。我们通过提供一系列多项式时间约简到度不超过3的C_3和C_4_free图的方法以及设计一个多项式时间算法来解决Abu-Khzam的猜想。

    A graph $H$ is a clique graph if $H$ is a vertex-disjoin union of cliques. Abu-Khzam (2017) introduced the $(a,d)$-{Cluster Editing} problem, where for fixed natural numbers $a,d$, given a graph $G$ and vertex-weights $a^*:\ V(G)\rightarrow \{0,1,\dots, a\}$ and $d^*{}:\ V(G)\rightarrow \{0,1,\dots, d\}$, we are to decide whether $G$ can be turned into a cluster graph by deleting at most $d^*(v)$ edges incident to every $v\in V(G)$ and adding at most $a^*(v)$ edges incident to every $v\in V(G)$. Results by Komusiewicz and Uhlmann (2012) and Abu-Khzam (2017) provided a dichotomy of complexity (in P or NP-complete) of $(a,d)$-{Cluster Editing} for all pairs $a,d$ apart from $a=d=1.$ Abu-Khzam (2017) conjectured that $(1,1)$-{Cluster Editing} is in P. We resolve Abu-Khzam's conjecture in affirmative by (i) providing a serious of five polynomial-time reductions to $C_3$-free and $C_4$-free graphs of maximum degree at most 3, and (ii) designing a polynomial-time algorithm for solving $(1,1)
    
[^173]: IsoVec: 控制词向量空间的相对同构性

    IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.05098](http://arxiv.org/abs/2210.05098)

    这项研究介绍了IsoVec，一种控制词向量空间相对同构性的方法，通过在Skip-gram损失函数中加入全局同构度度量，提高了训练后词向量空间的同构性，进而改善了跨语言映射效果。

    

    从单语言词向量空间提取高质量的翻译词典的能力取决于空间的几何相似性——它们的“同构度”。我们解决了跨语言映射出现问题的根本原因：即词向量训练导致底层空间不同构。我们将同构度的全局度量直接合并到Skip-gram损失函数中，成功地增加了训练后词向量空间的相对同构度，并提高了它们映射到共享的跨语言空间的能力。结果是在一般数据条件下，领域不匹配和训练算法不相似情况下改善了双语词汇表归纳的效果。我们在https://github.com/kellymarchisio/isovec 上发布了IsoVec。

    The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces -- their degree of "isomorphism." We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the underlying spaces being non-isomorphic. We incorporate global measures of isomorphism directly into the Skip-gram loss function, successfully increasing the relative isomorphism of trained word embedding spaces and improving their ability to be mapped to a shared cross-lingual space. The result is improved bilingual lexicon induction in general data conditions, under domain mismatch, and with training algorithm dissimilarities. We release IsoVec at https://github.com/kellymarchisio/isovec.
    
[^174]: EraseNet: 一种用于监督文档清洗的循环残差网络

    EraseNet: A Recurrent Residual Network for Supervised Document Cleaning. (arXiv:2210.00708v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.00708](http://arxiv.org/abs/2210.00708)

    本文介绍了一种名为EraseNet的循环残差网络，用于监督文档清洗。该方法利用全卷积自编码器架构来清洗脏乱文档，并能够恢复具有缺陷的文档以提高光学字符识别系统的性能。

    

    文档去噪被认为是计算机视觉中最具挑战性的任务之一。仍有数百万份待数字化的文档，但是由于自然和人为因素引起的文档退化等问题使得这一任务非常困难。本文介绍了一种使用全新的全卷积自编码器架构来清洗脏乱文档的监督方法。本文重点恢复具有缺陷的文档，例如由于文档老化引起的变形、复印时留下的折痕、随机的黑色斑点、轻微可见的文本等，并且提高图像质量以获得更好的光学字符识别系统(OCR)性能。在文档成为OCR系统的输入之前，从扫描文档中去除噪声是非常重要的一步，因为噪声可能严重影响OCR系统的性能。本文的实验结果表明，该模型能够学习各种常见和不寻常的噪声。

    Document denoising is considered one of the most challenging tasks in computer vision. There exist millions of documents that are still to be digitized, but problems like document degradation due to natural and man-made factors make this task very difficult. This paper introduces a supervised approach for cleaning dirty documents using a new fully convolutional auto-encoder architecture. This paper focuses on restoring documents with discrepancies like deformities caused due to aging of a document, creases left on the pages that were xeroxed, random black patches, lightly visible text, etc., and also improving the quality of the image for better optical character recognition system (OCR) performance. Removing noise from scanned documents is a very important step before the documents as this noise can severely affect the performance of an OCR system. The experiments in this paper have shown promising results as the model is able to learn a variety of ordinary as well as unusual noises a
    
[^175]: 图神经网络的通用Prompt调整方法

    Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15240](http://arxiv.org/abs/2209.15240)

    本文介绍了一种名为Graph Prompt Feature（GPF）的新方法，可通用地调整预先训练过的图神经网络模型，操作于输入的特征空间，能够对应任何形式的Prompt函数。

    

    近年来，Prompt调整在适应预训练模型方面引起了研究热潮。与语言领域采用的统一预训练策略不同，图形领域展示了多样化的预训练策略，设计适当的基于Prompt的图神经网络调整方法面临挑战。本文引入了一种名为Graph Prompt Feature (GPF) 的通用Prompt调整方法，可适用于任何预训练策略下的预训练图神经网络模型。GPF在输入图形的特征空间上操作，理论上可实现与任何形式的Prompt函数等效的效果。因此，我们不再需要明确说明每个预训练策略对应的Prompt函数。相反，我们采用GPF来实现调整。

    In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
    
[^176]: 用于指导疟疾诊断机器学习算法开发的度量标准

    Metrics to guide development of machine learning algorithms for malaria diagnosis. (arXiv:2209.06947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06947](http://arxiv.org/abs/2209.06947)

    该论文讨论了在机器学习算法开发疟疾诊断中指导的度量标准，包括了理解临床需求和任务相关度量标准两个关键因素，为解决当前ML工作忽视临床需求的问题提供了解决方案。

    

    自动化的疟疾诊断对于机器学习（ML）而言是一个困难但高价值的目标，有效的算法可以挽救许多儿童的生命。然而，当前的ML研究大多忽视了关键的用例约束，因此在临床上没有实用性。两个特别关键的因素有助于开发适用于临床现场环境的算法：（i）对ML解决方案必须满足的临床需求有清晰的理解；（ii）用于指导和评估ML模型的任务相关度量标准。忽视这些因素严重阻碍了过去在疟疾上的ML工作，因为由此产生的算法与临床需求不符。在本文中，我们在基于Giemsa染色血液薄膜的自动化疟疾诊断的背景下解决了这两个问题。首先，我们描述了为什么专业知识对于有效应用ML于疟疾诊断至关重要，并列举了提供该领域知识的技术文档和其他资源。其次，我们详细介绍了所需的度量标准以指导和评估ML模型。

    Automated malaria diagnosis is a difficult but high-value target for machine learning (ML), and effective algorithms could save many thousands of children's lives. However, current ML efforts largely neglect crucial use case constraints and are thus not clinically useful. Two factors in particular are crucial to developing algorithms translatable to clinical field settings: (i) Clear understanding of the clinical needs that ML solutions must accommodate; and (ii) task-relevant metrics for guiding and evaluating ML models. Neglect of these factors has seriously hampered past ML work on malaria, because the resulting algorithms do not align with clinical needs.  In this paper we address these two issues in the context of automated malaria diagnosis via microscopy on Giemsa-stained blood films. First, we describe why domain expertise is crucial to effectively apply ML to malaria, and list technical documents and other resources that provide this domain knowledge. Second, we detail perform
    
[^177]: PlaStIL：无需内存的稳定可塑性增量学习

    PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning. (arXiv:2209.06606v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.06606](http://arxiv.org/abs/2209.06606)

    PlaStIL是一种无需内存的增量学习方法，通过在最早的增量状态中使用冻结的特征提取器来保证稳定性，同时使用部分微调模型来引入可塑性，以找到可塑性和稳定性之间的平衡。

    

    在增量学习中，需要同时具备可塑性和稳定性，以便从新数据中学习同时保留过去的知识。由于灾难性遗忘，当没有内存缓冲区可用时，在这两个属性之间找到平衡尤为挑战。主流方法需要存储两个深度模型，因为它们使用知识蒸馏从之前的增量状态进行微调并集成新类。我们提出了一种方法，与这些方法的参数数量相似，但以不同的方式分配参数，以便更好地在可塑性和稳定性之间找到平衡。在初始状态之后，我们冻结特征提取器，这是一种已经被转移学习增量方法采用的方法。最早的增量状态中的类使用冻结的提取器进行训练，以确保稳定性。最近的类别使用部分微调模型进行预测，以引入可塑性。我们提出的可塑性层可以添加到任何转移学习方法中。

    Plasticity and stability are needed in class-incremental learning in order to learn from new data while preserving past knowledge. Due to catastrophic forgetting, finding a compromise between these two properties is particularly challenging when no memory buffer is available. Mainstream methods need to store two deep models since they integrate new classes using fine-tuning with knowledge distillation from the previous incremental state. We propose a method which has similar number of parameters but distributes them differently in order to find a better balance between plasticity and stability. Following an approach already deployed by transfer-based incremental methods, we freeze the feature extractor after the initial state. Classes in the oldest incremental states are trained with this frozen extractor to ensure stability. Recent classes are predicted using partially fine-tuned models in order to introduce plasticity. Our proposed plasticity layer can be incorporated to any transfer
    
[^178]: 使用学习到的MDP同态映射的简单状态-动作抽象方法

    A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06356](http://arxiv.org/abs/2209.06356)

    本论文提出了一种在离散动作空间中构建同态映射的新方法，通过使用环境动力学的部分模型来推断相同状态的状态动作对，从而减小状态-动作空间的大小。

    

    动物能够在有限的经验中迅速推断出等价奖励和转移动力学的状态动作对集合。与此相反，现代强化学习系统必须通过反复试错来学习状态动作对集合的值等价性，这通常需要大量样本。已经提出了MDP同态映射的方法，将环境的观察MDP简化为抽象MDP，可以实现更高效的策略学习。因此，当可以事先构建适当的MDP同态映射时，可以取得令人印象深刻的样本效率改进，通常通过利用环境的对称性来实现。我们提出了一种新颖的方法来构建离散动作空间中的同态映射，该方法使用环境动力学的部分模型来推断哪些状态动作对导致相同的状态，从而减小状态-动作空间的大小。

    Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by
    
[^179]: 通过广义随机优势统计比较分类器

    Statistical Comparisons of Classifiers by Generalized Stochastic Dominance. (arXiv:2209.01857v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.01857](http://arxiv.org/abs/2209.01857)

    这篇论文通过采用决策理论的最新发展，提出了一种基于广义随机优势的分类器比较框架，该框架通过解决易处理的线性规划问题进行操作，并通过适应的两样本观察随机化测试进行统计测试。

    

    尽管成为机器学习算法发展的一个关键问题，但如何在多个数据集上根据多个标准比较分类器仍然没有共识。每个比较框架都面临至少三个基本挑战：质量标准的多样性，数据集的多样性以及数据集的随机选择。本文通过采用最近在决策理论中的发展，为这场激烈的辩论增添了新的视角。基于所谓的偏好系统，我们的框架通过广义随机优势的概念对分类器进行排名，强大地绕过了繁琐且往往自相矛盾的聚合依赖。此外，我们还展示了广义随机优势可以通过解决易处理的线性规划问题进行操作，并通过适应的两样本观察随机化测试进行统计测试。这确实为我们提供了一个强大的框架。

    Although being a crucial question for the development of machine learning algorithms, there is still no consensus on how to compare classifiers over multiple data sets with respect to several criteria. Every comparison framework is confronted with (at least) three fundamental challenges: the multiplicity of quality criteria, the multiplicity of data sets and the randomness of the selection of data sets. In this paper, we add a fresh view to the vivid debate by adopting recent developments in decision theory. Based on so-called preference systems, our framework ranks classifiers by a generalized concept of stochastic dominance, which powerfully circumvents the cumbersome, and often even self-contradictory, reliance on aggregates. Moreover, we show that generalized stochastic dominance can be operationalized by solving easy-to-handle linear programs and moreover statistically tested employing an adapted two-sample observation-randomization test. This yields indeed a powerful framework fo
    
[^180]: 关于在经典数据上建立经典和量子机器学习之间的学习差异

    On establishing learning separations between classical and quantum machine learning with classical data. (arXiv:2208.06339v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.06339](http://arxiv.org/abs/2208.06339)

    本文讨论了在经典数据上找到量子学习算法能够比任何经典学习算法更快学习的问题，并研究如何识别这样的学习问题。研究揭示了定义的微妙变化可以导致概念上显着不同的任务，这可能会导致分离或无分离。此外，通过研究已有的具有证明的量子加速学习问题，提炼出了一组更通用和充分的条件，以展示经典和量子学习者之间的差异。

    

    尽管经过多年努力，量子机器学习领域仅能在某些做作的密码学启发数据集的经典数据情况下展示量子学习的优势。在本文中，我们讨论了寻找量子学习算法可以比任何经典学习算法更快学习的学习问题的挑战，并研究如何识别此类学习问题。具体而言，我们反思了与此问题相关的计算学习理论的主要概念，并讨论了定义上的微妙变化如何意味着概念上显着不同的任务，这些任务可能导致分离或完全无分离。此外，我们研究了现有的具有证明的量子加速的学习问题，以提炼出一组更通用和充分条件（即“检查清单”），以展示学习问题在经典和量子学习器之间的差异。这些检查清单旨在简化对学习问题的方法。

    Despite years of effort, the quantum machine learning community has only been able to show quantum learning advantages for certain contrived cryptography-inspired datasets in the case of classical data. In this note, we discuss the challenges of finding learning problems that quantum learning algorithms can learn much faster than any classical learning algorithm, and we study how to identify such learning problems. Specifically, we reflect on the main concepts in computational learning theory pertaining to this question, and we discuss how subtle changes in definitions can mean conceptually significantly different tasks, which can either lead to a separation or no separation at all. Moreover, we study existing learning problems with a provable quantum speedup to distill sets of more general and sufficient conditions (i.e., ``checklists'') for a learning problem to exhibit a separation between classical and quantum learners. These checklists are intended to streamline one's approach to 
    
[^181]: 无Softmax的线性变换器

    Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.03341](http://arxiv.org/abs/2207.03341)

    这项研究提出了无softmax的线性变换器(SOFT)，用高斯核函数来逼近自注意机制，以改善视觉识别领域中现有方法的局限性。

    

    视觉变换器(ViTs)在视觉感知任务的最新成果中起到了推动作用。ViTs的核心自注意机制在计算和内存使用方面具有二次复杂度。这促使我们开发出在线性复杂度下逼近自注意的方法。然而，本研究的深入分析发现，现有方法在视觉识别方面要么在理论上有缺陷，要么在实践中无效。我们发现它们的局限性来源于在逼近过程中继承了基于softmax的自注意机制，即使用softmax函数对令牌特征向量之间的缩放点积进行归一化。由于存在这个softmax操作，挑战了任何后续的线性化工作。基于这一观点，我们提出了一系列无softmax的变换器(SOFT)。具体而言，我们采用高斯核函数来替代点积相似度，从而实现全自注意矩阵的逼近。

    Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under l
    
[^182]: 随机签到的隐私放大

    Privacy Amplification via Shuffled Check-Ins. (arXiv:2206.03151v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03151](http://arxiv.org/abs/2206.03151)

    本论文研究了一种称为随机签到的分布式计算协议，该协议通过差分隐私实现了严格的隐私保护，并通过隐私放大提高了现有工作的隐私计算。同时，引入了一种数值方法评估分布式环境下通用的洗牌机制，实验证明了该方法的有效性。

    

    我们研究了一种称为随机签到的分布式计算协议，它在没有任何其他信任假设的情况下实现了强隐私保护。与大多数现有工作不同，随机签到允许客户端独立和随机地决定参与计算，消除了服务器发起的子抽样的需要。利用差分隐私，我们证明随机签到通过隐私放大实现了严格的隐私保护，采用了基于Rényi差分隐私的新分析方法，改进了现有工作的隐私计算。我们还引入了一种数值方法来跟踪通用洗牌机制的隐私性能，包括高斯机制，这是文献中首次对分布式环境中局部/洗牌模型下的通用机制进行评估。实证研究也证明了所提出方法的有效性。

    We study a protocol for distributed computation called shuffled check-in, which achieves strong privacy guarantees without requiring any further trust assumptions beyond a trusted shuffler. Unlike most existing work, shuffled check-in allows clients to make independent and random decisions to participate in the computation, removing the need for server-initiated subsampling. Leveraging differential privacy, we show that shuffled check-in achieves tight privacy guarantees through privacy amplification, with a novel analysis based on R{\'e}nyi differential privacy that improves privacy accounting over existing work. We also introduce a numerical approach to track the privacy of generic shuffling mechanisms, including Gaussian mechanism, which is the first evaluation of a generic mechanism under the distributed setting within the local/shuffle model in the literature. Empirical studies are also given to demonstrate the efficacy of the proposed approach.
    
[^183]: 消息传递神经网络对于知识图谱补全真的有帮助吗？

    Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.10652](http://arxiv.org/abs/2205.10652)

    这项研究发现简单的多层感知器（MLP）模型在知识图谱补全任务上能够与消息传递神经网络（MPNNs）相媲美，暗示消息传递可能不像之前认为的那样关键。评分函数和损失函数设计对于模型性能有更大影响。

    

    知识图谱（KG）在各种应用中发挥了重要作用。尽管在创建和维护方面做出了巨大努力，但即使是最大的KG也远未完备。因此，知识图谱补全（KGC）已成为KG研究中最关键的任务之一。最近，这一领域的大量文献都集中在使用消息传递（图）神经网络（MPNNs）来学习强大的嵌入表示。这些方法的成功自然归因于相比于简单的多层感知器（MLP）模型，使用了额外的消息传递（MP）组件的MPNNs。在这项工作中，我们发现令人惊讶的是，简单的MLP模型能够达到与MPNNs相当的性能，这表明MP可能并不像之前认为的那样关键。通过进一步探索，我们展示了仔细的评分函数和损失函数设计对于KGC模型性能有更强的影响。这表明以前的工作中对评分函数设计、损失函数设计和MP的混淆。

    Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, wi
    
[^184]: 区块链网络中的协作学习用于网络攻击检测

    Collaborative Learning for Cyberattack Detection in Blockchain Networks. (arXiv:2203.11076v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2203.11076](http://arxiv.org/abs/2203.11076)

    本文旨在研究入侵攻击并为区块链网络开发一种新颖的网络攻击检测框架。通过实验室合成的网络攻击数据集，提出了一种协作学习模型，允许区块链节点共享学习知识以检测攻击，并增强整个区块链网络的安全性。

    

    本文旨在研究入侵攻击，并为区块链网络开发一种新颖的网络攻击检测框架。首先，在实验室中设计和实现一个区块链网络，该网络将用于生成真实的流量数据（包括正常数据和攻击数据），以用于我们的学习模型，并进行实时实验，评估我们提出的入侵检测框架的性能。据我们所知，这是第一个在实验室中合成的用于区块链网络的网络攻击数据集。然后，我们提出了一种新颖的协作学习模型，可以在区块链网络中实现高效部署以检测攻击。所提出的学习模型的主要思想是使区块链节点能够主动收集数据，共享从其数据中学到的知识，并与网络中的其他区块链节点交换知识。通过这种方式，我们不仅能够利用其他节点的知识进行攻击检测，还能够增强整个区块链网络的安全性。

    This article aims to study intrusion attacks and then develop a novel cyberattack detection framework for blockchain networks. Specifically, we first design and implement a blockchain network in our laboratory. This blockchain network will serve two purposes, i.e., to generate the real traffic data (including both normal data and attack data) for our learning models and implement real-time experiments to evaluate the performance of our proposed intrusion detection framework. To the best of our knowledge, this is the first dataset that is synthesized in a laboratory for cyberattacks in a blockchain network. We then propose a novel collaborative learning model that allows efficient deployment in the blockchain network to detect attacks. The main idea of the proposed learning model is to enable blockchain nodes to actively collect data, share the knowledge learned from its data, and then exchange the knowledge with other blockchain nodes in the network. In this way, we can not only levera
    
[^185]: 用于简单遗憾最小化的元学习

    Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12888](http://arxiv.org/abs/2202.12888)

    本论文提出了用于在赌博机中进行简单遗憾最小化的元学习框架，并提出了首个贝叶斯和频率派元学习算法。贝叶斯算法具有先验分布并且具有较小的元简单遗憾，而频率派算法更通用且可以在更多的设置中进行分析。通过将算法应用于不同的赌博机问题，我们验证了理论的有效性。

    

    我们提出了一个用于在赌博机中简单遗憾最小化的元学习框架。在这个框架中，学习代理与一系列赌博机任务进行交互，这些任务是从一个未知的先验分布中独立采样的，并学习其元参数以在未来任务中表现更好。我们提出了这个设置的第一个贝叶斯和频率派元学习算法。贝叶斯算法可以访问元参数的先验分布，并且其在$m$个赌博机任务中，时间界为$n$的元简单遗憾仅为$\tilde{O}(m / \sqrt{n})$。另一方面，频率派算法的元简单遗憾为$\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$。尽管遗憾更大，但频率派算法更通用，因为它不需要元参数的先验分布，并且可以在更多的设置中进行分析。我们通过将算法应用于几类赌博机问题来验证我们的理论。

    We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\tilde{O}(m / \sqrt{n})$. On the other hand, the meta simple regret of the frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$. While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them
    
[^186]: 超越单一模型的持续学习

    Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09826](http://arxiv.org/abs/2202.09826)

    本论文研究了超越单一模型的持续学习。通过使用集成模型，能够改善持续性能，但随着模型数量增加，计算成本也会增加。为了解决这个问题，提出了一种计算成本较低的算法，能够在运行时间上与单一模型相当，并享有集成的性能优势。

    

    在持续学习中，越来越多的研究集中在灾难性遗忘问题上。虽然有很多方法试图缓解这个问题，但大多数方法都假设在持续学习中只有一个模型。在这项工作中，我们质疑这个假设，并展示了使用集成模型可以是一个简单而有效的方法来提高持续性能。然而，随着模型数量的增加，集成的训练和推断成本可能会显著增加。受到这个限制的启发，我们研究了不同的集成模型，以了解它们在持续学习场景中的优点和缺点。最后，为了克服集成的高计算成本，我们利用神经网络子空间的最新进展，提出了一种计算成本较低的算法，其运行时间与单一模型相当，但享有集成的性能优势。

    A growing body of research in continual learning focuses on the catastrophic forgetting problem. While many attempts have been made to alleviate this problem, the majority of the methods assume a single model in the continual learning setup. In this work, we question this assumption and show that employing ensemble models can be a simple yet effective method to improve continual performance. However, ensembles' training and inference costs can increase significantly as the number of models grows. Motivated by this limitation, we study different ensemble models to understand their benefits and drawbacks in continual learning scenarios. Finally, to overcome the high compute cost of ensembles, we leverage recent advances in neural network subspace to propose a computationally cheap algorithm with similar runtime to a single model yet enjoying the performance benefits of ensembles.
    
[^187]: SAITS: 基于自注意力机制的时间序列插值方法

    SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08516](http://arxiv.org/abs/2202.08516)

    SAITS是一种基于自注意力机制的多元时间序列缺失值插值方法，通过两个对角线掩码自注意力块的加权组合，能够明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。

    

    时间序列中的缺失数据常常成为深入分析的障碍。插值是一种常见的解决方法，其核心问题是如何确定缺失值。本文提出了一种新颖的基于自注意力机制的多元时间序列缺失值插值方法——SAITS。通过联合优化的方式训练，SAITS通过两个对角线掩码自注意力块的加权组合来学习缺失值。对角线掩码自注意力块可以明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。同时，加权组合设计使得SAITS能够根据注意力图和缺失信息动态地分配来自两个对角线掩码自注意力块的学习表示的权重。广泛的实验证明，SAITS在时间序列插值方面优于现有的方法。

    Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-
    
[^188]: 《组合性作为词汇对称性》

    Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.12926](http://arxiv.org/abs/2201.12926)

    本文将组合性定义为对数据分布的对称性约束，而不是模型，通过自动发现数据转换并应用于训练数据，提高模型的组合归纳偏置。

    

    在语义解析、指令遵循和问题回答等任务中，标准的深度网络在从小数据集中进行组合泛化时会失败。许多现有方法通过强制实施句子解释的组合过程的模型架构来克服这个限制。本文提出了一个领域通用和模型无关的组合性形式，将其作为数据分布的对称性约束而不是模型。我们证明了，无论何时一个任务可以通过一个组合模型来解决，都存在一个相应的数据增强方案——将示例转换为其他合适示例的过程——可以为解决相同任务的任何训练模型赋予组合归纳偏置。我们描述了一个自动发现这些转换并将其应用于普通神经序列模型训练数据的过程，称为LEXSYM。与现有的组合数据增强过程不同，LEXSYM可以被快速部署。

    In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deplo
    
[^189]: 一种使用样本矩方法的非传统参数化密度估计

    A Non-Classical Parameterization for Density Estimation Using Sample Moments. (arXiv:2201.04786v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.04786](http://arxiv.org/abs/2201.04786)

    本文提出了一种使用样本矩方法进行非传统参数化密度估计的方法，通过平方Hellinger距离进行参数化，并通过凸优化得到唯一解。通过幂矩估计提出了估计器的统计性质和渐近误差上界，模拟结果验证了该方法的性能。

    

    概率密度估计是统计学和信号处理的核心问题。矩方法是密度估计的重要方法，但通常强烈依赖于可行函数的选择，这严重影响了性能。在本文中，我们提出了一种使用样本矩方法进行密度估计的非传统参数化方法，不需要选择这样的函数。该参数化是由平方Hellinger距离引起的，并且它的解被证明在不依赖于数据的简单先验条件下存在并且是唯一的，并且可以通过凸优化得到。通过幂矩估计提出了密度估计器的统计性质，以及估计器的渐近误差上界。给出了所提出的密度估计器在信号处理任务中的应用。通过与几种流行方法的比较，模拟结果验证了估计器的性能。

    Probability density estimation is a core problem of statistics and signal processing. Moment methods are an important means of density estimation, but they are generally strongly dependent on the choice of feasible functions, which severely affects the performance. In this paper, we propose a non-classical parametrization for density estimation using sample moments, which does not require the choice of such functions. The parametrization is induced by the squared Hellinger distance, and the solution of it, which is proved to exist and be unique subject to a simple prior that does not depend on data, and can be obtained by convex optimization. Statistical properties of the density estimator, together with an asymptotic error upper bound are proposed for the estimator by power moments. Applications of the proposed density estimator in signal processing tasks are given. Simulation results validate the performance of the estimator by a comparison to several prevailing methods. To the best 
    
[^190]: 通过多目标神经架构搜索学习可解释模型

    Learning Interpretable Models Through Multi-Objective Neural Architecture Search. (arXiv:2112.08645v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08645](http://arxiv.org/abs/2112.08645)

    本研究提出了一种多目标分布式神经架构搜索框架，旨在优化深度神经网络的任务性能和可解释性。利用非支配排序遗传算法（NSGA-II）和可解释人工智能（XAI）技术，奖励那些可以被领域专家更好理解的架构。

    

    深度学习的巨大进展在各个领域取得了前所未有的成就。尽管深度神经网络的性能无可置疑，但其架构设计和可解释性却并非易事。为了自动化神经网络架构设计，引入了神经架构搜索（NAS）的研究。最近的进展通过利用分布式计算和新颖的优化算法使这些方法更为实用。然而，在优化可解释性方面的研究仍相对较少。为此，我们提出了一个多目标分布式NAS框架，旨在优化任务性能和“可视识别性”，这是解释性的一种代理度量。我们利用非支配排序遗传算法（NSGA-II）和可解释人工智能（XAI）技术，奖励那些可以被领域专家更好理解的架构。该框架在几个图像分类数据集上进行了评估。

    Monumental advances in deep learning have led to unprecedented achievements across various domains. While the performance of deep neural networks is indubitable, the architectural design and interpretability of such models are nontrivial. Research has been introduced to automate the design of neural network architectures through neural architecture search (NAS). Recent progress has made these methods more pragmatic by exploiting distributed computation and novel optimization algorithms. However, there is little work in optimizing architectures for interpretability. To this end, we propose a multi-objective distributed NAS framework that optimizes for both task performance and "introspectability," a surrogate metric for aspects of interpretability. We leverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable AI (XAI) techniques to reward architectures that can be better comprehended by domain experts. The framework is evaluated on several image classification datase
    
[^191]: 以火攻敌：通过生成偏见转化进行对比去偏执，无需无偏样本

    Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation. (arXiv:2112.01021v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.01021](http://arxiv.org/abs/2112.01021)

    本文提出了一种无需明确偏见标签或无偏样本的新方法，通过生成偏见转化，使用图像翻译模型将一个偏见模式转换成另一个，同时保留任务相关信息，以实现对比去偏执。

    

    深度神经网络（DNN）虽然能够以超过容量的网络进行泛化，但在判别任务中常常依赖恶性偏见作为近道，而不是任务相关信息。为了解决这个问题，最近的研究利用与偏见相关的辅助信息，但这在实践中很难获得，或者筛选出少量的无偏样本进行去偏执。然而，这些方法的成功并不总是能得到保证，因为假设不一定能满足。在本文中，我们提出了一种新颖的方法，即通过生成偏见转换的对比去偏执（CDvG），它在没有明确的偏见标签或无偏样本的情况下工作。受到我们的观察启发，不仅判别模型，而且图像翻译模型也倾向于关注恶性偏见，CDvG利用图像翻译模型将一个偏见模式转换为另一个偏见模式，同时保留任务相关信息。此外，偏见转换的视图相互进行对比。

    Deep neural networks (DNNs), despite their impressive ability to generalize over-capacity networks, often rely heavily on malignant bias as shortcuts instead of task-related information for discriminative tasks. To address this problem, recent studies utilize auxiliary information related to the bias, which is rarely obtainable in practice, or sift through a handful of bias-free samples for debiasing. However, the success of these methods is not always guaranteed due to the unfulfilled presumptions. In this paper, we propose a novel method, Contrastive Debiasing via Generative Bias-transformation (CDvG), which works without explicit bias labels or bias-free samples. Motivated by our observation that not only discriminative models but also image translation models tend to focus on the malignant bias, CDvG employs an image translation model to transform one bias mode into another while preserving the task-relevant information. Additionally, the bias-transformed views are set against each
    
[^192]: 边际的魔鬼：基于边际的标签平滑用于网络校准

    The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.15430](http://arxiv.org/abs/2111.15430)

    这篇论文研究了深度神经网络的校准问题，发现传统的交叉熵损失会导致过度自信的预测。通过引入最大熵预测的损失函数，可以提高网络的校准性能。

    

    尽管深度神经网络有着卓越的性能，但最近的研究表明它们往往校准不良，导致过于自信的预测。过拟合会加剧校准不良现象，因为在训练过程中，交叉熵的最小化会促使预测的 softmax 概率与独热标签分配相匹配，从而使得正确类别的预 softmax 激活远大于其他激活。最近的文献证据表明，蕴含隐式或显式最大熵预测的损失函数能够实现最先进的校准性能。我们提供了目前最先进校准损失的统一约束最优化观点。具体而言，这些损失可以被视为对位势距离施加相等约束的线性惩罚（或拉格朗日函数）的近似。这指出了这种底层相等约束的重要局限性。

    In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscalibration can be exacerbated by overfitting due to the minimization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot label assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constr
    
[^193]: 从过参数化到可处理性: 负感知机的例子

    Tractability from overparametrization: The example of the negative perceptron. (arXiv:2110.15824v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15824](http://arxiv.org/abs/2110.15824)

    本研究考虑了负感知机问题，通过两个随机模型研究了在大数据条件下最大负间隔的上下界。

    

    在负感知机问题中，我们给定了$n$个数据点$({\boldsymbol x}_i,y_i)$，其中${\boldsymbol x}_i$是一个$d$维向量，$y_i\in\{+1,-1\}$是一个二进制标签。数据不是线性可分的，因此我们满足于找到具有最大\emph{负}间隔的线性分类器。换句话说，我们希望找到一个单位范数向量${\boldsymbol \theta}$，使得$\min_{i\le n}y_i\langle {\boldsymbol \theta},{\boldsymbol x}_i\rangle$最大化。这是一个非凸优化问题(相当于在一个多面体中找到最大范数向量)，我们研究了在两个随机模型下该问题的典型性质。我们考虑当$n,d\to \infty$且$n/d\to\delta$时的比例渐近性，并证明了最大间隔$\kappa_{\text{s}}(\delta)$或等价地其逆函数$\delta_{\text{s}}(\kappa)$的上下界。换句话说，$\delta_{\text{s}}(\kappa)$是过参数化阈值的一个度量。

    In the negative perceptron problem we are given $n$ data points $({\boldsymbol x}_i,y_i)$, where ${\boldsymbol x}_i$ is a $d$-dimensional vector and $y_i\in\{+1,-1\}$ is a binary label. The data are not linearly separable and hence we content ourselves to find a linear classifier with the largest possible \emph{negative} margin. In other words, we want to find a unit norm vector ${\boldsymbol \theta}$ that maximizes $\min_{i\le n}y_i\langle {\boldsymbol \theta},{\boldsymbol x}_i\rangle$. This is a non-convex optimization problem (it is equivalent to finding a maximum norm vector in a polytope), and we study its typical properties under two random models for the data.  We consider the proportional asymptotics in which $n,d\to \infty$ with $n/d\to\delta$, and prove upper and lower bounds on the maximum margin $\kappa_{\text{s}}(\delta)$ or -- equivalently -- on its inverse function $\delta_{\text{s}}(\kappa)$. In other words, $\delta_{\text{s}}(\kappa)$ is the overparametrization thresho
    
[^194]: 关于随机梯度下降方法的不同自适应批量大小选择策略的等价性研究

    On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods. (arXiv:2109.10933v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2109.10933](http://arxiv.org/abs/2109.10933)

    本研究证明了在特定选择的Θ和ν下，范数测试和内积/正交性测试在随机梯度下降方法的收敛速度方面是等价的，同时指出在最理想情况下，内积/正交性测试可以像范数测试一样廉价。

    

    本研究证明了在特定选择的Θ和ν下，\cite{Bol18}中提出的范数测试和内积/正交性测试在随机梯度下降方法的收敛速度方面是等价的，其中ϵ²=θ²+ν²。这里，ϵ控制梯度范数的相对统计误差，而θ和ν分别控制梯度在梯度方向和梯度正交方向上的相对统计误差。此外，我们证明了在最理想情况下，内积/正交性测试可以像范数测试一样廉价，如果θ和ν被最优选择，但是如果ϵ²=θ²+ν²，内积/正交性测试永远不会比范数测试更具计算可承受性。最后，我们提供了两个随机优化问题来说明我们的结果。

    In this study, we demonstrate that the norm test and inner product/orthogonality test presented in \cite{Bol18} are equivalent in terms of the convergence rates associated with Stochastic Gradient Descent (SGD) methods if $\epsilon^2=\theta^2+\nu^2$ with specific choices of $\theta$ and $\nu$. Here, $\epsilon$ controls the relative statistical error of the norm of the gradient while $\theta$ and $\nu$ control the relative statistical error of the gradient in the direction of the gradient and in the direction orthogonal to the gradient, respectively. Furthermore, we demonstrate that the inner product/orthogonality test can be as inexpensive as the norm test in the best case scenario if $\theta$ and $\nu$ are optimally selected, but the inner product/orthogonality test will never be more computationally affordable than the norm test if $\epsilon^2=\theta^2+\nu^2$. Finally, we present two stochastic optimization problems to illustrate our results.
    
[^195]: 批量强化学习中被动数据采集的诅咒

    The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.09973](http://arxiv.org/abs/2106.09973)

    本文研究了批量强化学习中被动数据采集的代价问题，并发现与主动数据采集相比，被动采集的样本复杂性呈指数级增加。

    

    在高风险应用中，主动实验可能被认为风险太大，因此通常会被动采集数据。虽然在简单情况下，如在赌博机中，被动和主动数据采集的效果相似，但在从带有可控状态的系统中收集数据时，被动采样的代价可能会更高。本文的主要重点是对这种代价的特征化。例如，在具有$\mathrm{S}$个状态和$\mathrm{A}$个动作的离散状态-动作马尔可夫决策过程(MDP)中学习时，我们展示了即使使用最佳（但被动选择的）日志记录策略，也需要（且足够）获得$\epsilon$-最优策略的$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$个回合，其中$H$是回合长度。请注意，这表明与主动数据采集相比，样本复杂性呈指数级增加，这个结果是可以预料的，但据我们所知，尚未发表。

    In high stake applications, active experimentation may be considered too risky and thus data are often collected passively. While in simple cases, such as in bandits, passive and active data collection are similarly effective, the price of passive sampling can be much higher when collecting data from a system with controlled states. The main focus of the current paper is the characterization of this price. For example, when learning in episodic finite state-action Markov decision processes (MDPs) with $\mathrm{S}$ states and $\mathrm{A}$ actions, we show that even with the best (but passively chosen) logging policy, $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$ episodes are necessary (and sufficient) to obtain an $\epsilon$-optimal policy, where $H$ is the length of episodes. Note that this shows that the sample complexity blows up exponentially compared to the case of active data collection, a result which is not unexpected, but, as far as we know, have not been published
    
[^196]: 结构化赌博机中的固定预算最佳臂识别

    Fixed-Budget Best-Arm Identification in Structured Bandits. (arXiv:2106.04763v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04763](http://arxiv.org/abs/2106.04763)

    本论文提出了一个通用易处理的算法，用于解决固定预算下的结构化赌博机最佳臂识别问题。通过从联合广义模型中基于均值奖励估计逐步消除次优臂，该算法具有在线性模型和广义线性模型中竞争力的错误保证，并且在GLM中是第一个具有固定预算BAI分析的实用算法。

    

    在固定预算设置下的最佳臂识别（BAI）是一个赌博机问题，学习代理在固定观测次数后最大化识别出最优（最佳）臂的概率。大多数关于该主题的研究都是针对非结构化问题，并且臂的数量较少，这限制了它们的适用性。我们提出了一种通用易处理的算法，通过从联合广义模型中基于均值奖励估计逐步消除次优臂来引入结构化。我们在线性模型和广义线性模型（GLM）中分析了我们的算法，并提出了基于G-最优设计的实际实现方法。在线性模型中，我们的算法在错误保证方面与先前的工作相比具有竞争力，并在经验上至少表现得不错。在GLM中，这是第一个具有固定预算BAI分析的实用算法。

    Best-arm identification (BAI) in a fixed-budget setting is a bandit problem where the learning agent maximizes the probability of identifying the optimal (best) arm after a fixed number of observations. Most works on this topic study unstructured problems with a small number of arms, which limits their applicability. We propose a general tractable algorithm that incorporates the structure, by successively eliminating suboptimal arms based on their mean reward estimates from a joint generalization model. We analyze our algorithm in linear and generalized linear models (GLMs), and propose a practical implementation based on a G-optimal design. In linear models, our algorithm has competitive error guarantees to prior works and performs at least as well empirically. In GLMs, this is the first practical algorithm with analysis for fixed-budget BAI.
    
[^197]: 广播残差学习用于高效关键词检测

    Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2106.04140](http://arxiv.org/abs/2106.04140)

    本研究提出了一种广播残差学习方法，用于高效关键词检测。通过配置大部分残差函数为1D时域卷积，并使用广播残差连接将时域输出扩展到频率-时域维度，该方法能够以小模型尺寸和计算负载实现高准确性。此外，还提出了基于广播残差学习的广播残差网络（BC-ResNet），并介绍了如何根据目标设备的资源来扩展模型。

    

    关键词检测是一个重要的研究领域，因为它在智能设备的唤醒和用户交互中起着关键作用。然而，在资源有限的设备（如手机）上高效运行的同时最小化错误是具有挑战性的。我们提出了一种广播残差学习方法，以实现小模型尺寸和计算负载下的高准确性。我们的方法将大部分残差函数配置为1D时域卷积，同时使用广播残差连接将时域输出扩展到频率-时域维度，使网络能够有效地表示有用的音频特征，计算量比传统的卷积神经网络要少得多。我们还提出了一种新颖的网络架构，广播残差网络（BC-ResNet），基于广播残差学习，并描述了如何根据目标设备的资源来扩展模型。

    Keyword spotting is an important research field because it plays a key role in device wake-up and user interaction on smart devices. However, it is challenging to minimize errors while operating efficiently in devices with limited resources such as mobile phones. We present a broadcasted residual learning method to achieve high accuracy with small model size and computational load. Our method configures most of the residual functions as 1D temporal convolution while still allows 2D convolution together using a broadcasted-residual connection that expands temporal output to frequency-temporal dimension. This residual mapping enables the network to effectively represent useful audio features with much less computation than conventional convolutional neural networks. We also propose a novel network architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted residual learning and describe how to scale up the model according to the target device's resources. BC-ResNets ach
    
[^198]: SCEI: 一种用于物联网系统的智能合约驱动的边缘智能框架

    SCEI: A Smart-Contract Driven Edge Intelligence Framework for IoT Systems. (arXiv:2103.07050v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.07050](http://arxiv.org/abs/2103.07050)

    本文提出了一种基于区块链和联邦学习的智能合约驱动的边缘智能框架，用于解决联邦学习在处理非iid数据集和信任问题上的挑战。创新的智能合约允许边缘设备达成一致，确定个性化模型的最佳权重。

    

    联邦学习（FL）通过在边缘设备上进行协作训练共享模型，同时保持数据隐私。FL在处理独立同分布（iid）数据集时非常有效，但在处理非iid数据集时很困难。已经提出了各种个性化方法，但这些方法无法处理数据分布偏斜等数据分布变化，在现实场景中经常出现（例如，智能交通系统中的驾驶行为随时间和位置的变化）。此外，陌生设备之间的信任问题和集中式聚合器的安全问题也带来了额外的挑战。为了解决这些挑战，本文提出了一种基于区块链和联邦学习的动态优化个性化深度学习方案。具体来说，区块链中实现的创新智能合约使分布式边缘设备能够就个性化模型的最佳权重达成一致。

    Federated learning (FL) enables collaborative training of a shared model on edge devices while maintaining data privacy. FL is effective when dealing with independent and identically distributed (iid) datasets, but struggles with non-iid datasets. Various personalized approaches have been proposed, but such approaches fail to handle underlying shifts in data distribution, such as data distribution skew commonly observed in real-world scenarios (e.g., driver behavior in smart transportation systems changing across time and location). Additionally, trust concerns among unacquainted devices and security concerns with the centralized aggregator pose additional challenges. To address these challenges, this paper presents a dynamically optimized personal deep learning scheme based on blockchain and federated learning. Specifically, the innovative smart contract implemented in the blockchain allows distributed edge devices to reach a consensus on the optimal weights of personalized models. Ex
    
[^199]: 通过一步式绳索多目标学习处理噪声标签及其在幽门螺杆菌分割中的应用

    Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.14956](http://arxiv.org/abs/2011.14956)

    本文研究了处理噪声标签的新方法，特别针对医学组织病理学图像分析中的困难情况。通过一步式绳索多目标学习，该方法克服了标签中存在的复杂噪声和评估策略不明确的问题。

    

    由于在许多现实场景中缺乏准确的地面实况标签，因此从噪声标签中学习是一个重要问题。在实践中，针对这个问题的不同方法首先对可能有噪声标签的实例进行一些纠正，然后用纠正信息更新预测模型。然而，在医学组织病理学全切片图像分析（MHWSIA）等特定领域中，专家往往难以或甚至无法手动实现无噪声的地面实况标签，导致标签存在复杂噪声。这种情况引发了两个更加困难的问题：1）由于标签中存在复杂噪声，先前方法纠正可能有噪声标签的实例的方法学存在局限性；2）由于收集无噪声地面实况标签非常困难，验证/测试的适当评估策略不明确。本文重点研究了缓解以上问题的方法。

    Learning from noisy labels is an important concern because of the lack of accurate ground-truth labels in plenty of real-world scenarios. In practice, various approaches for this concern first make some corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or even impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. In this paper, we focus on allevia
    
[^200]: 集成知识蒸馏用于CTR预测

    Ensemble Knowledge Distillation for CTR Prediction. (arXiv:2011.04106v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.04106](http://arxiv.org/abs/2011.04106)

    本论文针对CTR预测提出了一种集成知识蒸馏的模型训练策略，该策略能够通过教师模型将知识传输给学生模型，并取得了显著的准确性提升。

    

    最近，基于深度学习的模型在点击率（CTR）预测方面得到了广泛的研究，并在许多工业应用中提高了预测准确性。然而，目前的研究主要集中在构建复杂的网络架构来更好地捕捉复杂的特征交互和动态用户行为。增加的模型复杂性可能会减慢在线推断速度，并阻碍其在实时应用中的应用。相反，我们的工作针对的是一种基于知识蒸馏（KD）的新模型训练策略。KD是一种将来自教师模型的知识转移给学生模型的教师-学生学习框架。知识蒸馏策略不仅允许我们将学生模型简化为传统的神经网络模型，而且还在准确性上显著提高了超越最先进的教师模型。这些优点促使我们进一步探索使用强大的教师集合来进行更准确的学生模型训练。

    Recently, deep learning-based models have been widely studied for click-through rate (CTR) prediction and lead to improved prediction accuracy in many industrial applications. However, current research focuses primarily on building complex network architectures to better capture sophisticated feature interactions and dynamic user behaviors. The increased model complexity may slow down online inference and hinder its adoption in real-time applications. Instead, our work targets at a new model training strategy based on knowledge distillation (KD). KD is a teacher-student learning framework to transfer knowledge learned from a teacher model to a student model. The KD strategy not only allows us to simplify the student model as a vanilla DNN model but also achieves significant accuracy improvements over the state-of-the-art teacher models. The benefits thus motivate us to further explore the use of a powerful ensemble of teachers for more accurate student model training. We also propose s
    
[^201]: 深度强化学习中的迁移学习综述

    Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.07888](http://arxiv.org/abs/2009.07888)

    这篇综述调查了深度强化学习领域中的迁移学习方法的最新进展，并提供了一个对这些方法进行分类的框架。分析了它们的目标、方法学、兼容的强化学习背景以及实际应用，并探讨了迁移学习与其他相关主题之间的联系。

    

    强化学习是解决序列决策问题的学习范式。近年来，随着深度神经网络的快速发展，强化学习取得了显著的进展。除了在机器人和游戏等诸多领域中具有良好前景的强化学习，迁移学习作为一种解决强化学习面临的各种挑战的方法已经出现，通过从外部专业知识中转移知识，以提高学习过程的效率和效果。在这项综述中，我们系统地调查了深度强化学习领域中的迁移学习方法的最新进展。具体而言，我们提供了一个对最先进的迁移学习方法进行分类的框架，在此框架下分析了它们的目标、方法学、兼容的强化学习背景以及实际应用。我们还探讨了迁移学习与其他相关主题之间的联系。

    Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
    
[^202]: 三维点云分割的交叉形状注意力

    Cross-Shape Attention for Part Segmentation of 3D Point Clouds. (arXiv:2003.09053v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2003.09053](http://arxiv.org/abs/2003.09053)

    本文提出了一种新方法，在形状集合中通过交叉形状注意力机制来实现三维点云分割，通过评估点之间的交互程度和介导特征传播来提升结果精度和一致性。

    

    我们提出了一种深度学习方法，通过在形状集合中传递逐点特征表示来进行三维形状分割。我们提出了一种交叉形状注意力机制，以使一个形状的逐点特征与其他形状的逐点特征产生相互作用。该机制评估点之间的交互程度并在形状之间介导特征传播，从而改善了用于形状分割的结果的点逐点特征表示的精度和一致性。我们的方法还提出了一种形状检索度量，以选择适合每个测试形状的交叉形状注意力操作的形状。我们的实验证明，我们的方法在流行的PartNet数据集上实现了最先进的结果。

    We present a deep learning method that propagates point-wise feature representations across shapes within a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses both the degree of interaction between points and also mediates feature propagation across shapes, improving the accuracy and consistency of the resulting point-wise feature representations for shape segmentation. Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art results in the popular PartNet dataset.
    
[^203]: 基因组学的深度学习: 一个简洁的概述

    Deep Learning for Genomics: A Concise Overview. (arXiv:1802.00810v3 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/1802.00810](http://arxiv.org/abs/1802.00810)

    深度学习在基因组学领域面临独特的挑战，但通过深入了解任务需求，并与适当的深度架构相匹配，深度学习在基因组学中取得了成功。

    

    高通量测序等基因组研究的进展已将现代基因组学推向了"大数据"学科。这种数据爆炸不断挑战传统基因组学方法的使用。与强大算法的紧急需求相平行的是，深度学习在视觉、语音和文本处理等领域取得了成功。然而，对于基因组学来说，深度学习面临着独特的挑战，因为我们期望深度学习能够超出我们的知识探索基因组的解读。一个强大的深度学习模型应该依赖于对特定任务的深入了解。在本文中，我们简要讨论了从基因组学角度看不同深度学习模型的优势，以便将每个特定任务与适当的深度架构相匹配，并对发展现代基因组学深度学习架构的实践考虑进行了评述。我们还提供了深度学习在基因组学中的应用的简明回顾。

    Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into "big data" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applicati
    
[^204]: 生成对抗性训练器：使用生成对抗网络防御对抗扰动

    Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN. (arXiv:1705.03387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1705.03387](http://arxiv.org/abs/1705.03387)

    本论文提出了一种使用生成对抗网络来防御对抗扰动的新方法，通过交替训练分类器和生成器网络，生成器网络生成对抗扰动以欺骗分类器网络，同时分类器网络被训练以正确分类原始和对抗图像，这一过程使分类器网络对对抗扰动更加鲁棒，同时还能有效地降低网络的过拟合问题。

    

    我们提出了一种新颖的技术，使用生成对抗网络使神经网络对对抗性示例具有鲁棒性。我们交替训练分类器和生成器网络。生成器网络通过使用每个图像的梯度生成对抗扰动，从而轻松欺骗分类器网络。同时，分类器网络被训练以正确分类由生成器生成的原始和对抗图像。这些过程有助于使分类器网络对对抗扰动更加鲁棒。此外，我们的对抗训练框架有效地减少了过拟合，并且优于其他正则化方法，如Dropout。我们将我们的方法应用于CIFAR数据集的有监督学习中，实验结果表明我们的方法显著降低了网络的泛化误差。据我们所知，这是第一个使用生成对抗网络来改进监督学习的方法。

    We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.
    

