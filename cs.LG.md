# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Real-Time Radiance Fields for Single-Image Portrait View Synthesis.](http://arxiv.org/abs/2305.02310) | 该论文提出了一种适用于单张图片的实时辐射场合成方法，能够从单张未经过姿势调整的图像中推断和渲染出逼真的3D表示，并产生高质量的3D感知人像合成结果。 |
| [^2] | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages.](http://arxiv.org/abs/2305.02309) | 本研究旨在提高LLMs编程合成训练的效率，通过统一模型架构、学习方法、填充采样和数据分布，来提高训练模型的泛化能力。 |
| [^3] | [Calibrated Explanations: with Uncertainty Information and Counterfactuals.](http://arxiv.org/abs/2305.02305) | 该论文提出了一种新的特征重要性解释方法，Calibrated Explanations (CE)，它可以提供准确、稳定的解释，并且可以为概率估计和特征重要性权重提供不确定性量化信息，是一种快速、可靠且强健的解释方法。 |
| [^4] | [New Equivalences Between Interpolation and SVMs: Kernels and Structured Features.](http://arxiv.org/abs/2305.02304) | 本文提出了一种新的、灵活的分析框架，用于证明在任意再生核希尔伯特空间中特征的SVP条件 |
| [^5] | [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.](http://arxiv.org/abs/2305.02301) | 本研究提出了Distilling Step-by-Step机制，通过提取LLM基础信息为小型模型提供额外的监督训练，从而使它们胜过更大的LLM模型，并需更少的训练数据。 |
| [^6] | [Dynamic Sparse Training with Structured Sparsity.](http://arxiv.org/abs/2305.02299) | 本文提出了一种结构化稀疏动态训练（DST）方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持，可缩减参数和内存占用，同时相较于密集模型，具有减少推理时间的优势。 |
| [^7] | [Iranian License Plate Recognition Using a Reliable Deep Learning Approach.](http://arxiv.org/abs/2305.02292) | 本论文提出了一种利用深度学习方法识别伊朗车牌的解决方案，将车牌识别分为两步，第一步检测车牌区域，第二步识别车牌字符。 |
| [^8] | [Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models.](http://arxiv.org/abs/2305.02279) | 本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境 |
| [^9] | [Standardized Benchmark Dataset for Localized Exposure to a Realistic Source at 10$-$90 GHz.](http://arxiv.org/abs/2305.02260) | 本研究针对10$-$90 GHz范围的暴露情景，生成了一份综合的、可开源使用的数据集，可用于计算技术的评估和测试。 |
| [^10] | [An Adaptive Algorithm for Learning with Unknown Distribution Drift.](http://arxiv.org/abs/2305.02252) | 一种适应未知分布漂移的学习算法，相对于当前分布在不需要先验知识的情况下，学习一个函数族，且误差几乎与预先知道漂移大小的学习算法相同。此外，由于该算法适应数据，因此可以保证比依赖于漂移宽松限制的算法具有更好的学习效果。 |
| [^11] | [Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems.](http://arxiv.org/abs/2305.02251) | 本文调研了自动化科学发现，介绍了各种方法和最近的话题，并概述了闭环科学发现系统和自主发现系统，其中最大级别不需要任何人类干预。该研究旨在发展能够产生诺贝尔级成果的AI科学家。 |
| [^12] | [Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally.](http://arxiv.org/abs/2305.02247) | 本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。 |
| [^13] | [Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation.](http://arxiv.org/abs/2305.02231) | 该论文旨在探讨可信人工智能的构建，包括从法律、伦理和技术、社会角度确保其健壮性。实现真正可信的人工智能涉及到更广阔的愿景，考虑到伦理方面、风险方面、以及对七个技术需求的支持度和大局整体之关系。 |
| [^14] | [Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat.](http://arxiv.org/abs/2305.02220) | 本文介绍了使用大型语言模型从医生-患者对话中自动生成临床笔记的研究，采用少样本上下文学习法所生成笔记表现优秀，且可与人工编写的笔记媲美。 |
| [^15] | [LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning.](http://arxiv.org/abs/2305.02219) | 我们提出了LESS-VFL方法，用于竖直联邦学习中通信高效的特征选择。该方法通过短暂的预训练和本地特征选择，在减少通信成本的同时，可以从模型训练中删除虚假特征。 |
| [^16] | [Stream Efficient Learning.](http://arxiv.org/abs/2305.02217) | 本文介绍了“流高效学习”的概念，该概念旨在解决从数据流中机器学习的效率问题，其泛化性能不仅取决于接收到了多少数据，而且还取决于有多少数据能够及时有效地被利用，加上资源和速度的考虑。 |
| [^17] | [On the stability test for reproducing kernel Hilbert spaces.](http://arxiv.org/abs/2305.02213) | 论文研究了再生核希尔伯特空间的稳定性，提出了对核算子在仅取$\pm 1$值的测试函数上进行的简化稳定性测试方法。 |
| [^18] | [Deep Graph Representation Learning and Optimization for Influence Maximization.](http://arxiv.org/abs/2305.02200) | 该论文提出了一个名为DeepIM的新框架，用于解决影响力最大化中的困难问题，具有更强的泛化能力和适应能力。 |
| [^19] | [Multi-Head Graph Convolutional Network for Structural Connectome Classification.](http://arxiv.org/abs/2305.02199) | 本文提出了一种基于多头图卷积网络的机器学习模型，用于对大脑连接组进行分类。该模型通过不同的图卷积头部，包括边缘和节点，全面捕捉输入数据的表示。实验结果表明，该模型在性别分类任务上表现最佳，并能从连接组数据中提取互补和代表性特征。 |
| [^20] | [Rethinking Graph Lottery Tickets: Graph Sparsity Matters.](http://arxiv.org/abs/2305.02190) | 本文探讨了图彩票问题中图的稀疏性问题，提出了保留节点输入输出特征的对称修剪技术和保留特征图空间位置的修剪技术，并在基准数据集上取得了比现有技术更优秀的结果。 |
| [^21] | [Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using Continual Learning.](http://arxiv.org/abs/2305.02171) | 本文提出了一种持续推理的新方法，通过将神经符号系统与持续学习相结合，可以在处理非单调推理任务时获得更高的准确性。 |
| [^22] | [Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows.](http://arxiv.org/abs/2305.02164) | 本文提出了两个重要贡献：一是提出了条件切片Wasserstein流（CSWF），可以实现非参数条件建模，二是将局部连接和多尺度表示等视觉研究启发的技术引入到SWF中，从而大大提高了图像建模的效率和质量。 |
| [^23] | [Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space.](http://arxiv.org/abs/2305.02151) | 探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。 |
| [^24] | [Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level.](http://arxiv.org/abs/2305.02148) | 通过利用最新的深度学习语义分割、领域适配和半监督学习技术，该研究提出了一种半监督分割方法，可在细胞水平上对功能性组织单元进行分割，并取得了先进水平的结果。 |
| [^25] | [A Curriculum View of Robust Loss Functions.](http://arxiv.org/abs/2305.02139) | 本文提出了鲁棒损失函数的课程视角，以更直观的方式分析训练动态，指出欠拟合的原因是由于平均样本权重的降低而引起的，对噪声鲁棒性的优化则是通过对干净样本赋予更大的样本权重来实现的。进一步的研究表明，通过简单的课程修正可以提高鲁棒损失函数的性能，而训练进度对于鲁棒性也有着非常重要的影响。 |
| [^26] | [System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning.](http://arxiv.org/abs/2305.02128) | 本文介绍了一种名为“系统神经多样性”的方法，用于度量具有随机策略的多智能体系统的行为异质性，探讨了多样性对集体弹性和性能的影响。 |
| [^27] | [Bicubic++: Slim, Slimmer, Slimmest -- Designing an Industry-Grade Super-Resolution Network.](http://arxiv.org/abs/2305.02126) | Bicubic++是一种工业级超分辨率网络，通过在整个网络中使用输入图像的空间维度并学习快速可逆降级和较低分辨率特征来减少计算量。同时，我们通过全局结构剪枝和去偏差处理优化裁剪网络的PSNR，该方法能在所有SR数据集上为Bicubic上采样PSNR增加约1dB。 |
| [^28] | [Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services.](http://arxiv.org/abs/2305.02109) | 本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。 |
| [^29] | [Efficient Online Decision Tree Learning with Active Feature Acquisition.](http://arxiv.org/abs/2305.02093) | 论文提出了一种在线决策树学习的新方法，通过主动采集特征值来降低成本，同时使用后验抽样方案来保持在线预测的低遗憾度，该方法在多个基准测试中实现了最先进的性能。 |
| [^30] | [Understanding cirrus clouds using explainable machine learning.](http://arxiv.org/abs/2305.02090) | 本文使用机器学习模型研究了卷积云的驱动因素与云属性之间的关系，发现气象和气溶胶条件可以预测卷积云属性。功能属性方法还可以量化这些关系，了解哪些因素可以影响卷积云中的冰晶数浓度和冰水含量。 |
| [^31] | [Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning.](http://arxiv.org/abs/2305.02054) | 本文提出了一种基于地图的经验回放方法，通过将存储的转换组织成一种简洁的环境模型网络，以在减少内存大小的同时增加每个样本的相关性，从而有效解决强化学习中的遗忘问题。 |
| [^32] | [Low-complexity subspace-descent over symmetric positive definite manifold.](http://arxiv.org/abs/2305.02041) | 本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。 |
| [^33] | [Response-conditioned Turn-taking Prediction.](http://arxiv.org/abs/2305.02036) | 本文提出的响应条件模型结合了对话历史和下一个发言者想要表达的内容来预测一轮对话何时结束，并在Stanford对话数据集中表现出了最佳的预测和响应结果。 |
| [^34] | [Gym-preCICE: Reinforcement Learning Environments for Active Flow Control.](http://arxiv.org/abs/2305.02033) | 本文介绍了Gym-preCICE，这是一个用于设计和开发单一和多物理学AFC应用的RL环境的框架。它利用preCICE来处理控制器和AFC模拟环境之间的信息交换，无缝集成了基于现实物理的模拟工具箱与RL算法。 |
| [^35] | [Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification.](http://arxiv.org/abs/2305.02032) | 该论文提出一种基于无监督互相转换学习的算法，用于多吉格像素全切片图像分类，无需手动注释即可实现最先进的性能。 |
| [^36] | [LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning.](http://arxiv.org/abs/2305.02022) | LearnDefend是一种学习防御策略，能够有效地对抗联邦学习系统中的有针对性模型中毒攻击。它使用一个较小的防御数据集，估计客户端更新被污染的概率，通过学习毒数据检测器模型并使用耦合的优化方法估计毒数据检测器和客户端重要性模型。 |
| [^37] | [Commentary on explainable artificial intelligence methods: SHAP and LIME.](http://arxiv.org/abs/2305.02012) | 这篇评论对可解释人工智能方法 SHAP 和 LIME 进行了评述和比较，提出了一个框架且突出了它们的优缺点。 |
| [^38] | [fairml: A Statistician's Take on Fair Machine Learning Modelling.](http://arxiv.org/abs/2305.02009) | 这篇论文介绍了一个基于统计方法的公平机器学习建模包fairml，该包实现了分类、回归和核估计等方法，可以在保障公平和问责方面提供帮助。 |
| [^39] | [Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving.](http://arxiv.org/abs/2305.02008) | Zenseact Open Dataset是一个大规模、多样化且覆盖范围广的自动驾驶数据集，具有最高范围和分辨率的传感器以及详细的关键帧注释，专注于长程感知和多任务学习。 |
| [^40] | [Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?.](http://arxiv.org/abs/2305.01997) | 本文对当前医学/超声心动图图像分割方法进行了全面比较，提出了3D nnU-Net模型，解决了时间一致性和跨数据集方面的问题，并通过引入一个新的私有数据集，CARDINAL，来证明其在应用于临床中的优越性。 |
| [^41] | [A Survey on Dataset Distillation: Approaches, Applications and Future Directions.](http://arxiv.org/abs/2305.01975) | 数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。 |
| [^42] | [DPSeq: A Novel and Efficient Digital Pathology Classifier for Predicting Cancer Biomarkers using Sequencer Architecture.](http://arxiv.org/abs/2305.01968) | 本研究提出了一种名为DPSeq的新颖高效的数字病理分类器，通过微调一个集成了水平和竖直双向长短期记忆(BiLSTM)网络的序列器架构，来预测癌症生物标记物。实验结果表明，DPSeq表现出了在预测CRC中关键生物标记物方面的出色性能，优于大多数已发表的最先进分类器在同组患者群内的性能。 |
| [^43] | [SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method.](http://arxiv.org/abs/2305.01954) | SeqAug是一种模态不可知的数据增强方法，可以成功应用于单模态或多模态，通过从基础特征分布中重新采样来增强序列，并且与循环和Transformer架构兼容，取得了与最先进方法相当的结果。 |
| [^44] | [Experimental Design for Any $p$-Norm.](http://arxiv.org/abs/2305.01942) | 该论文提出了一种通用的$p$-范数目标实验设计问题的解决算法，可适用于各种特殊情况，并且是已知最好的界的插值。 |
| [^45] | [Exploring the Protein Sequence Space with Global Generative Models.](http://arxiv.org/abs/2305.01941) | 本章概述了使用蛋白质生成模型的三种方法：利用语言模型设计新的人工蛋白质，使用非Transformer架构的工作和应用于定向进化方法的应用。 |
| [^46] | [Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models.](http://arxiv.org/abs/2305.01939) | 证明了对于训练良好的AI模型，如果满足一定条件，将出现稀疏交互概念，这些概念能够描述输入变量之间的相互作用，并对模型推理分数产生影响。 |
| [^47] | [An Exploration of Conditioning Methods in Graph Neural Networks.](http://arxiv.org/abs/2305.01933) | 本文探究图神经网络中的三种条件处理方式：弱条件处理、强条件处理和纯条件处理，对于不同类别的GNNs，有不同的表现，实证研究表明这些条件处理方式对于GNN的性能有影响。 |
| [^48] | [Specification-Driven Neural Network Reduction for Scalable Formal Verification.](http://arxiv.org/abs/2305.01932) | 本文提出了一种基于规范的神经网络简化方法用于大规模形式化验证。该方法采用保守的简化方法，确保简化后的网络验证与原网络验证派生等价。简化后可将网络减少到小于5％的神经元数量，从而减少了相应的验证时间。 |
| [^49] | [MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction.](http://arxiv.org/abs/2305.01912) | 本文提出了一种名为MolKD的新颖方法，通过将化学反应与分子间的跨模态知识提取和转移，为分子表示学习提供辅助，从而提高分子属性预测的效果。 |
| [^50] | [Evolving Dictionary Representation for Few-shot Class-incremental Learning.](http://arxiv.org/abs/2305.01885) | 本文提出了一种基于演化字典表示的少样本类别递增学习方法，在基础会话中同时优化字典和特征提取骨干，在增量会话中仅微调字典以适应新类别，避免灾难性遗忘，并在多个基准数据集上实现了最先进的性能。 |
| [^51] | [A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems.](http://arxiv.org/abs/2305.01883) | 本文提出了一种轻量级CNN-Transformer模型，使用CNN嵌入层和部分自注意力，能更好学习输入数据中的空间特征并消除冗余。实验表明该模型在解决旅行商问题方面表现出更好的性能，例如TSP解决方案质量和GPU内存使用方面。 |
| [^52] | [Morphological Classification of Galaxies Using SpinalNet.](http://arxiv.org/abs/2305.01873) | 本文使用SpinalNet深度神经网络对星系进行形态分类，取得了高精度的结果。 |
| [^53] | [Pre-train and Search: Efficient Embedding Table Sharding with Pre-trained Neural Cost Models.](http://arxiv.org/abs/2305.01868) | 本文探索了一种基于预训练成本模型的高效分片方法，通过神经网络预测成本，并使用在线搜索确定最佳分片计划，实验结果表明其在嵌入表分片任务中表现很好。 |
| [^54] | [Unsupervised Improvement of Audio-Text Cross-Modal Representations.](http://arxiv.org/abs/2305.01864) | 本研究探索了无监督的方法来改进跨模态音频-文本表征学习，通过使用领域特定的筛选和软标注对比性损失，成功提高了零-shot分类性能。 |
| [^55] | [Inferential Moments of Uncertain Multivariable Systems.](http://arxiv.org/abs/2305.01841) | 本文提出了一种新的分析不确定多变量系统行为的方法，使用推断矩描述分布预计如何响应新信息，特别关注推断偏差，以改善情境感知能力。 |
| [^56] | [AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation.](http://arxiv.org/abs/2305.01836) | 本文提出了一个音视频定位与分割的框架AV-SAM。AV-SAM基于SAM模型，能够在Flickr-SoundNet和AVSBench数据集上达到竞争性的性能，在音视频任务中具有广泛的实际应用前景。 |
| [^57] | [Cortical analysis of heterogeneous clinical brain MRI scans for large-scale neuroimaging studies.](http://arxiv.org/abs/2305.01827) | 本文提出了一种新的方法，可以对任何分辨率和脉冲序列的临床脑部MRI扫描进行皮层重建、配准、分割和厚度估计，为大规模神经影像学研究、尤其是针对少数族裔和罕见疾病的研究提供了可能。 |
| [^58] | [Out-of-distribution detection algorithms for robust insect classification.](http://arxiv.org/abs/2305.01823) | 本文研究了用于昆虫分类鲁棒性的OOD检测算法，通过不同类别的技术评估，发现基于生成模型的方法是最有效的，能够在实际农业领域中应用。 |
| [^59] | [Unpaired Downscaling of Fluid Flows with Diffusion Bridges.](http://arxiv.org/abs/2305.01822) | 本文提出了一种基于扩散映射和生成模型的方法，能够在没有配对训练数据的情况下进行流体模拟的降尺度处理，该方法可有效地增强分辨率并校正上下文相关偏差。 |
| [^60] | [Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features.](http://arxiv.org/abs/2305.01807) | 本研究首次从理论上研究了基于协方差神经网络的可转移性，证明了当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。 |
| [^61] | [When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?.](http://arxiv.org/abs/2305.01801) | 本研究对多个神经推荐模型与传统模型进行比较，提出了一组评估策略来衡量其记忆性能、泛化性能和子群特定性能，揭示了在IMDB和Yelp数据集上，神经推荐模型与传统模型的差异性。 |
| [^62] | [MISNN: Multiple Imputation via Semi-parametric Neural Networks.](http://arxiv.org/abs/2305.01794) | MISNN是一种基于半参数神经网络的多重插补方法，具有高效性和良好的插补精度，尤其适用于高维数据和特征选择问题。 |
| [^63] | [Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy.](http://arxiv.org/abs/2305.01783) | 本文研究了城乡基于卫星的贫困映射中的代表性差异、预测误差中的系统偏差和公平性问题，并展示了这些现象如何影响基于预测地图的政策的有效性。 |
| [^64] | [Representation Learning via Manifold Flattening and Reconstruction.](http://arxiv.org/abs/2305.01777) | 本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。 |
| [^65] | [Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems.](http://arxiv.org/abs/2305.01773) | 本文提出了一种利用图神经网络建模大规模动态系统的深度状态空间模型，在保持多峰预测分布的准确性的同时，通过确定性矩匹配规则实现了无样本推断，提高了预测的效率和稳定性。 |
| [^66] | [DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness.](http://arxiv.org/abs/2305.01770) | DeCom是一种基于深度耦合因子分解机的RSV预测算法，该算法结合了正常的季节性RSV传播模式和COVID-19符合NPI措施下RSV传播的不稳定性，预测效果更加准确。 |
| [^67] | [Psychologically-Inspired Causal Prompts.](http://arxiv.org/abs/2305.01764) | 本文提出了三个因果提示语，涵盖了情感分类任务中人类的心理过程。这些提示语可以用来产生更准确和可解释的模型预测。 |
| [^68] | [Spatial-Temporal Networks for Antibiogram Pattern Prediction.](http://arxiv.org/abs/2305.01761) | 本文提出了一个新颖的问题，即抗生素敏感性图案预测，旨在预测未来哪些图案将出现，并解决了这一问题遇到的挑战。 |
| [^69] | [Adversarial Generative NMF for Single Channel Source Separation.](http://arxiv.org/abs/2305.01758) | 本文提出了一种基于对抗生成的非负矩阵分解单声道源分离方法，通过对抗训练NMF基，实现了在没有强监督数据可用的情况下提高重构信号质量的效果。 |
| [^70] | [Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles.](http://arxiv.org/abs/2305.01754) | 本文比较了多种不确定性量化方案，以提高NNIP的鲁棒性，发现集合方法一致表现更好，且高效。 |
| [^71] | [Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations.](http://arxiv.org/abs/2305.01747) | 本文提出了一种伪标签的泛化方法，称为贝叶斯伪标签，在半监督医学图像分割任务中应用效果良好。 |
| [^72] | [Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare.](http://arxiv.org/abs/2305.01738) | 本论文提出了一种利用因子化动作空间的线性Q函数分解形式的方法，用于解决离线强化学习中存在的动作组合问题，该方法在提高采样效率的同时并不牺牲策略最优性，通过模拟器和实际数据集的几个离线强化学习问题的实验表明，相较于标准方法，该方法具有更快的收敛速度、更好的性能和更高的采样效率。 |
| [^73] | [Expressive Mortality Models through Gaussian Process Kernels.](http://arxiv.org/abs/2305.01728) | 本研究基于高斯过程框架，利用核函数的加法和乘法结构设计了一个遗传编程算法，能够学习特定人群的年龄和年份特定的死亡率曲面，为不同人群中队列效应的存在性带来了新的见解，并提供了相对平滑程度的分析工作。 |
| [^74] | [Slow Kill for Big Data Learning.](http://arxiv.org/abs/2305.01726) | 本文提出了一种称为“慢杀”的技术，它利用非凸约束优化、自适应$\ell_2$收缩和逐步增加的学习率，可以在大规模数据上实现高效变量筛选和统计精度。 |
| [^75] | [DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation.](http://arxiv.org/abs/2305.01698) | 本文提出了DeepAqua，一种利用自我监督深度学习模型，使用知识蒸馏技术来从合成孔径雷达图像中分割水域的方法。该方法不需要手动注释，可用于大规模监测湿地变化。 |
| [^76] | [Predict NAS Multi-Task by Stacking Ensemble Models using GP-NAS.](http://arxiv.org/abs/2305.01667) | 本研究使用GP-NAS和交叉验证的堆叠集成模型，在NAS多任务中准确预测体系结构的性能，机型排名第一。 |
| [^77] | [BrainNPT: Pre-training of Transformer networks for brain network classification.](http://arxiv.org/abs/2305.01666) | 本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。 |
| [^78] | [A Novel Deep Learning based Model for Erythrocytes Classification and Quantification in Sickle Cell Disease.](http://arxiv.org/abs/2305.01663) | 本研究提出了一种深度学习模型，能够识别和量化血液样本图像中扭曲和正常形态的红细胞，并且在镰状细胞疾病这一模型疾病状态下表现良好。 |
| [^79] | [SIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework.](http://arxiv.org/abs/2305.01661) | 提出一种语音指令感知的飞行轨迹预测框架，通过融合即时的语音指令和飞行轨迹表示，解决了语音指令和飞行轨迹的模态差距问题，在多个真实世界数据集上表现优异。 |
| [^80] | [Data valuation: The partial ordinal Shapley value for machine learning.](http://arxiv.org/abs/2305.01660) | 本文提出了偏序 Shapley 值的定义，并提出三种算法来近似计算结果，以解决数据合作中顺序作用的问题。 |
| [^81] | [FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework.](http://arxiv.org/abs/2305.01658) | FlightBERT++提出了一种非自回归的多时域飞行轨迹预测框架，通过引入时域感知上下文生成器解决了误差累积和低效率的问题。 |
| [^82] | [Scalable Data Point Valuation in Decentralized Learning.](http://arxiv.org/abs/2305.01657) | 该文提出了一种名为DDVal的方法，用于在联邦和群智学习中的分散式数据估值，可以估算单个数据点的价值。DDVal基于共享深度特征，并通过k最近邻逼近方法来估算Shapley值，可用于同时向机构和个人奖励为分散式机器学习任务提供数据的贡献。同时，DDVal对机构的贡献进行了层次化的结论，并在实验证明其估算机构贡献的准确性较现有的联邦学习Shapley值逼近方法更高。 |
| [^83] | [Probabilistic Formal Modelling to Uncover and Interpret Interaction Styles.](http://arxiv.org/abs/2305.01656) | 本文使用机器学习和概率模型检验等计算方法，基于用户痕迹记录，揭示了移动应用程序中的交互风格。通过无监督的聚类推断和概率时间逻辑分析，我们发现用户在使用的前期和后期采用的交互风格有所不同。 |
| [^84] | [Predicting blood pressure under circumstances of missing data: An analysis of missing data patterns and imputation methods using NHANES.](http://arxiv.org/abs/2305.01655) | 本文使用美国国家健康和营养检查调查数据研究缺失数据模式，并揭示了多重插补是预测血压变化最有效的方法，插补模型应包括传统预测因素和可能与缺失数据相关的变量。 |
| [^85] | [How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?.](http://arxiv.org/abs/2305.01555) | 本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。 |
| [^86] | [A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning.](http://arxiv.org/abs/2305.01507) | 本文提出一种无需预设参数的ART拓扑聚类算法，通过引入参数估计方法实现持续学习，并在实验中证明其比现有聚类算法更优。 |
| [^87] | [Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees.](http://arxiv.org/abs/2305.01381) | 本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。 |
| [^88] | [Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels.](http://arxiv.org/abs/2305.01160) | 本论文提出了一种名为LC的新长尾识别方法，它能够更好地模拟真实标签分布，同时解决类别标签不平衡问题，从而在CIFAR-10，CIFAR-100和ImageNet基准数据集上显着优于现有方法。 |
| [^89] | [Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning.](http://arxiv.org/abs/2305.00312) | 该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。 |
| [^90] | [Neural Network Accelerated Process Design of Polycrystalline Microstructures.](http://arxiv.org/abs/2305.00003) | 通过神经网络加速工艺设计，减轻预测微结构演化的计算负担，并找到最佳加工路径。 |
| [^91] | [Medical Image Deidentification, Cleaning and Compression Using Pylogik.](http://arxiv.org/abs/2304.12322) | 提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。 |
| [^92] | [Automatically identifying dynamical systems from data.](http://arxiv.org/abs/2304.11182) | 该论文提出了一种从经验数据中自动识别动态规律的方法，这种方法能够较为准确地识别三维系统，具有在各种领域中理解复杂系统的潜力。 |
| [^93] | [Auditing and Generating Synthetic Data with Controllable Trust Trade-offs.](http://arxiv.org/abs/2304.10819) | 本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。 |
| [^94] | [Scheduling DNNs on Edge Servers.](http://arxiv.org/abs/2304.09961) | 本论文研究了如何加速为多个客户端运行边缘服务器DNN。批处理多个DNN请求可以显著加速处理时间。研究设计了一种新的调度算法，并开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。 |
| [^95] | [GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models.](http://arxiv.org/abs/2304.09875) | 本文提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。该分数捕捉了所有样本中的平均认证防攻击扰动水平，无需运行对抗性攻击。 |
| [^96] | [HEAT: A Highly Efficient and Affordable Training System for Collaborative Filtering Based Recommendation on CPUs.](http://arxiv.org/abs/2304.07334) | 这篇论文提出了HEAT训练系统，通过优化SimpleX在CPU上的操作，实现协同过滤的高效率训练 |
| [^97] | [HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction.](http://arxiv.org/abs/2304.07302) | HGWaveNet是一种双曲图神经网络，用于时间链接预测。它包括超曲率扩散图卷积和小波时间卷积两个关键模块，可有效聚合邻居信息和捕捉时间依赖，并在真实世界数据集上表现出更好的精度和效率。 |
| [^98] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^99] | [$(\alpha_D,\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives.](http://arxiv.org/abs/2302.14320) | 本文提出了一个双重目标GAN，通过使用可调的$\alpha$-loss来建模每个目标。在足够大的样本数和容量下，这类GAN的非零和游戏简化为最小化$f$-散度。最后，调整$(\alpha_D,\alpha_G)$可以缓解训练不稳定性。 |
| [^100] | [Differentiable Bootstrap Particle Filters for Regime-Switching Models.](http://arxiv.org/abs/2302.10319) | 本文提出了一种可微粒子滤波器，可以学习一组未知的动态和测量模型，并跟踪状态后验概率，在相关模型中表现出卓越的性能。 |
| [^101] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^102] | [Unsupervised Task Graph Generation from Instructional Video Transcripts.](http://arxiv.org/abs/2302.09173) | 本文提出了一种无监督的任务图生成方法，通过结合支持指导的语言模型的推理能力和聚类、排序组件，从执行真实世界活动的教学视频文本记录中生成任务图。实验结果表明该方法在ProceL和CrossTask数据集上比监督学习方法生成的任务图更加准确。 |
| [^103] | [A survey on online active learning.](http://arxiv.org/abs/2302.08893) | 在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。本文综述了在线主动学习的最新进展、基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范式中不同的评估指标。 |
| [^104] | [A Data Mining Approach for Detecting Collusion in Unproctored Online Exams.](http://arxiv.org/abs/2302.07014) | 提出了一个在疫情期间检测学生是否串通作弊的数据挖掘方法，并通过对远程考试事件日志数据的分析发现了群体作弊行为或者异常，同时建立了一个评估异常案例的经验规则。 |
| [^105] | [DocILE Benchmark for Document Information Localization and Extraction.](http://arxiv.org/abs/2302.05658) | 本文介绍了DocILE基准数据集，该数据集包含大量商务文件，可用于关键信息定位和提取以及行项目识别任务。该数据集具有55个类别的注释，超过以往发布的数据集，同时包括众多不同布局和未标记的文档，为该领域提供了有力的研究工具。 |
| [^106] | [Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.](http://arxiv.org/abs/2302.03857) | 该研究提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法，能够有效地加速对抗性对比学习（ACL）并维持其强鲁棒性和泛化性能。 |
| [^107] | [Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs.](http://arxiv.org/abs/2302.02865) | 本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。 |
| [^108] | [Identifiability of latent-variable and structural-equation models: from linear to nonlinear.](http://arxiv.org/abs/2302.02672) | 本文研究了潜变量和结构方程模型的可辨识性问题，展示了如何推广现有的模型，使其可以用于非线性问题，并且仍然保持可辨识性。 |
| [^109] | [How Bad is Top-$K$ Recommendation under Competing Content Creators?.](http://arxiv.org/abs/2302.01971) | 本文基于随机效用模型，研究了内容创作者在Top-K推荐下的竞争影响，证明了用户福利损失受小常数上界影响。 |
| [^110] | [Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems.](http://arxiv.org/abs/2302.01567) | 本文提出了一种基于深度强化学习（DRL）的新型在线错误检测方法。 |
| [^111] | [A novel framework for medium-term wind power prediction based on temporal attention mechanisms.](http://arxiv.org/abs/2302.01222) | 本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架（TPE-VMD-TFT），用于24小时和48小时之前的风电功率预测。在法国电力公司Engie的风能数据集上，所提出的方法表现良好。 |
| [^112] | [Explainable Multilayer Graph Neural Network for Cancer Gene Prediction.](http://arxiv.org/abs/2301.08831) | 基于多层图神经网络和多个生物网络的EMGNN方法可以准确预测癌症基因并提供可解释性。 |
| [^113] | [Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks.](http://arxiv.org/abs/2301.06683) | 本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。 |
| [^114] | [PlasmoFAB: A Benchmark to Foster Machine Learning for Plasmodium falciparum Protein Antigen Candidate Prediction.](http://arxiv.org/abs/2301.06454) | 该论文开发了一个称为PlasmoFAB的机器学习基准，可用于训练机器学习方法来探索Plasmodium falciparum蛋白抗原候选物。 |
| [^115] | [Efficient Activation Function Optimization through Surrogate Modeling.](http://arxiv.org/abs/2301.05785) | 本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。 |
| [^116] | [Forecasting through deep learning and modal decomposition in two-phase concentric jets.](http://arxiv.org/abs/2212.12731) | 本论文提出了一种将机器学习和单相流数值模拟相结合的方法来实时预测和改善两相流的燃油/空气混合物，从而提高涡扇发动机燃油室喷射器的性能。 |
| [^117] | [COmic: Convolutional Kernel Networks for Interpretable End-to-End Learning on (Multi-)Omics Data.](http://arxiv.org/abs/2212.02504) | COmic是一种结合了卷积核网络和通路诱导核的人工神经网络，用于（多）组学数据的稳健和可解释的端到端学习。 |
| [^118] | [Score-based denoising for atomic structure identification.](http://arxiv.org/abs/2212.02421) | 本论文提出了一种基于评分的去噪算法，可有效去除原子模拟过程中的热振动噪声，适用于不同原子间相互作用生成的模拟数据，并能提高分类方法的准确性。 |
| [^119] | [Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming.](http://arxiv.org/abs/2211.01317) | 本文提出了一种基于神经模型重新编程的迁移学习方法，并针对复杂输入数据提出了输入依赖NMR范式，能够有效地进行音乐风格分类。 |
| [^120] | [Cost-aware Generalized $\alpha$-investing for Multiple Hypothesis Testing.](http://arxiv.org/abs/2210.17514) | 本文提出了成本感知的通用α投资法进行多重假设检验，拓展了α投资规则以考虑样本大小，通过构建与自然对抗的博弈来优化α财富的期望回报(ERO)并提供最佳样本大小。经实证表明，该规则能更准确地拒绝虚假零假设。 |
| [^121] | [A Magnetic Framelet-Based Convolutional Neural Network for Directed Graphs.](http://arxiv.org/abs/2210.10993) | 本文介绍了一种基于磁性小波框架的有向图谱图卷积神经网络Framelet-MagNet，在滤波方面比传统方法更加有效，且在节点分类、链路预测和去噪等任务上表现优异。 |
| [^122] | [Discovering Many Diverse Solutions with Bayesian Optimization.](http://arxiv.org/abs/2210.10953) | ROBOT是一种新的贝叶斯优化方法，可以找到一组高性能、多样化的解决方案，解决了传统单目标贝叶斯优化方法只能找到一个最佳解决方案的局限性。 |
| [^123] | [Improving Your Graph Neural Networks: A High-Frequency Booster.](http://arxiv.org/abs/2210.08251) | 本文提出了一种高频率增强器，将高频信息融入到GNN中，提高异质性图的表达能力，实验结果表明比流行的基准模型性能提高了3.6%。 |
| [^124] | [KAIROS: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources.](http://arxiv.org/abs/2210.05889) | KAIROS是一个新颖的机器学习推断系统，利用异构计算硬件池和优化的推断查询分配，实现了最大化的查询吞吐量同时满足服务质量和成本预算限制。在产业级深度学习模型评估中，KAIROS相比于最优异构方案吞吐量增加了2倍，并且超过了其他现有方案。 |
| [^125] | [A Kernel-Based View of Language Model Fine-Tuning.](http://arxiv.org/abs/2210.05643) | 本文研究神经切线核 (NTK) 在描述预训练语言模型微调过程中的适用性。实验证明在14个NLP任务中使用掩码词预测问题作为下游任务，可以取得好的效果。 |
| [^126] | [Ensemble Reinforcement Learning in Continuous Spaces -- A Hierarchical Multi-Step Approach for Policy Training.](http://arxiv.org/abs/2209.14488) | 本文提出了一种新的集成强化学习算法，基于多步层次策略训练方法来训练基学习器集成，以提高学习稳定性和性能。 |
| [^127] | [Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence.](http://arxiv.org/abs/2207.06316) | 本文提出了一种带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\beta$-差异和其他稀疏约束。 |
| [^128] | [Convergence for score-based generative modeling with polynomial complexity.](http://arxiv.org/abs/2206.06227) | 本文证明了对于得分模型而言，从一个概率分布中抽样的核心机制，在$L^2(p)$准确估计$\nabla \ln p$后可以多项式收敛。同时提供了对基于得分的生成模型的理论分析，为使用退火程序生成样本提供了理论基础。 |
| [^129] | [Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria.](http://arxiv.org/abs/2206.00137) | 本文研究了机器学习算法在训练数据集存在偏见时现有公平标准的鲁棒性，探究了标记和测量误差对其影响。研究发现，一些约束可以在面对某些统计偏差时保持稳健，而另一些则会在训练偏见数据集时被显著违反。 |
| [^130] | [Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks.](http://arxiv.org/abs/2205.15171) | 提出一种新颖的模块化偏差缓解方法，在推理时间按需集成到核心模型中的独立去偏置子网络，在性别、种族和年龄等受保护属性的分类任务中，该方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。 |
| [^131] | [The Diminishing Returns of Masked Language Models to Science.](http://arxiv.org/abs/2205.11342) | 本文从科学角度出发，评估了14个领域特定的基于Transformer的模型在12个下游科学任务中的表现，发现增加模型大小、训练数据或计算时间并不总是会导致显着提高，可能会有出乎意料的性能差异。 |
| [^132] | [ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification.](http://arxiv.org/abs/2205.11332) | 本文提出了一种名为"ImGCL"的图形对比学习（GCL）算法框架，该框架能够自动自适应地平衡不平衡节点分类问题中从无标签节点（图）中学到的表示，通过整合在线聚类和逐步平衡采样方法，我们的算法可以有效地学习区分表示，实现与先进技术相当的性能。 |
| [^133] | [Reward Systems for Trustworthy Medical Federated Learning.](http://arxiv.org/abs/2205.00470) | 本文研究了如何通过奖励系统防止医疗联邦学习中的模型偏差，并提出了一种结合的奖励系统以提高预测性能和降低偏差。 |
| [^134] | [Streaming Algorithms for High-Dimensional Robust Statistics.](http://arxiv.org/abs/2204.12399) | 本研究提出了首个高维健壮统计学流算法，具有几乎最优的存储需求，特别是在Huber污染模型下的高维健壮均值估计任务中，提供了一个高效的单遍流算法。 |
| [^135] | [Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs.](http://arxiv.org/abs/2204.00853) | 本文提出了一种称为Adversarial Neon Beam（AdvNB）的物理攻击方法，通过获取对抗性氖光束的物理参数并且仅需要极少的查询就能执行物理攻击，该攻击方法在数字和物理测试中都可达成领先的攻击效果，是一种可怕的深度神经网络攻击方法。 |
| [^136] | [On the Convergence of SARSA with Linear Function Approximation.](http://arxiv.org/abs/2202.06828) | 本文通过对投影SARSA到有限区域的收敛速度的探究，取得了在带有线性函数逼近的SARSA算法收敛性方面的进展，发现收敛区域比想象的要小得多。 |
| [^137] | [HARFE: Hard-Ridge Random Feature Expansion.](http://arxiv.org/abs/2202.02877) | 论文提出了一种适用于高维稀疏可加函数的硬岭随机特征扩展方法（HARFE）模型，它可以通过应用基于硬阈值追踪的算法来进行近似计算，同时利用稀疏岭回归（SRR）表达式来取得稀疏模型选择和岭回归平滑之间的平衡，相比其他算法，HARFE方法在合成数据和真实数据集上具有更低的误差。 |
| [^138] | [Collaborative Learning in General Graphs with Limited Memorization: Complexity, Learnability, and Reliability.](http://arxiv.org/abs/2201.12482) | 本文提出了一个三阶段的轻量级随机游走算法，在有限的记忆和通讯带宽限制下完成了通用图上的协作学习，解决了连接图不完全和非良构的问题，并提高了学习过程的可靠性。 |
| [^139] | [Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule.](http://arxiv.org/abs/2201.11989) | 本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。 |
| [^140] | [Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning.](http://arxiv.org/abs/2111.08066) | 本文提出了一种利用行动影响规律和外生状态变量进行离线强化学习的算法，该算法在现实世界的许多领域中成立，包括金融市场，并在模拟和真实世界中的不同数据收集策略中优于现有的离线强化学习算法。 |
| [^141] | [Generalization of graph network inferences in higher-order graphical models.](http://arxiv.org/abs/2107.05729) | 本论文提出了递归因子图神经网络(RF-GNN)，用于实现对涉及多变量相互作用的图形模型的快速近似推断。在多个图形模型家族的实验中展示了RF-GNN在表达性图形模型中快速且准确地执行推断的潜力。 |

# 详细

[^1]: 适用于单张图像人像的实时辐射场合成

    Real-Time Radiance Fields for Single-Image Portrait View Synthesis. (arXiv:2305.02310v1 [cs.CV])

    [http://arxiv.org/abs/2305.02310](http://arxiv.org/abs/2305.02310)

    该论文提出了一种适用于单张图片的实时辐射场合成方法，能够从单张未经过姿势调整的图像中推断和渲染出逼真的3D表示，并产生高质量的3D感知人像合成结果。

    

    我们提出了一个单拍摄方法，可以从单张未经过姿势调整的图像（例如面部肖像）中推断和渲染出逼真的3D表示，并实时合成。 给定单个RGB输入，我们的图像编码器直接预测由神经辐射场的规范三面图表示，通过体渲染进行三维感知的新视图合成。我们的方法在消费级硬件上快速（24fps），且产生的质量高于需要测试时间优化的强GAN反演基线。为了训练三面图编码器管道，我们只使用合成数据，展示了如何从预训练的3D GAN中提取知识，并将其蒸馏成前馈编码器。技术贡献包括基于Vision Transformer的三面图编码器、相机数据增强策略以及针对合成数据训练的良好设计的损失函数。我们在最先进的方法上进行基准测试，在具有挑战性的现实世界场景中展示了显着的鲁棒性和图像质量改进。我们展示了我们的结果，表明它能够生成具有高质量的3D感知人像合成结果。

    We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our res
    
[^2]: CodeGen2：编程和自然语言LLM训练的经验

    CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])

    [http://arxiv.org/abs/2305.02309](http://arxiv.org/abs/2305.02309)

    本研究旨在提高LLMs编程合成训练的效率，通过统一模型架构、学习方法、填充采样和数据分布，来提高训练模型的泛化能力。

    

    大型语言模型在编程合成和理解任务的表示学习中展示了卓越的能力。学习到的表示质量似乎由神经比例定律作为模型参数和观察值的函数决定，同时通过可用数据和计算量的数量限制模型的性能。本研究尝试通过统一四个关键组件使LLMs的程序合成训练更加高效：（1）模型架构，（2）学习方法，（3）填充采样和（4）数据分布。

    Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.  In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a "free lunch" hypothesis. For data distributions, the eff
    
[^3]: 校准化解释：基于不确定性信息和反事实的解释模型

    Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])

    [http://arxiv.org/abs/2305.02305](http://arxiv.org/abs/2305.02305)

    该论文提出了一种新的特征重要性解释方法，Calibrated Explanations (CE)，它可以提供准确、稳定的解释，并且可以为概率估计和特征重要性权重提供不确定性量化信息，是一种快速、可靠且强健的解释方法。

    

    人工智能已经成为各种领域决策支持系统中不可或缺的一部分，但人工智能决策系统中预测模型缺乏透明度可能导致滥用或不使用。可解释人工智能旨在创建可以向人类用户解释其推理过程的人工智能系统。可解释人工智能中的局部解释可以提供关于特征重要性的个别预测原因的信息，但存在不稳定性等缺点。为了解决这些问题，我们提出了一种新的特征重要性解释方法，校准化解释(Calibrated Explanations，CE)，它基于 Venn-Abers，同时在生成特征重要性解释的同时校准底层模型。CE不仅提供快速、可靠、稳定和强健的解释，还提供概率估计和特征重要性权重的不确定性量化。此外，该方法是模型无关的，具有易于理解的条件规则，也可以生成反事实推理。

    Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
    
[^4]: 揭示插值和支持向量机之间的新等价性：核函数和结构化特征

    New Equivalences Between Interpolation and SVMs: Kernels and Structured Features. (arXiv:2305.02304v1 [stat.ML])

    [http://arxiv.org/abs/2305.02304](http://arxiv.org/abs/2305.02304)

    本文提出了一种新的、灵活的分析框架，用于证明在任意再生核希尔伯特空间中特征的SVP条件

    

    支持向量机（SVM）是一种监督学习算法，它通过核技巧将数据映射到高维特征空间，找到最大间隔线性分类器。最近的研究表明，在某些过度参数化的情况下，SVM的决策函数与最小范数标签插值完全重合。这种支持向量增殖（SVP）现象特别有趣，因为它使我们能够通过利用线性和核模型中无害插值的最近分析来理解SVM的性能。然而，先前关于SVP的工作对数据/特征分布和频谱做出了限制性假设。在本文中，我们提出了一种新的、灵活的分析框架，用于在任意再生核希尔伯特空间中，对标签的生成模型的一类灵活性特征进行SVP证明。我们提出了局限于一般有界正交系统族（例如Fourier函数族）特征的SVP条件

    The support vector machine (SVM) is a supervised learning algorithm that finds a maximum-margin linear classifier, often after mapping the data to a high-dimensional feature space via the kernel trick. Recent work has demonstrated that in certain sufficiently overparameterized settings, the SVM decision function coincides exactly with the minimum-norm label interpolant. This phenomenon of support vector proliferation (SVP) is especially interesting because it allows us to understand SVM performance by leveraging recent analyses of harmless interpolation in linear and kernel models. However, previous work on SVP has made restrictive assumptions on the data/feature distribution and spectrum. In this paper, we present a new and flexible analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert space with a flexible class of generative models for the labels. We present conditions for SVP for features in the families of general bounded orthonormal systems (e.g. Fourier f
    
[^5]: Distilling Step-by-Step！使用更少的训练数据和更小的模型尺寸胜过更大的语言模型

    Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])

    [http://arxiv.org/abs/2305.02301](http://arxiv.org/abs/2305.02301)

    本研究提出了Distilling Step-by-Step机制，通过提取LLM基础信息为小型模型提供额外的监督训练，从而使它们胜过更大的LLM模型，并需更少的训练数据。

    

    部署大型语言模型（LLM）面临内存效率低和计算密集度高的问题，研究人员通过微调或精炼使用LLM生成的标签来训练较小的任务特定模型。但是，要想达到LLM相当的性能，这需要大量的训练数据。我们引入了Distilling Step-by-Step，这是一种新的机制， (a)训练较小的模型比LLM表现更好，(b)并通过利用微调或精炼所需的更少的训练数据来实现。我们的方法在多任务训练框架中提取LLM基础，并作为额外的监督来训练小型模型。在四个NLP基准测试中，我们提出了三个发现：第一，与微调和精炼相比，我们的机制使用较少的标记/未标记训练示例取得更好的性能。第二，与LLM相比，即使使用更小的模型，我们也实现了更好的性能。

    Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
    
[^6]: 结构化稀疏动态训练

    Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])

    [http://arxiv.org/abs/2305.02299](http://arxiv.org/abs/2305.02299)

    本文提出了一种结构化稀疏动态训练（DST）方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持，可缩减参数和内存占用，同时相较于密集模型，具有减少推理时间的优势。

    

    动态稀疏训练在稀疏神经网络训练中取得了最先进的结果，并匹配了密集模型的泛化性，同时使得稀疏训练和推理成为可能。尽管得到的模型高度稀疏，理论上训练更便宜，但在实际硬件上，使用非结构化稀疏性加速依然具有人们所面临的挑战。在本文中，我们提出一种 DST 方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持。此外，我们通过理论分析和实证结果，证明了特定 N:M 稀疏方法（常数扇入）的泛化性能，并展示了一种缩减参数和内存占用的紧凑表示。经过对 PyTorch CPU 实现的简单表示进行推断，我们证明了相较于密集模型，该方法减少了推理时间。我们的源代码可在 https://github.com/calgaryml/condensed-sparsity 上获得。

    DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
    
[^7]: 一种可靠的深度学习方法在伊朗车牌识别上的应用

    Iranian License Plate Recognition Using a Reliable Deep Learning Approach. (arXiv:2305.02292v1 [cs.CV])

    [http://arxiv.org/abs/2305.02292](http://arxiv.org/abs/2305.02292)

    本论文提出了一种利用深度学习方法识别伊朗车牌的解决方案，将车牌识别分为两步，第一步检测车牌区域，第二步识别车牌字符。

    

    自动车牌识别（ALPR）是最近几年最具挑战性的问题之一。天气条件、视角、光线状况、不同的车牌字符等等因素都是ALPR的挑战。鉴于近年来深度神经网络领域取得的进展，可以使用一些类型的神经网络和基于它们的模型来执行伊朗车牌识别的任务。本文提出的方法是将车牌识别分为两步。第一步是从输入图像中检测出车牌的矩形。在第二步中，这些车牌从图像中裁剪出来进行字符识别。为第一步准备了3065张车牌照片数据集，第二步准备了3364张包括车牌字符的图像数据集。

    The issue of Automatic License Plate Recognition (ALPR) has been one of the most challenging issues in recent years. Weather conditions, camera angle of view, lighting conditions, different characters written on license plates, and many other factors are among the challenges for the issue of ALPR. Given the advances that have been made in recent years in the field of deep neural networks, some types of neural networks and models based on them can be used to perform the task of Iranian license plate recognition. In the proposed method presented in this paper, the license plate recognition is done in two steps. The first step is to detect the rectangles of the license plates from the input image. In the second step, these license plates are cropped from the image and their characters are recognized. For the first step, 3065 images including license plates and for the second step, 3364 images including characters of license plates have been prepared and considered as the desired datasets.
    
[^8]: Learngene: 从祖先模型中继承压缩知识到后代模型

    Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])

    [http://arxiv.org/abs/2305.02279](http://arxiv.org/abs/2305.02279)

    本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境

    

    在一个生物的连续进化过程中，它的基因积累了广泛的经验和知识，使新生后代能够快速适应其特定环境。受到这一观察的启发，我们提出了一种新的机器学习范 paradigm，即 Learngene，使学习模型能够融合基因的三个关键特征。 (i) 积累：知识在祖先模型的连续学习过程中积累。 (ii) 压缩：将积累的详尽知识压缩成更为紧凑的信息片段，即 Learngene。 (iii) 继承：将压缩的 Learngene 继承给后代模型，以便于适应新的环境。由于积累已在一些成熟的范式中得到研究，如大规模预训练和终身学习，因此我们专注于压缩和继承，这引发了三个关键问题，并为这些问题提供了初步的解决方案。

    During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
    
[^9]: 10$-$90 GHz范围内真实源定位暴露的标准基准数据集

    Standardized Benchmark Dataset for Localized Exposure to a Realistic Source at 10$-$90 GHz. (arXiv:2305.02260v1 [physics.med-ph])

    [http://arxiv.org/abs/2305.02260](http://arxiv.org/abs/2305.02260)

    本研究针对10$-$90 GHz范围的暴露情景，生成了一份综合的、可开源使用的数据集，可用于计算技术的评估和测试。

    

    缺乏免费公开的标准数据集是评估和剂量研究中新技术开发和测试的加重因素。本文针对10$-$90 GHz范围内的稳态暴露情景，对入射功率密度和皮肤表面最大温升的有限数据进行统计建模并生成综合的开源数据集，可用于对计算技术的测试和性能评估。

    The lack of freely available standardized datasets represents an aggravating factor during the development and testing the performance of novel computational techniques in exposure assessment and dosimetry research. This hinders progress as researchers are required to generate numerical data (field, power and temperature distribution) anew using simulation software for each exposure scenario. Other than being time consuming, this approach is highly susceptible to errors that occur during the configuration of the electromagnetic model. To address this issue, in this paper, the limited available data on the incident power density and resultant maximum temperature rise on the skin surface considering various steady-state exposure scenarios at 10$-$90 GHz have been statistically modeled. The synthetic data have been sampled from the fitted statistical multivariate distribution with respect to predetermined dosimetric constraints. We thus present a comprehensive and open-source dataset comp
    
[^10]: 一种适应未知分布漂移的学习算法

    An Adaptive Algorithm for Learning with Unknown Distribution Drift. (arXiv:2305.02252v1 [cs.LG])

    [http://arxiv.org/abs/2305.02252](http://arxiv.org/abs/2305.02252)

    一种适应未知分布漂移的学习算法，相对于当前分布在不需要先验知识的情况下，学习一个函数族，且误差几乎与预先知道漂移大小的学习算法相同。此外，由于该算法适应数据，因此可以保证比依赖于漂移宽松限制的算法具有更好的学习效果。

    

    我们开发和分析了一种学习未知分布漂移的通用技术。给定一个从漂移分布的最后$T$步中独立观测到的序列，我们的算法在$T$时刻不加区分地学习一个函数族，相对于当前分布。与以前的工作不同，我们的技术不需要关于漂移大小的先验知识。相反，该算法适应样本数据。在不明确估计漂移的情况下，该算法学习的函数族的误差几乎与预先知道漂移大小的学习算法相同。此外，由于我们的算法适应数据，它可以保证比依赖于漂移宽松限制的算法具有更好的学习效果。

    We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last $T$ steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time $T$. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift.
    
[^11]: 自动化科学发现：从方程式探索到自主发现系统

    Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems. (arXiv:2305.02251v1 [cs.AI])

    [http://arxiv.org/abs/2305.02251](http://arxiv.org/abs/2305.02251)

    本文调研了自动化科学发现，介绍了各种方法和最近的话题，并概述了闭环科学发现系统和自主发现系统，其中最大级别不需要任何人类干预。该研究旨在发展能够产生诺贝尔级成果的AI科学家。

    

    本文调研了自动化科学发现，从方程式探索和符号回归到自主发现系统和代理。从“宏观”和上下文角度讨论了各种方法，但也讨论了开放问题和最近的话题，如深度神经网络在这个领域中的各种角色，帮助发现人类可解释的知识。此外，我们将介绍闭环科学发现系统，从Adam系统的开创性工作到当前在材料科学和天文学等领域的努力。最后，我们将从机器学习的角度详细阐述自主性，并以自动驾驶的自主级别为类比。最大级别，第五级，定义为在生产科学知识时不需要任何人类干预。实现这一点是迈向解决Nobel Turing Grand Challenge的一步：开发能够产生诺贝尔级科学成果的AI科学家 - 能力的一步。

    The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality sc
    
[^12]: 毫不畏惧地选择：几乎所有的小批量训练方案都能够优化。

    Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])

    [http://arxiv.org/abs/2305.02247](http://arxiv.org/abs/2305.02247)

    本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。

    

    我们证明了带有确定性或随机性、数据独立的小批量梯度下降训练的匹配上下一般化误差界限，但批量选择规则是任意的。我们考虑光滑的Lipschitz-凸性/非凸性/强凸性损失函数，并证明了随机梯度下降的经典上限界限也适用于这样任意的非自适应批量调度，包括所有确定性的调度方案。进一步地，对于凸和强凸的损失函数，我们直接证明了在上述批量调度类上一致的一般化误差下的匹配下限界限，表明所有这样的批量调度都能达到最优的一般化。最后，对于光滑的（非Lipschitz）非凸性损失函数，我们证明了在所考虑的类别内，包括所有随机批处理方案，全批量（确定性）梯度下降是最优的。

    We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.
    
[^13]: 构建可信人工智能：从人工智能原则、伦理和主要需求到负责任的人工智能系统和监管

    Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation. (arXiv:2305.02231v1 [cs.CY])

    [http://arxiv.org/abs/2305.02231](http://arxiv.org/abs/2305.02231)

    该论文旨在探讨可信人工智能的构建，包括从法律、伦理和技术、社会角度确保其健壮性。实现真正可信的人工智能涉及到更广阔的愿景，考虑到伦理方面、风险方面、以及对七个技术需求的支持度和大局整体之关系。

    

    可信人工智能基于七个技术需求，分别从法律、伦理和技术、社会角度确保其健壮性。然而，实现真正可信的人工智能涉及到更广阔的愿景，包括系统生命周期中所有参与流程和参与者可信性的考量。一个更全面的愿景将考虑到伦理方面、风险方面、以下要件的支持度以及大局整体之关系。评估七个需求之技术方面、伦理方面和监管挑战方面。

    Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple
    
[^14]: 利用大型语言模型从医生-患者对话中生成临床笔记：来自MEDIQA-Chat的见解

    Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])

    [http://arxiv.org/abs/2305.02220](http://arxiv.org/abs/2305.02220)

    本文介绍了使用大型语言模型从医生-患者对话中自动生成临床笔记的研究，采用少样本上下文学习法所生成笔记表现优秀，且可与人工编写的笔记媲美。

    

    本文描述了我们在MEDIQA-Chat 2023共享任务中提交的自动临床笔记生成方案。我们报告了两种方法的结果：第一种是在共享任务数据上微调预训练语言模型（PLM），第二种是使用大型语言模型（LLM）的少样本上下文学习（ICL）。两种方法都取得了高性能，如通过自动度量标准（例如ROUGE，BERTScore）测量，并分别在所有提交的方案中排名第二和第一。专家审核表明，通过基于ICL的方法使用GPT-4生成的笔记与人工编写的笔记一样受欢迎，这使得它成为从医生-患者对话中自动生成笔记的有前途的路径。

    This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
    
[^15]: LESS-VFL：用于竖直联邦学习的通信高效特征选择

    LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning. (arXiv:2305.02219v1 [cs.LG])

    [http://arxiv.org/abs/2305.02219](http://arxiv.org/abs/2305.02219)

    我们提出了LESS-VFL方法，用于竖直联邦学习中通信高效的特征选择。该方法通过短暂的预训练和本地特征选择，在减少通信成本的同时，可以从模型训练中删除虚假特征。

    

    我们提出了LESS-VFL，这是一种用于具有竖直分割数据的分布式系统的通信高效特征选择方法。我们考虑一个由服务器和多个带有本地数据集的参与者组成的系统，这些数据集共享样本ID空间，但具有不同的特征集。参与者希望协作地训练一个用于预测任务的模型。作为训练的一部分，参与者希望在系统中删除不重要的特征，以提高泛化性能，效率和可解释性。在LESS-VFL中，经过短暂的预训练后，服务器优化其全局模型的一部分，以确定来自参与者模型的相关输出。这些信息与参与者共享，然后允许本地特征选择而无需通信。我们从理论上证明了LESS-VFL可以从模型训练中删除虚假特征。我们提供了广泛的经验证据表明LESS-VFL可以以其他特征选择算法的通信成本的一小部分获得高精度并删除虚假特征。

    We propose LESS-VFL, a communication-efficient feature selection method for distributed systems with vertically partitioned data. We consider a system of a server and several parties with local datasets that share a sample ID space but have different feature sets. The parties wish to collaboratively train a model for a prediction task. As part of the training, the parties wish to remove unimportant features in the system to improve generalization, efficiency, and explainability. In LESS-VFL, after a short pre-training period, the server optimizes its part of the global model to determine the relevant outputs from party models. This information is shared with the parties to then allow local feature selection without communication. We analytically prove that LESS-VFL removes spurious features from model training. We provide extensive empirical evidence that LESS-VFL can achieve high accuracy and remove spurious features at a fraction of the communication cost of other feature selection a
    
[^16]: 流数据高效学习

    Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])

    [http://arxiv.org/abs/2305.02217](http://arxiv.org/abs/2305.02217)

    本文介绍了“流高效学习”的概念，该概念旨在解决从数据流中机器学习的效率问题，其泛化性能不仅取决于接收到了多少数据，而且还取决于有多少数据能够及时有效地被利用，加上资源和速度的考虑。

    

    许多真实世界应用的数据往往随着时间的积累以流的形式进行。与传统的机器学习研究关注于从给定的训练数据集中学习不同，从数据流中学习不能忽视流入的数据流可能是无休止的、规模巨大、变化未知，并且假设有足够的计算/存储资源可以及时处理所有接收到的数据是不现实的。因此，从数据流中学习的泛化性能不仅取决于接收到了多少数据，而且取决于有多少数据能够被及时地有效利用，加上资源和速度的考虑，再加上学习算法的能力和问题的复杂度。为此，在本文中我们介绍了机器学习吞吐量的概念，定义了流高效学习，并提出了一个初步的理论框架。

    Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
    
[^17]: 关于再生核希尔伯特空间的稳定性检测

    On the stability test for reproducing kernel Hilbert spaces. (arXiv:2305.02213v1 [eess.SY])

    [http://arxiv.org/abs/2305.02213](http://arxiv.org/abs/2305.02213)

    论文研究了再生核希尔伯特空间的稳定性，提出了对核算子在仅取$\pm 1$值的测试函数上进行的简化稳定性测试方法。

    

    再生核希尔伯特空间(RKHSs)是一种特殊的希尔伯特空间，其所有评估泛函都是线性且有界的。它们与称为核的正定映射一一对应。稳定的RKHSs具有仅包含函数和绝对可积的附加属性。已知文献中RKHS稳定性的必要和充分条件：由核引起的积分算子必须作为从本质有界（测试）函数的空间$\mathcal{L}_{\infty}$到绝对可积函数的空间$\mathcal{L}_1$的映射被限制。考虑连续时间的Mercer(连续)核和整个离散时间类，我们证明了稳定性测试可以简化为对核算子在仅取$\pm 1$值(几乎处处)的测试函数上的研究。它们代表了研究RKHS中任何单个元素的稳定性所需的相同函数。通过这种方式，RKHS的稳定性测试被简化为涉及有限数量的测试函数的两级测试问题。

    Reproducing kernel Hilbert spaces (RKHSs) are special Hilbert spaces where all the evaluation functionals are linear and bounded. They are in one-to-one correspondence with positive definite maps called kernels. Stable RKHSs enjoy the additional property of containing only functions and absolutely integrable. Necessary and sufficient conditions for RKHS stability are known in the literature: the integral operator induced by the kernel must be bounded as map between $\mathcal{L}_{\infty}$, the space of essentially bounded (test) functions, and $\mathcal{L}_1$, the space of absolutely integrable functions. Considering Mercer (continuous) kernels in continuous-time and the entire discrete-time class, we show that the stability test can be reduced to the study of the kernel operator over test functions which assume (almost everywhere) only the values $\pm 1$. They represent the same functions needed to investigate stability of any single element in the RKHS. In this way, the RKHS stability
    
[^18]: 影响力最大化的深度图表示学习与优化

    Deep Graph Representation Learning and Optimization for Influence Maximization. (arXiv:2305.02200v1 [cs.SI])

    [http://arxiv.org/abs/2305.02200](http://arxiv.org/abs/2305.02200)

    该论文提出了一个名为DeepIM的新框架，用于解决影响力最大化中的困难问题，具有更强的泛化能力和适应能力。

    

    影响力最大化（IM）被定义为从社交网络中选择一组初始用户以最大化受影响用户的预期数量。研究人员在设计各种传统方法方面取得了巨大进展，其理论设计和性能提升接近于极限。在过去的几年中，基于学习的IM方法已经出现，相较于传统方法，它们具有更强的对未知图形的泛化能力。然而，基于学习的IM方法的发展仍然受到基本障碍的限制，包括：1）有效解决目标函数的困难性；2）表征多样化的基础扩散模式的困难性；以及3）在各种节点中心性约束的IM变体下适应解决方案的困难性。为了应对上述挑战，我们设计了一个新框架DeepIM来生成地表征种子集的潜在表示，并提出学习多样化信息的方法。

    Influence maximization (IM) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based IM methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based IM methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained IM variants. To cope with the above challenges, we design a novel framework DeepIM to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information di
    
[^19]: 多头图卷积网络在结构连接组分类中的应用

    Multi-Head Graph Convolutional Network for Structural Connectome Classification. (arXiv:2305.02199v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.02199](http://arxiv.org/abs/2305.02199)

    本文提出了一种基于多头图卷积网络的机器学习模型，用于对大脑连接组进行分类。该模型通过不同的图卷积头部，包括边缘和节点，全面捕捉输入数据的表示。实验结果表明，该模型在性别分类任务上表现最佳，并能从连接组数据中提取互补和代表性特征。

    

    本文致力于基于扩散磁共振图像提取的大脑连接数据的分类。我们提出了一种基于图卷积网络（GCN）的机器学习模型，该模型采用多头并行GCN机制，分别对大脑连接输入图进行处理。所提出的网络是一种简单的设计，采用不同的头部，涉及边缘和节点的图卷积，充分捕捉输入数据的表示。为了测试我们的模型从大脑连接数据中提取互补和代表性特征的能力，我们选择了性别分类任务。这表征了连接组在性别方面的变化程度，这对于提高我们对两性健康和疾病理解至关重要。本文在两个公开数据集PREVENT-AD（347个受试者）和OASIS3（771个受试者）上进行实验。结果表明，所提出的模型表现最佳。

    We tackle classification based on brain connectivity derived from diffusion magnetic resonance images. We propose a machine-learning model inspired by graph convolutional networks (GCNs), which takes a brain connectivity input graph and processes the data separately through a parallel GCN mechanism with multiple heads. The proposed network is a simple design that employs different heads involving graph convolutions focused on edges and nodes, capturing representations from the input data thoroughly. To test the ability of our model to extract complementary and representative features from brain connectivity data, we chose the task of sex classification. This quantifies the degree to which the connectome varies depending on the sex, which is important for improving our understanding of health and disease in both sexes. We show experiments on two publicly available datasets: PREVENT-AD (347 subjects) and OASIS3 (771 subjects). The proposed model demonstrates the highest performance compa
    
[^20]: 重新思考图彩票: 图的稀疏性很重要

    Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (arXiv:2305.02190v1 [cs.LG])

    [http://arxiv.org/abs/2305.02190](http://arxiv.org/abs/2305.02190)

    本文探讨了图彩票问题中图的稀疏性问题，提出了保留节点输入输出特征的对称修剪技术和保留特征图空间位置的修剪技术，并在基准数据集上取得了比现有技术更优秀的结果。

    

    彩票猜想 (LTH) 声称存在一种获胜的彩票 (即，一个经过适当修剪的子网络以及原始权重初始化)，它可以实现与原始密集网络相当的性能。最近的一项工作称为 UGS，扩展了 LTH 以修剪图神经网络 (GNN)，以有效加速 GNN 推理。UGS 同时使用相同的屏蔽机制修剪图邻接矩阵和模型权重，但由于图邻接矩阵和权重矩阵的角色非常不同，我们发现他们的稀疏化导致不同的性能特征。具体而言，我们发现当图的稀疏程度超过一定程度时，稀疏 GNN 的性能会显着下降。因此，我们提出了两种技术，以改善当图的稀疏程度较高时的 GNN 性能。首先，UGS 使用丢失公式修剪邻接矩阵，然而，该技术并未适当涉及邻接矩阵的所有元素。为了缓解这个问题，我们提出了一个对称的修剪技术，它可以在修剪邻接矩阵的同时保证每个节点的输入和输出特征被保留。其次，我们提出了一种新颖的稀疏化技术，它避免了直接修剪图结构，而是在保留特征图的空间位置的同时修剪了 GNN 层的特征通道，类似于图像卷积网络采用的策略。实验结果验证了我们提出的技术的有效性，在基准数据集上，在准确度和计算效率上超过了最先进的基线，分别达到了 7.0% 和 5.0x。

    Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the ad
    
[^21]: 持续推理：在神经符号 AI 中使用持续学习进行非单调推理

    Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using Continual Learning. (arXiv:2305.02171v1 [cs.AI])

    [http://arxiv.org/abs/2305.02171](http://arxiv.org/abs/2305.02171)

    本文提出了一种持续推理的新方法，通过将神经符号系统与持续学习相结合，可以在处理非单调推理任务时获得更高的准确性。

    

    尽管已经在相似性推理方面进行了广泛投资和令人瞩目的最近进展，但深度学习在更复杂的推理形式，如非单调和常识推理方面仍然存在困难。非单调是非经典推理的一个特性，通常在常识推理中看到，推理系统允许（与古典逻辑不同）作出可能稍后被撤回的结论，当有新信息可用时。神经符号系统（如逻辑张量网络）已被证明能够有效地使深度神经网络具有推理能力。在本文中，我们展示了通过将神经符号系统与持续学习方法相结合，LTN在处理非单调推理任务时可以获得更高水平的准确性。我们通过采用从知识和数据中学习和回忆的学习课程将持续学习加入到LTN中。我们称这个过程为“持续推理”，这是一种新的方法论。

    Despite the extensive investment and impressive recent progress at reasoning by similarity, deep learning continues to struggle with more complex forms of reasoning such as non-monotonic and commonsense reasoning. Non-monotonicity is a property of non-classical reasoning typically seen in commonsense reasoning, whereby a reasoning system is allowed (differently from classical logic) to jump to conclusions which may be retracted later, when new information becomes available. Neural-symbolic systems such as Logic Tensor Networks (LTN) have been shown to be effective at enabling deep neural networks to achieve reasoning capabilities. In this paper, we show that by combining a neural-symbolic system with methods from continual learning, LTN can obtain a higher level of accuracy when addressing non-monotonic reasoning tasks. Continual learning is added to LTNs by adopting a curriculum of learning from knowledge and data with recall. We call this process Continual Reasoning, a new methodolog
    
[^22]: 无参数条件和局部连接切片Wasserstein流量的生成建模

    Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])

    [http://arxiv.org/abs/2305.02164](http://arxiv.org/abs/2305.02164)

    本文提出了两个重要贡献：一是提出了条件切片Wasserstein流（CSWF），可以实现非参数条件建模，二是将局部连接和多尺度表示等视觉研究启发的技术引入到SWF中，从而大大提高了图像建模的效率和质量。

    

    切片Wasserstein流（SWF）是一种非参数生成建模的有前途的方法，但由于其发生生成质量的亚优性和缺乏条件建模能力而未被广泛采用。我们在这项工作中，为弥合这一差距做出了两个重要贡献。首先，基于一个愉悦的观察（在某些条件下），联合分布的SWF与条件分布的SWF相符，我们提出了条件切片Wasserstein流（CSWF），这是SWF的一个简单但有效的扩展，可实现非参数条件建模。其次，我们引入了适当的图像归纳偏置到SWF中，用两个技术受到视觉研究中的局部连接和多尺度表示的启发，大大提高了图像建模的效率和质量。通过全部改进，在进行纯非参数建模的同时，在有条件和无条件任务上实现了与许多深度参数化生成模型相当的生成性能。

    Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric
    
[^23]: 语言距离与多语言表示空间中的跨语言传递的相关性研究

    Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])

    [http://arxiv.org/abs/2305.02151](http://arxiv.org/abs/2305.02151)

    探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。

    

    先前的研究已经探讨了不同语言特征对跨语言传递性能的影响。本研究探讨了这种效应如何映射到表示空间中。过去的研究集中在微调期间多语言语言模型的跨语言对齐上的影响，而本研究研究的是由MLLMs生成的相应语言表示空间的绝对演变。我们特别强调语言特征的作用，并调查其与表示空间和跨语言传递性能的影响之间的相互关系。此外，本文提供了初步证据，说明如何利用这些发现增强对语言上相距较远的语言的传递能力。

    Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
    
[^24]: 细胞水平的功能性组织单元半监督分割方法

    Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level. (arXiv:2305.02148v1 [eess.IV])

    [http://arxiv.org/abs/2305.02148](http://arxiv.org/abs/2305.02148)

    通过利用最新的深度学习语义分割、领域适配和半监督学习技术，该研究提出了一种半监督分割方法，可在细胞水平上对功能性组织单元进行分割，并取得了先进水平的结果。

    

    我们提出了一种新的半监督学习技术与领域自适应的深度学习语义分割方法，用于细胞水平的功能性组织单元分割。该方法利用深度学习语义分割方法、领域自适应和半监督学习技术，可最小化HPA和HubMAP数据集之间的领域差异、类别不平衡和捕获设置影响。所提出的方法在功能性组织单元在细胞水平的分割方面，达到了与现有先进方法可比拟的结果。源代码可在 https://github.com/VSydorskyy/hubmap_2022_htt_solution 上获取。

    We present a new method for functional tissue unit segmentation at the cellular level, which utilizes the latest deep learning semantic segmentation approaches together with domain adaptation and semi-supervised learning techniques. This approach allows for minimizing the domain gap, class imbalance, and captures settings influence between HPA and HubMAP datasets. The presented approach achieves comparable with state-of-the-art-result in functional tissue unit segmentation at the cellular level. The source code is available at https://github.com/VSydorskyy/hubmap_2022_htt_solution
    
[^25]: 鲁棒损失函数的课程视角

    A Curriculum View of Robust Loss Functions. (arXiv:2305.02139v1 [cs.LG])

    [http://arxiv.org/abs/2305.02139](http://arxiv.org/abs/2305.02139)

    本文提出了鲁棒损失函数的课程视角，以更直观的方式分析训练动态，指出欠拟合的原因是由于平均样本权重的降低而引起的，对噪声鲁棒性的优化则是通过对干净样本赋予更大的样本权重来实现的。进一步的研究表明，通过简单的课程修正可以提高鲁棒损失函数的性能，而训练进度对于鲁棒性也有着非常重要的影响。

    

    鲁棒损失函数旨在应对标签噪声的负面影响，其鲁棒性通常由与训练动态无关的理论界限支持。然而，这些界限可能无法表征实证性能，因为目前尚不清楚为什么鲁棒损失函数会欠拟合。我们表明，大多数损失函数可以重写成具有相同类-分数间隔和不同样本加权函数形式的形式。所得到的课程视角提供了对训练动态的直观分析，有助于将欠拟合归因于平均样本权重的降低和将噪声鲁棒性归因于对干净样本赋予较大的样本权重。我们表明，对课程视角进行简单的修正可以使欠拟合的鲁棒损失函数与最先进的方法竞争，而训练进度可以极大地影响噪声鲁棒性，即使采用鲁棒损失函数。代码可在\url{github}中找到。

    Robust loss functions are designed to combat the adverse impacts of label noise, whose robustness is typically supported by theoretical bounds agnostic to the training dynamics. However, these bounds may fail to characterize the empirical performance as it remains unclear why robust loss functions can underfit. We show that most loss functions can be rewritten into a form with the same class-score margin and different sample-weighting functions. The resulting curriculum view provides a straightforward analysis of the training dynamics, which helps attribute underfitting to diminished average sample weights and noise robustness to larger weights for clean samples. We show that simple fixes to the curriculums can make underfitting robust loss functions competitive with the state-of-the-art, and training schedules can substantially affect the noise robustness even with robust loss functions. Code is available at \url{github}.
    
[^26]: 系统神经多样性：在多智能体学习中度量行为异质性

    System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning. (arXiv:2305.02128v1 [cs.MA])

    [http://arxiv.org/abs/2305.02128](http://arxiv.org/abs/2305.02128)

    本文介绍了一种名为“系统神经多样性”的方法，用于度量具有随机策略的多智能体系统的行为异质性，探讨了多样性对集体弹性和性能的影响。

    

    进化科学提供了多样性具有韧性的证据。然而，传统的多智能体强化学习技术通常强制要求同质性以增加训练样本的效率。当学习代理系统不受同质策略的限制时，个体代理可能会发展出不同的行为，从而产生有利于系统的新兴互补性。尽管如此，缺乏衡量学习代理系统中行为多样性的工具意味着我们无法深入了解多样性对集体弹性和性能的影响。在本文中，我们介绍了系统神经多样性（SND）：一种用于具有随机策略的多智能体系统的行为异质性度量方法，探讨并证明了其理论性质，并将其与跨学科领域中使用的最新行为多样性指标进行了比较。

    Evolutionary science provides evidence that diversity confers resilience. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individual agents may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this feat, there is a surprising lack of tools that measure behavioral diversity in systems of learning agents. Such techniques would pave the way towards understanding the impact of diversity in collective resilience and performance. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity for multi-agent systems where agents have stochastic policies. %over a continuous state space. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in cross-disciplinary domains. Through
    
[^27]: Bicubic++：设计一种工业级超分辨率网络——更小，更轻，更薄

    Bicubic++: Slim, Slimmer, Slimmest -- Designing an Industry-Grade Super-Resolution Network. (arXiv:2305.02126v1 [cs.CV])

    [http://arxiv.org/abs/2305.02126](http://arxiv.org/abs/2305.02126)

    Bicubic++是一种工业级超分辨率网络，通过在整个网络中使用输入图像的空间维度并学习快速可逆降级和较低分辨率特征来减少计算量。同时，我们通过全局结构剪枝和去偏差处理优化裁剪网络的PSNR，该方法能在所有SR数据集上为Bicubic上采样PSNR增加约1dB。

    

    我们提出了一个名为Bicubic++的实时轻量级单图像超分辨率（SR）网络。虽然整个网络都使用了输入图像的空间维度，但Bicubic++首先学习了图像的快速可逆降级和较低分辨率特征，以减少计算量。我们还构建了一个训练管道，其中应用了卷积层的端到端全局结构剪枝，而不使用像幅度和梯度范数这样的度量，并专注于优化裁剪网络在验证集上的PSNR。此外，我们还在实验证明了偏差项在增加PSNR时需要相当多的运行时间，因此我们还对卷积层应用了去偏差处理。我们的方法在所有测试的SR数据集上为Bicubic上采样PSNR增加了约1dB，并在720p输入和4K输出的情况下以FP16精度在RTX3090上运行1.17ms，在RTX3070上运行2.9ms。Bicubic++还获得了NTIRE 2023 RTSR Track 2 x3的胜利。

    We propose a real-time and lightweight single-image super-resolution (SR) network named Bicubic++. Despite using spatial dimensions of the input image across the whole network, Bicubic++ first learns quick reversible downgraded and lower resolution features of the image in order to decrease the number of computations. We also construct a training pipeline, where we apply an end-to-end global structured pruning of convolutional layers without using metrics like magnitude and gradient norms, and focus on optimizing the pruned network's PSNR on the validation set. Furthermore, we have experimentally shown that the bias terms take considerable amount of the runtime while increasing PSNR marginally, hence we have also applied bias removal to the convolutional layers. Our method adds ~1dB on Bicubic upscaling PSNR for all tested SR datasets and runs with ~1.17ms on RTX3090 and ~2.9ms on RTX3070, for 720p inputs and 4K outputs, both in FP16 precision. Bicubic++ won NTIRE 2023 RTSR Track 2 x3 
    
[^28]: 联邦学习与O-RAN的协同：面向多个分布式机器学习服务的弹性虚拟化架构

    Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])

    [http://arxiv.org/abs/2305.02109](http://arxiv.org/abs/2305.02109)

    本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。

    

    联邦学习是最流行的分布式机器学习技术，但是在现代无线网络中实现联邦学习面临着许多挑战，主要包括网络条件的动态性、系统中多个联邦学习服务/任务的并存以及联邦学习服务与其他网络服务的并行执行等。针对这些挑战，本文提出了一种名为动态多服务联邦学习（DMS-FL）的联邦学习泛型架构，并通过提出一种新的分布式机器学习架构——弹性虚拟化联邦学习（EV-FL）来解决DMS-FL中的三个未探索的设计问题。

    Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
    
[^29]: 带有主动特征获取的高效在线决策树学习

    Efficient Online Decision Tree Learning with Active Feature Acquisition. (arXiv:2305.02093v1 [cs.LG])

    [http://arxiv.org/abs/2305.02093](http://arxiv.org/abs/2305.02093)

    论文提出了一种在线决策树学习的新方法，通过主动采集特征值来降低成本，同时使用后验抽样方案来保持在线预测的低遗憾度，该方法在多个基准测试中实现了最先进的性能。

    

    构建决策树在线是一个经典的机器学习问题。现有工作通常假设每个进入的数据点的特征已经准备好了。然而，在许多实际应用中，特征值和标签都是未知的，只能以一定的成本获取。例如，在医学诊断中，医生必须选择对病人进行哪些测试（即进行昂贵的特征查询），以便做出诊断决策（即预测标签）。我们提供了一个新的视角来解决这个实际难题。我们的框架包括一个嵌入在线学习方案的主动计划预测器，我们研究了几个信息收集功能。具体而言，我们采用了一种基于自适应子模性的代理信息获取函数，以最小的成本主动查询特征值，同时使用后验抽样方案来保持在线预测的低遗憾度。我们在合成和真实数据集上展示了我们提出的方法的效率和有效性，在几个基准测试上实现了最先进的性能。

    Constructing decision trees online is a classical machine learning problem. Existing works often assume that features are readily available for each incoming data point. However, in many real world applications, both feature values and the labels are unknown a priori and can only be obtained at a cost. For example, in medical diagnosis, doctors have to choose which tests to perform (i.e., making costly feature queries) on a patient in order to make a diagnosis decision (i.e., predicting labels). We provide a fresh perspective to tackle this practical challenge. Our framework consists of an active planning oracle embedded in an online learning scheme for which we investigate several information acquisition functions. Specifically, we employ a surrogate information acquisition function based on adaptive submodularity to actively query feature values with a minimal cost, while using a posterior sampling scheme to maintain a low regret for online prediction. We demonstrate the efficiency a
    
[^30]: 使用可解释机器学习理解卷积云

    Understanding cirrus clouds using explainable machine learning. (arXiv:2305.02090v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.02090](http://arxiv.org/abs/2305.02090)

    本文使用机器学习模型研究了卷积云的驱动因素与云属性之间的关系，发现气象和气溶胶条件可以预测卷积云属性。功能属性方法还可以量化这些关系，了解哪些因素可以影响卷积云中的冰晶数浓度和冰水含量。

    

    卷积云是地球气候的关键调节因素，但它们与气象和气溶胶条件的关系是全球气候模型中最大的不确定性之一。本研究使用三年的卫星和再分析数据，研究卷积云驱动因素与云属性之间的关系。我们使用梯度提升机器学习模型和一个带有注意层的长短期记忆网络来预测冰水含量和冰晶数浓度。模型表明，气象和气溶胶条件可以预测卷积云属性，$R^2=0.49$。使用SHapley Additive exPlanations (SHAP)计算功能属性，以量化气象和气溶胶条件与卷积云属性之间的联系。例如，引起冰晶数浓度预测下降所需的超微米尘埃粒子的最小浓度为$2 \times 10^{-4}$ mg m\textsuperscript{-3}。观察前15个小时

    Cirrus clouds are key modulators of Earth's climate. Their dependencies on meteorological and aerosol conditions are among the largest uncertainties in global climate models. This work uses three years of satellite and reanalysis data to study the link between cirrus drivers and cloud properties. We use a gradient-boosted machine learning model and a Long Short-Term Memory (LSTM) network with an attention layer to predict the ice water content and ice crystal number concentration. The models show that meteorological and aerosol conditions can predict cirrus properties with $R^2 = 0.49$. Feature attributions are calculated with SHapley Additive exPlanations (SHAP) to quantify the link between meteorological and aerosol conditions and cirrus properties. For instance, the minimum concentration of supermicron-sized dust particles required to cause a decrease in ice crystal number concentration predictions is $2 \times 10^{-4}$ mg m\textsuperscript{-3}. The last 15 hours before the observat
    
[^31]: 基于地图的经验回放：强化学习中遗忘现象的内存节约解决方案

    Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning. (arXiv:2305.02054v1 [cs.LG])

    [http://arxiv.org/abs/2305.02054](http://arxiv.org/abs/2305.02054)

    本文提出了一种基于地图的经验回放方法，通过将存储的转换组织成一种简洁的环境模型网络，以在减少内存大小的同时增加每个样本的相关性，从而有效解决强化学习中的遗忘问题。

    

    深度强化学习代理在训练新数据时常常会遭受灾难性的遗忘，遗忘先前在输入空间中找到的解决方案。回放记忆是解决这个问题的常见方法，它会对旧和新的训练样本进行去关联和混洗。他们天真地按照状态过渡的顺序存储状态转变，而不考虑冗余性。我们介绍了一种基于Grow-When-Required（GWR）自组织网络的新型认知启发式回放内存方法，它类似于一种基于地图的世界认知模型。我们的方法将存储的转换组织成一个简洁的环境模型网络，将相似的样本合并以减少内存大小并增加样本之间的两两距离，从而增加每个样本的相关性。总体而言，我们的论文表明，基于地图的经验回放允许显着减少内存，只会产生轻微的性能下降。

    Deep Reinforcement Learning agents often suffer from catastrophic forgetting, forgetting previously found solutions in parts of the input space when training on new data. Replay Memories are a common solution to the problem, decorrelating and shuffling old and new training samples. They naively store state transitions as they come in, without regard for redundancy. We introduce a novel cognitive-inspired replay memory approach based on the Grow-When-Required (GWR) self-organizing network, which resembles a map-based mental model of the world. Our approach organizes stored transitions into a concise environment-model-like network of state-nodes and transition-edges, merging similar samples to reduce the memory size and increase pair-wise distance among samples, which increases the relevancy of each sample. Overall, our paper shows that map-based experience replay allows for significant memory reduction with only small performance decreases.
    
[^32]: 对称正定流形上低复杂度的子空间下降算法

    Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])

    [http://arxiv.org/abs/2305.02041](http://arxiv.org/abs/2305.02041)

    本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。

    

    本文提出了一种低复杂度的黎曼子空间下降算法，用于在对称正定（SPD）流形上对函数进行最小化。与现有的黎曼梯度下降变体不同的是，所提出的方法利用 carefully chosen 的子空间，使得更新可以写成迭代的 Cholesky 因子和一个稀疏矩阵的乘积形式。由此产生的更新避免了昂贵的矩阵操作，如矩阵指数和密集矩阵乘法，这些操作通常在几乎所有其他 Riemannian 优化算法中都是必需的。

    This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
    
[^33]: 响应条件的交替预测

    Response-conditioned Turn-taking Prediction. (arXiv:2305.02036v1 [cs.CL])

    [http://arxiv.org/abs/2305.02036](http://arxiv.org/abs/2305.02036)

    本文提出的响应条件模型结合了对话历史和下一个发言者想要表达的内容来预测一轮对话何时结束，并在Stanford对话数据集中表现出了最佳的预测和响应结果。

    

    先前解决对话系统中转换及响应生成的方法通常将其视为两个阶段的过程：首先通过对话历史来检测一轮对话是否已结束，然后系统生成一个合适的响应。然而，人类不仅仅因为很可能轮到自己了就发言，还要考虑自己想说的话是否适合当前位置。本文提出了一种模型（TurnGPT的扩展），它基于对话历史和下一个发言者要说的内容来预测一轮对话何时结束。我们发现，我们的模型在各种指标方面一直表现优异。在两种情景下，我们的模型的改进尤为显著，这两种情景下轮换预测仅由对话历史可能表现为模棱两可的：1）当前话语中包含陈述句和紧接着的疑问句；2）当前话语的结尾和所需响应在语义上匹配。将交替预测和响应排序视为一个阶段的过程，我们的模型在Stanford对话数据集中取得了最优秀的结果。

    Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our
    
[^34]: Gym-preCICE：用于主动流控制的强化学习环境

    Gym-preCICE: Reinforcement Learning Environments for Active Flow Control. (arXiv:2305.02033v1 [cs.LG])

    [http://arxiv.org/abs/2305.02033](http://arxiv.org/abs/2305.02033)

    本文介绍了Gym-preCICE，这是一个用于设计和开发单一和多物理学AFC应用的RL环境的框架。它利用preCICE来处理控制器和AFC模拟环境之间的信息交换，无缝集成了基于现实物理的模拟工具箱与RL算法。

    

    主动流控制(AFC)是指通过时间上的操纵流体流动以达到期望的性能或效率。AFC作为一个顺序优化任务，可以通过利用强化学习(RL)来进行动态优化。本文介绍了Gym-preCICE，这是一个完全符合Gymnasium (以前称为OpenAI Gym) API的Python适配器，用于设计和开发单一和多物理学AFC应用的RL环境。在演员-环境设置中，Gym-preCICE利用preCICE，一个用于分区多物理学模拟的开源耦合库，处理控制器(演员)和AFC模拟环境之间的信息交换。开发的框架实现了将基于现实物理的模拟工具箱与RL算法无缝非侵入式地集成。Gym-preCICE提供了一个设计RL环境来模拟AFC任务的框架，以及一个应用RL算法于各种AFC相关场景的游乐场。

    Active flow control (AFC) involves manipulating fluid flow over time to achieve a desired performance or efficiency. AFC, as a sequential optimisation task, can benefit from utilising Reinforcement Learning (RL) for dynamic optimisation. In this work, we introduce Gym-preCICE, a Python adapter fully compliant with Gymnasium (formerly known as OpenAI Gym) API to facilitate designing and developing RL environments for single- and multi-physics AFC applications. In an actor-environment setting, Gym-preCICE takes advantage of preCICE, an open-source coupling library for partitioned multi-physics simulations, to handle information exchange between a controller (actor) and an AFC simulation environment. The developed framework results in a seamless non-invasive integration of realistic physics-based simulation toolboxes with RL algorithms. Gym-preCICE provides a framework for designing RL environments to model AFC tasks, as well as a playground for applying RL algorithms in various AFC-relat
    
[^35]: 无监督互相转换学习用于多吉格像素全切片图像分类

    Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification. (arXiv:2305.02032v1 [cs.CV])

    [http://arxiv.org/abs/2305.02032](http://arxiv.org/abs/2305.02032)

    该论文提出一种基于无监督互相转换学习的算法，用于多吉格像素全切片图像分类，无需手动注释即可实现最先进的性能。

    

    在计算病理学的新兴领域中，吉格像素全切片图像（WSI）的分类是一项重要的预测任务。深度学习模型在 WSI 分类方面的研究成果急剧增加，其中包括肿瘤检测或从 WSI 预测分子突变等临床应用。大多数方法需要由专家病理学家进行昂贵和劳动密集型的手动注释。弱监督的多实例学习（MIL）方法最近表现出优异的性能；然而，它们仍需要大量标记的训练数据集，这需要专业病理学家对每个切片进行仔细检查。在本研究中，我们提出了一种完全无监督的基于互相转换学习的 WSI 分类算法。来自吉格像素 WSI（即图像补丁）的实例被转换成潜在空间，然后被反向转换到原始空间。使用转换损失，产生伪标签，并使用基于转换器的聚类方法进行清理。我们提出的方法在两个公开可用的多吉格像素 WSI 数据集上实现了最先进的性能，而不需要任何手动注释。

    Classification of gigapixel Whole Slide Images (WSIs) is an important prediction task in the emerging area of computational pathology. There has been a surge of research in deep learning models for WSI classification with clinical applications such as cancer detection or prediction of molecular mutations from WSIs. Most methods require expensive and labor-intensive manual annotations by expert pathologists. Weakly supervised Multiple Instance Learning (MIL) methods have recently demonstrated excellent performance; however, they still require large slide-level labeled training datasets that need a careful inspection of each slide by an expert pathologist. In this work, we propose a fully unsupervised WSI classification algorithm based on mutual transformer learning. Instances from gigapixel WSI (i.e., image patches) are transformed into a latent space and then inverse-transformed to the original space. Using the transformation loss, pseudo-labels are generated and cleaned using a transf
    
[^36]: LearnDefend：学习对抗联邦学习中的有针对性的模型中毒攻击

    LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning. (arXiv:2305.02022v1 [cs.LG])

    [http://arxiv.org/abs/2305.02022](http://arxiv.org/abs/2305.02022)

    LearnDefend是一种学习防御策略，能够有效地对抗联邦学习系统中的有针对性模型中毒攻击。它使用一个较小的防御数据集，估计客户端更新被污染的概率，通过学习毒数据检测器模型并使用耦合的优化方法估计毒数据检测器和客户端重要性模型。

    

    面向联邦学习系统的有针对性模型中毒攻击构成了巨大的威胁。最近的研究显示，目标边缘案例型攻击（对输入空间的一小部分进行针对性攻击）几乎无法通过现有的防御策略进行反击。本文旨在通过使用较小的防御数据集设计一种学习防御策略来应对此类攻击。防御数据集可以由联邦学习任务的中央管理机构收集，其中应包含一些被污染的和没有被污染的示例。所提出的框架LearnDefend会估计客户端更新具有恶意的概率。防御数据集中的示例不需要事先标记为被污染或未被污染。我们还学习了一个可用于标记防御数据集中每个示例为干净或污染的毒数据检测器模型。我们使用耦合的优化方法来估计毒数据检测器和客户端重要性模型。我们的实验表明，LearnDefend能够成功应对有针对性模型中毒攻击。

    Targeted model poisoning attacks pose a significant threat to federated learning systems. Recent studies show that edge-case targeted attacks, which target a small fraction of the input space are nearly impossible to counter using existing fixed defense strategies. In this paper, we strive to design a learned-defense strategy against such attacks, using a small defense dataset. The defense dataset can be collected by the central authority of the federated learning task, and should contain a mix of poisoned and clean examples. The proposed framework, LearnDefend, estimates the probability of a client update being malicious. The examples in defense dataset need not be pre-marked as poisoned or clean. We also learn a poisoned data detector model which can be used to mark each example in the defense dataset as clean or poisoned. We estimate the poisoned data detector and the client importance models in a coupled optimization approach. Our experiments demonstrate that LearnDefend is capable
    
[^37]: 可解释人工智能方法评述：SHAP 和 LIME

    Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])

    [http://arxiv.org/abs/2305.02012](http://arxiv.org/abs/2305.02012)

    这篇评论对可解释人工智能方法 SHAP 和 LIME 进行了评述和比较，提出了一个框架且突出了它们的优缺点。

    

    可解释人工智能（XAI）方法已经发展出来，将机器学习模型的黑匣子转化为更易理解的形式。这些方法有助于传达模型的工作原理，旨在使机器学习模型更透明，并增加最终用户对其输出的信任。 SHapley Additive exPlanations（SHAP）和Local Interpretable Model Agnostic Explanation（LIME）是两种在表格数据中广泛使用的XAI方法。在这篇评论中，我们讨论了两种方法的可解释性度量是如何生成的，并提出了一个解释它们输出的框架，突出了它们的优缺点。

    eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
    
[^38]: 公平机器学习建模的统计学观点

    fairml: A Statistician's Take on Fair Machine Learning Modelling. (arXiv:2305.02009v1 [stat.ML])

    [http://arxiv.org/abs/2305.02009](http://arxiv.org/abs/2305.02009)

    这篇论文介绍了一个基于统计方法的公平机器学习建模包fairml，该包实现了分类、回归和核估计等方法，可以在保障公平和问责方面提供帮助。

    

    机器学习在关键保障公平和问责的应用中的采用，导致文献中出现了大量模型建议，主要以约束条件的优化问题形式减少或消除敏感属性对响应的影响。虽然这种方法从理论上来看非常灵活，但所得的模型有些黑盒性质：很少有关于其统计属性的表述，关于其应用最佳实践的讨论，以及如何将其扩展到其他问题而不是其最初设计用途的研究。此外，估计每个模型需要特定的实现，涉及适当的求解器，从软件工程角度来看是不太理想的。在本文中，我们介绍了fairml R软件包，该软件包实现了我们之前的工作（Scutari，Panero和Proissl 2022）以及文献中的相关模型。fairml是围绕分类、回归和核估计的统计方法设计的。

    The adoption of machine learning in applications where it is crucial to ensure fairness and accountability has led to a large number of model proposals in the literature, largely formulated as optimisation problems with constraints reducing or eliminating the effect of sensitive attributes on the response. While this approach is very flexible from a theoretical perspective, the resulting models are somewhat black-box in nature: very little can be said about their statistical properties, what are the best practices in their applied use, and how they can be extended to problems other than those they were originally designed for. Furthermore, the estimation of each model requires a bespoke implementation involving an appropriate solver which is less than desirable from a software engineering perspective.  In this paper, we describe the fairml R package which implements our previous work (Scutari, Panero, and Proissl 2022) and related models in the literature. fairml is designed around cla
    
[^39]: Zenseact开放数据集：一个大规模且多样化的自动驾驶多模态数据集

    Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving. (arXiv:2305.02008v1 [cs.CV])

    [http://arxiv.org/abs/2305.02008](http://arxiv.org/abs/2305.02008)

    Zenseact Open Dataset是一个大规模、多样化且覆盖范围广的自动驾驶数据集，具有最高范围和分辨率的传感器以及详细的关键帧注释，专注于长程感知和多任务学习。

    

    现有的自动驾驶（AD）数据集通常缺乏多样性和长程能力，而更关注于 360度感知和时间推理。为填补这一缺口，我们介绍了Zenseact开放数据集（ZOD），这是一个在欧洲各国收集两年的大规模且多样化的多模态数据集，覆盖面积是现有数据集的9倍。与可比较数据集相比，ZOD拥有最高范围和分辨率传感器，同时配备了2D和3D对象（长达245m）、道路实例/语义分割、交通标志识别和道路分类的详细关键帧注释。我们相信这种独特组合将有助于突破长程感知和多任务学习难题。该数据集由Frames、 Sequences和 Drives三部分组成，旨在包含数据多样性，支持时空学习、传感器融合、定位和映射。Frames由10万个筛选后的相机图像和两秒钟的其他支持数据组成。

    Existing datasets for autonomous driving (AD) often lack diversity and long-range capabilities, focusing instead on 360{\deg} perception and temporal reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a large-scale and diverse multimodal dataset collected over two years in various European countries, covering an area 9x that of existing datasets. ZOD boasts the highest range and resolution sensors among comparable datasets, coupled with detailed keyframe annotations for 2D and 3D objects (up to 245m), road instance/semantic segmentation, traffic sign recognition, and road classification. We believe that this unique combination will facilitate breakthroughs in long-range perception and multi-task learning. The dataset is composed of Frames, Sequences, and Drives, designed to encompass both data diversity and support for spatio-temporal learning, sensor fusion, localization, and mapping. Frames consist of 100k curated camera images with two seconds of other support
    
[^40]: 超声心动图体积指数的提取：哪种深度学习方案可以应用于临床？

    Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?. (arXiv:2305.01997v1 [eess.IV])

    [http://arxiv.org/abs/2305.01997](http://arxiv.org/abs/2305.01997)

    本文对当前医学/超声心动图图像分割方法进行了全面比较，提出了3D nnU-Net模型，解决了时间一致性和跨数据集方面的问题，并通过引入一个新的私有数据集，CARDINAL，来证明其在应用于临床中的优越性。

    

    基于深度学习的方法已经成为超声心动图图像自动分析的主要手段，利用多个由专家注释的开放数据集（其中CAMUS是最大的公共数据库之一）。然而，由于存在一些问题，如预测的时间一致性和跨数据集的推广能力等问题，这些模型仍然被临床医生认为是不可靠的。因此，本文提出了对当前表现最佳的医学/超声心动图图像分割方法进行全面比较，并特别关注了时间一致性和跨数据集方面。我们介绍了一个名为CARDINAL的新的私有数据集，其包括心尖两腔和心尖四腔序列，并具有完整心脏周期的参考分割。我们展示了所提出的3D nnU-Net优于替代的2D和循环分割方法，同时也报告了在CARDINAL上训练的最佳模型在测试数据集上的良好表现。

    Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when test
    
[^41]: 数据集蒸馏研究综述: 方法、应用和未来方向

    A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])

    [http://arxiv.org/abs/2305.01975](http://arxiv.org/abs/2305.01975)

    数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。

    

    随着训练集的不断增长和训练最先进的模型成本越来越高，数据集蒸馏在机器学习中越来越受到关注。通过合成高信息密度的数据集，数据集蒸馏提供了一系列潜在应用，包括支持持续学习、神经架构搜索和隐私保护。尽管近年来有了一些进展，但我们缺乏对方法和应用的全面理解。我们的调查旨在填补这一空白，首先提出一种数据集蒸馏的分类方法，对现有方法进行特征化，然后系统地回顾数据模态和相关应用。此外，我们总结了挑战并讨论了这一研究领域的未来方向。

    Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.
    
[^42]: DPSeq：一种新颖高效的数字病理分类器，使用序列器架构预测癌症生物标记物（arXiv:2305.01968v1 [eess.IV]）

    DPSeq: A Novel and Efficient Digital Pathology Classifier for Predicting Cancer Biomarkers using Sequencer Architecture. (arXiv:2305.01968v1 [eess.IV])

    [http://arxiv.org/abs/2305.01968](http://arxiv.org/abs/2305.01968)

    本研究提出了一种名为DPSeq的新颖高效的数字病理分类器，通过微调一个集成了水平和竖直双向长短期记忆(BiLSTM)网络的序列器架构，来预测癌症生物标记物。实验结果表明，DPSeq表现出了在预测CRC中关键生物标记物方面的出色性能，优于大多数已发表的最先进分类器在同组患者群内的性能。

    

    在数字病理任务中，变压器已经取得了最先进的结果，超过了卷积神经网络(CNN)。然而，变压器通常过于复杂且资源密集。在本研究中，我们开发了一种名为DPSeq的新颖高效的数字病理分类器，通过微调一个集成了水平和竖直双向长短期记忆(BiLSTM)网络的序列器架构，来预测癌症生物标记物。使用两个国际数据集：癌症基因组图谱(TCGA)和分子与细胞肿瘤学(MCO)中的结肠癌(CRC)的血红蛋白和嗪啉染色组织学图像，对DPSeq的预测性能进行了一系列实验评估。 DPSeq表现出了在预测CRC中关键生物标记物（MSI状态、高突变、CIMP状态、BRAF突变、TP53突变和染色体不稳定性[CING]）方面的出色性能，优于大多数已发表的最先进分类器在同组患者群内的性能。

    In digital pathology tasks, transformers have achieved state-of-the-art results, surpassing convolutional neural networks (CNNs). However, transformers are usually complex and resource intensive. In this study, we developed a novel and efficient digital pathology classifier called DPSeq, to predict cancer biomarkers through fine-tuning a sequencer architecture integrating horizon and vertical bidirectional long short-term memory (BiLSTM) networks. Using hematoxylin and eosin (H&E)-stained histopathological images of colorectal cancer (CRC) from two international datasets: The Cancer Genome Atlas (TCGA) and Molecular and Cellular Oncology (MCO), the predictive performance of DPSeq was evaluated in series of experiments. DPSeq demonstrated exceptional performance for predicting key biomarkers in CRC (MSI status, Hypermutation, CIMP status, BRAF mutation, TP53 mutation and chromosomal instability [CING]), outperforming most published state-of-the-art classifiers in a within-cohort interna
    
[^43]: SeqAug: 序列特征重采样技术——一种模态不可知的数据增强方法

    SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method. (arXiv:2305.01954v1 [cs.CL])

    [http://arxiv.org/abs/2305.01954](http://arxiv.org/abs/2305.01954)

    SeqAug是一种模态不可知的数据增强方法，可以成功应用于单模态或多模态，通过从基础特征分布中重新采样来增强序列，并且与循环和Transformer架构兼容，取得了与最先进方法相当的结果。

    

    数据增强是提高各种机器学习应用性能的常用技术。本文提出了SeqAug，一种针对从序列中提取的特征的模态不可知的增强方法。SeqAug的核心思想是通过从基础特征分布中重新采样来增强序列。重新采样通过随机选择特征维度并沿时间轴对它们进行置换来实现。在CMU-MOSEI上的实验验证了SeqAug的模态不可知性；它可以成功地应用于单模态或多模态。我们进一步验证了它与循环和Transformer架构的兼容性，并证明了它具有与最先进结果相当的性能。

    Data augmentation is a prevalent technique for improving performance in various machine learning applications. We propose SeqAug, a modality-agnostic augmentation method that is tailored towards sequences of extracted features. The core idea of SeqAug is to augment the sequence by resampling from the underlying feature distribution. Resampling is performed by randomly selecting feature dimensions and permuting them along the temporal axis. Experiments on CMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully applied to a single modality or multiple modalities. We further verify its compatibility with both recurrent and transformer architectures, and also demonstrate comparable to state-of-the-art results.
    
[^44]: 任意$p$-范数的实验设计

    Experimental Design for Any $p$-Norm. (arXiv:2305.01942v1 [cs.DS])

    [http://arxiv.org/abs/2305.01942](http://arxiv.org/abs/2305.01942)

    该论文提出了一种通用的$p$-范数目标实验设计问题的解决算法，可适用于各种特殊情况，并且是已知最好的界的插值。

    

    我们考虑了一个通用的$p$-范数目标，用于实验设计问题，并将一些经典目标（D/A/E-设计）作为特殊情况进行了概括。我们证明了一种随机局部搜索方法提供了一个统一的算法来解决所有的$p$-范数问题。这提供了第一个适用于普遍$p$-范数目标的近似算法，并且是特殊情况下已知最优界的一个很好的插值。

    We consider a general $p$-norm objective for experimental design problems that captures some well-studied objectives (D/A/E-design) as special cases. We prove that a randomized local search approach provides a unified algorithm to solve this problem for all $p$. This provides the first approximation algorithm for the general $p$-norm objective, and a nice interpolation of the best known bounds of the special cases.
    
[^45]: 用全局生成模型探索蛋白质序列空间

    Exploring the Protein Sequence Space with Global Generative Models. (arXiv:2305.01941v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.01941](http://arxiv.org/abs/2305.01941)

    本章概述了使用蛋白质生成模型的三种方法：利用语言模型设计新的人工蛋白质，使用非Transformer架构的工作和应用于定向进化方法的应用。

    

    最近，训练图像和语言的专用大规模架构的进步深刻地影响了计算机视觉和自然语言处理领域。语言模型，例如最近的ChatGPT和GPT4，在处理、翻译和生成人类语言方面表现出了卓越的能力。这些突破也反映在蛋白质研究中，导致了短时间内许多新方法的迅速发展，表现出了前所未有的性能。特别是，语言模型在蛋白质研究中得到了广泛的应用，因为它们被用于嵌入蛋白质、生成新的蛋白质和预测三级结构。在本书章节中，我们概述了蛋白质生成模型的使用，回顾了1）用于设计新的人工蛋白质的语言模型，2）使用非Transformer架构的工作，以及3）应用于定向进化方法的应用。

    Recent advancements in specialized large-scale architectures for training image and language have profoundly impacted the field of computer vision and natural language processing (NLP). Language models, such as the recent ChatGPT and GPT4 have demonstrated exceptional capabilities in processing, translating, and generating human languages. These breakthroughs have also been reflected in protein research, leading to the rapid development of numerous new methods in a short time, with unprecedented performance. Language models, in particular, have seen widespread use in protein research, as they have been utilized to embed proteins, generate novel ones, and predict tertiary structures. In this book chapter, we provide an overview of the use of protein generative models, reviewing 1) language models for the design of novel artificial proteins, 2) works that use non-Transformer architectures, and 3) applications in directed evolution approaches.
    
[^46]: 证明AI模型中稀疏符号概念的出现

    Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models. (arXiv:2305.01939v1 [cs.LG])

    [http://arxiv.org/abs/2305.01939](http://arxiv.org/abs/2305.01939)

    证明了对于训练良好的AI模型，如果满足一定条件，将出现稀疏交互概念，这些概念能够描述输入变量之间的相互作用，并对模型推理分数产生影响。

    

    本文旨在证明训练良好的AI模型中出现符号概念的现象。我们证明，如果（1）模型输出相对于输入变量的高阶导数均为零，（2）AI模型可用于遮挡样本且输入样本较少遮挡时会产生更高的置信度，（3）AI模型在遮挡样本上的置信度并不会显著降低，则AI模型将编码稀疏交互概念。每个交互概念表示特定一组输入变量之间的相互作用，并对模型推理分数产生一定的数值影响。具体而言，我们证明了模型的推理分数总是可以表示为所有交互概念的交互效应之和。事实上，我们希望证明出现符号概念的条件非常普遍。这意味着对于大多数AI模型，我们通常可以使用少量的交互概念来模拟模型。

    This paper aims to prove the emergence of symbolic concepts in well-trained AI models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the AI model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the AI model does not significantly degrade on occluded samples, then the AI model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most AI models, we can usually use a small number of interactive concepts to mimic the mode
    
[^47]: 图神经网络中的条件方法探究

    An Exploration of Conditioning Methods in Graph Neural Networks. (arXiv:2305.01933v1 [cs.LG])

    [http://arxiv.org/abs/2305.01933](http://arxiv.org/abs/2305.01933)

    本文探究图神经网络中的三种条件处理方式：弱条件处理、强条件处理和纯条件处理，对于不同类别的GNNs，有不同的表现，实证研究表明这些条件处理方式对于GNN的性能有影响。

    

    基于消息传递的图神经网络（GNN）的灵活性和有效性在图结构数据的深度学习中取得了相当的进步。在这种方法中，GNN基于其邻居递归地更新节点表示，并通过使用节点和边属性向量来获得表现力。例如，在物理和化学等计算任务中，使用边属性（如相对位置或距离）被证明是必要的。在本文中，我们关注的不是要使用什么类型的属性，而是如何在此信息的基础上进行条件处理，以提高模型性能。我们考虑了三种类型的条件处理：弱条件处理、强条件处理和纯条件处理，分别与基于连接的条件处理、门控和因果依赖于属性的变换相关。这种分类提供了一个统一的观点来看待不同类别的GNN，从可分离卷积到各种形式的消息传递网络。我们在三个常见的任务上进行了实证研究，表明GNNs的性能受到该条件方式的影响。

    The flexibility and effectiveness of message passing based graph neural networks (GNNs) induced considerable advances in deep learning on graph-structured data. In such approaches, GNNs recursively update node representations based on their neighbors and they gain expressivity through the use of node and edge attribute vectors. E.g., in computational tasks such as physics and chemistry usage of edge attributes such as relative position or distance proved to be essential. In this work, we address not what kind of attributes to use, but how to condition on this information to improve model performance. We consider three types of conditioning; weak, strong, and pure, which respectively relate to concatenation-based conditioning, gating, and transformations that are causally dependent on the attributes. This categorization provides a unifying viewpoint on different classes of GNNs, from separable convolutions to various forms of message passing networks. We provide an empirical study on th
    
[^48]: 基于规范的神经网络简化方法，用于大规模形式化验证

    Specification-Driven Neural Network Reduction for Scalable Formal Verification. (arXiv:2305.01932v1 [cs.LG])

    [http://arxiv.org/abs/2305.01932](http://arxiv.org/abs/2305.01932)

    本文提出了一种基于规范的神经网络简化方法用于大规模形式化验证。该方法采用保守的简化方法，确保简化后的网络验证与原网络验证派生等价。简化后可将网络减少到小于5％的神经元数量，从而减少了相应的验证时间。

    

    在神经网络在安全关键环境中部署之前，形式验证是必不可少的。然而，现有的神经网络形式验证方法还无法处理涉及大量神经元的实际问题。本文提出了一种新方法来解决这个挑战：保守的神经网络简化方法，确保简化后的网络验证派生出原网络的验证。我们的方法同时构造简化网络，验证原始网络及其规范。简化将所有输出相似的非线性层神经元合并，适用于具有任何类型的激活函数，如ReLU，sigmoid和tanh的神经网络。我们的评估表明，我们的方法可以将网络减少到小于神经元数的5％，因此可以将验证时间相似减少。

    Formal verification of neural networks is essential before their deployment in safety-critical settings. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems that involve a large number of neurons. In this work, we propose a novel approach to address this challenge: A conservative neural network reduction approach that ensures that the verification of the reduced network implies the verification of the original network. Our approach constructs the reduction on-the-fly, while simultaneously verifying the original network and its specifications. The reduction merges all neurons of a nonlinear layer with similar outputs and is applicable to neural networks with any type of activation function such as ReLU, sigmoid, and tanh. Our evaluation shows that our approach can reduce a network to less than 5% of the number of neurons and thus to a similar degree the verification time is reduced.
    
[^49]: MolKD: 在分子属性预测中提取化学反应中的跨模态知识

    MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction. (arXiv:2305.01912v1 [cs.LG])

    [http://arxiv.org/abs/2305.01912](http://arxiv.org/abs/2305.01912)

    本文提出了一种名为MolKD的新颖方法，通过将化学反应与分子间的跨模态知识提取和转移，为分子表示学习提供辅助，从而提高分子属性预测的效果。

    

    如何有效地表示分子是分子属性预测和药物发现中长期存在的挑战。本文研究了这个问题，提出了将化学领域知识，特别是与化学反应相关的知识，纳入到学习有效分子表示中。 然而，化学反应和分子之间固有的跨模态特性提出了重大挑战。因此，我们介绍了一种新的方法，即MolKD，它在化学反应中提取跨模态知识，以辅助分子属性预测。

    How to effectively represent molecules is a long-standing challenge for molecular property prediction and drug discovery. This paper studies this problem and proposes to incorporate chemical domain knowledge, specifically related to chemical reactions, for learning effective molecular representations. However, the inherent cross-modality property between chemical reactions and molecules presents a significant challenge to address. To this end, we introduce a novel method, namely MolKD, which Distills cross-modal Knowledge in chemical reactions to assist Molecular property prediction. Specifically, the reaction-to-molecule distillation model within MolKD transfers cross-modal knowledge from a pre-trained teacher network learning with one modality (i.e., reactions) into a student network learning with another modality (i.e., molecules). Moreover, MolKD learns effective molecular representations by incorporating reaction yields to measure transformation efficiency of the reactant-product 
    
[^50]: 适应新的类别而不遗忘旧的类别：基于演化字典表示的少样本类别递增学习

    Evolving Dictionary Representation for Few-shot Class-incremental Learning. (arXiv:2305.01885v1 [cs.LG])

    [http://arxiv.org/abs/2305.01885](http://arxiv.org/abs/2305.01885)

    本文提出了一种基于演化字典表示的少样本类别递增学习方法，在基础会话中同时优化字典和特征提取骨干，在增量会话中仅微调字典以适应新类别，避免灾难性遗忘，并在多个基准数据集上实现了最先进的性能。

    

    在不断变化的世界中，新的物体不断出现，一个真正的人工智能系统应该能够持续而有效地适应新出现的类别，而不会忘记旧有的类别。因此，在本文中，我们针对一个具有挑战性和实用性的继续学习场景——少样本类别递增学习进行研究，即在基础会话中为类别提供标记数据，但新的增量类别仅有非常有限的标记实例。为了解决这个问题，我们提出了一种新颖而简洁的方法，即引入深度字典学习，这是一种混合学习架构，结合了字典学习和视觉表示学习，以提供更好的空间来表征不同的类别。我们同时优化基础会话中的字典和特征提取骨干，并仅在增量会话中微调字典以适应新的类别，从而避免了灾难性遗忘。我们的方法在几个基准数据集上实现了最先进的性能，展示了演化字典表示在少样本类别递增学习中的有效性。

    New objects are continuously emerging in the dynamically changing world and a real-world artificial intelligence system should be capable of continual and effectual adaptation to new emerging classes without forgetting old ones. In view of this, in this paper we tackle a challenging and practical continual learning scenario named few-shot class-incremental learning (FSCIL), in which labeled data are given for classes in a base session but very limited labeled instances are available for new incremental classes. To address this problem, we propose a novel and succinct approach by introducing deep dictionary learning which is a hybrid learning architecture that combines dictionary learning and visual representation learning to provide a better space for characterizing different classes. We simultaneously optimize the dictionary and the feature extraction backbone in the base session, while only finetune the dictionary in the incremental session for adaptation to novel classes, which can 
    
[^51]: 一种轻量级CNN-Transformer模型用于学习旅行商问题

    A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])

    [http://arxiv.org/abs/2305.01883](http://arxiv.org/abs/2305.01883)

    本文提出了一种轻量级CNN-Transformer模型，使用CNN嵌入层和部分自注意力，能更好学习输入数据中的空间特征并消除冗余。实验表明该模型在解决旅行商问题方面表现出更好的性能，例如TSP解决方案质量和GPU内存使用方面。

    

    基于Transformer的模型即使在大规模旅行商问题（TSPs）中也表现出最先进的性能。然而，它们基于全连接的注意模型，存在大量的计算复杂性和GPU内存使用。我们提出了一种基于CNN嵌入层和部分自注意力的轻量级CNN-Transformer模型。与标准Transformer模型相比，我们的CNN-Transformer模型能够更好地学习输入数据中的空间特征。它还使用提出的部分自注意力消除了全连接注意模型中的相当数量的冗余。实验表明，与其他最先进的基于Transformer的模型相比，所提出的模型在TSP解决方案质量、GPU内存使用和推理时间方面都表现出更好的性能。我们的模型使用GPU内存约少20％，推理时间比其他最先进的基于Transformer的模型快45％。我们的代码公开可用，网址为https://g

    Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. We propose a lightweight CNN-Transformer model based on a CNN embedding layer and partial self-attention. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer models. It also removes considerable redundancy in fully connected attention models using the proposed partial self-attention. Experiments show that the proposed model outperforms other state-of-the-art Transformer-based models in terms of TSP solution quality, GPU memory usage, and inference time. Our model consumes approximately 20% less GPU memory usage and has 45% faster inference time compared with other state-of-the-art Transformer-based models. Our code is publicly available at https://g
    
[^52]: 使用SpinalNet对星系进行形态分类

    Morphological Classification of Galaxies Using SpinalNet. (arXiv:2305.01873v1 [cs.LG])

    [http://arxiv.org/abs/2305.01873](http://arxiv.org/abs/2305.01873)

    本文使用SpinalNet深度神经网络对星系进行形态分类，取得了高精度的结果。

    

    本文在Galaxy Zoo数据集上应用了一种名为SpinalNet的深度神经网络，它通过模仿人体的皮肤感觉系统逐步引入输入。SpinalNet中的输入分割使得中间层能够获取之前层的部分输入和输出，从而减少了中间层的权重收集量。结果，SpinalNet的作者报告说，在大多数他们测试的DNN中，不仅错误率显著下降，而且计算成本也大大降低。在将其应用于Galaxy Zoo数据集之后，我们能够对星系的不同类别和/或子类别进行分类。因此，我们得到了98.2％、95％和82％的高分类精度，分别在椭圆和螺旋星系之间、在这两者和不规则星系之间以及在10个星系子类别之间。

    Deep neural networks (DNNs) with a step-by-step introduction of inputs, which is constructed by imitating the somatosensory system in human body, known as SpinalNet have been implemented in this work on a Galaxy Zoo dataset. The input segmentation in SpinalNet has enabled the intermediate layers to take some of the inputs as well as output of preceding layers thereby reducing the amount of the collected weights in the intermediate layers. As a result of these, the authors of SpinalNet reported to have achieved in most of the DNNs they tested, not only a remarkable cut in the error but also in the large reduction of the computational costs. Having applied it to the Galaxy Zoo dataset, we are able to classify the different classes and/or sub-classes of the galaxies. Thus, we have obtained higher classification accuracies of 98.2, 95 and 82 percents between elliptical and spirals, between these two and irregulars, and between 10 sub-classes of galaxies, respectively.
    
[^53]: 预训练和搜索：基于预训练神经成本模型的高效嵌入表分片方法

    Pre-train and Search: Efficient Embedding Table Sharding with Pre-trained Neural Cost Models. (arXiv:2305.01868v1 [cs.LG])

    [http://arxiv.org/abs/2305.01868](http://arxiv.org/abs/2305.01868)

    本文探索了一种基于预训练成本模型的高效分片方法，通过神经网络预测成本，并使用在线搜索确定最佳分片计划，实验结果表明其在嵌入表分片任务中表现很好。

    

    在分布式训练中，将大型机器学习模型分片到多个设备上以平衡成本非常重要。由于分区是NP难问题且准确和高效地估算成本很困难，因此这是具有挑战性的。本文探索了一种“预训练和搜索”范式，用于实现高效的分片。该方法是预先训练一个通用的、永久存在的神经网络，来预测所有可能的分片的成本，这个网络就是一个高效的分片模拟器。在此预训练成本模型的基础上，我们进行在线搜索，以确定给定任何特定分片任务的最佳分片计划。在深度学习推荐模型（DLRMs）中，我们将此思想实例化，并提议了NeuroShard用于嵌入表分片。NeuroShard在扩展表上预先训练神经成本模型，以涵盖各种分片场景。然后，使用波束搜索和贪心网格搜索，分别确定最佳的列和表分片计划。实验结果表明

    Sharding a large machine learning model across multiple devices to balance the costs is important in distributed training. This is challenging because partitioning is NP-hard, and estimating the costs accurately and efficiently is difficult. In this work, we explore a "pre-train, and search" paradigm for efficient sharding. The idea is to pre-train a universal and once-for-all neural network to predict the costs of all the possible shards, which serves as an efficient sharding simulator. Built upon this pre-trained cost model, we then perform an online search to identify the best sharding plans given any specific sharding task. We instantiate this idea in deep learning recommendation models (DLRMs) and propose NeuroShard for embedding table sharding. NeuroShard pre-trains neural cost models on augmented tables to cover various sharding scenarios. Then it identifies the best column-wise and table-wise sharding plans with beam search and greedy grid search, respectively. Experiments show
    
[^54]: 无监督改进音频-文本跨模态表征

    Unsupervised Improvement of Audio-Text Cross-Modal Representations. (arXiv:2305.01864v1 [cs.SD])

    [http://arxiv.org/abs/2305.01864](http://arxiv.org/abs/2305.01864)

    本研究探索了无监督的方法来改进跨模态音频-文本表征学习，通过使用领域特定的筛选和软标注对比性损失，成功提高了零-shot分类性能。

    

    最近通过使用语言模型来获得跨模态音频-文本表征取得了进展，克服了使用预定义标签的传统训练方法的局限性。这使得社区能够在零-shot分类等任务上取得进展，否则是不可能的。然而，学习这样的表征需要大量的人工注释的音频-文本对。本文研究了使用未配对文本和音频改进这些表征学习框架的无监督方法。我们探索了领域非特定和领域特定的筛选方法，创建我们用于进一步改进模型的音频-文本对。我们还表明，当与软标注对比性损失结合使用领域特定筛选时，我们能够在下游声音事件分类或声学场景分类任务的零-shot分类性能方面取得显着的改进。

    Recent advances in using language models to obtain cross-modal audio-text representations have overcome the limitations of conventional training approaches that use predefined labels. This has allowed the community to make progress in tasks like zero-shot classification, which would otherwise not be possible. However, learning such representations requires a large amount of human-annotated audio-text pairs. In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio. We explore domain-unspecific and domain-specific curation methods to create audio-text pairs that we use to further improve the model. We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks.
    
[^55]: 不确定多变量系统的推断矩

    Inferential Moments of Uncertain Multivariable Systems. (arXiv:2305.01841v1 [physics.data-an])

    [http://arxiv.org/abs/2305.01841](http://arxiv.org/abs/2305.01841)

    本文提出了一种新的分析不确定多变量系统行为的方法，使用推断矩描述分布预计如何响应新信息，特别关注推断偏差，以改善情境感知能力。

    

    本文提出了一种使用称为“推断矩”的一组量来分析不确定多变量系统行为的新范式。边缘化是一种不确定性量化过程，它通过平均条件概率来量化所关注概率的期望值。推断矩是描述分布预计如何响应新信息的高阶条件概率矩。本文研究了推断偏差，它是期望的概率波动，随着推断更新另一个变量而发生变化。我们以推断矩的形式找到了互信息的幂级数展开式，这意味着推断矩逻辑可能对通常使用信息论工具执行的任务有用。我们在两个应用中探讨了贝叶斯网络的推断偏差，以改善情境感知能力。

    This article offers a new paradigm for analyzing the behavior of uncertain multivariable systems using a set of quantities we call \emph{inferential moments}. Marginalization is an uncertainty quantification process that averages conditional probabilities to quantify the \emph{expected value} of a probability of interest. Inferential moments are higher order conditional probability moments that describe how a distribution is expected to respond to new information. Of particular interest in this article is the \emph{inferential deviation}, which is the expected fluctuation of the probability of one variable in response to an inferential update of another. We find a power series expansion of the Mutual Information in terms of inferential moments, which implies that inferential moment logic may be useful for tasks typically performed with information theoretic tools. We explore this in two applications that analyze the inferential deviations of a Bayesian Network to improve situational aw
    
[^56]: AV-SAM:音视频定位与分割任务中的万能分割模型

    AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation. (arXiv:2305.01836v1 [cs.CV])

    [http://arxiv.org/abs/2305.01836](http://arxiv.org/abs/2305.01836)

    本文提出了一个音视频定位与分割的框架AV-SAM。AV-SAM基于SAM模型，能够在Flickr-SoundNet和AVSBench数据集上达到竞争性的性能，在音视频任务中具有广泛的实际应用前景。

    

    万能分割模型（SAM）近来在视觉分割任务中表现出了卓越的效果。但是，在音视频任务中，比如视音频定位与分割，SAM的表现却鲜有实验探索。本文提出了一种简单而有效的基于SAM的音视频定位与分割框架AV-SAM，它可以生成相应于音频的声音对象掩码。具体来说，我们的AV-SAM简单地利用来自SAM中预训练图像编码器的视觉特征和音频特征的逐像素音视频融合来聚合跨模态表征。然后，将聚合后的跨模态特征馈入提示编码器和掩码解码器中生成最终的音视频分割掩码。我们在Flickr-SoundNet和AVSBench数据集上进行了广泛的实验。结果表明，与最先进的方法相比，所提出的AV-SAM在声音对象定位和分割方面可以取得竞争性的性能。

    Segment Anything Model (SAM) has recently shown its powerful effectiveness in visual segmentation tasks. However, there is less exploration concerning how SAM works on audio-visual tasks, such as visual sound localization and segmentation. In this work, we propose a simple yet effective audio-visual localization and segmentation framework based on the Segment Anything Model, namely AV-SAM, that can generate sounding object masks corresponding to the audio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion across audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Then, the aggregated cross-modal features are fed into the prompt encoder and mask decoder to generate the final audio-visual segmentation masks. We conduct extensive experiments on Flickr-SoundNet and AVSBench datasets. The results demonstrate that the proposed AV-SAM can achieve competitive performance on sounding object localization an
    
[^57]: 对临床脑部MRI扫描的大规模神经影像学研究进行的皮层分析（arXiv：2305.01827v1 [eess.IV]）

    Cortical analysis of heterogeneous clinical brain MRI scans for large-scale neuroimaging studies. (arXiv:2305.01827v1 [eess.IV])

    [http://arxiv.org/abs/2305.01827](http://arxiv.org/abs/2305.01827)

    本文提出了一种新的方法，可以对任何分辨率和脉冲序列的临床脑部MRI扫描进行皮层重建、配准、分割和厚度估计，为大规模神经影像学研究、尤其是针对少数族裔和罕见疾病的研究提供了可能。

    

    基于MRI的皮层表面分析是人类神经影像学中普遍存在的，例如用于皮层配准、分割或厚度估计。凹凸不平的皮层几何形状需要等距扫描（例如1mm MPRAGEs）和好的灰白质对比度进行3D重建。这排除了对于大多数出于临床目的而采集的脑部MRI扫描的分析。分析这些扫描将使神经影像学研究具备当前研究数据集无法实现的样本量，特别适用于少数族裔和罕见疾病的研究。本文提出了首个针对任何分辨率和脉冲序列的临床脑部MRI扫描进行皮层重建、配准、分割和厚度估计的方法。该方法包含学习组件和经典优化模块。前者使用领域随机化来训练CNN模型，以预测白质和脑外膜表面（符号距离函数）的隐式表示，达到等距的1mm的分辨率。

    Surface analysis of the cortex is ubiquitous in human neuroimaging with MRI, e.g., for cortical registration, parcellation, or thickness estimation. The convoluted cortical geometry requires isotropic scans (e.g., 1mm MPRAGEs) and good gray-white matter contrast for 3D reconstruction. This precludes the analysis of most brain MRI scans acquired for clinical purposes. Analyzing such scans would enable neuroimaging studies with sample sizes that cannot be achieved with current research datasets, particularly for underrepresented populations and rare diseases. Here we present the first method for cortical reconstruction, registration, parcellation, and thickness estimation for clinical brain MRI scans of any resolution and pulse sequence. The methods has a learning component and a classical optimization module. The former uses domain randomization to train a CNN that predicts an implicit representation of the white matter and pial surfaces (a signed distance function) at 1mm isotropic res
    
[^58]: 针对昆虫分类的鲁棒性OOD检测算法

    Out-of-distribution detection algorithms for robust insect classification. (arXiv:2305.01823v1 [cs.CV])

    [http://arxiv.org/abs/2305.01823](http://arxiv.org/abs/2305.01823)

    本文研究了用于昆虫分类鲁棒性的OOD检测算法，通过不同类别的技术评估，发现基于生成模型的方法是最有效的，能够在实际农业领域中应用。

    

    基于深度学习的方法已经取得了良好的昆虫分类准确性；但是这些模型大多数适用于受控环境下的应用，而在现实的农业领域中往往受到许多因素的干扰。其中一个主要的问题是输入的图像超出分布范围（例如车辆、动物、人类或模糊的昆虫或未经训练的昆虫类的图像）可能会导致错误的分类。OOD检测算法提供了一种解决这些问题的方法，它确保模型不会在非昆虫或未训练的昆虫类图像上进行错误的分类预测。我们生成并评估了最先进的OOD算法在昆虫检测分类器上的性能。这些算法代表了多种方法来解决OOD问题。具体来说，我们评估了基于不确定性、基于特征空间和基于生成模型的三类技术。我们的实验表明，基于生成模型的方法是检测OOD图像最有效的方法，在许多数据集上实现了接近1.0的AUC分数。

    Deep learning-based approaches have produced models with good insect classification accuracy; Most of these models are conducive for application in controlled environmental conditions. One of the primary emphasis of researchers is to implement identification and classification models in the real agriculture fields, which is challenging because input images that are wildly out of the distribution (e.g., images like vehicles, animals, humans, or a blurred image of an insect or insect class that is not yet trained on) can produce an incorrect insect classification. Out-of-distribution (OOD) detection algorithms provide an exciting avenue to overcome these challenge as it ensures that a model abstains from making incorrect classification prediction of non-insect and/or untrained insect class images. We generate and evaluate the performance of state-of-the-art OOD algorithms on insect detection classifiers. These algorithms represent a diversity of methods for addressing an OOD problem. Spe
    
[^59]: 没有配对数据的流体流动降尺度方法——扩散桥

    Unpaired Downscaling of Fluid Flows with Diffusion Bridges. (arXiv:2305.01822v1 [cs.LG])

    [http://arxiv.org/abs/2305.01822](http://arxiv.org/abs/2305.01822)

    本文提出了一种基于扩散映射和生成模型的方法，能够在没有配对训练数据的情况下进行流体模拟的降尺度处理，该方法可有效地增强分辨率并校正上下文相关偏差。

    

    我们提出了一种使用基于扩散映射的生成模型对理想化地球物理流体模拟进行降尺度的方法。通过分析从不同数据分布中绘制的图像的傅里叶光谱，我们展示了如何将两个独立的有条件扩散模型链接在一起以用于域转换。所得到的转换是低分辨率和高分辨率数据集之间的扩散桥，可以在给定特定低分辨率特征的情况下生成新的高分辨率图像样本。生成新样本的能力使得可以计算任何感兴趣的统计量，而无需进行任何额外的校准或训练。我们的无监督设置还旨在在没有访问配对训练数据的情况下对图像进行降尺度处理。这种灵活性允许将多个源域和目标域组合在一起，而无需进行额外的训练。我们演示了这种方法如何提高地球物理流体模拟的分辨率并纠正上下文相关偏差。

    We present a method to downscale idealized geophysical fluid simulations using generative models based on diffusion maps. By analyzing the Fourier spectra of images drawn from different data distributions, we show how one can chain together two independent conditional diffusion models for use in domain translation. The resulting transformation is a diffusion bridge between a low resolution and a high resolution dataset and allows for new sample generation of high-resolution images given specific low resolution features. The ability to generate new samples allows for the computation of any statistic of interest, without any additional calibration or training. Our unsupervised setup is also designed to downscale images without access to paired training data; this flexibility allows for the combination of multiple source and target domains without additional training. We demonstrate that the method enhances resolution and corrects context-dependent biases in geophysical fluid simulations,
    
[^60]: 基于协方差神经网络的可转移学习和应用于解释性脑龄预测

    Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features. (arXiv:2305.01807v1 [cs.LG])

    [http://arxiv.org/abs/2305.01807](http://arxiv.org/abs/2305.01807)

    本研究首次从理论上研究了基于协方差神经网络的可转移性，证明了当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。

    

    图卷积网络（GCN）利用基于拓扑图的卷积操作来组合图上的信息进行推理任务。我们最近的工作中，通过使用协方差矩阵作为图来设计了一种类似于传统PCA数据分析方法的协方差神经网络（VNN），并具有显著的优势。本文首先从理论上研究了VNN的可转移性。可转移性的概念是从学习模型可以在“兼容”的数据集上泛化的直观期望中产生的。我们展示了VNN从GCN继承的无标度数据处理架构，并证明当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。

    Graph convolutional networks (GCN) leverage topology-driven graph convolutional operations to combine information across the graph for inference tasks. In our recent work, we have studied GCNs with covariance matrices as graphs in the form of coVariance neural networks (VNNs) that draw similarities with traditional PCA-driven data analysis approaches while offering significant advantages over them. In this paper, we first focus on theoretically characterizing the transferability of VNNs. The notion of transferability is motivated from the intuitive expectation that learning models could generalize to "compatible" datasets (possibly of different dimensionalities) with minimal effort. VNNs inherit the scale-free data processing architecture from GCNs and here, we show that VNNs exhibit transferability of performance over datasets whose covariance matrices converge to a limit object. Multi-scale neuroimaging datasets enable the study of the brain at multiple scales and hence, can validate
    
[^61]: 当新的不一定是更好的：深度学习是否真正受益于基于隐式反馈的推荐？

    When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?. (arXiv:2305.01801v1 [cs.IR])

    [http://arxiv.org/abs/2305.01801](http://arxiv.org/abs/2305.01801)

    本研究对多个神经推荐模型与传统模型进行比较，提出了一组评估策略来衡量其记忆性能、泛化性能和子群特定性能，揭示了在IMDB和Yelp数据集上，神经推荐模型与传统模型的差异性。

    

    最近几年，神经模型被多次宣传为推荐领域的最先进技术，但是多个研究表明，许多神经推荐模型的最新结果并不能可靠地复现。一个主要原因是现有的评估是在不一致的协议下进行的。因此，这些可重复性问题使人们难以了解实际上可以从这些神经模型中获得多少益处。因此，需要一个公平而全面的绩效比较来比较传统模型和神经模型。为此，我们进行了一项大规模、系统性的研究，比较了基于隐式数据的顶部推荐的最新神经推荐模型和传统模型。我们提出了一组评估策略，用于衡量推荐模型的记忆性能、泛化性能和子群特定性能。

    In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported state-of-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed.  Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-n recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. 
    
[^62]: MISNN: 基于半参数神经网络的多重插补方法

    MISNN: Multiple Imputation via Semi-parametric Neural Networks. (arXiv:2305.01794v1 [stat.ME])

    [http://arxiv.org/abs/2305.01794](http://arxiv.org/abs/2305.01794)

    MISNN是一种基于半参数神经网络的多重插补方法，具有高效性和良好的插补精度，尤其适用于高维数据和特征选择问题。

    

    多重插补方法（MI）被广泛应用于生物医学、社会和计量研究中的缺失值问题，以避免下游数据分析中的不当推断。在存在高维数据时，包括特征选择的插补模型，尤其是$\ell_1$规则化回归（如Lasso、自适应Lasso和弹性网络）是防止模型欠决定的常见选择。然而，使用特征选择进行MI是困难的：现有方法通常计算效率低下且性能差。我们提出了一种新颖而有效的算法MISNN，它结合了神经网络的近似能力，是一个通用和灵活的框架，兼容任何特征选择方法、任何神经网络结构、高/低维数据和一般缺失模式。通过实证实验，MISNN在计算效率和插补精度方面展现了比现有方法更大的优势。

    Multiple imputation (MI) has been widely applied to missing value problems in biomedical, social and econometric research, in order to avoid improper inference in the downstream data analysis. In the presence of high-dimensional data, imputation models that include feature selection, especially $\ell_1$ regularized regression (such as Lasso, adaptive Lasso, and Elastic Net), are common choices to prevent the model from underdetermination. However, conducting MI with feature selection is difficult: existing methods are often computationally inefficient and poor in performance. We propose MISNN, a novel and efficient algorithm that incorporates feature selection for MI. Leveraging the approximation power of neural networks, MISNN is a general and flexible framework, compatible with any feature selection method, any neural network architecture, high/low-dimensional data and general missing patterns. Through empirical experiments, MISNN has demonstrated great advantages over state-of-the-a
    
[^63]: 卫星衍生贫困地图中的公平性和代表性: 城乡差距及其对下游政策的影响证据

    Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy. (arXiv:2305.01783v1 [cs.LG])

    [http://arxiv.org/abs/2305.01783](http://arxiv.org/abs/2305.01783)

    本文研究了城乡基于卫星的贫困映射中的代表性差异、预测误差中的系统偏差和公平性问题，并展示了这些现象如何影响基于预测地图的政策的有效性。

    

    卫星图像衍生的贫困地图越来越多地被用来决定分配人道主义援助和政府资源等高风险政策问题。这些贫困地图通常通过使用相对较少的调查“地面真实性”数据训练机器学习算法，然后预测在存在图像但无调查的区域的贫困水平来构建的。本文利用十个国家的调查和卫星数据，研究了城乡基于卫星的贫困映射中的代表性差异，预测误差中的系统偏差和公平性问题，并展示了这些现象如何影响基于预测地图的政策的有效性。我们的发现强调了在将卫星衍生的贫困地图用于现实政策决策前进行仔细的误差和偏差分析的重要性。

    Poverty maps derived from satellite imagery are increasingly used to inform high-stakes policy decisions, such as the allocation of humanitarian aid and the distribution of government resources. Such poverty maps are typically constructed by training machine learning algorithms on a relatively modest amount of ``ground truth" data from surveys, and then predicting poverty levels in areas where imagery exists but surveys do not. Using survey and satellite data from ten countries, this paper investigates disparities in representation, systematic biases in prediction errors, and fairness concerns in satellite-based poverty mapping across urban and rural lines, and shows how these phenomena affect the validity of policies based on predicted maps. Our findings highlight the importance of careful error and bias analysis before using satellite-based poverty maps in real-world policy decisions.
    
[^64]: 通过流形展平和重构进行表示学习

    Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])

    [http://arxiv.org/abs/2305.01777](http://arxiv.org/abs/2305.01777)

    本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。

    

    本文提出了一种算法，可以从流形的有限样本中显式构建一对神经网络，用于线性化和重构嵌入子流形。我们所生成的神经网络称为展平网络（FlatNet），在理论上具有可解释性，在计算上可扩展性强，并且在测试数据上具有良好的泛化性能，这种平衡通常在基于流形的学习方法中难以实现。我们基于合成的高维流形数据和2D图像数据进行了实证实验，并与其他模型进行了比较。我们的代码是公开的。

    This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
    
[^65]: 大规模动态系统的深度状态空间模型的廉价和确定性推断

    Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems. (arXiv:2305.01773v1 [cs.LG])

    [http://arxiv.org/abs/2305.01773](http://arxiv.org/abs/2305.01773)

    本文提出了一种利用图神经网络建模大规模动态系统的深度状态空间模型，在保持多峰预测分布的准确性的同时，通过确定性矩匹配规则实现了无样本推断，提高了预测的效率和稳定性。

    

    图神经网络通常被用于建模相互作用的动态系统，因为它们优雅地适应于具有变化和大量代理的系统。虽然在确定性相互作用系统方面取得了很多进展，但对于有兴趣获得未来轨迹的预测分布的随机系统，模型更具挑战性。现有方法要么计算速度慢，因为它们依赖于蒙特卡罗抽样，要么做出简化假设，使得预测分布是单峰的。在本文中，我们提出了一个深度状态空间模型，它采用图神经网络来建模底层的相互作用动态系统。预测分布是多峰的，并且具有高斯混合模型的形式，其中高斯分量的矩可以通过确定性矩匹配规则计算。我们的矩匹配方案可以用于无样本推断，从而实现更有效和稳定的预测。实验结果表明，我们的方法能够有效地建模和预测随机系统的轨迹，即使存在巨大的不确定性。

    Graph neural networks are often used to model interacting dynamical systems since they gracefully scale to systems with a varying and high number of agents. While there has been much progress made for deterministic interacting systems, modeling is much more challenging for stochastic systems in which one is interested in obtaining a predictive distribution over future trajectories. Existing methods are either computationally slow since they rely on Monte Carlo sampling or make simplifying assumptions such that the predictive distribution is unimodal. In this work, we present a deep state-space model which employs graph neural networks in order to model the underlying interacting dynamical system. The predictive distribution is multimodal and has the form of a Gaussian mixture model, where the moments of the Gaussian components can be computed via deterministic moment matching rules. Our moment matching scheme can be exploited for sample-free inference, leading to more efficient and sta
    
[^66]: DeCom：基于深度耦合因子分解机的非药物干预感知的COVID-19后RSV预测

    DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness. (arXiv:2305.01770v1 [cs.LG])

    [http://arxiv.org/abs/2305.01770](http://arxiv.org/abs/2305.01770)

    DeCom是一种基于深度耦合因子分解机的RSV预测算法，该算法结合了正常的季节性RSV传播模式和COVID-19符合NPI措施下RSV传播的不稳定性，预测效果更加准确。

    

    呼吸道合胞病毒（RSV）是婴幼儿最危险的呼吸系统疾病之一。由于COVID-19的非药物干预（NPI）措施，RSV的季节性传播模式在2020年已经中断，并在2021年北半球提前数月出现转变。因此，理解COVID-19如何影响RSV并构建预测算法以预测COVID-19后RSV再出现的时间和强度至关重要。本文提出了一种名为DeCom的深度耦合张量分解机，用于COVID-19后RSV预测。DeCom利用张量分解和残差建模，能可靠地学习受COVID-19影响下的RSV传播，同时考虑正常季节性RSV传播模式和NPI。在真实的RSV数据集上的实验结果显示，DeCom比现有RSV预测算法更准确，并可最大程度地提高预测效果。

    Respiratory syncytial virus (RSV) is one of the most dangerous respiratory diseases for infants and young children. Due to the nonpharmaceutical intervention (NPI) imposed in the COVID-19 outbreak, the seasonal transmission pattern of RSV has been discontinued in 2020 and then shifted months ahead in 2021 in the northern hemisphere. It is critical to understand how COVID-19 impacts RSV and build predictive algorithms to forecast the timing and intensity of RSV reemergence in post-COVID-19 seasons. In this paper, we propose a deep coupled tensor factorization machine, dubbed as DeCom, for post COVID-19 RSV prediction. DeCom leverages tensor factorization and residual modeling. It enables us to learn the disrupted RSV transmission reliably under COVID-19 by taking both the regular seasonal RSV transmission pattern and the NPI into consideration. Experimental results on a real RSV dataset show that DeCom is more accurate than the state-of-the-art RSV prediction algorithms and achieves up 
    
[^67]: 受心理学启发的因果提示语

    Psychologically-Inspired Causal Prompts. (arXiv:2305.01764v1 [cs.CL])

    [http://arxiv.org/abs/2305.01764](http://arxiv.org/abs/2305.01764)

    本文提出了三个因果提示语，涵盖了情感分类任务中人类的心理过程。这些提示语可以用来产生更准确和可解释的模型预测。

    

    NLP数据集不仅仅含有输入输出对，还包含输入和输出变量之间的因果关系。本文以情感分类为例，探讨评论（X）和情感（Y）之间的因果关系。心理学研究表明，语言可以影响情绪，当一个人首次进行评分并在评论中进行自我合理化时（情感引起评论，即Y->X），与首先描述自己的经历并权衡利弊以做出最后评分时（评论引起情感，即X->Y），会引发不同的心理过程。此外，如果评注者通过心智理论（ToM）推断用户的原始评分，则这也是完全不同的心理过程（评论引起评分，即X-ToM-> Y）。本文将这三种情感分类的人类心理过程的因果机制转化为三个提示语，并在情感分类任务中应用这些提示语，以产生更准确和可解释的模型预测。

    NLP datasets are richer than just input-output pairs; rather, they carry causal relations between the input and output variables. In this work, we take sentiment classification as an example and look into the causal relations between the review (X) and sentiment (Y). As psychology studies show that language can affect emotion, different psychological processes are evoked when a person first makes a rating and then self-rationalizes their feeling in a review (where the sentiment causes the review, i.e., Y -> X), versus first describes their experience, and weighs the pros and cons to give a final rating (where the review causes the sentiment, i.e., X -> Y ). Furthermore, it is also a completely different psychological process if an annotator infers the original rating of the user by theory of mind (ToM) (where the review causes the rating, i.e., X -ToM-> Y ). In this paper, we verbalize these three causal mechanisms of human psychological processes of sentiment classification into three
    
[^68]: 空间-时间网络用于抗生素敏感性图案预测

    Spatial-Temporal Networks for Antibiogram Pattern Prediction. (arXiv:2305.01761v1 [cs.LG])

    [http://arxiv.org/abs/2305.01761](http://arxiv.org/abs/2305.01761)

    本文提出了一个新颖的问题，即抗生素敏感性图案预测，旨在预测未来哪些图案将出现，并解决了这一问题遇到的挑战。

    

    抗生素敏感性图案是对感染患者的抗生素耐药性检测结果进行周期性总结。抗生素敏感性图案有助于医生了解地区耐药性率并选择适当的处方抗生素。本文提出了一个新颖的问题，即抗生素敏感性图案预测，旨在预测未来哪些图案将出现。尽管该问题的重要性，但处理该问题会遇到一系列挑战，并且在文献中尚未得到探索。首先，抗生素敏感性图案不是独立同分布的，因为它们可能由于基因相似性而彼此紧密相关。

    An antibiogram is a periodic summary of antibiotic resistance results of organisms from infected patients to selected antimicrobial drugs. Antibiograms help clinicians to understand regional resistance rates and select appropriate antibiotics in prescriptions. In practice, significant combinations of antibiotic resistance may appear in different antibiograms, forming antibiogram patterns. Such patterns may imply the prevalence of some infectious diseases in certain regions. Thus it is of crucial importance to monitor antibiotic resistance trends and track the spread of multi-drug resistant organisms. In this paper, we propose a novel problem of antibiogram pattern prediction that aims to predict which patterns will appear in the future. Despite its importance, tackling this problem encounters a series of challenges and has not yet been explored in the literature. First of all, antibiogram patterns are not i.i.d as they may have strong relations with each other due to genomic similariti
    
[^69]: 基于对抗生成的非负矩阵分解单声道源分离方法

    Adversarial Generative NMF for Single Channel Source Separation. (arXiv:2305.01758v1 [eess.AS])

    [http://arxiv.org/abs/2305.01758](http://arxiv.org/abs/2305.01758)

    本文提出了一种基于对抗生成的非负矩阵分解单声道源分离方法，通过对抗训练NMF基，实现了在没有强监督数据可用的情况下提高重构信号质量的效果。

    

    对抗学习正则化函数的思想最近被引入了更广泛的反问题背景下。该方法的灵感在于意识到不仅需要学习组成所需信号的基本特征，而且或者更重要的是，需要学习避免表示中的哪些特征。在本文中，我们将应用这种方法来解决通过非负矩阵分解进行源分离的问题，并提出一种新的对抗训练NMF基的方法。我们通过数值实验表明，无论是图像还是音频分离，在没有强监督数据可用的情况下，都会明显提高重构信号的质量。

    The idea of adversarial learning of regularization functionals has recently been introduced in the wider context of inverse problems. The intuition behind this method is the realization that it is not only necessary to learn the basic features that make up a class of signals one wants to represent, but also, or even more so, which features to avoid in the representation. In this paper, we will apply this approach to the problem of source separation by means of non-negative matrix factorization (NMF) and present a new method for the adversarial training of NMF bases. We show in numerical experiments, both for image and audio separation, that this leads to a clear improvement of the reconstructed signals, in particular in the case where little or no strong supervision data is available.
    
[^70]: 单模型不确定性量化在神经网络势能中并不能始终优于模型集合

    Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles. (arXiv:2305.01754v1 [cs.LG])

    [http://arxiv.org/abs/2305.01754](http://arxiv.org/abs/2305.01754)

    本文比较了多种不确定性量化方案，以提高NNIP的鲁棒性，发现集合方法一致表现更好，且高效。

    

    神经网络常常将高置信度分配给其预测结果，即使是对于远离数据分布的点，这使得不确定性量化成为一项挑战。当它们被用于模拟材料系统中的原子间势时，这个问题会导致不真实的结构破坏模拟，或者导致偏误的统计和动力学不能反映真正的物理学。可微不确定性量化技术可以发现新的信息数据，推动鲁棒势的主动学习循环。然而，对于原子模拟存在各种不确定性量化技术，包括新开发的技术，不存在清晰的指导方针以确定哪种技术最为有效或适合特定情况。在本文中，我们比较了多个UQ方案来改进NN原子间势（NNIPs）的鲁棒性，特别是我们将现有的基于集合的方法与使用单一的确定性NNs的策略进行了比较：平均差异估计，深度证据回归和异方差回归。令人惊讶的是，我们发现集合方法在多个材料系统和UQ指标上表现一致更好，这表明它们在某些情况下仍然与更高效的单模型替代品竞争力相当，有时甚至更好。

    Neural networks (NNs) often assign high confidence to their predictions, even for points far out-of-distribution, making uncertainty quantification (UQ) a challenge. When they are employed to model interatomic potentials in materials systems, this problem leads to unphysical structures that disrupt simulations, or to biased statistics and dynamics that do not reflect the true physics. Differentiable UQ techniques can find new informative data and drive active learning loops for robust potentials. However, a variety of UQ techniques, including newly developed ones, exist for atomistic simulations and there are no clear guidelines for which are most effective or suitable for a given case. In this work, we examine multiple UQ schemes for improving the robustness of NN interatomic potentials (NNIPs) through active learning. In particular, we compare incumbent ensemble-based methods against strategies that use single, deterministic NNs: mean-variance estimation, deep evidential regression, 
    
[^71]: 带有有限注释的分割任务中的期望最大化伪标签方法研究

    Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])

    [http://arxiv.org/abs/2305.01747](http://arxiv.org/abs/2305.01747)

    本文提出了一种伪标签的泛化方法，称为贝叶斯伪标签，在半监督医学图像分割任务中应用效果良好。

    

    本文研究了半监督医学图像分割中的伪标签及其推广，伪标签通过利用未标记数据的原始推断作为自训练的伪标签，在半监督学习中取得了巨大的实证成功。我们建立了伪标签和期望最大化算法之间的联系，部分解释了其实证成功。在此基础上，我们展示了贝叶斯原理下伪标签的完全泛化，称为贝叶斯伪标签。然后，我们提供了一种变分方法来学习逼近贝叶斯伪标签，通过学习选择高质量伪标签的阈值。接下来，我们在医学图像分割的半监督学习中展示了伪标签和其推广贝叶斯伪标签的应用。

    We study pseudo labelling and its generalisation for semi-supervised segmentation of medical images. Pseudo labelling has achieved great empirical successes in semi-supervised learning, by utilising raw inferences on unlabelled data as pseudo labels for self-training. In our paper, we build a connection between pseudo labelling and the Expectation Maximization algorithm which partially explains its empirical successes. We thereby realise that the original pseudo labelling is an empirical estimation of its underlying full formulation. Following this insight, we demonstrate the full generalisation of pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then provide a variational approach to learn to approximate Bayesian Pseudo Labels, by learning a threshold to select good quality pseudo labels. In the rest of the paper, we demonstrate the applications of Pseudo Labelling and its generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of medical images
    
[^72]: 利用因子化动作空间在医疗保健中进行高效的离线强化学习

    Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare. (arXiv:2305.01738v1 [cs.LG])

    [http://arxiv.org/abs/2305.01738](http://arxiv.org/abs/2305.01738)

    本论文提出了一种利用因子化动作空间的线性Q函数分解形式的方法，用于解决离线强化学习中存在的动作组合问题，该方法在提高采样效率的同时并不牺牲策略最优性，通过模拟器和实际数据集的几个离线强化学习问题的实验表明，相较于标准方法，该方法具有更快的收敛速度、更好的性能和更高的采样效率。

    

    许多强化学习应用程序具有组合动作空间，其中每个动作是子动作的组合。标准强化学习方法忽略了这种固有的分解结构，导致可能对少见的子动作组合做出的推理没有意义；这在离线设置下尤其问题突出，因为数据可能受限。在这项工作中，我们提出了一种由因子化动作空间引起的线性Q函数分解的形式。我们研究了我们的方法的理论性质，确定了当用于近似Q函数时保证产生零偏差的情况。在具有理论保证的范围之外的情况下，我们表明我们的方法仍然是有用的，因为它提高了采样效率而不一定牺牲策略最优性，允许我们实现更好的偏差-方差权衡。在使用由医疗保健启示的模拟器和实际数据集进行的几个离线强化学习问题中，我们证明了我们的方法比标准方法具有更快的收敛速度、更好的性能和更高的采样效率。

    Many reinforcement learning (RL) applications have combinatorial action spaces, where each action is a composition of sub-actions. A standard RL approach ignores this inherent factorization structure, resulting in a potential failure to make meaningful inferences about rarely observed sub-action combinations; this is particularly problematic for offline settings, where data may be limited. In this work, we propose a form of linear Q-function decomposition induced by factored action spaces. We study the theoretical properties of our approach, identifying scenarios where it is guaranteed to lead to zero bias when used to approximate the Q-function. Outside the regimes with theoretical guarantees, we show that our approach can still be useful because it leads to better sample efficiency without necessarily sacrificing policy optimality, allowing us to achieve a better bias-variance trade-off. Across several offline RL problems using simulators and real-world datasets motivated by healthca
    
[^73]: 通过高斯过程核表达寿命模型的灵活性

    Expressive Mortality Models through Gaussian Process Kernels. (arXiv:2305.01728v1 [stat.ML])

    [http://arxiv.org/abs/2305.01728](http://arxiv.org/abs/2305.01728)

    本研究基于高斯过程框架，利用核函数的加法和乘法结构设计了一个遗传编程算法，能够学习特定人群的年龄和年份特定的死亡率曲面，为不同人群中队列效应的存在性带来了新的见解，并提供了相对平滑程度的分析工作。

    

    我们开发了一个灵活的高斯过程（GP）框架，用于学习年龄和年份特定的死亡率曲面的协方差结构。利用GP核的加法和乘法结构，我们设计一个遗传编程算法来搜索针对给定人群的最具表现力的核。我们的组合搜索基于年龄-期间-队列（APC）范例，以构建最能匹配死亡率数据集的时空动态的协方差先验。我们在合成案例研究中应用得到的遗传算法（GA）来验证GA恢复APC结构的能力，并在人类死亡数据库的实际国家级数据集上进行分析。我们的机器学习分析提供了有关不同人群中队列效应存在或不存在的新见解，以及沿年龄和年份维度的死亡率曲面的相对平滑程度。我们的建模工作是在Python的PyTorch库中完成的，提供了深入的分析。

    We develop a flexible Gaussian Process (GP) framework for learning the covariance structure of Age- and Year-specific mortality surfaces. Utilizing the additive and multiplicative structure of GP kernels, we design a genetic programming algorithm to search for the most expressive kernel for a given population. Our compositional search builds off the Age-Period-Cohort (APC) paradigm to construct a covariance prior best matching the spatio-temporal dynamics of a mortality dataset. We apply the resulting genetic algorithm (GA) on synthetic case studies to validate the ability of the GA to recover APC structure, and on real-life national-level datasets from the Human Mortality Database. Our machine-learning based analysis provides novel insight into the presence/absence of Cohort effects in different populations, and into the relative smoothness of mortality surfaces along the Age and Year dimensions. Our modelling work is done with the PyTorch libraries in Python and provides an in-depth 
    
[^74]: 大数据学习的慢杀技巧

    Slow Kill for Big Data Learning. (arXiv:2305.01726v1 [stat.ML])

    [http://arxiv.org/abs/2305.01726](http://arxiv.org/abs/2305.01726)

    本文提出了一种称为“慢杀”的技术，它利用非凸约束优化、自适应$\ell_2$收缩和逐步增加的学习率，可以在大规模数据上实现高效变量筛选和统计精度。

    

    大数据应用通常涉及大量的观察和特征，这为变量选择和参数估计带来了新的挑战。本文介绍了一种称为“慢杀”的新技术，它利用非凸约束优化、自适应$\ell_2$收缩和逐步增加的学习率。在慢杀迭代过程中，问题规模可以减小，这使其特别适用于大规模变量筛选。统计和优化之间的相互作用提供了有关控制分位数、步长和收缩参数以放松所需的正则性条件以实现所需统计精度的有价值的见解。实验结果表明慢杀在各种情况下优于现有算法，并且在大规模数据上具有高效的计算能力。

    Big-data applications often involve a vast number of observations and features, creating new challenges for variable selection and parameter estimation. This paper presents a novel technique called ``slow kill,'' which utilizes nonconvex constrained optimization, adaptive $\ell_2$-shrinkage, and increasing learning rates. The fact that the problem size can decrease during the slow kill iterations makes it particularly effective for large-scale variable screening. The interaction between statistics and optimization provides valuable insights into controlling quantiles, stepsize, and shrinkage parameters in order to relax the regularity conditions required to achieve the desired level of statistical accuracy. Experimental results on real and synthetic data show that slow kill outperforms state-of-the-art algorithms in various situations while being computationally efficient for large-scale data.
    
[^75]: DeepAqua:使用知识蒸馏方法从SAR图像自我监督分割湿地的语义信息

    DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation. (arXiv:2305.01698v1 [cs.CV])

    [http://arxiv.org/abs/2305.01698](http://arxiv.org/abs/2305.01698)

    本文提出了DeepAqua，一种利用自我监督深度学习模型，使用知识蒸馏技术来从合成孔径雷达图像中分割水域的方法。该方法不需要手动注释，可用于大规模监测湿地变化。

    

    遥感技术通过将语义分割技术应用于卫星图像已经显著提高了水的检测能力。然而，由于需要大量的注释数据，语义分割仍然是具有挑战性的。这在湿地检测方面尤其困难，因为水的范围随时间和空间的变化而变化，需要对同一区域进行多次注释。本文提出了DeepAqua，这是一种自我监督的深度学习模型，利用知识蒸馏方法在训练阶段消除了手动注释的需求。DeepAqua利用归一化差异水指数（NDWI）作为教师模型，训练卷积神经网络（CNN）以分割合成孔径雷达（SAR）图像中的水。为训练学生模型，我们利用光学和雷达水质掩蔽相重叠的情况，实现了开放及有植被水面的检测。我们的模型在湿地检测领域具有显著的计算机视觉技术进步，可用于大规模监测湿地变化。

    Remote sensing has significantly advanced water detection by applying semantic segmentation techniques to satellite imagery. However, semantic segmentation remains challenging due to the substantial amount of annotated data required. This is particularly problematic in wetland detection, where water extent varies over time and space, necessitating multiple annotations for the same area. In this paper, we present DeepAqua, a self-supervised deep learning model that leverages knowledge distillation to eliminate the need for manual annotations during the training phase. DeepAqua utilizes the Normalized Difference Water Index (NDWI) as a teacher model to train a Convolutional Neural Network (CNN) for segmenting water from Synthetic Aperture Radar (SAR) images. To train the student model, we exploit cases where optical- and radar-based water masks coincide, enabling the detection of both open and vegetated water surfaces. Our model represents a significant advancement in computer vision tec
    
[^76]: 使用GP-NAS堆叠集成模型预测NAS多任务

    Predict NAS Multi-Task by Stacking Ensemble Models using GP-NAS. (arXiv:2305.01667v1 [cs.LG])

    [http://arxiv.org/abs/2305.01667](http://arxiv.org/abs/2305.01667)

    本研究使用GP-NAS和交叉验证的堆叠集成模型，在NAS多任务中准确预测体系结构的性能，机型排名第一。

    

    在小样本训练中准确预测体系结构的性能是一项重要但不容易的任务。如何分析和训练数据集以克服过度拟合是我们应该处理的核心问题。同时，如果存在多任务问题，我们还应该考虑是否可以利用它们之间的相关性，并尽快估计。在这项研究中，Super Network基于ViT-Base构建了一个搜索空间。搜索空间包含深度、头数、mpl-ratio和embed-dim。我们首先进行了基于我们对该问题的理解的数据预处理，可以降低问题的复杂度和过度拟合的概率。然后我们尝试了不同类型的模型和不同的组合方式。最后，我们选择使用GP-NAS和交叉验证的堆叠集成模型。我们的堆叠模型在CVPR 2022赛道2挑战赛中排名第1。

    Accurately predicting the performance of architecture with small sample training is an important but not easy task. How to analysis and train dataset to overcome overfitting is the core problem we should deal with. Meanwhile if there is the mult-task problem, we should also think about if we can take advantage of their correlation and estimate as fast as we can. In this track, Super Network builds a search space based on ViT-Base. The search space contain depth, num-heads, mpl-ratio and embed-dim. What we done firstly are pre-processing the data based on our understanding of this problem which can reduce complexity of problem and probability of over fitting. Then we tried different kind of models and different way to combine them. Finally we choose stacking ensemble models using GP-NAS with cross validation. Our stacking model ranked 1st in CVPR 2022 Track 2 Challenge.
    
[^77]: BrainNPT：用于脑网络分类的Transformer网络的预训练

    BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.01666](http://arxiv.org/abs/2305.01666)

    本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。

    

    近年来，深度学习方法在脑成像分析方面的进展迅速，但往往受到有限标记数据的限制。在未标记数据上预训练的模型已在许多领域中展示了有前景的特征学习改进，包括自然语言处理和计算机视觉。然而，在脑网络分析中，这种技术尚未得到充分探索。在本文中，我们以Transformer网络为基础的预训练方法为重点，利用现有的未标记数据进行脑功能网络分类。首先，我们提出了一种基于Transformer的神经网络，名为BrainNPT，用于脑功能网络分类。所提出的方法利用<cls>标记作为分类嵌入向量，以便于Transformer模型有效地捕获脑网络的表示。其次，我们提出了两种预训练策略的预训练架构，用于BrainNPT模型，以利用未标记的脑网络数据来学习结构。

    Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged <cls> token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
    
[^78]: 一种新型的基于深度学习的镰状细胞疾病红细胞分类和定量模型

    A Novel Deep Learning based Model for Erythrocytes Classification and Quantification in Sickle Cell Disease. (arXiv:2305.01663v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.01663](http://arxiv.org/abs/2305.01663)

    本研究提出了一种深度学习模型，能够识别和量化血液样本图像中扭曲和正常形态的红细胞，并且在镰状细胞疾病这一模型疾病状态下表现良好。

    

    红细胞或血细胞在多种病理条件下形态发生改变，因此鉴定和量化不同的红细胞形态可以帮助诊断各种疾病并协助制定治疗策略。机器学习(ML)可以有效地用于识别和量化变形的红细胞形态。本文中，我们提出了一个定制的深度卷积神经网络(CNN)模型，用于分类和量化来自患有镰状细胞病(SCD)的患者血液样本图像中扭曲和正常形态的红细胞。我们选择SCD作为模型疾病状态，因为SCD患者血样中存在各种红细胞形态。为了分析，我们使用了428张SCD血样的原始显微镜图像，并生成了由10,377张单个细胞图像组成的数据集。我们专注于三种明确定义的红细胞形态，包括盘形细胞、椭圆形和镰刀形。我们使用包含18个层的CNN进行分类。

    The shape of erythrocytes or red blood cells is altered in several pathological conditions. Therefore, identifying and quantifying different erythrocyte shapes can help diagnose various diseases and assist in designing a treatment strategy. Machine Learning (ML) can be efficiently used to identify and quantify distorted erythrocyte morphologies. In this paper, we proposed a customized deep convolutional neural network (CNN) model to classify and quantify the distorted and normal morphology of erythrocytes from the images taken from the blood samples of patients suffering from Sickle cell disease ( SCD). We chose SCD as a model disease condition due to the presence of diverse erythrocyte morphologies in the blood samples of SCD patients. For the analysis, we used 428 raw microscopic images of SCD blood samples and generated the dataset consisting of 10, 377 single-cell images. We focused on three well-defined erythrocyte shapes, including discocytes, oval, and sickle. We used 18 layered
    
[^79]: SIA-FTP: 一种语音指令感知的飞行轨迹预测框架

    SIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework. (arXiv:2305.01661v1 [cs.SD])

    [http://arxiv.org/abs/2305.01661](http://arxiv.org/abs/2305.01661)

    提出一种语音指令感知的飞行轨迹预测框架，通过融合即时的语音指令和飞行轨迹表示，解决了语音指令和飞行轨迹的模态差距问题，在多个真实世界数据集上表现优异。

    

    通过语音通讯进行地空协商是确保空中交通管制（ATC）操作安全和效率的重要前提。但是，随着交通流量的增加，由于人为因素导致的错误指令给ATC安全带来了巨大威胁。现有的飞行轨迹预测（FTP）方法主要依赖于历史轨迹的飞行状态，在实时机动指令的预测上会出现显著的延迟，这不利于冲突检测。本文提出了一种名为SIA-FTP的语音指令感知FTP框架，通过包含即时的语音指令来支持高机动FTP任务。为了解决模态差距并最小化数据需求，我们提出了一种联合注意机制来融合语音指令嵌入和飞行轨迹表示。在多个真实世界数据集上评估了所提出的SIA-FTP，与现有的FTP方法相比取得了显著的改进。

    Ground-air negotiation via speech communication is a vital prerequisite for ensuring safety and efficiency in air traffic control (ATC) operations. However, with the increase in traffic flow, incorrect instructions caused by human factors bring a great threat to ATC safety. Existing flight trajectory prediction (FTP) approaches primarily rely on the flight status of historical trajectory, leading to significant delays in the prediction of real-time maneuvering instruction, which is not conducive to conflict detection. A major reason is that spoken instructions and flight trajectories are presented in different modalities in the current air traffic control (ATC) system, bringing great challenges to considering the maneuvering instruction in the FTP tasks. In this paper, a spoken instruction-aware FTP framework, called SIA-FTP, is innovatively proposed to support high-maneuvering FTP tasks by incorporating instant spoken instruction. To address the modality gap and minimize the data requ
    
[^80]: 数据估值：机器学习中的偏序 Shapley 值

    Data valuation: The partial ordinal Shapley value for machine learning. (arXiv:2305.01660v1 [cs.LG])

    [http://arxiv.org/abs/2305.01660](http://arxiv.org/abs/2305.01660)

    本文提出了偏序 Shapley 值的定义，并提出三种算法来近似计算结果，以解决数据合作中顺序作用的问题。

    

    在机器学习应用中，使用 Shapley 值进行数据估值已经成为一个流行的研究领域。然而，由于大多数研究缺乏关于数据合作中顺序作用的讨论，因此解决数据顺序的作用是一个挑战。为了解决这个问题，本文通过群论中的抽象代数研究了偏序 Shapley 值的定义。此外，由于偏序 Shapley 值的计算需要指数级别的时间，本文还提出了三个算法来近似计算结果，分别为截断蒙特卡洛算法、分类蒙特卡洛算法和分类截断蒙特卡洛算法。这三个算法的实现不同，但都可以通过一定程度的近似来加快计算速度。

    Data valuation using Shapley value has emerged as a prevalent research domain in machine learning applications. However, it is a challenge to address the role of order in data cooperation as most research lacks such discussion. To tackle this problem, this paper studies the definition of the partial ordinal Shapley value by group theory in abstract algebra. Besides, since the calculation of the partial ordinal Shapley value requires exponential time, this paper also gives three algorithms for approximating the results. The Truncated Monte Carlo algorithm is derived from the classic Shapley value approximation algorithm. The Classification Monte Carlo algorithm and the Classification Truncated Monte Carlo algorithm are based on the fact that the data points in the same class provide similar information, then we can accelerate the calculation by leaving out some data points in each class.
    
[^81]: FlightBERT++：一种非自回归多时域飞行轨迹预测框架

    FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])

    [http://arxiv.org/abs/2305.01658](http://arxiv.org/abs/2305.01658)

    FlightBERT++提出了一种非自回归的多时域飞行轨迹预测框架，通过引入时域感知上下文生成器解决了误差累积和低效率的问题。

    

    飞行轨迹预测是空中交通管制中的重要任务，可以帮助空管员更安全高效地管理空域。现有方法通常采用自回归方式执行多时域飞行轨迹预测任务，容易出现误差累积和低效率问题。本文提出了一种新的框架，称为FlightBERT++，以i）直接以非自回归方式预测多时域飞行轨迹，和ii）改善FlightBERT框架中二进制编码（BE）表示的限制。具体而言，所提出的框架通过通用的编码器-解码器架构实现，其中编码器从历史观测中学习时空模式，而解码器预测未来时间步的飞行状态。与传统架构相比，额外的时域感知上下文生成器（HACG）专门设计考虑先前的时域。

    Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
    
[^82]: 基于DDVal的分散式学习中可伸缩的数据点估值方法

    Scalable Data Point Valuation in Decentralized Learning. (arXiv:2305.01657v1 [cs.LG])

    [http://arxiv.org/abs/2305.01657](http://arxiv.org/abs/2305.01657)

    该文提出了一种名为DDVal的方法，用于在联邦和群智学习中的分散式数据估值，可以估算单个数据点的价值。DDVal基于共享深度特征，并通过k最近邻逼近方法来估算Shapley值，可用于同时向机构和个人奖励为分散式机器学习任务提供数据的贡献。同时，DDVal对机构的贡献进行了层次化的结论，并在实验证明其估算机构贡献的准确性较现有的联邦学习Shapley值逼近方法更高。

    

    对于联邦和群智学习中的数据估值研究，现有文献集中在估算客户端贡献上，并且在数据在客户端之间是独立同分布(IID)时表现最佳。然而，在实际的应用中，数据很少是IID分布的。我们提出了一种名为DDVal的方法，用于在联邦和群智学习中的分散式数据估值，可以估算单个数据点的价值。DDVal基于共享深度特征，并通过k最近邻逼近方法来估算Shapley值。这允许新的应用，例如同时向机构和个人奖励为分散式机器学习任务提供数据的贡献。通过DDVal对数据点进行估值，还能对机构的贡献进行层次化的结论，我们通过实验证明DDVal在估算机构贡献时的准确性比现有的联邦学习Shapley值逼近方法高。具体而言，它达到了...

    Existing research on data valuation in federated and swarm learning focuses on valuing client contributions and works best when data across clients is independent and identically distributed (IID). In practice, data is rarely distributed IID. We develop an approach called DDVal for decentralized data valuation, capable of valuing individual data points in federated and swarm learning. DDVal is based on sharing deep features and approximating Shapley values through a k-nearest neighbor approximation method. This allows for novel applications, for example, to simultaneously reward institutions and individuals for providing data to a decentralized machine learning task. The valuation of data points through DDVal allows to also draw hierarchical conclusions on the contribution of institutions, and we empirically show that the accuracy of DDVal in estimating institutional contributions is higher than existing Shapley value approximation methods for federated learning. Specifically, it reach
    
[^83]: 基于概率形式化建模的交互风格揭示与解释研究

    Probabilistic Formal Modelling to Uncover and Interpret Interaction Styles. (arXiv:2305.01656v1 [cs.HC])

    [http://arxiv.org/abs/2305.01656](http://arxiv.org/abs/2305.01656)

    本文使用机器学习和概率模型检验等计算方法，基于用户痕迹记录，揭示了移动应用程序中的交互风格。通过无监督的聚类推断和概率时间逻辑分析，我们发现用户在使用的前期和后期采用的交互风格有所不同。

    

    本文提出一种新的计算方法，基于机器学习推断混合隐马尔科夫模型和概率模型检验，用于揭示移动应用程序中的交互风格。这些风格用于重新设计应用程序，并使用相同的方法实施，部署，然后进行分析。这些数据集是用户痕迹记录，分别在两个版本的六个月部署期间收集，并分为不同的时间间隔。这些方法不针对任务或绝对指标（如参与度的度量），而是通过无监督的聚类推断和概率时间逻辑分析来揭示风格。对于两个版本，用户在使用的第一天/第一周/第一个月和第二个第三个月期间采用的风格有明显的区别，这是我们没有预料到的结果。

    We present a study using new computational methods, based on a novel combination of machine learning for inferring admixture hidden Markov models and probabilistic model checking, to uncover interaction styles in a mobile app. These styles are then used to inform a redesign, which is implemented, deployed, and then analysed using the same methods. The data sets are logged user traces, collected over two six-month deployments of each version, involving thousands of users and segmented into different time intervals. The methods do not assume tasks or absolute metrics such as measures of engagement, but uncover the styles through unsupervised inference of clusters and analysis with probabilistic temporal logic. For both versions there was a clear distinction between the styles adopted by users during the first day/week/month of usage, and during the second and third months, a result we had not anticipated.
    
[^84]: 预测缺失数据情况下的血压变化：使用NHANES分析缺失数据模式和插补方法

    Predicting blood pressure under circumstances of missing data: An analysis of missing data patterns and imputation methods using NHANES. (arXiv:2305.01655v1 [cs.LG])

    [http://arxiv.org/abs/2305.01655](http://arxiv.org/abs/2305.01655)

    本文使用美国国家健康和营养检查调查数据研究缺失数据模式，并揭示了多重插补是预测血压变化最有效的方法，插补模型应包括传统预测因素和可能与缺失数据相关的变量。

    

    世界卫生组织将心血管疾病定义为“心脏和血管的一组疾病”，包括冠心病和中风。这些疾病受到“中间风险因素”的影响，如高血压、高血糖、高血脂和肥胖。这些风险因素主要受生活方式和行为影响，包括身体不活动、不健康的饮食、高盐摄入以及吸烟和饮酒。然而，遗传学和社会/环境因素如贫困、压力和种族歧视也起着重要作用。本文作者使用美国国家健康和营养检查调查(NHANES)的数据，分析了缺失数据模式，并比较了各种插补方法以预测血压变化。研究发现，使用链式方程(多重插补)的插补方法是处理缺失数据的最有效方法，并且插补模型应包括传统的预测因素(如年龄、性别、种族/族裔、体重指数)，以及可能与缺失数据相关的变量(如收入、教育程度、糖尿病状态)。

    The World Health Organization defines cardio-vascular disease (CVD) as "a group of disorders of the heart and blood vessels," including coronary heart disease and stroke (WHO 21). CVD is affected by "intermediate risk factors" such as raised blood pressure, raised blood glucose, raised blood lipids, and obesity. These are predominantly influenced by lifestyle and behaviour, including physical inactivity, unhealthy diets, high intake of salt, and tobacco and alcohol use. However, genetics and social/environmental factors such as poverty, stress, and racism also play an important role. Researchers studying the behavioural and environmental factors associated with these "intermediate risk factors" need access to high quality and detailed information on diet and physical activity. However, missing data are a pervasive problem in clinical and public health research, affecting both randomized trials and observational studies. Reasons for missing data can vary substantially across studies bec
    
[^85]: 如何发挥大语言模型在少样本关系抽取中的能力？

    How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])

    [http://arxiv.org/abs/2305.01555](http://arxiv.org/abs/2305.01555)

    本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。

    

    语言模型的扩展已经彻底改变了广泛的自然语言处理任务，但是使用大型语言模型进行少样本关系抽取还没有得到全面探索。本文通过详细实验，研究了使用GPT-3.5进行少样本关系抽取的基本方法——上下文学习和数据生成。为了增强少样本性能，我们进一步提出了与任务相关的指导说明和约束模式下的数据生成。我们观察到，在上下文学习的情况下，可以实现与以前的提示学习方法相当的性能，而使用大型语言模型的数据生成可以推动以前的解决方案以在四个广泛研究的关系抽取数据集上获得新的最先进的少样本结果。我们希望我们的工作可以激发未来对大型语言模型在少样本关系抽取中的能力的研究。代码可以在 \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} 中找到。

    Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^86]: 一种无需预设参数的自适应共振理论拓扑聚类算法，实现持续学习

    A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning. (arXiv:2305.01507v1 [cs.NE])

    [http://arxiv.org/abs/2305.01507](http://arxiv.org/abs/2305.01507)

    本文提出一种无需预设参数的ART拓扑聚类算法，通过引入参数估计方法实现持续学习，并在实验中证明其比现有聚类算法更优。

    

    一般来说，在自适应共振理论（ART）算法中，节点学习过程中的相似度阈值（即警觉参数）对聚类性能有重大影响。此外，拓扑聚类算法中的边缘删除阈值在自组织过程中生成互相分离的聚类中起重要作用。在本文中，我们提出了一种新的无需预设参数的ART拓扑聚类算法，通过引入参数估计方法实现持续学习。针对合成数据集和真实世界数据集的实验结果表明，所提算法在无预设参数的情况下具有比现有聚类算法更优的聚类性能。

    In general, a similarity threshold (i.e., a vigilance parameter) for a node learning process in Adaptive Resonance Theory (ART)-based algorithms has a significant impact on clustering performance. In addition, an edge deletion threshold in a topological clustering algorithm plays an important role in adaptively generating well-separated clusters during a self-organizing process. In this paper, we propose a new parameter-free ART-based topological clustering algorithm capable of continual learning by introducing parameter estimation methods. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to the state-of-the-art clustering algorithms without any parameter pre-specifications.
    
[^87]: 基于LTL规范的样本有效无模型强化学习与优化保证

    Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])

    [http://arxiv.org/abs/2305.01381](http://arxiv.org/abs/2305.01381)

    本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。

    

    线性时间逻辑（LTL）广泛用于指定系统策略的高级目标，自主系统学习相对于这样的规范的最优策略是非常理想的。 但是，从LTL规范中学习最优策略并不轻松。我们提出了一种无模型强化学习（RL）方法，该方法可以有效地学习未知随机系统的最优策略，其中使用马尔可夫决策过程（MDP）进行建模。我们提出了一种新颖且更通用的乘积MDP、奖励结构和折扣机制，当与现成的无模型RL算法结合使用时，能够高效地学习最大化给定LTL规范满足概率的最优策略，并提供了更好的有关选择RL中关键参数以保证最优性的理论结果。为了直接评估学习策略，我们采用概率模型检查器PRISM来计算LTL规范的满足概率。

    Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
    
[^88]: 最大化潜在特征和真实标签之间的互信息实现长尾识别

    Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])

    [http://arxiv.org/abs/2305.01160](http://arxiv.org/abs/2305.01160)

    本论文提出了一种名为LC的新长尾识别方法，它能够更好地模拟真实标签分布，同时解决类别标签不平衡问题，从而在CIFAR-10，CIFAR-100和ImageNet基准数据集上显着优于现有方法。

    

    尽管对比学习方法在各种表示学习任务中表现出了优越的性能，但当训练数据集是长尾分布时，它们会遇到困难。许多研究人员已经将对比学习和逻辑斯蒂调整技术相结合来解决这个问题，但这些组合是临时的，并没有提供理论背景。本文的目的是提供背景并进一步提高性能。首先，我们证明了对比学习方法在长尾任务中遇到困难的根本原因是它们试图最大化潜在特征和输入数据之间的互信息最大化。由于不考虑真实标签的最大化，它们无法解决类别标签之间的不平衡问题。相反，我们将长尾识别任务解释为潜在特征和真实标签之间的互信息最大化。这种方法以一种有原则的方式集成了对比学习和逻辑斯蒂调整技术。其次，我们提出了一种新方法，称为潜在类别（LC）方法，它明确地模拟了真实标签的分布，并联合最大化潜在特征和真实标签之间的互信息。对包括CIFAR-10，CIFAR-100和ImageNet在内的基准数据集进行的大量实验表明，我们提出的方法在长尾识别任务上显着优于现有方法。

    Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a
    
[^89]: 在有限制的多目标联邦学习中优化隐私、效用和效率

    Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])

    [http://arxiv.org/abs/2305.00312](http://arxiv.org/abs/2305.00312)

    该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。

    

    传统上，联邦学习旨在优化单个目标，通常是效用。然而，为了使联邦学习系统值得信赖，它需要同时满足多个/多个目标，例如最大化模型性能、最小化隐私泄露和训练成本，并对恶意攻击具有鲁棒性。多目标优化（MOO）旨在同时优化多个相互冲突的目标，非常适合解决值得信赖的联合学习（TFL）的优化问题。在本文中，我们将MOO和TFL统一起来，通过制定约束的多目标联合学习（CMOFL）问题来解决此问题。在这种制定下，现有的MOO算法可以直接适用于TFL。不同于现有的CMOFL作品专注于效用、效率、公平性和鲁棒性，我们考虑优化隐私泄露以及效用损失和训练成本，这是TFL系统的三个主要目标之一。我们开发了两种改进的CMOFL算法，它们返回一组平衡良好的模型，满足隐私、效用和效率。基于两个真实世界的数据集的实验表明，我们的方法在隐私、效用和效率之间的权衡方面优于现有方法。

    Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
    
[^90]: 多晶微结构神经网络加速工艺设计

    Neural Network Accelerated Process Design of Polycrystalline Microstructures. (arXiv:2305.00003v1 [cs.CE])

    [http://arxiv.org/abs/2305.00003](http://arxiv.org/abs/2305.00003)

    通过神经网络加速工艺设计，减轻预测微结构演化的计算负担，并找到最佳加工路径。

    

    通过计算实验，找到维持理想材料结构的设计路径可以优化所需的材料性质。这需要采用多尺度方法来理解加工-（微观）结构-性能之间的相互作用，将宏观尺度（工艺参数）与中观（均质化的性质）和微观（晶体学纹理）尺度连接起来。由于问题的多尺度建模设置，可能的加工路径选择会随着决策树的加深而呈指数级增长，传统模拟器的速度达到关键计算阈值。为了减轻在给定加载条件下预测微结构演化的计算负担，我们开发了一种带物理约束的神经网络（NN）方法。该NN旨在学习每个基本过程下微观结构的演化。我们的方法在找到最佳加工路径方面是有效和鲁棒的。在本研究中，我们使用我们的方法成功地对铝微结构进行了优化。

    Computational experiments are exploited in finding a well-designed processing path to optimize material structures for desired properties. This requires understanding the interplay between the processing-(micro)structure-property linkages using a multi-scale approach that connects the macro-scale (process parameters) to meso (homogenized properties) and micro (crystallographic texture) scales. Due to the nature of the problem's multi-scale modeling setup, possible processing path choices could grow exponentially as the decision tree becomes deeper, and the traditional simulators' speed reaches a critical computational threshold. To lessen the computational burden for predicting microstructural evolution under given loading conditions, we develop a neural network (NN)-based method with physics-infused constraints. The NN aims to learn the evolution of microstructures under each elementary process. Our method is effective and robust in finding optimal processing paths. In this study, our
    
[^91]: 使用Pylogik进行医学影像去标识化和清洗压缩

    Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])

    [http://arxiv.org/abs/2304.12322](http://arxiv.org/abs/2304.12322)

    提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。

    

    应用大数据和机器学习在医疗记录信息方面须注意，必须清洗和去标识化数据。当受保护的健康信息嵌入在影像元数据中时，促进多中心合作中数据共享和协调变得尤其困难。我们提出了一个新的Python框架下的库，称为PyLogik，帮助解决超声图像特别具有挑战性的数据清洗问题，因为这些图像直接包含很多PHI。PyLogik通过一系列的文本检测/提取、过滤、阈值化、形态学和轮廓比较处理图像体积。这种方法去标识化图像，减小文件大小，并为深度学习和数据共享应用准备好了图像数据。为了评估PyLogik在兴趣区域（ROI）的识别有效性，随机抽取了50张心脏超声图像（超声心动图）进行处理。

    Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
    
[^92]: 从数据中自动识别动力系统

    Automatically identifying dynamical systems from data. (arXiv:2304.11182v1 [cs.LG])

    [http://arxiv.org/abs/2304.11182](http://arxiv.org/abs/2304.11182)

    该论文提出了一种从经验数据中自动识别动态规律的方法，这种方法能够较为准确地识别三维系统，具有在各种领域中理解复杂系统的潜力。

    

    从经验数据中发现描述系统动态的非线性微分方程是当代科学中的一个基本挑战。在这里，我们提出了一种方法来自动识别动态规律，该方法集成了去噪技术、稀疏回归和自助置信区间。我们使用一组随机初始条件的普通微分方程组，时序呈指数增长和各种信噪比进行评估。我们的算法一致识别三维系统，在时间序列适度的和高信号质量相对于背景噪声的情况下。通过准确识别动力系统，我们的方法具有潜力影响各种领域，如物理学和生物学以及工程学，在这些领域中理解复杂系统至关重要。

    Discovering nonlinear differential equations that describe system dynamics from empirical data is a fundamental challenge in contemporary science. Here, we propose a methodology to automatically identify dynamical laws by integrating denoising techniques, sparse regression, and bootstrap confidence intervals. We evaluate our method on well-known ordinary differential equations with an ensemble of random initial conditions, time series of increasing length, and varying signal-to-noise ratios. Our algorithm consistently identifies three-dimensional systems, given moderately-sized time series and high signal quality levels relative to background noise. By accurately identifying dynamical systems, our methodology has the potential to impact diverse fields, such as the physical and biological sciences, as well as engineering, where understanding complex systems is crucial.
    
[^93]: 可控的信任权衡下的合成数据审计与生成

    Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])

    [http://arxiv.org/abs/2304.10819](http://arxiv.org/abs/2304.10819)

    本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。

    

    现实中收集的数据往往存在偏差、不平衡，并且有泄露敏感和隐私信息的风险。这一事实引发了创建合成数据集的想法，以减轻真实数据中固有的风险、偏见、伤害和隐私问题。这个概念依赖于生成AI模型，以产生不偏执、保护隐私的合成数据，同时忠实于真实数据。在这种新范式中，我们如何知道这种方法是否兑现了其承诺？我们提出了一个审计框架，提供了对合成数据集和基于它们训练的AI模型的全面评估，围绕偏见和歧视的预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。我们通过审计多个生成模型在不同用例中展示了我们的框架，包括教育、医疗保健、银行、人力资源，以及从表格，时间序列到自然语言的不同模态。我们的用例展示了在合成数据生成中平衡信任和效用的权衡的重要性。

    Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
    
[^94]: 边缘服务器上的深度神经网络调度

    Scheduling DNNs on Edge Servers. (arXiv:2304.09961v1 [cs.NI])

    [http://arxiv.org/abs/2304.09961](http://arxiv.org/abs/2304.09961)

    本论文研究了如何加速为多个客户端运行边缘服务器DNN。批处理多个DNN请求可以显著加速处理时间。研究设计了一种新的调度算法，并开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。

    

    深度神经网络(DNN)已经广泛用于各种视频分析任务中。这些任务要求实时响应，由于移动设备处理能力有限，支持此类实时分析的常见方法是将处理离线到边缘服务器。本文考察如何加速为多个客户端运行边缘服务器DNN。我们观察到，批处理多个DNN请求可以显著加速处理时间。基于此观察，我们首先设计了一种新的调度算法，以利用运行相同DNN的所有请求的批处理优势。这很有说服力，因为只有少数DNN，许多请求倾向于使用同一个DNN。我们的算法是通用的，可以支持不同的目标，如最小化完成时间或最大化及时率。然后，我们扩展我们的算法以处理使用不同DNN的具有或不具有共享层的请求。最后，我们开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。

    Deep neural networks (DNNs) have been widely used in various video analytic tasks. These tasks demand real-time responses. Due to the limited processing power on mobile devices, a common way to support such real-time analytics is to offload the processing to an edge server. This paper examines how to speed up the edge server DNN processing for multiple clients. In particular, we observe batching multiple DNN requests significantly speeds up the processing time. Based on this observation, we first design a novel scheduling algorithm to exploit the batching benefits of all requests that run the same DNN. This is compelling since there are only a handful of DNNs and many requests tend to use the same DNN. Our algorithms are general and can support different objectives, such as minimizing the completion time or maximizing the on-time ratio. We then extend our algorithm to handle requests that use different DNNs with or without shared layers. Finally, we develop a collaborative approach to 
    
[^95]: GREAT分数：使用生成模型对对抗性扰动进行全局鲁棒性评估

    GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])

    [http://arxiv.org/abs/2304.09875](http://arxiv.org/abs/2304.09875)

    本文提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。该分数捕捉了所有样本中的平均认证防攻击扰动水平，无需运行对抗性攻击。

    

    目前对于对抗性鲁棒性的研究主要集中在聚合一组数据样本的局部鲁棒性结果上，以评估和排名不同的模型。然而，局部统计量可能无法很好地代表基础未知数据分布的真正全局鲁棒性。为了解决这一挑战，本文首次尝试提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。GREAT分数正式具有一个全局统计量的物理意义，捕捉来自生成模型的所有样本中的平均认证防攻击扰动水平。对于有限样本评估，我们还推导出样本复杂度和样本均值与真实均值之间的概率保证。GREAT分数有几个优点：（1）使用GREAT分数进行鲁棒性评估高效而且规模可扩展，无需运行对抗性攻击。

    Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
    
[^96]: HEAT：一种高效且经济实惠的基于CPU的协同过滤推荐训练系统

    HEAT: A Highly Efficient and Affordable Training System for Collaborative Filtering Based Recommendation on CPUs. (arXiv:2304.07334v1 [cs.DC])

    [http://arxiv.org/abs/2304.07334](http://arxiv.org/abs/2304.07334)

    这篇论文提出了HEAT训练系统，通过优化SimpleX在CPU上的操作，实现协同过滤的高效率训练

    

    协同过滤已被证明是推荐系统中最有效的技术之一。在所有协同过滤方法中，SimpleX是采用了新颖的损失函数和适当数量的负样本的最先进方法。然而，目前还没有对SimpleX在多核CPU上进行了优化，导致性能有限。为了解决这个问题，我们对现有的SimpleX实现进行了深入的分析，并确定了它们的性能瓶颈，包括(1)不规则的内存访问，(2)不必要的内存复制，(3)冗余计算。为了解决这些问题，我们提出了一种高效的CF训练系统(名为HEAT)，它充分发挥了现代CPU的多级缓存和多线程能力。具体而言，HEAT的优化有三个方面：(1)使用瓦片化技术增加数据局部性和减少缓存失效(从而减少读取延迟)；(2)使用随机梯度下降(SGD)和采样优化；(3)使用功能分区优化内存访问和计算

    Collaborative filtering (CF) has been proven to be one of the most effective techniques for recommendation. Among all CF approaches, SimpleX is the state-of-the-art method that adopts a novel loss function and a proper number of negative samples. However, there is no work that optimizes SimpleX on multi-core CPUs, leading to limited performance. To this end, we perform an in-depth profiling and analysis of existing SimpleX implementations and identify their performance bottlenecks including (1) irregular memory accesses, (2) unnecessary memory copies, and (3) redundant computations. To address these issues, we propose an efficient CF training system (called HEAT) that fully enables the multi-level caching and multi-threading capabilities of modern CPUs. Specifically, the optimization of HEAT is threefold: (1) It tiles the embedding matrix to increase data locality and reduce cache misses (thus reduce read latency); (2) It optimizes stochastic gradient descent (SGD) with sampling by par
    
[^97]: HGWaveNet: 一种用于时间链接预测的双曲图神经网络

    HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction. (arXiv:2304.07302v1 [cs.LG])

    [http://arxiv.org/abs/2304.07302](http://arxiv.org/abs/2304.07302)

    HGWaveNet是一种双曲图神经网络，用于时间链接预测。它包括超曲率扩散图卷积和小波时间卷积两个关键模块，可有效聚合邻居信息和捕捉时间依赖，并在真实世界数据集上表现出更好的精度和效率。

    

    时间链接预测旨在预测动态图中成对节点之间的未来边缘，是各种应用中非常重要的领域。然而，现有方法主要建立在均匀的欧几里得空间上，这与现实世界图形的幂律分布相矛盾，无法有效地表示节点之间的分层连接。针对这种特殊的数据特征，双曲几何提供了一种理想的替代方案，因为它具有指数扩展特性。在本文中，我们提出了HGWaveNet，一种新颖的双曲图神经网络，充分利用了超几何空间与数据分布之间的相适应性，用于时间链接预测。具体而言，我们设计了两个关键模块来分别学习空间拓扑结构和时间演变信息。一方面，超曲率扩散图卷积 (HDGC) 模块有效地聚合了更广泛的邻居信息。另一方面，基于小波的时间卷积 (WTC) 模块通过转换时间域特征来捕捉时间依赖性。在真实世界数据集上的实验表明，HGWaveNet在精度和效率方面优于现有的最先进方法。

    Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the ot
    
[^98]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^99]: $(\alpha_D,\alpha_G)$-GANs：通过双重目标来解决GAN训练不稳定性

    $(\alpha_D,\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives. (arXiv:2302.14320v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14320](http://arxiv.org/abs/2302.14320)

    本文提出了一个双重目标GAN，通过使用可调的$\alpha$-loss来建模每个目标。在足够大的样本数和容量下，这类GAN的非零和游戏简化为最小化$f$-散度。最后，调整$(\alpha_D,\alpha_G)$可以缓解训练不稳定性。

    

    为了解决GAN的训练不稳定性，我们引入了一类具有不同价值函数（目标）的双重目标GAN，特别地，我们使用可调的分类损失——$\alpha$-loss来建模每个目标，以得到由$(\alpha_D,\alpha_G)\in(0,\infty]^2$参数化的$(\alpha_D,\alpha_G)$-GAN。对于足够大的样本数和G、D的容量，我们展示了在$(\alpha_D,\alpha_G)$的适当条件下，导致的非零和游戏简化为最小化$f$-散度。在有限的样本数和容量的情况下，我们定义估计误差，以量化相对于无限样本下的最优设定而言生成器性能的差距，并得到了这个误差的上界，证明了在某些条件下它的阶有效。最后，我们强调了调整$(\alpha_D,\alpha_G)$在缓解合成2D高斯混合问题的训练不稳定性方面的价值。

    In an effort to address the training instabilities of GANs, we introduce a class of dual-objective GANs with different value functions (objectives) for the generator (G) and discriminator (D). In particular, we model each objective using $\alpha$-loss, a tunable classification loss, to obtain $(\alpha_D,\alpha_G)$-GANs, parameterized by $(\alpha_D,\alpha_G)\in (0,\infty]^2$. For sufficiently large number of samples and capacities for G and D, we show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\alpha_D,\alpha_G)$. In the finite sample and capacity setting, we define estimation error to quantify the gap in the generator's performance relative to the optimal setting with infinite samples and obtain upper bounds on this error, showing it to be order optimal under certain conditions. Finally, we highlight the value of tuning $(\alpha_D,\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture
    
[^100]: 可微粒子滤波器应用于状态转移模型

    Differentiable Bootstrap Particle Filters for Regime-Switching Models. (arXiv:2302.10319v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.10319](http://arxiv.org/abs/2302.10319)

    本文提出了一种可微粒子滤波器，可以学习一组未知的动态和测量模型，并跟踪状态后验概率，在相关模型中表现出卓越的性能。

    

    可微粒子滤波器是一种利用神经网络构建和学习参数化状态空间模型的新兴粒子滤波方法。在实际应用中，状态动态和测量结果都可以在一组可选模型中切换。本文提出了一种新的可微粒子滤波器，用于状态转移状态空间模型。该方法可以学习一组未知的动态和测量模型，并跟踪状态后验概率。我们评估了该算法在相关模型中的性能，并展示了与其他竞争算法相比的出色性能。

    Differentiable particle filters are an emerging class of particle filtering methods that use neural networks to construct and learn parametric state-space models. In real-world applications, both the state dynamics and measurements can switch between a set of candidate models. For instance, in target tracking, vehicles can idle, move through traffic, or cruise on motorways, and measurements are collected in different geographical or weather conditions. This paper proposes a new differentiable particle filter for regime-switching state-space models. The method can learn a set of unknown candidate dynamic and measurement models and track the state posteriors. We evaluate the performance of the novel algorithm in relevant models, showing its great performance compared to other competitive algorithms.
    
[^101]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^102]: 从教学视频文本转录中无监督生成任务图

    Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09173](http://arxiv.org/abs/2302.09173)

    本文提出了一种无监督的任务图生成方法，通过结合支持指导的语言模型的推理能力和聚类、排序组件，从执行真实世界活动的教学视频文本记录中生成任务图。实验结果表明该方法在ProceL和CrossTask数据集上比监督学习方法生成的任务图更加准确。

    

    本研究探讨了生成真实世界活动任务图的问题。与以往的方法不同，我们考虑提供执行真实世界活动（如制作咖啡）的教学视频文本记录，并旨在确定与任务相关的关键步骤及其依赖关系。我们提出了一种新颖的任务图生成方法，该方法结合了面向指导的语言模型的推理能力以及聚类和排序组件，在完全无监督的情况下生成准确的任务图。我们展示了该方法在ProceL和CrossTask数据集上生成的任务图比监督学习方法更加准确。

    This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets.
    
[^103]: 在线主动学习综述

    A survey on online active learning. (arXiv:2302.08893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08893](http://arxiv.org/abs/2302.08893)

    在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。本文综述了在线主动学习的最新进展、基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范式中不同的评估指标。

    

    在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。近年来，随着数据仅以未标记形式可用的实际应用日益增多，最小化与收集标记观测相关的成本问题引起了广泛关注。标注每个观测可以耗费大量的时间和成本，使得获取大量标记数据变得困难。为了克服这个问题，许多主动学习策略已经提出，旨在选择最具信息量的观测进行标记，以提高机器学习模型的性能。这些方法可以广泛地分为两类：静态基于池的和基于流的主动学习。基于池的主动学习涉及从封闭的未标记数据池中选择一部分观测，已成为许多调查和文献综述的重点。然而，随着在线数据流的不断增加，基于流的主动学习策略变得更加吸引人，因为它们允许模型适应新进数据。在本综述中，我们综述了在线主动学习的最新进展，讨论了基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范例中不同的评估指标。

    Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. Howev
    
[^104]: 在网络考试中检测串通的数据挖掘方法

    A Data Mining Approach for Detecting Collusion in Unproctored Online Exams. (arXiv:2302.07014v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2302.07014](http://arxiv.org/abs/2302.07014)

    提出了一个在疫情期间检测学生是否串通作弊的数据挖掘方法，并通过对远程考试事件日志数据的分析发现了群体作弊行为或者异常，同时建立了一个评估异常案例的经验规则。

    

    由于COVID-19疫情期间的预防性措施，许多大学提供了无监考的远程考试。我们提出了检测学生间可能的串通的方法，并将其应用于疫情期间远程考试的事件日志数据中。我们发现了一些有着明显相似考试的学生群体，同时将我们的发现与被监考控制组的结果进行了比较。基于这些，我们建立了一个评估“极其相似”的异常案例的经验规则。

    Due to the precautionary measures during the COVID-19 pandemic many universities offered unproctored take-home exams. We propose methods to detect potential collusion between students and apply our approach on event log data from take-home exams during the pandemic. We find groups of students with suspiciously similar exams. In addition, we compare our findings to a proctored control group. By this, we establish a rule of thumb for evaluating which cases are "outstandingly similar", i.e., suspicious cases.
    
[^105]: DocILE基准数据集用于文件信息定位和提取

    DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.05658](http://arxiv.org/abs/2302.05658)

    本文介绍了DocILE基准数据集，该数据集包含大量商务文件，可用于关键信息定位和提取以及行项目识别任务。该数据集具有55个类别的注释，超过以往发布的数据集，同时包括众多不同布局和未标记的文档，为该领域提供了有力的研究工具。

    

    本文介绍了DocILE基准数据集，用于关键信息定位和提取以及行项目识别任务的商务文件的最大数据集。 它包含6.7k个带注释的商务文件，100k个合成生成的文档以及近1M个未标记的文档，用于无监督的预训练。 该数据集具有特定于领域和任务的知识，具有以下关键特征：（i）在55个类别中注释，其粒度远远超过以前发布的关键信息提取数据集; （ii）行项目识别表示一项极具实用性的信息提取任务，在表格中必须将关键信息分配给项目; （iii）文档来自众多布局，测试集包括零-shot和少-shot案例以及训练集中常见的布局。基准数据集配有多个基线，包括RoBERTa、 LayoutLMv3和基于DETR的表格Transformer；

    This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domainand task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; app
    
[^106]: 高效的对抗性对比学习：基于鲁棒性感知的数据核心集选择

    Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03857](http://arxiv.org/abs/2302.03857)

    该研究提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法，能够有效地加速对抗性对比学习（ACL）并维持其强鲁棒性和泛化性能。

    

    对抗性对比学习（ACL）不需要昂贵的数据注释，但可以输出抵抗对抗性攻击并且适用于广泛下游任务的强鲁棒性表示。然而，ACL需要巨大的运行时间才能生成所有训练数据的对抗变体，这限制了其在大型数据集上的可扩展性。为了加速ACL，本文提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法。RCS不需要标签信息，搜索最小化表示分歧的信息子集，即自然数据和其虚拟对抗变体之间表示的距离。RCS的基本解法是遍历所有可能的子集，计算复杂度高。因此，我们在理论上将RCS转化为子模最大化的替代问题，利用贪心搜索是原问题的有效解决方案，同时具有原问题的最优性保证。实验结果表明，RCS可以通过减少训练数据量有效地加速ACL，并且仍然保持其强鲁棒性和泛化性能。

    Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
    
[^107]: 概率对比学习恢复了不确定性输入的正确估计

    Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02865](http://arxiv.org/abs/2302.02865)

    本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。

    

    最近，对比学习编码器被证明可以翻转数据生成过程：它们可以将每个输入（如图像）编码成生成该图像的真实潜变量（Zimmermann等人，2021）。然而，现实世界的观察结果通常存在内在的模糊性。例如，图像可能模糊或只显示3D物体的2D视图，因此可能有多个潜变量生成它们。这使得潜变量的真实后验概率具有异方差不确定性。在这种设置下，我们扩展了常见的InfoNCE目标和编码器，以预测潜变量分布而不是点。我们证明这些分布恢复了数据生成过程的正确后验分布，包括其不确定性水平的估计，该估计存在潜变量空间的旋转。除了提供校准的不确定性估计之外，这些后验分布还允许在图像检索中计算可信区间。它们包括具有与给定查询相同的潜变量的图像。

    Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
    
[^108]: 潜变量和结构方程模型的可辨识性：从线性到非线性

    Identifiability of latent-variable and structural-equation models: from linear to nonlinear. (arXiv:2302.02672v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02672](http://arxiv.org/abs/2302.02672)

    本文研究了潜变量和结构方程模型的可辨识性问题，展示了如何推广现有的模型，使其可以用于非线性问题，并且仍然保持可辨识性。

    

    在多变量统计学中，线性高斯模型通常是不可辨识的。在本文中，我们展示了如何推广现有的模型，使其可以用于非线性问题，并且仍然保持可辨识性。通过引入时间序列或观测到的辅助变量，我们可以克服非高斯性是否足够的限制，来实现这样的可辨识性。本文回顾了线性和非线性可辨识性理论。

    An old problem in multivariate statistics is that linear Gaussian models are often unidentifiable, i.e. some parameters cannot be uniquely estimated. In factor (component) analysis, an orthogonal rotation of the factors is unidentifiable, while in linear regression, the direction of effect cannot be identified. For such linear models, non-Gaussianity of the (latent) variables has been shown to provide identifiability. In the case of factor analysis, this leads to independent component analysis, while in the case of the direction of effect, non-Gaussian versions of structural equation modelling solve the problem. More recently, we have shown how even general nonparametric nonlinear versions of such models can be estimated. Non-Gaussianity is not enough in this case, but assuming we have time series, or that the distributions are suitably modulated by some observed auxiliary variables, the models are identifiable. This paper reviews the identifiability theory for the linear and nonlinear
    
[^109]: 竞争性内容创作者下的Top-K推荐有多糟糕？

    How Bad is Top-$K$ Recommendation under Competing Content Creators?. (arXiv:2302.01971v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.01971](http://arxiv.org/abs/2302.01971)

    本文基于随机效用模型，研究了内容创作者在Top-K推荐下的竞争影响，证明了用户福利损失受小常数上界影响。

    

    内容创作者在推荐平台上竞争曝光率，这种战略行为导致了内容分布的动态转移。然而，创作者的竞争如何影响用户福利，以及相关推荐如何影响长期动态仍然大部分未知。本文提出了这些研究问题的理论见解。我们在以下假设下建模创作者的竞争：1）平台采用无害的top-K推荐策略；2）用户决策遵循随机效用模型；3）内容创作者竞争用户互动，不知道事先他们的效用函数，因此应用任意的无悔学习算法来更新他们的策略。我们通过洛城价格的角度研究用户福利的保证，并展示了由于创作者竞争导致的用户福利损失份额始终受到$K$和用户决策随机性影响的小常数的上界约束。

    Content creators compete for exposure on recommendation platforms, and such strategic behavior leads to a dynamic shift over the content distribution. However, how the creators' competition impacts user welfare and how the relevance-driven recommendation influences the dynamics in the long run are still largely unknown.  This work provides theoretical insights into these research questions. We model the creators' competition under the assumptions that: 1) the platform employs an innocuous top-$K$ recommendation policy; 2) user decisions follow the Random Utility model; 3) content creators compete for user engagement and, without knowing their utility function in hindsight, apply arbitrary no-regret learning algorithms to update their strategies. We study the user welfare guarantee through the lens of Price of Anarchy and show that the fraction of user welfare loss due to creator competition is always upper bounded by a small constant depending on $K$ and randomness in user decisions; w
    
[^110]: 基于深度强化学习的网络物理系统在线错误检测

    Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems. (arXiv:2302.01567v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01567](http://arxiv.org/abs/2302.01567)

    本文提出了一种基于深度强化学习（DRL）的新型在线错误检测方法。

    

    可靠性是网络物理系统中主要的设计标准之一。这是由于CPS中存在一些关键应用程序，它们的失效是灾难性的。因此，在CPS中使用强大的错误检测和纠正机制是不可避免的。传统的容错方法包括冗余时间、硬件、信息和/或软件。然而，这些方法除了低错误覆盖率外，还会带来极大的开销，限制了它们的适用性。本文提出了一种基于深度强化学习（DRL）的新型错误检测方法。

    Reliability is one of the major design criteria in Cyber-Physical Systems (CPSs). This is because of the existence of some critical applications in CPSs and their failure is catastrophic. Therefore, employing strong error detection and correction mechanisms in CPSs is inevitable. CPSs are composed of a variety of units, including sensors, networks, and microcontrollers. Each of these units is probable to be in a faulty state at any time and the occurred fault can result in erroneous output. The fault may cause the units of CPS to malfunction and eventually crash. Traditional fault-tolerant approaches include redundancy time, hardware, information, and/or software. However, these approaches impose significant overheads besides their low error coverage, which limits their applicability. In addition, the interval between error occurrence and detection is too long in these approaches. In this paper, based on Deep Reinforcement Learning (DRL), a new error detection approach is proposed that
    
[^111]: 基于时间注意机制的中期风电功率预测新框架

    A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01222](http://arxiv.org/abs/2302.01222)

    本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架（TPE-VMD-TFT），用于24小时和48小时之前的风电功率预测。在法国电力公司Engie的风能数据集上，所提出的方法表现良好。

    

    风能是一种广泛分布、可再生和环保的能源，对缓解全球变暖和能源短缺具有重要作用。然而，由于其不确定性和波动性，大规模风电系统的网格集成具有挑战性。中期风电功率预测可以为能量调度提供基本依据，因此精确的风电功率预测至关重要。本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架。该框架基于变分模式分解（VMD）和时间融合变压器（TFT）定义了24小时和48小时之前的风电功率预测的TPE-VMD-TFT方法。在法国电力公司Engie的风能数据集上，结果表明所提出的方法优于其他方法。

    Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
    
[^112]: 基于可解释的多层图神经网络的癌症基因预测

    Explainable Multilayer Graph Neural Network for Cancer Gene Prediction. (arXiv:2301.08831v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08831](http://arxiv.org/abs/2301.08831)

    基于多层图神经网络和多个生物网络的EMGNN方法可以准确预测癌症基因并提供可解释性。

    

    癌症基因的识别是癌症基因组学研究中一个关键而具有挑战性的问题。现有的计算方法，包括深度图神经网络，未能利用多层基因-基因交互或提供有限的预测解释。这些方法仅限于单一生物网络，无法捕获肿瘤发生的完整复杂性。在不同的生物网络上训练的模型往往会产生不同甚至相反的癌症基因预测，阻碍了它们可信的应用。在这里，我们引入了一种可解释的多层图神经网络（EMGNN）方法，通过利用多个基因-基因交互网络和全癌症多组学数据来识别癌症基因。与传统的在单一生物网络上进行图学习不同，EMGNN 使用多层图神经网络从多个生物网络中学习以实现准确的癌症基因预测。我们的方法始终优于所有现有方法，

    The identification of cancer genes is a critical yet challenging problem in cancer genomics research. Existing computational methods, including deep graph neural networks, fail to exploit the multilayered gene-gene interactions or provide limited explanation for their predictions. These methods are restricted to a single biological network, which cannot capture the full complexity of tumorigenesis. Models trained on different biological networks often yield different and even opposite cancer gene predictions, hindering their trustworthy adaptation. Here, we introduce an Explainable Multilayer Graph Neural Network (EMGNN) approach to identify cancer genes by leveraging multiple genegene interaction networks and pan-cancer multi-omics data. Unlike conventional graph learning on a single biological network, EMGNN uses a multilayered graph neural network to learn from multiple biological networks for accurate cancer gene prediction. Our method consistently outperforms all existing methods,
    
[^113]: 手术聚合：一种用于协同学习的分布式医学影像数据和多样任务协调框架

    Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06683](http://arxiv.org/abs/2301.06683)

    本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。

    

    大规模的胸部X光数据集已经通过深度学习进行异常检测，并有潜力为许多临床应用提供巨大的益处。然而，每个数据集仅专注于检测患者可能同时出现的一部分发现，从而限制了其临床效用。因此，数据协调对于聚合这些数据集来训练具有完整胸部内可能出现的所有异常的临床实用、强大模型至关重要。为此，我们提出了手术聚合，一种协同学习框架，用于协调和聚合分布式异构数据集的知识，并带有部分疾病注释。我们在合成的iid数据集和具有部分注释的真实大规模非iid数据集上评估了手术聚合。我们的结果表明，手术聚合显著优于当前的策略，具有更好的通用性。

    Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
    
[^114]: PlasmoFAB：促进Plasmodium falciparum蛋白抗原候选预测的机器学习基准

    PlasmoFAB: A Benchmark to Foster Machine Learning for Plasmodium falciparum Protein Antigen Candidate Prediction. (arXiv:2301.06454v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2301.06454](http://arxiv.org/abs/2301.06454)

    该论文开发了一个称为PlasmoFAB的机器学习基准，可用于训练机器学习方法来探索Plasmodium falciparum蛋白抗原候选物。

    

    机器学习方法可以用于支持医疗保健相关研究领域的科学发现。然而，这些方法只有在能够基于高质量和策划良好的数据集进行训练时才能可靠地使用。目前，还不存在用于探索Plasmodium falciparum蛋白抗原候选物的此类数据集。寄生虫Plasmodium falciparum引发传染病疟疾。因此，识别潜在的抗原对于开发抗疟疾药物和疫苗至关重要。由于实验探索抗原候选物是昂贵和耗时的过程，应用机器学习方法支持这个过程有可能加速开发抗疟疾药物和疫苗，这是控制疟疾所必需的。结果：我们开发了PlasmoFAB，一个策划良好的基准，可以用于训练机器学习方法来探索Plasmodium falciparum蛋白抗原候选物。

    Motivation: Machine learning methods can be used to support scientific discovery in healthcare-related research fields. However, these methods can only be reliably used if they can be trained on high-quality and curated datasets. Currently, no such dataset for the exploration of Plasmodium falciparum protein antigen candidates exists. The parasite Plasmodium falciparum causes the infectious disease malaria. Thus, identifying potential antigens is of utmost importance for the development of antimalarial drugs and vaccines. Since exploring antigen candidates experimentally is an expensive and time-consuming process, applying machine learning methods to support this process has the potential to accelerate the development of drugs and vaccines, which are needed for fighting and controlling malaria. Results: We developed PlasmoFAB, a curated benchmark that can be used to train machine learning methods for the exploration of Plasmodium falciparum protein antigen candidates. We combined an ex
    
[^115]: 通过代理建模实现高效的激活函数优化

    Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05785](http://arxiv.org/abs/2301.05785)

    本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。

    

    精心设计的激活函数可以提高神经网络在许多机器学习任务中的性能。然而，人类很难构建最优激活函数，而当前的激活函数搜索算法过于昂贵。本文通过三个步骤旨在改进现有技术：首先，通过使用2,913个系统生成的激活函数从头训练卷积、残差和视觉变换器架构来创建 Act-Bench-CNN、Act-Bench-ResNet 和 Act-Bench-ViT 基准数据集。第二，开发了基于代理的方法用于优化基准空间，发现与模型预测分布和激活函数输出分布相关联的 Fisher 信息矩阵的频谱对性能的预测性很高。第三，使用代理在较少的函数评估次数中发现了改进的激活函数架构，同时在几个标准基准测试中实现了最先进的性能。

    Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
    
[^116]: 基于深度学习和模态分解的两相同心喷流预测

    Forecasting through deep learning and modal decomposition in two-phase concentric jets. (arXiv:2212.12731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12731](http://arxiv.org/abs/2212.12731)

    本论文提出了一种将机器学习和单相流数值模拟相结合的方法来实时预测和改善两相流的燃油/空气混合物，从而提高涡扇发动机燃油室喷射器的性能。

    

    本项工作旨在提高涡扇发动机燃油室喷射器的性能，从而意味着提高性能和减少污染物。为了实现实时预测和改善燃油/空气混合物，需要开发允许这种预测的模型。然而，到目前为止所做的工作涉及使用实验数据（难以测量）或完整问题的数值解（计算成本高昂）。后者涉及解决一个偏微分方程（PDE）系统。这些问题使得开发实时预测工具变得困难。因此，在本工作中，我们建议将机器学习与（相对更便宜的）单相流数值模拟结合起来，在存在切向不连续性的情况下估计两相流中的混合过程。在这个意义上，我们研究了两个提出的神经网络（NN）模型作为PDE代理模型的应用。NN通过预测未来动态来实现。

    This work aims to improve fuel chamber injectors' performance in turbofan engines, thus implying improved performance and reduction of pollutants. This requires the development of models that allow real-time prediction and improvement of the fuel/air mixture. However, the work carried out to date involves using experimental data (complicated to measure) or the numerical resolution of the complete problem (computationally prohibitive). The latter involves the resolution of a system of partial differential equations (PDE). These problems make difficult to develop a real-time prediction tool. Therefore, in this work, we propose using machine learning in conjunction with (complementarily cheaper) single-phase flow numerical simulations in the presence of tangential discontinuities to estimate the mixing process in two-phase flows. In this meaning we study the application of two proposed neural network (NN) models as PDE surrogate models. Where the future dynamics is predicted by the NN, gi
    
[^117]: COmic：用于（多）组学数据解释性端到端学习的卷积核网络

    COmic: Convolutional Kernel Networks for Interpretable End-to-End Learning on (Multi-)Omics Data. (arXiv:2212.02504v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2212.02504](http://arxiv.org/abs/2212.02504)

    COmic是一种结合了卷积核网络和通路诱导核的人工神经网络，用于（多）组学数据的稳健和可解释的端到端学习。

    

    随着近年来技术的进步，可用组学数据集的大小稳步增加。虽然这种样本量的增加可以用于改善医疗保健中相关预测任务的性能，但针对大型数据集优化的模型通常作为黑盒操作。在高风险场景（如医疗保健）中，使用黑盒模型会带来安全和保密问题。如果没有关于影响预测的分子因素和表型的解释，保健提供者只能盲目信任模型。我们提出了一种新的人工神经网络类型，名为卷积组学核网络（COmic）。通过将卷积核网络与通路诱导核相结合，我们的方法实现了在从几百个到几十万个样本的组学数据集上稳健和可解释的端到端学习。此外，COmic可以轻松适应多组学数据。

    Motivation: The size of available omics datasets is steadily increasing with technological advancement in recent years. While this increase in sample size can be used to improve the performance of relevant prediction tasks in healthcare, models that are optimized for large datasets usually operate as black boxes. In high stakes scenarios, like healthcare, using a black-box model poses safety and security issues. Without an explanation about molecular factors and phenotypes that affected the prediction, healthcare providers are left with no choice but to blindly trust the models. We propose a new type of artificial neural network, named Convolutional Omics Kernel Network (COmic). By combining convolutional kernel networks with pathway-induced kernels, our method enables robust and interpretable end-to-end learning on omics datasets ranging in size from a few hundred to several hundreds of thousands of samples. Furthermore, COmic can be easily adapted to utilize multi-omics data.  Result
    
[^118]: 基于评分的去噪算法在原子结构识别中的应用

    Score-based denoising for atomic structure identification. (arXiv:2212.02421v3 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2212.02421](http://arxiv.org/abs/2212.02421)

    本论文提出了一种基于评分的去噪算法，可有效去除原子模拟过程中的热振动噪声，适用于不同原子间相互作用生成的模拟数据，并能提高分类方法的准确性。

    

    本论文提出了一种有效的方法，用于在分子模拟过程中分析复杂动态时去除热振动噪声。我们的算法采用去噪评分函数，通过对合成噪声但在完美晶格上无干扰下进行训练，迭代地消除原子位置中的噪声和扰动。去噪后的结构清晰地展现了晶体的内在有序性，同时保留了与晶体缺陷相关的无序性。我们的去噪器无需依赖于原子间相互作用和显式模拟输入，完全基于几何学理论，且能够适用于大量不同原子间相互作用生成的模拟数据。这一去噪算法能够提高现有分类方法（如公共邻居分析和多面体模板匹配）的识别准确性，并在最近的热扰动结构基准数据集上达到了完美的分类准确性。

    We propose an effective method for removing thermal vibrations that complicate the task of analyzing complex dynamics in atomistic simulation of condensed matter. Our method iteratively subtracts thermal noises or perturbations in atomic positions using a denoising score function trained on synthetically noised but otherwise perfect crystal lattices. The resulting denoised structures clearly reveal underlying crystal order while retaining disorder associated with crystal defects. Purely geometric, agnostic to interatomic potentials, and trained without inputs from explicit simulations, our denoiser can be applied to simulation data generated from vastly different interatomic interactions. The denoiser is shown to improve existing classification methods such as common neighbor analysis and polyhedral template matching, reaching perfect classification accuracy on a recent benchmark dataset of thermally perturbed structures up to the melting point. Demonstrated here in a wide variety of a
    
[^119]: 基于跨模态神经模型重新编程的低资源音乐风格分类

    Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming. (arXiv:2211.01317v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.01317](http://arxiv.org/abs/2211.01317)

    本文提出了一种基于神经模型重新编程的迁移学习方法，并针对复杂输入数据提出了输入依赖NMR范式，能够有效地进行音乐风格分类。

    

    迁移学习方法在处理训练数据有限的任务时展现出了很好的效果。然而，微调预训练的神经网络来处理目标领域数据通常需要大量的内存和计算资源。本文提出了一种基于神经模型重新编程 (NMR) 的新方法，用于利用预训练模型进行低资源音乐分类。NMR旨在通过修改冻结的预训练模型的输入，将预训练模型从源域重新调整用于目标域。除了已知的与输入无关的重新编程方法外，我们还提出了一种先进的重新编程范式：输入依赖NMR，以增加对复杂输入数据（如音频）的适应性。实验结果表明，使用这种重新编程方法，基于大规模数据集预训练的神经模型成功地进行音乐风格分类。所提出的两种输入相关的NMR迁移学习方法表现优于传统的迁移学习方法。

    Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained models for low-resource (music) classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained model. In addition to the known, input-independent, reprogramming method, we propose an advanced reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outpe
    
[^120]: 成本感知的通用α投资法用于多重假设检验

    Cost-aware Generalized $\alpha$-investing for Multiple Hypothesis Testing. (arXiv:2210.17514v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17514](http://arxiv.org/abs/2210.17514)

    本文提出了成本感知的通用α投资法进行多重假设检验，拓展了α投资规则以考虑样本大小，通过构建与自然对抗的博弈来优化α财富的期望回报(ERO)并提供最佳样本大小。经实证表明，该规则能更准确地拒绝虚假零假设。

    

    本文探讨在数据收集具有非平凡成本时进行顺序多重假设检验的问题。该问题在识别疾病过程中的差异表达基因等生物实验中出现。本文构建在通用α投资框架上，能在顺序检验环境下进行误发现率控制。我们对α-财富的长期渐进行为进行了理论分析，引出了在α投资决策规则中考虑样本大小的思考。将检验过程视为与自然对抗的博弈，本文构建了一种决策规则，优化α财富的期望回报(ERO)，并为测试提供了最佳样本大小。经验证明，成本感知的ERO决策规则比其他方法正确地拒绝了更多的虚假零假设。本文扩展了成本感知的ERO投资至有限时间检验，使决策规则能够跨越多个检验分配样本。

    We consider the problem of sequential multiple hypothesis testing with nontrivial data collection cost. This problem appears, for example, when conducting biological experiments to identify differentially expressed genes in a disease process. This work builds on the generalized $\alpha$-investing framework that enables control of the false discovery rate in a sequential testing setting. We make a theoretical analysis of the long term asymptotic behavior of $\alpha$-wealth which motivates a consideration of sample size in the $\alpha$-investing decision rule. Posing the testing process as a game with nature, we construct a decision rule that optimizes the expected return (ERO) of $\alpha$-wealth and provides an optimal sample size for the test. Empirical results show that a cost-aware ERO decision rule correctly rejects more false null hypotheses than other methods. We extend cost-aware ERO investing to finite-horizon testing which enables the decision rule to allocate samples across ma
    
[^121]: 基于磁性小波框架的有向图卷积神经网络

    A Magnetic Framelet-Based Convolutional Neural Network for Directed Graphs. (arXiv:2210.10993v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10993](http://arxiv.org/abs/2210.10993)

    本文介绍了一种基于磁性小波框架的有向图谱图卷积神经网络Framelet-MagNet，在滤波方面比传统方法更加有效，且在节点分类、链路预测和去噪等任务上表现优异。

    

    谱图卷积网络是分析和处理图数据的强大工具，通常通过傅里叶变换进行频率过滤以获得具有选择性信息的表示。尽管研究表明谱图卷积网络可以通过基于小波框架的滤波得到增强，但绝大多数研究仅考虑无向图。在本文中，我们引入了基于磁性小波框架的有向图谱图卷积神经网络Framelet-MagNet。该模型将小波变换应用于有向图信号，形成更复杂的表示用于滤波。有向图小波基使用复值磁拉普拉斯构造，同时在实数和复数域中进行信号处理。我们在节点分类、链路预测和去噪等多方面对Framelet-MagNet的预测能力进行了实证验证，结果表明其优于多种最新模型。

    Spectral Graph Convolutional Networks (spectral GCNNs), a powerful tool for analyzing and processing graph data, typically apply frequency filtering via Fourier transform to obtain representations with selective information. Although research shows that spectral GCNNs can be enhanced by framelet-based filtering, the massive majority of such research only considers undirected graphs. In this paper, we introduce Framelet-MagNet, a magnetic framelet-based spectral GCNN for directed graphs (digraphs). The model applies the framelet transform to digraph signals to form a more sophisticated representation for filtering. Digraph framelets are constructed with the complex-valued magnetic Laplacian, simultaneously leading to signal processing in both real and complex domains. We empirically validate the predictive power of Framelet-MagNet over a range of state-of-the-art models in node classification, link prediction, and denoising.
    
[^122]: 用贝叶斯优化发现多样的解决方案

    Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10953](http://arxiv.org/abs/2210.10953)

    ROBOT是一种新的贝叶斯优化方法，可以找到一组高性能、多样化的解决方案，解决了传统单目标贝叶斯优化方法只能找到一个最佳解决方案的局限性。

    ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.

    贝叶斯优化是一种用于黑盒目标函数的高效优化的流行方法。传统的单目标贝叶斯优化方法只寻求找到一个最佳解决方案，这在解决方案后期可能变得棘手的情况下会有很大的局限性。为了解决这个问题，我们提出了一种名为ROBOT的排序贝叶斯优化方法，旨在找到一组高性能、多样化的解决方案，这些解决方案根据用户指定的多样性度量进行排序。我们在几个真实世界的应用中评估了ROBOT，并展示了它可以发现大量高性能的多样化解决方案，同时与寻找单个最佳解决方案相比，需要很少的额外函数评估。

    Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
    
[^123]: 提高图神经网络的表现：一种高频率增强器

    Improving Your Graph Neural Networks: A High-Frequency Booster. (arXiv:2210.08251v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08251](http://arxiv.org/abs/2210.08251)

    本文提出了一种高频率增强器，将高频信息融入到GNN中，提高异质性图的表达能力，实验结果表明比流行的基准模型性能提高了3.6%。

    

    图神经网络(GNN)具有学习图结构数据有效表示的潜力，其中最重要的应用是半监督节点分类。然而，在这个应用中，由于过度平滑和异质性等问题，GNN框架往往会失败。最流行的GNN被认为集中于消息传递框架，但最近的研究表明，从信号处理角度来看，这些GNN通常受到低通滤波器的限制。因此，我们将高频信息融入到GNN中以缓解这个普遍问题。本文认为，原始图的补集包括高通滤波器，并且提出补集拉普拉斯正则化(CLAR)以有效增强高频分量。实验结果证明，CLAR有助于GNN应对过度平滑，提高异质性图的表达能力，可以比流行的基准模型性能提高3.6%。

    Graph neural networks (GNNs) hold the promise of learning efficient representations of graph-structured data, and one of its most important applications is semi-supervised node classification. However, in this application, GNN frameworks tend to fail due to the following issues: over-smoothing and heterophily. The most popular GNNs are known to be focused on the message-passing framework, and recent research shows that these GNNs are often bounded by low-pass filters from a signal processing perspective. We thus incorporate high-frequency information into GNNs to alleviate this genetic problem. In this paper, we argue that the complement of the original graph incorporates a high-pass filter and propose Complement Laplacian Regularization (CLAR) for an efficient enhancement of high-frequency components. The experimental results demonstrate that CLAR helps GNNs tackle over-smoothing, improving the expressiveness of heterophilic graphs, which adds up to 3.6% improvement over popular basel
    
[^124]: KAIROS：利用异构云资源构建高效的机器学习推断系统

    KAIROS: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources. (arXiv:2210.05889v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2210.05889](http://arxiv.org/abs/2210.05889)

    KAIROS是一个新颖的机器学习推断系统，利用异构计算硬件池和优化的推断查询分配，实现了最大化的查询吞吐量同时满足服务质量和成本预算限制。在产业级深度学习模型评估中，KAIROS相比于最优异构方案吞吐量增加了2倍，并且超过了其他现有方案。

    

    在线推断正成为许多企业的关键服务产品，部署在云平台上以满足客户需求。尽管它们有利润产生能力，但这些服务需要在严格的服务质量和成本预算限制下运行。本文介绍了KAIROS，一个新颖的运行时框架，最大化查询吞吐量同时满足服务质量目标和成本预算。KAIROS设计并实现了新颖的技术，构建了一个异构计算硬件池，避免在线资源探索的额外开销，并优化地在运行时分配推断查询。我们使用产业级深度学习模型进行评估，结果显示KAIROS产生的吞吐量比最优异构方案高出2倍，并且在性能上超过了现有的方案，这些方案通过忽略它们的探索开销获得了优势实现。

    Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade deep learning (DL) models shows that KAIROS yields up to 2X the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.
    
[^125]: 基于核函数的语言模型微调视角

    A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05643](http://arxiv.org/abs/2210.05643)

    本文研究神经切线核 (NTK) 在描述预训练语言模型微调过程中的适用性。实验证明在14个NLP任务中使用掩码词预测问题作为下游任务，可以取得好的效果。

    

    在自然语言处理中，通过对预训练语言模型 (LMs) 进行微调，在低数据情况下解决 NLP 任务已经成为标准做法。但是，目前对于经验成功背后的理论机制了解很少，例如为什么在几十个训练点上微调一个有 $10^8$ 个或更多参数的模型不会导致过拟合。本文研究了神经切线核 (NTK) 在描述预训练语言模型的微调过程中的适用性。我们扩展了 NTK 形式化方法以应用于 Adam，并使用 Tensor Programs 描述了 NTK 适用于描述预训练语言模型微调更新的条件。我们在 14 个 NLP 任务上进行了广泛的实验验证了我们的理论，并表明通过提示将下游任务表述为掩码词预测问题可以取得良好的效果。

    It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting 
    
[^126]: 连续空间中的集成强化学习——一种多步层次策略训练方法

    Ensemble Reinforcement Learning in Continuous Spaces -- A Hierarchical Multi-Step Approach for Policy Training. (arXiv:2209.14488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14488](http://arxiv.org/abs/2209.14488)

    本文提出了一种新的集成强化学习算法，基于多步层次策略训练方法来训练基学习器集成，以提高学习稳定性和性能。

    

    最近，演员-评论家深度强化学习算法在解决各种具有高维连续状态和动作空间的复杂控制任务方面取得了杰出的成功。然而，现有的研究表明，演员-评论家强化学习算法通常无法有效地探索其学习环境，导致有限的学习稳定性和性能。为了解决这个问题，最近提出了几种集成强化学习算法来增加探索并稳定学习过程。然而，现有大多数集成算法没有明确地训练所有基学习器以共同优化集成的性能。本文提出了一种基于创新的多步集成方法来训练基学习器集成的新技术。这种训练技术使我们能够开发一种新的层次集成强化学习算法，有效地提高了学习稳定性和性能。

    Actor-critic deep reinforcement learning (DRL) algorithms have recently achieved prominent success in tackling various challenging reinforcement learning (RL) problems, particularly complex control tasks with high-dimensional continuous state and action spaces. Nevertheless, existing research showed that actor-critic DRL algorithms often failed to explore their learning environments effectively, resulting in limited learning stability and performance. To address this limitation, several ensemble DRL algorithms have been proposed lately to boost exploration and stabilize the learning process. However, most of existing ensemble algorithms do not explicitly train all base learners towards jointly optimizing the performance of the ensemble. In this paper, we propose a new technique to train an ensemble of base learners based on an innovative multi-step integration method. This training technique enables us to develop a new hierarchical learning algorithm for ensemble DRL that effectively p
    
[^127]: 带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法

    Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06316](http://arxiv.org/abs/2207.06316)

    本文提出了一种带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\beta$-差异和其他稀疏约束。

    

    本文提出了一种新的多元乘法更新方法，用于具有 $\beta$-差异和两个因子中的一个（比如说，激活矩阵）稀疏正则化的非负矩阵分解。标准的做法是限制字典的列具有单位范数，从而控制另一个因子（字典矩阵）的范数，以避免病态问题。我们的方法将原问题重新参数化为等价的标度不变的目标函数的优化问题。然后，我们导出块下降主导最小化算法，这些算法对于 $\ell_{1}$-正则化或更 "激进" 的对数正则化都可以产生简单的多元乘法更新。与其他最先进的方法相比，我们的算法在任何 $\beta$-差异（即任何 $\beta$ 的值）和其他稀疏约束上也具有通用性。

    This article introduces new multiplicative updates for nonnegative matrix factorization with the $\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\ell_{1}$-regularization or the more "aggressive" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\beta$-divergence (i.e., any value of $\beta$) and t
    
[^128]: 得分模型的多项式复杂度的重要性证明

    Convergence for score-based generative modeling with polynomial complexity. (arXiv:2206.06227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06227](http://arxiv.org/abs/2206.06227)

    本文证明了对于得分模型而言，从一个概率分布中抽样的核心机制，在$L^2(p)$准确估计$\nabla \ln p$后可以多项式收敛。同时提供了对基于得分的生成模型的理论分析，为使用退火程序生成样本提供了理论基础。

    

    得分模型（SGM）是一种学习概率分布并生成更多样本的高效方法。本文证明了SGM背后的核心机制即在$L^2(p)$准确估计$\nabla \ln p$后从概率密度$p$中抽样的多项式收敛保证。与之前的工作相比，我们不会产生随时间指数增长或遭受维数灾难的误差。我们的保证适用于任何平滑分布，且与其对数Sobolev常数多项式相关。使用我们的保证，我们对基于得分的生成模型进行了理论分析，将白噪声输入转换为从不同噪声尺度给定得分估计的学习数据分布的样本。我们的分析为使用退火程序生成好的样本提供了理论基础，因为我们的证明基本上是依赖于使用。

    Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $\nabla \ln p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using
    
[^129]: 社会偏见遇到数据偏见: 标注和测量误差对公平标准的影响

    Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria. (arXiv:2206.00137v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00137](http://arxiv.org/abs/2206.00137)

    本文研究了机器学习算法在训练数据集存在偏见时现有公平标准的鲁棒性，探究了标记和测量误差对其影响。研究发现，一些约束可以在面对某些统计偏差时保持稳健，而另一些则会在训练偏见数据集时被显著违反。

    

    尽管已经提出了许多公平标准来确保机器学习算法不会表现出或放大我们现有的社会偏见，但这些算法是在本身可能存在统计偏差的数据集上训练的。在本文中，我们研究了在算法训练在偏见数据集上时一些现有(人口)公平标准的鲁棒性。我们考虑了两种形式的数据集偏差：标记过程中的先前决策制定者的错误和对劣势个体特征的测量误差。我们在理论上证明了一些约束(例如人口均等性)在面对某些统计偏差时可以保持稳健，而另一些(例如平等机会)则会在训练偏见数据集时被显著违反。我们还分析了这些标准和决策制定者效用对偏见的敏感性。我们基于三个真实数据集(FICO、成人和德国信用评分数据集)提供了数值实验，支持我们的结论。

    Although many fairness criteria have been proposed to ensure that machine learning algorithms do not exhibit or amplify our existing social biases, these algorithms are trained on datasets that can themselves be statistically biased. In this paper, we investigate the robustness of a number of existing (demographic) fairness criteria when the algorithm is trained on biased data. We consider two forms of dataset bias: errors by prior decision makers in the labeling process, and errors in measurement of the features of disadvantaged individuals. We analytically show that some constraints (such as Demographic Parity) can remain robust when facing certain statistical biases, while others (such as Equalized Odds) are significantly violated if trained on biased data. We also analyze the sensitivity of these criteria and the decision maker's utility to biases. We provide numerical experiments based on three real-world datasets (the FICO, Adult, and German credit score datasets) supporting our 
    
[^130]: 带有属性删除子网络的模块化和按需偏差缓解方法

    Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15171](http://arxiv.org/abs/2205.15171)

    提出一种新颖的模块化偏差缓解方法，在推理时间按需集成到核心模型中的独立去偏置子网络，在性别、种族和年龄等受保护属性的分类任务中，该方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。

    

    社会偏见反映在大型预训练语言模型及其在下游任务中的微调版本中。常见的处理偏差的方法引入了额外的优化标准，并更新模型以达到新的去偏置状态。然而，在实践中，最终用户和从业人员可能更喜欢切换回原始模型，或仅对特定子集的保护属性应用去偏置。为了实现这一点，我们提出了一种新颖的模块化偏差缓解方法，包括独立高度稀疏的去偏置子网络，其中每个去偏置模块可以在推理时间按需集成到核心模型中。我们的方法借鉴了“diff”剪枝的概念，并提出了一种适合于各种表示分离优化的新型训练方式。我们在具有性别、种族和年龄等受保护属性的三个分类任务上进行了实验。结果表明，我们的模块化方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。

    Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
    
[^131]: 掩码语言模型在科学中的减弱收益

    The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.11342](http://arxiv.org/abs/2205.11342)

    本文从科学角度出发，评估了14个领域特定的基于Transformer的模型在12个下游科学任务中的表现，发现增加模型大小、训练数据或计算时间并不总是会导致显着提高，可能会有出乎意料的性能差异。

    

    基于Transformer的掩码语言模型，如BERT，在一般语料库上训练后在下游任务中表现出色。还已经证明这些模型的下游任务性能可以通过在更多数据上更长时间地预训练更大的模型来提高。在这项工作中，我们经验性地评估了这些结果在科学任务中的适用程度。我们使用了14个特定领域的基于Transformer的模型（包括ScholarBERT，一个新的 7.7 亿参数的科学聚焦掩码语言模型，预先训练了 高达 225B 个令牌），以评估训练数据、模型大小、预训练和微调时间对12个下游科学任务的影响。有趣的是，我们发现，在科学信息提取任务中，增加模型大小、训练数据或计算时间并不总是会导致显着的提高（即 >1% F1），如果有的话，可能会有出乎意料的性能差异，并提供了可能的解释。

    Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770M-parameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model sizes, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if at all, in scientific information extraction tasks and offered possible explanations for the surprising performance differences.
    
[^132]: ImGCL：重访图形对比学习在不平衡节点分类中的应用

    ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification. (arXiv:2205.11332v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11332](http://arxiv.org/abs/2205.11332)

    本文提出了一种名为"ImGCL"的图形对比学习（GCL）算法框架，该框架能够自动自适应地平衡不平衡节点分类问题中从无标签节点（图）中学到的表示，通过整合在线聚类和逐步平衡采样方法，我们的算法可以有效地学习区分表示，实现与先进技术相当的性能。

    

    图形对比学习（GCL）由于其在无标签节点/图形表示学习方面的超凡性能而受到了极大关注。 然而，在实践中，给定图形的未标记节点的潜在类别分布通常是不平衡的。这种高度不平衡的分类分布不可避免地降低了GCL中学习到的节点表示的质量。 实际上，我们在实验中发现，大多数最先进的GCL方法无法获得有区别的表示，并在不平衡节点分类中表现出差劲的性能。受此观察的启发，我们提出了一种基于不平衡节点分类的正式GCL框架（ImGCL），该框架可以自动和自适应地平衡从GCL中学到的表示，即使在高度不平衡的情况下也能够有效地学习区分表示，并实现与先进技术相当的性能。

    Graph contrastive learning (GCL) has attracted a surge of attention due to its superior performance for learning node/graph representations without labels. However, in practice, the underlying class distribution of unlabeled nodes for the given graph is usually imbalanced. This highly imbalanced class distribution inevitably deteriorates the quality of learned node representations in GCL. Indeed, we empirically find that most state-of-the-art GCL methods cannot obtain discriminative representations and exhibit poor performance on imbalanced node classification. Motivated by this observation, we propose a principled GCL framework on Imbalanced node classification (ImGCL), which automatically and adaptively balances the representations learned from GCL without labels. Specifically, we first introduce the online clustering based progressively balanced sampling (PBS) method with theoretical rationale, which balances the training sets based on pseudo-labels obtained from learned representat
    
[^133]: 可信医疗联邦学习的奖励系统

    Reward Systems for Trustworthy Medical Federated Learning. (arXiv:2205.00470v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00470](http://arxiv.org/abs/2205.00470)

    本文研究了如何通过奖励系统防止医疗联邦学习中的模型偏差，并提出了一种结合的奖励系统以提高预测性能和降低偏差。

    

    联邦学习（FL）已经引起了研究者和实践者的高度关注，用于训练医疗机器学习（ML）模型。确保这些模型的可信度至关重要。特别是，偏差（定义为模型在不同子群体中预测性能的差异）可能导致对特定子群体的不公平，这是不可取的现象。在本研究中，我们研究了医疗FL中到底存在多少偏差，并如何通过奖励系统防止过度偏差。我们首先使用Shapley值近似方法评估了跨silomedical FL中衡量机构预测性能和偏差的贡献方法。在第二步中，我们设计了不同的奖励系统，激励对高预测性能或低偏差作出贡献。然后，我们提出了一种结合的奖励系统，激励作出贡献以提高预测性能和降低偏差。我们使用多个医学胸透来评估我们的工作。

    Federated learning (FL) has received high interest from researchers and practitioners to train machine learning (ML) models for healthcare. Ensuring the trustworthiness of these models is essential. Especially bias, defined as a disparity in the model's predictive performance across different subgroups, may cause unfairness against specific subgroups, which is an undesired phenomenon for trustworthy ML models. In this research, we address the question to which extent bias occurs in medical FL and how to prevent excessive bias through reward systems. We first evaluate how to measure the contributions of institutions toward predictive performance and bias in cross-silo medical FL with a Shapley value approximation method. In a second step, we design different reward systems incentivizing contributions toward high predictive performance or low bias. We then propose a combined reward system that incentivizes contributions toward both. We evaluate our work using multiple medical chest X-ray
    
[^134]: 面向高维健壮统计学的流算法

    Streaming Algorithms for High-Dimensional Robust Statistics. (arXiv:2204.12399v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2204.12399](http://arxiv.org/abs/2204.12399)

    本研究提出了首个高维健壮统计学流算法，具有几乎最优的存储需求，特别是在Huber污染模型下的高维健壮均值估计任务中，提供了一个高效的单遍流算法。

    

    本文研究流模型下的高维健壮统计学问题。最近的一些工作针对一系列高维健壮估计任务提出了计算有效率的算法。不幸的是，所有之前的算法都需要存储整个数据集，使得内存至少呈二次方与维度同阶。本研究提出了首个高维健壮统计学流算法，其具有几乎最优的存储需求（以对数因子表示）。本文主要结果是在Huber污染模型下的高维健壮均值估计任务，我们给出了一个高效的单遍流算法，具有几乎最优的误差保证和空间复杂度几乎线性于维度。作为推论，我们得到了几个更复杂的任务的几乎最优空间复杂度的流算法，包括健壮协方差估计，健壮回归，更普遍的健壮随机优化等。

    We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimiz
    
[^135]: 对深度神经网络的鲁棒性物理世界对抗攻击：Adversarial Neon Beam（arXiv:2204.00853v2 [cs.CV] 已更新）

    Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.00853](http://arxiv.org/abs/2204.00853)

    本文提出了一种称为Adversarial Neon Beam（AdvNB）的物理攻击方法，通过获取对抗性氖光束的物理参数并且仅需要极少的查询就能执行物理攻击，该攻击方法在数字和物理测试中都可达成领先的攻击效果，是一种可怕的深度神经网络攻击方法。

    

    在物理世界中，光线会影响深度神经网络的性能。如今，许多基于深度神经网络的产品已经被应用到日常生活中。然而，光线产生的对抗扰动可能对这些系统产生极其危险的影响。本文提出了一种攻击方法，称为Adversarial Neon Beam（AdvNB），它可以通过极少的查询获取对抗性氖光束的物理参数来执行物理攻击。实验表明我们的算法在数字测试和物理测试中都可以达到领先的攻击效果。在数字环境中，攻击成功率达到了99.3％，在物理环境中，攻击成功率达到了100％。与最先进的物理攻击方法相比，我们的方法可以实现更好的物理扰动隐蔽性。此外，通过分析实验数据，我们发现...

    In the physical world, light affects the performance of deep neural networks. Nowadays, many products based on deep neural network have been put into daily life. There are few researches on the effect of light on the performance of deep neural network models. However, the adversarial perturbations generated by light may have extremely dangerous effects on these systems. In this work, we propose an attack method called adversarial neon beam (AdvNB), which can execute the physical attack by obtaining the physical parameters of adversarial neon beams with very few queries. Experiments show that our algorithm can achieve advanced attack effect in both digital test and physical test. In the digital environment, 99.3% attack success rate was achieved, and in the physical environment, 100% attack success rate was achieved. Compared with the most advanced physical attack methods, our method can achieve better physical perturbation concealment. In addition, by analyzing the experimental data, w
    
[^136]: 关于带有线性函数逼近的SARSA算法收敛性的研究

    On the Convergence of SARSA with Linear Function Approximation. (arXiv:2202.06828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06828](http://arxiv.org/abs/2202.06828)

    本文通过对投影SARSA到有限区域的收敛速度的探究，取得了在带有线性函数逼近的SARSA算法收敛性方面的进展，发现收敛区域比想象的要小得多。

    

    SARSA是一种经典的增强学习算法，该算法在与线性函数逼近相结合时会出现震荡现象。本文通过展示投影SARSA到有限区域的收敛速度，取得了在此方面的一定进展。惊人的是，只要奖励的大小不是太大，收敛区域比想象中的要小得多。值得注意的是，现有的关于线性SARSA收敛性的研究都需要SARSA策略改进算子的Lipschitz常数足够小；与之不同的是，我们的分析适用于任意Lipschitz常数，从而揭示了线性SARSA的新行为范式。

    SARSA, a classical on-policy control algorithm for reinforcement learning, is known to chatter when combined with linear function approximation: SARSA does not diverge but oscillates in a bounded region. However, little is known about how fast SARSA converges to that region and how large the region is. In this paper, we make progress towards this open problem by showing the convergence rate of projected SARSA to a bounded region. Importantly, the region is much smaller than the region that we project into, provided that the magnitude of the reward is not too large. Existing works regarding the convergence of linear SARSA to a fixed point all require the Lipschitz constant of SARSA's policy improvement operator to be sufficiently small; our analysis instead applies to arbitrary Lipschitz constants and thus characterizes the behavior of linear SARSA for a new regime.
    
[^137]: HARFE: 硬岭随机特征扩展方法

    HARFE: Hard-Ridge Random Feature Expansion. (arXiv:2202.02877v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.02877](http://arxiv.org/abs/2202.02877)

    论文提出了一种适用于高维稀疏可加函数的硬岭随机特征扩展方法（HARFE）模型，它可以通过应用基于硬阈值追踪的算法来进行近似计算，同时利用稀疏岭回归（SRR）表达式来取得稀疏模型选择和岭回归平滑之间的平衡，相比其他算法，HARFE方法在合成数据和真实数据集上具有更低的误差。

    

    本论文针对高维稀疏可加函数，提出一种随机特征模型——硬岭随机特征扩展方法（HARFE）。该方法利用基于硬阈值追踪的算法，应用于稀疏岭回归（SRR）问题，来近似计算相对于随机特征矩阵的系数。该SRR表达式在稀疏模型选择和岭回归平滑之间取得平衡，从而有利于处理噪声和异常值。此外，为了匹配加性函数假设，我们在随机特征矩阵中采用了随机稀疏连接模式。我们证明了HARFE方法会收敛至给定误差界限，具体取决于噪声和稀疏岭回归模型参数。基于合成数据和真实数据集的数值结果表明，HARFE方法的误差低于（或与）其他最先进算法相当。

    We propose a random feature model for approximating high-dimensional sparse additive functions called the hard-ridge random feature expansion method (HARFE). This method utilizes a hard-thresholding pursuit-based algorithm applied to the sparse ridge regression (SRR) problem to approximate the coefficients with respect to the random feature matrix. The SRR formulation balances between obtaining sparse models that use fewer terms in their representation and ridge-based smoothing that tend to be robust to noise and outliers. In addition, we use a random sparse connectivity pattern in the random feature matrix to match the additive function assumption. We prove that the HARFE method is guaranteed to converge with a given error bound depending on the noise and the parameters of the sparse ridge regression model. Based on numerical results on synthetic data as well as on real datasets, the HARFE approach obtains lower (or comparable) error than other state-of-the-art algorithms.
    
[^138]: 有限记忆下的通用图协作学习：复杂度、可学习性和可靠性分析

    Collaborative Learning in General Graphs with Limited Memorization: Complexity, Learnability, and Reliability. (arXiv:2201.12482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12482](http://arxiv.org/abs/2201.12482)

    本文提出了一个三阶段的轻量级随机游走算法，在有限的记忆和通讯带宽限制下完成了通用图上的协作学习，解决了连接图不完全和非良构的问题，并提高了学习过程的可靠性。

    

    本文考虑在任意连接图中有限记忆和通信带宽的智能体完成 K-臂赌博机问题。现有的研究通常假设通信图必须是完全或良构的，然而这在实践中并不总是成立。而有限的记忆和通信带宽也会限制已有经验的共享，甚至可能导致智能体向其他智能体传递虚假的经验信息，极大地限制了学习过程的可靠性。针对以上问题，我们提出了一个包含三个阶段的协作学习算法，每个阶段通过轻量级的随机游走实现最新经验的分享。

    We consider a K-armed bandit problem in general graphs where agents are arbitrarily connected and each of them has limited memorizing capabilities and communication bandwidth. The goal is to let each of the agents eventually learn the best arm. It is assumed in these studies that the communication graph should be complete or well-structured, whereas such an assumption is not always valid in practice. Furthermore, limited memorization and communication bandwidth also restrict the collaborations of the agents, since the agents memorize and communicate very few experiences. Additionally, an agent may be corrupted to share falsified experiences to its peers, while the resource limit in terms of memorization and communication may considerably restrict the reliability of the learning process. To address the above issues, we propose a three-staged collaborative learning algorithm. In each step, the agents share their latest experiences with each other through light-weight random walks in a ge
    
[^139]: 两时间尺度更新规则训练生成式对抗网络中的关键批次大小的存在和估计

    Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11989](http://arxiv.org/abs/2201.11989)

    本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。

    

    先前的研究表明，在理论和实践中，使用不同的学习率，如不同的恒定率或不同的衰减率等，使用两时间尺度更新规则（TTUR）有助于训练生成式对抗网络（GAN）。此外，批次大小对于使用TTUR训练GANs也很重要，两者都影响了训练所需的步骤数量。本文基于恒定学习率研究了批次大小与使用TTUR训练GANs所需步骤数量之间的关系。我们理论上表明，对于具有恒定学习率的TTUR，为了找到鉴别器和生成器损失函数的稳定点，所需步骤数随着批次大小的增加而减少，并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。然后，我们使用Fr'echet Inception Distance（FID）作为训练的性能测量，并提供了...

    Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
    
[^140]: 利用行动影响规律和外生状态变量进行离线强化学习

    Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning. (arXiv:2111.08066v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08066](http://arxiv.org/abs/2111.08066)

    本文提出了一种利用行动影响规律和外生状态变量进行离线强化学习的算法，该算法在现实世界的许多领域中成立，包括金融市场，并在模拟和真实世界中的不同数据收集策略中优于现有的离线强化学习算法。

    This paper proposes an algorithm for offline reinforcement learning that exploits the Action Impact Regularity (AIR) property, which holds in many real-world domains including financial markets, and outperforms existing algorithms across different data collection policies in simulated and real world.

    离线强化学习——从一批数据中学习策略——已知对于一般的MDP来说是困难的。这些结果促使我们需要关注离线强化学习可能可行的特定类别的MDP。在这项工作中，我们探索了一类受限制的MDP，以获得离线强化学习的保证。我们称之为行动影响规律（AIR）的关键属性是，行动主要影响状态的一部分（内生组件），并且对状态的其余部分（外生组件）影响有限。AIR是一个强假设，但它在许多现实世界的领域中仍然成立，包括金融市场。我们讨论了利用AIR属性的算法，并为基于Fitted-Q迭代的算法提供了理论分析。最后，我们证明了该算法在模拟和真实世界中的不同数据收集策略中优于现有的离线强化学习算法。

    Offline reinforcement learning -- learning a policy from a batch of data -is known to be hard for general MDPs. These results motivate the need to look at specific classes of MDPs where offline reinforcement learning might be feasible. In this work, we explore a restricted class of MDPs to obtain guarantees for offline reinforcement learning. The key property, which we call Action Impact Regularity (AIR), is that actions primarily impact a part of the state (an endogenous component) and have limited impact on the remaining part of the state (an exogenous component). AIR is a strong assumption, but it nonetheless holds in a number of real-world domains including financial markets. We discuss algorithms that exploit the AIR property, and provide a theoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing offline reinforcement learning algorithms across different data collection policies in simulated and real world
    
[^141]: 高阶图形模型中图网络推断的一般化

    Generalization of graph network inferences in higher-order graphical models. (arXiv:2107.05729v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2107.05729](http://arxiv.org/abs/2107.05729)

    本论文提出了递归因子图神经网络(RF-GNN)，用于实现对涉及多变量相互作用的图形模型的快速近似推断。在多个图形模型家族的实验中展示了RF-GNN在表达性图形模型中快速且准确地执行推断的潜力。

    

    概率图模型提供了一种描述复杂统计结构的强大工具，具有从控制机器人手臂到理解神经计算等许多科学和工程实际应用。这些图形模型的主要挑战是，在一般图形条件下，如边际化等推断是不可行的。这些推断通常由分布式消息传递算法（例如信任传播）近似，但是它不总能在具有循环的图形上表现良好，并且不总能轻松指定复杂的连续概率分布。在具有不可计算高阶交互作用的表达性图形模型中，这些困难经常出现。在本文中，我们定义了递归因子图神经网络（RF-GNN），以实现对涉及多变量相互作用的图形模型的快速近似推断。在几个图形模型家族的实验结果证明了RF-GNN在训练数据集之外的分布下的一般化，展示了RF-GNN在表达性图形模型中快速且准确地执行推断的潜力。

    Probabilistic graphical models provide a powerful tool to describe complex statistical structure, with many real-world applications in science and engineering from controlling robotic arms to understanding neuronal computations. A major challenge for these graphical models is that inferences such as marginalization are intractable for general graphs. These inferences are often approximated by a distributed message-passing algorithm such as Belief Propagation, which does not always perform well on graphs with cycles, nor can it always be easily specified for complex continuous probability distributions. Such difficulties arise frequently in expressive graphical models that include intractable higher-order interactions. In this paper we define the Recurrent Factor Graph Neural Network (RF-GNN) to achieve fast approximate inference on graphical models that involve many-variable interactions. Experimental results on several families of graphical models demonstrate the out-of-distribution g
    

