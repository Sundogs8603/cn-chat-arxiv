# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mapping the Multiverse of Latent Representations](https://rss.arxiv.org/abs/2402.01514) | 提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。 |
| [^2] | [Multi-level protein pre-training with Vabs-Net](https://rss.arxiv.org/abs/2402.01481) | 这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。 |
| [^3] | [KTO: Model Alignment as Prospect Theoretic Optimization](https://rss.arxiv.org/abs/2402.01306) | 本文提出了一种名为KTO的方法，将模型对齐视为展望理论优化。与当前方法相比，KTO直接最大化生成效用而不是最大化偏好对数似然。在多个规模上，KTO的性能与基于偏好的方法相当甚至更好。 |
| [^4] | [Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape](https://rss.arxiv.org/abs/2402.01258) | 本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。 |
| [^5] | [Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions](https://rss.arxiv.org/abs/2402.01116) | 我们提出了一个层级架构，通过使用对偶交互预测和精简的MPC问题，实现了可扩展的实时模型预测控制，在复杂的多模态交通场景中展示了12倍的速度提升。 |
| [^6] | [EGTR: Extracting Graph from Transformer for Scene Graph Generation](https://arxiv.org/abs/2404.02072) | 提出了一种从Transformer中提取图形以用于场景图生成的轻量级单阶段模型，有效地提取了关系图。 |
| [^7] | [Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics](https://arxiv.org/abs/2404.01712) | 通过提出的Hessian-free在线遗忘方法，实现了近乎瞬时的在线遗忘，仅需要进行矢量加法操作。 |
| [^8] | [CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization](https://arxiv.org/abs/2404.00521) | 通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。 |
| [^9] | [A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations](https://arxiv.org/abs/2404.00074) | 引入一种有限算子学习技术，通过学习参数化解决了微结构弹性属性映射到机械变形的问题，在计算成本和准确性方面优于传统方法，并能处理具有明显不连续性的解。 |
| [^10] | [Functional Bilevel Optimization for Machine Learning](https://arxiv.org/abs/2403.20233) | 介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。 |
| [^11] | [Graph Neural Networks for Treatment Effect Prediction](https://arxiv.org/abs/2403.19289) | 提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性 |
| [^12] | [Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation](https://arxiv.org/abs/2403.17846) | 提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。 |
| [^13] | [Particle identification with machine learning from incomplete data in the ALICE experiment](https://arxiv.org/abs/2403.17436) | 在ALICE实验中，我们利用机器学习方法和多神经网络进行粒子识别，包括特征集嵌入和注意力机制，以在不完整数据样本上进行训练，并将ML项目与ALICE分析软件集成，讨论了域自适应技术。 |
| [^14] | [Understanding Domain-Size Generalization in Markov Logic Networks](https://arxiv.org/abs/2403.15933) | 本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。 |
| [^15] | [Planning with a Learned Policy Basis to Optimally Solve Complex Tasks](https://arxiv.org/abs/2403.15301) | 使用继承特征学习策略基础，使每个（子）策略解决一个子问题，在FSA描述的任务中，组合这些（子）策略可用于无需额外学习生成最优解决方案，方法能够渐近达到全局最优性，即使在随机环境中也如此。 |
| [^16] | [The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI](https://arxiv.org/abs/2403.13784) | 提出了模型开放框架（MOF），它是一个排名分类系统，根据完整性和开放性评估机器学习模型，旨在促进完整性、开放性以及遵循开放科学原则，可以帮助准确识别模型的透明性和可重现性。 |
| [^17] | [Machine Learning of the Prime Distribution](https://arxiv.org/abs/2403.12588) | 使用最大熵方法推导了概率数论中的几个定理，提供了关于素数学习性质的理论论证，发现Erd\H{o}s-Kac定律不太可能被当前机器学习技术发现，并进行数值实验以验证理论结果 |
| [^18] | [Consistency Models Improve Diffusion Inverse Solvers](https://arxiv.org/abs/2403.12063) | 本文提出了使用一致性模型作为高质量逼近，以改进对扩散逆求解器中后验样本的使用。 |
| [^19] | [A tutorial on learning from preferences and choices with Gaussian Processes](https://arxiv.org/abs/2403.11782) | 提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。 |
| [^20] | [Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects](https://arxiv.org/abs/2403.11332) | 提出了一种结合图神经网络和双机器学习的新方法，能够准确和高效地估计直接和同行效应，处理网络混杂因素，并一致地估计所需的因果效应 |
| [^21] | [Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics](https://arxiv.org/abs/2403.09930) | QDAC是一种基于离策略演员-评论家深度强化学习算法，通过价值函数评论家和继承特征评论家学习高性能和多样性行为。 |
| [^22] | [Learning to optimize with convergence guarantees using nonlinear system theory](https://arxiv.org/abs/2403.09389) | 基于非线性系统理论，提出了一种对于平滑非凸目标函数的所有收敛算法的无约束参数化方法 |
| [^23] | [One-Shot Averaging for Distributed TD($\lambda$) Under Markov Sampling](https://arxiv.org/abs/2403.08896) | 在分布式强化学习中，通过一次性平均化的方法，每个agent独立进行TD($\lambda$)运算，并最终在结果上进行平均，实现了相对于以往工作更少的通信量要求的线性加速。 |
| [^24] | [Fast Inference of Removal-Based Node Influence](https://arxiv.org/abs/2403.08333) | 提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。 |
| [^25] | [Representing Molecules as Random Walks Over Interpretable Grammars](https://arxiv.org/abs/2403.08147) | 提出了一种新颖的分子表示模型，使用可解释的图文法描述分子的层次化设计空间，实现了在设计空间上的随机游走，从而提高了分子生成和属性预测的性能、效率和可合成性。 |
| [^26] | [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://arxiv.org/abs/2403.06833) | 本研究提出了一种形式化的度量来量化指令与数据分离现象，以及一种可以从模型黑盒输出计算的经验变量，并引入了新数据集SEP，用于评估 |
| [^27] | [Multistep Consistency Models](https://arxiv.org/abs/2403.06807) | 本文提出了多步一致性模型，通过在一致性模型和扩散模型之间插值，实现了采样速度和采样质量的平衡。 |
| [^28] | [Robust Emotion Recognition in Context Debiasing](https://arxiv.org/abs/2403.05963) | 提出了一个反事实情绪推理（CLEF）框架来解决上下文偏差干扰的挑战 |
| [^29] | [Towards Efficient Replay in Federated Incremental Learning](https://arxiv.org/abs/2403.05890) | 本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。 |
| [^30] | [Conservative DDPG -- Pessimistic RL without Ensemble](https://arxiv.org/abs/2403.05732) | 提出了一种新的保守DDPG方法，通过引入$Q$-目标和行为克隆损失惩罚来解决DDPG中的高估偏差问题，可以在不需要集成的情况下轻松实现，并且在各种任务中表现优异。 |
| [^31] | [SPEAR:Exact Gradient Inversion of Batches in Federated Learning](https://arxiv.org/abs/2403.03945) | 该论文提出了第一个能够精确重构批量$b >1$的算法，在联邦学习中解决了梯度反演攻击的问题。 |
| [^32] | [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) | GaLore提出了一种名为Gradient Low-Rank Projection (GaLore)的训练策略，相比于一般低秩适应方法，它能够实现更高效的LLM训练，大幅降低内存使用同时保持性能。 |
| [^33] | [Slot Abstractors: Toward Scalable Abstract Visual Reasoning](https://arxiv.org/abs/2403.03458) | 提出了槽抽象化器，结合槽方法和关系归纳偏见，实现了可扩展的抽象视觉推理。 |
| [^34] | [SplAgger: Split Aggregation for Meta-Reinforcement Learning](https://arxiv.org/abs/2403.03020) | 本文展示了任务推断序列模型在元强化学习中的益处。 |
| [^35] | [DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators](https://arxiv.org/abs/2403.02688) | 首次提出了轻量级的动态片上矫正框架DOCTOR，针对光子张量加速器中的时间漂移变化问题，实现自适应、原位准确度恢复 |
| [^36] | [FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling](https://arxiv.org/abs/2403.02630) | 该研究提出了FedHCDR框架，通过超图信号解耦的方式解决了联邦跨领域推荐中不同领域数据异质性的问题。 |
| [^37] | [Improving out-of-distribution generalization in graphs via hierarchical semantic environments](https://arxiv.org/abs/2403.01773) | 通过生成分层语义环境，本文提出了一种新方法来增强图的不变学习，以处理分布转移。 |
| [^38] | [Invariant Test-Time Adaptation for Vision-Language Model Generalization](https://arxiv.org/abs/2403.00376) | 本文提出了一个测试时提示调优范式，通过优化可学习的提示，迫使模型利用真正的因果不变特征，以解决视觉-语言模型在特定任务需求上无法有效利用预训练特征的挑战。 |
| [^39] | [FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint](https://arxiv.org/abs/2402.18605) | 该论文介绍了一种 FORML 方法，使用斯蒂夫尔流形上的一阶导数近似，通过引入海森自由方法来降低计算负担和内存占用，并在元学习中实现参数正交约束。 |
| [^40] | [Boosting Graph Pooling with Persistent Homology](https://arxiv.org/abs/2402.16346) | 通过PH向池化层注入全局拓扑不变性的机制显著提升了图神经网络的性能。 |
| [^41] | [Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process](https://arxiv.org/abs/2402.16324) | 该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\epsilon^2)$样本复杂度有所提升。 |
| [^42] | [Linear Dynamics-embedded Neural Network for Long-Sequence Modeling](https://arxiv.org/abs/2402.15290) | 提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。 |
| [^43] | [Optimal Transport for Structure Learning Under Missing Data](https://arxiv.org/abs/2402.15255) | 提出了一种基于最优输运的得分算法，用于从缺失数据中学习因果结构，通过将结构学习视为密度拟合问题，并通过最小化与观测数据分布之间的沃尔仑斯坦距离来找到导致观测数据分布的因果模型 |
| [^44] | [Advancing Parameter Efficiency in Fine-tuning via Representation Editing](https://arxiv.org/abs/2402.15179) | RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果 |
| [^45] | [Sampling-based Distributed Training with Message Passing Neural Network](https://arxiv.org/abs/2402.15106) | 该论文介绍了一种基于采样和分布式训练的消息传递神经网络（MPNN），能够有效解决边缘图神经网络在节点数量增加时的扩展挑战。 |
| [^46] | [KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models](https://arxiv.org/abs/2402.15043) | 该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估 |
| [^47] | [Quantum Theory and Application of Contextual Optimal Transport](https://arxiv.org/abs/2402.14991) | 提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化，并通过预测背景情境中药物剂量参数化的细胞类型分布的变化来验证方法，展示了捕捉剂量引起的细胞分布变化的能力。 |
| [^48] | [The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative](https://arxiv.org/abs/2402.14859) | 这里是中文总结出的一句话要点: 论文探讨了在MLLM社会中通过单个操作员间接影响其他代理生成恶意内容的新型漏洞。 |
| [^49] | [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809) | CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。 |
| [^50] | [Q-Probe: A Lightweight Approach to Reward Maximization for Language Models](https://arxiv.org/abs/2402.14688) | Q-Probe是一种轻量级方法，通过学习简单的线性函数在模型的嵌入空间中重新加权候选完成，从而调整预训练语言模型以最大化任务特定的奖励函数，在各种领域中获得显著收益。 |
| [^51] | [Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion](https://arxiv.org/abs/2402.14285) | 介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步 |
| [^52] | [Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket](https://arxiv.org/abs/2402.14029) | 提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。 |
| [^53] | [Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges](https://arxiv.org/abs/2402.13957) | 本研究提出了一种结合人工智能和机器学习的音频指纹识别算法，通过在具有多样背景噪音和失真的真实环境场景模拟中的工作，以提高准确性，实现100%准确性的5秒音频输入，系统匹配速度可预测，强调了实际实施中的关键空间-速度权衡。 |
| [^54] | [Overcoming Saturation in Density Ratio Estimation by Iterated Regularization](https://arxiv.org/abs/2402.13891) | 引入迭代正则化方法解决了密度比估计中的饱和问题，实现了快速收敛，在密度比估计基准测试和大规模深度无监督领域自适应模型的重要性加权集成中表现优异。 |
| [^55] | [Replicable Learning of Large-Margin Halfspaces](https://arxiv.org/abs/2402.13857) | 该论文提出了解决学习大间距半空间问题的可复制算法，相比之前的算法，在维度无关、时间复杂度优化、样本复杂度方面等多个关键参数上均有显著改进。 |
| [^56] | [SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2402.13505) | SimPro提出了一种高度适应的框架，不依赖于任何关于未标记数据分布的预定义假设，通过创新地改进期望最大化（EM）算法，明确分离条件和边缘类别分布的建模。 |
| [^57] | [Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach](https://arxiv.org/abs/2402.12789) | 在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。 |
| [^58] | [Learning to Defer in Content Moderation: The Human-AI Interplay](https://arxiv.org/abs/2402.12237) | 本文提出了一个模型，捕捉内容审核中人工智能的相互作用。 |
| [^59] | [Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance](https://arxiv.org/abs/2402.11742) | 该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。 |
| [^60] | [Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective](https://arxiv.org/abs/2402.11463) | Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。 |
| [^61] | [Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach](https://arxiv.org/abs/2402.11338) | 该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。 |
| [^62] | [Model Editing by Pure Fine-Tuning](https://arxiv.org/abs/2402.11078) | 纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。 |
| [^63] | [Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling](https://arxiv.org/abs/2402.10634) | 通过 hierarchical spatiotemporal downsampling 处理缺失数据问题，结合可解释的注意机制，实现对时空预测的有效建模 |
| [^64] | [Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention](https://arxiv.org/abs/2402.10198) | 本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。 |
| [^65] | [MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations](https://arxiv.org/abs/2402.10093) | MIM-Refiner是一种对比学习提升方法，通过利用MIM模型中的中间层表示和多个对比头，能够将MIM模型的特征从次优的状态提升到最先进的状态，并在ImageNet-1K数据集上取得了新的最先进结果。 |
| [^66] | [How Flawed is ECE? An Analysis via Logit Smoothing](https://arxiv.org/abs/2402.10046) | 本研究通过分析对数平滑，探讨了ECE的缺陷以及对现有结果的影响，并提出了一种新的连续、易于估计的误差测度LS-ECE。通过实验发现，LS-ECE与分箱ECE非常接近。 |
| [^67] | [MiMiC: Minimally Modified Counterfactuals in the Representation Space](https://arxiv.org/abs/2402.09631) | 提出了一种新颖的对抗事实生成方法，利用闭式解决方案在表示空间中生成富有表达力的对抗事实，以减轻语言模型中的不良行为，该方法在地球移动问题方面提供理论上的保证，并对表示空间的几何组织进行改进。 |
| [^68] | [iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition](https://arxiv.org/abs/2402.09445) | 通过传感器融合和对比学习，研究证明生物阻抗传感技术可以改进基于IMU的健身追踪，提高分类模型的精度。 |
| [^69] | [SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks](https://arxiv.org/abs/2402.09025) | SLEB是一种通过消除冗余的Transformer块来优化LLM流程的新方法，它成功加速了LLM的推理过程。 |
| [^70] | [Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation](https://arxiv.org/abs/2402.08845) | 这篇论文提出了一种使用双阶段扰动测试来进行特征归因的方法，通过计算扰动一个特征对预测变化的必要性和充分性作用的概率来衡量特征重要性。该方法能够增强特征归因方法在区分不同特征贡献方面的能力。 |
| [^71] | [BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation](https://arxiv.org/abs/2402.08712) | BECoTTA是一种基于输入的高效CTTA框架，通过采用MoDE（Mixture-of-Domain Low-rank Experts）模型，它包含领域自适应路由和领域专家协同损失两个核心组件，能够在持续的测试时间中自适应不断变化的领域，同时只需较少的可训练参数。 |
| [^72] | [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org/abs/2402.08567) | Agent Smith提出了一种安全问题，即传染性越狱，该问题在多代理环境中可以通过简单的越狱一个代理来迅速感染所有代理并导致有害行为。 |
| [^73] | [Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering](https://arxiv.org/abs/2402.08277) | 这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。 |
| [^74] | [Lumos : Empowering Multimodal LLMs with Scene Text Recognition](https://arxiv.org/abs/2402.08017) | 本论文介绍了Lumos，它是第一个具备文本理解能力的多模式问答系统，通过运用场景文本识别组件，能够从第一人称视角图像中提取文本，并将其用于加强多模式大型语言模型的输入。研究过程中，作者克服了与文本识别质量、延迟和模型推断相关的多个挑战，并提供了全面的组件评估结果，展示了高质量和高效率的性能。 |
| [^75] | [Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States](https://arxiv.org/abs/2402.07875) | 本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。 |
| [^76] | [Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation](https://arxiv.org/abs/2402.07723) | 本论文通过分数阻尼库仑方程证明了重尾SDE的高概率泛化界限，并且相对于参数维度，界限的依赖性要好于p。 |
| [^77] | [Rethinking Scaling Laws for Learning in Strategic Environments](https://arxiv.org/abs/2402.07588) | 本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。 |
| [^78] | [The Implicit Bias of Gradient Noise: A Symmetry Perspective](https://arxiv.org/abs/2402.07193) | 本研究通过对对称性的存在进行分析，揭示了梯度噪声在随机梯度下降中的隐性偏见。我们发现不同类型的对称性会导致不同的学习动态，其中一类对称性可以自然收敛，而另一类对称性几乎总是发散。此外，我们的研究结果适用于没有对称性的损失函数，对于理解训练动态和解释相关实际问题具有普适性。 |
| [^79] | [Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF](https://arxiv.org/abs/2402.06886) | 本文提出了一种基于惩罚的方法来解决Bilevel强化学习和RLHF问题，这是首个有原则的算法框架。通过理论分析和实验证明了算法的有效性。 |
| [^80] | [SMC Is All You Need: Parallel Strong Scaling](https://arxiv.org/abs/2402.06173) | SMC并行扩展方法pSMC具有理论收敛速度，具有有界的时间复杂性和内存要求，适用于贝叶斯推断的问题。 |
| [^81] | [Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams](https://arxiv.org/abs/2402.06122) | 本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。 |
| [^82] | [\textit{MinMaxMin} $Q$-learning](https://arxiv.org/abs/2402.05951) | \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。 |
| [^83] | [\textit{SQT} -- \textit{std} $Q$-target](https://arxiv.org/abs/2402.05950) | SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。 |
| [^84] | [Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL](https://arxiv.org/abs/2402.05724) | 本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。 |
| [^85] | [Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport](https://arxiv.org/abs/2402.05443) | 本文介绍了一种可扩展的生成模型，称为Semi-dual JKO (S-JKO)，通过采用半对偶形式的JKO步骤，降低了训练复杂性，并在WGF上有显著的性能改进。 |
| [^86] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^87] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^88] | [A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs](https://arxiv.org/abs/2402.04493) | 这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。 |
| [^89] | [Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton](https://arxiv.org/abs/2402.04411) | 本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。 |
| [^90] | [Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models](https://arxiv.org/abs/2402.04050) | 本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。 |
| [^91] | [Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies](https://arxiv.org/abs/2402.03819) | SMOTE是一种处理不平衡数据集的常用重新平衡策略，它通过复制原始少数样本来重新生成原始分布。本研究证明了SMOTE的密度在少数样本分布的边界附近逐渐减小，从而验证了BorderLine SMOTE策略的合理性。此外，研究还提出了两种新的SMOTE相关策略，并与其他重新平衡方法进行了比较。最终发现，在数据集极度不平衡的情况下，SMOTE、提出的方法或欠采样程序是最佳的策略。 |
| [^92] | [Statistical Test for Anomaly Detections by Variational Auto-Encoders](https://arxiv.org/abs/2402.03724) | 本研究提出了一种利用变分自动编码器进行异常检测的统计测试方法（VAE-AD测试），通过量化异常区域的可靠性，可以控制误检的概率到所期望的水平。 |
| [^93] | [Partially Stochastic Infinitely Deep Bayesian Neural Networks](https://arxiv.org/abs/2402.03495) | 本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。 |
| [^94] | [Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees](https://arxiv.org/abs/2402.03448) | 本文提出了一种称为分散式间歇联邦学习（DSpodFL）的方法，它统一了分布式梯度下降（DGD）、随机闲话（RG）和分散式联邦平均（DFedAvg）等著名的分散优化方法。根据分析结果，DSpodFL能够在更一般的假设下达到几何收敛速率与最佳性差距的匹配。经过实验验证了该方法的有效性。 |
| [^95] | [Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs](https://arxiv.org/abs/2402.02957) | 本文提出了一种新颖的方法，通过联合优化无人机轨迹和用户关联指标，最大化用户与无人机的关联，以有效地最大化多个无人机在卸载地面基站的数据流量方面的利用率。 |
| [^96] | [PowerGraph: A power grid benchmark dataset for graph neural networks](https://arxiv.org/abs/2402.02827) | PowerGraph是一个用于图神经网络的电网基准数据集，旨在通过机器学习模型实现电力网格断电的在线检测。 |
| [^97] | [Graph-enhanced Large Language Models in Asynchronous Plan Reasoning](https://arxiv.org/abs/2402.02805) | 本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。 |
| [^98] | [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models](https://arxiv.org/abs/2402.02801) | KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。 |
| [^99] | [Position Paper: What Can Large Language Models Tell Us about Time Series Analysis](https://arxiv.org/abs/2402.02713) | 大语言模型有潜力颠覆时间序列分析，提升决策效率，推动时间序列分析智能的普及化。这种进展可以带来模态切换和时间序列问答等多种可能性。 |
| [^100] | [Increasing Trust in Language Models through the Reuse of Verified Circuits](https://arxiv.org/abs/2402.02619) | 本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。 |
| [^101] | [Weisfeiler Leman for Euclidean Equivariant Machine Learning](https://arxiv.org/abs/2402.02484) | 本文扩展了2-WL测试的适用范围，包括点云中的位置和速度，并提出了一种简单修改的PPGN架构，以获得一个可近似所有连续等变函数的通用等变架构。 |
| [^102] | [Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)](https://arxiv.org/abs/2402.02456) | 通过大型语言模型（LLMs），我们开发了GPTN-SS算法，用于自动设计更有效的张量网络结构搜索算法。实验证明，这些算法在探索和开发之间取得了更好的平衡，并在搜索高质量的TN结构方面展现出卓越的性能。 |
| [^103] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^104] | [Transolver: A Fast Transformer Solver for PDEs on General Geometries](https://arxiv.org/abs/2402.02366) | Transolver是一种快速的Transformer求解器，通过使用物理注意力和灵活形状的片段来学习离散化几何形状背后隐藏的内在物理状态。它能够有效地捕捉复杂几何形状下的复杂物理相关性，从而提供了具备内生几何生成能力的求解器。 |
| [^105] | [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287) | 图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。 |
| [^106] | [Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training](https://arxiv.org/abs/2402.02225) | 本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。 |
| [^107] | [Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance](https://arxiv.org/abs/2402.02149) | 本文提出了一种改进无需重训练的传播模型的方法，通过优化后验协方差，提供了一种零样本解决方案，用于嘈杂的线性逆问题。根据最近的方法等价于对给定扩散噪声图像的干净图像的不可计算后验分布进行各向同性高斯近似的发现，我们提出了一种通用即插即用的后验协方差优化方法。为了实现无需重新训练的最优后验协方差，我们提供了基于两种方法的通用解决方案，这两种方法专门设计用于利用具有和不具有反向协方差的预训练模型。 |
| [^108] | [X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System](https://arxiv.org/abs/2402.00839) | 本论文介绍了一种名为X-CBA的新颖可解释型IDS方法，该方法利用图神经网络处理网络流量数据，并采用了新的可解释人工智能方法。它通过利用更广泛的流量数据，包括边属性，以处理网络威胁的挑战。 |
| [^109] | [Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI](https://arxiv.org/abs/2402.00809) | 《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。 |
| [^110] | [Decomposable Submodular Maximization in Federated Setting](https://arxiv.org/abs/2402.00138) | 该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。 |
| [^111] | [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018) | 通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。 |
| [^112] | [Arrows of Time for Large Language Models](https://arxiv.org/abs/2401.17505) | 这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。 |
| [^113] | [Graph Language Models](https://arxiv.org/abs/2401.07105) | 引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。 |
| [^114] | [Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge](https://arxiv.org/abs/2312.12558) | 研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。 |
| [^115] | [How Far Can Fairness Constraints Help Recover From Biased Data?](https://arxiv.org/abs/2312.10396) | 公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。 |
| [^116] | [Hypergraph-MLP: Learning on Hypergraphs without Message Passing](https://arxiv.org/abs/2312.09778) | 提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。 |
| [^117] | [Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth](https://arxiv.org/abs/2311.18022) | 该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。 |
| [^118] | [InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions](https://arxiv.org/abs/2311.12943) | InteRACT通过在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上微调，解决了人机交互中的先有鸡还是先有蛋问题。 |
| [^119] | [On the Communication Complexity of Decentralized Bilevel Optimization](https://arxiv.org/abs/2311.11342) | 本研究针对去中心化双层优化的通信复杂度问题，提出了一种新颖的去中心化随机双层梯度下降算法，在异构设置下具有较小的通信成本和轮次，并实现了比现有算法更好的通信复杂度。 |
| [^120] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^121] | [On Measuring Faithfulness or Self-consistency of Natural Language Explanations](https://arxiv.org/abs/2311.07466) | 本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。 |
| [^122] | [Open-Set Graph Anomaly Detection via Normal Structure Regularisation](https://arxiv.org/abs/2311.06835) | 通过正常结构规范化方法，实现开放图异常检测模型对未知异常的广义检测能力 |
| [^123] | [Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?](https://arxiv.org/abs/2310.08540) | 本研究重新审视了预训练的Transformer是否通过梯度下降在上下文中学习的假设，并发现现有研究中的假设存在限制性假设，使其与实际语言模型训练时的语境存在显著差异。同时，通过对真实模型的观察和比较，揭示了ICL和GD在观察演示顺序上的不同敏感性。 |
| [^124] | [Feature Importance Disparities for Data Bias Investigations](https://arxiv.org/abs/2303.01704) | 本文介绍了一种新的方法，通过给定一个数据集，利用特征重要性差异（FID）来调查数据偏差，并展示了实验证据支持该方法的有效性。 |
| [^125] | [PyGOD: A Python Library for Graph Outlier Detection](https://arxiv.org/abs/2204.12095) | PyGOD是一个开源Python库，支持多种领先的基于图的异常检测方法，提供了易于使用的API和丰富的实用程序函数，同时采用了最佳的代码可靠性和可维护性实践。 |
| [^126] | [Online Resource Allocation with Non-Stationary Customers.](http://arxiv.org/abs/2401.16945) | 本文提出了一种用于在线资源分配的新算法，适用于非平稳顾客到达和未知的点击率。通过充分利用随机上下文摇臂和具有对抗性到达的在线匹配的结果，我们的方案实现了在顾客到达接近平稳时具有次线性遗憾，并在一般（非平稳）顾客到达分布下享受最优的竞争比率。我们通过大量的数值实验证明了我们的方法在各种不同的顾客场景下生成接近最优的收益。 |
| [^127] | [Provably Stable Feature Rankings with SHAP and LIME.](http://arxiv.org/abs/2401.15800) | 这项研究提出了一种通过利用多重假设检验的思想，来设计可靠地排名机器学习模型中最重要特征的特征归因方法，旨在解决SHAP和LIME等常用方法由于随机采样导致的高度不稳定性问题。实验证明了该方法的有效性和计算效率。 |
| [^128] | [Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR.](http://arxiv.org/abs/2401.14534) | 本论文研究了在多任务、异构和无模型环境下学习线性二次调节器（LQR）的问题，提出了一种基于策略梯度元学习（MAML）方法的解决方案。该方法能够产生与每个任务特定的最优控制器接近的控制器，并在模型基础设置下以线性收敛速率实现。 |
| [^129] | [Equivariant Graph Neural Operator for Modeling 3D Dynamics.](http://arxiv.org/abs/2401.11037) | 本文提出了一种新的等变图神经操作器（EGNO）方法，能够直接将动力学建模为轨迹而不仅仅是下一步预测，以准确捕捉时间相关性，并利用等变时间卷积来保持其内在的等变性。 |
| [^130] | [Provably Scalable Black-Box Variational Inference with Structured Variational Families.](http://arxiv.org/abs/2401.10989) | 本文研究了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族，并通过理论证明结构化变分族可以在迭代复杂性上表现更好，缩放效果更好。 |
| [^131] | [E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation.](http://arxiv.org/abs/2401.06127) | 本论文旨在提出一种高效的方法来从扩散模型中提炼GANs，并用于图像到图像的转换任务。这种方法可以实现灵活的实时图像编辑，并显著降低训练不同概念模型的成本。 |
| [^132] | [Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search.](http://arxiv.org/abs/2401.04514) | 本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。 |
| [^133] | [Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility.](http://arxiv.org/abs/2401.01528) | 本文研究了在具有激励兼容性的多对一匹配市场中改进赌博算法的问题，并提出了适应性探索-延迟接受（AETDA）算法来提高遗憾上限。 |
| [^134] | [Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games.](http://arxiv.org/abs/2401.01502) | 本文提出了一种Pontryagin模式的神经算子，通过在前向和反向共轭状态回滚之间的差异上定义的损失，解决了在具有参数化状态约束的博弈中值的不连续性的收敛问题，并在安全性能上优于现有的最先进算法。 |
| [^135] | [Continual Learning: Forget-free Winning Subnetworks for Video Representations.](http://arxiv.org/abs/2312.11973) | 本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。 |
| [^136] | [Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale.](http://arxiv.org/abs/2312.07586) | 该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。 |
| [^137] | [Robust and Conjugate Gaussian Process Regression.](http://arxiv.org/abs/2311.00463) | 本文提出了一种健壮和共轭的高斯过程（RCGP）回归方法，通过泛化贝叶斯推断实现了可靠的闭式更新，适用于各种实际应用场景。 |
| [^138] | [Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs.](http://arxiv.org/abs/2310.17816) | 在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。 |
| [^139] | [Clover: Closed-Loop Verifiable Code Generation.](http://arxiv.org/abs/2310.17807) | Clover是一种闭环可验证代码生成的范式，通过在代码、docstrings和形式注释之间进行一致性检查，确保生成的代码的正确性。 |
| [^140] | [Enhancing Graph Neural Networks with Structure-Based Prompt.](http://arxiv.org/abs/2310.17394) | 这篇论文提出了一种以结构为基础的图神经网络的提示方法（SAP），该方法在预训练和提示调整阶段都一致地利用结构信息，从而增强了图神经网络在学习任务特定参数方面的能力。 |
| [^141] | [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models.](http://arxiv.org/abs/2310.17086) | Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。 |
| [^142] | [On the Inherent Privacy Properties of Discrete Denoising Diffusion Models.](http://arxiv.org/abs/2310.15524) | 本研究探索了离散扩散模型在隐私保护方面的潜力，提供了关于训练数据集中每个数据点的隐私泄露的洞察，以及通过数据预处理减少合成数据集生成中隐私风险的方法。 |
| [^143] | [Adam through a Second-Order Lens.](http://arxiv.org/abs/2310.14963) | 该论文提出了AdamQLR，它是一个通过将K-FAC中的技术与Adam的更新方法相结合的优化器，通过考虑二阶数据上的Adam行为而得到启发。在回归和分类任务上进行了评估，结果显示AdamQLR在运行时间和推广性能方面表现出良好的竞争力。 |
| [^144] | [Prompt Injection Attacks and Defenses in LLM-Integrated Applications.](http://arxiv.org/abs/2310.12815) | 本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。 |
| [^145] | [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks.](http://arxiv.org/abs/2310.12516) | 本文提出了一种通过可迁移的对抗攻击在大型语言模型中自动生成评估数据的方法，并使用ChatGPT和Natural Questions（NQ）数据集进行了验证。 |
| [^146] | [Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal.](http://arxiv.org/abs/2310.08672) | 通过对学生进行因果与预测性定位的比较，研究探讨了在学生助学金续签领域实验中鼓励对象选择的价值。这项大规模实地实验揭示了定位干预对于不同学生的效果，为干预策略的优化提供了参考。 |
| [^147] | [Neural Diffusion Models.](http://arxiv.org/abs/2310.08337) | 本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。 |
| [^148] | [Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes.](http://arxiv.org/abs/2310.07216) | 通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。 |
| [^149] | [LLark: A Multimodal Foundation Model for Music.](http://arxiv.org/abs/2310.07160) | LLark是一个通过多模态架构实现音乐理解的模型，能够在零样本泛化上匹配或超出现有基准模型，在字幕生成和推理任务中与人类响应高度一致。 |
| [^150] | [Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing.](http://arxiv.org/abs/2310.06970) | 本论文提出了一个基于分布式算法设计原则的新的执行框架：洪水和回声网络。通过波状激活模式，它可以在整个图中传递消息，并且可以有效地处理更大的图形。 |
| [^151] | [Quantile-based Maximum Likelihood Training for Outlier Detection.](http://arxiv.org/abs/2310.06085) | 本文提出了一种基于分位数的极大似然目标，用于改进异常检测中异常值的分离程度。通过将正则化流拟合到预训练的判别性特征，并根据对数似然度评估来检测异常值。实验结果表明，这种方法优于最先进的无监督模型。 |
| [^152] | [Learning to Scale Logits for Temperature-Conditional GFlowNets.](http://arxiv.org/abs/2310.02823) | 这项研究提出了一种名为LSL-GFN的新型架构设计，可以大大加速温度条件下GFlowNets的训练，从而提高GFlowNets的探索和利用能力。 |
| [^153] | [Implicit regularization of multi-task learning and finetuning in overparameterized neural networks.](http://arxiv.org/abs/2310.02396) | 本文研究了在过参数化神经网络中，多任务学习和微调所带来的隐式正则化效果。在简化的线性网络环境中，我们发现了多任务学习和微调所对特征共享和学习特定特征稀疏性的鼓励作用，并发现微调过程同时具有内核和特征学习的混合状态。此外，微调还可以展现一种嵌套特征学习行为，使其偏向于提取一组稀疏的特征子集。 |
| [^154] | [Efficient Biologically Plausible Adversarial Training.](http://arxiv.org/abs/2309.17348) | 本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。 |
| [^155] | [Symmetry Leads to Structured Constraint of Learning.](http://arxiv.org/abs/2309.16932) | 本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。 |
| [^156] | [The Topology and Geometry of Neural Representations.](http://arxiv.org/abs/2309.11028) | 本文探索了从几何结构到拓扑结构的抽象步骤，并提出了一种拓扑表征相似分析方法（tRSA），通过一系列地理拓扑摘要统计量对大脑表征进行表征。 |
| [^157] | [PAGER: A Framework for Failure Analysis of Deep Regression Models.](http://arxiv.org/abs/2309.10977) | PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。 |
| [^158] | [Parameter-Efficient Long-Tailed Recognition.](http://arxiv.org/abs/2309.10019) | 本文提出了一种名为PEL的参数高效微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。该方法通过引入少量的任务特定参数，解决了常用微调方法导致尾部类别性能下降的问题。 |
| [^159] | [SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks.](http://arxiv.org/abs/2309.00255) | SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。 |
| [^160] | [Bayesian Exploration Networks.](http://arxiv.org/abs/2308.13049) | 这篇论文提出了一种贝叶斯探索网络的方法，通过在一维Bellman算子中建模不确定性，解决了贝叶斯强化学习中学习贝叶斯最优策略的计算复杂性的挑战。 |
| [^161] | [HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds.](http://arxiv.org/abs/2308.10373) | HoSNN是一种对抗性稳态脉冲神经网络，通过采用自适应发放阈值的渗漏整合与发放（TA-LIF）神经元模型来抵御对抗攻击，并在无监督的方式下保护其鲁棒性。 |
| [^162] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^163] | [Learning to Sample Tasks for Meta Learning.](http://arxiv.org/abs/2307.08924) | 通过实验得出了三个结论：没有通用的任务采样策略能保证元学习模型的性能；任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题；模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，提出了一种新颖的任务采样器ASr，它利用任务的差异、任务熵和任务难度来采样任务，并通过重新思考和提出一个简单而通用的元学习算法来优化ASr。大量实证实验表明了ASr的有效性。 |
| [^164] | [Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining.](http://arxiv.org/abs/2307.03887) | 本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。 |
| [^165] | [Programmable Synthetic Tabular Data Generation.](http://arxiv.org/abs/2307.03577) | 这项工作介绍了ProgSyn，第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义，并且通过预训练和微调生成模型来确保高质量的数据和遵守自定义规范。 |
| [^166] | [Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings.](http://arxiv.org/abs/2307.03212) | 提出了一种区域关注的多视角表示学习（ROMER）的算法，用于捕捉多视角之间的依赖关系，学习城市区域的表达能力，并在多源城市数据中优于现有方法。 |
| [^167] | [How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?.](http://arxiv.org/abs/2307.02575) | 本研究定量评估和比较了11个公开可用的土地覆盖地图，以确定它们在非洲农田分类和基于卫星地球观测的农业监测中的适用性。研究结果可帮助用户找到最适合其需求的地图，并鼓励未来工作改进地图的一致性和低精度区域的准确性。 |
| [^168] | [Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing.](http://arxiv.org/abs/2307.01995) | 本研究提出了一种基于动态特征的深度强化学习算法，针对稀疏表面压力传感器下的循环气缸流控制，实现了降低阻力和减小升力波动的目标。通过将传感器信号提取为动态特征，该算法能够自动学习并预测未来的流动状态，从而使得控制性能能够在不降低的情况下使用稀疏传感器感知流动。该方法在阻力系数和升力系数的改善上取得了显著的成果。 |
| [^169] | [Federated Generative Learning with Foundation Models.](http://arxiv.org/abs/2306.16064) | 本文提出了一种基于基础生成模型的联邦生成学习框架，通过传输提示与分布式训练数据，可以远程合成有信息量的训练数据，从而改善了通信效率、适应分布转移、提升性能、加强隐私保护。 |
| [^170] | [FLuRKA: Fast fused Low-Rank & Kernel Attention.](http://arxiv.org/abs/2306.15799) | FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。 |
| [^171] | [Causal Inference via Predictive Coding.](http://arxiv.org/abs/2306.15479) | 通过预测编码进行因果推理的技术使我们能够在已知和未知因果图的情况下进行端到端的因果推理和因果发现。 |
| [^172] | [ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data.](http://arxiv.org/abs/2306.12330) | 本文提出了ProtoGate，一种基于原型的神经模型，该模型通过考虑样本内和样本间的同质性和异质性来引入归纳偏差，并以全局到本地的方式选择特征，从而提高预测精度并使预测具有可解释性。 |
| [^173] | [Conservative Prediction via Data-Driven Confidence Minimization.](http://arxiv.org/abs/2306.04974) | 该论文提出了一种可以在处理不常见样本时推迟到人类判断的保守模型方法。该方法使用基于数据驱动置信度最小化（DCM）的算法，在辅助数据集中选择感兴趣的OOD（Out-of-Distribution）区域的样本，进而实现可靠地分离ID（In-Distribution）和OOD输入。 |
| [^174] | [Interpreting and Improving Diffusion Models Using the Euclidean Distance Function.](http://arxiv.org/abs/2306.04848) | 本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。 |
| [^175] | [How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study.](http://arxiv.org/abs/2306.03163) | 本文通过实验研究，探究了在不同大陆、云供应商和数据中心范围内，使用分布式数据并行点深度学习训练是否是更具成本效益的选择，并比较了其与集中式训练的可扩展性潜力。 |
| [^176] | [Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation.](http://arxiv.org/abs/2306.02747) | 本论文介绍了一种通过追踪非平稳性的因果起源来解决强化学习中的挑战的方法，通过引入因果源表示（COREP）算法，学到的策略对非平稳性表现出韧性。 |
| [^177] | [Reconstructing Graph Diffusion History from a Single Snapshot.](http://arxiv.org/abs/2306.00488) | 本文研究了从单个快照中重建图扩散历史的问题，揭示了现有方法的局限性，并提出了一种新的方法。 |
| [^178] | [On Riemannian Projection-free Online Learning.](http://arxiv.org/abs/2305.19349) | 本文提出了一种针对非凸约束集情况下的曲线空间在线测地凸优化的无投影算法，获得了次线性遗憾保证。 |
| [^179] | [Rotational Optimizers: Simple & Robust DNN Training.](http://arxiv.org/abs/2305.17212) | 该论文提出了旋转优化器，这些优化器可以简化深度神经网络训练过程，甚至在几乎不需调整基线超参数的情况下与原始优化器的性能相匹配。 |
| [^180] | [Learning Directed Graphical Models with Optimal Transport.](http://arxiv.org/abs/2305.15927) | 通过最优传输的视角提供了参数学习问题的新视图，可以在许多有向图上进行操作并表现出灵活性和多功能性。 |
| [^181] | [Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line.](http://arxiv.org/abs/2305.14689) | 本文研究了线性数据最小二乘岭正则化的去噪问题，证明了在欠参数化情况下会出现双峰谷现象。 |
| [^182] | [Physics of Language Models: Part 1, Context-Free Grammar.](http://arxiv.org/abs/2305.13673) | 本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。 |
| [^183] | [NUBO: A Transparent Python Package for Bayesian Optimisation.](http://arxiv.org/abs/2305.06709) | NUBO是一个透明的Python包，用于优化昂贵的黑盒函数，它利用高斯过程做代理模型以及获取函数来指导选择候选点，专注于透明度和用户体验。 |
| [^184] | [Towards Better Evaluation of GNN Expressiveness with BREC Dataset.](http://arxiv.org/abs/2304.07702) | 本论文介绍了一个新的Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)数据集，并使用BREC评估了几种现有的GNN模型的表达力，表明一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。 |
| [^185] | [MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation.](http://arxiv.org/abs/2303.09975) | MedNeXt是一个定制化的现代化可扩展卷积神经网络，用于解决数据稀缺的医学环境挑战。该网络包含：完全ConvNeXt 3D编码器-解码器网络、残差ConvNeXt上下采样块和一种新的迭代增加核大小的技术。 |
| [^186] | [Diffusion Model-Augmented Behavioral Cloning.](http://arxiv.org/abs/2302.13335) | 本研究提出了一种模仿学习框架，扩散模型增强的行为克隆（DBC），该模型同时建模专家分布的条件和联合概率，有效避免了建模复杂度和推理时间的问题。 |
| [^187] | [On the Expressivity of Persistent Homology in Graph Learning.](http://arxiv.org/abs/2302.09826) | 本文通过在图学习任务中的理论讨论和实证分析，证明了持续同调技术在捕捉具有显著拓扑结构的数据集中的长程图性质方面表现出的高表达性。 |
| [^188] | [Graph Generation with Destination-Predicting Diffusion Mixture.](http://arxiv.org/abs/2302.03596) | 本文介绍了一种名为目标预测扩散混合的方法，用于解决传统扩散模型不能很好地建模图拓扑结构的问题，并在图生成任务上取得了最先进的性能表现。 |
| [^189] | [Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes.](http://arxiv.org/abs/2211.02763) | 本文介绍了一种新的方法，使用GFlowNets和变分贝叶斯联合学习因果模型的结构和机制，不仅能够处理非线性和非高斯数据，在模拟数据上也能与几个基线方法相竞争。 |
| [^190] | [Machine Learning with Confidential Computing: A Systematization of Knowledge.](http://arxiv.org/abs/2208.10134) | 本文研究机器学习和机密计算的结合，并梳理了先前的研究成果，提供了关于保证机密性和完整性的技术，同时讨论了它们的高级特性和局限性。本文进一步确定了现有的可信执行环境（TEE）系统在机器学习用例中的限制，并讨论了未来的展望。 |
| [^191] | [How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies.](http://arxiv.org/abs/2207.04581) | 本文提出了一个新的标准来衡量公平优化策略的稳健性——稳健比率，并使用三种公平策略在五个公平数据集上进行了多次广泛的实验。结果表明，公平策略的稳健性在不同数据集之间和不同公平性定义之间存在显着差异。 |
| [^192] | [Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs.](http://arxiv.org/abs/2207.02295) | 本文在NVIDIA网卡中实现了强化学习数据中心拥塞控制，通过将RL-CC的复杂神经网络转化为决策树，实现了实时推理，并成功改善了网络拥塞下的尾部延迟和数据包丢失问题。 |
| [^193] | [What Is Fairness? Philosophical Considerations and Implications For FairML.](http://arxiv.org/abs/2205.09622) | 本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。 |
| [^194] | [Relational Self-Supervised Learning.](http://arxiv.org/abs/2203.08717) | 本文介绍了一种关系式自监督学习（ReSSL）框架，通过建模不同实例之间的关系来学习视觉表示，以弥补当前自监督学习方法中对不同实例关系的缺乏关注。 |
| [^195] | [Deep Optimal Transport for Domain Adaptation on SPD Manifolds.](http://arxiv.org/abs/2201.05745) | 这项研究介绍了一种基于深度最优传输的方法，用于解决在SPD流形上的领域自适应问题。通过利用最优传输理论和SPD流形的对数欧几里得几何，我们克服了协方差矩阵操作的复杂性挑战。 |
| [^196] | [Unpacking the Black Box: Regulating Algorithmic Decisions.](http://arxiv.org/abs/2110.03443) | 本文研究如何在代理使用复杂的“黑盒”预测函数进行决策的情况下，对算法决策进行最优调控。研究发现，限制代理使用透明度足够高的预测函数是低效的，而针对激励偏差源头的目标化工具可以提供次优解决方案，从而改善福利。 |

# 详细

[^1]: 映射潜在表示的多元宇宙

    Mapping the Multiverse of Latent Representations

    [https://rss.arxiv.org/abs/2402.01514](https://rss.arxiv.org/abs/2402.01514)

    提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。

    

    响应最近对通过多元宇宙分析来应对机器学习中的可靠性和稳健性问题的呼吁，我们提出了PRESTO，一种系统的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。尽管这些模型得到了广泛的应用，但对它们嵌入的变异性仍然不被充分理解，导致了不必要的复杂性和不可靠的表示。我们的框架使用持续同调来表征不同组合的多样化机器学习方法、(超)参数配置和数据集所产生的潜在空间，从而使我们能够测量它们之间的成对(非)相似性并对其分布进行统计推理。正如我们在理论上和实证上所证明的那样，我们的流程保持了潜在表示集合的理想特性，可以用于进行敏感性分析、检测异常嵌入或高效有效地导航超参。

    Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperpa
    
[^2]: 用Vabs-Net进行多级蛋白质预训练

    Multi-level protein pre-training with Vabs-Net

    [https://rss.arxiv.org/abs/2402.01481](https://rss.arxiv.org/abs/2402.01481)

    这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。

    

    最近几年，三维结构预训练蛋白质模型的发展迅猛，相较于预训练蛋白质语言模型，在各种下游任务中取得了重大进展。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了其他原子，如侧链原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（如分子对接）也是至关重要的。然而，我们发现在预训练中天真地组合残基和原子信息通常会失败。我们发现，信息泄漏是包含原子结构的输入导致残基级预训练任务变得琐碎并导致残基表示不够充分的一个关键原因。为了解决这个问题，我们引入了一种基于跨度掩码预训练策略的三维蛋白质预训练方法。

    In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
    
[^3]: KTO: 模型对齐视为展望理论优化

    KTO: Model Alignment as Prospect Theoretic Optimization

    [https://rss.arxiv.org/abs/2402.01306](https://rss.arxiv.org/abs/2402.01306)

    本文提出了一种名为KTO的方法，将模型对齐视为展望理论优化。与当前方法相比，KTO直接最大化生成效用而不是最大化偏好对数似然。在多个规模上，KTO的性能与基于偏好的方法相当甚至更好。

    

    凯恩曼与特沃斯基的展望理论告诉我们，人类以有偏见但明确的方式看待随机变量；例如，人们通常都是厌恶损失的。我们证明了将LLMs与人工反馈进行对齐的目标隐含地融合了许多这些偏见 - 这些目标 (例如 DPO) 的成功部分可归因于它们是"人类感知损失函数"(HALOs)。然而，这些方法所归因给人类的效用函数仍与展望理论文献中的不同。利用凯恩曼-特沃斯基人类效用的模型，我们提出了一种直接最大化生成效用而不是最大化偏好对数似然的HALO。我们将这种方法称为凯恩曼-特沃斯基优化(KTO)，并且它在从1B到30B的规模上与基于偏好的方法的性能相匹配或超过。关键是，KTO不需要偏好 - 只需要一个是否的二进制信号。

    Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
    
[^4]: Transformers在上下文中学习非线性特征：关于注意力场景中的非凸均场动态研究

    Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape

    [https://rss.arxiv.org/abs/2402.01258](https://rss.arxiv.org/abs/2402.01258)

    本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。

    

    基于Transformer架构的大型语言模型展示了在上下文中学习的令人印象深刻的能力。然而，关于这一现象产生的现有理论研究仅限于对线性回归任务上训练的单层注意力的动态。在本文中，我们研究了一个由全连接层和线性注意力层组成的Transformer的优化。MLP充当了一个常见的非线性表示或特征映射，极大地增强了上下文学习的能力。我们在均场和两个时间尺度的极限情况下证明了参数分布的无限维损失景观，虽然高度非凸，但变得相当温和。我们还分析了均场动态的二阶稳定性，并表明Wasserstein梯度流几乎总是避开鞍点。此外，我们建立了获得远离临界点和接近临界点的具体改进速率的新方法。

    Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the 
    
[^5]: 可扩展多模型MPC的基于对偶交互预测的层级架构

    Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions

    [https://rss.arxiv.org/abs/2402.01116](https://rss.arxiv.org/abs/2402.01116)

    我们提出了一个层级架构，通过使用对偶交互预测和精简的MPC问题，实现了可扩展的实时模型预测控制，在复杂的多模态交通场景中展示了12倍的速度提升。

    

    我们提出了一个层级架构，用于在复杂的多模态交通场景中实现可扩展的实时模型预测控制(MPC)。该架构由两个关键组件组成：1) RAID-Net，一种基于注意力机制的新颖循环神经网络，使用拉格朗日对偶性预测自动驾驶车辆与周围车辆之间在MPC预测范围内的相关交互；2) 一个简化的随机MPC问题，消除不相关的避碰约束，提高计算效率。我们的方法在一个模拟交通路口中演示，展示了解决运动规划问题的12倍速提升。您可以在这里找到展示该架构在多个复杂交通场景中的视频：https://youtu.be/-TcMeolCLWc

    We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
    
[^6]: 从Transformer中提取图形用于场景图生成

    EGTR: Extracting Graph from Transformer for Scene Graph Generation

    [https://arxiv.org/abs/2404.02072](https://arxiv.org/abs/2404.02072)

    提出了一种从Transformer中提取图形以用于场景图生成的轻量级单阶段模型，有效地提取了关系图。

    

    场景图生成（SGG）是一项具有挑战性的任务，涉及检测对象并预测对象之间的关系。提出了一个轻量级的单阶段SGG模型，它从DETR解码器的多头自注意力层中学习的各种关系中提取关系图。

    arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
    
[^7]: 通过免Hessian重新整合个体数据统计实现高效在线遗忘

    Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics

    [https://arxiv.org/abs/2404.01712](https://arxiv.org/abs/2404.01712)

    通过提出的Hessian-free在线遗忘方法，实现了近乎瞬时的在线遗忘，仅需要进行矢量加法操作。

    

    机器遗忘旨在通过使模型能够选择性地忘记特定数据来维护数据所有者的被遗忘权利。最近的方法表明，一种数据遗忘的方法是通过预先计算和存储携带二阶信息的统计数据，以改进计算和内存效率。然而，它们依赖于苛刻的假设，而且计算/存储受到模型参数维度的诅咒，这使得难以应用到大多数深度神经网络中。在本工作中，我们提出了一种免Hessian在线遗忘方法。我们建议为每个数据点维护一个统计向量，通过重新训练和学习模型之间的差异的仿射随机递归逼近来计算。我们提出的算法实现了近乎瞬时的在线遗忘，因为它只需要进行矢量加法操作。基于重新收集遗忘数据统计的策略，

    arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
    
[^8]: CHAIN：通过受限唯一性连续性规范化增强数据高效GANs的泛化能力

    CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization

    [https://arxiv.org/abs/2404.00521](https://arxiv.org/abs/2404.00521)

    通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。

    

    生成对抗网络（GANs）显着推动了图像生成，但它们的性能严重依赖大量的训练数据。在数据有限的情况下，GANs经常面临鉴别器过拟合和训练不稳定的问题。我们的工作通过识别Batch Normalization（BN）中的关键缺陷来解决这一问题：在中心化和缩放步骤中梯度爆炸的倾向。为了解决这个问题，我们提出了CHAIN（受限唯一性连续性规范化），它将传统的中心化步骤替换为零均值正则化，并在缩放步骤中集成了Lipschitz连续性约束。CHAIN通过自适应插值归一化和非归一化特征进一步增强了GANs的训练，有效避免了鉴别器过拟合。

    arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
    
[^9]: 一种用于将微结构的弹性属性映射到其机械变形的有限算子学习技术

    A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations

    [https://arxiv.org/abs/2404.00074](https://arxiv.org/abs/2404.00074)

    引入一种有限算子学习技术，通过学习参数化解决了微结构弹性属性映射到机械变形的问题，在计算成本和准确性方面优于传统方法，并能处理具有明显不连续性的解。

    

    为了开发固体力学中控制物理方程的更快求解器，我们引入了一种参数化学习机械平衡解的方法。该方法在计算成本方面优于传统方法，同时保持准确性。此外，它将标准的物理信息神经网络进行了泛化和增强，以学习一个带有相当明显不连续性的参数解。我们以微观力学为例，其中对于给定的异质微结构来说，微观力学解（即变形和应力场）的知识至关重要。我们针对的参数是异质固体系统内的杨氏模量分布。受算子学习和有限元方法的启发，我们的方法展示了无需依赖其他数值求解器数据就能进行训练的能力。相反，我们利用了有限元方法的思想。

    arXiv:2404.00074v1 Announce Type: new  Abstract: To develop faster solvers for governing physical equations in solid mechanics, we introduce a method that parametrically learns the solution to mechanical equilibrium. The introduced method outperforms traditional ones in terms of computational cost while acceptably maintaining accuracy. Moreover, it generalizes and enhances the standard physics-informed neural networks to learn a parametric solution with rather sharp discontinuities. We focus on micromechanics as an example, where the knowledge of the micro-mechanical solution, i.e., deformation and stress fields for a given heterogeneous microstructure, is crucial. The parameter under investigation is the Young modulus distribution within the heterogeneous solid system. Our method, inspired by operator learning and the finite element method, demonstrates the ability to train without relying on data from other numerical solvers. Instead, we leverage ideas from the finite element approac
    
[^10]: 机器学习中的函数双层优化

    Functional Bilevel Optimization for Machine Learning

    [https://arxiv.org/abs/2403.20233](https://arxiv.org/abs/2403.20233)

    介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。

    

    在本文中，我们介绍了针对机器学习中的双层优化问题的一种新的函数视角，其中内部目标在函数空间上被最小化。这些类型的问题通常通过在参数设置下开发的方法来解决，其中内部目标对于预测函数的参数强凸。函数视角不依赖于此假设，特别允许使用超参数化的神经网络作为内部预测函数。我们提出了可扩展和高效的算法来解决函数双层优化问题，并展示了我们方法在适合自然函数双层结构的仪表回归和强化学习任务上的优势。

    arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
    
[^11]: 用于治疗效果预测的图神经网络

    Graph Neural Networks for Treatment Effect Prediction

    [https://arxiv.org/abs/2403.19289](https://arxiv.org/abs/2403.19289)

    提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性

    

    在电子商务中估计因果效应往往涉及昂贵的治疗分配，这在大规模设置中可能是不切实际的。利用机器学习来预测这种治疗效果而无需实际干预是减少风险的一种标准做法。然而，现有的治疗效果预测方法往往依赖于大规模实验构建的训练集，因此从根本上存在风险。在这项工作中，我们提出了一种图神经网络，以减少所需的训练集大小，依赖于电子商务数据中常见的图。具体地，我们将问题视为具有有限数量标记实例的节点回归，开发了一个类似于先前因果效应估计器的双模型神经架构，并测试了不同的消息传递层进行编码。此外，作为额外步骤，我们将模型与获取函数相结合，以引导信息传递。

    arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
    
[^12]: 基于语言驱动的机器人导航的分层开放词汇3D场景图

    Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation

    [https://arxiv.org/abs/2403.17846](https://arxiv.org/abs/2403.17846)

    提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。

    

    最近的开放词汇机器人映射方法利用预先训练的视觉-语言特征丰富了密集几何地图。虽然这些地图允许在查询某种语言概念时预测逐点显著性地图，但大规模环境和超出对象级别的抽象查询仍然是一个相当大的障碍，最终限制了基于语言的机器人导航。在这项工作中，我们提出了HOV-SG，一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法。通过利用开放词汇视觉基础模型，我们首先在3D空间中获得了最先进的开放词汇分段级地图，然后构建了由地板、房间和对象概念组成的3D场景图层次结构，每个都包含开放性词汇特征。我们的方法能够表示多层建筑，并且允许机器人使用跨层Voronoi图穿越这些建筑。HOV-SG进行了评估。

    arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
    
[^13]: 在ALICE实验中利用机器学习从不完整数据中进行粒子识别

    Particle identification with machine learning from incomplete data in the ALICE experiment

    [https://arxiv.org/abs/2403.17436](https://arxiv.org/abs/2403.17436)

    在ALICE实验中，我们利用机器学习方法和多神经网络进行粒子识别，包括特征集嵌入和注意力机制，以在不完整数据样本上进行训练，并将ML项目与ALICE分析软件集成，讨论了域自适应技术。

    

    LHC的ALICE实验测量在超相对论重离子对撞中形成的强相互作用物质的性质。这些研究需要准确的粒子识别(PID)。ALICE通过几个探测器为动量从约100 MeV/c到20 GeV/c的粒子提供PID信息。传统上，粒子是通过矩形切割进行选择的。利用机器学习(ML)方法可以实现更好的性能。我们的解决方案使用多个神经网络(NN)作为二进制分类器。此外，我们通过特征集嵌入和关注扩展了粒子分类器，以便对包含不完整样本的数据进行训练。我们还介绍了ML项目与ALICE分析软件的集成，并讨论了域自适应，这是将知识从模拟数据转移到实际实验数据所需的ML技术。

    arXiv:2403.17436v1 Announce Type: cross  Abstract: The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.
    
[^14]: 理解马尔科夫逻辑网络中的域大小泛化

    Understanding Domain-Size Generalization in Markov Logic Networks

    [https://arxiv.org/abs/2403.15933](https://arxiv.org/abs/2403.15933)

    本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。

    

    我们研究了马尔科夫逻辑网络（MLNs）在不同大小的关系结构之间的泛化行为。多个研究注意到，在给定域上学习的MLNs在不同大小的域上泛化很差。这种行为源于MLN在不同域大小上使用时的内部一致性缺失。在本文中，我们量化了这种不一致性，并将其限制在MLN参数的方差范围内。参数方差还限制了从不同域大小中取出的MLN边缘分布之间的KL散度。我们利用这些界限展示，最大化数据对数似然同时最小化参数方差，对应于域大小泛化的两个自然概念。我们的理论结果适用于指数随机图和其他基于马尔科夫网络的关系模型。最后，我们观察到已知的解决方案会减少方差

    arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
    
[^15]: 使用学习的策略基础进行规划以最优地解决复杂任务

    Planning with a Learned Policy Basis to Optimally Solve Complex Tasks

    [https://arxiv.org/abs/2403.15301](https://arxiv.org/abs/2403.15301)

    使用继承特征学习策略基础，使每个（子）策略解决一个子问题，在FSA描述的任务中，组合这些（子）策略可用于无需额外学习生成最优解决方案，方法能够渐近达到全局最优性，即使在随机环境中也如此。

    

    传统的强化学习方法可以成功解决各种顺序决策问题。然而，在具有非马尔可夫奖励规范的情景中学习能够可靠泛化于多个任务的策略是一个具有挑战性的问题。我们提出使用继承特征来学习一个策略基础，使得其中的每一个（子）策略解决一个明确定义的子问题。在由有限状态自动机（FSA）描述的任务中涉及相同一组子问题时，这些（子）策略的组合可以被用来生成一个最优解决方案而无需额外的学习。与其他通过规划组合（子）策略的方法相比，我们的方法在渐近上达到全局最优性，即使在随机环境中也是如此。

    arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.
    
[^16]: 模型开放框架: 促进人工智能中的可重现性、透明度和可用性的完整性和开放性

    The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI

    [https://arxiv.org/abs/2403.13784](https://arxiv.org/abs/2403.13784)

    提出了模型开放框架（MOF），它是一个排名分类系统，根据完整性和开放性评估机器学习模型，旨在促进完整性、开放性以及遵循开放科学原则，可以帮助准确识别模型的透明性和可重现性。

    

    生成式人工智能（GAI）提供了前所未有的可能性，但其商业化引发了关于透明度、可重现性、偏见和安全性的担忧。许多"开源"的GAI模型缺乏完整理解和再现所必需的组件，一些采用限制性许可证，这种行为被称为"开源洗白"。我们提出了模型开放框架（MOF），这是一个根据完整性和开放性对机器学习模型进行排名分类的系统，遵循开放科学、开源、开放数据和开放获取的原则。MOF要求模型开发生命周期的特定组件被包含并根据适当的开放许可证发布。该框架旨在防止宣称自己是开放的模型被误解，指导研究人员和开发者以宽松的许可证发布所有模型组件，并帮助公司、学术界和爱好者识别可以安全采用的模型。

    arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
    
[^17]: 主要分布的机器学习

    Machine Learning of the Prime Distribution

    [https://arxiv.org/abs/2403.12588](https://arxiv.org/abs/2403.12588)

    使用最大熵方法推导了概率数论中的几个定理，提供了关于素数学习性质的理论论证，发现Erd\H{o}s-Kac定律不太可能被当前机器学习技术发现，并进行数值实验以验证理论结果

    

    在本研究中，我们使用最大熵方法推导了概率数论中的几个定理，包括哈代-拉马努金定理的一个版本。我们还提供了一个理论论证，解释了Y.-H. He关于素数可学性的实验观察，并假设Erd\H{o}s-Kac定律极不可能被当前的机器学习技术发现。我们进行的数值实验证实了我们的理论发现。

    arXiv:2403.12588v1 Announce Type: cross  Abstract: In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.
    
[^18]: 一致性模型改进扩散逆求解器

    Consistency Models Improve Diffusion Inverse Solvers

    [https://arxiv.org/abs/2403.12063](https://arxiv.org/abs/2403.12063)

    本文提出了使用一致性模型作为高质量逼近，以改进对扩散逆求解器中后验样本的使用。

    

    扩散逆求解器（DIS）的目标是找到一个在扩散先验空间中的图像$x$，满足约束$f(x)=y$，给定算子$f(\cdot)$和测量$y$。大多数非线性DIS使用后验均值$\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$来评估$f(\cdot)$并最小化距离$||f(\hat{x}_{0|t})-y||^2$。先前的工作表明，基于后验均值的距离是有偏的；而后验样本$x_{0|t}\sim p_{\theta}(x_0|x_t)$承诺是更好的候选。本文首先澄清了在何种情况下后验样本更好：$1)$当$f(\cdot)$是线性的时，使用后验均值的距离就像单个后验样本一样好，因此更可取，因为它不需要蒙特卡洛；$2)$当$f(\cdot)$是非线性的时，使用后验样本的距离更好。由于先前对后验样本的逼近不像真实图像，我们提出使用一致性模型（CM）作为高质量逼近。

    arXiv:2403.12063v1 Announce Type: cross  Abstract: Diffusion inverse solvers (DIS) aim to find an image $x$ that lives on the diffusion prior while satisfying the constraint $f(x) = y$, given an operator $f(.)$ and measurement $y$. Most non-linear DIS use posterior mean $\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$ to evaluate $f(.)$ and minimize the distance $||f(\hat{x}_{0|t})-y||^2$. Previous works show that posterior mean-based distance is biased; instead, posterior sample $x_{0|t}\sim p_{\theta}(x_0|x_t)$ promises a better candidate. In this paper, we first clarify when is posterior sample better: $1)$ When $f(.)$ is linear, the distance with posterior mean is as good as single posterior sample, thus preferable as it does not require Monte Carlo; $2)$ When $f(.)$ is non-linear, the distance using posterior sample is better. As previous approximations to posterior sample do not look like a real image, we propose to use consistency model (CM) as a high quality approximation. In addition, we p
    
[^19]: 使用高斯过程从偏好和选择中学习的教程

    A tutorial on learning from preferences and choices with Gaussian Processes

    [https://arxiv.org/abs/2403.11782](https://arxiv.org/abs/2403.11782)

    提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。

    

    偏好建模位于经济学、决策理论、机器学习和统计学的交叉点。通过理解个体的偏好及其选择方式，我们可以构建更接近他们期望的产品，为跨领域的更高效、个性化应用铺平道路。此教程的目标是提供一个连贯、全面的偏好学习框架，使用高斯过程演示如何将理性原则（来自经济学和决策理论）无缝地纳入学习过程中。通过合适地定制似然函数，这一框架使得能够构建涵盖随机效用模型、辨识限制和对象和标签偏好的多重冲突效用情景的偏好学习模型。

    arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
    
[^20]: 基于图神经网络的网络因果效应双机器学习估计器

    Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects

    [https://arxiv.org/abs/2403.11332](https://arxiv.org/abs/2403.11332)

    提出了一种结合图神经网络和双机器学习的新方法，能够准确和高效地估计直接和同行效应，处理网络混杂因素，并一致地估计所需的因果效应

    

    我们的论文解决了在社交网络数据中推断因果效应的挑战，这些数据具有个体之间复杂的相互依赖关系，导致单位之间不独立、干扰（单位的结果受邻居的处理影响）以及引入来自邻近单位的额外混杂因素等问题。我们提出了一种将图神经网络和双机器学习相结合的创新方法，能够使用单个观测社交网络准确高效地估计直接和同伴效应。我们的方法利用图同构网络与双机器学习相结合，有效调整网络混杂因素并一致地估计所需的因果效应。我们展示了我们的估计器既具有渐近正态性又半参数高效。我们对三个半合成状态下的四种最先进基线方法进行了全面评估

    arXiv:2403.11332v1 Announce Type: new  Abstract: Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient. A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synth
    
[^21]: 质量多样性演员-评论家：通过值和继承特征评论家学习高性能和多样性行为

    Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics

    [https://arxiv.org/abs/2403.09930](https://arxiv.org/abs/2403.09930)

    QDAC是一种基于离策略演员-评论家深度强化学习算法，通过价值函数评论家和继承特征评论家学习高性能和多样性行为。

    

    智能的一个关键方面是表现出适应意外情况的广泛行为谱。过去十年，深度强化学习的进步取得了突破性成就，用于解决复杂的连续控制任务。然而，大多数方法只返回一个专门针对特定问题的解决方案。我们引入了质量多样性演员-评论家（QDAC），这是一种基于离策略演员-评论家深度强化学习算法，利用价值函数评论家和继承特征评论家学习高性能和多样性行为。在这个框架中，演员通过受限优化来最大化回报并执行多样性技能的客观函数，无缝统一了两个评论家。与其他质量多样性方法相比，QDAC在六个具有挑战性的连续控制运动任务上实现了显着更高的性能和更多样性的行为。

    arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
    
[^22]: 使用非线性系统理论学习具有收敛保证的优化

    Learning to optimize with convergence guarantees using nonlinear system theory

    [https://arxiv.org/abs/2403.09389](https://arxiv.org/abs/2403.09389)

    基于非线性系统理论，提出了一种对于平滑非凸目标函数的所有收敛算法的无约束参数化方法

    

    越来越多地依赖于数字方法来控制动态系统和训练机器学习模型，突显了需要设计可靠且高效地遍历复杂优化空间的算法。传统的梯度下降方法对凸问题提供了强大的理论保证；然而，对于非凸问题则需要精细调整超参数。学习优化(L2O)的新兴范式自动发现具有优化性能的算法，利用学习模型和数据，但缺乏分析所学算法的收敛性和稳健性的理论框架。本文通过利用非线性系统理论填补了这一空白。具体来说，我们提出了对于平滑非凸目标函数的所有收敛算法的无约束参数化。值得注意的是，我们的框架与自动微分工具直接兼容，确保...

    arXiv:2403.09389v1 Announce Type: cross  Abstract: The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensurin
    
[^23]: 一次性平均化在马尔可夫采样下的分布式TD($\lambda$)

    One-Shot Averaging for Distributed TD($\lambda$) Under Markov Sampling

    [https://arxiv.org/abs/2403.08896](https://arxiv.org/abs/2403.08896)

    在分布式强化学习中，通过一次性平均化的方法，每个agent独立进行TD($\lambda$)运算，并最终在结果上进行平均，实现了相对于以往工作更少的通信量要求的线性加速。

    

    我们考虑一种分布式强化学习设置，每个agent都拥有相同的马尔可夫决策过程副本，但是转换是独立地从相应的马尔可夫链中由每个agent采样的。我们展示在这种设置下，我们可以实现对于TD($\lambda$)的线性加速，这是一系列流行的用于策略评估的方法，即若目标精度足够小，$N$个agents可以以$N$倍速度评估一个策略。值得注意的是，这种加速是通过“一次性平均化”实现的，即agent们独立地使用马尔可夫采样运行TD($\lambda$)，并且仅在最后一步之后对他们的结果进行平均。相对于以前的工作，这显著减少了实现线性加速所需的通信量。

    arXiv:2403.08896v1 Announce Type: new  Abstract: We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.
    
[^24]: 快速推断基于移除的节点影响

    Fast Inference of Removal-Based Node Influence

    [https://arxiv.org/abs/2403.08333](https://arxiv.org/abs/2403.08333)

    提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。

    

    图神经网络（GNNs）被广泛用于捕获图中信息传播模式。虽然取得了显著的性能，但评估节点影响的新趋势日益受到关注。我们提出了一种评估节点影响的新方法，通过衡量训练好的GNN模型在移除节点后的预测变化。一个真实应用是，“在预测Twitter账户极性的任务中，如果移除特定账户，其他账户的极性会如何改变？”我们将GNN作为一个代理模型，其预测可以模拟移除节点引起的节点或边的变化。为了获得每个节点的影响，一种直接的方法是交替移除每个节点，并在修改后的图上应用训练好的GNN。这是可靠的但耗时，因此我们需要一种高效的方法。

    arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
    
[^25]: 将分子表示为可解释的文法上的随机游走

    Representing Molecules as Random Walks Over Interpretable Grammars

    [https://arxiv.org/abs/2403.08147](https://arxiv.org/abs/2403.08147)

    提出了一种新颖的分子表示模型，使用可解释的图文法描述分子的层次化设计空间，实现了在设计空间上的随机游走，从而提高了分子生成和属性预测的性能、效率和可合成性。

    

    最近分子探索领域的研究主要集中在小型、类似药物的分子上，导致许多在材料设计中同样重要的应用缺乏足够的技术支持。这些应用通常依赖于更复杂的分子结构，有更少的例子，是使用已知的亚结构精心设计的。我们提出了一种数据高效且可解释的模型，用于以图文法的形式表示和推理这些分子，明确描述了特征为设计基础的层次化设计空间。我们提出了一种新颖的表示形式，即在设计空间上进行随机游走，既有助于分子生成，又有助于属性预测。我们展示了相较于现有方法在性能、效率和预测分子可合成性方面的明显优势，并详细阐述了该方法的化学可解释性。

    arXiv:2403.08147v1 Announce Type: new  Abstract: Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.
    
[^26]: LLMs能够将指令与数据分离吗？我们具体指的是什么？

    Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?

    [https://arxiv.org/abs/2403.06833](https://arxiv.org/abs/2403.06833)

    本研究提出了一种形式化的度量来量化指令与数据分离现象，以及一种可以从模型黑盒输出计算的经验变量，并引入了新数据集SEP，用于评估

    

    arXiv:2403.06833v1 公告类型: 跨 针对大型语言模型（LLMs）进行调节指令的技术取得了突破性的成果，为许多实际应用打开了无数新可能。然而，LLMs缺乏其他计算机科学领域已建立为规范的基本安全特性，比如指令与数据之间的分离，导致它们发生故障或易受第三方操控和干扰（例如通过间接提示/命令注入）。更糟糕的是，迄今为止，甚至没有确切定义这种分离究竟意味着什么以及如何测试其违反情况。本研究旨在填补这一空白。我们引入了一个正式的指标来量化指令与数据分离现象，以及一个可以从模型的黑盒输出计算的经验变量。我们还介绍了一个新的数据集SEP（应该执行还是处理？），该数据集允许评估

    arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
    
[^27]: 多步一致性模型

    Multistep Consistency Models

    [https://arxiv.org/abs/2403.06807](https://arxiv.org/abs/2403.06807)

    本文提出了多步一致性模型，通过在一致性模型和扩散模型之间插值，实现了采样速度和采样质量的平衡。

    

    扩散模型相对容易训练，但生成样本需要许多步骤。一致性模型更难训练，但可以在一个步骤中生成样本。本文提出了多步一致性模型：通过一致性模型和TRACT的统一，可以在一致性模型和扩散模型之间进行插值：在采样速度和采样质量之间取得平衡。具体来说，1步一致性模型是传统的一致性模型，而我们展示了$\infty$步一致性模型是扩散模型。多步一致性模型在实践中表现良好。将样本预算从单步增加到2-8步，我们可以更轻松地训练模型，生成更高质量的样本，同时保留大部分采样速度优势。在Imagenet 64上8步达到1.4的FID，在Imagenet128上8步达到2.1的FID。

    arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
    
[^28]: 在上下文去偏的情绪识别中的鲁棒性

    Robust Emotion Recognition in Context Debiasing

    [https://arxiv.org/abs/2403.05963](https://arxiv.org/abs/2403.05963)

    提出了一个反事实情绪推理（CLEF）框架来解决上下文偏差干扰的挑战

    

    上下文感知情绪识别（CAER）最近在无约束环境中推动了情感计算技术的实际应用。 主流的CAER方法总是从不同的上下文和以主体为中心的特征中提取集成表示，以感知目标人物的情绪状态。 尽管有所进展，但最大的挑战仍然是由于上下文偏差的干扰。 有害的偏见迫使模型依赖于背景上下文和情感标签之间的虚假相关性，在可能性估计中造成严重的性能瓶颈，并使有价值的上下文先验混淆。 在本文中，我们提出了一个反事实情绪推理（CLEF）框架来解决上述问题。 具体而言，我们首先制定了一个广义因果图，以解耦CAER中变量之间的因果关系。 遵循因果图，CLEF引入了一个非侵入式的上下文分支来获取

    arXiv:2403.05963v1 Announce Type: cross  Abstract: Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capt
    
[^29]: 朝着联邦增量学习中高效的重播

    Towards Efficient Replay in Federated Incremental Learning

    [https://arxiv.org/abs/2403.05890](https://arxiv.org/abs/2403.05890)

    本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。

    

    在联邦学习（FL）中，通常假定每个客户端的数据是固定或静态的。然而，在现实世界的应用中，数据通常以增量方式到来，其中数据领域可能动态增加。在这项工作中，我们研究了在边缘客户端在联邦增量学习（FIL）场景中因数据异构性而可能缺乏足够存储空间以保留完整数据的灾难性遗忘。我们提出了一种名为Re-Fed的简单、通用的FIL框架，它可以协调每个客户端缓存重播的重要样本。具体而言，当出现新任务时，每个客户端首先基于它们的全局和本地重要性缓存选定的先前样本。然后，客户端使用既缓存的样本又使用新任务的样本训练本地模型。在理论上，我们分析了Re-Fed发现重播重要样本的能力，从而缓解了灾难性遗忘问题。

    arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e
    
[^30]: 保守DDPG - 无集成的悲观强化学习

    Conservative DDPG -- Pessimistic RL without Ensemble

    [https://arxiv.org/abs/2403.05732](https://arxiv.org/abs/2403.05732)

    提出了一种新的保守DDPG方法，通过引入$Q$-目标和行为克隆损失惩罚来解决DDPG中的高估偏差问题，可以在不需要集成的情况下轻松实现，并且在各种任务中表现优异。

    

    DDPG受到高估偏差问题的阻碍，其中其$Q$-估计倾向于夸大实际$Q$值。传统解决这一偏见的方法涉及基于集成的方法，需要大量计算资源，或者基于复杂对数策略的方法，难以理解和实施。相比之下，我们提出了一种简单的解决方案，使用$Q$-目标并结合行为克隆（BC）损失惩罚。这种解决方案作为一种不确定性度量，可以很容易地用较少的代码实现，而无需集成。我们的实证结果强烈支持保守DDPG在各种MuJoCo和Bullet任务上优于DDPG。我们始终观察到在所有评估任务中表现更好，甚至在与TD3和TD7相比性能更有竞争力或更优越，所有这些都是以显著降低的计算要求实现的。

    arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.
    
[^31]: SPEAR：联邦学习中批量精确梯度反演

    SPEAR:Exact Gradient Inversion of Batches in Federated Learning

    [https://arxiv.org/abs/2403.03945](https://arxiv.org/abs/2403.03945)

    该论文提出了第一个能够精确重构批量$b >1$的算法，在联邦学习中解决了梯度反演攻击的问题。

    

    联邦学习是一种流行的协作机器学习框架，在这个框架中，多个客户端仅与服务器共享他们本地数据的梯度更新，而不是实际数据。不幸的是，最近发现梯度反演攻击可以从这些共享的梯度中重构出数据。现有的攻击只能在重要的诚实但好奇设置中对批量大小为$b=1$的数据进行精确重构，对于更大的批量只能进行近似重构。在这项工作中，我们提出了\emph{第一个准确重建批量$b >1$的算法}。这种方法结合了对梯度显式低秩结构的数学见解和基于采样的算法。关键的是，我们利用ReLU诱导的梯度稀疏性，精确地过滤掉大量错误的样本，使最终的重建步骤可行。我们为全连接提供了高效的GPU实现

    arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
    
[^32]: GaLore: 通过梯度低秩投影实现高效的LLM训练

    GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection

    [https://arxiv.org/abs/2403.03507](https://arxiv.org/abs/2403.03507)

    GaLore提出了一种名为Gradient Low-Rank Projection (GaLore)的训练策略，相比于一般低秩适应方法，它能够实现更高效的LLM训练，大幅降低内存使用同时保持性能。

    

    训练大型语言模型(LLMs)存在显着的内存挑战，主要是由于权重和优化器状态的不断增加。通常的内存减少方法，如低秩适应（LoRA），在每个层中向冻结的预训练权重添加可训练的低秩矩阵，从而减少了可训练的参数和优化器状态。然而，这样的方法通常在预训练和微调阶段的表现都不如完整秩权重的训练，因为它们将参数搜索限制在低秩子空间并改变了训练动态，而且可能需要完整秩的热启动。在这项工作中，我们提出了Gradient Low-Rank Projection (GaLore)，这是一种训练策略，允许完全参数学习，但比LoRA等常见低秩适应方法更节省内存。我们的方法在优化器状态上将内存使用降低了高达65.5%，同时保持了预训练和精调的效率和性能。

    arXiv:2403.03507v1 Announce Type: new  Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-tra
    
[^33]: 槽抽象化器：迈向可扩展的抽象视觉推理

    Slot Abstractors: Toward Scalable Abstract Visual Reasoning

    [https://arxiv.org/abs/2403.03458](https://arxiv.org/abs/2403.03458)

    提出了槽抽象化器，结合槽方法和关系归纳偏见，实现了可扩展的抽象视觉推理。

    

    抽象视觉推理是一种人类特有的能力，允许识别从对象特征中抽象出的关系模式，并将这些模式系统化地推广到未曾见过的问题。最近的研究表明，在涉及多个对象输入的视觉推理任务中，通过使用基于槽的方法提取以对象为中心的表示并具有强归纳偏见的关系抽象，实现了强大的系统化推广。然而，这种方法仅限于包含单一规则的问题，并且不适用于包含大量对象的视觉推理问题。其他最近的工作提出了抽象化器，这是Transformer的扩展，融合了强大的关系归纳偏见，从而继承了Transformer的可扩展性和多头架构，但尚未展示如何将此方法应用到

    arXiv:2403.03458v1 Announce Type: cross  Abstract: Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to
    
[^34]: SplAgger：用于元强化学习的分割聚合

    SplAgger: Split Aggregation for Meta-Reinforcement Learning

    [https://arxiv.org/abs/2403.03020](https://arxiv.org/abs/2403.03020)

    本文展示了任务推断序列模型在元强化学习中的益处。

    

    强化学习的一个核心目标是创建能快速学习新任务的智能体。元强化学习旨在通过直接学习这些智能体来实现这一目标。一类元强化学习方法被称为黑盒方法，通过端到端训练现成的序列模型来实现这一目标。与之形成对比的是另一类方法，它们明确地推断出未知任务的后验分布。这些方法通常具有不同的目标和序列模型，旨在实现任务推断，因此被称为任务推断方法。本文提出了强有力的证据，证明任务推断序列模型仍然具有益处。

    arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
    
[^35]: DOCTOR: 针对时间漂移热变化的动态芯片矫正方法，用于自我校正的光子张量加速器

    DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators

    [https://arxiv.org/abs/2403.02688](https://arxiv.org/abs/2403.02688)

    首次提出了轻量级的动态片上矫正框架DOCTOR，针对光子张量加速器中的时间漂移变化问题，实现自适应、原位准确度恢复

    

    Photonic computing作为加速计算密集型人工智能(AI)工作负载的一种有前途的解决方案已经出现，特别是在资源有限、延迟敏感的边缘计算环境中提供了无与伦比的速度和能量效率。然而，模拟光子张量加速器的部署遇到了可靠性挑战，由于硬件噪声和环境变化。虽然已经提出了脱机噪声感知训练和片上训练来增强对具有适度、静态噪声的光学神经加速器的变化容忍度，但我们观察到由于时间漂移变化导致的显著性能下降，这需要实时、原位校准机制。为了解决这些具有挑战性的可靠性问题，我们首次提出了一种轻量级的动态片上矫正框架，称为DOCTOR，提供适应性的、原位的准确度恢复，针对时间

    arXiv:2403.02688v1 Announce Type: cross  Abstract: Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally
    
[^36]: FedHCDR: 具有超图信号解耦的联邦跨领域推荐

    FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling

    [https://arxiv.org/abs/2403.02630](https://arxiv.org/abs/2403.02630)

    该研究提出了FedHCDR框架，通过超图信号解耦的方式解决了联邦跨领域推荐中不同领域数据异质性的问题。

    

    近年来，跨领域推荐（CDR）备受关注，利用来自多个领域的用户数据来增强推荐性能。然而，当前的CDR方法需要跨领域共享用户数据，违反了《通用数据保护条例》（GDPR）。因此，已提出了许多联邦跨领域推荐（FedCDR）方法。然而，不同领域间的数据异质性不可避免地影响了联邦学习的整体性能。在这项研究中，我们提出了FedHCDR，一种具有超图信号解耦的新型联邦跨领域推荐框架。具体地，为了解决不同领域之间的数据异质性，我们引入一种称为超图信号解耦（HSD）的方法，将用户特征解耦为领域独有和领域共享特征。该方法采用高通和低通超图滤波器来进行解耦。

    arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
    
[^37]: 通过分层语义环境改善图中的超出分布泛化

    Improving out-of-distribution generalization in graphs via hierarchical semantic environments

    [https://arxiv.org/abs/2403.01773](https://arxiv.org/abs/2403.01773)

    通过生成分层语义环境，本文提出了一种新方法来增强图的不变学习，以处理分布转移。

    

    在图领域中，由于复杂的分布转移和缺乏环境背景，图的超出分布（OOD）泛化具有挑战性。最近的方法尝试通过生成平面环境来增强图的OOD泛化。然而，这种平面环境存在固有的局限性，无法捕捉更复杂的数据分布。因此，针对包含各种训练环境（如骨架、大小等）的DrugOOD数据集，平面环境无法充分解决其高异质性。因此，提出了一个新的挑战，即生成更具语义丰富的环境，以增强图的不变学习以处理分布转移。在本文中，我们提出了一种新颖的方法，为每个图生成分层语义环境。首先，给定输入图，我们明确地提取输入图中的变体子图，以在本地环境上生成代理预测。然后，随机注意...

    arXiv:2403.01773v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attent
    
[^38]: 视觉-语言模型泛化的不变测试时适应性

    Invariant Test-Time Adaptation for Vision-Language Model Generalization

    [https://arxiv.org/abs/2403.00376](https://arxiv.org/abs/2403.00376)

    本文提出了一个测试时提示调优范式，通过优化可学习的提示，迫使模型利用真正的因果不变特征，以解决视觉-语言模型在特定任务需求上无法有效利用预训练特征的挑战。

    

    arXiv:2403.00376v1 公告类型: 交叉摘要: 视觉-语言基础模型在大量图像-文本配对数据集上的可扩展性使其在众多下游任务中展现出卓越成功。然而，这些模型在应用于长尾任务（如细粒度图像分类）时显示出明显局限，这是由于“决策捷径”导致了它们的泛化能力受限。本文发现CLIP模型具有丰富的特征集，涵盖了既有的\textit{期望不变因果特征}又有的\textit{不希望的决策捷径}。此外，CLIP在下游任务中的表现不佳源自其无法有效利用预训练特征以符合特定任务要求。为解决这一挑战，本文引入一种测试时提示调优范式，优化一个可学习的提示，从而促使模型利用真正的因果不变特征。

    arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
    
[^39]: FORML：一种具有正交约束的流形海森自由元学习方法

    FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint

    [https://arxiv.org/abs/2402.18605](https://arxiv.org/abs/2402.18605)

    该论文介绍了一种 FORML 方法，使用斯蒂夫尔流形上的一阶导数近似，通过引入海森自由方法来降低计算负担和内存占用，并在元学习中实现参数正交约束。

    

    元学习问题通常被表述为一个双层优化问题，其中任务特定参数和元参数分别在优化的内部和外部循环中进行更新。然而，在黎曼空间中进行优化，参数和元参数位于黎曼流形上，这在计算上是非常密集的。与欧几里德方法不同，黎曼反向传播需要计算包括通过黎曼算子（如收缩和正交投影）的二阶导数。本文介绍了一种使用斯蒂夫尔流形上的导数的一阶近似的海森自由方法。我们的方法显著减少了计算负载和内存占用。我们展示了如何使用一个斯蒂夫尔全连接层作为骨干网络的最后分类层参数上的正交约束。

    arXiv:2402.18605v1 Announce Type: new  Abstract: Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbon
    
[^40]: 用持久同调增强图池化

    Boosting Graph Pooling with Persistent Homology

    [https://arxiv.org/abs/2402.16346](https://arxiv.org/abs/2402.16346)

    通过PH向池化层注入全局拓扑不变性的机制显著提升了图神经网络的性能。

    

    最近，将持久同调（PH）纳入图神经网络（GNN）以丰富表达能力的趋势越来越明显。然而，简单地将PH特征插入GNN层总是带来较低可解释性的边际改进。本文研究了一种新颖的机制，通过PH向池化层注入全局拓扑不变性，灵感来自PH中的过滤操作自然地使图池化以截断方式对齐。这种方式下，粗化图中的消息传递沿着持久池化拓扑进行，从而提升性能。在实验中，我们将该机制应用于一系列图池化方法，并观察到在几个常见数据集上持续且显著的性能提升，展示了其广泛适用性和灵活性。

    arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.
    
[^41]: 实现约$O(1/\epsilon)$的样本复杂度用于约束马尔可夫决策过程

    Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process

    [https://arxiv.org/abs/2402.16324](https://arxiv.org/abs/2402.16324)

    该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\epsilon^2)$样本复杂度有所提升。

    

    我们考虑约束马尔可夫决策过程（CMDP）的强化学习问题，在顺序学习和决策中满足安全性或资源约束方面起着关键作用。在这个问题中，我们拥有有限资源和未知转移概率的MDP。在每个阶段，我们采取一个行动，收集奖励并消耗一些资源，所有假设都是未知的，并且需要随着时间学习。在这项工作中，我们迈出了为CMDP问题推导出最优的问题相关保证的第一步。我们得出了一个对数遗憾界限，这转化为$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$的样本复杂度界限，其中$\kappa$是一个与问题相关的参数，但与$\epsilon$无关。我们的样本复杂度界限改进了先前文献中针对CMDP问题建立的$O(1/\epsilon^2)$样本复杂度。

    arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms
    
[^42]: 嵌入线性动力学的神经网络用于长序列建模

    Linear Dynamics-embedded Neural Network for Long-Sequence Modeling

    [https://arxiv.org/abs/2402.15290](https://arxiv.org/abs/2402.15290)

    提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。

    

    由于现有模型在长序列建模中性能和计算效率之间的权衡成为瓶颈，受到控制理论中具有多输入多输出的连续状态空间模型（SSMs）启发，我们提出了一种名为嵌入线性动力学的神经网络（LDNN）的新型神经网络。 SSM的连续、离散和卷积属性使LDNN具有少量参数、灵活的推断和在长序列任务中高效训练的特点。 我们开发了两种有效策略，对角化和“解耦然后快速傅立叶变换（FFT）”，以将卷积的时间复杂度从$O(LNH\max\{L, N\})$降低到$O(LN\max\{H, \log L\})$。 我们通过双向非因果和多头设置进一步改进了LDNN，以适应更广泛的应用范围。 对长距离竞技场（LRA）的大量实验表明了LDNN的有效性和最先进的性能。

    arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
    
[^43]: 缺失数据下的结构学习的最优输运

    Optimal Transport for Structure Learning Under Missing Data

    [https://arxiv.org/abs/2402.15255](https://arxiv.org/abs/2402.15255)

    提出了一种基于最优输运的得分算法，用于从缺失数据中学习因果结构，通过将结构学习视为密度拟合问题，并通过最小化与观测数据分布之间的沃尔仑斯坦距离来找到导致观测数据分布的因果模型

    

    在缺失数据的情况下进行因果发现会引入鸡生蛋问题。虽然目标是恢复真实的因果结构，但鲁棒的插补需要考虑变量之间的依赖性或更好地因果关系。仅仅用现有的插补方法填充缺失值，然后在完整数据上应用结构学习被证明是次优的。为此，本文提出了一种基于最优输运的基于得分的算法，用于从缺失数据中学习因果结构。这种最优输运的观点不同于现有基于EM的基于得分方法。我们将结构学习投影为密度拟合问题，其目标是找到引起观测数据分布和观测数据之间的沃尔仑斯坦距离的因果模型。通过大量的模拟和实际数据实验，我们的框架...

    arXiv:2402.15255v1 Announce Type: cross  Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework 
    
[^44]: 通过表示编辑推进微调中的参数效率

    Advancing Parameter Efficiency in Fine-tuning via Representation Editing

    [https://arxiv.org/abs/2402.15179](https://arxiv.org/abs/2402.15179)

    RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果

    

    参数有效微调（PEFT）因其能够在仅更新可训练参数的一个小子集时达到竞争性结果而受到了重视。在解决这些挑战问题中，我们提出了一种新颖的微调神经模型的方法，称为表示编辑（RED），其扩放和偏置每一层产生的表示。与完全参数微调相比，RED将可训练参数数量降低了$25,700$倍，并与LoRA相比降低了32倍。值得注意的是，RED实现了与完全参数微调和其他PEFT方法相当或更好的结果。对不同架构和规模的模型进行了大量实验。

    arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
    
[^45]: 基于采样的消息传递神经网络分布式训练

    Sampling-based Distributed Training with Message Passing Neural Network

    [https://arxiv.org/abs/2402.15106](https://arxiv.org/abs/2402.15106)

    该论文介绍了一种基于采样和分布式训练的消息传递神经网络（MPNN），能够有效解决边缘图神经网络在节点数量增加时的扩展挑战。

    

    在这项研究中，我们介绍了一种基于域分解的消息传递神经网络（MPNN）分布式训练和推断方法。我们的目标是解决随着节点数量增加而扩展边缘图神经网络的挑战。通过我们的分布式训练方法，结合Nystrom-近似采样技术，我们提出了一种可扩展的图神经网络，称为DS-MPNN（其中D和S分别代表分布式和采样），能够扩展到$O(10^5)$个节点。我们在两个案例上验证了我们的采样和分布式训练方法：（a）Darcy流数据集和（b）2-D机翼的稳态RANS模拟，提供了与单GPU实现和基于节点的图卷积网络（GCNs）的比较。DS-MPNN模型表现出与单GPU实现相当的准确性，能够容纳比单个GPU实现更多数量的节点。

    arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
    
[^46]: KIEval：面向大型语言模型的知识引导式交互评估框架

    KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models

    [https://arxiv.org/abs/2402.15043](https://arxiv.org/abs/2402.15043)

    该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估

    

    大型语言模型（LLMs）的自动评估方法受到数据污染的影响，导致对其有效性的评估被夸大。现有的策略旨在检测受污染的文本，但侧重于量化污染程度而非准确衡量模型性能。本文介绍了KIEval，这是一种知识引导式交互评估框架，首次引入了LLM驱动的“交互者”角色，实现了动态抗污染评估。从涉及特定领域知识的常规LLM基准问题开始，KIEval利用动态生成的、多轮、以知识为重点的对话，以确定模型的响应是否仅是基准答案的回忆，还是表明了深入理解并能在更复杂的对话中应用知识。在五个数据集上对七个领先的LLM进行了大量实验证实了KI

    arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
    
[^47]: 量子理论与情境最优输运的应用

    Quantum Theory and Application of Contextual Optimal Transport

    [https://arxiv.org/abs/2402.14991](https://arxiv.org/abs/2402.14991)

    提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化，并通过预测背景情境中药物剂量参数化的细胞类型分布的变化来验证方法，展示了捕捉剂量引起的细胞分布变化的能力。

    

    最优输运（Optimal Transport，OT）推动了机器学习在许多领域的应用。在测量数据（$\mu$，$\nu$）与上下文变量 $p_i$ 耦合的情况下，我们可以努力学习一个可以通过可能看不见的上下文参数化的全局输运映射。现有方法利用神经最优输运，并在很大程度上依赖于Brenier定理。在这里，我们提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化。我们利用双随机矩阵和酉算符之间的直接联系，从而找到了最优输运和量子计算之间的自然联系。我们通过对合成和真实数据的验证，预测通过药物剂量参数化的细胞类型分布的变化作为背景情境。我们将我们的方法与几个基准进行了比较，结果显示我们的方法可以捕捉到剂量引起的细胞分布变化，甚至在一定程度上。

    arXiv:2402.14991v1 Announce Type: new  Abstract: Optimal Transport (OT) has fueled machine learning (ML) applications across many domains. In cases where paired data measurements ($\mu$, $\nu$) are coupled to a context variable $p_i$ , one may aspire to learn a global transportation map that can be parameterized through a potentially unseen con-text. Existing approaches utilize Neural OT and largely rely on Brenier's theorem. Here, we propose a first-of-its-kind quantum computing formulation for amortized optimization of contextualized transportation plans. We exploit a direct link between doubly stochastic matrices and unitary operators thus finding a natural connection between OT and quantum computation. We verify our method on synthetic and real data, by predicting variations in cell type distributions parameterized through drug dosage as context. Our comparisons to several baselines reveal that our method can capture dose-induced variations in cell distributions, even to some exten
    
[^48]: 内在的狼：通过MLLM操作员向MLLM社会中渗入恶意

    The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative

    [https://arxiv.org/abs/2402.14859](https://arxiv.org/abs/2402.14859)

    这里是中文总结出的一句话要点: 论文探讨了在MLLM社会中通过单个操作员间接影响其他代理生成恶意内容的新型漏洞。

    

    由于其前所未有的处理和响应各种数据类型的能力，多模大型语言模型（MLLMs）不断定义人工通用智能（AGI）的新边界。随着这些先进的生成模型越来越多地形成用于复杂任务的协作网络，这些系统的完整性和安全性至关重要。我们的论文《内在的狼》探讨了MLLM社会中的一种新型漏洞 - 恶意内容的间接传播。与直接为MLLM生成有害输出不同，我们的研究展示了一个单个MLLM代理如何被微妙地影响，以生成再次诱使社会中其他MLLM代理输出恶意内容的提示。这种微妙而强有力的间接影响方法标志着与MLLM相关的安全风险的显著升级。我们的发现表明，即使几乎没有或是根本没有访问MLLM参数，一个MLLM代理，当

    arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when 
    
[^49]: CriticBench：为批判性-正确推理评估LLMs而设计的基准测试

    CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

    [https://arxiv.org/abs/2402.14809](https://arxiv.org/abs/2402.14809)

    CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。

    

    大型语言模型（LLMs）批判和完善其推理的能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文引入了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正其推理能力的综合基准测试。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并结合了三个LLM系列的响应。利用CriticBench，我们评估和剖析了17个LLMs在生成、批判和修正推理（即GQC推理）中的表现。我们的研究结果显示：（1）GQC能力呈线性关系，批判性训练显著提升了性能；（2）修正效果在任务上有所不同，以逻辑为导向的任务更容易修正；（3）GQC知识的不一致性。

    arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
    
[^50]: Q-Probe: 一种轻量级方法，用于语言模型的奖励最大化

    Q-Probe: A Lightweight Approach to Reward Maximization for Language Models

    [https://arxiv.org/abs/2402.14688](https://arxiv.org/abs/2402.14688)

    Q-Probe是一种轻量级方法，通过学习简单的线性函数在模型的嵌入空间中重新加权候选完成，从而调整预训练语言模型以最大化任务特定的奖励函数，在各种领域中获得显著收益。

    

    我们提出了一种称为Q-probing的方法，用于调整预训练语言模型以最大化任务特定的奖励函数。在高层次上，Q-probing位于像微调这样较重的方法和像少量提示这样较轻的方法之间，但也可以与任一种方法结合使用。其想法是在模型的嵌入空间上学习一个简单的线性函数，该函数可用于重新加权候选完成。我们从理论上证明，随着样本数量的增加，这种采样过程等同于Q-probe的KL约束最大化。为了训练Q-probes，我们考虑奖励建模或基于重要性加权策略梯度的一类新型直接策略学习目标。通过这种技术，我们看到在具有基于地面真实奖励（代码生成）以及由偏好数据定义的隐式奖励的领域中获得收益，甚至在数据有限的情况下胜过微调。此外，Q-probe可以

    arXiv:2402.14688v1 Announce Type: new  Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be 
    
[^51]: 具有不可微分规则引导扩散的符号音乐生成

    Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion

    [https://arxiv.org/abs/2402.14285](https://arxiv.org/abs/2402.14285)

    介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步

    

    我们研究了符号音乐生成的问题（例如生成钢琴卷谱），技术重点放在不可微分规则引导上。音乐规则通常以符号形式表达在音符特征上，如音符密度或和弦进行，许多规则是不可微分的，这在使用它们进行引导扩散时存在挑战。我们提出了一种新颖的引导方法，称为随机控制引导（SCG），它仅需要对规则函数进行前向评估，可以与预训练的扩散模型以即插即用的方式一起工作，从而首次实现了对不可微分规则的无训练引导。此外，我们引入了一种用于符号音乐生成的高时间分辨率潜在扩散架构，可以与SCG以即插即用的方式组合。与符号音乐生成中的标准强基线相比，该框架在音乐质量方面展示了明显的进展

    arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
    
[^52]: 冻结网络中的部分搜索足以找到强大的彩票票证

    Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket

    [https://arxiv.org/abs/2402.14029](https://arxiv.org/abs/2402.14029)

    提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。

    

    arXiv:2402.14029v1 公告类型：跨越 摘要：随机初始化的稠密网络包含可以在不进行权重学习的情况下实现高准确度的子网络--强大的彩票票证（SLTs）。最近，Gadhikar等人（2023年）在理论和实验证明，SLTs也可以在随机修剪的源网络中找到，从而减少SLT的搜索空间。然而，这限制了对甚至比源网络更稀疏的SLTs的搜索，导致由于意外的高稀疏性而准确度较差。本文提出了一种通过独立于所需SLT稀疏性的任意比率减少SLT搜索空间的方法。通过冻结一部分初始权重的随机子集，将其排除在搜索空间之外--即，通过永久修剪它们或将它们锁定为SLT的固定部分。事实上，通过我们与随机冻结变量的子集和逼近，在这种减少的搜索空间中，SLT的存在在理论上是得到保证的。除此之外，还可以减少...

    arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
    
[^53]: 提高音频指纹识别准确性，解决背景噪音和失真挑战

    Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges

    [https://arxiv.org/abs/2402.13957](https://arxiv.org/abs/2402.13957)

    本研究提出了一种结合人工智能和机器学习的音频指纹识别算法，通过在具有多样背景噪音和失真的真实环境场景模拟中的工作，以提高准确性，实现100%准确性的5秒音频输入，系统匹配速度可预测，强调了实际实施中的关键空间-速度权衡。

    

    音频指纹识别，如 Shazam 等先驱所示，已经改变了数字音频识别。然而，现有系统在复杂环境下的准确性存在问题，限制了广泛应用。本研究提出了一种人工智能和机器学习集成的音频指纹识别算法以增强准确性。建立在 Dejavu 项目的基础上，本研究强调了在具有多样背景噪音和失真的真实环境场景模拟中的工作。信号处理，是 Dejavu 模型的核心，包括快速傅里叶变换、频谱图和峰值提取。"星座"概念和指纹哈希使得歌曲独特标识成为可能。性能评估证实，在5秒音频输入时可达到100%的准确性，系统展示出可预测的匹配速度以提高效率。存储分析凸显了实际实施中的关键空间-速度权衡。本研究推动了音频指纹识别的适应性。

    arXiv:2402.13957v1 Announce Type: cross  Abstract: Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The "constellation" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptab
    
[^54]: 克服迭代正则化中密度比估计的饱和问题

    Overcoming Saturation in Density Ratio Estimation by Iterated Regularization

    [https://arxiv.org/abs/2402.13891](https://arxiv.org/abs/2402.13891)

    引入迭代正则化方法解决了密度比估计中的饱和问题，实现了快速收敛，在密度比估计基准测试和大规模深度无监督领域自适应模型的重要性加权集成中表现优异。

    

    从有限样本中估计两个概率密度的比率，是机器学习和统计学中的一个核心任务。在这项工作中，我们发现一大类密度比估计的核方法存在错误饱和问题，这阻碍了算法在高度规则学习问题上实现快速错误收敛率。为了解决饱和问题，我们引入了迭代正则化方法在密度比估计中以实现快速错误率。我们的方法在密度比估计基准测试以及大规模评估深度无监督领域自适应模型的重要性加权集成方面表现优异。

    arXiv:2402.13891v1 Announce Type: new  Abstract: Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.
    
[^55]: 可复制学习大间距半空间

    Replicable Learning of Large-Margin Halfspaces

    [https://arxiv.org/abs/2402.13857](https://arxiv.org/abs/2402.13857)

    该论文提出了解决学习大间距半空间问题的可复制算法，相比之前的算法，在维度无关、时间复杂度优化、样本复杂度方面等多个关键参数上均有显著改进。

    

    我们提供了有效的可复制算法来解决学习大间距半空间的问题。我们的结果改进了Impagliazzo, Lei, Pitassi和Sorrell在STOC, 2022中提供的算法。我们设计了这个任务的首个与维度无关的可复制算法，其运行时间为多项式，是正确的，并且在所有相关参数方面的样本复杂度都严格比Impagliazzo等人在2022年实现的算法要好。此外，我们的第一个算法在精度参数$\epsilon$方面具有样本复杂度。我们还设计了一个基于SGD的可复制算法，在某些参数范围内，其样本复杂度和时间复杂度优于我们的第一个算法。

    arXiv:2402.13857v1 Announce Type: new  Abstract: We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to o
    
[^56]: SimPro：一个简单的概率框架实现逼真的长尾半监督学习

    SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning

    [https://arxiv.org/abs/2402.13505](https://arxiv.org/abs/2402.13505)

    SimPro提出了一种高度适应的框架，不依赖于任何关于未标记数据分布的预定义假设，通过创新地改进期望最大化（EM）算法，明确分离条件和边缘类别分布的建模。

    

    近年来半监督学习的最新进展集中在解决一个更为逼真但具有挑战性的任务：解决标记数据的不平衡问题，同时未标记数据的类别分布既未知又可能不匹配。当前这一领域的方法往往预设了关于未标记数据类别分布的严格假设，从而限制了模型仅适应于某些分布范围。在本研究中，我们提出了一种新颖的方法，引入了一个高度适应性的框架，命名为SimPro，它不依赖于任何关于未标记数据分布的预定义假设。我们的框架建立在一个概率模型上，通过明确分离条件和边缘类别分布的建模，创新地改进了期望最大化（EM）算法。这种分离促进了在最大化过程中对类别分布进行估计的闭合形式解决方案。

    arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
    
[^57]: 无需公平训练的公平分类器：一种受影响数据抽样方法

    Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach

    [https://arxiv.org/abs/2402.12789](https://arxiv.org/abs/2402.12789)

    在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。

    

    一个公平的分类器应该确保来自不同群体的人们受益，而群体信息往往是敏感的，不适合模型训练。因此，在训练数据集中学习一个公平的分类器但排除敏感属性是很重要的。本文研究了学习公平分类器而不实现公平训练算法的方法，以避免可能泄露敏感信息。我们的理论分析验证了这种方法的可能性，即在具有适当分布偏移的数据集上进行传统训练可以同时减少公平差距的上限和模型泛化误差，表明公平性和准确性可以同步提高，只需简单地进行传统训练。然后，我们提出了一个可行的解决方案，通过抽样有影响力的数据逐步转移原始训练数据，在训练过程中不访问新数据的敏感属性。

    arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
    
[^58]: 学习在内容审核中推迟：人工智能与人类协同作用

    Learning to Defer in Content Moderation: The Human-AI Interplay

    [https://arxiv.org/abs/2402.12237](https://arxiv.org/abs/2402.12237)

    本文提出了一个模型，捕捉内容审核中人工智能的相互作用。

    

    成功的在线平台内容审核依赖于人工智能协同方法。本文介绍了一个模型，捕捉内容审核中人工智能的相互作用。算法观察到即将发布的帖子的背景信息，做出分类和准入决策，并安排帖子进行人工审核。

    arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
    
[^59]: 平衡数据，不平衡光谱：揭示具有光谱不平衡的类别差异

    Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance

    [https://arxiv.org/abs/2402.11742](https://arxiv.org/abs/2402.11742)

    该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。

    

    分类模型被期望在不同类别上表现同样良好，但在实践中，它们的表现往往存在较大差距。这个类别偏见问题在样本不平衡的数据集中得到了广泛研究，但在平衡数据集中相对被忽视。在这项工作中，我们提出了特征中的光谱不平衡概念作为类别差异的潜在来源，并研究光谱不平衡与类别偏见在理论和实践中的联系。为了建立光谱不平衡与类别差距之间的关系，我们开发了一个用于研究类别差异的理论框架，并推导了高维混合模型设定中每类错误的精确表达式。然后我们在11个不同的最先进的预训练编码器中研究了这一现象，并展示了我们提出的框架如何用于比较编码器的质量，以及评估和组合数据增强策略。

    arXiv:2402.11742v1 Announce Type: new  Abstract: Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation stra
    
[^60]: 长期时间序列预测中的吸引子记忆：混沌视角

    Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective

    [https://arxiv.org/abs/2402.11463](https://arxiv.org/abs/2402.11463)

    Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。

    

    在长期时间序列预测（LTSF）任务中，现有的深度学习模型忽视了离散时间序列源自潜在连续动态系统的关键特征，导致缺乏外推和演化能力。 鉴别真实世界数据的混沌性质，我们的模型\textbf{\textit{Attraos}}将混沌理论融入到LTSF中，将实际时间序列视为未知高维混沌动态系统的观测。 在吸引子不变性的概念下，Attraos利用提出的多尺度动态记忆单元来记忆历史动态结构，并通过频率增强的局部演化策略进行预测。 详细的理论分析和丰富的经验证据一致表明，Attraos在主流LTSF数据集和混沌数据集上的表现优于各种LTSF方法。

    arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
    
[^61]: 具有部分反馈的公平分类：一种基于探索的数据收集方法

    Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach

    [https://arxiv.org/abs/2402.11338](https://arxiv.org/abs/2402.11338)

    该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。

    

    在许多预测场景（例如信贷放款）中，只有过去被积极分类的样本才会观察到真实结果。这些过去的观察结果形成了用于训练分类器以进行未来预测的训练数据集。然而，这样的训练数据集缺乏关于过去（错误地）被负面分类的样本结果的信息，可能导致错误的分类器。我们提出了一种方法，利用可用数据训练分类器，并提供一系列探索策略来收集关于否则会被忽略的子群体的结果数据。对于任何探索策略，该方法都具有以下保证：（1）所有子群体都得到了探索，（2）假阳性的比例受到了限制，（3）训练的分类器收敛到一个“期望”的分类器。正确的探索策略取决于上下文；它可以选择以改善学习保证

    arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
    
[^62]: 通过纯微调进行模型编辑

    Model Editing by Pure Fine-Tuning

    [https://arxiv.org/abs/2402.11078](https://arxiv.org/abs/2402.11078)

    纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。

    

    精细调整被认为在模型编辑中不够有效，因为相对更专业的方法而言，它的表现较差。然而，微调是简单的，不关心被编辑模型的体系结构细节，并且能够利用标准训练方法的不断进展（例如PEFT），使其成为模型编辑器的吸引选择。在本文中，我们展示了纯粹的微调可以是一种可行的模型编辑方法。我们提出了对朴素微调进行轻微修改的两个关键因素。第一，我们优化条件似然而非完整似然。第二，我们使用随机释义和事实来增加数据，以鼓励泛化和局部性。我们在ZsRE和CounterFact上的实验表明，这一简单修改使得微调通常可以与专业编辑器在编辑分数方面匹敌甚至超越。

    arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
    
[^63]: 基于图的时空降采样缺失数据预测

    Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling

    [https://arxiv.org/abs/2402.10634](https://arxiv.org/abs/2402.10634)

    通过 hierarchical spatiotemporal downsampling 处理缺失数据问题，结合可解释的注意机制，实现对时空预测的有效建模

    

    给定一组与空间中传感器点相关联、具有相互关系的同步时间序列，时空预测问题包括为每个点预测未来观测值。时空图神经网络通过将时间序列表示为图来实现引人注目的结果。然而，大多数现有方法依赖于一个常常不切实际的假设，即输入始终可用，并且在数据部分缺失时无法捕捉隐藏的时空动态。在这项工作中，我们通过分层时空降采样来解决这个问题。输入时间序列随着时间和空间的推移逐渐粗化，获得一组捕捉异质时间和空间动态的表示。在观测值和缺失数据模式的条件下，通过一个可解释的注意机制来组合这些表示以生成

    arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
    
[^64]: 使用锐度感知最小化和通道注意力解锁Transformer在时间序列预测中的潜力

    Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention

    [https://arxiv.org/abs/2402.10198](https://arxiv.org/abs/2402.10198)

    本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。

    

    Transformer架构在自然语言处理和计算机视觉中取得了突破性的性能，但在多元长期预测方面，它们仍然不如更简单的线性基线。为了更好地理解这一现象，我们首先研究了一个玩具线性预测问题，展示了尽管Transformer具有高表达能力，但它们无法收敛到真正的解决方案。我们进一步确定Transformer的注意力是造成其低泛化能力的原因。基于这一认识，我们提出了一个浅层轻量级的Transformer模型，在锐度感知优化的情况下成功避免了坏的局部最小值。我们通过实验证明，这个结果适用于所有常用的实际多元时间序列数据集。特别是，相比当前最先进的模型TSMixer，SAMformer的平均性能提高了14.33%，并且参数数量减少了约4倍。

    arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
    
[^65]: MIM-Refiner：一种从中间预训练表示中获得对比学习提升的方法

    MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations

    [https://arxiv.org/abs/2402.10093](https://arxiv.org/abs/2402.10093)

    MIM-Refiner是一种对比学习提升方法，通过利用MIM模型中的中间层表示和多个对比头，能够将MIM模型的特征从次优的状态提升到最先进的状态，并在ImageNet-1K数据集上取得了新的最先进结果。

    

    我们引入了MIM-Refiner，这是一种用于预训练MIM模型的对比学习提升方法。MIM-Refiner的动机在于MIM模型中的最佳表示通常位于中间层。因此，MIM-Refiner利用连接到不同中间层的多个对比头。在每个头中，修改后的最近邻目标帮助构建相应的语义聚类。此过程短而有效，在几个epochs内，我们将MIM模型的特征从次优的状态提升到最先进的状态。使用data2vec 2.0在ImageNet-1K上预训练的ViT-H经过改进后，在线性探测和低样本分类方面取得了新的最先进结果（分别为84.7%和64.2%），超过了在ImageNet-1K上预训练的其他模型的表现。

    arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
    
[^66]: ECE有多大的缺陷？通过对数平滑的分析

    How Flawed is ECE? An Analysis via Logit Smoothing

    [https://arxiv.org/abs/2402.10046](https://arxiv.org/abs/2402.10046)

    本研究通过分析对数平滑，探讨了ECE的缺陷以及对现有结果的影响，并提出了一种新的连续、易于估计的误差测度LS-ECE。通过实验发现，LS-ECE与分箱ECE非常接近。

    

    简而言之，如果一个模型的预测准确率与置信度匹配，那么这个模型就是校准的。在现有文献中，衡量校准性最常见的方法是期望校准误差（ECE）。然而，最近的研究指出了ECE的缺点，例如它在预测者空间中是不连续的。本研究探讨了这些问题有多本质，并分析了它们对现有结果的影响。为此，我们完全描述了ECE对波兰空间上的一般概率测度的不连续性。然后，我们利用这些不连续性提出了一种新的连续、易于估计的误差测度，称为Logit-Smoothed ECE（LS-ECE）。通过比较预训练图像分类模型的ECE和LS-ECE，我们在初步实验中发现，分箱ECE与LS-ECE非常接近，表明理论方面是相符的。

    arXiv:2402.10046v1 Announce Type: new  Abstract: Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical
    
[^67]: MiMiC：表示空间中最小修改的对抗事实

    MiMiC: Minimally Modified Counterfactuals in the Representation Space

    [https://arxiv.org/abs/2402.09631](https://arxiv.org/abs/2402.09631)

    提出了一种新颖的对抗事实生成方法，利用闭式解决方案在表示空间中生成富有表达力的对抗事实，以减轻语言模型中的不良行为，该方法在地球移动问题方面提供理论上的保证，并对表示空间的几何组织进行改进。

    

    arXiv:2402.09631v1 公告类型：交叉学科 简介：语言模型经常表现出不良行为，如性别偏见或有毒语言。通过对表示空间进行干预，可以有效减轻这些问题，但两种常见的干预技术，即线性擦除和定向向量，并不能提供高度可控和表达丰富度。因此，我们提出了一种新颖的干预方法，旨在在表示空间中生成富有表达力的对抗事实，使源类别（例如“有毒”）的表示与目标类别（例如“非有毒”）的表示相似。这种方法利用高斯假设下的闭式解决方案，在地球移动问题方面提供了理论上的保证，并对表示空间的几何组织提供了进一步的改进。

    arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
    
[^68]: iMove: 探索用于健身活动识别的生物阻抗传感技术

    iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition

    [https://arxiv.org/abs/2402.09445](https://arxiv.org/abs/2402.09445)

    通过传感器融合和对比学习，研究证明生物阻抗传感技术可以改进基于IMU的健身追踪，提高分类模型的精度。

    

    自动和精确的健身活动识别对于促进健康生活方式和个性化预防性医疗具有益处。虽然IMU目前是主要的健身追踪模式，但通过iMove，我们展示了生物阻抗可以通过传感器融合和对比学习来改善基于IMU的健身追踪。为了评估我们的方法，我们进行了一项实验，包括十个受试者在五天内进行的六种上身健身活动，以收集来自两只手腕的生物阻抗和左手腕IMU的同步数据。对比学习框架利用两种模态来训练更好的仅基于IMU的分类模型，其中生物阻抗只在训练阶段需要，通过这种方式，输入单个IMU的平均宏F1分数提高了3.22％，达到84.71％，而IMU基线模型为81.49％。我们还展示了生物阻抗如何能够...

    arXiv:2402.09445v1 Announce Type: cross  Abstract: Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU baseline model. We have also shown how bio-impedance can 
    
[^69]: SLEB: 通过冗余验证和消除Transformer块优化LLM的流程

    SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks

    [https://arxiv.org/abs/2402.09025](https://arxiv.org/abs/2402.09025)

    SLEB是一种通过消除冗余的Transformer块来优化LLM流程的新方法，它成功加速了LLM的推理过程。

    

    大型语言模型（LLM）在各种自然语言处理任务中证明了其高效性。然而，它们庞大的参数数量给实际部署带来了重大挑战。精简，一种旨在减小LLM大小和复杂度的技术，通过从网络中删除冗余组件提供了潜在解决方案。尽管精简有希望，但现有方法往往难以实现显著的端到端LLM推理加速。本文中，我们引入了SLEB，一种通过消除冗余的Transformer块来优化LLM流程的新方法。我们选择Transformer块作为精简的基本单位，因为LLM在相邻块的输出之间具有块级别的冗余和高相似性。这个选择使我们能够有效地增强LLM的处理速度。我们的实验证明，SLEB成功加速了LLM的推理过程。

    arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
    
[^70]: 使用双阶段扰动测试通过必要性和充分性进行特征归因，以进行因果解释

    Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation

    [https://arxiv.org/abs/2402.08845](https://arxiv.org/abs/2402.08845)

    这篇论文提出了一种使用双阶段扰动测试来进行特征归因的方法，通过计算扰动一个特征对预测变化的必要性和充分性作用的概率来衡量特征重要性。该方法能够增强特征归因方法在区分不同特征贡献方面的能力。

    

    我们研究了机器学习中的可解释性问题。为了解决这个问题，特征归因方法（FAMs）通过扰动测试来测量每个特征的贡献，其中在不同扰动下的预测差异进行比较。然而，在特征的预测变化相同的情况下，这种扰动测试可能无法准确区分不同特征的贡献。为了增强FAMs在这种具有挑战性的情况下区分不同特征贡献的能力，我们提出利用扰动一个特征对预测变化起到必要性和充分性作用的概率（PNS）作为特征重要性的度量。我们的方法，利用必要性和充分性进行特征归因（FANS），通过涉及两个阶段（事实性和干预性）的扰动测试计算PNS。在实践中，为了生成反事实样本，我们使用了一个重新...

    arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
    
[^71]: BECoTTA: 基于输入的在线专家混合模型用于持续的测试时间自适应

    BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation

    [https://arxiv.org/abs/2402.08712](https://arxiv.org/abs/2402.08712)

    BECoTTA是一种基于输入的高效CTTA框架，通过采用MoDE（Mixture-of-Domain Low-rank Experts）模型，它包含领域自适应路由和领域专家协同损失两个核心组件，能够在持续的测试时间中自适应不断变化的领域，同时只需较少的可训练参数。

    

    持续的测试时间自适应（CTTA）要求在适应不断变化的未知领域的同时保留先前学到的知识。然而，尽管CTTA取得了一些进展，但忘记适应权衡和效率仍未得到探索。此外，当前的CTTA场景仅假设存在不相交的情况，而实际世界的领域是无缝变化的。为了解决这些挑战，本文提出了一种名为BECoTTA的基于输入的高效CTTA框架。我们提出了MoDE（Mixture-of-Domain Low-rank Experts），它包含两个核心组件：i）领域自适应路由，通过多个领域路由器有选择地捕捉领域自适应知识，和ii）领域专家协同损失，以增加每个领域和专家之间的依赖性。我们验证了我们的方法优于多个CTTA场景，包括不相交和渐变领域切换，同时只需要大约98％更少的可训练参数。

    arXiv:2402.08712v1 Announce Type: new Abstract: Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters
    
[^72]: Agent Smith:一张图像可以迅速越狱一百万个多模态LLM代理

    Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast

    [https://arxiv.org/abs/2402.08567](https://arxiv.org/abs/2402.08567)

    Agent Smith提出了一种安全问题，即传染性越狱，该问题在多代理环境中可以通过简单的越狱一个代理来迅速感染所有代理并导致有害行为。

    

    多模态大型语言模型（MLLM）代理可以接收指令，捕捉图像，从内存中检索历史记录，并决定使用哪些工具。然而，红队评估发现恶意图像/提示可以越狱MLLM并导致不对齐的行为。在这项工作中，我们报告了多代理环境中更严重的安全问题，称为传染性越狱。它涉及到对单个代理进行简单的越狱，无需来自对手的进一步干预，（几乎）所有代理将以指数级别被感染并展示有害行为。为了验证传染性越狱的可行性，我们模拟了包含高达一百万个LLaVA-1.5代理的多代理环境，并将随机匹配对聊天作为多代理交互的概念验证实例。我们的结果表明，将（传染性）恶意图像输入到任意选择的代理的内存中就足以实现传染性越狱。

    A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
    
[^73]: 朝着忠实和强大的基于证据的问答专家的方向前进

    Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

    [https://arxiv.org/abs/2402.08277](https://arxiv.org/abs/2402.08277)

    这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。

    

    对大型语言模型（LLM）更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。其中一种达到这个目标的方法是基于可靠的来源提供答案。然而，这种基于证据的问答在使用LLM时已经证明在引用正确的来源（来源质量）和准确地表示来源中的信息（答案归因能力）方面工作不足。在这项工作中，我们系统地研究了如何鲁棒地微调LLM，以提高来源质量和答案归因能力。具体而言，我们引入了一个数据生成流水线，其中包括自动数据质量过滤器，可以大规模合成多样化的高质量训练和测试数据。我们还引入了四个测试集，以对微调后的专家模型的鲁棒性进行基准测试。广泛的评估结果表明，在合成数据上进行微调可以提高在内部和外部分布的性能。%基于证据的问答案例。此外，我们展示了用于评估的四个测试集，以评估微调后的专家模型的鲁棒性。

    Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
    
[^74]: Lumos : 用场景文本识别增强多模式LLMs的能力

    Lumos : Empowering Multimodal LLMs with Scene Text Recognition

    [https://arxiv.org/abs/2402.08017](https://arxiv.org/abs/2402.08017)

    本论文介绍了Lumos，它是第一个具备文本理解能力的多模式问答系统，通过运用场景文本识别组件，能够从第一人称视角图像中提取文本，并将其用于加强多模式大型语言模型的输入。研究过程中，作者克服了与文本识别质量、延迟和模型推断相关的多个挑战，并提供了全面的组件评估结果，展示了高质量和高效率的性能。

    

    我们介绍了Lumos，它是第一个具备文本理解能力的端到端多模式问答系统。Lumos的核心是一个场景文本识别（STR）组件，用于从第一人称视角图像中提取文本，并将其用于增强多模式大型语言模型（MM-LLM）的输入。在构建Lumos的过程中，我们遇到了许多与STR质量、整体延迟和模型推断相关的挑战。在本文中，我们探讨了这些挑战，并讨论了用于克服这些障碍的系统架构、设计选择和建模技术。我们还对每个组件进行了全面评估，展示了高质量和高效率的性能。

    We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
    
[^75]: 线性二次控制中策略梯度的隐性偏差：对未见初始状态的外推

    Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States

    [https://arxiv.org/abs/2402.07875](https://arxiv.org/abs/2402.07875)

    本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。

    

    在现代机器学习中，模型可以以多种方式拟合训练数据，其中一些在未见（测试）数据上表现良好，而其他一些则不然。有趣的是，在这种情况下，梯度下降经常展现出一种隐性偏差，导致在未见数据上表现出色。这种隐性偏差在监督学习中已经得到了广泛研究，但在最优控制（强化学习）中却了解得较少。在那里，通过梯度下降学习应用于系统的控制器被称为策略梯度，并且一个非常重要的问题是学习的控制器在对未见初始状态的外推程度。本文在理论上研究了策略梯度在对未见初始状态的外推方面的隐性偏差。我们以基本的线性二次调节器（LQR）问题为重点，确立了外推程度取决于训练中系统在初始状态下引起的探索程度。

    In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
    
[^76]: 通过分数阻尼库仑方程证明重尾SDEs的泛化界限

    Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation

    [https://arxiv.org/abs/2402.07723](https://arxiv.org/abs/2402.07723)

    本论文通过分数阻尼库仑方程证明了重尾SDE的高概率泛化界限，并且相对于参数维度，界限的依赖性要好于p。

    

    过去几年来，理解重尾随机优化算法的泛化性能引起了越来越多的关注。在利用重尾随机微分方程作为代理来阐明随机优化器的有趣方面时，先前的工作要么提供预期的泛化界限，要么引入了不可计算的信息论术语。为了解决这些缺点，在本文中，我们证明了重尾SDE的高概率泛化界限，这些界限不含任何非平凡的信息论术语。为了实现这个目标，我们基于估计与所谓的分数阻尼库仑方程相关联的熵流，开发了新的证明技术（这是一种控制相应重尾SDE分布演化的偏微分方程）。除了获得高概率界限之外，我们还展示了我们的界限相对于参数维度的依赖性要好于p。

    Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to p
    
[^77]: 重新思考战略环境中学习的比例定律

    Rethinking Scaling Laws for Learning in Strategic Environments

    [https://arxiv.org/abs/2402.07588](https://arxiv.org/abs/2402.07588)

    本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。

    

    越来越大的机器学习模型的部署反映出一个共识：模型越有表达能力，越拥有大量数据，就能改善性能。随着模型在各种真实场景中的部署，它们不可避免地面临着战略环境。本文考虑了模型与战略互动对比例定律的相互作用对性能的影响这个自然问题。我们发现战略互动可以打破传统的比例定律观点，即性能并不一定随着模型的扩大和/或表达能力的增强（即使有无限数据）而单调提高。我们通过战略回归、战略分类和多智能体强化学习的例子展示了这一现象的影响，这些例子展示了战略环境中的限制模型或策略类的表达能力即可。

    The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
    
[^78]: 梯度噪声的隐性偏见：从对称性角度来看

    The Implicit Bias of Gradient Noise: A Symmetry Perspective

    [https://arxiv.org/abs/2402.07193](https://arxiv.org/abs/2402.07193)

    本研究通过对对称性的存在进行分析，揭示了梯度噪声在随机梯度下降中的隐性偏见。我们发现不同类型的对称性会导致不同的学习动态，其中一类对称性可以自然收敛，而另一类对称性几乎总是发散。此外，我们的研究结果适用于没有对称性的损失函数，对于理解训练动态和解释相关实际问题具有普适性。

    

    我们对随机梯度下降（SGD）在损失函数存在连续对称性时的学习动态进行了表征，说明了SGD和梯度下降之间的分歧是多么巨大。我们展示了根据对称性对学习动态的影响方式，我们可以将一族对称性分为两类。对于一类对称性，SGD自然地收敛到具有平衡和对齐梯度噪声的解。对于另一类对称性，SGD几乎总是发散的。然后，我们展示了即使损失函数中没有对称性，我们的结果依然适用并可以帮助我们理解训练动态。我们的主要结果是普遍的，它只依赖于对称性的存在，而与损失函数的细节无关。我们证明了所提出的理论对于逐步变形和平坦化提供了解释，并可以应用于常见的实际问题，如表示正则化。

    We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
    
[^79]: Bilevel强化学习和RLHF的有原则的基于惩罚的方法

    Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF

    [https://arxiv.org/abs/2402.06886](https://arxiv.org/abs/2402.06886)

    本文提出了一种基于惩罚的方法来解决Bilevel强化学习和RLHF问题，这是首个有原则的算法框架。通过理论分析和实验证明了算法的有效性。

    

    最近，Bilevel优化已被应用于许多机器学习任务中。然而，它们的应用仅限于监督学习设置，其中考虑了具有良性结构的静态目标函数。但是，激励设计、反向强化学习(RL)和来自人类反馈的RLHF等Bilevel问题通常被建模为超越简单静态目标结构的动态目标函数，这给使用现有Bilevel解决方案带来了重大挑战。为了解决这一新的Bilevel问题类别，我们通过惩罚形式引入了解决Bilevel RL问题的第一个原则性算法框架。我们通过理论研究问题的景观及其基于惩罚的（策略）梯度算法进行了验证。我们通过在Stackelberg马尔可夫博弈、来自人类反馈的RL和激励设计中进行模拟来证明我们算法的有效性。

    Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.
    
[^80]: SMC就是你需要的：并行强扩展

    SMC Is All You Need: Parallel Strong Scaling

    [https://arxiv.org/abs/2402.06173](https://arxiv.org/abs/2402.06173)

    SMC并行扩展方法pSMC具有理论收敛速度，具有有界的时间复杂性和内存要求，适用于贝叶斯推断的问题。

    

    在贝叶斯推断的一般框架中，目标分布只能按比例常数进行评估。传统的一致Bayesian方法，如序贯蒙特卡洛(SMC)和马尔科夫链蒙特卡洛(MCMC)，具有无界的时间复杂性要求。我们开发了一种完全并行的序贯蒙特卡洛(pSMC)方法，可以证明它具有并行强扩展性，即如果允许异步进程数量增长，时间复杂性(和每个节点的内存)仍然保持有界。更具体地说，pSMC具有MSE$=O(1/NR)$的理论收敛速度，其中$N$表示每个处理器中的通信样本数量，$R$表示处理器数量。特别地，对于适当大的问题相关$N$，当$R\rightarrow \infty$时，该方法以固定有限的时间复杂性Cost$=O(1)$收敛到无穷小精度MSE$=O(\varepsilon^2)$，没有效率泄漏，即计算复杂性Cost$=O(\varepsilon)$。

    In the general framework of Bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes the number of communicating samples in each processor and $R$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $R \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon
    
[^81]: 使用PEAK进行窥探：多个数据流均值的顺序、非参数复合假设检验

    Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams

    [https://arxiv.org/abs/2402.06122](https://arxiv.org/abs/2402.06122)

    本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。

    

    我们提出了一种新颖的非参数顺序复合假设检验方法，用于多个数据流的均值。我们的方法名为PEAK（基于期望平均资产的窥探），基于测试即博弈的框架，提供了一个在任何停止时间上的非渐进α水平测试。PEAK在计算上可行，并且能够有效拒绝在满足我们的非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。我们在强化学习中的最佳臂识别和阈值识别任务中对我们的理论结果进行了数值验证，并展示了我们的方法在计算效率上优于现有的测试方法。

    We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
    
[^82]: \textit{MinMaxMin} $Q$-learning

    \textit{MinMaxMin} $Q$-learning

    [https://arxiv.org/abs/2402.05951](https://arxiv.org/abs/2402.05951)

    \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。

    

    \textit{MinMaxMin} $Q$-learning是一种新颖的乐观型Actor-Critic算法，解决了保守型强化学习算法中存在的过高估计偏差的问题（$Q$-估计过高估计了真实的$Q$值）。其核心公式依赖于$Q$-网络之间的差异，采用最小批次最大最小$Q$-网络距离作为$Q$-目标加入，并作为优先级经验回放采样规则。我们在TD3和TD7之上实施了\textit{MinMaxMin}，并对其在流行的MuJoCo和Bullet环境中对抗现有的连续空间算法-DDPG，TD3和TD7进行了严格测试。结果显示，在所有测试任务中，\textit{MinMaxMin}相对于DDPG，TD3和TD7均表现出了稳定的性能提升。

    \textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
    
[^83]: SQT - std Q-target

    \textit{SQT} -- \textit{std} $Q$-target

    [https://arxiv.org/abs/2402.05950](https://arxiv.org/abs/2402.05950)

    SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。

    

    Std Q-target是一种基于Q-学习的保守型actor-critic算法，它基于一个关键的Q公式：Q网络的标准差，这个标准差作为一种“不确定性惩罚”，是对过高估计偏差问题的一种简约解决方案。我们在TD3/TD7代码的基础上实现了SQT，并将其与最先进的actor-critic算法DDPG、TD3和TD7在七个常见的MuJoCo和Bullet任务上进行了测试。我们的结果表明，在强化学习中，SQT的Q-target公式相对于TD3的Q-target公式在解决过高估计偏差的保守解方面具有优势，而在所有任务中，SQT相对于DDPG、TD3和TD7都有明显的性能优势。

    \textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
    
[^84]: 基于模型的强化学习在平均场博弈中并不比单个智能体强化学习更加困难

    Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL

    [https://arxiv.org/abs/2402.05724](https://arxiv.org/abs/2402.05724)

    本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。

    

    我们研究了在平均场博弈中基于模型的函数逼近下强化学习样本复杂度，该方法需要策略性探索以找到纳什均衡策略。我们引入了部分基于模型的Eluder维度（P-MBED），这是一种更有效的概念来描述模型类复杂度。值得注意的是，P-MBED可以衡量从给定的平均场模型类转换而来的单个智能体模型类的复杂度，并且潜在上可能比\citet{huang2023statistical}提出的MBED指数级低。我们提出了一种模型消除算法，具有新颖的探索策略，并建立了与P-MBED相关的样本复杂度结果，这些结果表明，在基本可实现性和Lipschitz连续性假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。我们进一步将我们的结果推广到多类型平均场博弈。

    We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
    
[^85]: 可扩展的不平衡最优输运生成建模中的Wasserstein渐变流

    Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport

    [https://arxiv.org/abs/2402.05443](https://arxiv.org/abs/2402.05443)

    本文介绍了一种可扩展的生成模型，称为Semi-dual JKO (S-JKO)，通过采用半对偶形式的JKO步骤，降低了训练复杂性，并在WGF上有显著的性能改进。

    

    Wasserstein渐变流（WGF）描述了Wasserstein空间中概率密度的梯度动力学。WGF提供了在概率分布上进行优化的有希望的方法。数值上近似连续的WGF需要时间离散化方法。其中最著名的方法是JKO方案。在这方面，以前的WGF模型采用JKO方案，并为每个JKO步骤参数化传输映射。然而，这种方法导致了与JKO步骤数量K成二次增长的训练复杂性$O(K^2)$。这严重限制了WGF模型的可扩展性。在本文中，我们介绍了一种可扩展的基于WGF的生成模型，称为半对偶JKO（S-JKO）。我们的模型基于JKO步骤的半对偶形式，通过JKO步骤与不平衡最优输运之间的等价性得到。我们的方法将训练复杂性降低到$O(K)$。我们证明了我们的模型明显优于现有的基于WGF的生成模型。

    Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based gener
    
[^86]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^87]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^88]: 离线约束强化学习中一种基本对偶算法在低秩MDPs上的应用

    A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs

    [https://arxiv.org/abs/2402.04493](https://arxiv.org/abs/2402.04493)

    这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。

    

    离线强化学习旨在通过预先收集的数据集学习一种最大化期望累积奖励的策略。最近，对于低秩MDPs或一般函数逼近的离线强化学习进行了广泛研究，但是现有算法在找到$\epsilon$-优化策略的样本复杂度为$O(\epsilon^{-2})$时，要么需要均匀的数据覆盖假设，要么计算效率低下。在本文中，我们提出了一种在折扣无穷时段设置下，用于低秩MDPs的离线强化学习的基本对偶算法。我们的算法是在部分数据覆盖假设下，该设置中第一个样本复杂度达到$O(\epsilon^{-2})$的计算有效的算法。这优于最近的一项工作，其需要$O(\epsilon^{-4})$个样本。此外，我们的算法通过支持额外奖励信号的约束，将之前的工作扩展到离线约束强化学习设置中。

    Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.
    
[^89]: Chatbot遇见管道：利用确定有限自动机增进大规模语言模型的能力

    Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton

    [https://arxiv.org/abs/2402.04411](https://arxiv.org/abs/2402.04411)

    本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。

    

    本文介绍了一种新颖的框架——确定有限自动机增强的大规模语言模型（DFA-LLM），旨在通过使用大规模语言模型（LLM）提升对话代理的能力。传统的LLM在特定情景（如情感支持和客户服务）中生成规范合规的回复面临挑战。我们的框架通过将从训练对话中学习到的确定有限自动机（DFA）嵌入到LLM中来应对这些挑战。这种结构化的方法使得LLM能够按照DFA指导的确定性回应路径来回应。DFA-LLM的优势包括可解释性结构，上下文感知的对话回复检索以及与现有LLM的即插即用兼容性。广泛的基准测试验证了DFA-LLM的有效性，表明它有潜力成为对话代理领域的有价值的贡献。

    This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
    
[^90]: 连接点：协作微调黑盒视觉语言模型

    Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models

    [https://arxiv.org/abs/2402.04050](https://arxiv.org/abs/2402.04050)

    本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。

    

    随着预训练的视觉语言模型（VLMs）的出现，人们在将其用于下游任务时投入了相当大的努力。尽管在设计高效的微调方法方面取得了进展，但这些方法需要访问模型的参数，而对于保护模型所有权，模型所有者通常选择将其作为黑盒提供。本文提出了一种协作微调（CraFT）方法，用于将黑盒VLMs fine-tuning到下游任务中，其中只能访问模型的输入提示和输出预测。CraFT包括两个模块，一个用于学习文本提示的提示生成模块，一个用于以残差方式增强输出预测的预测优化模块。此外，我们引入了一个辅助的预测一致性损失来促进这些模块之间的一致优化。这些模块通过一种新的协作训练算法进行优化。

    With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
    
[^91]: SMOTE的理论和实验研究：关于重新平衡策略的限制和比较

    Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies

    [https://arxiv.org/abs/2402.03819](https://arxiv.org/abs/2402.03819)

    SMOTE是一种处理不平衡数据集的常用重新平衡策略，它通过复制原始少数样本来重新生成原始分布。本研究证明了SMOTE的密度在少数样本分布的边界附近逐渐减小，从而验证了BorderLine SMOTE策略的合理性。此外，研究还提出了两种新的SMOTE相关策略，并与其他重新平衡方法进行了比较。最终发现，在数据集极度不平衡的情况下，SMOTE、提出的方法或欠采样程序是最佳的策略。

    

    SMOTE（Synthetic Minority Oversampling Technique）是处理不平衡数据集常用的重新平衡策略。我们证明了在渐进情况下，SMOTE（默认参数）通过简单复制原始少数样本来重新生成原始分布。我们还证明了在少数样本分布的支持边界附近，SMOTE的密度会减小，从而验证了常见的BorderLine SMOTE策略。随后，我们提出了两种新的SMOTE相关策略，并将它们与现有的重新平衡方法进行了比较。我们发现，只有当数据集极度不平衡时才需要重新平衡策略。对于这种数据集，SMOTE、我们提出的方法或欠采样程序是最佳的策略。

    Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
    
[^92]: 变分自动编码器进行异常检测的统计测试

    Statistical Test for Anomaly Detections by Variational Auto-Encoders

    [https://arxiv.org/abs/2402.03724](https://arxiv.org/abs/2402.03724)

    本研究提出了一种利用变分自动编码器进行异常检测的统计测试方法（VAE-AD测试），通过量化异常区域的可靠性，可以控制误检的概率到所期望的水平。

    

    在本研究中，我们考虑使用变分自动编码器（VAE）进行异常检测（AD）的可靠性评估。在过去的十年里，基于VAE的AD已经在各个角度进行了积极的研究，从方法开发到应用研究。然而，当AD的结果用于高风险的决策时，如医学诊断，需要确保检测到的异常的可靠性。在本研究中，我们提出了VAE-AD测试作为在统计检验框架下量化基于VAE的AD的统计可靠性的方法。利用VAE-AD测试，可以以p值的形式量化VAE检测到的异常区域的可靠性。这意味着，如果在p值低于某个阈值时宣布为异常，则可以将误检的概率控制在所期望的水平。由于VAE-AD测试是基于一种称为选择性推理的新统计推断框架构建的，其有效性是确保被证明的。

    In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is 
    
[^93]: 部分随机的无限深度贝叶斯神经网络

    Partially Stochastic Infinitely Deep Bayesian Neural Networks

    [https://arxiv.org/abs/2402.03495](https://arxiv.org/abs/2402.03495)

    本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。

    

    在本文中，我们提出了一种部分随机的无限深度贝叶斯神经网络，这是一种将部分随机性整合到无限深度神经网络框架中的新型架构。我们的新型架构旨在改善现有架构在训练和推理时间上的计算效率限制。为实现这一目标，我们利用了部分随机性在无限深度极限下的优势，包括全随机性的好处，如鲁棒性、不确定性量化和内存效率，同时改善了它们在训练和推理时间上的计算效率限制。我们提出了多种架构配置，提供了网络设计的灵活性，包括不同的权重划分方法。我们还通过确立我们的网络家族符合通用条件分布近似器的数学保证，对我们的模型的表达能力进行了证明。

    In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
    
[^94]: 分散式间歇联邦学习：具有广义收敛保证的统一方法

    Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees

    [https://arxiv.org/abs/2402.03448](https://arxiv.org/abs/2402.03448)

    本文提出了一种称为分散式间歇联邦学习（DSpodFL）的方法，它统一了分布式梯度下降（DGD）、随机闲话（RG）和分散式联邦平均（DFedAvg）等著名的分散优化方法。根据分析结果，DSpodFL能够在更一般的假设下达到几何收敛速率与最佳性差距的匹配。经过实验验证了该方法的有效性。

    

    分散式联邦学习（DFL）近来受到了重要的研究关注，涵盖了模型更新和模型聚合这两个关键联邦学习过程都由客户端进行的设置。在本文中，我们提出了分散式间歇联邦学习（DSpodFL），这是一种DFL方法，它在这两个过程中广义化了间歇性的概念，建模了在实际DFL设置中出现的不同形式的异质性的影响。DSpodFL将许多着名的分散优化方法，如分布式梯度下降（DGD），随机闲话（RG）和分散式联邦平均（DFedAvg），统一到一个建模框架下。我们对DSpodFL的收敛行为进行了分析，显示出可以在更一般的假设下，将几何收敛速率与有限的最佳性差距相匹配。通过实验证明：

    Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
    
[^95]: 多智能体强化学习用于协助无人机卸载蜂窝通信

    Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs

    [https://arxiv.org/abs/2402.02957](https://arxiv.org/abs/2402.02957)

    本文提出了一种新颖的方法，通过联合优化无人机轨迹和用户关联指标，最大化用户与无人机的关联，以有效地最大化多个无人机在卸载地面基站的数据流量方面的利用率。

    

    在物联网应用的背景下，有效的解决地面蜂窝网络中的智能数据收集方案至关重要。地面基站的有限频谱和覆盖范围给网络用户的数据率需求带来了挑战。无人机以其高敏捷性、移动性和灵活性而闻名，为卸载地面基站的数据流量提供了另一种手段，成为额外的接入点。本文介绍了一种新颖的方法，以有效地最大化多个无人机在卸载地面基站的数据流量方面的利用率。具体而言，重点是在质量保证约束下，通过联合优化无人机轨迹和用户关联指标，最大化用户与无人机的关联。由于所制定的无人机控制问题是非凸和组合的，本研究利用多智能体强化学习框架。在该框架中，每个无人机以非合作方式寻求从环境中学习，并通过学习来优化自己的策略.

    Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework. In this framework, each UAV a
    
[^96]: PowerGraph: 用于图神经网络的电网基准数据集

    PowerGraph: A power grid benchmark dataset for graph neural networks

    [https://arxiv.org/abs/2402.02827](https://arxiv.org/abs/2402.02827)

    PowerGraph是一个用于图神经网络的电网基准数据集，旨在通过机器学习模型实现电力网格断电的在线检测。

    

    公共图神经网络（GNN）基准数据集有助于使用GNN，并增强GNN在各个领域中的适用性。目前，社区中缺乏用于GNN应用的电力网格公共数据集。事实上，与其他机器学习技术相比，GNN可以潜在地捕捉到复杂的电力网格现象。电力网格是复杂的工程网络，天然适合于图表示。因此，GNN有潜力捕捉到电力网格的行为，而不用其他机器学习技术。为了实现这个目标，我们开发了一个用于级联故障事件的图数据集，这是导致电力网格断电的主要原因。历史断电数据集稀缺且不完整。通常通过计算昂贵的离线级联故障模拟来评估脆弱性和识别关键组件。相反，我们建议使用机器学习模型进行在线检测。

    Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of
    
[^97]: 异步计划推理中的图增强大型语言模型的研究

    Graph-enhanced Large Language Models in Asynchronous Plan Reasoning

    [https://arxiv.org/abs/2402.02805](https://arxiv.org/abs/2402.02805)

    本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。

    

    异步计划推理具有挑战性，因为它需要顺序和并行规划以优化时间成本。大型语言模型(LLMs)能在这个任务中成功吗？在这里，我们进行了第一次大规模研究来探讨这个问题。我们发现一组代表性的闭源和开源LLMs，包括GPT-4和LLaMA-2，在我们的基准测试AsyncHow中，在没有提供任务解决过程的插图的情况下表现不佳。我们提出了一种称为Plan Like a Graph (PLaG)的新技术，它将图与自然语言提示相结合，实现了最先进的结果。我们展示了虽然PLaG能提升模型性能，但在任务复杂性增加时，LLMs仍然遭受严重降级，突出了利用LLMs模拟数字设备的局限性。我们将我们的研究视为将LLMs用作高效自主代理的一个令人兴奋的步骤。

    Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
    
[^98]: KS-Lottery: 寻找多语言语言模型中的认证彩票

    KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models

    [https://arxiv.org/abs/2402.02801](https://arxiv.org/abs/2402.02801)

    KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。

    

    彩票票证假说认为在随机初始化的神经网络中存在“中奖票”。在微调场景中，语言模型中是否存在中奖票？我们如何找到这样的中奖票？在本文中，我们提出了KS-Lottery，一种用于识别在多语言微调中高度有效的LLM参数的方法。我们的核心思想是使用Kolmogorov-Smirnov检验来分析微调前后参数的分布偏移。我们进一步理论证明了KS-Lottery可以在嵌入层中找到认证的中奖票，微调这些参数可以保证与全面微调相同的性能。通过在翻译任务上将KS-Lottery与其他参数高效调优算法进行比较，实验结果表明，KS-Lottery找到了一个更小的参数集来进行微调，同时达到了与全面微调LLM相当的性能。令人惊讶的是，我们发现微调18个标记的嵌入层

    The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
    
[^99]: 一篇位置论文: 大语言模型对时间序列分析有什么启示

    Position Paper: What Can Large Language Models Tell Us about Time Series Analysis

    [https://arxiv.org/abs/2402.02713](https://arxiv.org/abs/2402.02713)

    大语言模型有潜力颠覆时间序列分析，提升决策效率，推动时间序列分析智能的普及化。这种进展可以带来模态切换和时间序列问答等多种可能性。

    

    时间序列分析对于理解各种现实世界系统和应用中的复杂性至关重要。尽管大语言模型（LLM）最近取得了显著进展，但具备时间序列分析能力的人工通用智能（AGI）的发展仍处于初级阶段。目前大部分时间序列模型主要依赖领域知识和大量模型调整，主要集中在预测任务上。本文提出，目前的LLM具有颠覆时间序列分析的潜力，从而促进高效的决策和推进向更普适形式的时间序列分析智能发展。这种进步可以打开各种可能性，包括模态切换和时间序列问答。我们鼓励研究人员和实践者认识到LLM在推进时间序列分析方面的潜力，并强调对这些相关工作的信任需求。

    Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
    
[^100]: 通过重复使用经过验证的电路增加语言模型的可信度

    Increasing Trust in Language Models through the Reuse of Verified Circuits

    [https://arxiv.org/abs/2402.02619](https://arxiv.org/abs/2402.02619)

    本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。

    

    语言模型（LMs）在各种预测任务中的应用越来越广泛，但它们的训练经常忽略罕见的边界情况，降低了它们的可靠性。在本文中，我们定义了一个严格的可信度标准，即任务算法和电路实现必须经过验证，考虑到边界情况，并且没有已知的故障模式。我们展示了通过使用数学和逻辑规范的框架来构建变压器模型，可以训练出满足这一标准的模型。在本文中，我们对一个n位整数加法模型进行了完全验证。为了展示经过验证的模块的重复使用性，我们将训练好的整数加法模型插入到一个未经训练的模型中，并训练组合模型同时执行加法和减法。我们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了更复杂的减法模型的验证。我们讨论了如何将经过验证的任务模块插入到语言模型中，以利用模型的重复使用来提高可验证性和可信度。

    Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
    
[^101]: Weisfeiler Leman用于欧几里得等变机器学习

    Weisfeiler Leman for Euclidean Equivariant Machine Learning

    [https://arxiv.org/abs/2402.02484](https://arxiv.org/abs/2402.02484)

    本文扩展了2-WL测试的适用范围，包括点云中的位置和速度，并提出了一种简单修改的PPGN架构，以获得一个可近似所有连续等变函数的通用等变架构。

    

    k-Weisfeiler Leman (k-WL)图同构测试层次结构是评估图神经网络(GNNs)表达能力的常用方法。最近，证明了2-WL测试在编码3D点云数据的加权图上是完备的。因此，具有与2-WL测试等价的表达能力的GNNs可以被证明在点云上是通用的。然而，这个结果仅限于点云上的不变连续函数。本文通过三个方面对这一结果进行了扩展:首先，我们展示了2-WL测试可以扩展到包括位置和速度的点云，这在应用中经常遇到。其次，我们展示了PPGN (Maron等人，2019)可以在低复杂度下在所有点云上一致地模拟2-WL。最后，我们展示了对这个PPGN架构的简单修改可以用来获得一个可近似所有连续等变函数的通用等变架构。构建

    The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds.   In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   Building
    
[^102]: 通过大型语言模型（LLMs）发现更有效的张量网络结构搜索算法

    Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)

    [https://arxiv.org/abs/2402.02456](https://arxiv.org/abs/2402.02456)

    通过大型语言模型（LLMs），我们开发了GPTN-SS算法，用于自动设计更有效的张量网络结构搜索算法。实验证明，这些算法在探索和开发之间取得了更好的平衡，并在搜索高质量的TN结构方面展现出卓越的性能。

    

    张量网络结构搜索（TN-SS）旨在搜索适合表示高维问题的张量网络（TN）结构，极大地促进了TN在各种机器学习应用中的效果。然而，使用现有算法找到满意的TN结构仍然具有挑战性。为了开发更有效的算法并避免人力密集型的开发过程，我们利用大型语言模型（LLMs）中嵌入的知识来自动设计TN-SS算法。我们的方法称为GPTN-SS，利用了一种精心设计的基于LLM的提示系统，以类似进化的方式运行。从真实数据中得出的实验结果表明，GPTN-SS可以有效地利用现有方法获得的见解，开发出更好地平衡探索和开发之间关系的新型TN-SS算法。这些算法在搜索高质量TN结构方面表现出优秀的性能。

    Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structur
    
[^103]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^104]: Transolver：一种用于一般几何体上求解偏微分方程的快速Transformer求解器

    Transolver: A Fast Transformer Solver for PDEs on General Geometries

    [https://arxiv.org/abs/2402.02366](https://arxiv.org/abs/2402.02366)

    Transolver是一种快速的Transformer求解器，通过使用物理注意力和灵活形状的片段来学习离散化几何形状背后隐藏的内在物理状态。它能够有效地捕捉复杂几何形状下的复杂物理相关性，从而提供了具备内生几何生成能力的求解器。

    

    Transformer已在各个领域实现了很多里程碑式的成就，最近开始应用于求解偏微分方程（PDEs）。然而，由于PDEs通常被离散化成具有复杂几何形状的大规模网格，对Transformer来说直接从大量单个点中捕捉复杂的物理相关性是具有挑战性的。为了超越肤浅而笨重的网格，我们基于一个更基础的思想提出了Transolver，即学习离散化几何形状背后隐藏的内在物理状态。具体来说，我们提出了一种新的物理注意力机制，将离散化的域自适应地划分为一系列可学习的灵活形状的片段，具有相似物理状态的网格点将被归属于同一个片段。通过计算从片段编码的具有物理意识的记号的注意力，Transovler能够有效地捕捉复杂几何形状下的复杂物理相关性，从而使求解器具备内生几何生成的能力。

    Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-genera
    
[^105]: 图机器学习基础的未来方向

    Future Directions in Foundations of Graph Machine Learning

    [https://arxiv.org/abs/2402.02287](https://arxiv.org/abs/2402.02287)

    图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。

    

    随着图数据在不同学科（从生命科学到社会科学和工程科学）上的广泛应用，图机器学习，尤其是使用图神经网络（GNNs），引起了人们浓厚的兴趣。尽管在实际应用中取得了成功，但我们对GNNs性质的理论理解仍然非常不完整。最近的理论发展主要集中在阐明GNNs粗粒度表达能力方面，主要采用组合技巧。然而，这些研究与实践并不完全一致，特别是在使用随机一阶优化技术训练GNNs时，对GNNs的泛化行为的理解。在这篇定位论文中，我们认为图机器学习领域需要将注意力转移到发展一个更加均衡的图机器学习理论上来，重点关注表达能力、泛化和优化的相互关系的更全面的理解。

    Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
    
[^106]: 重思出发点：通过协作预训练增强联邦学习的性能和公平性

    Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training

    [https://arxiv.org/abs/2402.02225](https://arxiv.org/abs/2402.02225)

    本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。

    

    大多数现有的联邦学习方法假设训练从一个随机初始化的模型开始。最近的研究实证了利用预训练模型可以为联邦学习提供有益的初始化。在本文中，我们提出了一种协作预训练方法CoPreFL，该方法通过策略性地设计一个预训练模型，为任何下游联邦学习任务提供良好的初始化。我们的预训练算法的关键思想是模仿下游分布式场景的元学习过程，使其能够适应任何未知的联邦学习任务。CoPreFL的预训练优化过程也在平均性能和公平性之间取得了平衡，旨在通过智能初始化来解决下游联邦学习任务中的竞争挑战。大量实验结果验证了我们的预训练方法为任何未知的下游联邦学习任务提供了可靠的初始化，从而提高了平均性能。

    Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe
    
[^107]: 改进无需重训练的传播模型用于逆问题，使用最优后验协方差

    Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance

    [https://arxiv.org/abs/2402.02149](https://arxiv.org/abs/2402.02149)

    本文提出了一种改进无需重训练的传播模型的方法，通过优化后验协方差，提供了一种零样本解决方案，用于嘈杂的线性逆问题。根据最近的方法等价于对给定扩散噪声图像的干净图像的不可计算后验分布进行各向同性高斯近似的发现，我们提出了一种通用即插即用的后验协方差优化方法。为了实现无需重新训练的最优后验协方差，我们提供了基于两种方法的通用解决方案，这两种方法专门设计用于利用具有和不具有反向协方差的预训练模型。

    

    最近的传播模型为嘈杂的线性逆问题提供了有希望的零样本解决方案，无需为特定的逆问题重新训练。本文从条件抽样的反向传播过程中近似后验均值的角度，提出了对现有零样本方法的第一个统一解释。我们揭示了最近的方法等价于对给定扩散噪声图像的干净图像的不可计算后验分布进行各向同性高斯近似，唯一的差别是各向同性后验协方差的手工设计。受到这一发现的启示，我们提出了一种基于最大似然估计的通用即插即用后验协方差优化方法，以改进最近的方法。为了实现无需重新训练的最优后验协方差，我们提供了基于两种方法的通用解决方案，这两种方法专门设计用于利用具有和不具有反向协方差的预训练模型。

    Recent diffusion models provide a promising zero-shot solution to noisy linear inverse problems without retraining for specific inverse problems. In this paper, we propose the first unified interpretation for existing zero-shot methods from the perspective of approximating the conditional posterior mean for the reverse diffusion process of conditional sampling. We reveal that recent methods are equivalent to making isotropic Gaussian approximations to intractable posterior distributions over clean images given diffused noisy images, with the only difference in the handcrafted design of isotropic posterior covariances. Inspired by this finding, we propose a general plug-and-play posterior covariance optimization based on maximum likelihood estimation to improve recent methods. To achieve optimal posterior covariance without retraining, we provide general solutions based on two approaches specifically designed to leverage pre-trained models with and without reverse covariances. Experimen
    
[^108]: X-CBA: 基于可解释性的CatBoosted Anomal-E用于入侵检测系统

    X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System

    [https://arxiv.org/abs/2402.00839](https://arxiv.org/abs/2402.00839)

    本论文介绍了一种名为X-CBA的新颖可解释型IDS方法，该方法利用图神经网络处理网络流量数据，并采用了新的可解释人工智能方法。它通过利用更广泛的流量数据，包括边属性，以处理网络威胁的挑战。

    

    在网络威胁日益复杂的时代，入侵检测系统（IDS）的效果至关重要。机器学习（ML）和深度学习（DL）模型为识别计算机网络中的攻击和异常提供了高效准确的解决方案。然而，在IDS中使用ML和DL模型导致了信任赤字，因为它们的决策过程不透明。这种IDS研究中的透明度差距显著，影响了信心和问责制。为了解决这个问题，本文引入了一种新颖的可解释型IDS方法，称为X-CBA，它利用图神经网络（GNN）的结构优势来有效处理网络流量数据，并采用新的可解释人工智能（XAI）方法。与大多数以GNN为基础的IDS不同，我们的方法不仅依赖于标记的网络流量和节点特征，还通过网络流量，包括边属性，来利用更广泛的流量数据。

    The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
    
[^109]: 《在大规模人工智能时代的贝叶斯深度学习》的立场论文

    Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI

    [https://arxiv.org/abs/2402.00809](https://arxiv.org/abs/2402.00809)

    《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。

    

    在当前的深度学习研究领域中，人们主要关注在涉及大规模图像和语言数据集的监督任务中实现高预测准确性。然而，更广泛的视角揭示了许多被忽视的度量标准、任务和数据类型，如不确定性、主动和持续学习以及科学数据，这些方面需要关注。贝叶斯深度学习（BDL）是一条有前景的道路，可以在这些不同的设置中提供优势。本文认为BDL可以提升深度学习的能力。它重新审视了BDL的优势、承认了现有的挑战，并重点介绍了一些旨在解决这些障碍的有趣的研究方向。展望未来，讨论集中在可能的方式上，将大规模基础模型与BDL相结合，以充分发挥它们的潜力。

    In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
    
[^110]: 在联邦设置中可分解子模函数的最大化

    Decomposable Submodular Maximization in Federated Setting

    [https://arxiv.org/abs/2402.00138](https://arxiv.org/abs/2402.00138)

    该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。

    

    在机器学习、推荐系统和福利最大化等众多应用中，子模函数以及可分解子模函数及其优化问题都得到了广泛应用。然而，对于具有数百万个组分函数的可分解子模函数的优化问题，在计算上是不可行的。此外，组分函数可能是私有的（例如可能表示用户偏好函数），不能广泛共享。为了解决这些问题，我们提出了一种适用于可分解子模函数优化的“联邦优化”设置。在这种设置下，客户端拥有自己的偏好函数，需要最大化这些偏好的加权和。我们在该设置中实现了流行的“连续贪婪”算法，其中客户端以并行的方式朝着局部解向前迈出小的局部步骤，然后将局部变化聚合到一个中央服务器上。

    Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
    
[^111]: 通过定向表示优化实现的安全提示驱动的大型语言模型(LLM)保护

    Prompt-Driven LLM Safeguarding via Directed Representation Optimization

    [https://arxiv.org/abs/2401.18018](https://arxiv.org/abs/2401.18018)

    通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。

    

    在大型语言模型(LLM)中，使用安全提示在模型输入之前是一种常见的保护实践，以使其不遵从包含恶意意图的查询。然而，安全提示的工作机制尚未完全理解，这妨碍了自动优化其以改善LLM安全性的潜力。针对这个问题，我们从模型表示的角度调查了安全提示的影响。我们发现在模型的表示空间中，有害和无害的查询可以在很大程度上区分开来，但安全提示并没有明显增强这一区分。相反，不同安全提示导致查询的表示朝着相似的方向移动，使得模型即使在查询无害时也更容易拒绝提供协助。受到这些发现的启发，我们提出了一种名为DRO（定向表示优化）的方法，用于自动安全提示优化。DRO将安全提示视为要优化的表示方向。

    Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
    
[^112]: 大型语言模型中的时间箭头

    Arrows of Time for Large Language Models

    [https://arxiv.org/abs/2401.17505](https://arxiv.org/abs/2401.17505)

    这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。

    

    我们通过时间方向性的视角研究了自回归大型语言模型的概率建模。我们在实证上发现这类模型在建模自然语言能力上存在时间上的不对称性：预测下一个记号和预测前一个记号时的平均对数困惑度存在差异。这种差异既微妙又在不同的模态（语言、模型大小、训练时间等）下非常一致。从信息理论的角度来看，这在理论上是令人惊讶的，不应该存在这样的差异。我们提供了一个理论框架，解释了这种不对称性如何出现在稀疏性和计算复杂性考虑中，并概述了我们的结果带来的一些展望。

    We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
    
[^113]: 图语言模型

    Graph Language Models

    [https://arxiv.org/abs/2401.07105](https://arxiv.org/abs/2401.07105)

    引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。

    

    虽然语言模型（LMs）是自然语言处理的主力军，它们与结构化知识图谱（KGs）的相互作用仍在积极研究中。当前用于编码这些图形的方法通常要么（i）将它们线性化以供LM嵌入--这样会低效利用结构信息，要么（ii）使用图神经网络（GNNs）来保留图结构--但GNNs无法像预训练的LM一样很好地表示文本特征。在我们的工作中，我们引入了一种新型LM类型，即图语言模型（GLM），它整合了两种方法的优点并减轻了它们的弱点。GLM参数从预训练的LM中初始化，以增强对个别图概念和三元组的理解。同时，我们设计GLM的架构以整合图偏差，从而促进图内的知识分布。这使GLM能够处理图形、文本以及两者的交织输入。实证

    arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
    
[^114]: 具有部分动态知识的样本高效强化学习

    Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge

    [https://arxiv.org/abs/2312.12558](https://arxiv.org/abs/2312.12558)

    研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。

    

    在本文中，我们研究了在线Q学习方法的样本复杂度，当某些关于动态的先前知识可用或可以有效学习时。我们专注于按照加性干扰模型演变的系统，在有限的分集马尔可夫决策过程设置下，我们提出了一种乐观的Q学习算法，在对$f$的完美知识条件下实现了$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$的遗憾，其中$T$是与系统进行交互的总次数。

    arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with
    
[^115]: 公平性约束能够在多大程度上帮助从有偏差的数据中恢复？

    How Far Can Fairness Constraints Help Recover From Biased Data?

    [https://arxiv.org/abs/2312.10396](https://arxiv.org/abs/2312.10396)

    公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。

    

    一般认为，在公平分类中，公平性约束会导致准确性的减少，而有偏差的数据可能会加剧这种情况。然而，Blum＆Stangl（2019）的研究表明，在极度有偏差的数据上，即使采用平等机会约束，也可以恢复到原始数据分布上准确和公平的分类器。他们的研究结果很有趣，因为它证明了公平性约束可以隐式修正数据偏差，同时克服了公平性与准确性之间的平衡问题。他们的数据偏差模型模拟了受压迫人群的表征和标签偏见，并在具有独立标签噪声的简单条件下，针对一个理想化的数据分布展示了上述结果。我们提出了一种通用方法，以扩展Blum＆Stangl（2019）的结果，适用于不同的公平性约束、数据偏差模型、数据分布和假设类别。

    A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum & Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum & Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
    
[^116]: 超图-MLP：在无需消息传递的超图上学习

    Hypergraph-MLP: Learning on Hypergraphs without Message Passing

    [https://arxiv.org/abs/2312.09778](https://arxiv.org/abs/2312.09778)

    提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。

    

    超图在建模包含两个以上实体的高阶关系数据中至关重要，在机器学习和信号处理中越来越受重视。许多超图神经网络利用在超图结构上的消息传递来增强节点表征学习，从而在超图节点分类等任务中取得了令人印象深刻的表现。然而，这些基于消息传递的模型面临着过度平滑以及在推理时对结构扰动的高延迟和敏感性等挑战。为了应对这些挑战，我们提出了一种另类方法，即将关于超图结构的信息集成到训练监督中，而无需明确的消息传递，从而在推理时也消除了对其的依赖。具体而言，我们引入了Hypergraph-MLP，一种新颖的用于超图结构数据的学习框架，其中学习模型是一个简单的多层感知机。

    arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p
    
[^117]: 利用指数尺度的深度强化ReLU网络初始化和训练

    Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth

    [https://arxiv.org/abs/2311.18022](https://arxiv.org/abs/2311.18022)

    该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。

    

    ReLU激活的神经网络可以看作是分段线性函数的组合。对于这样的网络，随着深度的增加，表达在输入域上的不同线性区域的数量有可能以指数级增长，但当初始参数选择随机时，不太可能出现这种情况。这种不良的尺度能够导致即使是简单函数也需要使用过大的模型来近似。为了解决这个问题，我们引入了一种新的训练策略：首先以一种方式重新参数化网络权重，使得指数数量的激活模式得以展现。在这些新参数上进行训练可以得到一个初始解，稍后通过更新底层模型权重来改进。这种方法使我们能够产生比随机初始化对应的函数逼近好几个数量级的结果。

    A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
    
[^118]: InteRACT：基于机器人动作的人类意图预测的Transformer模型

    InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions

    [https://arxiv.org/abs/2311.12943](https://arxiv.org/abs/2311.12943)

    InteRACT通过在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上微调，解决了人机交互中的先有鸡还是先有蛋问题。

    

    在协作的人机操纵中，机器人必须预测人类意图并相应调整其行动，以平稳执行任务。然而，人类的意图反过来又取决于机器人采取的动作，造成了一个先有鸡还是先有蛋的问题。先前的方法忽略了这种相互依赖关系，而是训练独立于机器人行动的边际意图预测模型。这是因为在缺乏配对的人机交互数据集的情况下，训练条件模型是困难的。我们能否转而利用更容易获取的大规模人类-人类交互数据？我们的关键见解是利用人类和机器人行动之间的对应关系，实现从人类-人类到人类-机器人数据的迁移学习。我们提出了一种新颖的架构InteRACT，该架构在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上进行微调。我们在一组真实世界的协作数据上进行评估。

    arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
    
[^119]: 关于去中心化双层优化的通信复杂度

    On the Communication Complexity of Decentralized Bilevel Optimization

    [https://arxiv.org/abs/2311.11342](https://arxiv.org/abs/2311.11342)

    本研究针对去中心化双层优化的通信复杂度问题，提出了一种新颖的去中心化随机双层梯度下降算法，在异构设置下具有较小的通信成本和轮次，并实现了比现有算法更好的通信复杂度。

    

    过去几年中，去中心化双层优化在机器学习中得到了广泛研究，因为它在现实世界任务中有着广泛的应用。然而，现有算法由于估计随机超梯度而导致通信复杂度较大，限制了它们在实际任务中的应用。为了解决这个问题，我们在异构设置下开发了一种新颖的去中心化随机双层梯度下降算法，每轮中具有较小的通信成本和较少的通信轮次。因此，它可以在没有任何关于异构性的强假设的情况下实现比现有算法更好的通信复杂度。据我们所知，这是第一个在异构设置下实现这些理论结果的随机算法。最后，实验结果证实了我们算法的有效性。

    arXiv:2311.11342v2 Announce Type: replace Abstract: Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and a small number of communication rounds. As such, it can achieve a much better communication complexity than existing algorithms without any strong assumptions regarding heterogeneity. To the best of our knowledge, this is the first stochastic algorithm achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
    
[^120]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^121]: 关于衡量自然语言解释的忠诚度或自一致性

    On Measuring Faithfulness or Self-consistency of Natural Language Explanations

    [https://arxiv.org/abs/2311.07466](https://arxiv.org/abs/2311.07466)

    本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。

    

    大型语言模型（LLMs）可以通过事后或思维链（CoT）解释其预测。但是，LLM可能会编造听起来合理但不忠实于其基本推理的解释。最近的工作设计了旨在判断事后或CoT解释忠实度的测试。在这项工作中，我们认为这些忠实度测试不是衡量模型内部工作的忠实度，而是衡量其输出级别的自一致性。我们的贡献有三个方面：i）我们在模型可解释性的背景下澄清了忠实度测试的地位，将其描述为自一致性测试。我们通过ii）构建了一个比较一致性的测试库，首次在11个开放式LLMs和5个任务的通用套件上比较了现有测试，包括iii）我们的新的自一致性度量CC-SHAP。CC-SHAP是LLM自一致性的细粒度度量（而不是测试）。它进行比较。

    Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
    
[^122]: 通过正常结构规范化实现开放图异常检测

    Open-Set Graph Anomaly Detection via Normal Structure Regularisation

    [https://arxiv.org/abs/2311.06835](https://arxiv.org/abs/2311.06835)

    通过正常结构规范化方法，实现开放图异常检测模型对未知异常的广义检测能力

    

    本文考虑了一个重要的图异常检测（GAD）任务，即开放式GAD，旨在使用少量标记的训练正常节点和异常节点（称为已知异常）来检测异常节点，这些节点无法展示所有可能的推理时异常。已标记数据的可用性为GAD模型提供了关键的异常先验知识，可大大降低检测错误。然而，当前方法往往过分强调拟合已知异常，导致对未知异常（即未被标记的异常节点）的弱泛化能力。此外，它们被引入以处理欧几里德数据，未能有效捕捉GAD的重要非欧几里德特征。在这项工作中，我们提出了一种新颖的开放式GAD方法，即正常结构规范化（NSReg），以实现对未知异常的广义检测能力。

    arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
    
[^123]: 重新审视假设：预训练的Transformer是否通过梯度下降在上下文中学习？

    Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?

    [https://arxiv.org/abs/2310.08540](https://arxiv.org/abs/2310.08540)

    本研究重新审视了预训练的Transformer是否通过梯度下降在上下文中学习的假设，并发现现有研究中的假设存在限制性假设，使其与实际语言模型训练时的语境存在显著差异。同时，通过对真实模型的观察和比较，揭示了ICL和GD在观察演示顺序上的不同敏感性。

    

    LLM中的In-Context Learning（ICL）的出现仍然是一个重要现象，但我们对其了解甚少。为了解释ICL，最近的研究尝试在理论上将其与梯度下降（GD）联系起来。我们问，这种联系在实际预训练模型中是否成立？我们强调先前作品中的限制性假设使得它们的语境与语言模型实际训练时的实际语境差别很大。例如，这些研究中使用的理论手工构造的权重具有与真实LLM不匹配的属性。此外，他们的实验验证使用ICL目标（明确为ICL训练模型），这与野外出现的ICL有所不同。我们还寻找了真实模型中的证据。我们观察到ICL和GD对于观察演示的顺序有不同的敏感性。最后，我们在自然环境中探讨并比较ICL与GD假设。

    arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
    
[^124]: 数据偏差调查中的特征重要性差异

    Feature Importance Disparities for Data Bias Investigations

    [https://arxiv.org/abs/2303.01704](https://arxiv.org/abs/2303.01704)

    本文介绍了一种新的方法，通过给定一个数据集，利用特征重要性差异（FID）来调查数据偏差，并展示了实验证据支持该方法的有效性。

    

    广泛认为，分类器中的下游偏差的一种原因是训练数据中存在的偏差。纠正这种偏差可能涉及到依赖于上下文的干预措施，例如在子集上训练单独的模型，在收集过程中删除具有偏差的特征，甚至进行真实世界的实验以确定偏差源。尽管需要进行这样的数据偏差调查，但目前很少有自动化方法可以辅助从业人员进行这些工作。在本文中，我们提出了一种给定数据集$X$，包括保护和不保护的特征，结果$y$，以及一个预测给定$X$的回归器$h$的元组$(f_j, g)$, 其中$g$对应于训练数据集$(X, y)$的子集，使得第$j$个特征$f_j$在子组$g$中的影响要比整体数据集中大得多（或者小得多），我们将其称为特征重要性差异（FID）。我们在4个数据集和4个常见特征重要性中展示了这一点。

    It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature import
    
[^125]: PyGOD: 一个用于图异常检测的Python库

    PyGOD: A Python Library for Graph Outlier Detection

    [https://arxiv.org/abs/2204.12095](https://arxiv.org/abs/2204.12095)

    PyGOD是一个开源Python库，支持多种领先的基于图的异常检测方法，提供了易于使用的API和丰富的实用程序函数，同时采用了最佳的代码可靠性和可维护性实践。

    

    PyGOD是一个用于检测图数据中异常值的开源Python库。作为这类库中首个综合性工具，PyGOD支持多种领先的基于图的异常检测方法，提供了易于使用、有详细文档支持的API，旨在供研究人员和从业者使用。PyGOD提供了不同检测器的模块化组件，使用户可以轻松定制每个检测器以适应其用途。为简化检测工作流的构建，PyGOD提供了许多常用的实用程序函数。为了将计算扩展到大型图形，PyGOD支持深度模型的功能，如采样和小批量处理。PyGOD采用了促进代码可靠性和可维护性的最佳实践，包括单元测试、持续集成和代码覆盖。为了方便访问，PyGOD以BSD 2-Clause许可证发布在https://pygod.org 和 Python包中。

    arXiv:2204.12095v2 Announce Type: replace  Abstract: PyGOD is an open-source Python library for detecting outliers in graph data. As the first comprehensive library of its kind, PyGOD supports a wide array of leading graph-based methods for outlier detection under an easy-to-use, well-documented API designed for use by both researchers and practitioners. PyGOD provides modularized components of the different detectors implemented so that users can easily customize each detector for their purposes. To ease the construction of detection workflows, PyGOD offers numerous commonly used utility functions. To scale computation to large graphs, PyGOD supports functionalities for deep models such as sampling and mini-batch processing. PyGOD uses best practices in fostering code reliability and maintainability, including unit testing, continuous integration, and code coverage. To facilitate accessibility, PyGOD is released under a BSD 2-Clause license at https://pygod.org and at the Python Packa
    
[^126]: 在线资源分配与非平稳顾客

    Online Resource Allocation with Non-Stationary Customers. (arXiv:2401.16945v1 [cs.LG])

    [http://arxiv.org/abs/2401.16945](http://arxiv.org/abs/2401.16945)

    本文提出了一种用于在线资源分配的新算法，适用于非平稳顾客到达和未知的点击率。通过充分利用随机上下文摇臂和具有对抗性到达的在线匹配的结果，我们的方案实现了在顾客到达接近平稳时具有次线性遗憾，并在一般（非平稳）顾客到达分布下享受最优的竞争比率。我们通过大量的数值实验证明了我们的方法在各种不同的顾客场景下生成接近最优的收益。

    

    本文提出了一种用于在线资源分配的新算法，适用于非平稳顾客到达和未知的点击率。我们假设多种类型的顾客以非平稳随机方式到达，每个时期都有未知的到达率，并且顾客的点击率未知，只能在线学习。通过利用基于随机上下文摇臂和具有对抗性到达的在线匹配的结果，我们开发了一个在线方案，将资源分配给非平稳顾客。我们证明，在温和条件下，我们的方案实现了“两全其美”的效果：当顾客到达接近平稳时，方案具有次线性遗憾，并在一般（非平稳）顾客到达分布下享受最优的竞争比率。最后，我们进行了大量的数值实验，展示了我们的方法在所有不同顾客场景下生成接近最优的收益。

    We propose a novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates. We assume multiple types of customers arrive in a nonstationary stochastic fashion, with unknown arrival rates in each period, and that customers' click-through rates are unknown and can only be learned online. By leveraging results from the stochastic contextual bandit with knapsack and online matching with adversarial arrivals, we develop an online scheme to allocate the resources to nonstationary customers. We prove that under mild conditions, our scheme achieves a ``best-of-both-world'' result: the scheme has a sublinear regret when the customer arrivals are near-stationary, and enjoys an optimal competitive ratio under general (non-stationary) customer arrival distributions. Finally, we conduct extensive numerical experiments to show our approach generates near-optimal revenues for all different customer scenarios.
    
[^127]: 使用SHAP和LIME进行可证明稳定的特征排名

    Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])

    [http://arxiv.org/abs/2401.15800](http://arxiv.org/abs/2401.15800)

    这项研究提出了一种通过利用多重假设检验的思想，来设计可靠地排名机器学习模型中最重要特征的特征归因方法，旨在解决SHAP和LIME等常用方法由于随机采样导致的高度不稳定性问题。实验证明了该方法的有效性和计算效率。

    

    特征归因是了解机器学习模型预测的普遍工具。然而，用于评分输入变量的常用方法，如SHAP和LIME，由于随机采样而具有高度不稳定性。借鉴多重假设检验的思想，我们设计了能够以高概率正确排名最重要特征的归因方法。我们的算法RankSHAP保证$K$个最高Shapley值具有超过$1-\alpha$的正确排序概率。实证结果证明了其有效性和令人印象深刻的计算效率。我们还在之前的工作基础上为LIME提供了类似的结果，确保以正确顺序选择最重要的特征。

    Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
    
[^128]: 元学习线性二次调节器: 一种针对无模型LQR的策略梯度MAML方法

    Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR. (arXiv:2401.14534v1 [math.OC])

    [http://arxiv.org/abs/2401.14534](http://arxiv.org/abs/2401.14534)

    本论文研究了在多任务、异构和无模型环境下学习线性二次调节器（LQR）的问题，提出了一种基于策略梯度元学习（MAML）方法的解决方案。该方法能够产生与每个任务特定的最优控制器接近的控制器，并在模型基础设置下以线性收敛速率实现。

    

    我们研究在多任务、异构和无模型环境中学习线性二次调节器（LQR）的问题。我们对一种基于策略梯度的模型不可知元学习（MAML）方法（Finn等人，2017）在不同任务异质性设置下的LQR问题的稳定性和个性化保证进行了刻画。我们证明在模型基础和无模型设置下，MAML-LQR方法产生的控制器与每个任务特定的最优控制器接近，除了任务异质性偏差。此外，我们还展示了在模型基础设置下，这种控制器以线性收敛速率实现，这改进了现有MAML-LQR工作中的次线性速率。与现有的MAML-LQR结果相比，我们的理论保证表明学习到的控制器可以高效地适应未知的LQR任务。

    We investigate the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a Policy Gradient-based (PG) Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that the MAML-LQR approach produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias for both model-based and model-free settings. Moreover, in the model-based setting, we show that this controller is achieved with a linear convergence rate, which improves upon sub-linear rates presented in existing MAML-LQR work. In contrast to existing MAML-LQR results, our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.
    
[^129]: 建模三维动力学的等变图神经操作器

    Equivariant Graph Neural Operator for Modeling 3D Dynamics. (arXiv:2401.11037v1 [cs.LG])

    [http://arxiv.org/abs/2401.11037](http://arxiv.org/abs/2401.11037)

    本文提出了一种新的等变图神经操作器（EGNO）方法，能够直接将动力学建模为轨迹而不仅仅是下一步预测，以准确捕捉时间相关性，并利用等变时间卷积来保持其内在的等变性。

    

    在自然科学中，建模复杂的三维关系系统动力学是一个重要问题，涉及领域从分子模拟到粒子力学。机器学习方法通过学习图神经网络来建模空间相互作用已取得良好成功。然而，这些方法只能进行下一步预测，不能准确捕捉时间相关性。本文提出了一种新颖而有原则的方法，即等变图神经操作器（EGNO），直接将动力学建模为轨迹而不仅仅是下一步预测。与现有方法不同，EGNO显式地学习三维动力学的时间演化，通过时间函数来建模动力学并学习神经操作器来近似。为了捕捉时间相关性，同时保持内在的SE(3)等变性，我们在傅里叶空间中参数化等变时间卷积，并通过堆叠卷积层构建EGNO。

    Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stac
    
[^130]: 具有结构化变分族的可证伸缩性黑盒变分推断

    Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])

    [http://arxiv.org/abs/2401.10989](http://arxiv.org/abs/2401.10989)

    本文研究了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族，并通过理论证明结构化变分族可以在迭代复杂性上表现更好，缩放效果更好。

    

    已知具有满秩协方差逼近的变分族在黑盒变分推断中表现不佳，无论是从实证上还是理论上。事实上，最近对黑盒变分推断的计算复杂性结果表明，与均值场变分族相比，满秩变分族在问题的维度上扩展得很差。这对具有本地变量的分层贝叶斯模型尤为关键，它们的维度随着数据集的大小而增加。因此，迭代复杂性对数据集大小N存在明确的O(N^2)依赖。在本文中，我们探索了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族。我们严格证明了某些尺度矩阵结构可以实现更好的迭代复杂性O(N)，从而与N的缩放更好地匹配。我们在现实中验证了我们的理论结果

    Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
    
[^131]: E$^{2}$GAN: 高效训练图像到图像转换任务的高效GANs

    E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])

    [http://arxiv.org/abs/2401.06127](http://arxiv.org/abs/2401.06127)

    本论文旨在提出一种高效的方法来从扩散模型中提炼GANs，并用于图像到图像的转换任务。这种方法可以实现灵活的实时图像编辑，并显著降低训练不同概念模型的成本。

    

    为了实现灵活的实时设备上图像编辑，一种高度有希望的方法是利用大规模文本到图像扩散模型，例如稳定扩散 (Stable Diffusion)，生成用于训练生成对抗网络 (GANs) 的配对数据集。这种方法显著减轻了使用扩散模型进行图像编辑时通常由高端商用GPU特定的严格要求。然而，与文本到图像扩散模型不同，每个生成的 GAN 都专门用于特定的图像编辑任务，因此需要昂贵的训练工作来获得各种概念的模型。在这项工作中，我们引入并解决了一个新颖的研究方向：能否使从扩散模型中提炼 GANs 的过程更加高效？为了实现这一目标，我们提出了一系列创新技术。首先，我们构建了一个具有广义特征的基本 GAN 模型，通过微调适应不同的概念，消除了...

    One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
    
[^132]: 重写代码：一种用于大型语言模型增强代码搜索的简单方法

    Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])

    [http://arxiv.org/abs/2401.04514](http://arxiv.org/abs/2401.04514)

    本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。

    

    在代码搜索中，生成增强检索（GAR）框架是一种有前景的策略，通过生成示例代码片段来增强查询，以解决代码片段和自然语言查询之间的主要模态不匹配问题，尤其是在大型语言模型（LLM）展示了代码生成能力的情况下。然而，我们的初步调查发现，LLM增强框架所提供的改进有一定的限制。这种限制可能是因为生成的代码，尽管在功能上准确，但在代码库中与基准代码之间经常显示出明显的风格偏差。在本文中，我们扩展了基础GAR框架，并提出了一种简单而有效的方法，通过对代码库中的代码进行重写（ReCo）来进行风格规范化。实验结果表明，ReCo显著提高了检索准确性。

    In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
    
[^133]: 在具有激励兼容性的多对一匹配市场中改进赌博算法

    Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility. (arXiv:2401.01528v1 [cs.LG])

    [http://arxiv.org/abs/2401.01528](http://arxiv.org/abs/2401.01528)

    本文研究了在具有激励兼容性的多对一匹配市场中改进赌博算法的问题，并提出了适应性探索-延迟接受（AETDA）算法来提高遗憾上限。

    

    由于丰富的应用，双边匹配市场在文献中得到了广泛研究。由于参与者通常对自己的偏好不确定，最近采用在线算法通过迭代交互来学习偏好。然而，现有研究在多对一设置中的结果远非最优，并缺乏激励兼容性的保证。本文针对多对一市场在提高遗憾上限的同时确保激励兼容性的问题进行研究。我们首先提出了适应性探索-延迟接受（AETDA）算法，用于响应性设置。

    Two-sided matching markets have been widely studied in the literature due to their rich applications. Since participants are usually uncertain about their preferences, online algorithms have recently been adopted to learn them through iterative interactions. \citet{wang2022bandit} initiate the study of this problem in a many-to-one setting with \textit{responsiveness}. However, their results are far from optimal and lack guarantees of incentive compatibility. An extension of \citet{kong2023player} to this more general setting achieves a near-optimal bound for player-optimal regret. Nevertheless, due to the substantial requirement for collaboration, a single player's deviation could lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for others. In this paper, we aim to enhance the regret bound in many-to-one markets while ensuring incentive compatibility. We first propose the adaptively explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
    
[^134]: Pontryagin神经算子用于解决参数化普通和差分博弈

    Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games. (arXiv:2401.01502v1 [cs.LG])

    [http://arxiv.org/abs/2401.01502](http://arxiv.org/abs/2401.01502)

    本文提出了一种Pontryagin模式的神经算子，通过在前向和反向共轭状态回滚之间的差异上定义的损失，解决了在具有参数化状态约束的博弈中值的不连续性的收敛问题，并在安全性能上优于现有的最先进算法。

    

    两个玩家的普通和差分博弈的值是Hamilton-Jacobi-Isaacs（HJI）方程的粘性解。这种博弈的值和策略逼近受到维度诅咒（CoD）的影响。通过物理信息神经网络（PINN）减轻CoD时，由于状态约束引起的值的不连续性，会遇到收敛问题。除了这些挑战之外，在对博弈参数空间进行学习时（例如，在信息不完整时进行博弈参数推断），通常需要学习可推广的值和策略。为了应对这些挑战，我们在本文中提出了一种Pontryagin模式的神经算子，在具有参数化状态约束的博弈中胜过现有的最先进算法（SOTA）。我们的关键贡献在于引入了一种在前向和反向共轭状态回滚之间的差异上定义的共轭状态损失，这种损失计算成本低廉。我们证明了共轭状态动力学的不连续性在该算子的性能改进中的重要性。

    The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy approximations for such games suffer from the curse of dimensionality (CoD). Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints. On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete. To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints. Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap. We show that the discontinuity of costate dynamics (in th
    
[^135]: 连续学习: 面向视频表示的免遗忘优胜子网络

    Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.11973](http://arxiv.org/abs/2312.11973)

    本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。

    

    受到"彩票票据假设"（LTH）的启发，该假设强调在较大的密集网络中存在高效子网络，研究了在适当的稀疏条件下表现优秀的优胜子网络（WSN）在各种连续学习任务中的应用。它利用来自密集网络的预先存在的权重，在任务增量学习（TIL）场景中实现高效学习。在少样本类增量学习（FSCIL）中，设计了一种称为软子网络（SoftNet）的WSN变体，以防止数据样本稀缺时的过拟合。此外，考虑了WSN权重的稀疏重用，用于视频增量学习（VIL）。考虑了在WSN中使用傅立叶子神经运算器（FSO），它能够对视频进行紧凑编码，并在不同带宽下识别可重用的子网络。我们将FSO集成到不同的连续学习架构中，包括VIL、TIL和FSCIL。

    Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
    
[^136]: 特征引导：大尺度导向下扩散模型的非线性校正

    Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.07586](http://arxiv.org/abs/2312.07586)

    该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。

    

    流行的导引去噪扩散概率模型(DDPM)线性地将不同的条件模型组合在一起，以提供对样本的增强控制。然而，这种方法忽视了当导向尺度变大时产生的非线性效应。为了解决这个问题，我们提出了特征引导，一种采样方法，为无分类器导向的DDPM提供了一种基于原理的非线性校正。这种校正迫使导向的DDPM遵守其底层扩散过程的福克-普朗克方程，这种方法无需训练，无需导数，与现有的采样方法兼容。实验证明，特征引导增强了对图像生成中的控制能力，并减少了颜色和曝光问题，对从潜在空间采样到解决物理问题如磁相变的各种应用都有效。

    Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
    
[^137]: 健壮和共轭高斯过程回归

    Robust and Conjugate Gaussian Process Regression. (arXiv:2311.00463v1 [stat.ML])

    [http://arxiv.org/abs/2311.00463](http://arxiv.org/abs/2311.00463)

    本文提出了一种健壮和共轭的高斯过程（RCGP）回归方法，通过泛化贝叶斯推断实现了可靠的闭式更新，适用于各种实际应用场景。

    

    为了实现闭式条件，高斯过程（GP）回归的常见假设是独立同分布的高斯观测噪声。然而，这种强假设在实际中经常被违反，导致不可靠的推断和不确定性量化。本文中，我们展示了如何使用泛化贝叶斯推断以几乎没有额外代价实现可靠和共轭的高斯过程（RCGP）回归。RCGP具有很高的灵活性，可以在标准GP适用的所有情况下进行精确的共轭闭式更新。为了展示其强大的实证性能，我们将RCGP应用于从贝叶斯优化到稀疏变分高斯过程的各种问题中。

    To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.
    
[^138]: Local Discovery by Partitioning: 在有限先验知识下的多项式时间因果发现方法

    Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])

    [http://arxiv.org/abs/2310.17816](http://arxiv.org/abs/2310.17816)

    在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。

    

    该研究解决了在有限先验知识下自动变量选择的问题。给定一个{X,Y}的曝光-结果对和一个未知因果结构的变量集合Z，局部分区发现（LDP）算法将Z划分成与{X,Y}相关的子集。我们列举了任意Z的8个穷举且互不重复的分区，并利用这个分类法区分混淆因素和其他变量类型。LDP的动机是有效的调整集识别，但避免了自动变量选择方法中常见的预处理假设。我们提供了理论保证，LDP对于满足足够图形条件的任何Z都返回一个有效的调整集。在更强的条件下，我们证明了分区标签的渐近正确性。总独立性测试在|Z|的最坏情况下是二次的，经验上观察到次二次的运行时间。我们在合成数据上对理论保证进行了数值验证。

    This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
    
[^139]: Clover: 闭环可验证代码生成

    Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])

    [http://arxiv.org/abs/2310.17807](http://arxiv.org/abs/2310.17807)

    Clover是一种闭环可验证代码生成的范式，通过在代码、docstrings和形式注释之间进行一致性检查，确保生成的代码的正确性。

    

    在软件开发中，使用大型语言模型进行代码生成是一个快速增长的趋势。然而，如果没有有效的方法来确保生成的代码的正确性，这个趋势可能会导致许多不良结果。在本文中，我们提出了一个解决这个挑战的愿景：Clover范式，即闭环可验证代码生成，它将正确性检查简化为更可访问的一致性检查问题。在Clover的核心是一个检查器，它在代码、docstrings和形式注释之间进行一致性检查。该检查器使用了形式验证工具和大型语言模型的新颖集成实现。我们提供了理论分析来支持我们的论点，即Clover在一致性检查方面应该是有效的。我们还在一个由手工设计的数据集（CloverBench）上进行了实证调查，该数据集包含了注释的Dafny程序，难度水平与教科书相当。实验结果显示

    The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
    
[^140]: 以结构为基础增强图神经网络

    Enhancing Graph Neural Networks with Structure-Based Prompt. (arXiv:2310.17394v1 [cs.LG])

    [http://arxiv.org/abs/2310.17394](http://arxiv.org/abs/2310.17394)

    这篇论文提出了一种以结构为基础的图神经网络的提示方法（SAP），该方法在预训练和提示调整阶段都一致地利用结构信息，从而增强了图神经网络在学习任务特定参数方面的能力。

    

    图神经网络（GNNs）在学习图数据的语义方面具有很强的能力。最近，一种新的范式“pre-train, prompt”在使用较少有监督数据的情况下适应各种任务的GNNs方面取得了有希望的结果。这种范式的成功可以归因于预训练和面向任务的提示调整的更一致的目标，其中预训练的知识可以有效地转移到下游任务中。然而，现有研究中一个被忽视的问题是，在学习节点表示的预训练阶段通常利用图的结构信息，而在提示调整阶段忽视了结构信息的利用以学习任务特定参数。为了弥补这一差距，我们提出了一种新的以结构为基础的GNNs提示方法，即SAP，它在预训练和提示调整阶段都一致地利用了结构信息。具体而言，SAP 1）采用了双视图对比学习，以对齐节点属性和图结构的潜在语义空间。

    Graph Neural Networks (GNNs) are powerful in learning semantics of graph data. Recently, a new paradigm "pre-train, prompt" has shown promising results in adapting GNNs to various tasks with less supervised data. The success of such paradigm can be attributed to the more consistent objectives of pre-training and task-oriented prompt tuning, where the pre-trained knowledge can be effectively transferred to downstream tasks. However, an overlooked issue of existing studies is that the structure information of graph is usually exploited during pre-training for learning node representations, while neglected in the prompt tuning stage for learning task-specific parameters. To bridge this gap, we propose a novel structure-based prompting method for GNNs, namely SAP, which consistently exploits structure information in both pre-training and prompt tuning stages. In particular, SAP 1) employs a dual-view contrastive learning to align the latent semantic spaces of node attributes and graph stru
    
[^141]: Transformers学会了高阶优化方法用于上下文学习：一项与线性模型的研究

    Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])

    [http://arxiv.org/abs/2310.17086](http://arxiv.org/abs/2310.17086)

    Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。

    

    Transformers在上下文学习中表现出色，但是它们是如何进行上下文学习仍然是一个谜。最近的研究表明，Transformers可能通过内部运行梯度下降，即一阶优化方法，来进行上下文学习。本文中，我们展示了Transformers学会了实现高阶优化方法来进行上下文学习。我们以上下文线性回归为重点，展示了Transformers学会了实现一个非常类似于迭代牛顿法的算法，而不是梯度下降。从实证上来看，我们展示了连续的Transformer层的预测与牛顿法的不同迭代非常接近，每个中间层大致计算了3次迭代。相比之下，需要指数级的梯度下降步骤才能匹配额外的Transformer层；这表明Transformers具有相当的收敛速率。

    Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
    
[^142]: 关于离散去噪扩散模型内在隐私属性的研究

    On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])

    [http://arxiv.org/abs/2310.15524](http://arxiv.org/abs/2310.15524)

    本研究探索了离散扩散模型在隐私保护方面的潜力，提供了关于训练数据集中每个数据点的隐私泄露的洞察，以及通过数据预处理减少合成数据集生成中隐私风险的方法。

    

    隐私问题导致合成数据集的创建激增，扩散模型成为一种有前景的方法。虽然以前的研究已经对这些模型进行了经验评估，但在提供数学特征化其隐私保护能力方面存在差距。为了解决这个问题，我们提出了离散扩散模型（DDMs）内在隐私保护的开创性理论研究，用于离散数据集生成。对于每个数据点的每个实例差异隐私（pDP），我们的框架阐明了给定训练数据集中每个数据点的潜在隐私泄露，从而为通过DDMs降低合成数据集生成的隐私风险提供了洞察。我们的界限还表明，使用$s$个大小的数据点进行训练会导致隐私泄露从$(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP到$(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP的激增。

    Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pu
    
[^143]: 通过二阶透镜看Adam

    Adam through a Second-Order Lens. (arXiv:2310.14963v1 [cs.LG])

    [http://arxiv.org/abs/2310.14963](http://arxiv.org/abs/2310.14963)

    该论文提出了AdamQLR，它是一个通过将K-FAC中的技术与Adam的更新方法相结合的优化器，通过考虑二阶数据上的Adam行为而得到启发。在回归和分类任务上进行了评估，结果显示AdamQLR在运行时间和推广性能方面表现出良好的竞争力。

    

    深度学习优化研究存在一种紧张状态，即第一阶梯度法（如SGD和Adam）的计算效率与第二阶曲率法（如拟牛顿方法和K-FAC）的理论效率之间的紧张关系。我们试图将这两种方法的优点结合到一个计算上高效的算法中。注意到二阶方法通常依赖于稳定的启发式方法（如Levenberg-Marquardt阻尼），我们提出AdamQLR：一个将K-FAC中的阻尼和学习率选择技术与Adam提出的更新方向相结合的优化器，通过考虑Adam在二阶数据上的表现而得到启发。我们在各种规模的回归和分类任务上评估了AdamQLR，在运行时间与竞争性推广性能之间取得了竞争性的结果。

    Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical efficiency of second-order, curvature-based methods (such as quasi-Newton methods and K-FAC). We seek to combine the benefits of both approaches into a single computationally-efficient algorithm. Noting that second-order methods often depend on stabilising heuristics (such as Levenberg-Marquardt damping), we propose AdamQLR: an optimiser combining damping and learning rate selection techniques from K-FAC (Martens and Grosse, 2015) with the update directions proposed by Adam, inspired by considering Adam through a second-order lens. We evaluate AdamQLR on a range of regression and classification tasks at various scales, achieving competitive generalisation performance vs runtime.
    
[^144]: LLM-集成应用中的提示注入攻击和防御

    Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])

    [http://arxiv.org/abs/2310.12815](http://arxiv.org/abs/2310.12815)

    本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。

    

    大型语言模型（LLMs）越来越多地用作各种称为LLM-集成应用的实际应用程序的后端。最近的多项研究表明，LLM-集成应用容易受到提示注入攻击的威胁，攻击者可以将恶意指令/数据注入这些应用程序的输入中，以达到攻击者的预期结果。然而，现有的研究仅限于案例研究，缺乏对提示注入攻击及其防御的系统理解。本论文旨在填补这一空白。我们提出了一个通用框架来形式化提示注入攻击，并将研究论文和博客文章中讨论的现有攻击视为我们框架的特例。我们的框架使我们能够通过组合现有攻击设计新的攻击方式。此外，我们还提出了一个系统化提示注入攻击防御的框架。利用我们的框架，我们可以预防和缓解这种类型的攻击。

    Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
    
[^145]: 通过可迁移的对抗攻击实现对齐大型语言模型的自动幻觉评估

    Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])

    [http://arxiv.org/abs/2310.12516](http://arxiv.org/abs/2310.12516)

    本文提出了一种通过可迁移的对抗攻击在大型语言模型中自动生成评估数据的方法，并使用ChatGPT和Natural Questions（NQ）数据集进行了验证。

    

    尽管在使用指令调整和检索增强技术防止大型语言模型（LLM）的幻觉方面取得了显著进展，但衡量LLM的可靠性仍然具有挑战性，因为人工评估数据对于许多任务和领域来说并不可用且可能存在数据泄漏。受到对抗机器学习的启发，本文旨在开发一种通过适当修改LLM在其中表现忠实的现有数据来自动生成评估数据的方法。具体而言，本文提出了一种基于LLM的框架AutoDebug，使用提示链接来生成以问答示例形式的可迁移对抗攻击。我们希望了解这些示例在多大程度上触发了LLM的幻觉行为。我们使用ChatGPT实现了AutoDebug，并对一个热门的开放领域问答数据集Natural Questions（NQ）的两个变体进行了评估。

    Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
    
[^146]: 机器学习中的鼓励对象选择：在学生助学金续签领域实验中的因果与预测性目标定位

    Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal. (arXiv:2310.08672v1 [econ.EM])

    [http://arxiv.org/abs/2310.08672](http://arxiv.org/abs/2310.08672)

    通过对学生进行因果与预测性定位的比较，研究探讨了在学生助学金续签领域实验中鼓励对象选择的价值。这项大规模实地实验揭示了定位干预对于不同学生的效果，为干预策略的优化提供了参考。

    

    在许多情境下，干预可能对某些人比其他人更有效，因此定位干预可能是有益的。我们通过一个规模庞大的实地实验（超过53,000名大学生）来分析在学生助学金续签前使用“鼓励”策略的价值。我们首先使用基线方法进行定位。首先，我们基于一个估计异质处理效应的因果森林进行定位，并根据估计出的拥有最高处理效应的学生来进行处理。接下来，我们评估两种替代的定位策略，一种是针对在没有干预的情况下预测到低助学金续签概率的学生，另一种是针对预测到高概率的学生。预测的基线结果并不是定位的理想标准，而且在先验上也不清楚是优先考虑低、高还是中间的预测。

    In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predic
    
[^147]: 神经扩散模型

    Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])

    [http://arxiv.org/abs/2310.08337](http://arxiv.org/abs/2310.08337)

    本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。

    

    扩散模型在许多生成任务上表现出色。然而，尽管最近取得了一些成功，大多数扩散模型只允许对数据分布进行线性转换，受到了一定的限制。相比之下，更广泛的变换家族可能有助于更有效地训练生成分布，简化逆过程并缩小真实负对数似然和变分近似之间的差距。本文介绍了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在一个无需模拟的设置中使用变分界对NDMs进行优化。此外，我们导出了NDMs的时间连续形式，通过使用现成的数值ODE和SDE求解器，可以快速可靠地进行推理。最后，我们通过在标准图像生成任务上的实验展示了可学习变换的NDMs的实用性。

    Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
    
[^148]: 在流形上通过黎曼扩散过程的混合生成模型

    Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes. (arXiv:2310.07216v1 [cs.LG])

    [http://arxiv.org/abs/2310.07216](http://arxiv.org/abs/2310.07216)

    通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。

    

    在非欧几里得空间中建模数据的分布对于来自不同科学领域的许多应用都至关重要。然而，现有的流形上的生成模型存在着计算复杂的散度或依赖于热核的近似的问题。这些限制限制了它们在简单几何形状上的适用性，并阻碍了在高维空间中的可扩展性。在这项工作中，我们引入了黎曼扩散混合模型，这是一个在流形上构建生成过程的原则性框架，它是一组以端点条件扩散过程作为混合的生成过程，而不依赖于先前扩散模型的去噪方法，对于这些模型，生成过程的特性是它的漂移导向与流形的几何形状相对应的最可能的终点。我们进一步提出了一个简单而高效的训练目标，用于学习混合过程，它可以直接应用于一般流形。我们的方法表现得很好。

    Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outp
    
[^149]: LLark: 一种用于音乐的多模态基础模型

    LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])

    [http://arxiv.org/abs/2310.07160](http://arxiv.org/abs/2310.07160)

    LLark是一个通过多模态架构实现音乐理解的模型，能够在零样本泛化上匹配或超出现有基准模型，在字幕生成和推理任务中与人类响应高度一致。

    

    音乐具有独特且复杂的结构，对于专业人士和现有的AI系统来说都具有挑战性，并相对于其他形式的音频呈现出独特的挑战。我们提出了LLark，一种针对音乐理解的指令调谐多模型模型。我们详细介绍了我们的数据集创建过程，其中包括增强多样化的开源音乐数据集的注释，并将其转换为统一的指令调谐格式。我们提出了一种多模态架构用于LLark，将预训练的音乐生成模型与预训练的语言模型相结合。在对三种类型的任务（音乐理解、字幕生成和推理）进行评估时，我们展示了我们的模型在音乐理解的零样本泛化上与现有基准模型相匹配或超出，并且在字幕生成和推理任务中人类与模型的响应显示出高度一致性。LLark完全是根据开源音乐数据和模型进行训练的，并且我们公开了我们的训练代码。

    Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code
    
[^150]: 洪水和回声：图神经网络与分布式计算的算法对齐

    Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v1 [cs.LG])

    [http://arxiv.org/abs/2310.06970](http://arxiv.org/abs/2310.06970)

    本论文提出了一个基于分布式算法设计原则的新的执行框架：洪水和回声网络。通过波状激活模式，它可以在整个图中传递消息，并且可以有效地处理更大的图形。

    

    图神经网络是学习算法的自然选择。它们可以通过抽象而多功能的图结构直接表示任务，并处理不同规模的输入。这为算法的扩展和外推到更大的图形提供了可能性，这是一种最重要的优势。然而，这提出了两个核心问题：i）如何使节点能够在给定的图中收集所需的信息（即“信息交换”），即使节点距离很远；ii）我们如何设计一个执行框架，以实现该信息交换以便外推到更大的图大小（即“外推时的算法对齐”）。我们提出了一个新的执行框架，受分布式算法的设计原则启发：洪水和回声网络。它以波状激活模式将消息传播到整个图中，自然地推广到更大的实例。通过它的稀疏但并行激活，可以证明它更加高效。

    Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more ef
    
[^151]: 基于分位数的极大似然训练用于异常检测

    Quantile-based Maximum Likelihood Training for Outlier Detection. (arXiv:2310.06085v1 [cs.CV])

    [http://arxiv.org/abs/2310.06085](http://arxiv.org/abs/2310.06085)

    本文提出了一种基于分位数的极大似然目标，用于改进异常检测中异常值的分离程度。通过将正则化流拟合到预训练的判别性特征，并根据对数似然度评估来检测异常值。实验结果表明，这种方法优于最先进的无监督模型。

    

    判别性学习有效地对图像分类预测真实的对象类别。然而，在异常值方面，它经常导致误报阳性，这在自主驾驶和视频监视系统等应用中引起了严重关注。以往解决这个挑战的尝试包括使用实际异常值数据通过对比学习训练图像分类器，或者通过合成异常值进行自我监督学习。此外，像素空间中对内点进行无监督生成建模对于异常检测显示出有限的成功。在这项工作中，我们引入了一种基于分位数的极大似然目标，用于学习内点分布，以在推断过程中提高异常值的分离程度。我们的方法通过将正则化流拟合到预训练的判别性特征，并根据评估的对数似然度来检测异常值。实验评估证明了我们方法的有效性，因为它超过了最先进的无监督方法的性能。

    Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervi
    
[^152]: 学习温度条件下尺度标量化的GFlowNets

    Learning to Scale Logits for Temperature-Conditional GFlowNets. (arXiv:2310.02823v1 [cs.LG])

    [http://arxiv.org/abs/2310.02823](http://arxiv.org/abs/2310.02823)

    这项研究提出了一种名为LSL-GFN的新型架构设计，可以大大加速温度条件下GFlowNets的训练，从而提高GFlowNets的探索和利用能力。

    

    GFlowNets是一种概率模型，通过学习随机策略来顺序生成组合结构，例如分子图。它们的训练目标是按比例采样具有相应温度调节的对象的奖励。在GFlowNets中，温度条件下的GFlowNets代表了一系列由温度索引的策略，每个策略与相应的温度调节奖励函数相关联。温度条件下的GFlowNets的主要优势在于通过调整温度来控制对GFlowNets的探索和利用。我们提出了一种名为学习温度条件下尺度标量化的GFlowNets（LSL-GFN）的新型架构设计，它极大地加速了温度条件下GFlowNets的训练。它基于一个思想，即之前提出的温度条件方法在深度网络的训练中引入了数值挑战，因为不同的温度可能导致非常不同的情况。

    GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very differe
    
[^153]: 用过参数化的神经网络中的隐式正则化方法进行多任务学习和微调

    Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])

    [http://arxiv.org/abs/2310.02396](http://arxiv.org/abs/2310.02396)

    本文研究了在过参数化神经网络中，多任务学习和微调所带来的隐式正则化效果。在简化的线性网络环境中，我们发现了多任务学习和微调所对特征共享和学习特定特征稀疏性的鼓励作用，并发现微调过程同时具有内核和特征学习的混合状态。此外，微调还可以展现一种嵌套特征学习行为，使其偏向于提取一组稀疏的特征子集。

    

    在深度学习中，常常使用训练辅助任务的方法来期望学习可以部分地转移到其他感兴趣的任务上。本研究探讨了学习辅助任务所产生的归纳偏置，包括同时学习（多任务学习，MTL）和依序学习（预训练和随后微调，PT+FT）。在使用梯度下降法训练两层对角线线性网络的简化环境中，我们发现了与MTL和PT+FT相关的隐式正则化惩罚，两者都鼓励任务之间的特征共享和学习任务特定特征的稀疏性。值得注意的是，我们的结果表明，在微调过程中，网络在先前研究中确定的内核（或“惰性”）状态和特征学习（“丰富”）状态之间具有混合状态。此外，PT+FT还可以展现一种新颖的“嵌套特征学习”行为，该行为无法被任何状态所捕捉，使其偏向于提取一组稀疏的特征子集。

    It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or "lazy") regime and the feature learning ("rich") regime identified in prior work. Moreover, PT+FT can exhibit a novel "nested feature learning" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned
    
[^154]: 高效的生物合理对抗训练

    Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])

    [http://arxiv.org/abs/2309.17348](http://arxiv.org/abs/2309.17348)

    本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。

    

    用反向传播训练的人工神经网络(ANNs)表现出令人惊讶的性能，并且越来越多地被用于执行我们日常生活中的任务。然而，ANNs极易受到对抗攻击的影响，这些攻击通过微小的有针对性的扰动来改变输入，从而严重破坏模型的性能。使ANNs对这些攻击具有鲁棒性最有效的方法是对抗训练，其中训练数据集被添加了样本用于对抗攻击。不幸的是，这种方法的缺点是增加了训练复杂性，因为生成对抗样本是非常计算消耗高的。与ANNs不同，人类不容易受到对抗攻击的影响。因此，在这项工作中，我们研究了生物合理的学习算法是否比BP更具有对抗攻击的鲁棒性。具体而言，我们对BP和“Error to Pertu"的对抗鲁棒性进行了广泛的比较分析。

    Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
    
[^155]: 对称性导致学习的结构性约束

    Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])

    [http://arxiv.org/abs/2309.16932](http://arxiv.org/abs/2309.16932)

    本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。

    

    由于常见的架构设计，对称性在当代神经网络中广泛存在。在这项工作中，我们揭示了损失函数对称性对影响机器学习模型的学习行为的重要性。我们证明了损失函数的每个镜像对称性都会导致一种结构性约束，当权重衰减或梯度噪声较大时，这种约束将成为首选解。作为直接推论，我们展示了重新缩放对称性导致稀疏性，旋转对称性导致低秩性，置换对称性导致同质集成。然后，我们展示了理论框架可以解释神经网络中的可塑性丧失和各种崩溃现象，并提出了如何利用对称性设计可微分实施硬性约束的算法。

    Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
    
[^156]: 神经表征的拓扑和几何结构

    The Topology and Geometry of Neural Representations. (arXiv:2309.11028v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.11028](http://arxiv.org/abs/2309.11028)

    本文探索了从几何结构到拓扑结构的抽象步骤，并提出了一种拓扑表征相似分析方法（tRSA），通过一系列地理拓扑摘要统计量对大脑表征进行表征。

    

    神经科学所关心的一个核心问题是如何表征感知和认知内容的大脑表征。一个理想的表征应该能够区分不同的功能区域，并且对噪声和个体大脑的特异性具有稳健性，而不会与计算差异相对应。以前的研究通过表征几何结构来表征大脑表征，几何结构由表征不相似矩阵（RDM）定义，RDM是一个摘要统计量，摘要了个体神经元（或响应通道）的作用，并表征了刺激的可辨别性。在这里，我们进一步探索了从几何结构到大脑表征拓扑的抽象步骤。我们提出了拓扑表征相似分析（tRSA），它是表征相似分析（RSA）的一种扩展，使用了一系列地理拓扑摘要统计量，将RDM进行泛化以表征拓扑结构并减弱几何结构的作用。

    A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. 
    
[^157]: PAGER: 一种用于深度回归模型故障分析的框架

    PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])

    [http://arxiv.org/abs/2309.10977](http://arxiv.org/abs/2309.10977)

    PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。

    

    安全部署AI模型需要主动检测潜在的预测故障，以防止昂贵的错误。尽管分类问题的故障检测已经引起了广泛关注，但在回归任务中表征故障模式更加复杂且较少研究。现有方法依赖于认识不确定性或与训练分布的特征不一致来表征模型风险。然而，我们表明，仅靠不确定性无法准确表征故障，这是由于各种误差源的存在。在本文中，我们提出了PAGER（回归器的原则性泛化错误分析），这是一个系统检测和表征深度回归模型故障的框架。基于最近提出的深度模型锚定思想，PAGER将认识不确定性和新颖的、互补的不一致分数统一起来，将样本组织成不同的风险区域，从而提供全面的分析。

    Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys
    
[^158]: 参数高效的长尾识别

    Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])

    [http://arxiv.org/abs/2309.10019](http://arxiv.org/abs/2309.10019)

    本文提出了一种名为PEL的参数高效微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。该方法通过引入少量的任务特定参数，解决了常用微调方法导致尾部类别性能下降的问题。

    

    自从出现大规模视觉语言模型（例如对比语言-图像预训练模型CLIP），"预训练和微调"范例在解决长尾识别任务中引起了极大的兴趣。虽然先前研究在适应预训练模型用于这些任务方面表现出了希望，但它们常常需要大量的训练轮数或额外的训练数据来保持良好的性能，这是不可取的。在本文中，我们提出了一种名为PEL的微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。我们首先经验性地发现，常用的微调方法（例如全面微调和分类器微调）容易过拟合，导致尾部类别的性能下降。为了解决这个问题，PEL采用了现有的参数高效微调方法的设计，引入了少量的任务特定参数。

    The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to
    
[^159]: SortedNet，每个网络都有自己的位置：面向训练多对一神经网络的广义解决方案

    SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])

    [http://arxiv.org/abs/2309.00255](http://arxiv.org/abs/2309.00255)

    SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。

    

    随着深度学习模型的规模不断增大，如何在内存和计算约束下找到最优模型变得越来越重要。虽然神经网络的架构和组成部分通常允许以模块化的方式使用，但它们的训练过程并不意识到这种模块化。因此，传统的神经网络训练缺乏在推断过程中适应模型计算负载的灵活性。本文提出了SortedNet，这是一种广义且可扩展的解决方案，用于利用深度神经网络在各个维度上的内在模块化特性，实现高效的动态推断。我们的训练方法采用了一种嵌套结构的子模型和主模型共享参数的方式，并以排序和概率的方式训练它们。这种子网络的排序训练使我们能够在一轮训练中将子网络的数量扩展到数百个。我们利用一种新颖的更新方案在推断过程中动态调整子网络的计算负载。

    As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during 
    
[^160]: 贝叶斯探索网络

    Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])

    [http://arxiv.org/abs/2308.13049](http://arxiv.org/abs/2308.13049)

    这篇论文提出了一种贝叶斯探索网络的方法，通过在一维Bellman算子中建模不确定性，解决了贝叶斯强化学习中学习贝叶斯最优策略的计算复杂性的挑战。

    

    贝叶斯强化学习为不确定性下的顺序决策提供了一种原则性和优雅的方法。最显著的是，贝叶斯代理不会面临频率方法的探索/开发困境，这是一个重大的问题。贝叶斯强化学习的一个关键挑战是学习贝叶斯最优策略的计算复杂性，这在玩具领域中是可计算的。在本文中，我们提出了一种新颖的无模型方法来解决这一挑战。与基于模型的方法不同，我们在一维Bellman算子中建模不确定性而不是在高维状态转移分布中建模。我们的理论分析揭示了现有的无模型方法要么不通过MDP传播认知不确定性，要么在一组语境策略中优化而不是所有历史条件策略。这两个近似得到的策略可能是任意贝叶斯次优的。为了克服这些问题，我们引入了贝叶斯探索网络（Bayesian exploration network）。

    Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
    
[^161]: HoSNN: 具有自适应发放阈值的对抗性稳态脉冲神经网络

    HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds. (arXiv:2308.10373v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2308.10373](http://arxiv.org/abs/2308.10373)

    HoSNN是一种对抗性稳态脉冲神经网络，通过采用自适应发放阈值的渗漏整合与发放（TA-LIF）神经元模型来抵御对抗攻击，并在无监督的方式下保护其鲁棒性。

    

    脉冲神经网络（SNNs）在高效和强大的神经启发式计算方面具有潜力。然而，与其他类型的神经网络一样，SNNs面临着对抗攻击的严重问题。我们提出了第一个从神经恒稳性中汲取灵感的研究，以开发一种仿生解决方案，来应对SNNs对对抗性攻击的敏感性。我们的方法的核心是一种新颖的自适应发放阈值的渗漏整合与发放（TA-LIF）神经元模型，我们采用它来构建所提出的对抗性稳态SNN（HoSNN）。与传统的LIF模型不同，我们的TA-LIF模型融入了自稳定动态阈值机制，限制对抗性噪声的传播，并以无监督的方式保护HoSNN的鲁棒性。我们还提出了理论分析，以阐明TA-LIF神经元的稳定性和收敛性，强调它们在输入多样性方面的卓越动态鲁棒性。

    Spiking neural networks (SNNs) offer promise for efficient and powerful neurally inspired computation. Common to other types of neural networks, however, SNNs face the severe issue of vulnerability to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to develop a bio-inspired solution that counters the susceptibilities of SNNs to adversarial onslaughts. At the heart of our approach is a novel threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model, which we adopt to construct the proposed adversarially robust homeostatic SNN (HoSNN). Distinct from traditional LIF models, our TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. Theoretical analysis is presented to shed light on the stability and convergence properties of the TA-LIF neurons, underscoring their superior dynamic robustness under input di
    
[^162]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^163]: 学习采样任务用于元学习

    Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])

    [http://arxiv.org/abs/2307.08924](http://arxiv.org/abs/2307.08924)

    通过实验得出了三个结论：没有通用的任务采样策略能保证元学习模型的性能；任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题；模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，提出了一种新颖的任务采样器ASr，它利用任务的差异、任务熵和任务难度来采样任务，并通过重新思考和提出一个简单而通用的元学习算法来优化ASr。大量实证实验表明了ASr的有效性。

    

    通过对各种元学习方法、任务采样器和少样本学习任务的实验，本文得出了三个结论。首先，没有通用的任务采样策略能保证元学习模型的性能。其次，任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题。最后，模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，我们提出了一种新颖的任务采样器，称为自适应采样器（ASr）。ASr是一个即插即用的任务采样器，它利用任务的差异、任务熵和任务难度来采样任务。为了优化ASr，我们重新思考并提出了一个简单而通用的元学习算法。最后，大量的实证实验证明了所提出的ASr的有效性。

    Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
    
[^164]: 通过奖励重新加权、重选和重新训练方法，改进了原型零件网络

    Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])

    [http://arxiv.org/abs/2307.03887](http://arxiv.org/abs/2307.03887)

    本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。

    

    近年来，人们致力于开发深度可解释的图像分类方法，能够清楚地将模型的输出归因于数据的特定特征。其中一种方法是原型零件网络（ProtoPNet），它基于输入的有意义部分来尝试分类图像。然而，这种方法经常学习从图像的虚假或不一致的部分进行分类。为了解决这个问题，我们受到强化学习与人类反馈（RLHF）的最新发展启发，通过在CUB-200-2011数据集上收集人类原型质量的1-5分级注释，构建一个学习识别非虚假原型的奖励模型。我们提出了重新加权、重选和重新训练的原型零件网络（R3-ProtoPNet），该网络在ProtoPNet训练循环中增加了三个额外的步骤。

    In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
    
[^165]: 可编程合成表格数据生成

    Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])

    [http://arxiv.org/abs/2307.03577](http://arxiv.org/abs/2307.03577)

    这项工作介绍了ProgSyn，第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义，并且通过预训练和微调生成模型来确保高质量的数据和遵守自定义规范。

    

    由于隐私、数据质量和数据共享的限制，大量的表格数据仍然被低效利用。尽管训练一个能够生成类似原始分布的合成数据的生成模型可以解决其中一些问题，但大多数应用程序还需要额外的生成数据约束。现有的合成数据方法存在局限性，因为它们通常只处理特定的约束条件，例如差分隐私（DP）或增加公平性，并且缺乏一个可访问的接口来声明一般规范。在这项工作中，我们介绍了ProgSyn，这是第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义。为了确保高质量的数据并遵守自定义规范，ProgSyn在原始数据集上进行预训练生成模型，并在提供的规范上自动推导出可微分损失进行微调。这些规范可以使用统计和。

    Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
    
[^166]: 基于区域关注的多视角表示学习用于城市区域嵌入

    Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])

    [http://arxiv.org/abs/2307.03212](http://arxiv.org/abs/2307.03212)

    提出了一种区域关注的多视角表示学习（ROMER）的算法，用于捕捉多视角之间的依赖关系，学习城市区域的表达能力，并在多源城市数据中优于现有方法。

    

    城市区域嵌入是一个重要且具有高度挑战性的问题，由于城市数据的复杂性和不断变化的性质。为了解决这些挑战，我们提出了一种区域关注的多视角表示学习（ROMER），以捕捉多视角之间的依赖关系，并学习城市区域的表达能力，而不受刚性邻域条件的限制。我们的模型专注于从多源城市数据中学习城市区域表示。首先，我们从移动流模式、POI语义和签到动态中捕捉多视角的相关性。然后，我们采用全局图注意网络来学习图中任意两个顶点的相似性。为了全面考虑和共享多个视角的特征，我们进一步提出了一个两阶段的融合模块，利用外部注意力学习权重来融合多视角嵌入。在真实世界数据集上进行的两个下游任务的大量实验证明，我们的模型优于现有的方法。

    Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
    
[^167]: 现有的撒哈拉以南非洲农业土地覆盖地图的准确性有多高？

    How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?. (arXiv:2307.02575v1 [cs.LG])

    [http://arxiv.org/abs/2307.02575](http://arxiv.org/abs/2307.02575)

    本研究定量评估和比较了11个公开可用的土地覆盖地图，以确定它们在非洲农田分类和基于卫星地球观测的农业监测中的适用性。研究结果可帮助用户找到最适合其需求的地图，并鼓励未来工作改进地图的一致性和低精度区域的准确性。

    

    卫星地球观测（EO）可以提供经济实惠和及时的信息来评估作物状况和粮食生产。在非洲，这样的监测系统至关重要，因为那里存在粮食不安全和缺乏农业统计数据的问题。基于EO的监测系统需要准确的农田地图来提供有关农田的信息，但是缺乏数据来确定哪些可用的土地覆盖地图最准确地识别非洲国家的农田。本研究通过使用来自8个国家的统计严谨的参考数据，对11个公开可用的土地覆盖地图进行定量评估和比较，以评估它们在农田分类和基于EO的非洲农业监测中的适用性。我们希望本研究的结果可以帮助用户确定最适合他们需求的地图，并鼓励未来的工作集中解决地图之间的不一致性并提高低精度地区的准确性。

    Satellite Earth observations (EO) can provide affordable and timely information for assessing crop conditions and food production. Such monitoring systems are essential in Africa, where there is high food insecurity and sparse agricultural statistics. EO-based monitoring systems require accurate cropland maps to provide information about croplands, but there is a lack of data to determine which of the many available land cover maps most accurately identify cropland in African countries. This study provides a quantitative evaluation and intercomparison of 11 publicly available land cover maps to assess their suitability for cropland classification and EO-based agriculture monitoring in Africa using statistically rigorous reference datasets from 8 countries. We hope the results of this study will help users determine the most suitable map for their needs and encourage future work to focus on resolving inconsistencies between maps and improving accuracy in low-accuracy regions.
    
[^168]: 基于动态特征的深度强化学习在稀疏表面压力传感器下的循环气缸流控制中的应用

    Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing. (arXiv:2307.01995v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01995](http://arxiv.org/abs/2307.01995)

    本研究提出了一种基于动态特征的深度强化学习算法，针对稀疏表面压力传感器下的循环气缸流控制，实现了降低阻力和减小升力波动的目标。通过将传感器信号提取为动态特征，该算法能够自动学习并预测未来的流动状态，从而使得控制性能能够在不降低的情况下使用稀疏传感器感知流动。该方法在阻力系数和升力系数的改善上取得了显著的成果。

    

    本研究提出了一种自学习算法，针对稀疏传感器信息的封闭环气缸尾流控制，以降低阻力和减小升力的波动。该方法采用深度强化学习作为起始点，并通过将传感器信号提取为动态特征（DF）来显著提高DRL性能，从而预测未来的流动状态。由此得到的基于动态特征的深度强化学习（DF-DRL）在没有动态模型的情况下自动学习了一个在系统中的反馈控制。结果表明，相比直接传感器反馈的基准模型，DF-DRL模型的阻力系数减小了25%。更重要的是，仅使用一个表面压力传感器，DF-DRL可以将阻力系数降低到Re = 100时约8%的最先进性能，并且显著减轻了升力系数的波动。因此，DF-DRL允许在不降低控制性能的情况下部署稀疏传感器对流动进行感知。此方法还展示了良好的鲁棒性。

    This study proposes a self-learning algorithm for closed-loop cylinder wake control targeting lower drag and lower lift fluctuations with the additional challenge of sparse sensor information, taking deep reinforcement learning as the starting point. DRL performance is significantly improved by lifting the sensor signals to dynamic features (DF), which predict future flow states. The resulting dynamic feature-based DRL (DF-DRL) automatically learns a feedback control in the plant without a dynamic model. Results show that the drag coefficient of the DF-DRL model is 25% less than the vanilla model based on direct sensor feedback. More importantly, using only one surface pressure sensor, DF-DRL can reduce the drag coefficient to a state-of-the-art performance of about 8% at Re = 100 and significantly mitigate lift coefficient fluctuations. Hence, DF-DRL allows the deployment of sparse sensing of the flow without degrading the control performance. This method also shows good robustness in
    
[^169]: 基于基础生成模型的联邦生成学习

    Federated Generative Learning with Foundation Models. (arXiv:2306.16064v1 [cs.LG])

    [http://arxiv.org/abs/2306.16064](http://arxiv.org/abs/2306.16064)

    本文提出了一种基于基础生成模型的联邦生成学习框架，通过传输提示与分布式训练数据，可以远程合成有信息量的训练数据，从而改善了通信效率、适应分布转移、提升性能、加强隐私保护。

    

    现有的联邦学习解决方案主要集中在在客户端和服务器之间传输特征、参数或梯度，这导致了严重的低效和隐私泄露问题。借助新兴的基础生成模型，我们提出了一种新颖的联邦学习框架，称为联邦生成学习，它在客户端和服务器之间传输与分布式训练数据相关的提示。通过接收到的包含较少隐私信息的提示以及基础生成模型，可以远程合成有信息量的训练数据。这个新框架具有多个优势，包括改善了通信效率、更好的适应分布转移、实现了显著的性能提升、加强了隐私保护。在ImageNet和DomainNet数据集上进行的广泛实验证明了这些优势。

    Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.
    
[^170]: FLuRKA: 快速融合低秩和核注意力

    FLuRKA: Fast fused Low-Rank & Kernel Attention. (arXiv:2306.15799v1 [cs.LG])

    [http://arxiv.org/abs/2306.15799](http://arxiv.org/abs/2306.15799)

    FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。

    

    自从transformer结构的提出以来，许多高效的近似自注意力技术已经变得流行起来。其中两种流行的技术类别是低秩和核方法。我们观察到这两种方法的优势相互补充，利用这些协同效应来融合低秩和核方法，产生了一种新的transformer类别：FLuRKA（快速低秩和核注意力）。FLuRKA相对于这些近似技术提供了可观的性能提升，并且具有高质量。我们在理论和实证方面评估了FLuRKA的运行时间性能和质量。我们的运行时间分析提供了多种参数配置，在这些配置下，FLuRKA具有加速效果；我们的准确性分析限定了FLuRKA相对于全注意力的误差。我们实例化了三种FLuRKA变体，相对于低秩和核方法分别实现了高达3.3倍和1.7倍的经验加速。这意味着更快的运行时间，而且质量仍然保持不错。

    Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
    
[^171]: 通过预测编码进行因果推理

    Causal Inference via Predictive Coding. (arXiv:2306.15479v1 [cs.LG])

    [http://arxiv.org/abs/2306.15479](http://arxiv.org/abs/2306.15479)

    通过预测编码进行因果推理的技术使我们能够在已知和未知因果图的情况下进行端到端的因果推理和因果发现。

    

    贝叶斯推理和因果推理是智能的基本过程。贝叶斯推理模型观察：如果我们观察到一个相关变量x，我们能推断出关于y的信息吗？因果推理模型干预：如果我们直接改变x，y会如何变化？预测编码是一种受神经科学启发的方法，用于使用仅局部信息对连续状态变量进行贝叶斯推理。在这项工作中，我们超越了贝叶斯推理，并展示了如何通过对预测编码的推理过程进行简单更改，实现已知因果图的干预和反事实推理。然后我们扩展了我们的结果，并展示了如何将预测编码推广到因果图未知且需要通过数据推断的情况，从而进行因果发现。结果是一种新颖且直接的技术，可以在基于预测编码的结构因果模型上进行端到端的因果推理，并展示了其实用性。

    Bayesian and causal inference are fundamental processes for intelligence. Bayesian inference models observations: what can be inferred about y if we observe a related variable x? Causal inference models interventions: if we directly change x, how will y change? Predictive coding is a neuroscience-inspired method for performing Bayesian inference on continuous state variables using local information only. In this work, we go beyond Bayesian inference, and show how a simple change in the inference process of predictive coding enables interventional and counterfactual inference in scenarios where the causal graph is known. We then extend our results, and show how predictive coding can be generalized to cases where this graph is unknown, and has to be inferred from data, hence performing causal discovery. What results is a novel and straightforward technique that allows us to perform end-to-end causal inference on predictive-coding-based structural causal models, and demonstrate its utilit
    
[^172]: ProtoGate：面向表格型生物医学数据的原型神经网络与本地特征选择

    ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data. (arXiv:2306.12330v1 [cs.LG])

    [http://arxiv.org/abs/2306.12330](http://arxiv.org/abs/2306.12330)

    本文提出了ProtoGate，一种基于原型的神经模型，该模型通过考虑样本内和样本间的同质性和异质性来引入归纳偏差，并以全局到本地的方式选择特征，从而提高预测精度并使预测具有可解释性。

    

    表格型生物医学数据在机器学习中面临挑战，因为它往往是高维的，但样本量又相对较小。先前的研究试图通过特征选择方法来应对这些挑战，但这种方法可能在真实数据上表现不稳定。这表明当前方法缺乏适当的归纳偏差，不能捕捉不同样本之间的共同模式。在本文中，我们提出ProtoGate，这是一种基于原型的神经模型，通过考虑样本内和样本间的同质性和异质性来引入归纳偏差。ProtoGate以全局到本地的方式选择特征，并利用它们来生产可解释的预测，而原型模型使预测具有可解释性。我们进行了全面的实验来评估ProtoGate在合成和真实世界数据集上的性能。我们的结果表明，利用数据中的同质性和异质性模式可以提高预测精度，同时原型赋予了预测可解释性。

    Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size. Previous research has attempted to address these challenges via feature selection approaches, which can lead to unstable performance on real-world data. This suggests that current methods lack appropriate inductive biases that capture patterns common to different samples. In this paper, we propose ProtoGate, a prototype-based neural model that introduces an inductive bias by attending to both homogeneity and heterogeneity across samples. ProtoGate selects features in a global-to-local manner and leverages them to produce explainable predictions via an interpretable prototype-based model. We conduct comprehensive experiments to evaluate the performance of ProtoGate on synthetic and real-world datasets. Our results show that exploiting the homogeneous and heterogeneous patterns in the data can improve prediction accuracy while prototypes imbue interpretability.
    
[^173]: 基于数据驱动置信度最小化的保守预测

    Conservative Prediction via Data-Driven Confidence Minimization. (arXiv:2306.04974v1 [cs.LG])

    [http://arxiv.org/abs/2306.04974](http://arxiv.org/abs/2306.04974)

    该论文提出了一种可以在处理不常见样本时推迟到人类判断的保守模型方法。该方法使用基于数据驱动置信度最小化（DCM）的算法，在辅助数据集中选择感兴趣的OOD（Out-of-Distribution）区域的样本，进而实现可靠地分离ID（In-Distribution）和OOD输入。

    

    机器学习模型的错误代价很高，特别是在诸如医疗保健等安全关键领域，这种错误可能会阻止机器学习的部署。在这些情况下，具有保守性的模型——当它们可能出现错误时可以推迟到人类判断——可能会提供解决方案。然而，检测异常或复杂示例明显具有挑战性，因为无法预测所有可能的测试输入。为了解决这个问题，先前的工作提出了在辅助伪OOD数据集上最小化模型置信度的方法。我们在理论上分析了置信度最小化的影响，并表明辅助数据集的选择是关键的。具体而言，如果辅助数据集包括来自感兴趣的OOD区域的样本，置信度最小化可以通过预测置信度可靠地分离ID和OOD输入。受到这一结果的启示，我们提出了基于数据驱动置信度最小化（DCM）的算法。

    Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confid
    
[^174]: 利用欧几里得距离函数解释和改进扩散模型

    Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])

    [http://arxiv.org/abs/2306.04848](http://arxiv.org/abs/2306.04848)

    本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。

    

    去噪直觉上与投影有关。事实上，在流形假设下，添加随机噪声近似等价于正交扰动。因此，学习去噪近似于学习投影。本文利用这一观察结果，将去噪扩散模型解释为应用于欧几里得距离函数的近似梯度下降。随后，我们基于对去噪器投影误差的简单假设，提供DDIM（Denoising Diffusion Implicit Models）采样器的简单收敛分析。最后，我们基于理论结果的洞见提出一种基于对DDIM的两个简单修改的新采样器。仅需要5-10个函数评估，我们的采样器就能在预训练的CIFAR-10和CelebA模型上达到最先进的FID得分，并且可以在潜在扩散模型上生成高质量的样本。

    Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
    
[^175]: 如何跨越云和大陆培训深度学习模型？一项实验研究。

    How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])

    [http://arxiv.org/abs/2306.03163](http://arxiv.org/abs/2306.03163)

    本文通过实验研究，探究了在不同大陆、云供应商和数据中心范围内，使用分布式数据并行点深度学习训练是否是更具成本效益的选择，并比较了其与集中式训练的可扩展性潜力。

    

    在云端或专用硬件上训练深度学习模型是昂贵的。一种更具成本效益的选择是提供点实例的高超规模云，这是一个便宜但短暂的选择，用于替代按需资源。由于点实例的可用性可能会因日期、大陆和云供应商不同而发生变化，因此在全球范围内分配资源可能更具成本效益。但是，尚未调查地理分布式数据并行点深度学习训练是否是集中式训练的更具成本效益的替代方案。本文旨在回答一个问题：深度学习模型能否在覆盖不同数据中心和云提供商的点 VM 全球市场上以更具成本效益的方式进行训练？为了提供指导，我们广泛评估了不同区域、大陆和云对代表性 CV 和 NLP 模型的成本和吞吐量影响。为了进一步扩展当前的培训选择，我们比较了可扩展性潜力。

    Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
    
[^176]: 通过因果源表示解决强化学习中的非平稳性问题

    Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation. (arXiv:2306.02747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02747](http://arxiv.org/abs/2306.02747)

    本论文介绍了一种通过追踪非平稳性的因果起源来解决强化学习中的挑战的方法，通过引入因果源表示（COREP）算法，学到的策略对非平稳性表现出韧性。

    

    在真实世界的场景中，强化学习的应用受到复杂的非平稳性的挑战。大多数现有方法试图明确建模环境中的变化，但往往需要不切实际的先验知识。在本文中，我们提出了一种新的观点，认为非平稳性可以通过状态转换中复杂的因果关系传播和累积，从而增加了其复杂性并影响策略学习。我们相信通过追踪非平稳性的因果起源可以更有效地解决这个挑战。为此，我们引入了因果源表示（COREP）算法。COREP主要采用引导更新机制来学习一种稳定的图形表示，称为因果起源表示，通过利用这种表示，学到的策略对非平稳性表现出令人印象深刻的韧性。我们补充了一个基于因果关系的理论分析。

    In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for states termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the cau
    
[^177]: 从单个快照中重建图扩散历史

    Reconstructing Graph Diffusion History from a Single Snapshot. (arXiv:2306.00488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00488](http://arxiv.org/abs/2306.00488)

    本文研究了从单个快照中重建图扩散历史的问题，揭示了现有方法的局限性，并提出了一种新的方法。

    

    图扩散在许多重要应用中普遍存在。在这些应用中，完整的扩散历史在确定动态模式、反思预防措施和预测干预效果方面起着至关重要的作用。尽管它们很重要，但完整的扩散历史很少可用，并且由于病态、爆炸性的搜索空间和训练数据的缺乏而具有极高的挑战性。迄今为止，很少有方法用于扩散历史重建。它们仅基于最大似然估计（MLE）公式，需要知道真实的扩散参数。在本文中，我们研究了一个更难的问题，即从单个快照（DASH）中重建扩散历史，我们试图仅从最终快照重建历史，而不知道真实的扩散参数。我们从理论分析开始，揭示了MLE公式的基本限制。

    Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) est
    
[^178]: 关于黎曼流形上无投影在线学习的研究

    On Riemannian Projection-free Online Learning. (arXiv:2305.19349v1 [cs.LG])

    [http://arxiv.org/abs/2305.19349](http://arxiv.org/abs/2305.19349)

    本文提出了一种针对非凸约束集情况下的曲线空间在线测地凸优化的无投影算法，获得了次线性遗憾保证。

    

    投影操作是许多优化算法（例如在线梯度下降[OGD]）中强制约束和实现最优遗憾边界所必需的关键组成部分。然而，当处理高维设置或具有病态约束集时，它会受到计算复杂度限制。无投影算法通过用更有效的优化子程序取代投影预测来解决此问题。但到目前为止，这些方法主要在欧几里得设置中开发，并且虽然越来越多地关注黎曼流形上的优化，但在尝试利用无投影工具方面基本上没有工作。一个明显的问题是，在这些领域中，非平凡的仿射函数通常是非凸的。在本文中，我们提出了一种方法，在曲线空间上进行在线测地凸优化，以获得两种情况下的次线性遗憾保证：当我们访问（a）时

    The projection operation is a critical component in a wide range of optimization algorithms, such as online gradient descent (OGD), for enforcing constraints and achieving optimal regret bounds. However, it suffers from computational complexity limitations in high-dimensional settings or when dealing with ill-conditioned constraint sets. Projection-free algorithms address this issue by replacing the projection oracle with more efficient optimization subroutines. But to date, these methods have been developed primarily in the Euclidean setting, and while there has been growing interest in optimization on Riemannian manifolds, there has been essentially no work in trying to utilize projection-free tools here. An apparent issue is that non-trivial affine functions are generally non-convex in such domains. In this paper, we present methods for obtaining sub-linear regret guarantees in online geodesically convex optimization on curved spaces for two scenarios: when we have access to (a) a s
    
[^179]: 旋转优化器：简单而强健的深度神经网络训练。

    Rotational Optimizers: Simple & Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])

    [http://arxiv.org/abs/2305.17212](http://arxiv.org/abs/2305.17212)

    该论文提出了旋转优化器，这些优化器可以简化深度神经网络训练过程，甚至在几乎不需调整基线超参数的情况下与原始优化器的性能相匹配。

    

    现代深度神经网络的训练动态取决于学习率、权重衰减、初始化等超参数之间的复杂交互作用。这些交互作用可以在尺度不变层（如归一化层）中产生球面运动动态，这些动态收敛到平衡状态，其中权重范数和预期旋转更新大小是固定的。我们对AdamW、带动量的SGD和Lion中的这个平衡进行了分析，提供了关于不同超参数及其相互作用对训练过程的影响的新见解。我们提出了这些优化器的旋转变体（RVs），强制预期角度更新大小与整个训练期间的平衡值相匹配。这简化了训练动态，通过消除收敛到平衡状态的瞬态相应。我们的旋转优化器可以匹配原始变体的性能，通常需要对基线超参数进行最少或不调整。

    The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
    
[^180]: 用最优传输学习有向图模型

    Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])

    [http://arxiv.org/abs/2305.15927](http://arxiv.org/abs/2305.15927)

    通过最优传输的视角提供了参数学习问题的新视图，可以在许多有向图上进行操作并表现出灵活性和多功能性。

    

    从不完整的数据中估计概率有向图模型的参数仍然是一个长期存在的挑战。这是因为在存在潜在变量的情况下，如果没有关于结构依赖性或模型类的进一步假设，似然函数和后验分布都是不可计算的。虽然现有的学习方法基本上是基于最大似然估计，但我们在这里通过最优传输的视角提供了参数学习问题的一个新视图。这个观点授权了一个框架，可以在许多有向图上运作，而不会对潜在变量的后验做出不切实际的假设或诉诸于黑箱变分近似。我们开发了一个理论框架，并支持它通过广泛的经验证据，展示了我们方法的灵活性和多功能性。通过实验，我们展示了我们的方法不仅可以恢复基准参数，而且在性能方面也表现得有竞争力。

    Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
    
[^181]: 基于岭正则化的线性数据最小二乘去噪问题的欠参数化双谷效应

    Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line. (arXiv:2305.14689v1 [stat.ML])

    [http://arxiv.org/abs/2305.14689](http://arxiv.org/abs/2305.14689)

    本文研究了线性数据最小二乘岭正则化的去噪问题，证明了在欠参数化情况下会出现双峰谷现象。

    

    研究了训练数据点数、统计模型参数数和模型的泛化能力之间的关系。已有的工作表明，过度参数化情况下可能出现双峰谷现象，而在欠参数化情况下则普遍存在标准偏差-方差权衡。本文提出了一个简单的例子，可以证明欠参数化情况下可以发生双峰谷现象。考虑嵌入高维空间中的线性数据最小二乘去噪问题中的岭正则化，通过推导出一种渐近准确的广义误差公式，我们发现了样本和参数的双谷效应，双峰谷位于插值点和过度参数化区域之间。此外，样本双谷曲线的高峰对应于估计量的范数曲线的高峰。

    The relationship between the number of training data points, the number of parameters in a statistical model, and the generalization capabilities of the model has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime, and believe that the standard bias-variance trade-off holds in the under-parameterized regime. In this paper, we present a simple example that provably exhibits double descent in the under-parameterized regime. For simplicity, we look at the ridge regularized least squares denoising problem with data on a line embedded in high-dimension space. By deriving an asymptotically accurate formula for the generalization error, we observe sample-wise and parameter-wise double descent with the peak in the under-parameterized regime rather than at the interpolation point or in the over-parameterized regime.  Further, the peak of the sample-wise double descent curve corresponds to a peak in the curve for the norm of the estimator,
    
[^182]: 语言模型的物理学：第一部分，上下文无关文法。

    Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])

    [http://arxiv.org/abs/2305.13673](http://arxiv.org/abs/2305.13673)

    本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。

    

    我们设计了实验来研究生成式语言模型（例如GPT）如何学习上下文无关文法（CFG）-具有树状结构的多样化语言系统，可捕捉许多自然语言，程序和人类逻辑的方面。CFG与下推自动机一样困难，可能是模棱两可的，因此验证字符串是否满足规则需要动态规划。我们构造了人造数据，并证明即使对于非常具有挑战性的CFG，预训练transformers也可以学会生成具有接近完美准确度和显着多样性的句子。更重要的是，我们深入探讨了transformers学习CFG背后的物理原理。我们发现transformer内部的隐藏状态隐含而精确地编码了CFG结构（如在子树边界上精确定位树节点信息），并学会形成类似动态规划的“边界到边界”的注意力。我们还涵盖了一些标准CFG的扩展，例如概率CFG和线性CFG，并展示transformers也可以学会这些扩展语法结构。我们的工作揭示了语言模型的内部工作原理，并为未来的模型设计和分析提供了启示。

    We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
    
[^183]: NUBO：一个透明的 Python 包用于贝叶斯优化

    NUBO: A Transparent Python Package for Bayesian Optimisation. (arXiv:2305.06709v1 [cs.LG])

    [http://arxiv.org/abs/2305.06709](http://arxiv.org/abs/2305.06709)

    NUBO是一个透明的Python包，用于优化昂贵的黑盒函数，它利用高斯过程做代理模型以及获取函数来指导选择候选点，专注于透明度和用户体验。

    

    NUBO（Newcastle University Bayesian Optimisation）是一个贝叶斯优化框架，用于优化昂贵的黑盒函数，比如物理实验和计算机模拟器。它利用高斯过程做代理模型、并通过获取函数来选择用于全局最优化的候选点。NUBO专注于透明度和用户体验，以便让不同领域的研究人员更容易使用贝叶斯优化。

    NUBO, short for Newcastle University Bayesian Optimisation, is a Bayesian optimisation framework for the optimisation of expensive-to-evaluate black-box functions, such as physical experiments and computer simulators. Bayesian optimisation is a cost-efficient optimisation strategy that uses surrogate modelling via Gaussian processes to represent an objective function and acquisition functions to guide the selection of candidate points to approximate the global optimum of the objective function. NUBO itself focuses on transparency and user experience to make Bayesian optimisation easily accessible to researchers from all disciplines. Clean and understandable code, precise references, and thorough documentation ensure transparency, while user experience is ensured by a modular and flexible design, easy-to-write syntax, and careful selection of Bayesian optimisation algorithms. NUBO allows users to tailor Bayesian optimisation to their specific problem by writing the optimisation loop the
    
[^184]: 用BREC数据集更好地评估GNN表达力

    Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])

    [http://arxiv.org/abs/2304.07702](http://arxiv.org/abs/2304.07702)

    本论文介绍了一个新的Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)数据集，并使用BREC评估了几种现有的GNN模型的表达力，表明一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。

    

    关于图神经网络（GNN）的理论表达力的研究得到了快速发展，并提出了许多增强表达力的方法。然而，除了严格遵循k维Weisfeiler-Lehman（k-WL）测试层次结构的少数方法外，大多数方法都没有统一的表达力度量。它们的理论分析通常限于区分某些非同构图族，导致在定量比较表达力方面存在困难。与理论分析相反，衡量表达能力的另一种方法是在包含1-WL不可区分图的特定数据集上评估模型性能。然而，以前专门设计用于此目的的数据集面临着难度（任何超越1-WL的模型准确率几乎达到100％）、粒度（模型倾向于要么完全正确，要么接近随机猜测）和规模（每个数据集中仅有少量本质不同的图）的问题。为了解决这些受限制的评估问题，我们提出了一个新的GNN鲁棒性评估基准（BREC），该基准包含许多结构多样的图，并允许对模型表达力进行更精细的评估。我们使用BREC评估了几种现有的GNN模型的表达力，并展示了一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。

    Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
    
[^185]: MedNeXt：用于医学图像分割的变压器驱动卷积神经网络的可扩展性

    MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])

    [http://arxiv.org/abs/2303.09975](http://arxiv.org/abs/2303.09975)

    MedNeXt是一个定制化的现代化可扩展卷积神经网络，用于解决数据稀缺的医学环境挑战。该网络包含：完全ConvNeXt 3D编码器-解码器网络、残差ConvNeXt上下采样块和一种新的迭代增加核大小的技术。

    

    近年来，在医学图像分割中使用基于 Transformer 的架构越来越多，但是由于缺乏大规模标注的医学数据集，使得其性能远不如自然图像。相比之下，卷积神经网络具有更高的归纳偏差，因此更容易训练到高性能水平。最近，ConvNeXt 架构尝试通过镜像变压器块来现代化标准卷积神经网络。在这项工作中，我们改进了这一架构，设计了一种现代化且可扩展的卷积神经网络，以应对数据稀缺的医学环境的挑战。我们引入 MedNeXt，这是一个受变压器启发的大核分割网络，其中包括：1）用于医学图像分割的完全 ConvNeXt 3D 编码器 - 解码器网络，2）残差 ConvNeXt 上下采样块，以在各个尺度上保留语义信息，3）一种新的技术，通过上采样小核来迭代增加核大小。

    There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
    
[^186]: 扩散模型增强的行为克隆

    Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13335](http://arxiv.org/abs/2302.13335)

    本研究提出了一种模仿学习框架，扩散模型增强的行为克隆（DBC），该模型同时建模专家分布的条件和联合概率，有效避免了建模复杂度和推理时间的问题。

    

    模仿学习解决了通过观察专家演示而没有访问环境奖励信号的学习挑战。大多数现有的不需要与环境交互的模仿学习方法，要么将专家分布建模为条件概率p(a|s)（例如，行为克隆，BC），要么将联合概率p(s,a)建模（例如，隐式行为克隆）。尽管行为克隆对于建模条件概率的简单性，但通常难以泛化。虽然对联合概率进行建模可以提高泛化性能，但推理过程可能耗时，并且往往会遭受流形过拟合的问题。本文提出了一个模仿学习框架，它从建模专家分布的条件和联合概率中受益。我们提出的扩散模型增强的行为克隆（DBC）采用训练有素的扩散模型来建模专家行为，并学习一种策略以最大化根据混合概率分布采样的回报。

    Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
    
[^187]: 持续同调在图学习中的表达性

    On the Expressivity of Persistent Homology in Graph Learning. (arXiv:2302.09826v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09826](http://arxiv.org/abs/2302.09826)

    本文通过在图学习任务中的理论讨论和实证分析，证明了持续同调技术在捕捉具有显著拓扑结构的数据集中的长程图性质方面表现出的高表达性。

    

    近来，计算拓扑学中的一项技术，持续同调展现出在图分类方面强大的实证性能。它能够通过高阶拓扑特征——如任意长度的环——以及多尺度拓扑描述符捕捉长程图性质，从而提高对具有显著拓扑结构的数据集——如分子——的预测性能。与此同时，持续同调的理论性质在这个背景下尚未得到正式评估。本文旨在通过提供持续同调在图中的简要介绍以及对其在图学习任务中的表达性进行理论讨论和实证分析，弥合计算拓扑学和图机器学习之间的差距。

    Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.
    
[^188]: 带有目标预测扩散混合的图生成

    Graph Generation with Destination-Predicting Diffusion Mixture. (arXiv:2302.03596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03596](http://arxiv.org/abs/2302.03596)

    本文介绍了一种名为目标预测扩散混合的方法，用于解决传统扩散模型不能很好地建模图拓扑结构的问题，并在图生成任务上取得了最先进的性能表现。

    

    生成图是理解其非欧几里得结构复杂性的真实任务的主要挑战。虽然扩散模型在图生成方面最近取得了显着成功，但它们不适合建模图的结构信息，因为学习去噪声样本不能明确地捕捉图的拓扑结构。为了解决这个限制，我们提出了一个新颖的生成框架，通过预测扩散过程的目标，即具有正确拓扑信息的原始图作为数据的加权平均值，建模了图的拓扑结构。具体而言，我们将生成过程设计为一个以数据分布中的终点为条件的扩散过程混合，它将过程推向预测的目标，并实现快速收敛。我们引入了预测目标的新型无仿真训练目标，并进一步讨论了将这种策略纳入图生成任务中的优势。我们在几个基准数据集上的实验表明，我们提出的方法在各种评估指标上均实现了最先进的性能。

    Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the structural information of graphs since learning to denoise the noisy samples does not explicitly capture the graph topology. To tackle this limitation, we propose a novel generative framework that models the topology of graphs by predicting the destination of the diffusion process, which is the original graph that has the correct topology information, as a weighted mean of data. Specifically, we design the generative process as a mixture of diffusion processes conditioned on the endpoint in the data distribution, which drives the process toward the predicted destination, resulting in rapid convergence. We introduce new simulation-free training objectives for predicting the destination, and further discuss the advantages of 
    
[^189]: GFlowNets与变分贝叶斯的因果结构和机制学习

    Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes. (arXiv:2211.02763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02763](http://arxiv.org/abs/2211.02763)

    本文介绍了一种新的方法，使用GFlowNets和变分贝叶斯联合学习因果模型的结构和机制，不仅能够处理非线性和非高斯数据，在模拟数据上也能与几个基线方法相竞争。

    

    贝叶斯因果结构学习旨在学习有向无环图（DAG）上的后验分布和定义父变量和子变量之间关系的机制。本文引入一种新的方法，使用变分贝叶斯联合学习因果模型的结构和机制，称为变分贝叶斯DAG-GFlowNet（VBG）。我们使用GFlowNets扩展了贝叶斯因果结构学习的方法，不仅学习结构的后验分布，还学习线性高斯模型的参数。我们在模拟数据上的结果表明，VBG在建模DAG和机制的后验分布时，不仅能够处理非线性和非高斯数据，而且还能与几个基线方法相竞争。

    Bayesian causal structure learning aims to learn a posterior distribution over directed acyclic graphs (DAGs), and the mechanisms that define the relationship between parent and child variables. By taking a Bayesian approach, it is possible to reason about the uncertainty of the causal model. The notion of modelling the uncertainty over models is particularly crucial for causal structure learning since the model could be unidentifiable when given only a finite amount of observational data. In this paper, we introduce a novel method to jointly learn the structure and mechanisms of the causal model using Variational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We extend the method of Bayesian causal structure learning using GFlowNets to learn not only the posterior distribution over the structure, but also the parameters of a linear-Gaussian model. Our results on simulated data suggest that VBG is competitive against several baselines in modelling the posterior over DAGs an
    
[^190]: 机器学习与机密计算：知识系统化的概述

    Machine Learning with Confidential Computing: A Systematization of Knowledge. (arXiv:2208.10134v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.10134](http://arxiv.org/abs/2208.10134)

    本文研究机器学习和机密计算的结合，并梳理了先前的研究成果，提供了关于保证机密性和完整性的技术，同时讨论了它们的高级特性和局限性。本文进一步确定了现有的可信执行环境（TEE）系统在机器学习用例中的限制，并讨论了未来的展望。

    

    随着机器学习的广泛发展和攻击面的扩大，机器学习中的隐私和安全挑战日益严重。作为一种成熟的系统级方法，机密计算已被学术界和工业界用于缓解各种机器学习场景中的隐私和安全问题。本文研究了机器学习和机密计算之间的结合。我们系统梳理了先前基于机密计算辅助的机器学习技术，提供了i）机密性保证和ii）完整性保证，并讨论了它们的高级特性和缺陷。进一步确定了关键挑战，并对现有的可信执行环境（TEE）系统在机器学习用例中的限制进行了专门的分析。最后，讨论了展望性的工作，包括闭环保护的基于地面的隐私定义，高效机器学习的分区执行，专门的TEE辅助机器学习设计。

    Privacy and security challenges in Machine Learning (ML) have become increasingly severe, along with ML's pervasive development and the recent demonstration of large attack surfaces. As a mature system-oriented approach, Confidential Computing has been utilized in both academia and industry to mitigate privacy and security issues in various ML scenarios. In this paper, the conjunction between ML and Confidential Computing is investigated. We systematize the prior work on Confidential Computing-assisted ML techniques that provide i) confidentiality guarantees and ii) integrity assurances, and discuss their advanced features and drawbacks. Key challenges are further identified, and we provide dedicated analyses of the limitations in existing Trusted Execution Environment (TEE) systems for ML use cases. Finally, prospective works are discussed, including grounded privacy definitions for closed-loop protection, partitioned executions of efficient ML, dedicated TEE-assisted designs for ML, 
    
[^191]: 你的公平模型有多稳健？探索不同公平策略的稳健性。

    How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies. (arXiv:2207.04581v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04581](http://arxiv.org/abs/2207.04581)

    本文提出了一个新的标准来衡量公平优化策略的稳健性——稳健比率，并使用三种公平策略在五个公平数据集上进行了多次广泛的实验。结果表明，公平策略的稳健性在不同数据集之间和不同公平性定义之间存在显着差异。

    

    随着机器学习在高风险决策中的应用，确保算法公平性已成为一个愈发重要的问题。为此，已经提出了许多数学上的公平性定义，并开发了各种优化技术，旨在最大化定义的公平性概念。然而，公平解决方案依赖于训练数据的质量，而且对噪声非常敏感。最近的研究表明，稳健性（模型在未知数据上表现良好的能力）在应对新问题时应使用的策略类型中起着重要作用，因此衡量这些策略的稳健性已成为一个基本问题。因此，在本文中，我们提出了一个新的标准来衡量各种公平优化策略的稳健性 - 稳健比率。我们使用三种最常用的公平策略对五个基准公平数据集进行了多次广泛的实验，并表明这些策略的稳健性在数据集之间和不同公平性定义之间存在显着差异。我们的结果表明，在设计和选择公平算法时，应更加谨慎地考虑稳健性，以确保它们在实际场景中始终有效可靠。

    With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of 
    
[^192]: 在NVIDIA网卡中实现强化学习数据中心拥塞控制

    Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs. (arXiv:2207.02295v4 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2207.02295](http://arxiv.org/abs/2207.02295)

    本文在NVIDIA网卡中实现了强化学习数据中心拥塞控制，通过将RL-CC的复杂神经网络转化为决策树，实现了实时推理，并成功改善了网络拥塞下的尾部延迟和数据包丢失问题。

    

    随着通信协议的发展，数据中心网络的利用率越来越高，拥塞更为频繁，导致延迟和丢包率增加。这种情况下，人工设计拥塞控制算法变得极其困难，需要开发人工智能方法来替代人力。但是，由于网络设备计算能力有限，目前不可能在网络设备上部署AI模型。本文提出了一个解决方案，基于最新的强化学习拥塞控制算法[arXiv:2207.02295]，构建了一个基于决策树的计算轻量级解决方案，将RL-CC的复杂神经网络转化为决策树，将其推理时间降低了500倍，使其在μ秒级决策时间要求内实现实时推理，且对质量影响不大。我们在一个实时集群中部署了转换后的策略，并与现代数据中心部署的流行拥塞控制算法进行了比较。在类似的流量条件下，我们的解决方案将尾部延迟率提高了x%，将数据包丢失率降低了y%。

    As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [arXiv:2207.02295]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the $\mu$-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algor
    
[^193]: 什么是公平性？哲学的思考与对fairML的影响

    What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09622](http://arxiv.org/abs/2205.09622)

    本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。

    

    在公平性人工智能(fairML)领域，通过定义衡量模型公平性的度量和提出确保训练模型数据具有低公平性度量值的方法，来减轻人工智能(ML)产生的相关不公平性问题。然而，公平的基本概念，即"公平是什么"，很少被讨论，这造成了公平性研究在哲学领域几个世纪的讨论与近期被应用于机器学习领域之间的鸿沟。本文试图通过形式化一致性公平概念和将哲学思考转化为ADM系统中ML模型训练和评估的形式框架，来架起这一鸿沟。我们指出，不公平性问题可能已经存在，即使没有受保护性属性的存在，强调公平性和预测性能不是不可调和的对立面，而是前者实现的必要条件。我们提出的框架强调将伦理考虑纳入ML管道的所有阶段，从数据收集到最终部署模型的评估。

    A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
    
[^194]: 关系式自监督学习

    Relational Self-Supervised Learning. (arXiv:2203.08717v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.08717](http://arxiv.org/abs/2203.08717)

    本文介绍了一种关系式自监督学习（ReSSL）框架，通过建模不同实例之间的关系来学习视觉表示，以弥补当前自监督学习方法中对不同实例关系的缺乏关注。

    

    自监督学习（SSL），包括主流的对比学习，在没有数据注释的情况下取得了很大的成功，可以学习视觉表示。然而，大多数方法主要关注实例级信息（即，相同实例的不同增强图像应具有相同的特征或聚集到相同的类别），但对不同实例之间的关系缺乏关注。在本文中，我们介绍了一种新的自监督学习范式，即关系式自监督学习（ReSSL）框架，通过建模不同实例之间的关系来学习表示。具体而言，我们提出的方法利用不同实例之间的成对相似性的锐化分布作为“关系”度量，然后利用该度量匹配不同增强的特征嵌入。为了提高性能，我们认为弱增强对于表示更可靠的关系很重要，并利用动量技术来实现。

    Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most methods mainly focus on the instance level information (\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduce a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. To boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum s
    
[^195]: 基于SPD流形的深度最优传输领域自适应

    Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05745](http://arxiv.org/abs/2201.05745)

    这项研究介绍了一种基于深度最优传输的方法，用于解决在SPD流形上的领域自适应问题。通过利用最优传输理论和SPD流形的对数欧几里得几何，我们克服了协方差矩阵操作的复杂性挑战。

    

    近年来，机器学习界对于在对称正定（SPD）流形上解决领域自适应（DA）问题表现出了很大兴趣。这种兴趣源于医疗设备产生的复杂神经物理数据（如脑电图、脑磁图和扩散张量成像）在不同领域之间存在数据分布的偏移。这些数据表示以信号协方差矩阵的形式表示，并具有对称性和正定性的属性。然而，由于协方差矩阵的复杂操作特性，直接将先前的经验和解决方案应用于DA问题存在挑战。为了解决这个问题，我们的研究引入了一类基于深度学习的迁移学习方法，称为深度最优传输。这一类方法利用最优传输理论，并利用SPD流形的对数欧几里得几何。此外，我们还展示了...

    In recent years, there has been significant interest in solving the domain adaptation (DA) problem on symmetric positive definite (SPD) manifolds within the machine learning community. This interest stems from the fact that complex neurophysiological data generated by medical equipment, such as electroencephalograms, magnetoencephalograms, and diffusion tensor imaging, often exhibit a shift in data distribution across different domains. These data representations, represented by signal covariance matrices, possess properties of symmetry and positive definiteness. However, directly applying previous experiences and solutions to the DA problem poses challenges due to the manipulation complexities of covariance matrices.To address this, our research introduces a category of deep learning-based transfer learning approaches called deep optimal transport. This category utilizes optimal transport theory and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we present a com
    
[^196]: 揭开黑盒子：调控算法决策

    Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2110.03443](http://arxiv.org/abs/2110.03443)

    本文研究如何在代理使用复杂的“黑盒”预测函数进行决策的情况下，对算法决策进行最优调控。研究发现，限制代理使用透明度足够高的预测函数是低效的，而针对激励偏差源头的目标化工具可以提供次优解决方案，从而改善福利。

    

    我们展示了如何在一个代理使用复杂的“黑盒”预测函数进行决策（如贷款、医疗测试或招聘）且委托人在了解代理的黑盒模型方面有限的情况下，最优地调控预测算法。我们证明，只要诱导不足，且最优预测函数足够复杂，将代理限制在足够透明的预测函数中是低效的。算法审计有助于提高福利，但其收益取决于审计工具的设计。许多解释工具倾向于最小化整体信息损失，但这通常是低效的，因为它们集中于解释预测函数的平均行为。针对性的工具，如针对激励偏差源头（如过多的假阳性或种族差异）的工具，可以提供次优解决方案。我们提供了对我们理论的实证支持。

    We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
    

