# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bifurcations and loss jumps in RNN training.](http://arxiv.org/abs/2310.17561) | 这篇论文研究了在RNN训练中的分歧现象和损失跳跃，并证明了在特定类型的RNN中存在着某些分歧。 |
| [^2] | [Towards Matching Phones and Speech Representations.](http://arxiv.org/abs/2310.17558) | 本研究探讨了学习手机类型的问题，并将其作为匹配聚类中心与手机嵌入的问题。通过生成伪标签和引入新的损失函数，我们改进了自监督表示，实验证明匹配结果捕捉到了手机之间的关系，并显著提高了手机分类的性能。 |
| [^3] | [Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient Descent.](http://arxiv.org/abs/2310.17556) | 这个论文提出了一个针对大规模场景下阻尼自然梯度下降的高效数值算法，通过使用Cholesky分解，解决了参数数量超过样本数量的问题，相比现有方法，该算法速度更快。 |
| [^4] | [Interactive Robot Learning from Verbal Correction.](http://arxiv.org/abs/2310.17555) | 本研究设计了一个新的基于大型语言模型（LLM）OLAF的交互式机器人学习系统，它允许日常用户通过口头纠正教授机器人，并能根据口头反馈更新机器人的视觉运动神经策略，从而避免重复错误。实验结果表明，在模拟和物理硬件上，该系统在长时间线的操纵任务中平均改善了20.0%的策略成功率。 |
| [^5] | [Model-Based Runtime Monitoring with Interactive Imitation Learning.](http://arxiv.org/abs/2310.17552) | 本文提出了一种基于模型的运行时监控算法，通过学习部署数据来检测系统异常并预测故障，旨在解决机器人学习中的泛化和鲁棒性挑战。 |
| [^6] | [Human-Guided Complexity-Controlled Abstractions.](http://arxiv.org/abs/2310.17550) | 本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。 |
| [^7] | [Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting.](http://arxiv.org/abs/2310.17544) | 这项研究提出了一种基于层次集成的特征选择方法，能够克服传统方法和最先进方法在非平稳和特征数目庞大且样本有限的情况下的局限性，并在合成和实际数据集上展示了更好的性能。 |
| [^8] | [EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving.](http://arxiv.org/abs/2310.17540) | 本研究发展了EqDrive模型，通过使用EqMotion等变粒子和人类预测模型以及多模式预测机制，在自动驾驶中实现了高效的车辆运动预测。该模型在模型容量较低、参数更少、训练时间显著缩短的情况下，取得了业界最先进的性能。 |
| [^9] | [Little Exploration is All You Need.](http://arxiv.org/abs/2310.17538) | 本研究提出了UCB$^\tau$算法，通过对任务困难度的调整，实现了更好的效能和较低的风险。 |
| [^10] | [Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity.](http://arxiv.org/abs/2310.17537) | 提出了一种受神经启发的方法，通过分段和回溯来克服深度强化学习中的灾难性遗忘问题，并通过内在奖励激励代理探索新的状态。 |
| [^11] | [SoK: Pitfalls in Evaluating Black-Box Attacks.](http://arxiv.org/abs/2310.17534) | 提出了一个评估黑盒攻击的分类法，揭示了未开发的威胁空间，并展示了在某些设置上已有技术的局限性。 |
| [^12] | [Learning Regularized Graphon Mean-Field Games with Unknown Graphons.](http://arxiv.org/abs/2310.17531) | 我们设计了用于学习图核均值场博弈的强化学习算法，通过近端策略优化算法GMFG-PPO以及核嵌入的方法来估计未知图核。我们的算法在收敛速度和效率方面取得了改进，并提供了相关的理论分析。 |
| [^13] | [Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models.](http://arxiv.org/abs/2310.17530) | 本研究评估了性别中立的预训练视觉-语言模型中的偏见和公平性，并发现预训练和微调后的偏见放大是相互独立的。此外，持续预训练对性别中立数据有利，可以在一些任务中促进公平。 |
| [^14] | [Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages.](http://arxiv.org/abs/2310.17526) | 本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。 |
| [^15] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^16] | [Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions.](http://arxiv.org/abs/2310.17502) | 本文提出了一种方法来生成不能与真实人类相关联的人工说话者嵌入，从而实现在语音合成中的声音和说话风格的直观和精细控制。这种方法不需要任何标签，并且在训练期间将真实人类的嵌入与人工可控嵌入相关联，从而在推理过程中确保了隐私。 |
| [^17] | [The IMS Toucan System for the Blizzard Challenge 2023.](http://arxiv.org/abs/2310.17499) | 我们改进了我们在Blizzard Challenge 2021中提交的系统，使用一个基于规则的文本到音素处理系统，并且设计了数据处理、训练和推理过程来处理Blizzard Challenge 2023的数据。我们的系统标识符是G。 |
| [^18] | [CBD: A Certified Backdoor Detector Based on Local Dominant Probability.](http://arxiv.org/abs/2310.17498) | 本文提出了第一个可信后门检测器（CBD），它基于一种新颖的、可调节的符合预测方案，即局部主导概率统计。CBD能够提供对分类器的检测推断结果，并给出攻击保证可检测的条件和假阳性率的概率上界。实验证明，具有更高鲁棒性的触发器和更小扰动幅度的攻击更容易被检测出来。 |
| [^19] | [Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach.](http://arxiv.org/abs/2310.17496) | 该论文提出了一种名为加权训练的方法，用于解决A/B测试中由数据训练循环引起的干扰。该方法通过训练模型预测每个数据点在实验组或控制组中出现的概率，并在模型训练过程中应用加权损失，从而实现了最小方差的估计结果，并且不会引起训练分布的变化。 |
| [^20] | [Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2310.17492) | 本研究提出了一种基于模拟器辅助的移动边缘调优的AI基础模型编排方法，该方法通过创新的模拟器适配器架构和混合多智能体深度强化学习策略，实现了高效部署和精调基础模型，从而提高了本地任务性能。 |
| [^21] | [FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing.](http://arxiv.org/abs/2310.17491) | FedPEAT是将辅助调优和参数高效微调应用于联邦学习的方法，能够提升基础人工智能模型的模型隐私和内存效率。 |
| [^22] | [Bias in Evaluation Processes: An Optimization-Based Model.](http://arxiv.org/abs/2310.17489) | 本研究提出了一个基于优化的模型，用于评估过程中的偏见分析。模型通过将真实效用分布转化为观察到的分布来考虑偏见，并对参数进行研究，以探究其对观察到的分布的影响。通过实证验证和数据拟合，我们证明了该模型的有效性。 |
| [^23] | [Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach.](http://arxiv.org/abs/2310.17485) | 这篇论文提出了一种采用深度多智能体强化学习方法解决公平协作车辆路径规划问题的方法。通过合作谈判博弈模型，并在生产中显式推理而不是访问特征函数的方式，有效降低了成本和计算复杂度。 |
| [^24] | [Secure short-term load forecasting for smart grids with transformer-based federated learning.](http://arxiv.org/abs/2310.17477) | 该论文提出了一种基于Transformer的联邦学习方法，用于智能电网的短期负荷预测。该方法可以提高数据隐私性，并在实验中展示出良好的性能。 |
| [^25] | [Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End Collaboration.](http://arxiv.org/abs/2310.17471) | 该论文分析了从数据、智能和网络的角度来实现6G原生AI的挑战，并提出了基于基础模型的6G原生AI框架，包括自定义方法和任务导向的AI工具包，以及新的云边缘协同合作范式。 |
| [^26] | [Cross-modal Active Complementary Learning with Self-refining Correspondence.](http://arxiv.org/abs/2310.17468) | 本文提出了一种跨模态主动互补学习框架（CRCL），通过使用新颖的主动互补损失（ACL）和高效的自我完善对应关系修正（SCC），改善了现有方法的鲁棒性。 |
| [^27] | [The statistical thermodynamics of generative diffusion models.](http://arxiv.org/abs/2310.17467) | 本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。 |
| [^28] | [Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation.](http://arxiv.org/abs/2310.17463) | 本文提出了一种新颖的贝叶斯神经控制微分方程方法，用于连续时间的治疗效果估计，该方法能够提供对潜在结果的后验预测分布，并给出了可靠的不确定性估计。 |
| [^29] | [Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion.](http://arxiv.org/abs/2310.17462) | 该论文提出了一种新的方法，利用物体运动的物理知识和简单的2D标注，从单目图像中实现精确的3D物体定位，无需昂贵的3D标注。经过实验证明，在真实数据实验中平均距离误差仅为6厘米，表明该方法在无法收集3D数据进行训练的情况下具有潜力。 |
| [^30] | [Coalitional Bargaining via Reinforcement Learning: An Application to Collaborative Vehicle Routing.](http://arxiv.org/abs/2310.17458) | 本论文提出了一种通过强化学习方法解决协作车辆路径规划中合作伙伴选择和补偿分配的问题的方法，并将问题建模为联合讨价还价博弈，通过隐式推理特征函数来减少计算复杂性。 |
| [^31] | [Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language.](http://arxiv.org/abs/2310.17437) | 这项研究在手语识别中提出了一个不受帧序列约束的概率模型，利用位置、运动和手形等特征进行手势分类，通过词袋模型方法探索手势的统计特征。 |
| [^32] | [Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models.](http://arxiv.org/abs/2310.17432) | 本论文提出了一种基于似然的离群检测方法，通过利用扩散模型受输入样本复杂性影响的观察，构建了复杂度校正似然比。实验结果表明该方法在离群检测方面达到了最先进的水平。 |
| [^33] | [Handshape recognition for Argentinian Sign Language using ProbSom.](http://arxiv.org/abs/2310.17427) | 本文的两个主要贡献是：首次创建了一个针对阿根廷手语的手势数据库，并提出了一种使用ProbSom进行图像处理和手势分类的技术，该技术在目前的研究中与其他方法进行了对比。 |
| [^34] | [Causal Modeling with Stationary Diffusions.](http://arxiv.org/abs/2310.17405) | 本文提出了一种新颖的因果推断方法，使用随机微分方程建模系统行为，不需要因果图的形式化。在多种情况下，该方法比传统方法更好地推广到未见干预的变量。 |
| [^35] | [Invariance Measures for Neural Networks.](http://arxiv.org/abs/2310.17404) | 本文提出了一种用于量化神经网络不变性的测量方法，该方法敏感且可解释，并能应用于任何神经网络模型。在仿射变换领域和CIFAR10和MNIST数据集上的验证表明，神经网络的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。这些测量方法将为不变性表示的新研究方向提供可能性。 |
| [^36] | [Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow.](http://arxiv.org/abs/2310.17403) | 本文对光流中的对抗性贴片攻击进行了研究，发现目前的检测和去除防御策略不仅降低了光流质量，同时也损害了抵御贴片攻击的鲁棒性。 |
| [^37] | [Enhancing Graph Neural Networks with Structure-Based Prompt.](http://arxiv.org/abs/2310.17394) | 这篇论文提出了一种以结构为基础的图神经网络的提示方法（SAP），该方法在预训练和提示调整阶段都一致地利用结构信息，从而增强了图神经网络在学习任务特定参数方面的能力。 |
| [^38] | [A Challenge in Reweighting Data with Bilevel Optimization.](http://arxiv.org/abs/2310.17386) | 在重新加权数据的任务中，经典的双层优化方法可能会导致次优解，使得最终的数据权重非常稀疏，这解释了为什么这种方法在实践中很少被使用。 |
| [^39] | [Multitask Online Learning: Listen to the Neighborhood Buzz.](http://arxiv.org/abs/2310.17385) | 我们提出了一种多任务在线学习的算法，代理只能通过邻居交换信息。我们的分析表明，当代理在相似的任务上操作时，我们的算法的遗憾值显著改善。此外，我们证明了算法在损失函数为线性函数时可以保护隐私。 |
| [^40] | [On the recognition of the game type based on physiological signals and eye tracking.](http://arxiv.org/abs/2310.17383) | 本文提出了一种基于生理信号和眼动追踪的游戏类型识别方法，并通过构建分类器在不同游戏和游戏间暂停的情况下进行了验证。研究结果表明该方法可以应用于智能监控和量化自身领域。 |
| [^41] | [Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle.](http://arxiv.org/abs/2310.17378) | 本文通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限，通过限制网络梯度对于输入数据在优化轨迹上的扰动的灵敏度，不显式地依赖网络的深度。 |
| [^42] | [Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning.](http://arxiv.org/abs/2310.17360) | 本文提出了一种统一的时空图学习方法，即统一时空扩散模型（USTD），用于处理物联网时代的各种学习任务。USTD综合考虑了时空数据中的不确定性，并利用共享的时空模式进行预测。此方法具有通用性和灵活性。 |
| [^43] | [Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning.](http://arxiv.org/abs/2310.17356) | 本论文提出了一种基于天空图像的机器学习预测太阳辐照度的新方法，通过提取天空图像特征并利用机器学习技术来估计太阳辐照度。经过与现有算法的比较，我们的方法在性能上表现出竞争力，并且具有更低的计算复杂度。(机器翻译) |
| [^44] | [Exploring the Trie of Rules: a fast data structure for the representation of association rules.](http://arxiv.org/abs/2310.17355) | 本文提出了一种新的数据结构，称为规则前缀树，用于存储由关联规则挖掘生成的规则集。这个数据结构能够高效地表示和提取有意义的挖掘知识。 |
| [^45] | [De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks.](http://arxiv.org/abs/2310.17341) | 本研究结合递归神经网络（RNN）和临时卷积神经网络（TCN），使用新的反应Smiles-like表示实现了全新的化学反应生成，并通过迁移学习发现微调协议对模型生成范围有重要影响。 |
| [^46] | [A multi-artifact EEG denoising by frequency-based deep learning.](http://arxiv.org/abs/2310.17335) | 本论文提出了一种基于频域的新型脑电图去噪模型，通过利用噪声频谱特征的先验知识，自适应地计算噪声分离的最优滤波器。经过实验证明，该模型在脑电图去噪方面取得了最优结果。 |
| [^47] | [On Forecast Stability.](http://arxiv.org/abs/2310.17332) | 本文研究了预测稳定性的两种类型：垂直稳定性和水平稳定性，并提出了一种适用于任何基础模型的简单线性插值方法来实现这种稳定性。这种方法可以产生准确而稳定的预测。 |
| [^48] | [CQM: Curriculum Reinforcement Learning with a Quantized World Model.](http://arxiv.org/abs/2310.17330) | CQM提出了一种新的课程增强学习方法，通过自动定义语义目标空间和提出课程目标，在解决复杂任务中取得了显著进展。该方法通过向量量化-变分自动编码器(VQ-VAE)将连续观察结果进行离散化，并通过图形恢复离散观察结果之间的时间关系，提供了不确定性和时间距离感知的课程目标。 |
| [^49] | [C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder.](http://arxiv.org/abs/2310.17325) | 本文提出了一个名为C-Disentanglement的框架，旨在发现具有因果关系且受混淆因素影响的生成因子，以提高数据生成的可控性和鲁棒性。 |
| [^50] | [Demonstration-Regularized RL.](http://arxiv.org/abs/2310.17303) | 通过演示-正则化提高强化学习的采样效率，并找到最优策略的样本复杂度，该复杂度与专家演示数量成反比。 |
| [^51] | [BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds.](http://arxiv.org/abs/2310.17281) | BEVContrast是一种在汽车激光雷达点云中使用BEV空间自我监督的方法，通过对比不同场景下的特征，提供了一种在计算成本和性能之间取得平衡的解决方案。 |
| [^52] | [Looping in the Human: Collaborative and Explainable Bayesian Optimization.](http://arxiv.org/abs/2310.17273) | 协作和可解释的贝叶斯优化框架(CoExBO)在贝叶斯优化中引入了循环，平衡了人工智能和人类的合作关系。它利用偏好学习将用户见解融合到优化中，解释每次迭代的候选选择，从而增强用户对优化过程的信任，并提供无害保证。 |
| [^53] | [Variance of ML-based software fault predictors: are we really improving fault prediction?.](http://arxiv.org/abs/2310.17264) | 本论文研究了基于机器学习的软件缺陷预测模型中的方差问题，并指出这种方差对于研究的可重复性和模型在实际应用中的性能有重要影响。 |
| [^54] | [fairret: a Framework for Differentiable Fairness Regularization Terms.](http://arxiv.org/abs/2310.17256) | 本论文介绍了一种称为fairret的可微公平性正则化项框架，通过模块化的目标量化偏见，并可以轻松集成到自动微分流程中。通过从线性分式统计角度定义公平性，可以高效计算多种类型的公平性正则化项。实验证明，fairret框架与基准相比在强制执行公平性时几乎不损失预测能力。 |
| [^55] | [IDENAS: Internal Dependency Exploration for Neural Architecture Search.](http://arxiv.org/abs/2310.17250) | IDENAS是一种集成神经架构搜索和特征选择的方法，通过探索内部依赖性来提高分类任务的性能。 |
| [^56] | [Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity.](http://arxiv.org/abs/2310.17247) | 本文发现神经网络中的grokking现象不仅局限于神经网络，还出现在其他算法和模型中。通过在数据集中添加虚假信息的维度，可以诱发grokking现象。研究表明，grokking现象在解决方案搜索受复杂性和错误指导的任何情况下可能发生。这对理解grokking现象提供了更广泛的理论支持。 |
| [^57] | [CROP: Conservative Reward for Model-based Offline Policy Optimization.](http://arxiv.org/abs/2310.17245) | CROP提出了一种保守奖励的模型训练方法用于基于模型的离线策略优化，通过同时最小化估计误差和随机动作奖励来实现保守的奖励估计。 |
| [^58] | [Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks.](http://arxiv.org/abs/2310.17238) | 本文提出了基于超图神经网络的联合实体和关系抽取方法，使用span剪枝机制减轻误差传播问题，通过构建超图进行高阶建模，实现多个实体和关系之间的交互。 |
| [^59] | [Codebook Features: Sparse and Discrete Interpretability for Neural Networks.](http://arxiv.org/abs/2310.17230) | 本研究提出了一种称为codebook特征的方法，通过将神经网络的连续特征量化为离散向量码的总和来实现稀疏和离散的隐藏状态。实验证明，神经网络在这种极端瓶颈条件下运行时性能下降适度，同时这种方法还提供了一种直观的神经网络行为控制方式。 |
| [^60] | [Beyond MLE: Convex Learning for Text Generation.](http://arxiv.org/abs/2310.17217) | 本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。 |
| [^61] | [Weakly-Supervised Surgical Phase Recognition.](http://arxiv.org/abs/2310.17209) | 本文提出了一种弱监督手术阶段识别的方法，在每帧的阶段预测中结合了图分割和自监督学习，使用了稀疏时间戳或少样本学习进行弱监督。该方法具有低复杂度，并在实验中展现了令人期待的性能。 |
| [^62] | [miditok: A Python package for MIDI file tokenization.](http://arxiv.org/abs/2310.17202) | MidiTok是一个用于将符号音乐进行分词的Python包，具有灵活性和扩展性，支持最流行的音乐分词方式，并提供了统一的API。 |
| [^63] | [Taming Gradient Variance in Federated Learning with Networked Control Variates.](http://arxiv.org/abs/2310.17200) | 本论文提出了一种名为FedNCV的新型框架，用于解决联邦学习中梯度方差的问题。通过在客户端和服务器级别实现REINFORCE Leave-One-Out (RLOO)作为控制变量单元，优化了本地梯度更新并提供了无偏和低方差的聚合梯度，实现了稳健的全局更新。 |
| [^64] | [How do Language Models Bind Entities in Context?.](http://arxiv.org/abs/2310.17191) | 通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。 |
| [^65] | [Adaptive important sampling for Deep Ritz.](http://arxiv.org/abs/2310.17185) | 该论文介绍了一种自适应重要采样方法，用于Deep Ritz方法求解偏微分方程。该方法利用两个深度神经网络，一个用于逼近解，另一个用于生成新的插值点以改进训练集。这种自适应采样方法通过最小化与插值点相关的变分损失来求解PDE，并使用深度生成模型来近似概率密度函数。 |
| [^66] | [Graphical Object-Centric Actor-Critic.](http://arxiv.org/abs/2310.17178) | 这项研究提出了一种新颖的以对象为中心的强化学习算法，将演员-评论家和基于模型的方法结合起来，利用解耦的对象表示有效地学习策略。该方法填补了以对象为中心的强化学习环境中高效且适用于离散或连续动作空间的世界模型的研究空白。 |
| [^67] | [A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays.](http://arxiv.org/abs/2310.17176) | 本研究提出了一个利用深度学习技术从全景X射线图像中进行牙齿分割和定位的方法。我们通过修改已有模型并引入注意力机制，实现了高精度和高性能的牙齿分割和定位。在公开数据集上的评估结果表明，我们的方法在牙齿实例分割和牙齿定位方面取得了优异的性能。 |
| [^68] | [DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic.](http://arxiv.org/abs/2310.17173) | DSAC-C是一种约束最大熵的鲁棒离散软-演员-评论家算法，通过额外的统计约束提供了对潜在领域变化的鲁棒性和在现实世界中安全部署的能力。 |
| [^69] | [Learning an Inventory Control Policy with General Inventory Arrival Dynamics.](http://arxiv.org/abs/2310.17168) | 本文解决了学习具有一般库存到货动态下的库存控制策略的问题，同时允许修改订购数量以满足供应商的限制，并将周期性审核库存控制问题定义为外部决策过程。 |
| [^70] | [Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise.](http://arxiv.org/abs/2310.17167) | 通过重新参数化扩散过程并直接估计图像和噪声，本文改进了去噪扩散模型，提高了图像生成的速度和质量。 |
| [^71] | [MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift.](http://arxiv.org/abs/2310.17159) | 本论文提出了一种新的损失函数，用于解决超出分布转换的校准问题。该方法基于最大熵原理，在训练过程中引入统计约束，以提供更好的模型校准效果，同时不牺牲准确性。实验证明该方法在合成和真实世界的基准上实现了最先进的校准效果。 |
| [^72] | [Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time.](http://arxiv.org/abs/2310.17157) | 该论文介绍了一种利用上下文稀疏性来提高大型语言模型推理效率的系统Deja Vu，通过预测输入相关的注意力头和MLP参数集合，可以在不影响模型质量和上下文学习能力的前提下加速推理过程。 |
| [^73] | [Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration.](http://arxiv.org/abs/2310.17153) | 提出了一种层次半隐变分推断方法（HSIVI），通过引入辅助分布逐层逐步地匹配来训练条件层，实现了更具表达力的多层构造半隐分布。该方法还可以加速扩散模型的采样过程，并显著提高了SIVI的表达能力。 |
| [^74] | [Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls.](http://arxiv.org/abs/2310.17152) | 本研究评估了在0.55T低场MRI中对健康控制者膝关节进行标记物定量的深度学习技术的可行性，并表明这些技术在分割软骨区域方面与3.0T几乎相当。 |
| [^75] | [Explainable Spatio-Temporal Graph Neural Networks.](http://arxiv.org/abs/2310.17149) | 这篇论文提出了一种可解释的时空图神经网络（STExplainer）框架，通过增强时空图神经网络的可解释性，在城市资源分配和政策制定方面具有应用潜力。 |
| [^76] | [Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation.](http://arxiv.org/abs/2310.17146) | 提出了一种半离线评估框架，用于强化学习中的定量和定性评估。通过人类用户提供未被观察到的反事实轨迹的注释，设计了一种基于重要性抽样和加权的新型OPE估计器系列。 |
| [^77] | [Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning.](http://arxiv.org/abs/2310.17139) | 本文研究了基于双模拟的表示方法在离线强化学习中的缺陷，并发现数据集中缺失的转换以及奖励缩放对其性能有重要影响。基于此，我们提出了在离线环境中应用期望值算子与适当的奖励缩放策略来解决这些问题。 |
| [^78] | [Large-Scale Gaussian Processes via Alternating Projection.](http://arxiv.org/abs/2310.17137) | 本论文提出了一种通过交替投影的迭代方法来解决高斯过程在大规模数据集上的训练问题，并证明了该方法具有线性收敛性。 |
| [^79] | [Unleashing the potential of GNNs via Bi-directional Knowledge Transfer.](http://arxiv.org/abs/2310.17132) | 本文通过研究发现GNN未充分利用内在的特征转换操作能力，提出了一种双向知识传递的插拔式方法，使GNN能够充分释放特征转换操作的潜力，而不需要修改原始架构。 |
| [^80] | [Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models.](http://arxiv.org/abs/2310.17120) | 本文通过对非结构化文本进行分析，揭示了目前主题分段模型在此类数据上的泛化能力不足，并提出了从头开始训练相对小规模的目标数据集来改善分段结果的方法。实证评估表明使用多种损失函数可以减轻非结构化对话数据集的不平衡效应。 |
| [^81] | [On the Convergence of CART under Sufficient Impurity Decrease Condition.](http://arxiv.org/abs/2310.17114) | 本研究通过研究回归设置下CART的收敛性，建立了足够不纯度减少条件下CART预测误差的上界，并提供了易于验证的足够条件。这对于决策树模型的应用具有重要意义。 |
| [^82] | [LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?.](http://arxiv.org/abs/2310.17110) | 本研究首次提出了评估大型语言模型（LLMs）在动态图上的时空理解能力的LLM4DyG基准，并通过广泛的实验分析了不同因素对模型性能的影响。 |
| [^83] | [Network Design through Graph Neural Networks: Identifying Challenges and Improving Performance.](http://arxiv.org/abs/2310.17100) | 这项研究通过分析先前的图神经网络研究，发现边缘的选择受到结构偏差的影响，而不是重要性，从而导致编辑时出现错误。为了改进编辑，研究者们提出了一种名为ORE的迭代编辑方法，通过编辑最高评分的边缘并重新嵌入图来刷新梯度，从而减少偏好性较高的边缘选择。 |
| [^84] | [Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult.](http://arxiv.org/abs/2310.17087) | 该论文研究了大学习率在非凸优化中产生的隐性偏差，包括稳定的边界、平衡和弹射，并通过发展新的全局收敛理论和研究良好规则性的目标函数，将这些现象归纳为同一现象的不同表现形式。 |
| [^85] | [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models.](http://arxiv.org/abs/2310.17086) | Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。 |
| [^86] | [Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images.](http://arxiv.org/abs/2310.17080) | 本研究针对地衣监测的自动化问题，提出了一种利用时间序列图像的实例分割方法。通过该方法可以准确估计地衣的生物量和状态，从而方便生态学家的工作。 |
| [^87] | [HCT: Hybrid Convnet-Transformer for Parkinson's disease detection and severity prediction from gait.](http://arxiv.org/abs/2310.17078) | 提出了一种基于混合ConvNet-Transformer的深度学习方法，用于检测和分级帕金森病。该方法利用ConvNets和Transformers的优势，准确地检测PD并确定其严重程度阶段，在相对于其他方法的比较中表现出卓越的性能，PD检测准确率达到97%。 |
| [^88] | [Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates.](http://arxiv.org/abs/2310.17074) | 通过大学习率的随机梯度下降训练的神经网络，由于其权重的振荡，能够在特征噪声数据上实现良好的泛化性能。 |
| [^89] | [Isometric Motion Manifold Primitives.](http://arxiv.org/abs/2310.17072) | Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods. |
| [^90] | [math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories.](http://arxiv.org/abs/2310.17064) | 该研究调查了将大型语言模型应用于形式化高级数学概念的可行性，并提出了一个可以批判性地审查和检查研究论文中数学推理的框架。 |
| [^91] | [Strategizing EV Charging and Renewable Integration in Texas.](http://arxiv.org/abs/2310.17056) | 本研究通过动态时间扭曲（DTW）聚类和k均值聚类方法，针对德克萨斯州的电动车充电和可再生能源整合问题提供了战略决策的复杂方法，为实现可持续和具有适应能力的能源未来的整合做出了贡献。 |
| [^92] | [BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation.](http://arxiv.org/abs/2310.17054) | 本文提出了一个计算高效的框架，通过利用黑盒控制来引导冻结的预训练语言模型（PTLM）生成更加常识性的文本输出。 |
| [^93] | [Learning to Rank for Active Learning via Multi-Task Bilevel Optimization.](http://arxiv.org/abs/2310.17044) | 本论文提出了一种通过学习的替代模型选择未标记实例进行数据获取的主动学习方法，通过双层多任务双层优化框架预测不同训练集的相对效用并确保泛化效果。 |
| [^94] | [StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling.](http://arxiv.org/abs/2310.17042) | StochGradAdam是一种利用随机梯度抽样加速神经网络训练的优化器，通过选择性梯度考虑，能够稳定收敛，提升鲁棒训练。在图像分类和分割任务中表现优异。 |
| [^95] | [Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting.](http://arxiv.org/abs/2310.17032) | 本研究比较了量子长短期记忆（QLSTM）和经典长短期记忆（LSTM）模型在太阳能发电量预测中的表现，发现QLSTM具有加快训练收敛速度和减小测试损失的优势，拥有吸纳复杂时间序列关系的潜力，但还需要进一步研究和优化。 |
| [^96] | [On the Identifiability and Interpretability of Gaussian Process Models.](http://arxiv.org/abs/2310.17023) | 本文研究了高斯过程模型中的可识别性和可解释性问题。对于单输出情况，我们发现Matern核混合的平滑性由最不平滑的组件决定，并且混合核等价于最不平滑的核组件。在多输出模型中，我们证明了协方差矩阵的可识别性，这表明乘法混合是可行的。 |
| [^97] | [Controlled Decoding from Language Models.](http://arxiv.org/abs/2310.17022) | 本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。 |
| [^98] | [Streaming Factor Trajectory Learning for Temporal Tensor Decomposition.](http://arxiv.org/abs/2310.17021) | 这篇论文提出了一种流式因子轨迹学习方法，用于时间张量分解。该方法使用高斯过程来灵活估计因子的时间演变，并开发了一种高效的在线滤波算法来估计因子状态的演变轨迹。 |
| [^99] | [Conditionally Combining Robot Skills using Large Language Models.](http://arxiv.org/abs/2310.17019) | 这篇论文介绍了一个扩展的Meta-World基准，称为“语言世界”，允许大型语言模型在模拟机器人环境中使用自然语言查询和脚本技能。同时，引入了计划条件行为克隆（PCBC）的方法，通过端到端演示对高级计划进行微调。实验结果表明，在少样本情况下，PCBC在语言世界中能够实现强大的性能。 |
| [^100] | [Faster Recalibration of an Online Predictor via Approachability.](http://arxiv.org/abs/2310.17002) | 本文介绍了通过布莱克韦尔可接近性定理将在线预测模型转化为校准预测的技术，并且相较于已有技术实现了更快的速度、灵活的校准误差和准确性权衡。 |
| [^101] | [Trust, but Verify: Robust Image Segmentation using Deep Learning.](http://arxiv.org/abs/2310.16999) | 本文描述了一种使用“信任，但要验证”方法进行深度学习图像分割的方法，通过辅助验证网络对分割进行预测，以此来验证深度神经网络的输出。这种方法对多种扰动具有鲁棒性，可以应对对抗性攻击，并且不依赖于黑盒神经网络来检测错误的分割。 |
| [^102] | [Towards Continually Learning Application Performance Models.](http://arxiv.org/abs/2310.16996) | 本论文提出了一种能够持续学习的性能模型，考虑到数据分布的漂移，缓解灾难性遗忘，并提高了泛化能力。最佳模型在学习系统变化引起的新数据分布的同时保持了准确性，并相对于简单方法，全面数据序列的预测准确性提高了2倍。 |
| [^103] | [STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants.](http://arxiv.org/abs/2310.16990) | STEER是一个用于语音助手的语义转向扩展识别模型，通过训练数据集和启发式规则进行转向意图预测，并在实验中展现出了良好的性能。 |
| [^104] | [Probabilistic Integral Circuits.](http://arxiv.org/abs/2310.16986) | 概率积分电路（PICs）是一种新的计算图语言，通过使用积分单元扩展了概率电路（PCs），可以处理连续潜变量，并提供了可追踪的推理和近似精度可调的层次连续混合物。 |
| [^105] | [Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark.](http://arxiv.org/abs/2310.16981) | 本文通过整合数据中心的AI技术，解决了合成数据生成过程中反映真实世界数据复杂细微差异的问题。通过提出新的评估框架，研究对合成数据生成技术的成功和限制提供了关键见解。 |
| [^106] | [Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement.](http://arxiv.org/abs/2310.16979) | 该论文提出了一种无监督领域自适应方法，通过伪标签自我修正和噪声像素定位来改进语义分割模型的性能。 |
| [^107] | [The Significance of Machine Learning in Clinical Disease Diagnosis: A Review.](http://arxiv.org/abs/2310.16978) | 本综述研究了机器学习算法在临床疾病诊断中的应用，特别关注提高准确性和计算效率的优化。该研究发现，通过利用先进的ML和AI方法，可以增强医疗保健相关方的诊断和治疗能力。 |
| [^108] | [Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference.](http://arxiv.org/abs/2310.16975) | 提出了两种神经网络方法来逼近静态和动态条件最优传输问题的解，实现了对条件概率分布的采样和密度估计，适用于贝叶斯推断。算法利用神经网络参数化传输映射以提高可扩展性。 |
| [^109] | [Privately Aligning Language Models with Reinforcement Learning.](http://arxiv.org/abs/2310.16960) | 本文研究了通过强化学习实现隐私保护的语言模型对齐问题，提出了一种新的差分隐私框架，并通过实验证明了其有效性和实用性。 |
| [^110] | [Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2310.16959) | 通过数据增强的参数高效微调方法在大型语言模型的文本安全分类器中改善了少样本通用性问题，相比于基准模型，在社交化学领域的F1得分提高了7-17%。 |
| [^111] | [Transferring a molecular foundation model for polymer property predictions.](http://arxiv.org/abs/2310.16958) | 本研究表明，使用在小分子上预训练并在聚合物性质上微调的Transformer模型，在聚合物性能预测任务中达到与在增强数据集上训练的模型相当的准确性。 |
| [^112] | [Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks.](http://arxiv.org/abs/2310.16955) | 本研究提出了一个对抗训练框架，使用有限的人类对手示例生成更有用的大规模对抗示例，有效提高了自然语言处理系统对于人类对手的鲁棒性。 |
| [^113] | [Causal Q-Aggregation for CATE Model Selection.](http://arxiv.org/abs/2310.16945) | 该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率 |
| [^114] | [Zephyr: Direct Distillation of LM Alignment.](http://arxiv.org/abs/2310.16944) | 本论文提出了一种直接蒸馏的语言模型对齐方法，使用AI反馈数据进行优化，在聊天任务上显著提高了意图对齐的效果，该方法只需要几个小时的训练时间且无需人工注释，实验证明在7B参数模型上超过了现有最好的开放访问模型。 |
| [^115] | [Exploring Behavior Discovery Methods for Heterogeneous Swarms of Limited-Capability Robots.](http://arxiv.org/abs/2310.16941) | 这篇论文研究了在具有有限能力的功能异质群体机器人中确定紧急行为的问题，结果表明，之前的方法不能发现许多有趣的行为，而迭代的人机协作发现过程比随机搜索、群体化学和自动化行为发现发现了更多行为。 |
| [^116] | [Diagnosing Alzheimer's Disease using Early-Late Multimodal Data Fusion with Jacobian Maps.](http://arxiv.org/abs/2310.16936) | 提出了一种早期-晚期融合方法，利用卷积神经网络进行自动特征提取并使用随机森林，在小数据集上具备竞争力的性能。此外，还引入了一个适应独特特征的鲁棒预处理流程。 |
| [^117] | [MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback.](http://arxiv.org/abs/2310.16917) | MimicTouch是一种新的框架，能够模仿人类的触觉引导控制策略，通过收集来自人类示范者的多模态触觉数据集，来学习并执行复杂任务。 |
| [^118] | [Transformer-based Atmospheric Density Forecasting.](http://arxiv.org/abs/2310.16912) | 本研究提出了一个基于Transformer的非线性架构，用于大气密度预测，以改进之前的线性传播方法。这对于空间态势感知具有重要意义。 |
| [^119] | [MACP: Efficient Model Adaptation for Cooperative Perception.](http://arxiv.org/abs/2310.16870) | 本文提出了MACP框架，通过将单个预训练模型配备合作能力来提高连接和自动驾驶车辆的感知能力。通过冻结大部分参数并添加几个轻量级模块，该框架能够有效利用合作观测，并在模拟和真实环境中胜过其他最先进的方法。 |
| [^120] | [An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation.](http://arxiv.org/abs/2310.16867) | 本研究提出了一种基于可解释性深度学习和生成式数据增强的精神分裂症诊断方法，通过使用卷积神经网络进行初步诊断，并利用WGAN-GP和VAE生成的合成数据集进行增强，显著提高了诊断准确性和模型可解释性。 |
| [^121] | [General Point Model with Autoencoding and Autoregressive.](http://arxiv.org/abs/2310.16861) | 提出了一种通用点模型（GPM），它在点云变换器中无缝整合了自编码和自回归任务。GPM通过掩码填充任务增强了自编码中的掩码预测，并在点云理解和生成任务中展现出竞争力强大的结果。 |
| [^122] | [Improvement in Alzheimer's Disease MRI Images Analysis by Convolutional Neural Networks Via Topological Optimization.](http://arxiv.org/abs/2310.16857) | 通过拓扑优化卷积神经网络，本研究改善了阿尔茨海默病MRI图像质量，提高了分类精度，对阿尔茨海默病的诊断具有潜在的转变性影响。 |
| [^123] | [Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs.](http://arxiv.org/abs/2310.16842) | 本研究通过优化LSTM单元，提出了一种在终端设备上进行能效推断的新方法。以交通速度预测为例，优化后的LSTM单元在FPGA上实现了较快的推断速度和较低的能耗，相比现有方法提高了吞吐量和能效。 |
| [^124] | [Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping.](http://arxiv.org/abs/2310.16826) | 本研究开发了一个使用深度学习技术进行流星检测的自动化流水线，能够有效地在包含静态元素的图像中检测流星，并利用梯度加权类激活映射方法准确地定位流星的位置。 |
| [^125] | [Multi-scale Diffusion Denoised Smoothing.](http://arxiv.org/abs/2310.16779) | 本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。 |
| [^126] | [Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks.](http://arxiv.org/abs/2310.16639) | 本论文提出了一种使用概念阻塞作为控制命令预测和用户车辆行为解释的方法，通过学习人类可理解的概念层解释顺序驾驶场景，同时获得竞争性性能和可解释性。 |
| [^127] | [Covariate Shift Adaptation Robust to Density-Ratio Estimation.](http://arxiv.org/abs/2310.16638) | 该论文研究了在协变量偏移下的密度比估计的罕见问题，提出了一种适应性方法来减轻密度比估计的偏差对模型的影响。 |
| [^128] | [Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data.](http://arxiv.org/abs/2310.15890) | 本文提出了一种用于异构数据的分散式学习方法，通过跨特征对比损失实现数据无关知识蒸馏，实验结果表明该方法在各种计算机视觉任务上取得了优越性能。 |
| [^129] | [COPF: Continual Learning Human Preference through Optimal Policy Fitting.](http://arxiv.org/abs/2310.15694) | 通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。 |
| [^130] | [Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias.](http://arxiv.org/abs/2310.14814) | 本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。 |
| [^131] | [Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation.](http://arxiv.org/abs/2310.13923) | 本文提出了一种名为“多样化的异常值曝光（DivOE）”的框架，通过生成多样化的、信息化的辅助异常值来进行有效的OOD检测。这种方法通过信息外推的方式合成了更多的异常值，从而改进了传统的异常值检测方法。 |
| [^132] | [DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics.](http://arxiv.org/abs/2310.13268) | 本论文提出了DPM-Solver-v3，一个基于经验模型统计的新型快速ODE求解器，用于优化扩散概率模型的采样效率，并提出了一种新的参数化方法以减小ODE解的离散化误差。同时，引入了多步方法和预测-校正框架来进一步改善采样质量。 |
| [^133] | [Equipping Federated Graph Neural Networks with Structure-aware Group Fairness.](http://arxiv.org/abs/2310.12350) | 本论文提出了一种名为F2GNN的方法，它旨在增强联邦图神经网络的群体公平性，解决了在联邦学习中减轻偏见的新挑战。 |
| [^134] | [DSAC-T: Distributional Soft Actor-Critic with Three Refinements.](http://arxiv.org/abs/2310.05858) | 本论文介绍了DSAC-T，通过评论者梯度调整、双值分布学习和基于方差的目标回报裁剪等三个改进对标准DSAC进行了改进，解决了标准DSAC存在的不稳定学习过程和对任务特定奖励缩放的问题，提高了算法的性能和适应性。 |
| [^135] | [Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions.](http://arxiv.org/abs/2310.02987) | 提出了使用方差减少的 Halpern 迭代来优化有限和单调包含问题的求解过程，具有更好的复杂度保证。 |
| [^136] | [MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models.](http://arxiv.org/abs/2310.02255) | 本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。 |
| [^137] | [Harnessing the Power of Choices in Decision Tree Learning.](http://arxiv.org/abs/2310.01551) | 该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。 |
| [^138] | [SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data.](http://arxiv.org/abs/2310.00270) | 这篇论文提出了一种名为SpatialRank的新颖空间事件排名方法，通过基于时空数据的NDCG优化来解决城市事件排名问题。 |
| [^139] | [Leave-one-out Distinguishability in Machine Learning.](http://arxiv.org/abs/2309.17310) | 这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。 |
| [^140] | [Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control.](http://arxiv.org/abs/2309.14597) | 本论文提供了新的视角，研究了连续控制中深度强化学习智能体性能不稳定的原因。通过对回报景观进行分析，发现了策略空间中的失败区域和策略品质的隐藏维度。此外，提出了一种分布感知的方法，改善了策略的鲁棒性。 |
| [^141] | [Statistically Valid Variable Importance Assessment through Conditional Permutations.](http://arxiv.org/abs/2309.07593) | 本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。 |
| [^142] | [Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness.](http://arxiv.org/abs/2309.03004) | 提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。 |
| [^143] | [COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers.](http://arxiv.org/abs/2309.01270) | 使用自监督学习和知识蒸馏的COMEDIAN提出了一种初始化时空Transformer的流程，用于动作定位任务。在SoccerNet-v2数据集上实验证明了其最先进的性能和有效性。 |
| [^144] | [Adaptive whitening with fast gain modulation and slow synaptic plasticity.](http://arxiv.org/abs/2308.13633) | 本研究提出了一个多时间尺度的自适应白化机制模型，使用快速增益调制和慢速突触可塑性相结合的方式来适应变化的感觉统计信息。 |
| [^145] | [Monte Carlo guided Diffusion for Bayesian linear inverse problems.](http://arxiv.org/abs/2308.07983) | 本研究提出了一种在贝叶斯框架下利用蒙特卡洛方法解决非完备线性逆问题的算法，该算法通过利用基于得分的生成模型的先验结构和Feynman-Kac模型，并进行顺序蒙特卡洛采样，表现出比竞争对手更好的性能。 |
| [^146] | [A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades.](http://arxiv.org/abs/2308.00855) | 对过去六十年间具有高引用和重要影响的机器学习研究进行了全面的分析，揭示了该领域中最具影响力的论文、作者和合作网络，并发现了热门研究主题和最新涌现的主题。 |
| [^147] | [TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning.](http://arxiv.org/abs/2307.14338) | 本研究显示了现有的检索增强表格深度学习解决方案与无检索基线相比几乎没有明显优势，但提出了一种能够充分利用检索增强的表格深度学习模型，解锁了其潜力。 |
| [^148] | [Language-based Action Concept Spaces Improve Video Self-Supervised Learning.](http://arxiv.org/abs/2307.10922) | 这项研究使用语言相关的自监督学习方法，将图像CLIP模型调整为适用于视频领域，并通过在动作概念空间中进行自蒸馏训练，提高了零样本和线性推测性能。 |
| [^149] | [Improving Multimodal Datasets with Image Captioning.](http://arxiv.org/abs/2307.10350) | 通过探索混合原始和生成的图像描述的不同策略，我们的方法在ImageNet上超过了当前最佳降噪方法2%，在38个任务中平均提高了4%，我们的最佳方法在Flickr和MS-COCO检索上也提升了2倍。 |
| [^150] | [Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation.](http://arxiv.org/abs/2307.07907) | 本文研究了如何在强化学习中使模型具有对虚假相关性的鲁棒性，这种虚假相关性是由于不可观察的混杂因素引起的，并且普遍存在于现实世界的任务中。 |
| [^151] | [Bootstrapping Vision-Language Learning with Decoupled Language Pre-training.](http://arxiv.org/abs/2307.07063) | 本文提出了一种新的方法，通过解耦语言预训练，集中在语言组件上，提供了优化应用大型语言模型的框架，有效改善了视觉语言学习的性能。 |
| [^152] | [PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks.](http://arxiv.org/abs/2307.05891) | 该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。 |
| [^153] | [Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory.](http://arxiv.org/abs/2307.04204) | 本文通过实证研究证明了梯度下降轨迹上的稳定边缘现象，并且对于特定的网络结构进行了轨迹对齐分析，建立了渐进尖锐化和稳定边缘现象，扩展了当前文献的研究结果。 |
| [^154] | [Learning Space-Time Continuous Neural PDEs from Partially Observed States.](http://arxiv.org/abs/2307.04110) | 该论文介绍了一种从部分观测状态学习时空连续神经PDE的新方法，该方法通过新颖的编码器设计和高效的概率框架，克服了先前方法的局限，实现了对复杂部分观测数据的网格独立建模。 |
| [^155] | [A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks.](http://arxiv.org/abs/2307.01951) | 本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。 |
| [^156] | [Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures.](http://arxiv.org/abs/2306.15012) | 本论文提出了一种用于从噪声混合物中恢复目标信号的统计分量分离方法，并且在图像降噪任务中展示了其优于标准降噪方法的表现。 |
| [^157] | [CEIL: Generalized Contextual Imitation Learning.](http://arxiv.org/abs/2306.14534) | CEIL是一种通用的广义模仿学习算法，通过学习后见嵌入函数和上下文策略来实现模仿学习的专家匹配目标。它能够适用于多种学习设置，包括从观测中学习、离线模仿学习、跨领域模仿学习和一次模仿学习设置。实验表明，CEIL相比之前的基准算法具有更高的样本效率。 |
| [^158] | [Can Differentiable Decision Trees Learn Interpretable Reward Functions?.](http://arxiv.org/abs/2306.13004) | 本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。 |
| [^159] | [Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis.](http://arxiv.org/abs/2306.08645) | 本研究提出了一种无需训练的扩散模型适应方法，用于处理变尺寸文本到图像合成。通过观察低分辨率图像的不完整对象描绘和高分辨率图像的重复无序呈现，提出了注意力熵与令牌数量变化的统计关系。这项工作实现了在保持视觉保真度的同时适应不同尺寸和长宽比的图像合成需求。 |
| [^160] | [Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits.](http://arxiv.org/abs/2306.07923) | 本文提出了第一个面向Oracle有效的悲观策略优化算法，它简化为监督学习，具有广泛的适用性，能够在上下文强化学习中优化策略。 |
| [^161] | [Gaussian Membership Inference Privacy.](http://arxiv.org/abs/2306.07273) | 本文提出了$f$-成员推断隐私($f$-MIP)概念，并分析了似然比成员推断攻击，提出了$\mu$-高斯成员推断隐私($\mu$-GMIP)保证，同时提供了一种分析性的成员推断攻击方法，避免了训练大量影子模型。强调了方差的重要性。 |
| [^162] | [General Transformation for Consistent Online Approximation Algorithms.](http://arxiv.org/abs/2306.07163) | 本文提出一个转换框架，可以将离线逼近算法转换为具有低ε-近似遗憾的在线算法，并成功应用于多种问题并实现了多项式时间的近似效果。 |
| [^163] | [On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks.](http://arxiv.org/abs/2306.05557) | 本文研究了GNN在测试时节点的本地同质性水平与其图的全局同质性水平偏离时的性能，并介绍一种新参数用于控制同质性，在生成的图中系统地研究本地同质性的影响。 |
| [^164] | [Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations.](http://arxiv.org/abs/2306.04618) | 本文提出了一个具有挑战性的基准协议，用于评估自然语言处理中的领域外鲁棒性。通过使用这个基准套件，作者们发现OOD与ID性能之间的关系并不总是一致的，并引入了一种名为LLMs的新方法，可以在多个任务上显著提高OOD鲁棒性。 |
| [^165] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^166] | [SACSoN: Scalable Autonomous Data Collection for Social Navigation.](http://arxiv.org/abs/2306.01874) | 本文介绍了一个名为SACSoN的自主导航机器人系统，可以在人类占用的现实场景中，通过视觉理解和学习，自主收集数据，实现更好的数据集拓展。 |
| [^167] | [Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction.](http://arxiv.org/abs/2306.00650) | 通过权重集成、多样性加权和先验校正，本研究提出了一种通用的测试时间适应方法，以解决因分布偏移而降低模型性能的问题。这是第一项涵盖如此广泛范围的工作，对实际应用至关重要。 |
| [^168] | [Efficient Diffusion Policies for Offline Reinforcement Learning.](http://arxiv.org/abs/2305.20081) | 本论文提出了高效扩散策略（EDP）用于解决离线强化学习中的两个关键挑战，即计算效率低和难以与最大似然的强化学习算法兼容。EDP通过近似构建动作来避免运行采样链，并在实验中得到验证。 |
| [^169] | [Spontaneous symmetry breaking in generative diffusion models.](http://arxiv.org/abs/2305.19693) | 本文揭示生成式扩散模型存在自发对称破缺现象，这将其生成动力学分为两种不同的“相”，提出了高斯后期初始化方案，能够显著提高模型性能。 |
| [^170] | [Label Embedding by Johnson-Lindenstrauss Matrices.](http://arxiv.org/abs/2305.19470) | 这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。 |
| [^171] | [Emergent representations in networks trained with the Forward-Forward algorithm.](http://arxiv.org/abs/2305.18353) | 研究表明使用Forward-Forward算法训练的网络内部表征具有高稀疏度，类别特定的集合，这与生物学观察到的皮层表征相似。 |
| [^172] | [No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions.](http://arxiv.org/abs/2305.17380) | 本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。 |
| [^173] | [Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference.](http://arxiv.org/abs/2305.16905) | 本文提出了拉普拉斯逼近神经加性模型，该模型从贝叶斯角度考虑加性结构，在恢复的特征交互中提供可信区间，提供可处理的边缘似然估计，可用于执行隐式特征选择并对特征对进行排名。 |
| [^174] | [Neural (Tangent Kernel) Collapse.](http://arxiv.org/abs/2305.16427) | 本文介绍了神经切向核（NTK）和神经崩溃（NC）之间的关系，证明了在具有块状NTK的DNN中会出现NC，并通过大规模实验支持理论的正确性。 |
| [^175] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^176] | [Unifying GANs and Score-Based Diffusion as Generative Particle Models.](http://arxiv.org/abs/2305.16150) | 本文提出了一个新框架，将生成器训练作为粒子模型的一个推广，从而统一了粒子和对抗生成模型。这个框架可以将生成器集成到基于分数扩散模型中，并在没有生成器的情况下训练GAN。 |
| [^177] | [Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness.](http://arxiv.org/abs/2305.15807) | 本文提出了带总成本限制的上下文信息决策问题（CBwK），通过对术语进行重新组合，对CBwK进行了优化，支持小于$T^{3/4}$的总成本约束，并通过对偶策略实现了平等的成本限制。 |
| [^178] | [Learning Rate Free Bayesian Inference in Constrained Domains.](http://arxiv.org/abs/2305.14943) | 我们的算法是学习率无关的约束域采样算法，并提出了一个统一框架，能够处理多种约束采样问题，实现了与现有算法相竞争的性能。 |
| [^179] | [Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers.](http://arxiv.org/abs/2305.14858) | Pre-RMSNorm和Pre-CRMSNorm Transformers是等效且高效的Pre-LN Transformers架构，可以统一使用两种主流归一化技术，LayerNorm和RMSNorm，从而加速和稳定Transformer模型的训练。 |
| [^180] | [SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models.](http://arxiv.org/abs/2305.14267) | 本论文提出了SEEDS，这是一种指数随机微分方程求解器，用于从扩散模型中进行快速高质量的抽样。与现有的慢速求解器相比，SEEDS能够以更高的质量进行求解，且不需要大量的数值函数评估。 |
| [^181] | [Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension.](http://arxiv.org/abs/2305.14077) | 这篇论文研究了固定维度下内核和神经网络的良性过拟合，发现良性过拟合的关键在于估计器的平滑度而不是维数，并证明在固定维度下中度导数的良性过拟合是不可能的。相反，我们证明了用序列核进行回归是可能出现良性过拟合的。 |
| [^182] | [Detecting and Mitigating Hallucinations in Multilingual Summarisation.](http://arxiv.org/abs/2305.13632) | 本文提出一种新的度量方法mFACT，可以在非英语摘要中评估其忠实性。本文还提出了一种简单有效的加权方法，可以通过跨语言转移减少摘要的幻觉问题。 |
| [^183] | [Squared Neural Families: A New Class of Tractable Density Models.](http://arxiv.org/abs/2305.13552) | 提出一种新的可计算密度模型类——平方神经分布族，其通过对神经网络的2范数进行平方和基于某个基础度量进行归一化，严格推广了经典指数族，具有闭性条件推断和可计算的边际分布。 |
| [^184] | [Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates.](http://arxiv.org/abs/2305.13082) | 本文提出了一种新的基于草图和投影的Newton方法，具有快速的全局收敛率，适用于自共轭函数，具有草图和投影方法的低迭代成本，全秩Newton类方法的最先进全局收敛率以及阻尼Newton方法的算法简单性。 |
| [^185] | [Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods.](http://arxiv.org/abs/2305.12283) | 本文提出一种基于非参数方法的无分布模型无偏回归校准方法，具有计算效率和统计一致性，并建立了校准误差的上限和下限的统计保证和优势。 |
| [^186] | [Sequential Memory with Temporal Predictive Coding.](http://arxiv.org/abs/2305.11982) | 该论文提出了一种基于PC的新型时序记忆模型，称为时间预测编码（tPC），可以通过生物可行的神经实现准确地记忆和检索连续输入。其中tPC可以被看作是一种经典异向性霍普菲尔德网络（AHN），具有更稳定的性能，并且可以编码上下文相关信息，区分在序列中出现的重复元素。 |
| [^187] | [Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation.](http://arxiv.org/abs/2305.11685) | 本研究提出了一种基于Transformer的语音自监督学习模型的通用压缩策略，通过重用注意力映射和蒸馏屏蔽来提高学生模型的语音表示质量，实现了较低的错误率。 |
| [^188] | [Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation.](http://arxiv.org/abs/2305.11531) | 基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。 |
| [^189] | [An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions.](http://arxiv.org/abs/2305.08175) | ResidualPlanner是一种用于带有高斯噪声的边缘的矩阵机制，既优化又可扩展，可以优化许多可以写成边际方差的凸函数的损失函数。 |
| [^190] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^191] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^192] | [Learning Controllable 3D Diffusion Models from Single-view Images.](http://arxiv.org/abs/2304.06700) | 该论文介绍了一种名为Control3Diff的三维扩散模型，结合了扩散模型和3D GANs的优点，可以用于单视图数据集的多功能、可控的三维感知图像合成。 |
| [^193] | [What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.](http://arxiv.org/abs/2303.11249) | 本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。 |
| [^194] | [Investigating Topological Order using Recurrent Neural Networks.](http://arxiv.org/abs/2303.11207) | 本研究通过循环神经网络有效地捕获了多体哈密顿量中的拓扑序，进一步证明了循环神经网络波函数是研究物态相的一个有力工具。 |
| [^195] | [Hierarchical clustering with OWA-based linkages, the Lance-Williams formula, and dendrogram inversions.](http://arxiv.org/abs/2303.05683) | 本研究通过使用基于OWA的链接，Lance-Williams公式和树枝反转技术，推广了凝聚层次聚类，并提供了一些条件用于保证结果的树枝图没有不美观的反转。 |
| [^196] | [The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models.](http://arxiv.org/abs/2303.03284) | 本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。 |
| [^197] | [Convolutional Visual Prompt for Robust Visual Perception.](http://arxiv.org/abs/2303.00198) | 本文介绍了一种针对稳健视觉感知的卷积视觉提示（CVP）方法，通过较少的可训练参数来避免自适应模型在无标签自监督测试时间设置下的过拟合，实验证明该方法可提高稳健性高达5.87%。 |
| [^198] | [Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces.](http://arxiv.org/abs/2303.00028) | 本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。 |
| [^199] | [Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms.](http://arxiv.org/abs/2302.13534) | 本文提出了对于多臂赌博机问题的改进的FTRL算法，通过使用一系列正则化器和新的学习率计划，不再需要假设存在唯一最优臂，并对某些正则化器的遗憾界限进行了改进。 |
| [^200] | [Lithium Metal Battery Quality Control via Transformer-CNN Segmentation.](http://arxiv.org/abs/2302.04824) | 通过Transformer-CNN分割方法，本研究提出了一种新的语义分割方法TransforCNN，能够准确地从XCT图像中分割出锂金属电池中的树枝晶。与其他算法相比，TransforCNN具有更好的性能。 |
| [^201] | [Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals.](http://arxiv.org/abs/2302.04449) | 本论文提出了阅读并奖励的框架，通过阅读Atari游戏开发者发布的指导手册，以提高强化学习算法在Atari游戏中的效率。该框架包含一个QA提取模块和一个推理模块，能够从指导手册中提取关键信息，并评估物体与智能体的交互效果。 |
| [^202] | [ZipLM: Inference-Aware Structured Pruning of Language Models.](http://arxiv.org/abs/2302.04089) | ZipLM是一种新型的语言模型压缩方法，能够在任何给定的推理环境中实现与目标运行速度相匹配的最先进压缩模型。与现有方法相比，ZipLM在速度和准确性之间取得了最佳的权衡，并且以更低的计算成本获得了更好的结果。 |
| [^203] | [Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.](http://arxiv.org/abs/2302.03857) | 该研究提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法，能够有效地加速对抗性对比学习（ACL）并维持其强鲁棒性和泛化性能。 |
| [^204] | [A Theory of Link Prediction via Relational Weisfeiler-Leman.](http://arxiv.org/abs/2302.02209) | 本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。 |
| [^205] | [RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion.](http://arxiv.org/abs/2302.01757) | 本文提出了一种适应于离散序列分类器的随机删除（RS-Del）平滑机制，提供针对编辑距离受限对抗性的鲁棒性证明。 |
| [^206] | [Improving the Timing Resolution of Positron Emission Tomography Detectors Using Boosted Learning -- A Residual Physics Approach.](http://arxiv.org/abs/2302.01681) | 本论文提出了一种残差物理方法，通过机器学习优化正电子发射断层扫描探测器，提高时间分辨率，减少患者接受的放射性剂量。 |
| [^207] | [Unconstrained Dynamic Regret via Sparse Coding.](http://arxiv.org/abs/2301.13349) | 本文探讨了在线线性优化（OLO）涉及无约束问题和动态遗憾问题的复杂性，提出了一种通过重新构造问题为稀疏编码的复杂度度量方式，在适应性和应用上有较好的应用价值。 |
| [^208] | [Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing.](http://arxiv.org/abs/2301.12930) | 通过引入异方差位置-尺度噪声函数模型，该论文在正确说明噪声分布的情况下，通过最大似然实现了最先进的准确性。但是，在用户错误指定噪声分布的形式时，分析表明因果推断的精度会急剧下降。因此，该论文提出通过因果模型选择实现稳定而准确的因果推断。 |
| [^209] | [Curvature Filtrations for Graph Generative Model Evaluation.](http://arxiv.org/abs/2301.12906) | 该论文使用图形曲率描述符和拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。 |
| [^210] | [Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning.](http://arxiv.org/abs/2301.12593) | 该论文提出了一种分布鲁棒安全强化学习框架，通过使用失真风险度量来处理模型不确定性。该方法不需要极小极大优化，具有高效且不依赖模型的特点。实验证明该框架能够在具有安全约束的控制任务中实现稳健的性能和安全性。 |
| [^211] | [Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty.](http://arxiv.org/abs/2301.11588) | 该论文提出了一种基于边界框的多目标贝叶斯优化方法，能够在输入不确定性下高效地识别风险衡量定义的帕累托前沿。该方法具有理论保证，并通过构建高概率边界框和选择下一个评估点的方法来减少不确定性。 |
| [^212] | [Finding Regions of Counterfactual Explanations via Robust Optimization.](http://arxiv.org/abs/2301.11113) | 该论文提出了一种通过稳健优化计算反事实解释（CE）区域的方法，使用户能够选择适当的措施以获得所需的结果，此方法在逻辑回归、决策树、随机森林和神经网络等最常见的机器学习方法上证明了收敛结果。 |
| [^213] | [Increasing Fairness via Combination with Learning Guarantees.](http://arxiv.org/abs/2301.10813) | 该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。 |
| [^214] | [Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction.](http://arxiv.org/abs/2301.08951) | 本文提出了一种基于时间条件生成模型的视频分解和预测方法，采用提升的对象和视图的潜在表征之间的解耦技术以及从预训练模型中学习隐式视角规则的方法解决了现有方法存在的部分或完全遮挡对象的形状无法被准确重建和新视角预测依赖于昂贵的视角注释的问题。 |
| [^215] | [Technical Report of Mixing Local Patterns.](http://arxiv.org/abs/2212.03654) | 本文旨在解决GNN在处理非同质图数据时性能不佳的问题，提出混合局部结构模式的概念，并从局部模式的随机性和近邻可聚合性两个方面深入研究，以实现更通用的GNN。 |
| [^216] | [Fairness and bias correction in machine learning for depression prediction: results from four different study populations.](http://arxiv.org/abs/2211.05321) | 本文研究了设计用于预测抑郁症的机器学习模型的公平性问题，并给出了有效的偏差矫正方法。这项研究强调了分析公平性以及透明报告的重要性。 |
| [^217] | [A weighted-variance variational autoencoder model for speech enhancement.](http://arxiv.org/abs/2211.00990) | 我们提出了一种加权方差生成模型，通过在参数学习中加权每个频谱图时间帧的贡献，并使用Gamma先验分布将复杂值高斯分布改为学生t分布，实现了更有效和更鲁棒的语音增强。 |
| [^218] | [Artificial intelligence in government: Concepts, standards, and a unified framework.](http://arxiv.org/abs/2210.17218) | 本论文研究了人工智能在政府中的应用，强调了标准化操作程序和符合社会规范期望的重要性，并指出了多学科研究者在概念上的碎片化问题。 |
| [^219] | [Learning Transferable Adversarial Robust Representations via Multi-view Consistency.](http://arxiv.org/abs/2210.10485) | 本论文提出了一种新颖的元对抗多视角表示学习框架，通过差异更新编码器参数并施加对抗攻击来增强元学习器的鲁棒性。此外，通过最大化视图之间的一致性，实现在未见领域和任务上的可迁移的鲁棒表示。 |
| [^220] | [Effective Targeted Attacks for Adversarial Self-Supervised Learning.](http://arxiv.org/abs/2210.10482) | 该论文提出了一种针对无监督对抗训练框架的正向挖掘方法，通过选择最令人困惑但相似的目标样本来生成有效的对手，以提高训练模型的鲁棒性。 |
| [^221] | [Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective.](http://arxiv.org/abs/2210.09933) | 在解释黑盒模型的方法中没有共识，本论文提出了一种基于项目反应理论的新方法eXirt，用于解释使用表格数据的树集成模型，并通过全局特征排名来解释模型输入和预测之间的关系。 |
| [^222] | [Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty.](http://arxiv.org/abs/2209.15543) | 本论文介绍了将贝叶斯神经网络应用于地热资源评估的方法，利用监督学习问题和已知特征图进行预测，并可用于找出潜力更高的区域进行进一步的调查。 |
| [^223] | [A framework for benchmarking clustering algorithms.](http://arxiv.org/abs/2209.09493) | 该论文开发了一个用于基准测试聚类算法的框架，旨在引入一种一致的方法进行测试。还汇总、改进和标准化了许多聚类基准数据集合，并包含了新数据集。提供了互动数据集浏览器、Python API的文档以及与其他编程语言进行框架交互的方式。 |
| [^224] | [Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks.](http://arxiv.org/abs/2209.06589) | 本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。 |
| [^225] | [Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach.](http://arxiv.org/abs/2207.04306) | 本文提出了一种在时间序列领域中进行离群检测的新方法，通过使用季节比例评分(SRS)来解决时间序列数据所带来的挑战。该方法通过将输入分解为按类别的语义组件和余项，并使用深度生成模型估计按类别的条件概率，从而计算季节比例评分，并通过识别阈值区间来检测离群样本。 |
| [^226] | [On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms.](http://arxiv.org/abs/2206.05869) | 本文讨论了混洗型梯度算法在过参数化设置下对一类非凸函数的收敛性，证明了其能够收敛到全局解，并且在计算复杂度上与一般凸函数的情况相当。 |
| [^227] | [SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks.](http://arxiv.org/abs/2206.05794) | 使用SGD和权重衰减训练深度ReLU神经网络会导致对于权重矩阵的秩最小化的偏差，特别是在使用较小批量大小、更高学习率或增加权重衰减时更为显著。此外，在中间神经网络崩溃时，学习的权重特别低秩。这种偏差与泛化之间存在关系。 |
| [^228] | [Neural Optimal Transport with General Cost Functionals.](http://arxiv.org/abs/2205.15403) | 该论文介绍了一种新颖的神经网络算法，用于计算具有一般成本函数的最优传输方案。相比于常见的欧几里得成本，这种方法更灵活，并允许使用辅助信息构建传输映射。此外，该方法还解决了在高维空间下处理新数据点的挑战，并提供了理论误差分析。作为应用，该论文构造了一个能够在保留类别结构的同时映射数据分布的成本函数。 |
| [^229] | [Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning.](http://arxiv.org/abs/2203.09249) | 本文针对联邦学习中的数据异构性问题，提出了一种无数据知识蒸馏方法，通过在服务器上对全局模型进行微调，可以有效地缓解模型聚合导致的性能下降，并提高联邦学习的效果。 |
| [^230] | [On Embeddings for Numerical Features in Tabular Deep Learning.](http://arxiv.org/abs/2203.05556) | 本文研究了表格深度学习中关于数值特征的嵌入方法，提出了两种不同的构建嵌入模块的方法，并通过实验证明，与传统模块相比，这些方法可以显著提升模型性能。这对于构建更强大的深度学习模型并在一些传统上适用于GBDT的基准上与之竞争具有重要的益处。 |
| [^231] | [Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2112.12458) | 这项研究提出了一种局部优势网络（LAN）算法，该算法通过对决架构和中心化评论家来学习合作多智能体的最佳响应策略，并在StarCraft II多智能体挑战基准测试上达到了最先进的性能。 |
| [^232] | [Road Network Guided Fine-Grained Urban Traffic Flow Inference.](http://arxiv.org/abs/2109.14251) | 本文提出了一种基于道路网络的交通流量推断方法，利用道路网络的先验知识全面学习细粒度交通流的道路感知空间分布，普遍适用于城市交通流量监测和调控方案。 |
| [^233] | [Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits.](http://arxiv.org/abs/2107.11419) | 本文研究了模型参数随时间变化的非稳态多臂赌博机问题，引入了自适应重置赌博机(ADR-bandit)算法，通过有限时间分析证明了ADR-bandit在全局变化的情况下具有几乎最优的性能，并且在稳定环境和非稳态环境中均具有最优的性能。 |
| [^234] | [Optimal Scoring Rule Design under Partial Knowledge.](http://arxiv.org/abs/2107.07420) | 本文研究了在委托人对代理人的信号分布部分了解的情况下，最优打分规则的设计问题。作者提出了一个最大最小优化的框架，来最大化在代理人信号分布的集合中最坏情况下回报的增加。对于有限集合，提出了高效的算法；对于无限集合，提出了完全多项式时间逼近方案。 |
| [^235] | [Revisiting Deep Learning Models for Tabular Data.](http://arxiv.org/abs/2106.11959) | 本论文重新审视了用于表格数据的深度学习模型，提出了两种简单且强大的深度学习架构作为性能基准，包括类似于ResNet的架构和适应于表格数据的Transformer架构。这些基准模型在不同问题上表现出有竞争力的性能。 |
| [^236] | [Improving Few-Shot Learning through Multi-task Representation Learning Theory.](http://arxiv.org/abs/2010.01992) | 本文通过多任务表示学习理论的最新进展，提出了一个新的基于谱的正则化项来改进少样本学习，并在实验证明了其有效性。 |
| [^237] | [Online Estimation and Community Detection of Network Point Processes for Event Streams.](http://arxiv.org/abs/2009.01742) | 本论文提出了一种在线变分推断算法，用于在网络事件流中估计潜在的社区结构。通过使用连续时间点过程潜在网络模型，可以捕捉动态事件到达的时间动态，并更新社区分配。 |

# 详细

[^1]: RNN训练中的分歧和损失跳跃

    Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])

    [http://arxiv.org/abs/2310.17561](http://arxiv.org/abs/2310.17561)

    这篇论文研究了在RNN训练中的分歧现象和损失跳跃，并证明了在特定类型的RNN中存在着某些分歧。

    

    循环神经网络（RNN）是用于建模和预测序列数据以及从观测时间序列中推断动力系统（DS）的常用机器学习工具。DS理论的概念已被用于进一步理解经过训练的RNN如何解决复杂任务以及训练过程本身。分歧是DS中特别重要的现象，包括RNN，在系统的一个或多个参数变化时，指系统的动力行为的拓扑（定性）变化。了解RNN的分歧结构将有助于推断其许多计算和动力属性，例如对参数变化的敏感性或训练过程中的行为。特别是，分歧可能解释RNN训练中观察到的突然损失跳跃，这可能严重阻碍训练过程。在这里，我们首先数学地证明了针对一类基于ReLU的RNN，确实存在一些分歧。

    Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associ
    
[^2]: 实现手机与语音表示的匹配

    Towards Matching Phones and Speech Representations. (arXiv:2310.17558v1 [cs.CL])

    [http://arxiv.org/abs/2310.17558](http://arxiv.org/abs/2310.17558)

    本研究探讨了学习手机类型的问题，并将其作为匹配聚类中心与手机嵌入的问题。通过生成伪标签和引入新的损失函数，我们改进了自监督表示，实验证明匹配结果捕捉到了手机之间的关系，并显著提高了手机分类的性能。

    

    学习手机类型从手机实例一直是一个长期存在但仍然尚未解决的问题。在本研究中，我们在自监督学习的背景下重新审视了这个问题，并将其提出为将聚类中心与手机嵌入匹配的问题。我们研究了两个关键属性，它们使匹配成为可能，即自监督表示的聚类中心是否减少了手机实例的变化性并且是否尊重手机之间的关系。然后，我们使用匹配结果生成伪标签，并引入了一种新的损失函数来改进自监督表示。我们的实验表明，匹配结果捕捉到了手机之间的关系。将新的损失函数与APC和CPC等常规的自监督损失一起训练，显著提高了下游手机分类的性能。

    Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.
    
[^3]: 大规模阻尼自然梯度下降的高效数值算法

    Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient Descent. (arXiv:2310.17556v1 [cs.LG])

    [http://arxiv.org/abs/2310.17556](http://arxiv.org/abs/2310.17556)

    这个论文提出了一个针对大规模场景下阻尼自然梯度下降的高效数值算法，通过使用Cholesky分解，解决了参数数量超过样本数量的问题，相比现有方法，该算法速度更快。

    

    我们提出了一种新的算法，用于在参数数量显著超过可用样本数量的大规模场景下高效求解阻尼Fisher矩阵。这个问题对于自然梯度下降和随机重构非常重要。我们的算法基于Cholesky分解，并且具有普适性。基准结果表明，该算法明显快于现有方法。

    We propose a new algorithm for efficiently solving the damped Fisher matrix in large-scale scenarios where the number of parameters significantly exceeds the number of available samples. This problem is fundamental for natural gradient descent and stochastic reconfiguration. Our algorithm is based on Cholesky decomposition and is generally applicable. Benchmark results show that the algorithm is significantly faster than existing methods.
    
[^4]: 从口头纠正中进行交互式机器人学习

    Interactive Robot Learning from Verbal Correction. (arXiv:2310.17555v1 [cs.RO])

    [http://arxiv.org/abs/2310.17555](http://arxiv.org/abs/2310.17555)

    本研究设计了一个新的基于大型语言模型（LLM）OLAF的交互式机器人学习系统，它允许日常用户通过口头纠正教授机器人，并能根据口头反馈更新机器人的视觉运动神经策略，从而避免重复错误。实验结果表明，在模拟和物理硬件上，该系统在长时间线的操纵任务中平均改善了20.0%的策略成功率。

    

    随着我们将机器人设计成能在家庭等非结构化环境中运行，学习和改进行为的能力越来越重要。在这项工作中，我们设计了一个基于大型语言模型（LLM）OLAF的新学习系统，它允许日常用户通过口头纠正教授机器人，当机器人犯错误时，例如说“停下你正在做的事情。你应该靠近杯子。” OLAF的一个关键特点是它能够根据口头反馈更新机器人的视觉运动神经策略，以避免将来重复错误。这与现有的基于LLM的机器人系统形成对比，后者仅仅遵循口头命令或纠正，而不会从中学习。在模拟和物理硬件上的实验中，我们证明了我们设计的有效性，用户教机器人执行长时间线的操纵任务，在策略成功率方面平均改善了20.0%。视频和更多结果可在ht找到。

    The ability to learn and refine behavior after deployment has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when the robot makes mistakes, e.g., by saying "Stop what you're doing. You should move closer to the cup." A key feature of OLAF is its ability to update the robot's visuomotor neural policy based on the verbal feedback to avoid repeating mistakes in the future. This is in contrast to existing LLM-based robotic systems, which only follow verbal commands or corrections but not learn from them. We demonstrate the efficacy of our design in experiments where a user teaches a robot to perform long-horizon manipulation tasks both in simulation and on physical hardware, achieving on average 20.0% improvement in policy success rate. Videos and more results are at ht
    
[^5]: 基于模型的运行时监控与交互式模仿学习

    Model-Based Runtime Monitoring with Interactive Imitation Learning. (arXiv:2310.17552v1 [cs.RO])

    [http://arxiv.org/abs/2310.17552](http://arxiv.org/abs/2310.17552)

    本文提出了一种基于模型的运行时监控算法，通过学习部署数据来检测系统异常并预测故障，旨在解决机器人学习中的泛化和鲁棒性挑战。

    

    机器人学习方法近年来取得了巨大的进展，但是泛化和鲁棒性的挑战仍然制约着它们的广泛应用。无法检测和解决潜在故障使得最先进的学习系统无法在高风险任务中投入使用。最近在交互式模仿学习方面的进展提出了一个有希望的框架，可以实现人机团队合作，使机器人能够安全操作并在长期部署中不断提高性能。然而，现有方法通常需要持续的人工监督和预先的反馈，限制了它们在实际领域的实用性。本文旨在赋予机器人在任务执行过程中监控和检测错误的能力。我们引入了一种基于模型的运行时监控算法，通过学习部署数据来检测系统异常并预测故障。与以前的工作不同，我们的方法能够预见未来的故障，而且不需要故障经验来进行训练。

    Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method l
    
[^6]: 人类引导的复杂度控制抽象化

    Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])

    [http://arxiv.org/abs/2310.17550](http://arxiv.org/abs/2310.17550)

    本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。

    

    神经网络通常学习任务特定的潜在表示，但这些表示无法推广到新的环境或任务。相反，人类在各种抽象级别（例如，“鸟”与“麻雀”）上学习离散表示（即概念或单词），并根据任务使用适当的抽象。受此启发，我们训练神经模型生成一系列离散表示，并通过调整表示分布的熵来控制表示的复杂性（大致上是为编码输入分配了多少位）。在微调实验中，仅使用少量带标签的示例用于新任务，我们展示了（1）调整表示以适当的复杂性水平支持最高的微调性能，以及（2）在一个人类参与者的研究中，用户能够根据离散表示的可视化来确定下游任务的适当复杂性水平。我们的结果表明一个有希望的方向。

    Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
    
[^7]: 基于层次集成的时间序列预测特征选择研究

    Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v1 [cs.LG])

    [http://arxiv.org/abs/2310.17544](http://arxiv.org/abs/2310.17544)

    这项研究提出了一种基于层次集成的特征选择方法，能够克服传统方法和最先进方法在非平稳和特征数目庞大且样本有限的情况下的局限性，并在合成和实际数据集上展示了更好的性能。

    

    我们研究了一种针对非平稳和样本有限的大量特征情况下的特征选择新的集成方法。我们的方法利用层次结构来利用特征之间的相互依赖关系。首先，使用特征子集训练机器学习模型，然后使用另一种算法更新模型的输出，以最小化目标损失。这种层次结构允许灵活的深度和特征选择。通过层次地利用特征之间的依赖关系，我们的方法克服了传统特征选择方法和特征重要性评分的局限性。该方法在合成和现实数据集上展示了其有效性，与传统方法和最先进的方法相比，性能具有可扩展性和稳定性。

    We study a novel ensemble approach for feature selection based on hierarchical stacking in cases of non-stationarity and limited number of samples with large number of features. Our approach exploits the co-dependency between features using a hierarchical structure. Initially, a machine learning model is trained using a subset of features, and then the model's output is updated using another algorithm with the remaining features to minimize the target loss. This hierarchical structure allows for flexible depth and feature selection. By exploiting feature co-dependency hierarchically, our proposed approach overcomes the limitations of traditional feature selection methods and feature importance scores. The effectiveness of the approach is demonstrated on synthetic and real-life datasets, indicating improved performance with scalability and stability compared to the traditional methods and state-of-the-art approaches.
    
[^8]: EqDrive: 自动驾驶的高效等变运动预测与多模式处理

    EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving. (arXiv:2310.17540v1 [cs.RO])

    [http://arxiv.org/abs/2310.17540](http://arxiv.org/abs/2310.17540)

    本研究发展了EqDrive模型，通过使用EqMotion等变粒子和人类预测模型以及多模式预测机制，在自动驾驶中实现了高效的车辆运动预测。该模型在模型容量较低、参数更少、训练时间显著缩短的情况下，取得了业界最先进的性能。

    

    在自动驾驶中预测车辆运动需要对车辆间的相互作用有深入的理解，并保持在欧几里得几何变换下的运动等变性。传统模型往往缺乏处理自动驾驶车辆中复杂动力学和场景中各主体之间交互关系所需的复杂性。因此，这些模型具有较低的模型容量，导致更高的预测误差和较低的训练效率。在我们的研究中，我们使用EqMotion，一个领先的等变粒子和人类预测模型，该模型还考虑到不变的主体间相互作用，用于多代理车辆运动预测任务。此外，我们使用多模式预测机制以概率化方式考虑多个可能的未来路径。通过利用EqMotion，我们的模型在参数更少（120万）和训练时间显著缩短（少于..）的情况下实现了业界最先进的性能。

    Forecasting vehicular motions in autonomous driving requires a deep understanding of agent interactions and the preservation of motion equivariance under Euclidean geometric transformations. Traditional models often lack the sophistication needed to handle the intricate dynamics inherent to autonomous vehicles and the interaction relationships among agents in the scene. As a result, these models have a lower model capacity, which then leads to higher prediction errors and lower training efficiency. In our research, we employ EqMotion, a leading equivariant particle, and human prediction model that also accounts for invariant agent interactions, for the task of multi-agent vehicle motion forecasting. In addition, we use a multi-modal prediction mechanism to account for multiple possible future paths in a probabilistic manner. By leveraging EqMotion, our model achieves state-of-the-art (SOTA) performance with fewer parameters (1.2 million) and a significantly reduced training time (less 
    
[^9]: 只需稍微探索：面对不确定性的乐观主义

    Little Exploration is All You Need. (arXiv:2310.17538v1 [cs.LG])

    [http://arxiv.org/abs/2310.17538](http://arxiv.org/abs/2310.17538)

    本研究提出了UCB$^\tau$算法，通过对任务困难度的调整，实现了更好的效能和较低的风险。

    

    “面对不确定性的乐观主义”是一种主张将探索奖励因子纳入考虑的原则，通常被假定成与状态-动作对访问次数的倒数平方根成比例($1/\sqrt{n}$)，其中$n$是访问某个特定状态-动作对的次数。然而，这种方法专注于"不确定性"，忽略了不同选项的固有"困难性"。为了解决这一空白，我们在多臂赌博问题中引入了标准UCB算法的一种新颖修改，提出了一个根据任务的难度进行调整的奖励项$1/n^\tau$，其中$\tau > 1/2$。我们的提议算法被标记为UCB$^\tau$，通过全面的遗憾和风险分析来证明其理论上的鲁棒性。在合成数据集上与标准UCB算法和Thompson采样算法进行比较评估表明，UCB$^\tau$不仅在效能上表现出色，还在各种环境中显示出较低的风险。

    The prevailing principle of "Optimism in the Face of Uncertainty" advocates for the incorporation of an exploration bonus, generally assumed to be proportional to the inverse square root of the visit count ($1/\sqrt{n}$), where $n$ is the number of visits to a particular state-action pair. This approach, however, exclusively focuses on "uncertainty," neglecting the inherent "difficulty" of different options. To address this gap, we introduce a novel modification of standard UCB algorithm in the multi-armed bandit problem, proposing an adjusted bonus term of $1/n^\tau$, where $\tau > 1/2$, that accounts for task difficulty. Our proposed algorithm, denoted as UCB$^\tau$, is substantiated through comprehensive regret and risk analyses, confirming its theoretical robustness. Comparative evaluations with standard UCB and Thompson Sampling algorithms on synthetic datasets demonstrate that UCB$^\tau$ not only outperforms in efficacy but also exhibits lower risk across various environmental co
    
[^10]: 神经启发的分段和回溯法克服好奇心中的灾难性遗忘

    Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity. (arXiv:2310.17537v1 [cs.AI])

    [http://arxiv.org/abs/2310.17537](http://arxiv.org/abs/2310.17537)

    提出了一种受神经启发的方法，通过分段和回溯来克服深度强化学习中的灾难性遗忘问题，并通过内在奖励激励代理探索新的状态。

    

    深度强化学习方法在一系列任务上表现出色，但在大型环境中稀疏奖励的困难探索任务中仍然存在困难。为了解决这个问题，可以使用通过前向模型预测误差生成的内在奖励，这些误差随着环境的熟悉程度而减少，并激励代理探索新的状态。虽然基于预测的内在奖励可以帮助代理解决困难的探索任务，但它们可能会遭受灾难性遗忘，并且实际上会在访问的状态上增加。我们首先研究了格状世界环境中灾难性遗忘的条件和原因。然后，我们提出了一种受人类和动物学习启发的新方法FARCuriosity。该方法依赖于分段和回溯：代理根据惊讶性对环境进行分段，并为每个分段使用不同的本地好奇模块（基于预测的内在奖励函数），以使模块不是在整个环境上进行训练。在每个分段中，代理可以同时存储并回溯先前的奖励值以应对灾难性遗忘。

    Deep reinforcement learning methods exhibit impressive performance on a range of tasks but still struggle on hard exploration tasks in large environments with sparse rewards. To address this, intrinsic rewards can be generated using forward model prediction errors that decrease as the environment becomes known, and incentivize an agent to explore novel states. While prediction-based intrinsic rewards can help agents solve hard exploration tasks, they can suffer from catastrophic forgetting and actually increase at visited states. We first examine the conditions and causes of catastrophic forgetting in grid world environments. We then propose a new method FARCuriosity, inspired by how humans and animals learn. The method depends on fragmentation and recall: an agent fragments an environment based on surprisal, and uses different local curiosity modules (prediction-based intrinsic reward functions) for each fragment so that modules are not trained on the entire environment. At each fragm
    
[^11]: SoK：评估黑盒攻击中的陷阱

    SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])

    [http://arxiv.org/abs/2310.17534](http://arxiv.org/abs/2310.17534)

    提出了一个评估黑盒攻击的分类法，揭示了未开发的威胁空间，并展示了在某些设置上已有技术的局限性。

    

    许多研究涉及对图像分类器的黑盒攻击。然而，这些研究对对手的知识假设不同，目前的文献缺乏围绕威胁模型进行组织的连贯性。为了系统化该领域的知识，我们提出了一个关于威胁空间的分类法，涵盖了反馈粒度、交互式查询的访问和攻击者可用的辅助数据的质量和数量三个维度。我们的新分类法提供了三个关键见解。1) 尽管有广泛文献，仍存在许多未开发的威胁空间，无法通过从已知领域的技术简单地改进来解决。我们通过将基于已知领域从完整置信向量访问的技术适应到访问前k个置信得分的较少研究的设置中来证明这一点，但同时也展示了它在仅获得预测标签的更严格设置中仍然不足。

    Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
    
[^12]: 学习带未知图核的规范化图均场博弈

    Learning Regularized Graphon Mean-Field Games with Unknown Graphons. (arXiv:2310.17531v1 [cs.GT])

    [http://arxiv.org/abs/2310.17531](http://arxiv.org/abs/2310.17531)

    我们设计了用于学习图核均值场博弈的强化学习算法，通过近端策略优化算法GMFG-PPO以及核嵌入的方法来估计未知图核。我们的算法在收敛速度和效率方面取得了改进，并提供了相关的理论分析。

    

    我们设计并分析了用于图均场博弈的强化学习算法。与之前需要精确图核值的工作不同，我们旨在学习当图核未知时的规范化图均场博弈的纳什均衡。我们的贡献有三个：首先，我们提出了用于图均场博弈的近端策略优化算法（GMFG-PPO），并通过一个估计预言机证明了其在T次迭代后以$O(T^{-1/3})$的速率收敛，改进了Xie等人（ICML，2021）的前期工作。其次，利用分布的核嵌入，我们设计了高效的算法来估计采样代理的转移核、奖励函数和图核。当代理的位置已知或未知时，推导了收敛速率。然后提供了运用GMFG-PPO优化算法和估计算法的结果。这些算法是首次专门设计用于学习图核的算法。

    We design and analyze reinforcement learning algorithms for Graphon Mean-Field Games (GMFGs). In contrast to previous works that require the precise values of the graphons, we aim to learn the Nash Equilibrium (NE) of the regularized GMFGs when the graphons are unknown. Our contributions are threefold. First, we propose the Proximal Policy Optimization for GMFG (GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$ after $T$ iterations with an estimation oracle, improving on a previous work by Xie et al. (ICML, 2021). Second, using kernel embedding of distributions, we design efficient algorithms to estimate the transition kernels, reward functions, and graphons from sampled agents. Convergence rates are then derived when the positions of the agents are either known or unknown. Results for the combination of the optimization algorithm GMFG-PPO and the estimation algorithm are then provided. These algorithms are the first specifically designed for learning graphons f
    
[^13]: 评估性别中立的预训练视觉-语言模型中的偏见和公平性

    Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])

    [http://arxiv.org/abs/2310.17530](http://arxiv.org/abs/2310.17530)

    本研究评估了性别中立的预训练视觉-语言模型中的偏见和公平性，并发现预训练和微调后的偏见放大是相互独立的。此外，持续预训练对性别中立数据有利，可以在一些任务中促进公平。

    

    已知预训练的机器学习模型会保持甚至放大数据中现有的偏见，这可能导致不公平的结果，最终影响用户体验。为了确保模型的性能不会对特定群体或人口产生歧视性行为，理解这些偏见偏向的机制至关重要。在本研究中，我们将性别偏见作为案例进行定义。我们量化了三个视觉-语言模型族群的预训练和微调后的偏见放大，并调查了这两个学习阶段之间的联系，评估了偏见放大对模型性能的影响。总的来说，我们发现预训练和微调后的偏见放大是相互独立的。然后，我们研究了在性别中立的数据上进行持续预训练的效果，发现这可以减少群体差距，即在VQAv2和检索任务中促进公平，而不会显著损害任务执行能力。

    Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task p
    
[^14]: 大型语言模型能否取代人类在系统评价过程中的角色？评估GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的效果。

    Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])

    [http://arxiv.org/abs/2310.17526](http://arxiv.org/abs/2310.17526)

    本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。

    

    系统评价对于指导实践、研究和政策至关重要，然而常常需要耗费大量时间和人力。大型语言模型（LLM）可能能够加快和自动化系统评价的过程，但是它们在这些任务中的表现尚未经过全面评估，而且还没有研究测试过迄今为止最大的LLM——GPT-4。本预注册研究采用“无人参与”的方法评估了GPT-4在标题/摘要筛选、全文审查和数据提取方面在不同文献类型和语言上的能力。尽管GPT-4在大多数任务中的准确度与人类表现相当，但结果受到偶然一致性和数据集不平衡的影响。在调整了这些因素后，数据提取方面表现出中等水平的准确度，在使用高可靠性提示进行筛选的研究中，筛选全文文献的表现水平在不同阶段和语言上均为无到中等。

    Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
    
[^15]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^16]: 通过发现主要方向实现人工说话者嵌入的可控生成

    Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions. (arXiv:2310.17502v1 [cs.SD])

    [http://arxiv.org/abs/2310.17502](http://arxiv.org/abs/2310.17502)

    本文提出了一种方法来生成不能与真实人类相关联的人工说话者嵌入，从而实现在语音合成中的声音和说话风格的直观和精细控制。这种方法不需要任何标签，并且在训练期间将真实人类的嵌入与人工可控嵌入相关联，从而在推理过程中确保了隐私。

    

    在一个具有直观和精细控制的语音合成系统中定制声音和说话风格是具有挑战性的，因为很少有带有合适标签的数据可用。此外，编辑现有人的声音也涉及道德问题。本文提出了一种方法，可以生成不能与真实人类相关联的人工说话者嵌入，而且能够在嵌入的声音和说话风格上提供直观和精细的控制，而不需要任何说话者或风格的标签。这些人工可控的嵌入可以在训练期间与真实人类的嵌入条件下输入到语音合成系统中，在推理过程中不牺牲隐私。

    Customizing voice and speaking style in a speech synthesis system with intuitive and fine-grained controls is challenging, given that little data with appropriate labels is available. Furthermore, editing an existing human's voice also comes with ethical concerns. In this paper, we propose a method to generate artificial speaker embeddings that cannot be linked to a real human while offering intuitive and fine-grained control over the voice and speaking style of the embeddings, without requiring any labels for speaker or style. The artificial and controllable embeddings can be fed to a speech synthesis system, conditioned on embeddings of real humans during training, without sacrificing privacy during inference.
    
[^17]: IMS Toucan系统用于Blizzard Challenge 2023

    The IMS Toucan System for the Blizzard Challenge 2023. (arXiv:2310.17499v1 [cs.CL])

    [http://arxiv.org/abs/2310.17499](http://arxiv.org/abs/2310.17499)

    我们改进了我们在Blizzard Challenge 2021中提交的系统，使用一个基于规则的文本到音素处理系统，并且设计了数据处理、训练和推理过程来处理Blizzard Challenge 2023的数据。我们的系统标识符是G。

    

    为了参加Blizzard Challenge 2023，我们改进了在Blizzard Challenge 2021中提交的系统。我们的方法是一个基于规则的文本到音素处理系统，包括对法语中的同音异形词进行基于规则的消歧。然后，我们使用基于Conformer和Glow的快速高效的非自回归合成架构将音素转换为中间表示 - 频谱图。一个基于GAN的神经声码器结合了最新的先进方法，将频谱图转换为最终的波形。我们精心设计了用于挑战数据的数据处理、训练和推理过程。我们的系统标识符是G。提供开源代码和演示。

    For our contribution to the Blizzard Challenge 2023, we improved on the system we submitted to the Blizzard Challenge 2021. Our approach entails a rule-based text-to-phoneme processing system that includes rule-based disambiguation of homographs in the French language. It then transforms the phonemes to spectrograms as intermediate representations using a fast and efficient non-autoregressive synthesis architecture based on Conformer and Glow. A GAN based neural vocoder that combines recent state-of-the-art approaches converts the spectrogram to the final wave. We carefully designed the data processing, training, and inference procedures for the challenge data. Our system identifier is G. Open source code and demo are available.
    
[^18]: CBD: 基于局部主导概率的可信后门检测器

    CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v1 [cs.LG])

    [http://arxiv.org/abs/2310.17498](http://arxiv.org/abs/2310.17498)

    本文提出了第一个可信后门检测器（CBD），它基于一种新颖的、可调节的符合预测方案，即局部主导概率统计。CBD能够提供对分类器的检测推断结果，并给出攻击保证可检测的条件和假阳性率的概率上界。实验证明，具有更高鲁棒性的触发器和更小扰动幅度的攻击更容易被检测出来。

    

    后门攻击是深度神经网络面临的常见威胁。在测试过程中，嵌入了后门触发器的样本将被后门模型误分类为对抗目标，而没有后门触发器的样本将被正确分类。本文提出了第一个可信后门检测器（CBD），它基于我们提出的局部主导概率统计的一种新颖的、可调节的符合预测方案。对于受检测的任何分类器，CBD提供了1）检测推断结果，2）攻击在同一分类域下保证可检测的条件，并且3）假阳性率的概率上界。我们的理论结果表明，具有更高鲁棒性的触发器、更小扰动幅度的攻击更有可能被有保证地检测出来。此外，我们在考虑各种后门类型的四个基准数据集上进行了大量实验证明。

    Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, 
    
[^19]: 解决A/B测试中数据训练循环引起的干扰：一种加权训练方法

    Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])

    [http://arxiv.org/abs/2310.17496](http://arxiv.org/abs/2310.17496)

    该论文提出了一种名为加权训练的方法，用于解决A/B测试中由数据训练循环引起的干扰。该方法通过训练模型预测每个数据点在实验组或控制组中出现的概率，并在模型训练过程中应用加权损失，从而实现了最小方差的估计结果，并且不会引起训练分布的变化。

    

    在现代推荐系统中，标准流程涉及使用历史数据训练机器学习模型来预测用户行为并持续改进推荐。然而，这些数据训练循环可能在A/B测试中引入干扰，其中控制组和实验组算法生成的数据，可能具有不同的分布，被合并在一起。为了解决这些挑战，我们提出了一种新颖的方法，称为加权训练。该方法包括训练一个模型来预测每个数据点出现在实验组或控制组数据中的概率，并在模型训练过程中应用加权损失。我们通过模拟研究证明了这种方法在所有估计量中具有最小的方差，且不会导致训练分布发生变化。我们通过模拟研究证明了与其他方法相比，我们的方法具有较低的偏差和方差。

    In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators without causing shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.
    
[^20]: 基于模拟器辅助的移动边缘调优的AI基础模型编排：一种多智能体深度强化学习方法

    Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach. (arXiv:2310.17492v1 [cs.AI])

    [http://arxiv.org/abs/2310.17492](http://arxiv.org/abs/2310.17492)

    本研究提出了一种基于模拟器辅助的移动边缘调优的AI基础模型编排方法，该方法通过创新的模拟器适配器架构和混合多智能体深度强化学习策略，实现了高效部署和精调基础模型，从而提高了本地任务性能。

    

    在当代人工智能中，高效部署和精调基础模型至关重要。本研究提出一种突破性的范式，将移动边缘计算（MEC）与基础模型集成，旨在增强用户设备上的本地任务性能。我们的方法的核心是创新的模拟器适配器架构，将基础模型分割为两个协同模块。这种设计不仅节省了计算资源，还确保了适应性和下游任务的精调效率。此外，我们引入了一种先进的资源分配机制，针对去中心化环境中的模拟器适配器结构的需求进行精调。为应对该系统所面临的挑战，我们采用了一种混合多智能体深度强化学习（DRL）策略，擅长处理混合离散-连续动作空间，确保动态和最优资源分配。我们进行了全面的模拟研究来验证我们的方法。

    The efficient deployment and fine-tuning of foundation models are pivotal in contemporary artificial intelligence. In this study, we present a groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation models, specifically designed to enhance local task performance on user equipment (UE). Central to our approach is the innovative Emulator-Adapter architecture, segmenting the foundation model into two cohesive modules. This design not only conserves computational resources but also ensures adaptability and fine-tuning efficiency for downstream tasks. Additionally, we introduce an advanced resource allocation mechanism that is fine-tuned to the needs of the Emulator-Adapter structure in decentralized settings. To address the challenges presented by this system, we employ a hybrid multi-agent Deep Reinforcement Learning (DRL) strategy, adept at handling mixed discrete-continuous action spaces, ensuring dynamic and optimal resource allocations. Our comprehensive simula
    
[^21]: FedPEAT: 联邦学习、参数高效微调与辅助调优在基础人工智能模型与移动边缘计算中的融合

    FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])

    [http://arxiv.org/abs/2310.17491](http://arxiv.org/abs/2310.17491)

    FedPEAT是将辅助调优和参数高效微调应用于联邦学习的方法，能够提升基础人工智能模型的模型隐私和内存效率。

    

    基础模型的出现，包括语言和视觉模型，改变了人工智能的领域，为各种应用提供了能力。部署和微调这些大型模型，如GPT-3和BERT，在当前的基础模型时代面临挑战。我们介绍了辅助调优（EAT）与参数高效微调（PEFT）相结合，形成了参数高效辅助调优（PEAT）。此外，我们将其扩展到联邦学习作为联邦PEAT（FedPEAT）。FedPEAT使用适配器、仿真器和PEFT进行联邦模型调优，提高模型隐私和内存效率。适配器调整预训练模型，而仿真器给出原始模型的紧凑表示，同时解决隐私和效率问题。我们的方法适用于各种神经网络，还利用深度强化学习进行超参数优化。我们在一个独特的场景中使用FedPEAT进行了测试，其中服务器参与协作联邦学习。

    The emergence of foundation models, including language and vision models, has reshaped AI's landscape, offering capabilities across various applications. Deploying and fine-tuning these large models, like GPT-3 and BERT, presents challenges, especially in the current foundation model era. We introduce Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning (PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses adapters, emulators, and PEFT for federated model tuning, enhancing model privacy and memory efficiency. Adapters adjust pre-trained models, while emulators give a compact representation of original models, addressing both privacy and efficiency. Adaptable to various neural networks, our approach also uses deep reinforcement learning for hyper-parameter optimization. We tested FedPEAT in a unique scenario with a server participating in collaborative federate
    
[^22]: 评估过程中的偏见：基于优化的模型

    Bias in Evaluation Processes: An Optimization-Based Model. (arXiv:2310.17489v1 [cs.CY])

    [http://arxiv.org/abs/2310.17489](http://arxiv.org/abs/2310.17489)

    本研究提出了一个基于优化的模型，用于评估过程中的偏见分析。模型通过将真实效用分布转化为观察到的分布来考虑偏见，并对参数进行研究，以探究其对观察到的分布的影响。通过实证验证和数据拟合，我们证明了该模型的有效性。

    

    在诸如招生和招聘等设置中，关于个人社会显著属性的偏见已被广泛记录在评估过程中。我们将这样的评估过程视为将个人对任务的真实效用分布转化为观察到的分布，并将其建模为在信息约束下的损失最小化问题的解。我们的模型具有两个参数，被视为导致偏见的因素：信息约束中的资源信息交换参数和损失函数中的风险厌恶参数。我们表征了由我们的模型产生的分布，并研究了参数对观察到的分布的影响。我们模型的输出丰富了可用于捕捉观察评估中群组间变化的分布类。我们通过拟合真实世界数据集来进行实证验证，并使用它来研究介入效果。

    Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interve
    
[^23]: 公平协作车辆路径规划：一种深度多智能体强化学习方法

    Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach. (arXiv:2310.17485v1 [cs.LG])

    [http://arxiv.org/abs/2310.17485](http://arxiv.org/abs/2310.17485)

    这篇论文提出了一种采用深度多智能体强化学习方法解决公平协作车辆路径规划问题的方法。通过合作谈判博弈模型，并在生产中显式推理而不是访问特征函数的方式，有效降低了成本和计算复杂度。

    

    协作车辆路径规划是指承运商通过共享运输请求并代表对方进行运输请求，以实现规模经济，从而降低成本、减少温室气体排放和道路拥堵。然而，应该由哪个承运商与谁合作，每个承运商应该得到多少补偿呢？传统的博弈论解方案计算成本高，特征函数随着代理人数量呈指数倍增。这就需要指数次解决车辆路径规划问题（NP-hard）。因此，我们提出将该问题建模为使用深度多智能体强化学习解决的合作谈判博弈，关键是代理人无法访问特征函数。相反，我们隐含地推理特征函数；因此，在生产中部署时，仅需评估昂贵的合作后车辆路径规划问题。

    Collaborative vehicle routing occurs when carriers collaborate through sharing their transportation requests and performing transportation requests on behalf of each other. This achieves economies of scale, thus reducing cost, greenhouse gas emissions and road congestion. But which carrier should partner with whom, and how much should each carrier be compensated? Traditional game theoretic solution concepts are expensive to calculate as the characteristic function scales exponentially with the number of agents. This would require solving the vehicle routing problem (NP-hard) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game solved using deep multi-agent reinforcement learning, where - crucially agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function; thus, when deployed in production, we only need to evaluate the expensive post-collaboration vehicle routing pr
    
[^24]: 基于Transformer的联邦学习的智能电网短期负荷预测的安全性

    Secure short-term load forecasting for smart grids with transformer-based federated learning. (arXiv:2310.17477v1 [cs.LG])

    [http://arxiv.org/abs/2310.17477](http://arxiv.org/abs/2310.17477)

    该论文提出了一种基于Transformer的联邦学习方法，用于智能电网的短期负荷预测。该方法可以提高数据隐私性，并在实验中展示出良好的性能。

    

    电力负荷预测是智能电网中的重要任务，可以帮助平衡供需。尽管先进的深度学习模型需要大量高分辨率数据来准确预测短期负荷，但细粒度的负荷曲线可能暴露用户的用电行为，引发隐私和安全问题。一种改善数据隐私的解决方案是联邦学习，即在私有数据上本地训练模型，并将训练好的模型参数合并和更新到全局服务器。因此，本文提出了一种新颖的基于Transformer的深度学习方法，结合联邦学习进行短期电力负荷预测。为了评估我们的结果，我们将我们的联邦学习架构与中心化和本地学习进行了基准测试，并将我们的模型性能与长短期记忆模型和卷积神经网络进行了比较。我们的模拟基于德国一所大学校园的数据集，结果表明Transformer-based federated learning具有良好的性能。

    Electricity load forecasting is an essential task within smart grids to assist demand and supply balance. While advanced deep learning models require large amounts of high-resolution data for accurate short-term load predictions, fine-grained load profiles can expose users' electricity consumption behaviors, which raises privacy and security concerns. One solution to improve data privacy is federated learning, where models are trained locally on private data, and only the trained model parameters are merged and updated on a global server. Therefore, this paper presents a novel transformer-based deep learning approach with federated learning for short-term electricity load prediction. To evaluate our results, we benchmark our federated learning architecture against central and local learning and compare the performance of our model to long short-term memory models and convolutional neural networks. Our simulations are based on a dataset from a German university campus and show that tran
    
[^25]: 基于基础模型的6G原生AI框架与云边缘协同合作

    Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End Collaboration. (arXiv:2310.17471v1 [cs.IT])

    [http://arxiv.org/abs/2310.17471](http://arxiv.org/abs/2310.17471)

    该论文分析了从数据、智能和网络的角度来实现6G原生AI的挑战，并提出了基于基础模型的6G原生AI框架，包括自定义方法和任务导向的AI工具包，以及新的云边缘协同合作范式。

    

    未来的无线通信网络有望超越以数据为中心、以设备为导向的连接方式，提供基于任务导向连接的智能沉浸式体验，特别是在预训练基础模型（PFM）的快速发展和6G原生人工智能（AI）的发展愿景下。因此，在6G中，重新定义设备和服务器之间的协作模式，构建原生智能库变得非常重要。在本文中，我们从数据、智能和网络的角度分析了实现6G原生AI的挑战。然后，我们基于基础模型提出了一个6G原生AI框架，提供了一种意图感知PFM的定制方法，展示了一个面向任务的AI工具包的构建，并概述了一种全新的云边缘协同合作范式。作为一个实际的使用案例，我们将该框架应用于编排，实现了无线通信中的最大速率之和。

    Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on task-oriented connections, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and servers and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, intelligence, and networks. Then, we propose a 6G native AI framework based on foundation models, provide a customization approach for intent-aware PFM, present a construction of a task-oriented AI toolkit, and outline a novel cloud-edge-end collaboration paradigm. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a wireless communic
    
[^26]: 跨模态主动互补学习与自我完善对应关系

    Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])

    [http://arxiv.org/abs/2310.17468](http://arxiv.org/abs/2310.17468)

    本文提出了一种跨模态主动互补学习框架（CRCL），通过使用新颖的主动互补损失（ACL）和高效的自我完善对应关系修正（SCC），改善了现有方法的鲁棒性。

    

    最近，图像和文本的匹配引起了学术界和工业界越来越多的关注，这是理解视觉和文本模态之间潜在对应关系的基础。然而，大多数现有方法隐式假设训练对是对齐良好的，而忽略了普遍存在的注释噪音，即噪声对应（NC），从而不可避免地导致性能下降。虽然一些方法尝试解决这种噪声，但仍面临两个挑战性问题：过度记忆/过拟合和对于NC的不可靠修正，特别是在高噪声下。为了解决这两个问题，我们提出了一个通用的跨模态鲁棒互补学习框架（CRCL），它通过利用一种新颖的主动互补损失（ACL）和高效的自我完善对应关系修正（SCC）来改进现有方法的鲁棒性。具体而言，ACL利用主动和互补的学习损失来减少提供错误信息的风险。

    Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous s
    
[^27]: 生成性扩散模型的统计热力学

    The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])

    [http://arxiv.org/abs/2310.17467](http://arxiv.org/abs/2310.17467)

    本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。

    

    生成性扩散模型在生成建模的许多领域取得了惊人的表现。虽然这些模型的基本思想来自非平衡物理学，但本文中我们表明，可以用平衡统计力学的工具来理解这些模型的许多方面。利用这种重构，我们展示了生成性扩散模型经历了与对称性破缺现象相对应的二阶相变。我们认为，这导致了一种稳定性形式，它是生成能力的核心，并可以用一组平均场临界指数来描述。最后，我们根据热力学的公式分析了将扩散模型与关联记忆网络连接的最近研究。

    Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
    
[^28]: 贝叶斯神经控制微分方程用于治疗效果估计

    Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation. (arXiv:2310.17463v1 [cs.LG])

    [http://arxiv.org/abs/2310.17463](http://arxiv.org/abs/2310.17463)

    本文提出了一种新颖的贝叶斯神经控制微分方程方法，用于连续时间的治疗效果估计，该方法能够提供对潜在结果的后验预测分布，并给出了可靠的不确定性估计。

    

    在个性化医学中，连续时间的治疗效果估计非常重要。然而，现有的方法只能给出潜在结果的点估计，忽略了不确定性的估计。毫无疑问，不确定性量化对于可靠的决策是至关重要的。为了填补这一空白，我们提出了一种新颖的贝叶斯神经控制微分方程(BNCDE)用于连续时间的治疗效果估计。在我们的BNCDE中，时间维度通过一组耦合的神经控制微分方程和神经随机微分方程建模，其中神经随机微分方程允许可行的变分贝叶斯推断。因此，对于给定的治疗序列，我们的BNCDE提供了有意义的潜在结果的后验预测分布。据我们所知，我们的方法是第一个专门提供治疗效果不确定性估计的神经方法。

    Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications. To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time. In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference. Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes. To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of tre
    
[^29]: 使用物理运动定律从2D标注中学习单目3D物体定位

    Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])

    [http://arxiv.org/abs/2310.17462](http://arxiv.org/abs/2310.17462)

    该论文提出了一种新的方法，利用物体运动的物理知识和简单的2D标注，从单目图像中实现精确的3D物体定位，无需昂贵的3D标注。经过实验证明，在真实数据实验中平均距离误差仅为6厘米，表明该方法在无法收集3D数据进行训练的情况下具有潜力。

    

    我们提出了一种新的方法，利用物体的运动物理知识和简单的2D标注，从单个校准相机的单个图像中精确定位3D物体，无需昂贵的3D标注。在训练过程中，模型能够推断出隐含的第三个维度，即使在训练时从未见过此信息。我们在合成和真实世界数据集上评估了该方法，并在真实数据实验中实现了平均距离误差仅为6厘米。结果表明该方法在无法收集3D数据进行训练的情况下，作为学习3D物体定位估计的一步具有潜力。

    We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
    
[^30]: 基于强化学习的联合讨价还价：应用于协作车辆路径规划

    Coalitional Bargaining via Reinforcement Learning: An Application to Collaborative Vehicle Routing. (arXiv:2310.17458v1 [cs.LG])

    [http://arxiv.org/abs/2310.17458](http://arxiv.org/abs/2310.17458)

    本论文提出了一种通过强化学习方法解决协作车辆路径规划中合作伙伴选择和补偿分配的问题的方法，并将问题建模为联合讨价还价博弈，通过隐式推理特征函数来减少计算复杂性。

    

    协作车辆路径规划是指多家运输公司通过共享运输信息和代表对方执行运输请求而进行合作。这样可以实现规模经济，从而降低成本、减少温室气体排放和道路拥挤。然而，哪家公司应该与哪家公司合作，每家公司应该得到多少补偿，传统的博弈论解决概念，如Shapley值或核值，由于特征函数随参与者数量呈指数级增长，难以计算用于实际的协作车辆路径规划问题。这将需要指数次解决车辆路径规划问题（一个NP难问题）。因此，我们提出将该问题建模为一种联合讨价还价博弈，重要的是，代理商不被赋予特征函数的访问权限。相反，我们隐式推理特征函数，从而消除了评估VRP的需要，减少了计算复杂性。

    Collaborative Vehicle Routing is where delivery companies cooperate by sharing their delivery information and performing delivery requests on behalf of each other. This achieves economies of scale and thus reduces cost, greenhouse gas emissions, and road congestion. But which company should partner with whom, and how much should each company be compensated? Traditional game theoretic solution concepts, such as the Shapley value or nucleolus, are difficult to calculate for the real-world problem of Collaborative Vehicle Routing due to the characteristic function scaling exponentially with the number of agents. This would require solving the Vehicle Routing Problem (an NP-Hard problem) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game where - crucially - agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function, and thus eliminate the need to evaluate the VRP an exp
    
[^31]: 不受帧序列约束的手语识别：针对阿根廷手语的概念验证

    Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language. (arXiv:2310.17437v1 [cs.CV])

    [http://arxiv.org/abs/2310.17437](http://arxiv.org/abs/2310.17437)

    这项研究在手语识别中提出了一个不受帧序列约束的概率模型，利用位置、运动和手形等特征进行手势分类，通过词袋模型方法探索手势的统计特征。

    

    自动手语识别（SLR）是人机交互和机器学习领域中的重要课题。它具有复杂的挑战，需要涉及各种知识领域，如视频处理、图像处理、智能系统和语言学。手语的稳健识别可以帮助翻译过程、听力障碍者的融入，以及对听觉人群进行手语教学。SLR系统通常使用隐马尔可夫模型、动态时间规整或类似模型来识别手势。这些技术利用帧的顺序来减少假设的数量。本文提出了一个通用的概率模型，用于手势分类，结合了基于不同类型特征（如位置、运动和手形）的子分类器。模型在所有分类步骤中采用词袋模型方法，以探索手势的统计特征。

    Automatic sign language recognition (SLR) is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language for the hearing population.  SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or similar models to recognize signs. Such techniques exploit the sequential ordering of frames to reduce the number of hypothesis. This paper presents a general probabilistic model for sign classification that combines sub-classifiers based on different types of features such as position, movement and handshape. The model employs a bag-of-words approach in all classification steps, to explore t
    
[^32]: 基于似然的去噪扩散概率模型的离群检测

    Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models. (arXiv:2310.17432v1 [cs.LG])

    [http://arxiv.org/abs/2310.17432](http://arxiv.org/abs/2310.17432)

    本论文提出了一种基于似然的离群检测方法，通过利用扩散模型受输入样本复杂性影响的观察，构建了复杂度校正似然比。实验结果表明该方法在离群检测方面达到了最先进的水平。

    

    使用生成模型进行数据集之间的离群检测已经得到广泛探索。我们展示了基于似然的离群检测可以通过利用扩散模型受输入样本复杂性影响的观察来扩展到扩散模型。目前，所有基于扩散模型的离群检测方法都是基于重构的。我们提出了一种新的离群检测似然比，称为复杂度校正似然比。我们的似然比是通过在各种噪声水平下使用个体模型的证据下界评估构造的。我们展示的结果与基于生成模型的最先进离群检测方法相媲美。

    Out-of-Distribution detection between dataset pairs has been extensively explored with generative models. We show that likelihood-based Out-of-Distribution detection can be extended to diffusion models by leveraging the fact that they, like other likelihood-based generative models, are dramatically affected by the input sample complexity. Currently, all Out-of-Distribution detection methods with Diffusion Models are reconstruction-based. We propose a new likelihood ratio for Out-of-Distribution detection with Deep Denoising Diffusion Models, which we call the Complexity Corrected Likelihood Ratio. Our likelihood ratio is constructed using Evidence Lower-Bound evaluations from an individual model at various noising levels. We present results that are comparable to state-of-the-art Out-of-Distribution detection methods with generative models.
    
[^33]: 使用ProbSom进行阿根廷手语手势识别的研究

    Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])

    [http://arxiv.org/abs/2310.17427](http://arxiv.org/abs/2310.17427)

    本文的两个主要贡献是：首次创建了一个针对阿根廷手语的手势数据库，并提出了一种使用ProbSom进行图像处理和手势分类的技术，该技术在目前的研究中与其他方法进行了对比。

    

    自动手语识别是人机交互和机器学习领域的重要课题。这既是一个复杂的挑战，需要涉及视频处理、图像处理、智能系统和语言学等多个知识领域的介入，又是一种强大的手语识别，可以辅助翻译过程和融入听障人士的整合。本文提出了两个主要贡献：首先，创建了一个针对阿根廷手语（LSA）的手势数据库，这是一个迄今为止几乎没有讨论过的话题。其次，利用一种名为ProbSom的自组织映射的监督自适应技术，进行图像处理、特征提取和后续手势分类。该技术与支持向量机（SVM）、随机森林和神经网络等现有技术进行了比较。

    Automatic sign language recognition is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people.  This paper offers two main contributions: first, the creation of a database of handshapes for the Argentinian Sign Language (LSA), which is a topic that has barely been discussed so far. Secondly, a technique for image processing, descriptor extraction and subsequent handshape classification using a supervised adaptation of self-organizing maps that is called ProbSom. This technique is compared to others in the state of the art, such as Support Vector Machines (SVM), Random Forests, and Neural Networks.  The database that was built conta
    
[^34]: 带有平稳扩散的因果建模

    Causal Modeling with Stationary Diffusions. (arXiv:2310.17405v1 [cs.LG])

    [http://arxiv.org/abs/2310.17405](http://arxiv.org/abs/2310.17405)

    本文提出了一种新颖的因果推断方法，使用随机微分方程建模系统行为，不需要因果图的形式化。在多种情况下，该方法比传统方法更好地推广到未见干预的变量。

    

    我们提出了一种新颖的因果推断方法。与使用因果图的结构方程不同，我们学习随机微分方程(SDE)，其平稳密度可以模拟系统在干预下的行为。这些平稳扩散模型不需要因果图的形式化，更不需要常见的无环性假设。我们展示了在多种情况下，它们比传统方法更好地推广到变量上的未见干预。我们的推断方法基于一个新的理论结果，该结果在再生核希尔伯特空间中表达了扩散的生成器的稳定条件。由此产生的核从稳态偏离(KDS)是一个值得独立关注的客观函数。

    We develop a novel approach towards causal inference. Rather than structural equations over a causal graph, we learn stochastic differential equations (SDEs) whose stationary densities model a system's behavior under interventions. These stationary diffusion models do not require the formalism of causal graphs, let alone the common assumption of acyclicity. We show that in several cases, they generalize to unseen interventions on their variables, often better than classical approaches. Our inference method is based on a new theoretical result that expresses a stationarity condition on the diffusion's generator in a reproducing kernel Hilbert space. The resulting kernel deviation from stationarity (KDS) is an objective function of independent interest.
    
[^35]: 神经网络的不变性测量

    Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])

    [http://arxiv.org/abs/2310.17404](http://arxiv.org/abs/2310.17404)

    本文提出了一种用于量化神经网络不变性的测量方法，该方法敏感且可解释，并能应用于任何神经网络模型。在仿射变换领域和CIFAR10和MNIST数据集上的验证表明，神经网络的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。这些测量方法将为不变性表示的新研究方向提供可能性。

    

    神经网络中的不变性对许多任务都是有用且必要的。然而，大多数神经网络模型中的不变性表示尚未被明确表征。我们提出了一种量化神经网络不变性的测量方法，该方法基于其内部表示。这些测量方法高效且可解释，并可应用于任何神经网络模型。与之前定义的测量方法相比，它们对不变性更为敏感。我们在仿射变换领域和CIFAR10和MNIST数据集上验证了这些测量方法及其属性，包括其稳定性和可解释性。利用这些测量方法，我们对CNN模型进行了首次分析，并展示了它们的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。我们相信这些测量方法将为不变性表示的新研究方向提供可能性。

    Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.
    
[^36]: 检测防御: 光流中对抗性贴片攻击的空洞承诺

    Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])

    [http://arxiv.org/abs/2310.17403](http://arxiv.org/abs/2310.17403)

    本文对光流中的对抗性贴片攻击进行了研究，发现目前的检测和去除防御策略不仅降低了光流质量，同时也损害了抵御贴片攻击的鲁棒性。

    

    当放置在任意场景位置时，对抗性贴片破坏了光流预测的可靠性。因此，它们对现实世界中的运动检测及其下游应用构成了真实威胁。潜在的解决方案是检测和去除对抗性贴片的防御策略，但其对底层运动预测的影响尚未被研究。在本文中，我们对当前可用的检测和去除防御策略ILP和LGS对一系列先进光流方法进行了彻底的研究，并阐明了它们对最终光流预测的质量和鲁棒性的副作用。特别地，我们实施了防御感知攻击，以调查当前的防御是否能够抵御考虑防御机制的攻击。我们的实验得出了两个令人惊讶的结果：检测和去除防御策略不仅降低了良好场景下的光流质量，同时也损害了针对贴片攻击的鲁棒性。

    Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patc
    
[^37]: 以结构为基础增强图神经网络

    Enhancing Graph Neural Networks with Structure-Based Prompt. (arXiv:2310.17394v1 [cs.LG])

    [http://arxiv.org/abs/2310.17394](http://arxiv.org/abs/2310.17394)

    这篇论文提出了一种以结构为基础的图神经网络的提示方法（SAP），该方法在预训练和提示调整阶段都一致地利用结构信息，从而增强了图神经网络在学习任务特定参数方面的能力。

    

    图神经网络（GNNs）在学习图数据的语义方面具有很强的能力。最近，一种新的范式“pre-train, prompt”在使用较少有监督数据的情况下适应各种任务的GNNs方面取得了有希望的结果。这种范式的成功可以归因于预训练和面向任务的提示调整的更一致的目标，其中预训练的知识可以有效地转移到下游任务中。然而，现有研究中一个被忽视的问题是，在学习节点表示的预训练阶段通常利用图的结构信息，而在提示调整阶段忽视了结构信息的利用以学习任务特定参数。为了弥补这一差距，我们提出了一种新的以结构为基础的GNNs提示方法，即SAP，它在预训练和提示调整阶段都一致地利用了结构信息。具体而言，SAP 1）采用了双视图对比学习，以对齐节点属性和图结构的潜在语义空间。

    Graph Neural Networks (GNNs) are powerful in learning semantics of graph data. Recently, a new paradigm "pre-train, prompt" has shown promising results in adapting GNNs to various tasks with less supervised data. The success of such paradigm can be attributed to the more consistent objectives of pre-training and task-oriented prompt tuning, where the pre-trained knowledge can be effectively transferred to downstream tasks. However, an overlooked issue of existing studies is that the structure information of graph is usually exploited during pre-training for learning node representations, while neglected in the prompt tuning stage for learning task-specific parameters. To bridge this gap, we propose a novel structure-based prompting method for GNNs, namely SAP, which consistently exploits structure information in both pre-training and prompt tuning stages. In particular, SAP 1) employs a dual-view contrastive learning to align the latent semantic spaces of node attributes and graph stru
    
[^38]: 在使用双层优化中重新加权数据的挑战

    A Challenge in Reweighting Data with Bilevel Optimization. (arXiv:2310.17386v1 [stat.ML])

    [http://arxiv.org/abs/2310.17386](http://arxiv.org/abs/2310.17386)

    在重新加权数据的任务中，经典的双层优化方法可能会导致次优解，使得最终的数据权重非常稀疏，这解释了为什么这种方法在实践中很少被使用。

    

    在许多情况下，我们使用一个大的训练集来训练模型，目标是在一个不同分布的较小测试集上获得良好的性能。为训练集中的每个数据点学习一个权重是一个有吸引力的解决方案，因为理想情况下它可以自动学习每个训练点在测试集上的泛化重要性。这个任务通常被形式化为一个双层优化问题。传统的双层求解器基于一个热启动策略，即同时学习模型的参数和数据权重。我们展示了这种联合动态可能导致次优解，其中最终的数据权重非常稀疏。这一发现说明了数据重新加权的困难，并提示了为什么这种方法在实践中很少被使用的原因。

    In many scenarios, one uses a large training set to train a model with the goal of performing well on a smaller testing set with a different distribution. Learning a weight for each data point of the training set is an appealing solution, as it ideally allows one to automatically learn the importance of each training point for generalization on the testing set. This task is usually formalized as a bilevel optimization problem. Classical bilevel solvers are based on a warm-start strategy where both the parameters of the models and the data weights are learned at the same time. We show that this joint dynamic may lead to sub-optimal solutions, for which the final data weights are very sparse. This finding illustrates the difficulty of data reweighting and offers a clue as to why this method is rarely used in practice.
    
[^39]: 多任务在线学习：倾听社区的喧嚣

    Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])

    [http://arxiv.org/abs/2310.17385](http://arxiv.org/abs/2310.17385)

    我们提出了一种多任务在线学习的算法，代理只能通过邻居交换信息。我们的分析表明，当代理在相似的任务上操作时，我们的算法的遗憾值显著改善。此外，我们证明了算法在损失函数为线性函数时可以保护隐私。

    

    我们研究了一种多任务在线学习的设置，其中代理只能在任意通信网络上与其邻居交换信息。我们介绍了一种分散算法$\texttt{MT-CO}_2\texttt{OL}$，其遗憾值取决于任务相似性和网络结构的相互作用。我们的分析表明，$\texttt{MT-CO}_2\texttt{OL}$的遗憾值（常数除外）永远不会比代理不共享信息时获得的上界差。另一方面，当相邻代理在相似的任务上操作时，我们的界限显著改善。此外，我们证明了在损失函数为线性函数时，我们的算法可以在隐私保护性上做到微不足道的影响。最后，我们提供了实验证据支持我们的理论。

    We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce $\texttt{MT-CO}_2\texttt{OL}$, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of $\texttt{MT-CO}_2\texttt{OL}$ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.
    
[^40]: 基于生理信号和眼动追踪的游戏类型识别

    On the recognition of the game type based on physiological signals and eye tracking. (arXiv:2310.17383v1 [cs.LG])

    [http://arxiv.org/abs/2310.17383](http://arxiv.org/abs/2310.17383)

    本文提出了一种基于生理信号和眼动追踪的游戏类型识别方法，并通过构建分类器在不同游戏和游戏间暂停的情况下进行了验证。研究结果表明该方法可以应用于智能监控和量化自身领域。

    

    通过自动解读信号，在情感计算和人类活动识别领域得到了许多应用。本文探讨了在特定信号集合的基础上是否可以识别认知活动的可能性。我们以参与者所玩游戏的识别作为问题探索的实验场所。我们构建了三种不同游戏（太空侵略者、俄罗斯方块、塔防）和游戏间暂停的分类器。我们在独立于玩家和依赖于玩家两种情况下验证了分类器。我们讨论了在依赖于玩家情景中的改进，以及在生物特征人员识别方面的改进。根据游戏分类结果，我们考虑了在智能监控和量化自身方面的潜在应用。

    Automated interpretation of signals yields many impressive applications from the area of affective computing and human activity recognition (HAR). In this paper we ask the question about possibility of cognitive activity recognition on the base of particular set of signals. We use recognition of the game played by the participant as a playground for exploration of the problem. We build classifier of three different games (Space Invaders, Tetris, Tower Defence) and inter-game pause. We validate classifier in the player-independent and player-dependent scenario. We discuss the improvement in the player-dependent scenario in the context of biometric person recognition. On the base of the results obtained in game classification, we consider potential applications in smart surveillance and quantified self.
    
[^41]: 基于切向空间中的灵敏度的ReLU网络的优化相关泛化界限

    Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])

    [http://arxiv.org/abs/2310.17378](http://arxiv.org/abs/2310.17378)

    本文通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限，通过限制网络梯度对于输入数据在优化轨迹上的扰动的灵敏度，不显式地依赖网络的深度。

    

    近年来，深度学习取得了一些非常有希望的结果，对于深度神经网络的泛化能力，然而文献仍然缺乏一个全面的理论解释为什么过度参数化的模型能够很好地泛化，同时拟合训练数据。在本文中，我们通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限。关键思想是将网络梯度对于输入数据在优化轨迹上的扰动的灵敏度限制在一个界限内。所得到的界限不显式地依赖网络的深度。我们的结果在MNIST和CIFAR-10数据集上得到实验证实。

    Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
    
[^42]: 《面向概率时空图学习的扩散模型统一化》

    Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning. (arXiv:2310.17360v1 [cs.LG])

    [http://arxiv.org/abs/2310.17360](http://arxiv.org/abs/2310.17360)

    本文提出了一种统一的时空图学习方法，即统一时空扩散模型（USTD），用于处理物联网时代的各种学习任务。USTD综合考虑了时空数据中的不确定性，并利用共享的时空模式进行预测。此方法具有通用性和灵活性。

    

    在物联网时代，时空图学习是一个基本问题，可以应用于智能城市、人类移动性和气候分析等众多网络应用。现有方法独立地解决不同的学习任务，并根据任务特点调整模型。然而，这些方法未能模拟时空数据中的固有不确定性。同时，它们专门设计的模型限制了其作为普适时空学习解决方案的通用性。在本文中，我们提出以统一视角建模学习任务，将其视为基于共享时空模式的条件信息预测。基于这一建议，我们引入了统一时空扩散模型（USTD），在考虑不确定性的扩散框架下统一处理任务。USTD的设计是全面的，包括一个共享时空编码器和基于注意力的去噪网络，针对不同任务进行。

    Spatio-temporal graph learning is a fundamental problem in the Web of Things era, which enables a plethora of Web applications such as smart cities, human mobility and climate analysis. Existing approaches tackle different learning tasks independently, tailoring their models to unique task characteristics. These methods, however, fall short of modeling intrinsic uncertainties in the spatio-temporal data. Meanwhile, their specialized designs limit their universality as general spatio-temporal learning solutions. In this paper, we propose to model the learning tasks in a unified perspective, viewing them as predictions based on conditional information with shared spatio-temporal patterns. Based on this proposal, we introduce Unified Spatio-Temporal Diffusion Models (USTD) to address the tasks uniformly within the uncertainty-aware diffusion framework. USTD is holistically designed, comprising a shared spatio-temporal encoder and attention-based denoising networks that are task-specific. 
    
[^43]: 基于天空图像的机器学习太阳辐照度预测

    Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning. (arXiv:2310.17356v1 [cs.CV])

    [http://arxiv.org/abs/2310.17356](http://arxiv.org/abs/2310.17356)

    本论文提出了一种基于天空图像的机器学习预测太阳辐照度的新方法，通过提取天空图像特征并利用机器学习技术来估计太阳辐照度。经过与现有算法的比较，我们的方法在性能上表现出竞争力，并且具有更低的计算复杂度。(机器翻译)

    

    提前预测发电厂的输出功率对于电网的稳定和确保不间断服务至关重要。然而，由于自然能源的混沌行为，预测可再生能源的难度较大。本文提出了一种从天空图像估计短期太阳辐照度的新方法。所提出的算法从天空图像中提取特征，并采用基于学习的技术来估计太阳辐照度。通过使用两个公开可用的天空图像数据集来评估所提出的机器学习(ML)算法的性能。这些数据集包含16年的超过350,000个图像，从2004年到2020年，每个图像的对应全球水平辐照度(GHI)作为基准。与文献中提出的计算复杂度较高的最先进算法相比，我们的方法在现时预测和预测方面实现了具有竞争力的结果，但计算复杂度更低。

    Ahead-of-time forecasting of the output power of power plants is essential for the stability of the electricity grid and ensuring uninterrupted service. However, forecasting renewable energy sources is difficult due to the chaotic behavior of natural energy sources. This paper presents a new approach to estimate short-term solar irradiance from sky images. The~proposed algorithm extracts features from sky images and use learning-based techniques to estimate the solar irradiance. The~performance of proposed machine learning (ML) algorithm is evaluated using two publicly available datasets of sky images. The~datasets contain over 350,000 images for an interval of 16 years, from 2004 to 2020, with the corresponding global horizontal irradiance (GHI) of each image as the ground truth. Compared to the state-of-the-art computationally heavy algorithms proposed in the literature, our approach achieves competitive results with much less computational complexity for both nowcasting and forecast
    
[^44]: 探索规则前缀树：一种用于关联规则表示的高效数据结构

    Exploring the Trie of Rules: a fast data structure for the representation of association rules. (arXiv:2310.17355v1 [cs.LG])

    [http://arxiv.org/abs/2310.17355](http://arxiv.org/abs/2310.17355)

    本文提出了一种新的数据结构，称为规则前缀树，用于存储由关联规则挖掘生成的规则集。这个数据结构能够高效地表示和提取有意义的挖掘知识。

    

    关联规则挖掘技术在事务性数据库上的实现可以生成大量的顺序数据。从大量关联规则中提取洞察力被发现是一个具有挑战性的过程。当检查一组规则时，最基本的问题是如何高效地汇总和表示有意义的挖掘知识。许多算法和策略已经被开发出来来解决知识提取的问题；然而，这个过程的有效性可能受到数据结构的限制。更好的数据结构可以有效地影响知识提取过程的速度。本文提出了一种新的数据结构，称为规则前缀树，用于存储由关联规则挖掘生成的规则集。结果数据结构是一个由预先挖掘的规则组成的前缀树图结构。这个图将规则以路径的方式存储在前缀树中，类似的规则会互相覆盖。树中的每个节点表示一条规则。

    Association rule mining techniques can generate a large volume of sequential data when implemented on transactional databases. Extracting insights from a large set of association rules has been found to be a challenging process. When examining a ruleset, the fundamental question is how to summarise and represent meaningful mined knowledge efficiently. Many algorithms and strategies have been developed to address issue of knowledge extraction; however, the effectiveness of this process can be limited by the data structures. A better data structure can sufficiently affect the speed of the knowledge extraction process. This paper proposes a novel data structure, called the Trie of rules, for storing a ruleset that is generated by association rule mining. The resulting data structure is a prefix-tree graph structure made of pre-mined rules. This graph stores the rules as paths within the prefix-tree in a way that similar rules overlay each other. Each node in the tree represents a rule whe
    
[^45]: 通过暂时卷积神经网络实现的全新化学反应生成

    De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])

    [http://arxiv.org/abs/2310.17341](http://arxiv.org/abs/2310.17341)

    本研究结合递归神经网络（RNN）和临时卷积神经网络（TCN），使用新的反应Smiles-like表示实现了全新的化学反应生成，并通过迁移学习发现微调协议对模型生成范围有重要影响。

    

    我们在使用新颖的反应Smiles-like表示（CGRSmiles）时，将递归神经网络（RNN）和临时卷积神经网络（TCN）相结合，以实现全新的反应生成，并直接融合了原子映射。递归神经网络以其自回归特性而闻名，并经常在语言建模中使用，直接应用于SMILES生成。相对较新的TCN具有类似的性质，具有广泛的感受野，并遵守自然语言处理（NLP）所需的因果性。通过TCN和RNN表达的两种潜在表示的组合相比仅使用RNN时具有更好的性能。此外，研究还表明，通过迁移学习将不同的微调协议应用于感兴趣的数据集时，对模型的生成范围有深远影响。

    We present here a combination of two networks, Recurrent Neural Networks (RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction generation using the novel Reaction Smiles-like representation of reactions (CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks are known for their autoregressive properties and are frequently used in language modelling with direct application to SMILES generation. The relatively novel TCNs possess similar properties with wide receptive field while obeying the causality required for natural language processing (NLP). The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone. Additionally, it is shown that different fine-tuning protocols have a profound impact on generative scope of the model when applied on a dataset of interest via transfer learning.
    
[^46]: 基于频率的深度学习进行多种信号伪迹的脑电图去噪

    A multi-artifact EEG denoising by frequency-based deep learning. (arXiv:2310.17335v1 [cs.LG])

    [http://arxiv.org/abs/2310.17335](http://arxiv.org/abs/2310.17335)

    本论文提出了一种基于频域的新型脑电图去噪模型，通过利用噪声频谱特征的先验知识，自适应地计算噪声分离的最优滤波器。经过实验证明，该模型在脑电图去噪方面取得了最优结果。

    

    脑电图（EEG）信号在神经科学研究和脑机接口、神经疾病诊断等临床应用中起着基础作用。这些信号通常是神经活动和噪声的组合，噪声来自多种来源，包括眼动和肌肉运动等生理伪迹。在这种情况下，我们面临的挑战是区分神经活动和与噪声相关的来源。我们开发了一种新颖的脑电图去噪模型，该模型在频域内工作，利用关于噪声频谱特征的先验知识，自适应地计算噪声分离的最优卷积滤波器。该模型经过训练，学习了将噪声和噪声信号的谱特征与非线性转换相连的经验关系，从而实现信号去噪。在EEGdenoiseNet数据集上的性能评估表明，所提出的模型在时间和空间上均取得了最优结果。

    Electroencephalographic (EEG) signals are fundamental to neuroscience research and clinical applications such as brain-computer interfaces and neurological disorder diagnosis. These signals are typically a combination of neurological activity and noise, originating from various sources, including physiological artifacts like ocular and muscular movements. Under this setting, we tackle the challenge of distinguishing neurological activity from noise-related sources. We develop a novel EEG denoising model that operates in the frequency domain, leveraging prior knowledge about noise spectral features to adaptively compute optimal convolutional filters for noise separation. The model is trained to learn an empirical relationship connecting the spectral characteristics of noise and noisy signal to a non-linear transformation which allows signal denoising. Performance evaluation on the EEGdenoiseNet dataset shows that the proposed model achieves optimal results according to both temporal and
    
[^47]: 关于预测稳定性

    On Forecast Stability. (arXiv:2310.17332v1 [cs.LG])

    [http://arxiv.org/abs/2310.17332](http://arxiv.org/abs/2310.17332)

    本文研究了预测稳定性的两种类型：垂直稳定性和水平稳定性，并提出了一种适用于任何基础模型的简单线性插值方法来实现这种稳定性。这种方法可以产生准确而稳定的预测。

    

    预测通常不是在真空中产生的，而是在商业环境中生成的，预测是定期生成的，并且彼此之间互相影响。对于决策来说，预测不会任意变化，并且在某种程度上是稳定的可能很重要。然而，在预测文献中，这个领域只受到了有限的关注。在本文中，我们探索了两种我们称之为垂直稳定性和水平稳定性的预测稳定性类型。现有的文献工作只适用于某些基础模型，将这些框架扩展成与任何基础模型兼容并不容易。此外，这些框架只能使预测垂直稳定。为了填补这个空白，我们提出了一种基于简单线性插值的方法，适用于垂直和水平稳定化任何基础模型的预测。该方法可以产生准确而稳定的预测。

    Forecasts are typically not produced in a vacuum but in a business context, where forecasts are generated on a regular basis and interact with each other. For decisions, it may be important that forecasts do not change arbitrarily, and are stable in some sense. However, this area has received only limited attention in the forecasting literature. In this paper, we explore two types of forecast stability that we call vertical stability and horizontal stability. The existing works in the literature are only applicable to certain base models and extending these frameworks to be compatible with any base model is not straightforward. Furthermore, these frameworks can only stabilise the forecasts vertically. To fill this gap, we propose a simple linear-interpolation-based approach that is applicable to stabilise the forecasts provided by any base model vertically and horizontally. The approach can produce both accurate and stable forecasts. Using N-BEATS, Pooled Regression and LightGBM as the
    
[^48]: CQM：具有量化的世界模型的课程增强学习

    CQM: Curriculum Reinforcement Learning with a Quantized World Model. (arXiv:2310.17330v1 [cs.LG])

    [http://arxiv.org/abs/2310.17330](http://arxiv.org/abs/2310.17330)

    CQM提出了一种新的课程增强学习方法，通过自动定义语义目标空间和提出课程目标，在解决复杂任务中取得了显著进展。该方法通过向量量化-变分自动编码器(VQ-VAE)将连续观察结果进行离散化，并通过图形恢复离散观察结果之间的时间关系，提供了不确定性和时间距离感知的课程目标。

    

    最近的课程增强学习在解决复杂任务方面取得了显著进展，通过提出一系列代理任务的顺序来实现。然而，先前的方法在生成高维空间中的课程目标时经常面临挑战。因此，它们通常依赖于手动指定的目标空间。为了减轻这个限制并提高课程的可扩展性，我们提出了一种新的课程方法，它自动定义包含课程过程的关键信息的语义目标空间，并在其上提出课程目标。为了定义语义目标空间，我们的方法通过向量量化-变分自动编码器(VQ-VAE)将连续观察结果进行离散化，并通过图形恢复离散观察结果之间的时间关系。与此同时，我们的方法提出了不确定性和时间距离感知的课程目标，这些目标在自动组合的目标空间中收敛到最终目标。我们证明了所提出的方法

    Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the propose
    
[^49]: C-Disentanglement: 带有混淆因子归纳偏差的因果独立生成因子的发现

    C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])

    [http://arxiv.org/abs/2310.17325](http://arxiv.org/abs/2310.17325)

    本文提出了一个名为C-Disentanglement的框架，旨在发现具有因果关系且受混淆因素影响的生成因子，以提高数据生成的可控性和鲁棒性。

    

    表征学习假设实际世界的数据是由几个语义上有意义的生成因子（即变异源）产生的，并旨在在潜在空间中发现它们。这些因子被期望是因果上解缠的，意味着不同的因子被编码为单独的潜在变量，并且一个因子的变化不会影响其他因子的值。与统计独立性相比，因果解缠允许更可控的数据生成，提高了鲁棒性和更好的泛化性能。然而，大多数现有工作在发现过程中假设没有混淆因素，即生成因子没有共同的原因，因此只获得统计独立性。在本文中，我们认识到在发现因果生成因子中建立混淆因素的模型的重要性。然而，如果没有适当的归纳偏差，这些因素是无法识别的。我们通过引入一个名为混淆-解缠（C-Di）的框架来填补这一空白。

    Representation learning assumes that real-world data is generated by a few semantically meaningful generative factors (i.e., sources of variation) and aims to discover them in the latent space. These factors are expected to be causally disentangled, meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. Compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. However, most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Di
    
[^50]: 通过演示-正则化强化学习增强采样效率

    Demonstration-Regularized RL. (arXiv:2310.17303v1 [stat.ML])

    [http://arxiv.org/abs/2310.17303](http://arxiv.org/abs/2310.17303)

    通过演示-正则化提高强化学习的采样效率，并找到最优策略的样本复杂度，该复杂度与专家演示数量成反比。

    

    通过将专家演示纳入其中，可以在提高强化学习(SRL)的采样效率方面产生经验效果。本文在理论上量化这些额外信息降低了SRL的采样复杂性的程度。具体而言，我们研究了通过KL正则化利用专家演示学习的策略的演示-正则化强化学习。我们的研究发现，在有限状态下，在$\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$的样本复杂度内，使用$N^{\mathrm{E}}$个专家演示能够找到最优策略，并在线性马尔科夫决策过程中，在$\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$的样本复杂度内找到最优策略，其中$\varepsilon$是目标精度，$H$是规定，$A$是动作的数量，$S$是有限状态的数量，在线性情况下，$d$是特征空间的维数。

    Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in finite and $\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight con
    
[^51]: BEVContrast: 汽车激光雷达点云BEV空间的自我监督方法

    BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds. (arXiv:2310.17281v1 [cs.CV])

    [http://arxiv.org/abs/2310.17281](http://arxiv.org/abs/2310.17281)

    BEVContrast是一种在汽车激光雷达点云中使用BEV空间自我监督的方法，通过对比不同场景下的特征，提供了一种在计算成本和性能之间取得平衡的解决方案。

    

    我们提出了一种自我监督汽车激光雷达点云3D主干的简单高效的方法。我们设计了一种对相同场景中捕获的激光雷达扫描特征进行对比的损失函数。我们将对比方法从点级别（PointContrast）到分段级别（TARL）进行了比较，并在Bird's Eye View平面的2D单元级别定义了我们的对比。所得到的单元级表示在计算成本上既保持了PointContrast的简单性（单元表示计算成本低），同时也超越了TARL的性能。

    We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast, which uses a contrast at the level of points, to the state-of-the-art TARL, which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird's Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in do
    
[^52]: 将循环引入人类：协作和可解释的贝叶斯优化

    Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])

    [http://arxiv.org/abs/2310.17273](http://arxiv.org/abs/2310.17273)

    协作和可解释的贝叶斯优化框架(CoExBO)在贝叶斯优化中引入了循环，平衡了人工智能和人类的合作关系。它利用偏好学习将用户见解融合到优化中，解释每次迭代的候选选择，从而增强用户对优化过程的信任，并提供无害保证。

    

    像许多优化器一样，贝叶斯优化在获得用户信任方面常常存在不足，因为其不透明性。虽然已经尝试开发面向人类的优化器，但它们通常假设用户知识是明确且无误的，并主要将用户作为优化过程的监督者。我们放宽了这些假设，提出了一种更平衡的人工智能和人类合作伙伴关系，即我们的协作和可解释的贝叶斯优化（CoExBO）框架。CoExBO使用偏好学习来无缝地将人类见解整合到优化中，从而产生与用户使用偏好一致的算法建议。CoExBO解释其每次迭代的候选选择，以培养信任，使用户更清楚地掌握优化的过程。此外，CoExBO提供无害保证，允许用户犯错误；即使在极端对抗性干扰下，算法也会渐进地收敛。

    Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
    
[^53]: 基于机器学习的软件缺陷预测模型的方差：我们真的在改进缺陷预测吗？

    Variance of ML-based software fault predictors: are we really improving fault prediction?. (arXiv:2310.17264v1 [cs.SE])

    [http://arxiv.org/abs/2310.17264](http://arxiv.org/abs/2310.17264)

    本论文研究了基于机器学习的软件缺陷预测模型中的方差问题，并指出这种方差对于研究的可重复性和模型在实际应用中的性能有重要影响。

    

    随着软件系统越来越复杂并持续增长，软件质量保证活动变得越来越困难。此外，对大规模系统进行测试的成本也更高。为了有效分配质量保证资源，研究人员提出了利用机器学习来预测有缺陷代码区域的缺陷预测（FP）。然而，机器学习算法通常利用随机元素来增加预测模型的泛化能力和训练过程的效率。这些随机元素，也称为引入非确定性因素（NI）的因素，导致训练过程中的方差，从而导致预测准确性和训练时间的方差。这种方差对于研究的可重复性构成了挑战。更重要的是，在实验室中，虽然缺陷预测模型可能表现良好（例如，通常涉及多次运行和平均结果），但高方差会限制其应用范围。

    Software quality assurance activities become increasingly difficult as software systems become more and more complex and continuously grow in size. Moreover, testing becomes even more expensive when dealing with large-scale systems. Thus, to effectively allocate quality assurance resources, researchers have proposed fault prediction (FP) which utilizes machine learning (ML) to predict fault-prone code areas. However, ML algorithms typically make use of stochastic elements to increase the prediction models' generalizability and efficiency of the training process. These stochastic elements, also known as nondeterminism-introducing (NI) factors, lead to variance in the training process and as a result, lead to variance in prediction accuracy and training time. This variance poses a challenge for reproducibility in research. More importantly, while fault prediction models may have shown good performance in the lab (e.g., often-times involving multiple runs and averaging outcomes), high var
    
[^54]: fairret：一种可微公平性正则化项的框架

    fairret: a Framework for Differentiable Fairness Regularization Terms. (arXiv:2310.17256v1 [cs.LG])

    [http://arxiv.org/abs/2310.17256](http://arxiv.org/abs/2310.17256)

    本论文介绍了一种称为fairret的可微公平性正则化项框架，通过模块化的目标量化偏见，并可以轻松集成到自动微分流程中。通过从线性分式统计角度定义公平性，可以高效计算多种类型的公平性正则化项。实验证明，fairret框架与基准相比在强制执行公平性时几乎不损失预测能力。

    

    目前的机器学习公平性工具仅接受有限范围的公平性定义，并且与自动微分库的整合较少，尽管这些库在现代机器学习流程中起着核心作用。我们引入了一种公平性正则化项（fairret）的框架，以模块化目标的形式量化偏见，并且可以轻松地集成到自动微分流程中。通过采用线性分式统计的广义公平性定义，可以高效地计算出一类广泛的fairret。实验显示了它们的梯度行为以及与基准相比将公平性强制执行的实用性而最小化预测能力损失。我们的贡献包括fairret框架的PyTorch实现。

    Current tools for machine learning fairness only admit a limited range of fairness definitions and have seen little integration with automatic differentiation libraries, despite the central role these libraries play in modern machine learning pipelines.  We introduce a framework of fairness regularization terms (fairrets) which quantify bias as modular objectives that are easily integrated in automatic differentiation pipelines. By employing a general definition of fairness in terms of linear-fractional statistics, a wide class of fairrets can be computed efficiently. Experiments show the behavior of their gradients and their utility in enforcing fairness with minimal loss of predictive power compared to baselines. Our contribution includes a PyTorch implementation of the fairret framework.
    
[^55]: IDENAS: 内部依赖性探索用于神经架构搜索

    IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])

    [http://arxiv.org/abs/2310.17250](http://arxiv.org/abs/2310.17250)

    IDENAS是一种集成神经架构搜索和特征选择的方法，通过探索内部依赖性来提高分类任务的性能。

    

    机器学习是从不同数据集中提取有价值信息和进行各种预测的强大工具。传统算法依赖于明确定义的输入和输出变量，然而，在某些情况下，输入和输出变量之间的区别以及模型的底层关联（输入和输出）层是未知的。神经架构搜索（NAS）和特征选择已成为这些场景中的有希望的解决方案。该研究提出了IDENAS，一种基于内部依赖性的神经架构搜索方法，将NAS与特征选择相结合。该方法在涉及1D传感器和2D图像数据的分类问题中探索了完整的参数空间的内部依赖性。IDENAS采用了修改的编码器-解码器模型和顺序前向搜索（SFS）算法，将输入-输出配置搜索与嵌入式特征选择相结合。实验结果证明了IDENAS的优越性能。

    Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior perf
    
[^56]: 超越神经网络：模型复杂性的经验探索

    Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity. (arXiv:2310.17247v1 [cs.LG])

    [http://arxiv.org/abs/2310.17247](http://arxiv.org/abs/2310.17247)

    本文发现神经网络中的grokking现象不仅局限于神经网络，还出现在其他算法和模型中。通过在数据集中添加虚假信息的维度，可以诱发grokking现象。研究表明，grokking现象在解决方案搜索受复杂性和错误指导的任何情况下可能发生。这对理解grokking现象提供了更广泛的理论支持。

    

    在某些情况下，神经网络展现出一种称为“grokking”的现象，即它们在验证集上实现完美或接近完美的准确度，而在训练集上则早已达到相同的性能。本文发现，grokking不仅限于神经网络，还出现在其他设置中，例如高斯过程（GP）分类、GP回归和线性回归。我们还发现了一种通过添加包含虚假信息的维度来诱发基于算法的数据集中的grokking现象的机制。非神经结构中的这种现象的存在证明了grokking不局限于SGD或权重范数正则化。相反，grokking可能发生在任何由复杂性和错误指导解决方案搜索的情况中。基于这一洞察和我们在贝叶斯神经网络（BNN）和GP回归模型的训练轨迹中观察到的进一步趋势，我们在grokking的更一般的理论方面取得了进展。

    In some settings neural networks exhibit a phenomenon known as grokking, where they achieve perfect or near-perfect accuracy on the validation set long after the same performance has been achieved on the training set. In this paper, we discover that grokking is not limited to neural networks but occurs in other settings such as Gaussian process (GP) classification, GP regression and linear regression. We also uncover a mechanism by which to induce grokking on algorithmic datasets via the addition of dimensions containing spurious information. The presence of the phenomenon in non-neural architectures provides evidence that grokking is not specific to SGD or weight norm regularisation. Instead, grokking may be possible in any setting where solution search is guided by complexity and error. Based on this insight and further trends we see in the training trajectories of a Bayesian neural network (BNN) and GP regression model, we make progress towards a more general theory of grokking. Spe
    
[^57]: CROP: 保守奖励用于基于模型的离线策略优化

    CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])

    [http://arxiv.org/abs/2310.17245](http://arxiv.org/abs/2310.17245)

    CROP提出了一种保守奖励的模型训练方法用于基于模型的离线策略优化，通过同时最小化估计误差和随机动作奖励来实现保守的奖励估计。

    

    离线强化学习旨在使用收集到的数据进行策略优化，而无需进行在线交互。基于模型的方法在解决离线强化学习挑战方面特别有吸引力，因为它们能够通过使用模型生成数据来缓解离线数据的限制。之前的研究表明，在策略优化过程中将保守性引入模型或Q函数可以有效缓解离线强化学习中普遍存在的分布漂移问题。然而，关于奖励估计中保守性的影响的研究仍然不足。本文提出了一种新颖的基于模型的离线强化学习算法CROP，该算法在模型训练中保守地估计奖励。为了实现保守的奖励估计，CROP同时最小化估计误差和随机动作的奖励。理论分析表明，这种保守的奖励机制导致...（文章摘要未完，下同）

    Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads 
    
[^58]: 通过span剪枝和超图神经网络实现联合实体和关系抽取

    Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])

    [http://arxiv.org/abs/2310.17238](http://arxiv.org/abs/2310.17238)

    本文提出了基于超图神经网络的联合实体和关系抽取方法，使用span剪枝机制减轻误差传播问题，通过构建超图进行高阶建模，实现多个实体和关系之间的交互。

    

    实体和关系抽取（ERE）是信息提取中的重要任务，最近基于标记的流水线模型取得了最先进的性能，但仍然存在误差传播问题。此外，大多数当前的ERE模型在多个实体和关系之间不考虑高阶交互，而高阶建模可能会有益处。在这项工作中，我们提出了超图神经网络（HGNN）用于ERE，它是建立在PL-marker（最先进的基于标记的流水线模型）之上的。为了减轻误差传播，我们使用高召回剪枝器机制将实体的识别和标注负担从NER模块转移到我们模型的联合模块。对于高阶建模，我们构建了一个超图，其中节点是实体（由span剪枝器提供），以及其关系，并且超边编码了两个不同关系之间或关系与其相关的主体和宾语实体之间的交互。接着我们运行...

    Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run
    
[^59]: Codebook特征：神经网络的稀疏和离散可解释性

    Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])

    [http://arxiv.org/abs/2310.17230](http://arxiv.org/abs/2310.17230)

    本研究提出了一种称为codebook特征的方法，通过将神经网络的连续特征量化为离散向量码的总和来实现稀疏和离散的隐藏状态。实验证明，神经网络在这种极端瓶颈条件下运行时性能下降适度，同时这种方法还提供了一种直观的神经网络行为控制方式。

    

    理解神经网络是具有挑战性的，部分原因是由于它们的隐藏状态是密集和连续的。我们探讨了是否可以通过将连续特征量化为我们称之为codebook特征来训练神经网络，使其具有稀疏、离散且更易解释的隐藏状态。通过在每层引入向量量化瓶颈来微调神经网络，产生的codebook特征由从更大的codebook中选择的少量离散向量码的总和组成。令人惊讶的是，我们发现神经网络可以在这种极端瓶颈下运行，性能只有适度的下降。这种稀疏、离散的瓶颈还提供了一种直观的控制神经网络行为的方法：首先，找到在所需行为出现时激活的码，然后在生成过程中激活相同的码以引发该行为。我们通过训练codebook Transformers验证了我们的方法。

    Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on sever
    
[^60]: 超越MLE: 用于文本生成的凸学习方法

    Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])

    [http://arxiv.org/abs/2310.17217](http://arxiv.org/abs/2310.17217)

    本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。

    

    最大似然估计（MLE）是一种统计方法，用于估计最能解释观测数据的概率分布参数。在文本生成的背景下，MLE经常用于训练生成型语言模型，从而生成新的文本。然而，我们认为MLE并不总是必要且最优的，尤其是对于闭合型文本生成任务，如机器翻译。在这些任务中，模型的目标是生成最合适的响应，这并不一定需要使用MLE来估计整个数据分布。为此，我们提出了一种基于凸函数的新型训练目标类，使得文本生成模型能够集中于高概率的输出，而不需要估计整个数据分布。我们研究了在将凸函数应用于损失函数时的最优预测分布的理论性质，证明了凸函数可以使得最优预测分布变得更加锐利。

    Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt
    
[^61]: 弱监督手术阶段识别

    Weakly-Supervised Surgical Phase Recognition. (arXiv:2310.17209v1 [cs.CV])

    [http://arxiv.org/abs/2310.17209](http://arxiv.org/abs/2310.17209)

    本文提出了一种弱监督手术阶段识别的方法，在每帧的阶段预测中结合了图分割和自监督学习，使用了稀疏时间戳或少样本学习进行弱监督。该方法具有低复杂度，并在实验中展现了令人期待的性能。

    

    计算机辅助手术系统的关键要素是手术视频的阶段识别。现有的阶段识别算法需要对大量视频进行逐帧注释，这需要耗费时间和金钱。在这项工作中，我们将图分割的概念与自监督学习相结合，提出了一种随机游走解决方案，用于每帧的阶段预测。此外，我们的方法利用了两种形式的弱监督：稀疏时间戳或少样本学习。该算法具有低复杂度，并且可以在低数据环境下运行。我们通过在公开的Cholec80腹腔镜胆囊切除视频数据集上进行实验验证了我们的方法，在多种设置下展现了令人期待的性能。

    A key element of computer-assisted surgery systems is phase recognition of surgical videos. Existing phase recognition algorithms require frame-wise annotation of a large number of videos, which is time and money consuming. In this work we join concepts of graph segmentation with self-supervised learning to derive a random-walk solution for per-frame phase prediction. Furthermore, we utilize within our method two forms of weak supervision: sparse timestamps or few-shot learning. The proposed algorithm enjoys low complexity and can operate in lowdata regimes. We validate our method by running experiments with the public Cholec80 dataset of laparoscopic cholecystectomy videos, demonstrating promising performance in multiple setups.
    
[^62]: miditok：一个用于MIDI文件分词的Python包

    miditok: A Python package for MIDI file tokenization. (arXiv:2310.17202v1 [cs.LG])

    [http://arxiv.org/abs/2310.17202](http://arxiv.org/abs/2310.17202)

    MidiTok是一个用于将符号音乐进行分词的Python包，具有灵活性和扩展性，支持最流行的音乐分词方式，并提供了统一的API。

    

    最近自然语言处理的进展已经被适应到了符号音乐领域。语言模型，如Transformer，已经被用于符号音乐的各种任务，包括音乐生成、建模或转录，具有最先进的性能。这些模型开始被应用于实际产品中。为了对主干模型进行音乐的编码和解码，它们需要依靠分词器，分词器的作用是将音乐序列化为一系列不同元素（称为标记）的序列。MidiTok是一个开源库，可以以极高的灵活性和扩展性对符号音乐进行分词。它包含了最流行的音乐分词方式，并提供了统一的API。它旨在为每个人提供易于使用和扩展的工具。

    Recent progress in natural language processing has been adapted to the symbolic music modality. Language models, such as Transformers, have been used with symbolic music for a variety of tasks among which music generation, modeling or transcription, with state-of-the-art performances. These models are beginning to be used in production products. To encode and decode music for the backbone model, they need to rely on tokenizers, whose role is to serialize music into sequences of distinct elements called tokens. MidiTok is an open-source library allowing to tokenize symbolic music with great flexibility and extended features. It features the most popular music tokenizations, under a unified API. It is made to be easily used and extensible for everyone.
    
[^63]: 用网络控制变量驯服联邦学习中的梯度方差

    Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])

    [http://arxiv.org/abs/2310.17200](http://arxiv.org/abs/2310.17200)

    本论文提出了一种名为FedNCV的新型框架，用于解决联邦学习中梯度方差的问题。通过在客户端和服务器级别实现REINFORCE Leave-One-Out (RLOO)作为控制变量单元，优化了本地梯度更新并提供了无偏和低方差的聚合梯度，实现了稳健的全局更新。

    

    联邦学习是一种分散式的机器学习方法，面临着诸如沟通开销大、收敛速度慢和改进不稳定等重要挑战。这些挑战主要源于由异构客户端数据分布引起的梯度方差。为了解决这个问题，我们引入了一种新颖的网络控制变量（FedNCV）框架来进行联邦学习。我们在FedNCV框架的客户端和服务器级别都采用REINFORCE Leave-One-Out（RLOO）作为基本控制变量单元。在客户端级别，RLOO控制变量用于优化本地梯度更新，减轻数据样本引入的方差。一旦传输到服务器，基于RLOO的估计量进一步提供了无偏和低方差的聚合梯度，实现了稳健的全局更新。这种双侧应用被形式化为组合控制变量的线性组合。我们提供了一个数学公式来捕捉这个过程。

    Federated learning, a decentralized approach to machine learning, faces significant challenges such as extensive communication overheads, slow convergence, and unstable improvements. These challenges primarily stem from the gradient variance due to heterogeneous client data distributions. To address this, we introduce a novel Networked Control Variates (FedNCV) framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO) as a fundamental control variate unit in the FedNCV framework, implemented at both client and server levels. At the client level, the RLOO control variate is employed to optimize local gradient updates, mitigating the variance introduced by data samples. Once relayed to the server, the RLOO-based estimator further provides an unbiased and low-variance aggregated gradient, leading to robust global updates. This dual-side application is formalized as a linear combination of composite control variates. We provide a mathematical expression capturing this i
    
[^64]: 语言模型如何将实体绑定到上下文中?

    How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])

    [http://arxiv.org/abs/2310.17191](http://arxiv.org/abs/2310.17191)

    通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。

    

    为了正确使用上下文信息，语言模型（LMs）必须将实体与其属性进行绑定。例如，给定描述“绿色方块”和“蓝色圆形”的上下文，LMs必须将形状与它们对应的颜色进行绑定。我们分析LM表示并确定绑定ID机制：这是一种解决绑定问题的通用机制，我们在Pythia和LLaMA家族的每个足够大的模型中观察到。通过因果干预，我们展示了LMs内部激活通过将绑定ID向量附加到相应的实体和属性上来表示绑定信息。我们进一步展示了绑定ID向量形成连续的子空间，在这个子空间中，绑定ID向量之间的距离反映了它们的区别。总体而言，我们的结果揭示了LMs在上下文中表示符号知识的可解释策略，为理解大规模LMs中的一般上下文推理迈出了一步。

    To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
    
[^65]: 自适应重要采样方法在Deep Ritz解法中的应用

    Adaptive important sampling for Deep Ritz. (arXiv:2310.17185v1 [cs.LG])

    [http://arxiv.org/abs/2310.17185](http://arxiv.org/abs/2310.17185)

    该论文介绍了一种自适应重要采样方法，用于Deep Ritz方法求解偏微分方程。该方法利用两个深度神经网络，一个用于逼近解，另一个用于生成新的插值点以改进训练集。这种自适应采样方法通过最小化与插值点相关的变分损失来求解PDE，并使用深度生成模型来近似概率密度函数。

    

    我们引入了一种适应性采样方法，用于解决偏微分方程的Deep Ritz方法。我们使用两个深度神经网络。一个网络用于逼近偏微分方程的解，而另一个网络是一个深度生成模型，用于生成新的插值点以改进训练集。适应性采样过程包括两个主要步骤。第一步是使用Deep Ritz方法通过最小化与训练集中的插值点离散化相关的变分损失来求解偏微分方程。第二步涉及生成一个新的训练集，然后在后续计算中使用该训练集进一步提高当前近似解的准确性。我们将变分损失中的被积函数视为一个非标准化的概率密度函数（PDF），并使用称为有界KRnet的深度生成模型进行近似。新的样本及其相关的PDF值是从有界KRnet中获取的。

    We introduce an adaptive sampling method for the Deep Ritz method aimed at solving partial differential equations (PDEs). Two deep neural networks are used. One network is employed to approximate the solution of PDEs, while the other one is a deep generative model used to generate new collocation points to refine the training set. The adaptive sampling procedure consists of two main steps. The first step is solving the PDEs using the Deep Ritz method by minimizing an associated variational loss discretized by the collocation points in the training set. The second step involves generating a new training set, which is then used in subsequent computations to further improve the accuracy of the current approximate solution. We treat the integrand in the variational loss as an unnormalized probability density function (PDF) and approximate it using a deep generative model called bounded KRnet. The new samples and their associated PDF values are obtained from the bounded KRnet. With these ne
    
[^66]: 图形化的以对象为中心的Actor-Critic算法

    Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])

    [http://arxiv.org/abs/2310.17178](http://arxiv.org/abs/2310.17178)

    这项研究提出了一种新颖的以对象为中心的强化学习算法，将演员-评论家和基于模型的方法结合起来，利用解耦的对象表示有效地学习策略。该方法填补了以对象为中心的强化学习环境中高效且适用于离散或连续动作空间的世界模型的研究空白。

    

    最近在无监督的以对象为中心的表示学习及其在下游任务中的应用方面取得了重要进展。最新的研究支持这样一个观点，即在基于图像的以对象为中心的强化学习任务中采用解耦的对象表示能够促进策略学习。我们提出了一种新颖的以对象为中心的强化学习算法，将演员-评论家算法和基于模型的方法结合起来，以有效利用这些表示。在我们的方法中，我们使用一个变换器编码器来提取对象表示，并使用图神经网络来近似环境的动力学。所提出的方法填补了开发强化学习环境中可以用于离散或连续动作空间的高效以对象为中心的世界模型的研究空白。我们的算法在一个具有复杂视觉3D机器人环境和一个具有组合结构的2D环境中表现更好。

    There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-t
    
[^67]: 从全景X射线中进行牙齿分割和定位的深度学习方法

    A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])

    [http://arxiv.org/abs/2310.17176](http://arxiv.org/abs/2310.17176)

    本研究提出了一个利用深度学习技术从全景X射线图像中进行牙齿分割和定位的方法。我们通过修改已有模型并引入注意力机制，实现了高精度和高性能的牙齿分割和定位。在公开数据集上的评估结果表明，我们的方法在牙齿实例分割和牙齿定位方面取得了优异的性能。

    

    准确的牙齿分割和定位在现代口腔保健中是基础，可实现精确诊断、治疗计划和牙齿种植设计。本研究提出了一种综合的方法，利用深度学习技术从全景X射线图像中进行牙齿分割和定位。我们根据FUSegNet构建了我们的模型，这是一种最初用于创面分割的流行模型，并通过将基于网格的注意力门引入跳跃连接进行了修改。我们通过主成分分析（PCA）引入定向边界框（OBB）生成，以实现精确的牙齿定位估计。在公开可获得的DNS数据集上评估我们的方法，该数据集包括543个全景X射线图像，我们在牙齿实例分割中得到了最高的交并比（IoU）得分82.43%，Dice相似系数（DSC）得分90.37%，在OBB分析中，我们获得了旋转的交并比（RIoU）得分82.82%。

    Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
    
[^68]: DSAC-C: 约束最大熵用于鲁棒性离散化软-演员-评论家算法

    DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic. (arXiv:2310.17173v1 [cs.LG])

    [http://arxiv.org/abs/2310.17173](http://arxiv.org/abs/2310.17173)

    DSAC-C是一种约束最大熵的鲁棒离散软-演员-评论家算法，通过额外的统计约束提供了对潜在领域变化的鲁棒性和在现实世界中安全部署的能力。

    

    我们提出了一种新的Soft Actor-Critic (SAC)算法的扩展。我们认为基于最大熵原理，可以通过从替代评论家策略中得到的额外统计约束来进一步改进离散的SAC。此外，我们的研究结果表明，这些约束对于在现实世界中安全部署强化学习代理是至关重要的，并提供了对低数据情况下的Atari 2600游戏的分布内和分布外变体的理论分析和实证结果。

    We present a novel extension to the family of Soft Actor-Critic (SAC) algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC can be further improved via additional statistical constraints derived from a surrogate critic policy. Furthermore, our findings suggests that these constraints provide an added robustness against potential domain shifts, which are essential for safe deployment of reinforcement learning agents in the real-world. We provide theoretical analysis and show empirical results on low data regimes for both in-distribution and out-of-distribution variants of Atari 2600 games.
    
[^69]: 学习处理具有一般库存到货动态的库存控制策略

    Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])

    [http://arxiv.org/abs/2310.17168](http://arxiv.org/abs/2310.17168)

    本文解决了学习具有一般库存到货动态下的库存控制策略的问题，同时允许修改订购数量以满足供应商的限制，并将周期性审核库存控制问题定义为外部决策过程。

    

    本文在面对一般到货动态的情况下解决了学习和回测库存控制策略的问题，我们将其称为数量随时间到货模型（QOT）。在实际供应链中，我们还允许修改订购数量以满足供应商的限制，例如订购最低数量和批次大小约束。据我们所知，这是第一篇处理任意到货动态或任意后续处理的订购数量的研究。在最近的工作（Madeka等，2022）的基础上，我们同样将周期性审核库存控制问题定义为外部决策过程，其中大部分状态不受代理的控制。Madeka等人（2022）展示了如何构建一个模拟器来回放历史数据以解决这类问题。在我们的例子中，我们将一个深度生成模型纳入到货过程的历史回放中。

    In this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (QOT). We also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. To the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. Building upon recent work (Madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. Madeka et al. (2022) show how to construct a simulator that replays historic data to solve this class of problem. In our case, we incorporate a deep generative model for the arrivals process as part of the history replay. By formulat
    
[^70]: 通过同时估计图像和噪声改进去噪扩散模型

    Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])

    [http://arxiv.org/abs/2310.17167](http://arxiv.org/abs/2310.17167)

    通过重新参数化扩散过程并直接估计图像和噪声，本文改进了去噪扩散模型，提高了图像生成的速度和质量。

    

    本文介绍了两个关键的贡献，旨在通过反向扩散过程生成的图像的速度和质量。第一个贡献是通过以图像和噪声之间的四分之一圆弧上的角度重新参数化扩散过程，特别是设置传统的 $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$。这种重新参数化消除了两个奇异点，并允许将扩散演化表达为一个良好行为的常微分方程（ODE）。从而，可以有效地使用更高阶的ODE求解器，如Runge-Kutta方法。第二个贡献是直接使用我们的网络估计图像（$\mathbf{x}_0$）和噪声（$\mathbf{\epsilon}$），这使得逆向扩散过程中的更新步骤计算更加稳定，因为在过程的不同阶段准确估计图像和噪声都是至关重要的。在这些变化的基础上，我们的模型实现了...

    This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achie
    
[^71]: 最大熵损失：针对超出分布转换的校准的约束最大熵方法

    MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift. (arXiv:2310.17159v1 [cs.LG])

    [http://arxiv.org/abs/2310.17159](http://arxiv.org/abs/2310.17159)

    本论文提出了一种新的损失函数，用于解决超出分布转换的校准问题。该方法基于最大熵原理，在训练过程中引入统计约束，以提供更好的模型校准效果，同时不牺牲准确性。实验证明该方法在合成和真实世界的基准上实现了最先进的校准效果。

    

    我们提出了一种解决超出分布转换校准问题的新的损失函数。虽然有很多目标函数被提出来有效地在分布内校准模型，我们的研究发现它们在超出分布的情况下表现并不好。基于最大熵原理，我们在训练过程中引入有用的统计约束，以在不牺牲准确性的情况下提供更好的模型校准。我们提供了理论分析并通过实验证明我们的方法在实践中表现良好，在合成和真实世界的基准上实现了最先进的校准效果。

    We present a new loss function that addresses the out-of-distribution (OOD) calibration problem. While many objective functions have been proposed to effectively calibrate models in-distribution, our findings show that they do not always fare well OOD. Based on the Principle of Maximum Entropy, we incorporate helpful statistical constraints observed during training, delivering better model calibration without sacrificing accuracy. We provide theoretical analysis and show empirically that our method works well in practice, achieving state-of-the-art calibration on both synthetic and real-world benchmarks.
    
[^72]: Deja Vu: 上下文稀疏性在LLMs推理中的高效应用

    Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. (arXiv:2310.17157v1 [cs.LG])

    [http://arxiv.org/abs/2310.17157](http://arxiv.org/abs/2310.17157)

    该论文介绍了一种利用上下文稀疏性来提高大型语言模型推理效率的系统Deja Vu，通过预测输入相关的注意力头和MLP参数集合，可以在不影响模型质量和上下文学习能力的前提下加速推理过程。

    

    拥有数百亿参数的大型语言模型(LLMs)引发了一波新的令人兴奋的人工智能应用。然而，它们在推理时的计算成本很高。稀疏性是降低这种成本的一种自然方法，但是现有的方法要么需要昂贵的重新训练，要么放弃了LLM的上下文学习能力，要么在现代硬件上无法提供墙上时间速度提升。我们假设上下文稀疏性，即对于给定输入而言，产生与稠密模型大致相同输出的小的、输入相关的注意力头和MLP参数集合，可以解决这些问题。我们展示了上下文稀疏性的存在，展示了它可以被准确预测，并且我们可以利用它在墙上时间上加速LLM的推理，而不会影响LLM的质量或上下文学习能力。基于这些见解，我们提出了Deja Vu，一个使用低成本算法根据每层的输入实时预测上下文稀疏性的系统。

    Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along 
    
[^73]: 层次半隐变分推断及其在扩散模型加速中的应用

    Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration. (arXiv:2310.17153v1 [cs.LG])

    [http://arxiv.org/abs/2310.17153](http://arxiv.org/abs/2310.17153)

    提出了一种层次半隐变分推断方法（HSIVI），通过引入辅助分布逐层逐步地匹配来训练条件层，实现了更具表达力的多层构造半隐分布。该方法还可以加速扩散模型的采样过程，并显著提高了SIVI的表达能力。

    

    半隐变分推断（SIVI）通过以层次方式定义具有表达力的半隐分布，扩展了解析变分族。然而，当前SIVI方法常用的单层架构在目标后验具有复杂结构时可能不足够。本文提出了层次半隐变分推断方法，称为HSIVI，它将SIVI推广到允许更具表达力的多层构造半隐分布。通过引入在简单基础分布和目标分布之间插值的辅助分布，可以逐层逐步地匹配这些辅助分布来训练条件层。此外，给定预训练的评分网络，HSIVI可以用于加速扩散模型的采样过程，实现评分匹配目标。实验证明，HSIVI显著提高了SIVI的表达能力。

    Semi-implicit variational inference (SIVI) has been introduced to expand the analytical variational families by defining expressive semi-implicit distributions in a hierarchical manner. However, the single-layer architecture commonly used in current SIVI methods can be insufficient when the target posterior has complicated structures. In this paper, we propose hierarchical semi-implicit variational inference, called HSIVI, which generalizes SIVI to allow more expressive multi-layer construction of semi-implicit distributions. By introducing auxiliary distributions that interpolate between a simple base distribution and the target distribution, the conditional layers can be trained by progressively matching these auxiliary distributions one layer after another. Moreover, given pre-trained score networks, HSIVI can be used to accelerate the sampling process of diffusion models with the score matching objective. We show that HSIVI significantly enhances the expressiveness of SIVI on sever
    
[^74]: 技术注解：将在低场MRI 0.55T膝关节MRI gesunden 控制者进行训练的3.0T深度学习分割模型直接应用

    Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])

    [http://arxiv.org/abs/2310.17152](http://arxiv.org/abs/2310.17152)

    本研究评估了在0.55T低场MRI中对健康控制者膝关节进行标记物定量的深度学习技术的可行性，并表明这些技术在分割软骨区域方面与3.0T几乎相当。

    

    在本研究中，我们的目的是评估应用深度学习技术来量化健康控制者0.55T膝关节生物标记物的可行性，并与3.0T进行比较。该研究定性和定量地评估了0.55T下标准的骨骼和软骨分割算法的性能，比较了分割性能的差异、改进空间的区域以及0.55T和3.0T之间的软骨厚度值。初步结果表明，现有的基于深度学习的图像分割技术能够在0.55T的膝关节MRI中实现可行的转译，尤其在分割软骨区域方面，模型在Likert排名上表现几乎等同于3.0T。因此，0.55T低场可持续和易于安装的MRI可以用于评估健康控制者的膝关节。

    In the current study, our purpose is to evaluate the feasibility of applying deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in healthy controls scanned at 0.55T and compared with 3.0T. The current study assesses the performance of standard in-practice bone, and cartilage segmentation algorithms at 0.55T, both qualitatively and quantitatively, in terms of comparing segmentation performance, areas of improvement, and compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial results demonstrate a usable to good technical feasibility of translating existing quantitative deep-learning-based image segmentation techniques, trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition environment. Especially in terms of segmenting cartilage compartments, the models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be utilized for evaluating 
    
[^75]: 可解释的时空图神经网络

    Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])

    [http://arxiv.org/abs/2310.17149](http://arxiv.org/abs/2310.17149)

    这篇论文提出了一种可解释的时空图神经网络（STExplainer）框架，通过增强时空图神经网络的可解释性，在城市资源分配和政策制定方面具有应用潜力。

    

    时空图神经网络在各种现实世界的城市应用中，包括智能交通和公共安全等，因其有效建模时空依赖性的能力而受到广泛关注。然而，时空图神经网络的黑盒性质限制了其可解释性，阻碍了其在城市资源分配和政策制定相关场景中的应用。为了弥合这一差距，我们提出了一个可解释的时空图神经网络（STExplainer）框架，通过增强STGNNs的内在可解释性，使其能够同时提供准确的预测和忠实的解释。我们的框架将统一的时空图注意力网络与位置信息融合层作为STG编码器和解码器进行集成。此外，我们基于图信息瓶颈（GIB）原则提出了一种具有可解释目标的结构蒸馏方法，该方法通过实例化GIB原则来提高可解释性。

    Spatio-temporal graph neural networks (STGNNs) have gained popularity as a powerful tool for effectively modeling spatio-temporal dependencies in diverse real-world urban applications, including intelligent transportation and public safety. However, the black-box nature of STGNNs limits their interpretability, hindering their application in scenarios related to urban resource allocation and policy formulation. To bridge this gap, we propose an Explainable Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances STGNNs with inherent explainability, enabling them to provide accurate predictions and faithful explanations simultaneously. Our framework integrates a unified spatio-temporal graph attention network with a positional information fusion layer as the STG encoder and decoder, respectively. Furthermore, we propose a structure distillation approach based on the Graph Information Bottleneck (GIB) principle with an explainable objective, which is instantiated by the
    
[^76]: 半离线策略评估中的反事实增强重要性抽样

    Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])

    [http://arxiv.org/abs/2310.17146](http://arxiv.org/abs/2310.17146)

    提出了一种半离线评估框架，用于强化学习中的定量和定性评估。通过人类用户提供未被观察到的反事实轨迹的注释，设计了一种基于重要性抽样和加权的新型OPE估计器系列。

    

    在将强化学习应用于高风险领域时，使用观测数据进行定量和定性评估可以帮助从业人员了解新策略的泛化性能。然而，这种离网策略评估（OPE）在本质上存在限制，因为离线数据可能不反映由于应用新策略而导致的分布偏移。另一方面，通过根据新策略收集轨迹进行在线评估通常是不可行的，因为在这些领域部署新策略可能是不安全的。在这项工作中，我们提出了一种半离线评估框架，作为离线和在线评估之间的中间步骤，其中人类用户提供未被观察到的反事实轨迹的注释。虽然诱人地简单地用这些注释来增加现有数据，但我们表明这种天真的方法可能导致有偏的结果。相反，我们设计了一种基于重要性抽样（IS）和新颖加权的OPE估计器系列。

    In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting s
    
[^77]: 理解和解决基于双模拟的离线强化学习中的缺陷

    Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning. (arXiv:2310.17139v1 [cs.LG])

    [http://arxiv.org/abs/2310.17139](http://arxiv.org/abs/2310.17139)

    本文研究了基于双模拟的表示方法在离线强化学习中的缺陷，并发现数据集中缺失的转换以及奖励缩放对其性能有重要影响。基于此，我们提出了在离线环境中应用期望值算子与适当的奖励缩放策略来解决这些问题。

    

    虽然基于双模拟的方法在强化学习任务中学习鲁棒的状态表示具有潜力，但其在离线任务中的有效性并不理想。在某些情况下，其性能甚至明显低于替代方法。我们旨在理解为什么双模拟方法在在线环境中成功，但在离线任务中失效。我们的分析揭示了数据集中缺失的转换对双模拟原则的特别有害，导致估计无效。我们还揭示了奖励缩放在限制双模拟测量范围和其引起的值误差方面起着关键作用。基于这些发现，我们建议在离线强化学习设置中应用期望值算子进行表示学习，有助于防止对不完整数据的过拟合。同时，通过引入适当的奖励缩放策略，避免了表示空间中的特征崩溃风险。

    While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We imple
    
[^78]: 大规模高斯过程通过交替投影

    Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])

    [http://arxiv.org/abs/2310.17137](http://arxiv.org/abs/2310.17137)

    本论文提出了一种通过交替投影的迭代方法来解决高斯过程在大规模数据集上的训练问题，并证明了该方法具有线性收敛性。

    

    高斯过程（GP）超参数优化需要反复求解具有 nxn 核矩阵的线性系统。为了解决 O(n^3) 的时间复杂性问题，最近的研究采用了快速迭代数值方法，如共轭梯度（CG）。然而，随着数据集规模的增加，相应的核矩阵变得越来越病态，并且在没有分割的情况下仍然需要 O(n^2) 的空间。因此，虽然 CG 增加了可训练 GP 基于的数据集的大小，但现代数据集已经达到超出其适用范围的规模。在这项工作中，我们提出了一种只访问核矩阵的子块的迭代方法，有效地实现了小批量处理。我们的算法基于交替投影，每次迭代的时间和空间复杂度为 O(n)，解决了将 GP 扩展到非常大的数据集时的许多实际挑战。从理论上讲，我们证明了我们的方法具有线性收敛性，从实证的角度来看，我们证明了

    Gaussian process (GP) hyperparameter optimization requires repeatedly solving linear systems with $n \times n$ kernel matrices. To address the prohibitive $\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative numerical methods, like conjugate gradients (CG). However, as datasets increase in magnitude, the corresponding kernel matrices become increasingly ill-conditioned and still require $\mathcal{O}(n^2)$ space without partitioning. Thus, while CG increases the size of datasets GPs can be trained on, modern datasets reach scales beyond its applicability. In this work, we propose an iterative method which only accesses subblocks of the kernel matrix, effectively enabling \emph{mini-batching}. Our algorithm, based on alternating projection, has $\mathcal{O}(n)$ per-iteration time and space complexity, solving many of the practical challenges of scaling GPs to very large datasets. Theoretically, we prove our method enjoys linear convergence and empirically we demons
    
[^79]: 通过双向知识传递释放GNN的潜力

    Unleashing the potential of GNNs via Bi-directional Knowledge Transfer. (arXiv:2310.17132v1 [cs.LG])

    [http://arxiv.org/abs/2310.17132](http://arxiv.org/abs/2310.17132)

    本文通过研究发现GNN未充分利用内在的特征转换操作能力，提出了一种双向知识传递的插拔式方法，使GNN能够充分释放特征转换操作的潜力，而不需要修改原始架构。

    

    基于消息传递范式，已经有大量研究提出了各种各样令人印象深刻的特征传播机制来提高GNN的性能。然而，对于特征转换，消息传递框架的另一个重要操作，关注较少。在本文中，我们首先经验性地研究了几种典型GNN中特征转换操作的性能。意外的是，我们注意到GNN没有完全释放出内在的特征转换操作的能力。基于这一观察，我们提出了双向知识传递（BiKT），一种插拔式方法，旨在释放特征转换操作的潜力，而不修改原始架构。将特征转换操作视为与原始GNN共享参数的派生表示学习模型，通过该模型的直接预测提供了一种与拓扑无关的知识反馈，进一步指导了GNN的学习过程。

    Based on the message-passing paradigm, there has been an amount of research proposing diverse and impressive feature propagation mechanisms to improve the performance of GNNs. However, less focus has been put on feature transformation, another major operation of the message-passing framework. In this paper, we first empirically investigate the performance of the feature transformation operation in several typical GNNs. Unexpectedly, we notice that GNNs do not completely free up the power of the inherent feature transformation operation. By this observation, we propose the Bi-directional Knowledge Transfer (BiKT), a plug-and-play approach to unleash the potential of the feature transformation operations without modifying the original architecture. Taking the feature transformation operation as a derived representation learning model that shares parameters with the original GNN, the direct prediction by this model provides a topological-agnostic knowledge feedback that can further instru
    
[^80]: 使用语言模型对半结构化和非结构化对话数据进行主题分段

    Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])

    [http://arxiv.org/abs/2310.17120](http://arxiv.org/abs/2310.17120)

    本文通过对非结构化文本进行分析，揭示了目前主题分段模型在此类数据上的泛化能力不足，并提出了从头开始训练相对小规模的目标数据集来改善分段结果的方法。实证评估表明使用多种损失函数可以减轻非结构化对话数据集的不平衡效应。

    

    在自然语言处理中，将文档或对话根据其语义结构分解为多个连续片段是一个重要且具有挑战性的问题，可以帮助许多下游任务。然而，当前关于主题分段的研究往往集中在结构化文本的分段上。在本文中，我们全面分析了最先进的主题分段模型在非结构化文本上的泛化能力。我们发现：（a）目前在大规模结构化文本语料库（如Wiki-727K）上进行预训练的策略对于在非结构化对话数据上的可传递性并不有帮助。（b）从头开始使用相对小规模的目标非结构化领域数据集训练能显著提高分段结果。我们通过尝试多种损失函数来进行我们的主题分段方法的强化测试，以减轻非结构化对话数据集的不平衡效应。我们的实证评估表明Fo

    Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Fo
    
[^81]: CART在足够不纯度减少条件下的收敛性研究

    On the Convergence of CART under Sufficient Impurity Decrease Condition. (arXiv:2310.17114v1 [stat.ML])

    [http://arxiv.org/abs/2310.17114](http://arxiv.org/abs/2310.17114)

    本研究通过研究回归设置下CART的收敛性，建立了足够不纯度减少条件下CART预测误差的上界，并提供了易于验证的足够条件。这对于决策树模型的应用具有重要意义。

    

    决策树是一种灵活的机器学习模型，在许多应用中取得了成功。通常使用CART以递归贪婪的方式拟合决策树。本文研究了在回归设置下CART的收敛速率。首先，我们在足够不纯度减少条件下建立了CART的预测误差的上界，该结果改进了之前类似假设下的已知结果。此外，我们提供了一些示例证明误差界限无法通过常数或对数因子进一步改进。其次，我们介绍了一组易于验证的足够条件以满足不纯度减少条件。具体来说，我们证明了在加性模型的情况下，只要组件函数符合“局部反向波松不等式”，就可以满足不纯度减少条件。我们讨论了几个在非参数设置中众所周知的函数类。

    The decision tree is a flexible machine learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we investigate the convergence rate of CART under a regression setting. First, we establish an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition \cite{chi2022asymptotic} -- our result improves upon the known result by \cite{chi2022asymptotic} under a similar assumption. Furthermore, we provide examples that demonstrate the error bound cannot be further improved by more than a constant or a logarithmic factor. Second, we introduce a set of easily verifiable sufficient conditions for the SID condition. Specifically, we demonstrate that the SID condition can be satisfied in the case of an additive model, provided that the component functions adhere to a ``locally reverse Poincar{\'e} inequality". We discuss several well-known function classes in non-parametric es
    
[^82]: LLM4DyG：大型语言模型能解决动态图上的问题吗?

    LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. (arXiv:2310.17110v1 [cs.LG])

    [http://arxiv.org/abs/2310.17110](http://arxiv.org/abs/2310.17110)

    本研究首次提出了评估大型语言模型（LLMs）在动态图上的时空理解能力的LLM4DyG基准，并通过广泛的实验分析了不同因素对模型性能的影响。

    

    在越来越多地采用大型语言模型（LLMs）处理各种任务的时代，人们越来越关注探索LLMs在处理网络数据，特别是图形数据方面的能力。动态图在现实世界的网络数据中无处不在，它们捕捉了网络演化模式。评估LLMs在理解动态图上的时空信息方面的能力对于它们在Web应用中的采用至关重要，然而这在文献中尚未得到探索。本文通过首次提出在动态图上评估LLMs的时空理解能力来填补这一空白。具体而言，我们提出了LLM4DyG基准，其中包括九个特别设计的任务，考虑了LLMs在时态和空间维度上的能力评估。然后，我们进行了广泛的实验，分析了不同的数据生成器、数据统计、提示技术和LLMs对模型性能的影响。

    In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs' competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the mo
    
[^83]: 通过图神经网络进行网络设计：识别挑战并提高性能

    Network Design through Graph Neural Networks: Identifying Challenges and Improving Performance. (arXiv:2310.17100v1 [cs.LG])

    [http://arxiv.org/abs/2310.17100](http://arxiv.org/abs/2310.17100)

    这项研究通过分析先前的图神经网络研究，发现边缘的选择受到结构偏差的影响，而不是重要性，从而导致编辑时出现错误。为了改进编辑，研究者们提出了一种名为ORE的迭代编辑方法，通过编辑最高评分的边缘并重新嵌入图来刷新梯度，从而减少偏好性较高的边缘选择。

    

    图神经网络（GNN）的研究已经产生了一些策略，使用训练过的GNN的梯度来修改图的边，以实现网络设计的目标。然而，控制基于梯度的编辑的因素还没有得到充分研究，这导致选择边缘的原因以及编辑是否基于边缘的重要性变得模糊。因此，我们首先分析了先前作品中的梯度计算，阐明了影响编辑的因素，并突出了对结构属性过度依赖的潜在问题。具体来说，我们发现边缘可以实现高梯度，原因是结构偏差，而不是重要性，导致当这些因素与设计任务无关时出现错误的编辑。为了改进编辑，我们提出了ORE，一种迭代编辑方法，（a）编辑最高评分的边缘，（b）重新嵌入编辑后的图以刷新梯度，从而减少偏好性较高的边缘选择。我们通过一组拟议的设计任务对ORE进行了实证研究，每个任务都有一个外部验证方法，证明ORE可以改善编辑的性能。

    Graph Neural Network (GNN) research has produced strategies to modify a graph's edges using gradients from a trained GNN, with the goal of network design. However, the factors which govern gradient-based editing are understudied, obscuring why edges are chosen and if edits are grounded in an edge's importance. Thus, we begin by analyzing the gradient computation in previous works, elucidating the factors that influence edits and highlighting the potential over-reliance on structural properties. Specifically, we find that edges can achieve high gradients due to structural biases, rather than importance, leading to erroneous edits when the factors are unrelated to the design task. To improve editing, we propose ORE, an iterative editing method that (a) edits the highest scoring edges and (b) re-embeds the edited graph to refresh gradients, leading to less biased edge choices. We empirically study ORE through a set of proposed design tasks, each with an external validation method, demonst
    
[^84]: 良好的规则性创造了大学习率的隐性偏差：稳定的边界，平衡和弹射

    Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult. (arXiv:2310.17087v1 [cs.LG])

    [http://arxiv.org/abs/2310.17087](http://arxiv.org/abs/2310.17087)

    该论文研究了大学习率在非凸优化中产生的隐性偏差，包括稳定的边界、平衡和弹射，并通过发展新的全局收敛理论和研究良好规则性的目标函数，将这些现象归纳为同一现象的不同表现形式。

    

    当应用于非凸优化的梯度下降时，大学习率会产生各种隐性偏差，包括稳定的边界、平衡和弹射。这些现象无法用经典的优化理论很好地解释。尽管在理解这些隐性偏差方面已经取得了重要的理论进展，但仍然不清楚它们在哪些目标函数上会发生。本文对回答这个问题提供了一个初始的步骤，即这些隐性偏差实际上是同一冰山的各种尖端。当优化的目标函数具有一定的良好规则性，并与大学习率梯度下降对向更平坦区域移动的可证明偏好相结合时，就会产生这些非平凡的动力学现象。为了建立这个结果，我们发展了一个新的大学习率全局收敛理论，针对一族非凸函数。

    Large learning rates, when applied to gradient descent for nonconvex optimization, yield various implicit biases including the edge of stability (Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et al., 2020). These phenomena cannot be well explained by classical optimization theory. Though significant theoretical progress has been made in understanding these implicit biases, it remains unclear for which objective functions would they occur. This paper provides an initial step in answering this question, namely that these implicit biases are in fact various tips of the same iceberg. They occur when the objective function of optimization has some good regularity, which, in combination with a provable preference of large learning rate gradient descent for moving toward flatter regions, results in these nontrivial dynamical phenomena. To establish this result, we develop a new global convergence theory under large learning rates, for a family of nonconvex functi
    
[^85]: Transformers学会了高阶优化方法用于上下文学习：一项与线性模型的研究

    Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])

    [http://arxiv.org/abs/2310.17086](http://arxiv.org/abs/2310.17086)

    Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。

    

    Transformers在上下文学习中表现出色，但是它们是如何进行上下文学习仍然是一个谜。最近的研究表明，Transformers可能通过内部运行梯度下降，即一阶优化方法，来进行上下文学习。本文中，我们展示了Transformers学会了实现高阶优化方法来进行上下文学习。我们以上下文线性回归为重点，展示了Transformers学会了实现一个非常类似于迭代牛顿法的算法，而不是梯度下降。从实证上来看，我们展示了连续的Transformer层的预测与牛顿法的不同迭代非常接近，每个中间层大致计算了3次迭代。相比之下，需要指数级的梯度下降步骤才能匹配额外的Transformer层；这表明Transformers具有相当的收敛速率。

    Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
    
[^86]: 自动化地利用时间序列图像的实例分割来监测生态研究中的地衣

    Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images. (arXiv:2310.17080v1 [cs.CV])

    [http://arxiv.org/abs/2310.17080](http://arxiv.org/abs/2310.17080)

    本研究针对地衣监测的自动化问题，提出了一种利用时间序列图像的实例分割方法。通过该方法可以准确估计地衣的生物量和状态，从而方便生态学家的工作。

    

    地衣是由真菌、藻类和/或蓝细菌组成的共生生物，能在各种环境中繁荣生长。地衣在碳和氮循环中起着重要作用，并直接或间接地对生物多样性做出贡献。生态学家通常通过使用地衣作为指示剂来评估空气质量和栖息地情况来监测地衣。特别是生长在树上的附生地衣，是评估空气质量和环境健康的关键指标。一种新的监测附生地衣的方法是使用时间序列相机来收集地衣种群的图像。这些相机被纽芬兰和拉布拉多的生态学家用来随后分析和手动分割图像以确定地衣的状态和变化。这些方法耗时且容易受到观察者偏见的影响。在这项工作中，我们旨在自动化监测地衣长达一段时间，并估计它们的生物量和状态，以便为生态学家的任务提供帮助。

    Lichens are symbiotic organisms composed of fungi, algae, and/or cyanobacteria that thrive in a variety of environments. They play important roles in carbon and nitrogen cycling, and contribute directly and indirectly to biodiversity. Ecologists typically monitor lichens by using them as indicators to assess air quality and habitat conditions. In particular, epiphytic lichens, which live on trees, are key markers of air quality and environmental health. A new method of monitoring epiphytic lichens involves using time-lapse cameras to gather images of lichen populations. These cameras are used by ecologists in Newfoundland and Labrador to subsequently analyze and manually segment the images to determine lichen thalli condition and change. These methods are time-consuming and susceptible to observer bias. In this work, we aim to automate the monitoring of lichens over extended periods and to estimate their biomass and condition to facilitate the task of ecologists. To accomplish this, ou
    
[^87]: HCT：基于混合ConvNet-Transformer的帕金森病步态检测和严重程度预测

    HCT: Hybrid Convnet-Transformer for Parkinson's disease detection and severity prediction from gait. (arXiv:2310.17078v1 [cs.CV])

    [http://arxiv.org/abs/2310.17078](http://arxiv.org/abs/2310.17078)

    提出了一种基于混合ConvNet-Transformer的深度学习方法，用于检测和分级帕金森病。该方法利用ConvNets和Transformers的优势，准确地检测PD并确定其严重程度阶段，在相对于其他方法的比较中表现出卓越的性能，PD检测准确率达到97%。

    

    本文提出了一种新的基于混合ConvNet-Transformer架构的深度学习方法，用于从步态数据中检测和分级帕金森病（PD）。我们采用了一种两步法的方法，将问题分为两个子问题。我们的混合ConvNet-Transformer模型首先区分健康和帕金森病患者。如果患者是帕金森病患者，多类别的混合ConvNet-Transformer模型确定Hoehn和Yahr（H&Y）分数，以评估PD的严重程度阶段。我们的混合架构充分利用了Convolutional Neural Networks（ConvNets）和Transformers的优势，可以准确地检测PD并确定严重程度阶段。特别是，我们利用ConvNets来捕捉数据中的局部模式和相关性，而利用Transformers来处理输入信号中的长期依赖关系。我们证明了我们的混合方法相对于其他最先进的方法具有卓越的性能，PD检测准确率达到97％。

    In this paper, we propose a novel deep learning method based on a new Hybrid ConvNet-Transformer architecture to detect and stage Parkinson's disease (PD) from gait data. We adopt a two-step approach by dividing the problem into two sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy versus parkinsonian patients. If the patient is parkinsonian, a multi-class Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&Y) score to assess the PD severity stage. Our hybrid architecture exploits the strengths of both Convolutional Neural Networks (ConvNets) and Transformers to accurately detect PD and determine the severity stage. In particular, we take advantage of ConvNets to capture local patterns and correlations in the data, while we exploit Transformers for handling long-term dependencies in the input signal. We show that our hybrid method achieves superior performance when compared to other state-of-the-art methods, with a PD detection accuracy of 97%
    
[^88]: 带有大学习率的随机梯度下降的良性振荡

    Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates. (arXiv:2310.17074v1 [cs.LG])

    [http://arxiv.org/abs/2310.17074](http://arxiv.org/abs/2310.17074)

    通过大学习率的随机梯度下降训练的神经网络，由于其权重的振荡，能够在特征噪声数据上实现良好的泛化性能。

    

    在这项工作中，我们在理论上研究了使用大学习率训练的随机梯度下降（SGD）算法训练的神经网络（NN）的泛化特性。在这种训练方式下，我们的发现是，由于大学习率SGD训练引起的NN权重的振荡对NN的泛化有益，这有可能优于通过收敛较平滑的小学习率SGD训练的相同NN。基于这个发现，我们将这种现象称为“良性振荡”。我们解密这种现象的理论建立在深度学习的特征学习角度上。具体来说，我们考虑一个特征噪声数据生成模型，它包括（i）具有小的$\ell_2$-范数并出现在每个数据点中的弱特征；（ii）具有较大的$\ell_2$-范数但只出现在所有数据点的一部分中的强特征；和（iii）噪声。我们证明了通过振荡训练的NN能够在这个特征噪声数据生成模型上实现较好的泛化性能。

    In this work, we theoretically investigate the generalization properties of neural networks (NN) trained by stochastic gradient descent (SGD) algorithm with large learning rates. Under such a training regime, our finding is that, the oscillation of the NN weights caused by the large learning rate SGD training turns out to be beneficial to the generalization of the NN, which potentially improves over the same NN trained by SGD with small learning rates that converges more smoothly. In view of this finding, we call such a phenomenon "benign oscillation". Our theory towards demystifying such a phenomenon builds upon the feature learning perspective of deep learning. Specifically, we consider a feature-noise data generation model that consists of (i) weak features which have a small $\ell_2$-norm and appear in each data point; (ii) strong features which have a larger $\ell_2$-norm but only appear in a certain fraction of all data points; and (iii) noise. We prove that NNs trained by oscill
    
[^89]: 等距运动流形基元

    Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])

    [http://arxiv.org/abs/2310.17072](http://arxiv.org/abs/2310.17072)

    Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.

    

    运动流形基元（MMP）为给定任务生成一系列连续轨迹流形，每一个轨迹流形都能成功完成任务。它由对流形进行参数化的解码函数以及潜在坐标空间中的概率密度组成。本文首先展示了由于潜在空间中的几何扭曲，MMP的性能可能会显著降低--通过变形，我们指的是相似的运动在潜在空间中无法相邻。然后，我们提出了等距运动流形基元（IMMP），其潜在坐标空间保持了流形的几何结构。为此，我们建立和使用了一个Riemannian度量，用于运动空间（即，参数化曲线空间），我们称之为CurveGeom Riemannian度量。对于平面障碍避让运动和推动操纵任务的实验表明，IMMP明显优于现有的MMP方法。代码可在https://github.com/Gabe-YHLee/IMMP找到。

    The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
    
[^90]: math-PVS:一个将科学出版物映射到PVS理论的大型语言模型框架

    math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])

    [http://arxiv.org/abs/2310.17064](http://arxiv.org/abs/2310.17064)

    该研究调查了将大型语言模型应用于形式化高级数学概念的可行性，并提出了一个可以批判性地审查和检查研究论文中数学推理的框架。

    

    随着人工智能在各种应用领域的广泛采用，它在数学发现方面有巨大潜力，可以引导猜想生成，构造反例，协助形式化数学，并发现不同数学领域之间的联系，等等。尽管之前的工作利用计算机进行详尽的数学证明搜索，但基于大型语言模型的最近努力致力于将计算平台定位为数学研究过程中的合作贡献者。尽管目前的语言模型在逻辑和数学任务方面存在局限性，但越来越多的人对将定理证明系统与基础模型结合起来的兴趣日益增长。本研究调查了LLMs在形式化高级数学概念方面的适用性，并提出了一个能够批判性地审查和检查研究论文中数学推理的框架。

    As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the 
    
[^91]: 德克萨斯州的电动车充电策略和可再生能源整合

    Strategizing EV Charging and Renewable Integration in Texas. (arXiv:2310.17056v1 [eess.SY])

    [http://arxiv.org/abs/2310.17056](http://arxiv.org/abs/2310.17056)

    本研究通过动态时间扭曲（DTW）聚类和k均值聚类方法，针对德克萨斯州的电动车充电和可再生能源整合问题提供了战略决策的复杂方法，为实现可持续和具有适应能力的能源未来的整合做出了贡献。

    

    本研究探讨了电动车（EV）、可再生能源和智能电网技术在德克萨斯州的融合，解决了EV广泛采用所面临的挑战。研究关注电网稳定性问题、不协调的充电模式以及EV与可再生能源之间复杂的关系。通过动态时间扭曲（DTW）聚类和k均值聚类方法，根据总负荷和净负荷将每天进行分类，从而提供了对日常电力消耗和可再生能源生成模式的细致洞察。通过建立针对特定负荷特征的最佳充电和车辆对电网（V2G）时间窗口，本研究提供了一种在能源消耗和可再生能源整合方面进行战略决策的复杂方法。研究结果对于实现可持续和具有适应能力的能源未来的顺利整合做出了贡献。

    Exploring the convergence of electric vehicles (EVs), renewable energy, and smart grid technologies in the context of Texas, this study addresses challenges hindering the widespread adoption of EVs. Acknowledging their environmental benefits, the research focuses on grid stability concerns, uncoordinated charging patterns, and the complicated relationship between EVs and renewable energy sources. Dynamic time warping (DTW) clustering and k-means clustering methodologies categorize days based on total load and net load, offering nuanced insights into daily electricity consumption and renewable energy generation patterns. By establishing optimal charging and vehicle-to-grid (V2G) windows tailored to specific load characteristics, the study provides a sophisticated methodology for strategic decision-making in energy consumption and renewable integration. The findings contribute to the ongoing discourse on achieving a sustainable and resilient energy future through the seamless integration
    
[^92]: 提升语言模型生成中的常识能力：利用黑盒控制增强

    BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])

    [http://arxiv.org/abs/2310.17054](http://arxiv.org/abs/2310.17054)

    本文提出了一个计算高效的框架，通过利用黑盒控制来引导冻结的预训练语言模型（PTLM）生成更加常识性的文本输出。

    

    大型语言模型（LLM）如GPT-3已经展示了生成连贯且上下文相关的文本的强大能力。然而，在它们的成功之中，一个关键问题仍然存在：它们生成的输出有时仍然缺乏常识。此外，如果不可行的话，将整个LLM进行微调以获得更加常识性的输出是计算上代价昂贵的。在本文中，我们提出了一个计算高效的框架，将一个冻结的预训练语言模型（PTLM）引导向更加常识性的生成（即以有意义的方式产生包含一系列概念的合理输出）。具体地，我们首先构建了一个无需参考的评估器，通过将句子与一个动态常识知识库在四个不同关系方面相连来为句子分配一个常识得分。然后，我们使用评分器作为常识知识的参考，扩展了名为NADO的可控生成方法，训练了一个辅助头部来引导一个。

    Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a
    
[^93]: 通过多任务双层优化学习主动学习的排序方法

    Learning to Rank for Active Learning via Multi-Task Bilevel Optimization. (arXiv:2310.17044v1 [cs.LG])

    [http://arxiv.org/abs/2310.17044](http://arxiv.org/abs/2310.17044)

    本论文提出了一种通过学习的替代模型选择未标记实例进行数据获取的主动学习方法，通过双层多任务双层优化框架预测不同训练集的相对效用并确保泛化效果。

    

    主动学习是一种有前途的范例，通过有策略地请求标签来提高模型性能，以减少标注成本。然而，现有的主动学习方法往往依赖于昂贵的收购函数计算、大量的建模重训练以及与标记者的多轮互动。为解决这些局限性，我们提出了一种新的主动学习方法，通过学习的替代模型选择批次未标记实例进行数据获取。这种方法的一个关键挑战是开发一个良好泛化的收购函数，因为随着时间推移，作为效用函数输入的历史数据不断增长。我们的算法贡献是一个双层多任务双层优化框架，预测不同训练集的相对效用，以验证准确性为度量，并确保学习的收购函数有效泛化。对于验证准确性昂贵的情况，我们还提供了一个处理策略。

    Active learning is a promising paradigm to reduce the labeling cost by strategically requesting labels to improve model performance. However, existing active learning methods often rely on expensive acquisition function to compute, extensive modeling retraining and multiple rounds of interaction with annotators. To address these limitations, we propose a novel approach for active learning, which aims to select batches of unlabeled instances through a learned surrogate model for data acquisition. A key challenge in this approach is developing an acquisition function that generalizes well, as the history of data, which forms part of the utility function's input, grows over time. Our novel algorithmic contribution is a bilevel multi-task bilevel optimization framework that predicts the relative utility -- measured by the validation accuracy -- of different training sets, and ensures the learned acquisition function generalizes effectively. For cases where validation accuracy is expensive 
    
[^94]: StochGradAdam: 利用随机梯度抽样加速神经网络训练

    StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])

    [http://arxiv.org/abs/2310.17042](http://arxiv.org/abs/2310.17042)

    StochGradAdam是一种利用随机梯度抽样加速神经网络训练的优化器，通过选择性梯度考虑，能够稳定收敛，提升鲁棒训练。在图像分类和分割任务中表现优异。

    

    在深度学习优化领域中，本文介绍了StochGradAdam优化器，这是对广受赞誉的Adam算法的新颖改进。StochGradAdam的核心是其梯度抽样技术。该方法不仅确保稳定收敛，而且利用选择性梯度考虑的优势，通过减轻噪声或异常数据的影响和增强损失函数空间的探索，提升了鲁棒训练。在图像分类和分割任务中，StochGradAdam表现出优于传统Adam优化器的性能。通过在每次迭代中精心选择一部分梯度进行抽样，该优化器能够有效应对复杂模型的管理。本文从数学基础到偏差校正策略全面探讨了StochGradAdam的方法，展示了深度学习训练技术的可期进展。

    In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
    
[^95]: 在时间序列预测中，量子长短期记忆（QLSTM）与经典LSTM的比较研究：以太阳能预测为例

    Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting. (arXiv:2310.17032v1 [quant-ph])

    [http://arxiv.org/abs/2310.17032](http://arxiv.org/abs/2310.17032)

    本研究比较了量子长短期记忆（QLSTM）和经典长短期记忆（LSTM）模型在太阳能发电量预测中的表现，发现QLSTM具有加快训练收敛速度和减小测试损失的优势，拥有吸纳复杂时间序列关系的潜力，但还需要进一步研究和优化。

    

    在全球向可持续能源系统发展的过程中，准确预测太阳能发电量至关重要。在本研究中，我们对量子长短期记忆（QLSTM）和经典长短期记忆（LSTM）模型在太阳能发电预测方面进行了仔细比较。我们的实验结果显示，QLSTM具有显著优势，包括训练收敛速度加快和在初始阶段明显降低的测试损失，相比之下，经典LSTM模型。这些实证结果表明，QLSTM有潜力快速吸纳复杂的时间序列关系，这得益于量子现象（如叠加）。然而，要实现QLSTM的全部能力，还需要进一步研究模型在不同条件下的验证、系统的超参数优化、硬件噪声鲁棒性，以及相关可再生能源预测问题的应用。随着不断的进展，量子机器学习可以在预测和优化复杂问题方面带来变革。

    Accurately forecasting solar power generation is crucial in the global progression towards sustainable energy systems. In this study, we conduct a meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our controlled experiments reveal promising advantages of QLSTMs, including accelerated training convergence and substantially reduced test loss within the initial epoch compared to classical LSTMs. These empirical findings demonstrate QLSTM's potential to swiftly assimilate complex time series relationships, enabled by quantum phenomena like superposition. However, realizing QLSTM's full capabilities necessitates further research into model validation across diverse conditions, systematic hyperparameter optimization, hardware noise resilience, and applications to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in
    
[^96]: 论高斯过程模型的可识别性和可解释性

    On the Identifiability and Interpretability of Gaussian Process Models. (arXiv:2310.17023v1 [stat.ML])

    [http://arxiv.org/abs/2310.17023](http://arxiv.org/abs/2310.17023)

    本文研究了高斯过程模型中的可识别性和可解释性问题。对于单输出情况，我们发现Matern核混合的平滑性由最不平滑的组件决定，并且混合核等价于最不平滑的核组件。在多输出模型中，我们证明了协方差矩阵的可识别性，这表明乘法混合是可行的。

    

    本文对使用加性Matern核在单输出高斯过程（GP）模型中的普遍做法进行了批判性分析，并探讨了用于多输出GP模型的乘法Matern核的性质。对于单输出情况，我们推导出一系列理论结果，表明Matern核混合的平滑性由最不平滑的组件决定，并且具有这样核的GP实际上等价于最不平滑的核组件。此外，我们证明了各个核组件中的混合权重或参数均无法识别。然后，我们将注意力转向多输出GP模型，并分析了乘法核$K(x,y) = AK_0(x,y)$中协方差矩阵$A$的可识别性，其中$K_0$是标准的单输出核，如Matern。我们展示了$A$在乘法常数上是可识别的，这表明乘法混合是可行的。

    In this paper, we critically examine the prevalent practice of using additive mixtures of Mat\'ern kernels in single-output Gaussian process (GP) models and explore the properties of multiplicative mixtures of Mat\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\'ern. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well 
    
[^97]: 受控解码来自语言模型

    Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])

    [http://arxiv.org/abs/2310.17022](http://arxiv.org/abs/2310.17022)

    本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。

    

    我们提出了一种新颖的离策略强化学习方法，称为受控解码（CD），用于控制自回归语言模型的生成，以获得高回报的结果。CD通过值函数来解决离策略强化学习问题，该值函数被称为前缀评分器。前缀评分器在推理时用于引导生成向更高回报的结果。我们展示了前缀评分器可以从（可能是）离策略数据中训练出来，用于预测从部分解码的响应继续解码时的预期回报。我们在Reddit对话语料库上经验证明，CD作为一种控制机制是有效的。我们还展示了CD设计的模块化使其能够有效解决多目标强化学习问题，而不会增加任何复杂性。最后，我们展示了CD可以以一种新颖的分块方式在推理时应用，同样无需任何额外的操作。

    We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
    
[^98]: 流式因子轨迹学习用于时间张量分解

    Streaming Factor Trajectory Learning for Temporal Tensor Decomposition. (arXiv:2310.17021v1 [cs.LG])

    [http://arxiv.org/abs/2310.17021](http://arxiv.org/abs/2310.17021)

    这篇论文提出了一种流式因子轨迹学习方法，用于时间张量分解。该方法使用高斯过程来灵活估计因子的时间演变，并开发了一种高效的在线滤波算法来估计因子状态的演变轨迹。

    

    实际的张量数据通常具有时间信息。大多数现有的时间分解方法估计每个张量模式中对象的一组固定因子，因此无法捕捉对象表示的时间演变。更重要的是，我们缺乏从流数据中捕捉这种演变的有效方法，在现实世界的应用中很常见。为了解决这些问题，我们提出了用于时间张量分解的流式因子轨迹学习（SFTL）。我们使用高斯过程（GPs）来建模因子的轨迹，从而灵活地估计它们的时间演变。为了解决处理流数据时的计算挑战，我们通过构造等效的随机微分方程（SDE）将GPs转换为状态空间先验。我们开发了一种高效的在线滤波算法，在接收到新数据时估计涉及的因子状态的分解式运行后验。分解估计使我们能够同时学习多个因子的演变轨迹。

    Practical tensor data is often along with time information. Most existing temporal decomposition approaches estimate a set of fixed factors for the objects in each tensor mode, and hence cannot capture the temporal evolution of the objects' representation. More important, we lack an effective approach to capture such evolution from streaming data, which is common in real-world applications. To address these issues, we propose Streaming Factor Trajectory Learning (SFTL) for temporal tensor decomposition. We use Gaussian processes (GPs) to model the trajectory of factors so as to flexibly estimate their temporal evolution. To address the computational challenges in handling streaming data, we convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE). We develop an efficient online filtering algorithm to estimate a decoupled running posterior of the involved factor states upon receiving new data. The decoupled estimation enables us to co
    
[^99]: 使用大语言模型有条件地组合机器人技能

    Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])

    [http://arxiv.org/abs/2310.17019](http://arxiv.org/abs/2310.17019)

    这篇论文介绍了一个扩展的Meta-World基准，称为“语言世界”，允许大型语言模型在模拟机器人环境中使用自然语言查询和脚本技能。同时，引入了计划条件行为克隆（PCBC）的方法，通过端到端演示对高级计划进行微调。实验结果表明，在少样本情况下，PCBC在语言世界中能够实现强大的性能。

    

    这篇论文结合了两个贡献。首先，我们介绍了Meta-World基准的一个扩展，称为“语言世界”，它允许一个大型语言模型在一个模拟机器人环境中使用半结构化自然语言查询和用自然语言描述的脚本技能。通过使用与Meta-World相同的任务集，可以轻松比较语言世界和Meta-World的结果，从而对最近使用大型语言模型（LLMs）和使用深度强化学习的方法进行比较。其次，我们介绍了一种称为计划条件行为克隆（PCBC）的方法，它允许对高级计划的行为进行微调，使用端到端演示。使用语言世界，我们展示了PCBC能够在各种少样本情况下取得卓越的性能，通常只需一个演示即可实现任务的泛化。我们将语言世界作为开源软件提供，网址是https://...

    This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call "Language-World," which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https
    
[^100]: 通过可接近性更快地重新校准在线预测模型

    Faster Recalibration of an Online Predictor via Approachability. (arXiv:2310.17002v1 [cs.LG])

    [http://arxiv.org/abs/2310.17002](http://arxiv.org/abs/2310.17002)

    本文介绍了通过布莱克韦尔可接近性定理将在线预测模型转化为校准预测的技术，并且相较于已有技术实现了更快的速度、灵活的校准误差和准确性权衡。

    

    机器学习中的预测模型需要具有可信和可靠的特性，这通常至少意味着输出校准的概率。在线预测设置中，当结果序列可以由对手对抗地生成时，保证校准性可能特别困难。本文介绍了一种使用布莱克韦尔可接近性定理的技术，将可能不校准的在线预测模型的预测转化为校准的预测，而原模型的损失不会大幅增加。我们提出的算法在在线设置中以更快的速度实现了校准和准确性，相比已有的技术(arXiv:1607.03594)，我们的算法是第一个在在线设置中提供校准误差和准确性之间灵活权衡的算法。通过使用我们的技术，我们能够描述出可共同实现的校准和遗憾的空间。

    Predictive models in ML need to be trustworthy and reliable, which often at the very least means outputting calibrated probabilities. This can be particularly difficult to guarantee in the online prediction setting when the outcome sequence can be generated adversarially. In this paper we introduce a technique using Blackwell's approachability theorem for taking an online predictive model which might not be calibrated and transforming its predictions to calibrated predictions without much increase to the loss of the original model. Our proposed algorithm achieves calibration and accuracy at a faster rate than existing techniques arXiv:1607.03594 and is the first algorithm to offer a flexible tradeoff between calibration error and accuracy in the online setting. We demonstrate this by characterizing the space of jointly achievable calibration and regret using our technique.
    
[^101]: 信任，但要验证：使用深度学习进行鲁棒图像分割

    Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v1 [cs.CV])

    [http://arxiv.org/abs/2310.16999](http://arxiv.org/abs/2310.16999)

    本文描述了一种使用“信任，但要验证”方法进行深度学习图像分割的方法，通过辅助验证网络对分割进行预测，以此来验证深度神经网络的输出。这种方法对多种扰动具有鲁棒性，可以应对对抗性攻击，并且不依赖于黑盒神经网络来检测错误的分割。

    

    我们描述了一种用于验证深度神经网络在医学图像分割中的输出的方法，该方法对于多种类型的随机和最坏情况的扰动具有鲁棒性，即对抗性攻击。该方法基于作者最近提出的一种称为“信任，但要验证”的通用方法，其中辅助验证网络使用分割作为输入来对输入图像中的某些被遮蔽特征进行预测。设计良好的辅助网络将在输入分割准确时生成高质量的预测，但在分割不正确时生成低质量的预测。通过将这个网络的预测与原始图像进行检查，我们可以检测出错误的分割。然而，为了确保验证方法真正鲁棒，我们需要一种不依赖于黑盒神经网络的方法来检查预测的质量。事实上，我们展示了先前用于分割评估的方法无法应对鲁棒的情况。

    We describe a method for verifying the output of a deep neural network for medical image segmentation that is robust to several classes of random as well as worst-case perturbations i.e. adversarial attacks. This method is based on a general approach recently developed by the authors called ``Trust, but Verify" wherein an auxiliary verification network produces predictions about certain masked features in the input image using the segmentation as an input. A well-designed auxiliary network will produce high-quality predictions when the input segmentations are accurate, but will produce low-quality predictions when the segmentations are incorrect. Checking the predictions of such a network with the original image allows us to detect bad segmentations. However, to ensure the verification method is truly robust, we need a method for checking the quality of the predictions that does not itself rely on a black-box neural network. Indeed, we show that previous methods for segmentation evalua
    
[^102]: 迈向持续学习应用性能模型

    Towards Continually Learning Application Performance Models. (arXiv:2310.16996v1 [cs.LG])

    [http://arxiv.org/abs/2310.16996](http://arxiv.org/abs/2310.16996)

    本论文提出了一种能够持续学习的性能模型，考虑到数据分布的漂移，缓解灾难性遗忘，并提高了泛化能力。最佳模型在学习系统变化引起的新数据分布的同时保持了准确性，并相对于简单方法，全面数据序列的预测准确性提高了2倍。

    

    基于机器学习的性能模型越来越被用于构建关键的作业调度和应用程序优化决策。然而，传统上，这些模型假设随着时间的推移，数据分布不会发生改变。然而，由于生产HPC系统的复杂性和异构性，它们容易受到硬件退化、更换和/或软件补丁的影响，这可能导致数据分布漂移，从而对性能模型产生不利影响。为此，我们开发了一种持续学习的性能模型，考虑了分布漂移，缓解了灾难性遗忘，并提高了泛化能力。我们的最佳模型能够在学习系统变化引起的新数据分布的同时保持准确性，而且相对于简单方法，整个数据序列的预测准确性提高了2倍。

    Machine learning-based performance models are increasingly being used to build critical job scheduling and application optimization decisions. Traditionally, these models assume that data distribution does not change as more samples are collected over time. However, owing to the complexity and heterogeneity of production HPC systems, they are susceptible to hardware degradation, replacement, and/or software patches, which can lead to drift in the data distribution that can adversely affect the performance models. To this end, we develop continually learning performance models that account for the distribution drift, alleviate catastrophic forgetting, and improve generalizability. Our best model was able to retain accuracy, regardless of having to learn the new distribution of data inflicted by system changes, while demonstrating a 2x improvement in the prediction accuracy of the whole data sequence in comparison to the naive approach.
    
[^103]: STEER: 语义转向扩展识别用于语音助手

    STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])

    [http://arxiv.org/abs/2310.16990](http://arxiv.org/abs/2310.16990)

    STEER是一个用于语音助手的语义转向扩展识别模型，通过训练数据集和启发式规则进行转向意图预测，并在实验中展现出了良好的性能。

    

    在语音助手系统的背景下，转向是指用户发出后续命令，试图引导或澄清之前的指令的现象。我们提出了STEER，一个转向检测模型，用于预测后续命令是否是用户企图转向之前指令的尝试。由于冷启动问题，构建用于转向案例的训练数据集带来了挑战。为了克服这个问题，我们开发了启发式规则来采样选择加入使用数据，近似正负样本而无需任何标注。我们的实验结果显示了识别转向意图的良好性能，在我们采样的数据上超过95%的准确率。此外，STEER结合我们的采样策略，在人工评估集上表现出了强大的零样本性能，有效地与真实的转向场景相匹配。除了仅依赖用户的转录作为输入，我们还引入了STEER+，这是模型的增强版本。

    In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ 
    
[^104]: 概率积分电路

    Probabilistic Integral Circuits. (arXiv:2310.16986v1 [cs.LG])

    [http://arxiv.org/abs/2310.16986](http://arxiv.org/abs/2310.16986)

    概率积分电路（PICs）是一种新的计算图语言，通过使用积分单元扩展了概率电路（PCs），可以处理连续潜变量，并提供了可追踪的推理和近似精度可调的层次连续混合物。

    

    连续潜变量（LVs）是许多生成模型的关键组成部分，因为它们允许建模具有不可数个组成部分的表达性混合物。与连续的LV模型相比，概率电路（PCs）是由输入、求和和乘积单元组成的计算图表示的层次离散混合物。与连续的LV模型不同，PCs提供了可处理的推理，但仅限于具有分类（即无序）状态的离散LVs。我们通过引入概率积分电路（PICs）来弥合这些模型类，PICs是一种新的计算图语言，它使用表示连续LV的积分单元扩展了PCs。首先，PICs是符号计算图，在简单的情况下，可以完全可追踪，其中可以进行解析积分。实际上，我们使用轻量级神经网络参数化PICs，提供一个不可解的层次连续混合物，可以使用数值积分以任意精度逼近大型PCs。在....

    Continuous latent variables (LVs) are a key ingredient of many generative models, as they allow modelling expressive mixtures with an uncountable number of components. In contrast, probabilistic circuits (PCs) are hierarchical discrete mixtures represented as computational graphs composed of input, sum and product units. Unlike continuous LV models, PCs provide tractable inference but are limited to discrete LVs with categorical (i.e. unordered) states. We bridge these model classes by introducing probabilistic integral circuits (PICs), a new language of computational graphs that extends PCs with integral units representing continuous LVs. In the first place, PICs are symbolic computational graphs and are fully tractable in simple cases where analytical integration is possible. In practice, we parameterise PICs with light-weight neural nets delivering an intractable hierarchical continuous mixture that can be approximated arbitrarily well with large PCs using numerical quadrature. On s
    
[^105]: 通过数据中心的AI重新构想合成表格数据生成：一个全面的基准

    Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark. (arXiv:2310.16981v1 [cs.LG])

    [http://arxiv.org/abs/2310.16981](http://arxiv.org/abs/2310.16981)

    本文通过整合数据中心的AI技术，解决了合成数据生成过程中反映真实世界数据复杂细微差异的问题。通过提出新的评估框架，研究对合成数据生成技术的成功和限制提供了关键见解。

    

    合成数据在训练机器学习模型时是现实世界数据有限或无法访问的替代品。然而，确保合成数据反映现实世界数据的复杂细微差异是一项具有挑战性的任务。本文通过探索整合数据中心的AI技术的潜力，来指导合成数据生成过程，解决了这个问题。此外，我们还揭示了在合成数据生成过程中忽视这些数据配置文件的常常被忽视的后果，尽管看似统计上高度真实。随后，我们提出了一个新的框架来评估集成数据配置文件以指导更具代表性的合成数据的创建。在一项实证研究中，我们评估了五个最先进的表格数据生成模型在十一个不同的表格数据集上的性能。研究结果对当前合成数据生成技术的成功和限制提供了关键见解。

    Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation -- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation te
    
[^106]: 无监督领域自适应语义分割与伪标签自我修正

    Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])

    [http://arxiv.org/abs/2310.16979](http://arxiv.org/abs/2310.16979)

    该论文提出了一种无监督领域自适应方法，通过伪标签自我修正和噪声像素定位来改进语义分割模型的性能。

    

    基于深度学习的语义分割解决方案在与训练时不同特征的数据上测试时性能显著下降。使用来自新域的注释数据来适应模型并不总是切实可行的。无监督领域自适应（UDA）方法在实际操作条件下部署这些模型是至关重要的。最近的最先进（SOTA）UDA方法采用教师-学生自训练方法，其中教师模型用于生成新数据的伪标签，进而指导学生模型的训练过程。尽管此方法取得了很大成功，但却存在伪标签在训练过程中传播的噪声问题。为了解决这个问题，我们提出了一个辅助伪标签精炼网络（PRN），用于在线精炼伪标签，并定位可能存在噪声的像素点的预测标签。

    Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the 
    
[^107]: 机器学习在临床疾病诊断中的意义：一项综述

    The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])

    [http://arxiv.org/abs/2310.16978](http://arxiv.org/abs/2310.16978)

    本综述研究了机器学习算法在临床疾病诊断中的应用，特别关注提高准确性和计算效率的优化。该研究发现，通过利用先进的ML和AI方法，可以增强医疗保健相关方的诊断和治疗能力。

    

    鉴于各种疾病机制的复杂性和患者症状的多样性，有效的疾病诊断在全球范围内仍然具有重要意义。为了解决这些挑战，研究人员、医生和患者正转向机器学习（ML），一门人工智能（AI）学科，来开发解决方案。通过利用先进的ML和AI方法，医疗保健相关方可以获得增强的诊断和治疗能力。然而，目前缺乏针对提高准确性和计算效率的机器学习算法的研究。本研究调查了机器学习算法在时间序列医疗指标中改善心率数据传输的能力，特别关注准确性和效率的优化。通过探索在医疗应用中使用的各种ML算法，综述介绍了基于ML的疾病诊断（MLBDD）的最新趋势和方法。考虑的因素包括算法的性能、数据处理和特征选择。

    The global need for effective disease diagnosis remains substantial, given the complexities of various disease mechanisms and diverse patient symptoms. To tackle these challenges, researchers, physicians, and patients are turning to machine learning (ML), an artificial intelligence (AI) discipline, to develop solutions. By leveraging sophisticated ML and AI methods, healthcare stakeholders gain enhanced diagnostic and treatment capabilities. However, there is a scarcity of research focused on ML algorithms for enhancing the accuracy and computational efficiency. This research investigates the capacity of machine learning algorithms to improve the transmission of heart rate data in time series healthcare metrics, concentrating particularly on optimizing accuracy and efficiency. By exploring various ML algorithms used in healthcare applications, the review presents the latest trends and approaches in ML-based disease diagnosis (MLBDD). The factors under consideration include the algorith
    
[^108]: 条件最优传输的高效神经网络方法及贝叶斯推断中的应用

    Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference. (arXiv:2310.16975v1 [stat.ML])

    [http://arxiv.org/abs/2310.16975](http://arxiv.org/abs/2310.16975)

    提出了两种神经网络方法来逼近静态和动态条件最优传输问题的解，实现了对条件概率分布的采样和密度估计，适用于贝叶斯推断。算法利用神经网络参数化传输映射以提高可扩展性。

    

    我们提出了两种神经网络方法，分别逼近静态和动态条件最优传输问题的解。这两种方法可以对条件概率分布进行采样和密度估计，这是贝叶斯推断中的核心任务。我们的方法将目标条件分布表示为可处理的参考分布的转换，因此属于测度传输的框架。在该框架中，COT映射是一个典型的选择，具有唯一性和单调性等可取的属性。然而，相关的COT问题在中等维度下计算具有挑战性。为了提高可扩展性，我们的数值算法利用神经网络对COT映射进行参数化。我们的方法充分利用了COT问题的静态和动态表达形式的结构。PCP-Map将条件传输映射建模为部分输入凸神经网络（PICNN）的梯度。

    We present two neural network approaches that approximate the solutions of static and dynamic conditional optimal transport (COT) problems, respectively. Both approaches enable sampling and density estimation of conditional probability distributions, which are core tasks in Bayesian inference. Our methods represent the target conditional distributions as transformations of a tractable reference distribution and, therefore, fall into the framework of measure transport. COT maps are a canonical choice within this framework, with desirable properties such as uniqueness and monotonicity. However, the associated COT problems are computationally challenging, even in moderate dimensions. To improve the scalability, our numerical algorithms leverage neural networks to parameterize COT maps. Our methods exploit the structure of the static and dynamic formulations of the COT problem. PCP-Map models conditional transport maps as the gradient of a partially input convex neural network (PICNN) and 
    
[^109]: 通过强化学习实现隐私保护的语言模型对齐

    Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])

    [http://arxiv.org/abs/2310.16960](http://arxiv.org/abs/2310.16960)

    本文研究了通过强化学习实现隐私保护的语言模型对齐问题，提出了一种新的差分隐私框架，并通过实验证明了其有效性和实用性。

    

    在预训练和用户部署之间，通过强化学习对齐大型语言模型(LLMs)已经成为培训指令跟踪模型(如ChatGPT)的主流策略。本文在强化学习的基础上，引入差分隐私(DP)来研究隐私保护的LLMs对齐问题。我们研究了两种主要的范式：(i)不需要人工干预的强化学习对齐方法(如积极评价生成)，(ii)通过人类反馈的强化学习对齐方法(RLHF)(如以人类首选方式进行摘要生成)。我们提出了一种新的DP框架来实现强化学习的对齐，并证明了其正确性。实验结果验证了我们方法的有效性，能够在确保强隐私保护的同时，提供有竞争力的实用性。

    Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
    
[^110]: 通过数据增强的参数高效微调改善少样本通用性安全分类器

    Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning. (arXiv:2310.16959v1 [cs.LG])

    [http://arxiv.org/abs/2310.16959](http://arxiv.org/abs/2310.16959)

    通过数据增强的参数高效微调方法在大型语言模型的文本安全分类器中改善了少样本通用性问题，相比于基准模型，在社交化学领域的F1得分提高了7-17%。

    

    随着大型语言模型的广泛采用，出现了新的安全问题和政策，现有的安全分类器无法很好地进行泛化。如果我们只观察到少量违反新安全规则的示例，如何构建一个分类器来检测违规行为？本文研究了基于语言模型的文本安全分类器的领域通用少样本学习的新设置。与之前的少样本学习工作不同，这些新的安全问题很难发现，而且我们不能选择少量示例。我们证明了现有的少样本学习技术在这种情况下表现不佳，相反，我们提出了基于参数高效微调（PEFT）的数据增强训练方法，以及基于先前现有规则中的相似示例进行数据增强。我们通过实验证明，我们的相似性数据增强+提示微调（DAPT）方法在社交化学领域的F1分数上始终比不依赖于数据增强或PEFT的基准模型提高了7-17%。

    As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry mor
    
[^111]: 转移分子基础模型用于聚合物性能预测

    Transferring a molecular foundation model for polymer property predictions. (arXiv:2310.16958v1 [cs.LG])

    [http://arxiv.org/abs/2310.16958](http://arxiv.org/abs/2310.16958)

    本研究表明，使用在小分子上预训练并在聚合物性质上微调的Transformer模型，在聚合物性能预测任务中达到与在增强数据集上训练的模型相当的准确性。

    

    基于Transformer的大型语言模型在药物开发和材料发现等领域的设计优化方面具有显著潜力。自我监督的Transformer模型预训练需要大规模的数据集，然而在聚合物科学等专业领域，这些数据集往往非常稀疏。目前聚合物的最新方法通过数据增强生成额外的样本，但不可避免地增加了计算成本。相比之下，小分子的大规模开源数据集可通过迁移学习解决数据稀缺性问题。本研究表明，使用在小分子上预训练并在聚合物性质上微调的Transformer模型，在一系列基准预测任务上达到与在增强聚合物数据集上训练的模型相当的准确性。

    Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populated in topical areas such as polymer science. State-of-the-art approaches for polymers conduct data augmentation to generate additional samples but unavoidably incurs extra computational costs. In contrast, large-scale open-source datasets are available for small molecules and provide a potential solution to data scarcity through transfer learning. In this work, we show that using transformers pretrained on small molecules and fine-tuned on polymer properties achieve comparable accuracy to those trained on augmented polymer datasets for a series of benchmark prediction tasks.
    
[^112]: 打破、模仿、修复：通过生成人类攻击提高鲁棒性

    Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])

    [http://arxiv.org/abs/2310.16955](http://arxiv.org/abs/2310.16955)

    本研究提出了一个对抗训练框架，使用有限的人类对手示例生成更有用的大规模对抗示例，有效提高了自然语言处理系统对于人类对手的鲁棒性。

    

    现实世界中的自然语言处理系统需要对抗人类对手具有鲁棒性。收集人类对手的示例进行训练是一种有效但昂贵的解决方案。另一方面，训练针对小扰动（如词替换）的合成攻击实际上并不能提高对抗人类对手的鲁棒性。本文提出了一个对抗训练框架，使用有限的人类对手示例来生成更有用的大规模对抗示例。我们通过ANLI和仇恨言论检测基准数据集进行实验，这两个数据集是通过迭代的对抗人类和模型的过程收集得到的。与仅在观察到的人类攻击上进行训练相比，也在我们的合成对抗示例上进行训练可以提高模型对未来回合的鲁棒性。在ANLI上，我们看到了对当前攻击集的准确率提升（44.1% -> 50.1%），以及对两个未见过的人类生成攻击回合的准确率提升（32.5% -> 43%）。

    Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\,\to\,$43
    
[^113]: Causal Q-Aggregation for CATE Model Selection（CATE模型选择中的因果Q集成）

    Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])

    [http://arxiv.org/abs/2310.16945](http://arxiv.org/abs/2310.16945)

    该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率

    

    准确估计条件平均处理效应（CATE）是个性化决策的核心。尽管有大量用于CATE估计的模型，但由于因果推断的基本问题，模型选择是一项非常棘手的任务。最近的实证工作提供了有利于具有双重鲁棒性质的代理损失度量和模型集成的证据。然而，对于这些模型的理论理解还不够。直接应用先前的理论工作会由于模型选择问题的非凸性而导致次优的预测模型选择率。我们提供了现有主要CATE集成方法的遗憾率，并提出了一种基于双重鲁棒损失的Q集成的新的CATE模型集成方法。我们的主要结果表明，因果Q集成在预测模型选择的遗憾率上达到了统计上的最优值为$\frac{\log(M)}{n}$（其中$M$为模型数，$n$为样本数），加上高阶估计误差项

    Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
    
[^114]: Zephyr: 直接蒸馏语言模型对齐

    Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])

    [http://arxiv.org/abs/2310.16944](http://arxiv.org/abs/2310.16944)

    本论文提出了一种直接蒸馏的语言模型对齐方法，使用AI反馈数据进行优化，在聊天任务上显著提高了意图对齐的效果，该方法只需要几个小时的训练时间且无需人工注释，实验证明在7B参数模型上超过了现有最好的开放访问模型。

    

    我们的目标是生成一个与用户意图对齐的较小型语言模型。之前的研究表明，对较大的模型应用蒸馏的监督微调能显著提高任务准确性；然而，这些模型没有对齐，即它们不能很好地响应自然提示。为了蒸馏这个属性，我们尝试使用来自AI反馈（AIF）的偏好数据。从一个由教师模型排名的输出数据集开始，我们应用蒸馏的直接偏好优化（dDPO）来学习一个具有显著改进意图对齐的聊天模型。这种方法只需要几个小时的训练时间，在微调过程中无需任何额外的采样。最终结果Zephyr-7B在7B参数模型的聊天基准上取得了最先进的结果，并且无需人工注释。特别是，在MT-Bench上的结果显示，Zephyr-7B超越了最好的开放访问RLHF模型Llama2-Chat-70B。系统的代码、模型、数据和教程都可以获取。

    We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are availabl
    
[^115]: 探索有限能力异质群体机器人行为发现方法

    Exploring Behavior Discovery Methods for Heterogeneous Swarms of Limited-Capability Robots. (arXiv:2310.16941v1 [cs.RO])

    [http://arxiv.org/abs/2310.16941](http://arxiv.org/abs/2310.16941)

    这篇论文研究了在具有有限能力的功能异质群体机器人中确定紧急行为的问题，结果表明，之前的方法不能发现许多有趣的行为，而迭代的人机协作发现过程比随机搜索、群体化学和自动化行为发现发现了更多行为。

    

    我们研究了在具有有限能力的功能异质群体机器人中确定可能出现的紧急行为的问题。之前的工作考虑了对于同质群体的行为搜索，并提出使用新奇搜索以及手动指定或学习的行为空间进行聚类，以返回给用户一个紧急行为的分类法。在本文中，我们尝试更好地理解新奇搜索的作用以及使用聚类来发现新奇紧急行为的有效性。通过大量的实验和削减分析，我们在异质群体中研究了表示、进化搜索和各种聚类方法在搜索新奇行为方面的影响。我们的结果表明，之前的方法不能发现许多有趣的行为，而迭代的人机协作发现过程比随机搜索、群体化学和自动化行为发现发现了更多行为。

    We study the problem of determining the emergent behaviors that are possible given a functionally heterogeneous swarm of robots with limited capabilities. Prior work has considered behavior search for homogeneous swarms and proposed the use of novelty search over either a hand-specified or learned behavior space followed by clustering to return a taxonomy of emergent behaviors to the user. In this paper, we seek to better understand the role of novelty search and the efficacy of using clustering to discover novel emergent behaviors. Through a large set of experiments and ablations, we analyze the effect of representations, evolutionary search, and various clustering methods in the search for novel behaviors in a heterogeneous swarm. Our results indicate that prior methods fail to discover many interesting behaviors and that an iterative human-in-the-loop discovery process discovers more behaviors than random search, swarm chemistry, and automated behavior discovery. The combined discov
    
[^116]: 使用雅可比图谱的早期-晚期多模态数据融合方法诊断阿尔茨海默病

    Diagnosing Alzheimer's Disease using Early-Late Multimodal Data Fusion with Jacobian Maps. (arXiv:2310.16936v1 [cs.CV])

    [http://arxiv.org/abs/2310.16936](http://arxiv.org/abs/2310.16936)

    提出了一种早期-晚期融合方法，利用卷积神经网络进行自动特征提取并使用随机森林，在小数据集上具备竞争力的性能。此外，还引入了一个适应独特特征的鲁棒预处理流程。

    

    阿尔茨海默病（AD）是一种常见且具有破坏性的神经退行性疾病，影响着庞大的老年人群体。在所有的无症状和症状出现阶段检测AD对于早期干预和治疗至关重要。一项活跃的研究方向是探索机器学习方法，利用多模态数据融合胜过医学扫描的人工检查。然而，现有的多模态融合模型存在一些限制，包括冗余计算、复杂的架构和对缺失数据简单处理。此外，医学扫描的预处理流程仍然不够详细，并且很少为个体主题进行优化。在本文中，我们提出了一种高效的早期-晚期融合（ELF）方法，该方法利用卷积神经网络进行自动特征提取，并使用随机森林在小数据集上具备竞争力的性能。此外，我们引入了一个适应独特特征的鲁棒预处理流程

    Alzheimer's disease (AD) is a prevalent and debilitating neurodegenerative disorder impacting a large aging population. Detecting AD in all its presymptomatic and symptomatic stages is crucial for early intervention and treatment. An active research direction is to explore machine learning methods that harness multimodal data fusion to outperform human inspection of medical scans. However, existing multimodal fusion models have limitations, including redundant computation, complex architecture, and simplistic handling of missing data. Moreover, the preprocessing pipelines of medical scans remain inadequately detailed and are seldom optimized for individual subjects. In this paper, we propose an efficient early-late fusion (ELF) approach, which leverages a convolutional neural network for automated feature extraction and random forests for their competitive performance on small datasets. Additionally, we introduce a robust preprocessing pipeline that adapts to the unique characteristics
    
[^117]: MimicTouch: 使用多模态触觉反馈学习人类的控制策略

    MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])

    [http://arxiv.org/abs/2310.16917](http://arxiv.org/abs/2310.16917)

    MimicTouch是一种新的框架，能够模仿人类的触觉引导控制策略，通过收集来自人类示范者的多模态触觉数据集，来学习并执行复杂任务。

    

    在机器人技术和人工智能领域，触觉处理的整合变得越来越重要，特别是在学习执行像对准和插入这样复杂任务时。然而，现有研究主要依赖机器人遥操作数据和强化学习，忽视了人类受触觉反馈引导下的控制策略所提供的丰富见解。为了利用人类感觉，现有的从人类学习的方法主要利用视觉反馈，常常忽视了人类本能地利用触觉反馈完成复杂操作的宝贵经验。为了填补这一空白，我们引入了一种新框架"MimicTouch"，模仿人类的触觉引导控制策略。在这个框架中，我们首先从人类示范者那里收集多模态触觉数据集，包括人类触觉引导的控制策略来完成任务。接下来的步骤涉及指令的传递，其中机器人通过模仿人类的触觉引导策略来执行任务。

    In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instruc
    
[^118]: 基于Transformer的大气密度预测

    Transformer-based Atmospheric Density Forecasting. (arXiv:2310.16912v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.16912](http://arxiv.org/abs/2310.16912)

    本研究提出了一个基于Transformer的非线性架构，用于大气密度预测，以改进之前的线性传播方法。这对于空间态势感知具有重要意义。

    

    随着2025年太阳周期的高峰期临近和单个地磁暴对居住空间物体（RSOs）轨道的显著改变能力，对大气密度预测技术对于空间态势感知至关重要。尽管之前已经使用线性数据驱动方法（例如具有控制的动力模态分解（DMDc））来预测大气密度，但基于深度学习的预测具有捕捉数据中非线性的能力。通过从历史大气密度数据中学习多层权重，将当前大气密度状态与下一个时间步的大气密度状态的控制输入之间的映射中捕获数据集中的长期依赖关系。本研究通过为大气密度预测开发非线性的基于Transformer的架构，改进了之前的线性传播方法。

    As the peak of the solar cycle approaches in 2025 and the ability of a single geomagnetic storm to significantly alter the orbit of Resident Space Objects (RSOs), techniques for atmospheric density forecasting are vital for space situational awareness. While linear data-driven methods, such as dynamic mode decomposition with control (DMDc), have been used previously for forecasting atmospheric density, deep learning-based forecasting has the ability to capture nonlinearities in data. By learning multiple layer weights from historical atmospheric density data, long-term dependencies in the dataset are captured in the mapping between the current atmospheric density state and control input to the atmospheric density state at the next timestep. This work improves upon previous linear propagation methods for atmospheric density forecasting, by developing a nonlinear transformer-based architecture for atmospheric density forecasting. Empirical NRLMSISE-00 and JB2008, as well as physics-based
    
[^119]: MACP：高效的合作感知模型适应

    MACP: Efficient Model Adaptation for Cooperative Perception. (arXiv:2310.16870v1 [cs.CV])

    [http://arxiv.org/abs/2310.16870](http://arxiv.org/abs/2310.16870)

    本文提出了MACP框架，通过将单个预训练模型配备合作能力来提高连接和自动驾驶车辆的感知能力。通过冻结大部分参数并添加几个轻量级模块，该框架能够有效利用合作观测，并在模拟和真实环境中胜过其他最先进的方法。

    

    车辆间的通信极大地增强了连接和自动驾驶车辆的感知能力，通过实现信息共享以"穿透"遮挡物，从而显著提高性能。然而，从头开始开发和训练复杂的多智能体感知模型可能昂贵且不必要，因为现有的单智能体模型展现出了令人瞩目的泛化能力。在本文中，我们提出了一个名为MACP的新框架，将单个预训练模型配备了合作能力。我们通过确定从单智能体到合作环境的关键挑战，通过冻结大部分参数并添加几个轻量级模块来使模型适应。我们在实验中证明，所提出的框架可以有效利用合作观测，并在模拟和真实环境中胜过其他最先进的方法。

    Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to "see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception
    
[^120]: 基于可解释性深度学习和生成式数据增强的精神分裂症诊断方法研究

    An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])

    [http://arxiv.org/abs/2310.16867](http://arxiv.org/abs/2310.16867)

    本研究提出了一种基于可解释性深度学习和生成式数据增强的精神分裂症诊断方法，通过使用卷积神经网络进行初步诊断，并利用WGAN-GP和VAE生成的合成数据集进行增强，显著提高了诊断准确性和模型可解释性。

    

    本研究利用深度学习的方法，基于脑电图脑部记录，进行精神分裂症的自动诊断。该方法利用生成式数据增强技术，提高了诊断的准确性。为了能够利用时频特征，从原始信号中提取了频谱图。在探索了多种神经网络结构设置后，选择了适合的卷积神经网络(CNN)进行初步诊断。随后，利用Wasserstein GAN with Gradient Penalty(WGAN-GP)和变分自动编码器(VAE)生成了两个不同的合成数据集，以增强初始数据集并解决过拟合问题。使用VAE生成的增强数据集在准确度上提高了3.0％，达到了99.0％，同时得到了更低的损失值和更快的收敛速度。最后，我们使用局部可解释的模型无关解释(LIME)方法解决了对于黑盒模型的信任缺乏问题。

    In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LI
    
[^121]: 带有自编码和自回归的通用点模型

    General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])

    [http://arxiv.org/abs/2310.16861](http://arxiv.org/abs/2310.16861)

    提出了一种通用点模型（GPM），它在点云变换器中无缝整合了自编码和自回归任务。GPM通过掩码填充任务增强了自编码中的掩码预测，并在点云理解和生成任务中展现出竞争力强大的结果。

    

    大型语言模型的预训练架构包括多种类型，包括自编码模型、自回归模型和编码器-解码器模型。我们认为，只要将任何形式的模态量化为离散的符号，它都有可能从大型语言模型中获益。受到GLM的启发，我们提出了一种名为通用点模型（GPM）的模型，它无缝地将自编码和自回归任务整合到点云变换器中。该模型具有多功能性，可以用于下游点云表示任务的微调，以及无条件和有条件的生成任务。GPM通过各种形式的掩码填充任务增强了自编码中的掩码预测，在点云理解方面表现出更好的性能。此外，GPM在无条件点云生成任务中展现出竞争力强大的结果，甚至通过修改输入的条件信息展示了有条件生成任务的潜力。

    The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared t
    
[^122]: 通过拓扑优化卷积神经网络改进阿尔茨海默病MRI图像分析

    Improvement in Alzheimer's Disease MRI Images Analysis by Convolutional Neural Networks Via Topological Optimization. (arXiv:2310.16857v1 [eess.IV])

    [http://arxiv.org/abs/2310.16857](http://arxiv.org/abs/2310.16857)

    通过拓扑优化卷积神经网络，本研究改善了阿尔茨海默病MRI图像质量，提高了分类精度，对阿尔茨海默病的诊断具有潜在的转变性影响。

    

    本研究通过傅里叶拓扑优化技术，改善MRI图像质量，从而提高卷积神经网络对阿尔茨海默病的分类精度。通过优化边界增强、对比度调整和亮度调整等技术，改善了脑部结构的清晰度，减少了噪声并提高了对比度。使用VGG16、ResNet50、InceptionV3和Xception等CNN架构进行后优化分析，结果表明性能大幅度提升。综上所述，将傅里叶拓扑优化与CNN相结合，对阿尔茨海默病的精细分类具有潜在的转变性影响，有助于其诊断参数的改进。

    This research underscores the efficacy of Fourier topological optimization in refining MRI imagery, thereby bolstering the classification precision of Alzheimer's Disease through convolutional neural networks. Recognizing that MRI scans are indispensable for neurological assessments, but frequently grapple with issues like blurriness and contrast irregularities, the deployment of Fourier topological optimization offered enhanced delineation of brain structures, ameliorated noise, and superior contrast. The applied techniques prioritized boundary enhancement, contrast and brightness adjustments, and overall image lucidity. Employing CNN architectures VGG16, ResNet50, InceptionV3, and Xception, the post-optimization analysis revealed a marked elevation in performance. Conclusively, the amalgamation of Fourier topological optimization with CNNs delineates a promising trajectory for the nuanced classification of Alzheimer's Disease, portending a transformative impact on its diagnostic para
    
[^123]: 通过解决嵌入式FPGA中LSTM单元的吞吐量瓶颈，增强能效

    Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])

    [http://arxiv.org/abs/2310.16842](http://arxiv.org/abs/2310.16842)

    本研究通过优化LSTM单元，提出了一种在终端设备上进行能效推断的新方法。以交通速度预测为例，优化后的LSTM单元在FPGA上实现了较快的推断速度和较低的能耗，相比现有方法提高了吞吐量和能效。

    

    在物联网中处理传感器数据，嵌入式深度学习对于一维数据非常重要。过去，经常使用CNN因为它们对于特殊的嵌入式硬件比如FPGA来说很容易优化。本研究提出了一种针对在终端设备上进行能效推断的新型LSTM单元优化方法。以交通速度预测为案例研究，优化后的LSTM单元的简单LSTM模型在FPGA XC7S15（来自Spartan-7系列）上每秒可实现17534个推断，仅消耗每个推断3.8微焦耳的能量。相比现有方法，它的吞吐量至少提高了5.4倍，能效提高了1.37倍。

    To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
    
[^124]: 用于流星监测的深度机器学习：具有迁移学习和梯度加权类激活映射的进展

    Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping. (arXiv:2310.16826v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2310.16826](http://arxiv.org/abs/2310.16826)

    本研究开发了一个使用深度学习技术进行流星检测的自动化流水线，能够有效地在包含静态元素的图像中检测流星，并利用梯度加权类激活映射方法准确地定位流星的位置。

    

    近几十年来，光学探测系统在流星研究中的应用大幅增加，导致数据量大幅增加。自动化的流星检测工具对于研究连续的流星入射通量、寻找新鲜陨石和更好地了解我们的太阳系至关重要。关于流星检测，传统上通过手工进行流星和非流星图像的假阳性区分，这在时间上十分耗费。为了解决这个问题，我们开发了一个完全自动化的流水线，利用卷积神经网络（CNN）来分类候选流星检测。我们的新方法能够即使在包含云、月亮和建筑等静态元素的图像中，也能够检测到流星。为了准确地在每个帧中定位流星，我们采用了梯度加权类激活映射（Grad-CAM）技术。这种方法可以通过多通道梯度信号来辅助识别感兴趣的区域。

    In recent decades, the use of optical detection systems for meteor studies has increased dramatically, resulting in huge amounts of data being analyzed. Automated meteor detection tools are essential for studying the continuous meteoroid incoming flux, recovering fresh meteorites, and achieving a better understanding of our Solar System. Concerning meteor detection, distinguishing false positives between meteor and non-meteor images has traditionally been performed by hand, which is significantly time-consuming. To address this issue, we developed a fully automated pipeline that uses Convolutional Neural Networks (CNNs) to classify candidate meteor detections. Our new method is able to detect meteors even in images that contain static elements such as clouds, the Moon, and buildings. To accurately locate the meteor within each frame, we employ the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. This method facilitates the identification of the region of interest by mul
    
[^125]: 多尺度扩散去噪平滑

    Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])

    [http://arxiv.org/abs/2310.16779](http://arxiv.org/abs/2310.16779)

    本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。

    

    随着最近的扩散模型，随机平滑已成为少数几个切实可行的方法之一，为大规模预训练模型提供对抗鲁棒性。具体而言，可以通过简单的“去噪和分类”流程，即所谓的去噪平滑，在任何分类器上执行随机平滑，前提是有一个准确的去噪器可用，比如扩散模型。在本文中，我们研究了去噪平滑的准确度和认证鲁棒性之间的权衡：例如，我们质疑哪种扩散模型的表示形式能够最大化去噪平滑的认证鲁棒性。我们考虑了一个新的目标，旨在实现共同噪声水平下平滑分类器的鲁棒性，在共享扩散模型上进行精细调整，同时也为其认证鲁棒性补偿准确度的成本提供了一种新途径。

    Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
    
[^126]: 驾驶通过概念阻塞：解开可解释性瓶颈

    Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])

    [http://arxiv.org/abs/2310.16639](http://arxiv.org/abs/2310.16639)

    本论文提出了一种使用概念阻塞作为控制命令预测和用户车辆行为解释的方法，通过学习人类可理解的概念层解释顺序驾驶场景，同时获得竞争性性能和可解释性。

    

    在可解释的机器学习中，概念阻塞模型通过利用一组人为定义的概念在模型中编码信息，取得了成功。在人类辅助或自动驾驶的背景下，可解释性模型可以帮助用户接受和理解自动驾驶车辆所做的决策，并用于合理化和解释驾驶员或车辆的行为。我们提出了一种新的方法，使用概念阻塞作为控制命令预测和用户车辆行为解释的可视特征。我们学习了一个人类可理解的概念层，用于解释顺序驾驶场景同时学习车辆的控制命令。这种方法可以用来确定人类（或自动驾驶车辆）对首选缝隙或转向命令的改变是否由外部刺激或偏好的改变所引导。在我们的模型设置中，我们获得了具有竞争性的性能，同时获得了可解释性。

    Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup
    
[^127]: 适应密度比估计的协变量偏移适应

    Covariate Shift Adaptation Robust to Density-Ratio Estimation. (arXiv:2310.16638v1 [stat.ME])

    [http://arxiv.org/abs/2310.16638](http://arxiv.org/abs/2310.16638)

    该论文研究了在协变量偏移下的密度比估计的罕见问题，提出了一种适应性方法来减轻密度比估计的偏差对模型的影响。

    

    在一种情况下，我们可以访问具有协变量和结果的训练数据，而测试数据只包含协变量。在这种情况下，我们的主要目标是预测测试数据中缺失的结果。为了实现这个目标，我们在协变量偏移下训练参数回归模型，其中训练数据和测试数据之间的协变量分布不同。对于这个问题，现有研究提出了通过使用密度比的重要性加权来进行协变量偏移适应的方法。该方法通过对训练数据损失进行加权平均，每个权重是训练数据和测试数据之间的协变量密度比的估计，以近似测试数据的风险。尽管它允许我们获得一个最小化测试数据风险的模型，但其性能严重依赖于密度比估计的准确性。此外，即使密度比可以一致地估计，密度比的估计误差也会导致回归模型的估计器产生偏差。

    Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's 
    
[^128]: 跨特征对比损失用于异构数据的分散式深度学习

    Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v1 [cs.LG])

    [http://arxiv.org/abs/2310.15890](http://arxiv.org/abs/2310.15890)

    本文提出了一种用于异构数据的分散式学习方法，通过跨特征对比损失实现数据无关知识蒸馏，实验结果表明该方法在各种计算机视觉任务上取得了优越性能。

    

    当前最先进的分散式学习算法大多数假设数据分布是独立同分布（IID）。然而，在实际场景中，分散式数据集在代理之间可以具有显著的异构数据分布。在这项工作中，我们提出了一种新颖的针对异构数据的分散式学习方法，通过交叉特征上的无数据知识蒸馏和对比损失来提高性能。对于一对相邻代理，跨特征是从一个代理的数据获取的特征（即最后一个隐藏层的激活）关于另一个代理的模型参数。我们通过一系列详尽的实验在各种计算机视觉数据集（CIFAR-10、CIFAR-100、Fashion MNIST 和 ImageNet）、模型架构和网络拓扑上展示了所提出技术的有效性。我们的实验结果表明，所提出的方法在性能上取得了更好的表现。

    The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (
    
[^129]: COPF: 通过最优策略拟合实现持续学习人类偏好

    COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])

    [http://arxiv.org/abs/2310.15694](http://arxiv.org/abs/2310.15694)

    通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。

    

    强化学习通过人类反馈（RLHF）的技术是改善预训练语言模型（LM）以符合人类偏好的常用方法。然而，当前基于RLHF的LM在引入新的查询或反馈时需要完全重新训练，这是一项具有挑战性的任务，因为人类偏好在不同领域或任务之间可能会有所变化。由于所需的时间和计算资源以及与数据隐私相关的问题，重新训练LM在许多现实世界的情况下存在实际困难。为了解决这个限制，我们提出了一种新的方法，称为持续最优策略拟合（COPF），其中我们使用蒙特卡罗法估计一系列最优策略，然后通过函数正则化不断拟合策略序列。COPF包含一个单一的学习阶段，不需要复杂的强化学习。

    The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
    
[^130]: 在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练

    Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14814](http://arxiv.org/abs/2310.14814)

    本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。

    

    自训练是半监督学习中一种众所周知的方法。它包括对模型自信度高的未标记数据进行伪标签分配，并将其视为标记样本进行处理。对于神经网络，通常使用softmax预测概率作为自信度度量，尽管已知它们对错误预测也过于自信。当数据标注受到某种约束时，这种现象尤为明显，即样本选择偏差存在。为了解决这个问题，我们提出了一种新的自信度度量方法，称为$\mathcal{T}$-相似度，它基于线性分类器的集成预测多样性。我们通过研究稳定点并描述单个成员的多样性与其性能之间的关系来提供我们方法的理论分析。我们通过对三种不同伪标签策略的实验验证了我们自信度度量的好处。

    Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
    
[^131]: 通过信息外推实现多样化的异常值曝光进行外分布检测

    Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation. (arXiv:2310.13923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13923](http://arxiv.org/abs/2310.13923)

    本文提出了一种名为“多样化的异常值曝光（DivOE）”的框架，通过生成多样化的、信息化的辅助异常值来进行有效的OOD检测。这种方法通过信息外推的方式合成了更多的异常值，从而改进了传统的异常值检测方法。

    

    外分布（OOD）检测对于在现实世界应用中部署可靠的机器学习模型至关重要。在异常值曝光方面的最新进展已经显示出通过以信息化采样的辅助异常值对模型进行微调在OOD检测方面具有良好的结果。然而，先前的方法假设收集到的异常值可以足够大且代表性地覆盖ID和OOD数据之间的边界，这可能是不可行且具有挑战性的。在这项工作中，我们提出了一种新的框架，即“多样化的异常值曝光”（DivOE），通过利用给定的辅助异常值进行基于信息外推的有效OOD检测。具体而言，DivOE引入了一种新的学习目标，通过在训练过程中显式地合成更多信息化的异常值来使辅助分布多样化。它利用了多步优化方法来生成超出原始异常值的新异常值，这与许多异常值曝光的变体兼容。

    Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier ex
    
[^132]: DPM-Solver-v3: 使用经验模型统计改进的扩散ODE求解器

    DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v1 [cs.CV])

    [http://arxiv.org/abs/2310.13268](http://arxiv.org/abs/2310.13268)

    本论文提出了DPM-Solver-v3，一个基于经验模型统计的新型快速ODE求解器，用于优化扩散概率模型的采样效率，并提出了一种新的参数化方法以减小ODE解的离散化误差。同时，引入了多步方法和预测-校正框架来进一步改善采样质量。

    

    扩散概率模型（DPMs）在生成高保真度图像方面表现出色，但采样效率低下。最近的研究通过提出利用DPMs的特定ODE形式的快速ODE求解器来加速采样过程。然而，它们在推理过程中高度依赖特定参数化（如噪声/数据预测），这可能并不是最佳选择。在本研究中，我们提出了一种新的公式，以获得在采样过程中最佳参数化，以最小化ODE解的一阶离散化错误。基于这种公式，我们引入了几个在预训练模型上高效计算的系数，并提出了一个名为DPM-Solver-v3的新快速ODE求解器，我们称之为“经验模型统计”。我们进一步结合了多步方法和预测-校正框架，并提出了一些技术来改善在少量函数评估（NFE）或大规模数据集上的样本质量。

    Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call \textit{empirical model statistics}. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or
    
[^133]: 为联邦图神经网络提供结构感知群体公平性

    Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])

    [http://arxiv.org/abs/2310.12350](http://arxiv.org/abs/2310.12350)

    本论文提出了一种名为F2GNN的方法，它旨在增强联邦图神经网络的群体公平性，解决了在联邦学习中减轻偏见的新挑战。

    

    图神经网络（GNN）广泛应用于不同领域的各种图数据处理和分析任务。由于隐私和监管限制，对集中式图数据进行训练可能不可行。因此，联邦学习（FL）成为解决这一挑战的一种趋势性解决方案。然而，由于GNN可能从训练数据中继承历史偏见并导致歧视性预测，在分布式环境中，局部模型的偏见很容易传播到全局模型，这给在联邦GNN中减轻偏见带来了新的挑战。为了解决这一问题，我们提出了F2GNN，一种增强联邦GNN群体公平性的方法。由于偏见可能来自数据和学习算法，F2GNN旨在在联邦环境下减少这两种类型的偏见。

    Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
    
[^134]: DSAC-T: 带有三个改进的分布式软角色扮演者—评论者

    DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05858](http://arxiv.org/abs/2310.05858)

    本论文介绍了DSAC-T，通过评论者梯度调整、双值分布学习和基于方差的目标回报裁剪等三个改进对标准DSAC进行了改进，解决了标准DSAC存在的不稳定学习过程和对任务特定奖励缩放的问题，提高了算法的性能和适应性。

    

    强化学习在处理复杂的决策和控制任务方面已经被证明非常有效。然而，常见的无模型的强化学习方法往往面临严重的性能下降问题，这是由于众所周知的过估计问题所引起的。作为对这个问题的回应，我们最近引入了一种离线策略的强化学习算法，称为分布式软角色扮演者评论者（DSAC或DSAC-v1），它通过学习连续的高斯值分布来有效提高值估计的准确性。然而，标准DSAC也存在一些缺点，包括时而不稳定的学习过程和对任务特定的奖励缩放的需求，这可能会阻碍其在一些特殊任务中的整体性能和适应性。本文进一步引入了三个对标准DSAC的重要改进，以解决这些问题。这些改进包括评论者梯度调整、双值分布学习和基于方差的目标回报裁剪。修改后的强化学习算法称为DSAC-T。

    Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
    
[^135]: 方差减少的 Halpern 迭代在有限和单调包含中的应用

    Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions. (arXiv:2310.02987v1 [cs.LG])

    [http://arxiv.org/abs/2310.02987](http://arxiv.org/abs/2310.02987)

    提出了使用方差减少的 Halpern 迭代来优化有限和单调包含问题的求解过程，具有更好的复杂度保证。

    

    依赖对抗稳健性或多智能体环境的机器学习方法引发了解决博弈均衡问题的需求。在这些应用中，具有可计算逼近误差的方法非常理想，因为它们提供了可验证的终止准则。在这些应用的基础上，我们研究了模拟广泛类别均衡问题的有限和单调包含问题。我们的主要贡献是改进了经典的 Halpern 迭代方法，利用方差减少获得改进的算法复杂度保证，在有限和的 $n$ 个组成操作符中，“平均”地是互补协同或Lipschitz连续和单调，参数为 $L$。我们的方法的结果预测了最后的迭代和一个（compu）

    Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which $n$ component operators in the finite sum are ``on average'' either cocoercive or Lipschitz continuous and monotone, with parameter $L$. The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (compu
    
[^136]: MathVista: 用GPT-4V、Bard和其他大型多模态模型评估视觉场景中的数学推理能力

    MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02255](http://arxiv.org/abs/2310.02255)

    本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。

    

    大型语言模型（LLMs）和大型多模态模型（LMMs）在许多任务和领域中展示出令人印象深刻的问题解决能力，但它们在视觉环境中的数学推理能力尚未得到系统研究。为了弥补这一差距，我们提出了MathVista，这是一个综合了不同数学和视觉任务的挑战的基准测试。它包含了6141个例子，其中有28个现有的多模态数据集和3个新创建的数据集（即IQTest、FunctionQA和PaperQA）。完成这些任务需要精细的、深入的视觉理解和组合推理，这些都是当前最先进的基础模型所面临的困难。通过MathVista，我们对12个著名的基础模型进行了全面的定量评估。表现最好的GPT-4V模型的整体准确率为49.9%，明显优于第二名的Bard模型，相差15.1%。我们的深入分析揭示了

    Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
    
[^137]: 利用选择的能力优化决策树学习

    Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])

    [http://arxiv.org/abs/2310.01551](http://arxiv.org/abs/2310.01551)

    该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。

    

    我们提出了一种简单的对标准和经验成功的决策树学习算法（如ID3、C4.5和CART）进行推广的方法。这些算法凭借贪婪的特性，在机器学习领域发挥了关键作用：它们通过迭代地基于最佳属性进行划分来构建决策树。我们的算法Top-$k$则考虑$k$个最佳属性作为可能的划分，而不仅仅是单个最佳属性。我们通过理论和实证研究展示了这个简单推广方法的优势。首先，我们证明了一个“贪婪层次定理”，对于每个$k \in \mathbb{N}$，Top-$(k+1)$比Top-$k$更加强大：在某些数据分布下，前者可以达到$1-\varepsilon$的准确率，而后者只能达到$\frac1{2}+\varepsilon$的准确率。然后，我们通过大量实验表明，Top-$k$算法在决策树学习中优于两种主要方法：经典贪婪算法和较新的“最优决策树”算法。

    We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
    
[^138]: SpatialRank: 基于时空数据的城市事件排名与NDCG优化

    SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00270](http://arxiv.org/abs/2310.00270)

    这篇论文提出了一种名为SpatialRank的新颖空间事件排名方法，通过基于时空数据的NDCG优化来解决城市事件排名问题。

    

    城市事件排名问题旨在预测未来事件（如交通事故和犯罪事件）的风险最高的前k个地点。这个问题对公共安全和城市管理非常重要，尤其是在资源有限的情况下。然而，由于地点之间复杂而动态的时空相关性，空间中城市事件的不均匀分布，以及正确对具有相似特征的附近地点进行排名的困难，这个问题很具挑战性。前人的研究主要旨在准确预测所有地点的实际风险得分或事件计数。由于预测错误，由此得到的排名通常质量较低。学习排序方法直接优化诸如标准化折扣累积增益（NDCG）之类的指标，但不能处理地点之间存在的时空自相关性。在本文中，我们通过提出一种名为SpatialRank的新颖空间事件排名方法来弥合这一差距。

    The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
    
[^139]: 机器学习中的一次出训练数据的可辨识性分析

    Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])

    [http://arxiv.org/abs/2309.17310](http://arxiv.org/abs/2309.17310)

    这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。

    

    我们引入了一个新的分析框架，用于量化机器学习算法在训练集中包含少量数据点后输出分布的变化，我们将这个概念定义为一次出训练数据的可辨识性(LOOD)。这个问题对于衡量机器学习中的数据记忆和信息泄漏以及训练数据对模型预测的影响至关重要。我们使用高斯过程模型来建模机器学习算法的随机性，并通过对成员推断攻击使用广泛的经验分析验证了LOOD。我们的理论框架使我们能够研究信息泄漏的原因以及泄漏程度高的位置。例如，我们分析了激活函数对数据记忆的影响。此外，我们的方法允许我们优化...

    We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim
    
[^140]: 在连续控制中的噪声邻域中的策略优化

    Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])

    [http://arxiv.org/abs/2309.14597](http://arxiv.org/abs/2309.14597)

    本论文提供了新的视角，研究了连续控制中深度强化学习智能体性能不稳定的原因。通过对回报景观进行分析，发现了策略空间中的失败区域和策略品质的隐藏维度。此外，提出了一种分布感知的方法，改善了策略的鲁棒性。

    

    已知连续控制问题中的深度强化学习智能体在性能上会出现显著的不稳定性。本文从研究策略和回报之间的映射即回报景观的角度为这些行为提供了新的视角。我们发现，流行的算法在这个景观的噪声邻域中穿行，一个策略参数的单次更新会导致回报在很大范围内变化。通过对这些回报进行分布处理，我们对景观进行了映射，描述了策略空间中容易产生失败的区域，并揭示了策略品质的隐藏维度。我们还展示了景观的惊人结构，通过在参数空间中找到简单的路径来提高策略的稳定性。最后，我们开发了一个分布感知的方法，通过避开噪声邻域来提高策略的鲁棒性。综上所述，我们的结果为优化和评估提供了新的洞察。

    Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, eva
    
[^141]: 通过条件置换进行统计有效的变量重要性评估

    Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])

    [http://arxiv.org/abs/2309.07593](http://arxiv.org/abs/2309.07593)

    本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。

    

    在使用复杂学习器（如深度神经网络）处理大规模数据时，变量重要性评估已成为机器学习应用中的关键步骤。目前，基于移除的重要性评估是参考方法，特别是在需要统计保证来验证变量包含性时。通常，它们使用变量置换方案来实现。然而，这些方法在存在协变量之间的相关性时容易将不重要的变量误识别为重要变量。本文提出了一种系统方法来研究条件置换重要性（Conditional Permutation Importance，CPI），它是模型无关且计算效率高的方法，并提供了基准测试，用于评估当前最先进的变量重要性估计器。理论和实证结果表明，CPI通过提供准确的I型错误控制，克服了标准置换重要性的局限性。当与深度神经网络一起使用时，CPI始终显示出最高的准确性。

    Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
    
[^142]: 通过平坦极小值和对抗鲁棒性解释激活稀疏性的理论解释

    Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])

    [http://arxiv.org/abs/2309.03004](http://arxiv.org/abs/2309.03004)

    提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。

    

    最近对MLP层中的激活稀疏性的实证观察为大幅降低计算成本提供了机会。尽管有几项研究将其归因于训练动力学，但激活稀疏性的理论解释仅限于浅层网络、小训练步长以及修改的训练，尽管这种稀疏性已在通过vanilla协议进行大步骤训练的深层模型中被发现。为了填补这三个差距，我们提出了梯度稀疏性的概念作为激活稀疏性的源头，并基于此提出了一个理论解释，该解释将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。这个理论适用于经过LayerNorm标准训练的纯MLP，并且如果在训练过程中给权重添加噪声，还适用于Transformers或其他架构。为了消除其他来源的激活稀疏性，我们还进行了进一步的实证研究。

    A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
    
[^143]: COMEDIAN: 使用自监督学习和知识蒸馏的Transformer进行动作定位

    COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01270](http://arxiv.org/abs/2309.01270)

    使用自监督学习和知识蒸馏的COMEDIAN提出了一种初始化时空Transformer的流程，用于动作定位任务。在SoccerNet-v2数据集上实验证明了其最先进的性能和有效性。

    

    我们提出了COMEDIAN，一种使用自监督学习和知识蒸馏初始化时空Transformer进行动作定位的新型流程。动作定位是一种基于时间戳级别的时间动作检测任务。我们的流程包括三个步骤，其中有两个初始化阶段。首先，我们使用短视频作为输入对空间Transformer进行自监督初始化。此外，我们还初始化一个时序Transformer，通过与每个短视频片段对齐的预计算特征库的知识蒸馏，增强空间Transformer的输出与全局上下文。最后一步，我们对Transformer进行微调以适应动作定位任务。在SoccerNet-v2数据集上进行的实验表明，COMEDIAN具有最先进的性能，并验证了其预训练模式的有效性。我们的结果突显了我们预训练流程的几个优点，包括改进的性能和更快的收敛速度。

    We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence c
    
[^144]: 自适应白化：快速增益调制和慢速突触可塑性

    Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.13633](http://arxiv.org/abs/2308.13633)

    本研究提出了一个多时间尺度的自适应白化机制模型，使用快速增益调制和慢速突触可塑性相结合的方式来适应变化的感觉统计信息。

    

    早期感觉区的神经元能够迅速适应变化的感觉统计信息，通过对其个体响应的方差进行归一化以及减少响应之间的相关性。这些转换可以被视为一种自适应的白化过程。现有的自适应白化的机制模型只使用突触可塑性或增益调制作为适应的生物基质，然而，每个模型都有显著的局限性。在这项工作中，我们将这些方法统一起来，提出了一个规范性的多时间尺度机制模型，通过突触可塑性和增益调制的计算角色来自适应地进行白化。增益在快速时间尺度上根据当前的统计情况进行调整，而突触在慢速时间尺度上进行调整，学习输入统计中与情境无关的结构特性。我们的模型来自于一种新颖的多时间尺度

    Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
    
[^145]: 蒙特卡洛引导扩散的贝叶斯线性逆问题研究

    Monte Carlo guided Diffusion for Bayesian linear inverse problems. (arXiv:2308.07983v1 [stat.ML])

    [http://arxiv.org/abs/2308.07983](http://arxiv.org/abs/2308.07983)

    本研究提出了一种在贝叶斯框架下利用蒙特卡洛方法解决非完备线性逆问题的算法，该算法通过利用基于得分的生成模型的先验结构和Feynman-Kac模型，并进行顺序蒙特卡洛采样，表现出比竞争对手更好的性能。

    

    非完备的线性逆问题经常在各种应用中出现，从计算摄影到医学成像。最近的研究集中于使用基于得分的生成模型（SGMs），在填补问题中产生具有感知合理性的图像来解决这些问题。本研究利用SGM定义的先验结构来制定贝叶斯框架下的恢复问题，将其作为Feynman-Kac模型，该模型改编自用于构建基于得分扩散的前向扩散模型。为了解决这个Feynman-Kac问题，我们提出使用顺序蒙特卡洛方法。所提出的算法MCGdiff在理论上是有根据的，并且我们提供了数值模拟结果，表明它在处理非完备逆问题时优于竞争对手的基准方法。

    Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
    
[^146]: 对过去六十年间具有高引用和重要影响的机器学习研究的全面研究

    A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades. (arXiv:2308.00855v1 [cs.DL])

    [http://arxiv.org/abs/2308.00855](http://arxiv.org/abs/2308.00855)

    对过去六十年间具有高引用和重要影响的机器学习研究进行了全面的分析，揭示了该领域中最具影响力的论文、作者和合作网络，并发现了热门研究主题和最新涌现的主题。

    

    机器学习已经成为计算机科学和其他相关领域中一项重要的研究领域，推动其他感兴趣领域的进步。随着这个领域的不断发展，了解高引用论文的情况至关重要，以确定关键趋势、有影响力的作者以及迄今为止所做出的重要贡献。在本文中，我们对高引用机器学习论文进行了全面的文献计量分析。我们收集了一份数据集，包括从1959年到2022年的多年间内，备受推崇的机器学习会议和期刊的高引用论文。我们采用了各种文献计量技术对数据进行了分析，包括引用分析、合著分析、关键词分析和出版趋势分析。我们的研究结果揭示了机器学习社区中最具影响力的论文、高引用的作者以及合作网络。我们确定了热门研究主题，并揭示了最近崛起的主题。

    Machine learning (ML) has emerged as a prominent field of research in computer science and other related fields, thereby driving advancements in other domains of interest. As the field continues to evolve, it is crucial to understand the landscape of highly cited publications to identify key trends, influential authors, and significant contributions made thus far. In this paper, we present a comprehensive bibliometric analysis of highly cited ML publications. We collected a dataset consisting of the top-cited papers from reputable ML conferences and journals, covering a period of several years from 1959 to 2022. We employed various bibliometric techniques to analyze the data, including citation analysis, co-authorship analysis, keyword analysis, and publication trends. Our findings reveal the most influential papers, highly cited authors, and collaborative networks within the machine learning community. We identify popular research themes and uncover emerging topics that have recently 
    
[^147]: TabR：解锁检索增强的表格深度学习的潜力

    TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning. (arXiv:2307.14338v1 [cs.LG])

    [http://arxiv.org/abs/2307.14338](http://arxiv.org/abs/2307.14338)

    本研究显示了现有的检索增强表格深度学习解决方案与无检索基线相比几乎没有明显优势，但提出了一种能够充分利用检索增强的表格深度学习模型，解锁了其潜力。

    

    近年来，针对表格数据问题的深度学习模型受到越来越多的关注，而基于梯度提升决策树（GBDT）的算法仍然是一个强大的解决方案。随着自然语言处理和计算机视觉等其他领域的最新趋势，最近提出了几种检索增强的表格深度学习模型。对于给定的目标对象，检索模型从可用的（训练）数据中检索其他相关对象，例如最近邻居，并使用它们的特征甚至标签来进行更好的预测。然而，我们表明，现有的检索增强表格深度学习解决方案与适当调整的简单无检索基线相比，几乎没有或者只有微小的优势。因此，检索增强方法是否值得在表格深度学习中继续探索还不清楚。

    Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.  In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture wit
    
[^148]: 基于语言的动作概念空间改进视频自监督学习

    Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.10922](http://arxiv.org/abs/2307.10922)

    这项研究使用语言相关的自监督学习方法，将图像CLIP模型调整为适用于视频领域，并通过在动作概念空间中进行自蒸馏训练，提高了零样本和线性推测性能。

    

    最近的对比语言图像预训练方法已经实现了学习可传递和鲁棒的图像表示。然而，如何在少量监督的情况下将这些模型应用于视频领域仍然是一个未解决的问题。我们探索了朝着这个方向的简单步骤，使用与语言相关的自我监督学习，将图像CLIP模型调整为视频领域。我们修改了适用于时间建模的骨干网络，通过在动作概念空间中使用训练目标进行自蒸馏训练。使用相关文本提示从语言编码器提取的各个动作概念的特征向量构成了这个空间。我们提出了两个训练目标，概念蒸馏和概念对齐，既保留了原始表示的广泛性，又强化了动作和其属性之间的关系。我们的方法改进了三个动作识别基准上的零样本和线性推测性能。

    Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
    
[^149]: 改进视觉-语言多模态数据集的图像描述能力

    Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])

    [http://arxiv.org/abs/2307.10350](http://arxiv.org/abs/2307.10350)

    通过探索混合原始和生成的图像描述的不同策略，我们的方法在ImageNet上超过了当前最佳降噪方法2%，在38个任务中平均提高了4%，我们的最佳方法在Flickr和MS-COCO检索上也提升了2倍。

    

    大规模网络数据集在CLIP和Flamingo等大型视觉-语言模型的成功中起到了关键作用。然而，原始网络数据存在噪音，现有的降噪方法往往会以数据的多样性为代价。我们的研究聚焦于图像描述的质量作为噪音的一个主要来源，并研究了如何通过生成的描述增加含有含义不明确文本的网络抓取数据点的实用性。通过探索原始模式和生成模式两种不同的混合策略，我们在ImageNet上超过了DataComp基准提出的最佳降噪方法2%，在38个任务中平均提高了4%，给定128M的图像-文本对候选池。我们最好的方法在Flickr和MS-COCO检索上也提升了2倍。然后，我们分析了合成描述为什么是一种有效的文本监督来源。在尝试不同的图像描述模型的同时，我们还证明了模型在标准图像描述基准上的表现（例如，NoCaps CIDEr）并不一定是

    Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a
    
[^150]: 视而不见：针对虚假相关性的鲁棒强化学习

    Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation. (arXiv:2307.07907v1 [cs.LG])

    [http://arxiv.org/abs/2307.07907](http://arxiv.org/abs/2307.07907)

    本文研究了如何在强化学习中使模型具有对虚假相关性的鲁棒性，这种虚假相关性是由于不可观察的混杂因素引起的，并且普遍存在于现实世界的任务中。

    

    鲁棒性在强化学习中得到了广泛研究，用于处理各种不确定性形式，如随机扰动、罕见事件和恶意攻击。本文研究一种关键类型的鲁棒性，即对虚假相关性的鲁棒性，其中状态的不同部分没有因果关系，但却存在由不可观察的混杂因素引起的相关性。这些虚假相关性在现实世界的任务中很普遍，例如，自动驾驶汽车通常在白天观察到交通拥堵，夜晚观察到交通轻松，原因是不可观察的人类活动。模型在学习这种无用甚至有害相关性时，当测试案例的混杂因素偏离训练时，可能会造成灾难性的失败。尽管如此，使模型对虚假相关性具有鲁棒性仍面临重要挑战，因为由不可观察的混杂因素和强化学习的顺序结构形成的不确定性集很难进行表征和识别。现有的鲁棒算法往往无法应对这一挑战。

    Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust alg
    
[^151]: 使用解耦的语言预训练为视觉语言学习引入引导策略

    Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])

    [http://arxiv.org/abs/2307.07063](http://arxiv.org/abs/2307.07063)

    本文提出了一种新的方法，通过解耦语言预训练，集中在语言组件上，提供了优化应用大型语言模型的框架，有效改善了视觉语言学习的性能。

    

    本论文提出了一种新颖的方法，旨在优化冻结的大型语言模型（LLMs）在资源密集型视觉语言（VL）预训练中的应用。当前的范式使用视觉特征作为提示来引导语言模型，重点是确定与相应文本最相关的视觉特征。我们的方法不同，集中在语言组件上，具体是确定与视觉特征对齐的最佳提示。我们引入了Prompt-Transformer（P-Former），一种可以预测这些理想提示的模型，该模型仅在语言数据上进行训练，避免了图像-文本配对的需要。这种策略将端到端的VL训练过程巧妙地分为了额外的独立阶段。我们的实验证明，我们的框架显著提高了稳健的图像到文本基线（BLIP-2）的性能，并有效地缩小了使用4M或129M图像-文本对进行训练的模型之间的性能差距。

    We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
    
[^152]: 受PID控制器启发的偏差归纳法在部分可观测控制任务中的深度强化学习

    PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])

    [http://arxiv.org/abs/2307.05891](http://arxiv.org/abs/2307.05891)

    该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。

    

    深度强化学习（RL）已经展现出通过数据自己学习控制系统的巨大潜力。然而，深度RL面临的一个挑战是系统的完整状态通常不可观测。当出现这种情况时，策略需要利用观察历史来推断当前状态。同时，训练和测试环境之间的差异使得策略不会过度拟合训练时观察到的序列。因此，在历史记录编码器灵活提取相关信息的同时，要对环境变化具有鲁棒性，这是一个重要的平衡。为了达到这个平衡，我们寻求PID控制器的启发。我们断定PID控制器的成功表明，许多控制任务只需要求和和求差来累积信息。基于这个原则，我们提出了两种用于编码历史记录的架构：一种直接使用...

    Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
    
[^153]: 轨迹对齐：通过分叉理论理解稳定边缘现象

    Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory. (arXiv:2307.04204v1 [cs.LG])

    [http://arxiv.org/abs/2307.04204](http://arxiv.org/abs/2307.04204)

    本文通过实证研究证明了梯度下降轨迹上的稳定边缘现象，并且对于特定的网络结构进行了轨迹对齐分析，建立了渐进尖锐化和稳定边缘现象，扩展了当前文献的研究结果。

    

    Cohen等人（2021）通过实证研究梯度下降（GD）轨迹上损失Hessian的最大特征值（即锐度），观察到一种称为稳定边缘（EoS）的现象。锐度在培训的早期阶段增加（称为渐进尖锐化），最终接近阈值$2/\text{(步长)}$附近停滞。本文通过实证研究首先证明了当EoS现象发生时，不同的GD轨迹（经过适当的参数化）在一个特定的分叉图上对齐，而与初始化无关。然后，我们对一个二层全连接线性网络和一个使用单个数据点训练的单神经元非线性网络严格证明了这种轨迹对齐现象。我们的轨迹对齐分析建立了渐进尖锐化和EoS现象，涵盖并扩展了最近文献中的研究结果。

    Cohen et al. (2021) empirically study the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory and observe a phenomenon called the Edge of Stability (EoS). The sharpness increases at the early phase of training (referred to as progressive sharpening), and eventually saturates close to the threshold of $2 / \text{(step size)}$. In this paper, we start by demonstrating through empirical studies that when the EoS phenomenon occurs, different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization. We then rigorously prove this trajectory alignment phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point. Our trajectory alignment analysis establishes both progressive sharpening and EoS phenomena, encompassing and extending recent findings in the literature.
    
[^154]: 从部分观测状态中学习时空连续神经偏微分方程

    Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v1 [cs.LG])

    [http://arxiv.org/abs/2307.04110](http://arxiv.org/abs/2307.04110)

    该论文介绍了一种从部分观测状态学习时空连续神经PDE的新方法，该方法通过新颖的编码器设计和高效的概率框架，克服了先前方法的局限，实现了对复杂部分观测数据的网格独立建模。

    

    我们介绍了一种新颖的独立于网格的模型，用于从具有噪声和部分观测的不规则时空网格上学习偏微分方程（PDE）。我们提出了一个具有高效概率框架和改进的编码器设计的时空连续潜在神经PDE模型，以提高数据效率和网格独立性。潜在状态动力学由结合了插值法和线方法的PDE模型所控制。我们采用了均摊变分推理来近似后验估计，并利用多重射击技术来提高训练速度和稳定性。我们的模型在复杂的合成和真实世界数据集上展现出最先进的性能，克服了先前方法的局限，并有效处理部分观测数据。所提出的模型表现优于最近的方法，显示了其推进数据驱动PDE建模和实现复杂部分观测的网格独立建模的潜力。

    We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observe
    
[^155]: 图神经网络中特征演化的神经塌陷视角

    A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])

    [http://arxiv.org/abs/2307.01951](http://arxiv.org/abs/2307.01951)

    本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。

    

    图神经网络（GNNs）在图结构数据的分类任务中越来越受欢迎。然而，GNNs中图拓扑和特征演化之间的相互作用尚不清楚。本文以基于节点的分类为主题，以随机块模型图上的社区检测为例，通过“神经塌陷”现象来探索特征演化。当训练基于实例的深度分类器（例如图像分类）超过零训练误差点时，神经塌陷表现为最深层特征的类内变异性减少，并且类均值与特定的对称结构更加对齐。我们先从实证研究开始，显示类内变异性的减少在基于节点的分类环境中也普遍存在，但不及基于实例的案例那么明显。然后，我们从理论上研究了这种区别。具体而言，我们证明了即使在不考虑激活，图拓扑信息也能导致特征崩溃。

    Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimis
    
[^156]: 用于噪声混合物中目标信号恢复的统计分量分离

    Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v1 [stat.ML])

    [http://arxiv.org/abs/2306.15012](http://arxiv.org/abs/2306.15012)

    本论文提出了一种用于从噪声混合物中恢复目标信号的统计分量分离方法，并且在图像降噪任务中展示了其优于标准降噪方法的表现。

    

    当只对给定信号的特定属性感兴趣时，从一个加性混合物中分离信号可能是一个不必要地困难的问题。在本工作中，我们解决了更简单的“统计分量分离”问题，该问题专注于从噪声混合物中恢复目标信号的预定义统计描述量。假设可以获得噪声过程的样本，我们研究了一种方法，该方法旨在使受噪声样本污染的解决方案候选的统计特性与观测的混合物的统计特性匹配。首先，我们使用具有解析可追踪计算的简单示例分析了该方法的行为。然后，我们将其应用于图像降噪环境中，使用了1）基于小波的描述符，2）针对天体物理和ImageNet数据的ConvNet-based描述符。在第一种情况下，我们展示了我们的方法在大多数情况下比标准降噪方法更好地恢复了目标数据的描述符。此外，尽管不是为此目的构建的，它也表现出对目标信号描述符恢复的潜力。

    Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it pe
    
[^157]: CEIL: 广义上下文模仿学习

    CEIL: Generalized Contextual Imitation Learning. (arXiv:2306.14534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14534](http://arxiv.org/abs/2306.14534)

    CEIL是一种通用的广义模仿学习算法，通过学习后见嵌入函数和上下文策略来实现模仿学习的专家匹配目标。它能够适用于多种学习设置，包括从观测中学习、离线模仿学习、跨领域模仿学习和一次模仿学习设置。实验表明，CEIL相比之前的基准算法具有更高的样本效率。

    

    在本文中，我们提出了CEIL（广义上下文模仿学习），这是一种通用且广泛适用的模仿学习算法。受到后见信息匹配的形式化启发，我们通过显式学习一种后见嵌入函数以及使用后见嵌入的上下文策略，得到了CEIL。为了实现模仿学习的专家匹配目标，我们提倡优化一种上下文变量，使其偏向于模仿专家行为的上下文策略。除了典型的示范学习（LfD）设置外，CEIL还是一个通用的算法，可有效应用于多种设置，包括：1）从观测中学习（LfO），2）离线模仿学习，3）跨领域模仿学习（不匹配的专家）和4）一次模仿学习设置。在实证方面，我们在流行的MuJoCo任务（在线）和D4RL数据集（离线）上评估CEIL。与先前的最先进基准相比，我们展示了CEIL的更高样本效率。

    In this paper, we present \textbf{C}ont\textbf{E}xtual \textbf{I}mitation \textbf{L}earning~(CEIL), a general and broadly applicable algorithm for imitation learning (IL). Inspired by the formulation of hindsight information matching, we derive CEIL by explicitly learning a hindsight embedding function together with a contextual policy using the hindsight embeddings. To achieve the expert matching objective for IL, we advocate for optimizing a contextual variable such that it biases the contextual policy towards mimicking expert behaviors. Beyond the typical learning from demonstrations (LfD) setting, CEIL is a generalist that can be effectively applied to multiple settings including: 1)~learning from observations (LfO), 2)~offline IL, 3)~cross-domain IL (mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate CEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline). Compared to prior state-of-the-art baselines, we show that CEIL is more sample-effici
    
[^158]: 可微分决策树是否能够学习可解释的奖励函数?

    Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])

    [http://arxiv.org/abs/2306.13004](http://arxiv.org/abs/2306.13004)

    本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。

    

    学习人的意图和偏好的奖励函数越来越受到关注，但许多框架使用黑盒学习方法，难以解释。本文提出并评估了一种新颖方法，使用可微分决策树（DDT）从偏好中学习具有表达能力和可解释性的奖励函数，适用于低维和高维状态输入。我们在Cartpole、视觉网格世界环境和Atari游戏上评估了我们的算法，探讨了使用DDT学习可解释奖励函数的可行性。我们提供证据表明，学习到的奖励函数的树形结构有助于确定奖励函数与人类偏好的一致程度。我们可视化了学习到的奖励DDT，发现它们能够学习可解释的奖励函数，但树的离散性会影响强化学习在测试时的表现。

    There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
    
[^159]: 针对变尺寸文本到图像合成的无需训练的扩散模型适应

    Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis. (arXiv:2306.08645v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.08645](http://arxiv.org/abs/2306.08645)

    本研究提出了一种无需训练的扩散模型适应方法，用于处理变尺寸文本到图像合成。通过观察低分辨率图像的不完整对象描绘和高分辨率图像的重复无序呈现，提出了注意力熵与令牌数量变化的统计关系。这项工作实现了在保持视觉保真度的同时适应不同尺寸和长宽比的图像合成需求。

    

    近期，扩散模型（DMs）在文本到图像合成中表现出了最先进的性能，并引起了广泛关注。然而，遵循深度学习的传统，DMs在固定尺寸的图像上进行训练和评估。然而，用户需要不同尺寸和不同长宽比的各种图像。本文重点研究了如何在保持视觉保真度的同时，使文本到图像扩散模型能够处理这种多样性。首先，我们观察到，在合成过程中，低分辨率的图像会因为对象描绘不完整，而高分辨率的图像则会出现重复无序的呈现。接下来，我们建立了一个统计关系，该关系表明注意力熵随令牌数量变化而变化，这表明模型会按照图像分辨率的比例聚合空间信息。我们对观察结果的后续解释是，由于低分辨率的图像有限的空间信息，对象被不完整地描绘，而高分辨率的图像则会出现重复无序的表示。

    Diffusion models (DMs) have recently gained attention with state-of-the-art performance in text-to-image synthesis. Abiding by the tradition in deep learning, DMs are trained and evaluated on the images with fixed sizes. However, users are demanding for various images with specific sizes and various aspect ratio. This paper focuses on adapting text-to-image diffusion models to handle such variety while maintaining visual fidelity. First we observe that, during the synthesis, lower resolution images suffer from incomplete object portrayal, while higher resolution images exhibit repetitively disordered presentation. Next, we establish a statistical relationship indicating that attention entropy changes with token quantity, suggesting that models aggregate spatial information in proportion to image resolution. The subsequent interpretation on our observations is that objects are incompletely depicted due to limited spatial information for low resolutions, while repetitively disorganized p
    
[^160]: 面向Oracle的悲观策略优化：离线上下文强化学习中的计算有效性

    Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits. (arXiv:2306.07923v1 [cs.LG])

    [http://arxiv.org/abs/2306.07923](http://arxiv.org/abs/2306.07923)

    本文提出了第一个面向Oracle有效的悲观策略优化算法，它简化为监督学习，具有广泛的适用性，能够在上下文强化学习中优化策略。

    

    本文考虑在上下文强化学习中的策略优化问题，其中给定一个固定数据集的日志交互。虽然通常使用悲观惩罚来缓解分布偏移，但先前的实现并不计算有效。本文提出了第一个面向Oracle有效的悲观策略优化算法：它简化为监督学习，具有广泛的适用性。我们也得出了类似于先前工作中悲观方法的最佳统计保证。我们为离散和连续动作都实例化了我们的方法。我们在两种情况下进行了广泛的实验，显示出在各种配置中都比未正则化的策略优化更具优势。

    We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.
    
[^161]: 高斯成员推断隐私

    Gaussian Membership Inference Privacy. (arXiv:2306.07273v1 [cs.LG])

    [http://arxiv.org/abs/2306.07273](http://arxiv.org/abs/2306.07273)

    本文提出了$f$-成员推断隐私($f$-MIP)概念，并分析了似然比成员推断攻击，提出了$\mu$-高斯成员推断隐私($\mu$-GMIP)保证，同时提供了一种分析性的成员推断攻击方法，避免了训练大量影子模型。强调了方差的重要性。

    

    我们提出了一个新的隐私概念，称为$f$-成员推断隐私($f$-MIP)，它明确考虑了在成员推断攻击威胁模型下现实对手的能力。通过这样做，$f$-MIP提供了可解释的隐私保证和改进的效用(例如更好的分类准确率)。我们对噪声随机梯度下降(SGD)的似然比成员推断攻击进行了新颖的理论分析，得到了一个参数化的$f$-MIP保证族，称为$\mu$-高斯成员推断隐私($\mu$-GMIP)。我们的分析还产生了一种分析性的成员推断攻击，与以前的方法相比具有显著优势。首先，与现有方法不同，我们的攻击不需要训练数百个影子模型来逼近似然比。其次，我们的分析攻击使得$f$-MIP的简单审计成为可能。最后，我们的分析强调了方差的重要性。

    We propose a new privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model. By doing so $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). Our novel theoretical analysis of likelihood ratio-based membership inference attacks on noisy stochastic gradient descent (SGD) results in a parametric family of $f$-MIP guarantees that we refer to as $\mu$-Gaussian Membership Inference Privacy ($\mu$-GMIP). Our analysis additionally yields an analytical membership inference attack that offers distinct advantages over previous approaches. First, unlike existing methods, our attack does not require training hundreds of shadow models to approximate the likelihood ratio. Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, our analysis emphasizes the importance of vario
    
[^162]: 一般转换构建一致的在线近似算法

    General Transformation for Consistent Online Approximation Algorithms. (arXiv:2306.07163v1 [cs.LG])

    [http://arxiv.org/abs/2306.07163](http://arxiv.org/abs/2306.07163)

    本文提出一个转换框架，可以将离线逼近算法转换为具有低ε-近似遗憾的在线算法，并成功应用于多种问题并实现了多项式时间的近似效果。

    

    我们提出了一个转换框架，可用于从离线逼近算法中开发具有低ε-近似遗憾的在线算法。我们首先给出了一个将具有低平均敏感度的离线逼近算法转换为具有低ε-近似遗憾的在线算法的通用约简定理。然后，我们展示了可以使用coreset构造方法将离线逼近算法转换为低敏感度版本。为了展示我们方法的多样性，我们将其应用于各种问题，包括在线(k，z)-聚类、在线矩阵逼近和在线回归，并成功地为每个问题实现了对数多项式ε-近似遗憾。此外，我们证明，在所有三种情况下，我们的算法也享有低不一致性，这可能是某些在线应用程序所需的。

    We introduce a transformation framework that can be utilized to develop online algorithms with low $\epsilon$-approximate regret in the random-order model from offline approximation algorithms. We first give a general reduction theorem that transforms an offline approximation algorithm with low average sensitivity to an online algorithm with low $\epsilon$-approximate regret. We then demonstrate that offline approximation algorithms can be transformed into a low-sensitivity version using a coreset construction method. To showcase the versatility of our approach, we apply it to various problems, including online $(k,z)$-clustering, online matrix approximation, and online regression, and successfully achieve polylogarithmic $\epsilon$-approximate regret for each problem. Moreover, we show that in all three cases, our algorithm also enjoys low inconsistency, which may be desired in some online applications.
    
[^163]: 关于图神经网络中本地同质性水平的性能差异

    On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v1 [cs.SI])

    [http://arxiv.org/abs/2306.05557](http://arxiv.org/abs/2306.05557)

    本文研究了GNN在测试时节点的本地同质性水平与其图的全局同质性水平偏离时的性能，并介绍一种新参数用于控制同质性，在生成的图中系统地研究本地同质性的影响。

    

    GNN的研究强调高同质性（即相似类节点相互连接的倾向）与节点分类的强预测性能之间的关系。然而，最近的研究发现这种关系更加微妙，证明即使简单的GNN也可以在某些异质性环境中学习。为了弥合这些发现之间的差距，我们重新思考了先前作品中的假设，并确定数据集经常被视为在节点间具有恒定的同质性水平。为了更接近真实世界的数据集，我们理论上和实证地研究了GNN在测试时节点的本地同质性水平与其图的全局同质性水平偏离时的性能。为了帮助我们的理论分析，我们在同质性分析中常用的优先附加模型中引入了一个新参数，以控制生成的图中的本地同质性水平，从而实现系统的实证研究，探究本地同质性的影响。

    Research on GNNs has highlighted a relationship between high homophily (i.e., the tendency for nodes of a similar class to connect) and strong predictive performance in node classification. However, recent research has found the relationship to be more nuanced, demonstrating that even simple GNNs can learn in certain heterophilous settings. To bridge the gap between these findings, we revisit the assumptions made in previous works and identify that datasets are often treated as having a constant homophily level across nodes. To align closer to real-world datasets, we theoretically and empirically study the performance of GNNs when the local homophily level of a node deviates at test-time from the global homophily level of its graph. To aid our theoretical analysis, we introduce a new parameter to the preferential attachment model commonly used in homophily analysis to enable the control of local homophily levels in generated graphs, enabling a systematic empirical study on how local ho
    
[^164]: 重温自然语言处理中的领域外鲁棒性: 基准，分析和LLMs评估

    Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])

    [http://arxiv.org/abs/2306.04618](http://arxiv.org/abs/2306.04618)

    本文提出了一个具有挑战性的基准协议，用于评估自然语言处理中的领域外鲁棒性。通过使用这个基准套件，作者们发现OOD与ID性能之间的关系并不总是一致的，并引入了一种名为LLMs的新方法，可以在多个任务上显著提高OOD鲁棒性。

    

    本文重新审视自然语言处理(NLP)领域中领域外鲁棒性(OOD)的研究。我们发现以往研究中的分布转移设置普遍缺乏足够的挑战，限制了对OOD鲁棒性的准确评估。为了解决这些问题，我们提出了一个基准构建方案，确保了明确的区分和具有挑战性的分布转移。然后，我们介绍了BOSS，一个涵盖5个任务和20个数据集的用于评估OOT鲁棒性的基准套件。基于BOSS，我们对预训练语言模型进行了一系列实验，以分析和评估OOD鲁棒性。首先，我们研究了香草微调的ID和OOD性能之间的关系。我们确定了三种典型类型揭示了内在的学习机制，可能有助于预测OOD鲁棒性，并与ID数据集上的进展相关。然后，我们在BOSS上评估了5种经典方法，并发现它们的OOD性能并不总是与ID性能一致，这表明了特别评估OOD鲁棒性的重要性。最后，我们提出了一种名为LLMs（潜在语言模型）的新方法，可以在多个任务上显著提高OOD鲁棒性。

    This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
    
[^165]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^166]: SACSoN：面向社交导航的可扩展自主数据收集系统

    SACSoN: Scalable Autonomous Data Collection for Social Navigation. (arXiv:2306.01874v1 [cs.RO])

    [http://arxiv.org/abs/2306.01874](http://arxiv.org/abs/2306.01874)

    本文介绍了一个名为SACSoN的自主导航机器人系统，可以在人类占用的现实场景中，通过视觉理解和学习，自主收集数据，实现更好的数据集拓展。

    

    机器学习为构建符合社交规范的机器人系统提供了一个强大的工具，超越了对人类行为的简单预测模型。通过观察和理解过去经验中的人类交互，学习可以直接从数据中实现有效的社交导航行为。然而，在人类占用的环境中收集导航数据可能需要远程操作或持续监视，使得这个过程难以扩展。在本文中，我们提出了一个面向视觉导航的可扩展数据收集系统SACSoN，可以自主导航于挑战性的现实环境中的行人周围，并鼓励丰富的交互。SACSoN使用视觉观察来观察和回应其附近的人类。它将这种视觉理解与持续的学习和自主碰撞恢复系统相结合，从而限制了人操作员的参与，从而更好地扩展了数据集。我们使用这个系统来收集数据集

    Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. However, collecting navigation data in human-occupied environments may require teleoperation or continuous monitoring, making the process prohibitively expensive to scale. In this paper, we present a scalable data collection system for vision-based navigation, SACSoN, that can autonomously navigate around pedestrians in challenging real-world environments while encouraging rich interactions. SACSoN uses visual observations to observe and react to humans in its vicinity. It couples this visual understanding with continual learning and an autonomous collision recovery system that limits the involvement of a human operator, allowing for better dataset scaling. We use a this system to collect th
    
[^167]: 通过权重集成、多样性加权和先验校正实现通用的测试时间适应

    Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction. (arXiv:2306.00650v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.00650](http://arxiv.org/abs/2306.00650)

    通过权重集成、多样性加权和先验校正，本研究提出了一种通用的测试时间适应方法，以解决因分布偏移而降低模型性能的问题。这是第一项涵盖如此广泛范围的工作，对实际应用至关重要。

    

    由于测试时间可能发生分布偏移且可能严重降低模型的性能，在线测试时间适应（TTA）在部署后继续更新模型，利用当前的测试数据。为了处理通用TTA的问题，我们首先引入了变量因素域非稳态和时间相关性，并对所有实际相关的设置进行了详细分析，将其定义为通用TTA。我们希望强调的是，这是第一项涵盖如此广泛范围的工作，对于实际应用来说是不可或缺的。为了解决通用TTA的问题，我们确定并强调了一些自训练方法需要应对的挑战：1）模型偏差和在不同序列长度上进行熵最小化时出现的平凡解以及存在多个领域转移，2）泛化能力的丧失，加剧了适应性的问题。

    Since distribution shifts are likely to occur during test-time and can drastically decrease the model's performance, online test-time adaptation (TTA) continues to update the model after deployment, leveraging the current test data. Clearly, a method proposed for online TTA has to perform well for all kinds of environmental conditions. By introducing the variable factors domain non-stationarity and temporal correlation, we first unfold all practically relevant settings and define the entity as universal TTA. We want to highlight that this is the first work that covers such a broad spectrum, which is indispensable for the use in practice. To tackle the problem of universal TTA, we identify and highlight several challenges a self-training based method has to deal with: 1) model bias and the occurrence of trivial solutions when performing entropy minimization on varying sequence lengths with and without multiple domain shifts, 2) loss of generalization which exacerbates the adaptation to 
    
[^168]: 离线强化学习的高效扩散策略

    Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20081](http://arxiv.org/abs/2305.20081)

    本论文提出了高效扩散策略（EDP）用于解决离线强化学习中的两个关键挑战，即计算效率低和难以与最大似然的强化学习算法兼容。EDP通过近似构建动作来避免运行采样链，并在实验中得到验证。

    

    离线强化学习旨在从离线数据集中学习最优策略，其中策略的参数化是关键但常常被忽视。最近，Diffusion-QL通过使用扩散模型表示策略，显著提高了离线强化学习的性能，该模型的成功依赖于参数化的马尔可夫链进行采样，但是Diffusion-QL存在两个关键限制。1）在训练过程中，通过整个马尔可夫链进行前向和反向传播计算效率低下。2）由于扩散模型的似然函数难以计算，与基于最大似然的强化学习算法（如策略梯度方法）不兼容。因此，我们提出了高效扩散策略（EDP）来克服这两个挑战。EDP在训练过程中通过从损坏的动作中近似构建动作，避免了运行采样链。我们在D4RL基准测试中进行了大量实验。结果表明，EDP能够减少扩散的潜在计算复杂性。

    Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion po
    
[^169]: 生成式扩散模型中的自发对称破缺

    Spontaneous symmetry breaking in generative diffusion models. (arXiv:2305.19693v1 [cs.LG])

    [http://arxiv.org/abs/2305.19693](http://arxiv.org/abs/2305.19693)

    本文揭示生成式扩散模型存在自发对称破缺现象，这将其生成动力学分为两种不同的“相”，提出了高斯后期初始化方案，能够显著提高模型性能。

    

    生成式扩散模型近期成为了生成高维数据的主要方法。在本文中，我们展示了这些模型的动力学存在自发对称破缺，将生成式动力学分为两个不同的“相”：1）中心固定点周围的线性稳态动力学，2）朝向数据流形的吸引子动力学。这两种“相”由中心固定点稳定性变化所分隔，而不稳定窗口负责生成样本的多样性。我们使用理论和实证证据表明，准确地模拟早期动力学并不会对最终生成产生重要影响，因为早期涨落会回到中心固定点。为了利用这一见解，我们提出了一个高斯后期初始化方案，这显著提高了模型的性能，在快速取样器上实现了长达3倍的FID改进。

    Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, whi
    
[^170]: 用Johnson-Lindenstrauss矩阵进行标签嵌入

    Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])

    [http://arxiv.org/abs/2305.19470](http://arxiv.org/abs/2305.19470)

    这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。

    

    我们提出了一个基于Johnson-Lindenstrauss矩阵（JLMs）的简单且可扩展的极端多元分类框架。利用JLM的列来嵌入标签，将一个C类分类问题转化为具有$\cO(\log C)$输出维度的回归问题。我们得出了一个超量风险限制，阐明了计算效率和预测准确性之间的权衡，并进一步表明，在Massart噪声条件下，降维的惩罚会消失。我们的方法易于并行化，并且实验结果展示了在大规模应用中其有效性和可扩展性。

    We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
    
[^171]: Forward-Forward算法训练的网络中的突现表征

    Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v1 [cs.NE])

    [http://arxiv.org/abs/2305.18353](http://arxiv.org/abs/2305.18353)

    研究表明使用Forward-Forward算法训练的网络内部表征具有高稀疏度，类别特定的集合，这与生物学观察到的皮层表征相似。

    

    Backpropagation算法被广泛用于训练神经网络，但其缺乏生物学上的现实性。为了寻找一种更具生物学可行性的替代方案，并避免反向传播梯度，而是使用本地学习规则，最近介绍的Forward-Forward算法将Backpropagation的传递替换为两个前向传递。本研究表明，使用Forward-Forward算法获得的内部表征组织为稳健的，类别特定的集合，由极少量的有效单元(高稀疏度)组成。这与感觉处理过程中观察到的皮层表征非常相似。虽然在使用标准Backpropagation进行训练的模型中没有发现，但是在使用与Forward-Forward相同的训练目标进行优化的网络中也出现了稀疏性。这些结果表明，Forward-Forward提议的学习过程可能更接近生物学学习的现实情况。

    The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward ma
    
[^172]: 具有对抗性损失和转换的无遗憾在线强化学习

    No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])

    [http://arxiv.org/abs/2305.17380](http://arxiv.org/abs/2305.17380)

    本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。

    

    现有的对抗性马尔可夫决策过程的在线学习算法可以在与对手的$ T $轮交互之后实现${ O}(\sqrt{T})$的后悔，即使损失函数是由对手任意选择的，但前提是转移函数必须固定。这是因为已经有研究表明，对抗性转移函数使无悔学习变得不可能。尽管存在这种不可能性结果，我们开发了可以处理对抗性损失和对抗性转换的算法，后悔逐渐增加与对手的恶意程度成比例。更具体地说，我们首先提出了一种算法，它的后悔为$\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$，其中$C^{\textsf{P}}$表示转换函数的对抗性，最多可以为${O}(T)$。虽然此算法本身需要$C^{\textsf{P}}$的知识，但我们还开发了一种黑盒缩减方法来消除此要求。此外，我们还展示了一种进一步的方法，使得算法能够处理任意长度的锚定期。

    Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth
    
[^173]: 拉普拉斯逼近神经加性模型：贝叶斯推理提高解释性

    Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])

    [http://arxiv.org/abs/2305.16905](http://arxiv.org/abs/2305.16905)

    本文提出了拉普拉斯逼近神经加性模型，该模型从贝叶斯角度考虑加性结构，在恢复的特征交互中提供可信区间，提供可处理的边缘似然估计，可用于执行隐式特征选择并对特征对进行排名。

    

    深度神经网络（DNN）在许多领域取得了成功应用，但它们的黑盒性质阻碍了解释性。神经加性模型（NAM）解决了这个问题，将网络分为加性子网络，从而使输入特征和预测之间的交互变得明显。在本文中，我们从贝叶斯角度考虑加性结构，并开发了一个实用的拉普拉斯逼近方法。这种方法在以下三个方面提高了可解释性：a）它通过估计子网络的函数空间不确定性为恢复的特征交互提供可信区间；b）它提供可处理的边缘似然估计，可用于通过经验贝叶斯过程执行特征的隐式选择；c）它可用于对特征对进行排名，作为精细调整的交互模型候选。我们在几个基准数据集上实证表明，我们提出的拉普拉斯逼近神经加性模型（LA-NAM）提高了NAM模型的可解释性，并进一步揭示了学习到的子网络的交互结构。

    Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
    
[^174]: 神经（切向核）崩溃

    Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])

    [http://arxiv.org/abs/2305.16427](http://arxiv.org/abs/2305.16427)

    本文介绍了神经切向核（NTK）和神经崩溃（NC）之间的关系，证明了在具有块状NTK的DNN中会出现NC，并通过大规模实验支持理论的正确性。

    

    本文介绍了两个重要的概念：神经切向核（NTK），它捕捉深度神经网络（DNN）训练期间的演化和神经崩溃（NC）现象，它指的是经过良好训练的分类DNN最后一层特征中对称性和结构的出现。我们假设经验NTK与类标签对齐并形成块状结构，即同一类别的样本之间的相关性比不同类别的样本更强，基于这个假设，我们推导了使用均方误差（MSE）训练的DNN动态，并将其分解为可解释的阶段。此外，我们确定了一种不变量，捕捉了动态的本质，并用它证明了在具有块状NTK的DNN中会出现NC。我们进行了三种常见DNN架构和三个基准数据集的大规模数值实验来支持我们的理论。

    This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
    
[^175]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^176]: 统一GAN和基于分数扩散的粒子生成模型

    Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v1 [cs.LG])

    [http://arxiv.org/abs/2305.16150](http://arxiv.org/abs/2305.16150)

    本文提出了一个新框架，将生成器训练作为粒子模型的一个推广，从而统一了粒子和对抗生成模型。这个框架可以将生成器集成到基于分数扩散模型中，并在没有生成器的情况下训练GAN。

    

    基于粒子的深度生成模型，例如梯度流和基于分数的扩散模型，由于其惊人的性能而最近受到关注。传统上，通过微分方程来移动粒子分布的方法被普遍认为是与以前广泛使用的生成对抗网络（GAN）相对立的，后者涉及到训练一个向前的生成器网络。在本文中，我们质疑这种解释，并提出了一个统一粒子和对抗生成模型的新框架，通过将生成器训练作为粒子模型的推广。这表明，生成器是任何这样的生成模型的可选附件。因此，将生成器集成到基于分数扩散模型中，并在没有生成器的情况下训练GAN自然地出现在我们的框架中。我们通过实证测试这些原始模型的可行性，这些模型是我们框架可能应用的概念证明。

    Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions by differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper, we challenge this interpretation and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework.
    
[^177]: 带小总成本限制的上下文信息决策问题与背包问题的相关性，及其对公平性的应用

    Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness. (arXiv:2305.15807v1 [stat.ML])

    [http://arxiv.org/abs/2305.15807](http://arxiv.org/abs/2305.15807)

    本文提出了带总成本限制的上下文信息决策问题（CBwK），通过对术语进行重新组合，对CBwK进行了优化，支持小于$T^{3/4}$的总成本约束，并通过对偶策略实现了平等的成本限制。

    

    本文考虑了带有背包问题的上下文信息决策问题（CBwK），每一轮获得一个标量奖励和一个向量值的成本。我们的目标是最大化累计的奖励，并确保累计成本低于某个预定的成本限制。我们假设环境来自一个连续集合，成本可以带符号，并且未知的期望奖励和成本函数可以被一致地估计，这是文献中的一个典型假设。在这种情况下，迄今为止总成本约束至少要为$T^{3/4}$，其中$T$是轮数，并且甚至通常被假定为与$T$线性相关。然而，我们受到鼓舞，使用CBwK来强制实施实现组之间平均成本平等的公平性约束：与相应成本约束相关的预算应尽可能接近于阶数为$\sqrt{T}$级别的自然偏差。为此，我们介绍了一种基于对偶策略的方法。

    We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated -- a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order $T^{3/4}$, where $T$ is the number of rounds, and were even typically assumed to depend linearly on $T$. We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order $\sqrt{T}$. To that end, we introduce a dual strategy based on 
    
[^178]: 学习率无关的约束域Bayesian推断

    Learning Rate Free Bayesian Inference in Constrained Domains. (arXiv:2305.14943v1 [stat.ML])

    [http://arxiv.org/abs/2305.14943](http://arxiv.org/abs/2305.14943)

    我们的算法是学习率无关的约束域采样算法，并提出了一个统一框架，能够处理多种约束采样问题，实现了与现有算法相竞争的性能。

    

    我们引入了一套新的基于粒子的算法，用于在约束域内进行采样，这是完全与学习率无关的。我们的方法利用了凸优化中的硬币投注思想，以及约束采样作为概率测度空间上镜像优化问题的观点。基于这个观点，我们还提出了一个统一框架，用于几种现有的约束采样算法，包括镜像Langevin动力学和镜像Stein变分梯度下降。我们在一系列的数值实验中展示了算法的性能，包括从单纯形目标进行采样、带公平性约束进行采样以及后选择推断中的约束采样问题。我们的结果表明，我们的算法在不需要调整任何超参数的情况下，实现了与现有约束采样方法相竞争的性能。

    We introduce a suite of new particle-based algorithms for sampling on constrained domains which are entirely learning rate free. Our approach leverages coin betting ideas from convex optimisation, and the viewpoint of constrained sampling as a mirrored optimisation problem on the space of probability measures. Based on this viewpoint, we also introduce a unifying framework for several existing constrained sampling algorithms, including mirrored Langevin dynamics and mirrored Stein variational gradient descent. We demonstrate the performance of our algorithms on a range of numerical examples, including sampling from targets on the simplex, sampling with fairness constraints, and constrained sampling problems in post-selection inference. Our results indicate that our algorithms achieve competitive performance with existing constrained sampling methods, without the need to tune any hyperparameters.
    
[^179]: Pre-RMSNorm和Pre-CRMSNorm Transformers: 等效和高效的Pre-LN Transformers

    Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14858](http://arxiv.org/abs/2305.14858)

    Pre-RMSNorm和Pre-CRMSNorm Transformers是等效且高效的Pre-LN Transformers架构，可以统一使用两种主流归一化技术，LayerNorm和RMSNorm，从而加速和稳定Transformer模型的训练。

    

    Transformer模型在机器学习应用中取得了巨大成功。归一化技术，如Layer Normalization（LayerNorm，LN）和Root Mean Square Normalization（RMSNorm），在加速和稳定Transformer的训练中起着关键作用。虽然LayerNorm对输入向量进行重新中心化和重新缩放，而RMSNorm仅按其RMS值重新缩放向量。尽管RMSNorm在计算上更高效，但可能会损害Transformer的表示能力。目前关于首选归一化技术尚无共识，因为一些模型使用LayerNorm，而其他模型则使用RMSNorm，尤其是最近的大语言模型。将具有一种归一化的Transformer转换为另一种类型是一项具有挑战性的任务。虽然目前两种归一化类型之间存在争议，但我们提出了一种解决方案，即统一两种主流Transformer架构，Pre-LN和Pre-RMSNorm Transformers。通过去除固有的冗余均值信息来实现等效性和高效性。

    Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean informat
    
[^180]: SEEDS: 指数随机微分方程求解器，用于快速高质量地从扩散模型中进行抽样

    SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models. (arXiv:2305.14267v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14267](http://arxiv.org/abs/2305.14267)

    本论文提出了SEEDS，这是一种指数随机微分方程求解器，用于从扩散模型中进行快速高质量的抽样。与现有的慢速求解器相比，SEEDS能够以更高的质量进行求解，且不需要大量的数值函数评估。

    

    一类强大的生成模型被称为扩散概率模型(DPMs)已经引起了人们的关注。前向扩散过程逐渐向数据添加噪声，而模型逐渐学习去除噪声。从预训练的DPMs中进行抽样是通过解决由学习模型定义的微分方程(DE)来实现的，这一过程被证明是过于缓慢的。许多加速这一过程的努力包括设计强大的ODE求解器。尽管速度很快，但这些求解器通常无法达到现有慢速SDE求解器所达到的最佳质量。我们的目标是提出一种SDE求解器，能够在不需要几百或上千次NFE(数值函数评估)的情况下达到最佳质量。我们提出了随机显式指数无导数求解器(SEEDS)，在多个框架上改进和推广了指数积分器方法以适应随机情况。在仔细分析扩散SDE确切解的公式后，我们设计了SEEDS。

    A potent class of generative models known as Diffusion Probabilistic Models (DPMs) has become prominent. A forward diffusion process adds gradually noise to data, while a model learns to gradually denoise. Sampling from pre-trained DPMs is obtained by solving differential equations (DE) defined by the learnt model, a process which has shown to be prohibitively slow. Numerous efforts on speeding-up this process have consisted on crafting powerful ODE solvers. Despite being quick, such solvers do not usually reach the optimal quality achieved by available slow SDE solvers. Our goal is to propose SDE solvers that reach optimal quality without requiring several hundreds or thousands of NFEs to achieve that goal. We propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS), improving and generalizing Exponential Integrator approaches to the stochastic case on several frameworks. After carefully analyzing the formulation of exact solutions of diffusion SDEs, we craft SEEDS to a
    
[^181]: 警惕尖峰：固定维度下内核和神经网络的良性过拟合

    Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. (arXiv:2305.14077v1 [stat.ML])

    [http://arxiv.org/abs/2305.14077](http://arxiv.org/abs/2305.14077)

    这篇论文研究了固定维度下内核和神经网络的良性过拟合，发现良性过拟合的关键在于估计器的平滑度而不是维数，并证明在固定维度下中度导数的良性过拟合是不可能的。相反，我们证明了用序列核进行回归是可能出现良性过拟合的。

    

    过度参数化的神经网络训练达到接近零的训练误差的成功引起了人们对良性过拟合现象的极大兴趣，即使估计器插值嘈杂的训练数据，它们还是具有统计一致性。尽管某些学习方法的固定维度下已经确定了良性过拟合，但目前的文献表明，对于典型内核方法和宽神经网络的回归，良性过拟合需要高维度设置，其中维数随着样本大小的增加而增加。本文表明，估计器的平滑度是关键，而不是维数：只有当估计器的导数足够大时，良性过拟合才可能发生。我们将现有的不一致性结果推广到非插值模型和更多内核，以表明在固定维度下中度导数的良性过拟合是不可能的。相反，我们证明了用序列核进行回归是可能出现良性过拟合的。

    The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that benign overfitting is possible for regression with a seque
    
[^182]: 多语言摘要中的幻觉检测和缓解

    Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])

    [http://arxiv.org/abs/2305.13632](http://arxiv.org/abs/2305.13632)

    本文提出一种新的度量方法mFACT，可以在非英语摘要中评估其忠实性。本文还提出了一种简单有效的加权方法，可以通过跨语言转移减少摘要的幻觉问题。

    

    幻觉对于抽象摘要的神经模型的可靠性构成了重大挑战。虽然自动产生的摘要可能流畅，但通常缺乏对原始文档的忠实性。在低资源环境下，如跨语言转移，这个问题变得更加突出。由于现有的忠实性测量方法主要集中于英语，因此在跨语言环境中甚至衡量这种现象的程度也很困难。为了解决这个问题，作者首先提出了一种新的度量方法mFACT，通过从多个英语的忠实性测量结果中借鉴翻译基础知识为非英语摘要评估其忠实性。然后，他们提出了一种简单而有效的方法来通过跨语言转移减少幻觉，该方法将每个训练样本的损失乘以其忠实性得分。通过多种语言的广泛实验，作者证明了mFACT是最适合检测幻觉的度量方法。此外，他们发现他们的提出的加权方法可以缓解幻觉问题。

    Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
    
[^183]: 平方神经分布族：一种新的可计算密度模型类

    Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v1 [cs.LG])

    [http://arxiv.org/abs/2305.13552](http://arxiv.org/abs/2305.13552)

    提出一种新的可计算密度模型类——平方神经分布族，其通过对神经网络的2范数进行平方和基于某个基础度量进行归一化，严格推广了经典指数族，具有闭性条件推断和可计算的边际分布。

    

    概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一种新的概率分布类别，称为平方神经分布族（SNEFY），通过对神经网络的2范数进行平方并基于某个基础度量进行归一化。类似于无穷宽的神经网络和高斯过程之间的广泛联系的推理，我们展示了在许多感兴趣的情况下，SNEFY具有封闭形式的标准化常数，因此是灵活且完全可计算密度模型。SNEFY严格推广了经典的指数族，对于条件推断具有闭性，并且具有可计算的边际分布。我们在各种密度估计和条件密度估计任务中展示其实用性。

    Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.
    
[^184]: Sketch-and-Project Meets Newton Method: 具有低秩更新的全局$\mathcal O(k^{-2})$ 收敛性

    Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates. (arXiv:2305.13082v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2305.13082](http://arxiv.org/abs/2305.13082)

    本文提出了一种新的基于草图和投影的Newton方法，具有快速的全局收敛率，适用于自共轭函数，具有草图和投影方法的低迭代成本，全秩Newton类方法的最先进全局收敛率以及阻尼Newton方法的算法简单性。

    

    本文提出了一种新的基于草图和投影的Newton方法，具有快速的$\mathcal O(k^{-2})$ 全局收敛率，适用于自共轭函数。我们的方法可以从三个方面来看待：i) 作为一个草图和投影算法，对Newton方法的更新进行投影，ii) 作为在被草图化子空间中进行立方正则化的Newton方法，和 iii) 作为在被草图化子空间中进行阻尼Newton方法。SGN继承了这三个方面的优点：草图和投影方法的低迭代成本，全秩Newton类方法的最先进$\mathcal O(k^{-2})$全局收敛率以及阻尼Newton方法的算法简单性。最后，我们证明了它与基准算法具有相当的实证性能。

    In this paper, we propose the first sketch-and-project Newton method with fast $\mathcal O(k^{-2})$ global convergence rate for self-concordant functions. Our method, SGN, can be viewed in three ways: i) as a sketch-and-project algorithm projecting updates of Newton method, ii) as a cubically regularized Newton ethod in sketched subspaces, and iii) as a damped Newton method in sketched subspaces. SGN inherits best of all three worlds: cheap iteration costs of sketch-and-project methods, state-of-the-art $\mathcal O(k^{-2})$ global convergence rate of full-rank Newton-like methods and the algorithm simplicity of damped Newton methods. Finally, we demonstrate its comparable empirical performance to baseline algorithms.
    
[^185]: 非参数方法下的无分布模型无偏回归校准

    Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods. (arXiv:2305.12283v1 [cs.LG])

    [http://arxiv.org/abs/2305.12283](http://arxiv.org/abs/2305.12283)

    本文提出一种基于非参数方法的无分布模型无偏回归校准方法，具有计算效率和统计一致性，并建立了校准误差的上限和下限的统计保证和优势。

    

    本文研究回归模型的不确定性量化问题，提出一种基于非参数方法的校准方法，该方法不依赖于底层预测模型，具有计算效率和统计一致性。我们建立了校准误差的上限和下限，并证明了我们提出的方法的统计保证和优势。

    In this paper, we consider the uncertainty quantification problem for regression models. Specifically, we consider an individual calibration objective for characterizing the quantiles of the prediction model. While such an objective is well-motivated from downstream tasks such as newsvendor cost, the existing methods have been largely heuristic and lack of statistical guarantee in terms of individual calibration. We show via simple examples that the existing methods focusing on population-level calibration guarantees such as average calibration or sharpness can lead to harmful and unexpected results. We propose simple nonparametric calibration methods that are agnostic of the underlying prediction model and enjoy both computational efficiency and statistical consistency. Our approach enables a better understanding of the possibility of individual calibration, and we establish matching upper and lower bounds for the calibration error of our proposed methods. Technically, our analysis co
    
[^186]: 带有时间预测编码的时序记忆

    Sequential Memory with Temporal Predictive Coding. (arXiv:2305.11982v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.11982](http://arxiv.org/abs/2305.11982)

    该论文提出了一种基于PC的新型时序记忆模型，称为时间预测编码（tPC），可以通过生物可行的神经实现准确地记忆和检索连续输入。其中tPC可以被看作是一种经典异向性霍普菲尔德网络（AHN），具有更稳定的性能，并且可以编码上下文相关信息，区分在序列中出现的重复元素。

    

    对于生物体存储事件序列的时间顺序至关重要，然而大脑中支配时序记忆的计算机制仍不清楚。本文受到神经科学理论和预测编码（PC）在静态存储任务中的成功启示，提出了一种基于PC的新型时序记忆模型，称为时间预测编码（tPC）。我们展示了我们的tPC模型可以通过生物可行的神经实现准确地记忆和检索连续输入。重要的是，我们的分析研究表明，tPC可以被看作是一种具有隐式统计白化过程的经典异向性霍普菲尔德网络（AHN），这会在结构化输入的时序记忆任务中导致更稳定的性能。此外，我们发现具有多层结构的tPC可以编码上下文相关信息，因此可以区分在序列中出现的重复元素。

    Memorizing the temporal order of event sequences is critical for the survival of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC with a multi-layer structure can encode context-dependent information, thus distinguishing between repeating elements appearing in a sequence, a computation attribute
    
[^187]: 回收和精馏：带有注意力映射重用和蒸馏屏蔽的基于Transformer的语音自监督学习模型的通用压缩策略

    Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])

    [http://arxiv.org/abs/2305.11685](http://arxiv.org/abs/2305.11685)

    本研究提出了一种基于Transformer的语音自监督学习模型的通用压缩策略，通过重用注意力映射和蒸馏屏蔽来提高学生模型的语音表示质量，实现了较低的错误率。

    

    基于Transformer的语音自监督学习模型在各种语音处理任务中表现出惊人的性能。然而，语音 SSL 模型中庞大的参数数量需要压缩成更紧凑的模型，以便在学术界或小公司中更广泛地使用。本研究建议重用Transformer层之间的注意力映射，因此可以删除键和查询参数，同时保留层数。此外，我们提出了一种新的蒸馏策略，以提高学生模型的语音表示质量。我们扩展了蒸馏损失，利用遮罩和未遮罩的语音帧，充分利用教师模型的高质量表示。我们的通用压缩策略产生的学生模型在SUPERB基准测试中实现了7.72%的音素误差率（PER）和9.96%的单词错误率（WER）。

    Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
    
[^188]: 基于几何感知的自回归模型用于新型电磁量计几何模拟的泛化研究

    Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.11531](http://arxiv.org/abs/2305.11531)

    基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。

    

    在粒子物理数据分析中，生成对撞产物的模拟探测器响应至关重要，但计算非常昂贵。其中一个子探测器，电磁量计由于其单元格的高粒度和复杂的相互作用而占据了计算时间的主导地位。生成模型可以提供更快的样本生成，但目前需要大量努力来优化特定探测器几何形状的性能，通常需要许多网络来描述不同的单元格大小和排列方式，这些模型不能推广到其他几何形状。我们开发了一种“几何感知”自回归模型，学习电磁量计响应如何随几何形状变化，能够生成看不见的几何形状的模拟响应而无需其他训练。该几何感知模型在涉及关键响应的生成和真实分布之间的Wasserstein距离等指标上比基线模型优越50％。我们通过在二维空间中模拟具有前所未有的细粒度，并扩展到非平面几何形状，展示了该方法的可行性和速度。

    Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
    
[^189]: 一种优化且可扩展的矩阵机制用于扰动边缘数据下凸损失函数。

    An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions. (arXiv:2305.08175v1 [cs.DB])

    [http://arxiv.org/abs/2305.08175](http://arxiv.org/abs/2305.08175)

    ResidualPlanner是一种用于带有高斯噪声的边缘的矩阵机制，既优化又可扩展，可以优化许多可以写成边际方差的凸函数的损失函数。

    

    扰动的边缘数据是一种常见的保护数据隐私的形式，可用于诸如列联表分析、贝叶斯网络构建和合成数据生成等下游任务。我们提出了ResidualPlanner，这是一种用于带有高斯噪声的边缘的矩阵机制，既优化又可扩展。ResidualPlanner可以优化许多可以写成边际方差的凸函数的损失函数。此外，ResidualPlanner可以在几秒钟内优化大规模设置中的边缘准确性，即使之前的最先进技术（HDMM）也会占用过多的内存。甚至在具有100个属性的数据集上也可以在几分钟内运行。此外，ResidualPlanner还可以有效地计算每个边缘的方差/协方差值（之前的方法会很快失败）。

    Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.  We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly
    
[^190]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^191]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^192]: 从单视图图像学习可控三维扩散模型

    Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v1 [cs.CV])

    [http://arxiv.org/abs/2304.06700](http://arxiv.org/abs/2304.06700)

    该论文介绍了一种名为Control3Diff的三维扩散模型，结合了扩散模型和3D GANs的优点，可以用于单视图数据集的多功能、可控的三维感知图像合成。

    

    最近，扩散模型已经成为2D领域生成建模的事实标准。然而，由于获取三维基准数据进行训练的困难，将扩散模型扩展到三维领域是具有挑战性的。另一方面，将隐式三维表示集成到GANs中的3D GANs在仅训练单视图图像数据集时展示了显着的3D感知生成。然而，3D GANs没有提供精确控制图像合成的简单方法。为了解决这些挑战，我们提出了Control3Diff，这是一种结合了扩散模型和3D GANs优点的三维扩散模型，用于单视图数据集的多功能，可控的三维感知图像合成。Control3Diff明确地建模了潜在的潜在分布（可以是外部输入条件下的潜在分布），从而允许在扩散过程中直接控制。此外，我们的方法通用，可适用于任何类型的控制输入，使我们能够使用相同的基础体系结构对其进行训练。

    Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same di
    
[^193]: 什么让数据适合于局部连接神经网络？一种基于量子纠缠的必要且充分条件

    What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11249](http://arxiv.org/abs/2303.11249)

    本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。

    

    关于数据分布适用于深度学习的问题是一个基本的开放性问题。本文采用来自量子物理学的理论工具，针对包括卷积神经网络、循环神经网络和局部自注意力模型在内的广泛的局部连接神经网络，解决了这个问题。我们的主要理论结果是，在某些特征的规范划分下，当数据分布接受低量子纠缠时，特定的局部连接神经网络才能够准确地预测该数据分布。作为本结果的实际应用，我们导出了一种预处理方法，以增强数据分布适合局部连接神经网络的性能。在各种数据集上对广泛的模型进行实验，证明了我们的发现。我们希望我们使用量子纠缠将鼓励形式推理的物理工具来进一步采用。

    The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
    
[^194]: 利用循环神经网络研究拓扑序

    Investigating Topological Order using Recurrent Neural Networks. (arXiv:2303.11207v2 [cond-mat.str-el] UPDATED)

    [http://arxiv.org/abs/2303.11207](http://arxiv.org/abs/2303.11207)

    本研究通过循环神经网络有效地捕获了多体哈密顿量中的拓扑序，进一步证明了循环神经网络波函数是研究物态相的一个有力工具。

    

    循环神经网络被用于自然语言处理，但它在描述强关联量子多体系统方面也很有前途。本研究利用二维循环神经网络研究了两个典型展现拓扑序的多体哈密顿量。具体来说，我们证明了循环神经网络波函数可以通过估算其拓扑纠缠熵，有效地捕获扭曲编码和蜂巢格上 Bose-Hubbard 自旋液体的拓扑序。我们还发现，循环神经网络更倾向于利用很少纠缠状态的相干叠加。总的来说，我们的研究表明，循环神经网络波函数是研究不对称破缺之外物态相的强有力工具。

    Recurrent neural networks (RNNs), originally developed for natural language processing, hold great promise for accurately describing strongly correlated quantum many-body systems. Here, we employ 2D RNNs to investigate two prototypical quantum many-body Hamiltonians exhibiting topological order. Specifically, we demonstrate that RNN wave functions can effectively capture the topological order of the toric code and a Bose-Hubbard spin liquid on the kagome lattice by estimating their topological entanglement entropies. We also find that RNNs favor coherent superpositions of minimally-entangled states over minimally-entangled states themselves. Overall, our findings demonstrate that RNN wave functions constitute a powerful tool to study phases of matter beyond Landau's symmetry-breaking paradigm.
    
[^195]: 使用基于OWA的链接、Lance-Williams公式和树枝反转的层次聚类

    Hierarchical clustering with OWA-based linkages, the Lance-Williams formula, and dendrogram inversions. (arXiv:2303.05683v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05683](http://arxiv.org/abs/2303.05683)

    本研究通过使用基于OWA的链接，Lance-Williams公式和树枝反转技术，推广了凝聚层次聚类，并提供了一些条件用于保证结果的树枝图没有不美观的反转。

    

    基于有序加权平均（OWA）算子的凝聚层次聚类不仅推广了单个的、完全的和平均的链接，还包括基于一些最近或最远邻居、修剪和修整的点对相似性的平均值的集群间距。我们探索了著名的Lance-Williams更新公式与扩展的基于OWA的链接之间的关系，并通过无限系数序列生成权重。此外，我们提供了一些条件，以保证权重生成器产生的树枝图不会出现不美观的反转。

    Agglomerative hierarchical clustering based on Ordered Weighted Averaging (OWA) operators not only generalises the single, complete, and average linkages, but also includes intercluster distances based on a few nearest or farthest neighbours, trimmed and winsorised means of pairwise point similarities, amongst many others. We explore the relationships between the famous Lance-Williams update formula and the extended OWA-based linkages with weights generated via infinite coefficient sequences. Furthermore, we provide some conditions for the weight generators to guarantee the resulting dendrograms to be free from unaesthetic inversions.
    
[^196]: "Wasserstein Believer:通过可靠的潜在空间模型学习部分可观测环境下的信念更新"

    The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03284](http://arxiv.org/abs/2303.03284)

    本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。

    

    部分可观测马尔可夫决策过程（POMDP）是建模代理无法感知到完整状态的环境的有用工具。因此，代理需要考虑过去的观察和行动进行推理。但是，由于历史空间指数级增长，仅仅记住完整历史通常是不可行的。保持模拟真实状态的置信概率分布可以作为历史的充分统计量，但其计算需要访问环境的模型，因此也是不可行的。最先进的算法使用递归神经网络来压缩观察-行动历史以学习充分的统计量，但它们缺乏成功的保证并可能导致次优策略。为了克服这一点，我们提出了Wasserstein Belief Updater ，这是一种RL算法，它学习POMDP​​的潜在模型和置信更新的近似值。我们的方法具有理论成果。

    Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
    
[^197]: 针对稳健视觉感知的卷积视觉提示

    Convolutional Visual Prompt for Robust Visual Perception. (arXiv:2303.00198v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00198](http://arxiv.org/abs/2303.00198)

    本文介绍了一种针对稳健视觉感知的卷积视觉提示（CVP）方法，通过较少的可训练参数来避免自适应模型在无标签自监督测试时间设置下的过拟合，实验证明该方法可提高稳健性高达5.87%。

    

    视觉模型常常对未适应的离域样本很容易受攻击。虽然视觉提示为大规模视觉模型提供了一种轻量级的输入空间适应方法，但它们依赖于高维度的加性向量和标记数据。这导致在无标签的自监督测试时间设置下，调整模型容易出现过拟合。我们引入了卷积视觉提示（CVP）来实现无标签测试时间适应以提高稳健的视觉感知。CVP的结构化特性要求较少的可训练参数，仅为标准视觉提示的1％以下，从而抑制了过拟合。在广泛的离域视觉感知任务上进行的大量实验和分析表明，我们的方法是有效的，相比几种大规模模型，提高了稳健性高达5.87%。

    Vision models are often vulnerable to out-of-distribution (OOD) samples without adapting. While visual prompts offer a lightweight method of input-space adaptation for large-scale vision models, they rely on a high-dimensional additive vector and labeled data. This leads to overfitting when adapting models in a self-supervised test-time setting without labels. We introduce convolutional visual prompts (CVP) for label-free test-time adaptation for robust visual perception. The structured nature of CVP demands fewer trainable parameters, less than 1\% compared to standard visual prompts, combating overfitting. Extensive experiments and analysis on a wide variety of OOD visual perception tasks show that our approach is effective, improving robustness by up to 5.87% over several large-scale models.
    
[^198]: 基于稀疏高斯过程的连续和离散空间的回归传感器放置优化

    Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00028](http://arxiv.org/abs/2303.00028)

    本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。

    

    本文提出了一种基于稀疏高斯过程方法的传感器放置方案，用于监测温度、降水等空间（或时空）相关现象。与现有的基于高斯过程的传感器放置方法不同，我们将已知内核函数参数的稀疏高斯过程拟合到环境中随机采样的未标记位置，并通过学习得到的诱导点来解决连续空间的传感器放置问题。使用稀疏高斯过程避免了对环境进行离散化，并将计算复杂度从立方级别降低到线性级别。在候选传感器放置点集合的限制下，我们可以使用贪婪顺序选择算法来找到较好的解决方案。

    We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
    
[^199]: 对于多臂赌博机问题，FTRL算法与一般正则化和多个最优臂的最佳保证有所提升

    Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms. (arXiv:2302.13534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13534](http://arxiv.org/abs/2302.13534)

    本文提出了对于多臂赌博机问题的改进的FTRL算法，通过使用一系列正则化器和新的学习率计划，不再需要假设存在唯一最优臂，并对某些正则化器的遗憾界限进行了改进。

    

    本文研究设计自适应多臂赌博机算法，同时在随机设置和对抗设置中表现最优（通常称为最佳保证）。最近的一系列研究表明，当正确配置和分析时，原本设计用于对抗设置的Follow-the-Regularized-Leader（FTRL）算法实际上可以最优地适应随机设置。然而，这些结果关键依赖于存在唯一最优臂的假设。最近，Ito（2021）首次采取措施删除了一个特定FTRL算法对于$\frac{1}{2}$-Tsallis熵正则化的不可取唯一性假设。本文对这一结果进行了显著改进和推广，表明FTRL算法在广泛的正则化器和新的学习率计划下不需要唯一性。对于某些正则化器，我们的遗憾界限与前人的结果相比也有所提高。

    We study the problem of designing adaptive multi-armed bandit algorithms that perform optimally in both the stochastic setting and the adversarial setting simultaneously (often known as a best-of-both-world guarantee). A line of recent works shows that when configured and analyzed properly, the Follow-the-Regularized-Leader (FTRL) algorithm, originally designed for the adversarial setting, can in fact optimally adapt to the stochastic setting as well. Such results, however, critically rely on an assumption that there exists one unique optimal arm. Recently, Ito (2021) took the first step to remove such an undesirable uniqueness assumption for one particular FTRL algorithm with the $\frac{1}{2}$-Tsallis entropy regularizer. In this work, we significantly improve and generalize this result, showing that uniqueness is unnecessary for FTRL with a broad family of regularizers and a new learning rate schedule. For some regularizers, our regret bounds also improve upon prior results even when
    
[^200]: 锂金属电池质量控制通过Transformer-CNN分割

    Lithium Metal Battery Quality Control via Transformer-CNN Segmentation. (arXiv:2302.04824v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04824](http://arxiv.org/abs/2302.04824)

    通过Transformer-CNN分割方法，本研究提出了一种新的语义分割方法TransforCNN，能够准确地从XCT图像中分割出锂金属电池中的树枝晶。与其他算法相比，TransforCNN具有更好的性能。

    

    锂金属(Li)电池因其高理论能量密度而成为下一代电池系统的潜在选择。然而，由于异质性锂(Li)沉积，会形成称为树枝晶的缺陷，这阻碍了锂金属电池的发展和利用。观察树枝晶形态的非破坏性技术通常使用X射线计算机断层扫描(XCT)提供断面视图。为了获得电池内部的三维结构，图像分割成为定量分析XCT图像的必要条件。本研究提出了一种新的语义分割方法，使用一种名为TransforCNN的基于Transformer的神经网络，能够从XCT数据中分割出树枝晶。此外，我们将所提出的TransforCNN与U-Net、Y-Net和E-Net等三种算法进行了性能比较，这些算法是用于XCT分析的集成网络模型。我们的结果显示在评估过分割时使用TransforCNN的优势。

    Lithium metal battery (LMB) has the potential to be the next-generation battery system because of its high theoretical energy density. However, defects known as dendrites are formed by heterogeneous lithium (Li) plating, which hinders the development and utilization of LMBs. Non-destructive techniques to observe the dendrite morphology often use X-ray computed tomography (XCT) to provide cross-sectional views. To retrieve three-dimensional structures inside a battery, image segmentation becomes essential to quantitatively analyze XCT images. This work proposes a new semantic segmentation approach using a transformer-based neural network called TransforCNN that is capable of segmenting out dendrites from XCT data. In addition, we compare the performance of the proposed TransforCNN with three other algorithms, such as U-Net, Y-Net, and E-Net, consisting of an Ensemble Network model for XCT analysis. Our results show the advantages of using TransforCNN when evaluating over-segmentation me
    
[^201]: 阅读并获得回报：在与指导手册的帮助下学习玩Atari游戏

    Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04449](http://arxiv.org/abs/2302.04449)

    本论文提出了阅读并奖励的框架，通过阅读Atari游戏开发者发布的指导手册，以提高强化学习算法在Atari游戏中的效率。该框架包含一个QA提取模块和一个推理模块，能够从指导手册中提取关键信息，并评估物体与智能体的交互效果。

    

    长期以来，高样本复杂性一直是强化学习面临的挑战。然而，人类学习执行任务的方式不仅仅是通过交互或演示，还包括阅读非结构化文本文档，例如指导手册。指导手册和维基页面是最丰富的数据之一，它们可以提供有关宝贵特征、策略、任务特定的环境动态和奖励结构的信息，因此我们假设利用人写的指导手册来帮助学习特定任务的策略将导致更高效和更优秀的智能体。我们提出了阅读并奖励的框架。阅读并奖励通过阅读Atari游戏开发者发布的指导手册来加速RL算法。我们的框架包括一个QA提取模块，用于提取和总结指导手册中的相关信息，以及一个推理模块，根据指导手册中的信息评估物体-智能体的交互效果。一个辅助的反馈机制可以提高效果。

    High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
    
[^202]: ZipLM: 语言模型的推理感知结构化剪枝

    ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04089](http://arxiv.org/abs/2302.04089)

    ZipLM是一种新型的语言模型压缩方法，能够在任何给定的推理环境中实现与目标运行速度相匹配的最先进压缩模型。与现有方法相比，ZipLM在速度和准确性之间取得了最佳的权衡，并且以更低的计算成本获得了更好的结果。

    

    大规模语言模型（LLM）的突破性性能给计算和部署成本带来了重大负担。本文通过提出一种名为ZipLM的新型结构化压缩方法，向解决这个问题迈进。ZipLM在达到一组目标推理时速度优于现有的方法，并在任何给定的推理环境中匹配一组目标运行时加速度。具体而言，给定模型、数据集、推理环境以及一组加速度目标，ZipLM迭代地识别并删除损失时长权衡最差的组件。与先前专门用于后训练/一次性或逐渐压缩设置，并且仅适用于特定的模型家族（如BERT（编码器）或GPT（解码器））的方法不同，ZipLM在所有这些设置中生成了最先进的压缩模型。此外，与先前的蒸馏和修剪方法相比，ZipLM以更低的计算成本获得了更好的结果。

    The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and prunin
    
[^203]: 高效的对抗性对比学习：基于鲁棒性感知的数据核心集选择

    Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03857](http://arxiv.org/abs/2302.03857)

    该研究提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法，能够有效地加速对抗性对比学习（ACL）并维持其强鲁棒性和泛化性能。

    

    对抗性对比学习（ACL）不需要昂贵的数据注释，但可以输出抵抗对抗性攻击并且适用于广泛下游任务的强鲁棒性表示。然而，ACL需要巨大的运行时间才能生成所有训练数据的对抗变体，这限制了其在大型数据集上的可扩展性。为了加速ACL，本文提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法。RCS不需要标签信息，搜索最小化表示分歧的信息子集，即自然数据和其虚拟对抗变体之间表示的距离。RCS的基本解法是遍历所有可能的子集，计算复杂度高。因此，我们在理论上将RCS转化为子模最大化的替代问题，利用贪心搜索是原问题的有效解决方案，同时具有原问题的最优性保证。实验结果表明，RCS可以通过减少训练数据量有效地加速ACL，并且仍然保持其强鲁棒性和泛化性能。

    Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
    
[^204]: 基于关系Weisfeiler-Leman的链路预测理论

    A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02209](http://arxiv.org/abs/2302.02209)

    本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。

    

    图神经网络是用于图结构数据表示学习的重要模型。尽管我们已经很好地理解了这些模型在简单图上的能力和局限性，但对于知识图谱，我们的理解仍然不完整。本文的目标是为知识图谱中的图神经网络提供系统性的理解，以解决链路预测等重要任务。我们的分析涉及一种统一的视角、看似不相关的模型，并解锁了一系列其他模型。通过相应的关系Weisfeiler-Leman算法，表征了各种模型的表达能力。此分析被扩展以对图神经网络类别捕捉的函数类进行精确逻辑描述。提出的理论发现解释了一些广泛采用的实际设计选择的优点，并得到了经验验证。

    Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
    
[^205]: RS-Del: 随机删除对序列分类器的编辑距离鲁棒性证明

    RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion. (arXiv:2302.01757v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.01757](http://arxiv.org/abs/2302.01757)

    本文提出了一种适应于离散序列分类器的随机删除（RS-Del）平滑机制，提供针对编辑距离受限对抗性的鲁棒性证明。

    

    随机平滑是构建具有认证鲁棒性的分类器的主要方法。现有的随机平滑方法主要针对具有连续输入（如图像）的分类器，其中常常研究$\ell_p$范数受限的对抗性示例。然而，对于具有离散或可变大小输入（例如源代码）的分类器的研究较少，这些分类器需要不同的威胁模型和平滑机制。在本研究中，我们修改了随机平滑方法，以适用于离散序列分类器，以提供针对编辑距离受限的对抗性的可证明的鲁棒性。我们提出的平滑机制随机删除（RS-Del）应用了随机删除编辑，这种方式（也许令人惊讶地）足以提供针对对抗性删除、插入和替换编辑的鲁棒性。我们的认证证明不同于传统的Neyman-Pearson方法，在我们的情况下无法计算，而是围绕着另一种方式进行组织。

    Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where $\ell_p$-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism randomized deletion (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized aro
    
[^206]: 使用增强学习提高正电子发射断层扫描探测器的时间分辨率——一种残差物理方法

    Improving the Timing Resolution of Positron Emission Tomography Detectors Using Boosted Learning -- A Residual Physics Approach. (arXiv:2302.01681v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01681](http://arxiv.org/abs/2302.01681)

    本论文提出了一种残差物理方法，通过机器学习优化正电子发射断层扫描探测器，提高时间分辨率，减少患者接受的放射性剂量。

    

    人工智能正在进入医学成像领域，主要用于增强图像重建。然而，从信号检测到计算的整个处理过程的改进可能带来显著的益处。本研究提出了一种用于探测器优化的新颖且多功能的方法，使用了机器学习和残差物理。我们将该概念应用于正电子发射断层扫描（PET），目的是提高巧合时间分辨率（CTR）。PET通过检测闪烁探测器中的光子来可视化身体的代谢过程。改善CTR性能可以减少患者接受的放射性剂量。现代PET探测器具有复杂的物理和电子系统，需要专门的校准技术。传统方法主要依赖于成功描述主要探测器特性的分析公式。然而，当考虑高

    Artificial intelligence (AI) is entering medical imaging, mainly enhancing image reconstruction. Nevertheless, improvements throughout the entire processing, from signal detection to computation, potentially offer significant benefits. This work presents a novel and versatile approach to detector optimization using machine learning (ML) and residual physics. We apply the concept to positron emission tomography (PET), intending to improve the coincidence time resolution (CTR). PET visualizes metabolic processes in the body by detecting photons with scintillation detectors. Improved CTR performance offers the advantage of reducing radioactive dose exposure for patients. Modern PET detectors with sophisticated concepts and read-out topologies represent complex physical and electronic systems requiring dedicated calibration techniques. Traditional methods primarily depend on analytical formulations successfully describing the main detector characteristics. However, when accounting for high
    
[^207]: 通过稀疏编码实现无约束动态遗憾

    Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13349](http://arxiv.org/abs/2301.13349)

    本文探讨了在线线性优化（OLO）涉及无约束问题和动态遗憾问题的复杂性，提出了一种通过重新构造问题为稀疏编码的复杂度度量方式，在适应性和应用上有较好的应用价值。

    

    受时间序列预测的影响，本研究探讨了在线线性优化（OLO）在两个问题结构的耦合下的情况：域无界，而算法的性能是通过动态遗憾来衡量的。处理任一问题都要求遗憾界限依赖于比较序列的某些复杂度量度 - 特别是无约束OLO中的比较器范数，以及动态遗憾中的路径长度。与最近一篇文章(Jacobsen& Cutkosky，2022)适应这两个复杂度量度相比，我们提出了一种通过重新构造问题为稀疏编码的复杂度度量方式。可以通过一个简单的模块化框架实现适应性，这个框架自然地利用了环境更复杂的前置知识。同时，我们还提出了一种新的静态无约束OLO梯度自适应算法，使用了新颖的连续时间机制设计。这可能是具有独立兴趣的。

    Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
    
[^208]: 位置-尺度噪声模型中因果推断的最大似然与独立性检验比较研究

    Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing. (arXiv:2301.12930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12930](http://arxiv.org/abs/2301.12930)

    通过引入异方差位置-尺度噪声函数模型，该论文在正确说明噪声分布的情况下，通过最大似然实现了最先进的准确性。但是，在用户错误指定噪声分布的形式时，分析表明因果推断的精度会急剧下降。因此，该论文提出通过因果模型选择实现稳定而准确的因果推断。

    

    因果发现的一个基本问题是推断两个随机变量之间的正确因果方向。最近引入的异方差位置-尺度噪声函数模型 (LSNM) 结合了表达能力和可识别性保证，在正确指定噪声分布的情况下，通过最大似然实现了最先进的准确性。然而，我们通过广泛的实证评估表明，当用户错误指定噪声分布的形式时，精度会急剧下降。我们的分析表明，这种失败主要发生在反因果方向的条件方差小于因果方向的条件方差的情况下。作为一种替代方案，发现通过因果模型选择可以在缺乏噪声分布知识的情况下，实现稳定而准确的因果推断。

    A fundamental problem of causal discovery is cause-effect inference, learning the correct causal direction between two random variables. Significant progress has been made through modelling the effect as a function of its cause and a noise term, which allows us to leverage assumptions about the generating function class. The recently introduced heteroscedastic location-scale noise functional models (LSNMs) combine expressive power with identifiability guarantees. LSNM model selection based on maximizing likelihood achieves state-of-the-art accuracy, when the noise distributions are correctly specified. However, through an extensive empirical evaluation, we demonstrate that the accuracy deteriorates sharply when the form of the noise distribution is misspecified by the user. Our analysis shows that the failure occurs mainly when the conditional variance in the anti-causal direction is smaller than that in the causal direction. As an alternative, we find that causal model selection throu
    
[^209]: 图形生成模型评估的曲率滤波器

    Curvature Filtrations for Graph Generative Model Evaluation. (arXiv:2301.12906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12906](http://arxiv.org/abs/2301.12906)

    该论文使用图形曲率描述符和拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。

    

    图形生成模型评估需要了解分布级别上的图形差异，这需要能够以有效的方式利用图形的显著属性。曲率是图形的一种属性，最近开始证明其在描述图形方面很有用。然而，其表达性质、稳定性和在模型评估中的实际效用仍然很少被探索。我们将图形曲率描述符与拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。

    Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property of graphs, and has recently started to prove useful in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.
    
[^210]: 分布鲁棒安全强化学习的风险厌恶模型不确定性

    Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12593](http://arxiv.org/abs/2301.12593)

    该论文提出了一种分布鲁棒安全强化学习框架，通过使用失真风险度量来处理模型不确定性。该方法不需要极小极大优化，具有高效且不依赖模型的特点。实验证明该框架能够在具有安全约束的控制任务中实现稳健的性能和安全性。

    

    许多现实领域需要在不确定的环境中进行安全决策。本研究引入了一个深度强化学习框架来解决这个重要问题。我们考虑过渡模型的分布，并通过使用相干失真风险度量来对模型不确定性采取风险厌恶的观点。我们通过展示它等价于一类特定的分布鲁棒安全强化学习问题，为这个框架提供了鲁棒性保证。然而，与现有的深度强化学习鲁棒性方法不同，我们的表达不涉及极小化最大优化。这导致我们的方法可以高效、不依赖模型地在单个训练环境中仅需要标准数据收集来实施。在具有安全约束的连续控制任务的实验中，我们证明了我们的框架在部署时能够产生稳健的性能和安全性。

    Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test e
    
[^211]: 基于边界框的多目标贝叶斯优化在输入不确定性下的风险衡量

    Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty. (arXiv:2301.11588v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11588](http://arxiv.org/abs/2301.11588)

    该论文提出了一种基于边界框的多目标贝叶斯优化方法，能够在输入不确定性下高效地识别风险衡量定义的帕累托前沿。该方法具有理论保证，并通过构建高概率边界框和选择下一个评估点的方法来减少不确定性。

    

    在本研究中，我们提出了一种新颖的多目标贝叶斯优化（MOBO）方法，用于在输入不确定性（IU）存在的情况下高效地识别由风险衡量定义的帕累托前沿（PF）。现有的IU下帕累托优化的BO方法是特定风险或者没有理论保证的，而我们提出的方法涉及一般风险衡量并具有理论保证。所提方法的基本思想是假设黑箱函数的高斯过程（GP）模型，并使用GP模型构建风险衡量的高概率边界框。此外，为了减少非支配边界框的不确定性，我们提出了一种使用基于边界框的拟距离的最大值定义的最大最小距离选择下一个评估点的方法。作为理论分析，我们证明了该算法可以在有限次迭代中返回任意精确的解。

    In this study, we propose a novel multi-objective Bayesian optimization (MOBO) method to efficiently identify the Pareto front (PF) defined by risk measures for black-box functions under the presence of input uncertainty (IU). Existing BO methods for Pareto optimization in the presence of IU are risk-specific or without theoretical guarantees, whereas our proposed method addresses general risk measures and has theoretical guarantees. The basic idea of the proposed method is to assume a Gaussian process (GP) model for the black-box function and to construct high-probability bounding boxes for the risk measures using the GP model. Furthermore, in order to reduce the uncertainty of non-dominated bounding boxes, we propose a method of selecting the next evaluation point using a maximin distance defined by the maximum value of a quasi distance based on bounding boxes. As theoretical analysis, we prove that the algorithm can return an arbitrary-accurate solution in a finite number of iterati
    
[^212]: 通过稳健优化找出反事实解释的区域

    Finding Regions of Counterfactual Explanations via Robust Optimization. (arXiv:2301.11113v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11113](http://arxiv.org/abs/2301.11113)

    该论文提出了一种通过稳健优化计算反事实解释（CE）区域的方法，使用户能够选择适当的措施以获得所需的结果，此方法在逻辑回归、决策树、随机森林和神经网络等最常见的机器学习方法上证明了收敛结果。

    

    反事实解释在检测偏见和提高数据驱动分类模型的可解释性方面发挥着重要作用。一个反事实解释（CE）是一个最小的扰动数据点，使得模型的决策发生变化。现有大多数方法只能提供一个CE，可能对于用户来说是难以实现的。在这项工作中，我们推导出一种迭代方法来计算稳健CE，即在特征轻微扰动后仍然有效的CE。为此，我们的方法提供了整个CE区域，使用户能够选择适当的措施以获得所需的结果。我们使用了稳健优化的算法思想，并证明了最常见的机器学习方法（包括逻辑回归、决策树、随机森林和神经网络）的收敛结果。我们的实验结果表明，我们的方法可以有效地为各种常见数据集和分类模型生成全局最佳的稳健CE。

    Counterfactual explanations play an important role in detecting bias and improving the explainability of data-driven classification models. A counterfactual explanation (CE) is a minimal perturbed data point for which the decision of the model changes. Most of the existing methods can only provide one CE, which may not be achievable for the user. In this work we derive an iterative method to calculate robust CEs, i.e. CEs that remain valid even after the features are slightly perturbed. To this end, our method provides a whole region of CEs allowing the user to choose a suitable recourse to obtain a desired outcome. We use algorithmic ideas from robust optimization and prove convergence results for the most common machine learning methods including logistic regression, decision trees, random forests, and neural networks. Our experiments show that our method can efficiently generate globally optimal robust CEs for a variety of common data sets and classification models.
    
[^213]: 通过学习保证提高公平性

    Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10813](http://arxiv.org/abs/2301.10813)

    该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。

    

    随着机器学习系统在越来越多的现实场景中得到广泛应用，对于隐藏在机器学习模型中的潜在歧视的担忧正在增加。许多技术已经被开发出来以增强公平性，包括常用的群体公平性度量和几种结合集成学习的公平感知方法。然而，现有的公平度量只能关注其中之一，即群体公平性或个体公平性，它们之间的硬性兼容性暗示了即使其中之一得到满足，仍可能存在偏见。此外，现有的提升公平性的机制通常只提供经验结果来证明其有效性，但很少有论文讨论公平性是否可以在理论上得到保证。为了解决这些问题，本文提出了一种公平质量度量方法——判别风险，以反映个体和群体公平性两个方面。此外，我们还研究了p...

    The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
    
[^214]: 基于对象中心表征的视频分解和预测的时间条件生成建模

    Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.08951](http://arxiv.org/abs/2301.08951)

    本文提出了一种基于时间条件生成模型的视频分解和预测方法，采用提升的对象和视图的潜在表征之间的解耦技术以及从预训练模型中学习隐式视角规则的方法解决了现有方法存在的部分或完全遮挡对象的形状无法被准确重建和新视角预测依赖于昂贵的视角注释的问题。

    

    人类在从多个视角感知世界时，即使某个对象被完全遮挡，也能以组合方式理解完整物体。与此同时，人类能够在观察多个视角之后想象新视角。最近多视图对象中心学习的显着进步仍存在一些未解决的问题：1）部分或完全遮挡对象的形状无法被准确重建。2）新视角预测依赖于昂贵的视角注释，而不是视图表征中的隐式规则。本文提出了一种用于视频的时间条件生成模型。为了准确重建对象的完整形状，我们增强了对象和视图的潜在表征之间的解耦，其中时间条件的视图的潜在表征与Transformer一起联合推断，然后输入到Slot Attention Networks (SANs)的顺序扩展中。我们进一步提出了从预训练模型中学习隐式视角规则的方法，从而消除了显式视角注释的需要。实验结果表明，我们的方法在部分和完全遮挡对象完成和新视角预测任务中优于现有方法。

    When perceiving the world from multiple viewpoints, humans have the ability to reason about the complete objects in a compositional manner even when an object is completely occluded from certain viewpoints. Meanwhile, humans are able to imagine novel views after observing multiple viewpoints. Recent remarkable advances in multi-view object-centric learning still leaves some unresolved problems: 1) The shapes of partially or completely occluded objects can not be well reconstructed. 2) The novel viewpoint prediction depends on expensive viewpoint annotations rather than implicit rules in view representations. In this paper, we introduce a time-conditioned generative model for videos. To reconstruct the complete shape of an object accurately, we enhance the disentanglement between the latent representations of objects and views, where the latent representations of time-conditioned views are jointly inferred with a Transformer and then are input to a sequential extension of Slot Attention
    
[^215]: 混合局部模式技术报告

    Technical Report of Mixing Local Patterns. (arXiv:2212.03654v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03654](http://arxiv.org/abs/2212.03654)

    本文旨在解决GNN在处理非同质图数据时性能不佳的问题，提出混合局部结构模式的概念，并从局部模式的随机性和近邻可聚合性两个方面深入研究，以实现更通用的GNN。

    

    图神经网络（GNN）在同质图数据上表现出卓越的性能，但在处理非同质图数据时却远远不如同质图数据，这是由于GNN的固有低通滤波特性所致。在面对分析具有不同同质性属性的复杂现实世界图表时，不应忽略图中潜在的混合局部结构模式。因此，必须充分考虑上述两个问题，即（\textbf{Q1}）和（\textbf{Q2}），以实现更通用的GNN。为此，我们尝试从两个方面深入了解它们，分别是\textbf{（A1）：局部模式的随机性}和\textbf{（A2）：近邻可聚合性}。

    Graph neural networks (GNNs) have shown remarkable performance on homophilic graph data while being far less impressive when handling non-homophilic graph data due to the inherent low-pass filtering property of GNNs. In the face of analyzing complex real-world graphs with different homophily properties, the latent mixed local structural patterns in graphs should not be neglected. Therefore, the two questions, i.e., (\textbf{Q1}) and (\textbf{Q2}) as motioned above, should be well considered on the way to implementing a more generic GNN. For this purpose, we attempt to get deeper insights into them from two points, respectively, \textbf{(A1): Randomness of local patterns}, and \textbf{(A2): Aggregability of near-neighbors}.
    
[^216]: 机器学习在抑郁症预测中的公平性与偏差矫正：来自四个不同研究人群的结果。

    Fairness and bias correction in machine learning for depression prediction: results from four different study populations. (arXiv:2211.05321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05321](http://arxiv.org/abs/2211.05321)

    本文研究了设计用于预测抑郁症的机器学习模型的公平性问题，并给出了有效的偏差矫正方法。这项研究强调了分析公平性以及透明报告的重要性。

    

    在心理保健中，尤其是在不受关注的人群中，存在着相当程度的污名化和不平等，这种不平等会扩散到收集的数据中。如果不适当地考虑机器学习(ML)模型所学的数据，这些模型就会强化已经存在于社会中的结构性偏差。在这里，我们对设计用于预测抑郁症的ML模型的偏差进行了有系统的研究，并在四个不同的案例研究中涵盖了不同的国家和人群。我们发现标准的ML方法显示出常规偏差行为。然而，我们展示了标准的缓解技术以及我们自己的事后方法可以有效地降低不公平偏差的级别。我们提供了实用的建议，以开发预测抑郁症风险的ML模型，在真实世界中提高公平性和信任度。没有单一最好的预测抑郁症的ML模型可以提供结果的平等。这强调了在模型选择过程中分析公平性以及透明报告的重要性。

    A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations, which spreads through collected data. When not properly accounted for, machine learning (ML) models learned from data can reinforce the structural biases already present in society. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. However, we show that standard mitigation techniques, and our own post-hoc method, can be effective in reducing the level of unfair bias. We provide practical recommendations to develop ML models for depression risk prediction with increased fairness and trust in the real world. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about t
    
[^217]: 一种加权方差变分自编码器模型用于语音增强

    A weighted-variance variational autoencoder model for speech enhancement. (arXiv:2211.00990v2 [cs.SD] CROSS LISTED)

    [http://arxiv.org/abs/2211.00990](http://arxiv.org/abs/2211.00990)

    我们提出了一种加权方差生成模型，通过在参数学习中加权每个频谱图时间帧的贡献，并使用Gamma先验分布将复杂值高斯分布改为学生t分布，实现了更有效和更鲁棒的语音增强。

    

    我们针对基于变分自编码器的语音增强问题进行研究，其中涉及在时频（TF）域中学习语音先验分布。通常假设生成模型为零均值复数高斯分布，其中语音信息编码在方差中作为潜在变量的函数。与常用方法相比，我们提出了一种加权方差生成模型，其中每个频谱图时间帧在参数学习中的贡献加权。我们对权重施加Gamma先验分布，这将有效地导致使用学生t分布而不是高斯分布进行语音生成建模。我们基于所提出的生成模型开发了高效的训练和语音增强算法。我们的实验结果在频谱图自编码和语音增强方面证明了所提出方法的有效性和鲁棒性，相比标准的未加权方差模型。

    We address speech enhancement based on variational autoencoders, which involves learning a speech prior distribution in the time-frequency (TF) domain. A zero-mean complex-valued Gaussian distribution is usually assumed for the generative model, where the speech information is encoded in the variance as a function of a latent variable. In contrast to this commonly used approach, we propose a weighted variance generative model, where the contribution of each spectrogram time-frame in parameter learning is weighted. We impose a Gamma prior distribution on the weights, which would effectively lead to a Student's t-distribution instead of Gaussian for speech generative modeling. We develop efficient training and speech enhancement algorithms based on the proposed generative model. Our experimental results on spectrogram auto-encoding and speech enhancement demonstrate the effectiveness and robustness of the proposed approach compared to the standard unweighted variance model.
    
[^218]: 政府中的人工智能：概念，标准和统一框架

    Artificial intelligence in government: Concepts, standards, and a unified framework. (arXiv:2210.17218v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2210.17218](http://arxiv.org/abs/2210.17218)

    本论文研究了人工智能在政府中的应用，强调了标准化操作程序和符合社会规范期望的重要性，并指出了多学科研究者在概念上的碎片化问题。

    

    最近人工智能特别是生成式语言模型的进展使政府改变的希望变得能够实现。鉴于新型人工智能系统的先进能力，至关重要的是使用标准操作程序将其嵌入，并符合社会的规范期望。多个领域的学者开始概念化人工智能应用的不同形式，强调其潜在的好处和风险。然而，文献仍然碎片化，社会科学领域的研究者如公共管理和政治科学，以及快速发展的人工智能，机器学习和机器人技术领域都在相对孤立的情况下发展概念。虽然有呼吁对政府中的人工智能进行形式化研究，但对于理解将人工智能嵌入公共领域的后果所需的理论观点的全面综合描述仍然缺乏。

    Recent advances in artificial intelligence (AI), especially in generative language modelling, hold the promise of transforming government. Given the advanced capabilities of new AI systems, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI applications may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full depth of theoretical perspectives needed to understand the consequences of embedding AI into a publi
    
[^219]: 通过多视角一致性学习可迁移的对抗鲁棒表示

    Learning Transferable Adversarial Robust Representations via Multi-view Consistency. (arXiv:2210.10485v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10485](http://arxiv.org/abs/2210.10485)

    本论文提出了一种新颖的元对抗多视角表示学习框架，通过差异更新编码器参数并施加对抗攻击来增强元学习器的鲁棒性。此外，通过最大化视图之间的一致性，实现在未见领域和任务上的可迁移的鲁棒表示。

    

    尽管在少样本学习问题上取得了成功，但大多数元学习模型只关注在干净样本上的良好性能，因此在面对对抗性扰动样本时容易崩溃。近期的一些工作表明，对抗学习和元学习的结合可以增强元学习器对对抗攻击的鲁棒性，但它们未能实现在未见领域和任务上的一般化对抗鲁棒性，这是元学习的最终目标。为了解决这个挑战，我们提出了一个新颖的元对抗多视角表示学习框架，该框架具有双编码器。具体而言，我们通过首先使用两个不同增强样本的数据实例之间的差异来更新编码器的参数，并进一步施加一种新颖的无标签对抗攻击来最大化它们的差异。然后，我们通过最大化视图之间的一致性来学习可迁移的鲁棒表示跨领域

    Despite the success on few-shot learning problems, most meta-learned models only focus on achieving good performance on clean examples and thus easily break down when given adversarially perturbed samples. While some recent works have shown that a combination of adversarial learning and meta-learning could enhance the robustness of a meta-learner against adversarial attacks, they fail to achieve generalizable adversarial robustness to unseen domains and tasks, which is the ultimate goal of meta-learning. To address this challenge, we propose a novel meta-adversarial multi-view representation learning framework with dual encoders. Specifically, we introduce the discrepancy across the two differently augmented samples of the same data instance by first updating the encoder parameters with them and further imposing a novel label-free adversarial attack to maximize their discrepancy. Then, we maximize the consistency across the views to learn transferable robust representations across doma
    
[^220]: 针对对抗自监督学习的有效目标攻击

    Effective Targeted Attacks for Adversarial Self-Supervised Learning. (arXiv:2210.10482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10482](http://arxiv.org/abs/2210.10482)

    该论文提出了一种针对无监督对抗训练框架的正向挖掘方法，通过选择最令人困惑但相似的目标样本来生成有效的对手，以提高训练模型的鲁棒性。

    

    最近，无监督对抗训练（AT）一直被认为是在没有任何标签信息的模型中实现鲁棒性的手段。以往的无监督AT研究主要集中在实现自监督学习（SSL）框架，通过最大化每个实例的分类损失来生成对抗性样本。然而，我们观察到，简单地通过无目标对抗攻击来最大化自监督训练损失往往会生成无效的对手，这些对手可能无法帮助提高训练模型的鲁棒性，特别是对于没有负样本的非对比性SSL框架。为了解决这个问题，我们提出了一种针对对抗SSL框架的新型正向挖掘方法来生成有效的对手。具体而言，我们介绍了一种基于熵和相似性的算法，为给定实例选择最令人困惑但相似的目标样本，然后扰动给定实例。

    Recently, unsupervised adversarial training (AT) has been highlighted as a means of achieving robustness in models without any label information. Previous studies in unsupervised AT have mostly focused on implementing self-supervised learning (SSL) frameworks, which maximize the instance-wise classification loss to generate adversarial examples. However, we observe that simply maximizing the self-supervised training loss with an untargeted adversarial attack often results in generating ineffective adversaries that may not help improve the robustness of the trained model, especially for non-contrastive SSL frameworks without negative examples. To tackle this problem, we propose a novel positive mining for targeted adversarial attack to generate effective adversaries for adversarial SSL frameworks. Specifically, we introduce an algorithm that selects the most confusing yet similar target example for a given instance based on entropy and similarity, and subsequently perturbs the given ins
    
[^221]: 基于项目反应理论的解释（eXirt）：一种在信任视角下解释树集成模型的模型特定方法

    Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective. (arXiv:2210.09933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09933](http://arxiv.org/abs/2210.09933)

    在解释黑盒模型的方法中没有共识，本论文提出了一种基于项目反应理论的新方法eXirt，用于解释使用表格数据的树集成模型，并通过全局特征排名来解释模型输入和预测之间的关系。

    

    近年来，XAI研究人员正规范化提案和开发新方法来解释黑盒模型，但在社区中对于使用哪种方法来解释这些模型尚无一致意见，而这种选择几乎直接与特定方法的流行度相关。诸如Ciu、Dalex、Eli5、Lofo、Shap和Skater等方法通过特征相关性的全局排名来解释黑盒模型，这些方法基于不同的方法学生成全局解释，说明模型输入如何解释其预测。在这种情况下，使用了41个数据集、4种树集成算法（Light Gradient Boosting、CatBoost、Random Forest和Gradient Boosting）和6种XAI方法来支持推出一种名为eXirt的新的XAI方法，该方法基于项目反应理论（IRT），旨在解释使用关于二分类问题的表格数据的树集成黑盒模型。在第一组分析中，164个全局特征

    In recent years, XAI researchers have been formalizing proposals and developing new methods to explain black box models, with no general consensus in the community on which method to use to explain these models, with this choice being almost directly linked to the popularity of a specific method. Methods such as Ciu, Dalex, Eli5, Lofo, Shap and Skater emerged with the proposal to explain black box models through global rankings of feature relevance, which based on different methodologies, generate global explanations that indicate how the model's inputs explain its predictions. In this context, 41 datasets, 4 tree-ensemble algorithms (Light Gradient Boosting, CatBoost, Random Forest, and Gradient Boosting), and 6 XAI methods were used to support the launch of a new XAI method, called eXirt, based on Item Response Theory IRT and aimed at tree-ensemble black box models that use tabular data referring to binary classification problems. In the first set of analyses, the 164 global featur
    
[^222]: 贝叶斯神经网络在地热资源评估中的应用：带有不确定性的预测

    Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty. (arXiv:2209.15543v2 [physics.geo-ph] UPDATED)

    [http://arxiv.org/abs/2209.15543](http://arxiv.org/abs/2209.15543)

    本论文介绍了将贝叶斯神经网络应用于地热资源评估的方法，利用监督学习问题和已知特征图进行预测，并可用于找出潜力更高的区域进行进一步的调查。

    

    我们考虑将机器学习应用于地热资源潜力评估。我们定义了一个监督学习问题，利用美国内华达州的10个地质和地球物理特征图来界定广泛区域内的地热潜力。我们有一组相对较小的正样本训练点（已知资源或活跃发电厂）和负样本训练点（已知钻探点但存在不适合的地热条件），并利用这些训练点来约束和优化人工神经网络进行分类任务。主要目标是在已知定义特征的大面积地理区域内预测未知点的地热资源潜力。这些预测可以用于确定有前景的区域进行详细调查。我们描述了我们的工作从定义特定的神经网络架构到训练和优化试验的演变。通过分析，我们揭示了不可避免的问题。

    We consider the application of machine learning to the evaluation of geothermal resource potential. A supervised learning problem is defined where maps of 10 geological and geophysical features within the state of Nevada, USA are used to define geothermal potential across a broad region. We have available a relatively small set of positive training sites (known resources or active power plants) and negative training sites (known drill sites with unsuitable geothermal conditions) and use these to constrain and optimize artificial neural networks for this classification task. The main objective is to predict the geothermal resource potential at unknown sites within a large geographic area where the defining features are known. These predictions could be used to target promising areas for further detailed investigations. We describe the evolution of our work from defining a specific neural network architecture to training and optimization trials. Upon analysis we expose the inevitable pro
    
[^223]: 一个用于基准测试聚类算法的框架

    A framework for benchmarking clustering algorithms. (arXiv:2209.09493v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09493](http://arxiv.org/abs/2209.09493)

    该论文开发了一个用于基准测试聚类算法的框架，旨在引入一种一致的方法进行测试。还汇总、改进和标准化了许多聚类基准数据集合，并包含了新数据集。提供了互动数据集浏览器、Python API的文档以及与其他编程语言进行框架交互的方式。

    

    聚类算法的评估可以涉及在各种基准问题上运行它们，并将其输出与专家提供的参考真实分组进行比较。不幸的是，许多研究论文和研究生论文只考虑了少数数据集。而且，很少考虑到在给定问题集上可以有许多同样有效的聚类方法的事实。为了克服这些限制，我们开发了一个框架，其目的是引入一种一致的方法来测试聚类算法。此外，我们还汇总、改进和标准化了机器学习和数据挖掘文献中提到的许多聚类基准数据集合，并包含了具有不同维度、大小和聚类类型的新数据集。还有一个互动数据集浏览器、Python API的文档以及如何与其他编程语言（如R或MATLAB）进行框架交互的描述。

    The evaluation of clustering algorithms can involve running them on a variety of benchmark problems, and comparing their outputs to the reference, ground-truth groupings provided by experts. Unfortunately, many research papers and graduate theses consider only a small number of datasets. Also, the fact that there can be many equally valid ways to cluster a given problem set is rarely taken into account. In order to overcome these limitations, we have developed a framework whose aim is to introduce a consistent methodology for testing clustering algorithms. Furthermore, we have aggregated, polished, and standardised many clustering benchmark dataset collections referred to across the machine learning and data mining literature, and included new datasets of different dimensionalities, sizes, and cluster types. An interactive datasets explorer, the documentation of the Python API, a description of the ways to interact with the framework from other programming languages such as R or MATLAB
    
[^224]: 多模块图神经网络的灵活表征促进更好的泛化能力

    Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06589](http://arxiv.org/abs/2209.06589)

    本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    

    图神经网络（GNN）已成为处理图结构数据的学习与推断的强大模型，但对于扩展到更大的图以及推广到从未见过的数据的基本限制的了解还不足。本文使用随机图生成器系统地研究了图的大小和结构属性如何影响GNN的预测性能，并提出多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
    
[^225]: 时间序列领域的离群检测：一种新颖的季节比例评分方法

    Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach. (arXiv:2207.04306v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04306](http://arxiv.org/abs/2207.04306)

    本文提出了一种在时间序列领域中进行离群检测的新方法，通过使用季节比例评分(SRS)来解决时间序列数据所带来的挑战。该方法通过将输入分解为按类别的语义组件和余项，并使用深度生成模型估计按类别的条件概率，从而计算季节比例评分，并通过识别阈值区间来检测离群样本。

    

    在实际应用中安全部署时间序列分类器依赖于能够检测不是由训练数据生成的数据，这个任务被称为离群检测(out-of-distribution, OOD)。本文考虑了时间序列领域中ODO检测的新问题。我们讨论了时间序列数据所带来的独特挑战，并解释了为什么来自图像领域的先前方法效果不佳。受到这些挑战的启发，本文提出了一种新颖的季节比例评分(SRS)方法。SRS包括三个关键算法步骤。首先，将每个输入分解为按类别的语义组件和余项。然后，利用这个分解使用深度生成模型来估计输入和余项的按类别的条件概率。从这些估计值计算季节比例评分。最后，从正常分布数据中识别出阈值区间来检测离群样本。实验

    Safe deployment of time-series classifiers for real-world applications relies on the ability to detect the data which is not generated from the same distribution as training data. This task is referred to as out-of-distribution (OOD) detection. We consider the novel problem of OOD detection for the time-series domain. We discuss the unique challenges posed by time-series data and explain why prior methods from the image domain will perform poorly. Motivated by these challenges, this paper proposes a novel {\em Seasonal Ratio Scoring (SRS)} approach. SRS consists of three key algorithmic steps. First, each input is decomposed into class-wise semantic component and remainder. Second, this decomposition is employed to estimate the class-wise conditional likelihoods of the input and remainder using deep generative models. The seasonal ratio score is computed from these estimates. Third, a threshold interval is identified from the in-distribution data to detect OOD examples. Experiments on 
    
[^226]: 关于混洗型梯度算法收敛到全局解的论述

    On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms. (arXiv:2206.05869v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05869](http://arxiv.org/abs/2206.05869)

    本文讨论了混洗型梯度算法在过参数化设置下对一类非凸函数的收敛性，证明了其能够收敛到全局解，并且在计算复杂度上与一般凸函数的情况相当。

    

    随机梯度下降（SGD）算法是许多机器学习任务中的选择方法，因其可扩展性和处理大规模问题的效率而受青睐。在本文中，我们专注于混洗版本的SGD，该版本与主流的实际启发式方法相匹配。我们展示了在过参数化设置下，对于一类非凸函数，混洗SGD收敛到全局解。我们的分析使用了比先前文献更宽松的非凸假设。尽管如此，我们仍然保持了混洗SGD在一般凸设置中所取得的期望计算复杂度。

    Stochastic gradient descent (SGD) algorithm is the method of choice in many machine learning tasks thanks to its scalability and efficiency in dealing with large-scale problems. In this paper, we focus on the shuffling version of SGD which matches the mainstream practical heuristics. We show the convergence to a global solution of shuffling SGD for a class of non-convex functions under over-parameterized settings. Our analysis employs more relaxed non-convex assumptions than previous literature. Nevertheless, we maintain the desired computational complexity as shuffling SGD has achieved in the general convex setting.
    
[^227]: SGD和权重衰减在神经网络中被证明会引入低秩偏差

    SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05794](http://arxiv.org/abs/2206.05794)

    使用SGD和权重衰减训练深度ReLU神经网络会导致对于权重矩阵的秩最小化的偏差，特别是在使用较小批量大小、更高学习率或增加权重衰减时更为显著。此外，在中间神经网络崩溃时，学习的权重特别低秩。这种偏差与泛化之间存在关系。

    

    我们研究了使用随机梯度下降（SGD）在训练深度ReLU神经网络时学习低秩权重矩阵的偏差。我们的结果表明，使用小批量SGD和权重衰减来训练神经网络会导致对于权重矩阵的秩最小化的偏差。具体而言，我们通过理论和实验证明，当使用较小的批量大小、更高的学习率或增加的权重衰减时，这种偏差更加显著。此外，我们预测并通过实验证明，权重衰减是实现这种偏差的必要条件。此外，我们还发现在中间神经网络崩溃的情况下，学习的权重特别低秩。与先前的文献不同，我们的分析不依赖于关于数据、收敛性或权重矩阵优化的假设。此外，它适用于任意宽度或深度的各种神经网络结构。最后，我们通过实验证明了这种偏差与泛化之间的关系。

    We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank weight matrices when training deep ReLU neural networks. Our results show that training neural networks with mini-batch SGD and weight decay causes a bias towards rank minimization over the weight matrices. Specifically, we show, both theoretically and empirically, that this bias is more pronounced when using smaller batch sizes, higher learning rates, or increased weight decay. Additionally, we predict and observe empirically that weight decay is necessary to achieve this bias. In addition, we show that in the presence of intermediate neural collapse, the learned weights are particularly low-rank. Unlike previous literature, our analysis does not rely on assumptions about the data, convergence, or optimality of the weight matrices. Furthermore, it applies to a wide range of neural network architectures of any width or depth. Finally, we empirically investigate the connection between this bias and generalization, 
    
[^228]: 具有一般成本函数的神经最优传输

    Neural Optimal Transport with General Cost Functionals. (arXiv:2205.15403v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15403](http://arxiv.org/abs/2205.15403)

    该论文介绍了一种新颖的神经网络算法，用于计算具有一般成本函数的最优传输方案。相比于常见的欧几里得成本，这种方法更灵活，并允许使用辅助信息构建传输映射。此外，该方法还解决了在高维空间下处理新数据点的挑战，并提供了理论误差分析。作为应用，该论文构造了一个能够在保留类别结构的同时映射数据分布的成本函数。

    

    我们引入了一种新颖的基于神经网络的算法，用于计算具有一般成本函数的最优传输方案。与常见的欧几里得成本（如$\ell^1$或$\ell^2$）不同，这种函数提供了更大的灵活性，并允许使用辅助信息（如类别标签）来构建所需的传输映射。现有的一般成本方法是离散的，并且在实践中存在限制，即它们不能提供样本外的估计。我们解决了针对一般成本设计连续的最优传输方法的挑战，该方法能够推广到高维空间（如图像）中的新数据点。此外，我们还对我们恢复的传输方案进行了理论误差分析。作为应用，我们构造了一个成本函数，用于在保留类别结构的同时映射数据分布。

    We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Existing methods for general costs are discrete and have limitations in practice, i.e. they do not provide an out-of-sample estimation. We address the challenge of designing a continuous OT approach for general costs that generalizes to new data points in high-dimensional spaces, such as images. Additionally, we provide the theoretical error analysis for our recovered transport plans. As an application, we construct a cost functional to map data distributions while preserving the class-wise structure.
    
[^229]: 通过无数据知识蒸馏对全局模型进行微调，用于非独立同分布联邦学习

    Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09249](http://arxiv.org/abs/2203.09249)

    本文针对联邦学习中的数据异构性问题，提出了一种无数据知识蒸馏方法，通过在服务器上对全局模型进行微调，可以有效地缓解模型聚合导致的性能下降，并提高联邦学习的效果。

    

    联邦学习是一种在隐私约束下的新兴分布式学习范 paradigm，数据异构性是联邦学习中的主要挑战之一，导致了收敛速度慢和性能下降。大部分现有方法只通过限制客户端的局部模型更新来解决异构性挑战，忽视了直接全局模型聚合所导致的性能下降问题。相反，我们提出了一种无数据知识蒸馏方法来对服务器上的全局模型进行微调(FedFTG)，从而缓解了直接模型聚合问题。具体来说，FedFTG通过生成器探索本地模型的输入空间，并使用它将本地模型的知识转移到全局模型上。此外，我们提出了一种硬样本挖掘方案，实现了在训练过程中的有效知识蒸馏。此外，我们开发了定制的标签采样和类级集成方法，以最大程度地利用知识，从而隐式地缓解了分布问题。

    Federated Learning (FL) is an emerging distributed learning paradigm under privacy constraint. Data heterogeneity is one of the main challenges in FL, which results in slow convergence and degraded performance. Most existing approaches only tackle the heterogeneity challenge by restricting the local model update in client, ignoring the performance drop caused by direct global model aggregation. Instead, we propose a data-free knowledge distillation method to fine-tune the global model in the server (FedFTG), which relieves the issue of direct model aggregation. Concretely, FedFTG explores the input space of local models through a generator, and uses it to transfer the knowledge from local models to the global model. Besides, we propose a hard sample mining scheme to achieve effective knowledge distillation throughout the training. In addition, we develop customized label sampling and class-level ensemble to derive maximum utilization of knowledge, which implicitly mitigates the distrib
    
[^230]: 关于数值特征在表格深度学习中的嵌入

    On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.05556](http://arxiv.org/abs/2203.05556)

    本文研究了表格深度学习中关于数值特征的嵌入方法，提出了两种不同的构建嵌入模块的方法，并通过实验证明，与传统模块相比，这些方法可以显著提升模型性能。这对于构建更强大的深度学习模型并在一些传统上适用于GBDT的基准上与之竞争具有重要的益处。

    

    最近，类似Transformer的深度架构在表格数据问题上展现出了强大的性能。与传统模型（如MLP）不同，这些架构将数值特征的标量值映射到高维嵌入中，然后在主干网络中将它们混合。本文认为，数值特征的嵌入在表格深度学习中是一个未充分探索的自由度，它允许构建更强大的深度学习模型，并与传统上适用于GBDT的基准进行竞争。我们首先描述了构建嵌入模块的两种概念上不同的方法：第一种基于标量值的分段线性编码，第二种利用周期性激活函数。然后，我们凭经验证明，与基于线性层和ReLU激活的传统模块相比，这两种方法可以显著提高模型性能。重要的是，我们还展示了嵌入数值特征的益处。

    Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial 
    
[^231]: 用于合作多智能体强化学习的局部优势网络

    Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.12458](http://arxiv.org/abs/2112.12458)

    这项研究提出了一种局部优势网络（LAN）算法，该算法通过对决架构和中心化评论家来学习合作多智能体的最佳响应策略，并在StarCraft II多智能体挑战基准测试上达到了最先进的性能。

    

    近期成功的离策略多智能体强化学习（MARL）算法，用于合作的部分可观测环境，主要关注于寻找分解的值函数，导致复杂的网络结构。我们的LAN算法建立在独立Q学习者的结构基础上，采用一种完全不同的方法，利用对决架构通过个体优势函数为每个智能体学习分散的最佳响应策略。通过一个中心化的评论家稳定学习，评论家的主要目标是减少个体优势的移动目标问题。评论家的网络大小与智能体数量无关，在学习后被丢弃。在StarCraft II多智能体挑战基准测试上的评估结果显示，LAN达到了最先进的性能，并且对于智能体数量具有高度可扩展性，为MARL研究开辟了一个有前景的替代方向。

    Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.
    
[^232]: 道路网络引导的城市细粒度交通流推断

    Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.14251](http://arxiv.org/abs/2109.14251)

    本文提出了一种基于道路网络的交通流量推断方法，利用道路网络的先验知识全面学习细粒度交通流的道路感知空间分布，普遍适用于城市交通流量监测和调控方案。

    

    精确推断细粒度交通流量是一个新兴但至关重要的问题，它可以帮助极大地减少所需交通监测传感器的数量以节省成本。本文发现交通流量与道路网络具有很高的相关性，但之前的研究中完全忽略了这一点，或者仅将其视为外部因素。为了解决这个问题，我们提出了一种新的路网感知交通流量放大器（RATFM），它明确利用道路网络的先验知识，全面学习细粒度交通流的道路感知空间分布。具体而言，我们首先引入了一个多方向1D卷积层来提取道路网络的语义特征。随后，我们将道路网络特征和粗粒度流量特征结合在一起，规范化道路相关交通流的短距离空间分布建模。此外，我们将道路网络特征作为查询来捕获长距离路段交通流量的关联。

    Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
    
[^233]: 具有全局非稳态的有限时间分析的多臂赌博机问题

    Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits. (arXiv:2107.11419v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.11419](http://arxiv.org/abs/2107.11419)

    本文研究了模型参数随时间变化的非稳态多臂赌博机问题，引入了自适应重置赌博机(ADR-bandit)算法，通过有限时间分析证明了ADR-bandit在全局变化的情况下具有几乎最优的性能，并且在稳定环境和非稳态环境中均具有最优的性能。

    

    我们考虑模型参数随时间变化的非稳态多臂赌博机问题。我们引入了自适应重置赌博机(ADR-bandit)，这是一个利用数据流文献中的自适应窗口技术的赌博机算法类。我们首先提供了关于自适应窗口技术产生的估计器质量的新保证，这对于独立的研究是有意义的。此外，我们在两种典型环境下对ADR-bandit进行了有限时间分析：一种是突变环境，其中变化是瞬时发生的；另一种是渐变环境，其中变化是逐渐发生的。我们证明了当突变或渐变的变化以我们称为全局变化的协同方式发生时，ADR-bandit具有几乎最优的性能。我们证明了在假设这种全局变化的情况下，强制探索是不必要的。与现有的非稳态赌博机算法不同，ADR-bandit在稳定环境和非稳态环境中均具有最优的性能。

    We consider nonstationary multi-armed bandit problems where the model parameters of the arms change over time. We introduce the adaptive resetting bandit (ADR-bandit), a bandit algorithm class that leverages adaptive windowing techniques from literature on data streams. We first provide new guarantees on the quality of estimators resulting from adaptive windowing techniques, which are of independent interest. Furthermore, we conduct a finite-time analysis of ADR-bandit in two typical environments: an abrupt environment where changes occur instantaneously and a gradual environment where changes occur progressively. We demonstrate that ADR-bandit has nearly optimal performance when abrupt or gradual changes occur in a coordinated manner that we call global changes. We demonstrate that forced exploration is unnecessary when we assume such global changes. Unlike the existing nonstationary bandit algorithms, ADR-bandit has optimal performance in stationary environments as well as nonstation
    
[^234]: 部分知识下的最优打分规则设计

    Optimal Scoring Rule Design under Partial Knowledge. (arXiv:2107.07420v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2107.07420](http://arxiv.org/abs/2107.07420)

    本文研究了在委托人对代理人的信号分布部分了解的情况下，最优打分规则的设计问题。作者提出了一个最大最小优化的框架，来最大化在代理人信号分布的集合中最坏情况下回报的增加。对于有限集合，提出了高效的算法；对于无限集合，提出了完全多项式时间逼近方案。

    

    本文研究了当委托人对代理人的信号分布部分了解时，最优适当打分规则的设计。最近的工作表明，在委托人完全了解代理人的信号分布的假设下，可以确定增加代理人回报的最大适当打分规则，当代理人选择访问昂贵信号以完善其先验预测的后验信念时。在我们的设置中，委托人只知道代理人的信号分布属于一组分布中的某个。我们将打分规则设计问题制定为最大最小优化问题，最大化分布集合中最坏情况下回报的增加。当分布集合有限时，我们提出了一种高效的算法来计算最优打分规则，并设计了一种完全多项式时间逼近方案，适用于各种无限集合的分布。我们进一步指出，广泛使用的打分规则，如二次方打分规则。

    This paper studies the design of optimal proper scoring rules when the principal has partial knowledge of an agent's signal distribution. Recent work characterizes the proper scoring rules that maximize the increase of an agent's payoff when the agent chooses to access a costly signal to refine a posterior belief from her prior prediction, under the assumption that the agent's signal distribution is fully known to the principal. In our setting, the principal only knows about a set of distributions where the agent's signal distribution belongs. We formulate the scoring rule design problem as a max-min optimization that maximizes the worst-case increase in payoff across the set of distributions.  We propose an efficient algorithm to compute an optimal scoring rule when the set of distributions is finite, and devise a fully polynomial-time approximation scheme that accommodates various infinite sets of distributions. We further remark that widely used scoring rules, such as the quadratic 
    
[^235]: 重新审视用于表格数据的深度学习模型

    Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11959](http://arxiv.org/abs/2106.11959)

    本论文重新审视了用于表格数据的深度学习模型，提出了两种简单且强大的深度学习架构作为性能基准，包括类似于ResNet的架构和适应于表格数据的Transformer架构。这些基准模型在不同问题上表现出有竞争力的性能。

    

    现有关于表格数据的深度学习的文献提出了各种新颖的架构，并在各种数据集上报告了有竞争力的结果。然而，这些提出的模型通常没有进行适当的比较，现有的研究常常使用不同的基准和实验方案。因此，对于研究人员和实践者而言，什么样的模型性能最好是不清楚的。另外，该领域仍然缺乏有效的基准模型，即在不同问题上提供有竞争力性能的易于使用的模型。在这项工作中，我们对表格数据的主要DL架构进行了概述，并通过确定两种简单而强大的深度架构，提高了表格DL的基准。第一种是类似于ResNet的架构，结果显示它是常见的先前工作中常缺失的强基准。第二个模型是我们针对表格数据的Transformer架构的简单适应，在性能上超过了其他解决方案。

    The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solution
    
[^236]: 通过多任务表示学习理论改进少样本学习

    Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2010.01992](http://arxiv.org/abs/2010.01992)

    本文通过多任务表示学习理论的最新进展，提出了一个新的基于谱的正则化项来改进少样本学习，并在实验证明了其有效性。

    

    本文考虑多任务表示（MTR）学习的框架，目标是利用源任务来学习一个表示，减少解决目标任务的样本复杂性。我们首先回顾了MTR理论的最新进展，并展示了在该框架内对流行的元学习算法进行分析可以提供新的见解。特别地，我们强调了梯度优化和度量优化算法在实践中的根本区别，并提出了一个理论分析来解释它。最后，我们利用得到的见解通过一种新的基于谱的正则化项来提高元学习方法的性能，并通过在少样本分类基准上的实验研究验证了其有效性。据我们所知，这是将MTR理论的最新学习界限应用于少样本分类任务的首次贡献。

    In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms in practice and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the performance of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on few-shot classification benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice for the task of few-shot classification.
    
[^237]: 在线估计和网络事件流的社区检测中的网络点过程

    Online Estimation and Community Detection of Network Point Processes for Event Streams. (arXiv:2009.01742v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2009.01742](http://arxiv.org/abs/2009.01742)

    本论文提出了一种在线变分推断算法，用于在网络事件流中估计潜在的社区结构。通过使用连续时间点过程潜在网络模型，可以捕捉动态事件到达的时间动态，并更新社区分配。

    

    网络建模的一个共同目标是揭示节点之间的潜在社区结构。对于许多现实世界中的网络，真实的连接都由作为流到达的事件组成，然后将其聚合形成边缘，忽略了动态的时间组件。考虑到这些交互的时间动态的一种自然方法是使用点过程作为网络模型中的基础进行社区检测。计算复杂性影响着这种方法在大型稀疏网络上的可扩展性。为了克服这一挑战，我们提出了一种快速在线变分推断算法，用于估计网络上基于动态事件到达的潜在结构，使用连续时间点过程潜在网络模型。我们描述了适用于捕捉社区结构的网络模型的此过程。当在网络上观察到新事件时，可以学习该结构，并更新推断的社区分配。我们研究了这种方法的理论性质。

    A common goal in network modeling is to uncover the latent community structure present among nodes. For many real-world networks, the true connections consist of events arriving as streams, which are then aggregated to form edges, ignoring the dynamic temporal component. A natural way to take account of these temporal dynamics of interactions is to use point processes as the foundation of network models for community detection. Computational complexity hampers the scalability of such approaches to large sparse networks. To circumvent this challenge, we propose a fast online variational inference algorithm for estimating the latent structure underlying dynamic event arrivals on a network, using continuous-time point process latent network models. We describe this procedure for networks models capturing community structure. This structure can be learned as new events are observed on the network, updating the inferred community assignments. We investigate the theoretical properties of suc
    

