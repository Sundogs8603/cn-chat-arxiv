# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Stochastic Two Points Method for Deep Model Zeroth-order Optimization](https://rss.arxiv.org/abs/2402.01621) | 本文介绍了一种针对大型深度模型的零阶优化方法——随机两点法，通过前向传递来更新模型。并且通过理论分析和实验证明了其在优化目标上的高效性并超越其他方法。 |
| [^2] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^3] | [Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL](https://arxiv.org/abs/2404.02113) | 提出了一种新方法来调整和评估终身强化学习代理，在此方法中，只有实验数据的一小部分可用于超参数调整，针对终身强化学习的研究进展可能被不当的经验方法所阻碍 |
| [^4] | [R2D2 image reconstruction with model uncertainty quantification in radio astronomy](https://arxiv.org/abs/2403.18052) | 本论文研究了射电天文学中的R2D2图像重建方法，通过研究学习模型系列的不确定性，提出了一种使用集成平均方法进行不确定性量化的方式。 |
| [^5] | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) | 逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。 |
| [^6] | [GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning](https://arxiv.org/abs/2403.17833) | GPFL提出了一个基于梯度投影的客户选择框架，通过比较本地和全局下降方向来衡量客户价值，并采用利用-探索机制来提高性能。在实验中表现出在非独立同分布场景中优于基线，在FEMINST测试准确性方面实现超过9%的改善。 |
| [^7] | [Do LLM Agents Have Regret? A Case Study in Online Learning and Games](https://arxiv.org/abs/2403.16843) | 通过研究在线学习和博弈论中的基准决策设置，评估LLM代理的交互行为和性能，以了解它们在多代理环境中的潜力和限制。 |
| [^8] | [Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning](https://arxiv.org/abs/2403.16049) | 提出了一种利用卡托图方法的深度学习框架，可预测未安装数据的站点需求并实现长期预测。 |
| [^9] | [Explore until Confident: Efficient Exploration for Embodied Question Answering](https://arxiv.org/abs/2403.15941) | 通过利用大型视觉-语言模型的语义推理能力，结合深度信息和视觉提示，提出了一种方法来解决具身问答中的有效探索和回答问题的挑战 |
| [^10] | [Simplified Diffusion Schr\"odinger Bridge](https://arxiv.org/abs/2403.14623) | 介绍了简化后的扩散薛定谔桥（DSB），通过与基于得分的生成模型（SGM）的统一解决了复杂数据生成中的限制，提高了性能并加快了收敛速度。 |
| [^11] | [Optimal Flow Matching: Learning Straight Trajectories in Just One Step](https://arxiv.org/abs/2403.13117) | 该论文提出了一种新颖的最优流匹配方法，能够在一步中学习实现二次成本下的直线 OT 位移。 |
| [^12] | [Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems](https://arxiv.org/abs/2403.08585) | 通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。 |
| [^13] | [Mean-Field Microcanonical Gradient Descent](https://arxiv.org/abs/2403.08362) | 提出了均场微正则梯度下降方法，通过同时采样多个弱耦合数据点，在控制熵损失的同时在似然拟合方面表现良好。 |
| [^14] | [Can Generative Models Improve Self-Supervised Representation Learning?](https://arxiv.org/abs/2403.05966) | 本文引入了一个新的框架，通过利用生成模型生成语义一致的图像增强，丰富了自监督学习的方法，实验结果表明提高了学到的视觉质量。 |
| [^15] | [Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model](https://arxiv.org/abs/2403.05571) | 本文提出了一种具有约束扩散模型的高效和保证安全的非凸轨迹优化框架，通过结合扩散模型和数值求解器，保证了计算效率和约束满足。 |
| [^16] | [UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining](https://arxiv.org/abs/2403.04822) | UniTable 提出了一个统一框架，通过自监督预训练实现表结构识别，将不同的任务和目标统一到语言建模中，在四个最大的TSR数据集上表现出最先进的性能。 |
| [^17] | [Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization](https://arxiv.org/abs/2403.04720) | 提出了一种新的基于编码器的表格数据集表示方法，与现有方法不同，能够自动提取重要的元特征，同时在两个常见的元任务上进行了评估 |
| [^18] | [A mechanism-informed reinforcement learning framework for shape optimization of airfoils](https://arxiv.org/abs/2403.04329) | 提出了一种基于机制的强化学习框架，用于处理流体动力学控制下的气动外型优化，并整合了多种策略以保证精确性和高效性。 |
| [^19] | [Mutual Information Estimation via Normalizing Flows](https://arxiv.org/abs/2403.02187) | 通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。 |
| [^20] | [Iterated $Q$-Network: Beyond the One-Step Bellman Operator](https://arxiv.org/abs/2403.02107) | 引入了迭代$Q$-网络（iQN）方法，通过一次考虑多次迭代的贝尔曼算子来改进值基强化学习方法，在理论上可行，并在实验中展示其在游戏和控制环境中的优势。 |
| [^21] | [Critical windows: non-asymptotic theory for feature emergence in diffusion models](https://arxiv.org/abs/2403.01633) | 我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。 |
| [^22] | [Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection](https://arxiv.org/abs/2403.01485) | 本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法 |
| [^23] | [Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation](https://arxiv.org/abs/2402.18839) | 本文基于Flow Matching发展了条件生成理论，通过使用广义连续性方程的数学框架而非流匹配中的连续性方程，实现了一种新颖的流基条件分布生成方法。 |
| [^24] | [Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion](https://arxiv.org/abs/2402.17886) | 本文提出了一种基于去噪扩散过程的零阶扩散蒙特卡洛算法，克服了非对数凹分布采样中的亚稳定性问题，并证明其采样精度具有倒多项式依赖。 |
| [^25] | [Intensive Care as One Big Sequence Modeling Problem](https://arxiv.org/abs/2402.17501) | 将医疗保健视为序列建模问题，通过将患者与医疗提供者之间的交互表示为事件流，实现对未来事件（如诊断和治疗选择）进行预测。 |
| [^26] | [Towards Generalizing Inferences from Trials to Target Populations](https://arxiv.org/abs/2402.17042) | 本研究在试图解决从试验结果推广到目标种群的外部有效性挑战方面取得了重要进展 |
| [^27] | [PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA](https://arxiv.org/abs/2402.16902) | PRoLoRA是一个新的部分旋转增强低秩适应方法，通过引入广播减少、旋转增强、部分共享细化和修正初始化策略等四个组件，实现了对LoRA的优势提升，避免了其他参数共享方法的缺点，具有更高的参数效率和可扩展性。 |
| [^28] | [Language-guided Skill Learning with Temporal Variational Inference](https://arxiv.org/abs/2402.16354) | 该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。 |
| [^29] | [Conformalized Selective Regression](https://arxiv.org/abs/2402.16300) | 通过利用一致性预测，提供基于模型特定偏差的置信度量，以解决选择性回归中不确定性测量的方法。 |
| [^30] | [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187) | 现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。 |
| [^31] | [Foundation Policies with Hilbert Representations](https://arxiv.org/abs/2402.15567) | 该研究提出了一个新颖的无监督框架，用于从未标记的离线数据中预训练通用政策，以捕获多样化、最优、长时域行为。 |
| [^32] | [Explorations of Self-Repair in Language Models](https://arxiv.org/abs/2402.15390) | 自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。 |
| [^33] | [Open Ad Hoc Teamwork with Cooperative Game Theory](https://arxiv.org/abs/2402.15259) | 提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路 |
| [^34] | [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992) | 本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。 |
| [^35] | [A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health](https://arxiv.org/abs/2402.14807) | 提出了一种决策语言模型DLM，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。 |
| [^36] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^37] | [Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions](https://arxiv.org/abs/2402.14045) | 本文系统综述了在医学成像中应用低秩矩阵逼近（LRMA）和其派生物局部LRMA（LLRMA）的作品，并指出自2015年以来医学成像领域开始偏向于使用LLRMA，显示其在捕获医学数据中复杂结构方面的潜力和有效性。 |
| [^38] | [Geometry-Informed Neural Networks](https://arxiv.org/abs/2402.14009) | GINNs提出了一种新颖的几何信息神经网络范式，可以在几何任务中生成多样的解决方案，无需训练数据，采用显式多样性损失以及可微损失来减轻模态坍缩，并在实验中展示了其在各种复杂性场景中的高效性。 |
| [^39] | [Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions](https://arxiv.org/abs/2402.13777) | 深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。 |
| [^40] | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | 本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能 |
| [^41] | [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847) | 通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。 |
| [^42] | [Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering](https://arxiv.org/abs/2402.12302) | 提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。 |
| [^43] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^44] | [LoRA Training in the NTK Regime has No Spurious Local Minima](https://arxiv.org/abs/2402.11867) | LoRA训练在NTK模式下消除了虚假局部最小值，有助于梯度下降找到低秩解并实现良好的泛化。 |
| [^45] | [Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective](https://arxiv.org/abs/2402.11463) | Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。 |
| [^46] | [AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods](https://arxiv.org/abs/2402.11215) | AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。 |
| [^47] | [Incremental Sequence Labeling: A Tale of Two Shifts](https://arxiv.org/abs/2402.10447) | 提出了一种名为IS3的框架，旨在解决增量序列标记任务中的E2O和O2E两种重要的语义转变，通过使用知识蒸馏来维持对旧实体的判别能力。 |
| [^48] | [Polyhedral Complex Derivation from Piecewise Trilinear Networks](https://arxiv.org/abs/2402.10403) | 本文以三线性插值方法作为位置编码，提出了理论见解和分析网格提取方法，将高维曲面转换为平面，并引入了一种近似交点的方法，拓展了更广泛的应用。 |
| [^49] | [Accelerating Parallel Sampling of Diffusion Models](https://arxiv.org/abs/2402.09970) | 本文提出了一种并行化自回归过程来加速扩散模型的采样的方法，并引入了ParaTAA，一种通用的并行采样算法，可以显著减少推理步骤。 |
| [^50] | [Why are Sensitive Functions Hard for Transformers?](https://arxiv.org/abs/2402.09963) | 本文证明了在Transformer架构下，损失函数的空间受到输入敏感性的限制，从而解释了Transformer对敏感函数的困难。这一理论统一了关于Transformer学习能力和偏见的广泛观察。 |
| [^51] | [Unsupervised Evaluation of Code LLMs with Round-Trip Correctness](https://arxiv.org/abs/2402.08699) | 无需人工策划，我们提出了往返正确性（RTC）作为评估代码大型语言模型（LLMs）的替代方法，RTC可以在更广泛的真实世界软件领域对代码进行评估，并且与现有基准具有强相关性。 |
| [^52] | [Avoiding Catastrophe in Continuous Spaces by Asking for Help](https://arxiv.org/abs/2402.08062) | 在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。 |
| [^53] | [G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering](https://arxiv.org/abs/2402.07630) | 本论文提出了G-Retriever模型，该模型用于文本图理解和问题解答。该模型能够将用户的问题转化为文本回复，并突出显示图形的相关部分。与现有的方法不同，该模型适用于真实世界的文本图形，并可应用于不同的任务，包括场景图理解、常识推理和知识图推理。 |
| [^54] | [On the Distance from Calibration in Sequential Prediction](https://arxiv.org/abs/2402.07458) | 本论文研究了顺序预测中的标定距离，证明了存在一种预测算法可以在敌人选择的二进制序列上实现$O(\sqrt{T})$的标定距离，通过较低的标定距离进行准确近似。 |
| [^55] | [HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs](https://arxiv.org/abs/2402.07309) | 本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。 |
| [^56] | [For Better or For Worse? Learning Minimum Variance Features With Label Augmentation](https://arxiv.org/abs/2402.06855) | 本研究分析了标签增强方法中标签增强的作用。研究证明，在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练可以学习到更高方差特征。此外，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。 |
| [^57] | [Retrieve, Merge, Predict: Augmenting Tables with Data Lakes](https://arxiv.org/abs/2402.06282) | 本文通过对数据湖中的数据发现进行深入分析，着重于表格增强，提出了准确检索连接候选人的重要性和简单合并方法的效率，以及现有解决方案的好处和局限性。 |
| [^58] | [Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design](https://arxiv.org/abs/2402.05982) | Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences. |
| [^59] | [Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark](https://arxiv.org/abs/2402.05961) | 本文提出了一种名为基因引导GFlowNet (Genetic GFN) 的新方法，通过集成迭代遗传搜索和训练策略，该方法在实际分子优化基准测试中取得了16.213的最新得分，明显优于现有最佳得分15.185，同时在14个任务中超越了所有对比方法。 |
| [^60] | [Limits of Transformer Language Models on Algorithmic Learning](https://arxiv.org/abs/2402.05785) | Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。 |
| [^61] | [Improving Token-Based World Models with Parallel Observation Prediction](https://arxiv.org/abs/2402.05643) | 该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。 |
| [^62] | [The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding](https://arxiv.org/abs/2402.05626) | 本文研究了使用ReLU-like激活函数以经验平方损失训练的单隐藏层神经网络的损失景观，提出了稳定点条件和逃逸神经元的定义，并将鞍点逃逸与逃逸神经元的参数变化联系起来。 |
| [^63] | [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2402.05445) | 本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。 |
| [^64] | [On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling](https://arxiv.org/abs/2402.05098) | 本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。 |
| [^65] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^66] | [SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning](https://arxiv.org/abs/2402.04114) | 本文量化了联邦线性随机逼近算法中异质性偏差的影响，并提出SCAFFLSA作为一种改进方法来消除此偏差。在联邦时间差异学习中，该方法能够显著提高算法的复杂性。 |
| [^67] | [Return-Aligned Decision Transformer](https://arxiv.org/abs/2402.03923) | 本研究提出了返回对齐的决策Transformer（RADT），通过分离回报与传统输入序列，实现有效地将实际回报与目标回报对齐。 |
| [^68] | [Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution](https://arxiv.org/abs/2402.03771) | 该论文提出了一种名为强化学习来自打包奖励的新问题，其中学习器只能获取序列的打包奖励，在这种情况下探索未知的即时奖励是一项困难的任务，并引入了基于Transformer的方法来解决这个问题。 |
| [^69] | [Breaking the Curse of Dimensionality with Distributed Neural Computation](https://arxiv.org/abs/2402.03460) | 通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。 |
| [^70] | [Test-Time Adaptation for Depth Completion](https://arxiv.org/abs/2402.03312) | 该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。 |
| [^71] | [A Framework for Partially Observed Reward-States in RLHF](https://arxiv.org/abs/2402.03282) | 这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。 |
| [^72] | [Guidance with Spherical Gaussian Constraint for Conditional Diffusion](https://arxiv.org/abs/2402.03201) | 本文提出了一种用球面高斯约束的扩散算法（DSG），解决了在条件生成任务中采样过程中的流形偏离问题。这种算法通过优化将步骤限制在中间数据流形内，并能够使用较大的引导步长。 |
| [^73] | [Isotropy, Clusters, and Classifiers](https://arxiv.org/abs/2402.03191) | 同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。 |
| [^74] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^75] | [Diffusive Gibbs Sampling](https://arxiv.org/abs/2402.03008) | 扩散吉布斯采样是一种创新的采样方法，通过集成扩散模型并应用吉布斯采样，有效地从具有远程和断开模态特征的分布中采样，表现出比其他方法更好的混合性能，并在多种任务中取得显著改进的结果。 |
| [^76] | [Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives](https://arxiv.org/abs/2402.02968) | 本调查对多模态多任务视觉理解基础模型在道路场景中的应用进行了系统分析，展示了其多模态和多任务学习能力，在处理各种驾驶相关任务方面具有强大的适应性，为实现对周围场景的更全面理解做出了重要贡献。 |
| [^77] | [Glocal Hypergradient Estimation with Koopman Operator](https://arxiv.org/abs/2402.02741) | 本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。 |
| [^78] | [Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach](https://arxiv.org/abs/2402.02672) | 本论文提出了一种数据协作双机器学习（DC-DML）方法，该方法可以在保护分布式数据隐私的情况下估计条件平均治疗效果（CATE）模型。通过数值实验验证了该方法的有效性。该方法的三个主要贡献是：实现了对分布式数据上的非迭代通信的半参数CATE模型的估计和测试，提高了模型的鲁棒性。 |
| [^79] | [Variational DAG Estimation via State Augmentation With Stochastic Permutations](https://arxiv.org/abs/2402.02644) | 使用状态扩展和随机排列进行变分DAG估计的方法可以超越竞争的贝叶斯和非贝叶斯基准方法，从而在估计贝叶斯网络结构方面取得更好的性能。 |
| [^80] | [PoCo: Policy Composition from and for Heterogeneous Robot Learning](https://arxiv.org/abs/2402.02511) | PoCo是一种策略组合方法，通过组合不同模态和领域的数据分布，实现了从异构数据中训练通用机器人策略的目标。该方法可以实现场景级和任务级的广义操作技能学习，并在推理时通过任务级组合和分析成本函数进行策略行为的自适应调整。 |
| [^81] | [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382) | 本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。 |
| [^82] | [Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization](https://arxiv.org/abs/2402.02325) | 这篇论文揭示了具有动量的随机梯度下降算法平滑了目标函数，影响程度由多个超参数决定，同时提供了对动量改善泛化能力的理论解释和新见解。 |
| [^83] | [INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer](https://arxiv.org/abs/2402.02317) | INViT是一种具有不变嵌套视图转换器的解决路由问题的方法，通过强制嵌套设计和不变的视图，在编码器内部提高学习求解器的泛化能力，从而实现了在具有不同分布和不同问题规模的TSP和CVRP问题上卓越的泛化性能。 |
| [^84] | [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314) | 该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。 |
| [^85] | [Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems](https://arxiv.org/abs/2402.02190) | 本研究提出了连续张量放松方法(CTRA)，用于在组合优化问题中寻找多样化的解决方案。CTRA通过对离散决策变量进行连续放松，解决了寻找多样化解决方案的挑战。 |
| [^86] | [SymbolicAI: A framework for logic-based approaches combining generative models and solvers](https://arxiv.org/abs/2402.00854) | SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。 |
| [^87] | [Data-Driven Discovery of PDEs via the Adjoint Method](https://arxiv.org/abs/2401.17177) | 本文提出了一种通过伴随方法从数据中发现潜在偏微分方程的方法，展示了在给定光滑数据集的情况下，该方法可以恢复真实的PDE，但在存在噪声的情况下，准确性与PDE-FIND方法相当。 |
| [^88] | [Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes](https://arxiv.org/abs/2312.06353) | 本论文提出了一种名为FedKSeed的方法，使用零阶优化和有限的随机种子集合，实现了通信成本较低的联邦全参数调整。该方法使得在终端设备上可以进行亿级语言模型的联邦全参数调整，具有较高的性能表现。 |
| [^89] | [SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation](https://arxiv.org/abs/2310.18376) | SQLformer是一个用于文本到SQL翻译的深度自回归查询图生成模型，采用了特定的Transformer架构，并通过结构归纳偏差解决领域泛化和自然语言与SQL查询对齐的难题。 |
| [^90] | [Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View](https://arxiv.org/abs/2310.02124) | 通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。 |
| [^91] | [Context-Former: Stitching via Latent Conditioned Sequence Modeling.](http://arxiv.org/abs/2401.16452) | Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。 |
| [^92] | [Generating Likely Counterfactuals Using Sum-Product Networks.](http://arxiv.org/abs/2401.14086) | 由于用户需求和最近的法规要求，需要对AI系统所做出的决策进行解释。本论文提出了一种使用Sum-Product Networks模拟寻找高可能性反事实推理的系统，该系统能够提供满足多个常见要求的最佳解释。 |
| [^93] | [Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?.](http://arxiv.org/abs/2401.13544) | 本文介绍了一种超越概念瓶颈模型的方法，可以使黑盒模型可干预。通过基于概念的干预来影响模型的输出，并利用这种方法对黑盒模型进行微调。实验证明，微调可以提高干预的效果，并产生更好校准的预测。 |
| [^94] | [Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents.](http://arxiv.org/abs/2401.05821) | SCoBots是一种可解释的概念瓶颈代理，能够透明化整个决策流程，帮助领域专家理解和规范强化学习代理的行为，从而可能实现更好的人类对齐强化学习。 |
| [^95] | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning.](http://arxiv.org/abs/2401.05268) | AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。 |
| [^96] | [The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers.](http://arxiv.org/abs/2401.01537) | 这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。 |
| [^97] | [Self-Infilling Code Generation.](http://arxiv.org/abs/2311.17972) | 本文介绍了自补代码生成的通用框架，利用自补机制实现了中断和循环机制，使传统解码进程变得非单调。利用中断机制可以推迟生成代码，增强对输出的控制；利用循环机制可以循环更新和同步生成的每个部分。 |
| [^98] | [Optimistic Multi-Agent Policy Gradient for Cooperative Tasks.](http://arxiv.org/abs/2311.01953) | 本论文提出了一个通用的、简单的框架，在多智能体策略梯度方法中引入乐观更新，以缓解合作任务中的相对过度概括问题。通过使用一个泄漏化线性整流函数来重塑优势，我们的方法能够保持对潜在由其他代理引起的低回报个别动作的乐观态度。 |
| [^99] | [Castor: Causal Temporal Regime Structure Learning.](http://arxiv.org/abs/2311.01412) | “Castor”是一个用于学习多元时间序列数据中因果关系的框架，能够综合学习各个区域的因果图。它通过最大化得分函数来推断区域的数量，并学习每个区域中的线性或非线性因果关系。 |
| [^100] | [Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective.](http://arxiv.org/abs/2311.00094) | 本文指出，在离线强化学习任务中，除了表达性强的序列模型，可处理性也起着重要的作用。由于离线数据收集策略和环境动态的随机性，需要精确且高效地回答各种概率查询，以找到有奖励的动作。基于此，本文提出了Trifle（离线强化学习的可处理推理）方法，利用现代可处理概率模型来解决这个问题。 |
| [^101] | [Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference.](http://arxiv.org/abs/2310.16705) | 本文将变分推断重新框架为在变分参数空间上的概率分布优化问题，提出了沃瑟斯坦梯度下降方法来解决优化问题，有效性经过实证实验证实。 |
| [^102] | [FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering.](http://arxiv.org/abs/2310.16152) | 本文提出了一种FLTrojan攻击方法，通过选择性权重篡改，从联邦语言模型中泄露隐私敏感用户数据。通过观察到FL中中间轮次的模型快照可以引起更大的隐私泄露，并发现隐私泄露可以通过篡改模型的选择性权重来加剧。 |
| [^103] | [An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning.](http://arxiv.org/abs/2310.12274) | 提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。 |
| [^104] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^105] | [Unseen Image Synthesis with Diffusion Models.](http://arxiv.org/abs/2310.09213) | 本论文提出使用预训练的扩散模型在未见过的领域合成图像的方法，并理论上和经验上证明了这种方法的有效性。 |
| [^106] | [Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining.](http://arxiv.org/abs/2310.08566) | 通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。 |
| [^107] | [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.](http://arxiv.org/abs/2310.06387) | 本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。 |
| [^108] | [Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications.](http://arxiv.org/abs/2310.05269) | 联邦学习是一种安全分布式机器学习方法，通过整合云基础设施和区块链技术，实现了隐私安全和经济有效的通信。它通过本地训练模型并将结果上传到云端进行整体模型聚合，避免了直接暴露原始数据，具有高效、可伸缩和保护隐私的优势。 |
| [^109] | [GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks.](http://arxiv.org/abs/2310.03399) | GRAPES是一种自适应图采样方法，通过学习识别在训练图神经网络分类器时具有影响力的节点集合，解决了可扩展图神经网络中的内存问题，并且在准确性和可扩展性方面表现出色。 |
| [^110] | [Energy-Guided Continuous Entropic Barycenter Estimation for General Costs.](http://arxiv.org/abs/2310.01105) | 本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。 |
| [^111] | [Pre-training with Synthetic Data Helps Offline Reinforcement Learning.](http://arxiv.org/abs/2310.00771) | 本文研究表明，在离线深度强化学习中，使用合成数据进行预训练可以提高性能，而不一定需要语言预训练。此外，使用一步马尔科夫链生成的数据进行预训练可进一步改善性能。在一个流行的离线DRL算法中，使用简单的预训练方案也能获得性能提升。 |
| [^112] | [Machine Learning Clifford invariants of ADE Coxeter elements.](http://arxiv.org/abs/2310.00041) | 本文研究了在线性变换中的Clifford几何不变量的机器学习方法，并实现了对ADE Coxeter元素的计算和数据挖掘。我们发现，不变量的输出完全取决于简单根的选择和对应反射的排列顺序，存在巨大的退化性。 |
| [^113] | [Controlling Continuous Relaxation for Combinatorial Optimization.](http://arxiv.org/abs/2309.16965) | 本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。 |
| [^114] | [Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach.](http://arxiv.org/abs/2309.16858) | 我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。 |
| [^115] | [Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness.](http://arxiv.org/abs/2309.16096) | 本研究论证了数据分布的集中程度对于决定鲁棒分类器的存在与否至关重要，并展示了在数据分布集中在低维线性子空间的并集时，利用数据结构可以获得具有良好鲁棒性保证的分类器的方法。 |
| [^116] | [Maximum Diffusion Reinforcement Learning.](http://arxiv.org/abs/2309.15293) | 最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。 |
| [^117] | [Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models.](http://arxiv.org/abs/2309.15278) | 本文研究了如何对先前观察到但当前被遮挡的对象进行推理和规划，提出了利用转换器关系动力学编码轨迹历史的方法，并在多个挑战性任务中表现出色。 |
| [^118] | [Evidential uncertainties on rich labels for active learning.](http://arxiv.org/abs/2309.12494) | 本文提出了两种策略来应对主动学习中的不确定性问题，即采用Klir不确定性采样和证据学派不确定性采样，在考虑到标签中已存在的不确定性的基础上，对模型的不确定性进行分解和处理。 |
| [^119] | [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A".](http://arxiv.org/abs/2309.12288) | LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。 |
| [^120] | [Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning.](http://arxiv.org/abs/2309.11489) | Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。 |
| [^121] | [A Geometric Framework for Neural Feature Learning.](http://arxiv.org/abs/2309.10140) | 本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。 |
| [^122] | [Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch.](http://arxiv.org/abs/2309.07909) | 本文介绍了一种新颖且高效的基于扩散的数据增强技术DiffAug，通过扩散步骤实现增强数据和原始数据共享平滑的潜空间，旨在提高无监督对比学习的效果和效率。 |
| [^123] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^124] | [Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making.](http://arxiv.org/abs/2309.03426) | 这篇论文介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。 |
| [^125] | [Calibrated Explanations for Regression.](http://arxiv.org/abs/2308.16245) | 本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。 |
| [^126] | [IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency.](http://arxiv.org/abs/2308.12871) | 提出了一种名为IPA的在线深度学习推理管道自适应系统，通过动态配置批处理大小、复制和模型变体，以优化准确性、最小化成本并满足用户定义的延迟要求。 |
| [^127] | [${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.11842) | 本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。 |
| [^128] | [Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances.](http://arxiv.org/abs/2308.11129) | 本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。 |
| [^129] | [From Sparse to Soft Mixtures of Experts.](http://arxiv.org/abs/2308.00951) | 本文提出了一种Soft MoE模型，它是一种稀疏的、完全可微分的Transformer，通过隐式的软分配和只处理部分标记的方式解决了稀疏模型的训练不稳定性和推理成本高的问题，并在视觉识别任务中取得了比标准Transformer和其他MoE变体更好的性能。 |
| [^130] | [Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War.](http://arxiv.org/abs/2307.08840) | 本研究提出了使用基于贝叶斯安全策略学习的机会约束优化方法，在越南战争期间的军事安全评估中进行实证研究。研究结果对于解决高风险算法决策中的挑战具有重要意义。 |
| [^131] | [STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map.](http://arxiv.org/abs/2307.03374) | 本文提出了一种数据驱动方法，使用数据映射来解决多任务学习中的任务分组问题。这种方法具有可伸缩性和模块化，并且在大量任务下仍然有效。 |
| [^132] | [Interpolating Item and User Fairness in Recommendation Systems.](http://arxiv.org/abs/2306.10050) | 本文研究在推荐系统中平衡项目和用户公平性的框架，并通过低后悔的在线优化算法实现了维持收益同时实现公平推荐的目标。 |
| [^133] | [Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning.](http://arxiv.org/abs/2306.01896) | 本文提出了一种基于李雅普诺夫思想的奖励塑形方法，用于在持续任务的强化学习中应对无界状态空间，旨在鼓励代理器学习稳定性和最优策略。 |
| [^134] | [Smooth Monotonic Networks.](http://arxiv.org/abs/2306.01147) | 本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。 |
| [^135] | [Learning to solve Bayesian inverse problems: An amortized variational inference approach.](http://arxiv.org/abs/2305.20004) | 本文提出了一种基于深度神经网络的参数化表示后验分布的摊销变分推理方法，以实现实时推理的目的，可应用于流体力学中的参数估计和流场重构等领域。 |
| [^136] | [MADiff: Offline Multi-agent Learning with Diffusion Models.](http://arxiv.org/abs/2305.17330) | 本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。 |
| [^137] | [AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth.](http://arxiv.org/abs/2305.17193) | 利用弱监督学习范例对超分辨率显微镜进行分析具有发现新生物学的巨大潜力，并且可以加速探索亚细胞大分子和细胞器分子结构。 |
| [^138] | [Think Before You Act: Decision Transformers with Internal Working Memory.](http://arxiv.org/abs/2305.16338) | 该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。 |
| [^139] | [How to escape sharp minima.](http://arxiv.org/abs/2305.15659) | 本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。 |
| [^140] | [Multi-State RNA Design with Geometric Multi-Graph Neural Networks.](http://arxiv.org/abs/2305.14749) | 本论文提出了一种基于几何多图神经网络的多状态RNA设计方法，可以明确考虑和反映RNA构象多样性在其设计中。其能够显著提高原生序列的恢复，尤其适用于多状态和结构多样化的RNA。 |
| [^141] | [Constant Memory Attentive Neural Processes.](http://arxiv.org/abs/2305.14567) | 提出了一种不变存储的注意力神经过程 (CMANPs) 及其注意力块 CMAB，能在常数内存下进行条件、查询和更新操作，并在元回归和少样本回归任务上获得最先进的表现。 |
| [^142] | [Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates.](http://arxiv.org/abs/2305.13082) | 本文提出了一种新的基于草图和投影的Newton方法，具有快速的全局收敛率，适用于自共轭函数，具有草图和投影方法的低迭代成本，全秩Newton类方法的最先进全局收敛率以及阻尼Newton方法的算法简单性。 |
| [^143] | [Continual Multimodal Knowledge Graph Construction.](http://arxiv.org/abs/2305.08698) | 连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。 |
| [^144] | [Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline.](http://arxiv.org/abs/2304.06793) | Speck是一款智能视觉传感器SoC，该芯片集成了基于事件的相机和低功耗的sCNN计算架构，可显著降低单元生产成本。通过优化高度稀疏的计算和最小化延迟，Speck可以处理高速的稀疏数据流。 |
| [^145] | [Learning Personalized Decision Support Policies.](http://arxiv.org/abs/2304.06701) | 本文提出了一种学习个性化决策支持策略的算法 $\texttt{THREAD}$，可以为决策者提供不同形式的支持。同时，引入了 $\texttt{Modiste}$ 工具来提供个性化的医学诊断决策支持，使用 $\texttt{THREAD}$ 学习个性化决策支持策略，有效提高了预期的诊断正确性，并减少了严重并发症的风险，同时推荐了更少和更便宜的研究。 |
| [^146] | [Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation.](http://arxiv.org/abs/2304.05749) | 本研究提出了一种插入式模块——不确定性掩蔽混合（UmmU），它能够在中间层嵌入中进行不确定性估计，进而增强嵌入的不确定性，使其具有更好的泛化性能，从而显著提高了连续时间动态图网络的长期预测性能。 |
| [^147] | [A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates.](http://arxiv.org/abs/2303.16668) | 本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。 |
| [^148] | [Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection.](http://arxiv.org/abs/2302.12012) | 本研究对使用不同降维和分类技术进行癫痫发作检测进行了实证分析，通过离散小波变换和机器学习分类器，结合主成分分析、独立成分分析和线性判别分析等降维算法，选择特征来提高检测准确性。 |
| [^149] | [Discriminative Entropy Clustering and its Relation to K-means and SVM.](http://arxiv.org/abs/2301.11405) | 该论文介绍了判别熵聚类的相关理论及其与K-means和SVM的区别和相似之处。同时提出了一种新的损失函数，用于改进深度聚类的性能。 |
| [^150] | [ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference.](http://arxiv.org/abs/2211.11435) | 本研究提出了一种通用的无采样不确定性估计方法，通过训练网络在有和没有额外信息的情况下产生相同的输出，实现了与最先进方法相媲美的可靠性估计，同时显著降低了计算成本。 |
| [^151] | [Local Model Reconstruction Attacks in Federated Learning and their Uses.](http://arxiv.org/abs/2210.16205) | 本文研究了联邦学习中的本地模型重构攻击，攻击者可以通过重构客户端的模型更有效地触发其他攻击，并提出了一种新颖的基于模型的属性推导攻击，实证结果表明攻击有效，可应用于回归和分类任务。 |
| [^152] | [Self-Training: A Survey.](http://arxiv.org/abs/2202.12040) | 自主训练方法是一种半监督算法，无需额外假设数据分布，在低密度区域找到决策边界，并使用学习分类器的输出分数作为置信度，通过为无标签样本分配伪标签，迭代地学习分类器，从而丰富有标签训练数据。 |
| [^153] | [Long Story Short: Omitted Variable Bias in Causal Machine Learning.](http://arxiv.org/abs/2112.13398) | 在因果机器学习中，我们通过推导出遗漏变量偏差的尖锐上界，为广泛的线性泛函因果参数提供了一种简单而通用的方法。这种方法可以应用于许多传统的因果推断研究目标，并且仅取决于潜变量在结果和参数的Riesz表示器中所导致的额外变异。 |

# 详细

[^1]: 针对深度模型零阶优化的随机两点法

    Stochastic Two Points Method for Deep Model Zeroth-order Optimization

    [https://rss.arxiv.org/abs/2402.01621](https://rss.arxiv.org/abs/2402.01621)

    本文介绍了一种针对大型深度模型的零阶优化方法——随机两点法，通过前向传递来更新模型。并且通过理论分析和实验证明了其在优化目标上的高效性并超越其他方法。

    

    大型基础模型，例如大型语言模型，在各种应用场景中表现出色。由于硬件预算或缺乏反向传播的访问权限，构建或完全微调这样的大模型通常是不可行的。零阶方法为解决这一挑战提供了一种有希望的方向，它只需要前向传递来更新模型。本文在无梯度情形下引入了一种高效的随机两点（S2P）方法。我们在一般和放松的平滑性假设下提出了S2P的理论收敛性质。理论性质还揭示了更快、更稳定的S2P变体——加速S2P（AS2P），通过利用我们的新收敛性质，更好地表示了深度模型在训练中的动力学。我们全面的实证结果表明，AS2P在优化大型深度模型（包括语言模型）的目标上非常有效，并且优于其他方法。

    Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms
    
[^2]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^3]: 针对未知进行调整：重新审视终身强化学习的评估策略

    Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL

    [https://arxiv.org/abs/2404.02113](https://arxiv.org/abs/2404.02113)

    提出了一种新方法来调整和评估终身强化学习代理，在此方法中，只有实验数据的一小部分可用于超参数调整，针对终身强化学习的研究进展可能被不当的经验方法所阻碍

    

    在继续或终身强化学习中，对环境的访问应该是有限的。如果我们希望设计的算法能够长时间运行，并不断适应新的、意想不到的情况，那么我们必须愿意在整个代理的整个生命周期内部署我们的代理而不调整它们的超参数。本文探讨了深度强化学习中 -- 甚至继续强化学习中 -- 具备对代理的部署环境具有无限制访问权的标准做法可能已经阻碍了对终身强化学习研究的进展。在本文中，我们提出了一种新的方法，用于调整和评估终身强化学习代理，其中只有实验数据的百分之一可以用于超参数调整。然后，我们对DQN和Soft Actor Critic在各种持续和非稳定领域进行了实证研究。我们发现这两种方法通常表现较好。

    arXiv:2404.02113v1 Announce Type: new  Abstract: In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform po
    
[^4]: 在射电天文学中量化模型不确定性的R2D2图像重建

    R2D2 image reconstruction with model uncertainty quantification in radio astronomy

    [https://arxiv.org/abs/2403.18052](https://arxiv.org/abs/2403.18052)

    本论文研究了射电天文学中的R2D2图像重建方法，通过研究学习模型系列的不确定性，提出了一种使用集成平均方法进行不确定性量化的方式。

    

    最近提出了用于天文学中射电干涉(RI)成像的“高动态范围成像的残差到残差DNN系列”(R2D2)方法。R2D2的重建形成为一系列残差图像，迭代地估计为以前迭代的图像估计和相关数据残差为输入的深度神经网络(DNN)的输出。在这项工作中，我们通过研究其学习模型系列的不确定性来研究R2D2图像估计过程的稳健性。采用集成平均方法，可以训练多个系列，来自在每次迭代过程中对训练过程进行不同随机DNN初始化。由此产生的多个R2D2实例也可用于产生“R2D2样本”，从中经验均值和标准差赋予算法联合估计和不确定性量化功能。重点放在RI imag

    arXiv:2403.18052v1 Announce Type: cross  Abstract: The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imag
    
[^5]: LISA：用于高效内存大型语言模型微调的逐层重要性采样

    LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning

    [https://arxiv.org/abs/2403.17919](https://arxiv.org/abs/2403.17919)

    逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。

    

    机器学习领域自大型语言模型（LLMs）首次出现以来取得了令人瞩目的进展，然而它们巨大的内存消耗已成为大规模训练的主要障碍。虽然已经提出了诸如低秩调整（LoRA）之类的参数高效微调技术来缓解这一问题，但在大多数大规模微调设置中，它们的性能仍无法与完整参数训练相匹配。为弥补这一不足，我们研究了LoRA在微调任务中的逐层特性，并观察到不同层之间权重范数的异常偏斜。利用这一关键观察，我们发现了一个令人惊讶简单的训练策略，在记忆成本低于LoRA的情况下，在广泛的设置中优于LoRA和完整参数训练。我们将其命名为Layerwise Importance Sampled AdamW（LISA），这是LoRA的一个有希望的替代方案，应用了

    arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
    
[^6]: 基于梯度投影的客户选择框架用于高效的联邦学习

    GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning

    [https://arxiv.org/abs/2403.17833](https://arxiv.org/abs/2403.17833)

    GPFL提出了一个基于梯度投影的客户选择框架，通过比较本地和全局下降方向来衡量客户价值，并采用利用-探索机制来提高性能。在实验中表现出在非独立同分布场景中优于基线，在FEMINST测试准确性方面实现超过9%的改善。

    

    联邦学习客户选择对确定参与者客户以平衡模型准确性和通信效率至关重要。现有方法在处理数据异构性、计算负担和独立客户处理方面存在局限性。为解决这些挑战，我们提出了GPFL，通过比较本地和全局下降方向来衡量客户价值。我们还采用了一种利用-探索机制来增强性能。对FEMINST和CIFAR-10数据集的实验结果表明，GPFL在非独立同分布场景中优于基线，在FEMINST测试准确性方面实现超过9%的改善。此外，GPFL通过在联邦学习中的预选和参数重用展示出更短的计算时间。

    arXiv:2403.17833v1 Announce Type: new  Abstract: Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.
    
[^7]: LLM代理是否会感到后悔？在线学习和游戏案例研究

    Do LLM Agents Have Regret? A Case Study in Online Learning and Games

    [https://arxiv.org/abs/2403.16843](https://arxiv.org/abs/2403.16843)

    通过研究在线学习和博弈论中的基准决策设置，评估LLM代理的交互行为和性能，以了解它们在多代理环境中的潜力和限制。

    

    大型语言模型(LLMs)越来越多地被用于(交互式)决策制定，通过开发基于LLM的自主代理。尽管它们取得了不断的成功，但LLM代理在决策制定中的表现尚未通过定量指标进行充分调查，特别是在它们相互作用时的多代理设置中，这是实际应用中的典型场景。为了更好地理解LLM代理在这些交互环境中的限制，我们建议研究它们在在线学习和博弈论的基准决策设置中的相互作用，并通过\emph{后悔}性能指标进行评估。我们首先在经典(非平稳)在线学习问题中经验性地研究LLMs的无后悔行为，以及当LLM代理通过进行重复游戏进行交互时均衡的出现。然后我们对无后悔行为提供一些理论洞见。

    arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
    
[^8]: 通过基于卡托图辅助的深度学习增强开放系统的需求预测

    Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning

    [https://arxiv.org/abs/2403.16049](https://arxiv.org/abs/2403.16049)

    提出了一种利用卡托图方法的深度学习框架，可预测未安装数据的站点需求并实现长期预测。

    

    在各个领域预测时间模式面临着重大挑战，因为它们的轨迹细微且经常是非线性的。为了解决这一挑战，预测框架不断被完善，采用数据驱动的统计方法、数学模型和机器学习方法。最近，作为一个具有挑战性的系统之一，共享交通系统如共享单车由于城市约束和环境问题而备受关注。由于系统的开放性和各站点之间的使用模式不平衡，预测单车站点的租借和归还模式仍然是一项艰巨的任务。在本研究中，我们提出了一个深度学习框架，通过利用卡托图方法来预测租借和归还模式。卡托图方法有助于对新安装站点的需求进行预测，这些新站点没有训练数据，同时实现了长期预测，这是以前尚未实现的。

    arXiv:2403.16049v1 Announce Type: new  Abstract: Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply t
    
[^9]: 探索直到自信: 面向具身问答的高效探索

    Explore until Confident: Efficient Exploration for Embodied Question Answering

    [https://arxiv.org/abs/2403.15941](https://arxiv.org/abs/2403.15941)

    通过利用大型视觉-语言模型的语义推理能力，结合深度信息和视觉提示，提出了一种方法来解决具身问答中的有效探索和回答问题的挑战

    

    我们考虑了具身问答（EQA）的问题，这指的是在需要主动探索环境以收集信息直到对问题的答案有自信的具身代理，例如机器人。在这项工作中，我们利用大规模视觉-语言模型（VLMs）的强大语义推理能力来高效探索和回答这些问题。然而，在EQA中使用VLMs时存在两个主要挑战：它们没有内部记忆将场景映射以便规划如何随时间探索，并且它们的置信度可能被错误校准并可能导致机器人过早停止探索或过度探索。我们提出了一种方法，首先基于深度信息和通过视觉提示VLM来构建场景的语义地图-利用其对场景相关区域的广泛知识来进行探索。接下来，我们使用符合预测来校准VLM的置信度。

    arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
    
[^10]: 简化扩散薛定谔桥

    Simplified Diffusion Schr\"odinger Bridge

    [https://arxiv.org/abs/2403.14623](https://arxiv.org/abs/2403.14623)

    介绍了简化后的扩散薛定谔桥（DSB），通过与基于得分的生成模型（SGM）的统一解决了复杂数据生成中的限制，提高了性能并加快了收敛速度。

    

    这篇论文介绍了一种新的理论简化扩散薛定谔桥（DSB），便于将其与基于得分的生成模型（SGM）统一起来，解决了DSB在复杂数据生成方面的局限性，实现更快的收敛速度和增强的性能。通过将SGM作为DSB的初始解决方案，我们的方法利用了这两个框架的优势，确保了更高效的训练过程，并改进了SGM的性能。我们还提出了一种重新参数化技术，尽管存在理论近似，但实际上提高了网络的拟合能力。我们进行了大量的实验证实，证实了简化的DSB的有效性，展示了其显著的改进。我们相信这项工作的贡献为先进的生成建模铺平了道路。

    arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
    
[^11]: 最优流匹配：在一步中学习直线轨迹

    Optimal Flow Matching: Learning Straight Trajectories in Just One Step

    [https://arxiv.org/abs/2403.13117](https://arxiv.org/abs/2403.13117)

    该论文提出了一种新颖的最优流匹配方法，能够在一步中学习实现二次成本下的直线 OT 位移。

    

    在过去几年中，流匹配方法在生成建模中得到了蓬勃发展。社区追求的一个引人注目的属性是能够学习具有直线轨迹的流，这些轨迹实现了最优输运（OT）置换。直线性对于快速集成学习流的路径至关重要。不幸的是，大多数现有的流直线化方法都基于非平凡的迭代过程，在训练过程中积累误差或利用启发式小批量OT近似。为解决这一问题，我们开发了一种新颖的最优流匹配方法，仅通过一次流匹配步骤即可为二次成本恢复直线OT置换。

    arXiv:2403.13117v1 Announce Type: cross  Abstract: Over the several recent years, there has been a boom in development of flow matching methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the optimal transport (OT) displacements. Straightness is crucial for fast integration of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative procedures which accumulate the error during training or exploit heuristic minibatch OT approximations. To address this issue, we develop a novel optimal flow matching approach which recovers the straight OT displacement for the quadratic cost in just one flow matching step.
    
[^12]: 通过预条件化改善最小二乘问题随机梯度下降的隐式正则化

    Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems

    [https://arxiv.org/abs/2403.08585](https://arxiv.org/abs/2403.08585)

    通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。

    

    随机梯度下降（SGD）在实践中表现出强大的算法正则化效果，在现代机器学习的泛化中起着重要作用。然而，先前的研究发现，SGD的泛化性能有时会比岭回归差，这是由于沿不同维度的优化不均匀造成的。预条件化通过重新平衡沿不同方向的优化来提供解决这一问题的自然方法。然而，预条件化能够提升SGD的泛化性能的程度以及它是否能够填补现有与岭回归之间的差距仍不确定。本文研究了预条件化对最小二乘问题中SGD的泛化性能的影响。我们全面比较了预条件化SGD与（标准和预条件化）岭回归。我们的研究对了解和改善SGD做出了几项关键贡献。

    arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit
    
[^13]: 均场微正则梯度下降

    Mean-Field Microcanonical Gradient Descent

    [https://arxiv.org/abs/2403.08362](https://arxiv.org/abs/2403.08362)

    提出了均场微正则梯度下降方法，通过同时采样多个弱耦合数据点，在控制熵损失的同时在似然拟合方面表现良好。

    

    微正则梯度下降是一种能量模型的采样过程，可实现高维分布的高效采样，通过梯度下降将样本从高熵分布（如高斯白噪声）转运至低能区域。我们将这一模型置于正则化流的框架中，显示它通常会由于在下降过程中失去不必要的熵而过度拟合。为解决这一问题，我们提出了均场微正则梯度下降，同时采样多个弱耦合数据点，允许更好地控制熵损失，同时在似然拟合方面付出较小代价。我们将这些模型应用于金融时间序列的背景中，展示了在合成和真实数据上的改进。

    arXiv:2403.08362v1 Announce Type: cross  Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.
    
[^14]: 能生成模型改进自监督表示学习吗？

    Can Generative Models Improve Self-Supervised Representation Learning?

    [https://arxiv.org/abs/2403.05966](https://arxiv.org/abs/2403.05966)

    本文引入了一个新的框架，通过利用生成模型生成语义一致的图像增强，丰富了自监督学习的方法，实验结果表明提高了学到的视觉质量。

    

    自监督学习的快速发展突显了其利用无标签数据学习强大视觉表示的潜力。然而，现有的自监督学习方法，特别是那些利用同一图像的不同视图的方法，通常依赖于一组预定义的数据增强，这限制了变换的多样性和质量，导致表示不够优化。本文引入了一个新颖的框架，通过利用生成模型产生语义一致的图像增强，丰富了自监督学习范式。通过直接在源图像表示上进行条件生成模型，我们的方法能够生成多样的增强，同时保持源图像的语义，为自监督学习提供更丰富的数据集。我们的实验结果表明，我们的框架显著提高了学到的视觉的质量。

    arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu
    
[^15]: 具有约束扩散模型的高效和保证安全的非凸轨迹优化

    Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model

    [https://arxiv.org/abs/2403.05571](https://arxiv.org/abs/2403.05571)

    本文提出了一种具有约束扩散模型的高效和保证安全的非凸轨迹优化框架，通过结合扩散模型和数值求解器，保证了计算效率和约束满足。

    

    机器人轨迹优化面临一个具有挑战性的非凸问题，这是由于复杂的动力学和环境设置造成的。本文引入了一个通用且完全可并行化的框架，将扩散模型和数值求解器结合起来，用于非凸轨迹优化，确保计算效率和约束满足。提出了一种新颖的带有额外约束违反损失的约束扩散模型进行训练。它旨在在采样过程中近似局部最优解的分布，同时最小化约束违反。然后用样本作为数值求解器的初始猜测，来优化并得出最终解，并验证可行性和最优性。

    arXiv:2403.05571v1 Announce Type: cross  Abstract: Trajectory optimization in robotics poses a challenging non-convex problem due to complex dynamics and environmental settings. Traditional numerical optimization methods are time-consuming in finding feasible solutions, whereas data-driven approaches lack safety guarantees for the output trajectories. In this paper, we introduce a general and fully parallelizable framework that combines diffusion models and numerical solvers for non-convex trajectory optimization, ensuring both computational efficiency and constraint satisfaction. A novel constrained diffusion model is proposed with an additional constraint violation loss for training. It aims to approximate the distribution of locally optimal solutions while minimizing constraint violations during sampling. The samples are then used as initial guesses for a numerical solver to refine and derive final solutions with formal verification of feasibility and optimality. Experimental evalua
    
[^16]: UniTable: 朝向通过自监督预训练实现表结构识别的统一框架

    UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining

    [https://arxiv.org/abs/2403.04822](https://arxiv.org/abs/2403.04822)

    UniTable 提出了一个统一框架，通过自监督预训练实现表结构识别，将不同的任务和目标统一到语言建模中，在四个最大的TSR数据集上表现出最先进的性能。

    

    表格传达由人类创建的隐式约定的事实和数量数据，这往往是机器难以解析的。以往关于表结构识别（TSR）的工作主要集中在利用可用输入和工具的复杂特定任务组合上。我们提出了UniTable，这是一个统一TSR的训练框架，其训练范式结合了纯粹像素级输入的简单性以及来自各种未注释表格图像的自监督预训练（SSP）赋予的有效性和可伸缩性。我们的框架将所有三个TSR任务的训练目标 - 提取表结构，单元格内容和单元格边界框（bbox） - 统一为一个统一的与任务无关的训练目标：语言建模。广泛的定量和定性分析突显了UniTable在四个最大的TSR数据集上的最先进性能。

    arXiv:2403.04822v1 Announce Type: cross  Abstract: Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable's state-of-the-art (SOTA) performance on four of the largest TSR datasets. T
    
[^17]: 在超参数优化中重新思考基于编码器的热启动方法

    Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization

    [https://arxiv.org/abs/2403.04720](https://arxiv.org/abs/2403.04720)

    提出了一种新的基于编码器的表格数据集表示方法，与现有方法不同，能够自动提取重要的元特征，同时在两个常见的元任务上进行了评估

    

    有效地表示异质性表格数据集以用于元学习仍然是一个未解决的问题。以往的方法依赖于预定义的元特征，例如，统计量或标志点。基于编码器的模型，如Dataset2Vec，使我们能够在无人干预的情况下自动提取重要的元特征。本研究介绍了一个新颖的基于编码器的表格数据集表示，实现在liltab包中，该包可在GitHub上找到https://github.com/azoz01/liltab。我们的包基于[Iwata and Kumagai, 2020]提出的一个用于异质表格数据的已建立模型。所提出的方法采用一种不同于现有方法如Dataset2Vec 的编码特征关系的模型，生成与现有方法不同的替代表示。它们都利用了数据集相似性学习的基本假设。在这项工作中，我们在两个常见的元任务上评价了Dataset2Vec和liltab

    arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
    
[^18]: 一种基于机制的强化学习框架用于机翼气动外型优化

    A mechanism-informed reinforcement learning framework for shape optimization of airfoils

    [https://arxiv.org/abs/2403.04329](https://arxiv.org/abs/2403.04329)

    提出了一种基于机制的强化学习框架，用于处理流体动力学控制下的气动外型优化，并整合了多种策略以保证精确性和高效性。

    

    在本研究中，我们提出了一种基于机制的强化学习框架，用于机翼气动外型优化。通过利用双延迟深度确定性策略梯度算法的显著稳定性，我们的方法解决了受流体动力学控制的外型优化的复杂性。采用基于PDEs的求解器以保证准确性，即使在探索过程中配置和几何形状发生了极大变化。我们采用双加权残差网格细化策略，以确保目标函数的精确计算。为了简化迭代优化过程并处理几何变形，我们的方法集成了拉普拉斯平滑、自适应细化和B\'ezier拟合策略。这种组合不仅消除了网格纠缠，还保证了对机翼几何的精确操作。我们的神经网络架构利用B\'ezier曲线实现了高效的表示。

    arXiv:2403.04329v1 Announce Type: cross  Abstract: In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B\'ezier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry. Our neural network architecture leverages B\'ezier curves for efficient dime
    
[^19]: 通过正则化流进行互信息估计

    Mutual Information Estimation via Normalizing Flows

    [https://arxiv.org/abs/2403.02187](https://arxiv.org/abs/2403.02187)

    通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。

    

    我们提出了一种新颖的方法来解决互信息（MI）估计问题，即引入基于正则化流的估计器。该估计器将原始数据映射到具有已知互信息闭合形式表达式的目标分布。我们证明了我们的方法产生了原始数据的互信息估计。通过高维数据的实验结果展示了所提出估计器的优势。

    arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
    
[^20]: 迭代$Q$-网络：超越单步贝尔曼算子

    Iterated $Q$-Network: Beyond the One-Step Bellman Operator

    [https://arxiv.org/abs/2403.02107](https://arxiv.org/abs/2403.02107)

    引入了迭代$Q$-网络（iQN）方法，通过一次考虑多次迭代的贝尔曼算子来改进值基强化学习方法，在理论上可行，并在实验中展示其在游戏和控制环境中的优势。

    

    值基强化学习（RL）方法依赖于贝尔曼算子的应用，该算子需要从样本中进行近似。大多数方法包括交替应用贝尔曼算子和随后投影步骤到考虑的函数空间的迭代方案。然而，我们观察到这些算法可以通过一次考虑多次迭代的贝尔曼算子来改进。因此，我们引入了迭代$Q$-网络（iQN），这是一种新颖的方法，它学习一系列$Q$函数逼近，其中每个$Q$函数都作为下一个函数链中的目标。我们证明了iQN在理论上是可行的，并展示了它如何可以无缝地用于值基和演员-评论方法。我们在Atari$2600$游戏和连续控制MuJoCo环境中在实验上展示了它的优势。

    arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
    
[^21]: 批判性窗口：扩散模型中特征出现的非渐进理论

    Critical windows: non-asymptotic theory for feature emergence in diffusion models

    [https://arxiv.org/abs/2403.01633](https://arxiv.org/abs/2403.01633)

    我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。

    

    我们发展理论来理解图像生成扩散模型中一个有趣的属性，我们称之为批判性窗口。实证上观察到在采样过程中存在狭窄的时间间隔，在此期间会出现最终图像的特定特征，例如图像类别或背景颜色。而这种特性对于解释性是有利的，因为意味着可以将生成的特性定位到轨迹的一个小片段，但这似乎与扩散的连续性质相矛盾。我们提出了一个形式化框架来研究这些窗口，并表明对于来自混合强对数凹密度分布的数据，这些窗口可以用一定的跨组和组内分离度量来显式地约束。我们还为诸如良条件G的具体示例实例化了这些界限。

    arXiv:2403.01633v1 Announce Type: new  Abstract: We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned G
    
[^22]: 对深度生成模型的费舍尔信息度量进行近似用于检测离群分布

    Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection

    [https://arxiv.org/abs/2403.01485](https://arxiv.org/abs/2403.01485)

    本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法

    

    基于概率似然的深度生成模型，如基于评分的扩散模型和变分自动编码器，是近年来用于拟合高维数据分布（如图像、文本或音频）的先进机器学习模型之一。它们可以自然地应用于许多下游任务之一，即离群分布（OOD）检测。然而，Nalisnick等人的开创性工作表明，深度生成模型始终为OOD数据推断出比它们训练过的数据更高的对数似然，标志着一个悬而未决的问题。在这项工作中，我们分析了使用数据点对深度生成模型的参数梯度进行OOD检测，基于这样的简单直觉，即OOD数据的梯度范数应该大于训练数据。我们形式化地将梯度大小的度量量化为近似费舍尔信息度量。我们展示了费舍尔信息矩阵（FIM）具有较大的绝对值

    arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di
    
[^23]: 扩展流匹配：具有广义连续性方程的条件生成方法

    Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation

    [https://arxiv.org/abs/2402.18839](https://arxiv.org/abs/2402.18839)

    本文基于Flow Matching发展了条件生成理论，通过使用广义连续性方程的数学框架而非流匹配中的连续性方程，实现了一种新颖的流基条件分布生成方法。

    

    条件生成任务是生成模型中最重要的应用之一，迄今为止已经开发了许多基于著名扩散模型的方法，其中以基于引导的无分类器方法为首。然而，基于引导的方法的理论不仅要求用户微调“引导强度”，而且其目标向量场不一定对应于训练中使用的条件分布。本文基于流匹配发展了条件生成理论，流匹配是扩散方法的当前强大竞争者之一。受将概率路径解释为路径空间上的分布的启发，我们建立了一个新颖的流基条件分布生成理论，通过使用广义连续性方程的数学框架而不是流匹配中的连续性方程。这一理论自然地推导出一种方法

    arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
    
[^24]: 用于非对数凹分布的零阶采样方法：通过去噪扩散缓解亚稳定性

    Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion

    [https://arxiv.org/abs/2402.17886](https://arxiv.org/abs/2402.17886)

    本文提出了一种基于去噪扩散过程的零阶扩散蒙特卡洛算法，克服了非对数凹分布采样中的亚稳定性问题，并证明其采样精度具有倒多项式依赖。

    

    这篇论文考虑了基于其非对数凹分布未归一化密度查询的采样问题。首先描述了一个基于模拟去噪扩散过程的框架，即扩散蒙特卡洛（DMC），其得分函数通过通用蒙特卡洛估计器逼近。DMC是一个基于神谕的元算法，其中神谕是假设可以访问生成蒙特卡洛分数估计器的样本的访问。然后，我们提供了一个基于拒绝采样的这个神谕的实现，这将DMC转化为一个真正的算法，称为零阶扩散蒙特卡洛（ZOD-MC）。我们通过首先构建一个通用框架，即DMC的性能保证，而不假设目标分布为对数凹或满足任何等周不等式，提供了收敛分析。然后我们证明ZOD-MC对所需采样精度具有倒多项式依赖，尽管仍然受到...

    arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
    
[^25]: 重症监护作为一个大型序列建模问题

    Intensive Care as One Big Sequence Modeling Problem

    [https://arxiv.org/abs/2402.17501](https://arxiv.org/abs/2402.17501)

    将医疗保健视为序列建模问题，通过将患者与医疗提供者之间的交互表示为事件流，实现对未来事件（如诊断和治疗选择）进行预测。

    

    在医疗保健中，强化学习通常涉及狭窄的自包含任务，如脓毒症预测或麻醉控制。然而，先前的研究表明，通用模型（主要示例为大型语言模型）具有超越特定任务方法的潜力，因为它们具有隐式迁移学习的能力。为了实现保健基础模型的训练以及利用最先进的Transformer架构的能力，我们提出了保健作为序列建模的范式，其中患者和医疗提供者之间的交互被表示为事件流，诊断和治疗选择等任务被建模为对流中未来事件的预测。为了在实验中探索这一范式，我们开发了MIMIC-SEQ，这是一个序列建模基准，通过将来自MIMIC-IV数据集的异构临床记录转换为一种统一...

    arXiv:2402.17501v1 Announce Type: cross  Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a un
    
[^26]: 通向从试验推广推理到目标种群的泛化

    Towards Generalizing Inferences from Trials to Target Populations

    [https://arxiv.org/abs/2402.17042](https://arxiv.org/abs/2402.17042)

    本研究在试图解决从试验结果推广到目标种群的外部有效性挑战方面取得了重要进展

    

    随机对照试验（RCTs）在产生内部有效估计方面起着至关重要的作用，而对扩展这些发现以获得外部有效估计至关重要，以促进更广泛的科学探究。本文探讨了应对这些外部有效性挑战的前沿，概括了2023年秋季在布朗大学计算与实验数学研究所（ICERM）举行的一次跨学科研讨会的精华。该研讨会汇集了来自社会科学、医学、公共卫生、统计学、计算机科学和教育等各个领域的专家，以解决每个学科在推断实验结果方面面临的独特障碍。我们的研究提出了三个关键贡献：我们整合正在进行的努力，突出了

    arXiv:2402.17042v1 Announce Type: cross  Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting me
    
[^27]: PRoLoRA: 部分旋转增强更高效LoRA

    PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA

    [https://arxiv.org/abs/2402.16902](https://arxiv.org/abs/2402.16902)

    PRoLoRA是一个新的部分旋转增强低秩适应方法，通过引入广播减少、旋转增强、部分共享细化和修正初始化策略等四个组件，实现了对LoRA的优势提升，避免了其他参数共享方法的缺点，具有更高的参数效率和可扩展性。

    

    随着大型语言模型（LLMs）的快速扩展，同时服务多个LoRAs变得日益不切实际，导致成本不可承受，需要更具参数效率的微调方法。在这项工作中，我们引入了部分旋转增强低秩适应（PRoLoRA），这是一种由四个关键组件组成的层内共享机制：广播减少、旋转增强、部分共享细化和修正的初始化策略。作为LoRA的超集，PRoLoRA保留了其优势，并通过具有卓越的模型容量、实用可行性和广泛适用性，有效地避开了其他参数共享方法的缺点。实证实验表明，PRoLoRA在特定参数预算和性能目标情景下，具有显着更高的参数效率，并且可以扩展到更大的LLMs。值得注意的是，PRoLoRA在可训练参数减少一倍的情况下，

    arXiv:2402.16902v1 Announce Type: new  Abstract: With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA s
    
[^28]: 通过时间变分推断进行语言引导的技能学习

    Language-guided Skill Learning with Temporal Variational Inference

    [https://arxiv.org/abs/2402.16354](https://arxiv.org/abs/2402.16354)

    该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。

    

    我们提出了一种从专家演示中发现技能的算法。该算法首先利用大型语言模型（LLMs）来提出轨迹的初始分割。随后，一个分层变分推断框架将LLM生成的分割信息纳入其中，通过合并轨迹段来发现可重用的技能。为了进一步控制压缩和可重用性之间的权衡，我们引入了一个基于最小描述长度原则的新辅助目标，帮助引导这种技能发现过程。我们的结果表明，使用我们方法的Agent能够发现有助于加速学习的技能，在BabyAI（一个网格世界导航环境）以及ALFRED（一个家庭模拟环境）的新长期任务中胜过基线技能学习方法。

    arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
    
[^29]: Conformalized Selective Regression

    Conformalized Selective Regression

    [https://arxiv.org/abs/2402.16300](https://arxiv.org/abs/2402.16300)

    通过利用一致性预测，提供基于模型特定偏差的置信度量，以解决选择性回归中不确定性测量的方法。

    

    预测模型是否总是要提供预测？在追求最大预测性能的过程中，可靠性和公平性往往被忽视，尤其是关于不确定性的作用。选择性回归，也称为“拒绝选项”，允许模型在存在相当大的不确定性情况下放弃预测。尽管7十年前就最初提出了选择性回归的方法，但大多数方法主要集中在用于测量不确定性的基于分布的代理，尤其是条件方差。但这种关注忽视了模型特定偏差对模型性能的显著影响。本文提出了一种新的选择性回归方法，通过利用一致性预测，为基于模型特定偏差的个别预测提供有根据的置信度度量。此外，我们提出了一个标准化的评估框架，以便进行恰当的比较。

    arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
    
[^30]: 利用其优势攻击LLM水印

    Attacking LLM Watermarks by Exploiting Their Strengths

    [https://arxiv.org/abs/2402.16187](https://arxiv.org/abs/2402.16187)

    现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。

    

    生成模型的进展使得人工智能生成的文本、代码和图片能够在许多应用中模仿人类生成的内容。水印技术旨在将信息嵌入模型的输出中以验证其来源，对于减少对这些人工智能生成内容的滥用非常有用。然而，现有的水印方案仍然令人意外地容易受到攻击。具体而言，我们展示了现有的LLM水印系统共享的可取特性，例如质量保留、鲁棒性和公开检测API，反过来却使这些系统容易遭受各种攻击。我们在常见水印设计选择方面严格研究潜在攻击，并提出了缓解攻击的最佳实践和防御措施——建立了一套嵌入和检测LLM水印的实用指南。

    arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
    
[^31]: 具有希尔伯特表示的基础政策

    Foundation Policies with Hilbert Representations

    [https://arxiv.org/abs/2402.15567](https://arxiv.org/abs/2402.15567)

    该研究提出了一个新颖的无监督框架，用于从未标记的离线数据中预训练通用政策，以捕获多样化、最优、长时域行为。

    

    arXiv:2402.15567v1 进行类型：交叉 摘要：无监督和自监督目标，例如下一个令牌预测，已经使得可以从大量未标记的数据中预训练通用模型。然而，在强化学习（RL）中，从离线数据中找到一个真正通用且可扩展的无监督预训练目标以获取通用政策仍然是一个主要的开放问题。尽管已经提出了许多方法来实现通用的自监督RL，基于诸如基于目标的RL、行为克隆和无监督技能学习等原则，但这些方法在发现的行为多样性、需要高质量演示数据或缺乏明确的提示或适应机制以用于下游任务方面仍然存在局限性。在这项工作中，我们提出了一个新颖的无监督框架，用于预训练能够捕捉多样化、最优、长时域行为的通用政策，从未标记的离线数据中获取这些行为，以便它们可以快速适应

    arXiv:2402.15567v1 Announce Type: cross  Abstract: Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear prompting or adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly ada
    
[^32]: 在语言模型中自修复的探索

    Explorations of Self-Repair in Language Models

    [https://arxiv.org/abs/2402.15390](https://arxiv.org/abs/2402.15390)

    自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。

    

    先前研究狭窄分布的可解释性发现了自修复现象，即如果剥离大型语言模型中的组件，后续组件会改变其行为以进行补偿。我们的工作基于这些过去的文献，展示了当在完整的训练分布上剥离单个注意力头时，自修复存在于各种模型家族和尺寸上。我们进一步表明，在完整的训练分布上，自修复是不完美的，因为头部的原始直接效果并未完全恢复，并且是嘈杂的，因为自修复程度在不同提示之间显著变化（有时超过原始效果）。我们强调了促成自修复的两种不同机制，包括最终LayerNorm缩放因子的变化（可修复直接效果的30%）以及实现反擦除的稀疏神经元集。

    arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
    
[^33]: 采用合作博弈论的开放式即兴团队合作

    Open Ad Hoc Teamwork with Cooperative Game Theory

    [https://arxiv.org/abs/2402.15259](https://arxiv.org/abs/2402.15259)

    提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路

    

    即兴团队合作面临着一个具有挑战性的问题，需要设计一个能够与队友协作但没有先前协调或联合训练的智能体。开放式即兴团队合作进一步复杂化了这一挑战，考虑了具有不断变化的队友数量的环境，即开放式团队。现有解决这一问题的最先进方法是基于图神经网络的策略学习（GPL），利用了图神经网络的泛化能力来处理无限数量的智能体，有效应对开放式团队。GPL的性能优于其他方法，但其联合Q值表示对解释造成了挑战，阻碍了进一步发展这一研究方向和应用。本文建立了一种新的理论，从合作博弈论的角度为GPL中采用的联合Q值表示提供了一种解释。基于我们的理论，我们提出了一种基于

    arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
    
[^34]: 小型基准测试：用更少的示例评估LLM

    tinyBenchmarks: evaluating LLMs with fewer examples

    [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)

    本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。

    

    大型语言模型（LLMs）的多功能性导致创建了多种基准测试，彻底测试各种语言模型的能力。这些基准测试包含成千上万个示例，使得评估LLMs非常昂贵。本文研究了减少评估LLMs性能所需的评估次数的策略。例如，我们展示了要准确估计LLMs在MMLU上的性能（一个包含14K个示例的流行多选问答基准测试），只需要在100个精心挑选的示例上评估这个LLMs。我们发布了评估工具和流行基准测试的微型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和微型基准测试足以可靠且高效地重现原始评估结果。

    arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
    
[^35]: 用于公共卫生中动态不安静多臂老虎机任务的决策语言模型（DLM）

    A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health

    [https://arxiv.org/abs/2402.14807](https://arxiv.org/abs/2402.14807)

    提出了一种决策语言模型DLM，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。

    

    旨在降低孕产妇死亡率的努力在很大程度上依赖于预防保健计划，向高风险人群传播重要的健康信息。本文提出了DLM：一种用于RMAB的决策语言模型，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。

    arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
    
[^36]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^37]: 在医学成像中推进低秩和局部低秩矩阵逼近：系统文献综述与未来方向

    Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions

    [https://arxiv.org/abs/2402.14045](https://arxiv.org/abs/2402.14045)

    本文系统综述了在医学成像中应用低秩矩阵逼近（LRMA）和其派生物局部LRMA（LLRMA）的作品，并指出自2015年以来医学成像领域开始偏向于使用LLRMA，显示其在捕获医学数据中复杂结构方面的潜力和有效性。

    

    医学成像数据集的大容量和复杂性是存储、传输和处理的瓶颈。为解决这些挑战，低秩矩阵逼近（LRMA）及其派生物局部LRMA（LLRMA）的应用已显示出潜力。本文进行了系统文献综述，展示了在医学成像中应用LRMA和LLRMA的作品。文献的详细分析确认了应用于各种成像模态的LRMA和LLRMA方法。本文解决了现有LRMA和LLRMA方法所面临的挑战和限制。我们注意到，自2015年以来，医学成像领域明显偏向于LLRMA，显示了相对于LRMA在捕获医学数据中复杂结构方面的潜力和有效性。鉴于LLRMA所使用的浅层相似性方法的限制，我们建议使用先进语义图像分割来处理相似性。

    arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
    
[^38]: 几何信息神经网络

    Geometry-Informed Neural Networks

    [https://arxiv.org/abs/2402.14009](https://arxiv.org/abs/2402.14009)

    GINNs提出了一种新颖的几何信息神经网络范式，可以在几何任务中生成多样的解决方案，无需训练数据，采用显式多样性损失以及可微损失来减轻模态坍缩，并在实验中展示了其在各种复杂性场景中的高效性。

    

    我们引入了几何信息神经网络（GINNs）的概念，涵盖了（i）在几何约束下学习，（ii）神经场作为合适的表示，（iii）生成在几何任务中经常遇到的欠定系统的多样解决方案。值得注意的是，GINN的构建不需要训练数据，因此可以被纯约束驱动地视为生成建模。我们增加了显式的多样性损失来减轻模态坍缩。我们考虑了几种约束，特别是组件的连通性，我们通过莫尔斯理论将其转化为可微损失。在实验中，我们展示了在不断增加复杂性的二维和三维场景中，GINN学习范式的高效性。

    arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
    
[^39]: 离线策略学习的深度生成模型：教程、调查和未来方向展望

    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions

    [https://arxiv.org/abs/2402.13777](https://arxiv.org/abs/2402.13777)

    深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。

    

    深度生成模型(DGMs)在各个领域展示了巨大成功，特别是在使用从离线数据训练的模型生成文本、图像和视频方面。类似地，基于数据驱动的决策和机器人控制也需要从离线数据中学习一个生成函数作为策略或政策。在这种情况下，将深度生成模型应用于离线策略学习展现出巨大潜力，许多研究在这个方向上进行了探索。然而，这一领域仍然缺乏全面的评估，因此不同分支的发展相对独立。因此，我们提供了深度生成模型在离线策略学习应用方面的第一次系统性综述。具体而言，我们涵盖了五种主流深度生成模型，包括变分自动编码器、生成对抗网络、归一化流、变压器和扩散模型，以及它们的应用。

    arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
    
[^40]: ProSparse: 引入和增强大型语言模型内部激活稀疏性

    ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

    [https://arxiv.org/abs/2402.13516](https://arxiv.org/abs/2402.13516)

    本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能

    

    Activation sparsity指的是激活输出中存在许多弱贡献元素。作为使用ReLU激活函数的模型的普遍属性，已被证明是提高模型推理效率的一种有前途的范例。然而，大多数大型语言模型（LLMs）采用了没有内在激活稀疏性的激活函数（例如GELU和Swish）。一些最近的努力尝试引入ReLU或其变体作为替代激活函数，以帮助LLMs实现激活稀疏性和推理加速，但很少能同时获得高稀疏度和可比较的模型性能。本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动LLMs实现更高的激活稀疏性而不降低模型性能。具体来说，将LLMs的激活函数替换为ReLU后，ProSparse采用渐进稀疏正则化

    arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
    
[^41]: 调整过的语言模型更好的知识学习者

    Instruction-tuned Language Models are Better Knowledge Learners

    [https://arxiv.org/abs/2402.12847](https://arxiv.org/abs/2402.12847)

    通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。

    

    为了使基于大语言模型（LLM）的助手能够有效地适应不断发展的信息需求，必须能够通过持续在新数据上训练来更新它们的事实知识。传统做法涉及在新文档上持续预培训，然后根据问题-答案（QA）对进行指导调整。

    arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
    
[^42]: 谱聚类中特征向量的渐近高斯波动

    Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering

    [https://arxiv.org/abs/2402.12302](https://arxiv.org/abs/2402.12302)

    提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。

    

    谱聚类的性能依赖于相似矩阵的特征向量的条目波动，该波动直到现在仍未得到描述。本文表明，一般尖峰随机矩阵模型的信号+噪声结构被转移到相应的格拉姆核矩阵的特征向量上，并且它们的条目波动在大维度区域呈高斯分布。这种类似于中心极限定理的结果是准确预测谱聚类的分类性能的最后一块缺失的拼图。提出的证明非常通用，仅依赖于噪声的旋转不变性。对合成和真实数据的数值实验表明了这个现象的普适性。

    arXiv:2402.12302v1 Announce Type: cross  Abstract: The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.
    
[^43]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^44]: LoRA训练在NTK模式下没有虚假局部最小值

    LoRA Training in the NTK Regime has No Spurious Local Minima

    [https://arxiv.org/abs/2402.11867](https://arxiv.org/abs/2402.11867)

    LoRA训练在NTK模式下消除了虚假局部最小值，有助于梯度下降找到低秩解并实现良好的泛化。

    

    低秩适应（LoRA）已成为参数高效微调大型语言模型（LLM）的标准方法，但我们对LoRA的理论理解有限。在这项工作中，我们从理论上分析了在神经切向核（NTK）模式下使用LoRA微调，其中包含$N$个数据点，结果显示：(i) 全面微调（不使用LoRA）允许秩为$r\lesssim \sqrt{N}$的低秩解; (ii) 使用秩为$r\gtrsim \sqrt{N}$的LoRA消除了虚假的局部最小值，使梯度下降可以找到低秩解; (iii) 使用LoRA找到的低秩解具有良好的泛化性能。

    arXiv:2402.11867v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.
    
[^45]: 长期时间序列预测中的吸引子记忆：混沌视角

    Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective

    [https://arxiv.org/abs/2402.11463](https://arxiv.org/abs/2402.11463)

    Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。

    

    在长期时间序列预测（LTSF）任务中，现有的深度学习模型忽视了离散时间序列源自潜在连续动态系统的关键特征，导致缺乏外推和演化能力。 鉴别真实世界数据的混沌性质，我们的模型\textbf{\textit{Attraos}}将混沌理论融入到LTSF中，将实际时间序列视为未知高维混沌动态系统的观测。 在吸引子不变性的概念下，Attraos利用提出的多尺度动态记忆单元来记忆历史动态结构，并通过频率增强的局部演化策略进行预测。 详细的理论分析和丰富的经验证据一致表明，Attraos在主流LTSF数据集和混沌数据集上的表现优于各种LTSF方法。

    arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
    
[^46]: AdAdaGrad：自适应梯度方法的自适应批大小方案

    AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods

    [https://arxiv.org/abs/2402.11215](https://arxiv.org/abs/2402.11215)

    AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。

    

    随机梯度优化器中批量大小的选择对模型训练至关重要。然而，在训练过程中变化批大小的实践相对其他超参数较少探讨。我们研究了从自适应采样方法中导出的自适应批大小策略，传统上仅应用于随机梯度下降。考虑到学习速率和批大小之间的显著相互作用，以及自适应梯度方法在深度学习中的普及，我们强调在这些情境中需要自适应批大小策略。我们介绍了AdAdaGrad及其标量变体AdAdaGradNorm，它们在训练过程中逐渐增加批大小，同时使用AdaGrad和AdaGradNorm进行模型更新。我们证明了AdaGradNorm以高概率以$O(1/K)$的速度收敛，用于找到光滑非凸函数的一阶稳定点在$K$次迭代内。

    arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
    
[^47]: 增量序列标记：两种转变的故事

    Incremental Sequence Labeling: A Tale of Two Shifts

    [https://arxiv.org/abs/2402.10447](https://arxiv.org/abs/2402.10447)

    提出了一种名为IS3的框架，旨在解决增量序列标记任务中的E2O和O2E两种重要的语义转变，通过使用知识蒸馏来维持对旧实体的判别能力。

    

    增量序列标记任务涉及在保留对先前类别知识的同时，随时间不断学习新类别。我们的研究确定了两种重要的语义转变：E2O（模型将旧实体错误标记为非实体）和O2E（模型将非实体或旧实体标记为新实体）。先前的研究主要集中在解决E2O问题上，忽视了O2E问题。这种忽略导致模型在学习过程中对新数据样本进行分类时存在偏见，认为它们属于新类别。为了解决这些挑战，我们提出了一种新颖的框架，即无语义转变的增量顺序标记（IS3）。受到已确定的语义转变（E2O和O2E）的启发，IS3旨在缓解模型中的灾难性遗忘。至于E2O问题，我们使用知识蒸馏来维持模型对旧实体的判别能力。

    arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
    
[^48]: 从分段三线性网络中导出多面体复合体

    Polyhedral Complex Derivation from Piecewise Trilinear Networks

    [https://arxiv.org/abs/2402.10403](https://arxiv.org/abs/2402.10403)

    本文以三线性插值方法作为位置编码，提出了理论见解和分析网格提取方法，将高维曲面转换为平面，并引入了一种近似交点的方法，拓展了更广泛的应用。

    

    最近关于深度神经网络可视化的进展揭示了它们结构的见解，并且可以从连续分段仿射（CPWA）函数中提取网格。与此同时，神经表面表示学习的发展包括非线性位置编码，解决了诸如谱偏差之类的问题；然而，这在应用基于CPWA函数的网格提取技术方面带来了挑战。我们聚焦于三线性插值方法作为位置编码，提供了理论见解和分析的网格提取，展示了在奇拿尔约束下将高维曲面转换为三线性区域内的平面的过程。此外，我们引入了一种方法来近似三个高维曲面之间的交点，从而扩展了更广泛的应用。通过汉明距离和效率以及角距离来经验性地验证正确性和简洁性，同时检查了t之间的相关性

    arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
    
[^49]: 加速并行采样扩散模型

    Accelerating Parallel Sampling of Diffusion Models

    [https://arxiv.org/abs/2402.09970](https://arxiv.org/abs/2402.09970)

    本文提出了一种并行化自回归过程来加速扩散模型的采样的方法，并引入了ParaTAA，一种通用的并行采样算法，可以显著减少推理步骤。

    

    扩散模型已经成为图像生成的最先进生成模型。然而，由于其采样过程中固有的自回归性质，从扩散模型中进行采样通常耗时。在本文中，我们提出了一种新的方法，通过并行化自回归过程来加速扩散模型的采样。具体而言，我们将采样过程重新构建为通过固定点迭代解决三角非线性方程组的过程。通过这种创新的公式，我们探索了一些系统化的技术，进一步减少了求解过程所需的迭代步骤。应用这些技术，我们引入了ParaTAA，一种通用的、无需训练的并行采样算法，可以利用额外的计算和内存资源来增加采样速度。我们的实验表明，ParaTAA可以减少常见的顺序采样所需的推理步骤。

    arXiv:2402.09970v1 Announce Type: new  Abstract: Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampli
    
[^50]: 为什么Transformer对敏感函数困难?

    Why are Sensitive Functions Hard for Transformers?

    [https://arxiv.org/abs/2402.09963](https://arxiv.org/abs/2402.09963)

    本文证明了在Transformer架构下，损失函数的空间受到输入敏感性的限制，从而解释了Transformer对敏感函数的困难。这一理论统一了关于Transformer学习能力和偏见的广泛观察。

    

    经验研究发现，Transformer存在一系列的学习偏见和限制，如在学习计算简单形式语言（如PARITY）时的持久困难，以及对低阶函数的偏好。然而，现有的表达能力理论要么过度预测，要么低估了实际的学习能力。我们证明，在Transformer架构下，损失函数的空间受到输入敏感性的限制：输出对输入字符串的多个部分敏感的Transformer存在于参数空间中的孤立点，导致泛化中的低敏感性偏差。我们理论上和实证上证明了该理论统一了关于Transformer学习能力和偏见的广泛观察，如它们对低敏感性和低阶的泛化偏差，以及在长度泛化上的困难。

    arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
    
[^51]: 无监督评估具有往返正确性的代码LLMs

    Unsupervised Evaluation of Code LLMs with Round-Trip Correctness

    [https://arxiv.org/abs/2402.08699](https://arxiv.org/abs/2402.08699)

    无需人工策划，我们提出了往返正确性（RTC）作为评估代码大型语言模型（LLMs）的替代方法，RTC可以在更广泛的真实世界软件领域对代码进行评估，并且与现有基准具有强相关性。

    

    为了评估代码大型语言模型（LLMs），研究一直依赖于一些小的手动策划的基准，如HumanEval和MBPP，这些基准只代表了真实世界软件领域的一个狭窄部分。在这项工作中，我们引入了往返正确性（RTC）作为一种替代评估方法。RTC允许在更广泛的真实世界软件领域对代码LLM进行评估，而无需昂贵的人工策划。RTC的基本思想是我们可以要求模型做出预测（例如用自然语言描述一些代码），将该预测返回（例如从预测的描述中合成代码），并检查这个往返过程是否导致与原始输入语义等效的代码。我们展示了如何利用RTC来评估代码合成和编辑。我们发现RTC与现有狭窄领域代码合成基准上的模型性能强相关，同时也允许我们扩展到更广阔的领域。

    arXiv:2402.08699v1 Announce Type: cross Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader se
    
[^52]: 避免连续空间中的灾难：通过寻求帮助

    Avoiding Catastrophe in Continuous Spaces by Asking for Help

    [https://arxiv.org/abs/2402.08062](https://arxiv.org/abs/2402.08062)

    在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。

    

    大多数具有正式遗憾保证的强化学习算法假设所有错误都是可逆的，并依赖于尝试所有可能的选项。当一些错误是无法修复甚至是灾难性的时，这种方法会导致糟糕的结果。我们提出了一种上下文多臂赌博问题的变体，在这个问题中，目标是最小化发生灾难的概率。具体而言，我们假设每轮的回报代表了在该轮避免灾难的概率，并尝试最大化回报的乘积（总体避免灾难的概率）。为了给 agent 一些成功的机会，我们允许有限次向导师提问，并假设回报函数为 Lipschitz 连续的。我们提出了一种算法，当时间跨度增长时，它的遗憾和向导师查询率都趋近于 0，假设是一个连续的 1D 状态空间和相对"简单"的回报函数。我们还提供了一个匹配的下界：在没有简单性假设的情况下，任何算法要么不断查询异常的行为，要么每次查询完全相同的行为。

    Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
    
[^53]: G-Retriever: 用于文本图理解和问题解答的检索增强生成模型

    G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering

    [https://arxiv.org/abs/2402.07630](https://arxiv.org/abs/2402.07630)

    本论文提出了G-Retriever模型，该模型用于文本图理解和问题解答。该模型能够将用户的问题转化为文本回复，并突出显示图形的相关部分。与现有的方法不同，该模型适用于真实世界的文本图形，并可应用于不同的任务，包括场景图理解、常识推理和知识图推理。

    

    在给定具有文本属性的图形的情况下，我们使用户能够使用对话界面向图形提出问题。针对用户的问题，我们的方法提供文本回复并突出显示图形的相关部分。与现有的方法将大型语言模型(LLM)和图神经网络(GNN)以各种方式整合起来不同，它们大多集中在传统图任务(如节点、边和图分类)或者在小型或合成图上回答简单的图查询。相比之下，我们开发了一个灵活的问题回答框架，针对真实世界的文本图形，适用于多个应用，包括场景图理解、常识推理和知识图推理。为实现这个目标，我们首先用来自不同任务的数据开发了我们的图问题回答(GraphQA)基准测试。然后，我们提出了我们的G-Retriever方法，它集成了GNN和LLM的优势。

    Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, 
    
[^54]: 关于顺序预测中的标定距离研究

    On the Distance from Calibration in Sequential Prediction

    [https://arxiv.org/abs/2402.07458](https://arxiv.org/abs/2402.07458)

    本论文研究了顺序预测中的标定距离，证明了存在一种预测算法可以在敌人选择的二进制序列上实现$O(\sqrt{T})$的标定距离，通过较低的标定距离进行准确近似。

    

    我们研究了一种顺序二进制预测场景，在这种场景中，预测器的评估是以标定距离为基准的，标定距离定义为预测值与事后完全标定的预测集之间的$L_1$距离。这类似于最近由B{\l}asiok、Gopalan、Hu和Nakkiran（STOC 2023）提出的离线场景中的标定度量。标定距离是一种自然且直观的偏离完美标定的度量，并且满足不同于许多常见的标定度量（如$L_1$标定误差及其变种）的Lipschitz连续性属性。我们证明了存在一种预测算法，可以在对敌人选择的长度为$T$的二进制序列上，以期望$O(\sqrt{T})$的标定距离实现。在这个上界的核心是一个结构性结果，证明了标定距离可以通过较低的标定距离进行准确近似。

    We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a con
    
[^55]: HyperBERT:将混合超图感知层与语言模型用于文本属性超图上的节点分类

    HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs

    [https://arxiv.org/abs/2402.07309](https://arxiv.org/abs/2402.07309)

    本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。

    

    超图通过复杂的拓扑结构标记，表达多个实体之间的高阶相互作用，其中超边扮演重要角色。最近，基于超图的深度学习方法在学习文本属性超图上的节点分类问题中引起了越来越多的研究关注。然而，现有方法往往难以同时捕捉超图结构信息的全部内容和节点属性中的丰富语言属性，这在很大程度上影响了它们的效果和泛化能力。为了克服这些挑战，我们探索了如何通过为节点分类任务进一步增强预训练的BERT模型，引入专门的超图感知层。这些层将高阶结构归纳偏差引入语言模型中，从而提高模型利用超图结构中的高阶上下文信息和文本中的语义信息的能力。

    Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
    
[^56]: 更好还是更差？通过标签增强学习最小方差特征

    For Better or For Worse? Learning Minimum Variance Features With Label Augmentation

    [https://arxiv.org/abs/2402.06855](https://arxiv.org/abs/2402.06855)

    本研究分析了标签增强方法中标签增强的作用。研究证明，在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练可以学习到更高方差特征。此外，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。

    

    在过去的十年中，数据增强对于成功地训练深度学习模型在分类任务上发挥了关键作用。数据增强技术中的一个重要子类-包括标签平滑和Mixup-涉及在模型训练过程中修改输入数据和输入标签。在这项工作中，我们分析了此类方法中标签增强的作用。我们证明了在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练（包括权重衰减）可以学习到更高方差特征。我们的结果的一个重要后果是消极的：与标准训练相比，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。我们通过对合成数据和图像分类基准的一系列实验证明了我们的理论与实践的一致性。

    Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.
    
[^57]: 获取、合并、预测：通过数据湖增强表格

    Retrieve, Merge, Predict: Augmenting Tables with Data Lakes

    [https://arxiv.org/abs/2402.06282](https://arxiv.org/abs/2402.06282)

    本文通过对数据湖中的数据发现进行深入分析，着重于表格增强，提出了准确检索连接候选人的重要性和简单合并方法的效率，以及现有解决方案的好处和局限性。

    

    我们对数据湖中的数据发现进行了深入分析，重点是给定机器学习任务的表格增强。我们分析了三个主要步骤中使用的替代方法：检索可连接的表格、合并信息和预测结果表格。作为数据湖，本文使用了YADL（另一个数据湖）-我们开发的一种用于基准测试此数据发现任务的新型数据集-和Open Data US，一个被引用的真实数据湖。通过对这两个数据湖的系统性探索，我们的研究概述了准确检索连接候选人的重要性以及简单合并方法的效率。我们报告了现有解决方案的好处和局限性，旨在指导未来的研究。

    We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.
    
[^58]: Anfinsen Goes Neural: 一种用于条件抗体设计的图模型

    Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design

    [https://arxiv.org/abs/2402.05982](https://arxiv.org/abs/2402.05982)

    Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.

    

    抗体设计在推动治疗学方面起着关键作用。尽管深度学习在这个领域取得了快速进展，但现有方法对一般蛋白质知识的利用有限，并假设图模型违反蛋白质的经验发现。为了解决这些限制，我们提出了Anfinsen Goes Neural (AGN)，这是一个使用预训练的蛋白质语言模型(pLM)并编码了一种关于蛋白质的重要发现，即Anfinsen's dogma的图模型。我们的框架遵循序列生成和图神经网络(GNN)进行结构预测的两步过程。实验证明，我们的方法在基准实验中优于现有方法的结果。我们还解决了非自回归模型的一个关键限制，即它们倾向于生成具有过多重复标记的不现实序列。为了解决这个问题，我们引入了基于组合的正则化项到交叉熵目标中，可以实现有效的权衡。

    Antibody design plays a pivotal role in advancing therapeutics. Although deep learning has made rapid progress in this field, existing methods make limited use of general protein knowledge and assume a graphical model (GM) that violates empirical findings on proteins. To address these limitations, we present Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained protein language model (pLM) and encodes a seminal finding on proteins called Anfinsen's dogma. Our framework follows a two-step process of sequence generation with pLM and structure prediction with graph neural network (GNN). Experiments show that our approach outperforms state-of-the-art results on benchmark experiments. We also address a critical limitation of non-autoregressive models -- namely, that they tend to generate unrealistic sequences with overly repeating tokens. To resolve this, we introduce a composition-based regularization term to the cross-entropy objective that allows an efficient trade-off be
    
[^59]: 基因引导GFlowNets：在实际分子优化基准方面的进展

    Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark

    [https://arxiv.org/abs/2402.05961](https://arxiv.org/abs/2402.05961)

    本文提出了一种名为基因引导GFlowNet (Genetic GFN) 的新方法，通过集成迭代遗传搜索和训练策略，该方法在实际分子优化基准测试中取得了16.213的最新得分，明显优于现有最佳得分15.185，同时在14个任务中超越了所有对比方法。

    

    本文提出了一种新的GFlowNet变体，即基因引导GFlowNet (Genetic GFN)，它将迭代遗传搜索集成到GFlowNet中。遗传搜索有效地引导GFlowNet进入高回报区域，解决了全局过度探索导致的训练效率低下和探索有限区域的问题。此外，还引入了训练策略，如基于排名的重放训练和无监督最大似然预训练，以提高基因引导GFlowNet的样本效率。该方法在实际分子优化 (PMO) 领域的官方基准测试中显示了16.213的最新得分，明显优于基准测试中报告的最佳得分15.185。值得注意的是，我们的方法在23个任务中的14个任务中超过了所有对比方法，包括强化学习，贝叶斯优化，生成模型，GFlowNets和遗传算法。

    This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
    
[^60]: Transformer语言模型在算法学习上的限制

    Limits of Transformer Language Models on Algorithmic Learning

    [https://arxiv.org/abs/2402.05785](https://arxiv.org/abs/2402.05785)

    Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。

    

    我们分析了Transformer语言模型在学习离散算法方面的能力。为此，我们引入了两个要求组合多个离散子任务的新任务。我们通过从头开始训练LLaMA模型和在GPT-4和Gemini上提示来衡量学习学习原语的组合。我们观察到，目前最先进的Transformer语言模型的组合能力非常有限，并且在样本规模方面比为新的算法组合重新学习所有子任务效果更差。我们还提出了一个复杂性理论的定理，证明了记忆前馈模型上的梯度下降可以指数级地浪费数据。

    We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
    
[^61]: 通过并行观测预测改进基于令牌的世界模型

    Improving Token-Based World Models with Parallel Observation Prediction

    [https://arxiv.org/abs/2402.05643](https://arxiv.org/abs/2402.05643)

    该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。

    

    受到将Transformer应用于离散符号序列的成功启发，最近提出了基于令牌的世界模型（TBWMs）作为高效样本方法。在TBWMs中，世界模型将代理经验作为一种类似语言的令牌序列进行消耗，其中每个观测构成一个子序列。然而，在想象过程中，通过令牌逐个生成下一个观测的串行方式导致了严重的瓶颈问题，导致训练时间长、GPU利用率低和表示能力有限。为了解决这个瓶颈问题，我们设计了一种新颖的并行观测预测（POP）机制。POP通过一种针对我们的强化学习环境设计的新型前向模式来扩充了保持网络（RetNet）。我们将POP集成到一种名为REM（保持环境模型）的新型TBWM代理中，展示了比以前的TBWMs快15.4倍的想象能力。REM在Atari 100K基准测试的26个游戏中的12个游戏中达到超越人类水平的性能，并且在不到12小时的训练时间内完成训练。

    Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
    
[^62]: 浅层ReLU-like神经网络的损失景观：稳定点、鞍点逃逸和网络嵌入

    The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding

    [https://arxiv.org/abs/2402.05626](https://arxiv.org/abs/2402.05626)

    本文研究了使用ReLU-like激活函数以经验平方损失训练的单隐藏层神经网络的损失景观，提出了稳定点条件和逃逸神经元的定义，并将鞍点逃逸与逃逸神经元的参数变化联系起来。

    

    本文研究了使用ReLU-like激活函数以经验平方损失训练的单隐藏层神经网络的损失景观。由于激活函数是不可微的，目前还不清楚如何完全描述稳定点。我们提出了适用于非可微和可微情况的稳定点条件。此外，我们还展示了如果一个稳定点不包含“逃逸神经元”（通过一阶条件定义），那么它必定是一个局部最小值。此外，在标量输出情况下，逃逸神经元的存在保证了稳定点不是局部最小值。我们的结果进一步描述了从无穷小（消失）初始化开始的浅层ReLU-like网络的鞍点到鞍点的训练过程，直接将鞍点逃逸与逃逸神经元的参数变化联系起来。此外，我们还完全讨论了网络嵌入的方式。

    In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain "escape neurons", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network emb
    
[^63]: 准确的LLMs的LoRA-Finetuning量化通过信息保留

    Accurate LoRA-Finetuning Quantization of LLMs via Information Retention

    [https://arxiv.org/abs/2402.05445](https://arxiv.org/abs/2402.05445)

    本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。

    

    将LLMs的LoRA-finetuning量化研究得到准确但紧凑的LLMs以便在资源受限的硬件上部署。然而，现有的方法导致量化的LLMs严重退化，甚至无法从LoRA的调优中获益。本文提出了一种新颖的IR-QLoRA，通过信息保留推动带有LoRA的量化LLMs变得高度准确。所提出的IR-QLoRA主要依赖于两种从统一信息视角派生的技术：（1）基于统计的信息校准量化允许LLMs的量化参数精确保留原始信息；（2）基于调优的信息弹性连接使LoRA利用具有多样信息的弹性表示转换。综合实验证明，在2-4位宽下，IR-QLoRA可以显著提高LLaMA和LLaMA2系列的准确性，例如，4位LLaMA-7B相比于...

    The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
    
[^64]: 关于分散推断模型的扩散模型：基准测试和改进随机控制和采样

    On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling

    [https://arxiv.org/abs/2402.05098](https://arxiv.org/abs/2402.05098)

    本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。

    

    我们研究了训练扩散模型以从给定的非标准化密度或能量函数分布中采样的问题。我们对几种扩散结构推断方法进行了基准测试，包括基于模拟的变分方法和离策略方法（连续生成流网络）。我们的结果揭示了现有算法的相对优势，同时对过去的研究提出了一些质疑。我们还提出了一种新颖的离策略方法探索策略，基于目标空间中的局部搜索和回放缓冲区的使用，并证明它可以改善各种目标分布上的样本质量。我们研究的采样方法和基准测试的代码已公开在https://github.com/GFNOrg/gfn-diffusion，作为未来在分散推断模型上工作的基础。

    We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
    
[^65]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^66]: SCAFFLSA：量化和消除联邦线性随机逼近和时间差异学习中的异质性偏差

    SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning

    [https://arxiv.org/abs/2402.04114](https://arxiv.org/abs/2402.04114)

    本文量化了联邦线性随机逼近算法中异质性偏差的影响，并提出SCAFFLSA作为一种改进方法来消除此偏差。在联邦时间差异学习中，该方法能够显著提高算法的复杂性。

    

    本文对联邦线性随机逼近算法（FedLSA）进行了非渐进分析。我们明确量化了异质代理本地训练引入的偏差，并研究了该算法的样本复杂性。我们证明了FedLSA的通信复杂性与所需精度 $\epsilon$ 呈多项式关系，这限制了联邦的好处。为了克服这一问题，我们提出了SCAFFLSA，一种新型的FedLSA变体，它使用控制变量来校正本地训练的偏差，并在不对统计异质性做出任何假设的情况下证明了其收敛性。我们将所提出的方法应用于具有线性函数逼近的联邦时间差异学习，并分析了相应的复杂性改进。

    In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
    
[^67]: 返回对齐的决策Transformer

    Return-Aligned Decision Transformer

    [https://arxiv.org/abs/2402.03923](https://arxiv.org/abs/2402.03923)

    本研究提出了返回对齐的决策Transformer（RADT），通过分离回报与传统输入序列，实现有效地将实际回报与目标回报对齐。

    

    传统的离线强化学习方法旨在学习最大化累积奖励（即回报）的最优策略。然而，随着应用范围的扩大，训练能够最大化回报并使实际回报与指定目标回报对齐的智能体变得越来越重要，从而控制智能体的性能。决策Transformer（DT）通过监督学习优化生成以目标回报为条件的动作的策略，并配备了使用目标回报控制智能体的机制。尽管DT旨在对齐实际回报与目标回报，但我们在实验中发现了DT中实际回报与目标回报之间的差异。在本文中，我们提出了返回对齐的决策Transformer（RADT），旨在有效地将实际回报与目标回报对齐。我们的模型将回报从传统的输入序列中分离出来，传统输入序列通常包含回报、状态和动作。

    Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
    
[^68]: 来自打包奖励的强化学习：一种基于Transformer的实例级奖励重新分配方法

    Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution

    [https://arxiv.org/abs/2402.03771](https://arxiv.org/abs/2402.03771)

    该论文提出了一种名为强化学习来自打包奖励的新问题，其中学习器只能获取序列的打包奖励，在这种情况下探索未知的即时奖励是一项困难的任务，并引入了基于Transformer的方法来解决这个问题。

    

    在强化学习中，每个动作的即时奖励信号将为代理生成，以便代理学习如何最大化累积奖励以获取最优策略。然而，在许多实际应用中，代理无法获取即时奖励信号。相反，学习器只在路径的结束处获取奖励，其中路径的部分序列被定义为一个包。在这种情况下，学习器必须面对探索包中未知即时奖励的显著困难，这不能通过现有方法解决，包括仅考虑完整路径并忽略内部奖励分布的轨迹方法。为了正式研究这种情况，我们引入了一种新的强化学习设置，称为来自打包奖励的强化学习（Reinforcement Learning from Bagged Rewards，RLBR），只能获取序列的打包奖励。我们进行了理论研究，建立了RLBR与标准强化学习之间的联系。

    In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa
    
[^69]: 用分布式神经计算突破维度灾难

    Breaking the Curse of Dimensionality with Distributed Neural Computation

    [https://arxiv.org/abs/2402.03460](https://arxiv.org/abs/2402.03460)

    通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。

    

    我们提出了一种理论方法，利用分布式神经计算算法来克服维度灾难。我们的模块化分布式深度学习范式，称为“神经途径”，可以在多台机器上实现任意精度，同时仅加载少量参数到GPU VRAM中。具体地，我们证明了对于每个误差水平$\varepsilon>0$和每个Lipschitz函数$f:[0,1]^n\to \mathbb{R}$，我们可以构建一个神经途径模型，该模型能够在$[0,1]^n$上以$\varepsilon$精度均匀逼近$f$，并且仅需要在内存中加载$\mathcal{O}(\varepsilon^{-1})$个网络参数以及在前向传播期间加载$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$个网络参数。这改进了传统非分布式深度学习模型（即ReLU多层感知机）的最优界限，后者需要$\mathcal{O}(\varepsilon^{-n/2})$个参数来达到相同的精度。目前唯一的其他可用深度学习模型

    We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and $\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model
    
[^70]: 深度补全的测试时间自适应

    Test-Time Adaptation for Depth Completion

    [https://arxiv.org/abs/2402.03312](https://arxiv.org/abs/2402.03312)

    该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。

    

    当将在一些（源）数据集上训练的模型转移到目标测试数据时，常常会观察到性能下降，这是由于它们之间存在领域差距。现有的用于弥合这一差距的方法，如领域适应（DA），可能需要模型训练时使用的源数据（通常不可用），而其他方法，如无源DA，则需要多次通过测试数据。我们提出了一种在线测试时间自适应方法，用于深度补全，即从单个图像和相关的稀疏深度图推断出密集深度图的任务，以在一次通过中缩小性能差距。首先，我们对每种数据模态中的领域转移如何影响模型性能进行了研究。根据我们的观察，稀疏深度模态展现出比图像更小的协变量转移，因此我们设计了一个在源领域中训练的嵌入模块，它保留了从仅编码稀疏深度特征到编码图像和稀疏深度的特征的映射。在测试时间，我们使用这个嵌入模块实现自适应。

    It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
    
[^71]: 一个部分观察到的奖励状态在RLHF中的框架

    A Framework for Partially Observed Reward-States in RLHF

    [https://arxiv.org/abs/2402.03282](https://arxiv.org/abs/2402.03282)

    这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。

    

    最近几年来，强化学习从人类反馈（RLHF）的研究因其在LLMs的发展中起到的作用而变得重要。神经科学研究表明，人类对刺激的反应已知依赖于部分观察到的“内部状态”。不幸的是，当前的RLHF模型没有考虑到这一点。此外，大多数RLHF模型没有考虑到中间反馈，在实证研究中变得越来越重要，可以帮助提高样本复杂性和对齐性。为了解决这些局限性，我们将RLHF建模为部分观察到的奖励状态的强化学习（PORRL）。我们展示了从RLHF中两种主要形式的人类反馈 - 基数反馈和决斗反馈到PORRL的缩减。对于基数反馈，我们开发了通用的统计高效算法，并将它们实例化为POR-UCRL和POR-UCBVI。对于决斗反馈，我们表明，简单的基数反馈缩减不能达到亚线性的决斗回归。

    The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
    
[^72]: 用球面高斯约束进行条件扩散引导

    Guidance with Spherical Gaussian Constraint for Conditional Diffusion

    [https://arxiv.org/abs/2402.03201](https://arxiv.org/abs/2402.03201)

    本文提出了一种用球面高斯约束的扩散算法（DSG），解决了在条件生成任务中采样过程中的流形偏离问题。这种算法通过优化将步骤限制在中间数据流形内，并能够使用较大的引导步长。

    

    最近扩散模型的进展尝试通过利用可微的损失函数进行指导来处理条件生成任务，而无需额外的训练。虽然这些方法在一定程度上取得了成功，但它们往往在样本质量上做出妥协，并需要较小的引导步长，导致采样过程变长。本文揭示了在引导损失的采样过程中流形偏离的根本问题所在。我们通过建立损失引导的估计误差的特定下界从理论上证明了流形偏离的存在。为了减轻这个问题，我们提出了带有球面高斯约束（DSG）的扩散，从高维高斯分布的集中现象中汲取灵感。DSG通过优化有效地将引导步骤约束在中间数据流形内，并能够使用较大的引导步长。此外，我们提出了一个闭式公式。

    Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
    
[^73]: 同性质，聚类和分类器

    Isotropy, Clusters, and Classifiers

    [https://arxiv.org/abs/2402.03191](https://arxiv.org/abs/2402.03191)

    同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。

    

    最近，关于嵌入空间是否均匀利用所有维度（即是否具有同性质）的问题引起了讨论。有证据支持和反对在嵌入空间中实施同性质。在本文中，我们强调同性质对嵌入空间的要求与聚类的存在不兼容，这也对线性分类目标产生了负面影响。我们通过实验证明了这个事实，并用它来阐明文献中的先前结果。

    Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
    
[^74]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^75]: 扩散吉布斯采样

    Diffusive Gibbs Sampling

    [https://arxiv.org/abs/2402.03008](https://arxiv.org/abs/2402.03008)

    扩散吉布斯采样是一种创新的采样方法，通过集成扩散模型并应用吉布斯采样，有效地从具有远程和断开模态特征的分布中采样，表现出比其他方法更好的混合性能，并在多种任务中取得显著改进的结果。

    

    传统马尔可夫链蒙特卡洛（MCMC）方法在多模态分布的混合不足方面存在着挑战，特别是在贝叶斯推断和分子动力学等实际应用中。针对这个问题，我们提出了一种创新的采样方法——扩散吉布斯采样（DiGS），用于有效采样具有远程和断开模态特征的分布。DiGS集成了扩散模型的最新发展，利用高斯卷积创建一个辅助噪声分布，以在原始空间中连接孤立的模态，并应用吉布斯采样从两个空间中交替抽取样本。我们的方法在采样多模态分布方面表现出比并行温度法等最先进方法更好的混合性能。我们证明我们的采样器在各种任务中取得了显著改进的结果，包括高斯混合模型、贝叶斯神经网络和分子动力学。

    The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
    
[^76]: 探究多模态多任务基础模型用于道路场景理解：从学习视角的观点

    Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives

    [https://arxiv.org/abs/2402.02968](https://arxiv.org/abs/2402.02968)

    本调查对多模态多任务视觉理解基础模型在道路场景中的应用进行了系统分析，展示了其多模态和多任务学习能力，在处理各种驾驶相关任务方面具有强大的适应性，为实现对周围场景的更全面理解做出了重要贡献。

    

    基础模型的确对各个领域产生了深远的影响，成为显著塑造智能系统能力的关键组件。在智能车辆的背景下，利用基础模型的力量已被证明具有变革性，显著推进了视觉理解的进展。多模态多任务视觉理解基础模型(MM-VUFMs)具备多模态和多任务学习能力，能够有效地处理和融合来自不同模态的数据，并同时处理各种与驾驶相关的任务，具备强大的适应性，为对周围场景的更全面理解做出贡献。在本调查中，我们对专门设计用于道路场景的MM-VUFMs进行了系统分析。我们的目标不仅是提供对常见实践的综合概述，涉及任务特定模型、统一的多模态模型、统一的多任务模型和基础模型促进技术。

    Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techni
    
[^77]: 具有Koopman算子的全局超梯度估计

    Glocal Hypergradient Estimation with Koopman Operator

    [https://arxiv.org/abs/2402.02741](https://arxiv.org/abs/2402.02741)

    本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。

    

    基于梯度的超参数优化方法使用超梯度来更新超参数，即元标准的梯度与超参数的关系。先前的研究使用两种不同的更新策略：一种是使用模型训练完成后得到的全局超梯度来优化超参数，另一种是使用每个模型更新之后得到的局部超梯度。虽然全局超梯度具有可靠性，但计算成本显著；相反，局部超梯度速度快但常常不是最优的。在本文中，我们提出了glocal超梯度估计，将“全局”的质量与“局部”的效率结合起来。为此，我们使用Koopman算子理论来线性化超梯度的动态，以便可以仅通过使用局部超梯度的轨迹来高效地近似全局超梯度。因此，我们可以使用估计的全局超梯度贪婪地优化超参数，同时实现可靠性和效率。

    Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending "global" quality with "local" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo
    
[^78]: 对分布式数据的条件平均治疗效果估计：一种保护隐私的方法

    Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach

    [https://arxiv.org/abs/2402.02672](https://arxiv.org/abs/2402.02672)

    本论文提出了一种数据协作双机器学习（DC-DML）方法，该方法可以在保护分布式数据隐私的情况下估计条件平均治疗效果（CATE）模型。通过数值实验验证了该方法的有效性。该方法的三个主要贡献是：实现了对分布式数据上的非迭代通信的半参数CATE模型的估计和测试，提高了模型的鲁棒性。

    

    在医学和社会科学等各个领域中，对条件平均治疗效果（CATEs）的估计是一个重要的课题。如果分布在多个参与方之间的数据可以集中，可以对CATEs进行高精度的估计。然而，如果这些数据包含隐私信息，则很难进行数据聚合。为了解决这个问题，我们提出了数据协作双机器学习（DC-DML）方法，该方法可以在保护分布式数据隐私的情况下估计CATE模型，并通过数值实验对该方法进行了评估。我们的贡献总结如下三点。首先，我们的方法能够在分布式数据上进行非迭代通信的半参数CATE模型的估计和测试。半参数或非参数的CATE模型能够比参数模型更稳健地进行估计和测试，对于模型偏差的鲁棒性更强。然而，据我们所知，目前还没有提出有效的通信方法来估计和测试这些模型。

    Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
    
[^79]: 通过状态扩展和随机排列的方法进行变分DAG估计

    Variational DAG Estimation via State Augmentation With Stochastic Permutations

    [https://arxiv.org/abs/2402.02644](https://arxiv.org/abs/2402.02644)

    使用状态扩展和随机排列进行变分DAG估计的方法可以超越竞争的贝叶斯和非贝叶斯基准方法，从而在估计贝叶斯网络结构方面取得更好的性能。

    

    从观测数据中估计贝叶斯网络的结构，即有向无环图（DAG），是一个在统计和计算上都很困难的问题，在因果发现等领域有着重要应用。贝叶斯方法在解决这个任务方面是一个有希望的方向，因为它们允许进行不确定性量化，并处理众所周知的可识别性问题。从概率推断的角度来看，主要的挑战是（i）表示满足DAG约束的图的分布和（ii）估计底层组合空间的后验概率。我们提出了一种方法，通过在DAG和排列的扩展空间上构建联合分布来解决这些挑战。我们通过变分推断进行后验估计，在其中利用了离散分布的连续松弛。我们展示了我们的方法在一系列合成和实际数据上能够超越竞争的贝叶斯和非贝叶斯基准方法。

    Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
    
[^80]: PoCo: 来自和为异构机器人学习的策略组合

    PoCo: Policy Composition from and for Heterogeneous Robot Learning

    [https://arxiv.org/abs/2402.02511](https://arxiv.org/abs/2402.02511)

    PoCo是一种策略组合方法，通过组合不同模态和领域的数据分布，实现了从异构数据中训练通用机器人策略的目标。该方法可以实现场景级和任务级的广义操作技能学习，并在推理时通过任务级组合和分析成本函数进行策略行为的自适应调整。

    

    从异构数据中训练通用的机器人策略以处理不同任务是一个重大挑战。现有的机器人数据集在颜色、深度、触觉和姿态感等不同模态上存在差异，并在模拟、真实机器人和人类视频等不同领域收集。目前的方法通常会收集并汇集一个领域的所有数据，以训练一个单一策略来处理任务和领域的异构性，这是非常昂贵和困难的。在这项工作中，我们提出了一种灵活的方法，称为策略组合，通过组合用扩散模型表示的不同数据分布，将跨不同模态和领域的信息结合起来，以学习场景级和任务级的广义操作技能。我们的方法可以在多任务操作中使用任务级组合，并与分析成本函数组合，以在推理时调整策略行为。我们在模拟、人类和实际机器人数据上训练我们的方法。

    Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real 
    
[^81]: 重新审视视觉调整中提示词的力量

    Revisiting the Power of Prompt for Visual Tuning

    [https://arxiv.org/abs/2402.02382](https://arxiv.org/abs/2402.02382)

    本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。

    

    视觉提示调整（VPT）是一种很有前景的解决方案，它利用可学习的提示词来定制预训练模型，用于下游任务。然而，VPT及其变种经常遇到诸如提示初始化、提示长度和自监督预训练中性能不佳等挑战，阻碍了成功的上下文适应。本研究从探索训练过程中提示词与补丁令牌之间的相关性演变开始。受到提示令牌与补丁令牌之间往往具有高互信息的观察启发，我们提出利用下游令牌原型对提示进行初始化。该策略性初始化明显提高了微调性能，相比于VPT，无需增加计算开销，我们进一步优化令牌构造，使用简化的流程保持了出色的性能。详尽的实验表明，我们提出的方法胜过了其他方法。

    Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe
    
[^82]: 动量在隐式逐步优化中对目标函数的平滑作用的角色

    Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization

    [https://arxiv.org/abs/2402.02325](https://arxiv.org/abs/2402.02325)

    这篇论文揭示了具有动量的随机梯度下降算法平滑了目标函数，影响程度由多个超参数决定，同时提供了对动量改善泛化能力的理论解释和新见解。

    

    虽然具有动量的随机梯度下降（SGD）具有快速收敛和良好的泛化能力，但对此缺乏理论解释。本文展示了具有动量的SGD平滑了目标函数，其程度由学习率、批大小、动量因子、随机梯度的方差以及梯度范数的上界确定。这一理论发现揭示了为什么动量改善了泛化能力，并提供了关于动量因子等超参数作用的新见解。我们还提出了一种利用SGD动量平滑特性的隐式逐步优化算法，并提供了实验结果支持我们的观点，即SGD动量平滑了目标函数。

    While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
    
[^83]: INViT:一种具有不变嵌套视图转换器的可泛化解决路由问题的方法

    INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer

    [https://arxiv.org/abs/2402.02317](https://arxiv.org/abs/2402.02317)

    INViT是一种具有不变嵌套视图转换器的解决路由问题的方法，通过强制嵌套设计和不变的视图，在编码器内部提高学习求解器的泛化能力，从而实现了在具有不同分布和不同问题规模的TSP和CVRP问题上卓越的泛化性能。

    

    最近，深度强化学习在学习解决路由问题的快速启发式方法方面取得了有希望的结果。与此同时，大多数求解器在推广到未见过的分布或具有不同规模的分布时遇到了困难。为解决这个问题，我们提出了一种新的架构，称为不变嵌套视图转换器（INViT），它旨在通过在编码器内部强制一个嵌套设计以及不变的视图来促进学习求解器的泛化能力。它应用了修改的策略梯度算法，并结合了数据增强。我们证明了所提出的INViT在具有不同分布和不同问题规模的TSP和CVRP问题上取得了卓越的泛化性能。

    Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.
    
[^84]: 通过修正的缩放定律选择大型语言模型进行微调

    Selecting Large Language Model to Fine-tune via Rectified Scaling Law

    [https://arxiv.org/abs/2402.02314](https://arxiv.org/abs/2402.02314)

    该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。

    

    在日益增长的语言模型生态系统中，在众多选项中选择最合适的预训练模型进行微调成为了一个挑战。在资源受限的情况下，微调所有模型然后再进行选择是不现实的。在本文中，我们将这个资源受限的选择任务转化为预测微调性能，并且展示其与缩放定律之间的自然联系。与预训练不同，我们发现微调的缩放曲线不仅包括众所周知的“功率阶段”，还包括以前未被观察到的“预功率阶段”。我们还解释了为什么现有的缩放定律无法理论和实证地捕捉到这种相变现象。为了解决这个问题，我们将“预学习数据大小”概念引入到我们的修正缩放定律中，这克服了理论上的限制，并更好地适应实验结果。通过利用我们的定律，我们提出了一种新颖的语言模型选择算法，可以选择接近最优的模型。

    The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
    
[^85]: 在组合优化问题中寻找多样化解决方案的连续张量放松方法

    Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems

    [https://arxiv.org/abs/2402.02190](https://arxiv.org/abs/2402.02190)

    本研究提出了连续张量放松方法(CTRA)，用于在组合优化问题中寻找多样化的解决方案。CTRA通过对离散决策变量进行连续放松，解决了寻找多样化解决方案的挑战。

    

    在组合优化问题中，寻找最佳解是最常见的目标。然而，在实际场景中，单一解决方案可能不适用，因为目标函数和约束条件只是原始现实世界情况的近似值。为了解决这个问题，寻找具有不同特征的多样化解决方案和约束严重性的变化成为自然的方向。这种策略提供了在后处理过程中选择合适解决方案的灵活性。然而，发现这些多样化解决方案比确定单一解决方案更具挑战性。为了克服这一挑战，本研究引入了连续张量松弛退火 (CTRA) 方法，用于基于无监督学习的组合优化求解器。CTRA通过扩展连续松弛方法，将离散决策变量转换为连续张量，同时解决了多个问题。该方法找到了不同特征的多样化解决方案和约束严重性的变化。

    Finding the best solution is the most common objective in combinatorial optimization (CO) problems. However, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. To tackle this, finding (i) "heterogeneous solutions", diverse solutions with distinct characteristics, and (ii) "penalty-diversified solutions", variations in constraint severity, are natural directions. This strategy provides the flexibility to select a suitable solution during post-processing. However, discovering these diverse solutions is more challenging than identifying a single solution. To overcome this challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning-based CO solvers. CTRA addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. This method finds heterog
    
[^86]: SymbolicAI: 一个结合生成模型和求解器的基于逻辑的方法的框架

    SymbolicAI: A framework for logic-based approaches combining generative models and solvers

    [https://arxiv.org/abs/2402.00854](https://arxiv.org/abs/2402.00854)

    SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。

    

    我们介绍了SymbolicAI，这是一个多功能且模块化的框架，采用基于逻辑的方法来处理生成过程中的概念学习和流程管理。SymbolicAI通过将大型语言模型（LLM）作为语义解析器来执行基于自然语言和形式语言指令的任务，从而弥合了符号推理和生成式人工智能之间的差距，使生成模型与各种求解器无缝集成。我们利用概率编程原理来处理复杂任务，并利用可微分和经典编程范 paradigms 的各自优势。该框架引入了一系列多态的、组合的和自指的数据流操作，将LLM的输出与用户的目标对齐。因此，我们可以在具有零次和少次学习能力的各种基础模型之间进行过渡，并与擅长解决特定问题的专业化调优模型或求解器配合使用。

    We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
    
[^87]: 数据驱动的通过伴随方法发现偏微分方程

    Data-Driven Discovery of PDEs via the Adjoint Method

    [https://arxiv.org/abs/2401.17177](https://arxiv.org/abs/2401.17177)

    本文提出了一种通过伴随方法从数据中发现潜在偏微分方程的方法，展示了在给定光滑数据集的情况下，该方法可以恢复真实的PDE，但在存在噪声的情况下，准确性与PDE-FIND方法相当。

    

    在这项工作中，我们提出了一种通过伴随方法来发现给定数据的潜在偏微分方程（PDEs）的方法。我们的思路是以一般形式考虑参数化的PDE，并制定最小化PDE解与数据误差的优化问题。利用变分计算，我们得到了拉格朗日乘子（伴随方程）的演化方程，使我们能够直接计算出与PDE参数相关的目标函数的梯度。特别是对于一族参数化和非线性PDEs，我们展示了如何推导出相应的伴随方程。我们在这里展示了，在给定光滑数据集的情况下，所提出的伴随方法可以以机器精度恢复真实的PDE。然而，在存在噪声的情况下，伴随方法的准确性与著名的PDE-FIND（Rudy et al., 2017）方法相当。

    In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even th
    
[^88]: 使用通信成本低于18千字节的联邦全参数调整亿级语言模型

    Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes

    [https://arxiv.org/abs/2312.06353](https://arxiv.org/abs/2312.06353)

    本论文提出了一种名为FedKSeed的方法，使用零阶优化和有限的随机种子集合，实现了通信成本较低的联邦全参数调整。该方法使得在终端设备上可以进行亿级语言模型的联邦全参数调整，具有较高的性能表现。

    

    预训练的大型语言模型（LLM）需要通过细化调整来提高对自然语言指令的响应能力。联邦学习提供了一种在不牺牲数据隐私的情况下，利用终端设备上丰富的数据对LLM进行细化调整的方法。大多数现有的LLM联邦细化调整方法依赖于参数高效的细化调整技术，但可能无法达到全参数调整可能达到的性能高度。然而，由于巨大的通信成本，LLM的联邦全参数调整是一个非常困难的问题。本研究介绍了FedKSeed，它使用随机种子的有限集合进行零阶优化。它显著降低了服务器和终端之间的传输要求，仅需传输几个随机种子和标量梯度，仅占用几千字节的空间，使得在终端设备上能够进行亿级LLM的联邦全参数调整。在此基础上，我们开发了一种策略，实现了概率差异化种子采样，优先考虑一些种子，从而进一步提高了联邦全参数调整的效果。

    Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioriti
    
[^89]: SQLformer：深度自回归查询图生成用于文本到SQL翻译

    SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation

    [https://arxiv.org/abs/2310.18376](https://arxiv.org/abs/2310.18376)

    SQLformer是一个用于文本到SQL翻译的深度自回归查询图生成模型，采用了特定的Transformer架构，并通过结构归纳偏差解决领域泛化和自然语言与SQL查询对齐的难题。

    

    近年来，对于文本到SQL翻译的兴趣日益增长，这是将自然语言问题转化为可执行SQL查询的任务。这项技术具有潜在的潜力，可以使数据库中的数据提取民主化。然而，其中一些主要障碍包括领域泛化，即适应以前未见到的数据库，并且将自然语言问题与相应的SQL查询对齐。为了克服这些挑战，我们引入了SQLformer，这是一种针对执行文本到SQL翻译任务而设计的新型Transformer体系结构。我们的模型以自回归的方式预测SQL查询，并在编码器和解码器层中结合结构归纳偏差。这种偏差是由数据库表和列选择引导的，有助于解码器以广度优先搜索的规范顺序生成SQL查询的图形表示。全面的实验说明了现阶段的技术水平

    In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
    
[^90]: 探索LLM代理的协作机制：社会心理学视角

    Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View

    [https://arxiv.org/abs/2310.02124](https://arxiv.org/abs/2310.02124)

    通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。

    

    随着自然语言处理（NLP）系统越来越多地应用于复杂的社会环境中，一个迫切的问题出现了：这些NLP系统能否模仿类人类的协作智能，在由多个大型语言模型（LLMs）组成的多代理社会中？本文通过将实践实验与理论观点相结合，探究当代NLP系统之间的协作机制。我们构建了四个由LLM代理组成的独特“社会”，每个代理以特定的“特质”（随和或过于自信）为特征，并与不同的“思维模式”（辩论或反思）展开协作。通过在三个基准数据集上评估这些多代理社会，我们发现某些协作策略不仅胜过先前顶尖方法，而且优化了效率（使用更少的API令牌）。此外，我们的结果进一步说明LLM代理可以

    arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
    
[^91]: Context-Former：基于潜在条件序列建模的拼接技术

    Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])

    [http://arxiv.org/abs/2401.16452](http://arxiv.org/abs/2401.16452)

    Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。

    

    离线强化学习（RL）算法可以通过拼接次优轨迹来改善决策，从而获得更优的结果。这种能力是使RL能够学习优于行为策略的策略的关键因素。另一方面，Decision Transformer（DT）将决策建模为序列建模，展示了在离线RL基准测试中竞争性的性能，然而，最近的研究表明DT缺乏拼接能力，因此提高DT性能需要利用拼接能力。为了赋予DT拼接能力，我们将轨迹拼接抽象为专家匹配，并引入了我们的方法ContextFormer，通过模拟有限数量的专家轨迹的表示来集成基于情境信息的模仿学习（IL）和序列建模，以拼接次优轨迹片段。为了验证我们的观点，我们从两个角度进行实验证明：

    Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
    
[^92]: 使用Sum-Product Networks生成可能的反事实推理

    Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])

    [http://arxiv.org/abs/2401.14086](http://arxiv.org/abs/2401.14086)

    由于用户需求和最近的法规要求，需要对AI系统所做出的决策进行解释。本论文提出了一种使用Sum-Product Networks模拟寻找高可能性反事实推理的系统，该系统能够提供满足多个常见要求的最佳解释。

    

    由于用户需求和最近的法规（GDPR、AI法案），需要解释AI系统所做出的决策。这些决策往往只能在事后解释，反事实推理成为常见的解释方式。什么构成了最佳的反事实解释必须考虑多个方面，其中“样本距离”是最常见的。我们认为，这一要求经常会导致不太可能且因此价值有限的解释。在这里，我们提出了一个能够提供高可能性解释的系统。我们展示了使用混合整数优化（MIO）模拟寻找满足反事实推理的许多常见要求的最有可能解释。在此过程中，我们提出了Sum-Product Network（SPN）的MIO表达，并使用SPN估计反事实的可能性，这对独立的兴趣也有用。与生成反事实解释的几种方法进行数值比较。

    Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
    
[^93]: 超越概念瓶颈模型：如何使黑盒模型可干预？

    Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?. (arXiv:2401.13544v1 [cs.LG])

    [http://arxiv.org/abs/2401.13544](http://arxiv.org/abs/2401.13544)

    本文介绍了一种超越概念瓶颈模型的方法，可以使黑盒模型可干预。通过基于概念的干预来影响模型的输出，并利用这种方法对黑盒模型进行微调。实验证明，微调可以提高干预的效果，并产生更好校准的预测。

    

    最近，可解释的机器学习重新探索了概念瓶颈模型（CBM），包括从原始特征中逐步预测高级概念和从预测的概念中预测目标变量。这个模型类别的一个引人注目的优势是用户能够对预测的概念值进行干预，从而影响模型的下游输出。在这项工作中，我们介绍了一种方法，在已经训练好但本质上不可解释的神经网络上进行基于概念的干预，给定一个带有注释的验证集。此外，我们将模型的可干预性定义为基于概念干预的有效性的度量，并利用这个定义来对黑盒模型进行微调。实证上，我们探索了合成表格数据和自然图像基准上黑盒分类器的干预性。我们证明，微调提高了干预的效果，并经常产生更好校准的预测。

    Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the 
    
[^94]: 可解释概念瓶颈用于对齐强化学习代理

    Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])

    [http://arxiv.org/abs/2401.05821](http://arxiv.org/abs/2401.05821)

    SCoBots是一种可解释的概念瓶颈代理，能够透明化整个决策流程，帮助领域专家理解和规范强化学习代理的行为，从而可能实现更好的人类对齐强化学习。

    

    奖励稀疏性、难以归因的问题以及不对齐等等都是深度强化学习代理学习最优策略困难甚至不可能的原因之一。不幸的是，深度网络的黑盒特性阻碍了领域专家的参与，这些专家可以解释模型并纠正错误行为。为此，我们引入了连续概念瓶颈代理（SCoBots），通过整合连续的概念瓶颈层，使整个决策流程透明化。SCoBots不仅利用相关的对象属性，还利用关系概念。实验结果强有力地证明，SCoBots使领域专家能够有效理解和规范他们的行为，从而可能实现更好的人类对齐强化学习。通过这种方式，SCoBots使我们能够识别最简单且具有代表性的视频游戏Pong中的不对齐问题，并加以解决。

    Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
    
[^95]: AUTOACT：通过自主规划实现的自动代理学习

    AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])

    [http://arxiv.org/abs/2401.05268](http://arxiv.org/abs/2401.05268)

    AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。

    

    语言代理在各种复杂任务上取得了相当的性能。尽管在这个领域进行了不断的探索，但现有的语言代理系统仍然面临昂贵、不可重复的数据依赖问题，并且面临将单一模型应用于多个功能的挑战。为此，我们介绍了AutoAct，这是一个自动代理学习框架，不依赖于大规模带注释的数据和来自闭源模型（如GPT-4）的合成轨迹。给定有限的数据和工具库，AutoAct首先自动合成规划轨迹，不需要人类或强闭源模型的任何辅助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动区分，产生一个子代理组来完成任务。我们进行了多种LLMs的广泛实验，结果显示AutoAct在性能上优于或与其相当。

    Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
    
[^96]: 欺骗的艺术：使用动态触发器的强健后门攻击

    The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])

    [http://arxiv.org/abs/2401.01537](http://arxiv.org/abs/2401.01537)

    这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。

    

    由于人工智能行业的最新进展，机器学习作为服务（MLaaS）领域正在经历增长的实施。然而，这种增长引发了对AI防御机制的担忧，特别是对于来自不完全可信的第三方提供商的潜在隐蔽攻击。最近的研究发现，听觉后门可能使用某些修改作为其启动机制。DynamicTrigger作为一种方法被引入，用于进行使用巧妙设计的调整来确保损坏的样本与干净的样本无法区分的动态后门攻击。通过利用波动的信号采样率，并通过动态声音触发器（比如拍手声）对说话者身份进行掩盖，可以欺骗语音识别系统（ASR）。我们的实证测试表明，DynamicTrigger在隐蔽攻击中既有效又隐蔽，并在攻击过程中取得了令人印象深刻的成功率。

    The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
    
[^97]: 自补代码生成

    Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2311.17972](http://arxiv.org/abs/2311.17972)

    本文介绍了自补代码生成的通用框架，利用自补机制实现了中断和循环机制，使传统解码进程变得非单调。利用中断机制可以推迟生成代码，增强对输出的控制；利用循环机制可以循环更新和同步生成的每个部分。

    

    本文介绍了一种自补代码生成的通用框架，它将补充操作融入自回归解码中。我们的方法利用了最近的能够进行填充的代码语言模型可以自动进行填充的观察结果：补充操作旨在根据预定义的前缀和后缀填充中间内容，而自补机制顺序生成这些周围上下文和被填充内容。我们利用这种能力在传统解码中引入了新颖的中断和循环机制，使其进化为非单调过程。中断机制允许推迟生成特定的代码，直到确定的后缀建立，增强对输出的控制。同时，循环机制利用自补和从左到右解码的互补性，可以循环更新和同步每个生成部分。我们进行了大量实验来证明我们的方法的有效性。

    This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
    
[^98]: 乐观的多智能体策略梯度在合作任务中的应用

    Optimistic Multi-Agent Policy Gradient for Cooperative Tasks. (arXiv:2311.01953v1 [cs.LG])

    [http://arxiv.org/abs/2311.01953](http://arxiv.org/abs/2311.01953)

    本论文提出了一个通用的、简单的框架，在多智能体策略梯度方法中引入乐观更新，以缓解合作任务中的相对过度概括问题。通过使用一个泄漏化线性整流函数来重塑优势，我们的方法能够保持对潜在由其他代理引起的低回报个别动作的乐观态度。

    

    在合作多智能体学习任务中，由于过拟合其他智能体的次优行为，导致智能体收敛到次优联合策略，出现了相对过度概括（RO）问题。早期研究表明，乐观主义可以缓解使用表格化Q学习时的RO问题。然而，对于复杂任务来说，利用函数逼近乐观主义可能加剧过估计，从而失败。另一方面，最近的深度多智能体策略梯度（MAPG）方法在许多复杂任务上取得了成功，但在严重的RO情况下可能失败。我们提出了一个通用而简单的框架，以在MAPG方法中实现乐观更新并缓解RO问题。具体而言，我们使用一个泄漏化线性整流函数，其中一个超参数选择乐观程度以在更新策略时重新塑造优势。直观地说，我们的方法对可能由其他代理引起的回报较低的个别动作保持乐观态度。

    \textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal be
    
[^99]: Castor: 因果时序区域结构学习

    Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])

    [http://arxiv.org/abs/2311.01412](http://arxiv.org/abs/2311.01412)

    “Castor”是一个用于学习多元时间序列数据中因果关系的框架，能够综合学习各个区域的因果图。它通过最大化得分函数来推断区域的数量，并学习每个区域中的线性或非线性因果关系。

    

    揭示多元时间序列数据之间的因果关系是一个重要且具有挑战性的目标，涉及到从气候科学到医疗保健等各个学科的广泛范围。这些数据包含线性或非线性关系，并且通常遵循多个先验未知的区域。现有的因果发现方法可以从具有已知区域的异构数据中推断出摘要因果图，但在全面学习区域和相应的因果图方面存在不足。在本文中，我们介绍了CASTOR，这是一个新颖的框架，旨在学习由不同因果图统治的各种异构时间序列数据中的因果关系。通过EM算法通过最大化一个得分函数，CASTOR推断出区域的数量并学习每个区域中的线性或非线性因果关系。我们展示了CASTOR的稳健收敛性质，特别突出了其有效性。

    The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
    
[^100]: 表达建模对于离线强化学习不足：可处理的推理角度

    Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])

    [http://arxiv.org/abs/2311.00094](http://arxiv.org/abs/2311.00094)

    本文指出，在离线强化学习任务中，除了表达性强的序列模型，可处理性也起着重要的作用。由于离线数据收集策略和环境动态的随机性，需要精确且高效地回答各种概率查询，以找到有奖励的动作。基于此，本文提出了Trifle（离线强化学习的可处理推理）方法，利用现代可处理概率模型来解决这个问题。

    

    离线强化学习任务中，一种流行的范例是先将离线轨迹拟合到一个序列模型中，然后通过该模型提示高期望回报的动作。虽然普遍认为表达性更强的序列模型可以带来更好的性能，但本文强调了可处理性，即精确而高效地回答各种概率查询的能力，同样起着重要的作用。具体而言，由于离线数据收集策略和环境动态带来的基本随机性，需要进行高度非平凡的条件/约束生成，以引出有奖励的动作。虽然仍然可以近似处理这些查询，但我们观察到这种粗糙的估计显著削弱了表达性强的序列模型带来的好处。为了解决这个问题，本文提出了Trifle（离线强化学习的可处理推理），它利用了现代可处理概率模型（TPM）来弥合这个差距。

    A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
    
[^101]: 沃瑟斯坦梯度流在变分推断的变分参数空间上的应用

    Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference. (arXiv:2310.16705v1 [cs.LG])

    [http://arxiv.org/abs/2310.16705](http://arxiv.org/abs/2310.16705)

    本文将变分推断重新框架为在变分参数空间上的概率分布优化问题，提出了沃瑟斯坦梯度下降方法来解决优化问题，有效性经过实证实验证实。

    

    变分推断可以被看作是一个优化问题，其中变分参数被调整以使变分分布与真实后验尽可能接近。可以通过黑箱变分推断中的普通梯度下降或自然梯度变分推断中的自然梯度下降来解决优化任务。在本文中，我们将变分推断重新框架为在一个“变分参数空间”中定义的概率分布的目标优化问题。随后，我们提出了沃瑟斯坦梯度下降方法来解决这个优化问题。值得注意的是，这些优化技术，即黑箱变分推断和自然梯度变分推断，可以重新解释为所提出的沃瑟斯坦梯度下降的特定实例。为了提高优化效率，我们开发了实用的方法来数值求解离散梯度流。通过在一个合成数据集上的实证实验，我们验证了所提出方法的有效性。

    Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theore
    
[^102]: FLTrojan: 通过选择性权重篡改对联邦语言模型进行隐私泄露攻击

    FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])

    [http://arxiv.org/abs/2310.16152](http://arxiv.org/abs/2310.16152)

    本文提出了一种FLTrojan攻击方法，通过选择性权重篡改，从联邦语言模型中泄露隐私敏感用户数据。通过观察到FL中中间轮次的模型快照可以引起更大的隐私泄露，并发现隐私泄露可以通过篡改模型的选择性权重来加剧。

    

    联邦学习(Federated learning, FL)正成为许多技术应用中的关键组件，包括语言建模领域，其中个体FL参与者在其本地数据集中往往具有敏感的文本数据。然而，确定联邦语言模型中的隐私泄露程度并不简单，现有的攻击只是试图提取数据，而不考虑数据的敏感性或天真性。为了填补这一空白，在本文中，我们介绍了关于从联邦语言模型中泄露隐私敏感用户数据的两个新发现。首先，我们观察到FL中中间轮次的模型快照比最终训练模型能够造成更大的隐私泄露。其次，我们确定隐私泄露可以通过篡改模型的选择性权重来加剧，这些权重特别负责记忆敏感训练数据。我们展示了恶意客户端如何在FL中泄露其他用户的隐私敏感数据。

    Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
    
[^103]: 一图抵千言：使用多概念提示学习来学习对象级概念

    An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])

    [http://arxiv.org/abs/2310.12274](http://arxiv.org/abs/2310.12274)

    提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。

    

    文字反转是一种提示学习方法，它学习一种新的“单词”的嵌入表示图像风格和外观，使其能够整合到自然语言句子中生成新的合成图像。然而，即使对于可获得个别概念的嵌入，识别和整合一个场景中的多个对象级概念仍然面临着显著的挑战，这也得到了我们的实证测试的进一步证实。为了解决这个挑战，我们引入了一个多概念提示学习（MCPL）的框架，可以从一个句子-图像对中同时学习多个新的“词”。为了增强词概念相关性的准确性，我们提出了三种正则化技术：注意力掩码（AttnMask）将学习集中在相关区域；提示对比损失（PromptCL）将不同概念的嵌入分离开来；以及绑定形容词（Bind adj.）将新的“词”与已知词相关联。我们通过图像生成进行评估

    Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
    
[^104]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^105]: 使用扩散模型合成未见过的图像

    Unseen Image Synthesis with Diffusion Models. (arXiv:2310.09213v1 [cs.LG])

    [http://arxiv.org/abs/2310.09213](http://arxiv.org/abs/2310.09213)

    本论文提出使用预训练的扩散模型在未见过的领域合成图像的方法，并理论上和经验上证明了这种方法的有效性。

    

    当前生成领域的趋势是通过扩大模型规模和增加训练数据来实现通用领域表示，而我们在这项工作中选择相反的方向，通过使用预训练和冻结的去噪扩散概率模型（DDPMs）在单领域数据集上进行潜在采样和几何优化来合成未见过的领域图像。我们的关键观察是，即使是仅在单领域图像上进行预训练的DDPMs已经具备了足够的表示能力，可以通过反转潜在编码，并经过双向确定性扩散和去噪轨迹重构任意图像。这促使我们研究未见过图像领域中的潜在空间中沿去噪链的OOD样本的统计和几何行为。值得注意的是，我们在理论上和经验上都表明，反转的OOD样本也建立了高斯分布。

    While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are 
    
[^106]: 以Transformer为决策者：通过监督预训练实现可证明的上下文强化学习

    Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])

    [http://arxiv.org/abs/2310.08566](http://arxiv.org/abs/2310.08566)

    通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。

    

    在离线强化学习数据集上预训练的大型Transformer模型展示了令人惊叹的上下文强化学习能力，即当它们面对来自未知环境的交互轨迹时，它们能够做出良好的决策。然而，Transformer模型如何进行训练以执行上下文强化学习，在理论上尚未得到很好的理解。特别是，尚不清楚Transformer模型可以在上下文中执行哪些强化学习算法以及离线训练数据中的分布差异如何影响已学习的算法。本文提供了一个理论框架，分析了对上下文强化学习的监督预训练。这包括了两种最近提出的训练方法：算法蒸馏和决策预训练的Transformer模型。首先，在假设模型可实现的情况下，我们证明了经过监督预训练的Transformer模型将模仿专家算法在观察到的轨迹上的条件期望。广义误差的缩放范围与…

    Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
    
[^107]: 只需少量上下文示范即可实现越狱和对齐的语言模型

    Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])

    [http://arxiv.org/abs/2310.06387](http://arxiv.org/abs/2310.06387)

    本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。

    

    大规模语言模型（LLM）在各种任务中取得了显著的成功，但对其安全性和生成恶意内容的潜在风险的担忧也浮现出来。本文探讨了在相同上下文学习（ICL）中操纵LLM对齐能力的效果。我们发现，仅通过少量的上下文示范而无需微调，就可以操纵LLM增加或降低越狱概率，即回答恶意提示。基于这些观察，我们提出了用于越狱和对齐语言模型目的的相同上下文攻击（ICA）和相同上下文防御（ICD）方法。ICA通过构造恶意上下文指导模型生成有害输出，而ICD通过拒绝回答有害提示的示范来增强模型的稳健性。我们的实验证明了ICA和ICD在增加或降低对抗越狱攻击成功率方面的有效性。总的来说，我们揭示了ICL在越狱和对齐语言模型领域的潜力。

    Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
    
[^108]: 联邦学习：最新进展和应用的前沿综述

    Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications. (arXiv:2310.05269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05269](http://arxiv.org/abs/2310.05269)

    联邦学习是一种安全分布式机器学习方法，通过整合云基础设施和区块链技术，实现了隐私安全和经济有效的通信。它通过本地训练模型并将结果上传到云端进行整体模型聚合，避免了直接暴露原始数据，具有高效、可伸缩和保护隐私的优势。

    

    在具有客户和主机连接的机器学习（ML）系统领域，通过联邦学习（FL）作为安全分布式ML方法，可以有效地提高隐私安全性。FL通过使用区块链技术将ML模型传输到边缘服务器上，有效地将云基础设施与中心化和分散化系统的流程处理和数据存储需求相结合，注重可伸缩性、隐私考虑和经济高效的通信。在当前的FL实现中，数据所有者在本地训练模型，然后以权重、梯度和参数的形式将结果上传到云端进行整体模型聚合。这种创新消除了与物联网（IoT）客户和参与者直接与云中心通信原始和潜在机密数据的必要性。这不仅降低了与与传统云中心通信相关的费用，还保护了用户的隐私。

    In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with 
    
[^109]: GRAPES: 学习用于可扩展图神经网络的图采样

    GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])

    [http://arxiv.org/abs/2310.03399](http://arxiv.org/abs/2310.03399)

    GRAPES是一种自适应图采样方法，通过学习识别在训练图神经网络分类器时具有影响力的节点集合，解决了可扩展图神经网络中的内存问题，并且在准确性和可扩展性方面表现出色。

    

    图神经网络（GNNs）通过以不同方式聚合周围信息来学习图中节点的表示。随着这些网络的加深，由于邻域尺寸的增加，它们的感受野呈指数增长，导致高内存消耗。图采样通过对图中节点进行抽样来解决GNNs中的内存问题。通过这种方式，GNNs可以扩展到更大的图。大多数采样方法专注于固定的采样启发式算法，这可能无法推广到不同的结构或任务。我们引入了GRAPES，一种自适应的图采样方法，该方法学习识别用于训练GNN分类器的一组具有影响力的节点。GRAPES使用GFlowNet来学习给定分类目标的节点采样概率。我们在几个小规模和大规模图基准上评估了GRAPES，并展示了其在准确性和可扩展性方面的有效性。与现有的采样方法相比，GRAPES即使在采样比例较低的情况下仍保持高准确性。

    Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
    
[^110]: 基于能量导向的连续熵巴氏中心估计方法及其在一般成本问题中的应用

    Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])

    [http://arxiv.org/abs/2310.01105](http://arxiv.org/abs/2310.01105)

    本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。

    

    优化输运（OT）巴氏中心是一种在捕捉概率分布几何特性的同时对其进行平均的数学方法。本文提出了一种新颖的算法，用于近似计算任意OT成本函数的连续熵OT巴氏中心。我们的方法基于最近在机器学习社区中受到关注的基于弱OT的连续熵最优输运问题的对偶重构。除了创新性之外，我们的方法还具有以下若干优势特点：（i）我们建立了对恢复解的质量界限；（ii）该方法与基于能量的模型（EBMs）学习过程无缝连接，可以使用经过良好调整的算法解决感兴趣的问题；（iii）它提供了一种直观的优化方案，避免使用极小-极大、强化等复杂技巧。为了验证我们的方法，我们考虑了s

    Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
    
[^111]: 使用合成数据进行预训练有助于离线强化学习

    Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00771](http://arxiv.org/abs/2310.00771)

    本文研究表明，在离线深度强化学习中，使用合成数据进行预训练可以提高性能，而不一定需要语言预训练。此外，使用一步马尔科夫链生成的数据进行预训练可进一步改善性能。在一个流行的离线DRL算法中，使用简单的预训练方案也能获得性能提升。

    

    最近的研究表明，对于离线深度强化学习(DRL)，使用大型语言语料库预训练Decision Transformer可以提高下游性能。一个自然的问题是，这种性能提升是否只能通过语言预训练实现，还是可以通过不涉及语言的更简单的预训练方案实现。在本文中，我们首先证明了语言对于改善性能并不是必要的，实际上，使用合成的IID数据进行少量更新的预训练可以达到与使用大型语言语料库预训练相匹配的性能提升；此外，使用一步马尔科夫链生成的数据进行预训练可以进一步提高性能。受到这些实验结果的启发，我们进一步考虑了预训练Conservative Q-Learning(CQL)，这是一种流行的离线DRL算法，它基于Q-learning，并通常使用多层感知器(MLP)骨干。令人惊讶的是，使用简单的预训练方案也能在CQL算法中取得性能提升。

    Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
    
[^112]: 机器学习ADE Coxeter元素的Clifford不变量

    Machine Learning Clifford invariants of ADE Coxeter elements. (arXiv:2310.00041v1 [cs.LG])

    [http://arxiv.org/abs/2310.00041](http://arxiv.org/abs/2310.00041)

    本文研究了在线性变换中的Clifford几何不变量的机器学习方法，并实现了对ADE Coxeter元素的计算和数据挖掘。我们发现，不变量的输出完全取决于简单根的选择和对应反射的排列顺序，存在巨大的退化性。

    

    最近在线性变换的Clifford几何不变量上引起了广泛关注。这引发了对一类在根系、反射群、李群和李代数的背景中感兴趣的几何变换的不变量的研究: Coxeter变换。我们对$A_8$、$D_8$和$E_8$的所有Coxeter变换进行了详尽的计算，并使用高性能计算机计算了它们的不变量。这种计算代数的范式生成了一个数据集，可以利用数据科学的技术，如监督和无监督机器学习来挖掘。本文重点研究神经网络分类和主成分分析。由于输出 - 不变量 - 完全由简单根的选择和Coxeter元素中对应反射的排列顺序决定，我们期望在映射中存在巨大的退化性。

    There has been recent interest in novel Clifford geometric invariants of linear transformations. This motivates the investigation of such invariants for a certain type of geometric transformation of interest in the context of root systems, reflection groups, Lie groups and Lie algebras: the Coxeter transformations. We perform exhaustive calculations of all Coxeter transformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple roots and compute their invariants, using high-performance computing. This computational algebra paradigm generates a dataset that can then be mined using techniques from data science such as supervised and unsupervised machine learning. In this paper we focus on neural network classification and principal component analysis. Since the output -- the invariants -- is fully determined by the choice of simple roots and the permutation order of the corresponding reflections in the Coxeter element, we expect huge degeneracy in the mapping. This provides the
    
[^113]: 控制组合优化的连续放松

    Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])

    [http://arxiv.org/abs/2309.16965](http://arxiv.org/abs/2309.16965)

    本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。

    

    最近在组合优化（CO）问题中，图神经网络（GNNs）显示出巨大潜力。通过无监督学习找到近似解的受物理启发的GNN（PI-GNN）求解器在大规模CO问题上引起了极大关注。然而，对于相对密集图上的CO问题，贪婪算法的性能恶化，但对于PI-GNN求解器的性能却没有太多讨论。此外，由于PI-GNN求解器采用了放松策略，学习后需要从连续空间人工转换回原始离散空间，可能会破坏解的鲁棒性。本文通过数值实验证明了PI-GNN求解器在密集图上的CO问题的学习早期可能陷入局部解的情况，其中所有变量都为零。然后，我们通过控制连续性和离散性来解决这些问题。

    Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
    
[^114]: Transductive Learning的尖锐泛化：一种Transductive Local Rademacher Complexity方法

    Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])

    [http://arxiv.org/abs/2309.16858](http://arxiv.org/abs/2309.16858)

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。

    

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们的工作将传统的local rademacher complexity (LRC)的思想扩展到了transductive设置中，相对于典型的LRC方法在归纳设置中的分析有了相当大的变化。我们提出了一种基于Rademacher complex的局部化工具，可以应用于各种transductive learning问题，并在适当条件下得到了尖锐的界限。与LRC的发展类似，我们通过从独立变量的方差信息开始构建TLRC，将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。经过精心设计的...

    We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
    
[^115]: 对抗样本可能是可以避免的：数据集中性在对抗鲁棒性中的作用

    Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])

    [http://arxiv.org/abs/2309.16096](http://arxiv.org/abs/2309.16096)

    本研究论证了数据分布的集中程度对于决定鲁棒分类器的存在与否至关重要，并展示了在数据分布集中在低维线性子空间的并集时，利用数据结构可以获得具有良好鲁棒性保证的分类器的方法。

    

    现代机器学习分类器对于对抗样本的敏感性引发了理论结果，暗示这些对抗样本可能是不可避免的。然而，这些结果可能过于一般化，无法应用于自然数据分布。事实上，人类在涉及视觉的任务中表现出相当的鲁棒性。这种明显的矛盾推动我们更深入地探索一个问题：对抗样本是否真的是不可避免的？在这项工作中，我们理论上证明了数据分布的一个关键属性——对输入空间的小容积子集的集中程度，决定了是否存在一个鲁棒分类器。我们进一步证明，在数据分布集中在低维线性子空间的并集时，利用数据结构自然地得到享有良好鲁棒性保证的分类器，改进了在特定范围内可证明认证方法。

    The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
    
[^116]: 最大扩散强化学习

    Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])

    [http://arxiv.org/abs/2309.15293](http://arxiv.org/abs/2309.15293)

    最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。

    

    所有机器学习都建立在数据独立且同分布的假设上。然而，在强化学习中，当数据是依次从代理经验中收集而来时，这一假设通常不成立。因此，我们提出了一种名为最大扩散强化学习的方法，利用统计力学中的遍历过程来克服这些限制。我们的方法通过解耦代理的经验，可证明地使代理在单次部署中能够持续学习，而不受初始化方式的影响。此外，我们证明了我们的方法推广了众所周知的最大熵技术，并且通过在流行的基准测试中稳定超过了最先进的性能水平。我们的研究成果极大地促进了物理学、学习和控制的交叉领域，为强化学习代理（如行走机器人和自动驾驶汽车）的透明可靠决策提供了一条道路。

    The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
    
[^117]: 眼不见心不念：利用视频跟踪启用的记忆模型对未被观察到的对象进行推理和规划

    Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])

    [http://arxiv.org/abs/2309.15278](http://arxiv.org/abs/2309.15278)

    本文研究了如何对先前观察到但当前被遮挡的对象进行推理和规划，提出了利用转换器关系动力学编码轨迹历史的方法，并在多个挑战性任务中表现出色。

    

    机器人需要具有对先前观察到但当前被遮挡的对象的记忆，以在现实环境中可靠地工作。我们研究了将面向对象的记忆编码到多对象操纵推理和规划框架中的问题。我们提出了DOOM和LOOM，它们利用转换器关系动力学来编码给定部分视点云和对象发现与跟踪引擎的轨迹历史。我们的方法可以执行多个具有挑战性的任务，包括处理被遮挡的对象，新出现的对象，以及物体重新出现。通过广泛的仿真和真实世界实验，我们发现我们的方法在不同数量的对象和不同数量的干扰动作方面表现良好。此外，我们展示了我们的方法优于隐式记忆基线。

    Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
    
[^118]: 有关富标签在主动学习中的证据不确定性

    Evidential uncertainties on rich labels for active learning. (arXiv:2309.12494v1 [cs.LG])

    [http://arxiv.org/abs/2309.12494](http://arxiv.org/abs/2309.12494)

    本文提出了两种策略来应对主动学习中的不确定性问题，即采用Klir不确定性采样和证据学派不确定性采样，在考虑到标签中已存在的不确定性的基础上，对模型的不确定性进行分解和处理。

    

    最近关于主动学习的研究，尤其是关于不确定采样的研究，主要集中在将模型不确定性分解为可降低和不可降低的不确定性上。在本文中，我们提出简化计算阶段，并消除对观察结果的依赖，更重要的是考虑标签中已经存在的不确定性，即答题者的不确定性。我们提出了两种策略，Klir不确定性采样和证据学派不确定性采样，这两种方法都使用了信任函数理论来解决探索-利用问题。

    Recent research in active learning, and more precisely in uncertainty sampling, has focused on the decomposition of model uncertainty into reducible and irreducible uncertainties. In this paper, we propose to simplify the computational phase and remove the dependence on observations, but more importantly to take into account the uncertainty already present in the labels, \emph{i.e.} the uncertainty of the oracles. Two strategies are proposed, sampling by Klir uncertainty, which addresses the exploration-exploitation problem, and sampling by evidential epistemic uncertainty, which extends the reducible uncertainty to the evidential framework, both using the theory of belief functions.
    
[^119]: 翻转诅咒: 在大型语言模型中训练的"A是B"无法学习"B是A"

    The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])

    [http://arxiv.org/abs/2309.12288](http://arxiv.org/abs/2309.12288)

    LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。

    

    我们揭示了自回归大型语言模型（LLM）在泛化上的令人惊讶的失败。如果一个模型是基于"A是B"形式的句子进行训练，它不会自动推广到相反的方向"B是A"。这就是翻转诅咒。例如，如果一个模型是基于"Olaf Scholz是德国第九任总理"进行训练的，它不会自动能够回答问题"谁是德国第九任总理？"。此外，正确答案（"Olaf Scholz"）的可能性不会比随机名字更高。因此，模型在逻辑推断上存在基本失败，并且不会推广到它们训练集中的普遍模式（即如果出现"A是B"，则"B是A"更可能出现）。我们通过在虚构的陈述（如"Uriah Hawthorne是'Abyssal Melodies'的作曲家"）上对GPT-3和Llama-1进行微调，并展示它们无法正确回答"谁创作了'Abyssal Melodies'?"来提供翻转诅咒的证据。

    We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
    
[^120]: Text2Reward：针对强化学习的自动生成密集奖励函数的自动化框架

    Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])

    [http://arxiv.org/abs/2309.11489](http://arxiv.org/abs/2309.11489)

    Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。

    

    设计奖励函数是强化学习中长期以来的挑战；它需要专业知识或领域数据，导致开发成本高。为了解决这个问题，我们引入了Text2Reward，一个无需数据的框架，可基于大型语言模型（LLM）自动生成密集奖励函数。给定自然语言描述的目标，Text2Reward生成作为环境紧凑表示的可执行程序的密集奖励函数。与逆强化学习和最近使用LLM编写稀疏奖励代码的工作不同，Text2Reward生成可解释的、自由形式的密集奖励代码，可涵盖各种任务，利用现有软件包，并允许通过人类反馈进行迭代改进。我们在两个机器人操作基准（ManiSkill2，MetaWorld）和两个MuJoCo的运动环境上评估了Text2Reward。在17个操作任务中的13个任务中，使用生成的奖励代码训练的政策实现了类似或更好的性能。

    Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
    
[^121]: 一个基于神经特征学习的几何框架

    A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])

    [http://arxiv.org/abs/2309.10140](http://arxiv.org/abs/2309.10140)

    本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。

    

    我们提出了一个基于神经特征提取器的学习系统设计的新框架，通过利用特征空间中的几何结构。首先，我们引入了特征几何，它将统计依赖和特征统一到同一个具有几何结构的函数空间中。通过应用特征几何，我们将每个学习问题形式化为解决由学习设置指定的依赖组件的最佳特征近似解。我们提出了一种嵌套技术来设计学习算法，从数据样本中学习最佳特征，这可以应用于现有的网络架构和优化器。为了展示嵌套技术的应用，我们进一步讨论了多变量学习问题，包括条件推理和多模态学习，在这些问题中，我们提出了最佳特征并揭示了它们与经典方法的联系。

    We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
    
[^122]: 从头开始利用基于扩散的数据增强方法来提升无监督对比学习

    Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])

    [http://arxiv.org/abs/2309.07909](http://arxiv.org/abs/2309.07909)

    本文介绍了一种新颖且高效的基于扩散的数据增强技术DiffAug，通过扩散步骤实现增强数据和原始数据共享平滑的潜空间，旨在提高无监督对比学习的效果和效率。

    

    无监督对比学习方法近年来取得了显著的改进，尤其是通过数据增强策略产生稳健且具有泛化能力的表示。然而，现有的数据增强方法往往过于依赖先验知识或外部数据，限制了它们的有效性和效率。此外，大多数现有的数据增强策略在转换到其他研究领域时应用受限，特别是在科学相关数据的情况下。这种限制来源于这些领域中先验知识和标记数据的匮乏。为了解决这些问题，我们引入了一种新颖而高效的基于扩散的数据增强技术——DiffAug。DiffAug通过扩散步骤确保增强数据和原始数据共享一个平滑的潜空间。与传统方法不同，DiffAug首先对数据进行增强，使增强数据和原始数据在潜空间上平滑。

    Unsupervised contrastive learning methods have recently seen significant improvements, particularly through data augmentation strategies that aim to produce robust and generalizable representations. However, prevailing data augmentation methods, whether hand designed or based on foundation models, tend to rely heavily on prior knowledge or external data. This dependence often compromises their effectiveness and efficiency. Furthermore, the applicability of most existing data augmentation strategies is limited when transitioning to other research domains, especially science-related data. This limitation stems from the paucity of prior knowledge and labeled data available in these domains. To address these challenges, we introduce DiffAug-a novel and efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure that the augmented and original data share a smoothed latent space, which is achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug first 
    
[^123]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^124]: 相等的长期效益率：将静态公平性概念应用到顺序决策中

    Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])

    [http://arxiv.org/abs/2309.03426](http://arxiv.org/abs/2309.03426)

    这篇论文介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。

    

    机器学习模型的决策可能对时间产生长期影响，因此长期公平性成为一个重要考虑因素。先前的研究表明，在忽略长期影响时，简单地应用静态公平性准则实际上会加剧偏见。为了明确解决顺序决策中的偏见，最近的研究在马尔可夫决策过程（MDP）框架中制定了长期公平性概念。他们将长期偏见定义为每个时间步上静态偏见的总和。然而，我们证明了简单地对逐步偏见求和可能导致虚假的公平感，因为它未考虑到转换过程中不同时间步的重要性差异。在这项工作中，我们介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，它明确考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。此外，我们还展示了长期收益的策略梯度的性质。

    Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi
    
[^125]: 回归问题的校准解释

    Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])

    [http://arxiv.org/abs/2308.16245](http://arxiv.org/abs/2308.16245)

    本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。

    

    人工智能（AI）通常是现代决策支持系统（DSS）的一部分。在基于AI的DSS中使用的最佳预测模型缺乏透明度。可解释的人工智能（XAI）旨在创建能够向人类用户解释其理由的AI系统。XAI中的局部解释可以提供关于个别预测的原因的信息，即特征重要性。然而，现有局部解释方法的一个关键缺点是无法量化与特征重要性相关的不确定性。本文介绍了特征重要性解释方法Calibrated Explanations（CE）的扩展，之前只支持分类，现在支持标准回归和概率回归，即目标超过任意阈值的概率。回归问题的扩展保留了CE的所有优点，例如将底层模型的预测与置信度校准。

    Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
    
[^126]: IPA：推理管道自适应以实现高准确性和成本效益

    IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])

    [http://arxiv.org/abs/2308.12871](http://arxiv.org/abs/2308.12871)

    提出了一种名为IPA的在线深度学习推理管道自适应系统，通过动态配置批处理大小、复制和模型变体，以优化准确性、最小化成本并满足用户定义的延迟要求。

    

    在机器学习生产系统中，高效地优化多模型推理管道以实现快速、准确和成本效益的推理是一个关键挑战，考虑到它们对端到端延迟的严格要求。为了简化准确性和成本之间广阔而复杂的权衡空间的探索，提供者通常选择考虑其中之一。然而，挑战在于协调准确性和成本的权衡。为了应对这一挑战并提出一种有效管理推理管道中模型变体的解决方案，我们提出了IPA，一种在线深度学习推理管道自适应系统，它能够有效地利用每个深度学习任务的模型变体。模型变体是同一深度学习任务的预训练模型的不同版本，其资源需求、延迟和准确性有所不同。IPA通过动态配置批处理大小、复制和模型变体来优化准确性、最小化成本并满足用户定义的延迟SLA。

    Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs
    
[^127]: 基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法

    ${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])

    [http://arxiv.org/abs/2308.11842](http://arxiv.org/abs/2308.11842)

    本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。

    

    在自然界中识别和分析对称模式已经在各个科学领域取得了重要的发现，例如物理学中的引力定律的制定和化学结构研究的进展。本文着重于利用在某些协作多智体强化学习（MARL）问题中固有的欧几里德对称性，以及在许多应用中普遍存在的对称性。我们首先通过形式化地描述一类具有一般对称性概念的马尔可夫博弈，该概念允许存在对称的最优值和策略。受到这些性质的启发，我们设计了具有对称约束的神经网络结构，作为多智体演员-评论家方法的归纳偏差。这种归纳偏差在各种协作MARL基准测试中表现出卓越的性能，并具有零样本学习和在具有重复对称模式的未见场景中的迁移学习等令人印象深刻的泛化能力。

    Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
    
[^128]: 使用层次结构距离捕捉多层次图结构的变压器

    Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])

    [http://arxiv.org/abs/2308.11129](http://arxiv.org/abs/2308.11129)

    本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。

    

    图变压器需要强大的归纳偏差来得出有意义的注意力分数。然而，当前的提议很少涉及捕捉更长距离、层次结构或社区结构的方法，而这些在分子、社交网络和引用网络等各种图形中都会出现。在本文中，我们提出了一种层次距离结构编码（HDSE）方法，用于建模图中节点之间的层次距离，重点关注其多层次、层次化的性质。特别是，这产生了一个可以灵活与现有图变压器集成的框架，可以与其他位置表示同时应用。通过在12个真实世界数据集上进行大量实验，我们证明了我们的HDSE方法成功提升了各种类型的基线变压器，在10个基准数据集上获得了最先进的实证性能。

    Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
    
[^129]: 从稀疏到软性混合专家模型

    From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])

    [http://arxiv.org/abs/2308.00951](http://arxiv.org/abs/2308.00951)

    本文提出了一种Soft MoE模型，它是一种稀疏的、完全可微分的Transformer，通过隐式的软分配和只处理部分标记的方式解决了稀疏模型的训练不稳定性和推理成本高的问题，并在视觉识别任务中取得了比标准Transformer和其他MoE变体更好的性能。

    

    稀疏的专家模型(MoEs)可以在不增加训练或推理成本的情况下扩展模型容量。尽管它们取得了成功，但MoEs存在一些问题：训练不稳定、丢失标记、无法扩展专家数量或无效的微调。在这项工作中，我们提出了Soft MoE，一种完全可微分的稀疏Transformer，解决了这些挑战，并保持了MoEs的优点。Soft MoE通过向每个专家传递所有输入标记的不同加权组合来执行隐式的软分配。与其他MoE作品一样，Soft MoE中的专家只处理一部分（组合的）标记，以在较低的推理成本下实现更大的模型容量。在视觉识别方面，Soft MoE在标准Transformer（ViTs）和流行的MoE变体（Tokens Choice和Experts Choice）中表现出非常好的性能。例如，Soft MoE-Base/16的推理成本比ViT-Huge/14低10.5倍（墙钟时间降低了5.7倍），同时与其性能相当。

    Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its p
    
[^130]: 使用基于贝叶斯安全策略学习的机会约束优化方法：在越南战争期间的军事安全评估中的应用

    Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])

    [http://arxiv.org/abs/2307.08840](http://arxiv.org/abs/2307.08840)

    本研究提出了使用基于贝叶斯安全策略学习的机会约束优化方法，在越南战争期间的军事安全评估中进行实证研究。研究结果对于解决高风险算法决策中的挑战具有重要意义。

    

    在高风险决策场景，如刑事司法、医学和公共政策中，常常使用算法和数据驱动的决策和建议。本研究探讨了在越南战争期间，是否有可能改进一种安全评估算法，并使用在其引入后立即测量到的结果进行研究。这个实证应用提出了在高风险算法决策中经常遇到的几个方法学挑战。首先，在实施新算法之前，至关重要的是对更糟糕的结果风险进行表征和控制。其次，现有算法是确定性的，学习新算法需要透明的外推。第三，现有算法涉及常见但难以优化的离散决策表。为了应对这些挑战，我们引入了平均条件风险（ACRisk），首先量化了产生较差结果风险的方法。

    Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk th
    
[^131]: STG-MTL: 使用数据映射的可伸缩任务分组多任务学习

    STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map. (arXiv:2307.03374v1 [cs.LG])

    [http://arxiv.org/abs/2307.03374](http://arxiv.org/abs/2307.03374)

    本文提出了一种数据驱动方法，使用数据映射来解决多任务学习中的任务分组问题。这种方法具有可伸缩性和模块化，并且在大量任务下仍然有效。

    

    多任务学习是一种强大的技术，因其相对于传统的单任务学习而言具有性能改进而受到欢迎。然而，多任务学习往往具有挑战性，因为可能的任务分组数量呈指数增长，这使得选择最佳任务分组变得困难，并且一些分组可能会由于任务之间的负面干扰而导致性能下降。此外，现有解决方案严重受到可伸缩性问题的困扰，限制了实际应用。在本文中，我们提出了一种新的数据驱动方法，解决了这些挑战，并提供了一种基于手工特征，特别是数据映射的分类任务分组的可伸缩和模块化解决方案。我们的实验证明了该方法的有效性，即使在前所未有的任务数量下（高达100个）。

    Multi-Task Learning (MTL) is a powerful technique that has gained popularity due to its performance improvement over traditional Single-Task Learning (STL). However, MTL is often challenging because there is an exponential number of possible task groupings, which can make it difficult to choose the best one, and some groupings might produce performance degradation due to negative interference between tasks. Furthermore, existing solutions are severely suffering from scalability issues, limiting any practical application. In our paper, we propose a new data-driven method that addresses these challenges and provides a scalable and modular solution for classification task grouping based on hand-crafted features, specifically Data Maps, which capture the training behavior for each classification task during the MTL training. We experiment with the method demonstrating its effectiveness, even on an unprecedented number of tasks (up to 100).
    
[^132]: 在推荐系统中插值项目和用户公平性

    Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])

    [http://arxiv.org/abs/2306.10050](http://arxiv.org/abs/2306.10050)

    本文研究在推荐系统中平衡项目和用户公平性的框架，并通过低后悔的在线优化算法实现了维持收益同时实现公平推荐的目标。

    

    在多边平台中，平台与卖家（项目）和客户（用户）等各种各样的利益相关者互动，每个相关者都有自己的期望结果，寻找合适的平衡点变得非常复杂。在这项工作中，我们研究了“公平成本”，它捕捉了平台在平衡不同利益相关者利益时可能做出的妥协。出于这个目的，我们提出了一个公平推荐框架，其中平台在插值项目和用户公平性约束时最大化其收益。我们在一个更现实但具有挑战性的在线设置中进一步研究了公平推荐问题，在这种情况下，平台缺乏了解用户偏好的知识，只能观察二进制购买决策。为了解决这个问题，我们设计了一种低后悔的在线优化算法，它在维护平台收益的同时管理项目和用户公平性之间的权衡。我们的实验证明了我们提出的框架在实现公平推荐同时保持高收益方面的有效性。

    Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
    
[^133]: 应对持续任务强化学习中的无界状态空间

    Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning. (arXiv:2306.01896v1 [cs.LG])

    [http://arxiv.org/abs/2306.01896](http://arxiv.org/abs/2306.01896)

    本文提出了一种基于李雅普诺夫思想的奖励塑形方法，用于在持续任务的强化学习中应对无界状态空间，旨在鼓励代理器学习稳定性和最优策略。

    

    尽管深度强化学习（RL）算法已成功应用于许多任务，但它们无法外推且强烈依赖周期性重置，这限制了它们在许多现实世界设置中的适用性。针对这个问题，本文提出了一种基于李雅普诺夫思想的奖励塑形方法，以鼓励代理首先学习稳定性（即实现有界成本），然后再学习最优策略。理论上证明了奖励塑形技术减少了代理器的发散率，并通过实验进一步证实了这一点。

    While deep reinforcement learning (RL) algorithms have been successfully applied to many tasks, their inability to extrapolate and strong reliance on episodic resets inhibits their applicability to many real-world settings. For instance, in stochastic queueing problems, the state space can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover due to the lack of resets, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach 
    
[^134]: 平滑单调网络

    Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])

    [http://arxiv.org/abs/2306.01147](http://arxiv.org/abs/2306.01147)

    本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。

    

    单调性约束是统计建模中的强力正则化工具。它们可以在计算机支持的决策制定中支持公平性，并增加数据驱动科学模型的可信度。经典的min-max(MM)神经网络结构确保了单调性，但由于梯度消失而往往在训练过程中陷入不良局部最优。我们提出了对MM网络的简单修改，使用严格递增的平滑非线性函数来缓解这个问题。得到的平滑min-max(SMM)网络模块继承了MM架构的渐近逼近性质。它可以嵌入到更大的端到端深度学习系统中进行训练。在单调建模的神经网络方面，SMM模块要简单得多，计算需求也要少得多。尽管如此，在我们的实验中，它在泛化性能方面与替代神经和非神经方法相比表现得更为优异。

    Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
    
[^135]: 学习解决贝叶斯逆问题：一种摊销变分推理方法

    Learning to solve Bayesian inverse problems: An amortized variational inference approach. (arXiv:2305.20004v1 [stat.ML])

    [http://arxiv.org/abs/2305.20004](http://arxiv.org/abs/2305.20004)

    本文提出了一种基于深度神经网络的参数化表示后验分布的摊销变分推理方法，以实现实时推理的目的，可应用于流体力学中的参数估计和流场重构等领域。

    

    逆问题，即从实验数据中估计物理模型的参数，在科学和工程中普遍存在。贝叶斯公式是黄金标准，因为它可以缓解病态性问题并量化认识不确定性。然而，由于解析后验不通常可用，人们采用马尔可夫链蒙特卡罗采样或近似变分推理。但是，需要重新从头开始进行推理以适应每组新数据。这种缺点限制了贝叶斯公式在实时设置，例如工程系统的健康监测和医疗诊断中的适用性。本文的目标是开发一种方法，通过学习从数据到后验的贝叶斯逆映射，即从数据到后验的映射，实现实时推理的方法。我们的方法如下。我们使用基于深度神经网络的参数化表示后验分布。接下来，我们通过摊销变分推理学习网络参数，其中训练神经网络以预测从数据中预测后验分布，从而实现快速准确的推理。我们在流体力学中的一些逆问题上展示了我们方法的有效性，其中包括参数估计和流场重构。

    Inverse problems, i.e., estimating parameters of physical models from experimental data, are ubiquitous in science and engineering. The Bayesian formulation is the gold standard because it alleviates ill-posedness issues and quantifies epistemic uncertainty. Since analytical posteriors are not typically available, one resorts to Markov chain Monte Carlo sampling or approximate variational inference. However, inference needs to be rerun from scratch for each new set of data. This drawback limits the applicability of the Bayesian formulation to real-time settings, e.g., health monitoring of engineered systems, and medical diagnosis. The objective of this paper is to develop a methodology that enables real-time inference by learning the Bayesian inverse map, i.e., the map from data to posteriors. Our approach is as follows. We represent the posterior distribution using a parameterization based on deep neural networks. Next, we learn the network parameters by amortized variational inferenc
    
[^136]: MADiff：离线多智能体学习与扩散模型

    MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])

    [http://arxiv.org/abs/2305.17330](http://arxiv.org/abs/2305.17330)

    本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。

    

    扩散模型（DM）是一种强大的生成模型，最近在包括离线强化学习在内的各种场景中取得了巨大成功，其中策略通过在在线评估中产生轨迹来进行规划学习。然而，尽管单智能体学习显示了其有效性，但仍不清楚DM如何在多智能体问题中操作，其中代理商很难在独立建模每个代理商轨迹的情况下完成团队合作。在本文中，我们提出MADiff，一种新的生成式多智能体学习框架，以解决这个问题。MADiff是通过基于注意力的扩散模型来实现对多个扩散智能体行为的复杂协调建模。据我们所知，MADiff是第一个基于扩散的多智能体离线RL框架，它既可以行为为分散的政策，又可以为集中控制器，其中包括对手建模，并可用于多智能体轨迹预测。

    Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
    
[^137]: 基于AI的超分辨显微镜分析：在没有基准的情况下进行生物发现

    AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth. (arXiv:2305.17193v1 [q-bio.SC])

    [http://arxiv.org/abs/2305.17193](http://arxiv.org/abs/2305.17193)

    利用弱监督学习范例对超分辨率显微镜进行分析具有发现新生物学的巨大潜力，并且可以加速探索亚细胞大分子和细胞器分子结构。

    

    超分辨显微镜的纳米级分辨率现已使荧光分子定位工具能够用于研究整个细胞结构生物学。基于机器学习的超分辨数据分析具有发现新生物学的巨大潜力，而新生物学本身没有被发现过，也没有基准。在这里，我们描述了弱监督学习范例在超分辨显微镜中的应用以及其潜力，以实现对亚细胞大分子和细胞器分子结构的加速探索。

    The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.
    
[^138]: 深思熟虑：具有内部工作记忆的决策Transformer

    Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])

    [http://arxiv.org/abs/2305.16338](http://arxiv.org/abs/2305.16338)

    该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。

    

    基于大型语言模型（LLM）的决策制定代理已经展示了跨越多个任务的泛化能力。然而，它们的性能依赖于大规模的数据和计算。我们认为，这种低效性源于遗忘现象，即模型通过参数记忆其行为，在训练过程中。因此，新任务的训练可能会降低模型在先前任务上的性能。与LLM的隐式记忆机制不同，人脑利用分布式存储器存储记忆，以有效地管理和组织多种技能，减轻了遗忘现象。因此，我们建议使用内部工作记忆模块来存储、融合和检索不同下游任务的信息。评估结果表明，所提出的方法改善了Atari游戏和元世界物体操作任务的训练效率和泛化能力。此外，我们证明了记忆微调进一步增强了转化决策制定代理对新任务的适应性。

    Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
    
[^139]: 如何逃离锐化的极小值点

    How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])

    [http://arxiv.org/abs/2305.15659](http://arxiv.org/abs/2305.15659)

    本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。

    

    现代机器学习应用程序中的优化算法已经取得了显著的成功，这些算法被设计用来发现平坦的极小值点。为了解决这个算法问题，本文采用损失函数海森矩阵的迹来度量它的平坦程度，并形式化定义了近似平坦极小值点的概念。在此概念下，我们设计了有效地找到近似平坦极小值点的算法。针对一般的损失函数，我们提出了一种基于梯度的算法，可以有效地找到近似平坦的局部极小值点。算法的主要组件是使用从随机扰动迭代中计算的梯度来估计导致更平坦极小值点的方向。对于成本函数是训练数据上的经验风险的设置，我们提出了一种更快速的算法，受最近提出的实用算法——锐度感知最小化的启发。

    Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
    
[^140]: 基于几何多图神经网络的多状态RNA设计

    Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])

    [http://arxiv.org/abs/2305.14749](http://arxiv.org/abs/2305.14749)

    本论文提出了一种基于几何多图神经网络的多状态RNA设计方法，可以明确考虑和反映RNA构象多样性在其设计中。其能够显著提高原生序列的恢复，尤其适用于多状态和结构多样化的RNA。

    

    计算RNA设计在合成生物学和治疗开发方面具有广泛的应用。RNA多样的生物学功能的基础是它的构象灵活性，使单一序列能够采用多种不同的三维结构状态。目前，计算生物分子设计任务经常被提出为逆问题，即基于采用单一预期结构构象来设计序列。在这项工作中，我们提出了gRNAde，这是一个基于一组三维RNA骨架结构操作的几何RNA设计流程，以明确考虑和反映RNA构象多样性在其设计中。我们在一个新的大规模三维RNA设计数据集上演示了gRNAde的效用，特别适用于多状态和结构多样化的RNA，能够显著提高原生序列的恢复。我们的代码可在https://github.com/chaitjo/geometric-rna-design上获得。

    Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
    
[^141]: 不变存储的注意力神经过程

    Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])

    [http://arxiv.org/abs/2305.14567](http://arxiv.org/abs/2305.14567)

    提出了一种不变存储的注意力神经过程 (CMANPs) 及其注意力块 CMAB，能在常数内存下进行条件、查询和更新操作，并在元回归和少样本回归任务上获得最先进的表现。

    

    神经过程 (Neural Processes, NPs) 是估计预测不确定性的高效方法。NPs 包含一个编码条件数据集的条件阶段、一个使用编码预测的查询阶段和一个使用新数据点更新编码的更新阶段。然而，最先进的方法需要额外的内存，这个内存随数据集的大小呈线性或二次函数增长，限制了它们在低资源环境下的应用。在这项工作中，我们提出了不变存储的注意力神经过程 (Constant Memory Attentive Neural Processes, CMANPs)，它的条件、查询和更新阶段均只需要常数内存。在构建 CMANPs 时，我们提出了一种新型的通用注意力块，称为 Constant Memory Attention Block (CMAB)，它可以在常数内存中计算输出并进行常数的更新计算。实验结果表明，CMANPs 在元回归和少样本回归任务上实现了最先进的表现，同时保持常数内存复杂度。

    Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an
    
[^142]: Sketch-and-Project Meets Newton Method: 具有低秩更新的全局$\mathcal O(k^{-2})$ 收敛性

    Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates. (arXiv:2305.13082v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2305.13082](http://arxiv.org/abs/2305.13082)

    本文提出了一种新的基于草图和投影的Newton方法，具有快速的全局收敛率，适用于自共轭函数，具有草图和投影方法的低迭代成本，全秩Newton类方法的最先进全局收敛率以及阻尼Newton方法的算法简单性。

    

    本文提出了一种新的基于草图和投影的Newton方法，具有快速的$\mathcal O(k^{-2})$ 全局收敛率，适用于自共轭函数。我们的方法可以从三个方面来看待：i) 作为一个草图和投影算法，对Newton方法的更新进行投影，ii) 作为在被草图化子空间中进行立方正则化的Newton方法，和 iii) 作为在被草图化子空间中进行阻尼Newton方法。SGN继承了这三个方面的优点：草图和投影方法的低迭代成本，全秩Newton类方法的最先进$\mathcal O(k^{-2})$全局收敛率以及阻尼Newton方法的算法简单性。最后，我们证明了它与基准算法具有相当的实证性能。

    In this paper, we propose the first sketch-and-project Newton method with fast $\mathcal O(k^{-2})$ global convergence rate for self-concordant functions. Our method, SGN, can be viewed in three ways: i) as a sketch-and-project algorithm projecting updates of Newton method, ii) as a cubically regularized Newton ethod in sketched subspaces, and iii) as a damped Newton method in sketched subspaces. SGN inherits best of all three worlds: cheap iteration costs of sketch-and-project methods, state-of-the-art $\mathcal O(k^{-2})$ global convergence rate of full-rank Newton-like methods and the algorithm simplicity of damped Newton methods. Finally, we demonstrate its comparable empirical performance to baseline algorithms.
    
[^143]: 连续多模态知识图谱构建

    Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08698](http://arxiv.org/abs/2305.08698)

    连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。

    

    多模态知识图谱构建（MKGC）涉及使用多种形式的数据，如文本和图像，创建实体和关系的结构化表示。然而，现有的MKGC模型在处理动态现实场景中新增实体和关系方面面临挑战。目前的连续知识图谱构建设置主要关注从文本数据中提取实体和关系，忽视了其他多模态源。因此，有必要探索连续MKGC的挑战，以解决灾难性遗忘现象，并确保保留从不同形式的数据中提取的过去知识。本研究致力于通过开发终身MKGC基准数据集来研究这个复杂的主题。基于经验发现，当多媒体数据训练时，一些典型的MKGC模型可能在连续设置中意外表现不佳，与那些仅利用文本资源的模型相比。我们以实验证据为基础，总结出以下论点：连续多模态知识图谱构建面临着数据源变化导致的灾难性遗忘问题。

    Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
    
[^144]: Speck: 一款拥有低延迟327K神经元卷积神经网络处理管道的智能事件型视觉传感器

    Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline. (arXiv:2304.06793v1 [cs.NE])

    [http://arxiv.org/abs/2304.06793](http://arxiv.org/abs/2304.06793)

    Speck是一款智能视觉传感器SoC，该芯片集成了基于事件的相机和低功耗的sCNN计算架构，可显著降低单元生产成本。通过优化高度稀疏的计算和最小化延迟，Speck可以处理高速的稀疏数据流。

    

    需要从各种传感器中提取高级信息的边缘计算方案正在日益增长。为了解决这个问题，我们提出了一种智能视觉传感器SoC，具有基于事件的相机和低功耗的sCNN计算架构。该SoC的端到端设计简单，可作为独立应用或作为较大系统中的边缘节点。综合传感器和处理于单个芯片中可以显著降低单元生产成本。在处理管道中，对高度稀疏的计算进行优化，并将延迟最小化为$3.36\mu s$，以应对视觉传感器的事件驱动信号等高速信号的稀疏数据流。

    Edge computing solutions that enable the extraction of high level information from a variety of sensors is in increasingly high demand. This is due to the increasing number of smart devices that require sensory processing for their application on the edge. To tackle this problem, we present a smart vision sensor System on Chip (Soc), featuring an event-based camera and a low power asynchronous spiking Convolutional Neuronal Network (sCNN) computing architecture embedded on a single chip. By combining both sensor and processing on a single die, we can lower unit production costs significantly. Moreover, the simple end-to-end nature of the SoC facilitates small stand-alone applications as well as functioning as an edge node in a larger systems. The event-driven nature of the vision sensor delivers high-speed signals in a sparse data stream. This is reflected in the processing pipeline, focuses on optimising highly sparse computation and minimising latency for 9 sCNN layers to $3.36\mu s$
    
[^145]: 学习个性化决策支持策略

    Learning Personalized Decision Support Policies. (arXiv:2304.06701v1 [cs.LG])

    [http://arxiv.org/abs/2304.06701](http://arxiv.org/abs/2304.06701)

    本文提出了一种学习个性化决策支持策略的算法 $\texttt{THREAD}$，可以为决策者提供不同形式的支持。同时，引入了 $\texttt{Modiste}$ 工具来提供个性化的医学诊断决策支持，使用 $\texttt{THREAD}$ 学习个性化决策支持策略，有效提高了预期的诊断正确性，并减少了严重并发症的风险，同时推荐了更少和更便宜的研究。

    

    个体决策者可能需要不同形式的支持来提高决策结果，但重要的问题是，哪种形式的支持会在低成本下导致准确的决策。本文提出了学习决策支持策略的方法，它在给定输入时选择是否以及如何提供支持。我们考虑没有先验信息的决策者，并将学习各自的策略形式化为一个多目标优化问题，这个问题权衡了准确性和成本。使用随机环境的技术，我们提出了 $\texttt{THREAD}$，这是一种个性化决策支持策略的在线算法，并设计了一种超参数调整策略，以利用模拟人类行为来确定成本-性能权衡。我们提供计算实验来证明 $\texttt{THREAD}$ 相对于线下基线的优势。然后，我们推出了一个交互式工具 $\texttt{Modiste}$，它为现实中的医学诊断提供个性化决策支持。$\texttt{Modiste}$ 使用 $\texttt{THREAD}$ 为每位医生学习个性化的决策支持策略，并推荐个性化研究以优化患者的预期结果并将严重并发症的风险降至最低。使用电子健康记录数据，我们展示了 $\texttt{Modiste}$ 显著提高了预期的诊断正确性，并减少了严重并发症的风险，同时推荐了更少和更便宜的研究。

    Individual human decision-makers may benefit from different forms of support to improve decision outcomes. However, a key question is which form of support will lead to accurate decisions at a low cost. In this work, we propose learning a decision support policy that, for a given input, chooses which form of support, if any, to provide. We consider decision-makers for whom we have no prior information and formalize learning their respective policies as a multi-objective optimization problem that trades off accuracy and cost. Using techniques from stochastic contextual bandits, we propose $\texttt{THREAD}$, an online algorithm to personalize a decision support policy for each decision-maker, and devise a hyper-parameter tuning strategy to identify a cost-performance trade-off using simulated human behavior. We provide computational experiments to demonstrate the benefits of $\texttt{THREAD}$ compared to offline baselines. We then introduce $\texttt{Modiste}$, an interactive tool that pr
    
[^146]: 通过数据增强提高连续时间动态图网络长期预测性能

    Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation. (arXiv:2304.05749v1 [cs.LG])

    [http://arxiv.org/abs/2304.05749](http://arxiv.org/abs/2304.05749)

    本研究提出了一种插入式模块——不确定性掩蔽混合（UmmU），它能够在中间层嵌入中进行不确定性估计，进而增强嵌入的不确定性，使其具有更好的泛化性能，从而显著提高了连续时间动态图网络的长期预测性能。

    

    本研究侧重于对连续时间动态图网络（CTDGNs）进行长期预测（LTF），这对于现实世界建模非常重要。现有的CTDGN对于建模时间图数据非常有效，因为它们能够捕捉复杂的时间相关性，但由于对历史数据的实质要求，它们在LTF方面表现不佳，在大多数情况下也不切实际。为了解决这个问题，最直观的方法是数据增强。在本研究中，我们提出了一种插入式模块——不确定性掩蔽混合（UmmU），它能够进行不确定性估计，将不确定性引入CTDGN的中间层嵌入中，并进行掩蔽混合以进一步增强嵌入的不确定性，使其能够适用于更多情况，而且可以轻松地插入到任意CTDGN中，而不增加参数数量。我们在三个真实世界的动态图数据集上进行了全面的实验，并表明UmmU在LTF任务上明显优于现有方法。

    This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph dat
    
[^147]: 基于矩阵自回归的联邦学习拜占庭容错聚合方案

    A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])

    [http://arxiv.org/abs/2303.16668](http://arxiv.org/abs/2303.16668)

    本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。

    

    本文提出了FLANDERS，一种新颖的联邦学习（FL）聚合方案，可以抵御拜占庭攻击。FLANDERS将每个FL轮次中由客户端发送的本地模型更新视为矩阵值时间序列。然后，通过将实际观测与由矩阵自回归预测模型估计的观测进行比较，识别恶意客户端作为这个时间序列的异常值。在不同FL设置下对多个数据集进行的实验证明，FLANDERS在抵御拜占庭攻击方面与最强大的基线相匹配。此外，与现有的防御策略相比， FLANDERS即使在极其严重的攻击场景下仍然非常有效。

    In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
    
[^148]: 使用不同降维和分类技术的癫痫发作检测的经验分析

    Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12012](http://arxiv.org/abs/2302.12012)

    本研究对使用不同降维和分类技术进行癫痫发作检测进行了实证分析，通过离散小波变换和机器学习分类器，结合主成分分析、独立成分分析和线性判别分析等降维算法，选择特征来提高检测准确性。

    

    脑电图（EEG）是一种非侵入性检查，记录大脑的电活动。该检查用于帮助诊断各种脑问题。通过离散小波变换（DWT）和机器学习分类器，可以进行癫痫检测。在癫痫发作检测中，主要使用机器学习分类器和统计特征。EEG信号中的隐藏信息对于检测影响大脑的疾病很有用。有时，在时间和频率域内识别EEG的最小变化是非常困难的。DWT可以在不同频带进行信号良好的分解和特征提取。我们使用三个降维算法：主成分分析（PCA）、独立成分分析（ICA）和线性判别分析（LDA）。最后，通过融合规则选择特征。

    An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
    
[^149]: 判别熵聚类及其与K-means和SVM的关系

    Discriminative Entropy Clustering and its Relation to K-means and SVM. (arXiv:2301.11405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11405](http://arxiv.org/abs/2301.11405)

    该论文介绍了判别熵聚类的相关理论及其与K-means和SVM的区别和相似之处。同时提出了一种新的损失函数，用于改进深度聚类的性能。

    

    在判别模型中，最大化模型输入和输出之间的互信息形式上与 softmax 预测的“决策性”和“公平性”有关，从而激发了基于熵的无监督损失函数的使用。 最近，基于这样的损失函数的自我标记方法代表了深度聚类的最新研究方向。 首先，我们讨论了这种熵聚类方法的许多通用属性，包括它们与 K-means 和无监督 SVM 技术的关系。 我们证明与 K-均值有着根本的区别。另一方面，我们表明了与基于 SVM 的聚类的相似性，使我们能够将显式的边际最大化与熵聚类联系起来。最后，我们观察到交叉熵的常见形式对于伪标签错误不稳健。我们的新损失解决了这个问题，并导致了一种新的 EM 算法，在许多标准基准测试中改进了最新技术水平。

    Maximization of mutual information between the model's input and output is formally related to "decisiveness" and "fairness" of the softmax predictions, motivating such unsupervised entropy-based losses for discriminative models. Recent self-labeling methods based on such losses represent the state of the art in deep clustering. First, we discuss a number of general properties of such entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques. Disproving some earlier published claims, we point out fundamental differences with K-means. On the other hand, we show similarity with SVM-based clustering allowing us to link explicit margin maximization to entropy clustering. Finally, we observe that the common form of cross-entropy is not robust to pseudo-label errors. Our new loss addresses the problem and leads to a new EM algorithm improving the state of the art on many standard benchmarks.
    
[^150]: ZigZag: 通过两步推理实现的通用无采样不确定性估计

    ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference. (arXiv:2211.11435v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11435](http://arxiv.org/abs/2211.11435)

    本研究提出了一种通用的无采样不确定性估计方法，通过训练网络在有和没有额外信息的情况下产生相同的输出，实现了与最先进方法相媲美的可靠性估计，同时显著降低了计算成本。

    

    尽管深度网络产生有用的预测的能力已经被充分证明，但是估计这些预测的可靠性仍然具有挑战性。诸如MC-Dropout和Deep Ensembles之类的采样方法已经成为最流行的用于此目的的方法。不幸的是，它们在推理时需要进行许多前向传递，这会减慢速度。无采样方法可能更快，但存在其他缺点，例如不确定性估计的可信度较低、使用困难以及适用于不同类型的任务和数据的能力有限。在这项工作中，我们介绍了一种通用且易于部署的无采样方法，它以明显较低的计算成本获得与最先进方法相媲美的可靠性估计。其基本原理是训练网络在有和没有额外信息的情况下产生相同的输出。在推理时，当没有提供先验信息时，我们使用网络的自身预测。

    Whereas the ability of deep networks to produce useful predictions has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data.  In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own predicti
    
[^151]: 联邦学习中的本地模型重构攻击及其应用

    Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16205](http://arxiv.org/abs/2210.16205)

    本文研究了联邦学习中的本地模型重构攻击，攻击者可以通过重构客户端的模型更有效地触发其他攻击，并提出了一种新颖的基于模型的属性推导攻击，实证结果表明攻击有效，可应用于回归和分类任务。

    

    本文探讨了联邦学习中的本地模型重构攻击，攻击者可以通过窃听目标客户端与服务器之间的通信并重构受害者的本地/个性化模型，从而更有效地触发其他经典攻击。本地模型仅依赖于客户端的数据，但可能泄漏比服务器学习的全局模型更多的私有信息。此外，我们提出了一种新的基于模型的属性推导攻击，利用了本地模型重构攻击。我们提供了这种攻击的分析下界。使用真实世界数据集的实证结果表明，我们的本地重构攻击对于回归和分类任务都有效。此外，我们将我们的新颖的属性推导攻击与最先进的攻击进行了基准测试。

    In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
    
[^152]: 自主训练：一项综述

    Self-Training: A Survey. (arXiv:2202.12040v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12040](http://arxiv.org/abs/2202.12040)

    自主训练方法是一种半监督算法，无需额外假设数据分布，在低密度区域找到决策边界，并使用学习分类器的输出分数作为置信度，通过为无标签样本分配伪标签，迭代地学习分类器，从而丰富有标签训练数据。

    

    半监督算法旨在从少量有标签观测和大量无标签观测中学习预测函数。由于这个框架在许多应用中是相关的，因此它们在学术界和工业界都受到了很多关注。在现有的技术中，自主训练方法在近年来无疑引起了更大的关注。这些模型旨在在低密度区域找到决策边界，而不对数据分布作出额外的假设，并使用学习分类器的无符号输出分数或其边界作为置信度的指标。自主训练算法的工作原理是通过给具有大于某个阈值的边界的无标签训练样本分配伪标签，迭代地学习分类器。然后，使用伪标记的示例来增强有标签训练数据，并与有标签训练集一起训练一个新的分类器。

    Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this
    
[^153]: 《长话短说：因果机器学习中的遗漏变量偏差》

    Long Story Short: Omitted Variable Bias in Causal Machine Learning. (arXiv:2112.13398v4 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2112.13398](http://arxiv.org/abs/2112.13398)

    在因果机器学习中，我们通过推导出遗漏变量偏差的尖锐上界，为广泛的线性泛函因果参数提供了一种简单而通用的方法。这种方法可以应用于许多传统的因果推断研究目标，并且仅取决于潜变量在结果和参数的Riesz表示器中所导致的额外变异。

    

    我们推导了一类广泛的因果参数的遗漏变量偏差的一般但简单的尖锐上界，这些参数可以被认定为结果的条件期望函数的线性泛函。这样的泛函包括许多因果推断研究中的传统调查目标，例如（加权）潜在结果的平均值、平均处理效应（包括子组效应，如对待处理对象的影响）、（加权）平均导数和来自协变量分布变化的策略效应 - 全部适用于一般的非参数因果模型。我们的构造依赖于目标泛函的Riesz-Fréchet表示。具体来说，我们展示了偏差上界仅取决于潜变量在结果和感兴趣参数的Riesz表示器中所创建的附加变化。此外，在许多重要情况下（例如平均处理效应和平均导数）

    We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects and avearage derivative
    

