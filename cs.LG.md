# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968) | 该论文提出了一种数据精炼的方法，通过从LLM中提取知识来实现Prompt的压缩，确保压缩后的提示保持对原始提示的忠实性。 |
| [^2] | [TexTile: A Differentiable Metric for Texture Tileability](https://arxiv.org/abs/2403.12961) | TexTile是一种不同iable的度量方法，用于评估纹理图像的平铺性能，可以帮助更明智地合成和分析平铺纹理。 |
| [^3] | [WHAC: World-grounded Humans and Cameras](https://arxiv.org/abs/2403.12959) | 该研究提出了一个名为WHAC的框架，可以实现基于世界坐标系的丰富人体姿势和形状估计，同时进行摄像机姿势估计，无需依赖传统优化技术。 |
| [^4] | [Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models](https://arxiv.org/abs/2403.12952) | 引入了测试时间原型转移（TPS）框架，通过动态学习每个原型的转移向量，有效地弥合了领域差距并增强了类 |
| [^5] | [Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion](https://arxiv.org/abs/2403.12950) | 该研究建立了首个在对抗性多臂老虎机中优化且自适应的波达动态遗憾上界，揭示了在Condorcet和Borda之间严重非平稳性可学习性的基本差异 |
| [^6] | [On Safety in Safe Bayesian Optimization](https://arxiv.org/abs/2403.12948) | 本研究探讨了安全贝叶斯优化中的安全性问题，并提出了Real-\b{eta}-SafeOpt算法，有效保留了所有理论保证。 |
| [^7] | [Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes](https://arxiv.org/abs/2403.12946) | 本文研究了离线强化学习中线性马尔可夫决策过程的分布鲁棒性样本复杂度问题，提出了一种悲观模型算法并建立了其样本复杂性界限，能在高维状态-动作空间中提高学习策略的性能。 |
| [^8] | [Neural Differential Algebraic Equations](https://arxiv.org/abs/2403.12938) | 提出了神经微分代数方程（NDAEs）用于数据驱动的建模，展示了其在系统理论数据驱动建模任务中的适用性，并通过具体示例表明了其在噪声和外部干扰下的鲁棒性。 |
| [^9] | [Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts](https://arxiv.org/abs/2403.12918) | 提出了一种基于注意力引导的权重混合正则化方法，用于解决低资源文本上预训练语言模型微调时的稳定性和泛化能力问题 |
| [^10] | [Yell At Your Robot: Improving On-the-Fly from Language Corrections](https://arxiv.org/abs/2403.12910) | 该论文发现，通过人类提供的语言纠正，可以帮助机器人持续改进长久任务表现 |
| [^11] | [Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference](https://arxiv.org/abs/2403.12900) | 本文提出了Sprout框架，通过引入生成指令的概念，平衡了生态可持续性和高质量生成结果之间的需求，实现了大型语言模型推断服务碳排放的显著减少。 |
| [^12] | [Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport](https://arxiv.org/abs/2403.12887) | 该研究通过研究无限深度和任意宽度的ResNet的“均场”模型，探讨了深度神经网络训练过程中的梯度流收敛性，以更好地理解简单优化算法如何成功解决这一具有挑战性的优化问题。 |
| [^13] | [Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints](https://arxiv.org/abs/2403.12873) | 提出了一种使用天空摄像头图像和噪声输入改善太阳辐照预测准确性的数据节约机器学习模型。 |
| [^14] | [Wildfire danger prediction optimization with transfer learning](https://arxiv.org/abs/2403.12871) | 本研究利用迁移学习优化了野火危险预测，整合了CNNs和加拿大火灾天气指数，成功建立了一种计算野火风险级别的方法，并在识别烧毁区域方面取得了95%的准确率。 |
| [^15] | [A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection](https://arxiv.org/abs/2403.12864) | 本研究比较了不同深度学习架构在太空船数据异常检测中的有效性。 |
| [^16] | [D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation](https://arxiv.org/abs/2403.12861) | 提出了一种名为D-Cubed的新型轨迹优化方法，利用潜在扩散模型从玩耍数据集中训练，并引入了一种新颖的无梯度引导采样方法，可解决灵巧可变形物体操作任务。 |
| [^17] | [Primal Methods for Variational Inequality Problems with Functional Constraints](https://arxiv.org/abs/2403.12859) | 本文提出了一种简单的原始方法，称为约束梯度方法（CGM），用于解决具有多个功能约束的变分不等式问题。 |
| [^18] | [Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning](https://arxiv.org/abs/2403.12856) | 本文提出了一种无需专门神经网络组件的等变策略和不变值函数构建方法，在基于地图的路径规划中展示了等变集合和正则化如何提高样本效率和性能 |
| [^19] | [Policy Bifurcation in Safe Reinforcement Learning](https://arxiv.org/abs/2403.12847) | 我们的研究发现在安全强化学习中可能存在策略分叉现象，提出了拓扑分析以证明在一些情景下，策略需要具有不连续性或多值性，这对应于障碍物自由状态空间为非单连通时需要策略分叉的情况。 |
| [^20] | [MELTing point: Mobile Evaluation of Language Transformers](https://arxiv.org/abs/2403.12844) | 该研究对移动设备上大型语言模型（LLMs）的执行进行了首次系统性研究，并创建了自动化基础架构MELT来支持其评估和性能测试。 |
| [^21] | [Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects](https://arxiv.org/abs/2403.12830) | 本文介绍了针对机器近似遗忘审计任务的明确定义和有效指标，通过改变审计挑战的方式为非成员推断问题，开发了高效的审计指标，简化了评估过程 |
| [^22] | [FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer](https://arxiv.org/abs/2403.12821) | FlowerFormer是一种强大的图变换器，通过双向异步消息传递和基于流程的全局注意力，可以增强神经结构的表征学习。 |
| [^23] | [A Physics-embedded Deep Learning Framework for Cloth Simulation](https://arxiv.org/abs/2403.12820) | 该论文提出了一种基于物理的深度学习框架，可以直接编码布料模拟的物理特征，实现快速和实时模拟，并在不使用新数据训练的情况下通过测试表现出与基线的一致性。 |
| [^24] | [Dynamic Survival Analysis for Early Event Prediction](https://arxiv.org/abs/2403.12818) | 该研究通过动态生存分析方法在早期事件预测领域取得了重要进展，通过整合风险定位到警报策略中，实现了对临床事件指标的提升，并通过新的警报优先级方案显著改进了事件级指标（高达11%的AuPRC差异）。 |
| [^25] | [Neural Parameter Regression for Explicit Representations of PDE Solution Operators](https://arxiv.org/abs/2403.12764) | 神经参数回归框架在学习偏微分方程解算符方面利用物理信息神经网络技术，提高了参数效率和计算效率，具有出色的适应性和可扩展性。 |
| [^26] | [Tighter Confidence Bounds for Sequential Kernel Regression](https://arxiv.org/abs/2403.12732) | 通过使用鞅尾巴界限和无限维凸规划的有限维重构，建立了序贯核回归的新置信区间，证明其始终比现有的置信区间更紧凑，并将其应用于核赌博问题，提高了算法的性能表现。 |
| [^27] | [Posterior Uncertainty Quantification in Neural Networks using Data Augmentation](https://arxiv.org/abs/2403.12729) | 通过提出MixupMP方法，使用数据增强构建更现实的预测分布，从而解决了在神经网络中对后验不确定性进行量化时的基本模型类错误规范化问题。 |
| [^28] | [Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis](https://arxiv.org/abs/2403.12719) | 通过引入双层超图机制和梯度驱动流的新策略，本文提出了一种半监督多模态诊断框架，在阿尔茨海默病诊断中表现出优越性能。 |
| [^29] | [Addressing Source Scale Bias via Image Warping for Domain Adaptation](https://arxiv.org/abs/2403.12712) | 通过在训练过程中对突出的对象区域进行过采样的自适应注意力处理，以及针对对象区域采样的实例级变形引导，有效减轻域自适应中的源尺度偏差。 |
| [^30] | [Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition](https://arxiv.org/abs/2403.12710) | 提出了一种选择性、可解释的和运动一致的隐私属性混淆方法，通过人类选择的隐私模板实现可解释性，同时在动作识别中表现出更大的灵活性和性能优势。 |
| [^31] | [Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency](https://arxiv.org/abs/2403.12695) | 提出了一个用于医学图像分割的新颖联合半监督学习框架 |
| [^32] | [LNPT: Label-free Network Pruning and Training](https://arxiv.org/abs/2403.12690) | 本文介绍了LNPT，一种无标签网络修剪和训练的新框架，通过引入学习差距的概念，强调其准确相关性，以解决在智能设备上确定修剪结构的难题。 |
| [^33] | [SEVEN: Pruning Transformer Model by Reserving Sentinels](https://arxiv.org/abs/2403.12688) | SEVEN通过保留梯度噪声较小的权重，在剪枝Transformer模型时取得了优异的效果。 |
| [^34] | [Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision](https://arxiv.org/abs/2403.12687) | 该研究提出了一种新颖的音视频复合表达识别方法，通过情绪识别模型融合不同模态的情绪概率，并基于预定义规则进行决策，无需特定训练数据，具有潜力为人类基本和复合情感情境下的音视频数据注释提供智能工具。 |
| [^35] | [Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine](https://arxiv.org/abs/2403.12672) | 该研究提出了一种基于累积分布改进评分可解释性的度量，建立了使用可解释度量设置阈值的准则，并通过数值实验证明其合理性。 |
| [^36] | [Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making](https://arxiv.org/abs/2403.12664) | 提出了cattleia工具，可以解密用于回归、多类别和二元分类任务的AutoML集成模型，通过评估指标和新度量指标，分析集成模型的性能和重要性 |
| [^37] | [Zeolite Adsorption Property Prediction using Deep Learning](https://arxiv.org/abs/2403.12659) | 通过深度学习模型，本研究提出了一个比传统分子模拟快4到5个数量级的方法来预测沸石的吸附性能，并可以用于识别吸附位点。 |
| [^38] | [Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation](https://arxiv.org/abs/2403.12650) | 该论文提出了一种神经网络架构，用于解决高维参数化偏微分方程，模仿自适应有限元方法，实现对逼近误差的控制，并通过有效的后验误差估计器将逼近参数减少到很少，实现了问题自适应表示。 |
| [^39] | [Prompt-fused framework for Inductive Logical Query Answering](https://arxiv.org/abs/2403.12646) | 提出了一种名为Pro-QE的查询感知提示融合框架，能够整合现有查询嵌入方法并通过上下文信息聚合来解决知识图谱中新实体的嵌入问题。 |
| [^40] | [Automated Contrastive Learning Strategy Search for Time Series](https://arxiv.org/abs/2403.12641) | 本文介绍了微软开展的自动机器学习实践，用于自动对比学习各种时间序列数据集和任务的表示。 |
| [^41] | [A Practical Guide to Statistical Distances for Evaluating Generative Models in Science](https://arxiv.org/abs/2403.12636) | 本文提供了一种实用指南，介绍了四种常用统计距离的概念，以帮助评估生成模型，无需高深的数学和统计知识。 |
| [^42] | [SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition](https://arxiv.org/abs/2403.12609) | 该研究探索了音视频深度学习方法在野外情绪识别问题中的有效性，针对视频和音频模态分别基于微调的CNN和PDEM架构，比较了不同的时间建模和融合策略，并在ABAW'24挑战中取得了实验结果。 |
| [^43] | [On the Effectiveness of Heterogeneous Ensemble Methods for Re-identification](https://arxiv.org/abs/2403.12606) | 提出了一种新颖的集成方法，用于重识别工业实体，取得了在任务中最先进的性能，同时引入了五种不同的特征提取方法，并研究它们的组合。 |
| [^44] | [Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance](https://arxiv.org/abs/2403.12599) | 通过机器学习系统，根据未来无家可归的风险，准确识别需要支持的个人，提高了分配效率，公平和公正，避免了一部分人因当前流程的疏忽而无家可归。 |
| [^45] | [Machine Learning of the Prime Distribution](https://arxiv.org/abs/2403.12588) | 使用最大熵方法推导了概率数论中的几个定理，提供了关于素数学习性质的理论论证，发现Erd\H{o}s-Kac定律不太可能被当前机器学习技术发现，并进行数值实验以验证理论结果 |
| [^46] | [Equity through Access: A Case for Small-scale Deep Learning](https://arxiv.org/abs/2403.12562) | 通过引入PePR分数，研究人员展示了在资源有限的情况下，利用131种独特的DL架构在医学图像任务中的可行性。 |
| [^47] | [Confidence Self-Calibration for Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2403.12559) | 本文提出了一种针对多标签类增量学习中的信心自校准问题的方法，通过引入图卷积网络和最大熵正则化，改善了多标签信心校准，减少了过度自信的误报错误。 |
| [^48] | [Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs](https://arxiv.org/abs/2403.12553) | 该研究提出了一种名为CoDA-NO的神经算子，通过在象域或通道空间对函数进行标记，实现了多个PDE系统的自监督学习或预训练，为解决涉及耦合偏微分方程的多物理问题提供了新思路。 |
| [^49] | [AffineQuant: Affine Transformation Quantization for Large Language Models](https://arxiv.org/abs/2403.12544) | AffineQuant是一种用于大型语言模型的直接优化后训练量化方法，通过使用等效的仿射变换来扩展优化范围并显著减小量化误差。 |
| [^50] | [Contextualized Messages Boost Graph Representations](https://arxiv.org/abs/2403.12529) | 这篇论文提出了关于图神经网络在各个层次（节点级、邻域级和图级）的表示能力的新视角。 |
| [^51] | [Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training](https://arxiv.org/abs/2403.12511) | 本文利用前向自动微分计算梯度，提出了基于Frank-Wolfe算法的优化方法，收敛速度为次线性。 |
| [^52] | [Generalized Consistency Trajectory Models for Image Manipulation](https://arxiv.org/abs/2403.12510) | 本研究提出了广义一致性轨迹模型（GCTMs），能够在任何噪声分布和数据分布之间实现转换。 |
| [^53] | [Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices](https://arxiv.org/abs/2403.12503) | 该论文全面调查了与大型语言模型相关的安全和隐私问题，并提出了未来研究的有前途方向。 |
| [^54] | [A Trainable Feature Extractor Module for Deep Neural Networks and Scanpath Classification](https://arxiv.org/abs/2403.12493) | 提出了一个联合可训练的特征提取模块，用于将扫视路径转换为可直接用于深度神经网络的特征向量，以提高分类性能。 |
| [^55] | [NTK-Guided Few-Shot Class Incremental Learning](https://arxiv.org/abs/2403.12486) | 本文通过NTK对FSCIL模型的指导，致力于在增量学习中实现卓越泛化，通过优化NTK收敛和降低泛化误差来确保最佳性能。 |
| [^56] | [TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer](https://arxiv.org/abs/2403.12481) | TT-BLIP模型通过使用BLIP和Tri-Transformer技术，结合文本和图像的多模态信息提取，采用Multimodal Tri-Transformer融合特征，实现了增强的综合表征和改进的多模态数据分析。 |
| [^57] | [FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization](https://arxiv.org/abs/2403.12474) | 通过引入促进公平性的特征（F3）来中和图神经网络中的敏感偏见，进而提高预测性能和公平性的权衡。 |
| [^58] | [When Do "More Contexts" Help with Sarcasm Recognition?](https://arxiv.org/abs/2403.12469) | 本研究探讨了将更多上下文信息整合到模型中能够带来的对于讽刺识别的改进效果。 |
| [^59] | [Non-negative Contrastive Learning](https://arxiv.org/abs/2403.12459) | 非负对比学习(NCL)是对非负矩阵分解(NMF)的重新演绎，通过对特征施加非负约束来获得可解释的特征，保留了NMF的可解释属性，从而得到比标准对比学习(CL)更稀疏和解耦的表示 |
| [^60] | [Do Generated Data Always Help Contrastive Learning?](https://arxiv.org/abs/2403.12448) | 生成的高质量图像已成功应用于增强对比表示学习，但我们发现有时生成的数据甚至会对对比学习造成伤害，通过研究发现更强的数据膨胀应该伴随着更弱的增强。 |
| [^61] | [TransformMix: Learning Transformation and Mixing Strategies from Data](https://arxiv.org/abs/2403.12429) | 提出了一种名为TransformMix的自动化方法，可以从数据中学习更好的变换和混合增强策略。 |
| [^62] | [Transfer in Sequential Multi-armed Bandits via Reward Samples](https://arxiv.org/abs/2403.12428) | 提出了一种基于奖励样本转移的算法，在顺序多臂老虎机问题中显著改进了累积遗憾性能 |
| [^63] | [Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization](https://arxiv.org/abs/2403.12422) | Jetfire提出了一种针对transformers的高效准确的INT8训练方法，实现了与FP16训练基线相当的精度，为标准transformer块提供了1.42倍的训练加速和1.49倍的内存节省。 |
| [^64] | [STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model](https://arxiv.org/abs/2403.12418) | STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。 |
| [^65] | [On Predictive planning and counterfactual learning in active inference](https://arxiv.org/abs/2403.12417) | 主动推断中提出了基于“规划”和“从经验中学习”两种决策方案，并引入了一个混合模型以平衡数据复杂性，提供了可解释的智能决策制定框架。 |
| [^66] | [Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion](https://arxiv.org/abs/2403.12406) | 本文提出了RallyNet，这是一种新颖的用于羽毛球运动员行为的分层离线模仿学习模型，能够捕捉选手的决策依赖关系，解决了直接应用现有方法时可能遇到的层次结构和轮流采取行动导致的复合效应问题。 |
| [^67] | [Understanding Training-free Diffusion Guidance: Mechanisms and Limitations](https://arxiv.org/abs/2403.12404) | 本文旨在深入理解无需训练的扩散引导的操作机制和基本限制，提供了理论分析支持该方法，同时揭示了其更易受到对抗性梯度影响和较慢收敛的缺点。 |
| [^68] | [Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales](https://arxiv.org/abs/2403.12403) | 利用大型语言模型提取原因特征训练仇恨言论分类器，实现可解释的检测方法 |
| [^69] | [Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing](https://arxiv.org/abs/2403.12400) | 提出了一种基于BERT的深度学习模型CSI-BERT，可以在目标数据集上以自监督方式进行训练，捕捉不同子载波之间的顺序关系，从而实现更低的错误率和更快的速度，即使面对高丢包率。 |
| [^70] | [Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing](https://arxiv.org/abs/2403.12399) | 本论文提出了用于社区拉票的动态多步对抗性攻击，使得对手能够利用基于梯度的攻击来策划目标选民的操纵。 |
| [^71] | [FairSTG: Countering performance heterogeneity via collaborative sample-level optimization](https://arxiv.org/abs/2403.12391) | FairSTG通过协作样本级优化对抗性能异质性，提出了一个模型独立的面向公平性的时空图学习框架，通过协作挑战性样本与已学习样本的 mix-up 实现知识转移。 |
| [^72] | [An Aligning and Training Framework for Multimodal Recommendations](https://arxiv.org/abs/2403.12384) | 提出了一种名为AlignRec的对齐和训练框架，用于解决多模态推荐中的不对齐问题，通过将推荐目标分解为三个对齐部分，实现内容内部对齐、内容与分类ID之间的对齐以及用户和项目之间的对齐。 |
| [^73] | [Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising](https://arxiv.org/abs/2403.12382) | 提出了一种低迹适应的Noise2Noise（LoTA-N2N）模型，通过引入迹项的优化目标减少了自监督和监督方法之间的差异，提高了自监督学习的性能。 |
| [^74] | [Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model](https://arxiv.org/abs/2403.12372) | 提出了CrossTimeNet，一个新颖的跨领域自监督预训练学习框架，旨在解决不同领域时间序列数据特性差异带来的挑战，并能有效转换原始时间序列数据。 |
| [^75] | [Advancing Time Series Classification with Multimodal Language Modeling](https://arxiv.org/abs/2403.12371) | 用多模态语言建模的方法提出了InstructTime，将时间序列分类重新形成为一个学习生成的范例，以解决目标标签编码和跨领域模型学习的问题 |
| [^76] | [Semisupervised score based matching algorithm to evaluate the effect of public health interventions](https://arxiv.org/abs/2403.12367) | 提出了一种基于二次评分函数的一对一匹配算法，通过设计权重最小化配对训练单元之间的得分差异，同时最大化未配对训练单元之间的得分差异 |
| [^77] | [U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation](https://arxiv.org/abs/2403.12366) | 本文使用U-Net神经网络预测EnKF算法中的局部集合协方差，提出了UNetKF方法，其性能可以与传统方法相匹配或超越。 |
| [^78] | [DMAD: Dual Memory Bank for Real-World Anomaly Detection](https://arxiv.org/abs/2403.12362) | DMAD提出了一个双存储器银行的框架，用于处理统一的无监督和半监督异常检测场景，通过计算特征距离和特征注意力来增强表示学习，从而提高异常检测效果。 |
| [^79] | [Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation](https://arxiv.org/abs/2403.12354) | 提出了一种名为Sim2Real的深度学习框架，针对重建光谱学中的光谱信号重建问题，通过设备信息模拟数据的层次化增强策略实现了推理速度的显著提升。 |
| [^80] | [Friendly Sharpness-Aware Minimization](https://arxiv.org/abs/2403.12350) | 通过研究锐度感知最小化（SAM）的核心组件，引入“友好SAM”（F-SAM）进一步增强泛化性能，发现了批特定随机梯度噪声在对抗性扰动中扮演的关键角色，从而提升SAM的泛化能力。 |
| [^81] | [Stochastic Halpern iteration in normed spaces and applications to reinforcement learning](https://arxiv.org/abs/2403.12338) | 该论文分析了随机Halpern迭代在赋范空间中的Oracle复杂度，提出了改进的算法复杂度，进而在强化学习中提出了新的同步算法应用。 |
| [^82] | [Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems](https://arxiv.org/abs/2403.12335) | 引入了时间一致Koopman自编码器（tcKAE）来生成准确的长期预测，在有限嘈杂的训练数据下通过一致性正则化项增强了模型的稳健性和泛化能力。 |
| [^83] | [FedFisher: Leveraging Fisher Information for One-Shot Federated Learning](https://arxiv.org/abs/2403.12329) | 该论文提出了FedFisher算法，通过利用Fisher信息矩阵进行一次性联邦学习，能够在一次通信中训练出全局模型，并在理论上证明了随着神经网络宽度和客户端本地训练量的增加，FedFisher全局模型的误差会变得非常小。 |
| [^84] | [Methods for Generating Drift in Text Streams](https://arxiv.org/abs/2403.12328) | 文本数据中概念漂移是一个常见现象，而本文提出了四种文本漂移生成方法来帮助产生具有标记漂移的数据集 |
| [^85] | [GT-Rain Single Image Deraining Challenge Report](https://arxiv.org/abs/2403.12327) | 该报告回顾了GT-Rain单图像去雨挑战的结果，旨在研究真实场景中的雨天气象现象，并提供新颖的真实世界雨天图像数据集，激发创新思路以促进单图像去雨方法在真实图像上的发展。 |
| [^86] | [Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts](https://arxiv.org/abs/2403.12326) | 通过引入可学习提示到交叉注意力模块中，本文提出了一种新方法，用于从文本到图像生成模型中去除不良概念，实现了对模型效果的提升。 |
| [^87] | [Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional Computing on Embedded Devices](https://arxiv.org/abs/2403.12323) | 通过使用Hyperdimensional Computing（HDC）设计适用于智能手机、智能可穿戴设备和物联网的及时干预方法，增强了皮下酒精水平检测的效果。 |
| [^88] | [Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training](https://arxiv.org/abs/2403.12320) | 该论文提出了一种近似似然比方法，通过在反向传递过程中利用自然并行性，提供了一种高性能训练策略，以减轻神经网络训练中的计算和内存需求。 |
| [^89] | [Improving LoRA in Privacy-preserving Federated Learning](https://arxiv.org/abs/2403.12313) | 该论文提出了一种在隐私保护联邦学习中改进LoRA方法，解决了在此设置下LoRA不稳定的问题，特别是由于数据异构性、差分隐私噪声放大和超参数的敏感性引起的挑战。 |
| [^90] | [Reinforcement Learning from Delayed Observations via World Models](https://arxiv.org/abs/2403.12309) | 本文提出了一种通过世界模型处理观察延迟的方法，可有效处理部分可观察性，相比现有方法，实验表明其中一种方法可以胜过朴素方法达到30%的性能提升。 |
| [^91] | [Molecular Classification Using Hyperdimensional Graph Classification](https://arxiv.org/abs/2403.12307) | 我们的工作引入了一种基于高维计算的图学习方法，在分子分类中表现出与先进模型相媲美的结果，并且在性能上取得了显著的提升。 |
| [^92] | [FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications](https://arxiv.org/abs/2403.12285) | 引入了一种基于Llama 2 7B模型的金融领域特定的情感分类框架，通过微调模型来受益于其生成性质和全面的语言操作。 |
| [^93] | [Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices](https://arxiv.org/abs/2403.12278) | 随机舍入技术能有效隐式正则化高瘦矩阵，确保舍入后的矩阵具有完整的列秩。 |
| [^94] | [Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity](https://arxiv.org/abs/2403.12267) | 改进预训练数据质量对于提高CLIP性能比增加数据量更为有效，本研究提出了首个针对CLIP的理论严谨的数据选择方法，大幅提升了泛化性能。 |
| [^95] | [Adaptive LPD Radar Waveform Design with Generative Deep Learning](https://arxiv.org/abs/2403.12254) | 使用生成式深度学习设计自适应LPD雷达波形，以在不被发现的情况下有效地进行测距和感知 |
| [^96] | [Reference-based Metrics Disprove Themselves in Question Generation](https://arxiv.org/abs/2403.12242) | 基于参考文献的指标在问句生成中被推翻，作者提出了一个无需参考文献的多维标准评估方法。 |
| [^97] | [Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments](https://arxiv.org/abs/2403.12237) | 本文提出了一种通过将Transformer架构和演员-评论家强化学习模型相结合的新方法TRL-HPO，在资源受限的IoT环境中实现了高效的超参数优化，该方法在MNIST数据集上表现优良。 |
| [^98] | [Improving Generalization via Meta-Learning on Hard Samples](https://arxiv.org/abs/2403.12236) | 在学习的重加权(LRW)方法中，通过元学习中使用难以分类的实例作为验证集，来改善分类器的泛化性能，并提出了一个高效的算法来训练这个模型。 |
| [^99] | [Large-scale flood modeling and forecasting with FloodCast](https://arxiv.org/abs/2403.12226) | 本研究提出了FloodCast框架，通过结合多卫星观测和水动力建模模块，实现了快速、稳定、准确的大规模洪水建模和预测，其中引入了几何自适应的物理启发式神经求解器（GeoPINS）。 |
| [^100] | [Private graphon estimation via sum-of-squares](https://arxiv.org/abs/2403.12213) | 基于和平方法的私有图估计算法首次实现了学习随机块模型和图估计的纯节点差分隐私算法，具有多项式运行时间，与之前最佳的信息论节点私有机制具有相匹配的统计效用保证。 |
| [^101] | [Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions](https://arxiv.org/abs/2403.12212) | 本研究通过引入新方法，将标记分类任务重新构建为文本生成问题，评估了在巴西银行财报电话转录中使用的单语和多语言Transformer模型的性能。 |
| [^102] | [Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning](https://arxiv.org/abs/2403.12210) | 引入控制Lyapunov函数（CLF）来降低强化学习中的样本复杂性，为现实世界机器人场景的训练提供了新的高效方法。 |
| [^103] | [Useful Compact Representations for Data-Fitting](https://arxiv.org/abs/2403.12206) | 我们提出了新的紧凑表示方法，在大型确定性问题的软件实现中表现良好，特别适用于大型特征值计算、张量分解和非线性回归。 |
| [^104] | [Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight](https://arxiv.org/abs/2403.12203) | 在基于视觉的自主无人机竞速中，本研究提出了将强化学习和模仿学习相结合的新型训练框架，以克服样本效率和计算需求方面的挑战，并通过三个阶段的方法进行性能受限的自适应RL微调 |
| [^105] | [Compositional learning of functions in humans and machines](https://arxiv.org/abs/2403.12201) | 人类和神经网络模型通过结合学习和推理探索组合函数的能力，不仅能够进行简单的顺序函数串联，还能理解更复杂的交互函数组合，具有广泛的应用潜力。 |
| [^106] | [FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos](https://arxiv.org/abs/2403.12198) | FLex提出了一种用于立体内窥镜视频的新方法，通过联合优化神经辐射场和摄像机姿势，改善了内窥镜重建的效率和处理规模能力。 |
| [^107] | [PETScML: Second-order solvers for training regression problems in Scientific Machine Learning](https://arxiv.org/abs/2403.12188) | PETScML框架搭建了一个轻量级软件工具用于在科学机器学习中应用传统求解器来解决监督学习问题 |
| [^108] | [Approximation of RKHS Functionals by Neural Networks](https://arxiv.org/abs/2403.12187) | 本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型，并建立了逼近的普适性，推导了逆多重二次、高斯和Sobolev核引起的误差界限，证明神经网络可以准确逼近广义函数线性模型中的回归映射。 |
| [^109] | [The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection](https://arxiv.org/abs/2403.12166) | 提出一种利用核心子集选择进行数据重新加权的方法，有效优化了计算时间和模型性能，突显其作为模型训练的可扩展和精确解决方案的潜力。 |
| [^110] | [Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models](https://arxiv.org/abs/2403.12158) | 引入变分方法在Dirichlet混合模型中提供了封闭形式的解，显著提高了计算效率，可用于快速模型比较和稳健估计评估。 |
| [^111] | [Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification](https://arxiv.org/abs/2403.12151) | 大型语言模型与知识图谱结合，提高零样本对象状态分类性能 |
| [^112] | [Graph Neural Networks for Learning Equivariant Representations of Neural Networks](https://arxiv.org/abs/2403.12143) | 本研究提出了将神经网络表示为参数的计算图的方法，利用图神经网络和变压器来实现置换对称性，使得单个模型能够处理具有多种架构的神经计算图。 |
| [^113] | [Light Curve Classification with DistClassiPy: a new distance-based classifier](https://arxiv.org/abs/2403.12120) | 提出了一种新的基于距离度量的分类器DistClassiPy，通过比较不同类别对象之间的距离，实现了可变星的光变曲线分类，帮助增加分类结果的可解释性并减少计算成本。 |
| [^114] | [Transfer Learning for T-Cell Response Prediction](https://arxiv.org/abs/2403.12117) | 使用转换器模型进行T细胞响应预测，研究多域结构中的转移学习技术，提出领域感知评估方案。 |
| [^115] | [Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target](https://arxiv.org/abs/2403.12116) | 本研究提出了一种使用Winner-Take-All（WTA）选择性和生物启发的稳态机制相结合的“自定义目标”方法，旨在解决无监督学习方法在边缘AI硬件上的计算资源稀缺性问题。 |
| [^116] | [Deep learning automates Cobb angle measurement compared with multi-expert observers](https://arxiv.org/abs/2403.12115) | 该研究开发了深度学习软件，自动化测量脊柱侧凸的 Cobb 角度，提供清晰的可视化结果，解决了手动测量耗时且存在差异性的问题 |
| [^117] | [GCAM: Gaussian and causal-attention model of food fine-grained recognition](https://arxiv.org/abs/2403.12109) | 提出了一种采用高斯和因果关注模型进行食物细粒度识别的方法，通过训练获取目标区域上的高斯特征和从对象中提取细粒度特征来增强特征映射能力，同时采用反事实推理方法对抗数据漂移。 |
| [^118] | [Circular Belief Propagation for Approximate Probabilistic Inference](https://arxiv.org/abs/2403.12106) | 循环信念传播（CBP）是对Belief Propagation（BP）的扩展，通过学习检测和取消循环引起的消息反响来限制错误相关和信念放大的不利影响，在二元概率图上表现优于BP和先前提出的算法。 |
| [^119] | [Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation](https://arxiv.org/abs/2403.12100) | 本文引入了“移动树”数据结构，用于学习用户跨不同时间段的偏好，以提升下一个POI推荐任务的性能。 |
| [^120] | [Deep Generative Design for Mass Production](https://arxiv.org/abs/2403.12098) | 通过将与压铸和注塑相关的约束集成到生成设计中，利用二维深度图像将复杂的3D几何形状简化为可制造的轮廓，消除不可制造特性，将重点放在厚度和肋设计等制造重要方面。 |
| [^121] | [Are LLMs Good Cryptic Crossword Solvers?](https://arxiv.org/abs/2403.12094) | 本文建立了三种流行LLMs的基准结果，表明它们在难解填字游戏上的表现仍远远不及人类。 |
| [^122] | [Foundation Models and Information Retrieval in Digital Pathology](https://arxiv.org/abs/2403.12090) | 论文回顾了数字病理学中基础模型和信息检索领域的最新进展。 |
| [^123] | [The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported](https://arxiv.org/abs/2403.12082) | 通过小规模实验发现，从LLM中删除哈利波特内容比先前报道的更加困难。 |
| [^124] | [Evaluating Terrain-Dependent Performance for Martian Frost Detection in Visible Satellite Observations](https://arxiv.org/abs/2403.12080) | 研究提出了一种新颖的空间数据分区方法，揭示了地质背景对自动霜冻检测的影响，并提出了缓解观察到的偏差的方法。 |
| [^125] | [Beyond Beats: A Recipe to Song Popularity? A machine learning approach](https://arxiv.org/abs/2403.12079) | 该研究利用机器学习模型探讨预测歌曲流行度，结果显示流派是影响流行度的主要因素，同时揭示了时间趋势和特征间的复杂关系。 |
| [^126] | [Neuron-centric Hebbian Learning](https://arxiv.org/abs/2403.12076) | 提出了一种新的神经元中心的赫布学习模型，相较于传统的ABCD规则，将参数减少了从$5W$到$5N$。 |
| [^127] | [Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation](https://arxiv.org/abs/2403.12075) | 批评了文本到图像生成中模型对非明显攻击的鲁棒性，提出了Adversarial Nibbler Challenge以众包多样化的提示来纠正模型的安全问题 |
| [^128] | [Beyond Quantities: Machine Learning-based Characterization of Inequality in Infrastructure Quality Provision in Cities](https://arxiv.org/abs/2403.12074) | 通过机器学习的方法，超越了传统基于数量的基础设施不平等特征化研究，从而填补了城市不平等和环境正义考虑之间的研究空白。 |
| [^129] | [Floralens: a Deep Learning Model for the Portuguese Native Flora](https://arxiv.org/abs/2403.12072) | 本论文开发了一种用于从公开数据集构建生物分类群数据集以及利用深度卷积神经网络推导模型的简化方法，并以葡萄牙本地植物为案例研究。 |
| [^130] | [Fairness Evaluation for Uplift Modeling in the Absence of Ground Truth](https://arxiv.org/abs/2403.12069) | 提出了一个框架，通过生成替代品来充当提升建模活动的反事实标签代理，从而进行更全面的二进制公平性评估。 |
| [^131] | [Process mining for self-regulated learning assessment in e-learning](https://arxiv.org/abs/2403.12068) | 通过在电子学习过程中应用过程挖掘技术，发现了学生的自主调节学习过程模型，有助于改善教学过程。 |
| [^132] | [Adapting SAM for Volumetric X-Ray Data-sets of Arbitrary Sizes](https://arxiv.org/abs/2403.12066) | 将Segment Anything Model (SAM)与基于切片的Flood Filling Networks (FFN)相结合，实现了对体积X射线数据集中的三维对象进行分割的新方法。 |
| [^133] | [Consistency Models Improve Diffusion Inverse Solvers](https://arxiv.org/abs/2403.12063) | 本文提出了使用一致性模型作为高质量逼近，以改进对扩散逆求解器中后验样本的使用。 |
| [^134] | [Mobile Application for Oral Disease Detection using Federated Learning](https://arxiv.org/abs/2403.12044) | 该研究开发了一个名为OralH的移动应用，利用联邦学习技术进行口腔疾病检测，用户可以通过口腔扫描进行自我评估，获得快速的口腔健康建议。 |
| [^135] | [Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions](https://arxiv.org/abs/2403.12007) | 本研究旨在定义通过数字行为变化干预支持癌症患者提高生活质量的有效参与方式，发现医生处方显著增加患者对移动数字行为变化干预的持续参与，同时指出每周参与一次已足以维持福祉，但内在动机可能需要更高水平的参与。 |
| [^136] | [Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs](https://arxiv.org/abs/2403.11755) | 提出了Meta-Prompting for Visual Recognition (MPVR)方法，通过仅需少量信息即可自动化零样本识别中的提示生成过程。 |
| [^137] | [Automated data processing and feature engineering for deep learning and big data applications: a survey](https://arxiv.org/abs/2403.11395) | 现代人工智能方法旨在设计能够直接从数据中学习的算法，自动化数据处理任务的兴起驱动了机器学习和大数据应用中利用大量复杂数据的发展。 |
| [^138] | [JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning](https://arxiv.org/abs/2403.11366) | 提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。 |
| [^139] | [CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations](https://arxiv.org/abs/2403.11220) | 提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能 |
| [^140] | [Incorporating Higher-order Structural Information for Graph Clustering](https://arxiv.org/abs/2403.11087) | 该论文提出了一种新颖的图聚类网络，利用图的高阶结构信息，并设计了一个图互信息极大化模块，有效地最大化了图级和节点级表示之间的互信息。 |
| [^141] | [DTOR: Decision Tree Outlier Regressor to explain anomalies](https://arxiv.org/abs/2403.10903) | DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。 |
| [^142] | [Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability](https://arxiv.org/abs/2403.10682) | 对GlassNet模型在预测玻璃稳定性参数方面的应用进行了评估，探索了使用这些参数来估计玻璃的形成能力的可行性。 |
| [^143] | [Approximation and bounding techniques for the Fisher-Rao distances](https://arxiv.org/abs/2403.10089) | 本文考虑了几种数值上稳健的Fisher-Rao距离的近似和界定技术，包括基于闭合形式1D子模型Fisher-Rao距离的通用上界以及取决于测地线或预测测地线是否闭合形式获得的几种通用近似方案，并提出了一种通用方法保证近似误差任意小。 |
| [^144] | [Fisher Mask Nodes for Language Model Merging](https://arxiv.org/abs/2403.09891) | 介绍了一种用于Transformers的新型模型合并方法，利用Fisher信息进行加权平均，提高了多任务模型的性能。 |
| [^145] | [FoldToken: Learning Protein Language via Vector Quantization and Beyond](https://arxiv.org/abs/2403.09673) | 通过将蛋白质序列和结构表示为离散符号，并创建新的蛋白质语言，从而构建了一种用于序列-结构共生产的创新方法。 |
| [^146] | [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | 通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。 |
| [^147] | [A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning](https://arxiv.org/abs/2403.09499) | 该研究提出了一种基于Q学习的算法，以实现在奶牛养殖中整合可再生能源，以改善电池管理，应对电能消耗波动和能源价格波动的挑战。 |
| [^148] | [Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research](https://arxiv.org/abs/2403.08438) | 本文研究了图神经网络研究中的再现性和几何内在维度性问题，并引入机器学习中的再现性本体论，以及探讨了维度诅咒对数据收集、表示和分析的挑战。 |
| [^149] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^150] | [How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance](https://arxiv.org/abs/2403.07310) | 本文通过高斯混合模型量化了群体不平衡对样本复杂性、收敛速率和平均以及群体级测试性能的影响，首次提供了ERM在群体级泛化的理论分析。 |
| [^151] | [RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models](https://arxiv.org/abs/2403.06420) | RLingua提出了一个框架，利用大型语言模型的内部知识来提高机器人操作中强化学习的样本效率。 |
| [^152] | [Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization](https://arxiv.org/abs/2403.04720) | 提出了一种新的基于编码器的表格数据集表示方法，与现有方法不同，能够自动提取重要的元特征，同时在两个常见的元任务上进行了评估 |
| [^153] | [Interactive Continual Learning: Fast and Slow Thinking](https://arxiv.org/abs/2403.02628) | 本文提出了一种基于交互的持续学习框架，通过多模型之间的合作交互，实现了更好的任务推导和内存检索。 |
| [^154] | [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046) | 证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。 |
| [^155] | [Cut Facets and Cube Facets of Lifted Multicut Polytopes](https://arxiv.org/abs/2402.16814) | 本文回答了提升多段切割多面体的哪些下界立方不等式和哪些切割不等式定义面的基本问题，以及判定切割不等式定义性的NP难性。 |
| [^156] | [Cameras as Rays: Pose Estimation via Ray Diffusion](https://arxiv.org/abs/2402.14817) | 提出了一种将相机姿势视为射线束的分布表示方法，结合空间图像特征，开发了基于回归和扩散的姿势估计方法，在CO3D数据集上取得了最先进的性能。 |
| [^157] | [Big data analytics to classify earthwork-related locations: A Chengdu study](https://arxiv.org/abs/2402.14698) | 使用大数据分析方法，研究者利用自卸车轨迹、城市兴趣点和土地覆盖数据，成功对城市灰尘污染源进行了分类，证明仅需有限数量特征即可实现高准确度分类。 |
| [^158] | [Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming](https://arxiv.org/abs/2402.13224) | 本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。 |
| [^159] | [Gaussian Process Neural Additive Models](https://arxiv.org/abs/2402.12518) | 本文提出了一种新的高斯过程神经加性模型（GP-NAM），通过随机傅里叶特征对高斯过程进行单层神经网络构建，可以实现具有凸目标函数和可训练参数数量随特征维度线性增长的优势，同时在性能上不亚于更深的NAM方法。 |
| [^160] | [TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks](https://arxiv.org/abs/2402.11137) | 提出了TuneTables上下文优化技术，通过开发一种新的提示调整策略，将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。 |
| [^161] | [Towards Reducing Diagnostic Errors with Interpretable Risk Prediction](https://arxiv.org/abs/2402.10109) | 本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，旨在通过增加证据的获取与减少诊断错误来降低诊断错误。模型使用神经加性模型进行预测，以证据为后盾，并给出个体化风险估计，特别针对诊断延迟和来自不完整鉴别的错误进行优化。 |
| [^162] | [Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram](https://arxiv.org/abs/2402.09450) | 本研究提出了一种叫做ST-MEM的模型，通过重构遮蔽的心电图数据来学习时空特征，该模型在心律失常分类任务中优于其他自监督学习方法。 |
| [^163] | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | 这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。 |
| [^164] | [Tuning-Free Stochastic Optimization](https://arxiv.org/abs/2402.07793) | 本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。 |
| [^165] | [Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation](https://arxiv.org/abs/2402.05946) | 本文提出了一种基于时间点过程的方法，通过揭示潜在因果规律来解释异常事件，以帮助在高风险系统如医疗保健中快速诊断和精确治疗规划。该方法通过期望最大化算法优化规则集和模型参数，实现了准确的规则发现和根因识别。 |
| [^166] | [Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects](https://arxiv.org/abs/2402.04906) | 本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。 |
| [^167] | [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068) | 检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。 |
| [^168] | [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/abs/2402.03094) | 本文提出了一种跨领域少样本目标检测器，通过增强的开集目标检测方法来解决跨领域数据差异带来的性能下降问题。 |
| [^169] | [A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer](https://arxiv.org/abs/2402.02464) | 这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。 |
| [^170] | [Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning](https://arxiv.org/abs/2402.02334) | 本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。 |
| [^171] | [Quantum Architecture Search with Unsupervised Representation Learning](https://arxiv.org/abs/2401.11576) | 通过利用无监督表示学习，量子架构搜索（QAS）的性能可以得以提升，而不需要耗费大量时间进行标记。 |
| [^172] | [Vertical Federated Image Segmentation](https://arxiv.org/abs/2401.07931) | 提出了一种垂直联邦学习（VFL）模型架构，适用于数据分散于不同数据孤岛的场景，解决了部分地区数据无法访问标记真相，但需要进行分类的问题。 |
| [^173] | [Hypergraph-MLP: Learning on Hypergraphs without Message Passing](https://arxiv.org/abs/2312.09778) | 提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。 |
| [^174] | [MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training](https://arxiv.org/abs/2312.08656) | MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。 |
| [^175] | [OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods](https://arxiv.org/abs/2312.08255) | 该研究介绍了一个名为OCTDL的开放获取光学相干断层扫描数据集，包括超过2000张标记有疾病组和视网膜病理的OCT图像，有助于诊断眼部状况。 |
| [^176] | [GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models](https://arxiv.org/abs/2312.03675) | GeoShapley是一种衡量机器学习模型中空间效应的博弈论方法，将位置视为模型预测博弈中的一名玩家，能够量化位置的重要性并与其他特征之间的协同作用进行量化。 |
| [^177] | [On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm](https://arxiv.org/abs/2312.03526) | 我们提出了一种新颖的计算效率高且有效的数据精炼范式RDED，以实现数据的多样性和现实性。 |
| [^178] | [Learning Multi-Pattern Normalities in the Frequency Domain for Efficient Time Series Anomaly Detection](https://arxiv.org/abs/2311.16191) | MACE是一种在频域中适应多正常模式且高效的时间序列异常检测方法，其创新之处在于采用优秀的模式提取机制，使模型能够通过检查数据样本与其服务正常模式之间的相关性来识别异常。 |
| [^179] | [Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning](https://arxiv.org/abs/2311.15056) | 通过知识子图学习的方法，本研究提出了一种解决药物相互作用预测中样本稀缺性挑战的图神经网络模型，能够准确且可解释地预测药物之间的相互作用。 |
| [^180] | [DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback](https://arxiv.org/abs/2311.10081) | 提出了一种名为DRESS的大型视觉语言模型，通过利用自然语言反馈来增强模型与人类之间的对齐和互动，解决了当前大型视觉-语言模型存在的对齐和多轮对话互动方面的两个关键问题 |
| [^181] | [Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data](https://arxiv.org/abs/2311.04829) | 提出了一种名为功能贝叶斯 Tucker 分解（FunBaT）的方法，用于将 Tucker 分解推广到连续索引的张量数据，利用高斯过程模型潜在函数。 |
| [^182] | [Solving High Frequency and Multi-Scale PDEs with Gaussian Processes](https://arxiv.org/abs/2311.04465) | 使用高斯过程框架来解决高频和多尺度偏微分方程，通过灵活捕捉主导频率的方法，估计混合权重并自动诱导稀疏性，有效解决了神经网络训练中的谱偏差问题。 |
| [^183] | [Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs](https://arxiv.org/abs/2311.04417) | 本研究提供了对这些商用AI/ML加速器的初步评估和比较，深入探讨它们的硬件和软件设计特点，以辨别它们的创新数据流架构和其他设计优化，承诺为AI/ML任务提供卓越性能和能量效率。 |
| [^184] | [TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications](https://arxiv.org/abs/2311.02971) | TabRepo引入了一个包含1310个模型在200个分类和回归数据集上评估预测结果和指标信息的数据集，展示了它在进行超参数优化、AutoML系统比较和迁移学习等方面的优势。 |
| [^185] | [One-Shot Strategic Classification Under Unknown Costs](https://arxiv.org/abs/2311.02761) | 本研究首次研究了在未知响应下一次性策略分类的情景，针对用户成本函数不确定性，提出解决方案并将任务定义为极小-极大问题。 |
| [^186] | [Pointer Networks with Q-Learning for OP Combinatorial Optimization](https://arxiv.org/abs/2311.02629) | 提出了Pointer Q-Network (PQN)方法，将Ptr-Nets和Q-learning相结合，利用其批评者性质，出色地捕获了嵌入图中的关系，从而有效解决了OP组合优化中的具体挑战 |
| [^187] | [Differentiable Euler Characteristic Transforms for Shape Classification](https://arxiv.org/abs/2310.07630) | 提出了一种不同iable Euler Characteristic Transform（DECT）计算层，能够实现端到端学习ECT，展现出与更复杂模型相当的性能。 |
| [^188] | [Light Schr\"odinger Bridge](https://arxiv.org/abs/2310.01174) | 提出了一种新颖的光谢尔宾格桥快速简单求解器，将谢尔宾格势能参数化为总和-指数二次函数，视为能量函数，实现了轻量级、无需模拟、理论上合理的求解器设计。 |
| [^189] | [Language Modeling Is Compression](https://arxiv.org/abs/2309.10668) | 大型语言模型被证明是强大的压缩器，压缩视角为扩展定律、标记化和上下文学习提供了新的见解。 |
| [^190] | [EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2308.07269) | EasyEdit提出了一种易于使用的知识编辑框架，针对大型语言模型的知识截断或谬误问题，支持各种最新的知识编辑方法，并可应用于多个知名的LLMs。 |
| [^191] | [Information decomposition in complex systems via machine learning](https://arxiv.org/abs/2307.04755) | 通过机器学习提出了一种实用且通用的方法，使用分布式信息瓶颈作为学习目标，来进行复杂系统中信息的分解 |
| [^192] | [Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation](https://arxiv.org/abs/2306.02960) | 提出了一种新颖的SNN-ANN混合架构，用于克服SNN在处理事件数据方面面临的挑战，以实现事件驱动的光流估计。 |
| [^193] | [Cross or Wait? Predicting Pedestrian Interaction Outcomes at Unsignalized Crossings](https://arxiv.org/abs/2304.08260) | 本文利用机器学习预测行人在无信号交叉口与车辆互动时的过马路行为，提出的神经网络模型在预测准确性和F1分数上取得了显著的改进。 |
| [^194] | [Oracle-Efficient Smoothed Online Learning for Piecewise Continuous Decision Making](https://arxiv.org/abs/2302.05430) | 引入广义括号数的概念，结合对手的约束与空间大小，通过Follow-the-Perturbed-Leader算法实现低遗憾，优化调用优化Oracle的次数以实现遗憾在多个问题中的有效应用。 |
| [^195] | [Toward a Theory of Causation for Interpreting Neural Code Models](https://arxiv.org/abs/2302.03788) | 该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。 |
| [^196] | [Smoothed Online Learning for Prediction in Piecewise Affine Systems](https://arxiv.org/abs/2301.11187) | 本文提出了基于平滑在线学习框架的算法，可以有效处理分段仿射系统中的预测和模拟问题，在弱光滑性假设下具有多项式遗憾度，并且在调用优化预测次数方面是高效的。 |
| [^197] | [When Layers Play the Lottery, all Tickets Win at Initialization](https://arxiv.org/abs/2301.10835) | 本文从层次剪枝的角度研究了LTH和初始化时的剪枝，在初始化阶段发现了获胜彩票的存在，从而消除了训练稠密网络的大量计算资源需求。 |
| [^198] | [Deep Recurrent Learning Through Long Short Term Memory and TOPSIS](https://arxiv.org/abs/2301.00693) | 该论文提出了一种基于长短期记忆（LSTM）和TOPSIS的分类算法，用于识别和排名云ERP的采用特征。 |
| [^199] | [Behave-XAI: Deep Explainable Learning of Behavioral Representational Data](https://arxiv.org/abs/2301.00016) | 本研究提出了Behave-XAI，通过深度卷积神经网络架构解决行为挖掘问题，并应用递归神经网络处理用户生理时间序列数据，实现了行为表征数据的深度可解释学习。 |
| [^200] | [Social-Aware Clustered Federated Learning with Customized Privacy Preservation](https://arxiv.org/abs/2212.13992) | 通过利用用户之间的社交关系，提出了SCFL，一种具有定制隐私保护的社交感知聚类联邦学习方案，实现了数据隐私和效率之间的平衡。 |
| [^201] | [Non-Coherent Over-the-Air Decentralized Gradient Descent](https://arxiv.org/abs/2211.10777) | 提出了一种适用于无线系统的DGD算法，通过无相干空中共识方案实现无需智能体协调、拓扑信息或信道状态信息的分布式优化。 |
| [^202] | [SFPDML: Securer and Faster Privacy-Preserving Distributed Machine Learning based on MKTFHE](https://arxiv.org/abs/2211.09353) | 基于MKTFHE，本文提出了一种更安全和更高效的隐私保护分布式机器学习方法，通过引入秘密分享和设计新的激活函数，解决了现有解密协议的安全风险和非线性计算问题。 |
| [^203] | [Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach](https://arxiv.org/abs/2210.12624) | 提出了一种随机多目标梯度校正（MoCo）方法，能够在不增加批量大小的情况下保证收敛，解决了多目标学习中梯度偏差导致性能下降的问题。 |
| [^204] | [Riemannian Stochastic Gradient Method for Nested Composition Optimization](https://arxiv.org/abs/2207.09350) | 提出了一种适用于嵌套函数组合的Riemannian随机梯度优化方法，可在有限次调用随机梯度oracle找到近似稳定点。 |
| [^205] | [Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization](https://arxiv.org/abs/2205.07473) | 本研究提出了一种双阶段转换算法，能够最小化量化误差、剪切误差和残余膜电位表示误差，实现了损失less的ANN-SNN转换，提高了在超低延迟条件下的性能。 |
| [^206] | [Bayesian Nonparametrics meets Data-Driven Robust Optimization.](http://arxiv.org/abs/2401.15771) | 本文提出了一种将贝叶斯非参数方法与最新的决策理论模型相结合的鲁棒优化准则，通过这种方法，可以在线性回归问题中获得有稳定性和优越性能的结果。 |
| [^207] | [Divide and not forget: Ensemble of selectively trained experts in Continual Learning.](http://arxiv.org/abs/2401.10191) | 连续学习中，我们提出了一种名为SEED的新方法，通过选择性训练最优的专家来解决遗忘和计算负担的问题，并在实验中展示了其高性能。 |
| [^208] | [Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling.](http://arxiv.org/abs/2401.09516) | 该论文提出了一种名为排序克里洛夫回收（SKR）的新方法，用于加速神经算子训练的数据生成。该方法解决了现有方法在解决PDE问题时计算冗余的问题，显著提高了数据生成效率。 |
| [^209] | [Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling.](http://arxiv.org/abs/2401.08876) | 本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。 |
| [^210] | [Probabilistic Modeling for Sequences of Sets in Continuous-Time.](http://arxiv.org/abs/2312.15045) | 本文提出了一个通用的连续时间序列集合的概率建模框架，适用于处理每个事件与一组项目相关联的情况。引入了适用于任何强度为基础的递归神经点过程模型的推理方法，可用于回答关于序列历史条件下的概率查询问题。 |
| [^211] | [Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks.](http://arxiv.org/abs/2312.12784) | 提出了一种基于图神经网络的快速准确芯片库特征化的机器学习模型，通过结合芯片结构，在各种工艺参数下预测精度高，并且相较于传统方法具有100倍的加速。 |
| [^212] | [Stronger Graph Transformer with Regularized Attention Scores.](http://arxiv.org/abs/2312.11730) | 本论文提出了一种新颖的边缘正则化技术版本，用于缓解图神经网络在内存问题上存在的困扰。与没有位置编码的Graph Transformer相比，应用了边缘正则化技术确实可以稳定地提高性能。 |
| [^213] | [TrojFST: Embedding Trojans in Few-shot Prompt Tuning.](http://arxiv.org/abs/2312.10467) | TrojFST是一种在少样本提示调优框架中进行后门攻击的方法，通过引入平衡的污染学习、选择性令牌污染和...等模块来解决构建基于提示的后门的困难。 |
| [^214] | [MCRAGE: Synthetic Healthcare Data for Fairness.](http://arxiv.org/abs/2310.18430) | MCRAGE是一种使用深度生成模型来增强不平衡的医疗数据集的方法，以解决少数群体在机器学习模型中的不公平问题。 |
| [^215] | [Transductive conformal inference with adaptive scores.](http://arxiv.org/abs/2310.18108) | 利用转导式一致推断方法进行一致性无分布保证的机器学习任务，并通过建立Polya球模型的联合分布和经验分布函数的浓度不等式，提供了自适应得分的可用性和更高准确性。 |
| [^216] | [TiC-CLIP: Continual Training of CLIP Models.](http://arxiv.org/abs/2310.16226) | 该论文提出了用于训练视觉-语言模型的大规模时间连续 (TiC) 基准，使用这些基准评估了现有模型的时间鲁棒性，并展示了一种简单有效的排练方法来持续训练模型。 |
| [^217] | [Learning Successor Representations with Distributed Hebbian Temporal Memory.](http://arxiv.org/abs/2310.13391) | 本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。 |
| [^218] | [PGA: Personalizing Grasping Agents with Single Human-Robot Interaction.](http://arxiv.org/abs/2310.12547) | 这项研究介绍了个性化抓取代理（PGA），它通过单一人机交互学习并定位和抓取个人物体。PGA通过用户提供的信息和用户环境中的原始图像，实现个性化物体抓取。 |
| [^219] | [The Relative Gaussian Mechanism and its Application to Private Gradient Descent.](http://arxiv.org/abs/2308.15250) | 本文介绍了相对高斯机制(RGM)，它利用了相对L2敏感性假设，在保护隐私的同时能够更精确地界定隐私损失。 |
| [^220] | [Warped geometric information on the optimisation of Euclidean functions.](http://arxiv.org/abs/2308.08305) | 使用扭曲几何学的概念，我们提出了一种在高维欧几里德空间中优化函数的方法，并通过在重新定义的黎曼流形上进行计算，找到了函数的最优解。 |
| [^221] | [A Pre-trained Data Deduplication Model based on Active Learning.](http://arxiv.org/abs/2308.00721) | 提出了一种基于主动学习的预训练去重模型，将Transformer和主动学习集成到端到端架构中，首次解决了语义级别的去重问题，同时采用R-Drop方法对每一轮标记数据进行数据增强。通过选择最有价值的数据进行去重模型训练，不仅降低了手动标记的成本，还提高了模型的泛化能力。 |
| [^222] | [FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning.](http://arxiv.org/abs/2307.13716) | FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。 |
| [^223] | [On-the-fly machine learning for parametrization of the effective Hamiltonian.](http://arxiv.org/abs/2307.08929) | 本论文提出了一种基于贝叶斯线性回归的即时机器学习方法，用于参数化有效哈密顿量。该方法在各种系统中都可以得到准确的结果，包括以前的方法无法处理的复杂系统。 |
| [^224] | [Layerwise Linear Mode Connectivity.](http://arxiv.org/abs/2307.06966) | 本文提出了一种分层线性模态连接方法用于联邦深度学习，通过解决模型漂移和高损失障壁的问题，能够有效提升全局模型的性能。 |
| [^225] | [T-MARS: Improving Visual Representations by Circumventing Text Feature Learning.](http://arxiv.org/abs/2307.03132) | T-MARS提出一种新的数据筛选方法，通过规避文本特征学习，改善了视觉表示的学习，解决了大型多模态数据集中存在的文本与图像重叠的问题。 |
| [^226] | [Adversarial Training Should Be Cast as a Non-Zero-Sum Game.](http://arxiv.org/abs/2306.11035) | 本论文提出了一种新的针对对抗性训练的非零和双层公式，实现了与最先进攻击相匹配并且能够达到与标准对抗性训练相同的鲁棒性水平。 |
| [^227] | [Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation.](http://arxiv.org/abs/2306.06192) | Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。 |
| [^228] | [Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise.](http://arxiv.org/abs/2306.00673) | 本文提出一种新算法，可以属性高效的学习低次多项式阈值函数，并能够在噪声下进行PAC学习。 |
| [^229] | [The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold.](http://arxiv.org/abs/2305.01604) | 本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。 |
| [^230] | [PopulAtion Parameter Averaging (PAPA).](http://arxiv.org/abs/2304.03094) | 提出一种新方法PopulAtion Parameter Averaging (PAPA)，能同时拥有集成的普遍性与权重平均的效率，可以显著提高模型性能。 |
| [^231] | [On student-teacher deviations in distillation: does it pay to disobey?.](http://arxiv.org/abs/2301.12923) | 通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。 |
| [^232] | [Copula Conformal Prediction for Multi-step Time Series Forecasting.](http://arxiv.org/abs/2212.03281) | 本文提出了一种 Copula 联合预测算法 CopulaCPTS，用于多元、多步时间序列预测，经过实验验证，其置信区间比现有技术更精准和更锐利。 |
| [^233] | [Improving Human Sequential Decision-Making with Reinforcement Learning.](http://arxiv.org/abs/2108.08454) | 该论文提出了一种利用机器学习算法从追踪数据中提取"最佳实践"并向人类传达的方法，以改进人类的顺序决策。通过一系列实验证实，在虚拟厨房管理任务中，这种方法能够显著提高性能。 |

# 详细

[^1]: LLMLingua-2: 高效且忠实的无任务Prompt压缩的数据精炼

    LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression

    [https://arxiv.org/abs/2403.12968](https://arxiv.org/abs/2403.12968)

    该论文提出了一种数据精炼的方法，通过从LLM中提取知识来实现Prompt的压缩，确保压缩后的提示保持对原始提示的忠实性。

    

    这篇论文关注于无任务的Prompt压缩，以提高泛化能力和效率。考虑到自然语言中的冗余性，现有方法通过根据从因果语言模型（如LLaMa-7B）获得的信息熵来删除token或词汇单位来压缩prompt。挑战在于信息熵可能是一个次优的压缩度量：(i)它仅利用单向上下文，可能无法捕获所有用于prompt压缩的关键信息；(ii)它与prompt压缩目标不一致。为了解决这些问题，我们提出了一种数据精炼过程，从LLM中获得知识以压缩prompt而不丢失关键信息，并同时引入了一个抽取式文本压缩数据集。我们将prompt压缩格式化为一个token分类问题，以确保压缩后的prompt与原始prompt的一致性。

    arXiv:2403.12968v1 Announce Type: new  Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and 
    
[^2]: TexTile：一种可微的纹理平铺度量

    TexTile: A Differentiable Metric for Texture Tileability

    [https://arxiv.org/abs/2403.12961](https://arxiv.org/abs/2403.12961)

    TexTile是一种不同iable的度量方法，用于评估纹理图像的平铺性能，可以帮助更明智地合成和分析平铺纹理。

    

    我们引入了TexTile，一种新颖的可微度量方式，用于量化纹理图像可以如何与自身连接而不引入重复伪影（即平铺性）。现有的可平铺纹理合成方法侧重于一般纹理质量，但缺乏对纹理固有可重复性属性的显式分析。相反，我们的TexTile度量有效评估纹理的可平铺性，为更明智的平铺纹理合成和分析打开了大门。在背后，TexTile被构建为一个二元分类器，仔细构建自不同风格、语义、规律和人类注释的大型纹理数据集。我们方法的关键在于一系列架构修改，使基线预训练图像分类器能够克服其在衡量平铺性方面的缺陷，以及针对增加鲁棒性的自定义数据增强和训练方案。

    arXiv:2403.12961v1 Announce Type: cross  Abstract: We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing rob
    
[^3]: WHAC: 世界基准人类与摄像机

    WHAC: World-grounded Humans and Cameras

    [https://arxiv.org/abs/2403.12959](https://arxiv.org/abs/2403.12959)

    该研究提出了一个名为WHAC的框架，可以实现基于世界坐标系的丰富人体姿势和形状估计，同时进行摄像机姿势估计，无需依赖传统优化技术。

    

    从单目视频中准确估计世界坐标系中具有比例的人类和摄像机轨迹是一项非常理想但具有挑战性和模糊定义的问题。在本研究中，我们旨在通过利用世界、人类和摄像机三个关键要素之间的协同作用，联合恢复表现力强的参数化人体模型（即SMPL-X）和相应的摄像机姿势。我们的方法基于两个关键观察。首先，相机框架下的SMPL-X估计方法可以轻松恢复绝对人类深度。其次，人体动作本质上提供绝对空间线索。通过整合这些见解，我们引入了一个名为WHAC的新颖框架，以促进基于世界的表现力强的人体姿势和形状估计（EHPS），同时进行摄像机姿势估计，无需依赖传统的优化技术。此外，我们提出了一个新的合成数据集WHAC-A-Mole，其中包含了精确注释的信息。

    arXiv:2403.12959v1 Announce Type: cross  Abstract: Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated h
    
[^4]: 只需转移它：测试时间原型转移用于视觉语言模型的零样本泛化

    Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models

    [https://arxiv.org/abs/2403.12952](https://arxiv.org/abs/2403.12952)

    引入了测试时间原型转移（TPS）框架，通过动态学习每个原型的转移向量，有效地弥合了领域差距并增强了类

    

    视觉语言模型（VLMs）的进展推动了计算机视觉领域的发展，特别是在零样本学习设置中。尽管它们很有前景，但这些模型的有效性在测试环境中往往会因为领域转移而降低。为了解决这个问题，我们引入了测试时间原型转移（TPS）框架，这是一种旨在使用标记测试输入来使VLM适应测试数据集的开创性方法。我们的方法基于在共享嵌入空间中调节每个类别的原型的概念。通过使用预先训练的文本编码器生成并缓存原型，TPS不仅促进了无需优化的原型重用进行后续预测，还让其能够无缝集成当前进展的提示工程技术。在测试时间，TPS仅基于给定的测试样本动态学习每个原型的转移向量，有效地弥合领域差距并增强类

    arXiv:2403.12952v1 Announce Type: cross  Abstract: Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing class
    
[^5]: 优化的自适应非平稳对抗性多臂老虎机在广义波达准则下

    Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion

    [https://arxiv.org/abs/2403.12950](https://arxiv.org/abs/2403.12950)

    该研究建立了首个在对抗性多臂老虎机中优化且自适应的波达动态遗憾上界，揭示了在Condorcet和Borda之间严重非平稳性可学习性的基本差异

    

    在对抗性多臂老虎机中，学习者接收臂之间的偏好反馈，并将某个臂的遗憾定义为其相对于优胜臂的次优性。更具挑战性和实践动机的非平稳对抗性多臂老虎机变体，在这种变体中，偏好随时间变化，已经成为近期多项工作的焦点。目标是设计出算法，而无需提前了解变化量。已知结果的大部分研究了孔多塞优胜者设置，其中优先于其他任何臂的臂在任何时候都存在。然而，这样的优胜者可能并不存在，为了对比，此问题的波达版本（始终有明确定义）却受到了很少关注。在这项工作中，我们建立了第一个最优和自适应的波达动态遗憾上界，突显了在孔多塞和波达之间的严重非平稳性可学习性的基本差异。

    arXiv:2403.12950v1 Announce Type: new  Abstract: In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorce
    
[^6]: 关于安全的安全贝叶斯优化

    On Safety in Safe Bayesian Optimization

    [https://arxiv.org/abs/2403.12948](https://arxiv.org/abs/2403.12948)

    本研究探讨了安全贝叶斯优化中的安全性问题，并提出了Real-\b{eta}-SafeOpt算法，有效保留了所有理论保证。

    

    优化未知函数在安全约束下是机器人学、生物医学工程和许多其他学科中的中心任务，安全贝叶斯优化(Safe Bayesian Optimization, BO)在这方面被越来越广泛地应用。由于这些应用的安全关键性质，保证这些算法的理论安全性能能够映射到现实世界中变得至关重要。在这项工作中，我们研究了流行类别SafeOpt类型算法的三个与安全相关的问题。首先，这些算法关键依赖于高斯过程 (Gaussian Process, GP) 回归的频率不确定性界限，但具体实现通常使用使所有安全保证无效的启发式方法。我们对这个问题进行了详细分析，并引入了Real-\b{eta}-SafeOpt，这是SafeOpt算法的一种变体，利用最近的GP上界，因此保留了所有理论保证。其次，我们确定了在复制.

    arXiv:2403.12948v1 Announce Type: new  Abstract: Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing k
    
[^7]: 线性马尔可夫决策过程的离线分布鲁棒性样本复杂度

    Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes

    [https://arxiv.org/abs/2403.12946](https://arxiv.org/abs/2403.12946)

    本文研究了离线强化学习中线性马尔可夫决策过程的分布鲁棒性样本复杂度问题，提出了一种悲观模型算法并建立了其样本复杂性界限，能在高维状态-动作空间中提高学习策略的性能。

    

    在离线强化学习（RL）中，缺乏积极探索需要关注模型的鲁棒性，以解决模拟和部署环境之间的差距，其中模拟和实际环境之间的差异可能严重损害学习策略的性能。为了以样本高效的方式赋予学习策略在高维状态-动作空间中的鲁棒性，本文考虑使用离线数据，通过总变差距离表征的不确定性集合，分布鲁棒线性马尔可夫决策过程（MDPs）的样本复杂性。我们开发了一种悲观模型算法，并在最小数据覆盖假设下建立了其样本复杂性界限，其性能至少比以前的方法优于$\tilde{O}(d)$，其中$d$是特征维度。

    arXiv:2403.12946v1 Announce Type: new  Abstract: In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed varia
    
[^8]: 神经微分代数方程

    Neural Differential Algebraic Equations

    [https://arxiv.org/abs/2403.12938](https://arxiv.org/abs/2403.12938)

    提出了神经微分代数方程（NDAEs）用于数据驱动的建模，展示了其在系统理论数据驱动建模任务中的适用性，并通过具体示例表明了其在噪声和外部干扰下的鲁棒性。

    

    微分代数方程（DAEs）描述了符合微分和代数约束的系统的时间演化。特别感兴趣的是包含其组件之间隐性关系（如守恒关系）的系统。在这里，我们提出适用于基于数据的DAE建模的神经微分代数方程（NDAEs）。这一方法建立在通用微分方程的概念之上；即，构建为受特定科学领域理论支持的一组神经常微分方程的模型。在这项工作中，我们展示了所提出的NDAEs抽象适用于相关系统理论数据驱动的建模任务。所示示例包括（i）油箱流形动态的逆问题和（ii）泵、油箱和管道网络的差异建模。我们的实验表明了所提方法对噪声和外部干扰的稳健性。

    arXiv:2403.12938v1 Announce Type: new  Abstract: Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extr
    
[^9]: 在低资源文本上进行预训练语言模型的可泛化和稳定微调

    Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts

    [https://arxiv.org/abs/2403.12918](https://arxiv.org/abs/2403.12918)

    提出了一种基于注意力引导的权重混合正则化方法，用于解决低资源文本上预训练语言模型微调时的稳定性和泛化能力问题

    

    预训练语言模型（PLMs）在自然语言处理（NLP）任务中取得了显著进展，但在低资源数据集上微调PLMs会面临诸如不稳定性和过拟合等重大挑战。先前的方法通过在下游任务上微调策略选择的子网络，同时保持其余权重固定为预训练权重来解决这些问题。然而，它们依赖于次优的子网络选择标准，导致次优解决方案。为了解决这些限制，我们提出了一种基于注意力引导的权重混合正则化方法，用于微调PLMs。我们的方法将每个网络权重表示为任务特定权重和预训练权重的混合，由可学习的注意力参数控制，提供对子网络选择的更精细控制。此外，我们在训练数据集的两个单独拆分上使用基于双层优化（BLO）的框架，进一步改进

    arXiv:2403.12918v1 Announce Type: cross  Abstract: Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving ge
    
[^10]: 对您的机器人大喊：从语言纠正中实时改进

    Yell At Your Robot: Improving On-the-Fly from Language Corrections

    [https://arxiv.org/abs/2403.12910](https://arxiv.org/abs/2403.12910)

    该论文发现，通过人类提供的语言纠正，可以帮助机器人持续改进长久任务表现

    

    合并语言和低级控制的分层策略已经被证明可以实现令人印象深刻的长视野机器人任务，通过利用预训练语言和视觉-语言模型（LLMs/VLMs）或在注释的机器人演示上训练的模型。但是，对于复杂和灵巧的技能，实现在长视野任务上高成功率仍然是一个主要挑战——任务越长，某个阶段失败的可能性就越大。人类可以通过直观自然的反馈帮助机器人持续改进其长视野任务性能吗？本文中我们发现：可以通过富含表达力的低级语言条件技能索引到高水平策略，并能够通过语言纠正的形式进行人类监督。我们表明，甚至精细的纠正，如小动作（“移动

    arXiv:2403.12910v1 Announce Type: cross  Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements ("move a
    
[^11]: 朝向可持续的GenAI：使用生成指令实现碳友好的大型语言模型推断

    Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference

    [https://arxiv.org/abs/2403.12900](https://arxiv.org/abs/2403.12900)

    本文提出了Sprout框架，通过引入生成指令的概念，平衡了生态可持续性和高质量生成结果之间的需求，实现了大型语言模型推断服务碳排放的显著减少。

    

    通过广泛应用的生成人工智能（GenAI）的迅速发展，引起了环境方面的重要关注，尤其是来自云和高性能计算基础设施的碳排放。本文提出了Sprout，一个创新的框架，旨在通过减少生成式大型语言模型（LLM）推断服务的碳足迹来解决这些问题。Sprout利用创新概念“生成指令”来引导自回归生成过程，从而增强碳效率。我们提出的方法精心平衡了对生态可持续性和高质量生成成果的需求。通过使用一个指令优化器来对用户提示进行生成指令的战略分配和一个原创的离线质量评估器，Sprout在实际评估中显著减少了40%以上的碳排放。

    arXiv:2403.12900v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of "generation directives" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations u
    
[^12]: 理解具有条件最优运输的无限深度和宽度ResNets的训练

    Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport

    [https://arxiv.org/abs/2403.12887](https://arxiv.org/abs/2403.12887)

    该研究通过研究无限深度和任意宽度的ResNet的“均场”模型，探讨了深度神经网络训练过程中的梯度流收敛性，以更好地理解简单优化算法如何成功解决这一具有挑战性的优化问题。

    

    我们研究了深度神经网络训练的梯度流的收敛性。 如果残差神经网络是非常深的架构的一个常见例子，那么由于目标的非凸性和非强凸性，它们的训练构成了一个具有挑战性的优化问题。 Yet, 在应用中，这些任务可以通过诸如梯度下降等简单的优化算法成功解决。 为了更好地理解这一现象，我们在这里专注于一个无限深度和任意宽度的ResNet的“均场”模型，其参数由层和参数的乘积集上的概率测度参数化，并在层集上具有常数边际。 实际上，在浅层神经网络的情况下，均场模型已被证明在用梯度流训练概率测度集上的Wasserstein度量时受益于简化的损失景观和良好的理论保证。 受这种方法的启发。

    arXiv:2403.12887v1 Announce Type: new  Abstract: We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, 
    
[^13]: 基于数据传输约束的短期太阳辐照预测

    Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints

    [https://arxiv.org/abs/2403.12873](https://arxiv.org/abs/2403.12873)

    提出了一种使用天空摄像头图像和噪声输入改善太阳辐照预测准确性的数据节约机器学习模型。

    

    我们报道了一种用于短期太阳辐照预测的数据节约的机器学习模型。该模型的输入包括天空摄像头图像，这些图像经过处理以满足数据传输约束。输出的辐照值经过转换，以关注未知的短期动态。受控制理论启发，通过使用噪声输入来反映未测量的变量，并显示这种方法能够显著改善模型预测结果。使用来自NREL太阳辐射研究实验室五年的数据创建了三组滚动训练验证集，并确定了时间的最佳表示形式、输入测量的最佳跨度以及最重要的模型输入数据（特征）。对于所选的测试数据，该模型与使用云量模型进行持续性比对，取得了平均绝对误差为74.34 $W/m^2$ 的成绩，而基准为134.35 $W/m^2$。

    arXiv:2403.12873v1 Announce Type: new  Abstract: We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance. The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints. The output irradiance values are transformed to focus on unknown short-term dynamics. Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably. Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features). For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model.
    
[^14]: 利用迁移学习优化野火危险预测

    Wildfire danger prediction optimization with transfer learning

    [https://arxiv.org/abs/2403.12871](https://arxiv.org/abs/2403.12871)

    本研究利用迁移学习优化了野火危险预测，整合了CNNs和加拿大火灾天气指数，成功建立了一种计算野火风险级别的方法，并在识别烧毁区域方面取得了95%的准确率。

    

    卷积神经网络（CNNs）已被证明在各种计算机科学领域中发挥重要作用，推动了目标检测、分类和异常检测方面的进展。本文探讨了将CNNs应用于分析地理空间数据，特别是用于识别受野火影响的区域。利用迁移学习技术，我们调整了CNN的超参数，并集成了加拿大火灾天气指数（FWI）来评估水分条件。该研究建立了一种方法，用于计算从0到5的野火风险级别，动态地与气象模式联系起来。值得注意的是，通过整合迁移学习，CNN模型在识别烧毁区域方面达到了95%的令人印象深刻的准确率。这项研究揭示了CNN的内在工作原理，以及它们在预测和减轻野火方面的实用性。通过结合迁移学习和CNN，本研究为野火危险预测贡献了一个强大的方法。

    arXiv:2403.12871v1 Announce Type: new  Abstract: Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection. This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas. Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions. The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns. Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\% in identifying burnt areas. This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires. By combining transfer learning and CNNs, this study contributes a robust appr
    
[^15]: 太空船异常检测的深度学习架构比较

    A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection

    [https://arxiv.org/abs/2403.12864](https://arxiv.org/abs/2403.12864)

    本研究比较了不同深度学习架构在太空船数据异常检测中的有效性。

    

    太空船操作具有极高的关键性，要求具有无可挑剔的可靠性和安全性。确保太空船的最佳性能需要及早检测和减轻异常情况，否则可能导致部件或任务失败。随着深度学习的出现，人们对利用这些复杂算法在空间操作中进行异常检测表现出了较大兴趣。本研究旨在比较各种深度学习架构在太空船数据异常检测中的有效性。正在研究的深度学习模型包括卷积神经网络（CNNs）、递归神经网络（RNNs）、长短期记忆（LSTM）网络以及基于Transformer的架构。这些模型中的每一个都是使用来自多个太空船任务的全面数据集进行训练和验证的，包括各种运行场景和异常类型。初步结果表明，虽然…

    arXiv:2403.12864v1 Announce Type: new  Abstract: Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while 
    
[^16]: D-Cubed：用于灵巧可变形操作的潜在扩散轨迹优化

    D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation

    [https://arxiv.org/abs/2403.12861](https://arxiv.org/abs/2403.12861)

    提出了一种名为D-Cubed的新型轨迹优化方法，利用潜在扩散模型从玩耍数据集中训练，并引入了一种新颖的无梯度引导采样方法，可解决灵巧可变形物体操作任务。

    

    精通可变形物体的灵巧机器人操作对于克服并行夹具在实际应用中的局限性至关重要。本文提出了一种名为D-Cubed的新型轨迹优化方法，利用从任务不可知的玩耍数据集训练的潜在扩散模型(LDM)来解决灵巧可变形物体操作任务。D-Cubed学习了一个技能潜在空间，使用VAE在玩耍数据集中编码短视野动作，并训练了一个LDM将技能潜在组合成技能轨迹，代表数据集中的长视野动作轨迹。为了优化目标任务的轨迹，我们引入了一种新颖的无梯度引导采样方法，利用反向扩散过程中的交叉熵方法。

    arXiv:2403.12861v1 Announce Type: cross  Abstract: Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications. Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. I
    
[^17]: 具有函数约束的变分不等式问题的原始方法

    Primal Methods for Variational Inequality Problems with Functional Constraints

    [https://arxiv.org/abs/2403.12859](https://arxiv.org/abs/2403.12859)

    本文提出了一种简单的原始方法，称为约束梯度方法（CGM），用于解决具有多个功能约束的变分不等式问题。

    

    约束变分不等式问题因其在包括机器学习和运筹学在内的各个领域的广泛应用而备受认可。 首次方法已成为解决这些问题的标准方法，因其简单性和可扩展性而受到重视。 传统上，它们通常依赖于投影或线性最小化展开器来导航可行集，但在实践中，这会在具有多个功能约束的情况下变得计算昂贵。 解决这些功能约束变分不等式问题的现有努力主要集中在基于Lagrange函数的原始-对偶算法上。 这些算法及其理论分析通常需要存在并且事先了解最佳拉格朗日乘数。 本文中，我们提出了一个简单的原始方法，称为约束梯度方法（CGM），用于处理功能约束的变分不等式问题。

    arXiv:2403.12859v1 Announce Type: cross  Abstract: Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequa
    
[^18]: 基于地图的路径规划中的等变集合和正则化的强化学习

    Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning

    [https://arxiv.org/abs/2403.12856](https://arxiv.org/abs/2403.12856)

    本文提出了一种无需专门神经网络组件的等变策略和不变值函数构建方法，在基于地图的路径规划中展示了等变集合和正则化如何提高样本效率和性能

    

    在强化学习（RL）中，利用环境的对称性可以显著增强效率、鲁棒性和性能。然而，确保深度RL策略和值网络分别是等变和不变的以利用这些对称性是一个重大挑战。相关工作尝试通过构造具有等变性和不变性的网络来设计，这限制了它们只能使用非常受限的组件库，进而阻碍了网络的表现能力。本文提出了一种构建等变策略和不变值函数的方法，而无需专门的神经网络组件，我们将其称为等变集合。我们进一步添加了一个正则化项，用于在训练过程中增加归纳偏差。在基于地图的路径规划案例研究中，我们展示了等变集合和正则化如何有益于样本效率和性能。

    arXiv:2403.12856v1 Announce Type: new  Abstract: In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.
    
[^19]: 安全强化学习中的策略分叉

    Policy Bifurcation in Safe Reinforcement Learning

    [https://arxiv.org/abs/2403.12847](https://arxiv.org/abs/2403.12847)

    我们的研究发现在安全强化学习中可能存在策略分叉现象，提出了拓扑分析以证明在一些情景下，策略需要具有不连续性或多值性，这对应于障碍物自由状态空间为非单连通时需要策略分叉的情况。

    

    安全强化学习为受限最优控制问题提供了先进的解决方案。现有的安全强化学习研究隐含地假设策略函数具有连续性，即策略以平稳、连续的方式将状态映射到动作；然而，我们的研究发现在某些情况下，可行策略应该是不连续或多值的，而在不连续的局部极小值之间插值可能会不可避免地导致约束违规。我们是第一个识别出这种现象生成机制的研究，并采用拓扑分析严谨地证明了安全强化学习中策略分叉的存在，这对应于可达元组的可收缩性。我们的定理揭示了在障碍物自由状态空间为非单连通的情景中，需要策略分叉，意味着其输出动作需要迅速响应状态的变化。

    arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu
    
[^20]: MELTing point: 移动语言转换器的评估

    MELTing point: Mobile Evaluation of Language Transformers

    [https://arxiv.org/abs/2403.12844](https://arxiv.org/abs/2403.12844)

    该研究对移动设备上大型语言模型（LLMs）的执行进行了首次系统性研究，并创建了自动化基础架构MELT来支持其评估和性能测试。

    

    Transformers已经彻底改变了机器学习领域，逐渐应用于日常任务，赋予我们的计算机“智能的火花”。然而，它们的运行时需求阻碍了它们在移动设备上的广泛部署。在个人设备变得越来越强大，以及迅速隐私问题变得更加紧迫的情况下，我们探讨了大型语言模型（LLMs）在移动设备上执行的现状。为了实现这一目标，我们创建了自己的自动化基础架构MELT，支持在设备上无界面执行和评估LLMs，并支持不同的模型、设备和框架，包括Android、iOS和Nvidia Jetson设备。我们评估了流行的指令微调的LLMs，并利用不同的框架来测量它们的端到端和细粒度性能，跟踪它们的内存和能耗需求。

    arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o
    
[^21]: 机器近似遗忘已经得到适当评估了吗？从审计到副作用

    Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects

    [https://arxiv.org/abs/2403.12830](https://arxiv.org/abs/2403.12830)

    本文介绍了针对机器近似遗忘审计任务的明确定义和有效指标，通过改变审计挑战的方式为非成员推断问题，开发了高效的审计指标，简化了评估过程

    

    随着对数据隐私和安全日益关注，彻底从机器学习模型中删除数据血统的机器近似遗忘变得至关重要。MLaaS提供者希望将其视为符合监管合规性的最终保障。然而，尽管其至关重要，隐私社区验证机器近似遗忘效果的方法发展和实施的速度令人失望，这一关键领域经常未能得到足够关注。本文旨在通过引入明确定义且有效的指标，为黑盒遗忘审计任务提供解决方法。我们将审计挑战转化为非成员推断问题，并开发出高效的审计指标。通过仅依赖原始和已遗忘模型--消除了训练额外阴影模型的需要--我们的方法简化了评估过程。

    arXiv:2403.12830v1 Announce Type: new  Abstract: The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation
    
[^22]: FlowerFormer: 使用基于流感知的图变换器增强神经结构编码

    FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer

    [https://arxiv.org/abs/2403.12821](https://arxiv.org/abs/2403.12821)

    FlowerFormer是一种强大的图变换器，通过双向异步消息传递和基于流程的全局注意力，可以增强神经结构的表征学习。

    

    特定神经网络架构的成功与其处理的数据集和任务密切相关；没有一种适合所有情况的解决方案。因此，人们付出了大量努力，以快速准确地估计神经结构在特定任务和数据集上的表现，而无需进行完整的训练或评估。神经结构编码在估计中起着至关重要的作用，而将架构视为图的基于图的方法表现出色。为了增强神经结构的表征学习，我们介绍了FlowerFormer，一种强大的图变换器，它融入了神经结构内的信息流。 FlowerFormer由两个关键组件组成：（a）受流程启发的双向异步消息传递；（b）建立在基于流程的掩码上的全局关注。我们广泛的实验表明，FlowerFormer优于现有神经结构。

    arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural 
    
[^23]: 一种基于物理的深度学习框架用于布料模拟

    A Physics-embedded Deep Learning Framework for Cloth Simulation

    [https://arxiv.org/abs/2403.12820](https://arxiv.org/abs/2403.12820)

    该论文提出了一种基于物理的深度学习框架，可以直接编码布料模拟的物理特征，实现快速和实时模拟，并在不使用新数据训练的情况下通过测试表现出与基线的一致性。

    

    精细的布料模拟长期以来一直是计算机图形学中所期望的。为改进受力交互、碰撞处理和数值积分，提出了各种方法。深度学习有潜力实现快速和实时模拟，但常见的神经网络结构通常需要大量参数来捕获布料动力学。本文提出了一种直接编码布料模拟物理特征的物理嵌入学习框架。卷积神经网络用于表示质点-弹簧系统的空间相关性，之后设计了三个分支来学习布料物理的线性、非线性和时间导数特征。该框架还可以通过传统模拟器或子神经网络与其他外部力和碰撞处理进行集成。模型在不使用新数据进行训练的情况下，在不同的布料动画案例中进行了测试。与基线的一致性

    arXiv:2403.12820v1 Announce Type: cross  Abstract: Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselin
    
[^24]: 早期事件预测的动态生存分析

    Dynamic Survival Analysis for Early Event Prediction

    [https://arxiv.org/abs/2403.12818](https://arxiv.org/abs/2403.12818)

    该研究通过动态生存分析方法在早期事件预测领域取得了重要进展，通过整合风险定位到警报策略中，实现了对临床事件指标的提升，并通过新的警报优先级方案显著改进了事件级指标（高达11%的AuPRC差异）。

    

    该研究通过动态生存分析（DSA）推动了医疗保健中的早期事件预测（EEP），通过将风险定位整合到警报策略中提高了临床事件指标。通过调整和评估DSA模型与传统EEP基准的比较，研究表明它们能够在时间步骤水平上匹配EEP模型，并通过新的警报优先级方案显著改善事件级指标（高达11%的AuPRC差异）。这种方法代表了医疗预测的重大进展，为早期事件预测和管理提供了更加细致和可操作的框架。

    arXiv:2403.12818v1 Announce Type: new  Abstract: This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.
    
[^25]: 神经参数回归用于PDE解算符的显式表示

    Neural Parameter Regression for Explicit Representations of PDE Solution Operators

    [https://arxiv.org/abs/2403.12764](https://arxiv.org/abs/2403.12764)

    神经参数回归框架在学习偏微分方程解算符方面利用物理信息神经网络技术，提高了参数效率和计算效率，具有出色的适应性和可扩展性。

    

    我们引入了神经参数回归（NPR），这是一个专门为学习偏微分方程（PDEs）中的解算符而开发的新颖框架。这种针对算子学习的方法通过采用物理信息神经网络（PINN，Raissi等，2019）技术来回归神经网络（NN）参数，超越了传统的DeepONets（Lu等，2021）。通过根据特定初始条件对每个解进行参数化，它有效地逼近了函数空间之间的映射。我们的方法通过合并低秩矩阵提高了参数效率，从而提高了计算效率和可扩展性。该框架表现出对新的初始和边界条件的出色适应性，允许快速微调和推理，甚至在分布之外的情况下也能实现。

    arXiv:2403.12764v1 Announce Type: new  Abstract: We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.
    
[^26]: 对于序贯核回归的更紧凑置信区间

    Tighter Confidence Bounds for Sequential Kernel Regression

    [https://arxiv.org/abs/2403.12732](https://arxiv.org/abs/2403.12732)

    通过使用鞅尾巴界限和无限维凸规划的有限维重构，建立了序贯核回归的新置信区间，证明其始终比现有的置信区间更紧凑，并将其应用于核赌博问题，提高了算法的性能表现。

    

    置信区间是严格量化预测不确定性的重要工具。它们可以指导探索与开发的权衡，并构成许多序贯学习和决策算法的核心组成部分。更紧凑的置信区间带来了具有更好经验性能和更好性能保证的算法。在这项工作中，我们使用鞅尾巴界限和无限维凸规划的有限维重构来建立序贯核回归的新置信区间。我们证明在这一设置中，我们的新置信区间始终比现有的更紧凑。我们将我们的置信区间应用于核赌博问题，其中未来的行动取决于先前的历史。当我们的置信区间取代现有的置信区间时，KernelUCB（GP-UCB）算法具有更好的经验性能，匹配的最坏情况性能保证和可比性。

    arXiv:2403.12732v1 Announce Type: cross  Abstract: Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and compara
    
[^27]: 使用数据增强在神经网络中对后验不确定性进行量化

    Posterior Uncertainty Quantification in Neural Networks using Data Augmentation

    [https://arxiv.org/abs/2403.12729](https://arxiv.org/abs/2403.12729)

    通过提出MixupMP方法，使用数据增强构建更现实的预测分布，从而解决了在神经网络中对后验不确定性进行量化时的基本模型类错误规范化问题。

    

    在这篇论文中，我们通过一个预测框架来处理深度学习中的不确定性量化问题，该框架通过指定有关未来未见数据的预测分布的假设来捕捉模型参数的不确定性。在这个观点下，我们展示了深度集成（Lakshminarayanan等，2017）是一个基本上错误规范化的模型类，因为它假设未来数据仅支持现有观察结果 -- 这种情况在实践中很少遇到。为了解决这个局限性，我们提出了MixupMP，一种使用流行的数据增强技术构建更现实的预测分布的方法。MixupMP作为深度集成的替代方案，其中每个集成成员都是在这个预测分布的随机模拟上训练的。基于最近提出的马丁格尔后验框架（Fong等，2023），MixupMP返回隐式样本。

    arXiv:2403.12729v1 Announce Type: cross  Abstract: In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data. Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly
    
[^28]: 双层超图网络用于多模式阿尔茨海默病诊断

    Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis

    [https://arxiv.org/abs/2403.12719](https://arxiv.org/abs/2403.12719)

    通过引入双层超图机制和梯度驱动流的新策略，本文提出了一种半监督多模态诊断框架，在阿尔茨海默病诊断中表现出优越性能。

    

    早期检测阿尔茨海默病前期阶段对于显著提高患者结果和生活质量至关重要。本文通过半监督多模态诊断框架来解决这一挑战。具体来说，我们引入了一种新的超图框架，可以在利用最少标签的情况下使多模态数据之间的高阶关系。我们首先引入了一个双层超图优化框架，同时学习图扩增策略和半监督分类器。这种双重学习策略被假设可以通过促进信息传播的新路径来增强模型的鲁棒性和泛化能力。其次，我们介绍了一种通过梯度驱动流更有效地生成伪标签的新策略。我们的实验结果表明，我们的框架在诊断阿尔茨海默病方面优于当前技术。

    arXiv:2403.12719v1 Announce Type: new  Abstract: Early detection of Alzheimer's disease's precursor stages is imperative for significantly enhancing patient outcomes and quality of life. This challenge is tackled through a semi-supervised multi-modal diagnosis framework. In particular, we introduce a new hypergraph framework that enables higher-order relations between multi-modal data, while utilising minimal labels. We first introduce a bilevel hypergraph optimisation framework that jointly learns a graph augmentation policy and a semi-supervised classifier. This dual learning strategy is hypothesised to enhance the robustness and generalisation capabilities of the model by fostering new pathways for information propagation. Secondly, we introduce a novel strategy for generating pseudo-labels more effectively via a gradient-driven flow. Our experimental results demonstrate the superior performance of our framework over current techniques in diagnosing Alzheimer's disease.
    
[^29]: 通过图像变形解决域自适应中的源尺度偏差问题

    Addressing Source Scale Bias via Image Warping for Domain Adaptation

    [https://arxiv.org/abs/2403.12712](https://arxiv.org/abs/2403.12712)

    通过在训练过程中对突出的对象区域进行过采样的自适应注意力处理，以及针对对象区域采样的实例级变形引导，有效减轻域自适应中的源尺度偏差。

    

    在视觉识别中，由于真实场景数据集中对象和图像大小分布的不平衡，尺度偏差是一个关键挑战。传统解决方案包括注入尺度不变性先验、在训练过程中对数据集在不同尺度进行过采样，或者在推断时调整尺度。虽然这些策略在一定程度上减轻了尺度偏差，但它们在跨多样化数据集时的适应能力有限。此外，它们会增加训练过程的计算负载和推断过程的延迟。在这项工作中，我们使用自适应的注意力处理——通过在训练过程中就地扭曲图像来对突出的对象区域进行过采样。我们发现，通过改变源尺度分布可以改善主干特征，我们开发了一个面向对象区域采样的实例级变形引导，以减轻域自适应中的源尺度偏差。我们的方法提高了对地理、光照和天气条件的适应性。

    arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
    
[^30]: 选择性的、可解释的和运动一致的隐私属性混淆用于动作识别

    Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition

    [https://arxiv.org/abs/2403.12710](https://arxiv.org/abs/2403.12710)

    提出了一种选择性、可解释的和运动一致的隐私属性混淆方法，通过人类选择的隐私模板实现可解释性，同时在动作识别中表现出更大的灵活性和性能优势。

    

    对于公众图像中被捕捉到的个人隐私的关注，引发了隐私保护动作识别的需求。现有方法通常存在一些问题，其中混淆应用于全局导致隐私敏感区域被隐藏，但也隐藏了动作识别中重要的环境区域，并且缺乏可解释性。我们强调当前范式的局限性，并提出一个解决方案：人类选择的隐私模板，通过设计提供可解释性，一种有选择性地隐藏属性且还导致时间一致性的混淆方案，在动作识别中具有重要性。我们的方法与架构无关，直接修改输入图像，而现有方法通常需要架构训练。我们的方法提供更大的灵活性，因为无需重新训练，并在性能上优于其他方案。

    arXiv:2403.12710v1 Announce Type: cross  Abstract: Concerns for the privacy of individuals captured in public imagery have led to privacy-preserving action recognition. Existing approaches often suffer from issues arising through obfuscation being applied globally and a lack of interpretability. Global obfuscation hides privacy sensitive regions, but also contextual regions important for action recognition. Lack of interpretability erodes trust in these new technologies. We highlight the limitations of current paradigms and propose a solution: Human selected privacy templates that yield interpretability by design, an obfuscation scheme that selectively hides attributes and also induces temporal consistency, which is important in action recognition. Our approach is architecture agnostic and directly modifies input imagery, while existing approaches generally require architecture training. Our approach offers more flexibility, as no retraining is required, and outperforms alternatives on
    
[^31]: 具有客户内和客户间一致性的医学图像分割的联合半监督学习

    Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency

    [https://arxiv.org/abs/2403.12695](https://arxiv.org/abs/2403.12695)

    提出了一个用于医学图像分割的新颖联合半监督学习框架

    

    医学图像分割在临床疾病诊断和医学图像分析中起着至关重要的作用。然而，由于放射科医师的不可或缺的领域专业知识，为分割任务标记医学图像是困难的。此外，考虑到医学图像的隐私和敏感性，从不同医疗机构建立一个集中式分割数据集是不切实际的。联邦学习旨在训练孤立客户的共享模型，而无需进行本地数据交换，这与医学数据的稀缺性和隐私性特征相吻合。为解决标签困难的问题，在集中式数据设置中已经提出了许多先进的半监督方法。至于联合学习，如何在这种分布式场景下进行半监督学习值得研究。在这项工作中，我们提出了一个新颖的用于医学图像分割的联合半监督学习框架。

    arXiv:2403.12695v1 Announce Type: cross  Abstract: Medical image segmentation plays a vital role in clinic disease diagnosis and medical image analysis. However, labeling medical images for segmentation task is tough due to the indispensable domain expertise of radiologists. Furthermore, considering the privacy and sensitivity of medical images, it is impractical to build a centralized segmentation dataset from different medical institutions. Federated learning aims to train a shared model of isolated clients without local data exchange which aligns well with the scarcity and privacy characteristics of medical data. To solve the problem of labeling hard, many advanced semi-supervised methods have been proposed in a centralized data setting. As for federated learning, how to conduct semi-supervised learning under this distributed scenario is worth investigating. In this work, we propose a novel federated semi-supervised learning framework for medical image segmentation. The intra-client
    
[^32]: LNPT：无标签网络修剪与训练

    LNPT: Label-free Network Pruning and Training

    [https://arxiv.org/abs/2403.12690](https://arxiv.org/abs/2403.12690)

    本文介绍了LNPT，一种无标签网络修剪和训练的新框架，通过引入学习差距的概念，强调其准确相关性，以解决在智能设备上确定修剪结构的难题。

    

    在训练之前修剪神经网络，使其能够部署在智能设备上。通过保留有助于泛化的权重，修剪后的网络可以在资源受限的智能设备上运行。我们提出了学习差距的概念，并强调它与泛化的准确相关性。实验表明，学习差距通过网络倒数第二层的特征图形式与泛化性能的变化相一致。我们提出了一种新的学习框架 LNPT，使得云端成熟网络能够提供在线指导。

    arXiv:2403.12690v1 Announce Type: new  Abstract: Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online gui
    
[^33]: SEVEN: 通过保留哨兵来剪枝Transformer模型

    SEVEN: Pruning Transformer Model by Reserving Sentinels

    [https://arxiv.org/abs/2403.12688](https://arxiv.org/abs/2403.12688)

    SEVEN通过保留梯度噪声较小的权重，在剪枝Transformer模型时取得了优异的效果。

    

    大规模Transformer模型已经在各种任务中展现出卓越的性能。然而，由于其可观的参数规模，它们的适用性受到限制，尤其是在移动设备上。鉴于Transformer模型相对于卷积神经网络的梯度是动态且错综复杂的，常用的剪枝方法往往会保留具有较大梯度噪声的权重。这导致被剪枝的模型对稀疏性和数据集敏感，表现出次优性能。符号下降（SD）是一种用于训练和微调Transformer模型的通用方法。在本文中，我们试图通过SD的累积过程描述Transformer模型上的噪声批梯度序列。我们利用这一设计动态评估权重的重要性分数。我们引入了SEVEN，特别偏向于具有持续高敏感度的权重，即梯度噪声较小的权重。

    arXiv:2403.12688v1 Announce Type: new  Abstract: Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved b
    
[^34]: 基于迟来的模态融合和基于规则的决策的音视频复合表达识别方法

    Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision

    [https://arxiv.org/abs/2403.12687](https://arxiv.org/abs/2403.12687)

    该研究提出了一种新颖的音视频复合表达识别方法，通过情绪识别模型融合不同模态的情绪概率，并基于预定义规则进行决策，无需特定训练数据，具有潜力为人类基本和复合情感情境下的音视频数据注释提供智能工具。

    

    本文介绍了第六届ABAW比赛的SUN团队针对复合表达识别挑战的结果。我们提出了一种新颖的音视频复合表达识别方法。我们的方法依赖于融合情绪概率水平的情绪识别模型，而关于复合表达预测的决策基于预定义规则。值得注意的是，我们的方法不使用任何特定于目标任务的训练数据。该方法在多语料训练和跨语料验证设置中进行评估。从挑战中的研究结果表明，所提出的方法有可能为在人类基本和复合情感情境下注释音视频数据的智能工具开发奠定基础。源代码已公开。

    arXiv:2403.12687v1 Announce Type: cross  Abstract: This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions. The source code is publicly available.
    
[^35]: 基于高斯伯努利受限玻尔兹曼机的异常检测评分可解释性改进

    Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine

    [https://arxiv.org/abs/2403.12672](https://arxiv.org/abs/2403.12672)

    该研究提出了一种基于累积分布改进评分可解释性的度量，建立了使用可解释度量设置阈值的准则，并通过数值实验证明其合理性。

    

    高斯伯努利受限玻尔兹曼机（GBRBM）常用于半监督异常检测，仅使用正常数据点进行训练。在基于GBRBM的异常检测中，根据边缘GBRBM的能量函数相同的评分来对正常和异常数据进行分类。然而，由于无法解释该评分，很难设置适当的分类阈值。本研究提出了一种基于累积分布改进评分可解释性的度量，并建立了使用可解释度量设置阈值的准则。数值实验结果表明，仅使用正常数据点设置阈值时，该准则是合理的。此外，由于识别度量涉及计算不可行的最小评分值的评估，我们还提出了一种最小评分的评估方法。

    arXiv:2403.12672v1 Announce Type: cross  Abstract: Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points. In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score
    
[^36]: 解密AutoML集成：cattleia在决策中的协助

    Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making

    [https://arxiv.org/abs/2403.12664](https://arxiv.org/abs/2403.12664)

    提出了cattleia工具，可以解密用于回归、多类别和二元分类任务的AutoML集成模型，通过评估指标和新度量指标，分析集成模型的性能和重要性

    

    在许多应用中，模型集成被证明比单个预测模型更好。因此，在自动化机器学习（AutoML）中，它是最常见的后处理技术。我们的工作提出了cattleia - 一种能解密用于回归、多类别和二元分类任务的集成的应用程序。该工具与三个AutoML包构建的模型一起工作：auto-sklearn、AutoGluon和FLAML。我们从不同的角度分析了给定的集成。我们通过评估集成及其组件模型的评估指标进行了预测性能调查。我们通过引入新的度量指标来评估模型预测的多样性和互补性扩展验证视角。此外，我们应用可解释的人工智能（XAI）技术来检查v的重要性

    arXiv:2403.12664v1 Announce Type: cross  Abstract: In many applications, model ensembling proves to be better than a single predictive model. Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML). The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models. In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks. This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The given ensemble is analyzed from different perspectives. We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models. We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions. Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of v
    
[^37]: 使用深度学习预测沸石的吸附性能

    Zeolite Adsorption Property Prediction using Deep Learning

    [https://arxiv.org/abs/2403.12659](https://arxiv.org/abs/2403.12659)

    通过深度学习模型，本研究提出了一个比传统分子模拟快4到5个数量级的方法来预测沸石的吸附性能，并可以用于识别吸附位点。

    

    能够有效预测沸石的吸附性能对于加速新材料设计过程有很大的益处。本工作中，我们提出了一个比分子模拟快4到5个数量级的模型，用于吸附性能的预测。通过机器学习模型预测得到的结果与蒙特卡洛模拟得到的数值一致，验证了该模型可用于性能预测，同时还展示了该模型可以用于识别吸附位点。

    arXiv:2403.12659v1 Announce Type: cross  Abstract: The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials. The existing configuration space for these materials is wide, while existing molecular simulation methods are computationally expensive. In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular simulations. To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo simulations, confirming that the model can be used for property prediction. Furthermore, we show that the model can be used for identifying adsorption sites. Finally, we
    
[^38]: 自适应多层神经网络用于带误差估计的参数化偏微分方程

    Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation

    [https://arxiv.org/abs/2403.12650](https://arxiv.org/abs/2403.12650)

    该论文提出了一种神经网络架构，用于解决高维参数化偏微分方程，模仿自适应有限元方法，实现对逼近误差的控制，并通过有效的后验误差估计器将逼近参数减少到很少，实现了问题自适应表示。

    

    为了解决高维参数化偏微分方程（pPDEs），提出了一种神经网络架构，旨在将模型数据的参数映射到对应的有限元解。为了提高训练效率并实现对逼近误差的控制，该网络模仿了自适应有限元方法（AFEM）。它输出一个粗网格解和一系列在AFEM中产生的修正，允许跟踪网络各层中的误差衰减。观察到的误差通过可靠的基于残差的后验误差估计器来测量，从而将逼近网络输出的参数减少到很少。这导致了在局部细化网格上的问题自适应表示。此外，AFEM的每个解都是在分层基础上离散化的。对于架构而言，采用了卷积神经网络（CNNs）。

    arXiv:2403.12650v1 Announce Type: cross  Abstract: To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented. It is constructed to map parameters of the model data to corresponding finite element solutions. To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM). It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network. The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network. This leads to a problem adapted representation of the solution on locally refined grids. Furthermore, each solution of the AFEM is discretized in a hierarchical basis. For the architecture, convolutional neural networks (CNNs)
    
[^39]: 基于提示融合框架的归纳逻辑查询回答

    Prompt-fused framework for Inductive Logical Query Answering

    [https://arxiv.org/abs/2403.12646](https://arxiv.org/abs/2403.12646)

    提出了一种名为Pro-QE的查询感知提示融合框架，能够整合现有查询嵌入方法并通过上下文信息聚合来解决知识图谱中新实体的嵌入问题。

    

    在知识图谱（KG）上回答逻辑查询对机器推理提出了重大挑战，这个任务的主要障碍来自KG的固有不完整性。现有研究主要集中在解决KG中缺失边的问题，从而忽视了另一个不完整性方面：新实体的出现。此外，大部分现有方法倾向于单独对每个逻辑运算符进行推理，而不是在推理过程中全面分析查询作为一个整体。在本文中，我们提出了一个名为Pro-QE的查询感知提示融合框架，它可以整合现有的查询嵌入方法，并通过上下文信息聚合来解决新实体的嵌入。此外，介绍了一个通过编码符号查询生成的查询提示，用于从整体视角收集与查询相关的信息。

    arXiv:2403.12646v1 Announce Type: new  Abstract: Answering logical queries on knowledge graphs (KG) poses a significant challenge for machine reasoning. The primary obstacle in this task stems from the inherent incompleteness of KGs. Existing research has predominantly focused on addressing the issue of missing edges in KGs, thereby neglecting another aspect of incompleteness: the emergence of new entities. Furthermore, most of the existing methods tend to reason over each logical operator separately, rather than comprehensively analyzing the query as a whole during the reasoning process. In this paper, we propose a query-aware prompt-fused framework named Pro-QE, which could incorporate existing query embedding methods and address the embedding of emerging entities through contextual information aggregation. Additionally, a query prompt, which is generated by encoding the symbolic query, is introduced to gather information relevant to the query from a holistic perspective. To evaluate
    
[^40]: 用于时间序列的自动对比学习策略搜索

    Automated Contrastive Learning Strategy Search for Time Series

    [https://arxiv.org/abs/2403.12641](https://arxiv.org/abs/2403.12641)

    本文介绍了微软开展的自动机器学习实践，用于自动对比学习各种时间序列数据集和任务的表示。

    

    近年来，对比学习（CL）已成为时间序列的主要表示学习范式。现有文献中大多数方法侧重于通过人类启发式方法手动构建特定的对比学习策略（CLS）以应用于特定数据集和任务。然而，手动开发CLS通常需要对数据集和任务有过多的先验知识，例如在医疗保健领域对医学时间序列的专业认知，以及大量的人力和大量实验来确定详细的学习配置。本文介绍了微软的自动机器学习（AutoML）实践，该实践自动学习对比学习各种时间序列数据集和任务的表示，即自动化对比学习（AutoCL）。

    arXiv:2403.12641v1 Announce Type: new  Abstract: In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive 
    
[^41]: 用于科学中评估生成模型的统计距离的实用指南

    A Practical Guide to Statistical Distances for Evaluating Generative Models in Science

    [https://arxiv.org/abs/2403.12636](https://arxiv.org/abs/2403.12636)

    本文提供了一种实用指南，介绍了四种常用统计距离的概念，以帮助评估生成模型，无需高深的数学和统计知识。

    

    生成模型在许多科学领域中是非常宝贵的，因为它们能够捕捉高维和复杂的分布，例如逼真的图像、蛋白质结构和连接组。本研究旨在为理解流行的统计距离概念提供一个易于理解的入口点，只需要数学和统计学的基础知识。我们专注于代表不同方法论的四种常用统计距离概念：使用低维投影（Sliced-Wasserstein; SW)、使用分类器获取距离（Classifier Two-Sample Tests; C2ST)、通过核进行嵌入（Maximum Mean Discrepancy; MMD) 或神经网络（Fr\'echet Inception Distance; FID)。我们强调每个距离背后的直觉，并解释它们的优点、可伸缩性、复杂性和缺陷。

    arXiv:2403.12636v1 Announce Type: new  Abstract: Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distanc
    
[^42]: SUN团队对ABAW 2024比赛的贡献：音视频Valence-Arousal估计和表情识别

    SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition

    [https://arxiv.org/abs/2403.12609](https://arxiv.org/abs/2403.12609)

    该研究探索了音视频深度学习方法在野外情绪识别问题中的有效性，针对视频和音频模态分别基于微调的CNN和PDEM架构，比较了不同的时间建模和融合策略，并在ABAW'24挑战中取得了实验结果。

    

    随着情绪在人类交流中起着核心作用，自动情绪识别在过去二十年中吸引了越来越多的关注。尽管多模态系统在实验室控制的数据上表现出较高的性能，但它们仍远未能在非实验室控制的“野外”数据上提供生态效度。本文研究了用于野外情绪识别的音视频深度学习方法。我们特别探讨基于微调的卷积神经网络（CNN）和Public Dimensional Emotion Model (PDEM) 的架构对视频和音频模态的有效性。我们比较了使用这些多阶段训练的模态特定深度神经网络（DNN）的嵌入进行替代时间建模和融合策略。我们报告了在Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) 挑战协议下使用AffWild2数据集的结果。

    arXiv:2403.12609v1 Announce Type: new  Abstract: As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.
    
[^43]: 关于异质集成方法在重识别中的有效性

    On the Effectiveness of Heterogeneous Ensemble Methods for Re-identification

    [https://arxiv.org/abs/2403.12606](https://arxiv.org/abs/2403.12606)

    提出了一种新颖的集成方法，用于重识别工业实体，取得了在任务中最先进的性能，同时引入了五种不同的特征提取方法，并研究它们的组合。

    

    在这篇论文中，我们介绍了一种新颖的集成方法，用于重识别工业实体，使用芯片木托盘和镀锌金属板的图像作为数据集示例。我们的算法将常用复杂的孪生神经网络替换为一组简化的基本模型，提供更广泛的适用性，特别是在硬件受限的场景下。每个集成子模型使用给定数据的不同类型的提取特征作为其输入，允许在比更复杂的最先进模型需要的训练时间的一小部分内创建有效的集成。我们在任务中达到了最先进的性能，Rank-1准确率超过77%，Rank-10准确率超过99%，并介绍了五种不同的特征提取方法，并使用不同的集成方法研究它们的组合。

    arXiv:2403.12606v1 Announce Type: new  Abstract: In this contribution, we introduce a novel ensemble method for the re-identification of industrial entities, using images of chipwood pallets and galvanized metal plates as dataset examples. Our algorithms replace commonly used, complex siamese neural networks with an ensemble of simplified, rudimentary models, providing wider applicability, especially in hardware-restricted scenarios. Each ensemble sub-model uses different types of extracted features of the given data as its input, allowing for the creation of effective ensembles in a fraction of the training duration needed for more complex state-of-the-art models. We reach state-of-the-art performance at our task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%, and introduce five distinct feature extraction approaches, and study their combination using different ensemble methods.
    
[^44]: 通过机器学习指导的租金补助分发防止因驱逐而造成的无家可归

    Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance

    [https://arxiv.org/abs/2403.12599](https://arxiv.org/abs/2403.12599)

    通过机器学习系统，根据未来无家可归的风险，准确识别需要支持的个人，提高了分配效率，公平和公正，避免了一部分人因当前流程的疏忽而无家可归。

    

    租金补助计划为个人提供财政援助，以防止由驱逐引起的住房不稳定并避免无家可归。由于这些计划在资源限制下运作，它们必须决定谁是优先考虑的对象。通常，资金是通过一种反应性或先到先得的分配过程分发的，这种过程不系统地考虑未来无家可归的风险。我们与宾夕法尼亚州阿勒格尼县合作，探索了一种积极的分配方法，根据他们未来无家可归的风险优先考虑面临驱逐的个人。我们的机器学习系统利用州和县的行政数据准确识别需要支持的个人，其表现优于至少20%的简单优先级处理方法，同时在种族和性别上公平公正。此外，我们的方法将识别出28%的个人，这些人被当前的处理流程所忽视并最终无家可归。除了改进

    arXiv:2403.12599v1 Announce Type: cross  Abstract: Rental assistance programs provide individuals with financial assistance to prevent housing instabilities caused by evictions and avert homelessness. Since these programs operate under resource constraints, they must decide who to prioritize. Typically, funding is distributed by a reactive or first-come-first serve allocation process that does not systematically consider risk of future homelessness. We partnered with Allegheny County, PA to explore a proactive allocation approach that prioritizes individuals facing eviction based on their risk of future homelessness. Our ML system that uses state and county administrative data to accurately identify individuals in need of support outperforms simpler prioritization approaches by at least 20% while being fair and equitable across race and gender. Furthermore, our approach would identify 28% of individuals who are overlooked by the current process and end up homeless. Beyond improvements 
    
[^45]: 主要分布的机器学习

    Machine Learning of the Prime Distribution

    [https://arxiv.org/abs/2403.12588](https://arxiv.org/abs/2403.12588)

    使用最大熵方法推导了概率数论中的几个定理，提供了关于素数学习性质的理论论证，发现Erd\H{o}s-Kac定律不太可能被当前机器学习技术发现，并进行数值实验以验证理论结果

    

    在本研究中，我们使用最大熵方法推导了概率数论中的几个定理，包括哈代-拉马努金定理的一个版本。我们还提供了一个理论论证，解释了Y.-H. He关于素数可学性的实验观察，并假设Erd\H{o}s-Kac定律极不可能被当前的机器学习技术发现。我们进行的数值实验证实了我们的理论发现。

    arXiv:2403.12588v1 Announce Type: cross  Abstract: In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.
    
[^46]: 通过获取赋权：支持小规模深度学习的案例

    Equity through Access: A Case for Small-scale Deep Learning

    [https://arxiv.org/abs/2403.12562](https://arxiv.org/abs/2403.12562)

    通过引入PePR分数，研究人员展示了在资源有限的情况下，利用131种独特的DL架构在医学图像任务中的可行性。

    

    深度学习（DL）的最新进展得益于大规模数据和计算力的提升。这些大规模资源被用于训练日益庞大的模型，而这些模型在计算、数据、能源和碳排放方面消耗巨大。这些成本正在成为研究人员和从业者面临的新型准入障碍，特别是对于那些在全球南方地区资源有限的人。在这项工作中，我们全面审视了现有视觉任务的DL模型，并展示了它们在资源有限的环境中的实用性。为了考虑DL模型的资源消耗，我们引入了一个衡量性能与资源单元的新指标，我们称之为PePR分数。通过使用131种独特的DL架构（跨度从1M到130M个可训练参数）和三个医学图像数据集，我们获取了有关性能和资源之间关系的趋势。

    arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
    
[^47]: 多标签类增量学习的信心自校准

    Confidence Self-Calibration for Multi-Label Class-Incremental Learning

    [https://arxiv.org/abs/2403.12559](https://arxiv.org/abs/2403.12559)

    本文提出了一种针对多标签类增量学习中的信心自校准问题的方法，通过引入图卷积网络和最大熵正则化，改善了多标签信心校准，减少了过度自信的误报错误。

    

    多标签类增量学习（MLCIL）中的部分标签挑战是在训练期间只有新类别被标记，而过去和未来标签仍然不可用。这个问题会导致由于错误地高置信度多标签预测而出现大量误报错误，加剧了在不同标签空间内的灾难性遗忘。在本文中，我们旨在在MLCIL中改进多标签信心校准，并提出了一种信心自校准（CSC）方法。首先，为了标签关系校准，我们引入一个类增量图卷积网络，通过构建可学习的、动态扩展的标签关系图来连接孤立的标签空间。然后，为了信心校准，我们针对每个多标签增量提出了一种最大熵正则化，通过对过于自信的输出分布进行惩罚，促进了信心的自校准。

    arXiv:2403.12559v1 Announce Type: cross  Abstract: The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable. This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space. In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for label relationship calibration, we introduce a class-incremental graph convolutional network that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship graph. Then, for confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions. Our 
    
[^48]: 为求解多物理学偏微分方程预训练象域关注神经算子

    Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs

    [https://arxiv.org/abs/2403.12553](https://arxiv.org/abs/2403.12553)

    该研究提出了一种名为CoDA-NO的神经算子，通过在象域或通道空间对函数进行标记，实现了多个PDE系统的自监督学习或预训练，为解决涉及耦合偏微分方程的多物理问题提供了新思路。

    

    现有的神经算子架构在解决涉及耦合偏微分方程的多物理问题时面临挑战，这是由于复杂的几何形状、物理变量之间的相互作用以及缺乏大量高分辨率训练数据所致。为了解决这些问题，我们提出了象域关注神经算子（CoDA-NO），该算子将函数在象域或通道空间上进行标记，实现了多个PDE系统的自监督学习或预训练。具体来说，我们将位置编码、自注意力和归一化层扩展到函数空间。CoDA-NO可以使用单个模型学习不同PDE系统的表示。通过考虑少样本学习设置，我们评估了CoDA-NO作为学习多物理PDE的骨干的潜力。在具有有限数据的复杂下游任务（如流体流动模拟和流固相互作用）中，我们发现CoDA-N

    arXiv:2403.12553v1 Announce Type: new  Abstract: Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-N
    
[^49]: AffineQuant：用于大型语言模型的仿射变换量化

    AffineQuant: Affine Transformation Quantization for Large Language Models

    [https://arxiv.org/abs/2403.12544](https://arxiv.org/abs/2403.12544)

    AffineQuant是一种用于大型语言模型的直接优化后训练量化方法，通过使用等效的仿射变换来扩展优化范围并显著减小量化误差。

    

    大规模语言模型（LLMs）所需的显著资源需求引起了人们对开发旨在压缩和加速神经网络的技术的极大兴趣。在这些技术中，后训练量化（PTQ）由于在训练背景下的显著压缩效率和成本效益而引起了相当大的关注。现有的LLMs后量化方法限制优化范围在前后量化权重之间的缩放变换。在本文中，我们提倡使用等效仿射变换进行直接优化（AffineQuant）的PTQ。这种方法扩展了优化范围，从而显著减少量化误差。此外，通过使用相应的逆矩阵，我们可以确保PTQ的前后量化输出之间的等效性，从而保持其效率和泛化性能。

    arXiv:2403.12544v1 Announce Type: new  Abstract: The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and genera
    
[^50]: 上下文化信息提升了图表示

    Contextualized Messages Boost Graph Representations

    [https://arxiv.org/abs/2403.12529](https://arxiv.org/abs/2403.12529)

    这篇论文提出了关于图神经网络在各个层次（节点级、邻域级和图级）的表示能力的新视角。

    

    近年来，图神经网络（GNN）因其处理以图表示的任意结构化数据的能力而引起了广泛关注。GNN通常遵循消息传递方案来本地更新节点特征表示。然后使用图读出函数创建整个图的表示。一些研究通过修改消息传递框架的聚合和组合策略提出了不同的GNN，常常受启发于启发式算法。然而，一些研究已经开始从基于图同构问题的理论角度探索GNN，该问题固有地假设可数的节点特征表示。然而，目前只有少数理论工作探索了具有不可数节点特征表示的GNN。本文提出了一个关于GNN在节点级、邻域级和图级的表示能力的新视角。

    arXiv:2403.12529v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs. GNNs generally follow the message-passing scheme to locally update node feature representations. A graph readout function is then employed to create a representation for the entire graph. Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics. Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-l
    
[^51]: 基于前向梯度的Frank-Wolfe优化用于高效训练深度神经网络

    Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training

    [https://arxiv.org/abs/2403.12511](https://arxiv.org/abs/2403.12511)

    本文利用前向自动微分计算梯度，提出了基于Frank-Wolfe算法的优化方法，收敛速度为次线性。

    

    使用基于梯度的方法训练深度神经网络需要在每个级别计算梯度。然而，使用反向传播或反向模式微分计算梯度需要消耗大量内存，使反向传播成为计算梯度的一种低效方法。本文重点分析了著名的Frank-Wolfe算法的性能，即有条件的梯度算法，通过访问前向自动微分以计算梯度。我们提供了深入的技术细节，显示所提出的算法通过访问在前向自动微分中获得的真梯度的有噪声估计， 即称为Projected Forward Gradient，收敛于最优解，收敛速度为次线性。相比之下，标准的Frank-Wolfe算法，在提供Projected Fors

    arXiv:2403.12511v1 Announce Type: new  Abstract: Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forwar
    
[^52]: 图像操作的广义一致性轨迹模型

    Generalized Consistency Trajectory Models for Image Manipulation

    [https://arxiv.org/abs/2403.12510](https://arxiv.org/abs/2403.12510)

    本研究提出了广义一致性轨迹模型（GCTMs），能够在任何噪声分布和数据分布之间实现转换。

    

    基于扩散的生成模型在无条件生成以及图像编辑和恢复等应用任务中表现出色。扩散模型的成功在于扩散的迭代性质：扩散将将噪声到数据的复杂映射过程分解为一系列简单的去噪任务。此外，通过在每个去噪步骤中注入引导项，我们能够对生成过程进行精细控制。然而，迭代过程也常常计算密集，通常需要进行数十次甚至数千次函数评估。虽然一致性轨迹模型（CTMs）可以在概率流ODE（PFODE）上任意时间点之间进行遍历，并且通过单次函数评估进行得分推导，但CTMs仅允许从高斯噪声转换为数据。因此，本文旨在通过提出广义CTMs（GCTMs）来发挥CTMs的全部潜力，实现在任何噪声分布和数据分布之间进行转换。

    arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
    
[^53]: 保护大型语言模型：威胁、漏洞和负责任的实践

    Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices

    [https://arxiv.org/abs/2403.12503](https://arxiv.org/abs/2403.12503)

    该论文全面调查了与大型语言模型相关的安全和隐私问题，并提出了未来研究的有前途方向。

    

    大型语言模型(LLMs)显著改变了自然语言处理(NLP)的格局。它们对各种任务产生了影响，从而彻底改变了我们处理语言理解和生成的方式。然而，除了它们引人注目的实用性外，LLMs还带来了重要的安全和风险考虑。这些挑战需要仔细研究，以确保负责任的部署，并防范潜在的漏洞。本研究全面调查了与LLMs相关的安全和隐私问题，从五个主题角度进行：安全和隐私问题、对抗性攻击的漏洞、LLMs误用可能造成的潜在危害、缓解策略以解决这些挑战，同时识别当前策略的局限性。最后，本文建议未来研究的有前途的方向，以增强LLMs的安全和风险管理。

    arXiv:2403.12503v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
    
[^54]: 一个用于深度神经网络和扫视路径分类的可训练特征提取模块

    A Trainable Feature Extractor Module for Deep Neural Networks and Scanpath Classification

    [https://arxiv.org/abs/2403.12493](https://arxiv.org/abs/2403.12493)

    提出了一个联合可训练的特征提取模块，用于将扫视路径转换为可直接用于深度神经网络的特征向量，以提高分类性能。

    

    扫视路径分类是眼动跟踪研究领域中的一个潜在应用领域，可能应用于医学、制造业以及各个领域学生的培训系统。在本文中，我们提出了一个用于深度神经网络的可训练特征提取模块。该模块的目的是将扫视路径转换为一个特征向量，直接用于深度神经网络架构。基于深度神经网络的反向传播误差，该特征提取模块调整其参数以提高分类性能。因此，我们的特征提取模块与深度神经网络联合可训练。这个特征提取模块的动机源于传统的基于直方图的方法，通常计算扫视路径上的分布。我们在三个公共数据集上评估了我们的模块，并将其与现有方法进行了比较。

    arXiv:2403.12493v1 Announce Type: cross  Abstract: Scanpath classification is an area in eye tracking research with possible applications in medicine, manufacturing as well as training systems for students in various domains. In this paper we propose a trainable feature extraction module for deep neural networks. The purpose of this module is to transform a scanpath into a feature vector which is directly useable for the deep neural network architecture. Based on the backpropagated error of the deep neural network, the feature extraction module adapts its parameters to improve the classification performance. Therefore, our feature extraction module is jointly trainable with the deep neural network. The motivation to this feature extraction module is based on classical histogram-based approaches which usually compute distributions over a scanpath. We evaluated our module on three public datasets and compared it to the state of the art approaches.
    
[^55]: 基于NTK引导的少样本类增量学习

    NTK-Guided Few-Shot Class Incremental Learning

    [https://arxiv.org/abs/2403.12486](https://arxiv.org/abs/2403.12486)

    本文通过NTK对FSCIL模型的指导，致力于在增量学习中实现卓越泛化，通过优化NTK收敛和降低泛化误差来确保最佳性能。

    

    尽管反遗忘FSCIL学习者在增量会话中表现出色，但他们往往更注重减少知识流失，而忽视了模型潜在获取知识的能力。本文通过神经切向核（NTK）的视角深入探讨了FSCIL模型泛化的基础。我们主要的设计重点在于确保最优NTK收敛和NTK相关的泛化误差，作为卓越泛化的理论基础。为了达到全局最优的NTK收敛，我们采用了一个植根于数学原理的元学习机制，指导扩展网络内的优化过程。此外，为了减少NTK相关的泛化误差，我们从基础层面开始，优化构成其泛化损失的相关因素。具体地，我们通过在基础会话上启动自监督预训练来塑造初始ne

    arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne
    
[^56]: TT-BLIP：使用BLIP和Tri-Transformer增强假新闻检测

    TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer

    [https://arxiv.org/abs/2403.12481](https://arxiv.org/abs/2403.12481)

    TT-BLIP模型通过使用BLIP和Tri-Transformer技术，结合文本和图像的多模态信息提取，采用Multimodal Tri-Transformer融合特征，实现了增强的综合表征和改进的多模态数据分析。

    

    arXiv:2403.12481v1 公告类型：新   摘要：检测假新闻受到了极大关注。许多先前的方法将独立编码的单模态数据进行串联，忽略了综合多模态信息的好处。此外，对于文本和图像缺乏专门的特征提取进一步限制了这些方法。本文介绍了一种名为TT-BLIP的端到端模型，该模型对三种类型的信息应用了引导式语言-图像预训练用于统一的视觉-语言理解和生成（BLIP）：BERT 和 BLIP\textsubscript{Txt} 用于文本，ResNet 和 BLIP\textsubscript{Img} 用于图像，以及用于多模态信息的双向 BLIP 编码器。多模态三角变换器使用三种类型的多头注意机制融合三模态特征，确保了增强表示和改进的多模态数据分析。实验使用了两个假新闻数据集，微博和Gossipcop。 结果表明，

    arXiv:2403.12481v1 Announce Type: new  Abstract: Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results in
    
[^57]: FairSIN：通过敏感信息中和实现图神经网络中的公平性

    FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization

    [https://arxiv.org/abs/2403.12474](https://arxiv.org/abs/2403.12474)

    通过引入促进公平性的特征（F3）来中和图神经网络中的敏感偏见，进而提高预测性能和公平性的权衡。

    

    尽管图神经网络（GNNs）在对图结构数据进行建模方面取得了显著成功，但与其他机器学习模型一样，GNNs也容易根据敏感属性（如种族和性别）做出有偏见的预测。为了公平考虑，最近的最先进方法提出从输入或表示中过滤掉敏感信息，例如删除边或屏蔽特征。然而，我们认为基于此类过滤策略可能也会过滤掉一些非敏感的特征信息，导致在预测性能和公平性之间产生次优的权衡。为解决这一问题，我们提出一种创新的中和基础范式，即在信息传递之前将额外的促进公平性的特征（F3）纳入节点特征或表示中。这些F3预期在统计上中和节点表示中的敏感偏见，并提供额外的非敏感信息。

    arXiv:2403.12474v1 Announce Type: new  Abstract: Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nons
    
[^58]: 更多背景信息何时有助于识别讽刺？

    When Do "More Contexts" Help with Sarcasm Recognition?

    [https://arxiv.org/abs/2403.12469](https://arxiv.org/abs/2403.12469)

    本研究探讨了将更多上下文信息整合到模型中能够带来的对于讽刺识别的改进效果。

    

    讽刺识别具有挑战性，因为它需要理解文本的真实意图，这与字面意义相反或不同。先前的研究通过开发一系列提供更丰富$contexts$（例如情感或文化细微差别）给模型的方法来解决这一挑战。虽然已经单独证明了这些方法的有效性，但没有研究系统地评估它们的集体效果。因此，额外背景信息能够提高对讽刺的识别程度仍然不清楚。本文探讨了集成现有方法所带来的改进，通过将更多背景信息整合到模型中。为此，我们制定了一个框架，可以集成多个上下文线索并测试不同方法。在对三个讽刺识别基准测试的四种方法进行评估时，我们实现了现有的最先进性能，并演示了逐步添加更多co有益之处。

    arXiv:2403.12469v1 Announce Type: new  Abstract: Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more co
    
[^59]: 非负对比学习

    Non-negative Contrastive Learning

    [https://arxiv.org/abs/2403.12459](https://arxiv.org/abs/2403.12459)

    非负对比学习(NCL)是对非负矩阵分解(NMF)的重新演绎，通过对特征施加非负约束来获得可解释的特征，保留了NMF的可解释属性，从而得到比标准对比学习(CL)更稀疏和解耦的表示

    

    深度表示在以黑盒方式转移到下游任务时表现出了良好的性能。然而，它们固有的不可解释性仍然是一个重大挑战，因为这些特征通常对人类理解而言是不透明的。在本文中，我们提出了非负对比学习（NCL），这是对非负矩阵分解（NMF）的复兴，旨在得出可解释的特征。NCL的力量在于强制将非负约束应用于特征，这让人想起NMF能够提取与样本集群紧密对齐的特征的能力。NCL不仅在数学上与NMF目标很好地对齐，而且保留了NMF的可解释属性，使得与标准对比学习（CL）相比，得到了更加稀疏和解耦的表示。从理论上，我们为NCL的可识别性和下游泛化性能提供了保证。从经验上看，我们展示了这些

    arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
    
[^60]: 生成的数据总是有助于对比学习吗？

    Do Generated Data Always Help Contrastive Learning?

    [https://arxiv.org/abs/2403.12448](https://arxiv.org/abs/2403.12448)

    生成的高质量图像已成功应用于增强对比表示学习，但我们发现有时生成的数据甚至会对对比学习造成伤害，通过研究发现更强的数据膨胀应该伴随着更弱的增强。

    

    对比学习（CL）已经成为无监督视觉表示学习中最成功的范式之一，然而它往往依赖大量手工数据增强。随着生成模型的兴起，特别是扩散模型，生成接近真实数据分布的逼真图像的能力得到了很好的认可。这些生成的高质量图像已成功应用于增强对比表示学习，一种称为“数据膨胀”的技术。然而，我们发现生成的数据（甚至来自像DDPM这样的好扩散模型）有时甚至会对对比学习造成伤害。我们从数据膨胀和数据增强的角度探讨了这种失败的原因。我们首次揭示了更强的数据膨胀应该伴随着更弱的增强，反之亦然的互补作用。我们还提供了严格的理论解释。

    arXiv:2403.12448v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanatio
    
[^61]: 从数据中学习变换和混合策略的TransformMix

    TransformMix: Learning Transformation and Mixing Strategies from Data

    [https://arxiv.org/abs/2403.12429](https://arxiv.org/abs/2403.12429)

    提出了一种名为TransformMix的自动化方法，可以从数据中学习更好的变换和混合增强策略。

    

    数据扩充通过合成更多的训练样本提高深度学习模型的泛化能力。样本混合是一种流行的数据扩充方法，通过组合现有样本来创建附加数据。最近的样本混合方法，如Mixup和Cutmix，采用简单的混合操作来混合多个输入。尽管这种经验法则方法在一些计算机视觉任务中表现出一定的性能提升，但它盲目地混合图像，并且不会自动适应不同的数据集。一个对特定数据集有效的混合策略通常不能很好地推广到其他数据集。如果方法没有正确配置，可能会创建误导性的混合图像，从而危及样本混合扩充的有效性。在这项工作中，我们提出了一种自动化方法TransformMix，从数据中学习更好的变换和混合扩充策略。具体地说，TransformMix应用

    arXiv:2403.12429v1 Announce Type: cross  Abstract: Data augmentation improves the generalization power of deep learning models by synthesizing more training samples. Sample-mixing is a popular data augmentation approach that creates additional data by combining existing samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs. Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically. A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets. If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations. In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data. In particular, TransformMix applies
    
[^62]: 基于奖励样本的顺序多臂老虎机中的转移

    Transfer in Sequential Multi-armed Bandits via Reward Samples

    [https://arxiv.org/abs/2403.12428](https://arxiv.org/abs/2403.12428)

    提出了一种基于奖励样本转移的算法，在顺序多臂老虎机问题中显著改进了累积遗憾性能

    

    我们考虑了一个顺序随机多臂老虎机问题，在这个问题中，代理与老虎机在多个轮次中进行交互。臂的奖励分布在一个轮次中保持不变，但在不同轮次之间可能会发生变化。我们提出了一种基于UCB的算法，用于从先前轮次中转移奖励样本，并改善所有轮次中的累积遗憾性能。我们为我们的算法提供了遗憾性能分析和实证结果，表明相比没有转移的标准UCB算法，我们的算法有显著改进。

    arXiv:2403.12428v1 Announce Type: new  Abstract: We consider a sequential stochastic multi-armed bandit problem where the agent interacts with bandit over multiple episodes. The reward distribution of the arms remain constant throughout an episode but can change over different episodes. We propose an algorithm based on UCB to transfer the reward samples from the previous episodes and improve the cumulative regret performance over all the episodes. We provide regret analysis and empirical results for our algorithm, which show significant improvement over the standard UCB algorithm without transfer.
    
[^63]: Jetfire: 使用INT8数据流和按块量化的高效准确Transformer预训练方法

    Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization

    [https://arxiv.org/abs/2403.12422](https://arxiv.org/abs/2403.12422)

    Jetfire提出了一种针对transformers的高效准确的INT8训练方法，实现了与FP16训练基线相当的精度，为标准transformer块提供了1.42倍的训练加速和1.49倍的内存节省。

    

    预训练transformer通常耗时较长。完全量化训练（FQT）是一种加速预训练的有前途的方法。然而，大多数FQT方法采用量化-计算-反量化的过程，这往往导致在transformers中使用时出现次优的加速和显著的性能降级，原因是高内存访问开销和低精度计算。在这项工作中，我们提出了Jetfire，一种针对transformers的高效准确INT8训练方法。我们的方法采用INT8数据流来优化内存访问，并采用按块量化方法来保持预先训练的transformers的准确性。大量实验证明，我们的INT8 FQT方法达到了与FP16训练基线相当的精度，并且在transformers的INT8训练方面优于现有方法。此外，针对标准的transformer块，我们的方法提供了1.42倍的端到端训练加速和1.49倍的内存节省。

    arXiv:2403.12422v1 Announce Type: new  Abstract: Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory
    
[^64]: STG-Mamba: 通过选择性状态空间模型进行时空图学习

    STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model

    [https://arxiv.org/abs/2403.12418](https://arxiv.org/abs/2403.12418)

    STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    

    Spatial-Temporal Graph（STG）数据具有动态性、异质性和非平稳性特点，导致时空图学习持续面临挑战。近年来，提出了各种基于GNN的方法，主要集中于模拟STG网络中节点个体之间的关系，忽略了随时间存在的STG系统本质特征的建模重要性。相反，现代选择性状态空间模型（SSSMs）提出了一种将STG网络视为系统的新方法，并精心探索了STG系统在时间维度上的动态状态演变。在本工作中，我们引入了Spatial-Temporal Graph Mamba（STG-Mamba），作为首个利用强大的选择性状态空间模型进行STG学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
    
[^65]: 关于主动推断中的预测规划和反事实学习

    On Predictive planning and counterfactual learning in active inference

    [https://arxiv.org/abs/2403.12417](https://arxiv.org/abs/2403.12417)

    主动推断中提出了基于“规划”和“从经验中学习”两种决策方案，并引入了一个混合模型以平衡数据复杂性，提供了可解释的智能决策制定框架。

    

    随着人工智能的快速发展，理解智能行为的基础变得越来越重要。作为一种行为的通用理论，主动推断提供了一种原则性的方法来探究规划和决策制定中的复杂性基础。本文研究了主动推断中基于“规划”和“从经验中学习”两种决策方案。此外，我们还介绍了一个混合模型，以在这些策略之间平衡数据复杂性与协作，利用两者的优势促进平衡决策制定。我们在一个需要代理适应性的具有挑战性的网格世界情景中评估了我们提出的模型。此外，我们的模型提供了分析各种参数演变的机会，提供宝贵的见解，并为智能决策制定提供了一个可解释的框架。

    arXiv:2403.12417v1 Announce Type: new  Abstract: Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.
    
[^66]: 通过经验背景和布朗运动进行羽毛球运动员行为的离线模仿

    Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion

    [https://arxiv.org/abs/2403.12406](https://arxiv.org/abs/2403.12406)

    本文提出了RallyNet，这是一种新颖的用于羽毛球运动员行为的分层离线模仿学习模型，能够捕捉选手的决策依赖关系，解决了直接应用现有方法时可能遇到的层次结构和轮流采取行动导致的复合效应问题。

    

    在动态和快节奏的基于轮次的体育运动中，羽毛球作为一种需要选手依赖变化的决策的固有范例而脱颖而出。虽然在顺序决策的离线专家数据中学习的进展在各个领域中都有所涉及，但如何从离线羽毛球比赛中模仿人类选手的比赛行为在很大程度上尚未被探索。复制对手的行为有益于选手，使他们能够在比赛前有方向地进行战略发展。然而，直接应用现有方法会受到比赛的内在层次结构和由于轮流采取行动的选手轮次性质而产生的复合效应的困扰。在本文中，我们提出了RallyNet，一种新颖的用于羽毛球运动员行为的分层离线模仿学习模型：（i）RallyNet通过将决策过程建模为...

    arXiv:2403.12406v1 Announce Type: new  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as 
    
[^67]: 理解无需训练的扩散引导：机制与局限性

    Understanding Training-free Diffusion Guidance: Mechanisms and Limitations

    [https://arxiv.org/abs/2403.12404](https://arxiv.org/abs/2403.12404)

    本文旨在深入理解无需训练的扩散引导的操作机制和基本限制，提供了理论分析支持该方法，同时揭示了其更易受到对抗性梯度影响和较慢收敛的缺点。

    

    向预先训练的扩散模型添加额外控制已成为越来越流行的研究领域，在计算机视觉、强化学习和科学人工智能等领域有广泛应用。最近，一些研究提出使用在干净图像上预训练的现成网络进行无需训练的扩散引导。这种方法实现了零样本条件生成，适用于通用控制格式，看起来提供了无需训练的扩散引导中的免费午餐。本文旨在对无需训练的引导的运行机制和基本限制进行更深入的理解。我们提供了一项支持无需训练引导的理论分析，从优化的角度区分了它与基于分类器的（或者无分类器的）引导。为了阐明它们的缺点，我们在理论上证明了无需训练方法更容易受到对抗性梯度的影响，并表现出更慢的收敛速度。

    arXiv:2403.12404v1 Announce Type: new  Abstract: Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free diffusion guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of the operational mechanisms and fundamental limitations of training-free guidance. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free methods are more susceptible to adversarial gradients and exhibit slower conv
    
[^68]: 利用大型语言模型提取的原因生成可解释的仇恨言论检测方法

    Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales

    [https://arxiv.org/abs/2403.12403](https://arxiv.org/abs/2403.12403)

    利用大型语言模型提取原因特征训练仇恨言论分类器，实现可解释的检测方法

    

    尽管社交媒体平台是用户进行人际讨论和表达观点的重要场所，但社交媒体提供的外立面和匿名性可能导致用户发布仇恨言论和冒犯性内容。鉴于这些平台的庞大规模，自动识别和标记仇恨言论的需求日益迫切。尽管存在几种仇恨言论检测方法，但大多数黑盒方法在设计上不具有可解释性或可解释性。为解决解释性不足，本文提出使用最先进的大型语言模型（LLM）从输入文本中提取原因特征，训练基础仇恨言论分类器，从而通过设计实现忠实的可解释性。我们的框架有效地结合了LLM的文本理解能力和最先进仇恨言论分类器的判别能力，使这些分类器

    arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
    
[^69]: 寻找丢失的数据：一种受BERT启发的方法应对无线传感中的数据包丢失

    Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing

    [https://arxiv.org/abs/2403.12400](https://arxiv.org/abs/2403.12400)

    提出了一种基于BERT的深度学习模型CSI-BERT，可以在目标数据集上以自监督方式进行训练，捕捉不同子载波之间的顺序关系，从而实现更低的错误率和更快的速度，即使面对高丢包率。

    

    尽管已经开发了各种用于Wi-Fi传感的深度学习方法，但数据包丢失常常导致信道状态信息（CSI）的非连续估计，从而对学习模型的性能产生负面影响。为了克服这一挑战，我们提出了一种基于双向编码器表示转换器（BERT）的CSI恢复深度学习模型，称为CSI-BERT。CSI-BERT可以在目标数据集上以自监督方式进行训练，而无需额外的数据。此外，与传统的插值方法不同，传统方法通常只关注一个子载波，CSI-BERT捕捉了不同子载波之间的顺序关系。实验结果表明，与传统插值方法相比，即使面对高丢包率，CSI-BERT实现了更低的错误率和更快的速度。此外，通过利用从CSI-BERT获得的恢复的CSI，其他深度学习方法能够更好地处理无线频谱感知问题。

    arXiv:2403.12400v1 Announce Type: cross  Abstract: Despite the development of various deep learning methods for Wi-Fi sensing, package loss often results in noncontinuous estimation of the Channel State Information (CSI), which negatively impacts the performance of the learning models. To overcome this challenge, we propose a deep learning model based on Bidirectional Encoder Representations from Transformers (BERT) for CSI recovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner on the target dataset without the need for additional data. Furthermore, unlike traditional interpolation methods that focus on one subcarrier at a time, CSI-BERT captures the sequential relationships across different subcarriers. Experimental results demonstrate that CSI-BERT achieves lower error rates and faster speed compared to traditional interpolation methods, even when facing with high loss rates. Moreover, by harnessing the recovered CSI obtained from CSI-BERT, other deep learning
    
[^70]: 将网络选举化：用于社区拉票的动态多步对抗性攻击

    Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing

    [https://arxiv.org/abs/2403.12399](https://arxiv.org/abs/2403.12399)

    本论文提出了用于社区拉票的动态多步对抗性攻击，使得对手能够利用基于梯度的攻击来策划目标选民的操纵。

    

    在今天的世界中，对于社区拉票的在线社交网络操纵问题是一个真正关注的问题。受选民模型、网络上的观点和极化动态的研究启发，我们将社区拉票建模为一个通过对GNN进行基于梯度的攻击而在网络上进行的动态过程。现有的GNN攻击都是单步的，没有考虑网络中信息传播的动态级联特性。我们考虑了一个现实的场景，即对手使用GNN作为代理来预测和操纵选民偏好，特别是不确定的选民。对GNN的基于梯度的攻击通知对手可以进行战略操纵，以使得目标选民入教。具体而言，我们探讨了$\textit{社区拉票的最小预算攻击}$（MBACC）。我们证明了MBACC问题是NP困难的，并提出了动态多步对抗性社区拉票（MAC）来解决这一问题。MAC m

    arXiv:2403.12399v1 Announce Type: new  Abstract: The problem of online social network manipulation for community canvassing is of real concern in today's world. Motivated by the study of voter models, opinion and polarization dynamics on networks, we model community canvassing as a dynamic process over a network enabled via gradient-based attacks on GNNs. Existing attacks on GNNs are all single-step and do not account for the dynamic cascading nature of information diffusion in networks. We consider the realistic scenario where an adversary uses a GNN as a proxy to predict and manipulate voter preferences, especially uncertain voters. Gradient-based attacks on the GNN inform the adversary of strategic manipulations that can be made to proselytize targeted voters. In particular, we explore $\textit{minimum budget attacks for community canvassing}$ (MBACC). We show that the MBACC problem is NP-Hard and propose Dynamic Multi-Step Adversarial Community Canvassing (MAC) to address it. MAC m
    
[^71]: FairSTG: 通过协作样本级优化对抗性能异质性

    FairSTG: Countering performance heterogeneity via collaborative sample-level optimization

    [https://arxiv.org/abs/2403.12391](https://arxiv.org/abs/2403.12391)

    FairSTG通过协作样本级优化对抗性能异质性，提出了一个模型独立的面向公平性的时空图学习框架，通过协作挑战性样本与已学习样本的 mix-up 实现知识转移。

    

    arXiv:2403.12391v1 公告类型：跨越  摘要：时空学习在移动计算技术中发挥着关键作用，以赋予智能城市更强大的功能。尽管现有研究已经在整个数据集上取得了准确的预测，但它们仍然忽视了样本之间的显著性能异质性。在这项工作中，我们将性能异质性定为不公平时空学习的原因，这不仅会降低模型的实际功能，还会给现实世界的城市应用带来严重的潜在风险。为了填补这一空白，我们提出了一个模型独立的面向公平性的时空图学习框架FairSTG，借鉴了利用已充分学习样本的优势来协作挑战性样本的思想。具体而言，FairSTG包括一个时空特征提取器用于模型初始化，一个用于知识转移的协作表示增强模块，从已充分学习样本到困难样本。

    arXiv:2403.12391v1 Announce Type: cross  Abstract: Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples a
    
[^72]: 一种用于多模态推荐的对齐和训练框架

    An Aligning and Training Framework for Multimodal Recommendations

    [https://arxiv.org/abs/2403.12384](https://arxiv.org/abs/2403.12384)

    提出了一种名为AlignRec的对齐和训练框架，用于解决多模态推荐中的不对齐问题，通过将推荐目标分解为三个对齐部分，实现内容内部对齐、内容与分类ID之间的对齐以及用户和项目之间的对齐。

    

    随着多媒体应用的发展，多模态推荐正在发挥着重要作用，因为它们可以利用超越用户交互的丰富上下文。现有方法主要将多模态信息视为辅助，用于帮助学习ID特征；然而，多模态内容特征和ID特征之间存在语义差距，直接将多模态信息作为辅助使用会导致用户和项目表示的不对齐。本文首先系统地研究了多模态推荐中的不对齐问题，并提出了一种名为AlignRec的解决方案。在AlignRec中，推荐目标被分解为三个对齐部分，即内容内部对齐，内容与分类ID之间的对齐以及用户和项目之间的对齐。每个对齐部分都由特定的目标函数来表征，并整合到我们的多模态推荐中。

    arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
    
[^73]: 零样本自监督盲图像去噪的低迹适应

    Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising

    [https://arxiv.org/abs/2403.12382](https://arxiv.org/abs/2403.12382)

    提出了一种低迹适应的Noise2Noise（LoTA-N2N）模型，通过引入迹项的优化目标减少了自监督和监督方法之间的差异，提高了自监督学习的性能。

    

    深度学习的去噪器已经成为图像去噪领域最近发展的焦点。在过去几年里，人们越来越关注开发自监督去噪网络，这些网络只需嘈杂图像，而无需干净的基准图像进行训练。然而，目前的自监督方法与监督方法之间仍存在性能差距。此外，这些方法通常依赖于对噪声特性的假设，从而限制了它们在真实场景中的适用性。受Frobenius范数扩展特性的启发，我们发现加入一个迹项可以减少自监督和监督方法之间的优化目标差异，从而提高自监督学习的性能。为了利用这一洞见，我们提出了一个迹约束损失函数，并设计了低迹适应Noise2Noise（LoTA-N2N）模型，弥合了

    arXiv:2403.12382v1 Announce Type: cross  Abstract: Deep learning-based denoiser has been the focus of recent development on image denoising. In the past few years, there has been increasing interest in developing self-supervised denoising networks that only require noisy images, without the need for clean ground truth for training. However, a performance gap remains between current self-supervised methods and their supervised counterparts. Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios. Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between self-supervised and supervised methods, thereby enhancing the performance of self-supervised learning. To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges 
    
[^74]: 使用语言模型的跨领域预训练学习可传递的时间序列分类器

    Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model

    [https://arxiv.org/abs/2403.12372](https://arxiv.org/abs/2403.12372)

    提出了CrossTimeNet，一个新颖的跨领域自监督预训练学习框架，旨在解决不同领域时间序列数据特性差异带来的挑战，并能有效转换原始时间序列数据。

    

    自监督预训练（SSL）的进展显著推动了可传递的时间序列表示领域的发展，这对于增强下游任务非常有用。尽管有效，但大多数现有工作在跨领域SSL预训练方面存在困难，错过了集成不同领域模式和特征的宝贵机会。我们提出了CrossTimeNet，这是一个新颖的跨领域SSL学习框架，旨在从各种领域学习可传递的知识，从而大大增强目标下游任务的效果。

    arXiv:2403.12372v1 Announce Type: new  Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a seque
    
[^75]: 用多模态语言建模推动时间序列分类

    Advancing Time Series Classification with Multimodal Language Modeling

    [https://arxiv.org/abs/2403.12371](https://arxiv.org/abs/2403.12371)

    用多模态语言建模的方法提出了InstructTime，将时间序列分类重新形成为一个学习生成的范例，以解决目标标签编码和跨领域模型学习的问题

    

    针对时间序列分类的进展，通过审查先前的研究，大多数现有方法采用了一种常见的学习分类范式 - 时间序列分类模型试图学习序列输入和目标标签之间的关系，这些目标标签由一热分布编码。尽管有效，但这种范式隐藏了两个固有限制：(1) 使用一热分布对目标类别进行编码不能反映标签之间的可比性和相似性，(2) 非常难以在领域间学习可转移模型，这极大地阻碍了通用服务范式的发展。在这项工作中，我们提出了InstructTime，一个重塑时间序列分类为学习生成范式的新尝试。依靠预训练语言模型强大的生成能力，核心思想是将时间序列的分类作为一个多模态理解任务来制定，其中任务特定

    arXiv:2403.12371v1 Announce Type: new  Abstract: For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specif
    
[^76]: 半监督计分匹配算法评估公共卫生干预效果

    Semisupervised score based matching algorithm to evaluate the effect of public health interventions

    [https://arxiv.org/abs/2403.12367](https://arxiv.org/abs/2403.12367)

    提出了一种基于二次评分函数的一对一匹配算法，通过设计权重最小化配对训练单元之间的得分差异，同时最大化未配对训练单元之间的得分差异

    

    多元匹配算法在观察性研究中“配对”相似的研究单元，以消除由于缺乏随机性而引起的潜在偏倚和混杂效应。我们提出了一种基于二次评分函数的新型一对一匹配算法，权重$\beta$被设计为最小化配对训练单元之间的得分差异，同时最大化未配对训练单元之间的得分差异。

    arXiv:2403.12367v1 Announce Type: cross  Abstract: Multivariate matching algorithms "pair" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of "pairs" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a "training" set of paired units by domain experts, is practically intriguing.   We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights $\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units
    
[^77]: U-Net Kalman Filter（UNetKF）：一种机器学习辅助的集合数据同化示例

    U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation

    [https://arxiv.org/abs/2403.12366](https://arxiv.org/abs/2403.12366)

    本文使用U-Net神经网络预测EnKF算法中的局部集合协方差，提出了UNetKF方法，其性能可以与传统方法相匹配或超越。

    

    机器学习技术在天气和气候科学中越来越受欢迎。数据同化（DA）将观测和数值模型相结合，有很大潜力用于融合机器学习和人工智能（ML/AI）技术。本文使用U-Net，一种卷积神经网络（CNN）类型，来预测集合卡尔曼滤波器（EnKF）算法的局部集合协方差。在一个2层准地转流模型中，使用EnKF DA实验的数据训练U-Nets。训练后的U-Nets被用于预测与流动相关的局部误差协方差矩阵，在U-Net Kalman Filter（UNetKF）实验中进行比较，这些结果与传统的三维变分（3DVar）、集合三维变分（En3DVar）和EnKF方法进行了比较。UNetKF的性能可以与3DVar、En3DVar或EnKF相匹配或超越。我们还展示了经过训练的U-Nets可以转移到更高的地方。

    arXiv:2403.12366v1 Announce Type: new  Abstract: Machine learning techniques have seen a tremendous rise in popularity in weather and climate sciences. Data assimilation (DA), which combines observations and numerical models, has great potential to incorporate machine learning and artificial intelligence (ML/AI) techniques. In this paper, we use U-Net, a type of convolutional neutral network (CNN), to predict the localized ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm. Using a 2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA experiments. The trained U-Nets are then used to predict the flow-dependent localized error covariance matrices in U-Net Kalman Filter (UNetKF) experiments, which are compared to traditional 3-dimensional variational (3DVar), ensemble 3DVar (En3DVar) and EnKF methods. The performance of UNetKF can match or exceed that of 3DVar, En3DVar or EnKF. We also demonstrate that trained U-Nets can be transferred to a higher-r
    
[^78]: DMAD: 用于现实世界异常检测的双存储器银行

    DMAD: Dual Memory Bank for Real-World Anomaly Detection

    [https://arxiv.org/abs/2403.12362](https://arxiv.org/abs/2403.12362)

    DMAD提出了一个双存储器银行的框架，用于处理统一的无监督和半监督异常检测场景，通过计算特征距离和特征注意力来增强表示学习，从而提高异常检测效果。

    

    训练一个统一模型被认为更适用于实际工业异常检测场景，因为它具有泛化能力和存储效率。然而，这种独家使用正常数据的多类设置忽视了现实世界中少量但重要的可访问异常注释。为了解决现实世界异常检测的挑战，我们提出了一个名为DMAD的新框架，该框架增强了用于异常检测的表示学习。这个框架在一个统一（多类）设置中处理无监督和半监督场景。DMAD采用双存储器银行来计算正常和异常模式之间的特征距离和特征注意力，从而封装关于正常和异常实例的知识。然后利用这些知识来构建用于异常分数学习的增强表示。我们在MVTec-AD和VisA数据集上评估了DMAD。

    arXiv:2403.12362v1 Announce Type: cross  Abstract: Training a unified model is considered to be more suitable for practical industrial anomaly detection scenarios due to its generalization ability and storage efficiency. However, this multi-class setting, which exclusively uses normal data, overlooks the few but important accessible annotated anomalies in the real world. To address the challenge of real-world anomaly detection, we propose a new framework named Dual Memory bank enhanced representation learning for Anomaly Detection (DMAD). This framework handles both unsupervised and semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a dual memory bank to calculate feature distance and feature attention between normal and abnormal patterns, thereby encapsulating knowledge about normal and abnormal instances. This knowledge is then used to construct an enhanced representation for anomaly score learning. We evaluated DMAD on the MVTec-AD and VisA datasets. The resu
    
[^79]: 重建光谱学中的Sim2Real：利用增强设备信息数据模拟的深度学习

    Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation

    [https://arxiv.org/abs/2403.12354](https://arxiv.org/abs/2403.12354)

    提出了一种名为Sim2Real的深度学习框架，针对重建光谱学中的光谱信号重建问题，通过设备信息模拟数据的层次化增强策略实现了推理速度的显著提升。

    

    本文提出了一种基于深度学习（DL）的框架，即Sim2Real，用于重建光谱信号，在有效数据采样和快速推理时间方面进行了重点研究。该工作聚焦于在仅有设备信息模拟数据可用于训练的极端情况下，重建真实世界光谱信号的挑战。为了有效利用这种模拟的数据，引入了一种分层数据增强策略，以缓解领域转移的不利影响，并设计了一个用于光谱信号重建的神经网络。通过使用从我们的分光仪设备测量的真实数据集进行的实验，证明了Sim2Real在推理过程中取得了显著的加速。

    arXiv:2403.12354v1 Announce Type: new  Abstract: This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient data sampling and fast inference time. The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated data are available for training. Such device-informed simulated data are much easier to collect than real-world data but exhibit large distribution shifts from their real-world counterparts. To leverage such simulated data effectively, a hierarchical data augmentation strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented data is designed. Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference w
    
[^80]: 友好的锐度感知最小化

    Friendly Sharpness-Aware Minimization

    [https://arxiv.org/abs/2403.12350](https://arxiv.org/abs/2403.12350)

    通过研究锐度感知最小化（SAM）的核心组件，引入“友好SAM”（F-SAM）进一步增强泛化性能，发现了批特定随机梯度噪声在对抗性扰动中扮演的关键角色，从而提升SAM的泛化能力。

    

    锐度感知最小化（SAM）在改善深度神经网络训练方面发挥了重要作用，通过最小化训练损失和损失的锐度。尽管在实际中取得了成功，但SAM背后的增强泛化的机制仍然不清楚，限制了其在深度学习优化中的进展。在这项工作中，我们研究了SAM的核心组件以提升泛化，引入了“友好SAM”（F-SAM）来进一步增强SAM的泛化。我们的研究揭示了批特定随机梯度噪声在对抗扰动中的关键作用，即当前小批量梯度，这显著影响了SAM的泛化性能。通过将SAM中的对抗性扰动分解为完整梯度和随机梯度噪声组件，我们发现仅依赖完整梯度组件会降低泛化能力，而排除完整梯度组件会导致性能提升。

    arXiv:2403.12350v1 Announce Type: new  Abstract: Sharpness-Aware Minimization (SAM) has been instrumental in improving deep neural network training by minimizing both training loss and loss sharpness. Despite the practical success, the mechanisms behind SAM's generalization enhancements remain elusive, limiting its progress in deep learning optimization. In this work, we investigate SAM's core components for generalization improvement and introduce "Friendly-SAM" (F-SAM) to further enhance SAM's generalization. Our investigation reveals the key role of batch-specific stochastic gradient noise within the adversarial perturbation, i.e., the current minibatch gradient, which significantly influences SAM's generalization performance. By decomposing the adversarial perturbation in SAM into full gradient and stochastic gradient noise components, we discover that relying solely on the full gradient component degrades generalization while excluding it leads to improved performance. The possibl
    
[^81]: 随机Halpern迭代在赋范空间中的应用及其在强化学习中的应用

    Stochastic Halpern iteration in normed spaces and applications to reinforcement learning

    [https://arxiv.org/abs/2403.12338](https://arxiv.org/abs/2403.12338)

    该论文分析了随机Halpern迭代在赋范空间中的Oracle复杂度，提出了改进的算法复杂度，进而在强化学习中提出了新的同步算法应用。

    

    我们分析了具有方差减少的随机Halpern迭代的Oracle复杂度，旨在近似有界和收缩算子的不动点在一个有限维赋范空间中。我们表明，如果底层的随机Oracle具有一致有界的方差，则我们的方法展现出总的Oracle复杂度为$ \tilde{O} (\varepsilon^{-5})$，改进了最近为随机Krasnoselskii-Mann迭代建立的速率。此外，我们建立了 $\Omega (\varepsilon^{-3})$的下界，适用于广泛范围的算法，包括所有带有小批处理的平均迭代。通过适当修改我们的方法，我们推导出了在算子为 $\gamma$-收缩的情况下一个 $O(\varepsilon^{-2}(1-\gamma)^{-3})$复杂度上界。作为一个应用，我们提出了新的用于平均奖励和折扣奖励马尔可夫决策过程的同步算法。

    arXiv:2403.12338v1 Announce Type: cross  Abstract: We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, f
    
[^82]: 一种用于预测动态系统的时间一致的Koopman自编码器

    Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems

    [https://arxiv.org/abs/2403.12335](https://arxiv.org/abs/2403.12335)

    引入了时间一致Koopman自编码器（tcKAE）来生成准确的长期预测，在有限嘈杂的训练数据下通过一致性正则化项增强了模型的稳健性和泛化能力。

    

    缺乏足够高质量的数据经常是高维时空动态系统数据驱动建模中关键挑战。Koopman自编码器（KAEs）利用深度神经网络（DNNs）的表达能力、自编码器的降维能力以及Koopman算子的谱特性，学习具有更简单线性动态的降阶特征空间。然而，KAEs的有效性受限于有限而嘈杂的训练数据集，导致泛化能力较差。为解决这一问题，我们引入了一种称为时间一致Koopman自编码器（tcKAE）的模型，旨在即使在受限且嘈杂的训练数据情况下生成准确的长期预测。这是通过强制在不同时间步上保持预测一致性的一致性正则化项实现的，从而增强了tcKAE相对于现有模型的稳健性和泛化能力。

    arXiv:2403.12335v1 Announce Type: new  Abstract: Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justifica
    
[^83]: FedFisher：利用Fisher信息进行一次性联邦学习

    FedFisher: Leveraging Fisher Information for One-Shot Federated Learning

    [https://arxiv.org/abs/2403.12329](https://arxiv.org/abs/2403.12329)

    该论文提出了FedFisher算法，通过利用Fisher信息矩阵进行一次性联邦学习，能够在一次通信中训练出全局模型，并在理论上证明了随着神经网络宽度和客户端本地训练量的增加，FedFisher全局模型的误差会变得非常小。

    

    标准的联邦学习(FL)算法通常需要服务器和客户端之间的多轮通信，这具有几个缺点，包括需要恒定的网络连通性，重复投入计算资源，以及容易受到隐私攻击的影响。一次性FL是一种新的范例，旨在通过使服务器在一轮通信中训练全局模型来解决这一挑战。在这项工作中，我们提出了FedFisher，一种新颖的用于一次性FL的算法，该算法利用在本地客户端模型上计算的Fisher信息矩阵，受到FL的贝叶斯视角的启发。首先，我们从两层过参数化的ReLU神经网络的理论角度对FedFisher进行了分析，并展示了我们的一次性FedFisher全局模型的误差会在神经网络的宽度和客户端本地训练量增加时变得无限小。接下来，我们提出了实用的F的变种

    arXiv:2403.12329v1 Announce Type: new  Abstract: Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of F
    
[^84]: 生成文本流中漂移的方法

    Methods for Generating Drift in Text Streams

    [https://arxiv.org/abs/2403.12328](https://arxiv.org/abs/2403.12328)

    文本数据中概念漂移是一个常见现象，而本文提出了四种文本漂移生成方法来帮助产生具有标记漂移的数据集

    

    arXiv：2403.12328v1 公告类型：跨越 摘要：系统和个体不断产生数据。 在互联网上，人们分享他们的知识，情感和意见，提供关于服务和产品的评论等。 自动从这些文本数据中学习可以为组织和机构提供见解，从而防止财务影响，例如。 为了随时间学习文本数据，机器学习系统必须考虑概念漂移。 概念漂移是现实世界数据集中的频繁现象，对应于时间上的数据分布更改。 例如，当情感变化或单词含义随时间调整时，就会发生概念漂移。 尽管概念漂移在实际应用中很常见，但具有标记漂移的基准数据集在文献中很少见。 为弥补这一差距，本文提供了四种文本漂移生成方法，以便简化产生具有标记漂移的数据集。 这些方法已应用于Ye

    arXiv:2403.12328v1 Announce Type: cross  Abstract: Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Ye
    
[^85]: GT-Rain单图像去雨挑战报告

    GT-Rain Single Image Deraining Challenge Report

    [https://arxiv.org/abs/2403.12327](https://arxiv.org/abs/2403.12327)

    该报告回顾了GT-Rain单图像去雨挑战的结果，旨在研究真实场景中的雨天气象现象，并提供新颖的真实世界雨天图像数据集，激发创新思路以促进单图像去雨方法在真实图像上的发展。

    

    本报告回顾了2023年CVPR UG2+研讨会上GT-Rain单图像去雨挑战的结果。该竞赛旨在研究真实场景中的多雨天气现象，提供一个新颖的真实世界雨天图像数据集，并激发进一步发展真实图像单图像去雨方法的创新思路。参赛者在GT-Rain数据集上进行训练，并在包含15个额外场景的数据集扩展上进行评估。GT-Rain中的场景由真实雨天图像和雨停后拍摄的真实图像组成。挑战中共有275名参与者注册，其中55名参加了最终的测试阶段。

    arXiv:2403.12327v1 Announce Type: cross  Abstract: This report reviews the results of the GT-Rain challenge on single image deraining at the UG2+ workshop at CVPR 2023. The aim of this competition is to study the rainy weather phenomenon in real world scenarios, provide a novel real world rainy image dataset, and to spark innovative ideas that will further the development of single image deraining methods on real images. Submissions were trained on the GT-Rain dataset and evaluated on an extension of the dataset consisting of 15 additional scenes. Scenes in GT-Rain are comprised of real rainy image and ground truth image captured moments after the rain had stopped. 275 participants were registered in the challenge and 55 competed in the final testing phase.
    
[^86]: 使用可学习提示从文本到图像生成模型中去除不良概念

    Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts

    [https://arxiv.org/abs/2403.12326](https://arxiv.org/abs/2403.12326)

    通过引入可学习提示到交叉注意力模块中，本文提出了一种新方法，用于从文本到图像生成模型中去除不良概念，实现了对模型效果的提升。

    

    生成模型已经展示出在从文本描述中生成视觉上令人印象深刻的内容方面具有显著潜力。然而，在未经筛选的互联网数据上训练这些模型存在学习和随后传播不良概念（如受版权保护或不道德内容）的风险。在本文中，我们提出了一种新方法，通过将可学习提示结合到交叉注意力模块中，从文本到图像生成模型中去除不良概念。这可学习提示充当附加内存，将不良概念的知识转移到其中，并减少这些概念对模型参数和相应文本输入的依赖。由于这种知识转移到提示中，消除这些不良概念更加稳定，并对其他概念影响最小。我们在稳定扩散模型上展示了我们方法的有效性，展示了其优势。

    arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
    
[^87]: 利用嵌入式设备上的超维计算增强皮下酒精水平检测

    Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional Computing on Embedded Devices

    [https://arxiv.org/abs/2403.12323](https://arxiv.org/abs/2403.12323)

    通过使用Hyperdimensional Computing（HDC）设计适用于智能手机、智能可穿戴设备和物联网的及时干预方法，增强了皮下酒精水平检测的效果。

    

    酒精消费对个人健康有重要影响，过量饮酒会导致更严重后果。促进健康饮酒习惯的一种方法是实施及时干预，即在酗酒时段发送指示醉酒的及时通知。然而，干预机制的复杂性或侵入性可能会阻止个人在实践中使用它们。先前的研究使用收集的运动数据和传统机器学习（ML）算法来对酗酒时段进行分类，但对于移动设备来说，其准确性和计算效率并不实用。因此，我们选择使用超维计算（HDC）设计一个适用于智能手机、智能可穿戴设备和物联网部署的及时干预方法。HDC是一个在处理实时传感器数据方面取得验证结果的框架。

    arXiv:2403.12323v1 Announce Type: new  Abstract: Alcohol consumption has a significant impact on individuals' health, with even more pronounced consequences when consumption becomes excessive. One approach to promoting healthier drinking habits is implementing just-in-time interventions, where timely notifications indicating intoxication are sent during heavy drinking episodes. However, the complexity or invasiveness of an intervention mechanism may deter an individual from using them in practice. Previous research tackled this challenge using collected motion data and conventional Machine Learning (ML) algorithms to classify heavy drinking episodes, but with impractical accuracy and computational efficiency for mobile devices. Consequently, we have elected to use Hyperdimensional Computing (HDC) to design a just-in-time intervention approach that is practical for smartphones, smart wearables, and IoT deployment. HDC is a framework that has proven results in processing real-time sensor
    
[^88]: 近似似然比：Boosting神经网络训练的前向并行框架

    Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training

    [https://arxiv.org/abs/2403.12320](https://arxiv.org/abs/2403.12320)

    该论文提出了一种近似似然比方法，通过在反向传递过程中利用自然并行性，提供了一种高性能训练策略，以减轻神经网络训练中的计算和内存需求。

    

    arXiv:2403.12320v1 发布类型: 跨领域 摘要: 在神经网络训练中，与反向传播相比，高计算复杂性和对神经网络的额外假设等问题使得高效、符合生物学的替代方案仍然是一个挑战，这些限制了对更深层次网络的可扩展性。似然比方法提供了一种有前途的梯度估计策略，但在部署多个数据副本以减少估计方差时，受到显著的内存消耗的限制。在本文中，我们引入了一种似然比（LR）方法的近似技术，以减轻梯度估计中的计算和内存需求。通过利用LR在反向传递过程中的自然并行性，我们进一步提供了一种高性能训练策略，该策略同时管道化了前向和反向传递，使其更适用于专用硬件的计算。大量实验证明了这种方法的有效性。

    arXiv:2403.12320v1 Announce Type: cross  Abstract: Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks. The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance. In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation. By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware. Extensive experiments demonstrate the effectiveness of the appr
    
[^89]: 在隐私保护联邦学习中改进 LoRA

    Improving LoRA in Privacy-preserving Federated Learning

    [https://arxiv.org/abs/2403.12313](https://arxiv.org/abs/2403.12313)

    该论文提出了一种在隐私保护联邦学习中改进LoRA方法，解决了在此设置下LoRA不稳定的问题，特别是由于数据异构性、差分隐私噪声放大和超参数的敏感性引起的挑战。

    

    低秩适应（LoRA）是最流行的面向任务特定参数高效微调（PEFT）方法之一，对于预训练语言模型具有良好的性能和计算效率。LoRA在每个冻结的预训练模型模块顶部注入了两个可训练秩分解矩阵的乘积。然而，在隐私保护联邦学习设置中应用时，LoRA可能由于以下事实而变得不稳定：1）数据异构性和多步本地更新的影响不可忽略，2）为了保证差分隐私（DP）而强制执行的添加噪声可能会被放大，3）最终性能容易受到超参数的影响。导致这些现象的关键因素之一是本地客户端联合优化两个低秩矩阵与中央服务器分别聚合它们之间的不一致。因此，本文提出了一种高效且有效的方法

    arXiv:2403.12313v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effectiv
    
[^90]: 通过世界模型从延迟观察中进行强化学习

    Reinforcement Learning from Delayed Observations via World Models

    [https://arxiv.org/abs/2403.12309](https://arxiv.org/abs/2403.12309)

    本文提出了一种通过世界模型处理观察延迟的方法，可有效处理部分可观察性，相比现有方法，实验表明其中一种方法可以胜过朴素方法达到30%的性能提升。

    

    在标准的强化学习设置中，代理通常假定在采取行动后立即获得关于行动效果的反馈。然而，在实践中，由于物理限制，这一假设可能不成立，这可能会严重影响RL算法的性能。本文侧重于解决部分可观察环境中的观察延迟问题。我们提出利用世界模型来处理观察延迟，世界模型已经在整合过去观察和学习动态方面取得成功。通过将延迟POMDP降低为具有世界模型的延迟MDP，我们的方法可以有效处理部分可观察性，其中现有方法在可观察性降低时实现次优性能甚至迅速下降。实验证明，我们的其中一种方法可以在视觉输入延迟环境下胜过朴素的基于模型的方法达到30%。此外，我们还在视觉输入延迟环境中评估了我们的方法。

    arXiv:2403.12309v1 Announce Type: cross  Abstract: In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the f
    
[^91]: 使用高维图分类的分子分类

    Molecular Classification Using Hyperdimensional Graph Classification

    [https://arxiv.org/abs/2403.12307](https://arxiv.org/abs/2403.12307)

    我们的工作引入了一种基于高维计算的图学习方法，在分子分类中表现出与先进模型相媲美的结果，并且在性能上取得了显著的提升。

    

    我们的工作通过利用高维计算引入了一种创新的图学习方法。图作为一种广泛接受的信息传递方法，在学习中的利用引起了巨大关注。这在化学信息学领域尤为显著，那里从图表示中学习发挥着关键作用。该领域内一个重要的应用涉及跨不同分子结构识别癌细胞。我们提出了一个基于HDC的模型，展示了与先进模型如图神经网络（GNNs）或Weisfieler-Lehman图核（WL）相媲美的曲线下面积结果。此外，它在超越先前提出的高维计算图学习方法以及在性能提升方面取得了显著成果，培养阶段加速40倍，推断时间改进了15倍，相对于GNN

    arXiv:2403.12307v1 Announce Type: cross  Abstract: Our work introduces an innovative approach to graph learning by leveraging Hyperdimensional Computing. Graphs serve as a widely embraced method for conveying information, and their utilization in learning has gained significant attention. This is notable in the field of chemoinformatics, where learning from graph representations plays a pivotal role. An important application within this domain involves the identification of cancerous cells across diverse molecular structures.   We propose an HDC-based model that demonstrates comparable Area Under the Curve results when compared to state-of-the-art models like Graph Neural Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it outperforms previously proposed hyperdimensional computing graph learning methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x acceleration in the training phase and a 15x improvement in inference time compared to GNN a
    
[^92]: FinLlama：用于算法交易应用的金融情感分类

    FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications

    [https://arxiv.org/abs/2403.12285](https://arxiv.org/abs/2403.12285)

    引入了一种基于Llama 2 7B模型的金融领域特定的情感分类框架，通过微调模型来受益于其生成性质和全面的语言操作。

    

    金融新闻在线有多个来源影响市场走势和交易员的决策。这突出了准确情感分析的需求，除了需要适当的算法交易技术来做出更明智的交易决策。标准的基于词典的情感方法已经证明它们在辅助金融决策方面的能力。然而，众所周知它们存在与上下文敏感性和词序相关的问题。大型语言模型（LLMs）也可以在这个背景下使用，但它们不是特定于金融领域的，并且往往需要大量的计算资源。为了促进一种特定于金融领域的LLM框架，我们引入了一种基于Llama 2 7B基础模型的新方法，以便从其生成性质和全面的语言操作中受益。这是通过在少部分监督金融情感数据上微调Llama2 7B模型来实现的。

    arXiv:2403.12285v1 Announce Type: new  Abstract: There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment 
    
[^93]: 随机舍入隐式正则化高瘦矩阵

    Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices

    [https://arxiv.org/abs/2403.12278](https://arxiv.org/abs/2403.12278)

    随机舍入技术能有效隐式正则化高瘦矩阵，确保舍入后的矩阵具有完整的列秩。

    

    受到随机舍入在机器学习和大规模深度神经网络模型训练中的流行，我们考虑实矩阵$\mathbf{A}$的随机近似舍入，其中行数远远多于列数。我们提供了新颖的理论证据，并通过大量实验评估支持，高概率下，随机舍入矩阵的最小奇异值远离零--无论$\mathbf{A}$接近奇异还是$\mathbf{A}$奇异。换句话说，随机舍入\textit{隐式正则化}高瘦矩阵$\mathbf{A}$，使得舍入后的版本具有完整的列秩。我们的证明利用了随机矩阵理论中的有力结果，以及随机舍入误差不集中在低维列空间的思想。

    arXiv:2403.12278v1 Announce Type: new  Abstract: Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.
    
[^94]: 数据高效的对比语言-图像预训练：优先考虑数据质量而非数量

    Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity

    [https://arxiv.org/abs/2403.12267](https://arxiv.org/abs/2403.12267)

    改进预训练数据质量对于提高CLIP性能比增加数据量更为有效，本研究提出了首个针对CLIP的理论严谨的数据选择方法，大幅提升了泛化性能。

    

    对比语言-图像预训练（CLIP）是在大规模图像字幕数据集上学习表示，能够实现显著的零次通用化。然而，这样的模型需要大量的预训练数据。改进预训练数据的质量已被证明比增加数量更有效地提高了CLIP的性能。然而，找到能够证明达到最佳泛化效果的小训练数据子集一直是一个悬而未决的问题。在本文中，我们提出了第一个针对CLIP的理论严谨的数据选择方法。我们展示了能够证明实现卓越泛化性能的子集接近保留完整数据的图像和字幕的交叉协方差。我们在ConceptualCaptions3M和ConceptualCaptions12M上进行了大量实验证明，\method\找到的子集在ImageNet上的准确性比下一个最佳基线提高了2.7倍和1.4倍。

    arXiv:2403.12267v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \method\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet an
    
[^95]: 使用生成式深度学习设计自适应LPD雷达波形

    Adaptive LPD Radar Waveform Design with Generative Deep Learning

    [https://arxiv.org/abs/2403.12254](https://arxiv.org/abs/2403.12254)

    使用生成式深度学习设计自适应LPD雷达波形，以在不被发现的情况下有效地进行测距和感知

    

    我们提出了一种新颖的基于学习的方法，用于自适应生成低概率检测（LPD）雷达波形，使其与其操作环境融为一体。我们设计的波形旨在遵循一个与环境中的无线电频率（RF）背景无法区分的分布，同时仍然有效用于测距和感知。为此，我们使用了一种无监督的对抗性学习框架；我们的生成器网络生成旨在困惑评论家网络的波形，评论家网络被优化为区分生成的波形与背景。为了确保我们生成的波形对于感知仍然有效，我们引入并最小化了一个基于模糊函数的损失函数到生成的波形上。我们通过将我们生成的波形的单脉冲可检测性与使用单独训练的检测神经网络来比较传统LPD波形的性能来评估我们方法的性能。我们发现我们的方法能够ge

    arXiv:2403.12254v1 Announce Type: cross  Abstract: We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing. To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can ge
    
[^96]: 基于参考文献的指标在问句生成中被推翻

    Reference-based Metrics Disprove Themselves in Question Generation

    [https://arxiv.org/abs/2403.12242](https://arxiv.org/abs/2403.12242)

    基于参考文献的指标在问句生成中被推翻，作者提出了一个无需参考文献的多维标准评估方法。

    

    BLEU和BERTScore等基于参考文献的指标被广泛用于评估问句生成(QG)。本研究在SQuAD和HotpotQA等QG基准数据集上发现，使用人工编写的参考文献并不能保证基于参考文献的指标的有效性。大多数QG基准数据集只有一个参考文献；我们复制了注释过程并收集了另一个参考文献。预期好的指标应该对人工验证的问题的评分不会低于生成的问题。然而，在我们新收集的参考文献上，基于参考文献的指标的结果却证明了这些指标本身是错误的。我们提出了一个无需参考文献的指标，由多维标准组成，如自然性、可回答性和复杂性，利用大型语言模型。这些标准不受限于单个参考问题的句法或语义，该指标也不需要多样化的参考文献。实验证明我们的方法

    arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
    
[^97]: 面向资源受限的IoT环境的高效基于Transformer的超参数优化

    Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments

    [https://arxiv.org/abs/2403.12237](https://arxiv.org/abs/2403.12237)

    本文提出了一种通过将Transformer架构和演员-评论家强化学习模型相结合的新方法TRL-HPO，在资源受限的IoT环境中实现了高效的超参数优化，该方法在MNIST数据集上表现优良。

    

    超参数优化（HPO）过程对于找到表现最佳的卷积神经网络（CNNs）至关重要。HPO的自动化过程以其可观的计算占用和缺乏透明度而闻名；这两个因素在资源受限的物联网（IoT）环境中至关重要。本文通过提出一种结合Transformer架构和演员-评论家强化学习（RL）模型的新方法TRL-HPO，旨在解决这些问题，TRL-HPO配备了多头注意力，实现了并行化和渐进生成层。我们通过在MNIST数据集上评估TRL-HPO，并将其与从头开始构建CNN模型的最新方法进行比较，从而从经验上验证了这些假设。结果显示，在相同时间范围内，TRL-HPO的分类结果优于这些方法的结果6.8%，证明了TRL-HPO的高效性。

    arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for 
    
[^98]: 通过元学习在困难样本上改善泛化性能

    Improving Generalization via Meta-Learning on Hard Samples

    [https://arxiv.org/abs/2403.12236](https://arxiv.org/abs/2403.12236)

    在学习的重加权(LRW)方法中，通过元学习中使用难以分类的实例作为验证集，来改善分类器的泛化性能，并提出了一个高效的算法来训练这个模型。

    

    学习的重加权(LRW)方法用一个优化准则为训练实例分配权重，以便在一个代表性验证数据集上最大化性能。我们提出并形式化了LRW训练中的优化验证集选择的问题，以改善分类器的泛化性能。特别地，我们展示了在验证集中使用难以分类的实例既与理论相关，又有强有力的实证证据支持泛化。我们提供了一个高效的算法来训练这个元优化模型，以及一个简单的两次训练启发式方法以进行谨慎的比较研究。我们证明，与易验证数据一起的LRW表现始终比难验证数据一起的LRW表现差，从而确立了我们的元优化问题的有效性。我们提出的算法在各种数据集和领域转移挑战上胜过了各种基线。

    arXiv:2403.12236v1 Announce Type: new  Abstract: Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Ima
    
[^99]: 使用FloodCast进行大规模洪水建模与预测

    Large-scale flood modeling and forecasting with FloodCast

    [https://arxiv.org/abs/2403.12226](https://arxiv.org/abs/2403.12226)

    本研究提出了FloodCast框架，通过结合多卫星观测和水动力建模模块，实现了快速、稳定、准确的大规模洪水建模和预测，其中引入了几何自适应的物理启发式神经求解器（GeoPINS）。

    

    大规模水动力模型通常依赖于固定分辨率的空间网格和模型参数，同时会产生高计算成本。这限制了它们准确预测洪水峰值并发布时间关键的危险警告的能力。在这项工作中，我们构建了一个快速、稳定、准确、分辨率不变、几何自适应的洪水建模和预测框架，可以在大规模下运行，名为FloodCast。该框架包括两个主要模块：多卫星观测和水动力建模。在多卫星观测模块中，提出了实时无监督变化检测方法和降雨处理与分析工具，以充分利用多卫星观测在大规模洪水预测中的潜力。在水动力建模模块中，引入了一种几何自适应的物理启发式神经求解器（GeoPINS），受益于不需要训练的要求。

    arXiv:2403.12226v1 Announce Type: new  Abstract: Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for trainin
    
[^100]: 通过二次和方法进行私有图估计

    Private graphon estimation via sum-of-squares

    [https://arxiv.org/abs/2403.12213](https://arxiv.org/abs/2403.12213)

    基于和平方法的私有图估计算法首次实现了学习随机块模型和图估计的纯节点差分隐私算法，具有多项式运行时间，与之前最佳的信息论节点私有机制具有相匹配的统计效用保证。

    

    我们开发了用于学习随机块模型和图估计的第一个纯节点差分隐私算法，对于任意常数个块，具有多项式运行时间。统计效用保证与先前最佳的信息论（指数时间）节点私有机制相匹配。该算法基于一个基于指数机制的得分函数，该函数定义为依赖于块数量的二次和松弛。我们结果的关键要素是：(1) 在形式上定义为二次优化在双重随机矩阵的多胞体上的距离的特征化块图定义，(2) 一般的多项式优化的和平方法在任意多胞体上的收敛结果，以及(3) 执行利普希茨扩展的得分函数作为二次和算法范例的一般方法。

    arXiv:2403.12213v1 Announce Type: cross  Abstract: We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks. The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems. The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks. The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm.
    
[^101]: 评估命名实体识别：比较分析巴西公司财报电话转录上的单语和多语言Transformer模型

    Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions

    [https://arxiv.org/abs/2403.12212](https://arxiv.org/abs/2403.12212)

    本研究通过引入新方法，将标记分类任务重新构建为文本生成问题，评估了在巴西银行财报电话转录中使用的单语和多语言Transformer模型的性能。

    

    命名实体识别（NER）是一种从文本文档中提取信息的自然语言处理技术。然而，现有关于NER的大部分研究都集中在英语文档上，导致缺乏专门针对葡萄牙语财务领域的数据集。本研究解决了金融领域内NER需求，并侧重于从巴西银行财报电话转录中提取的葡萄牙语文本。通过整理包括384个转录的综合数据集，并利用弱监督技术进行注释，我们评估了在葡萄牙语（BERTimbau和PTT5）训练的单语模型以及多语言模型（mBERT和mT5）的性能。值得注意的是，我们引入了一种新方法，将标记分类任务重新构建为文本生成问题，从而实现T5模型的微调和评估。在模型微调之后，

    arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
    
[^102]: 将控制Lyapunov函数分解以实现高效的强化学习

    Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning

    [https://arxiv.org/abs/2403.12210](https://arxiv.org/abs/2403.12210)

    引入控制Lyapunov函数（CLF）来降低强化学习中的样本复杂性，为现实世界机器人场景的训练提供了新的高效方法。

    

    近期使用强化学习（RL）的方法已被证明对于训练智能体在未知环境中表现成功。然而，RL并未广泛应用于现实世界的机器人场景。本文构建于现有将RL中的奖励函数重塑的工作基础之上，引入了一个控制Lyapunov函数（CLF），证明能减少样本复杂性。然而，此公式需要知道系统的一个CLF，但由于缺乏通用方法，通常很难确定一个合适的CLF。现有工作可以通过哈密尔顿-雅可比可达性程序计算低维CLFs。然而，这类方法在高维系统上变得难以处理，这是我们...

    arXiv:2403.12210v1 Announce Type: cross  Abstract: Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we
    
[^103]: 用于数据拟合的实用紧凑表示

    Useful Compact Representations for Data-Fitting

    [https://arxiv.org/abs/2403.12206](https://arxiv.org/abs/2403.12206)

    我们提出了新的紧凑表示方法，在大型确定性问题的软件实现中表现良好，特别适用于大型特征值计算、张量分解和非线性回归。

    

    在没有2阶导数信息的最小化问题中，估计Hessian矩阵的方法非常有效。然而，传统技术生成的稠密矩阵对于大型问题是难以承受的。有限内存紧凑表示将稠密数组表示为低秩表示，已成为大型确定性问题软件实现的最新技术。我们开发了由向量选择参数化的新紧凑表示，并且对于特定选择，它们可以简化为现有的著名公式。我们展示了紧凑表示在大型特征值计算、张量分解和非线性回归中的有效性。

    arXiv:2403.12206v1 Announce Type: cross  Abstract: For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.
    
[^104]: 基于模仿的增强学习为基于视觉的敏捷飞行引导引导

    Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight

    [https://arxiv.org/abs/2403.12203](https://arxiv.org/abs/2403.12203)

    在基于视觉的自主无人机竞速中，本研究提出了将强化学习和模仿学习相结合的新型训练框架，以克服样本效率和计算需求方面的挑战，并通过三个阶段的方法进行性能受限的自适应RL微调

    

    我们在基于视觉的自主无人机竞速的背景下，将强化学习（RL）的有效性和模仿学习（IL）的效率结合在一起。我们专注于直接处理视觉输入，而无需明确的状态估计。虽然强化学习通过试错提供了一个学习复杂控制器的通用框架，但面临着样本效率和计算需求的挑战，因为视觉输入的维度较高。相反，IL在从视觉演示中学习方面表现出效率，但受到演示质量的限制，并面临诸如协变量漂移的问题。为了克服这些限制，我们提出了一个结合RL和IL优势的新型训练框架。我们的框架包括三个阶段：使用特权状态信息的师傅策略的初始训练，使用IL将此策略蒸馏为学生策略，以及性能受限的自适应RL微调

    arXiv:2403.12203v1 Announce Type: cross  Abstract: We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL's advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tunin
    
[^105]: 人类和机器中的函数构成学习

    Compositional learning of functions in humans and machines

    [https://arxiv.org/abs/2403.12201](https://arxiv.org/abs/2403.12201)

    人类和神经网络模型通过结合学习和推理探索组合函数的能力，不仅能够进行简单的顺序函数串联，还能理解更复杂的交互函数组合，具有广泛的应用潜力。

    

    学习和组成函数的能力对于人类在有效学习和推理中至关重要，使其能够灵活泛化，例如根据已知烹饪过程创造新菜肴。除了函数的顺序链式串联外，现有的语言学文献表明，人类可以理解更复杂的交互函数组合，其中输出产生取决于由不同函数排序引起的上下文变化。将调查扩展到视觉领域，我们开发了一个函数学习范例，以探索人类和神经网络模型在不同交互条件下学习和推理具有组合函数的能力。在对个体函数进行简要训练后，对人类参与者进行了评估，以组合两个学习过的函数，涵盖四种主要的交互类型，其中包括应用第一个函数会创建或删除的情况。

    arXiv:2403.12201v1 Announce Type: new  Abstract: The ability to learn and compose functions is foundational to efficient learning and reasoning in humans, enabling flexible generalizations such as creating new dishes from known cooking processes. Beyond sequential chaining of functions, existing linguistics literature indicates that humans can grasp more complex compositions with interacting functions, where output production depends on context changes induced by different function orderings. Extending the investigation into the visual domain, we developed a function learning paradigm to explore the capacity of humans and neural network models in learning and reasoning with compositional functions under varied interaction conditions. Following brief training on individual functions, human participants were assessed on composing two learned functions, in ways covering four main interaction types, including instances in which the application of the first function creates or removes the c
    
[^106]: FLex: 用于立体内窥镜视频的姿势和动态辐射场联合优化

    FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos

    [https://arxiv.org/abs/2403.12198](https://arxiv.org/abs/2403.12198)

    FLex提出了一种用于立体内窥镜视频的新方法，通过联合优化神经辐射场和摄像机姿势，改善了内窥镜重建的效率和处理规模能力。

    

    内窥镜场景的重建对于各种医疗应用是一个重要的资产，从术后分析到教育培训。最近，神经渲染在具有变形组织的内窥镜重建中显示出有希望的结果。然而，该设置一直局限于静态内窥镜、有限的变形或需要外部跟踪设备来获取内窥镜相机的摄像机姿势信息。通过FLex，我们解决了在高度动态的变形组织环境中移动内窥镜的挑战性设置。我们提出了将隐式场景分为多个重叠的4D神经辐射场(NeRFs)以及一个联合优化方案，从头开始联合优化重建和摄像机姿势。这提高了易用性，并允许扩展重建能力以处理5,000帧以上的手术视频；与

    arXiv:2403.12198v1 Announce Type: cross  Abstract: Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared 
    
[^107]: PETScML：科学机器学习中用于训练回归问题的二阶求解器

    PETScML: Second-order solvers for training regression problems in Scientific Machine Learning

    [https://arxiv.org/abs/2403.12188](https://arxiv.org/abs/2403.12188)

    PETScML框架搭建了一个轻量级软件工具用于在科学机器学习中应用传统求解器来解决监督学习问题

    

    在最近几年中，我们见证了科学机器学习作为一种数据驱动工具的出现，通过深度学习技术分析计算科学和工程应用产生的数据。这些方法的核心是监督训练算法，用于学习神经网络实现，这是一个通常使用随机梯度方法求解的高度非凸优化问题。我们引入了一个基于便携式和可扩展科学计算工具包的轻量级软件框架，以弥合深度学习软件和用于无约束最小化的传统求解器之间的差距。

    arXiv:2403.12188v1 Announce Type: new  Abstract: In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications. At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods. However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization. We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization. We em
    
[^108]: 神经网络逼近RKHS函数型

    Approximation of RKHS Functionals by Neural Networks

    [https://arxiv.org/abs/2403.12187](https://arxiv.org/abs/2403.12187)

    本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型，并建立了逼近的普适性，推导了逆多重二次、高斯和Sobolev核引起的误差界限，证明神经网络可以准确逼近广义函数线性模型中的回归映射。

    

    受到时间序列和图像等丰富功能性数据的启发，人们越来越感兴趣将这些数据整合到神经网络中，并从函数空间到R（即函数型）学习映射。本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型。我们建立了对RKHS上函数型逼近的普适性。具体来说，我们推导了通过逆多重二次、高斯和Sobolev核引起的明确误差界限。此外，我们将我们的研究应用于函数回归，证明了神经网络可以准确逼近广义函数线性模型中的回归映射。现有的功能性学习作品需要积分型基函数展开与一组预定义的基函数。通过在RKHS中利用插值正交投影，我们提出的网络是...

    arXiv:2403.12187v1 Announce Type: cross  Abstract: Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is 
    
[^109]: 少数个体的力量：利用核心子集选择加速和优化数据重新加权

    The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection

    [https://arxiv.org/abs/2403.12166](https://arxiv.org/abs/2403.12166)

    提出一种利用核心子集选择进行数据重新加权的方法，有效优化了计算时间和模型性能，突显其作为模型训练的可扩展和精确解决方案的潜力。

    

    随着机器学习任务不断发展，趋势是收集更大的数据集并训练规模越来越大的模型。虽然这提高了准确性，但也将计算成本提高到不可持续的水平。针对这一问题，我们的工作旨在在计算效率和模型准确性之间取得微妙的平衡，这是该领域中一直存在的挑战。我们引入了一种利用核心子集选择进行重新加权的新方法，有效优化了计算时间和模型性能。通过专注于 strategically selected coreset，我们的方法提供了一个稳健的表示，因为它有效地最小化了异常值的影响。然后，重新校准的权重被映射回并传播到整个数据集。我们的实验结果证实了这种方法的有效性，突显了它作为模型训练的可扩展和精确解决方案的潜力。

    arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
    
[^110]: 变分方法用于Dirichlet混合模型中KL散度的高效估计

    Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models

    [https://arxiv.org/abs/2403.12158](https://arxiv.org/abs/2403.12158)

    引入变分方法在Dirichlet混合模型中提供了封闭形式的解，显著提高了计算效率，可用于快速模型比较和稳健估计评估。

    

    本研究致力于在Dirichlet混合模型（DMM）中高效估计Kullback-Leibler（KL）散度，这对于对成分数据进行聚类至关重要。尽管DMM的重要性，但获得KL散度的解析解仍然是困难的。过去的方法依赖于计算密集型的蒙特卡洛方法，这促使我们引入了一种新颖的变分方法。我们的方法提供了一个封闭形式的解，显著提高了计算效率，可以快速进行模型比较和稳健的估计评估。实际数据和模拟数据的验证显示，我们的方法比传统的基于蒙特卡洛的方法更加高效准确，为快速探索不同DMM模型和推进成分数据的统计分析开辟了新途径。

    arXiv:2403.12158v1 Announce Type: cross  Abstract: This study tackles the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM), crucial for clustering compositional data. Despite the significance of DMMs, obtaining an analytically tractable solution for KL Divergence has proven elusive. Past approaches relied on computationally demanding Monte Carlo methods, motivating our introduction of a novel variational approach. Our method offers a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations. Validation using real and simulated data showcases its superior efficiency and accuracy over traditional Monte Carlo-based methods, opening new avenues for rapid exploration of diverse DMM models and advancing statistical analyses of compositional data.
    
[^111]: 将大型语言模型中的领域特定内容融入知识图谱，以增强零样本对象状态分类

    Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification

    [https://arxiv.org/abs/2403.12151](https://arxiv.org/abs/2403.12151)

    大型语言模型与知识图谱结合，提高零样本对象状态分类性能

    

    领域特定知识可以显著有助于解决各种视觉任务，但生成这种知识需要大量人力和时间成本。本研究探讨了大型语言模型（LLMs）在通过语义嵌入生成和提供领域特定信息方面的潜力。为实现这一目标，将LLM集成到一个流程中，该流程在视觉基础零样本对象状态分类任务的背景下利用知识图谱和预训练的语义向量。通过广泛的消融研究彻底研究了LLM的行为。我们的研究结果表明，将基于LLM的嵌入与通用的预训练嵌入结合使用可以显著提高性能。借鉴这一消融研究的见解，我们对竞争模型进行了比较分析，从而突出了最新的表现水平。

    arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
    
[^112]: 用于学习神经网络等变表示的图神经网络

    Graph Neural Networks for Learning Equivariant Representations of Neural Networks

    [https://arxiv.org/abs/2403.12143](https://arxiv.org/abs/2403.12143)

    本研究提出了将神经网络表示为参数的计算图的方法，利用图神经网络和变压器来实现置换对称性，使得单个模型能够处理具有多种架构的神经计算图。

    

    处理其他神经网络参数的神经网络在诸如分类隐式神经表示、生成神经网络权重和预测泛化错误等领域中得到应用。然而，现有方法要么忽视神经网络中固有的置换对称性，要么依赖复杂的权重共享模式来实现等变性，同时忽略网络架构本身的影响。在本文中，我们提出将神经网络表示为参数的计算图，这使我们能够利用强大的保留置换对称性的图神经网络和变压器。因此，我们的方法使得单个模型能够对具有多样架构的神经计算图进行编码。我们展示了我们的方法在包括分类和编辑隐式神经表示、预测泛化错误等多种任务中的有效性。

    arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
    
[^113]: 使用DistClassiPy进行光变曲线分类：一种新的基于距离的分类器

    Light Curve Classification with DistClassiPy: a new distance-based classifier

    [https://arxiv.org/abs/2403.12120](https://arxiv.org/abs/2403.12120)

    提出了一种新的基于距离度量的分类器DistClassiPy，通过比较不同类别对象之间的距离，实现了可变星的光变曲线分类，帮助增加分类结果的可解释性并减少计算成本。

    

    天文学领域的巡天调查的兴起引领了时域天文学中大数据时代的到来，使得数据科学和机器学习成为研究天体对象的必备工具。我们探讨了使用不同距离度量来辅助对象分类的方法。为此，我们开发了一种基于距离度量的新分类器，称为DistClassiPy。直接使用距离度量是一种在时域天文学中尚未探索的方法，但基于距离的方法可以帮助增加分类结果的可解释性并减少计算成本。具体而言，我们通过比较不同类别对象之间的距离，对可变星的光变曲线进行分类。通过对10个类别中的6000颗可变星的目录应用18种距离度量，我们展示了分类和维度缩减。

    arXiv:2403.12120v1 Announce Type: cross  Abstract: The rise of synoptic sky surveys has ushered in an era of big data in time-domain astronomy, making data science and machine learning essential tools for studying celestial objects. Tree-based (e.g. Random Forests) and deep learning models represent the current standard in the field. We explore the use of different distance metrics to aid in the classification of objects. For this, we developed a new distance metric based classifier called DistClassiPy. The direct use of distance metrics is an approach that has not been explored in time-domain astronomy, but distance-based methods can aid in increasing the interpretability of the classification result and decrease the computational costs. In particular, we classify light curves of variable stars by comparing the distances between objects of different classes. Using 18 distance metrics applied to a catalog of 6,000 variable stars in 10 classes, we demonstrate classification and dimensio
    
[^114]: 转移学习用于T细胞响应预测

    Transfer Learning for T-Cell Response Prediction

    [https://arxiv.org/abs/2403.12117](https://arxiv.org/abs/2403.12117)

    使用转换器模型进行T细胞响应预测，研究多域结构中的转移学习技术，提出领域感知评估方案。

    

    我们研究特定给定肽段的T细胞响应预测，这可以是向个性化癌症疫苗发展迈出重要一步的关键。

    arXiv:2403.12117v1 Announce Type: cross  Abstract: We study the prediction of T-cell response for specific given peptides, which could, among other applications, be a crucial step towards the development of personalized cancer vaccines. It is a challenging task due to limited, heterogeneous training data featuring a multi-domain structure; such data entail the danger of shortcut learning, where models learn general characteristics of peptide sources, such as the source organism, rather than specific peptide characteristics associated with T-cell response.   Using a transformer model for T-cell response prediction, we show that the danger of inflated predictive performance is not merely theoretical but occurs in practice. Consequently, we propose a domain-aware evaluation scheme. We then study different transfer learning techniques to deal with the multi-domain structure and shortcut learning. We demonstrate a per-source fine tuning approach to be effective across a wide range of peptid
    
[^115]: 基于自定义生物启发目标的无监督端到端训练

    Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target

    [https://arxiv.org/abs/2403.12116](https://arxiv.org/abs/2403.12116)

    本研究提出了一种使用Winner-Take-All（WTA）选择性和生物启发的稳态机制相结合的“自定义目标”方法，旨在解决无监督学习方法在边缘AI硬件上的计算资源稀缺性问题。

    

    当前的无监督学习方法依赖于通过深度学习技术（如自监督学习）进行端到端训练，具有较高的计算需求，或者采用通过类似Hebbian学习的生物启发方法逐层训练，使用与监督学习不兼容的局部学习规则。为了解决这一挑战，在这项工作中，我们引入了一种使用网络最终层的胜者通吃（WTA）选择性的“自定义目标”，并通过生物启发的稳态机制进行正则化。

    arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
    
[^116]: 深度学习与多专家观察员相比自动化了Cobb角度测量

    Deep learning automates Cobb angle measurement compared with multi-expert observers

    [https://arxiv.org/abs/2403.12115](https://arxiv.org/abs/2403.12115)

    该研究开发了深度学习软件，自动化测量脊柱侧凸的 Cobb 角度，提供清晰的可视化结果，解决了手动测量耗时且存在差异性的问题

    

    脊柱侧凸是一种常见疾病，其特点是脊柱异常曲度导致畸形，需要精确的评估方法以进行有效的诊断和管理。 Cobb 角度是一种广泛使用的脊柱侧凸量化方法，用于测量倾斜椎骨之间的曲度程度。然而，手动测量 Cobb 角度耗时且劳动密集，存在着显著的观察者间和观察者内的差异性。为了解决这些挑战以及某些现有自动化方法中缺乏可解释性的问题，我们创建了完全自动化的软件，不仅可以精确测量 Cobb 角度，还可以清晰可视化这些测量结果。该软件集成了基于深度神经网络的脊柱区域检测和分割、脊柱中心线识别、标记最明显倾斜的椎骨以及在原始图像上直接可视化 Cobb 角度等功能。

    arXiv:2403.12115v1 Announce Type: cross  Abstract: Scoliosis, a prevalent condition characterized by abnormal spinal curvature leading to deformity, requires precise assessment methods for effective diagnosis and management. The Cobb angle is a widely used scoliosis quantification method that measures the degree of curvature between the tilted vertebrae. Yet, manual measuring of Cobb angles is time-consuming and labor-intensive, fraught with significant interobserver and intraobserver variability. To address these challenges and the lack of interpretability found in certain existing automated methods, we have created fully automated software that not only precisely measures the Cobb angle but also provides clear visualizations of these measurements. This software integrates deep neural network-based spine region detection and segmentation, spine centerline identification, pinpointing the most significantly tilted vertebrae, and direct visualization of Cobb angles on the original images
    
[^117]: GCAM: 食物细粒度识别的高斯因果关注模型

    GCAM: Gaussian and causal-attention model of food fine-grained recognition

    [https://arxiv.org/abs/2403.12109](https://arxiv.org/abs/2403.12109)

    提出了一种采用高斯和因果关注模型进行食物细粒度识别的方法，通过训练获取目标区域上的高斯特征和从对象中提取细粒度特征来增强特征映射能力，同时采用反事实推理方法对抗数据漂移。

    

    目前，大多数食物识别依赖于深度学习进行分类。然而，这些方法在有效区分视觉上相似的食物样本方面存在困难，突出了需要解决食物识别中的细粒度问题的迫切性。为了缓解这些挑战，我们提出了采用高斯和因果关注模型进行细粒度物体识别。特别是，我们训练以获得目标区域上的高斯特征，然后从对象中提取细粒度特征，从而增强目标区域的特征映射能力。为了对抗由不均匀数据分布导致的数据漂移，我们采用反事实推理方法。通过使用反事实干预，我们分析了学习的图像注意机制对网络预测的影响，使网络能够获取更有用的细粒度图像识别注意权重。

    arXiv:2403.12109v1 Announce Type: cross  Abstract: Currently, most food recognition relies on deep learning for category classification. However, these approaches struggle to effectively distinguish between visually similar food samples, highlighting the pressing need to address fine-grained issues in food recognition. To mitigate these challenges, we propose the adoption of a Gaussian and causal-attention model for fine-grained object recognition.In particular, we train to obtain Gaussian features over target regions, followed by the extraction of fine-grained features from the objects, thereby enhancing the feature mapping capabilities of the target regions. To counteract data drift resulting from uneven data distributions, we employ a counterfactual reasoning approach. By using counterfactual interventions, we analyze the impact of the learned image attention mechanism on network predictions, enabling the network to acquire more useful attention weights for fine-grained image recogn
    
[^118]: 循环信念传播用于近似概率推断

    Circular Belief Propagation for Approximate Probabilistic Inference

    [https://arxiv.org/abs/2403.12106](https://arxiv.org/abs/2403.12106)

    循环信念传播（CBP）是对Belief Propagation（BP）的扩展，通过学习检测和取消循环引起的消息反响来限制错误相关和信念放大的不利影响，在二元概率图上表现优于BP和先前提出的算法。

    

    Belief Propagation（BP）是一种简单的概率推断算法，通过在表示概率分布的图中的节点之间传递消息来实现。其类似神经网络的特性表明，它可能对神经科学和人工智能有广泛的应用。不幸的是，当应用于无环图时，BP仅仅是精确的，这限制了该算法的潜力。在本文中，我们提出了循环信念传播（CBP），这是BP的一个扩展，通过学习检测和取消循环引起的消息反响而限制了错误相关和信念放大的不利影响。我们通过涉及二元概率图的数值实验表明，CBP远远优于BP，并与先前提出的算法相比取得了良好的性能。

    arXiv:2403.12106v1 Announce Type: new  Abstract: Belief Propagation (BP) is a simple probabilistic inference algorithm, consisting of passing messages between nodes of a graph representing a probability distribution. Its analogy with a neural network suggests that it could have far-ranging applications for neuroscience and artificial intelligence. Unfortunately, it is only exact when applied to cycle-free graphs, which restricts the potential of the algorithm. In this paper, we propose Circular Belief Propagation (CBP), an extension of BP which limits the detrimental effects of message reverberation caused by cycles by learning to detect and cancel spurious correlations and belief amplifications. We show in numerical experiments involving binary probabilistic graphs that CBP far outperforms BP and reaches good performance compared to that of previously proposed algorithms.
    
[^119]: 通过移动树学习时间段偏好进行下一个POI推荐

    Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation

    [https://arxiv.org/abs/2403.12100](https://arxiv.org/abs/2403.12100)

    本文引入了“移动树”数据结构，用于学习用户跨不同时间段的偏好，以提升下一个POI推荐任务的性能。

    

    下一个兴趣点（POI）推荐任务旨在根据用户当前的签到轨迹提供POI的动态排名。本任务的推荐性能取决于通过基于位置的社交网络（LBSNs）数据全面了解用户个性化的行为模式。本文介绍了一种名为“移动树”的创新数据结构，用于分层描述用户的签到记录。移动树包含多粒度时间段节点，以学习用户跨不同时间段的偏好。同时，我们提出了移动树网络（MTNet）

    arXiv:2403.12100v1 Announce Type: cross  Abstract: Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet
    
[^120]: 深度生成设计用于大规模生产

    Deep Generative Design for Mass Production

    [https://arxiv.org/abs/2403.12098](https://arxiv.org/abs/2403.12098)

    通过将与压铸和注塑相关的约束集成到生成设计中，利用二维深度图像将复杂的3D几何形状简化为可制造的轮廓，消除不可制造特性，将重点放在厚度和肋设计等制造重要方面。

    

    生成设计（GD）作为一种革命性的设计方法已经发展，采用先进的算法和人工智能来创造超越传统限制的多样化和创新性解决方案。尽管取得了成功，但生成设计在复杂设计的可制造性方面面临着重大挑战，通常需要进行大量手动修改，因为标准制造过程存在限制，并且依赖于并不适合大规模生产的增材制造技术。我们的研究通过将与压铸和注塑相关的约束集成到GD中，通过利用二维深度图像，引入了一种创新框架来解决这些可制造性问题。这种方法将复杂的三维几何形状简化为可制造的轮廓，去除不可制造的悬挑等不可行特性，并允许直接考虑厚度和肋设计等重要制造方面。

    arXiv:2403.12098v1 Announce Type: cross  Abstract: Generative Design (GD) has evolved as a transformative design approach, employing advanced algorithms and AI to create diverse and innovative solutions beyond traditional constraints. Despite its success, GD faces significant challenges regarding the manufacturability of complex designs, often necessitating extensive manual modifications due to limitations in standard manufacturing processes and the reliance on additive manufacturing, which is not ideal for mass production. Our research introduces an innovative framework addressing these manufacturability concerns by integrating constraints pertinent to die casting and injection molding into GD, through the utilization of 2D depth images. This method simplifies intricate 3D geometries into manufacturable profiles, removing unfeasible features such as non-manufacturable overhangs and allowing for the direct consideration of essential manufacturing aspects like thickness and rib design. 
    
[^121]: LLMs是一个好的难解填字游戏求解器吗？

    Are LLMs Good Cryptic Crossword Solvers?

    [https://arxiv.org/abs/2403.12094](https://arxiv.org/abs/2403.12094)

    本文建立了三种流行LLMs的基准结果，表明它们在难解填字游戏上的表现仍远远不及人类。

    

    难解填字游戏是一种谜题，不仅依赖于一般知识，还依赖于求解者在不同层面上操纵语言并处理各种类型的文字游戏。先前的研究表明，即使对于现代NLP模型来说，解决这类谜题也是一项挑战。然而，大型语言模型（LLMs）的能力尚未在这一任务上进行测试。在本文中，我们为三种流行的LLMs -- LLaMA2、Mistral和ChatGPT建立了基准结果，显示它们在这一任务上的表现仍远远不及人类。

    arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
    
[^122]: 数字病理学中的基础模型和信息检索

    Foundation Models and Information Retrieval in Digital Pathology

    [https://arxiv.org/abs/2403.12090](https://arxiv.org/abs/2403.12090)

    论文回顾了数字病理学中基础模型和信息检索领域的最新进展。

    

    这篇论文回顾了数字病理学中基础模型、LLM、生成式人工智能、信息检索和内容检索的最新技术。

    arXiv:2403.12090v1 Announce Type: cross  Abstract: The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology
    
[^123]: 生还的男孩：从LLM中删除哈利波特比报道的更困难

    The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported

    [https://arxiv.org/abs/2403.12082](https://arxiv.org/abs/2403.12082)

    通过小规模实验发现，从LLM中删除哈利波特内容比先前报道的更加困难。

    

    最近的研究声称"我们有效地抹除了模型生成或回忆哈利波特相关内容的能力。"然而，一项小规模实验表明这一说法过于宽泛。少于十次试验导致重复和具体提及哈利波特，包括"啊，我明白了！"麻瓜"是特里·普拉切特的哈利波特系列中使用的术语...''。

    arXiv:2403.12082v1 Announce Type: cross  Abstract: Recent work arXiv.2310.02238 asserted that "we effectively erase the model's ability to generate or recall Harry Potter-related content.'' This claim is shown to be overbroad. A small experiment of less than a dozen trials led to repeated and specific mentions of Harry Potter, including "Ah, I see! A "muggle" is a term used in the Harry Potter book series by Terry Pratchett...''
    
[^124]: 在火星可见光卫星观测中评估地形依赖性对霜冻检测性能的影响

    Evaluating Terrain-Dependent Performance for Martian Frost Detection in Visible Satellite Observations

    [https://arxiv.org/abs/2403.12080](https://arxiv.org/abs/2403.12080)

    研究提出了一种新颖的空间数据分区方法，揭示了地质背景对自动霜冻检测的影响，并提出了缓解观察到的偏差的方法。

    

    火星表面的季节性结霜和融冰被假设推动着气候过程以及形成和演化地貌特征，如冲沟。过去的研究集中于使用来自轨道高分辨率可见光观测手动分析火星北半球中纬度地区霜冻周期的行为。全球扩展这些研究需要利用数据科学技术，如卷积神经网络，自动检测霜冻。然而，霜冻存在的可见指标可能会根据霜冻所叠加的地质背景而有显著变化。在这项研究中，我们(1)提出了一种新颖的空间数据分区方法，以减少模型性能估计中的偏差，(2)说明了地质背景如何影响自动霜冻检测，(3)提出了缓解自动霜冻检测中观察到的偏差的方法。

    arXiv:2403.12080v1 Announce Type: cross  Abstract: Seasonal frosting and defrosting on the surface of Mars is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. Past studies have focused on manually analyzing the behavior of the frost cycle in the northern mid-latitude region of Mars using high-resolution visible observations from orbit. Extending these studies globally requires automating the detection of frost using data science techniques such as convolutional neural networks. However, visible indications of frost presence can vary significantly depending on the geologic context on which the frost is superimposed. In this study, we (1) present a novel approach for spatially partitioning data to reduce biases in model performance estimation, (2) illustrate how geologic context affects automated frost detection, and (3) propose mitigations to observed biases in automated frost detection.
    
[^125]: 超越节奏：歌曲流行的秘诀？一种机器学习方法

    Beyond Beats: A Recipe to Song Popularity? A machine learning approach

    [https://arxiv.org/abs/2403.12079](https://arxiv.org/abs/2403.12079)

    该研究利用机器学习模型探讨预测歌曲流行度，结果显示流派是影响流行度的主要因素，同时揭示了时间趋势和特征间的复杂关系。

    

    音乐流行度预测引起了工业界和学术界的极大关注，这得益于数据驱动算法和Spotify等流媒体平台的兴起。本研究旨在探讨使用涵盖1957年至2020年各种流派的30,000首歌曲数据集，探讨各种机器学习模型在预测歌曲流行度方面的预测能力。通过普通最小二乘（OLS）、多元自适应回归样条（MARS）、随机森林和XGBoost算法来分析歌曲特征及其对流行度的影响。研究结果显示，普通最小二乘（OLS）回归分析表明流派是流行度的主要影响因素，而且随时间可见显著的趋势。MARS建模突出了变量之间的复杂关系，尤其是与特征如器乐度和时长相关。随机森林和XGBoost模型强调了流派，尤其是电子舞曲在预测中的重要性。

    arXiv:2403.12079v1 Announce Type: cross  Abstract: Music popularity prediction has garnered significant attention in both industry and academia, fuelled by the rise of data-driven algorithms and streaming platforms like Spotify. This study aims to explore the predictive power of various machine learning models in forecasting song popularity using a dataset comprising 30,000 songs spanning different genres from 1957 to 2020. Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse song characteristics and their impact on popularity. Results: Ordinary Least Squares (OLS) regression analysis reveals genre as the primary influencer of popularity, with notable trends over time. MARS modelling highlights the complex relationship between variables, particularly with features like instrumentalness and duration. Random Forest and XGBoost models underscore the importance of genre, especially EDM, in predict
    
[^126]: 神经元中心的赫布学习

    Neuron-centric Hebbian Learning

    [https://arxiv.org/abs/2403.12076](https://arxiv.org/abs/2403.12076)

    提出了一种新的神经元中心的赫布学习模型，相较于传统的ABCD规则，将参数减少了从$5W$到$5N$。

    

    大脑学习机制背后最引人注目的能力之一是通过结构和功能可塑性调整其突触。尽管突触在传递信息到整个大脑中起着基本作用，但几项研究表明，是神经元的激活产生了对突触的改变。然而，大多数为人工神经网络（NNs）设计的可塑性模型，如ABCD规则，侧重于突触而不是神经元，因此优化突触特定的赫布参数。然而，这种方法增加了优化过程的复杂性，因为每个突触都与多个赫布参数相关联。为了克服这一限制，我们提出了一种新颖的可塑性模型，称为神经元中心的赫布学习（NcHL），其优化侧重于神经元而不是突触特定的赫布参数。与ABCD规则相比，NcHL将参数减少从$5W$到$5N$。

    arXiv:2403.12076v1 Announce Type: cross  Abstract: One of the most striking capabilities behind the learning mechanisms of the brain is the adaptation, through structural and functional plasticity, of its synapses. While synapses have the fundamental role of transmitting information across the brain, several studies show that it is the neuron activations that produce changes on synapses. Yet, most plasticity models devised for artificial Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than neurons, therefore optimizing synaptic-specific Hebbian parameters. This approach, however, increases the complexity of the optimization process since each synapse is associated to multiple Hebbian parameters. To overcome this limitation, we propose a novel plasticity model, called Neuron-centric Hebbian Learning (NcHL), where optimization focuses on neuron- rather than synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces the parameters from $5W$ to $5N$
    
[^127]: Adversarial Nibbler: 一种用于识别文本到图像生成中多样化危害的开放式红队方法

    Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation

    [https://arxiv.org/abs/2403.12075](https://arxiv.org/abs/2403.12075)

    批评了文本到图像生成中模型对非明显攻击的鲁棒性，提出了Adversarial Nibbler Challenge以众包多样化的提示来纠正模型的安全问题

    

    随着文本到图像（T2I）生成AI模型的崛起，评估模型对于不明显攻击的稳健性以减少生成冒犯性图像变得至关重要。通过专注于"隐性对抗"提示（触发T2I模型生成不安全图像的非明显原因），我们独立辨别出一组难以发现的安全问题，人类创造力很适合揭示这些问题。为此，我们构建了Adversarial Nibbler Challenge，这是一个红队方法，用于众包一组多样化的隐性对抗性提示。我们已汇总一套最先进的T2I模型，采用简单用户界面来识别和注释危害，并吸引广泛人群来捕捉在标准测试中可能被忽视的长尾安全问题。挑战在连续回合中进行，以实现对T2I模型中安全隐患的持续发现和分析。

    arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
    
[^128]: 超越数量：基于机器学习的城市基础设施质量不平等特征化

    Beyond Quantities: Machine Learning-based Characterization of Inequality in Infrastructure Quality Provision in Cities

    [https://arxiv.org/abs/2403.12074](https://arxiv.org/abs/2403.12074)

    通过机器学习的方法，超越了传统基于数量的基础设施不平等特征化研究，从而填补了城市不平等和环境正义考虑之间的研究空白。

    

    本研究的目标是对城市地区的基础设施质量不平等进行特征化。尽管越来越多的文献已经意识到特征化城市基础设施不平等的重要性，并提供了定量指标以指导城市发展规划，但大多数现有方法主要集中在测量基础设施的数量上，假定更多的基础设施更好。此外，现有研究主要集中在基于指数的方法上，其中城市地区基础设施供给状况是根据设定的主观权重确定的。对基础设施数量的关注和使用来自主观权重的指数已经妨碍了适当地研究基础设施不平等与城市不平等和环境正义考虑之间的关系。鉴此，我们提出一种基于机器学习的方法

    arXiv:2403.12074v1 Announce Type: cross  Abstract: The objective of this study is to characterize inequality in infrastructure quality across urban areas. While a growing of body of literature has recognized the importance of characterizing infrastructure inequality in cities and provided quantified metrics to inform urban development plans, the majority of the existing approaches focus primarily on measuring the quantity of infrastructure, assuming that more infrastructure is better. Also, the existing research focuses primarily on index-based approaches in which the status of infrastructure provision in urban areas is determined based on assumed subjective weights. The focus on infrastructure quantity and use of indices obtained from subjective weights has hindered the ability to properly examine infrastructure inequality as it pertains to urban inequality and environmental justice considerations. Recognizing this gap, we propose a machine learning-based approach in which infrastruct
    
[^129]: Floralens：一种用于葡萄牙本地植物的深度学习模型

    Floralens: a Deep Learning Model for the Portuguese Native Flora

    [https://arxiv.org/abs/2403.12072](https://arxiv.org/abs/2403.12072)

    本论文开发了一种用于从公开数据集构建生物分类群数据集以及利用深度卷积神经网络推导模型的简化方法，并以葡萄牙本地植物为案例研究。

    

    机器学习技术，特别是深度卷积神经网络，在许多公民科学平台中对生物物种进行基于图像的识别是至关重要的。然而，构建足够大小和样本的数据集来训练网络以及网络架构的选择本身仍然很少有文献记录，因此不容易被复制。在本文中，我们开发了一种简化的方法，用于从公开可用的研究级数据集构建生物分类群的数据集，并利用这些数据集使用谷歌的AutoML Vision云服务提供的现成深度卷积神经网络来推导模型。我们的案例研究是葡萄牙本地植物，基于由葡萄牙植物学会提供的高质量数据集，并通过添加来自iNaturalist、Pl@ntNet和Observation.org的采集数据进行扩展。我们发现通过谨慎地

    arXiv:2403.12072v1 Announce Type: cross  Abstract: Machine-learning techniques, namely deep convolutional neural networks, are pivotal for image-based identification of biological species in many Citizen Science platforms. However, the construction of critically sized and sampled datasets to train the networks and the choice of the network architectures itself remains little documented and, therefore, does not lend itself to be easily replicated. In this paper, we develop a streamlined methodology for building datasets for biological taxa from publicly available research-grade datasets and for deriving models from these datasets using off-the-shelf deep convolutional neural networks such as those provided by Google's AutoML Vision cloud service. Our case study is the Portuguese native flora, anchored in a high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica, scaled up by adding sampled data from iNaturalist, Pl@ntNet, and Observation.org. We find that with a careful
    
[^130]: 缺乏地面真相情况下提升建模的公平评估

    Fairness Evaluation for Uplift Modeling in the Absence of Ground Truth

    [https://arxiv.org/abs/2403.12069](https://arxiv.org/abs/2403.12069)

    提出了一个框架，通过生成替代品来充当提升建模活动的反事实标签代理，从而进行更全面的二进制公平性评估。

    

    人工智能自动决策系统采用加速，对算法决策公平性进行评估面临挑战，特别是在缺乏地面真相的情况下。本文提出了一个框架来克服缺少地面真相的问题，通过产生替代品来充当提升建模活动的反事实标签代理。我们利用替代地面真相进行更全面的二进制公平性评估。

    arXiv:2403.12069v1 Announce Type: cross  Abstract: The acceleration in the adoption of AI-based automated decision-making systems poses a challenge for evaluating the fairness of algorithmic decisions, especially in the absence of ground truth. When designing interventions, uplift modeling is used extensively to identify candidates that are likely to benefit from treatment. However, these models remain particularly susceptible to fairness evaluation due to the lack of ground truth on the outcome measure since a candidate cannot be in both treatment and control simultaneously. In this article, we propose a framework that overcomes the missing ground truth problem by generating surrogates to serve as a proxy for counterfactual labels of uplift modeling campaigns. We then leverage the surrogate ground truth to conduct a more comprehensive binary fairness evaluation. We show how to apply the approach in a comprehensive study from a real-world marketing campaign for promotional offers and d
    
[^131]: 在电子学习中利用过程挖掘进行自主调节学习评估

    Process mining for self-regulated learning assessment in e-learning

    [https://arxiv.org/abs/2403.12068](https://arxiv.org/abs/2403.12068)

    通过在电子学习过程中应用过程挖掘技术，发现了学生的自主调节学习过程模型，有助于改善教学过程。

    

    在最近几十年里，电子学习场景中的内容评估得到了广泛改进。然而，电子学习过程可能导致空间和时间上的差距，这对于不仅内容评估，还关于学生核心技能如自主学习的评估提出了有趣的挑战。本研究旨在利用过程挖掘技术发现电子学习课程中学生的自主调节学习过程。我们在Moodle 2.0平台上的一个学期课程中应用了在教育领域中称为Inductive Miner的新算法对101名大学生的互动迹线进行分析。通过提取自平台事件日志中的21629条迹线数据，发现了有助于改善教学过程的学生自我调节模型。Inductive Miner算法在此数据集中发现了适合通过和未通过学生的最佳模型，以适应两者的适应性。

    arXiv:2403.12068v1 Announce Type: cross  Abstract: Content assessment has broadly improved in e-learning scenarios in recent decades. However, the eLearning process can give rise to a spatial and temporal gap that poses interesting challenges for assessment of not only content, but also students' acquisition of core skills such as self-regulated learning. Our objective was to discover students' self-regulated learning processes during an eLearning course by using Process Mining Techniques. We applied a new algorithm in the educational domain called Inductive Miner over the interaction traces from 101 university students in a course given over one semester on the Moodle 2.0 platform. Data was extracted from the platform's event logs with 21629 traces in order to discover students' self-regulation models that contribute to improving the instructional process. The Inductive Miner algorithm discovered optimal models in terms of fitness for both Pass and Fail students in this dataset, as we
    
[^132]: 适应任意尺寸体积X射线数据集的SAM调整

    Adapting SAM for Volumetric X-Ray Data-sets of Arbitrary Sizes

    [https://arxiv.org/abs/2403.12066](https://arxiv.org/abs/2403.12066)

    将Segment Anything Model (SAM)与基于切片的Flood Filling Networks (FFN)相结合，实现了对体积X射线数据集中的三维对象进行分割的新方法。

    

    目标：我们提出了一种新方法，通过将Segment Anything Model (SAM)与基于切片的Flood Filling Networks (FFN)相结合，实现X射线计算机断层扫描（CT）数据中的体积实例分割，用于非破坏性测试（NDT）。我们的工作评估了SAM在体积NDT数据集上的性能，并展示了它在具有挑战性的成像场景中对实例进行分割的有效性。方法：我们实现并评估了扩展基于图像的SAM算法以用于体积数据集的技术，从而利用FFN的空间适应性来实现三维对象的分割。基于切片的SAM方法利用了FFN的能力来分割任何大小的对象。我们还探讨了使用密集提示指导SAM将分割的切片组合以提高分割精度。结果：我们的研究表明，将SAM与FFN结合用于体积实例分割任务的潜力。

    arXiv:2403.12066v1 Announce Type: cross  Abstract: Objective: We propose a new approach for volumetric instance segmentation in X-ray Computed Tomography (CT) data for Non-Destructive Testing (NDT) by combining the Segment Anything Model (SAM) with tile-based Flood Filling Networks (FFN). Our work evaluates the performance of SAM on volumetric NDT data-sets and demonstrates its effectiveness to segment instances in challenging imaging scenarios. Methods: We implemented and evaluated techniques to extend the image-based SAM algorithm fo the use with volumetric data-sets, enabling the segmentation of three-dimensional objects using FFN's spatially adaptability. The tile-based approach for SAM leverages FFN's capabilities to segment objects of any size. We also explore the use of dense prompts to guide SAM in combining segmented tiles for improved segmentation accuracy. Results: Our research indicates the potential of combining SAM with FFN for volumetric instance segmentation tasks, part
    
[^133]: 一致性模型改进扩散逆求解器

    Consistency Models Improve Diffusion Inverse Solvers

    [https://arxiv.org/abs/2403.12063](https://arxiv.org/abs/2403.12063)

    本文提出了使用一致性模型作为高质量逼近，以改进对扩散逆求解器中后验样本的使用。

    

    扩散逆求解器（DIS）的目标是找到一个在扩散先验空间中的图像$x$，满足约束$f(x)=y$，给定算子$f(\cdot)$和测量$y$。大多数非线性DIS使用后验均值$\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$来评估$f(\cdot)$并最小化距离$||f(\hat{x}_{0|t})-y||^2$。先前的工作表明，基于后验均值的距离是有偏的；而后验样本$x_{0|t}\sim p_{\theta}(x_0|x_t)$承诺是更好的候选。本文首先澄清了在何种情况下后验样本更好：$1)$当$f(\cdot)$是线性的时，使用后验均值的距离就像单个后验样本一样好，因此更可取，因为它不需要蒙特卡洛；$2)$当$f(\cdot)$是非线性的时，使用后验样本的距离更好。由于先前对后验样本的逼近不像真实图像，我们提出使用一致性模型（CM）作为高质量逼近。

    arXiv:2403.12063v1 Announce Type: cross  Abstract: Diffusion inverse solvers (DIS) aim to find an image $x$ that lives on the diffusion prior while satisfying the constraint $f(x) = y$, given an operator $f(.)$ and measurement $y$. Most non-linear DIS use posterior mean $\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$ to evaluate $f(.)$ and minimize the distance $||f(\hat{x}_{0|t})-y||^2$. Previous works show that posterior mean-based distance is biased; instead, posterior sample $x_{0|t}\sim p_{\theta}(x_0|x_t)$ promises a better candidate. In this paper, we first clarify when is posterior sample better: $1)$ When $f(.)$ is linear, the distance with posterior mean is as good as single posterior sample, thus preferable as it does not require Monte Carlo; $2)$ When $f(.)$ is non-linear, the distance using posterior sample is better. As previous approximations to posterior sample do not look like a real image, we propose to use consistency model (CM) as a high quality approximation. In addition, we p
    
[^134]: 使用联邦学习进行口腔疾病检测的移动应用

    Mobile Application for Oral Disease Detection using Federated Learning

    [https://arxiv.org/abs/2403.12044](https://arxiv.org/abs/2403.12044)

    该研究开发了一个名为OralH的移动应用，利用联邦学习技术进行口腔疾病检测，用户可以通过口腔扫描进行自我评估，获得快速的口腔健康建议。

    

    嘴巴经常被视为身体内部状态的一扇窗户，对反映一个人的整体健康起着重要作用。口腔卫生不佳会导致严重疾病，如心脏病、癌症和糖尿病，而不恰当的护理会导致不适、疼痛和昂贵的治疗。联邦学习（FL）用于目标检测可用于此用例，因患者口腔图像数据的敏感性。FL通过在本地设备上存储用于目标检测的图像并在边缘上训练模型来确保数据隐私。更新的权重被联合到中央服务器，其中所有收集到的权重通过联邦平均算法进行更新。最后，我们开发了一个名为OralH的移动应用，提供用户友好的解决方案，允许人们通过口腔扫描进行自我评估并提供快速的口腔健康见解。

    arXiv:2403.12044v1 Announce Type: cross  Abstract: The mouth, often regarded as a window to the internal state of the body, plays an important role in reflecting one's overall health. Poor oral hygiene has far-reaching consequences, contributing to severe conditions like heart disease, cancer, and diabetes, while inadequate care leads to discomfort, pain, and costly treatments. Federated Learning (FL) for object detection can be utilized for this use case due to the sensitivity of the oral image data of the patients. FL ensures data privacy by storing the images used for object detection on the local device and trains the model on the edge. The updated weights are federated to a central server where all the collected weights are updated via The Federated Averaging algorithm. Finally, we have developed a mobile app named OralH which provides user-friendly solutions, allowing people to conduct self-assessments through mouth scans and providing quick oral health insights. Upon detection o
    
[^135]: 确定通过移动数字行为变化干预提高癌症患者福祉的有效参与方式

    Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions

    [https://arxiv.org/abs/2403.12007](https://arxiv.org/abs/2403.12007)

    本研究旨在定义通过数字行为变化干预支持癌症患者提高生活质量的有效参与方式，发现医生处方显著增加患者对移动数字行为变化干预的持续参与，同时指出每周参与一次已足以维持福祉，但内在动机可能需要更高水平的参与。

    

    数字行为变化干预（DBCI）正在支持新健康行为的发展。评估它们的有效性对于改进它们和理解成功因素至关重要。然而，特别是在受伦理限制的小规模研究中，开发者的全面指导仍然有限。本研究基于CAPABLE项目，旨在定义通过DBCI支持癌症患者提高生活质量的有效参与方式。我们确定了衡量参与度的指标，探讨了患者和临床医生对DBCI的兴趣，并提出了在这种背景下评估DBCI影响的假设。我们的研究结果表明，医生的处方显着增加了患者对移动DBCI的持续参与。此外，尽管每周一次参与DBCI就足以维持福祉，但从外在动机向内在动机的转变可能需要更高水平的参与。

    arXiv:2403.12007v1 Announce Type: cross  Abstract: Digital Behavior Change Interventions (DBCIs) are supporting development of new health behaviors. Evaluating their effectiveness is crucial for their improvement and understanding of success factors. However, comprehensive guidance for developers, particularly in small-scale studies with ethical constraints, is limited. Building on the CAPABLE project, this study aims to define effective engagement with DBCIs for supporting cancer patients in enhancing their quality of life. We identify metrics for measuring engagement, explore the interest of both patients and clinicians in DBCIs, and propose hypotheses for assessing the impact of DBCIs in such contexts. Our findings suggest that clinician prescriptions significantly increase sustained engagement with mobile DBCIs. In addition, while one weekly engagement with a DBCI is sufficient to maintain well-being, transitioning from extrinsic to intrinsic motivation may require a higher level o
    
[^136]: 使用元提示自动化LLMs进行零样本视觉识别

    Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs

    [https://arxiv.org/abs/2403.11755](https://arxiv.org/abs/2403.11755)

    提出了Meta-Prompting for Visual Recognition (MPVR)方法，通过仅需少量信息即可自动化零样本识别中的提示生成过程。

    

    大型语言模型（LLM）生成的类别特定提示的提示集成已经被证明是增强视觉语言模型（VLMs）零样本识别能力的有效方法。为了获得这些类别特定提示，现有方法依赖于手工为LLMs设计提示，以生成下游任务的VLM提示。然而，这需要手动编写这些任务特定提示，而且它们可能无法涵盖与感兴趣类别相关的各种视觉概念和任务特定风格。为了有效地将人类排除在循环之外，并完全自动化零样本识别的提示生成过程，我们提出了用于视觉识别的元提示（MPVR）。仅以目标任务的少量自然语言描述形式以及一系列相关类别标签作为输入，MPVR自动产生一个多样化的类别提示集。

    arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
    
[^137]: 用于深度学习和大数据应用的自动化数据处理和特征工程：一项调查

    Automated data processing and feature engineering for deep learning and big data applications: a survey

    [https://arxiv.org/abs/2403.11395](https://arxiv.org/abs/2403.11395)

    现代人工智能方法旨在设计能够直接从数据中学习的算法，自动化数据处理任务的兴起驱动了机器学习和大数据应用中利用大量复杂数据的发展。

    

    现代人工智能（AI）的方法旨在设计能够直接从数据中学习的算法。这种方法取得了令人印象深刻的成果，并在AI的发展中做出了重要贡献，特别是在监督深度学习领域。它也简化了机器学习系统的设计，因为学习过程是高度自动化的。然而，并非所有传统深度学习流程中的数据处理任务都已自动化。在大多数情况下，数据必须在可以用于训练之前经过手动收集、预处理并通过数据增强进一步扩展。最近，出现了用于自动化这些任务的特殊技术。数据处理任务的自动化驱动力是利用大量复杂、异构数据进行机器学习和大数据应用。如今，基于自动化机器学习（A

    arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
    
[^138]: JORA: 用于检索增强微调的JAX张量并行LoRA库

    JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning

    [https://arxiv.org/abs/2403.11366](https://arxiv.org/abs/2403.11366)

    提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。

    

    《JORA: JAX张量并行LoRA库用于检索增强微调》通过介绍一种新的框架，提供了一种适用于检索增强生成（RAG）任务的PEFT兼容微调Llama-2模型的方法，利用分布式训练，独特地利用了JAX的即时编译（JIT）和张量分片，实现了资源的高效管理，从而实现了加速微调并降低内存需求。这一进展显著提高了微调大型语言模型（LLMs）用于复杂RAG应用的可扩展性和可行性，甚至在GPU资源有限的系统上也能取得显著改进。

    arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
    
[^139]: CPA-Enhancer：链式思维驱动自适应增强器用于未知退化下的目标检测

    CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations

    [https://arxiv.org/abs/2403.11220](https://arxiv.org/abs/2403.11220)

    提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能

    

    目前，已经广泛研究了在已知单一退化情况下的目标检测方法。然而，现有方法需要先验知识来确定退化类型，并为每种类型训练一个单独的模型，从而限制了它们在不可预测环境中的实际应用。为了解决这一挑战，我们提出了一种链式思维（CoT）驱动的自适应增强器CPA-Enhancer，用于未知退化情况下的目标检测。具体而言，CPA-Enhancer在CoT提示的逐步指导下逐步调整其增强策略，这些提示编码了与退化相关的信息。据我们所知，这是首个利用CoT提示进行目标检测任务的工作。总的来说，CPA-Enhancer是一个即插即用的增强模型，可以集成到任何通用检测器中，在不事先知道退化类型的情况下，在受损图像上实现显著提升。实验结果表明，CPA-E

    arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
    
[^140]: 结合高阶结构信息进行图聚类

    Incorporating Higher-order Structural Information for Graph Clustering

    [https://arxiv.org/abs/2403.11087](https://arxiv.org/abs/2403.11087)

    该论文提出了一种新颖的图聚类网络，利用图的高阶结构信息，并设计了一个图互信息极大化模块，有效地最大化了图级和节点级表示之间的互信息。

    

    聚类在数据挖掘中具有深远意义。近年来，图卷积网络(GCN)作为一种强大的深度聚类工具崭露头角，集成了图结构信息和节点属性。然而，大多数现有方法忽略了图的高阶结构信息。显然，位于同一聚类中的节点可以建立远距连接。此外，最近的深度聚类方法通常应用自监督模块来监控模型的训练过程，仅关注节点属性而忽略图结构。本文提出了一种新颖的图聚类网络，充分利用了图的结构信息。为了捕捉高阶结构信息，我们设计了一个图互信息极大化模块，有效地最大化了图级和节点级表示之间的互信息，并采用了一个包含三元自监督模块的模型。

    arXiv:2403.11087v1 Announce Type: new  Abstract: Clustering holds profound significance in data mining. In recent years, graph convolutional network (GCN) has emerged as a powerful tool for deep clustering, integrating both graph structural information and node attributes. However, most existing methods ignore the higher-order structural information of the graph. Evidently, nodes within the same cluster can establish distant connections. Besides, recent deep clustering methods usually apply a self-supervised module to monitor the training process of their model, focusing solely on node attributes without paying attention to graph structure. In this paper, we propose a novel graph clustering network to make full use of graph structural information. To capture the higher-order structural information, we design a graph mutual infomax module, effectively maximizing mutual information between graph-level and node-level representations, and employ a trinary self-supervised module that includ
    
[^141]: DTOR：决策树异常值回归器用于解释异常

    DTOR: Decision Tree Outlier Regressor to explain anomalies

    [https://arxiv.org/abs/2403.10903](https://arxiv.org/abs/2403.10903)

    DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。

    

    解释异常值的出现以及其产生机制在各种领域中可能非常重要。故障、欺诈、威胁等问题，除了被正确识别之外，通常需要有效的解释以有效执行可操作的对抗措施。越来越广泛地使用复杂的机器学习方法来识别异常值，使得这样的解释更具挑战性。我们提出了决策树异常值回归器（DTOR），这是一种通过估计异常检测模型生成的异常分数来为单个数据点生成基于规则的解释的技术。这是通过首先应用决策树回归器来计算估计分数，然后提取与数据点分数相关联的相对路径来实现的。我们的结果表明，即使在具有大量特征的数据集中，DTOR的鲁棒性也得到了证实。此外，与其他基于规则的方法相比

    arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
    
[^142]: 对GlassNet在玻璃稳定性和形成能力的物理启发式机器学习进行评估

    Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability

    [https://arxiv.org/abs/2403.10682](https://arxiv.org/abs/2403.10682)

    对GlassNet模型在预测玻璃稳定性参数方面的应用进行了评估，探索了使用这些参数来估计玻璃的形成能力的可行性。

    

    玻璃构成了许多现代应用的基础，也在未来医疗和环境应用中具有巨大潜力。然而，它们的结构复杂性和庞大的组成空间使得对某些应用进行设计和优化具有挑战性。玻璃加工特别重要的是估计给定组成的玻璃成形能力（GFA）。然而，关于玻璃形成的物理机制仍存在许多未解之谜，特别是在氧化玻璃中。显而易见，用于估计GFA的代理属性将在玻璃加工和设计中非常有用，但寻找这样一个替代性属性已被证明是困难的。在这里，我们探讨了一个开源的预训练NN模型GlassNet的应用，该模型可以预测计算玻璃稳定性（GS）所需的特征温度，并评估使用这些物理启发式ML（PIML）预测的GS参数来估计形成能力的可行性。

    arXiv:2403.10682v1 Announce Type: cross  Abstract: Glasses form the basis of many modern applications and also hold great potential for future medical and environmental applications. However, their structural complexity and large composition space make design and optimization challenging for certain applications. Of particular importance for glass processing is an estimate of a given composition's glass-forming ability (GFA). However, there remain many open questions regarding the physical mechanisms of glass formation, especially in oxide glasses. It is apparent that a proxy for GFA would be highly useful in glass processing and design, but identifying such a surrogate property has proven itself to be difficult. Here, we explore the application of an open-source pre-trained NN model, GlassNet, that can predict the characteristic temperatures necessary to compute glass stability (GS) and assess the feasibility of using these physics-informed ML (PIML)-predicted GS parameters to estimat
    
[^143]: 用于Fisher-Rao距离的近似和界定技术

    Approximation and bounding techniques for the Fisher-Rao distances

    [https://arxiv.org/abs/2403.10089](https://arxiv.org/abs/2403.10089)

    本文考虑了几种数值上稳健的Fisher-Rao距离的近似和界定技术，包括基于闭合形式1D子模型Fisher-Rao距离的通用上界以及取决于测地线或预测测地线是否闭合形式获得的几种通用近似方案，并提出了一种通用方法保证近似误差任意小。

    

    统计模型的两个概率分布之间的Fisher-Rao距离被定义为Fisher信息度量诱导的Riemannian测地距离。为了以闭合形式计算Fisher-Rao距离，我们需要（1）推导出Fisher-Rao测地线的公式，以及（2）沿着这些测地线积分Fisher长度元素。我们考虑了几种数值上稳健的Fisher-Rao距离的近似和界定技术：首先，我们基于子模型的闭合形式1D Fisher-Rao距离报告了Fisher-Rao距离的通用上界。其次，我们描述了几种通用的近似方案，取决于Fisher-Rao测地线或预测测地线是否能以闭合形式获得。特别地，我们获得了一种通用的方法，可以保证在提供Fisher-Rao预测测地线和严格的下界和上界时近似产生任意小的附加误差。

    arXiv:2403.10089v1 Announce Type: cross  Abstract: The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available
    
[^144]: Fisher Mask节点用于语言模型合并

    Fisher Mask Nodes for Language Model Merging

    [https://arxiv.org/abs/2403.09891](https://arxiv.org/abs/2403.09891)

    介绍了一种用于Transformers的新型模型合并方法，利用Fisher信息进行加权平均，提高了多任务模型的性能。

    

    微调预训练模型在下游性能方面具有显著优势。预训练模型（如BERT及其衍生物）在自然语言处理中的普遍性也导致了任务特定微调模型的激增。在多任务场景中，由于这些模型通常只能很好地执行一项任务，因此需要额外的训练或集成。模型合并这一不断增长的领域提供了一个解决方案，解决了将多个任务特定模型合并为单个多任务模型的挑战。在本研究中，我们引入了一种新颖的用于Transformers的模型合并方法，结合了先前Fisher加权平均和Fisher信息在模型修剪中的应用的见解。通过利用Transformer架构内的mask节点的Fisher信息，我们设计了一个计算效率高的加权平均方案。我们的方法展现出了稳定且显著的性能。

    arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
    
[^145]: FoldToken：通过矢量量化及更多方法学习蛋白质语言

    FoldToken: Learning Protein Language via Vector Quantization and Beyond

    [https://arxiv.org/abs/2403.09673](https://arxiv.org/abs/2403.09673)

    通过将蛋白质序列和结构表示为离散符号，并创建新的蛋白质语言，从而构建了一种用于序列-结构共生产的创新方法。

    

    是否存在一种同时描述蛋白质序列和结构的外语？由于连续3D点表示的蛋白质结构与离散序列的对比建模方式，长期以来一直存在挑战。我们引入了\textbf{FoldTokenizer}，将蛋白质序列-结构表示为离散符号。这种创新方法涉及将残基类型和结构投射到一个离散空间中，通过一个信息保存的重构损失进行指导。我们将学习到的离散符号称为\textbf{FoldToken}，而FoldTokens的序列则成为一种新的蛋白质语言，将蛋白质序列-结构转化为一种统一的形态。我们将创建的蛋白质语言应用于普通主干修补和抗体设计任务，构建了首个GPT风格模型(\textbf{FoldGPT})用于具有良好结果的序列-结构共生产。我们成功的关键在于显著的增强

    arXiv:2403.09673v1 Announce Type: cross  Abstract: Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancem
    
[^146]: MM1：多模式LLM预训练的方法、分析与见解

    MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training

    [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)

    通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。

    

    在这项工作中，我们讨论了构建高性能的多模式大型语言模型（MLLMs）。具体来说，我们研究了各种架构组件和数据选择的重要性。通过对图像编码器、视觉语言连接器和各种预训练数据选择进行仔细和全面的消融实验，我们确定了几个关键的设计经验。例如，我们展示了对大规模多模式预训练使用仔细混合的图像标题、交替图像文本和仅文本数据对于在多个基准测试中实现最新潮（SOTA）的少样本结果至关重要，与其他已发表的预训练结果相比。此外，我们表明图像编码器连同图像分辨率和图像标记计数具有重要影响，而视觉语言连接器设计相对重要性较小。通过扩大所提出的方法，我们构建了MM1，一个多模式模型系列。

    arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
    
[^147]: 使用Q学习的奶牛养殖场电池管理的强化学习方法

    A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning

    [https://arxiv.org/abs/2403.09499](https://arxiv.org/abs/2403.09499)

    该研究提出了一种基于Q学习的算法，以实现在奶牛养殖中整合可再生能源，以改善电池管理，应对电能消耗波动和能源价格波动的挑战。

    

    奶牛养殖消耗大量能源，是农业中一个能源密集型的部门。将可再生能源集成到奶牛养殖中可以帮助应对这一挑战。有效的电池管理对于整合可再生能源发电至关重要。管理电池的充电和放电由于电能消耗的波动、可再生能源发电的间歇性以及能源价格的波动而面临重大挑战。人工智能（AI）有潜力显著改善奶牛养殖中可再生能源的利用，然而在这一特定领域中进行的研究有限。本研究以爱尔兰作为案例研究，以实现其以可再生能源利用为核心的2030年能源战略。这项研究提出了一种基于Q学习的算法，用于安排电池的充电和放电。

    arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
    
[^148]: 再现性和几何内在维度性：对图神经网络研究的调查

    Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research

    [https://arxiv.org/abs/2403.08438](https://arxiv.org/abs/2403.08438)

    本文研究了图神经网络研究中的再现性和几何内在维度性问题，并引入机器学习中的再现性本体论，以及探讨了维度诅咒对数据收集、表示和分析的挑战。

    

    机器学习研究中复制和可再现性的困难近年来成为一个突出的话题。确保机器学习研究结果的可靠性需要可再现性，通过使用相同的代码和数据验证研究结果的可靠性。这促进了开放和可访问的研究、稳健的实验工作流程以及新发现的快速整合。评估研究出版物支持再现性的程度是本文的一个目标。为此，我们引入了一个机器学习中的再现性本体论，并将其应用于图神经网络的方法。在这些努力的基础上，我们转向机器学习中的另一个关键挑战，即维度诅咒，它在数据收集、表示和分析方面带来挑战，使得更难找到代表性数据。

    arXiv:2403.08438v1 Announce Type: cross  Abstract: Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data 
    
[^149]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^150]: 推动少数群体份额如何影响泛化？关于一层隐藏层神经网络在群体不平衡上的理论研究

    How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance

    [https://arxiv.org/abs/2403.07310](https://arxiv.org/abs/2403.07310)

    本文通过高斯混合模型量化了群体不平衡对样本复杂性、收敛速率和平均以及群体级测试性能的影响，首次提供了ERM在群体级泛化的理论分析。

    

    群体不平衡一直是经验风险最小化（ERM）中已知的问题，其中取得的高平均准确率伴随着少数群体的低准确率。尽管有算法努力改善少数群体的准确性，但关于ERM在各个群体上的理论泛化分析仍然难以实现。通过用高斯混合模型表达群体不平衡问题，本文量化了各个群体对样本复杂性、收敛速率以及平均和群体级测试性能的影响。虽然我们的理论框架集中在使用一层隐藏层神经网络进行二分类，但据我们所知，我们首次提供了ERM在群体级泛化的理论分析，除了通常研究的平均泛化性能。我们的理论结果的一些见解包括当所有群体级协方差都在...

    arXiv:2403.07310v1 Announce Type: cross  Abstract: Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in th
    
[^151]: RLingua：利用大型语言模型改善在机器人操作中的强化学习样本效率

    RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models

    [https://arxiv.org/abs/2403.06420](https://arxiv.org/abs/2403.06420)

    RLingua提出了一个框架，利用大型语言模型的内部知识来提高机器人操作中强化学习的样本效率。

    

    强化学习（RL）已经证明了其在解决各种任务中的能力，但以其低样本效率而声名狼藉。在本文中，我们提出了RLingua，这是一个可以利用大型语言模型（LLMs）的内部知识来减少机器人操作中RL的样本复杂性的框架。为此，我们首先介绍了如何通过提示工程提取LLMs的先验知识，从而生成特定任务的初步基于规则的机器人控制器。尽管不完美，LLM生成的机器人控制器被用于在rollout时以衰减概率生成动作样本，从而提高RL的样本效率。我们采用了演员-评论家框架，并修改了演员损失，以使策略学习朝着LLM生成的控制器规范化。RLingua还提供了一种改善不完美的LLM生成机器人控制器的新方法。我们展示了RLing

    arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing
    
[^152]: 在超参数优化中重新思考基于编码器的热启动方法

    Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization

    [https://arxiv.org/abs/2403.04720](https://arxiv.org/abs/2403.04720)

    提出了一种新的基于编码器的表格数据集表示方法，与现有方法不同，能够自动提取重要的元特征，同时在两个常见的元任务上进行了评估

    

    有效地表示异质性表格数据集以用于元学习仍然是一个未解决的问题。以往的方法依赖于预定义的元特征，例如，统计量或标志点。基于编码器的模型，如Dataset2Vec，使我们能够在无人干预的情况下自动提取重要的元特征。本研究介绍了一个新颖的基于编码器的表格数据集表示，实现在liltab包中，该包可在GitHub上找到https://github.com/azoz01/liltab。我们的包基于[Iwata and Kumagai, 2020]提出的一个用于异质表格数据的已建立模型。所提出的方法采用一种不同于现有方法如Dataset2Vec 的编码特征关系的模型，生成与现有方法不同的替代表示。它们都利用了数据集相似性学习的基本假设。在这项工作中，我们在两个常见的元任务上评价了Dataset2Vec和liltab

    arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
    
[^153]: 交互式持续学习: 快速与缓慢思考

    Interactive Continual Learning: Fast and Slow Thinking

    [https://arxiv.org/abs/2403.02628](https://arxiv.org/abs/2403.02628)

    本文提出了一种基于交互的持续学习框架，通过多模型之间的合作交互，实现了更好的任务推导和内存检索。

    

    高级生命形式通过神经认知机制的协同互动，终身不断地获取和传递知识。与此相反，当代机器学习范式在模拟持续学习的方面存在局限性。然而，大型语言模型的出现为通过与这些模型的交互实现持续学习提供了希望。本文基于补充学习系统理论，提出了一种新颖的交互式持续学习（ICL）框架，通过各种规模模型之间的协作交互实现。具体而言，我们将ViT模型指定为第一系统，将多模态LLM指定为第二系统。为了使内存模块能够从类信息中推导任务并增强Set2Set检索，我们提出了Class-Knowledge-Task Multi-Head Attention (CKT-MHA)。此外，为了通过增强的几何检索改进第一系统的内存检索

    arXiv:2403.02628v1 Announce Type: cross  Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric r
    
[^154]: 一个镜子的库：低维深度神经网络是具有反射特征的凸Lasso模型

    A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features

    [https://arxiv.org/abs/2403.01046](https://arxiv.org/abs/2403.01046)

    证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。

    

    我们证明在1-D数据上训练神经网络等价于解决一个带有固定、明确定义的特征字典矩阵的凸Lasso问题。具体的字典取决于激活函数和深度。我们考虑具有分段线性激活函数的两层网络，深窄的ReLU网络最多有4层，以及具有符号激活和任意深度的矩形和树网络。有趣的是，在ReLU网络中，第四层创建代表训练数据关于自身的反射的特征。Lasso表示法揭示了全局最优网络和解空间的洞察。

    arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
    
[^155]: 切割面和立方面的提升多段切割多面体

    Cut Facets and Cube Facets of Lifted Multicut Polytopes

    [https://arxiv.org/abs/2402.16814](https://arxiv.org/abs/2402.16814)

    本文回答了提升多段切割多面体的哪些下界立方不等式和哪些切割不等式定义面的基本问题，以及判定切割不等式定义性的NP难性。

    

    提升多段切割问题在计算机视觉领域有着多样的应用。基于线性规划的精确算法需要对提升多段切割多面体有所了解。尽管最近取得了一些进展，但关于这些多面体的两个基本问题一直未能解决：哪些下界立方不等式定义了面，哪些切割不等式定义了面？在本文中，我们通过建立必要、充分且高效可判定的条件来回答第一个问题。至于第二个问题，我们表明判断切割不等式的面定义性是NP难的。这完成了对提升多段切割多面体的规范面的分析。

    arXiv:2402.16814v2 Announce Type: replace-cross  Abstract: The lifted multicut problem has diverse applications in the field of computer vision. Exact algorithms based on linear programming require an understanding of lifted multicut polytopes. Despite recent progress, two fundamental questions about these polytopes have remained open: Which lower cube inequalities define facets, and which cut inequalities define facets? In this article, we answer the first question by establishing conditions that are necessary, sufficient and efficiently decidable. Toward the second question, we show that deciding facet-definingness of cut inequalities is NP-hard. This completes the analysis of canonical facets of lifted multicut polytopes.
    
[^156]: 摄像头作为射线: 通过射线扩散进行姿势估计

    Cameras as Rays: Pose Estimation via Ray Diffusion

    [https://arxiv.org/abs/2402.14817](https://arxiv.org/abs/2402.14817)

    提出了一种将相机姿势视为射线束的分布表示方法，结合空间图像特征，开发了基于回归和扩散的姿势估计方法，在CO3D数据集上取得了最先进的性能。

    

    估计相机姿势是3D重建的基本任务，鉴于视图稀疏（<10），该任务仍具有挑战性。与现有方法不同，后者追求相机外参的全局参数化的自上而下预测，我们提出了一种将相机姿势视为射线束的分布表示。这种表示允许与空间图像特征紧密耦合，提高了姿势精度。我们观察到，这种表示自然适用于集合级别的Transformer，并开发了一种基于回归的方法，将图像块映射到相应的射线上。为了捕捉稀疏视角姿势推断中固有的不确定性，我们调整了这种方法，学习了一个去噪扩散模型，使我们能够采样合理的模式，同时提高性能。我们提出的方法，既是基于回归，也是基于扩散的，在CO3D相机姿势估计方面展现出了最先进的性能。

    arXiv:2402.14817v1 Announce Type: cross  Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while
    
[^157]: 大数据分析用于分类与土方相关的地点：成都研究

    Big data analytics to classify earthwork-related locations: A Chengdu study

    [https://arxiv.org/abs/2402.14698](https://arxiv.org/abs/2402.14698)

    使用大数据分析方法，研究者利用自卸车轨迹、城市兴趣点和土地覆盖数据，成功对城市灰尘污染源进行了分类，证明仅需有限数量特征即可实现高准确度分类。

    

    空气污染显著加剧，导致全球范围内的严重健康后果。土方相关的地点（ERLs）是城市灰尘污染的重要来源。长期以来，ERLs的有效管理一直是政府和环境机构面临的挑战之一，主要原因包括其分类分属不同的监管部门、信息障碍、数据更新延迟，以及对不同源头灰尘污染的抑制措施的缺乏。为解决这些挑战，我们利用自卸车轨迹、城市兴趣点（POI）和土地覆盖数据对城市灰尘污染源进行分类。我们比较了几种预测模型，并利用实际数据研究了特征与灰尘污染源之间的关系。结果表明，通过有限数量的特征可以实现高准确度的分类。这种方法已成功实施在一个名为的系统中。

    arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
    
[^158]: 通过用户行为建模和随机规划控制大型电动汽车充电站

    Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming

    [https://arxiv.org/abs/2402.13224](https://arxiv.org/abs/2402.13224)

    本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。

    

    本文介绍了一个电动汽车充电站（EVCS）模型，该模型融合了真实世界的约束条件，如插槽功率限制、合同阈值超限惩罚以及电动汽车（EVs）的早期断开。我们提出了一个在不确定性下控制EVCS的问题形式，并实施了两种多阶段随机规划方法，利用用户提供的信息，即模型预测控制和二阶段随机规划。该模型解决了充电会话开始和结束时间以及能量需求的不确定性。基于驻留时间依赖随机过程的用户行为模型增强了成本降低的同时保持客户满意度。通过使用真实世界数据集进行的22天模拟展示了两种提出方法相对于两个基线的优势。两阶段方法证明了针对早期断开的鲁棒性，考虑了更多

    arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
    
[^159]: 高斯过程神经加性模型

    Gaussian Process Neural Additive Models

    [https://arxiv.org/abs/2402.12518](https://arxiv.org/abs/2402.12518)

    本文提出了一种新的高斯过程神经加性模型（GP-NAM），通过随机傅里叶特征对高斯过程进行单层神经网络构建，可以实现具有凸目标函数和可训练参数数量随特征维度线性增长的优势，同时在性能上不亚于更深的NAM方法。

    

    深度神经网络已经在许多领域引起了革命，但它们的黑盒特性有时也阻碍了它们在医疗保健和金融等领域的广泛应用，这些领域需要可解释和可解释的模型。最近发展出的神经加性模型（NAMs）是在面向表格数据集的可解释深度学习方向上迈出的重要一步。在本文中，我们提出了一种新的NAM子类，它使用通过随机傅里叶特征对高斯过程进行单层神经网络构建，我们称之为高斯过程神经加性模型（GP-NAM）。GP-NAM具有凸目标函数和随特征维度线性增长的可训练参数数量的优势。与更深的NAM方法相比，它在性能上没有损失，因为GPs非常适合学习复杂的非参数单变量函数。我们在多个表格数据集上展示了GP-NAM的性能。

    arXiv:2402.12518v1 Announce Type: cross  Abstract: Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular
    
[^160]: TuneTables：可扩展先验数据拟合网络的上下文优化

    TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks

    [https://arxiv.org/abs/2402.11137](https://arxiv.org/abs/2402.11137)

    提出了TuneTables上下文优化技术，通过开发一种新的提示调整策略，将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。

    

    针对表格分类传统上依赖于从零开始训练的问题，最近提出了一个名为先验数据拟合网络（PFN）的突破性方法，挑战了这种方法。类似于大型语言模型，PFN利用预训练和上下文学习，在单次前向传递中在新任务上取得强大表现。然而，当前的PFN存在限制，阻碍了它们的广泛采用。特别是，TabPFN在小型表格数据集上取得非常强劲的性能，但并不适用于数据集大小大于1000的预测。在这项工作中，我们通过为PFN开发上下文优化技术，克服了这些限制，大幅提高了PFN的性能。具体来说，我们提出了TuneTables，一种将大型数据集压缩为较小学习上下文的新型提示调整策略。TuneTables将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。

    arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas
    
[^161]: 用可解释的风险预测方法降低诊断错误

    Towards Reducing Diagnostic Errors with Interpretable Risk Prediction

    [https://arxiv.org/abs/2402.10109](https://arxiv.org/abs/2402.10109)

    本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，旨在通过增加证据的获取与减少诊断错误来降低诊断错误。模型使用神经加性模型进行预测，以证据为后盾，并给出个体化风险估计，特别针对诊断延迟和来自不完整鉴别的错误进行优化。

    

    许多诊断错误发生是因为临床医生无法轻易获取病人电子病历中的相关信息。本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，最终目标是增加证据的获取与减少诊断错误。我们提出了一种神经加性模型来进行带有个体化风险估计的以证据为后盾的预测，在临床医生仍然不确定的时间点上，旨在特别减轻诊断延迟和源于不完整鉴别的错误。为了训练这样一个模型，需要推断出事件性的“真实”诊断的时间粒度细致的回顾性标签。我们使用LLMs来保证输入文本是在可以进行自信的诊断之前的。我们使用LLMs来检索初始的证据池，然后进行细化。

    arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
    
[^162]: 引导遮蔽表示学习以捕捉心电图的时空关系

    Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram

    [https://arxiv.org/abs/2402.09450](https://arxiv.org/abs/2402.09450)

    本研究提出了一种叫做ST-MEM的模型，通过重构遮蔽的心电图数据来学习时空特征，该模型在心律失常分类任务中优于其他自监督学习方法。

    

    心电图（ECG）广泛用作监测心脏起源的电信号的诊断工具。近年来，机器学习的研究努力集中在使用ECG信号进行各种疾病筛查的应用上。然而，适应疾病筛查应用是具有挑战性的，因为标记的ECG数据有限。通过自监督学习（SSL）实现通用表示是克服标记数据稀缺性的常用方法；然而，在ECG数据上纯粹应用SSL，而不考虑ECG信号固有的时空关系，可能会产生次优的结果。本文介绍了ST-MEM（时空遮蔽心电图建模），该模型通过重构遮蔽的12导联ECG数据来学习时空特征。在各种实验设置中，ST-MEM在心律失常分类任务中的性能优于其他SSL基线方法。

    arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
    
[^163]: 攻击、防御和评估LLM对话安全性的调查

    Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

    [https://arxiv.org/abs/2402.09283](https://arxiv.org/abs/2402.09283)

    这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。

    

    arXiv:2402.09283v1 公告类型: 新的摘要: 大型语言模型（LLMs）在对话应用中已经很常见。然而，它们可能被误用生成有害回复的风险引起了严重的社会关切，并激发了LLM对话安全性的最新研究。因此，在此调查中，我们提供了最近研究的全面概述，涵盖了LLM对话安全性的三个关键方面：攻击、防御和评估。我们的目标是提供一个结构化的摘要，增进对LLM对话安全性的理解，并鼓励进一步研究这一重要课题。为了方便参考，我们根据我们的分类法对所有在此调查中提到的研究进行了分类，可在以下网址找到：https://github.com/niconi19/LLM-conversation-safety。

    arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
    
[^164]: 无调参的随机优化

    Tuning-Free Stochastic Optimization

    [https://arxiv.org/abs/2402.07793](https://arxiv.org/abs/2402.07793)

    本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。

    

    大规模机器学习问题使得调参的成本越来越高昂。这导致了需要能够即时自我调整的算法的需求。我们将“无调参”算法的概念形式化，即只给出问题参数的粗略提示即可与最优调参优化算法的性能相匹配，误差为对数多项式因子。我们特别考虑能够与最优调参的随机梯度下降(SGD)相匹配的算法。当优化的域是有界的时候，我们证明了调参自由与SGD的匹配是可能的，并且通过几个现有算法实现了这一点。我们证明了当优化的域是无界的时候，对于最小化凸平滑或者Lipschitz函数的任务，无调参优化是不可能的。我们讨论了在无界域中，何种情况下可以实现无调参优化。特别地，我们展示了最近提出的 DoG 和 DoWG 算法在噪声分布足够时是无调参的。

    Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf
    
[^165]: 揭示潜在因果规律：一种基于时间点过程的异常事件解释方法

    Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation

    [https://arxiv.org/abs/2402.05946](https://arxiv.org/abs/2402.05946)

    本文提出了一种基于时间点过程的方法，通过揭示潜在因果规律来解释异常事件，以帮助在高风险系统如医疗保健中快速诊断和精确治疗规划。该方法通过期望最大化算法优化规则集和模型参数，实现了准确的规则发现和根因识别。

    

    在高风险系统如医疗保健中，理解异常事件背后的因果原因是至关重要的，例如患者健康状况的突然变化。揭示因果原因有助于快速诊断和精确治疗规划。在本文中，我们提出了一种自动化方法来揭示解释观察事件的“如果-那么”逻辑规则。我们引入了时间点过程来建模所关注事件，并发现一组潜在规则来解释事件的发生。为了实现这一点，我们采用了期望最大化（EM）算法。在E步中，我们计算每个事件被每个发现的规则解释的可能性。在M步中，我们更新规则集和模型参数，以增强可能性函数的下界。值得注意的是，我们以微分的方式优化规则集。我们的方法在发现规则和识别根本原因方面表现出准确的性能。我们使用合成数据展示了它的有希望的结果。

    In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using syntheti
    
[^166]: 预测个体治疗效果的一致性蒙特卡洛元学习模型

    Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects

    [https://arxiv.org/abs/2402.04906](https://arxiv.org/abs/2402.04906)

    本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。

    

    认识干预效果，即治疗效果，对于决策至关重要。用条件平均治疗效果 (CATE) 估计等方法通常只提供治疗效果的点估计，而常常需要额外的不确定性量化。因此，我们提出了一个新方法，即一致性蒙特卡洛 (CMC) 元学习模型，利用一致性预测系统、蒙特卡洛采样和 CATE 元学习模型，来产生可用于个性化决策的预测分布。此外，我们展示了结果噪声分布的特定假设如何严重影响这些不确定性预测。尽管如此，CMC框架展示了强大的实验覆盖范围，同时保持较小的区间宽度，以提供真实个体治疗效果的估计。

    Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
    
[^167]: 检索以解释：基于语言模型的证据驱动预测

    Retrieve to Explain: Evidence-driven Predictions with Language Models

    [https://arxiv.org/abs/2402.04068](https://arxiv.org/abs/2402.04068)

    检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。

    

    机器学习模型，尤其是语言模型，往往难以深入分析。黑盒模型可能掩盖了模型训练中的问题和有害偏差。对于人机协作过程来说，不透明的预测可能导致缺乏信任，限制模型的影响，即使模型的性能很好。为了解决这些问题，我们引入了检索以解释（Retrieve to Explain，简称R2E）。R2E是一种基于检索的语言模型，根据文档语料库中的证据，使用Shapley值来确定证据对最终预测的相对重要性，并根据自然语言模板将结构化数据纳入其中。R2E能够在不重新训练的情况下适应新的证据，并且能够通过模板化将结构化数据纳入到自然语言中。我们在通过分析已发表的科学文献进行药物靶点鉴定的实际案例中进行了评估，结果显示该模型在预测临床试验结果方面优于行业标准的基因学方法。

    Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
    
[^168]: 跨领域少样本目标检测通过增强的开集目标检测器

    Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector

    [https://arxiv.org/abs/2402.03094](https://arxiv.org/abs/2402.03094)

    本文提出了一种跨领域少样本目标检测器，通过增强的开集目标检测方法来解决跨领域数据差异带来的性能下降问题。

    

    本文解决了跨领域少样本目标检测（CD-FSOD）的挑战，旨在开发一个准确的目标检测器，用最少的标记样本来检测新领域的目标。虽然基于转换器的开集检测器（例如DE-ViT）在开放词汇目标检测和传统的少样本目标检测方面表现出色，能够检测到训练过程中没有见过的类别，我们自然会提出两个关键问题：1）这种开集检测方法能否容易地推广到CD-FSOD？2）如果不能，如何在面对显著的领域差异时增强开集方法的结果？为了回答第一个问题，我们引入了几个衡量领域差异的指标，并建立了一个具有多样领域度量值的新的CD-FSOD基准。在这个基准上评估了一些最先进的开集目标检测方法，在域外数据集中观察到明显的性能下降。这表明采用这些方法在CD-FSOD上失败了。

    This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
    
[^169]: 一张图值千言：使用纯Transformer将图形欧拉化

    A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer

    [https://arxiv.org/abs/2402.02464](https://arxiv.org/abs/2402.02464)

    这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。

    

    我们能否将非欧几里德图形建模为纯语言甚至欧几里德向量，同时保留其固有信息？非欧几里德性质一直是图形建模中的长期挑战。尽管最近的GNN和Graphformer努力将图形编码为欧几里德向量，但从向量中恢复出原始图形仍然是一个挑战。我们引入了GraphsGPT，它具有一个将非欧几里德图形转换为在欧几里德空间中可学习图形单词的Graph2Seq编码器，以及一个从图形单词重构原始图形以确保信息等价性的GraphGPT解码器。我们在100M个分子上预训练了GraphsGPT，并得到一些有趣的发现：(1) 预训练的Graph2Seq在图形表示学习方面表现出色，在8/9个图形分类和回归任务上取得了最新成果。(2) 预训练的GraphGPT作为一个强大的图形生成器，其能够进行无条件和有条件的图形生成。(3) Graph2Seq+Gr

    Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
    
[^170]: 深度表格学习需要算术特征交互

    Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning

    [https://arxiv.org/abs/2402.02334](https://arxiv.org/abs/2402.02334)

    本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。

    

    直到最近，关于深度模型在表格数据上的有效归纳偏见的问题仍然没有答案。本文调查了算术特征交互对于深度表格学习的必要性假设。为了测试这一观点，我们创建了一个具有轻微特征交互假设的合成表格数据集，并研究了一种改进的Transformer架构，使其能够进行算术特征交互，称为AMFormer。结果显示，AMFormer在细粒度表格数据建模、训练数据效率和泛化方面优于强对手。这归因于其并行的加性和乘性注意力操作符和基于提示的优化，这有助于在具有算术工程特征的扩展空间中分离表格样本。我们在真实世界数据上进行了广泛的实验，也验证了AMFormer的一致有效性、效率和合理性，表明它已经建立了强有力的归纳能力。

    Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
    
[^171]: 利用无监督表示学习进行量子架构搜索

    Quantum Architecture Search with Unsupervised Representation Learning

    [https://arxiv.org/abs/2401.11576](https://arxiv.org/abs/2401.11576)

    通过利用无监督表示学习，量子架构搜索（QAS）的性能可以得以提升，而不需要耗费大量时间进行标记。

    

    使用无监督表示学习进行量子架构搜索（QAS）代表了一种前沿方法，有望在嘈杂的中间规模量子（NISQ）设备上实现潜在的量子优势。大多数QAS算法将它们的搜索空间和搜索算法结合在一起，因此通常需要在搜索过程中评估大量的量子电路。基于预测的QAS算法可以通过直接根据电路结构估计电路的性能来缓解这个问题。然而，高性能的预测器通常需要耗费大量时间进行标记，以获得大量带标签的量子电路。最近，一个经典的神经架构搜索算法Arch2vec启发我们，表明架构搜索可以从将无监督表示学习与搜索过程分离中获益。无监督表示学习是否能帮助QAS

    arXiv:2401.11576v2 Announce Type: replace-cross  Abstract: Utilizing unsupervised representation learning for quantum architecture search (QAS) represents a cutting-edge approach poised to realize potential quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) devices. Most QAS algorithms combine their search space and search algorithms together and thus generally require evaluating a large number of quantum circuits during the search process. Predictor-based QAS algorithms can alleviate this problem by directly estimating the performance of circuits according to their structures. However, a high-performance predictor generally requires very time-consuming labeling to obtain a large number of labeled quantum circuits. Recently, a classical neural architecture search algorithm Arch2vec inspires us by showing that architecture search can benefit from decoupling unsupervised representation learning from the search process. Whether unsupervised representation learning can help QAS w
    
[^172]: 垂直联邦图像分割

    Vertical Federated Image Segmentation

    [https://arxiv.org/abs/2401.07931](https://arxiv.org/abs/2401.07931)

    提出了一种垂直联邦学习（VFL）模型架构，适用于数据分散于不同数据孤岛的场景，解决了部分地区数据无法访问标记真相，但需要进行分类的问题。

    

    随着人工智能解决图像相关问题的流行，人们越来越关注数据隐私和获取方式。在许多情况下，信息存储在不同的数据孤岛中，开发人员难以将所有信息整合到适合机器学习模型开发的方式中。与此同时，部分本地化数据区域可能无法访问标记的真相。这表明它们有能力通过数字方式得出结论，但在缺乏相关信息的情况下无法进行分类。尤其在尝试开发通常需要该功能的基于图像的解决方案时，这样的确定通常是微不足道的。鉴于此，我们提出了一种创新的垂直联邦学习（VFL）模型架构，该架构可以在这一常见条件下运行。

    arXiv:2401.07931v2 Announce Type: replace-cross  Abstract: With the popularization of AI solutions for image based problems, there has been a growing concern for both data privacy and acquisition. In a large number of cases, information is located on separate data silos and it can be difficult for a developer to consolidate all of it in a fashion that is appropriate for machine learning model development. Alongside this, a portion of these localized data regions may not have access to a labelled ground truth. This indicates that they have the capacity to reach conclusions numerically, but are not able to assign classifications amid a lack of pertinent information. Such a determination is often negligible, especially when attempting to develop image based solutions that often necessitate this capability. With this being the case, we propose an innovative vertical federated learning (VFL) model architecture that can operate under this common set of conditions. This is the first (and curr
    
[^173]: 超图-MLP：在无需消息传递的超图上学习

    Hypergraph-MLP: Learning on Hypergraphs without Message Passing

    [https://arxiv.org/abs/2312.09778](https://arxiv.org/abs/2312.09778)

    提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。

    

    超图在建模包含两个以上实体的高阶关系数据中至关重要，在机器学习和信号处理中越来越受重视。许多超图神经网络利用在超图结构上的消息传递来增强节点表征学习，从而在超图节点分类等任务中取得了令人印象深刻的表现。然而，这些基于消息传递的模型面临着过度平滑以及在推理时对结构扰动的高延迟和敏感性等挑战。为了应对这些挑战，我们提出了一种另类方法，即将关于超图结构的信息集成到训练监督中，而无需明确的消息传递，从而在推理时也消除了对其的依赖。具体而言，我们引入了Hypergraph-MLP，一种新颖的用于超图结构数据的学习框架，其中学习模型是一个简单的多层感知机。

    arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p
    
[^174]: MaxK-GNN: 探索加速图神经网络训练的理论速度极限

    MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training

    [https://arxiv.org/abs/2312.08656](https://arxiv.org/abs/2312.08656)

    MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。

    

    在深度神经网络训练加速方面，GPU已经成为主流平台。 GPU在GNN上面临着诸多挑战，如工作负载不平衡和内存访问不规则，导致硬件利用不充分。现有解决方案例如PyG、DGL与cuSPARSE，以及GNNAdvisor框架部分解决了这些挑战，但内存流量仍然很显著。 我们认为，只有通过算法与系统创新的垂直优化才能实现显著的性能提升，而不是将加速优化视为“事后思考”（即（i）给定GNN算法，设计加速器，或（ii）给定硬件，主要优化GNN算法）。 本文介绍了MaxK-GNN，一种集成算法与系统创新的先进高性能GPU训练系统。 （i）我们引入了MaxK非线性并提供了MaxK非线性的理论分析，

    arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
    
[^175]: OCTDL：基于图像的深度学习方法的光学相干断层扫描数据集

    OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods

    [https://arxiv.org/abs/2312.08255](https://arxiv.org/abs/2312.08255)

    该研究介绍了一个名为OCTDL的开放获取光学相干断层扫描数据集，包括超过2000张标记有疾病组和视网膜病理的OCT图像，有助于诊断眼部状况。

    

    光学相干断层扫描（OCT）是一种非侵入性成像技术，在眼科学中具有广泛的临床应用。OCT可以可视化视网膜层，对早期检测和监测视网膜疾病起着重要作用。本文介绍了一个开放获取的OCT数据集（OCTDL），包括超过2000张根据疾病组和视网膜病理标记的OCT图像。该数据集包括患有老年性黄斑变性（AMD）、糖尿病黄斑水肿（DME）、玻璃体视网膜膜（ERM）、视网膜动脉闭塞（RAO）、视网膜静脉闭塞（RVO）和玻璃体黄斑界面疾病（VID）的患者的OCT记录。这些图像是使用Optovue Avanti RTVue XR采集的，采用了动态扫描长度的光栅扫描协议。

    arXiv:2312.08255v2 Announce Type: replace-cross  Abstract: Optical coherence tomography (OCT) is a non-invasive imaging technique with extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. This work presents an open-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology. The dataset consists of OCT records of patients with Age-related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using raster scanning protocols with dynamic scan length a
    
[^176]: GeoShapley：一种衡量机器学习模型中空间效应的博弈论方法

    GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models

    [https://arxiv.org/abs/2312.03675](https://arxiv.org/abs/2312.03675)

    GeoShapley是一种衡量机器学习模型中空间效应的博弈论方法，将位置视为模型预测博弈中的一名玩家，能够量化位置的重要性并与其他特征之间的协同作用进行量化。

    

    本文介绍了GeoShapley，这是一种衡量机器学习模型中空间效应的博弈论方法。GeoShapley通过将位置概念化为模型预测博弈中的一名玩家，扩展了博弈论中的Nobel Prize-winning Shapley值框架，从而使得可以量化位置的重要性以及位置与模型中其他特征之间的协同作用。GeoShapley是一种与模型无关的方法，可以应用于不同结构的统计学或黑盒机器学习模型中。GeoShapley的解释直接与用于解释空间效应的空间变化系数模型以及用于解释非空间效应的加法模型相连。利用模拟数据，验证了GeoShapley值与已知数据生成过程之间的关系，并用于对比七种统计学和机器学习模型。通过房价建模的实证示例来说明。

    arXiv:2312.03675v2 Announce Type: replace  Abstract: This paper introduces GeoShapley, a game theory approach to measuring spatial effects in machine learning models. GeoShapley extends the Nobel Prize-winning Shapley value framework in game theory by conceptualizing location as a player in a model prediction game, which enables the quantification of the importance of location and the synergies between location and other features in a model. GeoShapley is a model-agnostic approach and can be applied to statistical or black-box machine learning models in various structures. The interpretation of GeoShapley is directly linked with spatially varying coefficient models for explaining spatial effects and additive models for explaining non-spatial effects. Using simulated data, GeoShapley values are validated against known data-generating processes and are used for cross-comparison of seven statistical and machine learning models. An empirical example of house price modeling is used to illus
    
[^177]: 关于数据集的多样性和现实性：一种高效的数据集精炼范式

    On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm

    [https://arxiv.org/abs/2312.03526](https://arxiv.org/abs/2312.03526)

    我们提出了一种新颖的计算效率高且有效的数据精炼范式RDED，以实现数据的多样性和现实性。

    

    当今的机器学习要求在庞大的数据集上训练大型神经网络，因此面临高计算需求的挑战。数据集精炼作为一种最近兴起的策略，旨在压缩真实世界的数据集以进行高效训练。然而，这一研究领域目前在处理大规模和高分辨率的数据集时存在困难，阻碍了其实用性和可行性。为此，我们重新审视现有的数据集精炼方法，并确定了用于大规模真实世界应用的三个所需属性，即现实性，多样性和效率。为了解决这个问题，我们提出了RDED，一种新颖的计算效率高且有效的数据精炼范式，以实现数据的多样性和现实性。对各种神经结构和数据集进行的大量实证结果表明了RDED的进展：我们可以将完整的ImageNet-1K精炼为一个小数据集

    arXiv:2312.03526v2 Announce Type: replace-cross  Abstract: Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprisin
    
[^178]: 在频域中学习多模态正常性以实现高效的时间序列异常检测

    Learning Multi-Pattern Normalities in the Frequency Domain for Efficient Time Series Anomaly Detection

    [https://arxiv.org/abs/2311.16191](https://arxiv.org/abs/2311.16191)

    MACE是一种在频域中适应多正常模式且高效的时间序列异常检测方法，其创新之处在于采用优秀的模式提取机制，使模型能够通过检查数据样本与其服务正常模式之间的相关性来识别异常。

    

    异常检测显著提升了云系统的稳健性。尽管基于神经网络的方法最近展示了强大的优势，但它们在云环境中遇到了实际挑战：在维护每个服务的唯一模型的不切实际性与通过统一模型处理各种正常模式的有限能力之间存在矛盾，以及处理实时高负载和短期异常检测敏感性的问题。因此，我们提出了MACE，一种在频域中适应多正常模式且高效的时间序列异常检测方法。它有三个新颖的特点：（i）一种优秀的模式提取机制，能够处理各种正常模式并使用统一模型，使模型能够通过检查数据样本与其服务正常模式之间的相关性来识别异常。

    arXiv:2311.16191v2 Announce Type: replace-cross  Abstract: Anomaly detection significantly enhances the robustness of cloud systems. While neural network-based methods have recently demonstrated strong advantages, they encounter practical challenges in cloud environments: the contradiction between the impracticality of maintaining a unique model for each service and the limited ability to deal with diverse normal patterns by a unified model, as well as issues with handling heavy traffic in real time and short-term anomaly detection sensitivity.   Thus, we propose MACE, a multi-normal-pattern accommodated and efficient anomaly detection method in the frequency domain for time series anomaly detection. There are three novel characteristics of it: (i) a pattern extraction mechanism excelling at handling diverse normal patterns with a unified model, which enables the model to identify anomalies by examining the correlation between the data sample and its service normal pattern, instead of 
    
[^179]: 通过知识子图学习实现准确且可解释的药物相互作用预测

    Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning

    [https://arxiv.org/abs/2311.15056](https://arxiv.org/abs/2311.15056)

    通过知识子图学习的方法，本研究提出了一种解决药物相互作用预测中样本稀缺性挑战的图神经网络模型，能够准确且可解释地预测药物之间的相互作用。

    

    背景：发现潜在的药物相互作用(DDI)一直是临床治疗和药物开发中长期存在的挑战。最近，深度学习技术已用于DDI预测。然而，它们通常需要大量样本，而已知DDI较为罕见。方法：本文提出了基于图神经网络的KnowDDI方法来解决上述挑战。KnowDDI通过自适应地利用大型生物医学知识图中丰富的邻居信息增强药物表示。然后，它为每对药物学习一个知识子图以解释预测的DDI，其中每条边都与连接强度相关联，指示已知DDI的重要性或类似强度之间的连接在药物对之间是未知的情况下。因此，缺乏DDI的情况通过丰富的药物表示和传播的药物相似性得到隐含补偿。

    arXiv:2311.15056v2 Announce Type: replace-cross  Abstract: Background: Discovering potential drug-drug interactions (DDIs) is a long-standing challenge in clinical treatments and drug developments. Recently, deep learning techniques have been developed for DDI prediction. However, they generally require a huge number of samples, while known DDIs are rare.   Methods: In this work, we present KnowDDI, a graph neural network-based method that addresses the above challenge. KnowDDI enhances drug representations by adaptively leveraging rich neighborhood information from large biomedical knowledge graphs. Then, it learns a knowledge subgraph for each drug-pair to interpret the predicted DDI, where each of the edges is associated with a connection strength indicating the importance of a known DDI or resembling strength between a drug-pair whose connection is unknown. Thus, the lack of DDIs is implicitly compensated by the enriched drug representations and propagated drug similarities.   Resu
    
[^180]: DRESS：通过自然语言反馈指导大型视觉-语言模型与人类对齐和互动

    DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback

    [https://arxiv.org/abs/2311.10081](https://arxiv.org/abs/2311.10081)

    提出了一种名为DRESS的大型视觉语言模型，通过利用自然语言反馈来增强模型与人类之间的对齐和互动，解决了当前大型视觉-语言模型存在的对齐和多轮对话互动方面的两个关键问题

    

    我们提出了DRESS，一种大型视觉语言模型（LVLM），它创新性地利用来自大型语言模型的自然语言反馈（NLF），通过解决当前LVLM中存在的两个关键限制来增强其对齐和互动。首先，先前的LVLM通常仅依赖指导微调阶段来增强与人类偏好的对齐。如果不加入额外反馈，它们仍然容易生成无用、虚构或有害的响应。其次，虽然视觉指导调优数据通常以多轮对话格式结构化，但连续对话轮之间的连接和依赖关系较弱。这降低了有效多轮互动的能力。为了解决这些问题，我们提出了将NLF分为两种关键类型的新颖分类：批评和改进。批评性NLF识别响应的优点和缺点，并用于对齐

    arXiv:2311.10081v2 Announce Type: replace-cross  Abstract: We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align 
    
[^181]: 连续索引张量数据的功能贝叶斯 Tucker 分解

    Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data

    [https://arxiv.org/abs/2311.04829](https://arxiv.org/abs/2311.04829)

    提出了一种名为功能贝叶斯 Tucker 分解（FunBaT）的方法，用于将 Tucker 分解推广到连续索引的张量数据，利用高斯过程模型潜在函数。

    

    Tucker 分解是一种强大的张量模型，用于处理多方面数据。它通过将网格结构数据分解为核张量和一组对象表示（因子）之间的交互来展示低秩特性。这种分解的一个基本假设是每个方面或模式中都有有限的对象，对应于数据条目的离散索引。然而，现实世界中的数据往往并非自然地呈现在这种设置中。例如，地理数据以纬度和经度坐标的连续索引表示，无法直接适应张量模型。为了将 Tucker 分解推广到这种场景，我们提出了功能贝叶斯 Tucker 分解（FunBaT）。我们将连续索引数据视为 Tucker 核心和一组潜在函数之间的交互。我们使用高斯过程（GP）作为功能先验来建模潜在函数。然后，我们将每个高斯过程转换为

    arXiv:2311.04829v2 Announce Type: replace  Abstract: Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). A fundamental assumption of such decomposition is that there are finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However, real-world data is often not naturally posed in this setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To generalize Tucker decomposition to such scenarios, we propose Functional Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. We use Gaussian processes (GP) as functional priors to model the latent functions. Then, we convert each GP 
    
[^182]: 使用高斯过程解决高频和多尺度偏微分方程

    Solving High Frequency and Multi-Scale PDEs with Gaussian Processes

    [https://arxiv.org/abs/2311.04465](https://arxiv.org/abs/2311.04465)

    使用高斯过程框架来解决高频和多尺度偏微分方程，通过灵活捕捉主导频率的方法，估计混合权重并自动诱导稀疏性，有效解决了神经网络训练中的谱偏差问题。

    

    基于机器学习的求解器在物理模拟和科学计算中引起了广泛关注，其中一个著名示例是基于物理信息的神经网络（PINNs）。然而，PINNs在解决高频和多尺度偏微分方程时经常遇到困难，这可能是由于神经网络训练中的谱偏差引起的。为了解决这一问题，我们使用了高斯过程（GP）框架。为了灵活地捕捉主导频率，我们使用学生$t$混合或高斯混合来建模PDE解的功率谱。我们应用逆Fourier变换来获得协方差函数（通过Wiener-Khinchin定理）。从高斯混合谱导出的协方差对应于已知的谱混合核。接下来，我们在对数域中估计混合权重，我们表明这等效于放置Jeffreys先验。它自动诱导稀疏性，修剪过多的频率，并调整剩下的部分。

    arXiv:2311.04465v2 Announce Type: replace  Abstract: Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next, we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remai
    
[^183]: 评估新兴的AI/ML加速器：IPU、RDU和NVIDIA/AMD GPU

    Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs

    [https://arxiv.org/abs/2311.04417](https://arxiv.org/abs/2311.04417)

    本研究提供了对这些商用AI/ML加速器的初步评估和比较，深入探讨它们的硬件和软件设计特点，以辨别它们的创新数据流架构和其他设计优化，承诺为AI/ML任务提供卓越性能和能量效率。

    

    人工智能（AI）和机器学习（ML）应用的不断发展需要开发能够处理日益复杂和计算需求的专用硬件加速器。传统计算架构基于冯·诺伊曼模型，已经被当代AI/ML算法的要求超越，导致像Graphcore Intelligence Processing Unit (IPU)、Sambanova Reconfigurable Datafl····

    arXiv:2311.04417v2 Announce Type: replace-cross  Abstract: The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern t
    
[^184]: TabRepo：一个大规模的表格模型评估库及其AutoML应用

    TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications

    [https://arxiv.org/abs/2311.02971](https://arxiv.org/abs/2311.02971)

    TabRepo引入了一个包含1310个模型在200个分类和回归数据集上评估预测结果和指标信息的数据集，展示了它在进行超参数优化、AutoML系统比较和迁移学习等方面的优势。

    

    我们介绍了TabRepo，这是一个包含1310个模型在200个分类和回归数据集上评估预测结果和指标的新数据集。我们展示了我们的数据集的多种好处。首先，我们表明这个数据集可以进行诸如比较超参数优化与当前AutoML系统以及在使用预计算模型预测的同时考虑集成的分析。其次，我们展示了我们的数据集可以轻松用于执行迁移学习。特别是，我们展示应用标准迁移学习技术可以在准确性、运行时间和延迟方面胜过当前最先进的表格系统。

    arXiv:2311.02971v2 Announce Type: replace-cross  Abstract: We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
    
[^185]: 一次性策略分类在未知成本下的研究

    One-Shot Strategic Classification Under Unknown Costs

    [https://arxiv.org/abs/2311.02761](https://arxiv.org/abs/2311.02761)

    本研究首次研究了在未知响应下一次性策略分类的情景，针对用户成本函数不确定性，提出解决方案并将任务定义为极小-极大问题。

    

    策略分类的目标是学习对策略输入操纵具有鲁棒性的决策规则。之前的研究假设这些响应是已知的；而最近的一些研究处理未知响应，但它们专门研究重复模型部署的在线设置。然而，在许多领域，特别是在公共政策中，一个常见的激励用例中，多次部署是不可行的，甚至一个糟糕的轮次都是不可接受的。为了填补这一空白，我们首次引入了在未知响应下的一次性策略分类的正式研究，这需要在一次性选择一个分类器。着重关注用户成本函数中的不确定性，我们首先证明对于一类广泛的成本，即使对真实成本的小误差也可能在最坏情况下导致准确性降至极低水平。鉴于此，我们将任务框定为极小-极大问题，目标是识别

    arXiv:2311.02761v2 Announce Type: replace  Abstract: The goal of strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that these responses are known; while some recent works handle unknown responses, they exclusively study online settings with repeated model deployments. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use case$\unicode{x2014}$where multiple deployments are infeasible, or where even one bad round is unacceptable. To address this gap, we initiate the formal study of one-shot strategic classification under unknown responses, which requires committing to a single classifier once. Focusing on uncertainty in the users' cost function, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail trivial accuracy in the worst case. In light of this, we frame the task as a minimax problem, with the goal of identifying
    
[^186]: 带有Q-Learning的指针网络用于OP组合优化

    Pointer Networks with Q-Learning for OP Combinatorial Optimization

    [https://arxiv.org/abs/2311.02629](https://arxiv.org/abs/2311.02629)

    提出了Pointer Q-Network (PQN)方法，将Ptr-Nets和Q-learning相结合，利用其批评者性质，出色地捕获了嵌入图中的关系，从而有效解决了OP组合优化中的具体挑战

    

    Orienteering Problem（OP）在组合优化（CO）中提出了独特挑战，突显了其在物流、交付和运输规划中的广泛应用。由于OP的NP-hard性质，获得最优解本质上是复杂的。尽管指针网络（Ptr-Nets）在各种组合任务中表现出色，但它们在OP上的表现以及需要专注于未来回报或探索的任务方面仍有改进空间。认识到将强化学习（RL）方法与序列-序列模型相结合的潜能，本研究揭示了指针Q网络（PQN）。该方法结合了Ptr-Nets和Q-learning，由于其仅具批评者性质，它在捕获嵌入图中的关系方面表现出色，这是有效应对OP提出的具体挑战的基本要求。我们探讨了架构和功能

    arXiv:2311.02629v2 Announce Type: replace  Abstract: The Orienteering Problem (OP) presents a unique challenge in Combinatorial Optimization (CO), emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP, and duties requiring focus on future return or exploration, leaves room for improvement. Recognizing the potency combining Reinforcement Learning (RL) methods with sequence-to-sequence models, this research unveils the Pointer Q-Network (PQN). This method combines Ptr-Nets and Q-learning, which, thanks to its critic only nature, outstands in its capability of capturing relationships within an embedded graph, a fundamental requirement in order to effectively address the specific challenges presented by OP. We explore the architecture and functionalit
    
[^187]: 可微分欧拉特征变换用于形状分类

    Differentiable Euler Characteristic Transforms for Shape Classification

    [https://arxiv.org/abs/2310.07630](https://arxiv.org/abs/2310.07630)

    提出了一种不同iable Euler Characteristic Transform（DECT）计算层，能够实现端到端学习ECT，展现出与更复杂模型相当的性能。

    

    欧拉特征变换（ECT）已被证明是一种强大的表示方法，结合了形状和图形的几何和拓扑特征。然而，迄今为止，ECT无法学习特定任务的表示。我们克服了这一问题，并开发了一种新颖的计算层，可以使ECT以端到端的方式进行学习。我们的方法，Differentiable Euler Characteristic Transform（DECT），速度快，计算高效，同时在图形和点云分类任务中表现出与更复杂模型相当的性能。此外，我们展示了这种看似简单的统计量提供了与更复杂的拓扑深度学习层相同的拓扑表达能力。

    arXiv:2310.07630v2 Announce Type: replace  Abstract: The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion. Our method, the Differentiable Euler Characteristic Transform (DECT), is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks. Moreover, we show that this seemingly simple statistic provides the same topological expressivity as more complex topological deep learning layers.
    
[^188]: 光谢尔宾格桥

    Light Schr\"odinger Bridge

    [https://arxiv.org/abs/2310.01174](https://arxiv.org/abs/2310.01174)

    提出了一种新颖的光谢尔宾格桥快速简单求解器，将谢尔宾格势能参数化为总和-指数二次函数，视为能量函数，实现了轻量级、无需模拟、理论上合理的求解器设计。

    

    尽管计算谢尔宾格桥（SB）领域取得了最新进展，但大多数现有的SB求解器仍然过重，并需要对多个神经网络进行复杂优化。事实证明，目前不存在主要的求解器可以像聚类中的$k$-means方法、分类中的逻辑回归或离散最优输运中的Sinkhorn算法那样起到简单而有效的基准作用。我们解决了这个问题，提出了一种新颖的快速简单的SB求解器。我们的方法是最近出现在该领域的两个观点的巧妙结合：（a）使用总和-指数二次函数对谢尔宾格势能进行参数化，（b）将对数谢尔宾格势能视为能量函数。我们展示了将这些观点结合起来，可以产生一个轻量级、无需模拟且在理论上证明合理的SB求解器，具有简单直接的优化目标。因此，它是

    arXiv:2310.01174v2 Announce Type: replace  Abstract: Despite the recent advances in the field of computational Schrodinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., $k$-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schrodinger potentials with sum-exp quadratic functions and (b) viewing the log-Schrodinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it a
    
[^189]: 语言建模即为压缩

    Language Modeling Is Compression

    [https://arxiv.org/abs/2309.10668](https://arxiv.org/abs/2309.10668)

    大型语言模型被证明是强大的压缩器，压缩视角为扩展定律、标记化和上下文学习提供了新的见解。

    

    早已确立了预测模型可以转化为无损压缩器，反之亦然。近年来，机器学习社区集中精力训练越来越大、越来越强大的自监督（语言）模型。大型语言模型表现出令人印象深刻的预测能力，因此它们有望成为强大的压缩器。在这项工作中，我们主张通过压缩的视角来看待预测问题，并评估大型（基础）模型的压缩能力。我们展示了大型语言模型是强大的通用预测器，压缩视角提供了有关扩展定律、标记化和上下文学习的新见解。例如，Chinchilla 70B，虽然主要在文本上训练，但可以将ImageNet的补丁压缩为其原始大小的43.4%，将LibriSpeech样本压缩为其原始大小的16.4%，超越了特定领域的压缩器。

    arXiv:2309.10668v2 Announce Type: replace-cross  Abstract: It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors li
    
[^190]: EasyEdit：一种易于使用的大型语言模型知识编辑框架

    EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models

    [https://arxiv.org/abs/2308.07269](https://arxiv.org/abs/2308.07269)

    EasyEdit提出了一种易于使用的知识编辑框架，针对大型语言模型的知识截断或谬误问题，支持各种最新的知识编辑方法，并可应用于多个知名的LLMs。

    

    大型语言模型（LLMs）通常遭受知识截断或谬误问题，这意味着它们对未见事件不知情或生成具有不正确事实的文本，原因是数据过时/嘈杂。为此，出现了许多针对LLMs的知识编辑方法，旨在微妙地注入/编辑更新的知识或调整不良行为，同时将对不相关输入的影响最小化。然而，由于各种知识编辑方法之间存在显著差异，以及任务设置中的变化，社区中没有可用于知识编辑的标准实施框架，这妨碍了从业者将知识编辑应用于应用程序。为解决这些问题，我们提出了EasyEdit，一种易于使用的LLMs知识编辑框架。它支持各种尖端的知识编辑方法，并可以轻松应用于许多著名的LLMs，如T5、GPT-J、LlaMA等。从经验上来看，我们报告了kno

    arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
    
[^191]: 通过机器学习在复杂系统中进行信息分解

    Information decomposition in complex systems via machine learning

    [https://arxiv.org/abs/2307.04755](https://arxiv.org/abs/2307.04755)

    通过机器学习提出了一种实用且通用的方法，使用分布式信息瓶颈作为学习目标，来进行复杂系统中信息的分解

    

    向理解复杂系统迈出的基本步骤之一是识别对宏观尺度行为最相关的系统组件尺度上的变化。互信息提供了将系统各个尺度上的变化联系起来的自然手段，因为它独立于可观测量之间的功能关系。然而，描述信息如何分布在一组可观测量之间的方式在计算上具有挑战性，并且通常除了少数几次测量外是不可行的。本文提出了一种实用且通用的方法，利用机器学习来分解一组测量中包含的信息，通过联合优化每个测量的有损压缩。在分布式信息瓶颈作为学习目标的引导下，信息分解识别了与指定的宏观尺度行为最相关的系统状态测量中的变化。

    arXiv:2307.04755v2 Announce Type: replace  Abstract: One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale. Mutual information provides a natural means of linking variation across scales of a system due to its independence of functional relationship between observables. However, characterizing the manner in which information is distributed across a set of observables is computationally challenging and generally infeasible beyond a handful of measurements. Here we propose a practical and general methodology that uses machine learning to decompose the information contained in a set of measurements by jointly optimizing a lossy compression of each measurement. Guided by the distributed information bottleneck as a learning objective, the information decomposition identifies the variation in the measurements of the system state most relevant to specified mac
    
[^192]: 两全其美：混合SNN-ANN架构用于基于事件的光流估计

    Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation

    [https://arxiv.org/abs/2306.02960](https://arxiv.org/abs/2306.02960)

    提出了一种新颖的SNN-ANN混合架构，用于克服SNN在处理事件数据方面面临的挑战，以实现事件驱动的光流估计。

    

    在机器人领域，事件相机正在成为传统基于帧的相机的低功耗替代品，用于捕捉高速运动和高动态范围场景。这是由于它们的稀疏和异步事件输出。尖峰神经网络（SNNs）以其异步事件驱动的计算，显示出从这些事件流中提取时空特征的巨大潜力。相比之下，标准模拟神经网络（ANNs）未能有效处理事件数据。然而，由于具有额外的可训练参数（阈值和泄漏）、在更深层次上消失的尖峰以及不可微分的二值激活函数，训练SNNs是困难的。此外，在SNNs中，负责跟踪时间信息的额外数据结构——膜电位，必须在每个时间步获取和更新。为了克服这些挑战，我们提出了一种新颖的SNN-ANN混合

    arXiv:2306.02960v2 Announce Type: replace-cross  Abstract: In the field of robotics, event-based cameras are emerging as a promising low-power alternative to traditional frame-based cameras for capturing high-speed motion and high dynamic range scenes. This is due to their sparse and asynchronous event outputs. Spiking Neural Networks (SNNs) with their asynchronous event-driven compute, show great potential for extracting the spatio-temporal features from these event streams. In contrast, the standard Analog Neural Networks (ANNs) fail to process event data effectively. However, training SNNs is difficult due to additional trainable parameters (thresholds and leaks), vanishing spikes at deeper layers, and a non-differentiable binary activation function. Furthermore, an additional data structure, membrane potential, responsible for keeping track of temporal information, must be fetched and updated at every timestep in SNNs. To overcome these challenges, we propose a novel SNN-ANN hybrid
    
[^193]: 在无信号交叉口预测行人互动结果：穿越或等待？

    Cross or Wait? Predicting Pedestrian Interaction Outcomes at Unsignalized Crossings

    [https://arxiv.org/abs/2304.08260](https://arxiv.org/abs/2304.08260)

    本文利用机器学习预测行人在无信号交叉口与车辆互动时的过马路行为，提出的神经网络模型在预测准确性和F1分数上取得了显著的改进。

    

    预测行人与车辆互动时的行为是自动驾驶领域中最重要的挑战之一。行人过马路的行为受到各种互动因素的影响，包括到达时间、行人等待时间、斑马线的存在，以及行人和驾驶员的特性和个性特征。然而，这些因素尚未被充分探讨用于预测互动结果。本文利用机器学习来预测行人在无信号交叉口与车辆互动时的过马路行为，包括行人的过马路决策、过街开始时间（CIT）和过街持续时间（CD）。分布式模拟数据被用于预测和分析这些互动因素。与逻辑回归基线模型相比，我们提出的神经网络模型将预测的准确性和F1分数提高了4.46%。

    arXiv:2304.08260v2 Announce Type: replace-cross  Abstract: Predicting pedestrian behavior when interacting with vehicles is one of the most critical challenges in the field of automated driving. Pedestrian crossing behavior is influenced by various interaction factors, including time to arrival, pedestrian waiting time, the presence of zebra crossing, and the properties and personality traits of both pedestrians and drivers. However, these factors have not been fully explored for use in predicting interaction outcomes. In this paper, we use machine learning to predict pedestrian crossing behavior including pedestrian crossing decision, crossing initiation time (CIT), and crossing duration (CD) when interacting with vehicles at unsignalized crossings. Distributed simulator data are utilized for predicting and analyzing the interaction factors. Compared with the logistic regression baseline model, our proposed neural network model improves the prediction accuracy and F1 score by 4.46% an
    
[^194]: 针对分段连续决策制定的Oracle高效平滑在线学习

    Oracle-Efficient Smoothed Online Learning for Piecewise Continuous Decision Making

    [https://arxiv.org/abs/2302.05430](https://arxiv.org/abs/2302.05430)

    引入广义括号数的概念，结合对手的约束与空间大小，通过Follow-the-Perturbed-Leader算法实现低遗憾，优化调用优化Oracle的次数以实现遗憾在多个问题中的有效应用。

    

    平滑在线学习已经成为一种流行的框架，可以缓解在从经典学习转向对抗性学习时产生的统计和计算复杂性的显著损失。本文引入了一个新的复杂性概念，即广义括号数，将对手的约束与空间大小相结合，并展示了一个Follow-the-Perturbed-Leader实例可以实现低遗憾，并且优化调用优化Oracle的次数以实现平均遗憾的最佳缩放。

    arXiv:2302.05430v2 Announce Type: replace-cross  Abstract: Smoothed online learning has emerged as a popular framework to mitigate the substantial loss in statistical and computational complexity that arises when one moves from classical to adversarial learning. Unfortunately, for some spaces, it has been shown that efficient algorithms suffer an exponentially worse regret than that which is minimax optimal, even when the learner has access to an optimization oracle over the space. To mitigate that exponential dependence, this work introduces a new notion of complexity, the generalized bracketing numbers, which marries constraints on the adversary to the size of the space, and shows that an instantiation of Follow-the-Perturbed-Leader can attain low regret with the number of calls to the optimization oracle scaling optimally with respect to average regret. We then instantiate our bounds in several problems of interest, including online prediction and planning of piecewise continuous fu
    
[^195]: 面向解释神经代码模型的因果论理论

    Toward a Theory of Causation for Interpreting Neural Code Models

    [https://arxiv.org/abs/2302.03788](https://arxiv.org/abs/2302.03788)

    该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。

    

    Neural Language Models of Code，或者称为神经代码模型（NCMs），正在迅速从研究原型发展为商业开发者工具。因此，理解这些模型的能力和局限性变得至关重要。然而，这些模型的能力通常是使用自动化指标来衡量的，这些指标通常只能揭示它们真实性能的一部分。一般来说，NCMs的性能似乎很有前途，但目前关于这些模型如何做出决策仍有很多未知。因此，本文介绍了一种名为$do_{code}$的后验解释方法，该方法专门针对NCMs，能够解释模型的预测。$do_{code}$基于因果推断，以实现面向编程语言的解释。虽然$do_{code}$的理论基础可扩展到探索不同的模型属性，但我们提供了一个具体的实例，旨在减少影响...

    arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
    
[^196]: 平滑在线学习用于分段仿射系统中的预测

    Smoothed Online Learning for Prediction in Piecewise Affine Systems

    [https://arxiv.org/abs/2301.11187](https://arxiv.org/abs/2301.11187)

    本文提出了基于平滑在线学习框架的算法，可以有效处理分段仿射系统中的预测和模拟问题，在弱光滑性假设下具有多项式遗憾度，并且在调用优化预测次数方面是高效的。

    

    分段仿射（PWA）回归和规划问题对于在线学习、控制和机器人学的研究具有基础重要性，它为研究系统动态发生急剧变化提供了一个理论上和实证上可处理的设置。然而，由于穿越不同“片段”时出现的不连续性，学习在一般的顺序设置中是不可能的，实际算法被迫采用启发式方法。本文在最近开发的平滑在线学习框架基础上构建，并提供了第一个在弱光滑性假设下，具有多项式遗憾度并且在优化预测中高效的PWA系统预测和模拟算法；此外，我们将我们的结果应用到一步预测和多步模拟遗憾问题中。

    arXiv:2301.11187v2 Announce Type: replace-cross  Abstract: The problem of piecewise affine (PWA) regression and planning is of foundational importance to the study of online learning, control, and robotics, where it provides a theoretically and empirically tractable setting to study systems undergoing sharp changes in the dynamics. Unfortunately, due to the discontinuities that arise when crossing into different ``pieces,'' learning in general sequential settings is impossible and practical algorithms are forced to resort to heuristic approaches. This paper builds on the recently developed smoothed online learning framework and provides the first algorithms for prediction and simulation in PWA systems whose regret is polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our algorithms are efficient in the number of calls to an optimization oracle. We further apply our results to the problems of one-step prediction and multi-step simulation regret i
    
[^197]: 当层次抽奖时，所有票据在初始化时都获胜

    When Layers Play the Lottery, all Tickets Win at Initialization

    [https://arxiv.org/abs/2301.10835](https://arxiv.org/abs/2301.10835)

    本文从层次剪枝的角度研究了LTH和初始化时的剪枝，在初始化阶段发现了获胜彩票的存在，从而消除了训练稠密网络的大量计算资源需求。

    

    剪枝是减少深度网络计算成本的标准技术。许多剪枝中的进展利用了“彩票假设”（LTH）的概念。LTH揭示了在经过训练的稠密网络内部存在着能够达到类似准确度（即赢得彩票 - 获胜票）的稀疏子网络（票）。在初始化时进行剪枝注重于找到获胜票，而无需训练稠密网络。关于这些概念的研究表明子网络来自权重或滤波器剪枝的趋势。在这项工作中，我们从层次剪枝的视角研究了LTH和初始化时的剪枝。首先，我们确认了当剪枝过程移除层时获胜票的存在。借助这一观察结果，我们提出在初始化时发现这些获胜票，消除了训练初始（过参数化）稠密网络所需的大量计算资源的要求。大量实验...

    arXiv:2301.10835v2 Announce Type: replace  Abstract: Pruning is a standard technique for reducing the computational cost of deep networks. Many advances in pruning leverage concepts from the Lottery Ticket Hypothesis (LTH). LTH reveals that inside a trained dense network exists sparse subnetworks (tickets) able to achieve similar accuracy (i.e., win the lottery - winning tickets). Pruning at initialization focuses on finding winning tickets without training a dense network. Studies on these concepts share the trend that subnetworks come from weight or filter pruning. In this work, we investigate LTH and pruning at initialization from the lens of layer pruning. First, we confirm the existence of winning tickets when the pruning process removes layers. Leveraged by this observation, we propose to discover these winning tickets at initialization, eliminating the requirement of heavy computational resources for training the initial (over-parameterized) dense network. Extensive experiments 
    
[^198]: 通过长短期记忆和TOPSIS进行深度循环学习

    Deep Recurrent Learning Through Long Short Term Memory and TOPSIS

    [https://arxiv.org/abs/2301.00693](https://arxiv.org/abs/2301.00693)

    该论文提出了一种基于长短期记忆（LSTM）和TOPSIS的分类算法，用于识别和排名云ERP的采用特征。

    

    企业资源规划（ERP）软件将资源、数据汇聚在一起，以保持企业流程中的软件流畅。然而，云计算的廉价、简便和快速管理承诺促使企业所有者将单一体架构转变为基于数据中心/云的ERP。由于云ERP的开发涉及一个循环过程，即规划、实施、测试和升级，其采用被认为是一个深度递归神经网络问题。最终，提出了一个基于长短期记忆（LSTM）和TOPSIS的分类算法，用于识别和分别排名采用特征。我们的理论模型通过阐述关键参与者、服务、架构、功能，在参考模型上得到了验证。通过考虑技术、创新和抵抗问题，开展了一项定性调查，以就关键采用因素提出假设。

    arXiv:2301.00693v2 Announce Type: replace-cross  Abstract: Enterprise resource planning (ERP) software brings resources, data together to keep software-flow within business processes in a company. However, cloud computing's cheap, easy and quick management promise pushes business-owners for a transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP development involves a cyclic process, namely planning, implementing, testing and upgrading, its adoption is realized as a deep recurrent neural network problem. Eventually, a classification algorithm based on long short term memory (LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption features. Our theoretical model is validated over a reference model by articulating key players, services, architecture, functionalities. Qualitative survey is conducted among users by considering technology, innovation and resistance issues, to formulate hypotheses on key adoption factors.
    
[^199]: Behave-XAI：行为表征数据的深度可解释学习

    Behave-XAI: Deep Explainable Learning of Behavioral Representational Data

    [https://arxiv.org/abs/2301.00016](https://arxiv.org/abs/2301.00016)

    本研究提出了Behave-XAI，通过深度卷积神经网络架构解决行为挖掘问题，并应用递归神经网络处理用户生理时间序列数据，实现了行为表征数据的深度可解释学习。

    

    根据人工智能的最新趋势，AI系统需要就其提供的一般性和具体性决策、服务进行澄清。只有消费者对解释感到满意，例如为什么任何分类结果是任何给定时间的结果。这实际上激励我们在行为挖掘场景中使用可解释或人类可理解的AI，其中用户在数字平台上的参与度是从上下文（如情绪、活动、天气等）中确定的。然而，AI系统的输出并不总是系统上正确的，通常是系统上正确的，但明显不完美，从而造成困惑，例如，为什么做出这个决定？背后的原因是什么？在这种情况下，我们首先在深度卷积神经网络架构中制定了行为挖掘问题。最终，我们应用了递归神经网络，因为存在来自用户生理时间序列数据的情况。

    arXiv:2301.00016v2 Announce Type: replace  Abstract: According to the latest trend of artificial intelligence, AI-systems needs to clarify regarding general,specific decisions,services provided by it. Only consumer is satisfied, with explanation , for example, why any classification result is the outcome of any given time. This actually motivates us using explainable or human understandable AI for a behavioral mining scenario, where users engagement on digital platform is determined from context, such as emotion, activity, weather, etc. However, the output of AI-system is not always systematically correct, and often systematically correct, but apparently not-perfect and thereby creating confusions, such as, why the decision is given? What is the reason underneath? In this context, we first formulate the behavioral mining problem in deep convolutional neural network architecture. Eventually, we apply a recursive neural network due to the presence of time-series data from users physiolog
    
[^200]: 具有定制隐私保护的社交感知聚类联邦学习

    Social-Aware Clustered Federated Learning with Customized Privacy Preservation

    [https://arxiv.org/abs/2212.13992](https://arxiv.org/abs/2212.13992)

    通过利用用户之间的社交关系，提出了SCFL，一种具有定制隐私保护的社交感知聚类联邦学习方案，实现了数据隐私和效率之间的平衡。

    

    联邦学习（FL）的一个关键特性是保护端用户的数据隐私。然而，在FL中仍然存在通过交换梯度可能导致的潜在隐私泄漏。因此，最近的研究通常探讨微分隐私（DP）方法，通过向计算结果添加噪声来解决隐私问题，并具有较低的开销，但这些方法会降低模型性能。本文通过利用用户之间的普遍社交连接，平衡了数据隐私和效率。具体来说，我们提出了一种新颖的社交感知聚类联邦学习方案SCFL，其中相互信任的个体可以自由组成一个社交集群，并在每个集群内聚合他们的原始模型更新（例如梯度），然后上传到云端进行全局聚合。通过在社交群体中混合模型更新，对手只能窃听社交层组合的结果，而无法窃听到个体隐私。

    arXiv:2212.13992v2 Announce Type: replace-cross  Abstract: A key feature of federated learning (FL) is to preserve the data privacy of end users. However, there still exist potential privacy leakage in exchanging gradients under FL. As a result, recent research often explores the differential privacy (DP) approaches to add noises to the computing results to address privacy concerns with low overheads, which however degrade the model performance. In this paper, we strike the balance of data privacy and efficiency by utilizing the pervasive social connections between users. Specifically, we propose SCFL, a novel Social-aware Clustered Federated Learning scheme, where mutually trusted individuals can freely form a social cluster and aggregate their raw model updates (e.g., gradients) inside each cluster before uploading to the cloud for global aggregation. By mixing model updates in a social group, adversaries can only eavesdrop the social-layer combined results, but not the privacy of in
    
[^201]: 无相干空中分布式梯度下降

    Non-Coherent Over-the-Air Decentralized Gradient Descent

    [https://arxiv.org/abs/2211.10777](https://arxiv.org/abs/2211.10777)

    提出了一种适用于无线系统的DGD算法，通过无相干空中共识方案实现无需智能体协调、拓扑信息或信道状态信息的分布式优化。

    

    分布式梯度下降（DGD）是一种流行的算法，用于解决诸如远程感知、分布式推断、多智能体协调和联邦学习等各种领域的分布式优化问题。然而，在受到噪声、衰落和带宽受限的无线系统上执行DGD会带来挑战，需要调度传输以减轻干扰，并获取拓扑和信道状态信息，这在无线分布式系统中是复杂的任务。本文提出了一种专为无线系统定制的DGD算法。与现有方法不同，它在无需进行智能体协调、拓扑信息或信道状态信息的情况下运行。其核心是一种无相干空中（NCOTA）共识方案，利用了无线信道的噪声能量叠加特性。通过随机化传输策略来适应半双工操作，发射机将位置映射到

    arXiv:2211.10777v2 Announce Type: replace-cross  Abstract: Decentralized Gradient Descent (DGD) is a popular algorithm used to solve decentralized optimization problems in diverse domains such as remote sensing, distributed inference, multi-agent coordination, and federated learning. Yet, executing DGD over wireless systems affected by noise, fading and limited bandwidth presents challenges, requiring scheduling of transmissions to mitigate interference and the acquisition of topology and channel state information -- complex tasks in wireless decentralized systems. This paper proposes a DGD algorithm tailored to wireless systems. Unlike existing approaches, it operates without inter-agent coordination, topology information, or channel state information. Its core is a Non-Coherent Over-The-Air (NCOTA) consensus scheme, exploiting a noisy energy superposition property of wireless channels. With a randomized transmission strategy to accommodate half-duplex operation, transmitters map loca
    
[^202]: 基于MKTFHE的更安全和更快速的隐私保护分布式机器学习

    SFPDML: Securer and Faster Privacy-Preserving Distributed Machine Learning based on MKTFHE

    [https://arxiv.org/abs/2211.09353](https://arxiv.org/abs/2211.09353)

    基于MKTFHE，本文提出了一种更安全和更高效的隐私保护分布式机器学习方法，通过引入秘密分享和设计新的激活函数，解决了现有解密协议的安全风险和非线性计算问题。

    

    在近年来，分布式机器学习受到了极大关注。然而，隐私保护仍然是这一领域尚未解决的问题。多密钥同态加密（MKTFHE）是解决这一问题的有希望的候选方案之一。然而，在MKTFHE的解密过程中可能存在安全风险。此外，据我们所知，关于MKTFHE的最新工作仅支持布尔运算和线性运算，无法直接计算Sigmoid等非线性函数。因此，仍然很难在高性能下执行逻辑回归和神经网络等常见机器学习任务。本文首先发现了针对现有MKTFHE分布式解密协议的可能攻击，随后引入了秘密分享以提出更安全的解密协议。接下来，我们通过\emph{同态化器}和\emph{比较四重子}设计了一个新的MKTFHE友好的激活函数。

    arXiv:2211.09353v2 Announce Type: replace-cross  Abstract: In recent years, distributed machine learning has garnered significant attention. However, privacy continues to be an unresolved issue within this field. Multi-key homomorphic encryption over torus (MKTFHE) is one of the promising candidates for addressing this concern. Nevertheless, there may be security risks in the decryption of MKTFHE. Moreover, to our best known, the latest works about MKTFHE only support Boolean operation and linear operation which cannot directly compute the non-linear function like Sigmoid. Therefore, it is still hard to perform common machine learning such as logistic regression and neural networks in high performance. In this paper, we first discover a possible attack on the existing distributed decryption protocol for MKTFHE and subsequently introduce secret sharing to propose a securer one. Next, we design a new MKTFHE-friendly activation function via \emph{homogenizer} and \emph{compare quads}. Fin
    
[^203]: 缓解多目标学习中的梯度偏差：一种可证明收敛的随机方法

    Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach

    [https://arxiv.org/abs/2210.12624](https://arxiv.org/abs/2210.12624)

    提出了一种随机多目标梯度校正（MoCo）方法，能够在不增加批量大小的情况下保证收敛，解决了多目标学习中梯度偏差导致性能下降的问题。

    

    具有多个目标函数的机器学习问题通常出现在需要在多个性能指标（如公平性，安全性和准确性）之间进行权衡的多目标学习中；或者在多任务学习中，多个任务联合优化，共享它们之间的归纳偏差。然而，现有的随机多目标梯度方法及其变体（例如，MGDA，PCGrad，CAGrad等）都采用带偏差的噪声梯度方向，导致经验性能下降。为此，我们开发了一种用于多目标优化的随机多目标梯度校正（MoCo）方法。我们方法的独特之处在于，即使在非凸设置中也能保证收敛而不增加批量大小。对多任务监督学习和强化学习进行了模拟实验。

    arXiv:2210.12624v2 Announce Type: replace  Abstract: Machine learning problems with multiple objective functions appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic Multi-objective gradient Correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the non-convex setting. Simulations on multi-task supervised and reinforcement learning demonstra
    
[^204]: Riemannian随机梯度方法用于嵌套组合优化

    Riemannian Stochastic Gradient Method for Nested Composition Optimization

    [https://arxiv.org/abs/2207.09350](https://arxiv.org/abs/2207.09350)

    提出了一种适用于嵌套函数组合的Riemannian随机梯度优化方法，可在有限次调用随机梯度oracle找到近似稳定点。

    

    本文考虑了在Riemann流形上针对嵌套形式中包含期望的函数组合的优化问题，这类问题在强化学习中的策略评估或元学习中的模型定制等应用中越来越受欢迎。我们提出了一种Riemann随机组合梯度下降（R-SCGD）方法，用于两级组合优化，在$O(\epsilon^{-2})$次对外部函数的随机梯度oracle以及内部函数的随机函数和梯度oracles的调用中寻找期望平方Riemann梯度小于$\epsilon$的近似稳定点。

    arXiv:2207.09350v2 Announce Type: replace-cross  Abstract: This work considers optimization of composition of functions in a nested form over Riemannian manifolds where each function contains an expectation. This type of problems is gaining popularity in applications such as policy evaluation in reinforcement learning or model customization in meta-learning. The standard Riemannian stochastic gradient methods for non-compositional optimization cannot be directly applied as stochastic approximation of inner functions create bias in the gradients of the outer functions. For two-level composition optimization, we present a Riemannian Stochastic Composition Gradient Descent (R-SCGD) method that finds an approximate stationary point, with expected squared Riemannian gradient smaller than $\epsilon$, in $O(\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer function and stochastic function and gradient oracles of the inner function. Furthermore, we generalize the R-SCGD algo
    
[^205]: 实现超低延迟下损失less ANN-SNN转换的双相优化

    Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization

    [https://arxiv.org/abs/2205.07473](https://arxiv.org/abs/2205.07473)

    本研究提出了一种双阶段转换算法，能够最小化量化误差、剪切误差和残余膜电位表示误差，实现了损失less的ANN-SNN转换，提高了在超低延迟条件下的性能。

    

    神经脉冲网络 (SNNs) 以异步离散事件操作，显示出更高的能耗效率和稀疏计算。实现深度 SNNs 的一种常见方法是通过结合 ANN-SNN 转换来同时实现 ANN 的高效训练和 SNN 的高效推断。然而，精度损失通常是不可忽视的，尤其是在少量时间步下，这大大限制了 SNN 在延迟敏感的边缘设备上的应用。本文首先确定这种性能下降源自于 SNNs 中负面或溢出残余膜电位的错误表示。受此启发，我们将转换误差分解为三部分：量化误差、剪切误差和残余膜电位表示误差。基于这些洞察，我们提出了一个两阶段转换算法，分别最小化这些误差。此外，我们展示每个阶段都取得了显著的性能增益。

    arXiv:2205.07473v3 Announce Type: replace-cross  Abstract: Spiking neural networks (SNNs) operating with asynchronous discrete events show higher energy efficiency with sparse computation. A popular approach for implementing deep SNNs is ANN-SNN conversion combining both efficient training of ANNs and efficient inference of SNNs. However, the accuracy loss is usually non-negligible, especially under a few time steps, which restricts the applications of SNN on latency-sensitive edge devices greatly. In this paper, we first identify that such performance degradation stems from the misrepresentation of the negative or overflow residual membrane potential in SNNs. Inspired by this, we decompose the conversion error into three parts: quantization error, clipping error, and residual membrane potential representation error. With such insights, we propose a two-stage conversion algorithm to minimize those errors respectively. Besides, We show each stage achieves significant performance gains i
    
[^206]: 贝叶斯非参数方法与数据驱动鲁棒优化的结合

    Bayesian Nonparametrics meets Data-Driven Robust Optimization. (arXiv:2401.15771v1 [stat.ML])

    [http://arxiv.org/abs/2401.15771](http://arxiv.org/abs/2401.15771)

    本文提出了一种将贝叶斯非参数方法与最新的决策理论模型相结合的鲁棒优化准则，通过这种方法，可以在线性回归问题中获得有稳定性和优越性能的结果。

    

    训练机器学习和统计模型通常涉及优化数据驱动的风险准则。风险通常是根据经验数据分布计算的，但由于分布不确定性，这可能导致性能不稳定和不好的样本外表现。在分布鲁棒优化的精神下，我们提出了一个新颖的鲁棒准则，将贝叶斯非参数（即狄利克雷过程）理论和最近的平滑模糊规避偏好的决策理论模型的见解相结合。首先，我们强调了与标准正则化经验风险最小化技术的新连接，其中包括岭回归和套索回归。然后，我们从理论上证明了鲁棒优化过程在有限样本和渐近统计保证方面的有利性存在。对于实际实施，我们提出并研究了基于众所周知的狄利克雷过程表示的可行近似准则。

    Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet Process representations. 
    
[^207]: 分而不忘：连续学习中选择性训练专家的集成方法

    Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])

    [http://arxiv.org/abs/2401.10191](http://arxiv.org/abs/2401.10191)

    连续学习中，我们提出了一种名为SEED的新方法，通过选择性训练最优的专家来解决遗忘和计算负担的问题，并在实验中展示了其高性能。

    

    随着类增量学习的流行，模型能够拓宽应用范围，同时不忘记已经学到的知识。在这个领域中的一个趋势是使用混合专家技术，不同的模型共同解决任务。然而，这些专家通常会一次性使用整个任务的数据进行训练，这样会增加遗忘的风险和计算负担。为了解决这个问题，我们提出了一种名为SEED的新方法。SEED仅选择一个被认为最优的专家来处理给定的任务，并使用该任务的数据对这个专家进行微调。为此，每个专家用高斯分布表示每个类别，并根据这些分布的相似性选择最优专家。因此，SEED在保持集成方法的高稳定性的同时增加了专家之间的多样性和异质性。大量实验证明SEED达到了最先进的性能。

    Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performan
    
[^208]: 通过克里洛夫子空间回收加速神经算子的数据生成

    Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])

    [http://arxiv.org/abs/2401.09516](http://arxiv.org/abs/2401.09516)

    该论文提出了一种名为排序克里洛夫回收（SKR）的新方法，用于加速神经算子训练的数据生成。该方法解决了现有方法在解决PDE问题时计算冗余的问题，显著提高了数据生成效率。

    

    学习用于解决偏微分方程(PDE)的神经算子因其高推理效率而受到广泛关注。然而，训练这些算子需要生成大量带有解决方案的标记数据，即PDE问题及其解决方案。数据生成过程非常耗时，因为它涉及解决大量线性方程组以获得PDE的数值解。许多现有方法独立地解决这些系统，而不考虑它们的内在相似性，导致计算极其冗余。为了解决这个问题，我们提出了一种新颖的方法，即排序克里洛夫回收(SKR)，以提高解决这些系统的效率，从而显著加速神经算子训练的数据生成。据我们所知，SKR是第一个解决学习神经算子数据生成耗时性质的尝试。SKR的核心是克里洛夫子空间。

    Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
    
[^209]: 评估用于AI辅助图像标注的符合预测集的效用

    Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])

    [http://arxiv.org/abs/2401.08876](http://arxiv.org/abs/2401.08876)

    本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。

    

    随着深度神经网络在高风险领域中越来越常见，它们的缺乏可解释性使得不确定性量化变得具有挑战性。我们研究了用于表示AI辅助决策中的不确定性的符合预测集的效果。通过一项大型预注册实验，我们比较了符合预测集和显示Top-1和Top-k预测在AI辅助图像标注中的效用。我们发现，对于简单的图像，预测集的准确性与Top-1和Top-k显示相当或稍低，但在标记分布外（OOD）图像时，尤其是当集合大小较小时，预测集在辅助人类标注方面表现出色。我们的结果在实践中强调了符合预测集的实际挑战，并提供了相关建议。

    As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
    
[^210]: 连续时间序列集合的概率建模

    Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15045](http://arxiv.org/abs/2312.15045)

    本文提出了一个通用的连续时间序列集合的概率建模框架，适用于处理每个事件与一组项目相关联的情况。引入了适用于任何强度为基础的递归神经点过程模型的推理方法，可用于回答关于序列历史条件下的概率查询问题。

    

    在连续时间事件数据的统计参数模型工具箱中，神经标记时间点过程是一个有价值的补充。这些模型适用于每个事件与单个项目（单个事件类型或“标记”）相关联的序列，但不适用于每个事件与一组项目相关联的实际情况。本文中，我们开发了一个通用的连续时间集合数值数据建模框架，与任何基于强度的递归神经点过程模型兼容。此外，我们还开发了推理方法，可使用这些模型回答诸如“在考虑序列历史的条件下，项目A在项目B之前观察到的概率”等概率查询问题。由于问题设置的连续时间性质和每个事件的潜在结果空间的组合极大，对于神经模型来说，计算这些查询的精确答案通常是不可行的。

    Neural marked temporal point processes have been a valuable addition to the existing toolbox of statistical parametric models for continuous-time event data. These models are useful for sequences where each event is associated with a single item (a single type of event or a "mark") -- but such models are not suited for the practical situation where each event is associated with a set of items. In this work, we develop a general framework for modeling set-valued data in continuous-time, compatible with any intensity-based recurrent neural point process model. In addition, we develop inference methods that can use such models to answer probabilistic queries such as "the probability of item $A$ being observed before item $B$," conditioned on sequence history. Computing exact answers for such queries is generally intractable for neural models due to both the continuous-time nature of the problem setting and the combinatorially-large space of potential outcomes for each event. To address th
    
[^211]: 基于图神经网络的快速芯片库特征化用于设计技术共优化

    Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks. (arXiv:2312.12784v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.12784](http://arxiv.org/abs/2312.12784)

    提出了一种基于图神经网络的快速准确芯片库特征化的机器学习模型，通过结合芯片结构，在各种工艺参数下预测精度高，并且相较于传统方法具有100倍的加速。

    

    设计技术共优化在先进半导体工艺开发中实现功耗、性能和面积（PPA）的最佳化发挥着关键作用。芯片库特征化在设计技术共优化流程中至关重要，但传统方法耗时且昂贵。为了克服这些挑战，我们提出了一种基于图神经网络（GNN）的快速准确的芯片库特征化的机器学习模型。我们的模型考虑了芯片结构，并在各种工艺-电压-温度（PVT）角和技术参数上展示出高预测精度。在512个未见过的工艺角和一百万个测试数据点的验证中，我们的模型对于33种类型的单元的延迟、功率和输入引脚电容具有准确的预测，均方绝对百分比误差（MAPE）≤ 0.95%，与SPICE仿真相比加速了100倍。此外，我们还研究了系统级指标，如最差负松弛（WNS）、漏电功耗和动态...

    Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development. Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly. To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization. Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters. Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\le$ 0.95% and a speed-up of 100X compared with SPICE simulations. Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic 
    
[^212]: 强化图转换器与正则化关注分数

    Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11730](http://arxiv.org/abs/2312.11730)

    本论文提出了一种新颖的边缘正则化技术版本，用于缓解图神经网络在内存问题上存在的困扰。与没有位置编码的Graph Transformer相比，应用了边缘正则化技术确实可以稳定地提高性能。

    

    图神经网络以其内存消耗大而臭名昭著。最近发现，基于Transformer的GNN称为Graph Transformer在存在长程依赖性时可以获得更好的性能。然而，将图数据和Transformer架构相结合导致了记忆问题更加严重。我们提出了一种新颖的“边缘正则化技术”的版本，可以减轻对位置编码的需求，从而减轻GT的内存溢出问题。我们观察到，不清楚在位置编码的基础上是否有边缘正则化是有帮助的。然而，显然，应用我们的边缘正则化技术确实可以稳定地改善GT的性能，相比于没有位置编码的GT。

    Graph Neural Networks are notorious for its memory consumption. A recent Transformer-based GNN called Graph Transformer is shown to obtain superior performances when long range dependencies exist. However, combining graph data and Transformer architecture led to a combinationally worse memory issue. We propose a novel version of "edge regularization technique" that alleviates the need for Positional Encoding and ultimately alleviate GT's out of memory issue. We observe that it is not clear whether having an edge regularization on top of positional encoding is helpful. However, it seems evident that applying our edge regularization technique indeed stably improves GT's performance compared to GT without Positional Encoding.
    
[^213]: TrojFST: 将特洛伊木马嵌入到少样本提示调优中

    TrojFST: Embedding Trojans in Few-shot Prompt Tuning. (arXiv:2312.10467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10467](http://arxiv.org/abs/2312.10467)

    TrojFST是一种在少样本提示调优框架中进行后门攻击的方法，通过引入平衡的污染学习、选择性令牌污染和...等模块来解决构建基于提示的后门的困难。

    

    提示调优已经成为一种非常有效的方法，用于适应使用有限输入样本的预训练语言模型（PLM）来处理新的自然语言处理任务。然而，提示调优的成功导致对手试图针对该技术进行后门攻击。之前基于提示的后门攻击在少样本提示调优方面面临挑战，需要全模型微调或大规模训练数据集。我们观察到使用少样本提示调优构建基于提示的后门的困难，这涉及冻结PLM并在一组受限制的输入样本上调优软提示。这种方法引入了一个不平衡的污染数据集，容易过拟合并且缺乏注意力感知。为了解决这些挑战，我们在少样本提示调优框架中引入了TrojFST用于后门攻击。TrojFST包括三个模块：平衡的污染学习、选择性令牌污染和...

    Prompt-tuning has emerged as a highly effective approach for adapting a pre-trained language model (PLM) to handle new natural language processing tasks with limited input samples. However, the success of prompt-tuning has led to adversaries attempting backdoor attacks against this technique. Previous prompt-based backdoor attacks faced challenges when implemented through few-shot prompt-tuning, requiring either full-model fine-tuning or a large training dataset. We observe the difficulty in constructing a prompt-based backdoor using few-shot prompt-tuning, which involves freezing the PLM and tuning a soft prompt with a restricted set of input samples. This approach introduces an imbalanced poisoned dataset, making it susceptible to overfitting and lacking attention awareness. To address these challenges, we introduce TrojFST for backdoor attacks within the framework of few-shot prompt-tuning. TrojFST comprises three modules: balanced poison learning, selective token poisoning, and tro
    
[^214]: MCRAGE: 公平性的合成医疗数据

    MCRAGE: Synthetic Healthcare Data for Fairness. (arXiv:2310.18430v1 [stat.ML])

    [http://arxiv.org/abs/2310.18430](http://arxiv.org/abs/2310.18430)

    MCRAGE是一种使用深度生成模型来增强不平衡的医疗数据集的方法，以解决少数群体在机器学习模型中的不公平问题。

    

    在医疗领域，电子健康记录（EHR）是开发诊断、治疗和管理医疗资源的机器学习模型的关键训练数据。然而，医疗数据集在种族/民族、性别和年龄等敏感属性方面往往存在不平衡。在类不平衡的EHR数据集上训练的机器学习模型在部署时，对于少数群体的个体而言，表现显著不如多数群体的样本，这可能导致少数群体的不公平医疗结果。为了解决这个挑战，我们提出了一种名为Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE)的新方法，通过由深度生成模型生成的样本来增强不平衡的数据集。MCRAGE过程包括训练一个能够从少数群体中产生高质量合成EHR样本的条件去噪扩散概率模型（CDDPM）。

    In the field of healthcare, electronic health records (EHR) serve as crucial training data for developing machine learning models for diagnosis, treatment, and the management of healthcare resources. However, medical datasets are often imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and age. Machine learning models trained on class-imbalanced EHR datasets perform significantly worse in deployment for individuals of the minority classes compared to samples from majority classes, which may lead to inequitable healthcare outcomes for minority groups. To address this challenge, we propose Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE), a novel approach to augment imbalanced datasets using samples generated by a deep generative model. The MCRAGE process involves training a Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of generating high-quality synthetic EHR samples from underrepresented classes. We use this 
    
[^215]: 具有自适应得分的转导式一致推断

    Transductive conformal inference with adaptive scores. (arXiv:2310.18108v1 [stat.ME])

    [http://arxiv.org/abs/2310.18108](http://arxiv.org/abs/2310.18108)

    利用转导式一致推断方法进行一致性无分布保证的机器学习任务，并通过建立Polya球模型的联合分布和经验分布函数的浓度不等式，提供了自适应得分的可用性和更高准确性。

    

    一致推断是一种基本且多用途的工具，为许多机器学习任务提供了无分布保证。我们考虑转导式设置，在这种设置中，针对$m$个新样本进行决策，产生$m$个一致推断$p$值。虽然经典结果仅涉及其边际分布，但我们表明它们的联合分布遵循一个Polya球模型，并建立了它们的经验分布函数的浓度不等式。这些结果适用于任意可交换的得分，包括可以在训练阶段使用测试+校准样本的协变量以提高准确性的“自适应”得分。我们通过对当前感兴趣的两个机器学习任务（转导式迁移学习的区间预测和基于两类分类的新颖性检测）提供统一且在概率上的保证来演示这些理论结果的有用性。

    Conformal inference is a fundamental and versatile tool that provides distribution-free guarantees for many machine learning tasks. We consider the transductive setting, where decisions are made on a test sample of $m$ new points, giving rise to $m$ conformal $p$-values. {While classical results only concern their marginal distribution, we show that their joint distribution follows a P\'olya urn model, and establish a concentration inequality for their empirical distribution function.} The results hold for arbitrary exchangeable scores, including {\it adaptive} ones that can use the covariates of the test+calibration samples at training stage for increased accuracy. We demonstrate the usefulness of these theoretical results through uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification.
    
[^216]: TiC-CLIP: CLIP模型的持续训练

    TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])

    [http://arxiv.org/abs/2310.16226](http://arxiv.org/abs/2310.16226)

    该论文提出了用于训练视觉-语言模型的大规模时间连续 (TiC) 基准，使用这些基准评估了现有模型的时间鲁棒性，并展示了一种简单有效的排练方法来持续训练模型。

    

    保持大型基础模型与最新数据保持同步本身就是昂贵的。为了避免不断重新训练的高成本，持续训练这些模型至关重要。这个问题被缺乏大规模连续学习基准或基线所加剧。我们引入了用于训练视觉-语言模型的第一批 Web 规模时间连续（TiC）基准：TiC-DataCompt、TiC-YFCC 和 TiC-RedCaps，其中包含超过 127 亿个时间戳图像-文本对，跨越了 9 年的时间（2014-2022）。我们首先使用这些基准来策划各种动态评估，以衡量现有模型的时间鲁棒性。我们展示了 OpenAI 的 CLIP 模型（使用 2020 年的数据进行训练）在我们策划的从 2021 年到 2022 年的检索任务中，失去了约 8% 的零-shot准确率，而与 OpenCLIP 存储库中最近训练的模型相比。然后，我们研究如何高效地对时间连续数据进行训练。我们证明了一种简单的排练方法，从上次的训练中继续训练，可以实现有效的训练。

    Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
    
[^217]: 使用分布式Hebbian Temporal Memory学习继任者表示法

    Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])

    [http://arxiv.org/abs/2310.13391](http://arxiv.org/abs/2310.13391)

    本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。

    

    本文提出了一种新颖的方法来解决在线隐藏表示学习的挑战，该方法用于在不稳定的、部分可观测的环境中进行决策。所提出的算法，分布式Hebbian Temporal Memory (DHTM)，基于因子图形式和多组成神经元模型。DHTM旨在捕捉顺序数据关系并对未来观察作出累积预测，形成继任者表示。受新皮层的神经生理学模型启发，该算法利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则克服了传统时间记忆算法（如RNN和HMM）的不稳定性和慢速学习过程。实验结果表明，DHTM优于经典的LSTM，并与更先进的类似RNN的算法性能相当，在变化的环境中加速了继任者表示的时间差异学习。此外，我们还进行了比较。

    This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
    
[^218]: PGA: 个性化抓取代理与单一人机交互

    PGA: Personalizing Grasping Agents with Single Human-Robot Interaction. (arXiv:2310.12547v1 [cs.RO])

    [http://arxiv.org/abs/2310.12547](http://arxiv.org/abs/2310.12547)

    这项研究介绍了个性化抓取代理（PGA），它通过单一人机交互学习并定位和抓取个人物体。PGA通过用户提供的信息和用户环境中的原始图像，实现个性化物体抓取。

    

    语言条件化机器人抓取（LCRG）旨在开发基于自然语言指令的机器人来进行物体的接地和抓取。虽然能够识别个人物品如“我的钱包”的机器人可以更自然地与非专家用户交互，但当前的LCRG系统主要限制机器人只能理解一般表达。为此，我们引入了一个名为GraspMine的任务场景，并提供了一个新颖的数据集，旨在通过从单一人机交互中学习定位和抓取个人物体。为了解决GraspMine，我们提出了个性化抓取代理（PGA），通过将用户提供的信息传播到个人物体上，通过Reminiscence-用户环境中的一系列原始图像，获取个人物体信息。具体而言，PGA通过用户展示带有相关指示器的个人物体，并以旋转的方式检查物体来获取个人物体信息。根据所获得的信息，PGA为物体进行伪标签化。

    Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that ground and grasp objects based on natural language instructions. While robots capable of recognizing personal objects like "my wallet" can interact more naturally with non-expert users, current LCRG systems primarily limit robots to understanding only generic expressions. To this end, we introduce a task scenario GraspMine with a novel dataset that aims to locate and grasp personal objects given personal indicators via learning from a single human-robot interaction. To address GraspMine, we propose Personalized Grasping Agent (PGA), that learns personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user's environment. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in
    
[^219]: 相对高斯机制及其在私有梯度下降中的应用

    The Relative Gaussian Mechanism and its Application to Private Gradient Descent. (arXiv:2308.15250v1 [cs.LG])

    [http://arxiv.org/abs/2308.15250](http://arxiv.org/abs/2308.15250)

    本文介绍了相对高斯机制(RGM)，它利用了相对L2敏感性假设，在保护隐私的同时能够更精确地界定隐私损失。

    

    高斯机制(GM)是一种在发布之前向矢量查询添加高斯噪声的标准隐私保护机制。特别地，如果查询满足某种L2敏感性属性(任意两个相邻输入上输出之间的L2距离有界)，GM保证了Rényi差分隐私(RDP)。不幸的是，精确地界定L2敏感性可能很困难，从而导致松弛的隐私界限。在这项工作中，我们考虑了相对L2敏感性假设，在这种假设下，两个查询输出之间的距离界限也可能取决于它们的范数。利用这一假设，我们引入了相对高斯机制(RGM)，其中噪声的方差取决于输出的范数。我们在相对L2敏感性下证明了RDP参数的严格界限，并描述了因使用输出相关噪声而产生的隐私损失。特别地，我们展示了RGM可以自然地适应潜变量。

    The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a vector-valued query before releasing it, is a standard privacy protection mechanism. In particular, given that the query respects some L2 sensitivity property (the L2 distance between outputs on any two neighboring inputs is bounded), GM guarantees R\'enyi Differential Privacy (RDP). Unfortunately, precisely bounding the L2 sensitivity can be hard, thus leading to loose privacy bounds. In this work, we consider a Relative L2 sensitivity assumption, in which the bound on the distance between two query outputs may also depend on their norm. Leveraging this assumption, we introduce the Relative Gaussian Mechanism (RGM), in which the variance of the noise depends on the norm of the output. We prove tight bounds on the RDP parameters under relative L2 sensitivity, and characterize the privacy loss incurred by using output-dependent noise. In particular, we show that RGM naturally adapts to a latent variable that would
    
[^220]: 在欧几里德函数优化中的扭曲几何信息

    Warped geometric information on the optimisation of Euclidean functions. (arXiv:2308.08305v1 [stat.ML])

    [http://arxiv.org/abs/2308.08305](http://arxiv.org/abs/2308.08305)

    使用扭曲几何学的概念，我们提出了一种在高维欧几里德空间中优化函数的方法，并通过在重新定义的黎曼流形上进行计算，找到了函数的最优解。

    

    我们考虑了在潜在高维欧几里德空间中优化实值函数的基本任务，例如许多机器学习任务中的损失函数或统计推断中的概率分布的对数。我们使用扭曲黎曼几何概念，将欧几里德空间上的函数优化问题重新定义为一个带有扭曲度量的黎曼流形，并在该流形上找到函数的最优解。选择用于搜索域的扭曲度量引入了一个计算友好的度量张量，使得在流形上找到最优搜索方向与测地线变得更容易计算。沿测地线进行优化通常是不可行的，但我们表明在这个特定的流形中，我们可以解析地得到高达三阶的泰勒近似。一般情况下，这些对测地线的近似不会位于流形上，但我们构造了合适的回缩方程将这些近似重新映射到流形上。

    We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction m
    
[^221]: 基于主动学习的预训练数据去重模型

    A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])

    [http://arxiv.org/abs/2308.00721](http://arxiv.org/abs/2308.00721)

    提出了一种基于主动学习的预训练去重模型，将Transformer和主动学习集成到端到端架构中，首次解决了语义级别的去重问题，同时采用R-Drop方法对每一轮标记数据进行数据增强。通过选择最有价值的数据进行去重模型训练，不仅降低了手动标记的成本，还提高了模型的泛化能力。

    

    在大数据时代，数据质量问题日益突出。其中一个主要挑战是重复数据问题，这可能是由于数据的重复输入或多个数据源的合并导致的。这些"脏数据"问题严重限制了大数据的有效应用。为了解决数据去重的问题，我们提出了一种基于主动学习的预训练去重模型，这是首次利用主动学习解决语义级别的去重问题的工作。该模型构建在一个预训练的Transformer上，并通过细调将其应用于序列分类任务，首次将Transformer和主动学习集成到端到端架构中，以选择最有价值的数据进行去重模型训练，同时首次采用R-Drop方法对每一轮标记数据进行数据增强，既能降低手动标记的成本，也能提高模型的泛化能力。

    In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
    
[^222]: FedDRL: 一种基于分阶段强化学习的可信联邦学习模型融合方法

    FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])

    [http://arxiv.org/abs/2307.13716](http://arxiv.org/abs/2307.13716)

    FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。

    

    传统的联邦学习使用样本数量计算每个客户端模型的权重，并使用这个固定权重值来融合全局模型。然而，在实际场景中，每个客户端设备和数据的异质性导致每个客户端模型的质量存在差异。因此，对全局模型的贡献不仅仅取决于样本量。此外，如果客户端故意上传低质量或恶意模型，使用这些模型进行聚合将严重降低全局模型的准确性。传统的联邦学习算法没有解决这些问题。为了解决这个问题，我们提出了一种名为FedDRL的模型融合方法，它使用两个阶段的强化学习。在第一个阶段，我们的方法可以过滤掉恶意模型，并选择可信的客户端模型参与模型融合。在第二个阶段，FedDRL算法自适应地调整可信客户端模型的权重并聚合。

    Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
    
[^223]: 用于有效哈密顿量参数化的即时机器学习

    On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.08929](http://arxiv.org/abs/2307.08929)

    本论文提出了一种基于贝叶斯线性回归的即时机器学习方法，用于参数化有效哈密顿量。该方法在各种系统中都可以得到准确的结果，包括以前的方法无法处理的复杂系统。

    

    基于第一原理的有效哈密顿量被广泛用于预测和模拟铁电和弛豫铁电体的性质。然而，有效哈密顿量的参数化方法复杂，很难解决具有复杂相互作用和/或复杂组分系统。在这里，我们开发了一种基于贝叶斯线性回归的即时机器学习方法来参数化有效哈密顿量。参数化是在分子动力学模拟中完成的，每一步预测能量、力和应力以及相关的不确定性。当不确定性较大时，执行第一原理计算以重新训练参数。这种方法为计算任何所考虑系统的有效哈密顿量参数提供了一种通用和自动化的方式，包括以前的方法无法处理的复杂系统。以BaTiO3和Pb(Sc,Ta)O3为例展示了该方法的准确性。

    The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach compa
    
[^224]: 分层线性模态连接

    Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])

    [http://arxiv.org/abs/2307.06966](http://arxiv.org/abs/2307.06966)

    本文提出了一种分层线性模态连接方法用于联邦深度学习，通过解决模型漂移和高损失障壁的问题，能够有效提升全局模型的性能。

    

    在联邦设置中，我们在训练过程中多次对分离的本地模型进行聚合，以获得更强大的全局模型；最常见的聚合方法是参数的简单平均。理解在非凸设置（如联邦深度学习）中聚合何时以及为何有效是一个尚未解决的挑战，这阻碍了获得高性能的全局模型。在i.i.d.数据集上，频繁平均的联邦深度学习是成功的。然而，常见的观点是在独立训练期间，模型会相互漂移，因此在许多本地参数更新后平均可能不再起作用。这个问题可以从损失曲面的角度来看：对于非凸曲面上的点，平均值可能变得任意糟糕。通常用于解释联邦平均成功的局部凸性假设与经验证据相矛盾，显示不同模型之间存在高损失障壁。

    In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the v
    
[^225]: T-MARS：通过规避文本特征学习来改善视觉表示

    T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])

    [http://arxiv.org/abs/2307.03132](http://arxiv.org/abs/2307.03132)

    T-MARS提出一种新的数据筛选方法，通过规避文本特征学习，改善了视觉表示的学习，解决了大型多模态数据集中存在的文本与图像重叠的问题。

    

    大型网络来源的多模态数据集为学习通用视觉表示的新方法提供了动力，推动了计算机视觉的最新发展，并彻底改变了零样本和少样本识别。一个关键的决策问题是如何筛选这些日益庞大的数据集。本文提出了一种新的最先进的数据筛选方法，其动机是我们观察到近40%的LAION数据集的图像与说明存在重叠的文本。直觉上，这样的数据可能会浪费资源，因为它鼓励模型进行光学字符识别而不是学习视觉特征。然而，简单地将所有这些数据去除也可能浪费，因为这会丢弃包含视觉特征的图像（除了重叠的文本）。我们提出了一种简单而可扩展的方法来解决这个问题。

    Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
    
[^226]: 对抗训练应被视为一个非零和博弈

    Adversarial Training Should Be Cast as a Non-Zero-Sum Game. (arXiv:2306.11035v1 [cs.LG])

    [http://arxiv.org/abs/2306.11035](http://arxiv.org/abs/2306.11035)

    本论文提出了一种新的针对对抗性训练的非零和双层公式，实现了与最先进攻击相匹配并且能够达到与标准对抗性训练相同的鲁棒性水平。

    

    解决深度神经网络对抗性脆弱性的一个突出方法是采用对抗性训练的两个玩家零和范式，其中预测器被训练以对抗性选择的数据扰动。虽然这种方法很有前途，但是基于这种范式的算法并没有产生足够的鲁棒性，并且遭受病态行为，如强健的过拟合。为了理解这种缺陷，我们首先展示了在对抗训练算法中使用的常见基于代理的松弛方法使所训练分类器的稳健性没有任何保证。我们发现这个问题后，提出了一个新的非零和双层对抗训练公式，其中每个玩家优化不同的目标函数，我们的公式自然地产生了一个简单的算法框架，可以与最先进的攻击相匹配，并且在一些情况下，能够达到与标准对抗性训练相当的鲁棒性水平。

    One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially-chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness, and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers. The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function. Our formulation naturally yields a simple algorithmic framework that matches and in some cases outperforms state-of-the-art attacks, attains comparable levels of robustness to standard adversarial traini
    
[^227]: Ada-NAV：用于机器人导航的自适应轨迹优化策略学习方法

    Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])

    [http://arxiv.org/abs/2306.06192](http://arxiv.org/abs/2306.06192)

    Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。

    

    强化学习方法在学习机器人导航策略方面十分有效，但其采样效率低的问题也十分明显。在策略优化中，这种效率低下部分来自于未能适当地平衡探索与利用的问题，特别是在面对非静态时。为了加入探索与利用的平衡以提高采样效率，我们提出了Ada-NAV，一种自适应轨迹长度方案，其中长度随着策略的随机性（用其Shannon或差分熵表示）的减小而增加。我们的自适应轨迹长度方案由于更频繁的梯度更新强调了训练开始时的探索，后来则更强调利用。在网格世界，仿真机器人环境和真实世界机器人实验中，我们证明了该方法的优点，表现在性能和采样效率上均优于常数和随机采样的轨迹长度。在固定的样本预算下，相对于现有的基准方法，Ada-NAV的性能提高了高达46％，采样数量减少了高达80％。

    Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
    
[^228]: 属性高效的低次多项式阈值函数带噪声PAC学习

    Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise. (arXiv:2306.00673v1 [cs.DS])

    [http://arxiv.org/abs/2306.00673](http://arxiv.org/abs/2306.00673)

    本文提出一种新算法，可以属性高效的学习低次多项式阈值函数，并能够在噪声下进行PAC学习。

    

    低次多项式阈值函数（PTFs）的概念类在机器学习中起着基础作用。本文研究了$\mathbb{R}^n$上$K$稀疏度-$d$ PTFs的属性高效PAC学习，其中任何这样的概念仅依赖于输入的$K$个属性。我们的主要贡献是：提出一种新算法，在高斯边缘分布下，即使有$O(\epsilon^d)$的$\eta$被恶意噪声Bshouty et al. (2002)破坏，也可以在错误率$\epsilon$下以$O(\frac{K^{{4d}}}{\epsilon^{2d}}\cdot \log^{5d} n)$的样本PAC学习该类，算法运行时间为$({nd}/{\epsilon})^{O(d)}$。在此之前，仅为稀疏齐次超平面的特殊情况建立了属性高效的鲁棒算法。我们的关键因素是：1）将属性稀疏性转化为Hermite多项式基下chow向量的稀疏模式的结构结果；2）一种新的规范化方法，以及利用多项式近似的阈值函数的直接判别。

    The concept class of low-degree polynomial threshold functions (PTFs) plays a fundamental role in machine learning. In this paper, we study PAC learning of $K$-sparse degree-$d$ PTFs on $\mathbb{R}^n$, where any such concept depends only on $K$ out of $n$ attributes of the input. Our main contribution is a new algorithm that runs in time $({nd}/{\epsilon})^{O(d)}$ and under the Gaussian marginal distribution, PAC learns the class up to error rate $\epsilon$ with $O(\frac{K^{4d}}{\epsilon^{2d}} \cdot \log^{5d} n)$ samples even when an $\eta \leq O(\epsilon^d)$ fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces. Our key ingredients are: 1) a structural result that translates the attribute sparsity to a sparsity pattern of the Chow vector under the basis of Hermite polynomials, and 2) a novel 
    
[^229]: 多个深度网络的训练过程探索相同的低维流形

    The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])

    [http://arxiv.org/abs/2305.01604](http://arxiv.org/abs/2305.01604)

    本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。

    

    我们开发了信息几何技术来分析深度网络训练过程中预测轨迹。通过检查底层高维概率模型，我们揭示了训练过程探索的有效低维流形。具有各种体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化的网络在预测空间内位于同一流形上。我们研究了这种流形的细节，发现具有不同体系结构的网络遵循可区分的轨迹，但其他因素影响极小; 更大的网络沿着与较小的网络相似的流形训练，只是更快; 不同部分的初始化网络在相似的流形上向解决方案收敛。

    We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.
    
[^230]: PopulAtion Parameter Averaging (PAPA)（人口参数平均）

    PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])

    [http://arxiv.org/abs/2304.03094](http://arxiv.org/abs/2304.03094)

    提出一种新方法PopulAtion Parameter Averaging (PAPA)，能同时拥有集成的普遍性与权重平均的效率，可以显著提高模型性能。

    

    集成方法将多个模型的预测组合起来以提高性能，但需要更高的计算成本。为了避免这些成本，可以通过对多个神经网络的权重进行平均来将它们合并成一个（模型汤）。然而，这通常比集成表现更差。当权重足够相似（在权重或特征空间中）可以很好地平均，但足够不同以从组合中受益时，权重平均才是有益的。基于这个想法，我们提出了PopulAtion Parameter Averaging (PAPA)，一种将集成的普遍性与权重平均的效率相结合的方法。PAPA利用不同模型（在不同数据顺序，增强和正则化上训练）的人口，而偶尔（不要太频繁，也不要太稀疏）用网络的权重来代替人口权重的平均值。PAPA减少了平均值和集成之间的性能差距，提高了模型的性能。

    Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
    
[^231]: 关于知识蒸馏中的学生-教师偏差：违反规则是否有益？

    On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12923](http://arxiv.org/abs/2301.12923)

    通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。

    

    知识蒸馏（KD）被广泛用于通过训练学生模仿经过训练的“教师”网络的软概率来提高“学生”网络的测试准确性。然而，最近的研究表明，尽管被训练成适应教师的概率，学生不仅明显偏离这些概率，而且表现比教师更好。我们的研究旨在通过确定学生-教师偏差的确切性质，并论证它们与更好的泛化能力如何共存来解决这一看似矛盾的观察。首先，通过对图像和语言数据进行实验，我们确定这些偏差对应于学生系统性地夸大教师的自信水平。接下来，在一些简单的设置中，我们从理论和实证上建立了KD在收敛更快的过程中夸大了梯度下降的隐含偏差的证据。最后，

    Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
    
[^232]: Copula联合预测用于多步时间序列预测

    Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03281](http://arxiv.org/abs/2212.03281)

    本文提出了一种 Copula 联合预测算法 CopulaCPTS，用于多元、多步时间序列预测，经过实验验证，其置信区间比现有技术更精准和更锐利。

    

    精确的不确定性度量是构建强大可靠的机器学习系统的关键步骤。拟合预测是一种流行的无分布不确定性量化算法，因其易于实现、统计覆盖保证和对底层预测算法的多样性而受到欢迎。然而，现有的时间序列置信预测算法仅限于单步预测，未考虑时序依赖。本文提出一种 Copula 联合预测算法，用于多元、多步时间序列预测 CopulaCPTS。我们证明了 CopulaCPTS 具有有限的样本*有效性保证。在多个合成和真实世界的多元时间序列数据集上，我们展示了 CopulaCPTS 的多步预测可产生比现有技术更精准和更锐利的置信区间。

    Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.
    
[^233]: 用强化学习改进人类的顺序决策

    Improving Human Sequential Decision-Making with Reinforcement Learning. (arXiv:2108.08454v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.08454](http://arxiv.org/abs/2108.08454)

    该论文提出了一种利用机器学习算法从追踪数据中提取"最佳实践"并向人类传达的方法，以改进人类的顺序决策。通过一系列实验证实，在虚拟厨房管理任务中，这种方法能够显著提高性能。

    

    工作者花费大量时间学习如何做出正确决策。然而，评估一个给定决策的有效性可能很复杂，例如，决策结果通常是长期的，并且与原始决策之间存在复杂的关系。令人惊讶的是，尽管学习良好的决策策略很困难，但它们通常可以简洁明了地表达。针对顺序决策，我们设计了一种新颖的机器学习算法，能够从追踪数据中提取“最佳实践”并以可解释的“提示”的形式传达其见解给人类。我们的算法选择最佳的提示，以填补人类工作者采取的行动与最优策略之间的差距，同时考虑哪些行动对于提高性能具有重要意义。我们通过一系列随机对照实验评估了我们的方法，参与者管理一个虚拟厨房。我们的实验证明，提示可以显著提高性能。

    Workers spend a significant amount of time learning how to make good decisions. Evaluating the efficacy of a given decision, however, can be complicated -- e.g., decision outcomes are often long-term and relate to the original decision in complex ways. Surprisingly, even though learning good decision-making strategies is difficult, they can often be expressed in simple and concise forms. Focusing on sequential decision-making, we design a novel machine learning algorithm that is capable of extracting "best practices" from trace data and conveying its insights to humans in the form of interpretable "tips". Our algorithm selects the tip that best bridges the gap between the actions taken by human workers and those taken by the optimal policy in a way that accounts for which actions are consequential for achieving higher performance. We evaluate our approach through a series of randomized controlled experiments where participants manage a virtual kitchen. Our experiments show that the tip
    

