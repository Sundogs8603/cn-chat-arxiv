# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Supervised Algorithmic Fairness in Distribution Shifts: A Survey](https://rss.arxiv.org/abs/2402.01327) | 这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。 |
| [^2] | [Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling](https://arxiv.org/abs/2404.01078) | EmSHAP提出了基于能量模型的Shapley值估计方法，通过引入GRU来消除输入特征顺序的影响，从而可以有效近似任意特征子集下深度学习模型的Shapley值贡献函数。 |
| [^3] | [Machine Learning Robustness: A Primer](https://arxiv.org/abs/2404.00897) | 该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。 |
| [^4] | [The Topos of Transformer Networks](https://arxiv.org/abs/2403.18415) | 通过拓扑理论的视角，我们对Transformer架构的表达能力进行了理论分析，发现它具有高阶推理的特点，并与其他常见神经网络架构进行了对比。 |
| [^5] | [Mathematical Foundation and Corrections for Full Range Head Pose Estimation](https://arxiv.org/abs/2403.18104) | 该论文研究了头部姿势估计中的坐标系和欧拉角顺序定义问题，验证了之前作品中使用的姿势输出的正确性和绘图例程的有效性。 |
| [^6] | [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://arxiv.org/abs/2403.15246) | 该论文引入了FollowIR数据集，包含严格的说明书评估基准和训练集，帮助信息检索模型更好地遵循真实世界的说明书。议论基于TREC会议的历史，旨在使信息检索模型能够根据详细说明书理解和判断相关性。 |
| [^7] | [Anytime Neural Architecture Search on Tabular Data](https://arxiv.org/abs/2403.10318) | ATLAS是第一个专为表格数据设计的任意时间神经网络架构搜索方法，引入了过滤和优化方案，结合了训练-free和基于训练的架构评估两种范式的优势 |
| [^8] | [Towards a theory of model distillation](https://arxiv.org/abs/2403.09053) | 提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。 |
| [^9] | [Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement](https://arxiv.org/abs/2403.06659) | 通过Multimodal ECG Representation Learning (MERL)框架，本文提出了一种零样本心电图分类方法，结合了对ECG记录和相关报告的多模态学习，同时在测试阶段使用了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法来利用临床知识数据库。 |
| [^10] | [Towards Efficient Replay in Federated Incremental Learning](https://arxiv.org/abs/2403.05890) | 本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。 |
| [^11] | [Effectiveness Assessment of Recent Large Vision-Language Models](https://arxiv.org/abs/2403.04306) | 本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。 |
| [^12] | [SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS](https://arxiv.org/abs/2403.04161) | 提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。 |
| [^13] | [World Models for Autonomous Driving: An Initial Survey](https://arxiv.org/abs/2403.02622) | 世界模型在自主驾驶领域的重要性和作用，是通过准确预测未来事件和评估其影响来帮助决策过程，从而推动自主驾驶技术发展的革命性方法。 |
| [^14] | [On the Compressibility of Quantized Large Language Models](https://arxiv.org/abs/2403.01384) | 研究在内存受限设备上应用数据压缩技术以加速量化LLM推理过程的一项初步工作。 |
| [^15] | [Epsilon-Greedy Thompson Sampling to Bayesian Optimization](https://arxiv.org/abs/2403.00540) | 将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。 |
| [^16] | [Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy](https://arxiv.org/abs/2402.19379) | 该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。 |
| [^17] | [How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning](https://arxiv.org/abs/2402.18312) | 通过对LLMs进行机械性研究，我们发现模型在进行逐步推理时使用多个并行路径生成答案，同时存在功能性分歧。 |
| [^18] | [Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/abs/2402.17152) | 提出了HSTU架构，用于高基数、非平稳流推荐数据，性能优于基线方法高达65.8％的NDCG，并且比基于FlashAttention2的Transformer在8192长度序列上快5.3倍到15.2倍。 |
| [^19] | [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/abs/2402.16310) | 该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。 |
| [^20] | [Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs](https://arxiv.org/abs/2402.10517) | 任意精度LLM引入了一种轻量级方法，通过将不同大小LLMs量化为不同位宽（如3、4、...，n位）并叠加到内存中，显着降低了部署多个不同大小LLMs的高成本 |
| [^21] | [Emoji Driven Crypto Assets Market Reactions](https://arxiv.org/abs/2402.10481) | 该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。 |
| [^22] | [Tackling Negative Transfer on Graphs](https://arxiv.org/abs/2402.08907) | 图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。 |
| [^23] | [Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems](https://arxiv.org/abs/2402.08193) | 高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。 |
| [^24] | [Task-conditioned adaptation of visual features in multi-task policy learning](https://arxiv.org/abs/2402.07739) | 本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。 |
| [^25] | [Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy](https://arxiv.org/abs/2402.07270) | 该研究通过提出创新的评估方法和基于分类数据集的新型VQA基准，推动了对文本生成的视觉语言模型能力的理解。同时，他们还提出了使用语义层次和自动生成的后续问题来改进对细粒度分类任务上粗糙答案的评估。通过比较不同度量标准，他们在进行人工评估研究的基础上选择了最终的度量标准。 |
| [^26] | [The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks](https://arxiv.org/abs/2402.06357) | 本文提出了一种名为 SpongeNet 的新型海绵攻击，通过直接作用于预训练模型参数，成功增加了视觉模型的能耗，而且所需的样本数量更少。 |
| [^27] | [Accelerating PDE Data Generation via Differential Operator Action in Solution Space](https://arxiv.org/abs/2402.05957) | 通过在解空间中应用微分算子作用，我们提出了一种加速PDE数据生成的算法，名为DiffOAS。它能够在生成数据的同时提高数据的精度，并且在时间复杂度上比现有的方法更高效。 |
| [^28] | [CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines](https://arxiv.org/abs/2402.04400) | CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。 |
| [^29] | [Increasing Trust in Language Models through the Reuse of Verified Circuits](https://arxiv.org/abs/2402.02619) | 本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。 |
| [^30] | [Large Language Model Adaptation for Networking](https://arxiv.org/abs/2402.02338) | 本文首次研究了使用大型语言模型（LLM）适应网络问题的方法，通过利用LLM的预训练知识和强大推理能力，实现了“一模型适用于所有”的目标，并取得了更好的性能和更强的泛化能力。 |
| [^31] | [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253) | 这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。 |
| [^32] | [Graph Transformers without Positional Encodings](https://arxiv.org/abs/2401.17791) | 本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。 |
| [^33] | [Detecting algorithmic bias in medical AI-models](https://arxiv.org/abs/2312.02959) | 本文提出了一种创新的框架，用于检测医疗AI决策支持系统中的算法偏倚，通过采用CART算法有效地识别医疗AI模型中的潜在偏倚，并在合成数据实验和真实临床环境中验证了其有效性。 |
| [^34] | [Imitation Bootstrapped Reinforcement Learning](https://arxiv.org/abs/2311.02198) | 提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。 |
| [^35] | [Adversarial Examples Are Not Real Features](https://arxiv.org/abs/2310.18936) | 该论文从多个学习范式的角度重新审视对抗样本的理论，发现非鲁棒特征在多种无监督学习范式中的效用较差，揭示了这些特征并不像鲁棒特征或自然特征那样真正有用。 |
| [^36] | [Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://arxiv.org/abs/2310.05175) | 本研究发现LLMs中的激活异常值与网络层稀疏度的非均匀性相关，并提出了Outlier Weighed Layerwise Sparsity（OWL）作为剪枝LLMs到高稀疏度的秘密调味料。 |
| [^37] | [Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning](https://arxiv.org/abs/2204.04510) | 提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。 |
| [^38] | [Prompt Design and Engineering: Introduction and Advanced Methods.](http://arxiv.org/abs/2401.14423) | 本文介绍了提示设计与工程的主要概念，并回顾了基本和更高级的方法。 |
| [^39] | [Robust Multi-Modal Density Estimation.](http://arxiv.org/abs/2401.10566) | 本文提出了一种名为ROME的鲁棒多模态密度估计方法，该方法利用聚类将多模态样本集分割成多个单模态样本集，并通过简单的KDE估计来估计整体分布。这种方法解决了多模态、非正态和高相关分布估计的挑战。 |
| [^40] | [DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning.](http://arxiv.org/abs/2401.09243) | 本文介绍了DiffClone，一种通过扩散驱动的策略学习增强行为克隆代理的离线算法。在真实的在线物理机器人上的实验表明，采用MOCO微调的ResNet50的效果最好。 |
| [^41] | [A ripple in time: a discontinuity in American history.](http://arxiv.org/abs/2312.01185) | 该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。 |
| [^42] | [On Linear Separation Capacity of Self-Supervised Representation Learning.](http://arxiv.org/abs/2310.19041) | 本研究通过探究在多流形模型下，学习的表示何时可以线性分离流形，揭示了自监督学习在数据增强方面的额外好处，从而改善了线性分离能力的信息论最优速率。 |
| [^43] | [How do Language Models Bind Entities in Context?.](http://arxiv.org/abs/2310.17191) | 通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。 |
| [^44] | [KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models.](http://arxiv.org/abs/2310.15872) | 本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。 |
| [^45] | [Privacy Amplification for Matrix Mechanisms.](http://arxiv.org/abs/2310.15526) | 提出了通过抽样分析矩阵机制的隐私放大算法MMCC，证明了通过条件性组合定理可以将相关输出视为独立输出，并展示了将噪声添加到DP-FTRL算法时可以渐近地与DP-SGD算法中放大后添加的噪声相匹配。 |
| [^46] | [Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning.](http://arxiv.org/abs/2310.12952) | 该论文提出了一种基于相似性的多样性度量方法——Vendi分数族群，为科学和机器学习等领域准确测量多样性提供了灵活性。 |
| [^47] | [Circuit Component Reuse Across Tasks in Transformer Language Models.](http://arxiv.org/abs/2310.08744) | 这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。 |
| [^48] | [Beyond Memorization: Violating Privacy Via Inference with Large Language Models.](http://arxiv.org/abs/2310.07298) | 该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。 |
| [^49] | [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.](http://arxiv.org/abs/2310.06387) | 本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。 |
| [^50] | [Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks.](http://arxiv.org/abs/2310.03789) | 本研究将自适应核方法应用于两个师生模型，预测了特征学习和 Grokking 的性质，并展示了 Grokking 与相变理论之间的映射关系。 |
| [^51] | [Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods.](http://arxiv.org/abs/2310.02671) | 本论文研究了随机Softmax策略梯度方法的收敛性分析，提出了一种动态策略梯度的组合方法，并通过对参数进行反向训练来更好地利用相关性结构，实现向全局最优值的收敛。 |
| [^52] | [Locality-Aware Graph-Rewiring in GNNs.](http://arxiv.org/abs/2310.01668) | 本论文提出了一种在GNN中考虑局部性的图重连技术，以减少过度压缩、保持图的稀疏性，并改善长程交互的捕捉能力。 |
| [^53] | [Towards Causal Foundation Model: on Duality between Causal Inference and Attention.](http://arxiv.org/abs/2310.00809) | 该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。 |
| [^54] | [PLMM: Personal Large Models on Mobile Devices.](http://arxiv.org/abs/2309.14726) | 本文提出了一种从传统大型语言模型中提取的个人大型模型，该模型更适应于本地用户的个人信息，并且能够保护用户的隐私。该模型分为个人级别、专家级别和传统级别，同时还需要小型化以适应个人计算机或移动设备，并实现实时响应以提供更好的用户体验。 |
| [^55] | [Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains.](http://arxiv.org/abs/2309.13005) | 通过顺序自编码器实现反事实公平性追求的创新框架将环境信息和敏感属性与分类特征的嵌入表示分开，以提高模型在不同和陌生领域中的泛化能力，并解决公平问题。 |
| [^56] | [Frequency Convergence of Complexon Shift Operators.](http://arxiv.org/abs/2309.07169) | 本文研究了拓扑信号处理中复合子的可转移性，通过构造边际复合子和复合移位算子，研究其特征值和特征向量，并证明了复合子收敛时对应的复合移位算子的特征值会收敛到极限复合子的特征值。这些结果拓展了图信号处理框架。 |
| [^57] | [Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy.](http://arxiv.org/abs/2308.14104) | 本文介绍了一种通过结合全局信息和本地特征，实现对车辆路径问题的通用神经求解器的方法，并将其应用于解决现实世界的复杂问题。 |
| [^58] | [Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion.](http://arxiv.org/abs/2308.12517) | 本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。 |
| [^59] | [Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes.](http://arxiv.org/abs/2308.11267) | 本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。 |
| [^60] | [GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder.](http://arxiv.org/abs/2308.05882) | GPLaSDI是一种基于高斯过程的可解释潜空间动力学识别方法，通过深度自动编码器将完全阶数的PDE解映射到潜空间，并使用插值和解决ODE系统进行快速和准确的ROM预测。 |
| [^61] | [Speeding up Fourier Neural Operators via Mixed Precision.](http://arxiv.org/abs/2307.15034) | 通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。 |
| [^62] | [ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis.](http://arxiv.org/abs/2307.13883) | ExeDec是一种基于分解的合成策略，通过预测执行子目标并在每个步骤的程序执行的指导下逐步解决问题，实现了更好的合成性能和组合泛化能力。 |
| [^63] | [BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables.](http://arxiv.org/abs/2307.02891) | 本文提出了一种名为BaBE的方法，通过估计潜在解释变量来提高公平性。该方法通过结合贝叶斯推断和期望最大化方法，估计给定Z的每个群体E的最可能值。 |
| [^64] | [Are aligned neural networks adversarially aligned?.](http://arxiv.org/abs/2306.15447) | 我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。 |
| [^65] | [DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation.](http://arxiv.org/abs/2306.12422) | 本文提出了一种优先使用单调非增函数进行时间步采样的NeRF优化方法，以解决NeRF优化过程和得分蒸馏中均匀时间步采样之间的冲突。实验表明这种方法能够显著提高文本到3D内容的质量和多样性。 |
| [^66] | [A Simple and Effective Pruning Approach for Large Language Models.](http://arxiv.org/abs/2306.11695) | 本论文提出了一种称为Wanda的新颖、简单而有效的剪枝方法，用于大型语言模型，通过对每个输出上的权重按照最小幅度乘以对应的输入激活来进行剪枝，无需重新训练或更新权重。 |
| [^67] | [Identifiable causal inference with noisy treatment and no side information.](http://arxiv.org/abs/2306.10614) | 本论文提出了一种在没有侧面信息和具有复杂非线性依赖性的情况下，纠正因治疗变量不准确测量引起的因果效应估计偏差的模型，并证明了该模型的因果效应估计是可识别的。该方法使用了深度潜在变量模型和分摊权重变分客观函数进行训练。 |
| [^68] | [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.](http://arxiv.org/abs/2306.05426) | 本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。 |
| [^69] | [Spike-based computation using classical recurrent neural networks.](http://arxiv.org/abs/2306.03623) | 本文提出了一种新的脉冲神经网络方法，通过修改一种易于训练的循环神经网络的动态特性，使其产生基于脉冲的计算，并在进行了脉冲网络的训练后，在多个数据集上取得了最先进的性能。 |
| [^70] | [Navigating Explanatory Multiverse Through Counterfactual Path Geometry.](http://arxiv.org/abs/2306.02786) | 该论文提出了解释性多元宇宙的概念，用于导航和比较所有可能的反事实路径的几何关系。 |
| [^71] | [Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming.](http://arxiv.org/abs/2306.02568) | 该论文使用动态规划和Gumbel传播在VAE的潜在空间中获得结构化稀疏最优路径，从而使得模型可以依赖于未观察到的结构特征信息，并成功实现了文本转语音和歌声合成。 |
| [^72] | [Decentralized Federated Learning: A Survey and Perspective.](http://arxiv.org/abs/2306.01603) | 去中心化联邦学习通过消除中心服务器的需求，实现了直接通信从而节省了通信资源。本文提供了对DFL的全面调查、深入展望和扩展变体与分类的介绍，重点在于DFL的系统与详细前景。 |
| [^73] | [Fairness of ChatGPT.](http://arxiv.org/abs/2305.18569) | 本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。 |
| [^74] | [An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework.](http://arxiv.org/abs/2305.01322) | 该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。 |
| [^75] | [Decoupling Quantile Representations from Loss Functions.](http://arxiv.org/abs/2304.12766) | 本文提出了同时二元量化回归（SBQR）中分位数与预测概率的二元对偶性，使分位数表达式在不同tau值下的构造不再依赖于损失函数，从而在处理检测超出分布样本和调整模型方面表现突出。 |
| [^76] | [PopulAtion Parameter Averaging (PAPA).](http://arxiv.org/abs/2304.03094) | 提出一种新方法PopulAtion Parameter Averaging (PAPA)，能同时拥有集成的普遍性与权重平均的效率，可以显著提高模型性能。 |
| [^77] | [Core-based Trend Detection in Blockchain Networks.](http://arxiv.org/abs/2303.14241) | 本研究介绍了一种可伸缩的方法InnerCore用于分析区块链网络，实现了基于区块链网络对关键参与者的识别并提供情感指标，以实现自动化的趋势监测。 |
| [^78] | [PULSNAR -- Positive unlabeled learning selected not at random: class proportion estimation when the SCAR assumption does not hold.](http://arxiv.org/abs/2303.08269) | 本文提出了一种新的PU方法PULSNAR，即使在SCAR不成立时，也可以实现准确的α估计和校准模型。 |
| [^79] | [Analyzing And Editing Inner Mechanisms Of Backdoored Language Models.](http://arxiv.org/abs/2302.12461) | 本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。 |
| [^80] | [On marginal feature attributions of tree-based models.](http://arxiv.org/abs/2302.08434) | 该论文讨论了基于树模型的边际特征归因方法，与流行的TreeSHAP算法相比，边际Shapley值在相同函数的情况下保持一致，并且介绍了如何利用树模型的内部结构计算边际特征归因。 |
| [^81] | [Average-Constrained Policy Optimization.](http://arxiv.org/abs/2302.00808) | 本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。 |
| [^82] | [Low Variance Off-policy Evaluation with State-based Importance Sampling.](http://arxiv.org/abs/2212.03932) | 本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。 |
| [^83] | [Can Brain Signals Reveal Inner Alignment with Human Languages?.](http://arxiv.org/abs/2208.06348) | 本研究探索了脑信号和人类语言之间的关系，并介绍了一种名为MTAM的方法，该方法在情感分析和关系检测等下游应用中取得了新的最先进结果。 |
| [^84] | [Adversarially Robust PAC Learnability of Real-Valued Functions.](http://arxiv.org/abs/2206.12977) | 该论文研究了在实值函数中对抗鲁棒PAC学习性，发现有限胖折射维的类既可以在实现和不可知设置中被学习，凸函数类可以正确学习，而一些非凸函数类需要不正当的学习算法。 |
| [^85] | [Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction.](http://arxiv.org/abs/2206.09821) | 本论文提出了一种基于回归的超标概率预测方法，用于预测显著波高，通过利用预测来估计超标概率，取得了更好的效果。 |
| [^86] | [Collaborative Learning for Cyberattack Detection in Blockchain Networks.](http://arxiv.org/abs/2203.11076) | 本文旨在研究入侵攻击并为区块链网络开发一种新颖的网络攻击检测框架。通过实验室合成的网络攻击数据集，提出了一种协作学习模型，允许区块链节点共享学习知识以检测攻击，并增强整个区块链网络的安全性。 |
| [^87] | [Graph Neural Network Sensitivity Under Probabilistic Error Model.](http://arxiv.org/abs/2203.07831) | 本文研究了概率误差模型对图卷积网络（GCN）性能的影响，并证明了误差模型下邻接矩阵的受限性。通过实验验证了这种误差界限，并研究了GCN在这种概率误差模型下的准确性敏感性。 |

# 详细

[^1]: 在分布变化中的监督算法公平性：一项综述

    Supervised Algorithmic Fairness in Distribution Shifts: A Survey

    [https://rss.arxiv.org/abs/2402.01327](https://rss.arxiv.org/abs/2402.01327)

    这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。

    

    在分布变化下的监督公平机器学习是一个新兴领域，解决了面对从源领域到目标领域的数据分布变化时，如何保持公平和无偏预测的挑战。在现实世界的应用中，机器学习模型通常是在特定数据集上进行训练，但在部署时，数据分布可能因各种因素而随时间发生变化。这种变化可能导致不公平的预测，对特定通过敏感属性（如种族和性别）来表征的群体产生不均衡的影响。在这项调查中，我们对各种类型的分布变化进行了总结，并全面调查了基于这些变化的现有方法，在文献中突出了六种常用的方法。此外，这份调查列出了用于实证研究的公开可用数据集和评估指标。我们进一步探讨了与相关研究领域的交互关系，并讨论了其中的重要创新和贡献。

    Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
    
[^2]: 基于能量模型的准确Shapley值估计用于可解释深度学习预测建模

    Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling

    [https://arxiv.org/abs/2404.01078](https://arxiv.org/abs/2404.01078)

    EmSHAP提出了基于能量模型的Shapley值估计方法，通过引入GRU来消除输入特征顺序的影响，从而可以有效近似任意特征子集下深度学习模型的Shapley值贡献函数。

    

    作为可解释人工智能（XAI）的有利工具，Shapley值已被广泛用于解释基于深度学习的预测模型。然而，由于计算负载随着输入特征的增加呈指数级增长，准确且高效地估计Shapley值是一项困难任务。大多数现有的加速Shapley值估计方法必须在估计准确性和效率之间做出妥协。在本文中，我们提出了EmSHAP（基于能量模型的Shapley值估计），它可以有效地近似预期Shapley贡献函数/深度学习模型在任意特征子集下给出其余特征的情况。为了确定能量模型中的提议条件分布，引入了门控循环单元（GRU），通过将输入特征映射到隐藏空间，从而消除了输入特征顺序的影响。此外，还采用了动态掩蔽方案.

    arXiv:2404.01078v1 Announce Type: new  Abstract: As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is 
    
[^3]: 机器学习鲁棒性：入门指南

    Machine Learning Robustness: A Primer

    [https://arxiv.org/abs/2404.00897](https://arxiv.org/abs/2404.00897)

    该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。

    

    这一章节探讨了机器学习（ML）中稳健性的基本概念及其在确立人工智能（AI）系统的可信度中的重要作用。讨论始于稳健性的详细定义，将其描述为ML模型在各种不同和意外的环境条件下保持稳定性能的能力。ML鲁棒性通过多个视角进行了剖析：其与泛化能力的互补性；其作为可信AI的要求；其对抗性与非对抗性方面；其数量化指标；以及其可复现性和可解释性等指标。章节深入探讨了影响鲁棒性的因素，如数据偏差、模型复杂性以及ML流程不明确的风险。它从广泛的视角调查了鲁棒性评估的关键技术，包括对抗性攻击，包括数字和物理领域。

    arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
    
[^4]: Transformer网络的拓扑结构

    The Topos of Transformer Networks

    [https://arxiv.org/abs/2403.18415](https://arxiv.org/abs/2403.18415)

    通过拓扑理论的视角，我们对Transformer架构的表达能力进行了理论分析，发现它具有高阶推理的特点，并与其他常见神经网络架构进行了对比。

    

    Transformer神经网络已经远远超越所有其他神经网络架构，成为大型语言模型背后的引擎。我们通过拓扑理论的视角提供了对Transformer架构表达能力的理论分析。从这个观点出发，我们展示了许多常见的神经网络架构，如卷积网络、循环网络和图卷积网络，可以嵌入在分段线性函数的预拓扑中，但Transformer必然存在于其拓扑完备性中。特别地，这表明这两个网络家族实例化了不同的逻辑片段：前者是一阶的，而Transformers是高阶推理机。此外，我们还将拓扑理论与架构搜索和梯度下降进行了类比，将我们的分析纳入了控制论代理的框架中。

    arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.
    
[^5]: 数学基础和完整头部姿势估计的修正

    Mathematical Foundation and Corrections for Full Range Head Pose Estimation

    [https://arxiv.org/abs/2403.18104](https://arxiv.org/abs/2403.18104)

    该论文研究了头部姿势估计中的坐标系和欧拉角顺序定义问题，验证了之前作品中使用的姿势输出的正确性和绘图例程的有效性。

    

    许多有关头部姿势估计（HPE）的作品提供了用于从面部关键点或直接从头部区域图像中提取欧拉角的算法或基于神经网络的方法。然而，许多作品未提供所使用的坐标系和欧拉或Tait-Bryan角度顺序的清晰定义。本文彻底检查了300W-LP数据集中定义的欧拉角，头部姿势估计如3DDFA-v2、6D-RepNet、WHENet等，以及它们的欧拉角绘制例程的有效性。必要时，我们推断它们的坐标系和偏航、横滚、俯仰的顺序。

    arXiv:2403.18104v1 Announce Type: cross  Abstract: Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from pro
    
[^6]: FollowIR: 评估和教授信息检索模型以遵循说明书

    FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions

    [https://arxiv.org/abs/2403.15246](https://arxiv.org/abs/2403.15246)

    该论文引入了FollowIR数据集，包含严格的说明书评估基准和训练集，帮助信息检索模型更好地遵循真实世界的说明书。议论基于TREC会议的历史，旨在使信息检索模型能够根据详细说明书理解和判断相关性。

    

    现代大型语言模型（LLMs）能够遵循长且复杂的说明书，从而实现多样化的用户任务。然而，尽管信息检索（IR）模型使用LLMs作为其架构的支柱，几乎所有这些模型仍然只接受查询作为输入，没有说明书。对于最近一些接受说明书的模型来说，它们如何使用这些说明书还不清楚。我们引入了FollowIR数据集，其中包含严格的说明书评估基准，以及一个训练集，帮助IR模型学习更好地遵循现实世界的说明书。FollowIR基于TREC会议的悠久历史：正如TREC为人类标注员提供说明书（也称为叙述）来判断文档的相关性一样，因此IR模型应该能够根据这些详细说明书理解和确定相关性。我们的评估基准从三个经过深度判断的TREC收藏开始

    arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
    
[^7]: 基于表格数据的任意时间神经网络架构搜索

    Anytime Neural Architecture Search on Tabular Data

    [https://arxiv.org/abs/2403.10318](https://arxiv.org/abs/2403.10318)

    ATLAS是第一个专为表格数据设计的任意时间神经网络架构搜索方法，引入了过滤和优化方案，结合了训练-free和基于训练的架构评估两种范式的优势

    

    随着对表格数据分析的需求增加，从手动架构设计转变为神经网络架构搜索(NAS)。这种转变需要一种高效且响应灵敏的任意时间NAS方法，能够在任何给定的时间预算内返回当前的最佳架构，并随着预算分配的增加逐渐提高架构质量。然而，关于表格数据的任意时间NAS领域仍未被探索。为此，我们引入了ATLAS，第一个专为表格数据量身定制的任意时间NAS方法。ATLAS引入了一种新颖的两阶段过滤和优化方案，结合了训练-free和基于训练的架构评估两种范式的优势。具体来说，在过滤阶段，ATLAS采用了一种新的为表格数据专门设计的零成本代理，用于高效估计候选架构的性能。

    arXiv:2403.10318v1 Announce Type: new  Abstract: The increasing demand for tabular data analysis calls for transitioning from manual architecture design to Neural Architecture Search (NAS). This transition demands an efficient and responsive anytime NAS approach that is capable of returning current optimal architectures within any given time budget while progressively enhancing architecture quality with increased budget allocation. However, the area of research on Anytime NAS for tabular data remains unexplored. To this end, we introduce ATLAS, the first anytime NAS approach tailored for tabular data. ATLAS introduces a novel two-phase filtering-and-refinement optimization scheme with joint optimization, combining the strengths of both paradigms of training-free and training-based architecture evaluation. Specifically, in the filtering phase, ATLAS employs a new zero-cost proxy specifically designed for tabular data to efficiently estimate the performance of candidate architectures, th
    
[^8]: 走向模型蒸馏理论

    Towards a theory of model distillation

    [https://arxiv.org/abs/2403.09053](https://arxiv.org/abs/2403.09053)

    提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。

    

    蒸馏是将复杂的机器学习模型替换为简化模型来近似原模型的任务。尽管有许多实际应用，关于模型蒸馏的程度、所需运行时间和数据量的基本问题仍然大多未解。为了研究这些问题，我们开始了蒸馏的一般理论，以类似的方式定义了PAC-蒸馏 [Val84]，提出了提取训练权重中存储的知识的新算法，展示了如何通过使用“线性表示假设”将神经网络高效地蒸馏成简明明了的决策树表示，还证明了蒸馏可以比从头开始学习便宜得多，并在表征其复杂性方面取得了进展。

    arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
    
[^9]: 基于多模态学习和测试时临床知识增强的零样本心电图分类

    Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement

    [https://arxiv.org/abs/2403.06659](https://arxiv.org/abs/2403.06659)

    通过Multimodal ECG Representation Learning (MERL)框架，本文提出了一种零样本心电图分类方法，结合了对ECG记录和相关报告的多模态学习，同时在测试阶段使用了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法来利用临床知识数据库。

    

    心电图（ECG）是临床实践中用于检测心律失常疾病的非侵入性诊断工具。在未经注释的ECG数据中进行自监督学习（eSSL）方法显示出了表征学习的潜力，但往往忽视了可以在报告中找到的临床知识。本文通过多模态学习ECG记录和相关报告，提出了Multimodal ECG Representation Learning (MERL)框架，该框架能够使用文本提示进行零样本ECG分类，消除了下游任务中对训练数据的需求。在测试时，我们提出了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法，利用大型语言模型（LLM）来利用外部专家验证的临床知识数据库，生成更多关于患者病史的提示。

    arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
    
[^10]: 朝着联邦增量学习中高效的重播

    Towards Efficient Replay in Federated Incremental Learning

    [https://arxiv.org/abs/2403.05890](https://arxiv.org/abs/2403.05890)

    本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。

    

    在联邦学习（FL）中，通常假定每个客户端的数据是固定或静态的。然而，在现实世界的应用中，数据通常以增量方式到来，其中数据领域可能动态增加。在这项工作中，我们研究了在边缘客户端在联邦增量学习（FIL）场景中因数据异构性而可能缺乏足够存储空间以保留完整数据的灾难性遗忘。我们提出了一种名为Re-Fed的简单、通用的FIL框架，它可以协调每个客户端缓存重播的重要样本。具体而言，当出现新任务时，每个客户端首先基于它们的全局和本地重要性缓存选定的先前样本。然后，客户端使用既缓存的样本又使用新任务的样本训练本地模型。在理论上，我们分析了Re-Fed发现重播重要样本的能力，从而缓解了灾难性遗忘问题。

    arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e
    
[^11]: 最近大型视觉-语言模型的有效性评估

    Effectiveness Assessment of Recent Large Vision-Language Models

    [https://arxiv.org/abs/2403.04306](https://arxiv.org/abs/2403.04306)

    本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。

    

    大型视觉-语言模型(LVLMs)的出现代表着迈向人工通用智能的重要进步。然而，它们在专业和通用任务中的有效性程度需要进一步调查。本文旨在评估流行的LVLMs在专业和通用任务中的能力，旨在提供对这些创新方法的全面理解。为了评估它们在专业任务中的有效性，我们量身定制了一个包含自然、医疗和工业三种不同场景的全面测试平台，涵盖六项具有挑战性的任务。这些任务包括显著、伪装和透明物体检测，以及息肉和皮肤病变检测，以及工业异常检测。我们检验了最近三种开源LVLMs--MiniGPT-v2、LLaVA-1.5和Shikra--在视觉识别和定位领域的表现。

    arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
    
[^12]: SWAP-NAS: 适用于超快速NAS的样本级激活模式

    SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS

    [https://arxiv.org/abs/2403.04161](https://arxiv.org/abs/2403.04161)

    提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。

    

    无需训练的度量（即零成本代理）被广泛用于避免资源密集型的神经网络训练，尤其是在神经结构搜索（NAS）中。最近的研究表明，现有的无需训练的度量存在一些局限，比如在不同搜索空间和任务之间存在有限的关联性和差劲的泛化能力。因此，我们提出了样本级激活模式及其衍生物SWAP-Score，这是一种新颖的高性能无需训练的度量。它测量了网络在一批输入样本上的表现能力。SWAP-Score与不同搜索空间和任务中的真实性能强相关，在NAS-Bench-101/201/301和TransNAS-Bench-101上胜过了15种现有的无需训练的度量。SWAP-Score可以通过正则化进一步增强，这在基于单元的搜索空间中可以实现更高的相关性，并且在搜索过程中实现模型大小控制。例如，Spearman的排序

    arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
    
[^13]: 自主驾驶的世界模型：一项初步调查

    World Models for Autonomous Driving: An Initial Survey

    [https://arxiv.org/abs/2403.02622](https://arxiv.org/abs/2403.02622)

    世界模型在自主驾驶领域的重要性和作用，是通过准确预测未来事件和评估其影响来帮助决策过程，从而推动自主驾驶技术发展的革命性方法。

    

    在自主驾驶领域不断发展的背景下，准确预测未来事件并评估其影响对于安全和效率至关重要，关键地帮助决策过程。世界模型已经成为一种革命性方法，使自主驾驶系统能够综合和解释大量传感器数据，从而预测潜在的未来情景并弥补信息缺口。本文对自主驾驶中世界模型的当前状态和未来发展进行了初步审查，涵盖了其理论基础、实际应用以及旨在克服现有限制的正在进行的研究工作。强调了世界模型在推动自主驾驶技术发展中的重要作用，本调查旨在成为研究社区的基础参考，便于快速获得和应用。

    arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
    
[^14]: 关于量化大型语言模型的可压缩性

    On the Compressibility of Quantized Large Language Models

    [https://arxiv.org/abs/2403.01384](https://arxiv.org/abs/2403.01384)

    研究在内存受限设备上应用数据压缩技术以加速量化LLM推理过程的一项初步工作。

    

    部署大型语言模型（LLMs）到边缘或移动设备上具有显著优势，如增强数据隐私和实时处理能力。本文研究了将数据压缩技术应用于减少数据移动，从而加速内存受限设备上量化LLM的推理过程的初步步骤。

    arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
    
[^15]: Epsilon-Greedy Thompson Sampling用于贝叶斯优化

    Epsilon-Greedy Thompson Sampling to Bayesian Optimization

    [https://arxiv.org/abs/2403.00540](https://arxiv.org/abs/2403.00540)

    将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。

    

    Thompson采样（TS）被认为是解决贝叶斯优化中开发-探索困境的解决方案。 虽然它通过随机生成和最大化高斯过程（GP）后验的样本路径来优先进行探索，但TS在每次执行探索后通过收集关于真实目标函数的信息来弱化其开发功能。 本研究将在TS中引入$\varepsilon$-greedy策略，这是一种在强化学习中被广泛应用的选择策略，以改进其开发功能。 我们首先描述了TS应用于BO的两个极端，即通用TS和样本平均TS。前者和后者分别提倡探索和开发。 然后我们使用$\varepsilon$-greedy策略在两个极端之间随机切换。 $\varepsilon \in (0,1)$的小值优先考虑开发，反之亦然。 我们实证表明$\varepsilon$-greedy T

    arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
    
[^16]: 硅谷人群的智慧：LLM集成预测能力达到人群准确率水平

    Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy

    [https://arxiv.org/abs/2402.19379](https://arxiv.org/abs/2402.19379)

    该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。

    

    实践中人类预测准确性依赖于“群体智慧”效应，即通过聚合一群个体预测者的预测可以显著提高对未来事件的预测。过去关于大型语言模型（LLMs）预测能力的研究表明，作为个体预测者的前沿LLMs表现不佳，与人类群体预测比赛的黄金标准相比。我们通过使用一个由十二个LLMs组成的LLM集成方法，扩展了研究。我们将31个二元问题的聚合LLM预测与一个来自三个月预测比赛的925名人类预测者的群体预测进行比较。我们的主要分析表明，LLM群体的表现优于简单的无信息基准，并在统计上等效于人类群体。我们还观察到一种顺从效应，平均模型预测明显高于50%，尽管几乎是平等的。

    arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
    
[^17]: 如何逐步思考：对思维链推理的机械理解

    How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning

    [https://arxiv.org/abs/2402.18312](https://arxiv.org/abs/2402.18312)

    通过对LLMs进行机械性研究，我们发现模型在进行逐步推理时使用多个并行路径生成答案，同时存在功能性分歧。

    

    尽管大型语言模型（LLMs）展示了出色的推理能力，通过思维链（CoT）提示，但对于促进CoT生成的模型内部机制仍存在缺乏理解的问题。本文从机械性的角度研究了LLMs中表现出CoT推理的神经子结构。通过对LLaMA-2 7B应用于虚构本体论的多步推理的分析，我们展示了LLMs为逐步推理部署了多个并行答案生成路径。这些并行路径提供了来自输入问题上下文以及生成的CoT的序贯答案。我们观察到LLMs中间层存在引人瞩目的功能分歧。初始一半的令牌表示仍然强烈偏向预训练先验，而后半部分突然被上下文所取代。这种内部相位转变在不同的功能协同中体现出来。

    arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co
    
[^18]: 行动胜过言辞：用于生成推荐的千亿参数顺序转导器

    Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations

    [https://arxiv.org/abs/2402.17152](https://arxiv.org/abs/2402.17152)

    提出了HSTU架构，用于高基数、非平稳流推荐数据，性能优于基线方法高达65.8％的NDCG，并且比基于FlashAttention2的Transformer在8192长度序列上快5.3倍到15.2倍。

    

    大规模推荐系统的特点是依赖于高基数、异构特征，并且需要每天处理数十亿用户行为。尽管在成千上万个特征上训练了大量数据，但大多数行业中的深度学习推荐模型(DLRMs)在计算方面无法扩展。受到在语言和视觉领域取得成功的Transformer的启发，我们重新审视了推荐系统中的基本设计选择。我们将推荐问题重新构建为生成建模框架中的顺序转导任务（“生成推荐者”），并提出了一种针对高基数、非平稳流推荐数据设计的新架构HSTU。

    arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
    
[^19]: REPLAY: 对稀疏轨迹进行位置预测的人类移动时间变化规律建模

    REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories

    [https://arxiv.org/abs/2402.16310](https://arxiv.org/abs/2402.16310)

    该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。

    

    位置预测是根据历史用户移动轨迹来预测用户位置的技术。为了应对真实世界用户移动轨迹的固有稀疏问题，时空上下文被证明是非常有用的。现有的解决方案主要是将位置之间的时空距离纳入到移动轨迹中，要么通过将其作为附加输入提供给递归神经网络（RNNs），要么通过利用它们来寻找有信息的过去隐藏状态进行预测。然而，这种基于距离的方法未能捕捉人类移动的时间变化规律，例如，人类移动在早晨通常比其他时间更有规律；这暗示了实际时间戳的有用性。基于这一背景，我们提出了REPLAY，是一种通用的RNN架构，旨在捕捉时间变化的人类移动时间规律以进行位置预测。

    arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
    
[^20]: 任意精度LLM：多个不同大小LLM的低成本部署

    Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs

    [https://arxiv.org/abs/2402.10517](https://arxiv.org/abs/2402.10517)

    任意精度LLM引入了一种轻量级方法，通过将不同大小LLMs量化为不同位宽（如3、4、...，n位）并叠加到内存中，显着降低了部署多个不同大小LLMs的高成本

    

    最近，人们对压缩大型语言模型（LLMs）进行了相当多的努力，这些LLMs在各种应用中展示了突破性的能力，但由于其庞大的体积而导致部署成本高昂。与此同时，尽管多个不同大小的LLMs部署的成本在实际意义上很重要，但却受到的关注较少。因此，本文引入了“任意精度LLM”，将任意精度DNN的概念扩展到LLMs。解决了任意精度LLM中的挑战，我们提出了一种轻量级的LLMs任意精度量化方法，利用后训练量化框架，并开发了一个专门的软件引擎来实现其有效的服务。结果，我们的解决方案通过将以不同位宽（如3、4、…，n位）量化的LLMs叠加到内存足印中，显着降低了部署多个不同大小的LLMs的高成本。

    arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri
    
[^21]: 基于表情符号的加密资产市场反应

    Emoji Driven Crypto Assets Market Reactions

    [https://arxiv.org/abs/2402.10481](https://arxiv.org/abs/2402.10481)

    该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。

    

    在加密货币领域，诸如Twitter之类的社交媒体平台已经成为影响市场趋势和投资者情绪的关键因素。在我们的研究中，我们利用GPT-4和经过微调的基于BERT模型的多模态情感分析，重点关注表情符号情绪对加密货币市场的影响。通过将表情符号转化为可量化的情感数据，我们将这些见解与BTC价格和VCRIX指数等关键市场指标进行了相关联。这种方法可以用于开发旨在利用社交媒体元素识别和预测市场趋势的交易策略。关键是，我们的研究结果表明，基于表情符号情绪的策略可以有助于避免重大市场下挫，并有助于回报的稳定。这项研究强调了将先进的基于人工智能的分析整合到金融策略中的实际益处，并提供了一种新的方式来看待市场预测。

    arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
    
[^22]: 解决图上的负迁移问题

    Tackling Negative Transfer on Graphs

    [https://arxiv.org/abs/2402.08907](https://arxiv.org/abs/2402.08907)

    图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。

    

    迁移学习旨在通过利用从其他相关任务中学到的知识来提高目标任务上的学习效果。然而，当源任务和目标任务之间关系不密切时，学习性能可能会受到不利影响，这种现象被称为负迁移。本文研究了在图迁移学习中的负迁移问题，这是一个重要但尚未深入研究的领域。我们发现，在图结构数据中，与图像或文本不同，负迁移经常发生，即使源图和目标图在语义上有相似之处。具体来说，我们发现结构差异会大大增强图中节点嵌入之间的差异。为了缓解这个问题，我们带来了一个新的观点：对于语义相似的图，尽管结构差异会导致节点嵌入的分布差异，但它们对子图嵌入的影响可能较小。基于这个观点，我们引入了tw

    arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
    
[^23]: 高斯模型集成置信传播用于高维系统中的高效推断

    Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems

    [https://arxiv.org/abs/2402.08193](https://arxiv.org/abs/2402.08193)

    高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。

    

    高维模型中的高效推断仍然是机器学习中的一个核心挑战。本文介绍了一种名为高斯模型集成置信传播（GEnBP）算法的方法，该方法是集成卡尔曼滤波器和高斯置信传播（GaBP）方法的结合。GEnBP通过在图模型结构中传递低秩本地信息来更新集成模型。这种组合继承了每种方法的有利特性。集成技术使得GEnBP能够处理高维状态、参数和复杂的、嘈杂的黑箱生成过程。在图模型结构中使用本地信息确保了该方法适用于分布式计算，并能高效地处理复杂的依赖结构。当集成大小远小于推断维度时，GEnBP特别有优势。这种情况在空时建模、图像处理和物理模型反演等领域经常出现。GEnBP可以应用于一般性问题。

    Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
    
[^24]: 多任务策略学习中基于任务条件的视觉特征自适应

    Task-conditioned adaptation of visual features in multi-task policy learning

    [https://arxiv.org/abs/2402.07739](https://arxiv.org/abs/2402.07739)

    本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。

    

    成功地解决各种任务是自主代理的核心能力，这需要灵活地调整底层的决策策略，并且如我们在这项工作中所提出的，还需要调整底层的感知模块。一个类比的论证是人类的视觉系统，它使用自上而下的信号来专注于当前任务。类似地，在这项工作中，我们在多任务策略学习的上下文中，通过特定的下游任务来调整预先训练的大视觉模型。我们引入了基于任务条件的适配器，在不需要微调任何预先训练权重的情况下，与通过行为克隆训练的单一策略结合使用，能够解决多个任务。我们在策略和视觉适配器上根据任务嵌入进行条件化，如果任务是已知的，则可以在推理过程中选择任务嵌入，否则可以从一组示例演示中进行推断。为此，我们提出了一种新的基于优化的估计器。我们在...（摘要未完成）

    Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
    
[^25]: 通过利用分类数据集和其语义层次，开展视觉语言模型的开放式VQA评估

    Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy

    [https://arxiv.org/abs/2402.07270](https://arxiv.org/abs/2402.07270)

    该研究通过提出创新的评估方法和基于分类数据集的新型VQA基准，推动了对文本生成的视觉语言模型能力的理解。同时，他们还提出了使用语义层次和自动生成的后续问题来改进对细粒度分类任务上粗糙答案的评估。通过比较不同度量标准，他们在进行人工评估研究的基础上选择了最终的度量标准。

    

    评估文本生成的视觉语言模型是一项具有挑战性但至关重要的工作。通过解决现有视觉问答（VQA）基准的局限性并提出创新的评估方法，我们的研究旨在推动我们对这些模型能力的理解。我们提出了一种基于知名视觉分类数据集的新型VQA基准，可以对文本生成的视觉语言模型进行细粒度评估，并与判别性视觉语言模型进行比较。为了改善对细粒度分类任务上粗糙答案的评估，我们建议使用标签空间的语义层次来提出关于基准类别的自动生成的后续问题。最后，我们比较了传统的自然语言处理和基于LLM的度量标准来评估给定基准答案的模型预测问题。我们进行了人工评估研究，基于此决定最终度量标准的选择。

    The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be
    
[^26]: SpongeNet 攻击：深度神经网络的海绵权重中毒

    The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks

    [https://arxiv.org/abs/2402.06357](https://arxiv.org/abs/2402.06357)

    本文提出了一种名为 SpongeNet 的新型海绵攻击，通过直接作用于预训练模型参数，成功增加了视觉模型的能耗，而且所需的样本数量更少。

    

    海绵攻击旨在增加在硬件加速器上部署的神经网络的能耗和计算时间。现有的海绵攻击可以通过海绵示例进行推理，也可以通过海绵中毒在训练过程中进行。海绵示例利用添加到模型输入的扰动来增加能量和延迟，而海绵中毒则改变模型的目标函数来引发推理时的能量/延迟效应。在这项工作中，我们提出了一种新颖的海绵攻击，称为 SpongeNet。SpongeNet 是第一个直接作用于预训练模型参数的海绵攻击。我们的实验表明，相比于海绵中毒，SpongeNet 可以成功增加视觉模型的能耗，并且所需的样本数量更少。我们的实验结果表明，如果不专门针对海绵中毒进行调整（即减小批归一化偏差值），则毒害防御会失效。我们的工作显示出海绵攻击的影响。

    Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
    
[^27]: 在解空间中加速PDE数据生成的微分算子作用

    Accelerating PDE Data Generation via Differential Operator Action in Solution Space

    [https://arxiv.org/abs/2402.05957](https://arxiv.org/abs/2402.05957)

    通过在解空间中应用微分算子作用，我们提出了一种加速PDE数据生成的算法，名为DiffOAS。它能够在生成数据的同时提高数据的精度，并且在时间复杂度上比现有的方法更高效。

    

    最近数据驱动方法（如神经算子）在减少偏微分方程（PDEs）求解时间方面取得了显著进展。然而，这些方法面临的主要挑战之一是需要大量高精度的训练数据，在生成过程中需要显著的计算成本。为了解决这个挑战，我们提出了一种新颖的PDE数据集生成算法，即解空间中的微分算子作用（DiffOAS），它同时加快了数据生成过程并提高了生成数据的精度。具体而言，DiffOAS获取了几个基本的PDE解，并将它们组合以获得解。它对这些解应用微分算子，我们称之为“算子作用”，以高效生成精确的PDE数据点。理论分析表明，DiffOAS方法的时间复杂度比现有的生成方法低一个数量级。

    Recent advancements in data-driven approaches, such as Neural Operator (NO), have demonstrated their effectiveness in reducing the solving time of Partial Differential Equations (PDEs). However, one major challenge faced by these approaches is the requirement for a large amount of high-precision training data, which needs significant computational costs during the generation process. To address this challenge, we propose a novel PDE dataset generation algorithm, namely Differential Operator Action in Solution space (DiffOAS), which speeds up the data generation process and enhances the precision of the generated data simultaneously. Specifically, DiffOAS obtains a few basic PDE solutions and then combines them to get solutions. It applies differential operators on these solutions, a process we call 'operator action', to efficiently generate precise PDE data points. Theoretical analysis shows that the time complexity of DiffOAS method is one order lower than the existing generation meth
    
[^28]: 生成带有病人时间轴的电子健康记录的CEHR-GPT

    CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines

    [https://arxiv.org/abs/2402.04400](https://arxiv.org/abs/2402.04400)

    CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。

    

    合成电子健康记录（EHR）已成为推进医疗应用和机器学习模型的关键工具，特别是对于没有直接访问医疗数据的研究人员而言。尽管现有方法，如基于规则的方法和生成对抗网络（GAN），可以生成类似真实世界EHR数据的合成数据，但这些方法常常使用表格格式，忽略了病人历史的时间依赖性，限制了数据复制。最近，越来越多的人开始利用生成预训练转换器（GPT）来处理EHR数据。这使得可以进行疾病进展分析、人口估计、反事实推理和合成数据生成等应用。本研究关注合成数据生成，并演示了使用源自CEHR-BERT的特定病人表示训练GPT模型的能力，从而能够生成可无缝转换的病人序列。

    Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
    
[^29]: 通过重复使用经过验证的电路增加语言模型的可信度

    Increasing Trust in Language Models through the Reuse of Verified Circuits

    [https://arxiv.org/abs/2402.02619](https://arxiv.org/abs/2402.02619)

    本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。

    

    语言模型（LMs）在各种预测任务中的应用越来越广泛，但它们的训练经常忽略罕见的边界情况，降低了它们的可靠性。在本文中，我们定义了一个严格的可信度标准，即任务算法和电路实现必须经过验证，考虑到边界情况，并且没有已知的故障模式。我们展示了通过使用数学和逻辑规范的框架来构建变压器模型，可以训练出满足这一标准的模型。在本文中，我们对一个n位整数加法模型进行了完全验证。为了展示经过验证的模块的重复使用性，我们将训练好的整数加法模型插入到一个未经训练的模型中，并训练组合模型同时执行加法和减法。我们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了更复杂的减法模型的验证。我们讨论了如何将经过验证的任务模块插入到语言模型中，以利用模型的重复使用来提高可验证性和可信度。

    Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
    
[^30]: 大型语言模型用于网络的适应性

    Large Language Model Adaptation for Networking

    [https://arxiv.org/abs/2402.02338](https://arxiv.org/abs/2402.02338)

    本文首次研究了使用大型语言模型（LLM）适应网络问题的方法，通过利用LLM的预训练知识和强大推理能力，实现了“一模型适用于所有”的目标，并取得了更好的性能和更强的泛化能力。

    

    现在许多网络任务都使用深度学习（DL）来解决复杂的预测和系统优化问题。然而，目前基于DL的算法的设计哲学需要进行大量的工程开销，因为需要为不同的网络任务手动设计深度神经网络（DNN）。此外，DNN在未见过的数据分布/环境上的泛化性能较差。在大型语言模型（LLM）的最新成功的推动下，本文首次研究了LLM用于网络的适应性，以探索更可持续的设计哲学。凭借海量的预训练知识和强大的推理能力，LLM可以作为基础模型，并且有望在各种任务中实现“一模型适用于所有”，并具有更好的性能和更强的泛化能力。在本文中，我们提出了NetLLM，这是第一个有效地将LLM应用于解决网络问题的适应性框架。NetLLM解决了许多实际挑战。

    Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve "one model for all" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in L
    
[^31]: 大规模视觉-语言模型中的幻觉调查

    A Survey on Hallucination in Large Vision-Language Models

    [https://arxiv.org/abs/2402.00253](https://arxiv.org/abs/2402.00253)

    这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。

    

    大规模视觉-语言模型（LVLMs）的发展引起了人工智能领域越来越多的关注，因为它具有实际的实施潜力。然而，“幻觉”，或者更具体地说，即视觉内容与相应文本生成之间的不一致，在利用LVLMs方面提出了重大挑战。在这份综合调查中，我们对LVLM相关的幻觉进行了深入剖析，旨在建立一个概览并促进未来的缓解。我们首先澄清了LVLMs中幻觉概念，呈现了各种幻觉症状，并强调了LVLM幻觉固有的独特挑战。随后，我们概述了专门用于评估LVLM独特幻觉的基准和方法论。此外，我们深入调查了这些幻觉的根本原因，包括来自训练数据和模型组件的见解。我们还对现有的幻觉缓解方法进行了批判性的回顾。

    Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
    
[^32]: 不带位置编码的图变压器

    Graph Transformers without Positional Encodings

    [https://arxiv.org/abs/2401.17791](https://arxiv.org/abs/2401.17791)

    本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。

    

    最近，用于图表示学习的变压器越来越受欢迎，在各种数据集上取得了最先进的性能，无论是单独使用还是与消息传递图神经网络（MP-GNN）结合。将图归纳偏见融入天然与结构无关的变压器架构中，以结构或位置编码（PEs）的形式，是实现这些令人印象深刻的结果的关键。然而，设计这样的编码是棘手的，人们已经提出了不同的尝试来设计这样的编码，包括拉普拉斯特征向量、相对随机行走概率（RRWP）、空间编码、中心度编码、边缘编码等。在这项工作中，我们认为这些编码可能根本不需要，只要注意机制本身包含有关图结构的信息。我们介绍了Eigenformer，它使用一种新颖的谱感知注意机制，了解图的拉普拉斯谱，并通过实验证明

    Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
    
[^33]: 在医疗AI模型中检测算法偏倚

    Detecting algorithmic bias in medical AI-models

    [https://arxiv.org/abs/2312.02959](https://arxiv.org/abs/2312.02959)

    本文提出了一种创新的框架，用于检测医疗AI决策支持系统中的算法偏倚，通过采用CART算法有效地识别医疗AI模型中的潜在偏倚，并在合成数据实验和真实临床环境中验证了其有效性。

    

    随着机器学习和人工智能医疗决策支持系统日益普及，确保这些系统以公平、公正的方式提供患者结果变得同样重要。本文提出了一种创新的框架，用于检测医疗AI决策支持系统中的算法偏倚区域。我们的方法通过采用分类与回归树（CART）算法，在脓毒症预测背景下有效地识别医疗AI模型中的潜在偏倚。我们通过进行一系列合成数据实验验证了我们的方法，展示了其在受控环境中准确估计偏倚区域的能力。这一概念的有效性通过使用亚特兰大乔治亚州格雷迪纪念医院的电子病历进行实验进一步得到验证。这些测试展示了我们策略在临床中的实际应用。

    arXiv:2312.02959v3 Announce Type: replace-cross  Abstract: With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clini
    
[^34]: 模仿引导式强化学习

    Imitation Bootstrapped Reinforcement Learning

    [https://arxiv.org/abs/2311.02198](https://arxiv.org/abs/2311.02198)

    提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。

    

    尽管强化学习（RL）具有相当大的潜力，但机器人控制任务主要依赖模仿学习（IL）是因为其更好的样本效率。然而，收集能使IL推广到所有可能场景的全面专家演示是昂贵的，任何分布的转变都需要重新收集数据进行微调。因此，如果RL可以建立在IL的基础上作为一种高效的自我改进程序，那么它将具有吸引力。我们提出了一种模仿引导式强化学习（IBRL）的新框架，用于具有示范的高效抽样RL，首先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。与先前过度采样示范或用额外的模仿损失对RL进行正则化的工作相比，IBRL能够利用来自IL的高质量动作。

    arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
    
[^35]: 对抗样本不是真正的特征

    Adversarial Examples Are Not Real Features

    [https://arxiv.org/abs/2310.18936](https://arxiv.org/abs/2310.18936)

    该论文从多个学习范式的角度重新审视对抗样本的理论，发现非鲁棒特征在多种无监督学习范式中的效用较差，揭示了这些特征并不像鲁棒特征或自然特征那样真正有用。

    

    对抗样本的存在多年来一直是一个谜团，并引起了广泛的关注。一种由Ilyas等人提出的著名理论从数据的角度解释了对抗性脆弱性，即通过展示可以从对抗样本中提取非鲁棒特征，并且这些特征单独用于分类是有用的。然而，这种解释仍然相当反直觉，因为非鲁棒特征对于人类来说大部分是噪声特征。在本文中，我们从更大的背景下重新审视了这个理论，结合了多个学习范式。值得注意的是，我们发现与在监督学习中的良好效用相反，当将非鲁棒特征转移到其他无监督学习范式时（如对比学习、遮挡图像建模和扩散模型），它们的效用变差。这揭示了非鲁棒特征并不像在这些范式之间享有良好可迁移性的鲁棒特征或自然特征那样真正有用。同时，对于鲁棒特征而言，在自监督学习中的效用与监督学习中的效用相当。

    The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robus
    
[^36]: Outlier Weighed Layerwise Sparsity (OWL): 为剪枝LLMs达到高稀疏度提供缺失的秘密调味料

    Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity

    [https://arxiv.org/abs/2310.05175](https://arxiv.org/abs/2310.05175)

    本研究发现LLMs中的激活异常值与网络层稀疏度的非均匀性相关，并提出了Outlier Weighed Layerwise Sparsity（OWL）作为剪枝LLMs到高稀疏度的秘密调味料。

    

    大型语言模型（LLMs）以在各个领域展现出的卓越性能而闻名，在实际部署时由于模型庞大而面临挑战。为了解决这一挑战，人们努力将传统的网络剪枝技术应用于LLMs，发现可以在不影响性能的情况下一次性剪掉大量参数。现有的LLM剪枝策略一直坚持以等价稀疏度均匀剪裁所有层的做法，结果表现强劲。然而，这个观察结果与在视觉模型领域观察到的非均匀逐层稀疏的主流趋势相矛盾，后者通常会产生更好的结果。为了了解这种差异背后的原因，我们进行了全面研究，并发现与LLMs中异常值的出现强相关。

    arXiv:2310.05175v2 Announce Type: replace  Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Ins
    
[^37]: 将子图转化为节点让简单的图神经网络在子图表示学习上更强大和高效

    Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning

    [https://arxiv.org/abs/2204.04510](https://arxiv.org/abs/2204.04510)

    提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。

    

    子图表示学习已经成为一个重要的问题，并且通常使用专门的图神经网络来处理大型全局图。这些模型需要大量的内存和计算资源，但挑战子图的层次结构建模。在本文中，我们提出了子图到节点（S2N）转换的新颖公式，用于学习子图的表示。具体而言，给定全局图中的一组子图，我们通过粗略地将子图转换成节点来构建一个新的图。通过理论和实证证据，S2N不仅相比最先进的模型显著减少了内存和计算成本，而且通过捕捉子图的局部和全局结构也在性能上超过了它们。通过利用图粗化方法，我们的方法甚至在数据稀缺的情况下也优于基线模型。我们在八个基准测试上的实验表明，调整模型后效果出色。

    Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
    
[^38]: 提示设计与工程：介绍与高级方法

    Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])

    [http://arxiv.org/abs/2401.14423](http://arxiv.org/abs/2401.14423)

    本文介绍了提示设计与工程的主要概念，并回顾了基本和更高级的方法。

    

    提示设计与工程在过去几个月中成为了一个重要的学科。在本文中，我们介绍了主要概念，并回顾了提示设计与工程的基本和更高级的方法。

    Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
    
[^39]: 鲁棒的多模态密度估计

    Robust Multi-Modal Density Estimation. (arXiv:2401.10566v1 [cs.LG])

    [http://arxiv.org/abs/2401.10566](http://arxiv.org/abs/2401.10566)

    本文提出了一种名为ROME的鲁棒多模态密度估计方法，该方法利用聚类将多模态样本集分割成多个单模态样本集，并通过简单的KDE估计来估计整体分布。这种方法解决了多模态、非正态和高相关分布估计的挑战。

    

    多模态概率预测模型的发展引发了对综合评估指标的需求。虽然有几个指标可以表征机器学习模型的准确性（例如，负对数似然、Jensen-Shannon散度），但这些指标通常作用于概率密度上。因此，将它们应用于纯粹基于样本的预测模型需要估计底层密度函数。然而，常见的方法如核密度估计（KDE）已被证明在鲁棒性方面存在不足，而更复杂的方法在多模态估计问题中尚未得到评估。在本文中，我们提出了一种非参数的密度估计方法ROME（RObust Multi-modal density Estimator），它解决了估计多模态、非正态和高相关分布的挑战。ROME利用聚类将多模态样本集分割成多个单模态样本集，然后结合简单的KDE估计来得到总体的估计结果。

    Development of multi-modal, probabilistic prediction models has lead to a need for comprehensive evaluation metrics. While several metrics can characterize the accuracy of machine-learned models (e.g., negative log-likelihood, Jensen-Shannon divergence), these metrics typically operate on probability densities. Applying them to purely sample-based prediction models thus requires that the underlying density function is estimated. However, common methods such as kernel density estimation (KDE) have been demonstrated to lack robustness, while more complex methods have not been evaluated in multi-modal estimation problems. In this paper, we present ROME (RObust Multi-modal density Estimator), a non-parametric approach for density estimation which addresses the challenge of estimating multi-modal, non-normal, and highly correlated distributions. ROME utilizes clustering to segment a multi-modal set of samples into multiple uni-modal ones and then combines simple KDE estimates obtained for i
    
[^40]: DiffClone: 使用扩散驱动的策略学习增强机器人行为克隆

    DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])

    [http://arxiv.org/abs/2401.09243](http://arxiv.org/abs/2401.09243)

    本文介绍了DiffClone，一种通过扩散驱动的策略学习增强行为克隆代理的离线算法。在真实的在线物理机器人上的实验表明，采用MOCO微调的ResNet50的效果最好。

    

    机器人学习任务在计算上非常密集且硬件特定。因此，通过使用多样化的离线演示数据集来训练机器人操作代理，来应对这些挑战的方式非常吸引人。Train-Offline-Test-Online（TOTO）基准提供了一个经过精心策划的开源离线训练数据集，主要由专家数据组成，并提供了常见离线强化学习和行为克隆代理的基准分数。在本文中，我们介绍了DiffClone，一种增强行为克隆代理的离线算法，采用基于扩散的策略学习，并在测试时在真实的在线物理机器人上评估了我们的方法的有效性。同时，这也是我们在NeurIPS 2023举办的Train-Offline-Test-Online（TOTO）基准挑战赛中的官方提交。我们尝试了预训练的视觉表示和代理策略。在实验中，我们发现MOCO微调的ResNet50相比其他微调方法表现最好。

    Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
    
[^41]: 时间中的涟漪：美国历史中的不连续性

    A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01185](http://arxiv.org/abs/2312.01185)

    该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。

    

    在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\% - 95\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。

    In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
    
[^42]: 自监督表示学习的线性分离能力研究

    On Linear Separation Capacity of Self-Supervised Representation Learning. (arXiv:2310.19041v1 [stat.ML])

    [http://arxiv.org/abs/2310.19041](http://arxiv.org/abs/2310.19041)

    本研究通过探究在多流形模型下，学习的表示何时可以线性分离流形，揭示了自监督学习在数据增强方面的额外好处，从而改善了线性分离能力的信息论最优速率。

    

    自监督学习的最新进展强调了数据增强在从无标签数据中学习数据表示中的有效性。在这些增强表示之上训练线性模型可以得到一个熟练的分类器。尽管在实践中表现出色，但是数据增强如何将非线性数据结构解开为线性可分离表示的机制仍然不清楚。本文旨在通过研究在从多流形模型中绘制数据时，学习到的表示在何种条件下可以线性分离流形来填补这一差距。我们的研究揭示了数据增强除了提供观察数据外，还提供了额外的信息，从而可以改善线性分离容量的信息论最优速率。特别是，我们证明自监督学习可以以比无监督学习更小的距离线性分离流形，突显了数据增强的额外好处。

    Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. 
    
[^43]: 语言模型如何将实体绑定到上下文中?

    How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])

    [http://arxiv.org/abs/2310.17191](http://arxiv.org/abs/2310.17191)

    通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。

    

    为了正确使用上下文信息，语言模型（LMs）必须将实体与其属性进行绑定。例如，给定描述“绿色方块”和“蓝色圆形”的上下文，LMs必须将形状与它们对应的颜色进行绑定。我们分析LM表示并确定绑定ID机制：这是一种解决绑定问题的通用机制，我们在Pythia和LLaMA家族的每个足够大的模型中观察到。通过因果干预，我们展示了LMs内部激活通过将绑定ID向量附加到相应的实体和属性上来表示绑定信息。我们进一步展示了绑定ID向量形成连续的子空间，在这个子空间中，绑定ID向量之间的距离反映了它们的区别。总体而言，我们的结果揭示了LMs在上下文中表示符号知识的可解释策略，为理解大规模LMs中的一般上下文推理迈出了一步。

    To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
    
[^44]: KirchhoffNet：一种连接消息传递和连续深度模型的电路桥接神经网络

    KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])

    [http://arxiv.org/abs/2310.15872](http://arxiv.org/abs/2310.15872)

    本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。

    

    在本文中，我们利用了模拟电路的基本原理基赫霍夫电流定律，引入了一类独特的神经网络模型，称为基赫霍夫网络。基赫霍夫网络与消息传递神经网络和连续深度网络建立了密切联系。我们证明，即使在没有任何传统层（如卷积、池化或线性层）的情况下，基赫霍夫网络在MNIST数据集上取得了98.86%的测试准确度，与最先进的结果相当。让基赫霍夫网络更加有趣的是其在硬件领域的潜力。当代深度神经网络通常部署在GPU上。相反，基赫霍夫网络可以通过模拟电路来实现。此外，我们证明了无论在基赫霍夫网络内有多少参数，其正向计算都可以在1/f秒内完成，其中f表示硬件的时钟频率。这种特性表明，基赫霍夫网络具有潜力实现快速计算的硬件。

    In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
    
[^45]: 矩阵机制的隐私放大

    Privacy Amplification for Matrix Mechanisms. (arXiv:2310.15526v1 [cs.LG])

    [http://arxiv.org/abs/2310.15526](http://arxiv.org/abs/2310.15526)

    提出了通过抽样分析矩阵机制的隐私放大算法MMCC，证明了通过条件性组合定理可以将相关输出视为独立输出，并展示了将噪声添加到DP-FTRL算法时可以渐近地与DP-SGD算法中放大后添加的噪声相匹配。

    

    隐私放大利用数据选择中的随机性来提供更严格的差分隐私保证。这种分析对于DP-SGD在机器学习中的成功至关重要，但对于最新的最先进算法并不适用。这是因为这些算法，即DP-FTRL，使用矩阵机制来添加相关噪声，而不是像DP-SGD那样添加独立噪声。在本文中，我们提出了“MMCC”算法，该算法是通过抽样来分析任何通用矩阵机制的隐私放大的首个算法。MMCC接近一个下界，当ε→0时，它趋近于该下界。为了分析MMCC中的相关输出，我们证明可以将其视为独立输出，通过对先前输出进行调节。我们的“条件性组合定理”具有广泛的实用性：我们使用它来显示添加到二叉树DP-FTRL的噪声可以渐近地与通过放大添加到DP-SGD中的噪声相匹配。我们的放大算法也具有实际的经验性实用性。

    Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.  In this paper, we propose "MMCC", the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as $\epsilon\to0$. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our "conditional composition theorem" has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our amplification algorithm also has practical empirical util
    
[^46]: 《科学和机器学习的Vendi分数的近亲：基于相似性的多样性度量族群》

    Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning. (arXiv:2310.12952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.12952](http://arxiv.org/abs/2310.12952)

    该论文提出了一种基于相似性的多样性度量方法——Vendi分数族群，为科学和机器学习等领域准确测量多样性提供了灵活性。

    

    准确测量多样性对于许多科学领域至关重要，包括机器学习、生态学和化学。 Vendi分数是一种扩展了q=1阶Hill数的通用基于相似性的多样性度量方法，借鉴量子统计力学的思想。与生态学中的许多多样性度量方法不同，Vendi分数考虑了相似性，并且不需要了解集合中各个类别的普遍性来评估多样性。然而，Vendi分数对于给定集合中的每个项目都以与该项目的普遍性成比例的敏感度进行处理。这在项目普遍性存在显著不平衡的情况下是不可取的。在本文中，我们利用相似性扩展了其他Hill数，以提供对稀有或常见项目分配敏感度的灵活性。这导致了一族多样性度量方法——具有不同敏感度水平的Vendi分数，可应用于各种场景中。

    Measuring diversity accurately is important for many scientific fields, including machine learning (ML), ecology, and chemistry. The Vendi Score was introduced as a generic similarity-based diversity metric that extends the Hill number of order q=1 by leveraging ideas from quantum statistical mechanics. Contrary to many diversity metrics in ecology, the Vendi Score accounts for similarity and does not require knowledge of the prevalence of the categories in the collection to be evaluated for diversity. However, the Vendi Score treats each item in a given collection with a level of sensitivity proportional to the item's prevalence. This is undesirable in settings where there is a significant imbalance in item prevalence. In this paper, we extend the other Hill numbers using similarity to provide flexibility in allocating sensitivity to rare or common items. This leads to a family of diversity metrics -- Vendi scores with different levels of sensitivity -- that can be used in a variety o
    
[^47]: Transformer语言模型中跨任务的电路组件复用

    Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])

    [http://arxiv.org/abs/2310.08744](http://arxiv.org/abs/2310.08744)

    这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。

    

    最近在机制可解释性方面的研究表明，通过电路分析可以成功地逆向工程语言模型的行为。然而，一个常见的批评是每个电路都是任务特定的，因此这样的分析不能为更高级的理解模型做出贡献。在这项工作中，我们提出证据表明洞察力（关于特定头部的低级发现和关于一般算法的高级发现）确实可以在任务之间进行泛化。具体而言，我们研究了Wang等人（2022）在间接宾语识别任务（IOI）中发现的电路，并展示了这个电路在更大的GPT2模型上的重现，以及在看似不同的任务中大部分被复用来解决问题：彩色物体（Ippolito和Callison-Burch，2023）。我们提供证据表明两个任务底层的过程在功能上非常相似，并且在电路中的注意力头部之间有大约78％的重叠。我们进一步展示了一个概念验证干预实验

    Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
    
[^48]: 超越记忆：通过大型语言模型进行推理来侵犯隐私

    Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])

    [http://arxiv.org/abs/2310.07298](http://arxiv.org/abs/2310.07298)

    该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。

    

    目前关于大型语言模型（LLMs）的隐私研究主要集中在提取记忆训练数据的问题上。同时，模型的推理能力已大幅增强。这引发了一个关键问题，即当前的LLMs是否能够通过从推理时给出的文本中推断个人属性来侵犯个人隐私。在这项工作中，我们首次对预训练LLMs从文本中推断个人属性的能力进行了全面研究。我们构建了一个包含真实Reddit个人资料的数据集，并且显示当前的LLMs可以推断出各种各样的个人属性（例如，位置、收入、性别），在成本（100倍）和时间（240倍）上仅需人类的一小部分，达到了最高1的准确率达到85％，最高3的准确率达到95.8％。随着人们越来越多地与由LLM驱动的聊天机器人在生活的各个方面进行互动，我们还探讨了侵犯隐私的聊天机器人通过似乎无关的对话试图提取个人信息的新威胁。

    Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
    
[^49]: 只需少量上下文示范即可实现越狱和对齐的语言模型

    Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])

    [http://arxiv.org/abs/2310.06387](http://arxiv.org/abs/2310.06387)

    本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。

    

    大规模语言模型（LLM）在各种任务中取得了显著的成功，但对其安全性和生成恶意内容的潜在风险的担忧也浮现出来。本文探讨了在相同上下文学习（ICL）中操纵LLM对齐能力的效果。我们发现，仅通过少量的上下文示范而无需微调，就可以操纵LLM增加或降低越狱概率，即回答恶意提示。基于这些观察，我们提出了用于越狱和对齐语言模型目的的相同上下文攻击（ICA）和相同上下文防御（ICD）方法。ICA通过构造恶意上下文指导模型生成有害输出，而ICD通过拒绝回答有害提示的示范来增强模型的稳健性。我们的实验证明了ICA和ICD在增加或降低对抗越狱攻击成功率方面的有效性。总的来说，我们揭示了ICL在越狱和对齐语言模型领域的潜力。

    Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
    
[^50]: 好表示的液滴：在两层网络中 grokking 作为一阶相变

    Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v1 [stat.ML])

    [http://arxiv.org/abs/2310.03789](http://arxiv.org/abs/2310.03789)

    本研究将自适应核方法应用于两个师生模型，预测了特征学习和 Grokking 的性质，并展示了 Grokking 与相变理论之间的映射关系。

    

    深度神经网络 (DNN) 的一个关键特性是在训练过程中能够学习新的特征。这种深度学习的有趣方面在最近报道的 Grokking 现象中表现得最为明显。虽然主要体现为测试准确性的突变增加，但 Grokking 也被认为是一种超越懒惰学习/高斯过程 (GP) 的现象，涉及特征学习。在这里，我们将特征学习理论的最新发展，自适应核方法，应用于具有立方多项式和模加法教师的两个师生模型。我们在这些模型上提供了关于特征学习和 Grokking 性质的分析预测，并展示了 Grokking 与相变理论之间的映射关系。我们表明，在 Grokking 之后，DNN 的状态类似于一阶相变后的混合相。在这个混合相中，DNN 生成了与之前明显不同的教师的有用内部表示。

    A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the 
    
[^51]: 超越稳定性：随机Softmax策略梯度方法的收敛分析

    Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods. (arXiv:2310.02671v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2310.02671](http://arxiv.org/abs/2310.02671)

    本论文研究了随机Softmax策略梯度方法的收敛性分析，提出了一种动态策略梯度的组合方法，并通过对参数进行反向训练来更好地利用相关性结构，实现向全局最优值的收敛。

    

    马尔可夫决策过程（MDP）是一种形式化框架，用于建模和解决序贯决策问题。在有限时间范围内，这些问题与最优停止或特定供应链问题以及大型语言模型的训练相关。与无限时间范围内的MDP不同，最优策略并不是稳定的，策略必须在每个时期单独进行学习。实际上，往往同时训练所有参数，忽视了动态规划所暗示的内在结构。本文介绍了一种动态规划和策略梯度的组合方法，称为动态策略梯度，其中参数在时间上以反向方式进行训练。对于表格Softmax参数化，我们对同时和动态策略梯度在精确梯度和采样梯度设置下向全局最优值进行了收敛分析，且没有引入正则化。结果表明，使用动态策略梯度训练可以更好地利用相关性结构，并提供了收敛性证明。

    Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite-time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamic policy gradient, where the parameters are trained backwards in time. For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploi
    
[^52]: GNN中的局部感知图重连

    Locality-Aware Graph-Rewiring in GNNs. (arXiv:2310.01668v1 [cs.LG])

    [http://arxiv.org/abs/2310.01668](http://arxiv.org/abs/2310.01668)

    本论文提出了一种在GNN中考虑局部性的图重连技术，以减少过度压缩、保持图的稀疏性，并改善长程交互的捕捉能力。

    

    图神经网络（GNNs）是一种用于图上机器学习的流行模型，通常遵循消息传递范式，在对邻居节点的信息进行聚合时，节点的特征会被递归地更新。虽然在输入图上交换消息赋予了GNNs强大的归纳偏置，但也使得GNNs容易过度压缩，从而无法捕捉给定图中的长程交互。为了解决这个问题，图重连技术被提出作为一种改善信息流的手段，通过改变图的连接性。在这项工作中，我们确定了图重连的三个期望：（i）减少过度压缩，（ii）尊重图的局部性，以及（iii）保持图的稀疏性。我们强调了空间和频谱重连技术之间存在的根本权衡；尽管前者通常满足（i）和（ii）但不满足（iii），后者通常在满足（i）和（iii）的同时牺牲了（ii）。

    Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and (iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii)
    
[^53]: 指向因果基础模型: 因果推断与注意力的对偶关系

    Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])

    [http://arxiv.org/abs/2310.00809](http://arxiv.org/abs/2310.00809)

    该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。

    

    基于因果推断和注意力之间的对偶连接，我们提出了一种名为Causal Inference with Attention (CInA)的理论上完备的方法，利用多个无标签数据集进行自监督因果学习，并在新数据的未见任务上实现零样本因果推断。我们的实证结果表明了我们的方法在复杂任务中的有效性。

    Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
    
[^54]: PLMM：移动设备上的个人大型模型

    PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])

    [http://arxiv.org/abs/2309.14726](http://arxiv.org/abs/2309.14726)

    本文提出了一种从传统大型语言模型中提取的个人大型模型，该模型更适应于本地用户的个人信息，并且能够保护用户的隐私。该模型分为个人级别、专家级别和传统级别，同时还需要小型化以适应个人计算机或移动设备，并实现实时响应以提供更好的用户体验。

    

    在本文中，受到联邦学习的启发，我们提出了从传统大型语言模型中提取的个人大型模型，这些模型更适应本地用户的个人信息，如教育背景和爱好。我们将大型语言模型分为三个级别：个人级别，专家级别和传统级别。个人级别模型适应用户的个人信息，对用户的输入进行加密并保护其隐私。专家级别模型专注于合并特定领域的知识，如金融、IT和艺术。传统模型专注于普遍知识的发现和提升专家模型。在这样的分类中，个人模型直接与用户交互。对于整个系统来说，个人模型具有用户的（加密的）个人信息。此外，这些模型必须足够小以在个人计算机或移动设备上运行。最后，它们还必须实时响应，以提供更好的用户体验。

    Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
    
[^55]: 通过跨域序列自编码器实现反事实公平性追求

    Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])

    [http://arxiv.org/abs/2309.13005](http://arxiv.org/abs/2309.13005)

    通过顺序自编码器实现反事实公平性追求的创新框架将环境信息和敏感属性与分类特征的嵌入表示分开，以提高模型在不同和陌生领域中的泛化能力，并解决公平问题。

    

    鉴于领域转移在机器学习中普遍存在的挑战，已经开发出各种领域泛化（DG）技术，以提高处理分布外（OOD）数据时机器学习系统的性能。此外，在实际场景中，数据分布可能会在连续的序列领域中逐渐变化。虽然当前的方法主要集中在改进在这些新领域内的模型效果，但往往忽视了学习过程中的公平问题。为此，我们引入了一种创新的框架，称为具有顺序自编码器的反事实公平性感知领域泛化（CDSAE）。这种方法有效地将环境信息和敏感属性与分类特征的嵌入表示分开。这种并行分离不仅极大地提高了模型在不同和陌生的领域中的泛化能力，还有效地解决了相关挑战。

    Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
    
[^56]: 复合移位算子的频率收敛问题

    Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])

    [http://arxiv.org/abs/2309.07169](http://arxiv.org/abs/2309.07169)

    本文研究了拓扑信号处理中复合子的可转移性，通过构造边际复合子和复合移位算子，研究其特征值和特征向量，并证明了复合子收敛时对应的复合移位算子的特征值会收敛到极限复合子的特征值。这些结果拓展了图信号处理框架。

    

    拓扑信号处理(TSP)利用单纯形复合来建模比顶点和边更高阶的结构。本文研究了TSP的可转移性，通过一种称为复合子的广义高阶图的版本。我们回顾了复合子的概念，即单纯形复合序列的极限[1]。受图移位算子的积分算子形式的启发，我们根据复合子的所有可能尺寸的组件构造了边际复合子和复合移位算子(CSO)。我们研究了CSO的特征值和特征向量，并将它们与一类新的加权邻接矩阵相关联。我们证明，当一个单纯形复合序列收敛到一个复合子时，相应的CSO的特征值收敛到极限复合子的特征值。这些结果暗示了在大型单纯形复合或单纯形复合序列上的学习可转移性，从而推广了图信号处理框架。

    Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
    
[^57]: 通过具有可迁移本地策略的集成，实现对车辆路径问题的通用神经求解器

    Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy. (arXiv:2308.14104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14104](http://arxiv.org/abs/2308.14104)

    本文介绍了一种通过结合全局信息和本地特征，实现对车辆路径问题的通用神经求解器的方法，并将其应用于解决现实世界的复杂问题。

    

    机器学习已被应用于帮助解决NP困难的组合优化问题。目前一种常见的方式是通过深度神经网络学习构建解决方案，由于其高效性和对专业知识要求较少，因此越来越受到关注。然而，许多用于车辆路径问题（VRPs）的神经构建方法都集中在具有指定节点分布和有限规模的合成问题实例上，导致在通常涉及复杂且未知节点分布以及大规模问题的现实世界问题上性能较差。为了使神经VRP求解器更实用，我们设计了一个从本地可迁移拓扑特征中学习的辅助策略，称为本地策略，并与典型的构建策略（从VRP实例的全局信息中学习）相结合形成一个集成策略。通过联合训练，聚合的策略相互协作和互补，以提升泛化性能。

    Machine learning has been adapted to help solve NP-hard combinatorial optimization problems. One prevalent way is learning to construct solutions by deep neural networks, which has been receiving more and more attention due to the high efficiency and less requirement for expert knowledge. However, many neural construction methods for Vehicle Routing Problems (VRPs) focus on synthetic problem instances with specified node distributions and limited scales, leading to poor performance on real-world problems which usually involve complex and unknown node distributions together with large scales. To make neural VRP solvers more practical, we design an auxiliary policy that learns from the local transferable topological features, named local policy, and integrate it with a typical construction policy (which learns from the global information of VRP instances) to form an ensemble policy. With joint training, the aggregated policies perform cooperatively and complementarily to boost generaliza
    
[^58]: 不仅仅奖励，还有约束：用于腿式机器人运动的应用

    Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])

    [http://arxiv.org/abs/2308.12517](http://arxiv.org/abs/2308.12517)

    本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。

    

    早期的一些研究通过设计神经网络控制器并使用无模型强化学习来训练，展示了复杂机器人系统中令人印象深刻的控制性能。然而，这些具有自然动作风格和高任务性能的出色控制器是通过进行大量奖励工程而开发的，该过程非常费时费力，需要设计大量奖励项并确定合适的奖励系数。在这项工作中，我们提出了一种新的强化学习框架，用于训练同时包含奖励和约束的神经网络控制器。为了让工程师能够适当地反映他们对约束的意图并以最小的计算开销处理它们，我们提出了两种约束类型和一种高效的策略优化算法。该学习框架被应用于训练不同形态和物理属性的几个腿式机器人的运动控制器。

    Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
    
[^59]: 鲁棒拉格朗日和对抗性策略梯度在鲁棒约束马尔可夫决策过程中的应用

    Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])

    [http://arxiv.org/abs/2308.11267](http://arxiv.org/abs/2308.11267)

    本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。

    

    鲁棒约束马尔可夫决策过程（RCMDP）是一个最近应用于强化学习的任务建模框架，它通过使用不确定性集合在转移动态模型中提供了对错误的鲁棒性。模拟RCMDPs需要基于每个状态的值估计计算最坏情况下的动态，这种方法之前在鲁棒约束策略梯度（RCPG）中使用过。本文介绍了两种算法，分别称为具有鲁棒拉格朗日的RCPG和对抗性RCPG。具有鲁棒拉格朗日的RCPG通过使用拉格朗日而不是值或约束来计算最坏情况下的动态从而修改RCPG。对抗性RCPG也基于拉格朗日公式计算最坏情况下的动态，但是将其作为对抗策略直接和增量地学习。

    The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
    
[^60]: GPLaSDI: 基于高斯过程的可解释潜空间动力学识别方法通过深度自动编码器

    GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder. (arXiv:2308.05882v1 [cs.CE])

    [http://arxiv.org/abs/2308.05882](http://arxiv.org/abs/2308.05882)

    GPLaSDI是一种基于高斯过程的可解释潜空间动力学识别方法，通过深度自动编码器将完全阶数的PDE解映射到潜空间，并使用插值和解决ODE系统进行快速和准确的ROM预测。

    

    数值求解偏微分方程(PDEs)可能具有挑战性且计算成本高。这导致了减少阶数模型(ROMs)的发展，其精确性高于完全阶数模型(FOMs)但计算速度更快。最近，机器学习的进展实现了非线性投影方法的创建，例如潜空间动力学识别(LaSDI)。LaSDI使用自动编码器将完全阶数的PDE解映射到潜空间，并学习潜空间动力学的ODE系统。通过在减少的潜空间中插值和解决ODE系统，可以通过将预测的潜空间动力学输入解码器来进行快速且准确的ROM预测。在本文中，我们介绍了一种基于高斯过程(GP)的新型LaSDI框架，用于潜空间ODE插值。使用GP带来两个重要优势。首先，它能够量化ROM预测的不确定性。其次，利用这个预测。

    Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this predict
    
[^61]: 通过混合精度加速傅里叶神经算子

    Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])

    [http://arxiv.org/abs/2307.15034](http://arxiv.org/abs/2307.15034)

    通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。

    

    傅里叶神经算子（FNO）是一种强大的技术，用于学习偏微分方程（PDE）解算器的代理映射。对于许多现实世界的应用，通常需要高分辨率的数据点，训练时间和内存使用量都是重要瓶颈。虽然对于标准神经网络有混合精度训练技术，但那些只适用于有限维度上的实值数据类型，因此不能直接应用于在复值（傅里叶）域和函数空间中重要操作的FNO。另一方面，由于傅里叶变换本身就是一次近似（由于离散化误差的存在），我们不需要以完全精度执行操作。在这项工作中，我们（i）对使用全精度和混合精度训练的FNO进行内存和运行时间剖析，（ii）对混合精度训练的数值稳定性进行研究，以及（iii）设计了一种训练过程，大大缩短了训练时间和内存使用率。

    The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor
    
[^62]: ExeDec: 在神经程序合成中进行执行分解以实现组合泛化

    ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis. (arXiv:2307.13883v1 [cs.LG])

    [http://arxiv.org/abs/2307.13883](http://arxiv.org/abs/2307.13883)

    ExeDec是一种基于分解的合成策略，通过预测执行子目标并在每个步骤的程序执行的指导下逐步解决问题，实现了更好的合成性能和组合泛化能力。

    

    在编写程序时，人们通过将复杂任务分解为较小且更熟悉的子任务来解决。虽然衡量神经程序合成方法是否具有类似的能力是困难的，但我们可以衡量它们是否能够进行组合泛化，即经过训练在较简单的子任务上的模型是否能够解决更复杂的任务。在本文中，我们描述了在程序合成中希望的几种不同形式的组合泛化，并形成一个元基准，用于为两个受欢迎的数据集RobustFill和DeepCoder创建泛化任务。然后，我们提出了一种新颖的基于分解的合成策略ExeDec，它通过在每个步骤的程序执行的指导下逐步预测执行子目标来解决问题。与基线方法相比，ExeDec具有更佳的合成性能和大大改进的组合泛化能力。

    When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
    
[^63]: BaBE:通过估计潜在解释变量增强公平性

    BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])

    [http://arxiv.org/abs/2307.02891](http://arxiv.org/abs/2307.02891)

    本文提出了一种名为BaBE的方法，通过估计潜在解释变量来提高公平性。该方法通过结合贝叶斯推断和期望最大化方法，估计给定Z的每个群体E的最可能值。

    

    本文考虑了两个群体之间不公平歧视的问题，并提出了一种预处理方法来实现公平性。我们提出了一种基于贝叶斯推断和期望最大化方法的BaBE (Bayesian Bias Elimination)方法，用于估计给定Z的每个群体的E的最可能值。

    We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
    
[^64]: 对齐的神经网络是否对抗对齐？

    Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])

    [http://arxiv.org/abs/2306.15447](http://arxiv.org/abs/2306.15447)

    我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。

    

    大型语言模型现在被调整为与其创建者的目标保持一致，即"有益且无害"。这些模型应该对用户的问题给出有益的回答，但拒绝回答可能会造成伤害的请求。然而，对抗用户可以构建绕过对齐尝试的输入。在这项工作中，我们研究了在与构造最坏情况输入（对抗性样本）的对抗用户交互时，这些模型保持多大程度的对齐。这些输入被设计成导致模型发出本应禁止的有害内容。我们展示了现有基于自然语言处理的优化攻击手法在可靠攻击对齐文本模型方面的不足之处：即使在当前基于自然语言处理的攻击失败时，我们仍然可以通过蛮力方法找到对抗性输入。因此，当前攻击的失败不应被视为对齐文本模型在面对对抗性输入时仍然保持对齐的证明。但是近期大规模机器学习模型的趋势是多模态的。

    Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
    
[^65]: DreamTime: 一种改进的文本到3D内容创作优化策略

    DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. (arXiv:2306.12422v1 [cs.CV])

    [http://arxiv.org/abs/2306.12422](http://arxiv.org/abs/2306.12422)

    本文提出了一种优先使用单调非增函数进行时间步采样的NeRF优化方法，以解决NeRF优化过程和得分蒸馏中均匀时间步采样之间的冲突。实验表明这种方法能够显著提高文本到3D内容的质量和多样性。

    

    近期，预先训练了数十亿个图像-文本对的文本到图像扩散模型通过使用得分蒸馏来优化随机初始化的神经辐射场（NeRF），从而实现了文本到3D内容的创建。 然而，所得到的3D模型存在两个局限性：（a）质量问题，例如饱和的颜色和Janus问题；（b）与文本引导的图像合成相比，极低的多样性。本文表明，NeRF优化过程和得分蒸馏中均匀时间步采样之间的冲突是这些局限性的主要原因。为了解决这种冲突，我们建议优先使用单调非增函数进行时间步采样，这使得NeRF优化与扩散模型的采样过程相一致。大量实验表明，我们的简单重设计能够显著提高文本到3D内容的质量和多样性。

    Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled text-to-3D content creation by optimizing a randomly initialized Neural Radiance Fields (NeRF) with score distillation. However, the resultant 3D models exhibit two limitations: (a) quality concerns such as saturated color and the Janus problem; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between NeRF optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns NeRF optimization with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves text-to-3D content creation with higher quality and diversity.
    
[^66]: 大型语言模型的简单而有效的剪枝方法

    A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11695](http://arxiv.org/abs/2306.11695)

    本论文提出了一种称为Wanda的新颖、简单而有效的剪枝方法，用于大型语言模型，通过对每个输出上的权重按照最小幅度乘以对应的输入激活来进行剪枝，无需重新训练或更新权重。

    

    随着规模的增大，大型语言模型（LLMs）是网络剪枝方法的自然候选对象：这些方法在努力保持性能的同时，丢弃了网络权重的一个子集。然而，现有的方法要么需要重新训练，这对于十亿级别的LLMs来说很少可行，要么需要解决一个依赖二阶信息的权重重构问题，这也可能计算成本很高。在本文中，我们介绍了一种新颖的、简单但有效的剪枝方法，称为Wanda（基于权重和激活的剪枝），旨在对预训练的LLMs引入稀疏性。受到最近对LLMs中出现的大幅特征的发现的启发，我们的方法在每个输出上按照权重和对应的输入激活相乘的最小幅度来剪枝权重。值得注意的是，Wanda不需要重新训练或更新权重，剪枝后的LLM可以直接使用。我们在LLaMA和LLaMA-2上对我们的方法Wanda进行了彻底的评估。

    As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
    
[^67]: 带有嘈杂治疗和没有侧面信息的可识别因果推断

    Identifiable causal inference with noisy treatment and no side information. (arXiv:2306.10614v1 [cs.LG])

    [http://arxiv.org/abs/2306.10614](http://arxiv.org/abs/2306.10614)

    本论文提出了一种在没有侧面信息和具有复杂非线性依赖性的情况下，纠正因治疗变量不准确测量引起的因果效应估计偏差的模型，并证明了该模型的因果效应估计是可识别的。该方法使用了深度潜在变量模型和分摊权重变分客观函数进行训练。

    

    在某些因果推断场景中，治疗（即原因）变量的测量存在不准确性，例如在流行病学或计量经济学中。未能纠正测量误差的影响可能导致偏差的因果效应估计。以前的研究没有从因果视角研究解决这个问题的方法，同时允许复杂的非线性依赖关系并且不假设可以访问侧面信息。对于这样的场景，本论文提出了一个模型，它假设存在一个连续的治疗变量，该变量测量不准确。建立在现有测量误差模型的基础上，我们证明了我们的模型的因果效应估计是可识别的，即使没有测量误差方差或其他侧面信息的知识。我们的方法依赖于深度潜在变量模型，其中高斯条件由神经网络参数化，并且我们开发了一个分摊权重变分客观函数来训练该模型。

    In some causal inference scenarios, the treatment (i.e. cause) variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such as scenario, this paper proposes a model that assumes a continuous treatment variable which is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model where Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical resul
    
[^68]: SequenceMatch：带回溯的自回归序列模型的模仿学习

    SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05426](http://arxiv.org/abs/2306.05426)

    本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。

    

    在许多领域，自回归模型可以在预测下一个观测值的任务上获得高似然度。然而，这种最大似然（MLE）目标不一定与自回归生成高质量序列的下游用例相匹配。MLE目标按照数据分布下序列的频率加权，不提供模型在分布之外行为的指导，这会导致在自回归生成过程中复合误差。为了解决这个复合误差问题，我们将序列生成定为模仿学习（IL）问题。这使我们可以最小化自回归模型生成的序列分布和数据集序列之间的各种分歧，包括考虑出分布序列的分歧。IL框架还允许我们通过在生成过程中引入回格动作来引入回溯。这进一步减轻了复合效应。

    In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
    
[^69]: 经典循环神经网络的脉冲计算

    Spike-based computation using classical recurrent neural networks. (arXiv:2306.03623v1 [cs.NE])

    [http://arxiv.org/abs/2306.03623](http://arxiv.org/abs/2306.03623)

    本文提出了一种新的脉冲神经网络方法，通过修改一种易于训练的循环神经网络的动态特性，使其产生基于脉冲的计算，并在进行了脉冲网络的训练后，在多个数据集上取得了最先进的性能。

    

    脉冲神经网络是一种人工神经网络，其中神经元之间的通信仅由事件或所谓的脉冲组成。这种特性使得神经网络能够进行异步和稀疏计算，并因此在专用硬件上运行时大幅减少能源消耗。本文中，我们尝试采用一种对称的方法：修改一种已知的、易于训练的循环神经网络的动态特性，使其产生基于脉冲的计算。通过明确引入脉冲阈值和重置机制，我们使网络能够仅使用脉冲来执行前向和循环计算。然后，我们展示了这种修改后的构架既可以实现，同时在两个基准数据集上实现了最先进的性能，包括具有挑战性的ImageNet数据集。

    Spiking neural networks are a type of artificial neural networks in which communication between neurons is only made of events, also called spikes. This property allows neural networks to make asynchronous and sparse computations and therefore to drastically decrease energy consumption when run on specialized hardware. However, training such networks is known to be difficult, mainly due to the non-differentiability of the spike activation, which prevents the use of classical backpropagation. This is because state-of-the-art spiking neural networks are usually derived from biologically-inspired neuron models, to which are applied machine learning methods for training. Nowadays, research about spiking neural networks focuses on the design of training algorithms whose goal is to obtain networks that compete with their non-spiking version on specific tasks. In this paper, we attempt the symmetrical approach: we modify the dynamics of a well-known, easily trainable type of recurrent neural 
    
[^70]: 通过反事实路径几何导航解释性多元宇宙

    Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02786](http://arxiv.org/abs/2306.02786)

    该论文提出了解释性多元宇宙的概念，用于导航和比较所有可能的反事实路径的几何关系。

    

    反事实解释是解释（不透明的）预测模型决策的事实标准。其生成往往受到算法和特定领域约束的影响，如基于密度的可行性和属性的（不）可变性或变化的方向性，旨在最大化其在现实生活中的实用性。除了对反事实实例本身的要求之外，已知算法可行性路径与事实数据点之间的连接，即算法可诉求，已成为重要的技术考虑因素。尽管这两个要求确保了旅程的步骤和目的地的合理性，但目前的文献忽略了这种反事实路径的多样性。为了解决这个缺点，我们引入了一种新颖的解释性多元宇宙概念，涵盖了所有可能的反事实旅程；然后展示了如何导航、推理和比较这些轨迹的几何关系。

    Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
    
[^71]: Gumbel传播下的潜在最优路径变分贝叶斯动态规划

    Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming. (arXiv:2306.02568v1 [stat.ML])

    [http://arxiv.org/abs/2306.02568](http://arxiv.org/abs/2306.02568)

    该论文使用动态规划和Gumbel传播在VAE的潜在空间中获得结构化稀疏最优路径，从而使得模型可以依赖于未观察到的结构特征信息，并成功实现了文本转语音和歌声合成。

    

    我们提出了一种统一方法，使用动态规划和Gumbel传播在变分自编码器（VAE）的潜在空间中获取结构化稀疏最优路径。我们通过概率软化解，即随机最优路径，来解决经典最优路径问题，并将广泛的DP问题转化为有向无环图，其中所有可能的路径遵循Gibbs分布。我们通过Gumbel分布的属性显示Gibbs分布与消息传递算法的等价性，并提供了变分贝叶斯推理所需的所有要素。我们的方法获取了潜在最优路径，使生成任务的端到端训练成为可能，其中模型依赖于未观察到的结构特征的信息。我们验证了我们方法的行为，并展示了其在两个真实世界应用中的适用性：文本转语音和歌声合成。

    We propose a unified approach to obtain structured sparse optimal paths in the latent space of a variational autoencoder (VAE) using dynamic programming and Gumbel propagation. We solve the classical optimal path problem by a probability softening solution, called the stochastic optimal path, and transform a wide range of DP problems into directed acyclic graphs in which all possible paths follow a Gibbs distribution. We show the equivalence of the Gibbs distribution to a message-passing algorithm by the properties of the Gumbel distribution and give all the ingredients required for variational Bayesian inference. Our approach obtaining latent optimal paths enables end-to-end training for generative tasks in which models rely on the information of unobserved structural features. We validate the behavior of our approach and showcase its applicability in two real-world applications: text-to-speech and singing voice synthesis.
    
[^72]: 去中心化联邦学习：一份综述与展望

    Decentralized Federated Learning: A Survey and Perspective. (arXiv:2306.01603v1 [cs.LG])

    [http://arxiv.org/abs/2306.01603](http://arxiv.org/abs/2306.01603)

    去中心化联邦学习通过消除中心服务器的需求，实现了直接通信从而节省了通信资源。本文提供了对DFL的全面调查、深入展望和扩展变体与分类的介绍，重点在于DFL的系统与详细前景。

    

    联邦学习（FL）因其在共享知识的同时保护用户数据、保护隐私、提高学习效率并减少通信负载的能力而受到关注。去中心化联邦学习（DFL）是一种去中心化的网络架构，与集中式FL（CFL）相比，DFL消除了中心服务器的需求。DFL使客户端之间可以直接通信，从而显著节省了通信资源。本文对DFL进行了全面的调查和深入的展望。首先，对CFL的方法、挑战和变体进行了回顾，奠定了DFL的背景。然后，介绍了DFL的系统和详细的前景，包括迭代顺序、通信协议、网络拓扑、范例提议和时间变异性。接下来，基于DFL的定义，提出了几个扩展变体和分类并使用了最先进的技术。最后，除了总结DFL的优势与劣势，对DFL未来研究和应用前景进行了展望。

    Federated learning (FL) has been gaining attention for its ability to share knowledge while maintaining user data, protecting privacy, increasing learning efficiency, and reducing communication overhead. Decentralized FL (DFL) is a decentralized network architecture that eliminates the need for a central server in contrast to centralized FL (CFL). DFL enables direct communication between clients, resulting in significant savings in communication resources. In this paper, a comprehensive survey and profound perspective is provided for DFL. First, a review of the methodology, challenges, and variants of CFL is conducted, laying the background of DFL. Then, a systematic and detailed perspective on DFL is introduced, including iteration order, communication protocols, network topologies, paradigm proposals, and temporal variability. Next, based on the definition of DFL, several extended variants and categorizations are proposed with state-of-the-art technologies. Lastly, in addition to sum
    
[^73]: ChatGPT的公平性评估

    Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])

    [http://arxiv.org/abs/2305.18569](http://arxiv.org/abs/2305.18569)

    本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。

    

    理解和解决LLM中不公平的问题对于负责任的AI部署至关重要。然而，在将LLM应用于高风险领域时，尤其是关于公平评估方面，数量分析和深入研究的可用性有限。本文旨在提供一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，我们专注于评估ChatGPT在包括教育、犯罪学、金融和医疗保健等高风险领域的表现。为了进行全面的评估，我们考虑了群体公平性和个人公平性，并观察了在一系列有偏或无偏提示下ChatGPT输出的差异。该研究对于更深入的了解LLM的公平表现，便于偏见缓解，促进负责任的人工智能系统的发展具有意义。

    Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
    
[^74]: 基于Option框架的多模式探索自主非单体智能体

    An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])

    [http://arxiv.org/abs/2305.01322](http://arxiv.org/abs/2305.01322)

    该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。

    

    强化学习领域的探索研究主要关注“如何探索”的探索方式，而“何时探索”的探索研究一直没有成为重点。典型的探索行为通常将探索行为与智能体的开发利用行为绑定在一起。最近出现了非单体探索行为的研究，以研究人类和动物的模式切换行为。本研究的最终目的是使智能体能够自主决定何时探索或利用。我们在Option框架中描述了自主多模式探索的初始研究。通过比较实验结果，我们展示了我们的方法相对于现有的非单体探索方法的更高性能。

    Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
    
[^75]: 从损失函数中解耦分位数表达式

    Decoupling Quantile Representations from Loss Functions. (arXiv:2304.12766v1 [cs.LG])

    [http://arxiv.org/abs/2304.12766](http://arxiv.org/abs/2304.12766)

    本文提出了同时二元量化回归（SBQR）中分位数与预测概率的二元对偶性，使分位数表达式在不同tau值下的构造不再依赖于损失函数，从而在处理检测超出分布样本和调整模型方面表现突出。

    

    同时量化回归（SQR）技术用于估计深度学习模型的不确定性，但其应用受限于要求中位数分位数（τ = 0.5）处的解决方案必须最小化平均绝对误差（MAE）。本文通过展示同时二元量化回归（SBQR）中分位数与预测概率的二元对偶性来解决此问题。这使我们能够从损失函数中解耦分位数表达式的构造，使我们能够在中位分位数处分配任意分类器f(x)，并生成不同τ值的完整SBQR分位数表示的全谱。我们通过两个应用程序验证了我们的方法：（i）检测超出分布样本，其中我们显示分位数表示优于标准概率输出；（ii）调整模型，在这里，我们证明了分位数表示对失真的鲁棒性。

    The simultaneous quantile regression (SQR) technique has been used to estimate uncertainties for deep learning models, but its application is limited by the requirement that the solution at the median quantile ({\tau} = 0.5) must minimize the mean absolute error (MAE). In this article, we address this limitation by demonstrating a duality between quantiles and estimated probabilities in the case of simultaneous binary quantile regression (SBQR). This allows us to decouple the construction of quantile representations from the loss function, enabling us to assign an arbitrary classifier f(x) at the median quantile and generate the full spectrum of SBQR quantile representations at different {\tau} values. We validate our approach through two applications: (i) detecting out-of-distribution samples, where we show that quantile representations outperform standard probability outputs, and (ii) calibrating models, where we demonstrate the robustness of quantile representations to distortions. 
    
[^76]: PopulAtion Parameter Averaging (PAPA)（人口参数平均）

    PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])

    [http://arxiv.org/abs/2304.03094](http://arxiv.org/abs/2304.03094)

    提出一种新方法PopulAtion Parameter Averaging (PAPA)，能同时拥有集成的普遍性与权重平均的效率，可以显著提高模型性能。

    

    集成方法将多个模型的预测组合起来以提高性能，但需要更高的计算成本。为了避免这些成本，可以通过对多个神经网络的权重进行平均来将它们合并成一个（模型汤）。然而，这通常比集成表现更差。当权重足够相似（在权重或特征空间中）可以很好地平均，但足够不同以从组合中受益时，权重平均才是有益的。基于这个想法，我们提出了PopulAtion Parameter Averaging (PAPA)，一种将集成的普遍性与权重平均的效率相结合的方法。PAPA利用不同模型（在不同数据顺序，增强和正则化上训练）的人口，而偶尔（不要太频繁，也不要太稀疏）用网络的权重来代替人口权重的平均值。PAPA减少了平均值和集成之间的性能差距，提高了模型的性能。

    Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
    
[^77]: 基于核心的区块链网络趋势监测研究

    Core-based Trend Detection in Blockchain Networks. (arXiv:2303.14241v1 [cs.LG])

    [http://arxiv.org/abs/2303.14241](http://arxiv.org/abs/2303.14241)

    本研究介绍了一种可伸缩的方法InnerCore用于分析区块链网络，实现了基于区块链网络对关键参与者的识别并提供情感指标，以实现自动化的趋势监测。

    

    区块链在贸易金融中的应用越来越广泛，每天交易的资产价值达数十亿美元。然而，由于数据的规模和复杂性，对这些网络进行分析仍然具有挑战性。我们介绍了一种可伸缩的方法- InnerCore，用于识别基于区块链的网络中的关键参与者，并使用基于数据深度的内核分解和中心模式发现提供网络情感指标。 InnerCore是一种计算高效、非监督的方法，适用于分析大规模时间图。我们通过对LunaTerra的最近崩溃和以太坊的PoS切换进行案例研究，并使用一家领先的区块链分析公司收集的外部基本事实证明了它的有效性。我们的实验表明，InnerCore可以与合格的分析准确匹配，不需要人为干预，从而实现可伸缩的区块链分析和趋势检测。

    Blockchains are now significantly easing trade finance, with billions of dollars worth of assets being transacted daily. However, analyzing these networks remains challenging due to the large size and complexity of the data. We introduce a scalable approach called "InnerCore" for identifying key actors in blockchain-based networks and providing a sentiment indicator for the networks using data depth-based core decomposition and centered-motif discovery. InnerCore is a computationally efficient, unsupervised approach suitable for analyzing large temporal graphs. We demonstrate its effectiveness through case studies on the recent collapse of LunaTerra and the Proof-of-Stake (PoS) switch of Ethereum, using external ground truth collected by a leading blockchain analysis company. Our experiments show that InnerCore can match the qualified analysis accurately without human involvement, automating blockchain analysis and its trend detection in a scalable manner.
    
[^78]: PULSNAR -- 在SCAR假设不成立时选择正无标记学习：分类比例估计

    PULSNAR -- Positive unlabeled learning selected not at random: class proportion estimation when the SCAR assumption does not hold. (arXiv:2303.08269v1 [cs.LG])

    [http://arxiv.org/abs/2303.08269](http://arxiv.org/abs/2303.08269)

    本文提出了一种新的PU方法PULSNAR，即使在SCAR不成立时，也可以实现准确的α估计和校准模型。

    

    正无标记（PU）学习是半监督二元分类的一种，其中机器学习算法区分一组正实例（带有标签）和一组既有正类又有负类实例（没有标签）。在确认负例不可用或难以获取，并且在未标记的实例中发现正例具有价值的情况下（例如，在未测试的化合物中找到可行药物），PU学习具有广泛的应用。大多数PU学习算法认为选择正实例独立于它们的特征，即进行完全随机选择（SCAR）假设。然而，在许多现实世界的应用中，例如医疗保健，正实例不是SCAR（例如，严重情况更容易被诊断出），导致在无标记示例中估计阳性比例α和模型校准性能差，进而导致选择正例的不确定决策阈值。PU学习算法可以通过估计每个未标记实例是阳性的概率来估计α并提供校准模型。然而，当SCAR假设不成立时，现有方法表现不佳。本文提出了一种新的PU方法PULSNAR，即使在SCAR不成立时，也可以实现准确的α估计和校准模型。我们的方法基于对未标记示例分布的新假设，称为阳性均匀条件。实证研究表明，我们的方法在合成和实际数据集中均优于现有最先进的PU方法。

    Positive and Unlabeled (PU) learning is a type of semi-supervised binary classification where the machine learning algorithm differentiates between a set of positive instances (labeled) and a set of both positive and negative instances (unlabeled). PU learning has broad applications in settings where confirmed negatives are unavailable or difficult to obtain, and there is value in discovering positives among the unlabeled (e.g., viable drugs among untested compounds). Most PU learning algorithms make the selected completely at random (SCAR) assumption, namely that positives are selected independently of their features. However, in many real-world applications, such as healthcare, positives are not SCAR (e.g., severe cases are more likely to be diagnosed), leading to a poor estimate of the proportion, $\alpha$, of positives among unlabeled examples and poor model calibration, resulting in an uncertain decision threshold for selecting positives. PU learning algorithms can estimate $\alph
    
[^79]: 分析和编辑暗藏后门的语言模型的内部机制

    Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12461](http://arxiv.org/abs/2302.12461)

    本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。

    

    数据集中的毒化是对大型语言模型的潜在安全威胁，可能导致暗藏后门的模型。关于暗藏后门语言模型的内部机制以及它们如何处理触发输入（例如，切换至有毒语言）的描述尚未找到。本文研究基于Transformer的暗藏后门语言模型的内部表示，并确定早期层的MLP模块与初始嵌入投影结合是后门机制中最重要的部分。我们利用这些知识来删除、插入和修改后门机制，并用工程化替代物降低MLP模块输出的重要性。为此，我们引入了基于主要成分的低秩矩阵的PCP消融技术，用其替换变压器模块。我们在暗藏后门的玩具模型、暗藏后门的大型模型和非暗藏后门的开源模型上展示了我们的结果。我们表明我们可以改善后门的输出效果。

    Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
    
[^80]: 基于树模型的边际特征归因研究

    On marginal feature attributions of tree-based models. (arXiv:2302.08434v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08434](http://arxiv.org/abs/2302.08434)

    该论文讨论了基于树模型的边际特征归因方法，与流行的TreeSHAP算法相比，边际Shapley值在相同函数的情况下保持一致，并且介绍了如何利用树模型的内部结构计算边际特征归因。

    

    由于其强大和易于使用的特点，随机森林和梯度提升树集成等基于树的机器学习模型变得非常流行。为了解释这些模型，可以使用基于边际期望的局部特征归因方法，例如边际（干预）Shapley、Owen或Banzhaf值。这些方法对模型真实且实现不变，即仅依赖于模型的输入输出函数。通过提供两个（具有相似统计性质的）决策树来对比这一点，这两个决策树计算完全相同的函数，但“路径相关”的TreeSHAP方法给出了不同的特征排序，而边际Shapley值重合。此外，我们讨论了如何利用基于树模型的内部结构来帮助计算它们的边际特征归因，以得到线性博弈值。一个重要的观察是，这些函数在某个常数区间内是简单的（分段常数）函数。

    Due to their power and ease of use, tree-based machine learning models, such as random forests and gradient-boosted tree ensembles, have become very popular. To interpret them, local feature attributions based on marginal expectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values, may be employed. Such methods are true to the model and implementation invariant, i.e. dependent only on the input-output function of the model. We contrast this with the popular TreeSHAP algorithm by presenting two (statistically similar) decision trees that compute the exact same function for which the "path-dependent" TreeSHAP yields different rankings of features, whereas the marginal Shapley values coincide. Furthermore, we discuss how the internal structure of tree-based models may be leveraged to help with computing their marginal feature attributions according to a linear game value. One important observation is that these are simple (piecewise-constant) functions with respect to a c
    
[^81]: 平均限制策略优化

    Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00808](http://arxiv.org/abs/2302.00808)

    本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。

    

    有限制条件的强化学习对于各种应用变得越来越重要。通常，平均标准比折扣标准更合适。然而，针对平均限制 CMDP 的强化学习仍然是一个具有挑战性的问题。针对折扣限制 RL 问题设计的算法通常在平均 CMDP 环境下表现不佳。在本文中，我们引入了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法。平均限制策略优化（ACPO）算法的灵感来自基于信任区域方法的著名 PPO 类算法。我们发展了基本的平均 MDP 敏感性理论，然后在算法设计中使用相应的界限。我们提供了其性能的理论保证，并通过在各种具有挑战性的 MuJoCo 环境中进行大量实验工作，展示了该算法与其他常规算法相比的卓越表现。

    Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
    
[^82]: 基于状态的重要性抽样方法实现低方差的行为策略离线评估

    Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03932](http://arxiv.org/abs/2212.03932)

    本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    

    在强化学习的离线评估中，需要评估目标策略的性能，而这需要使用由行为策略采集的样本数据。传统的重要性抽样方法由于计算动作概率比值的乘积而导致方差增加，从而在涉及长期规划的任务中出现估计不准确的问题。本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
    
[^83]: 能否通过脑信号揭示人类语言的内部一致性？

    Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2208.06348](http://arxiv.org/abs/2208.06348)

    本研究探索了脑信号和人类语言之间的关系，并介绍了一种名为MTAM的方法，该方法在情感分析和关系检测等下游应用中取得了新的最先进结果。

    

    脑信号（如脑电图）和人类语言在许多下游任务中被广泛研究，但二者之间的联系尚未得到很好的探索。本研究探讨了脑电图和语言之间的关系和依赖性。在表示层面上，我们引入了一种名为MTAM（Multimodal Transformer Alignment Model）的方法，用于观察这两种模态之间的协调表示。我们使用了多种关系对齐技术，如典型相关分析和Wasserstein距离，作为损失函数来转换特征。在情感分析和关系检测等下游应用中，我们在ZuCo和K-EmoCon两个数据集上实现了新的最先进结果。我们的方法在情感分析方面使K-EmoCon数据集的F1分数提高了1.7％，ZuCo数据集提高了9.3％，在关系检测方面ZuCo数据集提高了7.4％。此外，我们提供了国际上最大的人类类比推理数据集的编码方案。

    Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
    
[^84]: 在实值函数中对抗鲁棒PAC学习性的研究

    Adversarially Robust PAC Learnability of Real-Valued Functions. (arXiv:2206.12977v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12977](http://arxiv.org/abs/2206.12977)

    该论文研究了在实值函数中对抗鲁棒PAC学习性，发现有限胖折射维的类既可以在实现和不可知设置中被学习，凸函数类可以正确学习，而一些非凸函数类需要不正当的学习算法。

    

    我们研究了在使用$\ell_p$损失和任意扰动集的回归设置中，对测试时对抗性攻击的稳健性。我们探讨了在这种情况下哪些函数类是PAC可学习的。我们表明有限胖折射维的类既可以在实现和不可知设置中被学习。此外，对于凸函数类，它们甚至可以正确学习。相比之下，一些非凸函数类显然需要不正当的学习算法。我们的主要技术基于构建一个由胖折射维决定大小的具有对抗鲁棒性的样本压缩方案。在此过程中，我们介绍了一个新颖的面向实值函数的不可知样本压缩方案，这可能是具有独立兴趣的。

    We study robustness to test-time adversarial attacks in the regression setting with $\ell_p$ losses and arbitrary perturbation sets. We address the question of which function classes are PAC learnable in this setting. We show that classes of finite fat-shattering dimension are learnable in both realizable and agnostic settings. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes provably require improper learning algorithms. Our main technique is based on a construction of an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension. Along the way, we introduce a novel agnostic sample compression scheme for real-valued functions, which may be of independent interest.
    
[^85]: 基于回归的超标概率预测方法用于显著波高预测

    Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction. (arXiv:2206.09821v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.09821](http://arxiv.org/abs/2206.09821)

    本论文提出了一种基于回归的超标概率预测方法，用于预测显著波高，通过利用预测来估计超标概率，取得了更好的效果。

    

    显著波高预测是海洋数据分析中的一个关键问题。预测显著波高对于估计波能产生是至关重要的。此外，及时预测大浪的到来对于确保航海作业的安全很重要。我们将预测显著波高的极端值作为超标概率预测问题。因此，我们旨在估计显著波高将超过预定义阈值的概率。通常使用概率二分类模型来解决这个任务。相反，我们提出了一种基于预测模型的新方法。该方法利用未来观测的预测来根据累积分布函数估计超标概率。我们使用来自加拿大哈利法克斯海岸的浮标数据进行了实验。结果表明，所提出的方法更好。

    Significant wave height forecasting is a key problem in ocean data analytics. Predicting the significant wave height is crucial for estimating the energy production from waves. Moreover, the timely prediction of large waves is important to ensure the safety of maritime operations, e.g. passage of vessels. We frame the task of predicting extreme values of significant wave height as an exceedance probability forecasting problem. Accordingly, we aim at estimating the probability that the significant wave height will exceed a predefined threshold. This task is usually solved using a probabilistic binary classification model. Instead, we propose a novel approach based on a forecasting model. The method leverages the forecasts for the upcoming observations to estimate the exceedance probability according to the cumulative distribution function. We carried out experiments using data from a buoy placed in the coast of Halifax, Canada. The results suggest that the proposed methodology is better
    
[^86]: 区块链网络中的协作学习用于网络攻击检测

    Collaborative Learning for Cyberattack Detection in Blockchain Networks. (arXiv:2203.11076v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2203.11076](http://arxiv.org/abs/2203.11076)

    本文旨在研究入侵攻击并为区块链网络开发一种新颖的网络攻击检测框架。通过实验室合成的网络攻击数据集，提出了一种协作学习模型，允许区块链节点共享学习知识以检测攻击，并增强整个区块链网络的安全性。

    

    本文旨在研究入侵攻击，并为区块链网络开发一种新颖的网络攻击检测框架。首先，在实验室中设计和实现一个区块链网络，该网络将用于生成真实的流量数据（包括正常数据和攻击数据），以用于我们的学习模型，并进行实时实验，评估我们提出的入侵检测框架的性能。据我们所知，这是第一个在实验室中合成的用于区块链网络的网络攻击数据集。然后，我们提出了一种新颖的协作学习模型，可以在区块链网络中实现高效部署以检测攻击。所提出的学习模型的主要思想是使区块链节点能够主动收集数据，共享从其数据中学到的知识，并与网络中的其他区块链节点交换知识。通过这种方式，我们不仅能够利用其他节点的知识进行攻击检测，还能够增强整个区块链网络的安全性。

    This article aims to study intrusion attacks and then develop a novel cyberattack detection framework for blockchain networks. Specifically, we first design and implement a blockchain network in our laboratory. This blockchain network will serve two purposes, i.e., to generate the real traffic data (including both normal data and attack data) for our learning models and implement real-time experiments to evaluate the performance of our proposed intrusion detection framework. To the best of our knowledge, this is the first dataset that is synthesized in a laboratory for cyberattacks in a blockchain network. We then propose a novel collaborative learning model that allows efficient deployment in the blockchain network to detect attacks. The main idea of the proposed learning model is to enable blockchain nodes to actively collect data, share the knowledge learned from its data, and then exchange the knowledge with other blockchain nodes in the network. In this way, we can not only levera
    
[^87]: 图形神经网络在概率误差模型下的敏感性

    Graph Neural Network Sensitivity Under Probabilistic Error Model. (arXiv:2203.07831v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.07831](http://arxiv.org/abs/2203.07831)

    本文研究了概率误差模型对图卷积网络（GCN）性能的影响，并证明了误差模型下邻接矩阵的受限性。通过实验验证了这种误差界限，并研究了GCN在这种概率误差模型下的准确性敏感性。

    

    图卷积网络（GCN）可以通过图卷积成功学习图信号表示。图卷积依赖于图滤波器，其中包含数据的拓扑依赖关系并传播数据特征。然而，在传播矩阵（例如邻接矩阵）中的估计误差可能对图滤波器和GCNs产生重大影响。本文研究概率图误差模型对GCN性能的影响。我们证明了在误差模型下的邻接矩阵受到图大小和误差概率函数的限制。我们进一步分析了带有自循环的归一化邻接矩阵的上界。最后，我们通过在合成数据集上运行实验来说明误差界限，并研究简单GCN在这种概率误差模型下的准确性敏感性。

    Graph convolutional networks (GCNs) can successfully learn the graph signal representation by graph convolution. The graph convolution depends on the graph filter, which contains the topological dependency of data and propagates data features. However, the estimation errors in the propagation matrix (e.g., the adjacency matrix) can have a significant impact on graph filters and GCNs. In this paper, we study the effect of a probabilistic graph error model on the performance of the GCNs. We prove that the adjacency matrix under the error model is bounded by a function of graph size and error probability. We further analytically specify the upper bound of a normalized adjacency matrix with self-loop added. Finally, we illustrate the error bounds by running experiments on a synthetic dataset and study the sensitivity of a simple GCN under this probabilistic error model on accuracy.
    

