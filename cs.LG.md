# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Reliability of Watermarks for Large Language Models.](http://arxiv.org/abs/2306.04634) | 本文研究了大型语言模型水印在混合其他文本来源时的可靠性，并提供了在实际应用中的建议。 |
| [^2] | [Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion.](http://arxiv.org/abs/2306.04633) | 本文提出了一种利用二维预训练模型，通过慢-快对比融合实现三维物体实例分割的方法，在大量物体的场景中具有可扩展性，同时不需要物体数量的上界限制，通过限制多视角一致性，提出了一个新的半真实数据集(Messy Rooms)。 |
| [^3] | [Generative Adversarial Shaders for Real-Time Realism Enhancement.](http://arxiv.org/abs/2306.04629) | 本文提出了一种高效、基于生成对抗渲染器的实时逼真度提高方法，该方法可以适应嵌入式和移动GPU等资源受限制的设备，并能够在使用对抗训练的情况下，实现目标图像集外观的忠实再现，提供与神经网络方法相当的质量。 |
| [^4] | [Yet Another Algorithm for Supervised Principal Component Analysis: Supervised Linear Centroid-Encoder.](http://arxiv.org/abs/2306.04622) | 我们提出了一种新的有监督降维技术SLCE，通过使用线性变换将类别的样本映射到其类别中心点来工作。该变换可以在低维度空间中重构样本以最小化中心点重构损失。SLCE可以用于许多监督学习任务中。 |
| [^5] | [Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning.](http://arxiv.org/abs/2306.04621) | 本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。 |
| [^6] | [Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations.](http://arxiv.org/abs/2306.04618) | 本文提出了一个具有挑战性的基准协议，用于评估自然语言处理中的领域外鲁棒性。通过使用这个基准套件，作者们发现OOD与ID性能之间的关系并不总是一致的，并引入了一种名为LLMs的新方法，可以在多个任务上显著提高OOD鲁棒性。 |
| [^7] | [Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach.](http://arxiv.org/abs/2306.04600) | 本文提出了一种基于物理约束卷积神经网络的方法，可以从受到系统误差影响的数据中还原出潜在物理系统的时空解。 |
| [^8] | [Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions.](http://arxiv.org/abs/2306.04597) | 该论文提出了一种数据干预策略，可以通过极少的训练数据减少预训练模型中的性别偏差，有效降低对任何性别的偏好倾向。 |
| [^9] | [Generalization Across Observation Shifts in Reinforcement Learning.](http://arxiv.org/abs/2306.04595) | 本文研究了强化学习中观测转移泛化问题，在基于仿真器学习的情况下，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。 |
| [^10] | [Proximity-Informed Calibration for Deep Neural Networks.](http://arxiv.org/abs/2306.04590) | 该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。 |
| [^11] | [Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations.](http://arxiv.org/abs/2306.04581) | 本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。 |
| [^12] | [Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review.](http://arxiv.org/abs/2306.04566) | 综述了机器学习、遥感和物联网方法在农业中的应用，这些技术的整合为农业提供了洞察和预测，提高了农业生产效率和可持续性。 |
| [^13] | [ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models.](http://arxiv.org/abs/2306.04563) | 这篇论文探讨了OpenAI的ChatGPT模型在幽默识别方面的能力，结果表明该模型的幽默并不是硬编码的，但大部分生成的笑话都不是新的，几乎是少量几组重复的。 |
| [^14] | [StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code.](http://arxiv.org/abs/2306.04556) | 本文提出了一个新的基准测试StudentEval，由初学者编写多个提示来测试Code LLM的性能，并发现这比现有基准测试更好地区分模型性能；分析提示发现学生提示技巧差异显著，非确定性LLM抽样可能会误导学生。 |
| [^15] | [Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning.](http://arxiv.org/abs/2306.04551) | 本文研究了领域内与领域外语言模型以及多任务与单任务训练的比较，并证明了通过临床训练的多任务语言模型在临床诊断推理任务中表现优异，建立了新的最优性能。 |
| [^16] | [Convergence of SARSA with linear function approximation: The random horizon case.](http://arxiv.org/abs/2306.04548) | 本文研究了SARSA算法在随机时限MDPs中的收敛性，证明了当行为策略与线性函数逼近的权重向量相关，Lipschitz常数足够小时，算法以概率一收敛。 |
| [^17] | [On the Design Fundamentals of Diffusion Models: A Survey.](http://arxiv.org/abs/2306.04542) | 本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。 |
| [^18] | [Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications.](http://arxiv.org/abs/2306.04539) | 本文研究在只有带标签的单模态数据和自然出现的多模态数据的情况下，如何量化多模态交互的挑战，并提出了两个下界和一个上界来量化多模态交互量。 |
| [^19] | [Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models.](http://arxiv.org/abs/2306.04529) | Git-Theta是一种用于协同开发机器学习模型的Git扩展，可支持高效的通信更新、自动模型合并以及有关两个版本之间差异的有意义报告。 |
| [^20] | [ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis.](http://arxiv.org/abs/2306.04527) | ContriMix是一种无需标识和手工调优的领域泛化技术，在显微镜图像中通过分离和学习生成合成图像的方式，解决了领域泛化的问题。 |
| [^21] | [Estimating Koopman operators with sketching to provably learn large scale dynamical systems.](http://arxiv.org/abs/2306.04520) | 本文提出利用随机投影技术优化了Koopman算子的估计器，加快了计算速度，并给出了精确的误差界限，提高了算法的可靠性。 |
| [^22] | [Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks.](http://arxiv.org/abs/2306.04519) | 提出了一种用于辅助任务下多任务学习的样本级加权算法SLGrad，通过样本特定的任务权重，消除有害的辅助信号并增强有用的任务信号，实现了泛化性能的提升。 |
| [^23] | [Optimal sensor placement for reconstructing wind pressure field around buildings using compressed sensing.](http://arxiv.org/abs/2306.04518) | 本文提出了一种数据驱动的稀疏传感器选择算法，能够通过有限的测量位置简洁地重建风压力场，显著减少传感器数量并提供了稳定和最佳的传感器布置结果。 |
| [^24] | [Hardness of Deceptive Certificate Selection.](http://arxiv.org/abs/2306.04505) | 本文证明了利用AFC在高保真度和完备性的设定下，选择无信息证书的任务是 NP-hard 的。 |
| [^25] | [Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers.](http://arxiv.org/abs/2306.04504) | 本文评估了ChatGPT在生物医学任务上的表现，发现在生物数据集训练样本较小时，零样例ChatGPT甚至优于精调生成式变压器模型。由此表明ChatGPT具有在生物医学领域成为有价值工具的潜力。 |
| [^26] | [Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.](http://arxiv.org/abs/2306.04502) | 本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。 |
| [^27] | [Optimal Fair Multi-Agent Bandits.](http://arxiv.org/abs/2306.04498) | 本文针对多智能体之间公平多臂赌博机学习问题提出了一种算法，通过分布式拍卖算法学习样本最优匹配，使用一种新的利用阶段和一种基于顺序统计的遗憾分析实现，相较于先前的结果遗憾阶数从$O(\log T \log\log T)$到了$O\left(N^3 \log N \log T \right)$，能够更好地处理多个智能体之间的依赖关系。 |
| [^28] | [Limits, approximation and size transferability for GNNs on sparse graphs via graphops.](http://arxiv.org/abs/2306.04495) | 本文从理论角度研究了图神经网络在稀疏图中推广的问题。通过GraphOps极限概念，我们开发了关于不同大小图形上的GNN之间距离的定量界限和结构属性的共享。 |
| [^29] | [Fair Column Subset Selection.](http://arxiv.org/abs/2306.04489) | 解决了公平的列子集选择问题，通过已知方法基于确定性杠杆分数采样，提出了一种有效算法，可以在1.5倍的大小下实现与两倍相同的近似保证。 |
| [^30] | [Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards.](http://arxiv.org/abs/2306.04488) | 本文提出了 rewarded soup 方法，通过结合多种代理奖励，实现微调权重插值，从而在整个偏好空间中实现帕累托最优广义化。该方法在强化学习任务上具有有效性。 |
| [^31] | [Training-Free Neural Active Learning with Initialization-Robustness Guarantees.](http://arxiv.org/abs/2306.04454) | 本研究提出了一种期望方差与高斯过程（EV-GP）标准用于神经主动学习，该方法不需要对神经网络进行训练，并且保证在数据选择时NN具有良好的预测性能和初始化鲁棒性。 |
| [^32] | [Multi-modal Latent Diffusion.](http://arxiv.org/abs/2306.04445) | 该论文提出一个新的方法来处理多模态数据，该方法使用了一组独立训练的单模态确定性自编码器，将单个潜在变量连接到公共潜在空间中，并通过掩蔽扩散模型实现了良好的生成质量和模态间连贯性。 |
| [^33] | [Fast Optimal Locally Private Mean Estimation via Random Projections.](http://arxiv.org/abs/2306.04444) | 提出了一种名为ProjUnit的算法框架，用于实现高效的本地隐私均值估计，通过随机投影低维空间实现最优解，且具有低通信复杂度和快速的服务器运行时间。 |
| [^34] | [Dual policy as self-model for planning.](http://arxiv.org/abs/2306.04440) | 该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。 |
| [^35] | [Faithful Knowledge Distillation.](http://arxiv.org/abs/2306.04431) | 本文研究了知识蒸馏中教师和学生之间的相对校准问题，提出了一个忠实的模仿框架来解决学生置信度和软标签的问题，并提供了一种实证和认证的方法来评估学生模型的鲁棒性。 |
| [^36] | [Balancing of competitive two-player Game Levels with Reinforcement Learning.](http://arxiv.org/abs/2306.04429) | 本研究使用PCGRL框架和强化学习算法，提出了一种自动平衡基于图块的竞争性双人游戏级别的架构，其中引入了新型基于交换的表示方法，并通过分析代理的交换行为来判断哪些图块类型影响平衡。 |
| [^37] | [Towards High-Performance Exploratory Data Analysis (EDA) Via Stable Equilibrium Point.](http://arxiv.org/abs/2306.04425) | 本文提出了一种基于稳定平衡点的框架，用于提高探索性数据分析的效率和解决方案质量，该方法能够为大规模数据集生成高质量的聚类和数据可视化。 |
| [^38] | [On Computing Optimal Tree Ensembles.](http://arxiv.org/abs/2306.04423) | 该论文提出了两种新算法以计算旨在各种度量方面最优的决策树集合，并且引入了“证明树技术”来大大改进可处理性结果。 |
| [^39] | [Policy-Based Self-Competition for Planning Problems.](http://arxiv.org/abs/2306.04403) | 本文介绍了一种基于策略的自对抗算法，该算法将历史策略纳入规划过程中，从而实现代理的效率提升和强大的轨迹查找。 |
| [^40] | [A Fair Classifier Embracing Triplet Collapse.](http://arxiv.org/abs/2306.04400) | 本文提出一种公平分类器，利用三元组损失的边际限制机器学习模型所造成的偏见，并采用三元组折叠方法。 |
| [^41] | [Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance.](http://arxiv.org/abs/2306.04396) | 本文提出了一种使用非对称梯度引导来指导扩散采样的反向过程的方法，以改善扩散图像翻译的风格转换和内容保留之间的平衡。 |
| [^42] | [Multilingual Clinical NER: Translation or Cross-lingual Transfer?.](http://arxiv.org/abs/2306.04384) | 本研究比较了翻译和跨语言迁移两种方法来执行临床领域命名实体识别（NER），并证明跨语言迁移比这两种翻译方法在法语和德语中都具有更好的性能。 |
| [^43] | [Get More for Less in Decentralized Learning Systems.](http://arxiv.org/abs/2306.04377) | 本文提出了一种名为JWINS的分布式学习系统，它仅通过稀疏化的方式共享部分模型参数，使用小波变换来补偿由稀疏化引起的信息损失，并通过随机通信截断来减少通信用量。实验证明，JWINS可以在发送更少的字节的情况下实现与完全共享分布式学习相似的准确性。 |
| [^44] | [Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching.](http://arxiv.org/abs/2306.04376) | 本文提出一种名为DFM框架的方法，用于量化标签偏移，并证明了其性能上限和鲁棒性。使用基于核的DFM版本可以提高效率、可扩展性和鲁棒性。 |
| [^45] | [Learning via Wasserstein-Based High Probability Generalisation Bounds.](http://arxiv.org/abs/2306.04375) | 本文证明了基于Wasserstein距离的PAC-Bayesian泛化界限的新颖性，并提出了算法框架，该界限显著扩展了PAC-Bayesian界限的范围，并在经典的学习问题中展现了改进的泛化误差。 |
| [^46] | [Label Aware Speech Representation Learning For Language Identification.](http://arxiv.org/abs/2306.04374) | 本论文提出一种新的语音表示学习框架，即基于标签感知的语音表示学习（LASR）框架，将自我监督表示学习和语言标签信息相结合进行预训练，通过三元组的目标函数将二者结合，提高了语种识别的准确性，并且能够应对噪声/丢失标签的情况。 |
| [^47] | [Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction.](http://arxiv.org/abs/2306.04366) | 本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。 |
| [^48] | [Bayesian Optimisation Against Climate Change: Applications and Benchmarks.](http://arxiv.org/abs/2306.04343) | 本论文综述了在气候变化应用中使用贝叶斯优化的可行性和应用案例，提出了四个主要应用领域和相应的公共基准或数据集，并鼓励开发更全面的基准数据集。 |
| [^49] | [Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from Dynamic Contrast-Enhanced MRI.](http://arxiv.org/abs/2306.04339) | 该研究提出了一种非配对深度学习方法，使用物理驱动的CycleGAN框架，可以在没有配对数据的情况下准确地估计动态增强磁共振成像药代动力学参数和动脉输入函数。 |
| [^50] | [Changing Data Sources in the Age of Machine Learning for Official Statistics.](http://arxiv.org/abs/2306.04338) | 本文总结了机器学习时代官方统计学中，数据源变更所带来的风险、责任和不确定性，并提供一份清单列出高频的变更起因和原因。 |
| [^51] | [CaptAinGlove: Capacitive and Inertial Fusion-Based Glove for Real-Time on Edge Hand Gesture Recognition for Drone Control.](http://arxiv.org/abs/2306.04319) | CaptAinGlove是一款基于纺织品的手套，用于识别用于无人机控制的手势，具有低功耗、隐私保护和较高的准确性。 |
| [^52] | [Timing Process Interventions with Causal Inference and Reinforcement Learning.](http://arxiv.org/abs/2306.04299) | 该论文使用合成数据进行实验，比较了因果推断和强化学习方法在定时过程干预方面的优劣。实验结果表明，强化学习在效果和稳定性方面均优于因果推断方法。 |
| [^53] | [Phrase Retrieval for Open-Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning.](http://arxiv.org/abs/2306.04293) | 本文提出了一种使用短语检索的方法直接预测答案，以解决多轮对话问答中传统方法的漏洞和效率问题，同时引入了对话依赖建模和对比学习策略，提高了模型的鲁棒性和效果。 |
| [^54] | [Revising deep learning methods in parking lot occupancy detection.](http://arxiv.org/abs/2306.04288) | 本文提出了一种基于EfficientNet架构的停车位占用检测算法，并在5个不同的数据集上进行了评估，性能得到提高。 |
| [^55] | [ColNav: Real-Time Colon Navigation for Colonoscopy.](http://arxiv.org/abs/2306.04269) | 该论文提出了一种实时导航系统，该系统向医生提供可操作和易理解的指导，以指导未检查的结肠区域，从而提高检查质量和发现异常病变的准确性。 |
| [^56] | [Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning.](http://arxiv.org/abs/2306.04265) | 本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。 |
| [^57] | [Self-Adjusting Weighted Expected Improvement for Bayesian Optimization.](http://arxiv.org/abs/2306.04262) | 本文提出了一种新的自适应加权期望改进方法（SAWEI），可以自动平衡探索不确定区域和利用有承诺区域之间的权衡。在COCO基准测试中，该方法表现出有利的性能。 |
| [^58] | [Accounting For Informative Sampling When Learning to Forecast Treatment Outcomes Over Time.](http://arxiv.org/abs/2306.04255) | 本文针对存在信息抽样的观测数据，提出一种通过逆强度加权来学习治疗效果的通用框架，并提出了一种新方法TESAR-CDE。 |
| [^59] | [Adversarial Sample Detection Through Neural Network Transport Dynamics.](http://arxiv.org/abs/2306.04252) | 本文提出基于神经网络动力学的对抗样本检测器，并通过规范化向量场的训练方法来提高其干净输入和异常输入的区分度。实验结果表明，该方法在不同攻击下的表现优于其他检测器，同时也提高了对抗检测器的测试准确率。 |
| [^60] | [Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks.](http://arxiv.org/abs/2306.04251) | SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。 |
| [^61] | [Data Mining for Faster, Interpretable Solutions to Inverse Problems: A Case Study Using Additive Manufacturing.](http://arxiv.org/abs/2306.04228) | 本文探讨了针对逆问题的数据挖掘算法，通过锥形截断的修改，可以显著加速高斯过程代理的速度而不影响预测准确度，同时使用Kohonen自组织映射可以在高维空间中更好地解释问题的解决方案。 |
| [^62] | [Normalization Layers Are All That Sharpness-Aware Minimization Needs.](http://arxiv.org/abs/2306.04226) | SAM算法中，仅扰动规范化层可优化模型性能，在不同的网络架构中都适用，稀疏扰动方法不行。这发现对SAM算法的有效性产生怀疑。 |
| [^63] | [Efficient Vision Transformer for Human Pose Estimation via Patch Selection.](http://arxiv.org/abs/2306.04225) | 该论文提出了一种基于补丁选择的高效视觉Transformer方法，大幅度提高了处理速度和降低计算复杂度，用于2D人体姿态估计方面。 |
| [^64] | [Causally Learning an Optimal Rework Policy.](http://arxiv.org/abs/2306.04223) | 本文利用双重/无偏机器学习方法研究了光电半导体制造中的返工步骤，为零件返工制定策略并从经验上估计它们的价值。 |
| [^65] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^66] | [DualHGNN: A Dual Hypergraph Neural Network for Semi-Supervised Node Classification based on Multi-View Learning and Density Awareness.](http://arxiv.org/abs/2306.04214) | 本文提出了DualHGNN，利用双重超图神经网络模型，将超图结构学习和表示学习相结合，以更好地实现半监督节点分类。 |
| [^67] | [Migrate Demographic Group For Fair GNNs.](http://arxiv.org/abs/2306.04212) | 该论文提出了一个名为FairMigration的新框架，可以动态迁移族群，而不是用原始的敏感属性来固定族群，以训练公平的GNN。 |
| [^68] | [Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction.](http://arxiv.org/abs/2306.04203) | 本论文提出了一种不依赖于大规模知识图谱或预训练语言模型的关系提取方法，通过利用语料库中实体的分层结构和关系分布，将预先训练的知识图谱嵌入到句子级上下文表示中，可以显著提高模型性能。 |
| [^69] | [Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models.](http://arxiv.org/abs/2306.04201) | 本文改进了高斯过程模型中的超参数学习，提出了一种混合训练方法来兼顾变分推断和期望传播方法，以优化超参数的学习目标，该方法实验结果表明有效性。 |
| [^70] | [An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to First Graders.](http://arxiv.org/abs/2306.04190) | 研究开发了两个新的ASR系统并与先前的研究进行比较，结果表明优化ASR系统提供给小学一年级学生的反馈可以提高他们的阅读进步。 |
| [^71] | [Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks.](http://arxiv.org/abs/2306.04186) | 本文提出了两种自监督音频表示学习方法：ATST-Clip和ATST-Frame；这两种方法使用基于Transformer的师生模型，通过音频实例辨别和音频序列重构两个自监督任务进行训练；评估结果表明，这两种方法在片段级和帧级下游任务上都优于其他最先进的预训练方法。 |
| [^72] | [Benchmarking Foundation Models with Language-Model-as-an-Examiner.](http://arxiv.org/abs/2306.04181) | 本文提出了一种新的基准测试框架，使用语言模型作为考官，可以无参考方式评估答案。这个框架解决了过去基准测试流程中的测试泄漏和评估自动化问题，并允许易于扩展，可以采用不同的语言模型作为考官。 |
| [^73] | [Optimal Transport Model Distributional Robustness.](http://arxiv.org/abs/2306.04178) | 本文提出了一种优化输运模型的分布鲁棒性框架，能够显著提高深度学习模型的鲁棒性，可灵活地将锐度感知纳入到单个模型、集成模型和贝叶斯神经网络的训练中。 |
| [^74] | [End-to-End Learning for Stochastic Optimization: A Bayesian Perspective.](http://arxiv.org/abs/2306.04174) | 本文提出了一种基于贝叶斯视角的随机优化端到端学习方法，为经验风险最小化和分布式鲁棒优化问题提供新的端到端学习算法，方式主要是训练决策映射。该方法在合成的newsvendor问题和经济分配问题上均表现出显著的效果，同时也发现决策映射神经网络架构对测试性能的影响很大。 |
| [^75] | [Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation.](http://arxiv.org/abs/2306.04169) | 本文提出了一种高效的求解加权低秩逼近问题的交替最小化框架，运行时间优化到了 n^2k，核心方法是一种高精度的多响应回归方法。 |
| [^76] | [Rethinking Weak Supervision in Helping Contrastive Learning.](http://arxiv.org/abs/2306.04160) | 本论文探讨了使用半监督和噪声标签为弱监督的对比学习，并建立了一个谱聚类框架用于转换弱监督信息。 |
| [^77] | [SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet.](http://arxiv.org/abs/2306.04148) | 该论文介绍了一种名为SANGEET的基于XML的公共数据集，用于存储北印度古典音乐作品的全面信息，并支持从机器学习视角进行音乐信息的数据驱动分析。 |
| [^78] | [UCTB: An Urban Computing Tool Box for Spatiotemporal Crowd Flow Prediction.](http://arxiv.org/abs/2306.04144) | UCTB是一个城市计算工具箱，它在时空人群流预测方面整合了多个领域知识和最先进的模型，可解决该领域复杂度高、知识多样、模型实现复杂的问题。 |
| [^79] | [A Survey on Generative Diffusion Models for Structured Data.](http://arxiv.org/abs/2306.04139) | 本文全面综述了在结构化数据领域中最近提出的扩散模型，介绍了其理论基础和应用场景。 |
| [^80] | [Answering Compositional Queries with Set-Theoretic Embeddings.](http://arxiv.org/abs/2306.04133) | 本文提出了一种基于盒状嵌入的方法来回答组合查询，该方法可以更准确地支持含有多个属性的查询。 |
| [^81] | [Multimodal Fusion Interactions: A Study of Human and Automatic Quantification.](http://arxiv.org/abs/2306.04125) | 本文比较研究了两种人类注释者可以用于注释多模态交互的分类，并提出了一种基于信息分解的分类学。 |
| [^82] | [Retrosynthesis Prediction with Local Template Retrieval.](http://arxiv.org/abs/2306.04123) | 本文介绍了一种新方法，RetroKNN，它使用基于本地反应模板的k最近邻检索结合神经网络预测来提高逆向合成模型的性能。 |
| [^83] | [MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation.](http://arxiv.org/abs/2306.04120) | MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。 |
| [^84] | [M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method.](http://arxiv.org/abs/2306.04118) | 本文提出了一种名为M$^3$Fair的新方法，可以在多个数据层面和多个敏感属性的情况下平衡训练数据分布，缓解医疗数据中的偏见和不公平现象，提高机器学习模型的公平性能和准确性能。 |
| [^85] | [Unbalanced Optimal Transport for Unbalanced Word Alignment.](http://arxiv.org/abs/2306.04116) | 本文通过最优输运，实现了既重视对齐又重视空对齐的不平衡单语词对齐，实验结果表明其在具有挑战性的数据集上表现出色。 |
| [^86] | [Quasi-Newton Updating for Large-Scale Distributed Learning.](http://arxiv.org/abs/2306.04111) | 本文提出了一种具有出色统计、计算和通信效率的分布式拟牛顿(DQN)框架，与现有方法相比，它不需要牛顿矩阵求逆或通信，并且通过理论证明和数值分析证明其统计特性和有限的样本性能。 |
| [^87] | [Membership inference attack with relative decision boundary distance.](http://arxiv.org/abs/2306.04109) | 本论文提出了一种新的标签仅成员身份推断攻击方法，使用相对决策边界距离来避免在不同的初始图像下产生相反的结果。 |
| [^88] | [BeMap: Balanced Message Passing for Fair Graph Neural Network.](http://arxiv.org/abs/2306.04107) | 本文提出了一种公平的消息传递方法，称为BeMap，旨在解决消息传递中的偏差放大问题，通过平衡感知的采样策略来平衡不同人口群体的1-hop邻居的数量。 |
| [^89] | [Phoenix: A Federated Generative Diffusion Model.](http://arxiv.org/abs/2306.04098) | 本文提出了 Phoenix 一种联邦式生成扩散模型，利用联邦学习技术跨多个数据源进行训练，实现生成质量更好的图像。该模型在保护数据隐私的前提下，提高了生成样本的数据多样性。 |
| [^90] | [A novel deeponet model for learning moving-solution operators with applications to earthquake hypocenter localization.](http://arxiv.org/abs/2306.04096) | 本研究介绍了X-DeepONet，这是一种新型的DeepONets变体，用于学习移动解算器，并应用于实时地震定位。通过将地震到时和速度模型的信息结合起来，X-DeepONet学习估计与地震源相关的走时场，并通过根网络解决了标准DeepONet无法捕获场的重定位的问题。 |
| [^91] | [Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks.](http://arxiv.org/abs/2306.04073) | Patch-level Routing in Mixture-of-Experts 可以证明在 CNN 中具有采样效率，其用于 patch-level 路由的两个机制分别是专家的有效参数数量显著降低以及选择相关特征，从而提高深度学习神经网络的特征利用效率。 |
| [^92] | [Simple High Quality OoD Detection with L2 Normalization.](http://arxiv.org/abs/2306.04072) | 本篇论文介绍了在ResNet模型训练中应用L2归一化技术，可以以几乎没有成本的方式实现与最先进的OoD检测性能相媲美的结果，测试时仅需移除L2归一化即可。 |
| [^93] | [Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings.](http://arxiv.org/abs/2306.04064) | 本文提出一种方法，通过自编码器生成通用鲁棒嵌入，将分类数据转换为向量，从而实现对抗训练，提高分类器的鲁棒性，并在多个数据集上取得了最先进的对抗鲁棒性性能表现。 |
| [^94] | [RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain.](http://arxiv.org/abs/2306.04054) | RescueSpeech是一个用于搜救领域语音识别的德语语音数据集，但目前最先进的方法仍无法令人满意。 |
| [^95] | [LLMZip: Lossless Text Compression using Large Language Models.](http://arxiv.org/abs/2306.04050) | 本研究使用大型语言模型提出了一种结合预测和无损压缩方案的英文文本压缩算法，并在初步实验中表现优于当前最先进的文本压缩方案。 |
| [^96] | [One-sided Matrix Completion from Two Observations Per Row.](http://arxiv.org/abs/2306.04049) | 本文研究了单边矩阵完成问题，在每行只有两个观测值的情况下，使用插值算法可以可靠恢复$X^TX$，进而恢复$X$的右奇异向量。 |
| [^97] | [Active Sparse Conversations for Improved Audio-Visual Embodied Navigation.](http://arxiv.org/abs/2306.04047) | 本文提出了CAVEN - 一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕回答以协助自主导航。该系统基于多模态分层强化学习方法，并使用三个低级策略进行引导。 |
| [^98] | [FedVal: Different good or different bad in federated learning.](http://arxiv.org/abs/2306.04040) | 本文提出了FedVal方法，它是一个不需要从客户端获取任何附加信息的全新方法，可同时具有稳健和公平性，并通过评分函数在服务器端验证客户端更新，以确定本地训练模型之间的最佳聚合平衡。 |
| [^99] | [Revisiting Neural Retrieval on Accelerators.](http://arxiv.org/abs/2306.04039) | 本文提出了一种名为混合对数模型（MoL）的非点积检索方法。该方法通过自适应地组合基本相似度函数来建模用户与文本之间的相似度，以更好地捕捉高阶用户-文本交互作用并进一步推广到长尾数据中。结合分层检索策略（h-indexer），本文成功将MoL扩展到单个GPU上的100M个文本语料库。 |
| [^100] | [Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification.](http://arxiv.org/abs/2306.04037) | 该论文定量分析了用于遥感图像分类的可解释人工智能技术，探究了不同属性的XAI方法，提供选取合适方法以深入了解模型决策的见解和建议。 |
| [^101] | [BokehOrNot: Transforming Bokeh Effect with Image Transformer and Lens Metadata Embedding.](http://arxiv.org/abs/2306.04032) | 本文提出了一种利用镜头元数据嵌入模型的新方法，将Bokeh效果从模糊到清晰和清晰到模糊进行转换，表现出自然的效果，并优于当前领先的Bokeh效果渲染和图像恢复模型。 |
| [^102] | [Intervention Generalization: A View from Factor Graph Models.](http://arxiv.org/abs/2306.04027) | 本文提出了一种基于因子图模型的“干预因子模型”(IFM)方法，仅基于对操纵系统分布的因子分解的最小假设，以实现从过去的实验到新的条件的跃迁。 |
| [^103] | [Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory.](http://arxiv.org/abs/2306.04026) | 本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。 |
| [^104] | [Energy-Based Models for Cross-Modal Localization using Convolutional Transformers.](http://arxiv.org/abs/2306.04021) | 本文提出了一种基于能量模型和卷积变换的跨模态定位方法，利用卫星图像进行地图构建，可在没有GPS的情况下实现精确的度量级别定位，并在KITTI数据集上取得了更高的定位精度。 |
| [^105] | [Green Steganalyzer: A Green Learning Approach to Image Steganalysis.](http://arxiv.org/abs/2306.04008) | Green Steganalyzer是一种用于图像隐写分析的基于绿色学习范式的新方法，能够通过像素异常预测、嵌入位置检测和决策融合来对图像进行识别，与现有深度学习模型相比，具有更低的计算复杂度和更小的模型大小。 |
| [^106] | [Randomized Schur Complement Views for Graph Contrastive Learning.](http://arxiv.org/abs/2306.04004) | 该论文介绍了一种基于舒尔补的随机拓扑增强器，用于图形对比学习。相比以往的方法，该技术通过生成增强视图来提高网络性能，并在节点和图分类基准上实现了最先进的结果。 |
| [^107] | [One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from Electromagnetic Solvers.](http://arxiv.org/abs/2306.04001) | 该论文提出了一种使用深度生成模型为基础的方法，使用一维深度图像先验拟合电磁求解器的S参数。通过与公开可用和专有的行业标准拟合方法的比较，实验结果表明该方法在重建质量和计算时间方面均有显着改进。 |
| [^108] | [Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification.](http://arxiv.org/abs/2306.03993) | 本文提出了一个新的实时在线无监督领域自适应设置（R$^2$OUDA），并引入了一个新颖的多摄像头系统R$^2$MMT。通过R$^2$OUDA，本文解决了现实应用中被忽略的四个主要限制，以实现真正的实时在线无监督领域自适应。 |
| [^109] | [Agent Performing Autonomous Stock Trading under Good and Bad Situations.](http://arxiv.org/abs/2306.03985) | 本文介绍了使用深度强化学习方法训练机器代理来自主执行股票交易的研究，并在不同市场环境下进行了评估。 |
| [^110] | [Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs.](http://arxiv.org/abs/2306.03984) | 本文提出了一种新的对话质量注释工作流程称为DQA，能够更准确和可推广地评估对话质量，尤其是通过一些客观对话属性的判断。 |
| [^111] | [Globally injective and bijective neural operators.](http://arxiv.org/abs/2306.03982) | 这篇论文研究了网络学习的运算符是否是单射和满射的情况，并给出了精确条件。它们提供的单射神经运算符是通用逼近器，并且使用有限秩神经网络实现它们，使得网络仍然单射。 |
| [^112] | [Explainable AI using expressive Boolean formulas.](http://arxiv.org/abs/2306.03976) | 提出了一种表达式布尔公式的可解释人工智能，利用本地优化技术训练分类模型，可以应用于信用评分和医疗病况的诊断，并具有未来应用的潜力。 |
| [^113] | [Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels.](http://arxiv.org/abs/2306.03968) | 本文提出了使用神经切向核的随机边际似然梯度，可以加速基于梯度的超参数优化过程。 |
| [^114] | [PILLAR: How to make semi-private learning more effective.](http://arxiv.org/abs/2306.03962) | 本文提出了一种计算效率高的算法 PILLAR，可以在半监督半私有（SP）学习中明显降低私有标记样本复杂度，并可以在实际数据集上高效运行，可以利用在公共数据上预训练的网络提取的特征，并在实验证明了其显著有效性。 |
| [^115] | [Recognition of Handwritten Japanese Characters Using Ensemble of Convolutional Neural Networks.](http://arxiv.org/abs/2306.03954) | 该研究提出了一种机器学习方法，即使用卷积神经网络集合来高效地识别手写汉字，能够实现高达99.4%的分类准确率。 |
| [^116] | [Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor.](http://arxiv.org/abs/2306.03951) | 该论文介绍了如何使用强化学习算法来实现CrazyFlie 2.X四轴飞行器的控制和导航，同时结合PID控制和灯塔定位系统，以实现更加精确的控制。 |
| [^117] | [Partial Inference in Structured Prediction.](http://arxiv.org/abs/2306.03949) | 本文研究了结构化预测中的问题，通过生成模型和凸优化算法，提出了可证明保证的部分标签恢复方法。 |
| [^118] | [A scientometric analysis of the effect of COVID-19 on the spread of research outputs.](http://arxiv.org/abs/2306.03941) | 本文通过科学计量分析COVID-19相关研究在全球和意大利的产量，发现美国和中国的研究活动最为活跃，在医学生物学领域的文献产出增长最快，并探讨了出版物数量和死亡人数之间的关系。 |
| [^119] | [Learning Causal Mechanisms through Orthogonal Neural Networks.](http://arxiv.org/abs/2306.03938) | 本文介绍了一种无监督的方法来学习一组独立机制的反向操作，以发现和分离一组独立的机制。 |
| [^120] | [Guiding The Last Layer in Federated Learning with Pre-Trained Models.](http://arxiv.org/abs/2306.03937) | 本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。 |
| [^121] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^122] | [Designing Decision Support Systems Using Counterfactual Prediction Sets.](http://arxiv.org/abs/2306.03928) | 本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。 |
| [^123] | [Multi-constrained Symmetric Nonnegative Latent Factor Analysis for Accurately Representing Large-scale Undirected Weighted Networks.](http://arxiv.org/abs/2306.03911) | 本研究提出一种多重约束的对称非负潜在因子分析模型，以准确地表示大规模无向加权网络，并克服了现有模型计算复杂度高和建模策略狭窄的缺点。 |
| [^124] | [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.](http://arxiv.org/abs/2306.03901) | ChatDB项目将SQL数据库作为符号内存，增强LLMs的复杂多跳推理能力。 |
| [^125] | [Deductive Verification of Chain-of-Thought Reasoning.](http://arxiv.org/abs/2306.03872) | 本文旨在通过应用演绎验证技术，使语言模型能够进行明确而严谨的演绎推理，以确保其推理过程的可信度。 |
| [^126] | [Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic Knowledge Graph and Text Mining Approach.](http://arxiv.org/abs/2306.03833) | 本研究提出了一种多模态动态知识驱动退出预测（MDKDP）框架，能够解决虚拟健康中不同利益相关者之间和医疗保健交付系统之间的信息不对称问题，提高了退出预测的性能。 |
| [^127] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^128] | [Vid2Act: Activate Offline Videos for Visual RL.](http://arxiv.org/abs/2306.03360) | Vid2Act是一种基于模型的强化学习方法，它通过使用世界模型来传输领域相关的动态和策略，从而显著提高了样本效率。 |
| [^129] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^130] | [Switching Autoregressive Low-rank Tensor Models.](http://arxiv.org/abs/2306.03291) | 该文提出了一种切换自回归低秩张量（SALT）模型，它将自回归隐Markov模型（ARHMM）和切换线性动态系统（SLDS）的优点结合起来，通过低秩参数化提高了模型性能。 |
| [^131] | [AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and Practices.](http://arxiv.org/abs/2306.03025) | 本文回顾了近期牙科CBCT成像中基于AI的病变检测、错颌分类、颊骨厚度测量、牙齿、颌骨、下颌骨、标志、轮廓和咽喉气道的分类和分割等任务，应用机器学习和深度学习算法等技术。 |
| [^132] | [A Large-Scale Study of Probabilistic Calibration in Neural Network Regression.](http://arxiv.org/abs/2306.02738) | 本研究对于神经网络的概率校准在回归问题中进行了最大的实证研究，发现正则化方法在校准和锐度之间提供了有利的平衡，而后续方法表现更优越，此外还证明了分位数重新校准可以被认为是合规性预测的一个特定情况。 |
| [^133] | [Federated Deep Learning for Intrusion Detection in IoT Networks.](http://arxiv.org/abs/2306.02715) | 本研究提出了一种基于联邦深度学习的入侵检测框架，可在保护数据隐私和局部性的同时实现高效的模型收敛，该框架在物联网入侵检测应用中具有潜力。 |
| [^134] | [Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context.](http://arxiv.org/abs/2306.02689) | 本文提出了一个新的深度学习框架Equity-Transformer来解决大规模的最小最大路径问题。该模型利用可扩展的深度学习模型进行顺序决策，并生成考虑公平工作负载的顺序动作。研究显示，Equity-Transformer在两个代表性最小最大路径问题中具有卓越的性能。 |
| [^135] | [Meta-SAGE: Scale Meta-Learning Scheduled Adaptation with Guided Exploration for Mitigating Scale Shift on Combinatorial Optimization.](http://arxiv.org/abs/2306.02688) | 本研究提出了一种名为Meta-SAGE的新方法，用于解决组合优化任务中深度强化学习模型可扩展性的问题。该方法通过比例元学习和时间表调整来适应模型，并真实地优化了相关任务的性能表现。 |
| [^136] | [bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark.](http://arxiv.org/abs/2306.02349) | 提出了bgGLUE，这是一个用于在保加利亚语上评估语言模型在自然语言理解（NLU）任务上的基准。该基准测试包括针对各种自然语言处理问题（例如，自然语言推理、事实检查、命名实体识别、情感分析、问答等）和机器学习任务的NLU任务，评估结果表明，在序列标记任务方面表现强劲，但需要更复杂的推理任务还有很大的提升空间。 |
| [^137] | [Fine-Tuning Language Models with Advantage-Induced Policy Alignment.](http://arxiv.org/abs/2306.02231) | 本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。 |
| [^138] | [SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization.](http://arxiv.org/abs/2306.01981) | SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。 |
| [^139] | [GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction.](http://arxiv.org/abs/2306.01951) | 本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。 |
| [^140] | [Physics-informed UNets for Discovering Hidden Elasticity in Heterogeneous Materials.](http://arxiv.org/abs/2306.01204) | 本文提出了一种新型的 UNet 神经网络模型 (El-UNet)，能够通过物理学知识增强和应变图像的分析，精确地推断生物软组织中材料参数的空间分布，具有高性能的优劣和计算效率。 |
| [^141] | [Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features.](http://arxiv.org/abs/2306.00934) | 基于PROVEXPLAINER框架，通过复制GNN-based security models的决策过程，利用决策树和图结构特征将抽象GNN决策边界投影到可解释的特征空间，以增强GNN安全模型的透明度和询问能力。 |
| [^142] | [Introduction to Medical Imaging Informatics.](http://arxiv.org/abs/2306.00421) | 本文介绍了医学成像信息学的基本概念和最新进展，包括图像处理、特征工程、机器学习、计算机视觉和深度学习，以及如何将它们应用于疾病检测、诊断和预后预测模型的开发。这对于理解信息学在医学中的作用以及其对患者护理的潜在影响具有重要意义。 |
| [^143] | [Learning Gaussian Mixture Representations for Tensor Time Series Forecasting.](http://arxiv.org/abs/2306.00390) | 本文提出了一种新的张量时间序列预测框架GMRL，它可以单独模拟时间、位置和源变量中所暗示的每个异构性组件，相比于最先进的基准方法，本文的方法显示出了优越性。 |
| [^144] | [Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning.](http://arxiv.org/abs/2306.00088) | 本文提出了一种面向关系计算的自动微分方法，实验表明该方法可达到非常大的数据集规模，并在大规模分布式机器学习中表现出与专用系统相媲美的竞争力。 |
| [^145] | [Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization.](http://arxiv.org/abs/2305.19903) | 本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。 |
| [^146] | [GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts.](http://arxiv.org/abs/2305.19111) | 本文提出了一种新的方法，使用GAN来训练Learnable-MPC策略，以便在演示者和模仿者代理不能相同时进行参数化的成本函数学习。 |
| [^147] | [Test-Time Training on Nearest Neighbors for Large Language Models.](http://arxiv.org/abs/2305.18466) | 该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。 |
| [^148] | [Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach.](http://arxiv.org/abs/2305.18088) | 本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。 |
| [^149] | [Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication.](http://arxiv.org/abs/2305.17341) | 本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。 |
| [^150] | [PyTorch Hyperparameter Tuning -- A Tutorial for spotPython.](http://arxiv.org/abs/2305.11930) | 本文介绍了如何将spotPython超参数调谐器集成到PyTorch训练工作流中，以提高机器或深度学习模型的性能，以CIFAR10图像分类器为例。 |
| [^151] | [Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks.](http://arxiv.org/abs/2305.05089) | 本文对于单隐藏层双曲正切结构给出了单元冗余和可约的功能等价类的算法刻画，并证明了这样的功能等价类是分段线性路径连通的集合。 |
| [^152] | [Multi-Domain Learning From Insufficient Annotations.](http://arxiv.org/abs/2305.02757) | 提出了一种名为多领域对比学习（MDCL）的新方法，在原有方法的基础上，利用来自标记和未标记数据的语义和结构信息，解决了不充分注释的问题，并在实验中取得了优异的成果。 |
| [^153] | [A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation.](http://arxiv.org/abs/2304.13840) | 本文提出了一个深度学习框架，用于训练Verilog自动完成模型，能够有效预测下一个token，为设计和验证自动化提供了解决方案。 |
| [^154] | [Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models.](http://arxiv.org/abs/2304.13835) | 本文通过收集和评估多方对话情况，探讨了模型在群体对话中需要具备的技能，发现新数据集MultiLIGHT可以在这个领域带来显着的进展。 |
| [^155] | [Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation.](http://arxiv.org/abs/2304.06051) | Open-TransMind是智能交通领域第一个基础模型挑战赛的新基准，旨在解决数据量少、泛化能力差以及缺乏多模态技术等典型挑战。 |
| [^156] | [Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing.](http://arxiv.org/abs/2304.06009) | 该论文进行了计算机视觉在物流和仓储中应用领域的文献综述，将文献分为监视和操作两个领域，并指出了未来研究方向和计算机视觉的最新发展。研究结果对于物流从业者也有参考价值。 |
| [^157] | [ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation.](http://arxiv.org/abs/2304.05977) | ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。 |
| [^158] | [NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs.](http://arxiv.org/abs/2304.04027) | 该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。 |
| [^159] | [Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge.](http://arxiv.org/abs/2304.03392) | 该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。 |
| [^160] | [BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware.](http://arxiv.org/abs/2303.17727) | BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。 |
| [^161] | [Language Models can Solve Computer Tasks.](http://arxiv.org/abs/2303.17491) | 本文研究表明，预训练的大型语言模型代理可以通过一个简单的提示方案使用自然语言执行计算机任务，该方法取得了很好的效果并在MiniWoB++基准测试中超越了监督学习和强化学习方法。 |
| [^162] | [Mixed Autoencoder for Self-supervised Visual Representation Learning.](http://arxiv.org/abs/2303.17152) | 本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。 |
| [^163] | [A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data.](http://arxiv.org/abs/2303.14068) | 本文提出了一种基于1D CNN-LSTM结构的船舶轨迹关联算法，采用多变量时间序列问题来解决 |
| [^164] | [What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.](http://arxiv.org/abs/2303.11249) | 本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。 |
| [^165] | [FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features.](http://arxiv.org/abs/2303.07852) | 该论文提出了一个新的胎儿模型超声数据集FPUS23，用于确定胎儿方向，胎位和解剖学特征。研究结果表明，FPUS23可以为临床超声监测工作流程带来改善，以及可能开发一个家庭使用的基于超声的胎儿监护平台。 |
| [^166] | [Meta-learning Control Variates: Variance Reduction with Limited Data.](http://arxiv.org/abs/2303.04756) | 该论文提出了一种元学习控制变量的方法，可在有限数据的情况下减小蒙特卡罗估计器的方差，并对多个任务进行处理。 |
| [^167] | [Extrapolative Controlled Sequence Generation via Iterative Refinement.](http://arxiv.org/abs/2303.04562) | 本文提出了一种名为ICE的新方法来解决外推控制序列生成问题，该方法使用迭代控制编辑技术，能够在自动设计领域，特别是药物研究领域取得较优的性能表现。 |
| [^168] | [Streaming Active Learning with Deep Neural Networks.](http://arxiv.org/abs/2303.02535) | 本文提出了VeSSAL算法，用于在流式设置下对深度神经网络进行批量主动学习。该算法不需要提前获得整个数据集的离线访问权限，可以从遇到的点的样本组中抽样查询标签。该方法可以在不确定性和查询样本多样性之间进行权衡，以达到所需的查询率，拓展了深度神经网络的应用范围。 |
| [^169] | [Do Machine Learning Models Learn Statistical Rules Inferred from Data?.](http://arxiv.org/abs/2303.01433) | 本文提出了一个框架SQRL，可以无监督地从模型的训练数据中推断出统计规则；在分类、目标检测和数据填充等任务中，最先进的模型通常会违反这些规则，但是通过在测试时间内对模型进行适应，违规行为可以显著减少并提高模型的性能。 |
| [^170] | [Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows.](http://arxiv.org/abs/2302.12235) | 本文研究如何通过将开放量子系统动力学重构为概率分布$Q$的偏微分方程（PDE）从而解决对正则化流的限制，在建模上实现无缝连接。通过此方法，我们可以使用现成的深度生成式模型（如正则化流）对Q函数进行建模。 |
| [^171] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^172] | [Group Fairness with Uncertainty in Sensitive Attributes.](http://arxiv.org/abs/2302.08077) | 该论文提出了一种基于自助法的算法来解决存在敏感属性不确定性的群体公平性问题，该算法在真实的信用贷款数据集上表现出优异的性能。 |
| [^173] | [Cliff-Learning.](http://arxiv.org/abs/2302.07348) | 本研究探究了基于基础模型的迁移学习在低数据状态下的数据缩放，发现了一种称为悬崖学习的现象，它反映了学习算法的先验知识与任务之间的兼容程度。 |
| [^174] | [A Modern Look at the Relationship between Sharpness and Generalization.](http://arxiv.org/abs/2302.07011) | 研究表明，尖度与泛化关系并不密切相关，而与某些训练参数呈正相关或负相关。 |
| [^175] | [Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems.](http://arxiv.org/abs/2302.04445) | 本文提出了一种名为量子多智能体演员-评论网络的新算法，用于自主构建一个强大的移动访问系统，利用多个无人机，并利用量子计算的原则以提高其训练过程和推理能力。 |
| [^176] | [Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy.](http://arxiv.org/abs/2302.03542) | 该论文提出了一种代理算法，通过使用一个易于访问的函数作为代理，可以以与原函数梯度下降相匹配的速度收敛，从而显著提高样本效率，并在机器学习中具有许多潜在应用。 |
| [^177] | [Protecting Language Generation Models via Invisible Watermarking.](http://arxiv.org/abs/2302.03162) | 本文提出了一种名为 GINSEW 的新方法，通过将秘密信号注入到每个目标标记的解码步骤的概率向量中，保护文本生成模型，有效识别出侵权行为，对模型的影响很小。 |
| [^178] | [Flat Seeking Bayesian Neural Networks.](http://arxiv.org/abs/2302.02713) | 本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。 |
| [^179] | [Rethinking Robust Contrastive Learning from the Adversarial Perspective.](http://arxiv.org/abs/2302.02502) | 研究揭示了在标准训练网络中对抗性和干净性表示之间存在重大差异，对抗训练能够缓解这种差异并促进表示收敛到一个通用集合中，无论使用哪种学习方案。增加对抗和干净表示之间的相似度可以增强网络的鲁棒性。 |
| [^180] | [Counterfactual Identifiability of Bijective Causal Models.](http://arxiv.org/abs/2302.02228) | 本文研究逆向可辨识双射因果模型，确立了其在三种常见因果结构下的逆向可辨识性，提出了一种实用的学习方法，可以用于有效的逆向预测估计。 |
| [^181] | [A Theory of Link Prediction via Relational Weisfeiler-Leman.](http://arxiv.org/abs/2302.02209) | 本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。 |
| [^182] | [Convergence Analysis of Sequencial Split Learning on Heterogeneous Data.](http://arxiv.org/abs/2302.01633) | 本文推导出了顺序分割学习在异构数据上收敛的保证，并且证明了它在异构数据上优于联邦平均算法。 |
| [^183] | [Dual Propagation: Accelerating Contrastive Hebbian Learning with Dyadic Neurons.](http://arxiv.org/abs/2302.01228) | 本论文提出了一种双向传播的方法，使用具有两个内在状态的二元组神经元模型来加速对比海比学习，并且只需要单个推断阶段，可降低计算时间和能量消耗。 |
| [^184] | [Interventional and Counterfactual Inference with Diffusion Models.](http://arxiv.org/abs/2302.00860) | 本论文提出了基于扩散模型的因果模型 (DCM)，它可以在只有观测数据和因果图可用的情况下进行干预和反事实推断，其具有较好的表现。同时，论文还提供了一种分析反事实估计的方法，可以应用于更广泛的场景。 |
| [^185] | [Revisiting Bellman Errors for Offline Model Selection.](http://arxiv.org/abs/2302.00141) | 本文重新审视 Bellman Errors，发现之前的Bellman Errors 方法需要在特定条件下才能表现良好，同时提出了更准确的 MSBE 估计器，在离散控制任务方面表现出色。 |
| [^186] | [Differentially Private Distributed Bayesian Linear Regression with MCMC.](http://arxiv.org/abs/2301.13778) | 提出了一种带有 MCMC 的差分隐私分布式贝叶斯线性回归算法，提供了快速版本，具有计算上的优势，并在实际数据和模拟数据上进行数字实验，结果表明算法能够提供全面的估计和预测。 |
| [^187] | [Planning Multiple Epidemic Interventions with Reinforcement Learning.](http://arxiv.org/abs/2301.12802) | 本文将找到最优化的疫情计划转化为马尔可夫决策过程，并利用强化学习算法来搜索最小化疾病和经济成本的计划。 |
| [^188] | [Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds.](http://arxiv.org/abs/2301.12485) | 本论文介绍了一种新型的蛋白质生成模型Genie，可以使用等变神经网络和扩散概率模型生成新颖、可设计、多样化的蛋白质结构。这将有助于开发新的蛋白质治疗和材料。 |
| [^189] | [Smooth Non-Stationary Bandits.](http://arxiv.org/abs/2301.12366) | 本文提出了一种非平稳两臂赌博机问题的策略，能够处理平滑变化，并证明了该策略在二次Lipschitz连续的情况下的遗憾为 $\tilde O(T^{3/5})$。 |
| [^190] | [Rigid body flows for sampling molecular crystal structures.](http://arxiv.org/abs/2301.11355) | 本文介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计。通过在单位四元数群上定义平滑和表现力强的流以及定义适当的密度，在旋转群上进行训练，我们可以成功地采样分子晶体结构。 |
| [^191] | [Random Grid Neural Processes for Parametric Partial Differential Equations.](http://arxiv.org/abs/2301.11040) | 本论文引入了一种新的随机网格神经算法，用于处理参数化偏微分方程，创新地将概率测度赋予空间域，形成高斯过程模型，提供了一种解决数据受噪声干扰问题的方法。 |
| [^192] | [Super-Resolution Analysis via Machine Learning: A Survey for Fluid Flows.](http://arxiv.org/abs/2301.10937) | 本文调研了基于机器学习的超分辨率分析在涡流流动中的应用研究，并提供了物理启发的模型设计来成功重建流场。该研究可用于处理数值和实验流动数据的超分辨率分析。 |
| [^193] | [Trustworthiness Score to Evaluate CNNs Predictions.](http://arxiv.org/abs/2301.08839) | 本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。 |
| [^194] | [Tracr: Compiled Transformers as a Laboratory for Interpretability.](http://arxiv.org/abs/2301.05062) | Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。 |
| [^195] | [Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data.](http://arxiv.org/abs/2301.00437) | 研究者证明对于均方误差和交叉熵损失，深度线性网络中出现的全局解在不同数据上都具有神经塌陷的特性，即最后一层特征会崩溃为类均值，而这些类均值是等角紧框架的顶点。 |
| [^196] | [Policy Gradient in Robust MDPs with Global Convergence Guarantee.](http://arxiv.org/abs/2212.10439) | 本文提出了一个新的双循环鲁棒策略梯度（DRPG）算法，是首个通用的RMDP策略梯度方法，通过单调地减少近似误差来保证在表格RMDP中全局最优策略的收敛性。 |
| [^197] | [Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization.](http://arxiv.org/abs/2212.09567) | 提出了 QTO（查询计算树优化）以有效地回答复杂逻辑查询，通过查询计算树上的正反向传播找到了精确的最优解，并利用了查询计算树中的独立性来减少搜索空间。 |
| [^198] | [Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics, Directional Convergence, and Equilibria.](http://arxiv.org/abs/2212.02457) | 协变量转移和对抗扰动对统计学习的稳健性提出了挑战。本文在无限维度的情况下研究了对抗协变量转移对外推区域的影响以及其对后续学习的平衡的影响。 |
| [^199] | [SinDDM: A Single Image Denoising Diffusion Model.](http://arxiv.org/abs/2211.16582) | 本文介绍了一种称为SinDDM的单图像去噪扩散模型。该模型使用单张图像进行训练，并可生成高质量的任意维度的样本。此外，它还可以通过外部监督进行指导，可用于多种任务，包括样式转换和文本引导生成。 |
| [^200] | [PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting.](http://arxiv.org/abs/2211.15046) | 本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。 |
| [^201] | [Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning.](http://arxiv.org/abs/2211.14666) | 本文提供了证据表明，脱耦表示与稀疏基预测器相结合可提高泛化能力。我们提出了一个实用方法来学习这种表示，并在少样本分类基准测试中取得了竞争性的结果。 |
| [^202] | [Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement.](http://arxiv.org/abs/2211.13585) | 本文提出了一种可持续优化长期用户参与度的框架，学习促进和维持长期参与的最优休息策略，避免了盲目推动用户增加消费导致燃尽、流失和成瘾的风险。 |
| [^203] | [A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs.](http://arxiv.org/abs/2211.02918) | 该研究提出了一种基于筛选的方法，使用多重推广步骤来学习与认知图一致的合理规则，能够学习更广泛的合理规则以反映认知图中的合理性考虑，实验发现其表现优于现有方法。 |
| [^204] | [The Numerical Stability of Hyperbolic Representation Learning.](http://arxiv.org/abs/2211.00181) | 本文研究了超几何表征学习中的数值不稳定性问题，比较了两种流行的超几何模型Poincar\'e球和Lorentz模型，发现Lorentz模型具有更好的数值稳定性和优化性能，同时提出一种新的欧几里得优化方案作为超几何学习的另一个选择。 |
| [^205] | [Explaining the Explainers in Graph Neural Networks: a Comparative Study.](http://arxiv.org/abs/2210.15304) | 该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。 |
| [^206] | [Global Contrastive Batch Sampling via Optimization on Sample Permutations.](http://arxiv.org/abs/2210.12874) | 本论文提出了一种有效的替代硬负例挖掘的全局对比批量采样方法GCBS，能够提高对比学习任务的性能表现，易于实现且适用于各种对比学习方法。 |
| [^207] | [AnalogVNN: A fully modular framework for modeling and optimizing photonic neural networks.](http://arxiv.org/abs/2210.10048) | AnalogVNN是一个基于PyTorch的光学神经网络仿真框架，可以模拟光电噪声、有限精度和信号归一化等影响，使用框架可训练和优化线性/卷积神经网络，得出在光学神经网络中影响准确性的归一化、激活函数、降低精度和噪音等因素，实现数字神经网络模型向其模拟光学神经网络模型的转换。 |
| [^208] | [Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability.](http://arxiv.org/abs/2210.08371) | 本文提出了一种针对大规模分布式学习场景的新型草图方案，通过节约通信成本实现联合学习，但是需要进行差分隐私处理以保护局部数据的隐私。 |
| [^209] | [SGD with Large Step Sizes Learns Sparse Features.](http://arxiv.org/abs/2210.05337) | 本文展示了SGD使用大步长训练能够学习稀疏特征，在训练过程中，通过步长调度，梯度和噪声相互作用，共同驱动SGD动态穿过神经网络的损失平面，从而发现稀疏表示。 |
| [^210] | [Digital Audio Forensics: Blind Human Voice Mimicry Detection.](http://arxiv.org/abs/2209.12573) | 本文介绍了一种利用深度学习方法，通过盲目检测输入音频的真实性，可以有效应对音频欺诈问题的分类器。而这种分类器不需要任何参考，能够在没有真实来源的情况下检测出模仿音频。 |
| [^211] | [Optimal Clustering by Lloyd Algorithm for Low-Rank Mixture Model.](http://arxiv.org/abs/2207.04600) | 本文提出了一种名为LrMM的低秩混合模型，结合了Lloyd算法和低秩逼近的聚类方法，能在较弱的限制下处理矩阵观测值，在达到最小化渐进次优的聚类误差率方面表现出众。 |
| [^212] | [Early Discovery of Emerging Entities in Persian Twitter with Semantic Similarity.](http://arxiv.org/abs/2207.02434) | 本论文提出了一种名为EEPT的在线聚类方法，该方法利用语义相似度在Twitter上发现新兴实体。通过新的评估指标，结果表明EEPT是有前途的并能够在实体建立之前发现重要的实体。 |
| [^213] | [Adversarially Robust PAC Learnability of Real-Valued Functions.](http://arxiv.org/abs/2206.12977) | 该论文研究了在实值函数中对抗鲁棒PAC学习性，发现有限胖折射维的类既可以在实现和不可知设置中被学习，凸函数类可以正确学习，而一些非凸函数类需要不正当的学习算法。 |
| [^214] | [A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel.](http://arxiv.org/abs/2206.12543) | 该论文提出了一种名为“逻辑和”的经验神经切向核近似方法，可以在计算量上显著降低，同时经过证明在宽的最终“读出”层的网络中初始化后收敛于真实的eNTK。 |
| [^215] | [Neural Diffusion Processes.](http://arxiv.org/abs/2206.03992) | 提出了一种新方法——神经扩散过程（NDPs），通过有限边缘学习从丰富的函数分布中进行采样。NDPs 可以捕获接近真实贝叶斯后验的函数分布，具有超越神经过程的表现，实现了多种下游任务，比如回归、隐式超参数边缘化、非高斯后验预测和全局优化。 |
| [^216] | [DevFormer: A Symmetric Transformer for Context-Aware Device Placement.](http://arxiv.org/abs/2205.13225) | 本文提出了DevFormer，一种用于硬件设计优化的对称Transformer，引入相对位置嵌入和动作置换对称性等强归纳偏置以有效捕捉硬件上下文，能有效地用有限的离线数据实现设计优化，并且在分离电容器放置问题中表现出众，可提高性能并减少组件数量。 |
| [^217] | [Boosting Tail Neural Network for Realtime Custom Keyword Spotting.](http://arxiv.org/abs/2205.12933) | 本文提出了一种 Boosting Tail Neural Network（BTNN）来改善实时自定义关键词检测的性能，该方法利用批量弱分类器来解决问题，在实验中相对于传统算法获得了显著表现改善。 |
| [^218] | [Gradient boosting for convex cone predict and optimize problems.](http://arxiv.org/abs/2204.06895) | 本文介绍了dboost，它是第一个为“预测，然后优化”问题设计的智能梯度提升实现。该框架支持凸二次锥规划，并通过自定义不动点映射的隐式微分来执行梯度提升，在实验中表现出色。 |
| [^219] | [Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing.](http://arxiv.org/abs/2203.12026) | 本文提出一种基于仿真集成的生物启发式搜索测试方法Deeper，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景，通过实证评估和与竞赛中的其他工具的比较展示了其性能的提高。 |
| [^220] | [Invariance in Policy Optimisation and Partial Identifiability in Reward Learning.](http://arxiv.org/abs/2203.07475) | 本文探讨了奖励学习中奖励函数的部分可识别性，并分析了这种部分可识别性对政策优化等下游任务的影响。同时提出了一个框架，对比奖励学习的数据源和下游任务，以其不变性为依据，对奖励学习的数据源的设计和选择产生影响。 |
| [^221] | [Temporal Difference Learning with Continuous Time and State in the Stochastic Setting.](http://arxiv.org/abs/2202.07960) | 该论文提出了两种连续时间策略评估问题的时间差分学习方法，并证明了它们的理论收敛速度。同时，这些方法还可以解释为解决线性PDE或线性BSDE的新型强化学习方法。 |
| [^222] | [AD-NEGF: An End-to-End Differentiable Quantum Transport Simulator for Sensitivity Analysis and Inverse Problems.](http://arxiv.org/abs/2202.05098) | 本文提出了一种名为AD-NEGF的端到端可微NEGF模型，用于量子输运模拟，可以应用于敏感性分析、反向设计等领域，并具有加速材料设计过程的潜力。 |
| [^223] | [HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments.](http://arxiv.org/abs/2111.10635) | 这篇论文介绍了一个名为Paddle-HeterPS的分布式框架，基于强化学习的调度方法可以高效地利用多种类型的计算资源，解决了分布式深度学习训练中多层次分配计算资源的问题。 |
| [^224] | [A scalable and fast artificial neural network syndrome decoder for surface codes.](http://arxiv.org/abs/2110.05854) | 本研究报道了一种基于人工神经网络的表面码综合征解码器，能够解码任意形状和大小的表面码，并取得了具有竞争性或优越的性能，显示出惊人的加速作用，这是实现大规模量子纠错的重要进展。 |
| [^225] | [Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners.](http://arxiv.org/abs/2009.02476) | 本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设，发现人们假设学习者具有高的折扣率和高度重视探索，并根据学习者进展调整教学策略。 |
| [^226] | [Gaussian Hierarchical Latent Dirichlet Allocation: Bringing Polysemy Back.](http://arxiv.org/abs/2002.10855) | 本论文提出了一种高斯分层潜在狄利克雷分配模型，通过引入层次结构恢复了捕捉多义性的能力，相对于基于高斯的模型具有更好的多义词检测性能，相对于潜在狄利克雷分配的层次模型具有更加简洁的主题表示。 |
| [^227] | [ROIPCA: An online memory-restricted PCA algorithm based on rank-one updates.](http://arxiv.org/abs/1911.11049) | 本文提出了基于秩一更新的ROIPCA和fROIPCA两种在线PCA算法，在内存限制的情况下，算法准确性好、运行时间短。其中fROIPCA为梯度算法，具有最优学习率。 |

# 详细

[^1]: 论大型语言模型水印的可靠性

    On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])

    [http://arxiv.org/abs/2306.04634](http://arxiv.org/abs/2306.04634)

    本文研究了大型语言模型水印在混合其他文本来源时的可靠性，并提供了在实际应用中的建议。

    

    大型语言模型(LLMs)已经开始应用于日常使用，并有能力在未来的十年内产生大量的文本。机器生成的文本可能会取代互联网上的人类写作文本，并有可能被用于恶意目的，如钓鱼攻击和社交媒体机器人。水印是一种简单有效的策略，通过使LLM生成的文本可检测和可记录，来降低这些伤害。然而，一个关键问题仍然存在：在现实中混合了其他的文本来源，被人类写作者或其他语言模型改写，被用于社交和技术领域的各种应用时，水印在实际设置中的可靠性如何？在本文中，我们探讨了不同的检测方案，量化了它们检测水印的能力，并确定在每个情况下需要观察多少机器生成的文本才能可靠地检测水印。我们特别强调了当水印与其他文本来源混合时水印的可靠性，并提供了未来使用LLM生成的文本水印的建议。

    Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
    
[^2]: 对比提升：通过慢-快对比融合实现三维物体实例分割

    Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v1 [cs.CV])

    [http://arxiv.org/abs/2306.04633](http://arxiv.org/abs/2306.04633)

    本文提出了一种利用二维预训练模型，通过慢-快对比融合实现三维物体实例分割的方法，在大量物体的场景中具有可扩展性，同时不需要物体数量的上界限制，通过限制多视角一致性，提出了一个新的半真实数据集(Messy Rooms)。

    

    由于缺乏大规模注释的三维数据集，三维物体实例分割是一项具有挑战性的任务。本文展示了可以通过利用二维预训练模型来有效解决该任务。我们提出了一种新颖的方法，通过神经场表示将2D分段向上提升到3D，并将它们通过慢-快对比融合。这种方法鼓励跨帧的多视角一致性。我们方法的核心是一个可扩展的、适用于具有大量物体的场景的慢-快聚类目标函数。与先前的方法不同的是，我们的方法不需要对物体数量或跨帧物体跟踪进行设置上界。为了展示慢-快聚类的可扩展性，我们创建了一个名为Messy Rooms的新的半真实数据集，其中场景中最多有500个物体。我们的方法在ScanNet、Hypersim和Replica数据集的具有挑战性的场景中表现优于现有技术。

    Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as o
    
[^3]: 基于生成对抗渲染器的实时逼真度提高方法

    Generative Adversarial Shaders for Real-Time Realism Enhancement. (arXiv:2306.04629v1 [cs.GR])

    [http://arxiv.org/abs/2306.04629](http://arxiv.org/abs/2306.04629)

    本文提出了一种高效、基于生成对抗渲染器的实时逼真度提高方法，该方法可以适应嵌入式和移动GPU等资源受限制的设备，并能够在使用对抗训练的情况下，实现目标图像集外观的忠实再现，提供与神经网络方法相当的质量。

    

    应用于现实增强方法，特别是在实时和资源受限制的场景中，现有方法的代价很高。这些方法只有在长时间运行和高带宽、高内存和高功率要求下才能产生高质量的结果。我们提出了一种高效替代方法：一种高性能、基于生成对抗渲染器的方法，将机器学习技术应用于实时应用程序，即使在资源受限的情况下，例如嵌入式和移动GPU。所提出的可学习渲染管道包括可微分的函数，可以使用对抗目标进行端到端训练，实现目标图像集外观的忠实再现，无需手动调整。该渲染管道针对目标设备进行了高度优化，提供时间稳定、比实际时间更快的结果，与许多神经网络方法的质量相媲美。

    Application of realism enhancement methods, particularly in real-time and resource-constrained settings, has been frustrated by the expense of existing methods. These achieve high quality results only at the cost of long runtimes and high bandwidth, memory, and power requirements. We present an efficient alternative: a high-performance, generative shader-based approach that adapts machine learning techniques to real-time applications, even in resource-constrained settings such as embedded and mobile GPUs. The proposed learnable shader pipeline comprises differentiable functions that can be trained in an end-to-end manner using an adversarial objective, allowing for faithful reproduction of the appearance of a target image set without manual tuning. The shader pipeline is optimized for highly efficient execution on the target device, providing temporally stable, faster-than-real time results with quality competitive with many neural network-based methods.
    
[^4]: 另一种用于监督主成分分析的算法：监督线性中心编码。

    Yet Another Algorithm for Supervised Principal Component Analysis: Supervised Linear Centroid-Encoder. (arXiv:2306.04622v1 [cs.LG])

    [http://arxiv.org/abs/2306.04622](http://arxiv.org/abs/2306.04622)

    我们提出了一种新的有监督降维技术SLCE，通过使用线性变换将类别的样本映射到其类别中心点来工作。该变换可以在低维度空间中重构样本以最小化中心点重构损失。SLCE可以用于许多监督学习任务中。

    

    我们提出了一种新的有监督降维技术，称为监督线性中心编码（SLCE），它是非线性中心编码（CE）\citep{ghosh2022supervised}的线性对应物。SLCE通过使用线性变换将类别的样本映射到其类别中心点来工作。该变换是一个投影，它重构一个点，使其与相应类别中心点的距离（即中心点重构损失）在环境空间中最小化。我们使用对称矩阵的特征分解导出了一个闭合形式解。我们对所提出方法的一些关键数学性质进行了详细分析和展示。%我们还提供了一种基于下降方法求解优化问题的迭代解决方案。我们建立了特征值与中心点重构损失之间的联系。与在环境空间中重构样本的主成分分析（PCA）相比，该变换可以在低维度空间中重构样本。

    We propose a new supervised dimensionality reduction technique called Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the nonlinear Centroid-Encoder (CE) \citep{ghosh2022supervised}. SLCE works by mapping the samples of a class to its class centroid using a linear transformation. The transformation is a projection that reconstructs a point such that its distance from the corresponding class centroid, i.e., centroid-reconstruction loss, is minimized in the ambient space. We derive a closed-form solution using an eigendecomposition of a symmetric matrix. We did a detailed analysis and presented some crucial mathematical properties of the proposed approach. %We also provide an iterative solution approach based solving the optimization problem using a descent method. We establish a connection between the eigenvalues and the centroid-reconstruction loss. In contrast to Principal Component Analysis (PCA) which reconstructs a sample in the ambient space, the transformation 
    
[^5]: 一次性对齐、提炼和扩充所有不平衡的半监督学习

    Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])

    [http://arxiv.org/abs/2306.04621](http://arxiv.org/abs/2306.04621)

    本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。

    

    在解决长尾半监督学习中的类别不平衡问题时，需面对未标记数据和已标记数据之间边缘分布的区别，前者通常是未知的且可能与后者不同，这导致了一些重大挑战。第一个挑战是在训练过程中避免使伪标签对目标分布的偏倚，如已标记数据或平衡分布。第二个挑战是确保推理时的平衡未标记分布。为应对这些挑战，我们提出了一个多方面的解决方案：通过灵活的分布对齐，逐渐将分类器从动态估计的未标记先验分布对齐到平衡分布；利用被基于阈值的方法舍弃的低置信度伪标签的软一致性正则化；以及一种将标记部分的输入数据扩展到未标记集的方案。

    Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
    
[^6]: 重温自然语言处理中的领域外鲁棒性: 基准，分析和LLMs评估

    Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])

    [http://arxiv.org/abs/2306.04618](http://arxiv.org/abs/2306.04618)

    本文提出了一个具有挑战性的基准协议，用于评估自然语言处理中的领域外鲁棒性。通过使用这个基准套件，作者们发现OOD与ID性能之间的关系并不总是一致的，并引入了一种名为LLMs的新方法，可以在多个任务上显著提高OOD鲁棒性。

    

    本文重新审视自然语言处理(NLP)领域中领域外鲁棒性(OOD)的研究。我们发现以往研究中的分布转移设置普遍缺乏足够的挑战，限制了对OOD鲁棒性的准确评估。为了解决这些问题，我们提出了一个基准构建方案，确保了明确的区分和具有挑战性的分布转移。然后，我们介绍了BOSS，一个涵盖5个任务和20个数据集的用于评估OOT鲁棒性的基准套件。基于BOSS，我们对预训练语言模型进行了一系列实验，以分析和评估OOD鲁棒性。首先，我们研究了香草微调的ID和OOD性能之间的关系。我们确定了三种典型类型揭示了内在的学习机制，可能有助于预测OOD鲁棒性，并与ID数据集上的进展相关。然后，我们在BOSS上评估了5种经典方法，并发现它们的OOD性能并不总是与ID性能一致，这表明了特别评估OOD鲁棒性的重要性。最后，我们提出了一种名为LLMs（潜在语言模型）的新方法，可以在多个任务上显著提高OOD鲁棒性。

    This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
    
[^7]: 一种基于物理约束卷积神经网络方法的数据误差修正工具

    Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach. (arXiv:2306.04600v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.04600](http://arxiv.org/abs/2306.04600)

    本文提出了一种基于物理约束卷积神经网络的方法，可以从受到系统误差影响的数据中还原出潜在物理系统的时空解。

    

    自然现象和工程系统的信息通常包含在数据中。 然而，数据可能会被模型和实验中的系统性误差所损坏。 本文提出了一种工具，即基于物理约束卷积神经网络（PC-CNN），通过从数据中删除系统误差来揭示潜在物理系统的时空解。 PC-CNN结合了系统控制方程和数据的信息。 我们重点关注由偏微分方程模拟的基础现象，例如线性对流，Burgers方程和二维湍流。

    Information on natural phenomena and engineering systems is typically contained in data. Data can be corrupted by systematic errors in models and experiments. In this paper, we propose a tool to uncover the spatiotemporal solution of the underlying physical system by removing the systematic errors from data. The tool is the physics-constrained convolutional neural network (PC-CNN), which combines information from both the systems governing equations and data. We focus on fundamental phenomena that are modelled by partial differential equations, such as linear convection, Burgers equation, and two-dimensional turbulence. First, we formulate the problem, describe the physics-constrained convolutional neural network, and parameterise the systematic error. Second, we uncover the solutions from data corrupted by large multimodal systematic errors. Third, we perform a parametric study for different systematic errors. We show that the method is robust. Fourth, we analyse the physical properti
    
[^8]: 语言模型性别去偏置：少样本数据干预方法降低性别偏见

    Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions. (arXiv:2306.04597v1 [cs.CL])

    [http://arxiv.org/abs/2306.04597](http://arxiv.org/abs/2306.04597)

    该论文提出了一种数据干预策略，可以通过极少的训练数据减少预训练模型中的性别偏差，有效降低对任何性别的偏好倾向。

    

    目前主流的去偏置技术大多集中在改变训练过程，而我们提出了一种数据干预策略，该策略是一种强大而简单的技术，可以减少预训练模型中的性别偏见。我们经验证明，只要导入10个去偏置的训练数据，即可显著减少任何性别的偏好倾向。由于我们的方法只需要少量的训练样本，因此是高度可行和实用的。通过大量实验，我们证明了该方法的有效性。

    Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation,
    
[^9]: 强化学习中的观测转移泛化问题研究

    Generalization Across Observation Shifts in Reinforcement Learning. (arXiv:2306.04595v1 [cs.LG])

    [http://arxiv.org/abs/2306.04595](http://arxiv.org/abs/2306.04595)

    本文研究了强化学习中观测转移泛化问题，在基于仿真器学习的情况下，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。

    

    学习到的策略对于环境变化的鲁棒性对于强化学习智能体在实际世界的应用和跨环境转移学习至关重要。本文关注于基于仿真器学习的情况下的观测变化问题，提出了一种基于等价度量的新型目标函数，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。

    Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a
    
[^10]: 深度神经网络的相似性校准

    Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])

    [http://arxiv.org/abs/2306.04590](http://arxiv.org/abs/2306.04590)

    该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。

    

    推理结果的可信度校准对于提供准确和可解释的不确定性估计至关重要，特别是在安全关键场景下。已有的校准方法通常忽略了接近度偏差的问题，即模型在低接近性数据（即分布的稀疏区域）中倾向于更自信，而在高接近性样本中表现出不一致的误校准，我们发现这一问题。我们在ImageNet预训练模型上研究了这一问题，并观察到：1）接近度偏差存在于各种模型架构和大小之间；2）基于Transformer的模型比基于CNN的模型更容易受到接近度偏差的影响；3）即使采用流行的校准算法如温度缩放，接近度偏差也会持续存在；4）模型在低接近性样本上的过拟合程度比高接近性样本更严重。在这些实证发现的基础上，我们提出了ProCal。

    Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
    
[^11]: 利用选项改进模仿学习对抗性示范的性能表现

    Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])

    [http://arxiv.org/abs/2306.04581](http://arxiv.org/abs/2306.04581)

    本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。

    

    在本文中，我们考虑了从教师或专家的演示中学习执行任务的问题，并提出了一种新的技术，可以识别未被对手显着修改的演示轨迹的部分，并使用时间上扩展的策略或选项进行学习。首先，我们定义了一种基于演示轨迹的空间和时间特征的轨迹分歧度量，以检测和丢弃已被对手显着修改的轨迹部分，并可能降低学习者的性能，如果用于学习。然后，我们使用基于选项的算法来分割轨迹，并只从已确定为可接受的轨迹部分中进行学习。我们提供了我们技术的理论结果，以表明修复部分轨迹可以改善学习效果。

    We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
    
[^12]: 农业中机器学习、遥感和物联网方法在产量预测方面的应用：一次重要的回顾

    Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review. (arXiv:2306.04566v1 [cs.LG])

    [http://arxiv.org/abs/2306.04566](http://arxiv.org/abs/2306.04566)

    综述了机器学习、遥感和物联网方法在农业中的应用，这些技术的整合为农业提供了洞察和预测，提高了农业生产效率和可持续性。

    

    遥感和机器学习在农业中的整合通过数据分析提供了洞察和预测，正在改变行业。这种结合导致了改善产量预测和水管理，从而实现了提高效率、获得更好的产量和更可持续的农业实践。通过整合这些技术，可以开发一个强大的农业移动或Web应用程序，为农民和决策者提供有价值的信息和工具，以改善作物管理和提高效率。本文对机器学习、遥感和物联网方法在产量预测方面的最新应用进行了重要的回顾。

    The integration of remote sensing and machine learning in agriculture is transforming the industry by providing insights and predictions through data analysis. This combination leads to improved yield prediction and water management, resulting in increased efficiency, better yields, and more sustainable agricultural practices. Achieving the United Nations' Sustainable Development Goals, especially "zero hunger," requires the investigation of crop yield and precipitation gaps, which can be accomplished through, the usage of artificial intelligence (AI), machine learning (ML), remote sensing (RS), and the internet of things (IoT). By integrating these technologies, a robust agricultural mobile or web application can be developed, providing farmers and decision-makers with valuable information and tools for improving crop management and increasing efficiency. Several studies have investigated these new technologies and their potential for diverse tasks such as crop monitoring, yield predi
    
[^13]: ChatGPT很有趣，但并不好笑！幽默对于大语言模型依然是一个挑战。

    ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. (arXiv:2306.04563v1 [cs.AI])

    [http://arxiv.org/abs/2306.04563](http://arxiv.org/abs/2306.04563)

    这篇论文探讨了OpenAI的ChatGPT模型在幽默识别方面的能力，结果表明该模型的幽默并不是硬编码的，但大部分生成的笑话都不是新的，几乎是少量几组重复的。

    

    幽默是人类交流中非常重要的一个方面，迄今为止尚未被解决。大型语言模型（LLMs）越来越能够捕捉隐含和上下文信息。特别是，OpenAI的ChatGPT最近引起了广泛的公众关注。基于GPT3的模型几乎可以达到与人类交流的水平，甚至能够讲笑话。但是，ChatGPT真的很有趣吗？作者针对模型的幽默感进行了测试，包括生成、解释和检测等环节，试图了解ChatGPT理解并再现人类幽默的能力。由于模型本身不可访问，作者采用了基于提示的实验方法。实验证据表明，大部分生成的笑话并不是由该模型新生成的，而是重复了少量的几组笑话。虽然系统可以准确地解释有效的笑话，但也会提出虚构的解释。

    Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional ex
    
[^14]: 学生编写的问题对大规模语言模型的基准测试: StudentEval

    StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. (arXiv:2306.04556v1 [cs.LG])

    [http://arxiv.org/abs/2306.04556](http://arxiv.org/abs/2306.04556)

    本文提出了一个新的基准测试StudentEval，由初学者编写多个提示来测试Code LLM的性能，并发现这比现有基准测试更好地区分模型性能；分析提示发现学生提示技巧差异显著，非确定性LLM抽样可能会误导学生。

    

    代码语言模型（LLMs）正在被快速部署，并且有证据表明它们可以提高专业程序员的生产力。目前的代码生成基准测试是通过评估模型是否可以根据专家提示生成正确的程序来进行的。本文提出了一个新的基准测试，包含每个问题的多个提示，由特定的非专业提示者（即初学者程序员）编写。StudentEval包含了由80名仅完成了一学期Python编程的学生编写的48个问题的1749个提示。我们的学生在与Code LLM互动工作时编写了这些提示，观察到成功率非常不稳定。我们使用StudentEval评估了5种Code LLM，并发现StudentEval比现有的基准测试更好地区分模型性能。我们分析了提示，并发现学生提示技巧存在显着差异。我们还发现，非确定性LLM抽样可能会让学生误以为他们的提示比实际效果更好（或更差）。

    Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or l
    
[^15]: 多任务训练结合领域内语言模型进行诊断推理

    Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v1 [cs.CL])

    [http://arxiv.org/abs/2306.04551](http://arxiv.org/abs/2306.04551)

    本文研究了领域内与领域外语言模型以及多任务与单任务训练的比较，并证明了通过临床训练的多任务语言模型在临床诊断推理任务中表现优异，建立了新的最优性能。

    

    生成人工智能是增强临床诊断决策支持和减少诊断错误的一种有前途的方向。为进一步发展临床人工智能系统，引入了诊断推理基准（DR.BENCH）作为全面的生成人工智能框架，由六个任务组成，代表临床推理的关键组成部分。本文进行了领域内与领域外语言模型以及多任务与单任务训练的比较分析，重点关注 DR.BENCH 的问题总结任务（Gao 等，2023）。我们证明，通过临床训练的多任务语言模型大幅优于其一般领域的对应模型，建立了新的最优性能， ROUGE-L 得分为 28.55。这项研究强调了领域特定训练在优化临床诊断推理任务中的价值。

    Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH (Gao et al., 2023). We demonstrate that a multi-task, clinically trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.
    
[^16]: 线性函数逼近下SARSA算法的收敛性: 随机时限情况

    Convergence of SARSA with linear function approximation: The random horizon case. (arXiv:2306.04548v1 [cs.LG])

    [http://arxiv.org/abs/2306.04548](http://arxiv.org/abs/2306.04548)

    本文研究了SARSA算法在随机时限MDPs中的收敛性，证明了当行为策略与线性函数逼近的权重向量相关，Lipschitz常数足够小时，算法以概率一收敛。

    

    在无限时间折扣马尔可夫决策问题(MDPs)中，SARSA强化学习算法结合线性函数逼近已被证明收敛。在本文中，我们研究了该算法在随机时限MDPs中的收敛性，这之前尚未得到证明。我们证明了，类似于无限时间折扣MDPs的早期结果，如果行为策略关于线性函数逼近的权重向量是$\varepsilon$-soft且与Lipschitz常数相关，并且Lipschitz常数足够小，则该算法将在考虑随机时限MDP时以概率一收敛。

    The reinforcement learning algorithm SARSA combined with linear function approximation has been shown to converge for infinite horizon discounted Markov decision problems (MDPs). In this paper, we investigate the convergence of the algorithm for random horizon MDPs, which has not previously been shown. We show, similar to earlier results for infinite horizon discounted MDPs, that if the behaviour policy is $\varepsilon$-soft and Lipschitz continuous with respect to the weight vector of the linear function approximation, with small enough Lipschitz constant, then the algorithm will converge with probability one when considering a random horizon MDP.
    
[^17]: 关于扩散模型的设计基础：综述

    On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])

    [http://arxiv.org/abs/2306.04542](http://arxiv.org/abs/2306.04542)

    本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。

    

    扩散模型是一种生成模型，通过逐渐添加和删除噪声来学习训练数据的潜在分布以生成数据。扩散模型的组成部分已经受到了广泛的关注，许多设计选择被提出。现有的评论主要关注高层次的解决方案，对组件的设计基础覆盖较少。本研究旨在通过提供一个全面而连贯的综述，针对扩散模型的组件设计选择进行分析。具体来说，我们将这个综述按照三个关键组件进行组织，即正向过程、逆向过程和采样过程。这使得我们可以提供扩散模型的细粒度透视，有助于未来研究分析个体组件、设计选择的适用性以及扩散模型的实现。

    Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
    
[^18]: 无标记多模态数据的多模态学习：保证和应用

    Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])

    [http://arxiv.org/abs/2306.04539](http://arxiv.org/abs/2306.04539)

    本文研究在只有带标签的单模态数据和自然出现的多模态数据的情况下，如何量化多模态交互的挑战，并提出了两个下界和一个上界来量化多模态交互量。

    

    在许多共同学习多个模态的机器学习系统中，一个核心的研究问题是理解多模态交互的本质：在从两个都没有的模态学习时出现了新的任务相关信息。我们在半监督的情况下研究这一交互量化的挑战，只使用带标签的单模态数据和自然出现的多模态数据（例如，无标签的图像和标题，视频和相应的音频）。利用精确的信息论交互定义，我们的主要贡献是推导下界和上界，量化这种半监督设置下的多模态交互量。我们提出了基于模态共享信息量和单独训练的单模态分类器之间的不一致性的两个下界，并通过连接到近似算法来推导上界。

    In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms fo
    
[^19]: Git-Theta: 一种用于协同开发机器学习模型的Git扩展

    Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models. (arXiv:2306.04529v1 [cs.LG])

    [http://arxiv.org/abs/2306.04529](http://arxiv.org/abs/2306.04529)

    Git-Theta是一种用于协同开发机器学习模型的Git扩展，可支持高效的通信更新、自动模型合并以及有关两个版本之间差异的有意义报告。

    

    目前，大多数机器学习模型都由中央团队培训，很少进行更新。相比之下，开放源代码软件开发通过分布式协作使用版本控制系统进行共享工件的迭代开发。为了支持机器学习模型的协作和不断改进，本文介绍 Git-Theta，一种针对机器学习模型的版本控制系统扩展。Git-Theta是Git的扩展，可精细跟踪模型参数的更改以及代码和其他工件。与现有的版本控制系统不同，Git-Theta利用检查点的结构支持高效的通信更新、自动模型合并以及有关两个模型版本之间差异的有意义报告。此外，Git-Theta还包括一插件系统，使用户能够轻松添加对不同机器学习框架的支持。

    Currently, most machine learning models are trained by centralized teams and are rarely updated. In contrast, open-source software development involves the iterative development of a shared artifact through distributed collaboration using a version control system. In the interest of enabling collaborative and continual improvement of machine learning models, we introduce Git-Theta, a version control system for machine learning models. Git-Theta is an extension to Git, the most widely used version control software, that allows fine-grained tracking of changes to model parameters alongside code and other artifacts. Unlike existing version control systems that treat a model checkpoint as a blob of data, Git-Theta leverages the structure of checkpoints to support communication-efficient updates, automatic model merges, and meaningful reporting about the difference between two versions of a model. In addition, Git-Theta includes a plug-in system that enables users to easily add support for 
    
[^20]: ContriMix：显微镜图像分析中基于无监督内容属性分离的领域泛化方法

    ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])

    [http://arxiv.org/abs/2306.04527](http://arxiv.org/abs/2306.04527)

    ContriMix是一种无需标识和手工调优的领域泛化技术，在显微镜图像中通过分离和学习生成合成图像的方式，解决了领域泛化的问题。

    

    针对组织学和荧光成像等显微镜图像中的领域泛化问题，我们提出了 ContriMix，它采用无监督学习方式分离出显微镜图像中的生物学内容和技术变异，并学习生成合成图像，避免了需要手工 fine-tuning 的问题。我们在组织学和荧光成像实验中验证了 ContriMix 的有效性，取得了基于领域泛化的最新成果。

    Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
    
[^21]: 利用草图技术估计Koopman算子并可靠地学习大规模动态系统

    Estimating Koopman operators with sketching to provably learn large scale dynamical systems. (arXiv:2306.04520v1 [stat.ML])

    [http://arxiv.org/abs/2306.04520](http://arxiv.org/abs/2306.04520)

    本文提出利用随机投影技术优化了Koopman算子的估计器，加快了计算速度，并给出了精确的误差界限，提高了算法的可靠性。

    

    Koopman算子理论允许使用非参数机器学习算法来预测和分析复杂的动态系统。本文提出利用随机投影（草图技术）提高基于核的Koopman算子估计器的计算效率。我们在合成和大规模分子动力学数据集上进行了广泛实验，并建立了非渐进误差界，给出了统计学习速率和计算效率之间的权衡的精确刻画。我们的经验和理论分析表明，经过改进的估计器在保证准确性的同时大大提高了计算效率。

    The theory of Koopman operators allows to deploy non-parametric machine learning algorithms to predict and analyze complex dynamical systems. Estimators such as principal component regression (PCR) or reduced rank regression (RRR) in kernel spaces can be shown to provably learn Koopman operators from finite empirical observations of the system's time evolution. Scaling these approaches to very long trajectories is a challenge and requires introducing suitable approximations to make computations feasible. In this paper, we boost the efficiency of different kernel-based Koopman operator estimators using random projections (sketching). We derive, implement and test the new "sketched" estimators with extensive experiments on synthetic and large-scale molecular dynamics datasets. Further, we establish non asymptotic error bounds giving a sharp characterization of the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analysis shows that
    
[^22]: 辅助任务下多任务学习的样本级加权

    Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks. (arXiv:2306.04519v1 [cs.LG])

    [http://arxiv.org/abs/2306.04519](http://arxiv.org/abs/2306.04519)

    提出了一种用于辅助任务下多任务学习的样本级加权算法SLGrad，通过样本特定的任务权重，消除有害的辅助信号并增强有用的任务信号，实现了泛化性能的提升。

    

    多任务学习(MTL)可以通过与相关任务共享表示来提高神经网络的泛化性能。然而，MTL也可能通过任务之间的有害干扰而降低性能。最近的工作采用任务特定的损失加权作为解决干扰的方法。然而，现有算法将任务视为原子性，缺乏将有害和有用信号明确分离的能力。为此，我们提出了SLGrad，一种用于辅助任务下多任务学习的样本级加权算法。通过样本特定的任务权重，SLGrad在训练过程中重新塑造任务分布，消除有害的辅助信号并增强有用的任务信号。在(半)合成数据集和常见的监督多任务问题上观察到了显著的泛化性能提升。

    Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.
    
[^23]: 通过压缩感知确定建筑周围风压力场的最佳传感器布置

    Optimal sensor placement for reconstructing wind pressure field around buildings using compressed sensing. (arXiv:2306.04518v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.04518](http://arxiv.org/abs/2306.04518)

    本文提出了一种数据驱动的稀疏传感器选择算法，能够通过有限的测量位置简洁地重建风压力场，显著减少传感器数量并提供了稳定和最佳的传感器布置结果。

    

    对于大型、复杂、空间扩展的结构物，如何确定最佳传感器部署位置以确保准确捕捉表面压力场，是保证下游任务（如数字孪生的发展）的重要问题。本文提出了一种数据驱动的稀疏传感器选择算法，旨在提供最多信息内容，通过有限的测量位置简洁地重建风压的空气动力特性，成功地降低了传感器数量。

    Deciding how to optimally deploy sensors in a large, complex, and spatially extended structure is critical to ensure that the surface pressure field is accurately captured for subsequent analysis and design. In some cases, reconstruction of missing data is required in downstream tasks such as the development of digital twins. This paper presents a data-driven sparse sensor selection algorithm, aiming to provide the most information contents for reconstructing aerodynamic characteristics of wind pressures over tall building structures parsimoniously. The algorithm first fits a set of basis functions to the training data, then applies a computationally efficient QR algorithm that ranks existing pressure sensors in order of importance based on the state reconstruction to this tailored basis. The findings of this study show that the proposed algorithm successfully reconstructs the aerodynamic characteristics of tall buildings from sparse measurement locations, generating stable and optimal
    
[^24]: 伪证书选择的难度

    Hardness of Deceptive Certificate Selection. (arXiv:2306.04505v1 [cs.LG])

    [http://arxiv.org/abs/2306.04505](http://arxiv.org/abs/2306.04505)

    本文证明了利用AFC在高保真度和完备性的设定下，选择无信息证书的任务是 NP-hard 的。

    

    最近，基于交互式证明系统的分类器在实现AI理论可解释性方面取得了进展。证明者从数据点中选择证书并将其发送给验证者，后者决定分类。在机器学习的背景下，这样的证书可以是信息性的特征。对于高度保真与完备性的设定，交换的证书必须与数据点的真实分类有高的相互信息。然而，这种保证依赖于数据集的非对称特征相关性的界限，这是迄今为止难以估计高维数据的属性。W\"aldchen等人猜测，利用AFC是计算上困难的，我们在这里证明了这一点。我们考虑恶意的证明者-验证者二元组，旨在利用AFC来实现高的完备性和保真性，同时使用没有信息的证书。我们证明了这个任务是$\mathsf{NP}$难的。

    Recent progress towards theoretical interpretability guarantees for AI has been made with classifiers that are based on interactive proof systems. A prover selects a certificate from the datapoint and sends it to a verifier who decides the class. In the context of machine learning, such a certificate can be a feature that is informative of the class. For a setup with high soundness and completeness, the exchanged certificates must have a high mutual information with the true class of the datapoint. However, this guarantee relies on a bound on the Asymmetric Feature Correlation of the dataset, a property that so far is difficult to estimate for high-dimensional data. It was conjectured in W\"aldchen et al. that it is computationally hard to exploit the AFC, which is what we prove here.  We consider a malicious prover-verifier duo that aims to exploit the AFC to achieve high completeness and soundness while using uninformative certificates. We show that this task is $\mathsf{NP}$-hard an
    
[^25]: 在生物医学任务上评估ChatGPT：与精调生成式变压器的零样例比较。

    Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])

    [http://arxiv.org/abs/2306.04504](http://arxiv.org/abs/2306.04504)

    本文评估了ChatGPT在生物医学任务上的表现，发现在生物数据集训练样本较小时，零样例ChatGPT甚至优于精调生成式变压器模型。由此表明ChatGPT具有在生物医学领域成为有价值工具的潜力。

    

    ChatGPT是OpenAI开发的大型语言模型。尽管其在各种任务上表现出色，但先前的工作尚未研究其在生物医学领域的能力。因此，本文旨在评估ChatGPT在各种基准生物医学任务上的性能，如关系提取、文档分类、问答和摘要。据我们所知，这是首次对ChatGPT在生物医学领域进行全面评估的工作。有趣的是，在训练集较小的生物医学数据集中，基于我们的评估结果，零样例ChatGPT甚至优于先进的精调生成式变压器模型，如BioGPT和BioBART。这表明ChatGPT在大型文本语料库上的预训练使其在生物医学领域具有相当的专业性。我们的发现表明，ChatGPT在生物医学领域具有成为各种任务的有价值工具的潜力。

    ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
    
[^26]: 自适应基于梯度的异常值去除的嘈杂标签学习方法

    Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])

    [http://arxiv.org/abs/2306.04502](http://arxiv.org/abs/2306.04502)

    本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。

    

    训练可靠和高性能模型需要准确和丰富的数据集，但即便是人工标注的数据集也会包含错误，更不用说自动标注的数据集了。现有的一些数据去噪方法主要集中于检测异常值并进行永久性去除，但这种方法很容易过度或者欠度过滤数据集。在本论文中，我们提出了一种新的自适应梯度异常值去除方法（AGRA），不同于在模型训练之前清洗数据集，我们的方法在训练过程中动态调整数据集。通过比较一组样本的累积梯度和单个样本的梯度，我们的方法可以决定是否在当前更新时保留对应的样本，以此来确定它是否有助于模型的学习效果。在多个数据集上进行的广泛评估表明，AGRA方法的有效性，并且全面的结果分析证实了我们方法的理论和实践收益。

    An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
    
[^27]: 公平多智能体赌博机的最优算法研究

    Optimal Fair Multi-Agent Bandits. (arXiv:2306.04498v1 [cs.LG])

    [http://arxiv.org/abs/2306.04498](http://arxiv.org/abs/2306.04498)

    本文针对多智能体之间公平多臂赌博机学习问题提出了一种算法，通过分布式拍卖算法学习样本最优匹配，使用一种新的利用阶段和一种基于顺序统计的遗憾分析实现，相较于先前的结果遗憾阶数从$O(\log T \log\log T)$到了$O\left(N^3 \log N \log T \right)$，能够更好地处理多个智能体之间的依赖关系。

    

    本文研究了在多个不相互通信的智能体之间进行公平的多臂赌博机学习的问题，这些智能体只有在同时访问同一个臂时才提供碰撞信息。我们提出了一种算法，其遗憾为$O\left(N^3 \log N \log T \right)$（假设奖励有界，但未知上界）。这大大改进了之前结果，其遗憾阶数为$O(\log T \log\log T)$，并且对智能体数量具有指数依赖性。结果是通过使用分布式拍卖算法来学习样本最优匹配，一种新的利用阶段，其长度来自于观察到的样本，以及一种基于顺序统计的遗憾分析实现的。仿真结果显示了遗憾对$\log T$的依存关系。

    In this paper, we study the problem of fair multi-agent multi-arm bandit learning when agents do not communicate with each other, except collision information, provided to agents accessing the same arm simultaneously. We provide an algorithm with regret $O\left(N^3 \log N \log T \right)$ (assuming bounded rewards, with unknown bound). This significantly improves previous results which had regret of order $O(\log T \log\log T)$ and exponential dependence on the number of agents. The result is attained by using a distributed auction algorithm to learn the sample-optimal matching, a new type of exploitation phase whose length is derived from the observed samples, and a novel order-statistics-based regret analysis. Simulation results present the dependence of the regret on $\log T$.
    
[^28]: 通过GraphOps探究GNN在稀疏图上的限制、逼近和大小可迁移性

    Limits, approximation and size transferability for GNNs on sparse graphs via graphops. (arXiv:2306.04495v1 [cs.LG])

    [http://arxiv.org/abs/2306.04495](http://arxiv.org/abs/2306.04495)

    本文从理论角度研究了图神经网络在稀疏图中推广的问题。通过GraphOps极限概念，我们开发了关于不同大小图形上的GNN之间距离的定量界限和结构属性的共享。

    

    这项工作从理论角度研究图神经网络能否推广到与其训练图不同的图形中。为了包括经常遇到的稀疏图，如有界度或幂律图，我们采取从图形导出运算符的视角，如组成GNN的聚合操作。这导致了最近介绍的GraphOps极限概念。我们展示了运算符视角如何允许我们开发关于有限GNN与其在无限图上的极限之间距离的定量界限，以及具有共享结构属性的不同大小图形上的GNN之间的距离，在验证各种图序列的规则性假设下。

    Can graph neural networks generalize to graphs that are different from the graphs they were trained on, e.g., in size? In this work, we study this question from a theoretical perspective. While recent work established such transferability and approximation results via graph limits, e.g., via graphons, these only apply non-trivially to dense graphs. To include frequently encountered sparse graphs such as bounded-degree or power law graphs, we take a perspective of taking limits of operators derived from graphs, such as the aggregation operation that makes up GNNs. This leads to the recently introduced limit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how the operator perspective allows us to develop quantitative bounds on the distance between a finite GNN and its limit on an infinite graph, as well as the distance between the GNN on graphs of different sizes that share structural properties, under a regularity assumption verified for various graph sequences. Our res
    
[^29]: 公平的列子集选择

    Fair Column Subset Selection. (arXiv:2306.04489v1 [cs.LG])

    [http://arxiv.org/abs/2306.04489](http://arxiv.org/abs/2306.04489)

    解决了公平的列子集选择问题，通过已知方法基于确定性杠杆分数采样，提出了一种有效算法，可以在1.5倍的大小下实现与两倍相同的近似保证。

    

    我们考虑公平的列子集选择问题。特别地，我们假设数据中存在两个群体，并且所选列子集必须相对于它们各自的最佳秩-k逼近提供良好的近似。我们证明了这种公平设置引入了重大挑战：为了扩展已知结果，人们不能做得比简单地选择原始方法的两倍列更好。我们采用了基于确定性杠杆分数采样的已知方法，并且在存在两个群体的情况下，仅仅采样适当大小的子集就变得NP难。而找到两倍于所需大小的子集则非常简单，我们提供了一种有效的算法，它可以在基本上1.5倍的大小的情况下实现相同的保证。我们通过对真实世界数据的广泛实验验证了我们的方法。

    We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data.
    
[^30]: 基于结合多样化奖励微调权重插值的帕累托最优对齐的奖励汤

    Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])

    [http://arxiv.org/abs/2306.04488](http://arxiv.org/abs/2306.04488)

    本文提出了 rewarded soup 方法，通过结合多种代理奖励，实现微调权重插值，从而在整个偏好空间中实现帕累托最优广义化。该方法在强化学习任务上具有有效性。

    

    基础模型首先在大量无参考数据集上进行预训练，然后在有标注的数据上进行微调。强化学习，特别是来自人类反馈的强化学习(RLHF)，可以进一步使网络与预期的使用相匹配。然而，代理奖励的缺陷可能会妨碍训练，导致次优结果；现实任务和人类意见的多样性加剧了这个问题。本文提出通过采用多策略方法来拥抱多样化奖励的异质性。我们的目标不是专注于单一的先验奖励，而是在整个偏好空间中实现帕累托最优广义化。为此，我们提出了 rewarded soup，首先独立地专门化多个网络(每个代理奖励一个)，然后在它们的权重之间进行线性插值。通过实验表明，这种方法是成功的，因为我们展示了当多样化奖励来自共享的预训练初始化时，权重仍然保持线性连接。我们展示了该方法在强化学习任务上的有效性。

    Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
    
[^31]: 无需训练的神经主动学习与初始化鲁棒性保证

    Training-Free Neural Active Learning with Initialization-Robustness Guarantees. (arXiv:2306.04454v1 [cs.LG])

    [http://arxiv.org/abs/2306.04454](http://arxiv.org/abs/2306.04454)

    本研究提出了一种期望方差与高斯过程（EV-GP）标准用于神经主动学习，该方法不需要对神经网络进行训练，并且保证在数据选择时NN具有良好的预测性能和初始化鲁棒性。

    

    现有的神经主动学习算法旨在通过选择数据进行标记来优化神经网络（NN）的预测性能。然而，除了良好的预测性能外，对于随机参数初始化的鲁棒性也是安全关键应用的重要要求。为此，我们引入了期望方差与高斯过程（EV-GP）标准来进行神经主动学习，该标准在理论上保证选择数据点可以导致训练的 NN 具有良好的预测性能和初始化鲁棒性。重要的是，我们的 EV-GP 标准是无需训练的，即在数据选择过程中不需要对 NN 进行任何训练，这使其在计算上更加高效。我们通过实验证明，我们的 EV-GP 标准与初始化鲁棒性和概括性能高度相关，并且表明它在两个期望方面的表现均优于基线方法，尤其是初始化鲁棒性方面。

    Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especiall
    
[^32]: 多模态潜在扩散

    Multi-modal Latent Diffusion. (arXiv:2306.04445v1 [cs.LG])

    [http://arxiv.org/abs/2306.04445](http://arxiv.org/abs/2306.04445)

    该论文提出一个新的方法来处理多模态数据，该方法使用了一组独立训练的单模态确定性自编码器，将单个潜在变量连接到公共潜在空间中，并通过掩蔽扩散模型实现了良好的生成质量和模态间连贯性。

    

    在现代应用中，多模态数据集无处不在，多模态变分自编码器是一类流行的模型，旨在学习不同模态的联合表示。然而，现有方法存在连贯性 - 质量折衷的问题，具有良好生成质量的模型缺乏模态间的生成连贯性，反之亦然。我们讨论现有方法的局限性，从而说明需要不同的方法。我们提出了一种新方法，该方法使用一组独立训练的单模态确定性自编码器，将单个潜在变量连接到公共潜在空间中，然后将其输入到掩蔽扩散模型中以实现生成建模。我们还引入了一种新的多时间训练方法，用于学习多模态扩散的条件得分网络。通过对几个多模态数据集的评估，我们的方法在生成质量和连贯性方面都明显优于竞争对手。

    Multi-modal data-sets are ubiquitous in modern applications, and multi-modal Variational Autoencoders are a popular family of models that aim to learn a joint representation of the different modalities. However, existing approaches suffer from a coherence-quality tradeoff, where models with good generation quality lack generative coherence across modalities, and vice versa. We discuss the limitations underlying the unsatisfactory performance of existing methods, to motivate the need for a different approach. We propose a novel method that uses a set of independently trained, uni-modal, deterministic autoencoders. Individual latent variables are concatenated into a common latent space, which is fed to a masked diffusion model to enable generative modeling. We also introduce a new multi-time training method to learn the conditional score network for multi-modal diffusion. Our methodology substantially outperforms competitors in both generation quality and coherence, as shown through an e
    
[^33]: 通过随机投影快速获得最优的本地隐私均值估计

    Fast Optimal Locally Private Mean Estimation via Random Projections. (arXiv:2306.04444v1 [cs.LG])

    [http://arxiv.org/abs/2306.04444](http://arxiv.org/abs/2306.04444)

    提出了一种名为ProjUnit的算法框架，用于实现高效的本地隐私均值估计，通过随机投影低维空间实现最优解，且具有低通信复杂度和快速的服务器运行时间。

    

    本文研究了欧几里得空间中高维向量的本地隐私均值估计问题。现有算法要么产生次优误差，要么具有高通信和/或运行时间复杂度。我们提出了一种新的算法框架ProjUnit，用于隐私均值估计的算法具有计算效率高、通信复杂度低且误差与最优解之间的差距最大为1 + o(1)。我们的框架实现起来非常简单：每个随机化器将其输入投影到一个随机的低维子空间中，对结果进行归一化，然后在低维空间中运行一个最优算法，例如PrivUnitG。此外，我们展示了通过适当地协调设备之间的随机投影矩阵，可以实现快速的服务器运行时间。我们通过随机投影的性质分析了算法的误差，并研究了两种实例。最后，我们的实验结果表明，ProjUnit相比现有方法具有显著的性能优势，特别是在高维高斯混合数据集上表现出色。

    We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace, normalizes the result, and then runs an optimal algorithm such as PrivUnitG in the lower-dimensional space. In addition, we show that, by appropriately correlating the random projection matrices across devices, we can achieve fast server run-time. We mathematically analyze the error of the algorithm in terms of properties of the random projections, and study two instantiations. Lastly, our experiments for private mean estimation and pr
    
[^34]: 以双策略为自模型的规划

    Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])

    [http://arxiv.org/abs/2306.04440](http://arxiv.org/abs/2306.04440)

    该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    

    规划是一种数据有效的决策策略，代理通过探索可能的未来状态来选择候选动作。当存在高维行动空间时，为了模拟未来状态，必须使用自己的决策策略来限制所需探索的动作数量。我们将用于模拟自己决策的模型称为代理的自我模型。尽管在规划行动时，世界模型通常与自我模型一起隐含地使用，但如何设计自我模型仍不清楚。受当前强化学习方法和神经科学的启发，我们探讨了使用精简策略网络作为自我模型的优缺点。在这样的双策略代理中，一个无模型策略和一个经过精简的策略分别用于无模型动作和计划动作。我们在一个生态相关的参数环境上的结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
    
[^35]: 忠实知识蒸馏

    Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])

    [http://arxiv.org/abs/2306.04431](http://arxiv.org/abs/2306.04431)

    本文研究了知识蒸馏中教师和学生之间的相对校准问题，提出了一个忠实的模仿框架来解决学生置信度和软标签的问题，并提供了一种实证和认证的方法来评估学生模型的鲁棒性。

    

    知识蒸馏是一种压缩神经网络使其能够在资源受限的系统中部署的成功方法，但过去的研究忽略了教师与学生之间在软置信度方面的相对校准问题。本文聚焦于一个教师-学生对中两个关键问题：（i）教师和学生是否在接近正确分类的数据样本时存在分歧，（ii）在数据样本周围，经过蒸馏的学生是否像教师一样自信。这些都是在安全关键环境中考虑从鲁棒教师中训练较小学生网络的部署时非常关键的问题。为了解决这些问题，我们引入了一个忠实的模仿框架来讨论置信度的相对校准，并提供实证和认证方法来评估学生的训练。

    Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva
    
[^36]: 使用强化学习平衡竞争双人游戏水平

    Balancing of competitive two-player Game Levels with Reinforcement Learning. (arXiv:2306.04429v1 [cs.LG])

    [http://arxiv.org/abs/2306.04429](http://arxiv.org/abs/2306.04429)

    本研究使用PCGRL框架和强化学习算法，提出了一种自动平衡基于图块的竞争性双人游戏级别的架构，其中引入了新型基于交换的表示方法，并通过分析代理的交换行为来判断哪些图块类型影响平衡。

    

    在竞争性的双人游戏中，游戏水平的平衡需要大量手动工作和测试，特别是在非对称的游戏水平中。本文提出了一种在最近推出的PCGRL框架中自动平衡基于图块的级别的架构。我们的架构分为三个部分：(1)级别生成器，(2)平衡代理和(3)奖励建模模拟。通过在模拟中反复玩游戏水平，平衡代理会根据所有玩家的相同胜率对其进行修改而受到奖励。为此，我们引入了一种新颖的基于交换的表示方法，以增加对可玩性的健壮性。我们表明，这种方法能够教会代理更好、更快地改变平衡级别，而不是简单的PCGRL。此外，通过分析代理的交换行为，我们可以得出哪些图块类型影响了平衡。

    The balancing process for game levels in a competitive two-player context involves a lot of manual work and testing, particularly in non-symmetrical game levels. In this paper, we propose an architecture for automated balancing of tile-based levels within the recently introduced PCGRL framework (procedural content generation via reinforcement learning). Our architecture is divided into three parts: (1) a level generator, (2) a balancing agent and, (3) a reward modeling simulation. By playing the level in a simulation repeatedly, the balancing agent is rewarded for modifying it towards the same win rates for all players. To this end, we introduce a novel family of swap-based representations to increase robustness towards playability. We show that this approach is capable to teach an agent how to alter a level for balancing better and faster than plain PCGRL. In addition, by analyzing the agent's swapping behavior, we can draw conclusions about which tile types influence the balancing mo
    
[^37]: 利用稳定平衡点推进高性能探索性数据分析

    Towards High-Performance Exploratory Data Analysis (EDA) Via Stable Equilibrium Point. (arXiv:2306.04425v1 [cs.LG])

    [http://arxiv.org/abs/2306.04425](http://arxiv.org/abs/2306.04425)

    本文提出了一种基于稳定平衡点的框架，用于提高探索性数据分析的效率和解决方案质量，该方法能够为大规模数据集生成高质量的聚类和数据可视化。

    

    探索性数据分析（EDA）是数据科学项目中的重要过程。在本文中，我们介绍了一种基于稳定平衡点（SEP）的框架，用于提高EDA的效率和解决方案质量。通过利用SEP作为代表点，我们的方法旨在为大规模数据集生成高质量的聚类和数据可视化。该方法的一个非常独特的属性是，SEP将直接编码数据集的聚类属性。与先前的最先进的聚类和数据可视化方法相比，所提出的方法允许在大规模数据分析任务中显着提高计算效率和解决方案质量。

    Exploratory data analysis (EDA) is a vital procedure for data science projects. In this work, we introduce a stable equilibrium point (SEP) - based framework for improving the efficiency and solution quality of EDA. By exploiting the SEPs to be the representative points, our approach aims to generate high-quality clustering and data visualization for large-scale data sets. A very unique property of the proposed method is that the SEPs will directly encode the clustering properties of data sets. Compared with prior state-of-the-art clustering and data visualization methods, the proposed methods allow substantially improving computing efficiency and solution quality for large-scale data analysis tasks.
    
[^38]: 计算最优树集的方法

    On Computing Optimal Tree Ensembles. (arXiv:2306.04423v1 [cs.LG])

    [http://arxiv.org/abs/2306.04423](http://arxiv.org/abs/2306.04423)

    该论文提出了两种新算法以计算旨在各种度量方面最优的决策树集合，并且引入了“证明树技术”来大大改进可处理性结果。

    

    随机森林和决策树集合是分类和回归的广泛应用方法。最近的算法进展允许计算旨在各种度量方面（如大小或深度）最优的决策树。我们不知道有关树集合的此类研究，并旨在为该领域做出贡献。主要的是，我们提供了两种新算法和相应的下限。首先，我们能够转移和大大改进决策树的可处理性结果，获得一个 $(6\delta D S)^S \cdot poly$-time 算法，其中 $S$ 是树集合中割数，$D$ 是最大域大小，$\delta$ 是两个示例之间存在不同特征的最大数量。为了达到这个目的，我们引入了证明树技术，这似乎对实践也很有前途。其次，我们表明，动态规划对于决策树已经取得了成功，而对于树集合也可能是可行的，提供了一个 $\ell^n \cdot poly$-t。

    Random forests and, more generally, (decision\nobreakdash-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We are not aware of such research for tree ensembles and aim to contribute to this area. Mainly, we provide two novel algorithms and corresponding lower bounds. First, we are able to carry over and substantially improve on tractability results for decision trees, obtaining a $(6\delta D S)^S \cdot poly$-time algorithm, where $S$ is the number of cuts in the tree ensemble, $D$ the largest domain size, and $\delta$ is the largest number of features in which two examples differ. To achieve this, we introduce the witness-tree technique which also seems promising for practice. Second, we show that dynamic programming, which has been successful for decision trees, may also be viable for tree ensembles, providing an $\ell^n \cdot poly$-t
    
[^39]: 基于策略的自对抗算法解决规划问题

    Policy-Based Self-Competition for Planning Problems. (arXiv:2306.04403v1 [cs.LG])

    [http://arxiv.org/abs/2306.04403](http://arxiv.org/abs/2306.04403)

    本文介绍了一种基于策略的自对抗算法，该算法将历史策略纳入规划过程中，从而实现代理的效率提升和强大的轨迹查找。

    

    AlphaZero类型的算法在单人任务上如果由于引导树搜索的价值网络无法足够准确地近似一次运算的结果，就有可能停止提高。处理这一问题的技术之一是通过自对抗来转化单人任务。主要思想是从代理的历史表现计算标量基线，并将一个序列的奖励重构为二进制输出，指示是否超过了基线。但是，此基线对代理效率提升的战略信息有限。我们利用自对抗的思想，直接将历史策略纳入规划过程中，而不是将其标量表现纳入。基于最近引入的Gumbel AlphaZero（GAZ），我们提出了GAZ'Play-to-Plan'（GAZ PTP）算法，代理通过规划反对自己的可能策略来学习找到强大的轨迹。我们展示了我们算法的有效性。

    AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent's historical performances and to reshape an episode's reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ 'Play-to-Plan' (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our 
    
[^40]: 一种采用三元组折叠的公平分类器

    A Fair Classifier Embracing Triplet Collapse. (arXiv:2306.04400v1 [cs.LG])

    [http://arxiv.org/abs/2306.04400](http://arxiv.org/abs/2306.04400)

    本文提出一种公平分类器，利用三元组损失的边际限制机器学习模型所造成的偏见，并采用三元组折叠方法。

    

    本文研究了三元组损失的行为，并展示了它如何被利用来限制机器学习模型所产生的偏见。我们的公平分类器在使用随机三元组选择时，在三元组损失的边际大于潜在空间中任意两点之间的最大距离时采用三元组折叠方法。

    In this paper, we study the behaviour of the triplet loss and show that it can be exploited to limit the biases created and perpetuated by machine learning models. Our fair classifier uses the collapse of the triplet loss when its margin is greater than the maximum distance between two points in the latent space, in the case of stochastic triplet selection.
    
[^41]: 使用非对称梯度引导来改进扩散图像翻译

    Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance. (arXiv:2306.04396v1 [cs.CV])

    [http://arxiv.org/abs/2306.04396](http://arxiv.org/abs/2306.04396)

    本文提出了一种使用非对称梯度引导来指导扩散采样的反向过程的方法，以改善扩散图像翻译的风格转换和内容保留之间的平衡。

    

    最近，扩散模型在图像翻译任务中取得了显着进展。然而，由于其随机性，通常存在着风格转换和内容保留之间的平衡。为了解决这些挑战，本文提出了一种使用非对称梯度引导来指导扩散采样的反向过程的方法。这导致了更快和更稳定的图像操作，适用于基于文本和图片的图像翻译。

    Diffusion models have shown significant progress in image translation tasks recently. However, due to their stochastic nature, there's often a trade-off between style transformation and content preservation. Current strategies aim to disentangle style and content, preserving the source image's structure while successfully transitioning from a source to a target domain under text or one-shot image conditions. Yet, these methods often require computationally intense fine-tuning of diffusion models or additional neural networks. To address these challenges, here we present an approach that guides the reverse process of diffusion sampling by applying asymmetric gradient guidance. This results in quicker and more stable image manipulation for both text-guided and image-guided image translation. Our model's adaptability allows it to be implemented with both image- and latent-diffusion models. Experiments show that our method outperforms various state-of-the-art models in image translation ta
    
[^42]: 多语言临床实体识别：翻译还是跨语言迁移？

    Multilingual Clinical NER: Translation or Cross-lingual Transfer?. (arXiv:2306.04384v1 [cs.CL])

    [http://arxiv.org/abs/2306.04384](http://arxiv.org/abs/2306.04384)

    本研究比较了翻译和跨语言迁移两种方法来执行临床领域命名实体识别（NER），并证明跨语言迁移比这两种翻译方法在法语和德语中都具有更好的性能。

    

    临床领域非英语文本中的命名实体识别（NER）等自然语言任务可能非常耗时和昂贵，因为缺乏标注数据。跨语言迁移（CLT）是一种绕过这个问题的方法，多语言大型语言模型可以在一个语言上对特定任务进行微调，并在另一个语言上提供高精度。然而，还有其他利用翻译模型的方法可以在目标语言中执行NER，而无需标注数据，可以通过翻译训练集或测试集。本文比较了跨语言迁移与这两种替代方法，以在法语和德语中执行临床NER，而不需要这些语言的任何训练数据。为此，我们发布了MedNERF，这是从法国药物处方中提取的医学NER测试集，并使用与英语数据集相同的指南进行了注释。通过对这个数据集和一个德语医学语料库的广泛实验，我们表明，跨语言迁移比两种翻译方法在两种目标语言中都具有更好的性能。

    Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medica
    
[^43]: 分布式学习系统中用更少的代价获得更多。（arXiv:2306.04377v1 [cs.DC]）

    Get More for Less in Decentralized Learning Systems. (arXiv:2306.04377v1 [cs.DC])

    [http://arxiv.org/abs/2306.04377](http://arxiv.org/abs/2306.04377)

    本文提出了一种名为JWINS的分布式学习系统，它仅通过稀疏化的方式共享部分模型参数，使用小波变换来补偿由稀疏化引起的信息损失，并通过随机通信截断来减少通信用量。实验证明，JWINS可以在发送更少的字节的情况下实现与完全共享分布式学习相似的准确性。

    

    分布式学习系统因为能够避免原始数据共享而仅通过交流模型参数保护了数据的机密性，在机器学习领域越来越受欢迎。但是，深度神经网络的庞大规模对分布式训练提出了重要挑战——每个节点需要交换数千万个数据，容易导致网络超负荷。本文提出JWINS：一种通信效率高且完全分布式的机器学习系统，仅通过稀疏化来共享部分参数。JWINS使用小波变换来限制因稀疏化导致的信息损失，并采用随机通信切断来降低通信用量，而不会影响训练模型的性能。通过96个非独立同分布数据集的分布式学习节点的实现效果，证明JWINS可以在发送64％更少的字节时实现与完全共享分布式学习的相似准确性。此外，在低通信预算下，JWINS胜过了最先进的计算方法。

    Decentralized learning (DL) systems have been gaining popularity because they avoid raw data sharing by communicating only model parameters, hence preserving data confidentiality. However, the large size of deep neural networks poses a significant challenge for decentralized training, since each node needs to exchange gigabytes of data, overloading the network. In this paper, we address this challenge with JWINS, a communication-efficient and fully decentralized learning system that shares only a subset of parameters through sparsification. JWINS uses wavelet transform to limit the information loss due to sparsification and a randomized communication cut-off that reduces communication usage without damaging the performance of trained models. We demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can achieve similar accuracies to full-sharing DL while sending up to 64% fewer bytes. Additionally, on low communication budgets, JWINS outperforms the state-of-the-art com
    
[^44]: 基于分布特征匹配的标签偏移量量化及其鲁棒性保证

    Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])

    [http://arxiv.org/abs/2306.04376](http://arxiv.org/abs/2306.04376)

    本文提出一种名为DFM框架的方法，用于量化标签偏移，并证明了其性能上限和鲁棒性。使用基于核的DFM版本可以提高效率、可扩展性和鲁棒性。

    

    量化学习处理在标签偏移下估计目标标签分布的任务。本文首先提出了一个统一的框架，分布特征匹配（DFM），将先前文献中引入的各种估计器恢复为特定实例。我们推导了DFM程序的一般性能界，改进了先前在特定情况下推导的界限的若干关键方面。然后，我们将这一分析扩展到研究DFM程序在未精确假设标签偏移量的情况下的鲁棒性，特别是在目标受到未知分布污染的情况下。这些理论发现在模拟和实际数据集上得到了详细的数字研究确认。我们还使用随机傅里叶特征原理介绍了一种高效，可扩展且具有鲁棒性的基于核的DFM版本。

    Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
    
[^45]: 基于Wasserstein的高概率泛化界限下的学习

    Learning via Wasserstein-Based High Probability Generalisation Bounds. (arXiv:2306.04375v1 [stat.ML])

    [http://arxiv.org/abs/2306.04375](http://arxiv.org/abs/2306.04375)

    本文证明了基于Wasserstein距离的PAC-Bayesian泛化界限的新颖性，并提出了算法框架，该界限显著扩展了PAC-Bayesian界限的范围，并在经典的学习问题中展现了改进的泛化误差。

    

    在结构风险最小化（SRM）中，最小化总体风险或泛化差距上限被广泛使用，这尤其是PAC-Bayesian学习的核心。尽管近年来其取得了成功并吸引了越来越多的关注，但PAC-Bayesian框架的局限是大多数界限涉及Kullback-Leibler（KL）散度项（或其变化），这可能表现出不规则行为并无法捕捉学习问题的底层几何结构，因此限制了其在实际应用中的使用。最近的一些研究企图用Wasserstein距离替换PAC-Bayesian界限中的KL散度。即使这些界限在一定程度上缓解了上述问题，但它们要么保持期望，要么对有界损失有效，要么难以在SRM框架中最小化。在这项工作中，我们为这一研究方向做出了贡献，证明了基于Wasserstein距离的PAC-Bayesian泛化界限的新颖性，并且我们的界限以显著性地扩展了PAC-Bayesian界限的范围，并在几种经典的学习问题中展现了改进的泛化误差。此外，我们提出了一种算法框架，用于学习新的界限，并在各种数据集上展示了有前途的实验结果。

    Minimising upper bounds on the population risk or the generalisation gap has been widely used in structural risk minimisation (SRM) - this is in particular at the core of PAC-Bayesian learning. Despite its successes and unfailing surge of interest in recent years, a limitation of the PAC-Bayesian framework is that most bounds involve a Kullback-Leibler (KL) divergence term (or its variations), which might exhibit erratic behavior and fail to capture the underlying geometric structure of the learning problem - hence restricting its use in practical applications. As a remedy, recent studies have attempted to replace the KL divergence in the PAC-Bayesian bounds with the Wasserstein distance. Even though these bounds alleviated the aforementioned issues to a certain extent, they either hold in expectation, are for bounded losses, or are nontrivial to minimize in an SRM framework. In this work, we contribute to this line of research and prove novel Wasserstein distance-based PAC-Bayesian ge
    
[^46]: 基于标签感知的语音表示学习用于语种识别

    Label Aware Speech Representation Learning For Language Identification. (arXiv:2306.04374v1 [cs.CL])

    [http://arxiv.org/abs/2306.04374](http://arxiv.org/abs/2306.04374)

    本论文提出一种新的语音表示学习框架，即基于标签感知的语音表示学习（LASR）框架，将自我监督表示学习和语言标签信息相结合进行预训练，通过三元组的目标函数将二者结合，提高了语种识别的准确性，并且能够应对噪声/丢失标签的情况。

    

    面向非语义任务，例如语言识别的语音表示学习方法，要么探索使用分类器模型的监督嵌入提取方法，要么使用原始数据的自我监督表示学习方法。在本文中，我们提出了一种新颖的框架，将自我监督表示学习与语言标签信息相结合进行预训练任务。这个框架被称为基于标签感知的语音表示（LASR）学习，并使用基于三元组的目标函数将语言标签与自我监督损失函数相结合。语音表示进一步针对下游任务进行微调。语言识别实验在两个公共数据集 - FLEURS和Dhwani上进行。在这些实验中，我们说明所提出的LASR框架在语言识别上优于现有技术。我们还报告了LASR方法对于噪声/丢失标签的鲁棒性分析。

    Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels 
    
[^47]: 基于GCN可信度预测的协同移动群感知的高效招募策略

    Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])

    [http://arxiv.org/abs/2306.04366](http://arxiv.org/abs/2306.04366)

    本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。

    

    协同移动群感知可以通过促进任务感知的团队合作来提高数据质量和覆盖范围，而工人招募则代表着一个复杂的多目标优化问题。现有策略主要关注工人本身的特征，忽略了工人之间的非对称信任关系，从而影响了任务效用评估的合理性。为解决这个问题，本文首先使用Mini-Batch K-Means聚类算法和边缘服务器来实现高效的分布式工人招募。利用历史数据和任务要求获得工人的能力类型和距离。使用工人社交网络中的信任导向图输入至图卷积网络（GCN）框架进行训练，捕获工人之间的非对称信任关系。通过工人之间的高信任值，防止CMCS场景下的隐私泄露。最终，利用预测的信任和工人能力构建了一个无向招募图，以实现有效的任务分配。实验结果表明，与现有方法相比，这种招募方法在招募准确度、任务完成时间和能量消耗方面表现优异。

    Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
    
[^48]: 基于贝叶斯优化的气候变化对抗：应用和基准研究

    Bayesian Optimisation Against Climate Change: Applications and Benchmarks. (arXiv:2306.04343v1 [cs.LG])

    [http://arxiv.org/abs/2306.04343](http://arxiv.org/abs/2306.04343)

    本论文综述了在气候变化应用中使用贝叶斯优化的可行性和应用案例，提出了四个主要应用领域和相应的公共基准或数据集，并鼓励开发更全面的基准数据集。

    

    贝叶斯优化是一种优化黑盒函数的强有力方法，在真实函数难以评估且没有渐进信息的设置中广受欢迎。贝叶斯优化可以优化许多气候变化中的优化问题，其中的模拟器模型不可用或难以从中抽取样本。虽然在气候相关应用中已经有了几个可行性演示贝叶斯优化的案例，但还没有统一的应用和基准研究。我们在这里提供这样的综述，以鼓励在重要和适宜的应用领域使用贝叶斯优化。我们确定了四个主要应用领域：材料发现、风电场布局、最优可再生能源控制和环境监测。对于每个领域，我们确定了易于使用和评估系统的公共基准或数据集，同时代表着真实世界问题。由于缺乏适当的基准，我们建议未来开发更全面的基准数据集。

    Bayesian optimisation is a powerful method for optimising black-box functions, popular in settings where the true function is expensive to evaluate and no gradient information is available. Bayesian optimisation can improve responses to many optimisation problems within climate change for which simulator models are unavailable or expensive to sample from. While there have been several feasibility demonstrations of Bayesian optimisation in climate-related applications, there has been no unifying review of applications and benchmarks. We provide such a review here, to encourage the use of Bayesian optimisation in important and well-suited application domains. We identify four main application domains: material discovery, wind farm layout, optimal renewable control and environmental monitoring. For each domain we identify a public benchmark or data set that is easy to use and evaluate systems against, while being representative of real-world problems. Due to the lack of a suitable benchma
    
[^49]: 基于非配对深度学习的动态增强磁共振成像药代动力学参数估计

    Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from Dynamic Contrast-Enhanced MRI. (arXiv:2306.04339v1 [eess.IV])

    [http://arxiv.org/abs/2306.04339](http://arxiv.org/abs/2306.04339)

    该研究提出了一种非配对深度学习方法，使用物理驱动的CycleGAN框架，可以在没有配对数据的情况下准确地估计动态增强磁共振成像药代动力学参数和动脉输入函数。

    

    动态增强磁共振成像 (DCE-MRI) 可以提供关于血管通透性和组织灌注的药代动力学参数信息。然而，传统的药代动力学参数估计方法涉及拟合示踪剂动力学模型，由于噪声动脉输入函数 (AIF) 的测量常常导致计算复杂性和低准确性。虽然一些深度学习方法已被提出来解决这些挑战，但大部分现有的方法依赖于有标签的 DCE-MRI 和已标注的药代动力学参数图。这依赖于标签数据会导致显著的时间和资源限制，可能会引入标签噪声，使得监督学习方法经常不实用。为了解决这些限制，我们在这里提出了一种新颖的非配对深度学习方法，用物理驱动的 CycleGAN 方法估计药代动力学参数和 AIF。我们提出的 CycleGAN 框架基于自对抗学习，通过学习两个不同分布之间的映射，可以在没有配对数据的情况下对药代动力学参数进行估计。

    DCE-MRI provides information about vascular permeability and tissue perfusion through the acquisition of pharmacokinetic parameters. However, traditional methods for estimating these pharmacokinetic parameters involve fitting tracer kinetic models, which often suffer from computational complexity and low accuracy due to noisy arterial input function (AIF) measurements. Although some deep learning approaches have been proposed to tackle these challenges, most existing methods rely on supervised learning that requires paired input DCE-MRI and labeled pharmacokinetic parameter maps. This dependency on labeled data introduces significant time and resource constraints, as well as potential noise in the labels, making supervised learning methods often impractical. To address these limitations, here we present a novel unpaired deep learning method for estimating both pharmacokinetic parameters and the AIF using a physics-driven CycleGAN approach. Our proposed CycleGAN framework is designed ba
    
[^50]: 机器学习时代统计学数据来源的变更

    Changing Data Sources in the Age of Machine Learning for Official Statistics. (arXiv:2306.04338v1 [stat.ML])

    [http://arxiv.org/abs/2306.04338](http://arxiv.org/abs/2306.04338)

    本文总结了机器学习时代官方统计学中，数据源变更所带来的风险、责任和不确定性，并提供一份清单列出高频的变更起因和原因。

    

    数据科学在官方统计数据的生产中变得越来越重要，因为它使大量数据的自动收集、处理和分析成为可能。随着这样的数据科学方法的应用，它使得报告变得更及时、更有深度和更具灵活性。然而，数据科学驱动的统计数据的质量和完整性取决于数据源和支持它们的机器学习技术的准确性和可靠性。特别地，数据源的变更是不可避免的，它们会引发重大的风险，在机器学习驱动的统计学中必须得到妥善处理。本文概述了在官方统计机器学习中，与数据源变更相关的主要风险、责任和不确定性。我们提供了一个清单，列出了数据源变更最常见的起因和原因，不仅是在技术层面，而且涉及所有权、伦理和法规等方面。

    Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, 
    
[^51]: CaptAinGlove：基于电容和惯性融合的手套，用于实时边缘手势识别，用于无人机控制。

    CaptAinGlove: Capacitive and Inertial Fusion-Based Glove for Real-Time on Edge Hand Gesture Recognition for Drone Control. (arXiv:2306.04319v1 [cs.LG])

    [http://arxiv.org/abs/2306.04319](http://arxiv.org/abs/2306.04319)

    CaptAinGlove是一款基于纺织品的手套，用于识别用于无人机控制的手势，具有低功耗、隐私保护和较高的准确性。

    

    我们提出了 CaptAinGlove，这是一种基于纺织品、低功率（1.15瓦）、注重隐私、实时边缘（RTE）手套解决方案，拥有很小的内存占用（2MB），旨在识别用于无人机控制的手势。我们采用轻量级卷积神经网络作为主干模型，并使用分层多模态融合减少功耗并提高准确性。该系统针对9个类别进行离线评估，包括八个手势命令和空闲状态，其F1分数为80%。对于RTE，我们获得了67％的F1分数（一个用户）。

    We present CaptAinGlove, a textile-based, low-power (1.15Watts), privacy-conscious, real-time on-the-edge (RTE) glove-based solution with a tiny memory footprint (2MB), designed to recognize hand gestures used for drone control. We employ lightweight convolutional neural networks as the backbone models and a hierarchical multimodal fusion to reduce power consumption and improve accuracy. The system yields an F1-score of 80% for the offline evaluation of nine classes; eight hand gesture commands and null activity. For the RTE, we obtained an F1-score of 67% (one user).
    
[^52]: 用因果推断和强化学习进行时间过程干预。

    Timing Process Interventions with Causal Inference and Reinforcement Learning. (arXiv:2306.04299v1 [cs.LG])

    [http://arxiv.org/abs/2306.04299](http://arxiv.org/abs/2306.04299)

    该论文使用合成数据进行实验，比较了因果推断和强化学习方法在定时过程干预方面的优劣。实验结果表明，强化学习在效果和稳定性方面均优于因果推断方法。

    

    从理解和预测过程到其优化，这场转变为企业和其他组织带来了巨大的利益。精确计时的过程干预是有效优化的基石。处方式过程监控(PresPM)是过程挖掘的子领域，专注于过程优化。新兴的PresPM文献确定了最先进的方法，因果推断(CI)和强化学习(RL)，但没有进行定量比较。大多数实验是使用历史数据进行的，导致评估方法的准确性问题，并阻止了在线RL。我们的贡献是通过使用合成数据对定时过程干预进行实验证明，使得真正的在线RL和与CI的比较成为可能，并允许对结果进行准确评估。我们的实验揭示了RL的策略胜过CI的策略，并且同时更加稳健。事实上，RL策略的结果比CI更可靠。

    The shift from the understanding and prediction of processes to their optimization offers great benefits to businesses and other organizations. Precisely timed process interventions are the cornerstones of effective optimization. Prescriptive process monitoring (PresPM) is the sub-field of process mining that concentrates on process optimization. The emerging PresPM literature identifies state-of-the-art methods, causal inference (CI) and reinforcement learning (RL), without presenting a quantitative comparison. Most experiments are carried out using historical data, causing problems with the accuracy of the methods' evaluations and preempting online RL. Our contribution consists of experiments on timed process interventions with synthetic data that renders genuine online RL and the comparison to CI possible, and allows for an accurate evaluation of the results. Our experiments reveal that RL's policies outperform those from CI and are more robust at the same time. Indeed, the RL polic
    
[^53]: 通过对比学习进行会话依赖建模的开放领域会话问答的短语检索

    Phrase Retrieval for Open-Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning. (arXiv:2306.04293v1 [cs.CL])

    [http://arxiv.org/abs/2306.04293](http://arxiv.org/abs/2306.04293)

    本文提出了一种使用短语检索的方法直接预测答案，以解决多轮对话问答中传统方法的漏洞和效率问题，同时引入了对话依赖建模和对比学习策略，提高了模型的鲁棒性和效果。

    

    开放领域会话问答(ODConvQA)的目标是通过一个追溯-阅读器模型来回答问题，在准确性和效率上存在问题。本文提出了一种使用短语检索的方法直接预测答案，以解决这一问题并提高模型性能。同时，我们首次研究了其在ODConvQA任务中的适用性，并引入了对话依赖建模和对比学习策略，提高了模型的鲁棒性和效果。

    Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when 
    
[^54]: 基于停车场占用检测的深度学习方法的改进

    Revising deep learning methods in parking lot occupancy detection. (arXiv:2306.04288v1 [cs.LG])

    [http://arxiv.org/abs/2306.04288](http://arxiv.org/abs/2306.04288)

    本文提出了一种基于EfficientNet架构的停车位占用检测算法，并在5个不同的数据集上进行了评估，性能得到提高。

    

    停车场引导系统作为智能城市发展的一部分已经成为一个流行的趋势。这些系统的关键部分是允许驾驶员在感兴趣的区域搜索可用停车位的算法。传统的方法是基于神经网络分类器应用于摄像头记录。然而，现有的系统在特定的视觉条件下缺乏泛化能力和适当的测试。在本研究中，我们广泛评估了最先进的停车位占用检测算法，将它们的预测质量与最近出现的视觉Transformer进行比较，并基于EfficientNet架构提出了一个新的管道。进行的计算实验已经证明，在我们的模型的情况下，性能有了提高，该模型在5种不同数据集上进行了评估。

    Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.
    
[^55]: ColNav: 结肠镜下的实时结肠导航系统

    ColNav: Real-Time Colon Navigation for Colonoscopy. (arXiv:2306.04269v1 [cs.CV])

    [http://arxiv.org/abs/2306.04269](http://arxiv.org/abs/2306.04269)

    该论文提出了一种实时导航系统，该系统向医生提供可操作和易理解的指导，以指导未检查的结肠区域，从而提高检查质量和发现异常病变的准确性。

    

    结肠镜检查是大肠癌筛查的主要方法之一，但检查质量可能受到操作者技术、训练、判断等方面的影响。本文提出了一种新的结肠镜导航系统，采用实时方法展示结肠的展开视图和指示未检查区域的局部指示器，向医生提供实时的指导。

    Colorectal cancer screening through colonoscopy continues to be the dominant global standard, as it allows identifying pre-cancerous or adenomatous lesions and provides the ability to remove them during the procedure itself. Nevertheless, failure by the endoscopist to identify such lesions increases the likelihood of lesion progression to subsequent colorectal cancer. Ultimately, colonoscopy remains operator-dependent, and the wide range of quality in colonoscopy examinations among endoscopists is influenced by variations in their technique, training, and diligence. This paper presents a novel real-time navigation guidance system for Optical Colonoscopy (OC). Our proposed system employs a real-time approach that displays both an unfolded representation of the colon and a local indicator directing to un-inspected areas. These visualizations are presented to the physician during the procedure, providing actionable and comprehensible guidance to un-surveyed areas in real-time, while seaml
    
[^56]: 置换等变图框架在异质半监督学习中的应用

    Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])

    [http://arxiv.org/abs/2306.04265](http://arxiv.org/abs/2306.04265)

    本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。

    

    异质图的本质与同质图显著不同，这表明1-hop以外的聚合方式并引起早期图神经网络模型的困难。本文展示了一种新的多尺度提取方法，通过构建具有置换等变性，高效性和稀疏性的Haar-type图框架，在图上深度学习任务中实现。我们进一步使用我们构建的图框架设计了图框架神经网络模型PEGFAN。实验在合成数据集和9个基准数据集上进行，与其他最先进的模型进行性能比较。结果表明，我们的模型在某些异质图数据集（包括相对较大和更密集的连接的大部分异质数据集）上可以达到最佳性能，并在其余数据集上具有竞争性能。

    The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
    
[^57]: 基于贝叶斯优化的自适应加权期望改进方法

    Self-Adjusting Weighted Expected Improvement for Bayesian Optimization. (arXiv:2306.04262v1 [cs.LG])

    [http://arxiv.org/abs/2306.04262](http://arxiv.org/abs/2306.04262)

    本文提出了一种新的自适应加权期望改进方法（SAWEI），可以自动平衡探索不确定区域和利用有承诺区域之间的权衡。在COCO基准测试中，该方法表现出有利的性能。

    

    贝叶斯优化（BO）是一种基于代理模型，对小型评估预算的黑箱问题进行优化的高效算法类。BO管道本身高度可配置，涉及初始设计、代理模型和获取功能（AF）的许多不同设计选择。然而，我们对如何为手头问题选择合适的组件的理解非常有限。在本文中，我们的重点是AF的定义，其主要目的是平衡对高不确定性区域和对好解决方案有高承诺的区域之间的探索和利用的权衡。我们提出了自适应加权期望改进方法（SAWEI），其中我们让探索 - 利用权衡以数据驱动的方式进行自适应，基于BO的收敛准则。在COCO基准平台的无噪声黑箱BBOB函数上，我们的方法相对于手工制作的基线表现出有利的任何时间性能，并且是一个稳健的SP。

    Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient algorithms for optimizing black-box problems with small evaluation budgets. The BO pipeline itself is highly configurable with many different design choices regarding the initial design, surrogate model, and acquisition function (AF). Unfortunately, our understanding of how to select suitable components for a problem at hand is very limited. In this work, we focus on the definition of the AF, whose main purpose is to balance the trade-off between exploring regions with high uncertainty and those with high promise for good solutions. We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let the exploration-exploitation trade-off self-adjust in a data-driven manner, based on a convergence criterion for BO. On the noise-free black-box BBOB functions of the COCO benchmarking platform, our method exhibits a favorable any-time performance compared to handcrafted baselines and serves as a robust def
    
[^58]: 当学习随时间预测治疗效果时考虑信息抽样

    Accounting For Informative Sampling When Learning to Forecast Treatment Outcomes Over Time. (arXiv:2306.04255v1 [stat.ML])

    [http://arxiv.org/abs/2306.04255](http://arxiv.org/abs/2306.04255)

    本文针对存在信息抽样的观测数据，提出一种通过逆强度加权来学习治疗效果的通用框架，并提出了一种新方法TESAR-CDE。

    

    机器学习在准确预测治疗效果随时间变化方面具有巨大潜力，这最终可以使更多实际应用中采用个性化治疗策略成为可能。然而，机器学习领域在这个主题上被大量忽视的一个重要挑战是观测数据中存在信息抽样。当实例在时间上不规则观测时，抽样时间通常不是随机的，而是具有信息性的 - 取决于实例的特征、过去的结果和施用的治疗方案。在本文中，我们将信息抽样形式化为一个协变换移问题，并证明如果不适当地考虑它会限制治疗效果的准确估计。为了克服这个挑战，我们提出了一个通用框架，用于在存在信息抽样的情况下学习治疗效果，并提出了一种新的方法，TESAR-CDE，来实现这个框架。

    Machine learning (ML) holds great potential for accurately forecasting treatment outcomes over time, which could ultimately enable the adoption of more individualized treatment strategies in many practical applications. However, a significant challenge that has been largely overlooked by the ML literature on this topic is the presence of informative sampling in observational data. When instances are observed irregularly over time, sampling times are typically not random, but rather informative -- depending on the instance's characteristics, past outcomes, and administered treatments. In this work, we formalize informative sampling as a covariate shift problem and show that it can prohibit accurate estimation of treatment outcomes if not properly accounted for. To overcome this challenge, we present a general framework for learning treatment outcomes in the presence of informative sampling using inverse intensity-weighting, and propose a novel method, TESAR-CDE, that instantiates this f
    
[^59]: 基于神经网络动力学的对抗样本检测方法

    Adversarial Sample Detection Through Neural Network Transport Dynamics. (arXiv:2306.04252v1 [cs.LG])

    [http://arxiv.org/abs/2306.04252](http://arxiv.org/abs/2306.04252)

    本文提出基于神经网络动力学的对抗样本检测器，并通过规范化向量场的训练方法来提高其干净输入和异常输入的区分度。实验结果表明，该方法在不同攻击下的表现优于其他检测器，同时也提高了对抗检测器的测试准确率。

    

    本文提出了一种基于神经网络离散动态系统的对抗样本检测器。通过比较输入通过层遵循的离散向量场，该检测器可以将干净输入从异常输入中区分出来。我们还展示了在训练过程中对该向量场进行规范化可以使网络在数据分布的支持上更加规则化，从而使干净输入的激活更容易与异常输入的激活区分开来。在实验中，我们将我们的检测器与其他检测器进行了比较，并展示了规范化网络动态可以提高将内部嵌入用作输入的对抗检测器的性能，同时也可以提高测试准确率。

    We propose a detector of adversarial samples that is based on the view of neural networks as discrete dynamic systems. The detector tells clean inputs from abnormal ones by comparing the discrete vector fields they follow through the layers. We also show that regularizing this vector field during training makes the network more regular on the data distribution's support, thus making the activations of clean inputs more distinguishable from those of abnormal ones. Experimentally, we compare our detector favorably to other detectors on seen and unseen attacks, and show that the regularization of the network's dynamics improves the performance of adversarial detectors that use the internal embeddings as inputs, while also improving test accuracy.
    
[^60]: 随机坍缩：如何利用梯度噪声使SGD动态趋向更简单的子网络

    Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])

    [http://arxiv.org/abs/2306.04251](http://arxiv.org/abs/2306.04251)

    SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。

    

    本文揭示了随机梯度下降（SGD）的一个强烈隐式偏好，它将过度表达的网络驱动到更简单的子网络，从而大大减少了独立参数的数量，并提高了泛化能力。为了揭示这个偏好，我们识别了不变集，或者说是SGD未修改的参数空间的子集。我们专注于两类不变集，它们对应于现代架构中常见的更简单的子网络。我们的分析揭示了SGD在这些简单不变集方面具有随机吸引性的特性。我们根据损失景观在不变集周围的曲率和随机梯度引入的噪声之间的竞争建立了一种随机吸引性的充分条件。值得注意的是，我们发现增加噪声水平会增强吸引力，导致与鞍点或训练损失的局部极大值相关的吸引不变集的出现。

    In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
    
[^61]: 数据挖掘在逆问题中加速解决方案的可解释性: 以添加制造为例

    Data Mining for Faster, Interpretable Solutions to Inverse Problems: A Case Study Using Additive Manufacturing. (arXiv:2306.04228v1 [cs.LG])

    [http://arxiv.org/abs/2306.04228](http://arxiv.org/abs/2306.04228)

    本文探讨了针对逆问题的数据挖掘算法，通过锥形截断的修改，可以显著加速高斯过程代理的速度而不影响预测准确度，同时使用Kohonen自组织映射可以在高维空间中更好地解释问题的解决方案。

    

    解决逆问题，即找到产生期望输出值的输入值，是具有挑战性的。解决此类问题通常需要大量的计算，而且在高维输入空间中理解解决方案也很困难。本文针对添加制造中的问题，解决了这两个问题，旨在使逆问题的解决更加容易，并利用其结果。首先，我们着重研究了高斯过程代理，描述了通过对锥形截断的简单修改，如何显著加速代理而不失预测准确度。其次，我们展示了 Kohonen 自组织映射可以用于在高维输入空间中直观地解释逆问题的解决方案。针对我们的数据集，由于并非所有输入维度都同样重要，我们展示了使用加权距离得到更好组织映射的关系。

    Solving inverse problems, where we find the input values that result in desired values of outputs, can be challenging. The solution process is often computationally expensive and it can be difficult to interpret the solution in high-dimensional input spaces. In this paper, we use a problem from additive manufacturing to address these two issues with the intent of making it easier to solve inverse problems and exploit their results. First, focusing on Gaussian process surrogates that are used to solve inverse problems, we describe how a simple modification to the idea of tapering can substantially speed up the surrogate without losing accuracy in prediction. Second, we demonstrate that Kohonen self-organizing maps can be used to visualize and interpret the solution to the inverse problem in the high-dimensional input space. For our data set, as not all input dimensions are equally important, we show that using weighted distances results in a better organized map that makes the relations
    
[^62]: 规范化层是锐度感知最小化所需的一切

    Normalization Layers Are All That Sharpness-Aware Minimization Needs. (arXiv:2306.04226v1 [cs.LG])

    [http://arxiv.org/abs/2306.04226](http://arxiv.org/abs/2306.04226)

    SAM算法中，仅扰动规范化层可优化模型性能，在不同的网络架构中都适用，稀疏扰动方法不行。这发现对SAM算法的有效性产生怀疑。

    

    锐度感知最小化（SAM）旨在减少最小值的锐度，并已被证明在各种情况下提高了泛化性能。在这项工作中，我们证明在SAM的对抗步骤中只扰动仿射规范化参数（仅占总参数的0.1%以下）优于扰动所有参数。这一发现适用于不同的SAM变体和ResNet（批量归一化）以及Vision Transformer（层归一化）架构。我们考虑了替代的稀疏扰动方法，并发现这些方法在如此极端的稀疏水平下无法实现类似的性能提升，表明这种行为是规范化层特有的。虽然我们的发现重新证实了SAM在提高泛化性能方面的有效性，但它们对减少锐度是否唯一导致性能提高产生了怀疑。我们的实验代码可在 https://github.com/mueller-mp/SAM-ON 上公开获取。

    Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (comprising less than 0.1% of the total parameters) in the adversarial step of SAM outperforms perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness. The code for our experiments is publicly available at https://github.com/mueller-mp/SAM-ON.
    
[^63]: 基于补丁选择的高效视觉Transformer在人体姿态估计中的应用

    Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v1 [cs.CV])

    [http://arxiv.org/abs/2306.04225](http://arxiv.org/abs/2306.04225)

    该论文提出了一种基于补丁选择的高效视觉Transformer方法，大幅度提高了处理速度和降低计算复杂度，用于2D人体姿态估计方面。

    

    虽然卷积神经网络在2D人体姿态估计方面已经取得了广泛成功，但是视觉Transformer作为卷积神经网络的有力替代者，通过提高最先进的性能而崭露头角。 然而，视觉Transformer的二次计算复杂度限制了其在处理高分辨率图像和长视频方面的适用性。为了解决这一挑战，我们提出了一种简单的方法来减少视觉Transformer的计算复杂度，基于选择和处理少量最具信息的补丁，而忽略其他地方的补丁。我们利用轻量级姿态估计网络来指导补丁选择过程，确保所选补丁包含最重要的信息。我们在三个广泛使用的2D姿态估计基准（即COCO、MPII和OCHuman）上的实验结果表明，我们提出的方法在显著提高速度和降低计算复杂度方面具有很好的效果，虽然性能略有下降。

    While Convolutional Neural Networks (CNNs) have been widely successful in 2D human pose estimation, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, boosting state-of-the-art performance. However, the quadratic computational complexity of ViTs has limited their applicability for processing high-resolution images and long videos. To address this challenge, we propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others. We leverage a lightweight pose estimation network to guide the patch selection process, ensuring that the selected patches contain the most important information. Our experimental results on three widely used 2D pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the effectiveness of our proposed methods in significantly improving speed and reducing computational complexity with a slight drop in performance.
    
[^64]: 学习制定最佳返工策略的因果关系研究

    Causally Learning an Optimal Rework Policy. (arXiv:2306.04223v1 [stat.ML])

    [http://arxiv.org/abs/2306.04223](http://arxiv.org/abs/2306.04223)

    本文利用双重/无偏机器学习方法研究了光电半导体制造中的返工步骤，为零件返工制定策略并从经验上估计它们的价值。

    

    在制造业中，返工是一种旨在消除错误或纠正不符合所需质量标准的产品的可选生产步骤。重新加工生产批次涉及重复以前的生产阶段，并进行调整以确保最终产品符合所需规格。虽然提供了改善产量从而增加生产批次收入的机会，但返工步骤也会产生额外的成本。此外，重新加工已满足目标规格的零件可能会损坏它们并降低产量。本文应用双重/无偏机器学习（DML）来估计光电半导体制造中颜色转换过程中一次返工步骤对最终产品产量的条件处理效应。 我们利用DoubleML实现制定零件返工策略并从经验上估计它们的价值。从我们的因果机器学习分析中

    In manufacturing, rework refers to an optional step of a production process which aims to eliminate errors or remedy products that do not meet the desired quality standards. Reworking a production lot involves repeating a previous production stage with adjustments to ensure that the final product meets the required specifications. While offering the chance to improve the yield and thus increase the revenue of a production lot, a rework step also incurs additional costs. Additionally, the rework of parts that already meet the target specifications may damage them and decrease the yield. In this paper, we apply double/debiased machine learning (DML) to estimate the conditional treatment effect of a rework step during the color conversion process in opto-electronic semiconductor manufacturing on the final product yield. We utilize the implementation DoubleML to develop policies for the rework of components and estimate their value empirically. From our causal machine learning analysis we 
    
[^65]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^66]: DualHGNN: 基于多视图学习和密度感知的半监督节点分类的双重超图神经网络

    DualHGNN: A Dual Hypergraph Neural Network for Semi-Supervised Node Classification based on Multi-View Learning and Density Awareness. (arXiv:2306.04214v1 [cs.LG])

    [http://arxiv.org/abs/2306.04214](http://arxiv.org/abs/2306.04214)

    本文提出了DualHGNN，利用双重超图神经网络模型，将超图结构学习和表示学习相结合，以更好地实现半监督节点分类。

    

    基于图的半监督节点分类已被证明是许多具有高研究价值和意义的应用中的最先进方法。大多数现有方法仅基于原始的内在或人为建立的图结构，这可能无法准确反映数据之间的“真实”相关性，并且对下游图神经网络中的半监督节点分类不是最优的。此外，现有的基于图的方法大多利用显式图结构，而一些隐式信息，例如密度信息，也可以提供潜在的信息，可以进一步利用。为了解决这些限制，本文提出了DualHGNN，一种新的双重连接模型，将超图结构学习和超图表示学习同时集成在统一的架构中。

    Graph-based semi-supervised node classification has been shown to become a state-of-the-art approach in many applications with high research value and significance. Most existing methods are only based on the original intrinsic or artificially established graph structure which may not accurately reflect the "true" correlation among data and are not optimal for semi-supervised node classification in the downstream graph neural networks. Besides, while existing graph-based methods mostly utilize the explicit graph structure, some implicit information, for example, the density information, can also provide latent information that can be further exploited. To address these limitations, this paper proposes the Dual Hypergraph Neural Network (DualHGNN), a new dual connection model integrating both hypergraph structure learning and hypergraph representation learning simultaneously in a unified architecture. The DualHGNN first leverages a multi-view hypergraph learning network to explore the o
    
[^67]: 迁移族群以实现公平GNN

    Migrate Demographic Group For Fair GNNs. (arXiv:2306.04212v1 [cs.LG])

    [http://arxiv.org/abs/2306.04212](http://arxiv.org/abs/2306.04212)

    该论文提出了一个名为FairMigration的新框架，可以动态迁移族群，而不是用原始的敏感属性来固定族群，以训练公平的GNN。

    

    由于图形学习的卓越性能，图神经网络（GNN）已被应用于许多场景。然而，在设计GNN时常常忽略公平性。因此，训练数据中的有偏信息很容易影响普通的GNN，导致对特定人口群体（根据敏感属性，如种族和年龄划分）的偏见结果。已经有一些努力来解决公平性问题。然而，现有的公平技术通常通过原始敏感属性将族群进行划分，并假定它们是固定的。与原始敏感属性相关的有偏信息将通过训练过程，无论实施公平技术与否。迫切需要解决这个问题，以训练公平的GNN。为了解决这个问题，我们提出了一个全新的框架，FairMigration，它可以动态迁移族群，而不是用原始的敏感属性固定它们。FairMigration由两个训练阶段组成。

    Graph Neural networks (GNNs) have been applied in many scenarios due to the superior performance of graph learning. However, fairness is always ignored when designing GNNs. As a consequence, biased information in training data can easily affect vanilla GNNs, causing biased results toward particular demographic groups (divided by sensitive attributes, such as race and age). There have been efforts to address the fairness issue. However, existing fair techniques generally divide the demographic groups by raw sensitive attributes and assume that are fixed. The biased information correlated with raw sensitive attributes will run through the training process regardless of the implemented fair techniques. It is urgent to resolve this problem for training fair GNNs. To tackle this problem, we propose a brand new framework, FairMigration, which can dynamically migrate the demographic groups instead of keeping that fixed with raw sensitive attributes. FairMigration is composed of two training s
    
[^68]: 利用知识图谱嵌入增强关系提取的上下文表示

    Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction. (arXiv:2306.04203v1 [cs.CL])

    [http://arxiv.org/abs/2306.04203](http://arxiv.org/abs/2306.04203)

    本论文提出了一种不依赖于大规模知识图谱或预训练语言模型的关系提取方法，通过利用语料库中实体的分层结构和关系分布，将预先训练的知识图谱嵌入到句子级上下文表示中，可以显著提高模型性能。

    

    关系提取是自然语言处理中至关重要且具有挑战性的任务。近年来出现了多种方法，表现出在解决任务方面的显着性能；然而，这些方法中大多数依赖于大规模知识图谱或预先训练在海量语料库上的语言模型的大量数据。本文关注如何有效利用语料库提供的知识来创建高性能模型。我们的目标是展示，在不引入外部知识的情况下，通过利用语料库中实体的分层结构和关系分布，可以显著提高关系提取模型的性能。因此，本文提出了一种基于将预先训练的知识图谱嵌入到句子级上下文表示中的关系提取方法。我们进行了一系列实验，发现结果令人兴奋且很有意思。

    Relation extraction task is a crucial and challenging aspect of Natural Language Processing. Several methods have surfaced as of late, exhibiting notable performance in addressing the task; however, most of these approaches rely on vast amounts of data from large-scale knowledge graphs or language models pretrained on voluminous corpora. In this paper, we hone in on the effective utilization of solely the knowledge supplied by a corpus to create a high-performing model. Our objective is to showcase that by leveraging the hierarchical structure and relational distribution of entities within a corpus without introducing external knowledge, a relation extraction model can achieve significantly enhanced performance. We therefore proposed a relation extraction approach based on the incorporation of pretrained knowledge graph embeddings at the corpus scale into the sentence-level contextual representation. We conducted a series of experiments which revealed promising and very interesting res
    
[^69]: 在高斯过程模型的近似推断中改善超参数学习

    Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models. (arXiv:2306.04201v1 [cs.LG])

    [http://arxiv.org/abs/2306.04201](http://arxiv.org/abs/2306.04201)

    本文改进了高斯过程模型中的超参数学习，提出了一种混合训练方法来兼顾变分推断和期望传播方法，以优化超参数的学习目标，该方法实验结果表明有效性。

    

    在具有非共轭似然函数的高斯过程（GP）模型中，近似推断与模型超参数的学习纠缠在一起。我们改进了 GP 模型中的超参数学习，并关注变分推断（VI）与学习目标之间的相互作用。虽然 VI 对边缘似然函数的下界是推断近似后验的合适目标，但我们发现像期望传播（EP）中直接逼近边缘似然函数是更适合超参数优化的学习目标。我们设计了一个混合训练过程，将最佳效果结合到一起：利用共轭计算 VI 进行推断，并使用类似于 EP 的边缘似然函数逼近进行超参数学习。我们比较了 VI、EP、Laplace 近似和我们提出的训练过程，并在广泛的数据集上经验证明了我们的提议的有效性。

    Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.
    
[^70]: 基于ASR的阅读教学辅导系统：对小学生反馈进行优化的探讨

    An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to First Graders. (arXiv:2306.04190v1 [cs.CL])

    [http://arxiv.org/abs/2306.04190](http://arxiv.org/abs/2306.04190)

    研究开发了两个新的ASR系统并与先前的研究进行比较，结果表明优化ASR系统提供给小学一年级学生的反馈可以提高他们的阅读进步。

    

    近年来，人们对应用自动语音识别（ASR）于阅读练习中的兴趣日益增长。在一项先前的研究中，我们介绍了一款基于ASR的荷兰语阅读辅导软件，旨在帮助正在学习阅读的小学一年级学生及时获得反馈。结果表明，ASR在阅读的初级阶段有潜力帮助学生提高阅读准确性和流畅度。在本研究中，我们使用现有语料库（JASMIN）中的儿童语音开发了两个新的ASR系统，并将结果与先前的研究进行比较。我们使用 Cohen's Kappa、Matthews相关系数（MCC）、精确度、召回率和F-measures等评估方法，以单词为单位分析ASR系统的正确/错误分类，并与人工判断进行比较。我们发现新开发的ASR系统在与人类判断的一致性和正确的拒绝错误语音方面有所改进。结果表明，优化ASR系统提供给小学一年级学生的反馈可以提高他们的阅读进步。

    The interest in employing automatic speech recognition (ASR) in applications for reading practice has been growing in recent years. In a previous study, we presented an ASR-based Dutch reading tutor application that was developed to provide instantaneous feedback to first-graders learning to read. We saw that ASR has potential at this stage of the reading process, as the results suggested that pupils made progress in reading accuracy and fluency by using the software. In the current study, we used children's speech from an existing corpus (JASMIN) to develop two new ASR systems, and compared the results to those of the previous study. We analyze correct/incorrect classification of the ASR systems using human transcripts at word level, by means of evaluation measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC), precision, recall and F-measures. We observe improvements for the newly developed ASR systems regarding the agreement with human-based judgment and correct reje
    
[^71]: 自监督音频师生Transformer用于片段级和帧级任务

    Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks. (arXiv:2306.04186v1 [eess.AS])

    [http://arxiv.org/abs/2306.04186](http://arxiv.org/abs/2306.04186)

    本文提出了两种自监督音频表示学习方法：ATST-Clip和ATST-Frame；这两种方法使用基于Transformer的师生模型，通过音频实例辨别和音频序列重构两个自监督任务进行训练；评估结果表明，这两种方法在片段级和帧级下游任务上都优于其他最先进的预训练方法。

    

    近年来，自监督学习已成为学习音频表示的流行方法。音频自监督预训练的最终目标是将知识传递到下游音频任务中，通常包括片段级和帧级任务。为了应对这两种任务，本文提出了两种自监督音频表示学习方法：ATST-Clip和ATST-Frame，分别用于学习片段级和帧级表示。这两种方法都使用基于Transformer的师生模型，并通过音频实例辨别和音频序列重构两个自监督任务进行训练。评估结果显示，我们提出的方法在片段级和帧级下游任务上均优于几个最先进的预训练方法。

    In recent years, self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. The ultimate goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. Clip-level tasks classify the scene or sound of an entire audio clip, e.g. audio tagging, instrument recognition, etc. While frame-level tasks detect event-level timestamps from an audio clip, e.g. sound event detection, speaker diarization, etc. Prior studies primarily evaluate on clip-level downstream tasks. Frame-level tasks are important for fine-grained acoustic scene/event understanding, and are generally more challenging than clip-level tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes two self-supervised audio representation learning methods: ATST-Clip and ATST-Frame, responsible for learning clip-level and frame-level representations, respectively. ATST stands for Aud
    
[^72]: 基于语言模型的“考官”对基础模型进行基准测试

    Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])

    [http://arxiv.org/abs/2306.04181](http://arxiv.org/abs/2306.04181)

    本文提出了一种新的基准测试框架，使用语言模型作为考官，可以无参考方式评估答案。这个框架解决了过去基准测试流程中的测试泄漏和评估自动化问题，并允许易于扩展，可以采用不同的语言模型作为考官。

    

    已经建立了许多基准测试来评估基础模型在开放式问答方面的表现，它是测试模型理解和生成语言的能力的全面测试。大多数工作集中在提出新的数据集，然而，我们在之前的基准测试流程中看到了两个主要问题，即测试泄漏和评估自动化。在本文中，我们提出了一种新的基准测试框架，语言模型作为考官（LMAE），其中LM作为知识渊博的考官，根据其知识制定问题并以无参考方式评估答案。我们的框架允许易于扩展，因为可以采用各种LM作为考官，并且可以不断更新问题，给予更多样化的触发主题。为了更全面和公正地评估，我们设计了三个策略：（1）我们指示LM考官在许多领域生成问题。

    Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains 
    
[^73]: 优化输运模型的分布鲁棒性

    Optimal Transport Model Distributional Robustness. (arXiv:2306.04178v1 [cs.LG])

    [http://arxiv.org/abs/2306.04178](http://arxiv.org/abs/2306.04178)

    本文提出了一种优化输运模型的分布鲁棒性框架，能够显著提高深度学习模型的鲁棒性，可灵活地将锐度感知纳入到单个模型、集成模型和贝叶斯神经网络的训练中。

    

    分布鲁棒性是一种有希望的框架，用于训练深度学习模型，使其对抗性例子和数据分布变化的影响更小。先前的工作主要集中在利用数据空间的分布鲁棒性上。在本文中，我们探讨了一种基于最优输运的模型空间分布鲁棒性框架。具体而言，我们研究了在给定中心模型分布的Wasserstein球中的模型分布，该模型分布最大化了损失。我们开发出了理论，允许我们学习最佳的鲁棒中心模型分布。有趣的是，通过我们开发的理论，我们可以通过考虑特定形式的中心模型分布（如单个模型上的Dirac delta分布，多个模型上的均匀分布和一般的贝叶斯神经网络）来灵活地将锐度感知的概念纳入到单个模型、集成模型和贝叶斯神经网络的训练中。此外，我们证明了所提出的框架显著提高了在各种基准数据集上的深度学习模型的分布鲁棒性。

    Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in data space. In this work, we explore an optimal transport-based distributional robustness framework on model spaces. Specifically, we examine a model distribution in a Wasserstein ball of a given center model distribution that maximizes the loss. We have developed theories that allow us to learn the optimal robust center model distribution. Interestingly, through our developed theories, we can flexibly incorporate the concept of sharpness awareness into training a single model, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution, such as a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrat
    
[^74]: 基于贝叶斯视角的随机优化端到端学习方法

    End-to-End Learning for Stochastic Optimization: A Bayesian Perspective. (arXiv:2306.04174v1 [math.OC])

    [http://arxiv.org/abs/2306.04174](http://arxiv.org/abs/2306.04174)

    本文提出了一种基于贝叶斯视角的随机优化端到端学习方法，为经验风险最小化和分布式鲁棒优化问题提供新的端到端学习算法，方式主要是训练决策映射。该方法在合成的newsvendor问题和经济分配问题上均表现出显著的效果，同时也发现决策映射神经网络架构对测试性能的影响很大。

    

    我们提出了一种基于贝叶斯视角的随机优化端到端学习方法，该方法采用了标准端到端学习算法的思想，训练了一个后验贝叶斯行动映射。在此基础上，我们为解决经验风险最小化和分布式鲁棒优化问题提出了新的端到端学习算法。通过合成的newsvendor问题和基于真实数据的经济分配问题的数值结果，我们展示了不同训练方案之间的关键差异以及决策映射神经网络架构对测试性能的影响。

    We develop a principled approach to end-to-end learning in stochastic optimization. First, we show that the standard end-to-end learning algorithm admits a Bayesian interpretation and trains a posterior Bayes action map. Building on the insights of this analysis, we then propose new end-to-end learning algorithms for training decision maps that output solutions of empirical risk minimization and distributionally robust optimization problems, two dominant modeling paradigms in optimization under uncertainty. Numerical results for a synthetic newsvendor problem illustrate the key differences between alternative training schemes. We also investigate an economic dispatch problem based on real data to showcase the impact of the neural network architecture of the decision maps on their test performance.
    
[^75]: 高效交替最小化及其在加权低秩逼近中的应用

    Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v1 [cs.LG])

    [http://arxiv.org/abs/2306.04169](http://arxiv.org/abs/2306.04169)

    本文提出了一种高效的求解加权低秩逼近问题的交替最小化框架，运行时间优化到了 n^2k，核心方法是一种高精度的多响应回归方法。

    

    加权低秩逼近是数值线性代数中的一个基本问题，在机器学习中有许多应用。本文提出了一种高效且鲁棒的交替最小化框架，用于求解该问题，并将运行时间从 n^2k^2 优化到了 n^2k。在我们的框架的核心是一种高精度的多响应回归方法。

    Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n \times n}$, a parameter $k$, the goal is to output two matrices $U, V \in \mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized, where $\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression
    
[^76]: 在对比学习中重新思考弱监督的作用

    Rethinking Weak Supervision in Helping Contrastive Learning. (arXiv:2306.04160v1 [cs.LG])

    [http://arxiv.org/abs/2306.04160](http://arxiv.org/abs/2306.04160)

    本论文探讨了使用半监督和噪声标签为弱监督的对比学习，并建立了一个谱聚类框架用于转换弱监督信息。

    

    对比学习既可以在有监督学习中表现出色，也可用于无监督学习，在半监督学习和噪声标签学习等领域中也被引入。尽管实证证据表明半监督标签可以改善对比学习的表示，但仍不清楚噪声标签学习是否可以直接用于训练，而不是进行手动去噪后再使用。因此，我们建立了一个统一的理论框架，在弱监督下探讨了用于对比学习的半监督和噪声标签之间的机械差异。具体而言，我们研究了最直观的联合训练有监督和无监督对比损失的范式。通过将弱监督信息转换为基于后验概率的谱聚类框架下的相似性图，我们提出了一种新的训练方法。

    Contrastive learning has shown outstanding performances in both supervised and unsupervised learning, and has recently been introduced to solve weakly supervised learning problems such as semi-supervised learning and noisy label learning. Despite the empirical evidence showing that semi-supervised labels improve the representations of contrastive learning, it remains unknown if noisy supervised information can be directly used in training instead of after manual denoising. Therefore, to explore the mechanical differences between semi-supervised and noisy-labeled information in helping contrastive learning, we establish a unified theoretical framework of contrastive learning under weak supervision. Specifically, we investigate the most intuitive paradigm of jointly training supervised and unsupervised contrastive losses. By translating the weakly supervised information into a similarity graph under the framework of spectral clustering based on the posterior probability of weak labels, w
    
[^77]: SANGEET: 一种基于XML的北印度古典音乐研究开放数据集

    SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet. (arXiv:2306.04148v1 [cs.SD])

    [http://arxiv.org/abs/2306.04148](http://arxiv.org/abs/2306.04148)

    该论文介绍了一种名为SANGEET的基于XML的公共数据集，用于存储北印度古典音乐作品的全面信息，并支持从机器学习视角进行音乐信息的数据驱动分析。

    

    获得一种广泛应用的丰富音乐数据集非常重要。目前，可用的数据集主要专注于存储语音或乐器录音数据，并忽略了其视觉表现和检索需求。本文试图构建一个名为SANGEET的基于XML的公共数据集，存储著名音乐学家Pt. Vishnu Narayan Bhatkhande创作的北印度古典音乐作品的全面信息。SANGEET以标准化的方式保存了任何给定作品的所需信息，包括元数据、结构、符记、节奏和旋律信息，以便于音乐信息的易于存储和提取。该数据集旨在为音乐信息研究任务提供真实基础信息，因此支持从机器学习视角进行多项数据驱动分析。我们通过演示其在旋律预测任务和歌曲生成任务中的能力，展示了该数据集的实用性。

    It is very important to access a rich music dataset that is useful in a wide variety of applications. Currently, available datasets are mostly focused on storing vocal or instrumental recording data and ignoring the requirement of its visual representation and retrieval. This paper attempts to build an XML-based public dataset, called SANGEET, that stores comprehensive information of Hindustani Sangeet (North Indian Classical Music) compositions written by famous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the required information of any given composition including metadata, structural, notational, rhythmic, and melodic information in a standardized way for easy and efficient storage and extraction of musical information. The dataset is intended to provide the ground truth information for music information research tasks, thereby supporting several data-driven analysis from a machine learning perspective. We present the usefulness of the dataset by demonstrating i
    
[^78]: UCTB：面向时空人群流预测的城市计算工具箱

    UCTB: An Urban Computing Tool Box for Spatiotemporal Crowd Flow Prediction. (arXiv:2306.04144v1 [cs.LG])

    [http://arxiv.org/abs/2306.04144](http://arxiv.org/abs/2306.04144)

    UCTB是一个城市计算工具箱，它在时空人群流预测方面整合了多个领域知识和最先进的模型，可解决该领域复杂度高、知识多样、模型实现复杂的问题。

    

    时空人群流预测是智慧城市关键技术之一，目前存在两个主要问题：首先，人群流与多个领域知识因素相关，但由于应用场景的多样性，难以合理全面地使用领域知识；其次，随着深度学习技术的发展，相关技术的实现变得越来越复杂，复制先进模型已成为一个耗时和繁琐的任务。为了解决这些问题，我们设计并实现了一个名为UCTB的时空人群流预测工具箱，同时整合了多个时空领域知识和最先进的模型。相关代码和支持文档已在 https://github.com/uctb/UCTB 开源。

    Spatiotemporal crowd flow prediction is one of the key technologies in smart cities. Currently, there are two major pain points that plague related research and practitioners. Firstly, crowd flow is related to multiple domain knowledge factors; however, due to the diversity of application scenarios, it is difficult for subsequent work to make reasonable and comprehensive use of domain knowledge. Secondly, with the development of deep learning technology, the implementation of relevant techniques has become increasingly complex; reproducing advanced models has become a time-consuming and increasingly cumbersome task. To address these issues, we design and implement a spatiotemporal crowd flow prediction toolbox called UCTB (Urban Computing Tool Box), which integrates multiple spatiotemporal domain knowledge and state-of-the-art models simultaneously. The relevant code and supporting documents have been open-sourced at https://github.com/uctb/UCTB.
    
[^79]: 结构化数据生成扩散模型综述

    A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])

    [http://arxiv.org/abs/2306.04139](http://arxiv.org/abs/2306.04139)

    本文全面综述了在结构化数据领域中最近提出的扩散模型，介绍了其理论基础和应用场景。

    

    最近，生成扩散模型（generative diffusion models）在深度生成模型领域取得了突破性成果，展现了在许多应用中的出色表现。与此同时，结构化数据（包括表格和时间序列数据）在深度学习研究界中受到的关注相对较少，尽管其无处不在且应用广泛。因此，与计算机视觉和自然语言处理等其他数据形式相比，利用扩散模型对结构化数据建模的文献及其综述仍然缺乏。因此，本文介绍了在结构化数据领域中最近提出的扩散模型的全面综述。首先，本综述提供了基于得分的扩散模型理论的简要概述，随后又详细描述了在数据驱动的通用任务和特定领域应用中使用结构化数据的大部分开创性工作的技术描述。最后，

    In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
    
[^80]: 使用集合论嵌入回答组合查询

    Answering Compositional Queries with Set-Theoretic Embeddings. (arXiv:2306.04133v1 [cs.IR])

    [http://arxiv.org/abs/2306.04133](http://arxiv.org/abs/2306.04133)

    本文提出了一种基于盒状嵌入的方法来回答组合查询，该方法可以更准确地支持含有多个属性的查询。

    

    在诸如分面导航和推荐系统等许多重要任务中，紧凑而稳健地表示项目-属性关系的需求是必要的。此项任务的一种流行机器学习方法通过向量的高点积表示项目具有属性，这种表示不仅是密集的，而且还能够修正有噪声和不完整的数据。虽然该方法适用于通过单个属性（例如“喜剧电影”）检索项目的查询，但我们发现向量嵌入并不太准确支持组合查询（例如既是喜剧电影又是英国的，但不是浪漫电影）。为了解决这些集合论组合，本文提出用盒状嵌入替换向量，盒状嵌入是一种区域为基础的表示方法，可以看作可学习的文氏图。我们引入了一个新的组合查询基准数据集，并提供了实验和分析结果，以深入了解盒状嵌入的行为。

    The need to compactly and robustly represent item-attribute relations arises in many important tasks, such as faceted browsing and recommendation systems. A popular machine learning approach for this task denotes that an item has an attribute by a high dot-product between vectors for the item and attribute -- a representation that is not only dense, but also tends to correct noisy and incomplete data. While this method works well for queries retrieving items by a single attribute (such as \emph{movies that are comedies}), we find that vector embeddings do not so accurately support compositional queries (such as movies that are comedies and British but not romances). To address these set-theoretic compositions, this paper proposes to replace vectors with box embeddings, a region-based representation that can be thought of as learnable Venn diagrams. We introduce a new benchmark dataset for compositional queries, and present experiments and analysis providing insights into the behavior o
    
[^81]: 多模态融合交互: 人类和自动量化研究

    Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])

    [http://arxiv.org/abs/2306.04125](http://arxiv.org/abs/2306.04125)

    本文比较研究了两种人类注释者可以用于注释多模态交互的分类，并提出了一种基于信息分解的分类学。

    

    在几乎所有多模态问题和应用中，多模态融合多种异构和互联的信号是一个基本挑战。为了进行多模态融合，我们需要理解模态可以展现的交互类型：每种模态如何单独提供对任务有用的信息，以及当存在其他模态时这些信息如何变化。在本文中，我们对人类注释者如何被利用来注释多模态交互的两种分类进行了比较研究：(1) 部分标签，其中不同随机分配的注释者注释给定第一个、第二个和两个模态的标签，以及(2) 反事实标签，其中同一注释者被要求在给出第一个模态之前注释标签，然后给出第二个模态，并要求他们明确地推理他们的答案如何改变，然后提出基于信息分解的另一种分类学。

    Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotator
    
[^82]: 基于本地模板检索的逆向合成预测

    Retrosynthesis Prediction with Local Template Retrieval. (arXiv:2306.04123v1 [cs.AI])

    [http://arxiv.org/abs/2306.04123](http://arxiv.org/abs/2306.04123)

    本文介绍了一种新方法，RetroKNN，它使用基于本地反应模板的k最近邻检索结合神经网络预测来提高逆向合成模型的性能。

    

    逆向合成是药物探索中预测给定目标分子反应物的重要任务。近年来，基于机器学习的逆向合成方法取得了良好的结果。本文介绍了RetroKNN，一种基于本地反应模板检索的方法，通过非参数检索进一步提高基于模板的系统性能。我们首先构建了一个原子模板存储库和一个键模板存储库，其中包含训练数据中的本地模板，然后在推理过程中使用k最近邻（KNN）搜索从这些模板中检索。检索到的模板与神经网络预测相结合作为最终输出。此外，我们还提出了一种轻量级适配器，以调整基于隐藏表示和检索模板的神经网络和KNN预测的权重。我们在两个广泛使用的基准测试USPTO-50K和USPTO-MIT上进行了全面的实验。尤其是在top-1精度方面。

    Retrosynthesis, which predicts the reactants of a given target molecule, is an essential task for drug discovery. In recent years, the machine learing based retrosynthesis methods have achieved promising results. In this work, we introduce RetroKNN, a local reaction template retrieval method to further boost the performance of template-based systems with non-parametric retrieval. We first build an atom-template store and a bond-template store that contain the local templates in the training data, then retrieve from these templates with a k-nearest-neighbor (KNN) search during inference. The retrieved templates are combined with neural network predictions as the final output. Furthermore, we propose a lightweight adapter to adjust the weights when combing neural network and KNN predictions conditioned on the hidden representation and the retrieved templates. We conduct comprehensive experiments on two widely used benchmarks, the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy
    
[^83]: MESSY估计：基于最大熵的随机和符号密度估计

    MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])

    [http://arxiv.org/abs/2306.04120](http://arxiv.org/abs/2306.04120)

    MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。

    

    我们引入了基于最大熵的随机和符号密度估计方法MESSY。所提出的方法使用梯度流的矩将概率密度函数从样本中恢复为符号表达式，并将ansatz作为驱动力。特别地，我们构建了一个基于梯度的漂移扩散过程，将未知分布函数的样本与猜测的符号表达式相连。然后，我们展示出当猜测分布具有最大熵形式时，可以通过使用提供的样本的矩构建的线性方程组高效地找到该分布的参数。此外，我们使用符号回归来探索平滑函数的空间，并找到导致最大熵泛函指数的最优基函数，以获得良好条件。该方法在随机搜索的每次迭代中的成本与样本数量呈线性关系，与变量数量呈二次关系，使其可扩展到高维问题。数值实验显示出所提出方法的有效性和普适性，与现有的最新方法相比。

    We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
    
[^84]: M$^3$Fair：多层次多敏感属性加权方法减轻医疗数据中的偏见

    M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method. (arXiv:2306.04118v1 [cs.LG])

    [http://arxiv.org/abs/2306.04118](http://arxiv.org/abs/2306.04118)

    本文提出了一种名为M$^3$Fair的新方法，可以在多个数据层面和多个敏感属性的情况下平衡训练数据分布，缓解医疗数据中的偏见和不公平现象，提高机器学习模型的公平性能和准确性能。

    

    在数据驱动的人工智能范式中，模型严重依赖于大量的训练数据。然而，样本分布失衡等因素可能导致医疗数据中的偏见和不公平问题。在医疗人工智能中，如种族、性别、年龄和医疗状况等敏感属性通常与歧视或偏见有关。这些属性在医疗保健中可能对个人获得的护理质量产生重大影响。因此，检测和减轻数据中的偏差对于提高健康公平至关重要。偏差缓解方法包括前处理、中处理和后处理。其中，加权（RW）是一种广泛使用的前处理方法，能够在平衡机器学习性能和公平性能方面表现良好。RW调整特定敏感属性的样本权重以解决偏差问题。然而，现有的RW方法在平衡多个敏感属性和处理多层次数据方面效果有限。本文提出了 M$^3$Fair，一种新颖的多层次多敏感属性加权方法，可以平衡不同数据层次上多个敏感属性的分布。M$^3$Fair在维持机器学习性能的同时有效平衡了不同敏感属性的公平性能。在基准医疗数据集上的实验表明，M$^3$Fair在公平性和准确性方面优于现有方法。

    In the data-driven artificial intelligence paradigm, models heavily rely on large amounts of training data. However, factors like sampling distribution imbalance can lead to issues of bias and unfairness in healthcare data. Sensitive attributes, such as race, gender, age, and medical condition, are characteristics of individuals that are commonly associated with discrimination or bias. In healthcare AI, these attributes can play a significant role in determining the quality of care that individuals receive. For example, minority groups often receive fewer procedures and poorer-quality medical care than white individuals in US. Therefore, detecting and mitigating bias in data is crucial to enhancing health equity. Bias mitigation methods include pre-processing, in-processing, and post-processing. Among them, Reweighting (RW) is a widely used pre-processing method that performs well in balancing machine learning performance and fairness performance. RW adjusts the weights for samples wit
    
[^85]: 不平衡的最优输运在不平衡的单语词对齐中的应用

    Unbalanced Optimal Transport for Unbalanced Word Alignment. (arXiv:2306.04116v1 [cs.CL])

    [http://arxiv.org/abs/2306.04116](http://arxiv.org/abs/2306.04116)

    本文通过最优输运，实现了既重视对齐又重视空对齐的不平衡单语词对齐，实验结果表明其在具有挑战性的数据集上表现出色。

    

    单语词对齐对于模型化句子之间的语义交互至关重要。特别地，空对齐是一种普遍且关键的现象，用于处理语义上不相似的句子。识别空对齐本身就有助于推断句子的语义相似性，因为它表明存在信息不平等。本研究展示了最优输运的家族（平衡、部分和不平衡输运）是自然且强大的方法，即使没有量身定制的技术也能实现既重视对齐又重视空对齐的不平衡单语词对齐。我们的广泛实验涵盖无监督和监督设置，并表明我们的通用OT-based对齐方法在具有高空对齐频率的具有挑战性的数据集上是能竞争最先进技术的。

    Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques. Our extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies.
    
[^86]: 大规模分布式学习的拟牛顿更新

    Quasi-Newton Updating for Large-Scale Distributed Learning. (arXiv:2306.04111v1 [cs.LG])

    [http://arxiv.org/abs/2306.04111](http://arxiv.org/abs/2306.04111)

    本文提出了一种具有出色统计、计算和通信效率的分布式拟牛顿(DQN)框架，与现有方法相比，它不需要牛顿矩阵求逆或通信，并且通过理论证明和数值分析证明其统计特性和有限的样本性能。

    

    分布式计算对于现代统计分析至关重要。本文提出了一种具有出色的统计、计算和通信效率的分布式拟牛顿(DQN)框架。在DQN方法中，不需要牛顿矩阵求逆或通信，这大大减少了所提出方法的计算和通信复杂性。值得注意的是，现有的相关方法只分析数值收敛，并需要发散的迭代次数才能收敛。然而，我们研究了DQN方法的统计特性，并在温和条件下理论上证明了结果估计器在少量迭代下的统计效率。广泛的数值分析证明了有限的样本性能。

    Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.
    
[^87]: 使用相对决策边界距离的成员身份推断攻击

    Membership inference attack with relative decision boundary distance. (arXiv:2306.04109v1 [cs.LG])

    [http://arxiv.org/abs/2306.04109](http://arxiv.org/abs/2306.04109)

    本论文提出了一种新的标签仅成员身份推断攻击方法，使用相对决策边界距离来避免在不同的初始图像下产生相反的结果。

    

    成员身份推断攻击是机器学习中最流行的隐私攻击之一，旨在预测给定样本是否包含在目标模型的训练集中。标签仅成员身份推断攻击是一种变体，利用样本的鲁棒性，并受到更多关注，因为它假设敌手只能访问输入样本的预测标签。然而，由于决策边界距离受随机初始图像的影响很大，因此即使对于相同的输入样本，攻击者可能会得到相反的结果。在本文中，我们提出了一种新的攻击方法，称为标签仅设置中的多类自适应成员身份推断攻击。在早期的攻击迭代中，所有目标类别的决策边界距离已被遍历，随后的攻击迭代会继续使用最短的决策边界距离以获得稳定和最佳的决策边界距离。

    Membership inference attack is one of the most popular privacy attacks in machine learning, which aims to predict whether a given sample was contained in the target model's training set. Label-only membership inference attack is a variant that exploits sample robustness and attracts more attention since it assumes a practical scenario in which the adversary only has access to the predicted labels of the input samples. However, since the decision boundary distance, which measures robustness, is strongly affected by the random initial image, the adversary may get opposite results even for the same input samples. In this paper, we propose a new attack method, called muti-class adaptive membership inference attack in the label-only setting. All decision boundary distances for all target classes have been traversed in the early attack iterations, and the subsequent attack iterations continue with the shortest decision boundary distance to obtain a stable and optimal decision boundary distan
    
[^88]: BeMap：平衡的消息传递方法用于公平的图神经网络。

    BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])

    [http://arxiv.org/abs/2306.04107](http://arxiv.org/abs/2306.04107)

    本文提出了一种公平的消息传递方法，称为BeMap，旨在解决消息传递中的偏差放大问题，通过平衡感知的采样策略来平衡不同人口群体的1-hop邻居的数量。

    

    图神经网络（GNN）通过迭代地聚合每个节点的局部邻域信息来表现出强大的实证性能，即消息传递。然而，具体证据显示，图神经网络可能对某些人口群体存在偏见，这要求考虑算法的公正性。尽管越来越多的努力在保证图神经网络的算法公平性，但在训练期间往往并不明确考虑消息传递在GNN中引起的偏差。本文首先研究了消息传递中的偏差放大问题。我们通过经验证据和理论证明，当来自不同人口群体的1-hop邻居不平衡时，消息传递可能会放大偏差。在这些分析的指导下，我们提出了BeMap，一种公平的消息传递方法，利用平衡感知的采样策略来平衡每个节点的1-hop邻居的数量。

    Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
    
[^89]: Phoenix：一种联邦式生成扩散模型

    Phoenix: A Federated Generative Diffusion Model. (arXiv:2306.04098v1 [cs.LG])

    [http://arxiv.org/abs/2306.04098](http://arxiv.org/abs/2306.04098)

    本文提出了 Phoenix 一种联邦式生成扩散模型，利用联邦学习技术跨多个数据源进行训练，实现生成质量更好的图像。该模型在保护数据隐私的前提下，提高了生成样本的数据多样性。

    

    生成人工智能在实现图像、视频和音频等多样且逼真的视觉内容方面取得了惊人的进展。然而，对大型集中式数据集进行生成模型的训练可能会在数据隐私、安全和可访问性方面带来挑战。联邦学习是一种使用分散技术协作训练共享深度学习模型的方法，同时保留个体边缘设备上的训练数据以保护数据隐私。本文提出了一种使用联邦学习技术跨多个数据源训练去噪扩散概率模型（DDPM）的新方法。扩散模型是一种新兴的生成模型，在实现优质图像方面比生成对抗网络（GANs）具有更好的效果。我们提出的 Phoenix 方法是一种无条件的扩散模型，利用策略改进了生成样本的数据多样性，即使是在训练统计杂质数据的情况下。

    Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity
    
[^90]: 一种学习移动解算器的新型Deeponet模型及其在地震震源定位中的应用

    A novel deeponet model for learning moving-solution operators with applications to earthquake hypocenter localization. (arXiv:2306.04096v1 [cs.LG])

    [http://arxiv.org/abs/2306.04096](http://arxiv.org/abs/2306.04096)

    本研究介绍了X-DeepONet，这是一种新型的DeepONets变体，用于学习移动解算器，并应用于实时地震定位。通过将地震到时和速度模型的信息结合起来，X-DeepONet学习估计与地震源相关的走时场，并通过根网络解决了标准DeepONet无法捕获场的重定位的问题。

    

    由人类活动引起的地震活动对公共安全构成了重大威胁，强调了准确和及时的地震震源定位的必要性。本研究介绍了X-DeepONet，这是一种新型的深度算子网络 (DeepONets) 变体，用于学习带参数的偏微分方程 (PDEs) 的移动解算器，并应用于实时地震定位。利用神经算子的力量，X-DeepONet 学习估计与地震源相关的走时场，通过将地震到时和速度模型的信息结合起来。类似于 DeepONet，X-DeepONet 包括主干网络和分支网络。此外，我们引入了一个根网络，它不仅将标准的 DeepONet 乘法算子作为输入，还将加法和减法算子作为输入。我们表明，在具有移动场的问题中，DeepONet 的标准乘法操作无法捕获场的重定位，而X-DeepONet 的根网络可以很好地解决这个问题。

    Seismicity induced by human activities poses a significant threat to public safety, emphasizing the need for accurate and timely earthquake hypocenter localization. In this study, we introduce X-DeepONet, a novel variant of deep operator networks (DeepONets), for learning moving-solution operators of parametric partial differential equations (PDEs), with application to real-time earthquake localization. Leveraging the power of neural operators, X-DeepONet learns to estimate traveltime fields associated with earthquake sources by incorporating information from seismic arrival times and velocity models. Similar to the DeepONet, X-DeepONet includes a trunk net and a branch net. Additionally, we introduce a root network that not only takes the standard DeepONet's multiplication operator as input, it also takes addition and subtraction operators. We show that for problems with moving fields, the standard multiplication operation of DeepONet is insufficient to capture field relocation, while
    
[^91]: Mixture-of-Experts 中的 Patch-level 路由对于 CNN 可以证明具有采样效率

    Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks. (arXiv:2306.04073v1 [cs.LG])

    [http://arxiv.org/abs/2306.04073](http://arxiv.org/abs/2306.04073)

    Patch-level Routing in Mixture-of-Experts 可以证明在 CNN 中具有采样效率，其用于 patch-level 路由的两个机制分别是专家的有效参数数量显著降低以及选择相关特征，从而提高深度学习神经网络的特征利用效率。

    

    在深度学习中，Mixture-of-Experts（MoE）根据每个样本或每个标记激活一个或几个专家（子网络），从而大大减少计算量。最近提出的 Patch-level routing in MoE (pMoE) 将每个输入分为 $n$ 个 patch（或 token） 并通过优先路由将 $l$ 个 patch ($l \ll n$) 发送到每个专家。pMoE 在维持测试准确性的同时，展现出在减少训练和推断成本方面的巨大实际价值。然而，pMoE 和一般的 MoE 的理论解释仍然含糊不清。在使用两层卷积神经网络（CNN）混合的监督分类任务中，我们首次展示了 pMoE 可以通过多项式阶数中 $n/l$ 的因子证明减少所需的训练样本数以实现理想的泛化（即样本复杂度），并且胜过其具有相同或甚至更大容量的单专家对应项。pMoE 的优势主要归因于两个机制：1）每个专家的有效参数数量大大降低，2）通过 Patch-level routing 选择相关特征的能力使得深度学习网络中特征空间的利用效率提高。我们的分析表明，pMoE 是一个原则性和有效的深度神经网络训练策略，对 pMoE 的理解可以潜在地推进 MoE 的理论研究。

    In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed \underline{p}atch-level routing in \underline{MoE} (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantag
    
[^92]: L2归一化技术在简单高质量OoD检测中的应用

    Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])

    [http://arxiv.org/abs/2306.04072](http://arxiv.org/abs/2306.04072)

    本篇论文介绍了在ResNet模型训练中应用L2归一化技术，可以以几乎没有成本的方式实现与最先进的OoD检测性能相媲美的结果，测试时仅需移除L2归一化即可。

    

    我们在标准的ResNet模型训练中提出了一种简单的修改方法--在特征空间中进行L2归一化--能够产生与最先进的OoD检测性能相媲美的结果。当在测试时移除L2归一化时，特征向量的L2范数成为网络不确定性的一个惊人的替代者，而当没有L2归一化训练时，这种行为却没有那么有效。直观上，熟悉的图像会产生大的向量，而陌生的图像则会产生小的向量。值得注意的是，在训练时几乎没有额外的成本，在测试时也没有成本。

    We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
    
[^93]: 通过通用鲁棒嵌入实现分类数据的可转移对抗鲁棒性

    Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings. (arXiv:2306.04064v1 [cs.LG])

    [http://arxiv.org/abs/2306.04064](http://arxiv.org/abs/2306.04064)

    本文提出一种方法，通过自编码器生成通用鲁棒嵌入，将分类数据转换为向量，从而实现对抗训练，提高分类器的鲁棒性，并在多个数据集上取得了最先进的对抗鲁棒性性能表现。

    

    很多场景下，对分类数据的鲁棒性需要更高的关注度。然而，分类数据具有类别特征，现有的优化程序无法直接解决这个问题。本文提出一种方法，利用自编码器生成通用鲁棒嵌入，将分类数据转化到一个空间中，并实现对抗训练，从而提高分类器的鲁棒性，并保持对于干净数据高分类准确度。结果表明，本文方法应用在不同的分类数据集上时，具有最先进的对抗鲁棒性能力。

    Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on tabular data. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain categorical features, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training).  In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via universal robust embeddings tailored to categorical data. These embeddings, created u
    
[^94]: RescueSpeech：用于搜救领域语音识别的德语语料库

    RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain. (arXiv:2306.04054v1 [eess.AS])

    [http://arxiv.org/abs/2306.04054](http://arxiv.org/abs/2306.04054)

    RescueSpeech是一个用于搜救领域语音识别的德语语音数据集，但目前最先进的方法仍无法令人满意。

    

    尽管语音识别在最近得到了进一步发展，但在嘈杂、回声的环境中准确转录对话和情感表达仍然存在一定难度。在搜救领域尤其如此，因为转录救援队成员之间的对话对支持实时决策至关重要。搜救场景中语音数据和相关背景噪声的稀缺性使得部署健壮的语音识别系统变得困难。为了解决这个问题，我们创建并公开了一个名为RescueSpeech的德语语音数据集。该数据集包括模拟救援演习的真实语音录音。此外，我们还发布了竞争性训练配方和预训练模型。我们的研究表明，目前最先进的方法所达到的性能水平仍远未能令人满意。

    Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems.  To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.
    
[^95]: LLMZip：使用大型语言模型的无损文本压缩

    LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v1 [cs.IT])

    [http://arxiv.org/abs/2306.04050](http://arxiv.org/abs/2306.04050)

    本研究使用大型语言模型提出了一种结合预测和无损压缩方案的英文文本压缩算法，并在初步实验中表现优于当前最先进的文本压缩方案。

    

    本文使用大型语言模型LLaMA-7B对英语熵的渐近上界提出了新估计值，并提出了一种结合大型语言模型预测与无损压缩方案的英文文本压缩算法。初步实验结果显示，我们的算法优于当前最先进的文本压缩方案，如BSC、ZPAQ和paq8h。

    We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \cite{cover1978convergent}, \cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.
    
[^96]: 从每行两个观测来看的单边矩阵完成问题

    One-sided Matrix Completion from Two Observations Per Row. (arXiv:2306.04049v1 [cs.LG])

    [http://arxiv.org/abs/2306.04049](http://arxiv.org/abs/2306.04049)

    本文研究了单边矩阵完成问题，在每行只有两个观测值的情况下，使用插值算法可以可靠恢复$X^TX$，进而恢复$X$的右奇异向量。

    

    给定一个低秩矩阵$X$的一些观测值，矩阵完成问题是推测缺失值的问题，它是形式化描述一系列需要估计缺失数据的现实世界设置。然而，当观测到的条目太少而无法完成矩阵时，可以可靠恢复基础矩阵的哪些其他方面？我们研究了一个这样的问题设置，即“单边”矩阵完成，我们的目标是恢复$X$的右奇异向量，即使在无法恢复左奇异向量的情况下，即当行数大于列数且观测的很少时。我们提出了一个自然算法，涉及到矩阵$X^TX$中缺失值的插值，并证明即使在每行只有两个观测值的情况下，只要我们有至少$\Omega(r^2 d \log d)$行，其中$r$为秩，$d$为列数，我们就可以可靠地恢复$X^TX$。我们评估了我们的算法在单边矩阵完成问题以及推荐任务上的表现，并发现它在所考虑的设置下优于现有的方法。

    Given only a few observed entries from a low-rank matrix $X$, matrix completion is the problem of imputing the missing entries, and it formalizes a wide range of real-world settings that involve estimating missing data. However, when there are too few observed entries to complete the matrix, what other aspects of the underlying matrix can be reliably recovered? We study one such problem setting, that of "one-sided" matrix completion, where our goal is to recover the right singular vectors of $X$, even in the regime where recovering the left singular vectors is impossible, which arises when there are more rows than columns and very few observations. We propose a natural algorithm that involves imputing the missing values of the matrix $X^TX$ and show that even with only two observations per row in $X$, we can provably recover $X^TX$ as long as we have at least $\Omega(r^2 d \log d)$ rows, where $r$ is the rank and $d$ is the number of columns. We evaluate our algorithm on one-sided reco
    
[^97]: 用于改进视听融合导航的主动稀疏对话

    Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])

    [http://arxiv.org/abs/2306.04047](http://arxiv.org/abs/2306.04047)

    本文提出了CAVEN - 一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕回答以协助自主导航。该系统基于多模态分层强化学习方法，并使用三个低级策略进行引导。

    

    为了高效地导航到一个听觉目标，一个具有固定自主权的实体必须不仅要有能力有效地使用视听线索, 而且还要有能力在不牺牲自主性的情况下主动寻求人类/神谕的帮助，例如，当不确定导航到哪里寻找嘈杂或间歇性听觉目标时。因此，我们提出了CAVEN-一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕的自由形式自然语言回答。在CAVEN的核心是一个多模态分层强化学习(RL)设置，它配备了一个高级策略，该策略经过训练，可以在每一步从三个低级策略中选择一个，即：(i)使用视听线索进行导航，或(ii)向神谕提出问题并接收短或详细的回答，或(iii)提问普遍问题(当不确定该问什么时)并获得指导

    Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
    
[^98]: FedVal：联邦学习中的不同好坏

    FedVal: Different good or different bad in federated learning. (arXiv:2306.04040v1 [cs.LG])

    [http://arxiv.org/abs/2306.04040](http://arxiv.org/abs/2306.04040)

    本文提出了FedVal方法，它是一个不需要从客户端获取任何附加信息的全新方法，可同时具有稳健和公平性，并通过评分函数在服务器端验证客户端更新，以确定本地训练模型之间的最佳聚合平衡。

    

    联邦学习系统容易受到恶意攻击的影响，攻击者可能会通过各种毒化攻击来破坏训练模型。此外，FL在解决团体偏见方面也面临新的挑战，例如确保不同人口群体的公平性能。传统方法需要对数据进行集中处理，而FL系统并没有这个功能。本文提出了一个名为FedVal的全新方法，既具有稳健性又具有公平性，其不需要从客户端获取任何可能引发隐私问题并危及FL系统完整性的附加信息。为此，我们提出了一个基于服务器端验证方法的创新评分函数，该方法评估客户端更新并确定本地训练模型之间的最佳聚合平衡。我们的研究表明，这种方法不仅能够有效保护模型免受毒化攻击，而且还可用于减少群体偏见和随后的问题。

    Federated learning (FL) systems are susceptible to attacks from malicious actors who might attempt to corrupt the training model through various poisoning attacks. FL also poses new challenges in addressing group bias, such as ensuring fair performance for different demographic groups. Traditional methods used to address such biases require centralized access to the data, which FL systems do not have. In this paper, we present a novel approach FedVal for both robustness and fairness that does not require any additional information from clients that could raise privacy concerns and consequently compromise the integrity of the FL system. To this end, we propose an innovative score function based on a server-side validation method that assesses client updates and determines the optimal aggregation balance between locally-trained models. Our research shows that this approach not only provides solid protection against poisoning attacks but can also be used to reduce group bias and subsequen
    
[^99]: 重新思考加速器上的神经检索

    Revisiting Neural Retrieval on Accelerators. (arXiv:2306.04039v1 [cs.LG])

    [http://arxiv.org/abs/2306.04039](http://arxiv.org/abs/2306.04039)

    本文提出了一种名为混合对数模型（MoL）的非点积检索方法。该方法通过自适应地组合基本相似度函数来建模用户与文本之间的相似度，以更好地捕捉高阶用户-文本交互作用并进一步推广到长尾数据中。结合分层检索策略（h-indexer），本文成功将MoL扩展到单个GPU上的100M个文本语料库。

    

    信息检索需要从大量文本中找出与用户需求相关的文本。其中，建模用户与文本（item）的相似度是信息检索的关键。常见的做法是将用户与文本的相似度表示为两个学习嵌入的点积，又被称为最大内积检索（MIPS）。虽然这种方法具有效率，但是它无法捕捉复杂的用户-文本交互作用。本文提出了一种基于加速器的非点积检索方法：混合对数模型（Mixture of Logits，MoL）。该方法通过自适应地组合基本相似度函数来建模用户与文本之间的相似度，以更好地捕捉高阶用户-文本交互作用并进一步推广到长尾数据中。本文还结合一种分层检索策略（h-indexer）将MoL扩展到单个GPU上的100M个文本语料库。

    Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency
    
[^100]: 遥感图像分类的可解释人工智能方法的主要贡献的定量分析

    Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v1 [cs.LG])

    [http://arxiv.org/abs/2306.04037](http://arxiv.org/abs/2306.04037)

    该论文定量分析了用于遥感图像分类的可解释人工智能技术，探究了不同属性的XAI方法，提供选取合适方法以深入了解模型决策的见解和建议。

    

    我们提出了一种综合分析定量评估可解释人工智能（XAI）技术用于遥感图像分类的方法。我们的方法利用最先进的机器学习方法在多种模态下执行遥感图像分类。我们通过XAI方法定性地研究了模型的结果。此外，我们通过所需属性的各种类别来定量比较XAI方法。通过我们的分析，我们提供了选择最合适的XAI方法以加深对模型决策过程理解的见解和建议。此工作的代码是公开可用的。

    We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.
    
[^101]: BokehOrNot：利用图像变换器和镜头元数据嵌入转换Bokeh效果。

    BokehOrNot: Transforming Bokeh Effect with Image Transformer and Lens Metadata Embedding. (arXiv:2306.04032v1 [cs.CV])

    [http://arxiv.org/abs/2306.04032](http://arxiv.org/abs/2306.04032)

    本文提出了一种利用镜头元数据嵌入模型的新方法，将Bokeh效果从模糊到清晰和清晰到模糊进行转换，表现出自然的效果，并优于当前领先的Bokeh效果渲染和图像恢复模型。

    

    Bokeh效果是高端相机与广角镜头所产生的一种视觉效应，提供了愉悦的视觉体验。Bokeh效果的转换任务旨在根据另一组镜头与光圈的组合来产生期望的效果。目前的模型在渲染特定的Bokeh效果方面存在局限性，主要是从清晰到模糊的转换。本文提出了一种新的通用方法，将镜头元数据嵌入模型，并使用新发布的Bokeh效果转换数据集（BETD）[3]中的α掩模计算损失。基于上述技术，我们提出了BokehOrNot模型，它能够产生具有不同镜头和光圈尺寸组合的模糊到清晰和清晰到模糊的Bokeh效果。我们提出的模型优于当前领先的Bokeh渲染和图像恢复模型，并呈现出自然的Bokeh效果。我们的代码可用。

    Bokeh effect is an optical phenomenon that offers a pleasant visual experience, typically generated by high-end cameras with wide aperture lenses. The task of bokeh effect transformation aims to produce a desired effect in one set of lenses and apertures based on another combination. Current models are limited in their ability to render a specific set of bokeh effects, primarily transformations from sharp to blur. In this paper, we propose a novel universal method for embedding lens metadata into the model and introducing a loss calculation method using alpha masks from the newly released Bokeh Effect Transformation Dataset(BETD) [3]. Based on the above techniques, we propose the BokehOrNot model, which is capable of producing both blur-to-sharp and sharp-to-blur bokeh effect with various combinations of lenses and aperture sizes. Our proposed model outperforms current leading bokeh rendering and image restoration models and renders visually natural bokeh effects. Our code is available
    
[^102]: 因子图模型视角下的干预泛化

    Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])

    [http://arxiv.org/abs/2306.04027](http://arxiv.org/abs/2306.04027)

    本文提出了一种基于因子图模型的“干预因子模型”(IFM)方法，仅基于对操纵系统分布的因子分解的最小假设，以实现从过去的实验到新的条件的跃迁。

    

    因果推断的一个目标是从过去的实验和观察数据推广到新的条件。在训练数据中提供足够多的实验的情况下，理论上可能最终学习从新的实验条件到感兴趣的结果的映射，但是处理大量可能的干预组合空间很困难。在典型的稀疏实验设计下，如果不依赖于重的规则化或先验分布，这种映射是不适当的。这样的假设可能是可靠的，也可能是不可靠的，很难辩护或测试。本文从因子图模型的语言角度深入探讨如何保证从过去的实验到新的条件的跃迁，仅基于对操纵系统分布的因子分解的最小假设。假设的“干预因子模型”可能并不总是有用的，但是它很方便地处理了大量可能的干预空间。

    One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
    
[^103]: 价值函数即控制障碍函数：使用控制理论验证学习策略

    Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])

    [http://arxiv.org/abs/2306.04026](http://arxiv.org/abs/2306.04026)

    本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    

    尽管强化学习具有高度的通用性和可伸缩性，但验证策略行为的难度对于安全关键应用程序构成了挑战。为了解决这个问题，我们建议将控制理论中使用的验证方法应用于学习的价值函数。通过分析安全维护的简单任务结构，我们推导出将值函数与控制障碍函数相联系的原始定理。受此启发，我们提出了新的度量方法，以验证安全控制任务中的价值函数，并提出了改善学习的实际实施细节。除了提出证书学习的新方法外，我们的工作为RL策略解锁了丰富的控制理论验证方法，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
    
[^104]: 利用能量模型进行跨模态定位的卷积变换方法

    Energy-Based Models for Cross-Modal Localization using Convolutional Transformers. (arXiv:2306.04021v1 [cs.CV])

    [http://arxiv.org/abs/2306.04021](http://arxiv.org/abs/2306.04021)

    本文提出了一种基于能量模型和卷积变换的跨模态定位方法，利用卫星图像进行地图构建，可在没有GPS的情况下实现精确的度量级别定位，并在KITTI数据集上取得了更高的定位精度。

    

    本文提出了一种使用基于能量模型（EBMs）的新型框架，用于在没有GPS的情况下定位一个搭载有测距传感器的地面车辆相对于卫星图像的位置。本方法利用卫星图像进行地图构建，这种地图是广泛可用和易于获取的，而且具有全面的覆盖面。我们提出了一种使用卷积变换的方法，在跨模态的情况下实现了精确的度量级别定位，这是由于稀疏的测距传感器读数和丰富的卫星图像之间外观差异的极度之大而具有挑战性。我们的模型是端到端训练的，并且在KITTI数据集上实验表明，相比最先进的方法，本文的方法可以实现更高的定位精度。

    We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of satellite images as map priors, which are widely available, easily accessible, and provide comprehensive coverage. We propose a method using convolutional transformers that performs accurate metric-level localization in a cross-modal manner, which is challenging due to the drastic difference in appearance between the sparse range sensor readings and the rich satellite imagery. We train our model end-to-end and demonstrate our approach achieving higher accuracy than the state-of-the-art on KITTI,
    
[^105]: 绿色学习方法在图像隐写分析中的应用: Green Steganalyzer

    Green Steganalyzer: A Green Learning Approach to Image Steganalysis. (arXiv:2306.04008v1 [eess.IV])

    [http://arxiv.org/abs/2306.04008](http://arxiv.org/abs/2306.04008)

    Green Steganalyzer是一种用于图像隐写分析的基于绿色学习范式的新方法，能够通过像素异常预测、嵌入位置检测和决策融合来对图像进行识别，与现有深度学习模型相比，具有更低的计算复杂度和更小的模型大小。

    

    本文提出了一种基于绿色学习范式的图像隐写分析新方法，称为Green Steganalyzer（GS）。GS由三个模块组成：1）基于像素的异常预测，2）嵌入位置检测，3）图像级别检测的决策融合。通过对图像进行补丁分解、采用Saab变换进行特征提取和进行自监督学习来预测其中心像素的异常得分，GS在第一个模块中进行处理。在第二个模块中，GS分析像素和邻域的异常得分，以找到更高嵌入概率的像素。在第三个模块中，GS聚焦于更高嵌入概率的像素，并将它们的异常得分融合起来进行最终的图像级别分类。与现有的深度学习模型相比，GS在相同检测性能下，计算复杂度更低，模型大小更小，并能对S-UNIWARD、WOW和HILL隐写方案获得可比的检测性能。

    A novel learning solution to image steganalysis based on the green learning paradigm, called Green Steganalyzer (GS), is proposed in this work. GS consists of three modules: 1) pixel-based anomaly prediction, 2) embedding location detection, and 3) decision fusion for image-level detection. In the first module, GS decomposes an image into patches, adopts Saab transforms for feature extraction, and conducts self-supervised learning to predict an anomaly score of their center pixel. In the second module, GS analyzes the anomaly scores of a pixel and its neighborhood to find pixels of higher embedding probabilities. In the third module, GS focuses on pixels of higher embedding probabilities and fuses their anomaly scores to make final image-level classification. Compared with state-of-the-art deep-learning models, GS achieves comparable detection performance against S-UNIWARD, WOW and HILL steganography schemes with significantly lower computational complexity and a smaller model size, ma
    
[^106]: 面向图形对比学习的随机舒尔补视图

    Randomized Schur Complement Views for Graph Contrastive Learning. (arXiv:2306.04004v1 [cs.LG])

    [http://arxiv.org/abs/2306.04004](http://arxiv.org/abs/2306.04004)

    该论文介绍了一种基于舒尔补的随机拓扑增强器，用于图形对比学习。相比以往的方法，该技术通过生成增强视图来提高网络性能，并在节点和图分类基准上实现了最先进的结果。

    

    我们介绍了一种基于舒尔补的随机拓扑增强器，用于图形对比学习（GCL）。给定图拉普拉斯矩阵，该技术生成其随机舒尔补的无偏估计，并将相应的图形视为增强视图。我们讨论了我们方法的优点，提供了理论说明，并展示了与图扩散的联系。与以往的努力不同，我们通过改变后续GCL阶段（如编码和对比）的设计选择来有计划地研究增强器的实证有效性。对节点和图分类基准的广泛实验表明，我们的技术始终优于预定义和自适应增强方法，以实现最先进的结果。

    We introduce a randomized topological augmentor based on Schur complements for Graph Contrastive Learning (GCL). Given a graph laplacian matrix, the technique generates unbiased approximations of its Schur complements and treats the corresponding graphs as augmented views. We discuss the benefits of our approach, provide theoretical justifications and present connections with graph diffusion. Unlike previous efforts, we study the empirical effectiveness of the augmentor in a controlled fashion by varying the design choices for subsequent GCL phases, such as encoding and contrasting. Extensive experiments on node and graph classification benchmarks demonstrate that our technique consistently outperforms pre-defined and adaptive augmentation approaches to achieve state-of-the-art results.
    
[^107]: 用一维深度图像先验进行电磁求解器S参数曲线拟合

    One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from Electromagnetic Solvers. (arXiv:2306.04001v1 [cs.LG])

    [http://arxiv.org/abs/2306.04001](http://arxiv.org/abs/2306.04001)

    该论文提出了一种使用深度生成模型为基础的方法，使用一维深度图像先验拟合电磁求解器的S参数。通过与公开可用和专有的行业标准拟合方法的比较，实验结果表明该方法在重建质量和计算时间方面均有显着改进。

    

    在集成电路封装中建模信号完整性的一个关键问题是需要在所需频带内进行多个S参数测量以获得足够的分辨率。使用电磁场求解器获得这些样本通常是计算昂贵的。因此，常见方法是选择所需样本的一小部分，并使用适当的拟合机制重新创建密集采样的宽带表示。我们提出了一种基于深度生成模型的方法来使用一维深度图像先验（DIP）拟合来自EM求解器的S参数。DIP是一种技术，它优化随机初始化的卷积神经网络的权重以适应从噪声或欠定测量中的信号。我们设计了一个自定义架构，并提出了一种新颖的正则化技术，灵感来自平滑样条，以惩罚不连续的跳跃。我们在两种类型的无源滤波器上实验比较了DIP与公开可用的和专有的行业标准拟合方法，并展示了重建质量和计算时间的显着改进。

    A key problem when modeling signal integrity for passive filters and interconnects in IC packages is the need for multiple S-parameter measurements within a desired frequency band to obtain adequate resolution. These samples are often computationally expensive to obtain using electromagnetic (EM) field solvers. Therefore, a common approach is to select a small subset of the necessary samples and use an appropriate fitting mechanism to recreate a densely-sampled broadband representation. We present the first deep generative model-based approach to fit S-parameters from EM solvers using one-dimensional Deep Image Prior (DIP). DIP is a technique that optimizes the weights of a randomly-initialized convolutional neural network to fit a signal from noisy or under-determined measurements. We design a custom architecture and propose a novel regularization inspired by smoothing splines that penalizes discontinuous jumps. We experimentally compare DIP to publicly available and proprietary indus
    
[^108]: 实时在线无监督领域自适应用于现实世界人员再识别

    Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification. (arXiv:2306.03993v1 [cs.CV])

    [http://arxiv.org/abs/2306.03993](http://arxiv.org/abs/2306.03993)

    本文提出了一个新的实时在线无监督领域自适应设置（R$^2$OUDA），并引入了一个新颖的多摄像头系统R$^2$MMT。通过R$^2$OUDA，本文解决了现实应用中被忽略的四个主要限制，以实现真正的实时在线无监督领域自适应。

    

    随着无监督领域自适应（UDA）在人员再识别中的流行，最近提出的在线无监督领域自适应（OUDA）尝试通过引入数据流的考虑来弥合实际应用的差距。然而，这仍然无法真正代表真实的现实应用。本文定义了现实世界实时在线无监督领域自适应（R$^2$OUDA）。R$^2$OUDA设置了真正的现实世界实时OUDA的舞台，揭示了当前研究中经常被忽略的四个主要限制：系统生成的人员图像、子集分布选择、基于时间的数据流分割和基于分段的时间约束。为了解决这个新的R$^2$OUDA设置的所有方面，本文进一步提出了现实世界实时在线流式互相平均教学（R$^2$MMT），这是一种新颖的多摄像头系统。

    Following the popularity of Unsupervised Domain Adaptation (UDA) in person re-identification, the recently proposed setting of Online Unsupervised Domain Adaptation (OUDA) attempts to bridge the gap towards practical applications by introducing a consideration of streaming data. However, this still falls short of truly representing real-world applications. This paper defines the setting of Real-world Real-time Online Unsupervised Domain Adaptation (R$^2$OUDA) for Person Re-identification. The R$^2$OUDA setting sets the stage for true real-world real-time OUDA, bringing to light four major limitations found in real-world applications that are often neglected in current research: system generated person images, subset distribution selection, time-based data stream segmentation, and a segment-based time constraint. To address all aspects of this new R$^2$OUDA setting, this paper further proposes Real-World Real-Time Online Streaming Mutual Mean-Teaching (R$^2$MMT), a novel multi-camera sy
    
[^109]: 机器代理在良性和恶性情境下的自主股票交易

    Agent Performing Autonomous Stock Trading under Good and Bad Situations. (arXiv:2306.03985v1 [cs.LG])

    [http://arxiv.org/abs/2306.03985](http://arxiv.org/abs/2306.03985)

    本文介绍了使用深度强化学习方法训练机器代理来自主执行股票交易的研究，并在不同市场环境下进行了评估。

    

    股票交易是财务管理的一种流行方式。然而，市场和经济环境不稳定，通常不能预测。此外，从事股票交易需要时间和精力来分析、制定策略和做出决策。如果一个机器代理能够辅助甚至执行分析和建模过去数据，然后生成自主交易策略，那将是方便和有效的。近年来，强化学习已被证明在涉及基于时间序列数据的决策制定策略达到目标的各种任务中具有鲁棒性。在本项目中，我们开发了一个模拟股票交易环境的管道，并使用深度强化学习方法，包括深度Q学习、深度SARSA和策略梯度方法，训练了一个机器代理来自动化股票交易过程。我们在相对良好（2021年之前）和恶劣（2021年-2022年）情况下评估了我们的平台。

    Stock trading is one of the popular ways for financial management. However, the market and the environment of economy is unstable and usually not predictable. Furthermore, engaging in stock trading requires time and effort to analyze, create strategies, and make decisions. It would be convenient and effective if an agent could assist or even do the task of analyzing and modeling the past data and then generate a strategy for autonomous trading. Recently, reinforcement learning has been shown to be robust in various tasks that involve achieving a goal with a decision making strategy based on time-series data. In this project, we have developed a pipeline that simulates the stock trading environment and have trained an agent to automate the stock trading process with deep reinforcement learning methods, including deep Q-learning, deep SARSA, and the policy gradient method. We evaluate our platform during relatively good (before 2021) and bad (2021 - 2022) situations. The stocks we've eva
    
[^110]: 面向任务型对话的更准确和可推广的评估指标探索

    Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v1 [cs.CL])

    [http://arxiv.org/abs/2306.03984](http://arxiv.org/abs/2306.03984)

    本文提出了一种新的对话质量注释工作流程称为DQA，能够更准确和可推广地评估对话质量，尤其是通过一些客观对话属性的判断。

    

    评估交互质量对于改进口语对话系统至关重要。现有的对话质量估计方法要么侧重于评估单个对话轮次的质量，要么从终端用户立即在交互之后收集对话级别的质量测量数据。与这些方法相比，我们引入了一种新的对话级别注释工作流程称为对话质量注释（DQA）。DQA专家注释员评估整个对话的质量，并标记对话的目标完成和用户情感等属性。本文的贡献是，我们展示了：（i）尽管对话质量不能完全分解成对话级别属性，但某些客观对话属性与对话质量的判断之间存在着强关系；（ii）对于对话级别质量估计任务，一个在对话级别注释上训练的监督模型优于仅基于聚合轮次级别特征的方法；以及（iii）使用DQA相比现有方法能够得到更准确和可推广的对话质量评估。

    Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the 
    
[^111]: 全球可测和可逆神经运算符

    Globally injective and bijective neural operators. (arXiv:2306.03982v1 [cs.LG])

    [http://arxiv.org/abs/2306.03982](http://arxiv.org/abs/2306.03982)

    这篇论文研究了网络学习的运算符是否是单射和满射的情况，并给出了精确条件。它们提供的单射神经运算符是通用逼近器，并且使用有限秩神经网络实现它们，使得网络仍然单射。

    

    最近，在运算学习领域，网络从基本上无限维度的视角学习函数空间之间的运算符，我们针对网络学习的运算符是单射和满射的情况进行了研究。

    Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank case, is subtler in the infinite-rank case and is proved using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not `lost' 
    
[^112]: 使用表达式布尔公式的可解释人工智能

    Explainable AI using expressive Boolean formulas. (arXiv:2306.03976v1 [cs.AI])

    [http://arxiv.org/abs/2306.03976](http://arxiv.org/abs/2306.03976)

    提出了一种表达式布尔公式的可解释人工智能，利用本地优化技术训练分类模型，可以应用于信用评分和医疗病况的诊断，并具有未来应用的潜力。

    

    我们提出并实现了一个可解释的机器学习分类模型，基于表达式布尔公式的可解释人工智能（XAI）。潜在的应用包括信用评分和医疗病况的诊断。布尔公式定义了一个规则，根据该规则将输入数据分类为可调整复杂度（或可解释性）。这样的公式可以包含应用于一个或多个布尔变量的任何运算符，因此具有比更严格的基于规则和基于树的方法更高的表达性。分类器使用本地优化技术进行训练，有效地搜索可行公式的空间。浅的规则可以通过快速整数线性规划（ILP）或二次无约束二进制优化（QUBO）求解器确定，可能由专用硬件或量子设备提供动力。我们通过执行本地优化器的表达能力和效率与这些设备的快速操作相结合，为未来的AI研究提供了新思路。

    We propose and implement an interpretable machine learning classification model for Explainable AI (XAI) based on expressive Boolean formulas. Potential applications include credit scoring and diagnosis of medical conditions. The Boolean formula defines a rule with tunable complexity (or interpretability), according to which input data are classified. Such a formula can include any operator that can be applied to one or more Boolean variables, thus providing higher expressivity compared to more rigid rule-based and tree-based approaches. The classifier is trained using native local optimization techniques, efficiently searching the space of feasible formulas. Shallow rules can be determined by fast Integer Linear Programming (ILP) or Quadratic Unconstrained Binary Optimization (QUBO) solvers, potentially powered by special purpose hardware or quantum devices. We combine the expressivity and efficiency of the native local optimizer with the fast operation of these devices by executing n
    
[^113]: 使用神经切向核的随机边际似然梯度。

    Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels. (arXiv:2306.03968v1 [stat.ML])

    [http://arxiv.org/abs/2306.03968](http://arxiv.org/abs/2306.03968)

    本文提出了使用神经切向核的随机边际似然梯度，可以加速基于梯度的超参数优化过程。

    

    选择深度学习中的超参数对其有效性有重大影响，但需要人工努力和专业知识。最近的研究表明，使用拉普拉斯近似的贝叶斯模型选择能够像使用梯度优化标准神经网络参数一样优化这些超参数，并使用训练数据进行训练。然而，估计单个超参数梯度需要通过整个数据集，限制了这些算法的可伸缩性。在这项研究中，我们通过引入线性化拉普拉斯逼近的下限来克服这个问题。与以前的估计器不同，这些下限适用于基于随机梯度的优化，并允许在估计精度和计算复杂性之间进行权衡。我们使用线性化拉普拉斯的函数空间形式导出了它们，这可以使用神经切向核进行估计。在实验中，我们展示了这些估计器可以显著加速基于梯度的超参数优化过程。

    Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires manual effort and expertise. Recent works show that Bayesian model selection with Laplace approximations can allow to optimize such hyperparameters just like standard neural network parameters using gradients and on the training data. However, estimating a single hyperparameter gradient requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approximation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against computational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estimators can significantly accelerate gradient-based hyp
    
[^114]: PILLAR：如何使半私有学习更有效

    PILLAR: How to make semi-private learning more effective. (arXiv:2306.03962v1 [cs.LG])

    [http://arxiv.org/abs/2306.03962](http://arxiv.org/abs/2306.03962)

    本文提出了一种计算效率高的算法 PILLAR，可以在半监督半私有（SP）学习中明显降低私有标记样本复杂度，并可以在实际数据集上高效运行，可以利用在公共数据上预训练的网络提取的特征，并在实验证明了其显著有效性。

    

    在半监督半私有（SP）学习中，学习者可以访问公共的未标记数据和私有的标记数据。我们提出了一种计算效率高的算法，假设数据符合一定条件，可以明显降低私有标记样本复杂度，并可以在实际数据集上高效运行。为此，我们利用在公共数据（标记或未标记）上预训练的网络提取的特征，这些特征的分布可能与进行SP学习的分布显著不同。为了验证其实证有效性，我们提出了多种在严格的隐私约束（\(\epsilon=0.1\))和低数据量情况下的实验。在所有这些设置中，我们的算法表现出显著优于使用类似数量的公共数据的现有基线的性能。

    In Semi-Supervised Semi-Private (SP) learning, the learner has access to both public unlabelled and private labelled data. We propose a computationally efficient algorithm that, under mild assumptions on the data, provably achieves significantly lower private labelled sample complexity and can be efficiently run on real-world datasets. For this purpose, we leverage the features extracted by networks pre-trained on public (labelled or unlabelled) data, whose distribution can significantly differ from the one on which SP learning is performed. To validate its empirical effectiveness, we propose a wide variety of experiments under tight privacy constraints (\(\epsilon=0.1\)) and with a focus on low-data regimes. In all of these settings, our algorithm exhibits significantly improved performance over available baselines that use similar amounts of public data.
    
[^115]: 使用卷积神经网络集合识别手写日语字符

    Recognition of Handwritten Japanese Characters Using Ensemble of Convolutional Neural Networks. (arXiv:2306.03954v1 [cs.CV])

    [http://arxiv.org/abs/2306.03954](http://arxiv.org/abs/2306.03954)

    该研究提出了一种机器学习方法，即使用卷积神经网络集合来高效地识别手写汉字，能够实现高达99.4%的分类准确率。

    

    日本书写系统复杂，包括平假名、片假名和汉字三种字符类型。汉字包括数千个不同的字符，进一步增加了对字符的识别和理解难度。将手写日语字符转换为数字文本对于数据分析、翻译、学习和文化保护都很有用。本研究提出了一种机器学习方法来分析和识别手写日语字符（汉字）。研究使用了三个卷积神经网络（CNN）集合来识别手写日语汉字，并利用 MNIST、K-MNIST、Kuzushiji-49（K49）以及 Kuzushiji-Kanji（K-Kanji）中前150个重要类别的四个数据集进行性能评估。结果表明，使用提出的CNN-ensemble架构识别手写字符具有可行性，在MNIST上实现了99.4％、96.4％、95.0％和96.4％的分类准确率。

    The Japanese writing system is complex, with three character types of Hiragana, Katakana, and Kanji. Kanji consists of thousands of unique characters, further adding to the complexity of character identification and literature understanding. Being able to translate handwritten Japanese characters into digital text is useful for data analysis, translation, learning and cultural preservation. In this study, a machine learning approach to analyzing and recognizing handwritten Japanese characters (Kanji) is proposed. The study used an ensemble of three convolutional neural networks (CNNs) for recognizing handwritten Kanji characters and utilized four datasets of MNIST, K-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the Kuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results indicate feasibility of using proposed CNN-ensemble architecture for recognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4% classification accuracy on MNIST
    
[^116]: 基于强化学习的CrazyFlie 2.X四轴飞行器控制

    Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor. (arXiv:2306.03951v1 [cs.RO])

    [http://arxiv.org/abs/2306.03951](http://arxiv.org/abs/2306.03951)

    该论文介绍了如何使用强化学习算法来实现CrazyFlie 2.X四轴飞行器的控制和导航，同时结合PID控制和灯塔定位系统，以实现更加精确的控制。

    

    本项目旨在探索经典控制算法（如PID）和现代强化学习算法之间的协同作用，以开发一种实用的控制机制来控制CrazyFlie 2.X四轴飞行器。主要目标是使用强化学习策略来执行PID调整，次要目标是利用第一个任务的经验，通过与灯塔定位系统集成实现导航控制。在导航方面考虑了两种方法：一种是使用预定义的有限运动基元进行深度Q学习的离散导航问题，另一种是使用深度强化学习进行连续导航。 RL训练的模拟将在gym-pybullet-drones上进行，该平台是一种基于强化学习的开源gym环境，并使用stable-baselines3提供RL实施。

    The objective of the project is to explore synergies between classical control algorithms such as PID and contemporary reinforcement learning algorithms to come up with a pragmatic control mechanism to control the CrazyFlie 2.X quadrotor. The primary objective would be performing PID tuning using reinforcement learning strategies. The secondary objective is to leverage the learnings from the first task to implement control for navigation by integrating with the lighthouse positioning system. Two approaches are considered for navigation, a discrete navigation problem using Deep Q-Learning with finite predefined motion primitives, and deep reinforcement learning for a continuous navigation approach. Simulations for RL training will be performed on gym-pybullet-drones, an open-source gym-based environment for reinforcement learning, and the RL implementations are provided by stable-baselines3
    
[^117]: 结构化预测中的部分推断

    Partial Inference in Structured Prediction. (arXiv:2306.03949v1 [cs.LG])

    [http://arxiv.org/abs/2306.03949](http://arxiv.org/abs/2306.03949)

    本文研究了结构化预测中的问题，通过生成模型和凸优化算法，提出了可证明保证的部分标签恢复方法。

    

    本文探讨了在结构化预测中部分推断的问题。使用生成模型方法，研究在标签图上最大化一种包含一元和二元因子的评分函数的任务。通过采用两阶段凸优化算法进行标签恢复，分析了大多数标签可恢复的条件。提出了一种新的Karush-Kuhn-Tucker（KKT）条件和原始对偶构造的观点，并提供了具有可证明保证的部分恢复的统计和拓扑要求。

    In this paper, we examine the problem of partial inference in the context of structured prediction. Using a generative model approach, we consider the task of maximizing a score function with unary and pairwise potentials in the space of labels on graphs. Employing a two-stage convex optimization algorithm for label recovery, we analyze the conditions under which a majority of the labels can be recovered. We introduce a novel perspective on the Karush-Kuhn-Tucker (KKT) conditions and primal and dual construction, and provide statistical and topological requirements for partial recovery with provable guarantees.
    
[^118]: COVID-19对研究产出传播的科学计量分析

    A scientometric analysis of the effect of COVID-19 on the spread of research outputs. (arXiv:2306.03941v1 [cs.DL])

    [http://arxiv.org/abs/2306.03941](http://arxiv.org/abs/2306.03941)

    本文通过科学计量分析COVID-19相关研究在全球和意大利的产量，发现美国和中国的研究活动最为活跃，在医学生物学领域的文献产出增长最快，并探讨了出版物数量和死亡人数之间的关系。

    

    2020年Sars-COV-2大流行的蔓延对我们所有人的生活轨迹产生了巨大影响。这种快速传播也导致涉及COVID-19的不同方面主题的研究产量增加。不幸的是，意大利是最早大规模卷入该病暴发的国家之一。在本文中，我们提出了一种广泛科学计量分析研究产量的方法，包括全球范围（疫情开始后的前两年内产生的全部文献）和本地层面（意大利附属机构作者撰写的COVID-19文献）。我们的结果表明，美国和中国是发表数量最多的研究活跃国家，并且机构之间合作的数量根据地理距离而异。此外，我们确定了医学生物学领域在文献产出方面增长最快。此外，我们还进一步探讨了出版物数量与死亡人数之间的关系。

    The spread of the Sars-COV-2 pandemic in 2020 had a huge impact on the life course of all of us. This rapid spread has also caused an increase in the research production in topics related to COVID-19 with regard to different aspects. Italy has, unfortunately, been one of the first countries to be massively involved in the outbreak of the disease. In this paper we present an extensive scientometric analysis of the research production both at global (entire literature produced in the first 2 years after the beginning of the pandemic) and local level (COVID-19 literature produced by authors with an Italian affiliation). Our results showed that US and China are the most active countries in terms of number of publications and that the number of collaborations between institutions varies according to geographical distance. Moreover, we identified the medical-biological as the fields with the greatest growth in terms of literature production. Furthermore, we also better explored the relations
    
[^119]: 通过正交神经网络学习因果机制

    Learning Causal Mechanisms through Orthogonal Neural Networks. (arXiv:2306.03938v1 [cs.LG])

    [http://arxiv.org/abs/2306.03938](http://arxiv.org/abs/2306.03938)

    本文介绍了一种无监督的方法来学习一组独立机制的反向操作，以发现和分离一组独立的机制。

    

    人类智能的重要特征之一是能够从低级感官数据中推导出高级抽象。这种推理的一个重要组成部分是发现模块化的生成机制。本文探讨了一个问题，即如何在完全无监督的情况下从失真的数据点中学习一组独立机制的反向操作。我们提出并通过实验结果证明，现有机器学习解决方案的一个重要弱点在于跨模块多样化不足。解决人工智能与机器智能之间的这一重要差距是模式识别系统的重要挑战。为此，本文提出了一种无监督方法，可以发现和分离一组独立的机制。

    A fundamental feature of human intelligence is the ability to infer high-level abstractions from low-level sensory data. An essential component of such inference is the ability to discover modularized generative mechanisms. Despite many efforts to use statistical learning and pattern recognition for finding disentangled factors, arguably human intelligence remains unmatched in this area.  In this paper, we investigate a problem of learning, in a fully unsupervised manner, the inverse of a set of independent mechanisms from distorted data points. We postulate, and justify this claim with experimental results, that an important weakness of existing machine learning solutions lies in the insufficiency of cross-module diversification. Addressing this crucial discrepancy between human and machine intelligence is an important challenge for pattern recognition systems.  To this end, our work proposes an unsupervised method that discovers and disentangles a set of independent mechanisms from u
    
[^120]: 在联邦学习中使用预训练模型引导最后一层

    Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])

    [http://arxiv.org/abs/2306.03937](http://arxiv.org/abs/2306.03937)

    本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。

    

    联邦学习(Fl)是一种新兴的范式，允许在不共享数据的情况下跨多个参与者训练模型。最近的一些工作开始考虑使用预训练模型作为现有FL算法的初始化点的影响; 但是，这些方法忽略了集中式学习设置中大量有效的迁移学习文献。在这里，我们重新审视了先前工作中考虑预训练模型的FL问题，并将其扩展到一组计算机视觉迁移学习问题。我们首先观察到，在许多情况下，仅拟合线性分类头是高效且有效的。然后，我们展示了在FL设置中，使用最近类均值(NCM)拟合分类器可以精确地且比现有提议高效地完成，同时获得了强大的性能。最后，我们证明了使用两阶段方法获得分类器，然后微调模型可以产生更好的结果。

    Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
    
[^121]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^122]: 使用反事实预测集设计决策支持系统

    Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])

    [http://arxiv.org/abs/2306.03928](http://arxiv.org/abs/2306.03928)

    本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。

    

    分类任务的决策支持系统通常被设计用于预测地面实况标签的值。然而，由于它们的预测并不完美，这些系统还需要让人类专家了解何时以及如何使用这些预测来更新自己的预测。不幸的是，这被证明是具有挑战性的。最近有人认为，另一种类型的决策支持系统可能会避开这个挑战。这些系统不是提供单个标签预测，而是使用符合预测器构建一组标签预测值，即预测集，并强制要求专家从预测集中预测一个标签值。然而，这些系统的设计和评估迄今仍依赖于样式化的专家模型，这引发了人们对它们的承诺的质疑。本文从在线学习的角度重新审视了这种系统的设计，并开发了一种不需要。

    Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
    
[^123]: 多重约束对称非负潜在因子分析以准确表示大规模无向加权网络

    Multi-constrained Symmetric Nonnegative Latent Factor Analysis for Accurately Representing Large-scale Undirected Weighted Networks. (arXiv:2306.03911v1 [cs.LG])

    [http://arxiv.org/abs/2306.03911](http://arxiv.org/abs/2306.03911)

    本研究提出一种多重约束的对称非负潜在因子分析模型，以准确地表示大规模无向加权网络，并克服了现有模型计算复杂度高和建模策略狭窄的缺点。

    

    无向加权网络在大数据应用中经常出现，涉及众多节点之间的复杂交互，例如生物信息学应用中的蛋白质相互作用网络。对称高维不完整矩阵可以平滑地描述这样的无向加权网络，其中包含节点交互行为和局部复杂性等丰富的信息。为了从对称高维不完整矩阵中提取所需的知识，分析模型应该仔细考虑其对称拓扑结构，以描述无向加权网络的固有对称性。将无向加权网络表示为潜在因子矩阵的方法，借鉴了对称感知模型金字塔的成功，例如利用唯一的潜在因子矩阵来严格表示SHDI对称性的对称非负矩阵分解模型。但是，它们存在以下缺点：1）计算复杂度高；2）其建模策略狭窄其表示特征，使其不适用于真实世界中更复杂和多样化的UWN数据集。

    An Undirected Weighted Network (UWN) is frequently encountered in a big-data-related application concerning the complex interactions among numerous nodes, e.g., a protein interaction network from a bioinformatics application. A Symmetric High-Dimensional and Incomplete (SHDI) matrix can smoothly illustrate such an UWN, which contains rich knowledge like node interaction behaviors and local complexes. To extract desired knowledge from an SHDI matrix, an analysis model should carefully consider its symmetric-topology for describing an UWN's intrinsic symmetry. Representation learning to an UWN borrows the success of a pyramid of symmetry-aware models like a Symmetric Nonnegative Matrix Factorization (SNMF) model whose objective function utilizes a sole Latent Factor (LF) matrix for representing SHDI's symmetry rigorously. However, they suffer from the following drawbacks: 1) their computational complexity is high; and 2) their modeling strategy narrows their representation features, maki
    
[^124]: ChatDB: 将数据库作为符号内存增强LLMs

    ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03901](http://arxiv.org/abs/2306.03901)

    ChatDB项目将SQL数据库作为符号内存，增强LLMs的复杂多跳推理能力。

    

    大语言模型（LLMs）与内存是计算通用的。然而，主流LLMs没有充分利用内存，并且设计受到生物大脑的严重影响。由于其近似性质和错误累积倾向，传统神经内存机制不能支持LLMs模拟复杂推理。在本文中，我们从现代计算机架构中寻求灵感，为复杂的多跳推理增强LLMs符号内存。这样的符号内存框架被实例化为一个LLM和一组SQL数据库，其中LLM生成SQL指令以操作SQL数据库。我们在一个需要复杂推理的合成数据集上验证了所提出的内存框架的有效性。 项目网站位于https://chatdatabase.github.io/。

    Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .
    
[^125]: 应用演绎验证技术验证思维链的推理过程

    Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03872](http://arxiv.org/abs/2306.03872)

    本文旨在通过应用演绎验证技术，使语言模型能够进行明确而严谨的演绎推理，以确保其推理过程的可信度。

    

    大语言模型在各种推理任务中受益匪浅，特别是应用思维链提示可以使模型产生更全面的推理过程。然而，思维链的强调中间推理步骤可能会不慎导致产生幻觉和累积错误，从而限制模型解决复杂推理任务的能力。本文灵感来自于人类如何进行细致的演绎逻辑推理过程来解决任务，我们旨在使语言模型能够进行明确而严谨的演绎推理，并通过自我验证确保推理过程的可信度。然而，即使是像ChatGPT这样先进的模型，直接验证整个演绎推理过程的有效性也是具有挑战性的。因此，我们提出将推理验证过程分解为一系列逐步的子过程，每个过程只接收其必要的上下文和前提条件。

    Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
    
[^126]: 虚拟健康中的患者预测：一种多模态动态知识图谱和文本挖掘方法

    Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic Knowledge Graph and Text Mining Approach. (arXiv:2306.03833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03833](http://arxiv.org/abs/2306.03833)

    本研究提出了一种多模态动态知识驱动退出预测（MDKDP）框架，能够解决虚拟健康中不同利益相关者之间和医疗保健交付系统之间的信息不对称问题，提高了退出预测的性能。

    

    虚拟健康被誉为医疗保健交付中的改变性力量。然而，它的退出问题是至关重要的，会导致较差的健康结果，增加健康、社会和经济成本。及时预测患者的退出使股东能够采取积极的步骤，解决患者的问题，可能提高保留率。为了解决这些信息不对称问题，我们提出了一种多模态动态知识驱动退出预测（MDKDP）框架，该框架从在线和离线医疗保健交付系统的医生患者对话、各个股东的动态和复杂网络中学习隐式和显式知识。我们通过与中国最大的虚拟健康平台之一合作来评估MDKDP。MDKDP提高了退出预测的性能。

    Virtual health has been acclaimed as a transformative force in healthcare delivery. Yet, its dropout issue is critical that leads to poor health outcomes, increased health, societal, and economic costs. Timely prediction of patient dropout enables stakeholders to take proactive steps to address patients' concerns, potentially improving retention rates. In virtual health, the information asymmetries inherent in its delivery format, between different stakeholders, and across different healthcare delivery systems hinder the performance of existing predictive methods. To resolve those information asymmetries, we propose a Multimodal Dynamic Knowledge-driven Dropout Prediction (MDKDP) framework that learns implicit and explicit knowledge from doctor-patient dialogues and the dynamic and complex networks of various stakeholders in both online and offline healthcare delivery systems. We evaluate MDKDP by partnering with one of the largest virtual health platforms in China. MDKDP improves the 
    
[^127]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^128]: Vid2Act：为视觉强化学习激活离线视频

    Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])

    [http://arxiv.org/abs/2306.03360](http://arxiv.org/abs/2306.03360)

    Vid2Act是一种基于模型的强化学习方法，它通过使用世界模型来传输领域相关的动态和策略，从而显著提高了样本效率。

    

    在离线视频数据集上预训练强化学习模型是提高其在线任务效率的有前途的方法，但由于跨域中任务、动态和行为的固有不匹配性而具有挑战性。最近，一种名为APV的模型避免了离线数据集中的伴随动作记录，而是专注于在源域内预训练与任务无关的、不涉及操作的世界模型。我们提出了Vid2Act，一种基于模型的强化学习方法，它学习从离线到在线环境中传输有价值的动作条件动态和潜在有用的动作演示。其主要思想是不仅将世界模型用作行为学习的模拟器，还将其用作测量领域相关性的工具，以便进行动态表示传输和策略传输。具体地，我们通过域选择知识蒸馏损失训练世界模型生成一组时间变化的任务相似度。这些相似度有两个目的：（i）自适应地将最相关的领域的动态传输到在线环境，和（ii）在在线环境中指导代理集中执行任务相关的动作。在Atari和DMControl连续控制任务上的实验结果表明了我们方法的有效性，其在样本效率方面大大优于之前的最先进的离线强化学习方法。

    Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
    
[^129]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^130]: 切换自回归低秩张量模型

    Switching Autoregressive Low-rank Tensor Models. (arXiv:2306.03291v1 [cs.LG])

    [http://arxiv.org/abs/2306.03291](http://arxiv.org/abs/2306.03291)

    该文提出了一种切换自回归低秩张量（SALT）模型，它将自回归隐Markov模型（ARHMM）和切换线性动态系统（SLDS）的优点结合起来，通过低秩参数化提高了模型性能。

    

    时序分析中一个重要的问题是对具有时变动力学的系统进行建模。共同连续和离散潜态的概率模型为这样的数据提供了可解释、高效和实验性有用的描述。常用的模型包括自回归隐Markov模型（ARHMM）和切换线性动态系统（SLDS），它们各有优缺点。ARHMM允许精确推理和简单的参数估计，但在对长依赖关系建模时具有参数密集性，因此容易出现过拟合。相比之下，通过马尔可夫潜态动力学，SLDS可以以参数高效的方式捕捉长距离依赖性，但困难的参数估计任务和一个难以处理的似然函数却是其具有挑战性的地方。在本文中，我们提出了切换自回归低秩张量（SALT）模型，该模型保留了两种方法的优点，同时改善了其局限性。SALT将ARHMM的张量参数化为低秩形式。

    An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose switching autoregressive low-rank tensor (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank 
    
[^131]: 牙科领域锥形束CT的人工智能技术：趋势和实践

    AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and Practices. (arXiv:2306.03025v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2306.03025](http://arxiv.org/abs/2306.03025)

    本文回顾了近期牙科CBCT成像中基于AI的病变检测、错颌分类、颊骨厚度测量、牙齿、颌骨、下颌骨、标志、轮廓和咽喉气道的分类和分割等任务，应用机器学习和深度学习算法等技术。

    

    锥形束计算机断层扫描（CBCT）是一种在牙科领域常用的成像技术，可以产生关于牙齿、颌骨及周围组织的详细三维图像，用于诊断和规划治疗多种口腔疾病。人工智能技术的最新发展在CBCT成像中使得其在诊断价值、精度和效率方面得到了显著提升。本文回顾了近期牙科CBCT成像中的人工智能趋势和实践，主要介绍了基于CBCT图像的病变检测、错颌分类、颊骨厚度测量、牙齿、颌骨、下颌骨、标志、轮廓和咽喉气道的分类和分割等任务，涉及机器学习算法、深度学习算法和超分辨率技术。

    Cone-beam computed tomography (CBCT) is a popular imaging modality in dentistry for diagnosing and planning treatment for a variety of oral diseases with the ability to produce detailed, three-dimensional images of the teeth, jawbones, and surrounding structures. CBCT imaging has emerged as an essential diagnostic tool in dentistry. CBCT imaging has seen significant improvements in terms of its diagnostic value, as well as its accuracy and efficiency, with the most recent development of artificial intelligence (AI) techniques. This paper reviews recent AI trends and practices in dental CBCT imaging. AI has been used for lesion detection, malocclusion classification, measurement of buccal bone thickness, and classification and segmentation of teeth, alveolar bones, mandibles, landmarks, contours, and pharyngeal airways using CBCT images. Mainly machine learning algorithms, deep learning algorithms, and super-resolution techniques are used for these tasks. This review focuses on the pote
    
[^132]: 神经网络回归中概率校准的大规模研究

    A Large-Scale Study of Probabilistic Calibration in Neural Network Regression. (arXiv:2306.02738v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02738](http://arxiv.org/abs/2306.02738)

    本研究对于神经网络的概率校准在回归问题中进行了最大的实证研究，发现正则化方法在校准和锐度之间提供了有利的平衡，而后续方法表现更优越，此外还证明了分位数重新校准可以被认为是合规性预测的一个特定情况。

    

    准确的概率预测对于最优决策非常重要。虽然神经网络的误校准主要研究了分类问题，但我们对于回归问题中较少探讨的误校准进行了研究。我们进行了迄今为止最大的实证研究，评估神经网络的概率校准。我们还分析了重新校准、合规和正则化方法来增强概率校准的性能。此外，我们引入了新颖的可微分重新校准和正则化方法，揭示了它们的有效性的新见解。我们的研究发现，正则化方法在校准和锐度之间提供了有利的平衡。后续方法表现出更优越的概率校准，我们将其归因于合规性预测的有限样本覆盖保证。此外，我们还证明，分位数重新校准可以被认为是合规性预测的一个特定情况。我们的研究是完全可再生的。

    Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction. Our study is fully reproducible
    
[^133]: 基于联邦深度学习的物联网入侵检测

    Federated Deep Learning for Intrusion Detection in IoT Networks. (arXiv:2306.02715v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.02715](http://arxiv.org/abs/2306.02715)

    本研究提出了一种基于联邦深度学习的入侵检测框架，可在保护数据隐私和局部性的同时实现高效的模型收敛，该框架在物联网入侵检测应用中具有潜力。

    

    物联网技术的大幅增长和攻击向量和威胁行为的不断演化，显著增加了网络安全风险。新型攻击可能会危及物联网设备，以获取对敏感数据的访问或控制它们以部署进一步的恶意活动。检测新型攻击通常依赖于人工智能解决方案。在分布式物联网系统中实现基于AI的入侵检测的常见方法是以集中方式实现。然而，此方法可能会侵犯数据隐私和保密性。此外，集中式数据采集禁止IDS的扩展。因此，物联网生态系统中的入侵检测解决方案需要朝分散化方向发展。联邦学习由于能够在保留数据机密性和局部性的同时进行协作学习而受到了广泛的关注。尽管如此，大多数基于联邦学习的物联网系统IDS都是在不切实际的数据分布条件下设计的。为此，我们设计了一个代表物联网生态系统中实际数据分布的实验，并提出了一种基于联邦深度学习（FDL）框架的物联网入侵检测方法。我们的方法整合了CNN模型和门控机制，以实现有效的模型收敛，同时保护数据隐私和局部性。实验结果表明，我们的FDL框架显著优于其他最先进的方法，表明其在实际物联网入侵检测应用中具有潜力。

    The vast increase of IoT technologies and the ever-evolving attack vectors and threat actors have increased cyber-security risks dramatically. Novel attacks can compromise IoT devices to gain access to sensitive data or control them to deploy further malicious activities. The detection of novel attacks often relies upon AI solutions. A common approach to implementing AI-based IDS in distributed IoT systems is in a centralised manner. However, this approach may violate data privacy and secrecy. In addition, centralised data collection prohibits the scale-up of IDSs. Therefore, intrusion detection solutions in IoT ecosystems need to move towards a decentralised direction. FL has attracted significant interest in recent years due to its ability to perform collaborative learning while preserving data confidentiality and locality. Nevertheless, most FL-based IDS for IoT systems are designed under unrealistic data distribution conditions. To that end, we design an experiment representative o
    
[^134]: 将NP困难的最小最大路径问题作为具有公平背景的顺序生成来解决

    Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. (arXiv:2306.02689v1 [cs.LG])

    [http://arxiv.org/abs/2306.02689](http://arxiv.org/abs/2306.02689)

    本文提出了一个新的深度学习框架Equity-Transformer来解决大规模的最小最大路径问题。该模型利用可扩展的深度学习模型进行顺序决策，并生成考虑公平工作负载的顺序动作。研究显示，Equity-Transformer在两个代表性最小最大路径问题中具有卓越的性能。

    

    最小最大路径问题旨在最小化所有代理商协同访问所有城市的最大旅游长度，即完成时间。这些问题包括有影响力的实际应用，但被认为是NP困难的。现有方法面临挑战，特别是在需要协调众多代理商覆盖数千个城市的大规模问题中。本文提出了一个新的深度学习框架来解决大规模的最小最大路径问题。我们将多个代理商的同时决策建模为顺序生成过程，允许利用可扩展的深度学习模型进行顺序决策。在顺序近似问题中，我们提出了一个可扩展的上下文Transformer模型Equity-Transformer，它生成考虑其他代理商之间公平工作负载的顺序动作。Equity-Transformer的有效性通过其在两个代表性最小最大路径问题中具有卓越的性能得到证明。

    Min-max routing problems aim to minimize the maximum tour length among agents as they collaboratively visit all cities, i.e., the completion time. These problems include impactful real-world applications but are known as NP-hard. Existing methods are facing challenges, particularly in large-scale problems that require the coordination of numerous agents to cover thousands of cities. This paper proposes a new deep-learning framework to solve large-scale min-max routing problems. We model the simultaneous decision-making of multiple agents as a sequential generation process, allowing the utilization of scalable deep-learning models for sequential decision-making. In the sequentially approximated problem, we propose a scalable contextual Transformer model, Equity-Transformer, which generates sequential actions considering an equitable workload among other agents. The effectiveness of Equity-Transformer is demonstrated through its superior performance in two representative min-max routing 
    
[^135]: Meta-SAGE：用引导探索的规划方法和比例一元学习进行协同优化规模偏移问题

    Meta-SAGE: Scale Meta-Learning Scheduled Adaptation with Guided Exploration for Mitigating Scale Shift on Combinatorial Optimization. (arXiv:2306.02688v1 [cs.LG])

    [http://arxiv.org/abs/2306.02688](http://arxiv.org/abs/2306.02688)

    本研究提出了一种名为Meta-SAGE的新方法，用于解决组合优化任务中深度强化学习模型可扩展性的问题。该方法通过比例元学习和时间表调整来适应模型，并真实地优化了相关任务的性能表现。

    

    本文提出了一种称之为Meta-SAGE的新方法，旨在改善组合优化（CO）任务的深度强化学习模型的可扩展性。本方法通过建议两个组件来在测试时间适应预训练模型以解决规模问题：一个是比例元学习器（SML），另一个是具有引导探索和时间表调整功能的scheduled adaptation with guided exploration（SAGE）。实验结果表明，Meta-SAGE优于以前的适应方法，并显著提高了代表性CO任务的可扩展性。

    This paper proposes Meta-SAGE, a novel approach for improving the scalability of deep reinforcement learning models for combinatorial optimization (CO) tasks. Our method adapts pre-trained models to larger-scale problems in test time by suggesting two components: a scale meta-learner (SML) and scheduled adaptation with guided exploration (SAGE). First, SML transforms the context embedding for subsequent adaptation of SAGE based on scale information. Then, SAGE adjusts the model parameters dedicated to the context embedding for a specific instance. SAGE introduces locality bias, which encourages selecting nearby locations to determine the next location. The locality bias gradually decays as the model is adapted to the target instance. Results show that Meta-SAGE outperforms previous adaptation methods and significantly improves scalability in representative CO tasks. Our source code is available at https://github.com/kaist-silab/meta-sage
    
[^136]: bgGLUE：保加利亚通用语言理解评估基准

    bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark. (arXiv:2306.02349v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02349](http://arxiv.org/abs/2306.02349)

    提出了bgGLUE，这是一个用于在保加利亚语上评估语言模型在自然语言理解（NLU）任务上的基准。该基准测试包括针对各种自然语言处理问题（例如，自然语言推理、事实检查、命名实体识别、情感分析、问答等）和机器学习任务的NLU任务，评估结果表明，在序列标记任务方面表现强劲，但需要更复杂的推理任务还有很大的提升空间。

    

    我们提出了bgGLUE（保加利亚通用语言理解评估），这是一个用于在保加利亚语上评估语言模型在自然语言理解（NLU）任务上的基准。我们的基准包括针对各种自然语言处理问题（例如，自然语言推理、事实检查、命名实体识别、情感分析、问答等）和机器学习任务（序列标记、文档级分类和回归）的NLU任务。我们进行了首次系统评估保加利亚语预训练语言模型，在基准测试中跨足了九个任务，比较和对比了结果。评估结果表明，在序列标记任务方面表现强劲，但需要更复杂的推理任务还有很大的提升空间。我们将bgGLUE与微调和评估代码一起公开提供，以及在https://bgglue.github.io/上提供公共排行榜，希望它能促进更进一步的发展。

    We present bgGLUE(Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io/, and we hope that it will enable further advancements in developi
    
[^137]: 采用优势诱导策略对齐的Fine-Tuning语言模型

    Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02231](http://arxiv.org/abs/2306.02231)

    本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。

    

    人类反馈强化学习（RLHF）已经成为将大型语言模型（LLMs）与人类偏好对齐的可靠方法。在众多RLHF技术中，接近策略优化（PPO）是最常用的方法之一。然而，尽管PPO很流行，但它可能会遭受模式崩溃、不稳定和效率低下的问题。我们展示了一种新颖的算法--基于估计优势的平方误差损失函数的优势诱导策略对齐（APA），可以减轻这些问题。我们通过实验证明，当使用单独的奖励模型作为评估器时，APA在语言任务中始终比PPO表现出更好的性能。此外，与PPO相比，APA可以更稳定地控制模型与初始策略的偏差，确保模型提高性能而不会崩溃为确定性输出。除了经验结果之外，我们还提供了APA的理论分析。

    Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
    
[^138]: SGEM：通过序列级广义熵最小化实现自动语音识别的测试时自适应

    SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.01981](http://arxiv.org/abs/2306.01981)

    SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。

    

    在许多实际情况下，自动语音识别（ASR）模型经常暴露于数据分布的变化，导致错误的预测。为了解决这个问题，最近提出了一种现有的测试时自适应（TTA）方法，可以在没有源数据的情况下调整预训练的ASR模型以适应未标记的测试实例。尽管有了不错的性能提升，但这项工作仅依赖于简单的贪心解码，并在帧级别上跨越时间步长进行调整，这在模型输出的序列性质下可能不是最优的。出于这个动机，我们提出了一个新的TTA框架，称为SGEM，用于一般ASR模型。为了处理序列输出，SGEM首先利用波束搜索来探索候选输出标志，并选择最可信的标志。然后，它利用广义熵最小化和负抽样作为无监督目标来适应模型。在各种领域的转变下，SGEM实现了三种主流ASR模型的最新性能。

    Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
    
[^139]: GAD-NR: 通过邻域重构实现图形异常检测

    GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01951](http://arxiv.org/abs/2306.01951)

    本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。

    

    图形异常检测（GAD）是一种技术，用于识别图中的异常节点，在网络安全、欺诈检测、社交媒体垃圾检测和其他各种领域中有应用。GAD的常见方法是图自编码器（GAEs），它将图形数据编码成节点表示，并根据这些表示来评估图形的重构质量，以识别异常。然而，现有的GAE模型主要针对直接链接重构进行优化，导致在潜在空间中连接图中的节点被聚类。因此，它们擅长检测聚类型结构异常，但对不符合聚类的更复杂的结构异常存在困难。为了解决这个限制，我们提出了一种新颖的解决方案，称为GAD-NR，它是GAE的一个新变体，融合邻域重构进行图形异常检测。GAD-NR的目标是重构节点的整个邻域，涵盖本地结构

    Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
    
[^140]: 物理学知识增强的 UNet 用于发现异质材料中隐藏的弹性

    Physics-informed UNets for Discovering Hidden Elasticity in Heterogeneous Materials. (arXiv:2306.01204v1 [cs.LG])

    [http://arxiv.org/abs/2306.01204](http://arxiv.org/abs/2306.01204)

    本文提出了一种新型的 UNet 神经网络模型 (El-UNet)，能够通过物理学知识增强和应变图像的分析，精确地推断生物软组织中材料参数的空间分布，具有高性能的优劣和计算效率。

    

    生物软组织常常由于结构成分的变化而具有复杂的机械特性。本文提出了一种新颖的基于 UNet 的反演弹性神经网络模型 (El-UNet)，将应变图作为输入图像、正常应力边界条件和区域物理信息，以推断力学参数的空间分布。我们展示了 El-UNet 在估计等向性线性弹性的未知参数和应力分布方面，与全连接的物理学知识增强神经网络相比，具有更高的性能，无论是在精度还是计算成本方面。我们对 El-UNet 的不同变体进行了分类，并提出了一种自适应空间损失加权方法。为了验证我们的反演模型，我们进行了各种等向异质材料有限元模拟产生的合成数据。El-UNet 比全连接物理学知识增强的实现更快、更准确。

    Soft biological tissues often have complex mechanical properties due to variation in structural components. In this paper, we develop a novel UNet-based neural network model for inversion in elasticity (El-UNet) to infer the spatial distributions of mechanical parameters from strain maps as input images, normal stress boundary conditions, and domain physics information. We show superior performance, both in terms of accuracy and computational cost, by El-UNet compared to fully-connected physics-informed neural networks in estimating unknown parameters and stress distributions for isotropic linear elasticity. We characterize different variations of El-UNet and propose a self-adaptive spatial loss weighting approach. To validate our inversion models, we performed various finite-element simulations of isotropic domains with heterogenous distributions of material parameters to generate synthetic data. El-UNet is faster and more accurate than the fully-connected physics-informed implementat
    
[^141]: 基于权威图结构特征对基于GNN的IDS检测进行解释

    Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features. (arXiv:2306.00934v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.00934](http://arxiv.org/abs/2306.00934)

    基于PROVEXPLAINER框架，通过复制GNN-based security models的决策过程，利用决策树和图结构特征将抽象GNN决策边界投影到可解释的特征空间，以增强GNN安全模型的透明度和询问能力。

    

    复杂神经网络模型的黑匣子本质妨碍了它们在安全领域的普及，因为它们缺乏逻辑解释和可执行后续行动的预测。为了增强在系统来源分析中使用的图神经网络（GNN）安全模型的透明度和问责制，我们提出了PROVEXPLAINER，一种将抽象GNN决策边界投影到可解释特征空间的框架。我们首先使用简单且可解释的模型，如决策树（DT），复制基于GNN的安全模型的决策过程。为了最大化替代模型的准确性和保真度，我们提出了一种基于经典图论的图结构特征，并通过安全领域知识的广泛数据研究对其进行了增强。我们的图结构特征与系统来源领域中的问题空间行动密切相关，这使检测结果可用人类语言描述和解释。

    The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.  We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human languag
    
[^142]: 医学成像信息学简介

    Introduction to Medical Imaging Informatics. (arXiv:2306.00421v1 [eess.IV])

    [http://arxiv.org/abs/2306.00421](http://arxiv.org/abs/2306.00421)

    本文介绍了医学成像信息学的基本概念和最新进展，包括图像处理、特征工程、机器学习、计算机视觉和深度学习，以及如何将它们应用于疾病检测、诊断和预后预测模型的开发。这对于理解信息学在医学中的作用以及其对患者护理的潜在影响具有重要意义。

    

    医学成像信息学是将医学成像和信息学的原理结合起来，以改善医学图像的获取、管理和解释为目的的快速增长的领域。本章介绍了医学成像信息学的基本概念，包括图像处理、特征工程和机器学习。同时，本章还讨论了计算机视觉和深度学习技术的最新进展，以及它们如何用于开发新的定量图像标记和疾病检测、诊断和预后预测模型。通过涵盖医学成像信息学的基本知识，本章为理解信息学在医学中的作用及其对患者护理的潜在影响提供了基础。

    Medical imaging informatics is a rapidly growing field that combines the principles of medical imaging and informatics to improve the acquisition, management, and interpretation of medical images. This chapter introduces the basic concepts of medical imaging informatics, including image processing, feature engineering, and machine learning. It also discusses the recent advancements in computer vision and deep learning technologies and how they are used to develop new quantitative image markers and prediction models for disease detection, diagnosis, and prognosis prediction. By covering the basic knowledge of medical imaging informatics, this chapter provides a foundation for understanding the role of informatics in medicine and its potential impact on patient care.
    
[^143]: 学习高斯混合表示用于张量时间序列预测

    Learning Gaussian Mixture Representations for Tensor Time Series Forecasting. (arXiv:2306.00390v1 [cs.LG])

    [http://arxiv.org/abs/2306.00390](http://arxiv.org/abs/2306.00390)

    本文提出了一种新的张量时间序列预测框架GMRL，它可以单独模拟时间、位置和源变量中所暗示的每个异构性组件，相比于最先进的基准方法，本文的方法显示出了优越性。

    

    张量时间序列（TTS）数据是高维空间中一维时间序列的一般化，是现实场景中万能的存在，特别是在涉及多源时空数据的监测系统中（例如交通需求和空气污染物）。与建模时间序列或多元时间序列相比，在最近几年已经受到广泛关注并取得了巨大进展的情况下，张量时间序列付出的努力较少。由于其高维和复杂的内部结构，正确处理张量时间序列是一个更具挑战性的任务。在本文中，我们开发了一种新的TTS预测框架，该框架旨在单独模拟时间、位置和源变量中所暗示的每个异构性组件。我们将此框架命名为GMRL，即高斯混合表示学习。在两个实际TTS数据集上的实验结果验证了我们的方法相对于最先进的基准方法的优越性。

    Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world scenarios, especially in monitoring systems involving multi-source spatio-temporal data (e.g., transportation demands and air pollutants). Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. Properly coping with the tensor time series is a much more challenging task, due to its high-dimensional and complex inner structure. In this paper, we develop a novel TTS forecasting framework, which seeks to individually model each heterogeneity component implied in the time, the location, and the source variables. We name this framework as GMRL, short for Gaussian Mixture Representation Learning. Experiment results on two real-world TTS datasets verify the superiority of our approach compared with the state-of-the-art baseli
    
[^144]: 面向大规模机器学习的关系计算自动微分

    Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning. (arXiv:2306.00088v1 [cs.LG])

    [http://arxiv.org/abs/2306.00088](http://arxiv.org/abs/2306.00088)

    本文提出了一种面向关系计算的自动微分方法，实验表明该方法可达到非常大的数据集规模，并在大规模分布式机器学习中表现出与专用系统相媲美的竞争力。

    

    关系数据模型被设计用于大规模数据管理和分析。本文探讨了如何求解关系计算中的自动微分问题。我们在实验中展示了一个运行自动微分关系算法的关系引擎可以轻松扩展到非常大的数据集，并且在大规模分布式机器学习方面与最先进的专用系统相竞争。

    The relational data model was designed to facilitate large-scale data management and analytics. We consider the problem of how to differentiate computations expressed relationally. We show experimentally that a relational engine running an auto-differentiated relational algorithm can easily scale to very large datasets, and is competitive with state-of-the-art, special-purpose systems for large-scale distributed machine learning.
    
[^145]: 使用子图特定因子嵌入归一化改善GNN的表达能力

    Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19903](http://arxiv.org/abs/2305.19903)

    本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。

    

    图神经网络（GNN）已经成为一类处理图结构数据的强大学习架构。然而，现有的GNN通常忽略了节点感应子图中的重要结构特征，从而限制了它们在各种下游任务中的表达能力。本文旨在通过设计一种专用的即插即用归一化方案——SUbgraph-sPEcific FactoR Embedded Normalization（SuperNorm）来加强GNN的代表性能力，该方案明确考虑了每个节点感应子图内部连接的信息。为此，我们在标准的BatchNorm开始和结束时嵌入了子图特定因子，并纳入图实例特定统计数据以提高区分能力。同时，我们提供了理论分析支持，指出通过改善的SuperNorm，任意GNN至少与1-WL测试一样能够区分非同构图。

    Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
    
[^146]: GAN-MPC:使用非相同专家的演示训练具有参数化成本函数的模型预测控制器

    GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts. (arXiv:2305.19111v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.19111](http://arxiv.org/abs/2305.19111)

    本文提出了一种新的方法，使用GAN来训练Learnable-MPC策略，以便在演示者和模仿者代理不能相同时进行参数化的成本函数学习。

    

    模型预测控制（MPC）是机器人实际应用中轨迹优化的流行方法。MPC策略可以在运动动力学和安全性约束下优化轨迹参数，并在安全保障、最优性、泛化性、可解释性和说明性方面提供保证。然而，有些行为是复杂的，手工制作MPC目标函数是困难的。一种名为可学习MPC的特殊类别的MPC策略利用来自专家演示的模仿学习解决了这个问题。然而，它们要求演示者和模仿者代理相同，这在许多机器人的实际应用中很难满足。在本文中，我们解决了当演示者和模仿者没有共享相同动力学且状态空间可能部分重叠时训练可学习MPC策略的实际问题。我们提出了一种新的方法，使用生成对抗网络（GAN）来最小化Jensen-Shannon距离来进行学习。

    Model predictive control (MPC) is a popular approach for trajectory optimization in practical robotics applications. MPC policies can optimize trajectory parameters under kinodynamic and safety constraints and provide guarantees on safety, optimality, generalizability, interpretability, and explainability. However, some behaviors are complex and it is difficult to hand-craft an MPC objective function. A special class of MPC policies called Learnable-MPC addresses this difficulty using imitation learning from expert demonstrations. However, they require the demonstrator and the imitator agents to be identical which is hard to satisfy in many real world applications of robotics. In this paper, we address the practical problem of training Learnable-MPC policies when the demonstrator and the imitator do not share the same dynamics and their state spaces may have a partial overlap. We propose a novel approach that uses a generative adversarial network (GAN) to minimize the Jensen-Shannon di
    
[^147]: 基于最近邻的大语言模型的测试时间训练

    Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])

    [http://arxiv.org/abs/2305.18466](http://arxiv.org/abs/2305.18466)

    该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。

    

    最近的许多工作都旨在在测试时从数据库中检索相关信息以增强语言模型。我们通过直接在测试时使用其标准训练设置对检索到的数据对模型进行微调，避免了提示工程的需要。为此，我们建立了一个基于“Pile”数据集的文本嵌入的大规模分布式最近邻索引。给定一个语言模型的查询，我们的系统检索查询的邻居，并在对应于这些邻居的文本数据上微调模型。令人惊讶的是，检索和训练仅20个邻居，每个邻居仅进行一次梯度迭代，就显著提高了在“Pile”基准测试中超过二十个语言建模任务的性能。例如，测试时间训练显著缩小了小型GPT2模型和GPTNeo模型之间的性能差距，后者是专门对“Pile”进行收敛训练的，体积却是前者的十倍以上。然而，其方法的成功还取决于充分的索引质量和大小。

    Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
    
[^148]: 利用分子对接和机器学习回归方法的药物重用以靶向COVID-19 3CL Protease

    Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.18088](http://arxiv.org/abs/2305.18088)

    本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。

    

    COVID-19疫情已经成为全球健康危机，迫切需要快速鉴定潜在的治疗药物。为了应对这一挑战，药物重用是省时省力的唯一解决方案。本研究使用Zinc数据库对全球已批准（包括FDA批准）的5903种药物进行筛选，作为潜在的COVID-19治疗药物，以靶向SARS-CoV-2的主要蛋白酶3CL。我们使用Autodock-Vina进行分子对接，检查药物分子的功效。为了提高药物重用的效率，我们采用决策树、额外树、MLP、KNN、XGBoost和梯度提升等多个机器学习回归方法建模结合药物的结合亲和力。计算结果表明，决策树回归（DTR）模型具有改进的统计措施R2和RMSE。这些模拟结果有助于识别具有高结合亲和力和有利的结合能的药物。

    The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
    
[^149]: 使用优化空间的同态矩阵乘法来改进隐私保护PCA

    Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])

    [http://arxiv.org/abs/2305.17341](http://arxiv.org/abs/2305.17341)

    本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。

    

    主成分分析（PCA）是机器学习和数据分析领域中的重要技术。本研究提出了一种新的方法，使用近似数值算术同态加密方案进行隐私保护PCA。我们基于一种被称为PowerMethod的PCA常规方法，该方法以协方差矩阵作为输入，并产生与数据集的第一主成分对应的近似特征向量。我们的方法在效率、准确性和可扩展性方面优于以前的方法（如Pandas CSCML 21）。为了实现这样的效率和准确性，我们实现了以下优化：（i）优化了同态矩阵乘法技术（Jiang等人SIGSAC 2018），该技术在协方差矩阵的计算中起着关键作用；（ii）设计了一个有效的同态电路来同态计算协方差矩阵；（iii）设计了一种新颖且高效的同态加密方案用于特征值和特征向量的计算。

    Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
    
[^150]: PyTorch的超参数调整——面向spotPython的教程

    PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])

    [http://arxiv.org/abs/2305.11930](http://arxiv.org/abs/2305.11930)

    本文介绍了如何将spotPython超参数调谐器集成到PyTorch训练工作流中，以提高机器或深度学习模型的性能，以CIFAR10图像分类器为例。

    

    超参数调整（或超参数优化）的目标是优化超参数以提高机器或深度学习模型的性能。spotPython是知名超参数调谐器SPOT的Python版本，SPOT已经在R编程环境中为统计分析开发了十年以上。PyTorch是一种基于GPU和CPU的深度学习优化张量库。本文展示了如何将spotPython超参数调谐器集成到PyTorch训练工作流中。以CIFAR10图像分类器为例，介绍了spotPython以及与Ray Tune的简短比较。本文讨论了两种方法的优缺点。我们展示了spotPython的使用经验，以及如何使用hook在训练过程中自动调整参数。

    The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
    
[^151]: 可约的双曲正切网络的功能等价和路径连通性

    Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks. (arXiv:2305.05089v1 [cs.NE])

    [http://arxiv.org/abs/2305.05089](http://arxiv.org/abs/2305.05089)

    本文对于单隐藏层双曲正切结构给出了单元冗余和可约的功能等价类的算法刻画，并证明了这样的功能等价类是分段线性路径连通的集合。

    

    理解人工神经网络的学习过程需要澄清学习发生的参数空间的结构。神经网络参数的功能等价类是实现相同输入输出函数的参数集合。然而，存在一小部分可约参数，其功能等价类由网络单元之间的冗余造成，因而具有更丰富的功能等价类。本文对于单隐藏层双曲正切结构给出了单元冗余和可约的功能等价类的算法刻画。我们证明了这样的功能等价类是分段线性路径连通的集合，并且对于具有多数冗余单元的参数，这些集合的直径最多为7线段。

    Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's functional equivalence class is the set of parameters implementing the same input--output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of reducible parameters, with richer functional equivalence classes caused by redundancies among the network's units.  In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.
    
[^152]: 不充分标注下的多领域学习

    Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])

    [http://arxiv.org/abs/2305.02757](http://arxiv.org/abs/2305.02757)

    提出了一种名为多领域对比学习（MDCL）的新方法，在原有方法的基础上，利用来自标记和未标记数据的语义和结构信息，解决了不充分注释的问题，并在实验中取得了优异的成果。

    

    多领域学习(MDL)指同时在来自不同领域的数据集上构建一个模型或一组模型。传统方法强调域共享信息的提取和域私有信息的保留，遵循共享-私有架构(SP模型)，这比单领域学习具有明显优势。然而，在每个领域中有限的已注释数据的可用性，严重阻碍了传统监督MDL方法在实际应用中的有效性。本文介绍了一种称为多领域对比学习(MDCL)的新方法，通过捕获来自标记和未标记数据的语义和结构信息，缓解了不充分注释的影响。具体而言，MDCL包括两个模块：域间语义对齐和域内对比。前者旨在将不同领域中相同语义类别的已标注实例在共享的隐空间中对齐，而后者旨在在每个领域内最大化分离来自不同类别的实例。我们在三个多领域学习任务上进行实验，包括图像分类、情感分析和假新闻检测，结果表明我们提出的MDCL方法在各种注释方案下优于现有的最先进MDL方法。

    Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd
    
[^153]: 面向设计和验证自动化的Verilog自动完成的深度学习框架

    A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation. (arXiv:2304.13840v1 [cs.LG])

    [http://arxiv.org/abs/2304.13840](http://arxiv.org/abs/2304.13840)

    本文提出了一个深度学习框架，用于训练Verilog自动完成模型，能够有效预测下一个token，为设计和验证自动化提供了解决方案。

    

    创新的电子设计自动化（EDA）解决方案对于满足越来越复杂的电子设备的设计要求至关重要。 Verilog是一种广泛用于数字电路设计和验证的硬件描述语言，并使用特定的EDA工具进行合成。然而，编写代码是一项重复且耗时的任务。本文首先提出了一种新颖的深度学习框架，用于训练Verilog自动完成模型，其次提供了从开源存储库获取的Verilog数据集。该框架涉及将预先训练的模型集成到通用编程语言数据上，然后在相似于目标下游任务的数据集上进行微调。通过比较在多个评估指标上训练不同子集的预训练模型与提议的Verilog数据集，验证了该方法的有效性。实验表明，该框架相比基线模型实现了更好的BLEU，ROUGE-L和chrF得分，并且有效预测了部分编写的Verilog语句的下一个token。

    Innovative Electronic Design Automation (EDA) solutions are important to meet the design requirements for increasingly complex electronic devices. Verilog, a hardware description language, is widely used for the design and verification of digital circuits and is synthesized using specific EDA tools. However, writing code is a repetitive and time-intensive task. This paper proposes, primarily, a novel deep learning framework for training a Verilog autocompletion model and, secondarily, a Verilog dataset of files and snippets obtained from open-source repositories. The framework involves integrating models pretrained on general programming language data and finetuning them on a dataset curated to be similar to a target downstream task. This is validated by comparing different pretrained models trained on different subsets of the proposed Verilog dataset using multiple evaluation metrics. These experiments demonstrate that the proposed framework achieves better BLEU, ROUGE-L, and chrF sco
    
[^154]: 多方聊天：人类和模型中的群聊对话代理

    Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])

    [http://arxiv.org/abs/2304.13835](http://arxiv.org/abs/2304.13835)

    本文通过收集和评估多方对话情况，探讨了模型在群体对话中需要具备的技能，发现新数据集MultiLIGHT可以在这个领域带来显着的进展。

    

    当前的对话研究主要研究成对（双方）对话，并没有涉及到多于两个人在一起对话的日常情景。本文使用LIGHT环境构建接地对话来收集和评估多方对话情况。我们对比在新数据集MultiLIGHT上训练的模型和现有的成对训练的对话模型以及带有少量提示的大型语言模型。我们发现，我们将公开发布MultiLIGHT数据集，这将有助于在群体设置中带来显着的改进。

    Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
    
[^155]: 开放式智能交通基础模型挑战赛的新基准与基准数据集 - Open-TransMind

    Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])

    [http://arxiv.org/abs/2304.06051](http://arxiv.org/abs/2304.06051)

    Open-TransMind是智能交通领域第一个基础模型挑战赛的新基准，旨在解决数据量少、泛化能力差以及缺乏多模态技术等典型挑战。

    

    随着近年来计算能力和深度学习算法的不断提升，基础模型越来越受欢迎。由于其强大的能力和出色的性能，这种技术被越来越多的行业采用和应用。在智能交通行业中，人工智能面临着以下典型挑战：数据量少、泛化能力差以及缺乏多模态技术。基础模型技术可以显著缓解上述问题。为解决这些问题，我们设计了第一个基础模型挑战，旨在增加基础模型技术在交通场景中的普及度，并促进智能交通行业的快速发展。该挑战分为两个赛道：全能型和跨模态图像检索。此外，我们为这两个赛道提供了一个新的基线和基准数据，称为Open-TransMind。据我们所知，这是智能交通领域的第一个基础模型基准。

    With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
    
[^156]: 文献综述：计算机视觉在物流和仓储中的应用

    Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing. (arXiv:2304.06009v1 [cs.CV])

    [http://arxiv.org/abs/2304.06009](http://arxiv.org/abs/2304.06009)

    该论文进行了计算机视觉在物流和仓储中应用领域的文献综述，将文献分为监视和操作两个领域，并指出了未来研究方向和计算机视觉的最新发展。研究结果对于物流从业者也有参考价值。

    

    计算机视觉在物流和仓储中的应用具有巨大的自动化潜力。为了利用这个潜力，我们对该领域的研究进行了结构化的文献综述。所有文献都被分类为应用和用于解决任务的计算机视觉技术。关于应用，我们将文献分为两个领域：监视和操作。此外，我们指出了未来研究的方向，并链接到适用于物流的计算机视觉的最新发展。最后，我们提供现有数据集和工业解决方案的概述。我们得出结论，虽然已经研究了许多研究领域，但未来研究的潜力仍然巨大。我们的分析结果对物流从业者也很有帮助，因为我们提供了开发和测试的现有解决方案的概述。

    Computer vision applications in transportation logistics and warehousing have a huge potential for process automation. We present a structured literature review on research in the field to help leverage this potential. All literature is categorized w.r.t. the application, i.e. the task it tackles and w.r.t. the computer vision techniques that are used. Regarding applications, we subdivide the literature in two areas: Monitoring, i.e. observing and retrieving relevant information from the environment, and manipulation, where approaches are used to analyze and interact with the environment. In addition to that, we point out directions for future research and link to recent developments in computer vision that are suitable for application in logistics. Finally, we present an overview of existing datasets and industrial solutions. We conclude that while already many research areas have been investigated, there is still huge potential for future research. The results of our analysis are als
    
[^157]: ImageReward：学习和评估文本到图像生成的人类喜好

    ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])

    [http://arxiv.org/abs/2304.05977](http://arxiv.org/abs/2304.05977)

    ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。

    

    本文提出一种通用的文本到图像生成人类喜好奖励模型ImageReward，旨在解决生成模型中存在的各种问题，并使其与人类价值和偏好保持一致。该奖励模型的训练基于我们的系统注释流程，其中包括评分和排名组件，迄今已收集了137k的专家比较数据集。在人类评估中，ImageReward的表现优于现有的评分方法（例如比CLIP高38.6\%），因此它是一种有前途的用于评估和改进文本到图像合成的自动度量标准。该奖励模型通过\texttt {image-reward}程序包公开提供，网址为\url{https://github.com/THUDM/ImageReward}。

    We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
    
[^158]: NeBLa: 使用神经啤酒-兰伯特法从全景放射线图中重建口腔结构的三维模型

    NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])

    [http://arxiv.org/abs/2304.04027](http://arxiv.org/abs/2304.04027)

    该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。

    

    全景X线片（全景放射线图，PX）是常用于牙科检查的成像模式。然而，与3D锥形束计算机断层扫描（CBCT）相比，PX的适用性有限，因为PX只提供口腔结构的二维扁平图像。在本文中，我们提出了一个新的框架，用于从真实的PX图像估计3D口腔结构。由于PX和CBCT数据的匹配不多，我们在训练时使用了从CBCT模拟的PX，但在推理时使用了真实的全景放射线片。我们提出了一种新的光线采样方法，受到全景放射线成像原理的启发，利用啤酒-兰伯特定律导出渲染函数生成模拟全景放射线图。我们的模型由三个部分组成：转换模块，生成模块和精炼模块。转换模块将真实的全景放射线图转换为模拟的训练图像风格。生成模块利用射线采样方法得到的模拟全景放射线图约束下的输入图像生成3D结构。精炼模块改善了3D结构的平滑性和一致性。实验结果表明，我们提出的方法可以从全景放射线片提供的有限信息中生成精确的3D牙科模型。

    Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
    
[^159]: 应用机器学习和领域知识个性化数字健康行为变革干预

    Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])

    [http://arxiv.org/abs/2304.03392](http://arxiv.org/abs/2304.03392)

    该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。

    

    我们正在开发一种虚拟教练系统，帮助患者坚持行为变革干预（BCI）。我们的系统预测患者是否会执行目标行为，并使用反事实例子进行特征控制，以指导个性化BCI。我们使用具有不同响应水平的模拟患者数据评估了我们的预测模型。

    We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
    
[^160]: BOLT：一种用于在普通CPU硬件上训练和部署大规模神经网络的自动化深度学习框架。

    BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])

    [http://arxiv.org/abs/2303.17727](http://arxiv.org/abs/2303.17727)

    BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。

    

    有效地在普通CPU硬件上进行大规模神经网络的训练和推理对于民主化深度学习能力具有巨大的实际意义。目前，由数十亿个参数组成的大规模模型的训练过程需要广泛使用专用硬件加速器（例如GPU），这些加速器仅限于少数具有相当财务资源的机构。此外，训练和部署这些模型通常会带来惊人的碳足迹。在本文中，我们通过引入BOLT，一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库来解决这些挑战。BOLT提供了一个灵活的高级API，用于构建模型，该API对于现有流行的深度学习框架的用户来说是熟悉的。通过自动调整专用超参数，BOLT也抽象掉了稀疏网络训练的算法细节。

    Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
    
[^161]: 语言模型能够解决计算机任务

    Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])

    [http://arxiv.org/abs/2303.17491](http://arxiv.org/abs/2303.17491)

    本文研究表明，预训练的大型语言模型代理可以通过一个简单的提示方案使用自然语言执行计算机任务，该方法取得了很好的效果并在MiniWoB++基准测试中超越了监督学习和强化学习方法。

    

    能够在计算机上执行通用任务的代理可以通过自动化重复任务和协助复杂问题的解决来提高效率和生产力。理想情况下，这些代理应该能够通过自然语言命令解决新的计算机任务。然而，先前解决这个问题的方法需要大量专家示范和任务特定的奖励函数，这两者对于新任务来说都不切实际。在这项工作中，我们展示了一个预先训练的大型语言模型（LLM）代理可以使用一个简单的提示方案（RCI），通过自然语言指导执行计算机任务，并在批评和改进输出的过程中取得很好的效果。RCI方法在自动化计算机任务方面明显优于现有的LLM方法，并在MiniWoB++基准测试中超越了监督学习（SL）和强化学习（RL）方法。RCI方法使用每个任务仅有的少数示范，与最新的SL+RL方法相竞争。

    Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
    
[^162]: 混合自编码器用于自监督视觉表示学习

    Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])

    [http://arxiv.org/abs/2303.17152](http://arxiv.org/abs/2303.17152)

    本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。

    

    掩码自编码器（MAE）通过随机遮盖图像补丁和重建在各种视觉任务上展现出卓越的性能。然而，MAE的有效数据增强策略仍然是未解决的问题，不同于对比学习中的策略。本文研究了用于MAE的普遍混合增强。我们首先证明了朴素混合将由于相互信息的增加而降低模型性能。为了解决这个问题，我们提出了同源识别方法，一种辅助的预文本任务，不仅通过明确要求每个补丁识别同源补丁来缓解相互信息的增加，而且还可以执行面向对象的自监督预训练以获得更好的下游密集感知性能。通过大量的实验证明，我们的混合自编码器（MixedAE）在不同的遮蔽图像建模（MIM）增强中实现了最先进的转移结果。

    Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen
    
[^163]: 基于CNN-LSTM架构的 AIS 数据船舶轨迹关联算法

    A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data. (arXiv:2303.14068v1 [cs.LG])

    [http://arxiv.org/abs/2303.14068](http://arxiv.org/abs/2303.14068)

    本文提出了一种基于1D CNN-LSTM结构的船舶轨迹关联算法，采用多变量时间序列问题来解决

    

    在海洋监测中，区分正常和异常的船舶运动模式对于及时识别潜在威胁至关重要。为了实现该目标，需要使用轨迹关联算法，将由运动参数组成的时序观测结果与相应的船只关联。本文提出了一种基于1D CNN-LSTM结构的轨迹关联框架，将这一追踪任务视为多变量时间序列问题来解决。

    In marine surveillance, distinguishing between normal and anomalous vessel movement patterns is critical for identifying potential threats in a timely manner. Once detected, it is important to monitor and track these vessels until a necessary intervention occurs. To achieve this, track association algorithms are used, which take sequential observations comprising geological and motion parameters of the vessels and associate them with respective vessels. The spatial and temporal variations inherent in these sequential observations make the association task challenging for traditional multi-object tracking algorithms. Additionally, the presence of overlapping tracks and missing data can further complicate the trajectory tracking process. To address these challenges, in this study, we approach this tracking task as a multivariate time series problem and introduce a 1D CNN-LSTM architecture-based framework for track association. This special neural network architecture can capture the spat
    
[^164]: 什么让数据适合于局部连接神经网络？一种基于量子纠缠的必要且充分条件

    What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11249](http://arxiv.org/abs/2303.11249)

    本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。

    

    关于数据分布适用于深度学习的问题是一个基本的开放性问题。本文采用来自量子物理学的理论工具，针对包括卷积神经网络、循环神经网络和局部自注意力模型在内的广泛的局部连接神经网络，解决了这个问题。我们的主要理论结果是，在某些特征的规范划分下，当数据分布接受低量子纠缠时，特定的局部连接神经网络才能够准确地预测该数据分布。作为本结果的实际应用，我们导出了一种预处理方法，以增强数据分布适合局部连接神经网络的性能。在各种数据集上对广泛的模型进行实验，证明了我们的发现。我们希望我们使用量子纠缠将鼓励形式推理的物理工具来进一步采用。

    The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
    
[^165]: 一份带有深度神经网络评估的羊水胎儿模型超声数据集，用于评估胎儿方向，胎位和解剖学特征

    FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features. (arXiv:2303.07852v1 [eess.IV])

    [http://arxiv.org/abs/2303.07852](http://arxiv.org/abs/2303.07852)

    该论文提出了一个新的胎儿模型超声数据集FPUS23，用于确定胎儿方向，胎位和解剖学特征。研究结果表明，FPUS23可以为临床超声监测工作流程带来改善，以及可能开发一个家庭使用的基于超声的胎儿监护平台。

    

    超声成像是评估胎儿在妊娠期间的生长，发展和整体健康状况最突出的技术之一。为了改善临床工作流程并可能开发一个家庭使用的基于超声的胎儿监护平台，我们提出了一个新颖的胎儿模型超声数据集FPUS23，它可用于确定（1）用于估计胎儿生物计量值的正确诊断平面，（2）胎儿方向，（3）它们的解剖结构和（4）23周孕龄时胎儿模型解剖学的边界框。整个数据集由15,728张图像组成，用于训练建立在ResNet34骨干网络上的四种不同的深度神经网络模型，以检测上述胎儿特征和使用情况。我们还评估了使用我们的FPUS23数据集训练的模型。

    Ultrasound imaging is one of the most prominent technologies to evaluate the growth, progression, and overall health of a fetus during its gestation. However, the interpretation of the data obtained from such studies is best left to expert physicians and technicians who are trained and well-versed in analyzing such images. To improve the clinical workflow and potentially develop an at-home ultrasound-based fetal monitoring platform, we present a novel fetus phantom ultrasound dataset, FPUS23, which can be used to identify (1) the correct diagnostic planes for estimating fetal biometric values, (2) fetus orientation, (3) their anatomical features, and (4) bounding boxes of the fetus phantom anatomies at 23 weeks gestation. The entire dataset is composed of 15,728 images, which are used to train four different Deep Neural Network models, built upon a ResNet34 backbone, for detecting aforementioned fetus features and use-cases. We have also evaluated the models trained using our FPUS23 da
    
[^166]: 元学习控制变量：有限数据中方差缩减的方法

    Meta-learning Control Variates: Variance Reduction with Limited Data. (arXiv:2303.04756v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.04756](http://arxiv.org/abs/2303.04756)

    该论文提出了一种元学习控制变量的方法，可在有限数据的情况下减小蒙特卡罗估计器的方差，并对多个任务进行处理。

    

    控制变量是减小蒙特卡罗估计器方差的有力工具，但在样本数量较小的情况下构建有效的控制变量可能具有挑战性。本文表明，当需要计算大量相关积分时，即使每个任务的样本数很少，也可以利用这些积分任务之间的相似性来提高性能。我们所提出的元学习CV（Meta-CVs）方法可用于处理数百个或数千个任务，并通过实证评估表明，在这种情况下，Meta-CVs可以显著减小方差。我们的理论分析确定了Meta-CVs成功训练的一般条件。

    Control variates can be a powerful tool to reduce the variance of Monte Carlo estimators, but constructing effective control variates can be challenging when the number of samples is small. In this paper, we show that when a large number of related integrals need to be computed, it is possible to leverage the similarity between these integration tasks to improve performance even when the number of samples per task is very small. Our approach, called meta learning CVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our empirical assessment indicates that Meta-CVs can lead to significant variance reduction in such settings, and our theoretical analysis establishes general conditions under which Meta-CVs can be successfully trained.
    
[^167]: 迭代修正的外推控制序列生成

    Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04562](http://arxiv.org/abs/2303.04562)

    本文提出了一种名为ICE的新方法来解决外推控制序列生成问题，该方法使用迭代控制编辑技术，能够在自动设计领域，特别是药物研究领域取得较优的性能表现。

    

    本研究探讨了外推控制生成问题，即生成属性值超出训练数据范围的序列。在自动设计领域，尤其是药物研究领域，这个任务至关重要，目标是设计出比现有序列更好（例如更稳定）的新型蛋白质。因此，按照定义，目标序列及其属性值超出训练分布，挑战现有直接生成目标序列方法。本研究提出了迭代控制外推（ICE）方法，通过迭代地对序列进行局部编辑来实现外推。我们使用合成的序列对对模型进行训练，演示微小的属性值改进。自然语言任务（情感分析）和两个蛋白质工程任务（ACE2稳定性和AAV适应性）的结果表明，ICE方法明显优于现有的最先进方法。

    We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
    
[^168]: 深度神经网络的流式主动学习

    Streaming Active Learning with Deep Neural Networks. (arXiv:2303.02535v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02535](http://arxiv.org/abs/2303.02535)

    本文提出了VeSSAL算法，用于在流式设置下对深度神经网络进行批量主动学习。该算法不需要提前获得整个数据集的离线访问权限，可以从遇到的点的样本组中抽样查询标签。该方法可以在不确定性和查询样本多样性之间进行权衡，以达到所需的查询率，拓展了深度神经网络的应用范围。

    

    主动学习最自然的形式或许是在线学习问题。然而，以前的使用深度神经网络的主动学习方法假定提前获得整个数据集的离线访问权限。本文提出了一种新的算法VeSSAL，用于在流式设置下批量进行深度神经网络的主动学习，该算法根据遇到的点的样本组抽样查询标签。我们的方法在不需要手动调整超参数的情况下，在不确定性和查询样本的多样性之间进行权衡，以达到所需的查询率。总的来说，我们将深度神经网络的适用范围扩大到更加实际的主动学习场景中，比如涉及HCI和大型裂变数据集的应用。

    Active learning is perhaps most naturally posed as an online learning problem. However, prior active learning approaches with deep neural networks assume offline access to the entire dataset ahead of time. This paper proposes VeSSAL, a new algorithm for batch active learning with deep neural networks in streaming settings, which samples groups of points to query for labels at the moment they are encountered. Our approach trades off between uncertainty and diversity of queried samples to match a desired query rate without requiring any hand-tuned hyperparameters. Altogether, we expand the applicability of deep neural networks to realistic active learning scenarios, such as applications relevant to HCI and large, fractured datasets.
    
[^169]: 机器学习模型是否能学习从数据中推断出的统计规则？

    Do Machine Learning Models Learn Statistical Rules Inferred from Data?. (arXiv:2303.01433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01433](http://arxiv.org/abs/2303.01433)

    本文提出了一个框架SQRL，可以无监督地从模型的训练数据中推断出统计规则；在分类、目标检测和数据填充等任务中，最先进的模型通常会违反这些规则，但是通过在测试时间内对模型进行适应，违规行为可以显著减少并提高模型的性能。

    

    机器学习模型可能会在大量数据中隐藏一些重要的错误，而这些错误通常违反了人类直觉的规则。然而，以人类知识为基础的规则往往不易扩展或正式化。因此，我们提出了一种框架SQRL，它将基于逻辑的方法与统计推断相结合，无需监督即可从模型的训练数据中推导出这些规则，并量化模型已经学习到这些规则的程度。我们进一步展示了如何在测试时间内调整模型以减少规则违规并产生更连贯的预测结果。SQRL可以在视觉、表格和语言场景下的数据集中生成多达30万条规则。我们发现最先进的分类、目标检测和数据填充模型违反了这些规则高达158K次。而测试时间的适应可以将这些违规行为减少高达68.7%，并提高相对性能高达32%。

    Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model's training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7% with relative performance improvement up to 32%. SQRL is available a
    
[^170]: Q-Flow：基于正则化流量的开放量子动力学微分方程生成建模

    Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows. (arXiv:2302.12235v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2302.12235](http://arxiv.org/abs/2302.12235)

    本文研究如何通过将开放量子系统动力学重构为概率分布$Q$的偏微分方程（PDE）从而解决对正则化流的限制，在建模上实现无缝连接。通过此方法，我们可以使用现成的深度生成式模型（如正则化流）对Q函数进行建模。

    

    研究开放量子系统动态可以在基础物理学以及应用于量子工程和量子计算方面实现突破。由于密度矩阵$\rho$ 是描述此类系统动态的基本工具，其维数很高，因此定制的深度生成式神经网络对于建模$\rho$至关重要。然而，$\rho$的复数性质和正则化限制，以及其复杂的动态特性，阻碍了开放量子系统与深度生成式建模的最新进展之间的流畅连接。在本文中，我们通过将开放量子系统动力学重构为相应概率分布 $Q$ 的偏微分方程（PDE），即Husimi Q函数，从而解决了这一限制。因此，我们使用现成的深度生成式模型（如正则化流）无缝地对Q函数进行建模。此外，我们还开发了新方法来学习正则化流动演化控制。

    Studying the dynamics of open quantum systems can enable breakthroughs both in fundamental physics and applications to quantum engineering and quantum computation. Since the density matrix $\rho$, which is the fundamental description for the dynamics of such systems, is high-dimensional, customized deep generative neural networks have been instrumental in modeling $\rho$. However, the complex-valued nature and normalization constraints of $\rho$, as well as its complicated dynamics, prohibit a seamless connection between open quantum systems and the recent advances in deep generative modeling. Here we lift that limitation by utilizing a reformulation of open quantum system dynamics to a partial differential equation (PDE) for a corresponding probability distribution $Q$, the Husimi Q function. Thus, we model the Q function seamlessly with off-the-shelf deep generative models such as normalizing flows. Additionally, we develop novel methods for learning normalizing flow evolution govern
    
[^171]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^172]: 带有敏感属性不确定性的群体公平性

    Group Fairness with Uncertainty in Sensitive Attributes. (arXiv:2302.08077v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08077](http://arxiv.org/abs/2302.08077)

    该论文提出了一种基于自助法的算法来解决存在敏感属性不确定性的群体公平性问题，该算法在真实的信用贷款数据集上表现出优异的性能。

    

    在高风险应用中，学习一个公平的预测模型对于减少针对少数群体的有偏决策至关重要。学习这样一个模型的常见方法是解决一个优化问题，该问题在适当的群体公平性约束下最大化模型的预测能力。然而，在实际应用中，敏感属性通常会缺失或存在噪声，从而导致不确定性。我们证明，仅在不确定的敏感属性上实施公平约束可能无法实现训练时没有不确定性的模型所达到的公平水平。为了克服这个限制，我们提出了一种基于自助法的算法，该算法在存在敏感属性不确定性的情况下实现了目标的公平水平。该算法通过高斯分析进行引导，实现“独立概念”的公平性，我们提出了一个鲁棒的二次约束二次问题，以确保具有不确定敏感属性的严格公平性保证。我们在一个真实的信用贷款数据集上评估了我们的算法，结果显示出当敏感属性存在不确定性时，它优于现有的公平学习算法。

    Learning a fair predictive model is crucial to mitigate biased decisions against minority groups in high-stakes applications. A common approach to learn such a model involves solving an optimization problem that maximizes the predictive power of the model under an appropriate group fairness constraint. However, in practice, sensitive attributes are often missing or noisy resulting in uncertainty. We demonstrate that solely enforcing fairness constraints on uncertain sensitive attributes can fall significantly short in achieving the level of fairness of models trained without uncertainty. To overcome this limitation, we propose a bootstrap-based algorithm that achieves the target level of fairness despite the uncertainty in sensitive attributes. The algorithm is guided by a Gaussian analysis for the independence notion of fairness where we propose a robust quadratically constrained quadratic problem to ensure a strict fairness guarantee with uncertain sensitive attributes. Our algorithm
    
[^173]: 悬崖学习

    Cliff-Learning. (arXiv:2302.07348v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07348](http://arxiv.org/abs/2302.07348)

    本研究探究了基于基础模型的迁移学习在低数据状态下的数据缩放，发现了一种称为悬崖学习的现象，它反映了学习算法的先验知识与任务之间的兼容程度。

    

    我们研究了基于基础模型进行迁移学习在低下游数据状态下的数据缩放。我们观察到了一个有趣的现象，我们称之为悬崖学习。悬崖学习是指在数据缩放法则的某些区域中，性能的提升速度快于幂律速度的现象（即在对数缩放图上的凹形区域）。我们对基础模型的悬崖学习进行了深入调查并研究了这一现象的玩具模型。我们观察到悬崖学习的程度反映了学习算法的先验知识和所学任务之间的兼容程度。

    We study the data-scaling of transfer learning from foundation models in the low-downstream-data regime. We observe an intriguing phenomenon which we call cliff-learning. Cliff-learning refers to regions of data-scaling laws where performance improves at a faster than power law rate (i.e. regions of concavity on a log-log scaling plot). We conduct an in-depth investigation of foundation-model cliff-learning and study toy models of the phenomenon. We observe that the degree of cliff-learning reflects the degree of compatibility between the priors of a learning algorithm and the task being learned.
    
[^174]: 尖度与泛化关系的现代研究

    A Modern Look at the Relationship between Sharpness and Generalization. (arXiv:2302.07011v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07011](http://arxiv.org/abs/2302.07011)

    研究表明，尖度与泛化关系并不密切相关，而与某些训练参数呈正相关或负相关。

    

    尖度是一种有希望与深度网络泛化相关的量，当在训练过程中优化时，可以改善泛化能力。然而，标准尖度并不是神经网络重参数化不变的，并因此提出了重参数化不变尖度定义，最著名的是自适应尖度。本文全面探索各种自适应尖度定义在从头开始训练Imagenet和CIFAR-10到微调Imagenet和MNLI的CLIP和BERT的各种设置中的泛化性能。我们主要关注变形器，尽管它们被广泛使用，但对其尖度的了解仍很有限。总体而言，我们观察到尖度与泛化关系不大，而与一些训练参数（如学习率）呈正相关或负相关。

    Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness (Kwon et al., 2021). But does it really capture generalization in modern practical settings? We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on ImageNet and CIFAR-10 to fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlat
    
[^175]: 多无人机系统中的合作移动访问的量子多智能体演员-评论网络

    Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems. (arXiv:2302.04445v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.04445](http://arxiv.org/abs/2302.04445)

    本文提出了一种名为量子多智能体演员-评论网络的新算法，用于自主构建一个强大的移动访问系统，利用多个无人机，并利用量子计算的原则以提高其训练过程和推理能力。

    

    本文提出了一种名为量子多智能体演员-评论网络（QMACN）的新算法，用于自主构建一个强大的移动访问系统，利用多个无人机（UAV）。在促进多个无人机之间的协作方面，多智能体强化学习（MARL）技术的应用被认为是一种有前途的方法。这些方法使无人机能够集体学习，在共享环境中优化它们的行动，最终导致更有效的合作行为。此外，我们的研究运用了量子计算（QC）的原则，以增强涉及的无人机的训练过程和推理能力。通过利用量子计算的独特计算优势，我们的方法旨在提高无人机系统的整体有效性。然而，使用QC引入可扩展性挑战，因为近中间规模量子（NISQ）限制。

    This paper proposes a novel algorithm, named quantum multi-agent actor-critic networks (QMACN) for autonomously constructing a robust mobile access system employing multiple unmanned aerial vehicles (UAVs). In the context of facilitating collaboration among multiple unmanned aerial vehicles (UAVs), the application of multi-agent reinforcement learning (MARL) techniques is regarded as a promising approach. These methods enable UAVs to learn collectively, optimizing their actions within a shared environment, ultimately leading to more efficient cooperative behavior. Furthermore, the principles of a quantum computing (QC) are employed in our study to enhance the training process and inference capabilities of the UAVs involved. By leveraging the unique computational advantages of quantum computing, our approach aims to boost the overall effectiveness of the UAV system. However, employing a QC introduces scalability challenges due to the near intermediate-scale quantum (NISQ) limitation ass
    
[^176]: 两种损失比一种更好：使用更便宜的代理加快优化

    Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy. (arXiv:2302.03542v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03542](http://arxiv.org/abs/2302.03542)

    该论文提出了一种代理算法，通过使用一个易于访问的函数作为代理，可以以与原函数梯度下降相匹配的速度收敛，从而显著提高样本效率，并在机器学习中具有许多潜在应用。

    

    我们提出了一种算法，通过使用相关的、易于访问的函数作为代理，来最小化一个难以计算梯度的目标函数。我们的算法基于代理的近似近端点迭代，结合来自目标函数的相对较少的随机梯度。当目标函数与代理之间的差异是$\delta$-平滑时，我们的算法保证以与$\delta$-平滑目标函数上的随机梯度下降相匹配的速率收敛，这可以显著提高样本效率。我们的算法在机器学习中有许多潜在应用，并提供了一种利用合成数据、物理模拟器、混合公共和私人数据等的原则性方法。

    We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.
    
[^177]: 通过隐形水印保护语言生成模型

    Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.03162](http://arxiv.org/abs/2302.03162)

    本文提出了一种名为 GINSEW 的新方法，通过将秘密信号注入到每个目标标记的解码步骤的概率向量中，保护文本生成模型，有效识别出侵权行为，对模型的影响很小。

    

    语言生成模型是许多应用的有力支持者。许多这样的模型提供免费或经济实惠的 API 访问，这使它们可能受到模型抽取攻击的威胁。为了保护知识产权并确保这些模型的公正使用，已经提出了各种技术，例如词汇水印和同义词替换。然而，这些方法可能会被明显的对策如“同义词随机化”等所抵消。为了解决这个问题，我们提出了 GINSEW，一种新的方法，用于通过蒸馏保护文本生成模型。我们的方法的关键思想是将秘密信号注入到每个目标标记的解码步骤的概率向量中。然后，我们可以通过探测嫌疑的模型来检测秘密消息是否由受保护的模型蒸馏而来。实验结果表明，GINSEW 可以有效地识别出侵权行为，对生成模型的影响极小。

    Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the ge
    
[^178]: 扁平化贝叶斯神经网络

    Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02713](http://arxiv.org/abs/2302.02713)

    本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。

    

    贝叶斯神经网络（BNN）通过对模型参数施加先验分布并基于观测数据推断后验分布，为深度学习模型提供了概率解释。从后验分布中采样的模型可用于提供集成预测和量化预测不确定性。众所周知，具有较低尖度的深度学习模型具有更好的泛化能力。然而，现有的后验推论对于尖度/扁平化并不具备意识性，可能导致从其采样的模型具有较高的尖度。在本文中，我们针对扁平化对后验进行了理论、贝叶斯设定和变分推断方法的开发。具体地，我们从扁平化意义上推断的模型以及估计该扁平化意义后验的最佳近似后验，具有更好的扁平化性质，因此可能具有更高的泛化能力。我们对几个基准数据集进行了实验评估，并证明我们的方法在样本外准确性和不确定性估计方面优于现有方法。

    Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime
    
[^179]: 从对抗角度重新思考鲁棒对比学习

    Rethinking Robust Contrastive Learning from the Adversarial Perspective. (arXiv:2302.02502v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02502](http://arxiv.org/abs/2302.02502)

    研究揭示了在标准训练网络中对抗性和干净性表示之间存在重大差异，对抗训练能够缓解这种差异并促进表示收敛到一个通用集合中，无论使用哪种学习方案。增加对抗和干净表示之间的相似度可以增强网络的鲁棒性。

    

    为了推进对鲁棒深度学习的理解，我们探讨了对抗训练对自监督和监督对比学习以及监督学习的影响。我们的分析揭示了标准训练网络中对抗性和干净性表示之间存在重大差异，这种差异得到了显著缓解并促进了表示收敛到一个通用集合中，无论使用哪种学习方案。此外，增加对抗和干净表示之间的相似度，特别是在网络末端附近，可以增强网络的鲁棒性。这些发现为设计和训练高效而鲁棒的深度学习网络提供了有价值的见解。我们的代码已在\url{https://github.com/softsys4ai/CL-Robustness}上发布。

    To advance the understanding of robust deep learning, we delve into the effects of adversarial training on self-supervised and supervised contrastive learning alongside supervised learning. Our analysis uncovers significant disparities between adversarial and clean representations in standard-trained networks across various learning algorithms. Remarkably, adversarial training mitigates these disparities and fosters the convergence of representations toward a universal set, regardless of the learning scheme used. Additionally, increasing the similarity between adversarial and clean representations, particularly near the end of the network, enhances network robustness. These findings offer valuable insights for designing and training effective and robust deep learning networks. Our code is released at \textcolor{magenta}{\url{https://github.com/softsys4ai/CL-Robustness}}.
    
[^180]: 逆向可辨识双射因果模型

    Counterfactual Identifiability of Bijective Causal Models. (arXiv:2302.02228v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02228](http://arxiv.org/abs/2302.02228)

    本文研究逆向可辨识双射因果模型，确立了其在三种常见因果结构下的逆向可辨识性，提出了一种实用的学习方法，可以用于有效的逆向预测估计。

    

    本文研究具有双射生成机制（BGM）的因果模型中的逆向可辨识性，其中BGM是一种广泛使用于文献中的因果模型类。我们确立了三种常见的具有未观察到混杂变量因果结构的逆向可辨识性，并提出了一种实用的学习方法，将学习BGM转化为结构化生成建模。学习到的BGM可以实现有效的逆向预测估计，并可以使用各种深度条件生成模型获得。我们在一个视觉任务中评估了我们的技术，并展示了它在现实世界视频流媒体仿真任务中的应用。

    We study counterfactual identifiability in causal models with bijective generation mechanisms (BGM), a class that generalizes several widely-used causal models in the literature. We establish their counterfactual identifiability for three common causal structures with unobserved confounding, and propose a practical learning method that casts learning a BGM as structured generative modeling. Learned BGMs enable efficient counterfactual estimation and can be obtained using a variety of deep conditional generative models. We evaluate our techniques in a visual task and demonstrate its application in a real-world video streaming simulation task.
    
[^181]: 基于关系Weisfeiler-Leman的链路预测理论

    A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02209](http://arxiv.org/abs/2302.02209)

    本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。

    

    图神经网络是用于图结构数据表示学习的重要模型。尽管我们已经很好地理解了这些模型在简单图上的能力和局限性，但对于知识图谱，我们的理解仍然不完整。本文的目标是为知识图谱中的图神经网络提供系统性的理解，以解决链路预测等重要任务。我们的分析涉及一种统一的视角、看似不相关的模型，并解锁了一系列其他模型。通过相应的关系Weisfeiler-Leman算法，表征了各种模型的表达能力。此分析被扩展以对图神经网络类别捕捉的函数类进行精确逻辑描述。提出的理论发现解释了一些广泛采用的实际设计选择的优点，并得到了经验验证。

    Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
    
[^182]: 异构数据上顺序分割学习的收敛性分析

    Convergence Analysis of Sequencial Split Learning on Heterogeneous Data. (arXiv:2302.01633v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01633](http://arxiv.org/abs/2302.01633)

    本文推导出了顺序分割学习在异构数据上收敛的保证，并且证明了它在异构数据上优于联邦平均算法。

    

    联邦学习（FL）和分割学习（SL）是分布式机器学习的两种流行范例。通过将计算密集部分卸载到服务器，SL对于在资源受限设备上进行深层模型训练非常有前途，但仍缺乏严格的收敛性分析。在本文中，我们推导出了顺序SL（SSL，进行顺序模型训练的SL基本情形）在异构数据上对于强化/一般/非凸目标的收敛保证。值得注意的是，所得到的保证表明，在异构数据上，SSL比联邦平均（FedAvg，FL中最流行的算法）更好。我们在极端异构数据上通过实验证实了这个反直觉的分析结果。

    Federated Learning (FL) and Split Learning (SL) are two popular paradigms of distributed machine learning. By offloading the computation-intensive portions to the server, SL is promising for deep model training on resource-constrained devices, yet still lacking of rigorous convergence analysis. In this paper, we derive the convergence guarantees of Sequential SL (SSL, the vanilla case of SL that conducts the model training in sequence) for strongly/general/non-convex objectives on heterogeneous data. Notably, the derived guarantees suggest that SSL is better than Federated Averaging (FedAvg, the most popular algorithm in FL) on heterogeneous data. We validate the counterintuitive analysis result empirically on extremely heterogeneous data.
    
[^183]: 双向传播：用二元神经元加速对比海比学习

    Dual Propagation: Accelerating Contrastive Hebbian Learning with Dyadic Neurons. (arXiv:2302.01228v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01228](http://arxiv.org/abs/2302.01228)

    本论文提出了一种双向传播的方法，使用具有两个内在状态的二元组神经元模型来加速对比海比学习，并且只需要单个推断阶段，可降低计算时间和能量消耗。

    

    基于活动差异的学习算法，如对比海比学习和平衡传播，已被提出作为生物合理的替代方案，用于替代误差反向传播。然而，在传统的数字芯片上，这些算法需要解决一个昂贵的推断问题两次，使这些方法比反向传播慢两个数量级以上。在模拟情况下，平衡传播可能是快速和能量高效的学习方法，但是状态仍然需要推断和存储两次。受提升的神经网络和分室神经元模型的启发，我们提出了一种简单的基于能量的分室神经元模型，称为双向传播，其中每个神经元是一个由两个内在状态组成的二元组。在推断时，这些内在状态通过它们的差异和平均值编码误差/活动的二元性。这种方法的优点是只需要单个推断阶段，并且推断可以由神经元本身进行，从而减少计算时间和能量消耗。

    Activity difference based learning algorithms-such as contrastive Hebbian learning and equilibrium propagation-have been proposed as biologically plausible alternatives to error back-propagation. However, on traditional digital chips these algorithms suffer from having to solve a costly inference problem twice, making these approaches more than two orders of magnitude slower than back-propagation. In the analog realm equilibrium propagation may be promising for fast and energy efficient learning, but states still need to be inferred and stored twice. Inspired by lifted neural networks and compartmental neuron models we propose a simple energy based compartmental neuron model, termed dual propagation, in which each neuron is a dyad with two intrinsic states. At inference time these intrinsic states encode the error/activity duality through their difference and their mean respectively. The advantage of this method is that only a single inference phase is needed and that inference can be 
    
[^184]: 利用扩散模型进行干预和反事实推断

    Interventional and Counterfactual Inference with Diffusion Models. (arXiv:2302.00860v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00860](http://arxiv.org/abs/2302.00860)

    本论文提出了基于扩散模型的因果模型 (DCM)，它可以在只有观测数据和因果图可用的情况下进行干预和反事实推断，其具有较好的表现。同时，论文还提供了一种分析反事实估计的方法，可以应用于更广泛的场景。

    

    我们考虑在只有观测数据和因果图可用的因果充分设置中回答观测、干预和反事实查询的问题。利用扩散模型的最新发展，我们引入了基于扩散的因果模型 (DCM)，来学习生成独特的潜在编码的因果机制。这些编码使我们能够在干预下直接采样和进行反事实推断。扩散模型在这里是一个自然的选择，因为它们可以将每个节点编码为一个代表外生噪声的潜在表示。我们的实证评估表明，在回答因果查询方面，与现有的最先进方法相比，有显着的改进。此外，我们提供了理论结果，为分析一般编码器-解码器模型中的反事实估计提供一种方法，这对我们提出的方法以外的设置可能也有用。

    We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings. These encodings enable us to directly sample under interventions and perform abduction for counterfactuals. Diffusion models are a natural fit here, since they can encode each node to a latent representation that acts as a proxy for exogenous noise. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Furthermore, we provide theoretical results that offer a methodology for analyzing counterfactual estimation in general encoder-decoder models, which could be useful in settings beyond our proposed approach.
    
[^185]: 重新审视 Bellman Errors 用于离线模型选择

    Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00141](http://arxiv.org/abs/2302.00141)

    本文重新审视 Bellman Errors，发现之前的Bellman Errors 方法需要在特定条件下才能表现良好，同时提出了更准确的 MSBE 估计器，在离散控制任务方面表现出色。

    

    离线模型选择（OMS）即在只有已记录数据的情况下从众多策略中选择最佳策略，对于在实际环境下应用离线RL至关重要。一个经过广泛探讨的想法是根据相关Q函数的均方Bellman误差（MSBE）选择策略。然而，之前的研究一直在使用Bellman误差时无法获得足够的OMS性能，导致许多研究人员放弃此想法。为此，本文阐述了为什么之前的结果使用Bellman误差时会看到悲观的结果，并确定了基于Bellman误差的OMS算法将表现良好的条件。此外，我们开发了一个比之前方法更准确的MSBE的新的估计器。我们的估计器在不同的离散控制任务（包括 Atari 游戏）上获得了出色的OMS性能。

    Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
    
[^186]: 带有MCMC的差分隐私分布式贝叶斯线性回归

    Differentially Private Distributed Bayesian Linear Regression with MCMC. (arXiv:2301.13778v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13778](http://arxiv.org/abs/2301.13778)

    提出了一种带有 MCMC 的差分隐私分布式贝叶斯线性回归算法，提供了快速版本，具有计算上的优势，并在实际数据和模拟数据上进行数字实验，结果表明算法能够提供全面的估计和预测。

    

    我们提出了一种新颖的分布式差分隐私线性回归的贝叶斯推断框架。我们考虑到在多个参与方拥有部分数据并分享其部分的某些总结统计信息的分布式环境中。我们开发了一种新型的生成式统计模型，用于私下共享的统计信息，该模型利用了线性回归摘要统计信息之间的有用分布关系。回归系数的贝叶斯估计主要通过马尔可夫链蒙特卡罗算法进行，同时我们还提供了一种快速版本，以在一次迭代中执行贝叶斯估计。所提出的方法具有计算上的优势。我们在实际数据和模拟数据上提供了数字实验结果，这些结果表明所提出的算法能够提供全面的估计和预测。

    We propose a novel Bayesian inference framework for distributed differentially private linear regression. We consider a distributed setting where multiple parties hold parts of the data and share certain summary statistics of their portions in privacy-preserving noise. We develop a novel generative statistical model for privately shared statistics, which exploits a useful distributional relation between the summary statistics of linear regression. Bayesian estimation of the regression coefficients is conducted mainly using Markov chain Monte Carlo algorithms, while we also provide a fast version to perform Bayesian estimation in one iteration. The proposed methods have computational advantages over their competitors. We provide numerical results on both real and simulated data, which demonstrate that the proposed algorithms provide well-rounded estimation and prediction.
    
[^187]: 利用强化学习规划多种传染病干预方案

    Planning Multiple Epidemic Interventions with Reinforcement Learning. (arXiv:2301.12802v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12802](http://arxiv.org/abs/2301.12802)

    本文将找到最优化的疫情计划转化为马尔可夫决策过程，并利用强化学习算法来搜索最小化疾病和经济成本的计划。

    

    应对流行病需要制定计划，描述何时以及如何应用不同的干预措施，比如要求佩戴口罩、接种疫苗、关闭学校或工作场所等。最优的计划将以最小的生命损失、疾病负担和经济成本遏制疫情。在现实情况下，寻找最优计划是一个难以解决的计算问题。我们将这个问题表述为马尔可夫决策过程，并提出一种独特的方法，能够表示对于任何由常微分方程定义的疾病模型上的多个连续干预措施。我们展示了如何有效地应用最先进的演员-评论家强化学习算法（PPO和SAC），以在连续和复杂的状态空间中搜索最小化疾病和经济成本的计划。

    Combating an epidemic entails finding a plan that describes when and how to apply different interventions, such as mask-wearing mandates, vaccinations, school or workplace closures. An optimal plan will curb an epidemic with minimal loss of life, disease burden, and economic cost. Finding an optimal plan is an intractable computational problem in realistic settings. Policy-makers, however, would greatly benefit from tools that can efficiently search for plans that minimize disease and economic costs especially when considering multiple possible interventions over a continuous and complex action space given a continuous and equally complex state space. We formulate this problem as a Markov decision process. Our formulation is unique in its ability to represent multiple continuous interventions over any disease model defined by ordinary differential equations. We illustrate how to effectively apply state-of-the-art actor-critic reinforcement learning algorithms (PPO and SAC) to search fo
    
[^188]: 通过等变扩散定向氨基酸云生成新颖、可设计和多样化的蛋白质结构

    Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds. (arXiv:2301.12485v3 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.12485](http://arxiv.org/abs/2301.12485)

    本论文介绍了一种新型的蛋白质生成模型Genie，可以使用等变神经网络和扩散概率模型生成新颖、可设计、多样化的蛋白质结构。这将有助于开发新的蛋白质治疗和材料。

    

    蛋白质在细胞中发挥着各种功能过程。创造具有设计结构和功能的新蛋白质将有助于工程化细胞行为和开发基于蛋白质的治疗和材料。基于结构的蛋白质设计旨在寻找可设计（可以由蛋白质序列实现的结构），新颖（与天然蛋白质有不同的几何形状）和多样化（涵盖广泛几何形状范围）的结构。尽管蛋白质结构预测的进展使得预测新蛋白质序列的结构成为可能，但序列和结构的组合空间限制了搜索方法的实用性。生成模型提供了一种有力的替代方法，通过隐含地学习复杂数据分布的低维结构。在这里，我们利用最近噪声扩散概率模型和等变神经网络的进展，开发了名为Genie的蛋白质生成模型。

    Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of prote
    
[^189]: 平滑的非平稳连续赌博机

    Smooth Non-Stationary Bandits. (arXiv:2301.12366v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12366](http://arxiv.org/abs/2301.12366)

    本文提出了一种非平稳两臂赌博机问题的策略，能够处理平滑变化，并证明了该策略在二次Lipschitz连续的情况下的遗憾为 $\tilde O(T^{3/5})$。

    

    在许多在线决策应用中，环境都是非平稳的，因此使用能够处理变化的赌博算法至关重要。大多数现有方法是为了保护非平滑变化而设计的，仅受到总变差或时间上的Lipschitz性的限制，其中它们保证$\tilde \Theta(T^{2/3})$的遗憾。然而，在实践中，环境经常以平稳的方式改变，因此这种算法可能会在这些设置中产生比必要更高的遗憾，并且不利用变化率的信息。我们研究了一个非平稳的两臂赌博机问题，假设臂的平均回报是一个$\beta$-H\''older函数，即它是$(\beta-1)$次Lipschitz连续可微分的，我们展示了一个策略，对于$\beta=2$，它的遗憾为$\tilde O(T^{3/5})$，从而首次在平滑和非平滑之间进行了区分。我们通过一个任意$\Omg(T^{(\beta+1)/(2\beta+1)})$的下界来补充这个结果，说明了这个问题的困难程度。

    In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time, where they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice environments are often changing {\bf smoothly}, so such algorithms may incur higher-than-necessary regret in these settings and do not leverage information on the rate of change. We study a non-stationary two-armed bandits problem where we assume that an arm's mean reward is a $\beta$-H\"older function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously differentiable. We show the first separation between the smooth and non-smooth regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$. We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound for any int
    
[^190]: 用于采样分子晶体结构的刚体流

    Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11355](http://arxiv.org/abs/2301.11355)

    本文介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计。通过在单位四元数群上定义平滑和表现力强的流以及定义适当的密度，在旋转群上进行训练，我们可以成功地采样分子晶体结构。

    

    正则化流(NF)是一类强大的生成模型，由于其高度灵活和表现力，近年来广受欢迎。在本文中，我们介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计，例如晶体中的分子。我们的方法基于两个关键思想:首先，我们在单位四元数群上定义平滑和表现力强的流，从而可以捕捉刚体的连续旋转运动;其次，我们利用单位四元数的双覆盖特性，在旋转群上定义一个适当的密度。这确保我们的模型可以使用标准的基于似然方法或基于热力学目标密度的变分推断进行训练。我们通过训练两个分子示例的Boltzmann生成器来评估该方法，即四面体系统的多模态密度。

    Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
    
[^191]: 面向参数化偏微分方程的随机网格神经过程

    Random Grid Neural Processes for Parametric Partial Differential Equations. (arXiv:2301.11040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11040](http://arxiv.org/abs/2301.11040)

    本论文引入了一种新的随机网格神经算法，用于处理参数化偏微分方程，创新地将概率测度赋予空间域，形成高斯过程模型，提供了一种解决数据受噪声干扰问题的方法。

    

    我们引入了一类新的基于可扩展变分神经过程的具有空间随机物理学和数据信息的深度潜在模型，用于处理参数化偏微分方程(PDE)。我们通过将概率测度赋予空间域来实现这一目的，这使我们能够将配点网格视为随机变量来边际化。通过适应这种空间统计视图，我们以产生解场的高斯过程模型作为结果，解决参数PDE的正向和反向问题。这些随机网格的实现为反向物理信息深度学习框架提出了一系列独特的挑战，我们提出了一种名为Grid Invariant Convolutional Networks(GICNets)的新架构来克服这些挑战。我们进一步展示了如何以原则性的方式将有噪声的数据纳入我们的物理知识模型中，以改进对一些可能有可用数据但测量结果被噪声污染的问题的预测能力。

    We introduce a new class of spatially stochastic physics and data informed deep latent models for parametric partial differential equations (PDEs) which operate through scalable variational neural processes. We achieve this by assigning probability measures to the spatial domain, which allows us to treat collocation grids probabilistically as random variables to be marginalised out. Adapting this spatial statistics view, we solve forward and inverse problems for parametric PDEs in a way that leads to the construction of Gaussian process models of solution fields. The implementation of these random grids poses a unique set of challenges for inverse physics informed deep learning frameworks and we propose a new architecture called Grid Invariant Convolutional Networks (GICNets) to overcome these challenges. We further show how to incorporate noisy data in a principled manner into our physics informed model to improve predictions for problems where data may be available but whose measurem
    
[^192]: 基于机器学习的超分辨率分析：流体流动研究综述

    Super-Resolution Analysis via Machine Learning: A Survey for Fluid Flows. (arXiv:2301.10937v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2301.10937](http://arxiv.org/abs/2301.10937)

    本文调研了基于机器学习的超分辨率分析在涡流流动中的应用研究，并提供了物理启发的模型设计来成功重建流场。该研究可用于处理数值和实验流动数据的超分辨率分析。

    

    本文概述了基于机器学习的超分辨率重构用于涡流流动的研究。超分辨率旨在从低分辨率数据中获取高分辨率的流场，通常用于图像重构。除了调研各种最新的超分辨应用外，我们还提供了两维衰减同性湍流的超分辨率分析案例研究。我们展示了基于物理启发的模型设计使得从空间受限测量中成功地重建了涡流流场。我们还讨论了基于机器学习的超分辨率分析在流体流动应用中的挑战和展望。本研究得出的见解可用于处理数值和实验流动数据的超分辨率分析。

    This paper surveys machine-learning-based super-resolution reconstruction for vortical flows. Super resolution aims to find the high-resolution flow fields from low-resolution data and is generally an approach used in image reconstruction. In addition to surveying a variety of recent super-resolution applications, we provide case studies of super-resolution analysis for an example of two-dimensional decaying isotropic turbulence. We demonstrate that physics-inspired model designs enable successful reconstruction of vortical flows from spatially limited measurements. We also discuss the challenges and outlooks of machine-learning-based super-resolution analysis for fluid flow applications. The insights gained from this study can be leveraged for super-resolution analysis of numerical and experimental flow data.
    
[^193]: 用于评估CNN预测的可信度分数

    Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08839](http://arxiv.org/abs/2301.08839)

    本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.

    由于卷积神经网络（CNN）的黑盒特性，无法在操作期间持续验证CNN，这使得开发人员和监管机构难以对使用CNN的自主系统的部署获得信心。在操作期间，了解CNN的预测何时可信或可疑对安全至关重要。基本方法是使用模型的输出置信度分数来评估预测是否可信或可疑。然而，模型的置信度分数是来自黑盒计算的结果，因此缺乏透明度，使得很难将可信度归因于预测。我们引入了可信度分数（TS），这是一种简单的度量标准，提供了一种更透明和有效的方式来提供CNN预测的信心。该度量标准通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
    
[^194]: Tracr: 编译变压器模型作为可解释性实验室

    Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05062](http://arxiv.org/abs/2301.05062)

    Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。

    

    我们展示了如何将可读性强的程序编译成标准的仅解码变压器模型。我们的编译器Tracr生成具有已知结构的模型，可以用于设计实验。例如，我们使用它来研究执行多步算法的变压器中的“叠加”。此外，Tracr编译模型的已知结构可以作为评估可解释方法的真实基准。通常，由于变压器学习的“程序”是未知的，因此不清楚解释是否成功。我们通过实现和检查包括计算令牌频率、排序和括号检查在内的程序来演示我们的方法。我们在https://github.com/deepmind/tracr提供了Tracr的开源实现。

    We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
    
[^195]: 深度线性网络中的神经塌陷:从平衡到不平衡的数据

    Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00437](http://arxiv.org/abs/2301.00437)

    研究者证明对于均方误差和交叉熵损失，深度线性网络中出现的全局解在不同数据上都具有神经塌陷的特性，即最后一层特征会崩溃为类均值，而这些类均值是等角紧框架的顶点。

    

    现代深度神经网络在图像分类和自然语言处理等任务中表现出色，但令人惊讶的是，这些具有大量参数的复杂系统在训练到收敛时，它们的最后一层特征和分类器在经典数据集上表现出相同的结构性质。特别地，观察到最后一层特征会崩溃为类均值，并且这些类均值是等角紧框架(simplex Equiangular Tight Frame)的顶点。这种现象被称为神经塌陷(NC)。最近的论文理论上证明了在简化的“无约束特征模型”训练问题的全局最小值中出现了$\mathcal{NC}$。在这个语境下，我们进一步证明了在常用的均方误差(MSE)和交叉熵(CE)损失下，深度线性网络中也会发生$\mathcal{NC}$现象，表明全局解在不同数据上都具有$\mathcal{NC}$的特性。

    Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
    
[^196]: 具有全局收敛保证的鲁棒MDP中的策略梯度

    Policy Gradient in Robust MDPs with Global Convergence Guarantee. (arXiv:2212.10439v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10439](http://arxiv.org/abs/2212.10439)

    本文提出了一个新的双循环鲁棒策略梯度（DRPG）算法，是首个通用的RMDP策略梯度方法，通过单调地减少近似误差来保证在表格RMDP中全局最优策略的收敛性。

    

    鲁棒马尔可夫决策过程（RMDP）提供了一个计算可靠策略以应对模型错误的有前途的框架。许多成功的强化学习算法基于各种变化的策略梯度方法，但将这些方法适用于RMDP是具有挑战性的。因此，RMDP对于大型实际领域的适用性仍然有限。本文提出了一种新的双循环鲁棒策略梯度（DRPG），这是首个通用的RMDP策略梯度方法。与之前的鲁棒策略梯度算法相比，DRPG单调地减少近似误差，以保证在表格RMDP中收敛到全局最优策略。我们介绍了一种新颖的参数转移核，通过基于梯度的方法解决内部循环鲁棒策略。最后，我们的数值实验证明了我们新算法的效用，并确认了它的全局收敛特性。

    Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.
    
[^197]: 通过查询计算树优化在知识图谱上回答复杂逻辑查询

    Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization. (arXiv:2212.09567v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09567](http://arxiv.org/abs/2212.09567)

    提出了 QTO（查询计算树优化）以有效地回答复杂逻辑查询，通过查询计算树上的正反向传播找到了精确的最优解，并利用了查询计算树中的独立性来减少搜索空间。

    

    在不完整的知识图谱上回答复杂逻辑查询是一项具有挑战性的任务，并已得到广泛研究。嵌入式方法需要训练复杂查询，并且不能很好地泛化到分布查询结构之外。最近的工作将此任务作为端到端优化问题进行了框架化，只需要预训练的链接预测器。然而，由于指数级的组合搜索空间，最优解只能被近似，限制了最终的准确性。在本文中，我们提出了 QTO（查询计算树优化），可以有效地找到精确的最优解。QTO通过在树状计算图（即查询计算树）上进行正反向传播来找到最优解。特别地，QTO利用了查询计算树中编码的独立性来减少搜索空间，在优化过程中仅涉及局部计算。在三个数据集上的实验表明，QTO获得了令人满意的结果。

    Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains sta
    
[^198]: 协变量转移的祝福和诅咒：对抗学习动态、方向收敛和平衡的影响

    Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics, Directional Convergence, and Equilibria. (arXiv:2212.02457v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.02457](http://arxiv.org/abs/2212.02457)

    协变量转移和对抗扰动对统计学习的稳健性提出了挑战。本文在无限维度的情况下研究了对抗协变量转移对外推区域的影响以及其对后续学习的平衡的影响。

    

    协变量分布转移和对抗扰动对传统统计学习框架的稳健性提出了挑战：测试协变量分布中的轻微转移能显著影响基于训练分布学习的统计模型性能。当外推发生时，即协变量转移到训练分布稀缺的区域时，模型性能通常会降低，因此，学习模型信息很少。为了稳健性和正则化考虑，建议采用对抗扰动技术，然而，需要对给定学习模型时对抗协变量转移的外推区域进行仔细研究。本文在无限维度的设置中精确刻画了外推区域，在回归和分类方面进行了研究。研究了对抗协变量转移对随后的平衡学习的影响。

    Covariate distribution shifts and adversarial perturbations present robustness challenges to the conventional statistical learning framework: mild shifts in the test covariate distribution can significantly affect the performance of the statistical model learned based on the training distribution. The model performance typically deteriorates when extrapolation happens: namely, covariates shift to a region where the training distribution is scarce, and naturally, the learned model has little information. For robustness and regularization considerations, adversarial perturbation techniques are proposed as a remedy; however, careful study needs to be carried out about what extrapolation region adversarial covariate shift will focus on, given a learned model. This paper precisely characterizes the extrapolation region, examining both regression and classification in an infinite-dimensional setting. We study the implications of adversarial covariate shifts to subsequent learning of the equi
    
[^199]: SinDDM: 一种单图像去噪扩散模型

    SinDDM: A Single Image Denoising Diffusion Model. (arXiv:2211.16582v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16582](http://arxiv.org/abs/2211.16582)

    本文介绍了一种称为SinDDM的单图像去噪扩散模型。该模型使用单张图像进行训练，并可生成高质量的任意维度的样本。此外，它还可以通过外部监督进行指导，可用于多种任务，包括样式转换和文本引导生成。

    

    去噪扩散模型（DDMs）在图像生成、编辑和修复方面取得了惊人的性能提升。但是，现有的DDMs需要使用非常大的数据集进行训练。本文介绍一种在单张图像上训练DDM的框架，称之为SinDDM。我们的方法通过使用多尺度扩散过程学习训练图像的内部统计信息。为了推动反向扩散过程，我们使用了一个完全卷积的轻量级去噪器，它受噪声水平和尺度的调节。该体系结构允许以粗略到细节的方式生成任意维度的样本。正如我们所说明的，SinDDM生成多样化的高质量样本，在包括样式转换和谐调在内的广泛任务中都适用。此外，它可以轻松地受到外部监督的指导。特别地，我们展示了如何使用预训练的CLIP模型从单张图像进行文本引导生成。

    Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.
    
[^200]: 基于配对互补时间循环一致对抗网络的雷达降水预测方法

    PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15046](http://arxiv.org/abs/2211.15046)

    本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。

    

    降水预测方法已经在过去几个世纪里得到了很好的发展，因为雨水对人类生活有着至关重要的影响。现有的降水预测模型包括定量降水预测 (QPF) 模型、卷积长短期记忆 (ConvLSTM) 模型以及最新的 MetNet-2 等多种复杂的方法。本文提出了基于配对互补时间循环一致对抗网络 (PCT-CycleGAN) 的雷达降水预测方法，受对抗生成网络 (CycleGAN) 强大的图像转换性能启发。PCT-CycleGAN 使用两个具有向前和向后时间动态的生成器网络生成时序性，每个生成器网络学习一个庞大的一对一映射，以逼近表示每个方向上的时间动态的映射函数。为了创建配对互补循环之间的强健时间因果关系，我们应用了循环一致性损失和对抗性损失。广泛的实验证明，PCT-CycleGAN 在准确性和推广能力方面优于现有的技术方法。

    The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
    
[^201]: 脱耦表示与稀疏性之间的协同作用：多任务学习中的泛化和可识别性

    Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning. (arXiv:2211.14666v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14666](http://arxiv.org/abs/2211.14666)

    本文提供了证据表明，脱耦表示与稀疏基预测器相结合可提高泛化能力。我们提出了一个实用方法来学习这种表示，并在少样本分类基准测试中取得了竞争性的结果。

    

    尽管脱耦表示经常被认为对下游任务有益，但目前的经验和理论理解仍然有限。本文提供证据表明，与稀疏基预测器相结合的脱耦表示可提高泛化能力。在多任务学习的背景下，我们证明了一个新的可识别性结果，该结果提供了最大稀疏基预测器产生脱耦表示的条件。受这个理论结果的启发，我们提出了一种基于稀疏促进双层优化问题学习脱耦表示的实用方法。最后，我们探索了一个基于组 Lasso 多类 SVM 基预测器的元学习版本的算法，为此，我们推导出了一个可行的对偶公式。它在标准的少样本分类基准测试上获得了竞争性的结果，而每个任务仅使用了一小部分学习到的表示。

    Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations.
    
[^202]: 学习建议休息：可持续优化长期用户参与度。

    Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement. (arXiv:2211.13585v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13585](http://arxiv.org/abs/2211.13585)

    本文提出了一种可持续优化长期用户参与度的框架，学习促进和维持长期参与的最优休息策略，避免了盲目推动用户增加消费导致燃尽、流失和成瘾的风险。

    

    优化用户参与度是现代推荐系统的一个关键目标，但是盲目地推动用户增加消费会增加燃尽、流失甚至成瘾的风险。为了促进数字福利，现在大多数平台都提供定期提示用户休息的服务，但这些服务必须手动设置，因此可能对用户和系统都不是最优的。在本文中，我们研究了休息在推荐中的作用，并提出了一个框架，用于学习促进和维持长期参与的最优休息策略。基于推荐动态易受到积极和消极反馈的概念，我们将推荐表示为一个 Lotka-Volterra 动态系统，其中休息减少为最优控制问题。我们提出了一种高效的学习算法，提供了理论保证，并在半合成数据上经验性地证明了我们方法的实用性。

    Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that recommendation dynamics are susceptible to both positive and negative feedback, we cast recommendation as a Lotka-Volterra dynamical system, where breaking reduces to a problem of optimal control. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically demonstrate the utility of our approach on semi-synthetic data.
    
[^203]: 基于筛选的一般方法来学习认知图的合理限制

    A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs. (arXiv:2211.02918v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.02918](http://arxiv.org/abs/2211.02918)

    该研究提出了一种基于筛选的方法，使用多重推广步骤来学习与认知图一致的合理规则，能够学习更广泛的合理规则以反映认知图中的合理性考虑，实验发现其表现优于现有方法。

    

    认知图是概率论证的认知方法的一种推广。Hunter提出了一个二次推广框架来从众包数据中学习认知约束。然而，学习到的认知约束只反映了用户从数据中的信念，而没有考虑到认知图中编码的合理性。与此同时，当前框架只能生成反映代理人信​​任程度而不是是否相信一个论点的认知约束。为了解决这些问题，我们提出了一个基于筛选的方法，使用多重推广步骤从数据集中生成一组与其认知图一致的合理规则。这种方法能够学习更广泛的合理规则，以反映认知图中的合理性考虑。实验结果表明，我们的方法在学习合理规则的精度和多样性方面优于现有方法。

    Epistemic graphs are a generalization of the epistemic approach to probabilistic argumentation. Hunter proposed a 2-way generalization framework to learn epistemic constraints from crowd-sourcing data. However, the learnt epistemic constraints only reflect users' beliefs from data, without considering the rationality encoded in epistemic graphs. Meanwhile, the current framework can only generate epistemic constraints that reflect whether an agent believes an argument, but not the degree to which it believes in it. The major challenge to achieving this effect is that the computational complexity will increase sharply when expanding the variety of constraints, which may lead to unacceptable time performance. To address these problems, we propose a filtering-based approach using a multiple-way generalization step to generate a set of rational rules which are consistent with their epistemic graphs from a dataset. This approach is able to learn a wider variety of rational rules that reflect
    
[^204]: 超几何表征学习的数值稳定性

    The Numerical Stability of Hyperbolic Representation Learning. (arXiv:2211.00181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00181](http://arxiv.org/abs/2211.00181)

    本文研究了超几何表征学习中的数值不稳定性问题，比较了两种流行的超几何模型Poincar\'e球和Lorentz模型，发现Lorentz模型具有更好的数值稳定性和优化性能，同时提出一种新的欧几里得优化方案作为超几何学习的另一个选择。

    

    由于超球的容量随半径的指数增长，超几何空间能够将具有层次结构的数据集嵌入其中而不失真。然而，这种指数增长的性质常常导致数值不稳定性，使得训练超几何学习模型有时会导致灾难性的NaN问题和浮点算术中遇到无法表示的值。在本文中，我们对两种广泛使用的超几何模型——Poincar\'e球和Lorentz模型的局限性进行了仔细的分析。我们首先展示了，在64位算术系统下，Poincar\'e球相对于Lorentz模型具有更大的能力来正确表示点。然后，我们从优化的角度理论上验证了Lorentz模型优于Poincar\'e球的优越性。鉴于两种模型的数值限制，我们确定一种欧几里得优化方案，在Poincar\'e球和Lorentz模型之外为超几何学习提供了一种新的方案。

    Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Eucli
    
[^205]: 图神经网络中的解释方法：一项比较研究

    Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15304](http://arxiv.org/abs/2210.15304)

    该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。

    

    在图神经网络的快速发展后，GNN已经在许多科学和工程领域应用广泛，这促使需要方法来理解它们的决策过程。最近几年，GNN解释器开始出现，有多种方法，一些是新颖的，一些是从其他领域改编而来的。为了整理这种海量的解释方法，一些研究在各种可解释性指标方面对不同的解释器性能进行了基准测试。然而，这些早期的工作没有尝试提供关于不同的GNN体系结构更或不易解释的洞察，也没有说明在给定环境中应该选择哪种解释器。在本次调查中，我们通过设计系统性实验研究，对八个代表性体系结构上训练的十种解释器在六个精心设计的图和节点分类数据集上进行了测试，填补了这些空白，并提供了关键的观点。

    Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
    
[^206]: 基于样本排列优化的全局对比批量采样

    Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12874](http://arxiv.org/abs/2210.12874)

    本论文提出了一种有效的替代硬负例挖掘的全局对比批量采样方法GCBS，能够提高对比学习任务的性能表现，易于实现且适用于各种对比学习方法。

    

    对比学习最近在各种任务中取得了最先进的性能。许多对比学习方法使用挖掘的硬负例来在训练期间使批处理更加信息丰富，但这些方法效率低下，因为它们增加了与挖掘负例数成比例的纪元长度，并需要频繁更新最近邻居索引或从最近的批次中进行挖掘。在这项工作中，我们提供了另一种硬负例挖掘的替代方案：全局对比批量采样（GCBS），一种有效的近似批处理分配问题，它上界了对比学习设置中的全局损失和训练损失之间的差距$\mathcal{L}^{Global} - \mathcal{L}^{Train}$。通过实验，我们发现GCBS改善了句子嵌入和代码搜索任务的最先进性能。此外，GCBS易于实现，因为它只需要少量附加代码，不需要维护外部数据结构，如最近邻居索引，并且适用于各种对比学习方法。

    Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbo
    
[^207]: AnalogVNN：完全模块化的光子神经网络建模和优化框架

    AnalogVNN: A fully modular framework for modeling and optimizing photonic neural networks. (arXiv:2210.10048v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10048](http://arxiv.org/abs/2210.10048)

    AnalogVNN是一个基于PyTorch的光学神经网络仿真框架，可以模拟光电噪声、有限精度和信号归一化等影响，使用框架可训练和优化线性/卷积神经网络，得出在光学神经网络中影响准确性的归一化、激活函数、降低精度和噪音等因素，实现数字神经网络模型向其模拟光学神经网络模型的转换。

    

    AnalogVNN 是一个基于 PyTorch 构建的仿真框架，可以模拟光子神经网络加速器中存在的光电噪声、有限精度和信号归一化等影响，我们使用此框架训练和优化具有最多 9 层和约 1.7 百万参数的线性和卷积神经网络，并深入了解归一化、激活函数、降低精度和噪音对模拟光学神经网络准确性的影响。通过遵循 PyTorch 中存在的相同层结构设计，AnalogVNN 框架允许用户仅使用几行代码将大多数数字神经网络模型转换为其模拟光学神经网络模型，充分利用 PyTorch 提供的开源优化、深度学习和 GPU 加速库。代码可在 https://analogvnn.github.io 上获得。

    AnalogVNN, a simulation framework built on PyTorch which can simulate the effects of optoelectronic noise, limited precision, and signal normalization present in photonic neural network accelerators. We use this framework to train and optimize linear and convolutional neural networks with up to 9 layers and ~1.7 million parameters, while gaining insights into how normalization, activation function, reduced precision, and noise influence accuracy in analog photonic neural networks. By following the same layer structure design present in PyTorch, the AnalogVNN framework allows users to convert most digital neural network models to their analog counterparts with just a few lines of code, taking full advantage of the open-source optimization, deep learning, and GPU acceleration libraries available through PyTorch. Code is available at https://analogvnn.github.io
    
[^208]: 针对低带宽通道和漏洞的一阶方法的草图技术：高效算法

    Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability. (arXiv:2210.08371v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08371](http://arxiv.org/abs/2210.08371)

    本文提出了一种针对大规模分布式学习场景的新型草图方案，通过节约通信成本实现联合学习，但是需要进行差分隐私处理以保护局部数据的隐私。

    

    草图技术是大规模机器学习中最基础的工具之一，通过对原始大问题进行随机压缩到更低维度的方式实现运行时和内存的节省。本文提出了一种新颖的一阶方法草图方案，用于大规模分布式学习场景中，以节约分布式代理之间的通信成本，同时保证算法的收敛。基于一个高维度 $d$ 的梯度信息，代理通过草图矩阵 $R\in \mathbb{R}^{s\times d}$ 处理这些压缩信息，在传递时只传输草图后的信息，而接收方会通过反草图矩阵 $R^\top$ 进行还原。利用这种框架，我们开发了具有较低通信成本的联合学习算法。然而，如此随机草图并不能直接保护局部数据的隐私。我们表明，在应用差分隐私后，梯度泄漏问题仍然存在。

    Sketching is one of the most fundamental tools in large-scale machine learning. It enables runtime and memory saving via randomly compressing the original large problem into lower dimensions. In this paper, we propose a novel sketching scheme for the first order method in large-scale distributed learning setting, such that the communication costs between distributed agents are saved while the convergence of the algorithms is still guaranteed. Given gradient information in a high dimension $d$, the agent passes the compressed information processed by a sketching matrix $R\in \mathbb{R}^{s\times d}$ with $s\ll d$, and the receiver de-compressed via the de-sketching matrix $R^\top$ to ``recover'' the information in original dimension. Using such a framework, we develop algorithms for federated learning with lower communication costs. However, such random sketching does not protect the privacy of local data directly. We show that the gradient leakage problem still exists after applying the
    
[^209]: SGD使用大步长训练能够学习稀疏特征

    SGD with Large Step Sizes Learns Sparse Features. (arXiv:2210.05337v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05337](http://arxiv.org/abs/2210.05337)

    本文展示了SGD使用大步长训练能够学习稀疏特征，在训练过程中，通过步长调度，梯度和噪声相互作用，共同驱动SGD动态穿过神经网络的损失平面，从而发现稀疏表示。

    

    本文展示了随机梯度下降（SGD）在神经网络训练中动力学的重要特征。我们发现：常用的大步长会导致迭代从山谷的一侧跳到另一侧导致损失稳定，同时这种稳定性会引起一个隐含的、垂直于跳跃方向的随机动态，将其偏向于稀疏预测器。此外，我们实验证明，长时间使用大步长可保持SGD在损失平面中的高度，进而能更好地实现隐式正则化和发现稀疏表示。值得注意的是，这里没有使用任何显式正则化，因此正则化效果完全来自于受步长调度影响的SGD训练动态。因此，我们的发现揭示了通过步长调度，梯度和噪声如何共同驱动SGD动态穿过神经网络的损失平面。我们通过展示幂律步长调度匹配奥恩斯坦-乌伦贝克过程的理论预测并导致最稳健和最稀疏的表示来证明这些发现。

    We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward sparse predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the SGD training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks. We justify these find
    
[^210]: 数字音频取证：盲目检测人类语音模仿

    Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2209.12573](http://arxiv.org/abs/2209.12573)

    本文介绍了一种利用深度学习方法，通过盲目检测输入音频的真实性，可以有效应对音频欺诈问题的分类器。而这种分类器不需要任何参考，能够在没有真实来源的情况下检测出模仿音频。

    

    音频是人类交流中使用最广泛的方式之一，但同时也很容易被误用来欺骗人们。随着人工智能的革命，相关技术现在对几乎所有人都可用，这使得犯罪和伪造变得更加简单。本篇论文介绍了一种深度学习方法，开发了一个分类器，可以盲目分类输入音频为真实或者模仿；“盲目”指的是能够在没有参考或真实来源的情况下检测仿制音频的能力。所提出的模型是在一个大型音频数据集中提取的一组重要特征上进行训练的，以得到一个分类器，该分类器被用于测试不同音频的相同特征集。数据提取自两个原始数据集，特别为这项工作而编写;一个全英文数据集和一个混合数据集（阿拉伯语加英语）。这些数据集已通过GitHub以原始形式提供给研究社区，网址为https://github.com/SaSs7/Datas

    Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
    
[^211]: 低秩混合模型的Lloyd算法最优聚类

    Optimal Clustering by Lloyd Algorithm for Low-Rank Mixture Model. (arXiv:2207.04600v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2207.04600](http://arxiv.org/abs/2207.04600)

    本文提出了一种名为LrMM的低秩混合模型，结合了Lloyd算法和低秩逼近的聚类方法，能在较弱的限制下处理矩阵观测值，在达到最小化渐进次优的聚类误差率方面表现出众。

    

    本文研究了基于矩阵观测值的聚类的计算和统计限制。我们提出了一种低秩混合模型(LrMM)，该模型从经典的高斯混合模型(GMM)中演化而来，用于处理矩阵观测值，假设种群中心矩阵的低秩性。通过集成Lloyd算法和低秩逼近，设计了一种计算高效的聚类方法。一旦初始化良好，该算法收敛快速，具有一种指数型的聚类误差率，达到最小化的渐进次优性。与GMM相比，我们展示了基于张量的谱方法提供了一个良好的初始聚类。最小化渐进次优的聚类误差率由分离强度决定，即种群中心矩阵之间的最小距离。通过利用低秩性，所提出的算法在分离强度的要求上具有较弱的限制。但与GMM不同的是，LrMM的计算难度被特征化为一个最优控制问题。

    This paper investigates the computational and statistical limits in clustering matrix-valued observations. We propose a low-rank mixture model (LrMM), adapted from the classical Gaussian mixture model (GMM) to treat matrix-valued observations, which assumes low-rankness for population center matrices. A computationally efficient clustering method is designed by integrating Lloyd's algorithm and low-rank approximation. Once well-initialized, the algorithm converges fast and achieves an exponential-type clustering error rate that is minimax optimal. Meanwhile, we show that a tensor-based spectral method delivers a good initial clustering. Comparable to GMM, the minimax optimal clustering error rate is decided by the separation strength, i.e., the minimal distance between population center matrices. By exploiting low-rankness, the proposed algorithm is blessed with a weaker requirement on the separation strength. Unlike GMM, however, the computational difficulty of LrMM is characterized b
    
[^212]: 早期利用语义相似度在波斯语Twitter上发现新兴实体

    Early Discovery of Emerging Entities in Persian Twitter with Semantic Similarity. (arXiv:2207.02434v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.02434](http://arxiv.org/abs/2207.02434)

    本论文提出了一种名为EEPT的在线聚类方法，该方法利用语义相似度在Twitter上发现新兴实体。通过新的评估指标，结果表明EEPT是有前途的并能够在实体建立之前发现重要的实体。

    

    发现新兴实体（EEs）是指在它们被认可之前就找到它们的过程。这些实体对个人、公司和政府都至关重要。其中许多实体可以在社交媒体平台上发现，例如Twitter。近年来，这些实体已成为学术界和工业界的研究重点。与任何机器学习问题一样，数据的可用性是这一问题的主要挑战之一。本文提出了EEPT，这是一种在线聚类方法，能够在不需要训练数据集的情况下发现EEs。此外，由于缺乏适当的评估指标，本文使用了一种新的衡量标准来评估结果。结果表明，EEPT是有前途的，并能在实体建立之前发现重要的实体。

    Discovering emerging entities (EEs) is the problem of finding entities before their establishment. These entities can be critical for individuals, companies, and governments. Many of these entities can be discovered on social media platforms, e.g. Twitter. These identities have been the spot of research in academia and industry in recent years. Similar to any machine learning problem, data availability is one of the major challenges in this problem. This paper proposes EEPT. That is an online clustering method able to discover EEs without any need for training on a dataset. Additionally, due to the lack of a proper evaluation metric, this paper uses a new metric to evaluate the results. The results show that EEPT is promising and finds significant entities before their establishment.
    
[^213]: 在实值函数中对抗鲁棒PAC学习性的研究

    Adversarially Robust PAC Learnability of Real-Valued Functions. (arXiv:2206.12977v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12977](http://arxiv.org/abs/2206.12977)

    该论文研究了在实值函数中对抗鲁棒PAC学习性，发现有限胖折射维的类既可以在实现和不可知设置中被学习，凸函数类可以正确学习，而一些非凸函数类需要不正当的学习算法。

    

    我们研究了在使用$\ell_p$损失和任意扰动集的回归设置中，对测试时对抗性攻击的稳健性。我们探讨了在这种情况下哪些函数类是PAC可学习的。我们表明有限胖折射维的类既可以在实现和不可知设置中被学习。此外，对于凸函数类，它们甚至可以正确学习。相比之下，一些非凸函数类显然需要不正当的学习算法。我们的主要技术基于构建一个由胖折射维决定大小的具有对抗鲁棒性的样本压缩方案。在此过程中，我们介绍了一个新颖的面向实值函数的不可知样本压缩方案，这可能是具有独立兴趣的。

    We study robustness to test-time adversarial attacks in the regression setting with $\ell_p$ losses and arbitrary perturbation sets. We address the question of which function classes are PAC learnable in this setting. We show that classes of finite fat-shattering dimension are learnable in both realizable and agnostic settings. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes provably require improper learning algorithms. Our main technique is based on a construction of an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension. Along the way, we introduce a novel agnostic sample compression scheme for real-valued functions, which may be of independent interest.
    
[^214]: 一种对经验神经切向核的快速且有根据的近似方法。

    A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel. (arXiv:2206.12543v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.12543](http://arxiv.org/abs/2206.12543)

    该论文提出了一种名为“逻辑和”的经验神经切向核近似方法，可以在计算量上显著降低，同时经过证明在宽的最终“读出”层的网络中初始化后收敛于真实的eNTK。

    

    经验神经切向核（eNTK）可以很好地理解给定网络的表示：它们通常比无限宽NTK计算便宜得多，适用范围更广。然而，对于具有O个输出单元（例如O类分类器）的网络，N个输入的eNTK的大小为$NO\times NO$，需要$O((NO)^2)$的内存和高达$O((NO)^3)$的计算量。因此，大多数现有的应用程序使用少数几个近似值之一，可以产生$N\times N$内核矩阵，从而节省数量级的计算，但没有或极少有理论依据。我们证明了其中一种近似方法，我们称之为“逻辑和”，对于任何具有宽的最终“读出”层的网络，在初始化时收敛于真实的eNTK。我们的实验展示了这个近似方法在各种不同设置中的各种用途的质量。

    Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite width NTKs. For networks with O output units (e.g. an O-class classifier), however, the eNTK on N inputs is of size $NO \times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$ computation. Most existing applications have therefore used one of a handful of approximations yielding $N \times N$ kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call "sum of logits", converges to the true eNTK at initialization for any network with a wide final "readout" layer. Our experiments demonstrate the quality of this approximation for various uses across a range of settings.
    
[^215]: 神经扩散过程

    Neural Diffusion Processes. (arXiv:2206.03992v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03992](http://arxiv.org/abs/2206.03992)

    提出了一种新方法——神经扩散过程（NDPs），通过有限边缘学习从丰富的函数分布中进行采样。NDPs 可以捕获接近真实贝叶斯后验的函数分布，具有超越神经过程的表现，实现了多种下游任务，比如回归、隐式超参数边缘化、非高斯后验预测和全局优化。

    

    与函数元学习分布相关的神经网络方法具有灵活性增强和推断复杂性降低的优点。在去噪扩散模型用于生成建模方面取得成功的基础上，我们提出了一种新方法——神经扩散过程（NDPs），通过有限边缘学习从丰富的函数分布中进行采样。通过引入自定义注意力块，我们能够将随机过程的属性（如可交换性）直接纳入 NDP 的架构中。我们从实证角度证明了 NDPs 可以捕获接近真实贝叶斯后验的函数分布，表明它们能够成功模拟高斯过程的行为并超越神经过程的表现。NDPs 可以进行多种下游任务，包括回归、隐式超参数边缘化、非高斯后验预测和全局优化。

    Neural network approaches for meta-learning distributions over functions have desirable properties such as increased flexibility and a reduced complexity of inference. Building on the successes of denoising diffusion models for generative modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns to sample from a rich distribution over functions through its finite marginals. By introducing a custom attention block we are able to incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs can capture functional distributions close to the true Bayesian posterior, demonstrating that they can successfully emulate the behaviour of Gaussian processes and surpass the performance of neural processes. NDPs enable a variety of downstream tasks, including regression, implicit hyperparameter marginalisation, non-Gaussian posterior prediction and global optimisation.
    
[^216]: DevFormer: 一种用于上下文感知硬件布局的对称Transformer

    DevFormer: A Symmetric Transformer for Context-Aware Device Placement. (arXiv:2205.13225v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13225](http://arxiv.org/abs/2205.13225)

    本文提出了DevFormer，一种用于硬件设计优化的对称Transformer，引入相对位置嵌入和动作置换对称性等强归纳偏置以有效捕捉硬件上下文，能有效地用有限的离线数据实现设计优化，并且在分离电容器放置问题中表现出众，可提高性能并减少组件数量。

    

    本文提出了 DevFormer，一种新颖的基于Transformer的架构，用于解决硬件设计优化等问题。我们通过引入相对位置嵌入和动作置换对称性等强归纳偏置来有效捕捉硬件上下文并使用有限的离线数据实现有效的设计优化。我们将DevFoemer应用于分离电容器放置问题，并展示它在模拟和实际硬件中优于现有方法，可在减少超过30％的组件数量的同时提高性能。最后，我们展示了我们的方法在其他基于离线上下文学习的协作任务中取得了有希望的结果。

    In this paper, we present DevFormer, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply DevFoemer to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\%$. Finally, we show that our approach achieves promising results in other offline contextual learning-based co
    
[^217]: 实时自定义关键词检测中的 Boosting Tail Neural Network

    Boosting Tail Neural Network for Realtime Custom Keyword Spotting. (arXiv:2205.12933v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2205.12933](http://arxiv.org/abs/2205.12933)

    本文提出了一种 Boosting Tail Neural Network（BTNN）来改善实时自定义关键词检测的性能，该方法利用批量弱分类器来解决问题，在实验中相对于传统算法获得了显著表现改善。

    

    本文提出了一种 Boosting Tail Neural Network（BTNN），用于提高实时自定义关键词检测（RCKS）的性能。受大脑科学启发，我们利用一批弱分类器来解决困难的问题。实验表明，我们的方法在唤醒率和误报率方面性能更好，在与仅使用一个强分类器的传统算法相比较时，获得了18％的相对改善。

    In this paper, we propose a Boosting Tail Neural Network (BTNN) for improving the performance of Realtime Custom Keyword Spotting (RCKS) that is still an industrial challenge for demanding powerful classification ability with limited computation resources. Inspired by Brain Science that a brain is only partly activated for a nerve simulation and numerous machine learning algorithms are developed to use a batch of weak classifiers to resolve arduous problems, which are often proved to be effective. We show that this method is helpful to the RCKS problem. The proposed approach achieve better performances in terms of wakeup rate and false alarm.  In our experiments compared with those traditional algorithms that use only one strong classifier, it gets 18\% relative improvement. We also point out that this approach may be promising in future ASR exploration.
    
[^218]: 基于梯度提升的凸锥预测和优化问题

    Gradient boosting for convex cone predict and optimize problems. (arXiv:2204.06895v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.06895](http://arxiv.org/abs/2204.06895)

    本文介绍了dboost，它是第一个为“预测，然后优化”问题设计的智能梯度提升实现。该框架支持凸二次锥规划，并通过自定义不动点映射的隐式微分来执行梯度提升，在实验中表现出色。

    

    预测模型通常独立于决策优化进行优化。智能预测优化（SPO）框架优化预测模型以最小化下游决策遗憾。本文提出了dboost，针对“预测，然后优化”问题的第一个通用的智能梯度提升实现。该框架支持凸二次锥规划，通过自定义不动点映射的隐式微分来执行梯度提升。与最先进的SPO方法的实验比较表明，dboost可以进一步减少样本外决策遗憾。

    Prediction models are typically optimized independently from decision optimization. A smart predict then optimize (SPO) framework optimizes prediction models to minimize downstream decision regret. In this paper we present dboost, the first general purpose implementation of smart gradient boosting for `predict, then optimize' problems. The framework supports convex quadratic cone programming and gradient boosting is performed by implicit differentiation of a custom fixed-point mapping. Experiments comparing with state-of-the-art SPO methods show that dboost can further reduce out-of-sample decision regret.
    
[^219]: 一种基于仿真集成的生物启发式搜索测试方法在ADAS案例研究中的应用

    Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2203.12026](http://arxiv.org/abs/2203.12026)

    本文提出一种基于仿真集成的生物启发式搜索测试方法Deeper，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景，通过实证评估和与竞赛中的其他工具的比较展示了其性能的提高。

    

    本文介绍Deeper的扩展版本，它是一种基于搜索实现的仿真集成测试解决方案，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景。在新版本中，我们利用了一组新的生物启发式搜索算法-遗传算法（GA）、（μ+λ）和（μ，λ）进化策略（ES）以及粒子群优化（PSO），这些算法利用质量种子种群以及为建模测试场景使用的特定领域交叉和突变操作。为了展示Deeper中新测试生成器的能力，我们进行了实证评估，并与SBST 2021的五个参赛工具的结果进行了比较。我们的评估结果表明，在新版本中，Deeper中的新测试生成器不仅在以前的版本上有了很大提升，而且...

    This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
    
[^220]: 政策优化中的不变性及奖励学习中的部分可识别性

    Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. (arXiv:2203.07475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07475](http://arxiv.org/abs/2203.07475)

    本文探讨了奖励学习中奖励函数的部分可识别性，并分析了这种部分可识别性对政策优化等下游任务的影响。同时提出了一个框架，对比奖励学习的数据源和下游任务，以其不变性为依据，对奖励学习的数据源的设计和选择产生影响。

    

    对于复杂的现实任务，手动设计奖励函数通常是非常具有挑战性的。为了解决这个问题，可以使用奖励学习从数据中推断奖励函数。然而，即使在无限数据的情况下，通常也会有多个奖励函数可以很好地拟合数据。这意味着奖励函数只能被部分地识别。在这项工作中，我们正式描述了在几种流行的奖励学习数据源（包括专家演示和轨迹比较）下奖励函数的部分可识别性。我们还分析了这种部分可识别性对于几项下游任务（例如政策优化）的影响。我们在一个框架中统一了我们的结果，该框架通过其不变性对比数据源和下游任务，并对奖励学习的数据源的设计和选择产生影响。

    It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.
    
[^221]: 连续时间和状态下的时间差分学习（随机场景中）

    Temporal Difference Learning with Continuous Time and State in the Stochastic Setting. (arXiv:2202.07960v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07960](http://arxiv.org/abs/2202.07960)

    该论文提出了两种连续时间策略评估问题的时间差分学习方法，并证明了它们的理论收敛速度。同时，这些方法还可以解释为解决线性PDE或线性BSDE的新型强化学习方法。

    

    我们考虑连续时间策略评估的问题。这意味着通过观察来学习与未受控制的连续时间随机动态和奖励函数相关联的价值函数。我们提出了两种使用逐渐减少的时间步长的著名TD（0）方法的原始变体。一种是无模型的，另一种是基于模型的。对于两种方法，我们证明了理论收敛速度，并通过数值模拟进行了验证。或者，这些方法可以解释为近似解决线性PDE（偏微分方程）或线性BSDE（反向随机微分方程）的新型强化学习方法。

    We consider the problem of continuous-time policy evaluation. This consists in learning through observations the value function associated with an uncontrolled continuous-time stochastic dynamic and a reward function. We propose two original variants of the well-known TD(0) method using vanishing time steps. One is model-free and the other is model-based. For both methods, we prove theoretical convergence rates that we subsequently verify through numerical simulations. Alternatively, those methods can be interpreted as novel reinforcement learning approaches for approximating solutions of linear PDEs (partial differential equations) or linear BSDEs (backward stochastic differential equations).
    
[^222]: AD-NEGF：一种用于敏感性分析和反问题的端到端可微量子输运模型

    AD-NEGF: An End-to-End Differentiable Quantum Transport Simulator for Sensitivity Analysis and Inverse Problems. (arXiv:2202.05098v2 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2202.05098](http://arxiv.org/abs/2202.05098)

    本文提出了一种名为AD-NEGF的端到端可微NEGF模型，用于量子输运模拟，可以应用于敏感性分析、反向设计等领域，并具有加速材料设计过程的潜力。

    

    自70年代提出以来，非平衡格林函数（NEGF）方法已被公认为量子输运模拟的标准方法。尽管它在模拟精度方面表现出优越性，但巨大的计算成本使其无法承受高通量模拟任务，例如敏感性分析、反向设计等。本研究提出了AD-NEGF，据我们所知是第一个端到端可微NEGF模型，用于量子输运模拟。我们使用PyTorch实现整个数值过程，并采用隐式层技术设计了自定义的反向传递，以提供可负担的梯度信息，同时保证正向模拟的正确性。所提出的模型经过应用验证，用于计算不同物理量、实验参数拟合和掺杂优化，证明了它在通过基于梯度的参数优化加速材料设计过程方面的潜力。

    Since proposed in the 70s, the Non-Equilibrium Green Function (NEGF) method has been recognized as a standard approach to quantum transport simulations. Although it achieves superiority in simulation accuracy, the tremendous computational cost makes it unbearable for high-throughput simulation tasks such as sensitivity analysis, inverse design, etc. In this work, we propose AD-NEGF, to our best knowledge the first end-to-end differentiable NEGF model for quantum transport simulations. We implement the entire numerical process in PyTorch, and design customized backward pass with implicit layer techniques, which provides gradient information at an affordable cost while guaranteeing the correctness of the forward simulation. The proposed model is validated with applications in calculating differential physical quantities, empirical parameter fitting, and doping optimization, which demonstrates its capacity to accelerate the material design process by conducting gradient-based parameter op
    
[^223]: HeterPS：基于强化学习调度的异构环境下分布式深度学习

    HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2111.10635](http://arxiv.org/abs/2111.10635)

    这篇论文介绍了一个名为Paddle-HeterPS的分布式框架，基于强化学习的调度方法可以高效地利用多种类型的计算资源，解决了分布式深度学习训练中多层次分配计算资源的问题。

    

    深度神经网络利用许多层和大量参数实现了优秀的性能。DNN模型的训练过程通常处理具有许多稀疏特征的大规模输入数据，这会产生高延迟和I/O成本，而某些层的计算成本很高。训练过程通常利用分布式计算资源来减少训练时间。此外，多种类型的计算资源，如CPU和GPU等，也可用于分布式训练过程。因此，多层次地分配计算资源对训练过程至关重要。为了通过异构计算资源高效地训练DNN模型，我们提出了一种分布式框架Paddle-Heterogeneous Parameter Server（Paddle-HeterPS），由分布式架构和基于强化学习的调度方法组成。与现有框架相比，Paddle-HeterPS的优点有三个。

    Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
    
[^224]: 一种可扩展、快速的基于人工神经网络的表面码综合征解码器

    A scalable and fast artificial neural network syndrome decoder for surface codes. (arXiv:2110.05854v4 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2110.05854](http://arxiv.org/abs/2110.05854)

    本研究报道了一种基于人工神经网络的表面码综合征解码器，能够解码任意形状和大小的表面码，并取得了具有竞争性或优越的性能，显示出惊人的加速作用，这是实现大规模量子纠错的重要进展。

    

    表面码纠错提供了实现可扩展容错量子计算的高度有前途的途径。当作为稳定码进行操作时，表面码计算包括一个综合征解码步骤，其中使用测量的稳定码算符来确定物理量子比特中的错误的适当纠正。译码算法经历了实质性的发展，最近的工作纳入了机器学习（ML）技术。尽管初步结果令人鼓舞，但基于ML的综合征译码器仍然局限于具有低延迟的小规模演示，并且无法处理需要进行晶格手术和编织的边界条件和各种形状的表面码。在这里，我们报道了一种基于人工神经网络（ANN）的可扩展快速综合征解码器，能够解码任意形状和大小的表面码，并且可以处理数据量子比特受到极化误差模型的影响。基于对5000万个随机量子错误实例的严格训练，ANN解码器实现了与最先进的译码算法相比的竞争性或优越性能，同时保持高解码准确性，呈现出了显著的加速效果。我们的工作展示了使用基于机器学习的译码方法实现大规模量子纠错的重大进展。

    Surface code error correction offers a highly promising pathway to achieve scalable fault-tolerant quantum computing. When operated as stabilizer codes, surface code computations consist of a syndrome decoding step where measured stabilizer operators are used to determine appropriate corrections for errors in physical qubits. Decoding algorithms have undergone substantial development, with recent work incorporating machine learning (ML) techniques. Despite promising initial results, the ML-based syndrome decoders are still limited to small scale demonstrations with low latency and are incapable of handling surface codes with boundary conditions and various shapes needed for lattice surgery and braiding. Here, we report the development of an artificial neural network (ANN) based scalable and fast syndrome decoder capable of decoding surface codes of arbitrary shape and size with data qubits suffering from the depolarizing error model. Based on rigorous training over 50 million random qu
    
[^225]: 使用机器教学研究教授强化学习者时人类的假设

    Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.02476](http://arxiv.org/abs/2009.02476)

    本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设，发现人们假设学习者具有高的折扣率和高度重视探索，并根据学习者进展调整教学策略。

    

    为成功教学，需要对学习者学习方式进行假设，即学习者如何使用来自世界的经验来更新其内部状态。本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设。研究重点是一种常见的强化学习方法 Q-learning，通过行为实验考察人们的假设。为了达到此目的，我们首先建立了一个规范标准，将问题形式化为机器教学优化问题。为了解决机器教学优化问题，我们使用深度学习逼近方法来模拟学习者在环境中的表现，并学习预测反馈如何影响学习者的内部状态。在教授理想化的探索利用任务时，人们对学习者的学习和折扣率有哪些假设？在行为实验中，我们发现人们可以相对高效和准确地教导 Q-学习者这项任务。人们倾向于假设学习者具有高的折扣率，并高度重视探索。此外，人们会根据学习者的进展调整自己的教学策略。

    Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
    
[^226]: 高斯分层潜在狄利克雷分配：再现多义词

    Gaussian Hierarchical Latent Dirichlet Allocation: Bringing Polysemy Back. (arXiv:2002.10855v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2002.10855](http://arxiv.org/abs/2002.10855)

    本论文提出了一种高斯分层潜在狄利克雷分配模型，通过引入层次结构恢复了捕捉多义性的能力，相对于基于高斯的模型具有更好的多义词检测性能，相对于潜在狄利克雷分配的层次模型具有更加简洁的主题表示。

    

    话题模型被广泛应用于发现一组文档的潜在表示。两种典型的模型是潜在狄利克雷分配和高斯潜在狄利克雷分配，前者使用单词上的多项式分布，后者使用预训练的单词嵌入向量上的多元高斯分布作为潜在主题表示。与潜在狄利克雷分配相比，高斯潜在狄利克雷分配在捕捉“银行”等词的多义性方面存在限制。在本文中，我们证明高斯分层潜在狄利克雷分配通过为模型可以用于表示给定文档的主题集合引入层次结构，可以恢复捕捉多义性的能力。我们的高斯分层潜在狄利克雷分配相对于基于高斯的模型显著提高了多义词检测的能力，并提供了比基于潜在狄利克雷分配的层次模型更为简洁的主题表示。

    Topic models are widely used to discover the latent representation of a set of documents. The two canonical models are latent Dirichlet allocation, and Gaussian latent Dirichlet allocation, where the former uses multinomial distributions over words, and the latter uses multivariate Gaussian distributions over pre-trained word embedding vectors as the latent topic representations, respectively. Compared with latent Dirichlet allocation, Gaussian latent Dirichlet allocation is limited in the sense that it does not capture the polysemy of a word such as ``bank.'' In this paper, we show that Gaussian latent Dirichlet allocation could recover the ability to capture polysemy by introducing a hierarchical structure in the set of topics that the model can use to represent a given document. Our Gaussian hierarchical latent Dirichlet allocation significantly improves polysemy detection compared with Gaussian-based models and provides more parsimonious topic representations compared with hierarch
    
[^227]: ROIPCA：一种基于秩一更新的在线内存受限PCA算法

    ROIPCA: An online memory-restricted PCA algorithm based on rank-one updates. (arXiv:1911.11049v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.11049](http://arxiv.org/abs/1911.11049)

    本文提出了基于秩一更新的ROIPCA和fROIPCA两种在线PCA算法，在内存限制的情况下，算法准确性好、运行时间短。其中fROIPCA为梯度算法，具有最优学习率。

    

    主成分分析（PCA）是数据分析中基本的算法。其内存受限的在线版本在许多现代应用中非常有用，其中数据过大而无法在内存中存储，或者数据到达时为一系列项目。在本文中，我们提出了ROIPCA和fROIPCA两种基于秩一更新的在线PCA算法。虽然ROIPCA通常更准确，但fROIPCA更快，并且具有可比较的准确性。我们展示了fROIPCA与现有流行的在线PCA梯度算法之间的关系，并且特别证明了fROIPCA实际上是具有最优学习率的梯度算法。我们在数值上证明了我们的算法在准确性和运行时间方面比现有的最先进算法具有优势。

    Principal components analysis (PCA) is a fundamental algorithm in data analysis. Its memory-restricted online versions are useful in many modern applications, where the data are too large to fit in memory, or when data arrive as a stream of items. In this paper, we propose ROIPCA and fROIPCA, two online PCA algorithms that are based on rank-one updates. While ROIPCA is typically more accurate, fROIPCA is faster and has comparable accuracy. We show the relation between fROIPCA and an existing popular gradient algorithm for online PCA, and in particular, prove that fROIPCA is in fact a gradient algorithm with an optimal learning rate. We demonstrate numerically the advantages of our algorithms over existing state-of-the-art algorithms in terms of accuracy and runtime.
    

