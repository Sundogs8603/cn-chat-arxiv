# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation.](http://arxiv.org/abs/2307.15053) | 本文批判性审视了(Normalised) Discounted Cumulative Gain作为Top-n推荐离线评估指标的方法，并研究了何时可以期望这些指标逼近在线实验的金标准结果。 |
| [^2] | [A Transformer-based Approach for Arabic Offline Handwritten Text Recognition.](http://arxiv.org/abs/2307.15045) | 本文提出了基于Transformer的阿拉伯离线手写文本识别方法，通过引入Transformer Transducer和标准的序列到序列Transformer架构，解决了循环神经网络的并行化和语言规则不考虑的问题，具有较高的准确性和速度。 |
| [^3] | [Universal and Transferable Adversarial Attacks on Aligned Language Models.](http://arxiv.org/abs/2307.15043) | 这项研究提出了一种简单而有效的攻击方法，能够使对齐的语言模型生成不良行为，而不依赖于人工设计，通过自动化方法产生对抗性后缀，并在实践中取得改进。 |
| [^4] | [Speeding up Fourier Neural Operators via Mixed Precision.](http://arxiv.org/abs/2307.15034) | 通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。 |
| [^5] | [Self-Supervised Graph Transformer for Deepfake Detection.](http://arxiv.org/abs/2307.15019) | 这篇论文介绍了一种利用自监督预训练模型的深度伪造检测框架，具有出色的泛化能力，可以应对常见的破坏，并实现特征解释性。 |
| [^6] | [Samplable Anonymous Aggregation for Private Federated Data Analysis.](http://arxiv.org/abs/2307.15017) | 本论文在解决每个设备持有私有数据情况下的私有统计和私有联邦学习设计中，提出了一个简单的原语，以实现高效的算法，并在不需要强信任假设的情况下保护隐私。 |
| [^7] | [How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges.](http://arxiv.org/abs/2307.15016) | 本研究探索了Google Bard在理解和解释文本问题条件下的视觉数据方面的能力，并发现Bard在各种视觉场景中仍然存在困境，这凸显出在视觉理解方面存在重要的差距。 |
| [^8] | [Harnessing Synthetic Active Particles for Physical Reservoir Computing.](http://arxiv.org/abs/2307.15010) | 本研究展示了利用合成活性微粒系统进行物理储层计算，通过自组织和自耦合的特性，实现了具有噪声非线性动力学的预测任务。 |
| [^9] | [A LLM Assisted Exploitation of AI-Guardian.](http://arxiv.org/abs/2307.15008) | 本文研究了LLM是否能辅助进行对抗性机器学习研究，以AI-Guardian为案例评估了其鲁棒性。研究发现，我们成功破解了AI-Guardian的防御机制，并且通过指示和引导GPT-4实施攻击算法的方法非常有效和高效。 |
| [^10] | [Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability.](http://arxiv.org/abs/2307.15007) | 本文旨在搭建一座桥梁，将后期解释性和内在可解释性相结合，以解释复杂的黑盒模型的行为，并解决解释的忠实性和可验证性之间的问题。 |
| [^11] | [Thinker: Learning to Plan and Act.](http://arxiv.org/abs/2307.14993) | Thinker算法通过引入世界模型和模型交互动作使强化学习代理实现自主规划，消除了手工设计规划算法的需求，并且在Sokoban游戏和Atari 2600基准测试中取得了state-of-the-art的性能。 |
| [^12] | [Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs.](http://arxiv.org/abs/2307.14988) | 本论文介绍了一种增量计算的神经网络方法，通过离散化中间值并过滤不必要的修改，实现了对动态输入的高效推理。 |
| [^13] | [Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models.](http://arxiv.org/abs/2307.14971) | 本文提出了一种新的三维到二维的生成式预训练方法，通过生成视图图像作为预训练方案，帮助三维模型更好地理解点云的几何结构和立体关系，并在实验证明了其优越性。 |
| [^14] | [Learning locally dominant force balances in active particle systems.](http://arxiv.org/abs/2307.14970) | 通过无监督聚类和稀疏推理算法，我们学习了自组织活动粒子系统中解释宏观模式形成的局部主导力平衡。分析结果表明，传播带是由密度梯度驱动的局部对齐相互作用形成的，而稳态星环是由扭曲引起的负压缩性机制塑造的。 |
| [^15] | [Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification.](http://arxiv.org/abs/2307.14959) | 本文研究了在高度不平衡的医学图像分类中通过自监督先验实现联邦模型聚合。根据利用公开可用的自监督辅助网络发现，在每个客户端上局部使用共享的预训练模型可以得到一致的发散测量结果。基于这些发现，提出了一个通过自监督先验引导全局模型优化的动态平衡模型聚合方法，用于实现高度鲁棒和无偏的全局模型。 |
| [^16] | [Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space.](http://arxiv.org/abs/2307.14953) | 本文提出了一种基于字典学习和最优传输的MSDA框架，通过将每个域表示为字典原子的Wasserstein重心来缓解数据分布偏移。根据该字典，提出了两种新的MSDA方法，分别基于目标域标记样本的重构和在原子分布上学习的分类器的集成。在多个基准测试集上进行的实验证明，这些方法在分类任务上取得了显著的改进效果。 |
| [^17] | [Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning.](http://arxiv.org/abs/2307.14952) | 本文提出了一种分层的网络容错和拜占庭鲁棒的社交学习算法，解决了网络通信故障和敌对攻击的挑战，并设计了一种稀疏信息融合规则和具有可证明收敛性的算法。 |
| [^18] | [A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs.](http://arxiv.org/abs/2307.14940) | 本论文提出了一种自适应惩罚算法，用于将约束的自然系统集成到神经常微分方程中，通过引入先验知识提高了模型的可解释性，并通过数值实验证明了其有效性。 |
| [^19] | [Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops.](http://arxiv.org/abs/2307.14938) | 本文提出了一种计算效率高的神经网络控制系统区间可达性分析框架，通过引入包含函数和构建嵌入系统来捕捉系统和神经网络控制器之间的相互作用。 |
| [^20] | [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback.](http://arxiv.org/abs/2307.14936) | 本文提出了一种新的框架RRTF，以增强预训练的大型语言模型在代码生成方面的能力。PanGu-Coder2是该框架的实现，在多个基准测试中均表现出色，优于其他先前的Code LLMs。 |
| [^21] | [Solving Data Quality Problems with Desbordante: a Demo.](http://arxiv.org/abs/2307.14935) | Desbordante是一个旨在解决数据质量问题的工具，通过发现和验证复杂统计信息来帮助现代数据科学家进行数据概要分析。它提供了与现有工具的适当集成，同时考虑到工业级工作负载，并提供描述性的解释来解释模式缺失的原因。 |
| [^22] | [Graph-based Polyphonic Multitrack Music Generation.](http://arxiv.org/abs/2307.14928) | 这篇论文介绍了一种基于图的多音轨音乐生成方法，通过引入深度变分自动编码器，分别生成音乐图的结构和内容，实现了通过指定乐器来条件生成音乐，为音乐共创的人机互动提供了新的可能性。 |
| [^23] | [Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems.](http://arxiv.org/abs/2307.14921) | 该论文开发了一个基准测试工具，使用机器学习模型在两个HPC系统上进行材料分割性能分析，并发现Vulcanite在大多数测试中具有更快的模型时间，但容易受到环境因素影响，而Onyx的模型时间在所有测试中保持一致。 |
| [^24] | [NSA: Naturalistic Support Artifact to Boost Network Confidence.](http://arxiv.org/abs/2307.14917) | 这项工作提出了自然支持性物件（NSA）的概念，用于训练稳健的视觉人工智能模型以抵御自然破坏。NSAs是通过使用DC-GAN进行物件训练生成的自然外观物件。 |
| [^25] | [Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions.](http://arxiv.org/abs/2307.14906) | 本文介绍了一种使用优化的负采样和损失函数扩展基于会话的Transformer推荐系统，该系统在大规模电商数据集上通过集成负采样和列表损失函数实现了较高的推荐准确性，并在实践中表现出潜力。 |
| [^26] | [CodeLens: An Interactive Tool for Visualizing Code Representations.](http://arxiv.org/abs/2307.14902) | CodeLens是一种可视化代码表示的交互式工具，支持多种表示方法和编程语言，开发人员能够快速理解和探索代码。 |
| [^27] | [Generative convective parametrization of dry atmospheric boundary layer.](http://arxiv.org/abs/2307.14857) | 该研究提出了一种基于生成对抗网络的干燥对流边界层参数化方案，通过结合经典混合层理论中的自相似层生长物理学，显著改善了合成湍流场的统计预测性能。 |
| [^28] | [Counterfactual Explanations for Graph Classification Through the Lenses of Density.](http://arxiv.org/abs/2307.14849) | 本文提出了一种基于密度的反事实搜索框架，利用图的主要特征生成图分类器的实例级反事实解释。 |
| [^29] | [Kernelised Normalising Flows.](http://arxiv.org/abs/2307.14839) | 本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。 |
| [^30] | [Fading memory as inductive bias in residual recurrent networks.](http://arxiv.org/abs/2307.14823) | 通过使用残差连接作为归纳偏差，我们提出了一种弱耦合残差循环网络，并研究了其对网络性能、动力学和记忆属性的影响。我们发现，几种不同形式的残差连接可以增加网络的表达能力，并产生有效的归纳偏差。 |
| [^31] | [Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction.](http://arxiv.org/abs/2307.14788) | 本文提出了一个多阶段概率方法用于轨迹预测，引入了一种新的深度特征聚类方法，并提出了一种新颖的基于距离的排名建议，该系统在处理分布转移时表现更好，同时在轨迹数据中表现优于无环深度生成模型。 |
| [^32] | [Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset.](http://arxiv.org/abs/2307.14783) | Emotion4MIDI是一个包含12k个带情感标签的符号音乐数据集，通过在GoEmotions数据集上训练情感分类模型，并应用于两个大规模的MIDI数据集的歌词，提供了一个宝贵的资源来探索音乐和情感之间的联系，并开发可以根据特定情感生成音乐的模型。 |
| [^33] | [MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data.](http://arxiv.org/abs/2307.14778) | 本研究提出了MATNilm框架，能够通过样本增强和共享的分层拆分结构，在有限标记数据下提高非侵入式负载监测的性能。 |
| [^34] | [Towards Practicable Sequential Shift Detectors.](http://arxiv.org/abs/2307.14758) | 这项研究讨论了在机器学习模型中分布变迁的有害影响，提出了顺序变迁检测器的可行部署需求，并推荐了未来研究的重要方向。 |
| [^35] | [Fair Machine Unlearning: Data Removal while Mitigating Disparities.](http://arxiv.org/abs/2307.14754) | 本研究提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的机器学习方法。 |
| [^36] | [FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks.](http://arxiv.org/abs/2307.14751) | FLARE是第一个用于验证疑似深度强化学习策略是否是另一个策略的非法副本的指纹机制，通过使用通用对抗性掩码作为指纹，并测量动作一致性值来验证被盗策略的真实所有权。 |
| [^37] | [Semantic Image Completion and Enhancement using GANs.](http://arxiv.org/abs/2307.14748) | 本文介绍了使用生成对抗网络（GAN）进行语义图像补全和增强的方法。 |
| [^38] | [A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory.](http://arxiv.org/abs/2307.14732) | 这篇论文提出了一个综合方法来分析足球中的1对1射门情况，利用了机器学习、理论模型和博弈论。通过使用机器学习模型估计预期收益和特征提取，能够量化分析射门决策的策略，从而为决策提供客观依据。 |
| [^39] | [Understanding Silent Failures in Medical Image Classification.](http://arxiv.org/abs/2307.14729) | 这项研究通过对医学应用中的分类系统进行分析，发现目前的置信度评分函数无法可靠地防止隐性失败，强调了对数据中失败根本原因的深入理解的重要性。引入了SF-Visuals，一个通过潜在空间聚类来可视化偏移和失败的交互式分析工具。 |
| [^40] | [TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting.](http://arxiv.org/abs/2307.14680) | TimeGNN是一种学习动态时间图表示的方法，可以捕捉多个系列之间模式的演变以及多个系列之间的相关性。它在推断时间上比其他方法快4到80倍，并且达到了相当的预测性能。 |
| [^41] | [Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification.](http://arxiv.org/abs/2307.14675) | 本研究使用物理信息神经网络对风力涡轮机的历史数据进行建模，通过施加物理约束，成功地预测了涡轮机的功率、转矩和功率系数。这对于优化涡轮机运行和早期故障检测具有重要的实际应用价值。 |
| [^42] | [Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment.](http://arxiv.org/abs/2307.14668) | 本文提出了一个模型不可知的后处理框架 xOrder，用于实现二分排名的公平性且能保持算法分类性能。它通过优化一个加权效用的和来确定不同受保护组之间的最佳路径，并兼容各种分类模型和排名公平性指标。实验证明，xOrder能够在保持分类性能的同时实现较好的公平性。 |
| [^43] | [Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance.](http://arxiv.org/abs/2307.14657) | 本论文研究了恶意软件检测和分类中的机器学习模型，发现数据集的性质和分布以及训练数据集的家族和样本数量对性能有影响，并且静态和动态特征相互补充。 |
| [^44] | [Machine Learning based Parameter Sensitivity of Regional Climate Models -- A Case Study of the WRF Model for Heat Extremes over Southeast Australia.](http://arxiv.org/abs/2307.14654) | 本研究通过基于机器学习的方法，探究了WRF模型在澳大利亚东南部对表面气象变量敏感性的研究，为热极端事件的准确模拟和评估提供了重要的帮助。 |
| [^45] | [Speed Limits for Deep Learning.](http://arxiv.org/abs/2307.14653) | 研究使用随机热力学方法，根据权重分布间的Wasserstein-2距离和熵产生速率，提供了对深度学习网络从初始状态到完全训练的最大速度限制。通过应用于线性和可线性化的神经网络，结果表明，在某些缩放假设下，学习在某种程度上是最优的。 |
| [^46] | [Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models.](http://arxiv.org/abs/2307.14648) | 本论文研究了将去噪扩散概率模型从像素空间转换到小波空间的方法，设计了SFUNet架构，能够同时捕捉到空间和频率领域中的相关性，从而生成质量更高的图像。 |
| [^47] | [MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy.](http://arxiv.org/abs/2307.14643) | 本文提出了一种基于最大类间差异和最小冗余的非参数特征选择算法(MVMR-FS)，通过使用有监督和无监督核密度估计来度量特征之间的相关性和冗余，并提出了最大类间差异和最小冗余的准则(MVMR)，以辅助特征选择。 |
| [^48] | [Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?.](http://arxiv.org/abs/2307.14642) | 本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。 |
| [^49] | [Fact-Checking of AI-Generated Reports.](http://arxiv.org/abs/2307.14634) | 本文提出了一种使用相关联的图像对AI生成报告进行事实核查的新方法，以区分报告中的真假句子。这对加快临床工作流程，提高准确性并降低总体成本具有重要意义。 |
| [^50] | [Rapid and Scalable Bayesian AB Testing.](http://arxiv.org/abs/2307.14628) | 该论文提出了一种快速可扩展的贝叶斯AB测试方法，通过应用分层贝叶斯估计来克服了常见的AB测试分析方法所存在的限制，包括多变量设计、相关性、顺序测试和汇总过去测试知识等问题。这种方法能够增加统计功效、实现顺序测试和渐进式早停止，同时降低错误判断的风险。 |
| [^51] | [BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning.](http://arxiv.org/abs/2307.14623) | BubbleML是一个用于机器学习的多物理数据集，通过物理驱动模拟获得准确的地面真实信息，并在各种沸腾场景中验证了其可靠性和潜力。 |
| [^52] | [Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior.](http://arxiv.org/abs/2307.14619) | 本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。 |
| [^53] | [Self-Contrastive Graph Diffusion Network.](http://arxiv.org/abs/2307.14613) | 提出了一种名为自对比图扩散网络（SCGDN）的新型框架，通过注意力模块和扩散模块实现对高阶结构和特征信息的优秀嵌入。与现有的方法不同，SCGDN是一种无增强的方法，避免了“采样偏差”和语义漂移问题。 |
| [^54] | [Complete and separate: Conditional separation with missing target source attribute completion.](http://arxiv.org/abs/2307.14609) | 本论文提出了一种新的条件分离方法，通过训练一个模型来提取语义数据，然后利用这个模型来改进多条件分离网络的性能。实验证明，这个方法接近具有完整语义信息的预设模型的性能水平，并且与最佳性能的单条件模型相媲美。 |
| [^55] | [HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting.](http://arxiv.org/abs/2307.14596) | HUTFormer是一种用于长期交通预测的分层U-Net Transformer模型，通过利用多尺度表示来解决长期预测中的挑战和问题。 |
| [^56] | [MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation.](http://arxiv.org/abs/2307.14588) | 提出了一种名为MCPA的多尺度交叉感知器注意力网络，用于二维医学图像分割。该网络通过多尺度交叉感知器模块捕捉局部相关性，并有效地融合全局特征，以克服UNet架构的局限性。 |
| [^57] | [Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning.](http://arxiv.org/abs/2307.14568) | 本研究评估了在深度强化学习中考虑安全约束的自主导航，在比较了安全和不安全两种学习策略后发现，安全策略能够生成更安全的轨迹，避免碰撞，而不影响整体性能。 |
| [^58] | [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples.](http://arxiv.org/abs/2307.14565) | Auto-Tables系统能自动合成多步转换的流水线，将非关系式表格转换为关系式表格，解决了非技术用户使用SQL分析工具的痛点。 |
| [^59] | [Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application.](http://arxiv.org/abs/2307.14549) | 本论文提出了解决在线推荐系统中具有多个游戏的睡眠赌博问题的高效算法，该算法能够保证理论性能，并且后悔上界为$\bigO(kN^2\sqrt{T\log T})$。 |
| [^60] | [Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum.](http://arxiv.org/abs/2307.14531) | 本文引入了Modified Spectrum Kernels（MSKs）构造核族，通过预条件梯度下降方法，实现了对宽神经网络归纳偏差的控制，并在不改变最终解的情况下，加速了训练速度。 |
| [^61] | [Optimal Estimation in Mixed-Membership Stochastic Block Models.](http://arxiv.org/abs/2307.14530) | 本论文研究了重叠社区检测问题，在混合成员随机块模型的基础上提出了一个新的估计器，并建立了估计误差的极小下界。 |
| [^62] | [Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM.](http://arxiv.org/abs/2307.14528) | 本论文提出了一种基于采样损失值和自适应步长的SGD变体，通过解决有限和问题，达到了较佳的收敛速度，并开发了一种逐渐学习损失值的算法。 |
| [^63] | [Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad.](http://arxiv.org/abs/2307.14527) | 该论文介绍了在荒野搜救中应用计算机视觉系统的挑战，提出了使用EfficientDET模型和无监督RX光谱分类器的方法，但在真实世界中存在假阳性的问题。 |
| [^64] | [Bug Characterization in Machine Learning-based Systems.](http://arxiv.org/abs/2307.14512) | 该论文研究了机器学习系统中错误的特征和维护挑战，并比较了ML和非ML错误。通过对使用最流行的ML框架的GitHub仓库进行调查，作者发现了一些有关错误的见解。 |
| [^65] | [The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions.](http://arxiv.org/abs/2307.14502) | 本文研究了在语音增强中，使用与嘈杂数据语言完全匹配的自监督语音表示损失函数训练的模型的性能更好。与传统的频谱图或时间域损失函数相比，这些增强模型具有特定语言的特性。 |
| [^66] | [A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing.](http://arxiv.org/abs/2307.14500) | 本研究提出了一个预测数字信息参与的新模型READ，通过融合认知偏差、计算语言学和自然语言处理，成功预测了英文字的参与水平，并区分了更具吸引力的词汇。 |
| [^67] | [HUGE: Huge Unsupervised Graph Embeddings with TPUs.](http://arxiv.org/abs/2307.14490) | 本文提出了一种利用TPU进行高性能图嵌入的架构，能够处理具有数十亿节点和数万亿边的图。验证了嵌入空间质量。 |
| [^68] | [Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization.](http://arxiv.org/abs/2307.14482) | 本研究评估了自动医学图像分割模型在适应新的图像采集和疾病类型方面的泛化能力，结果显示使用多样化数据训练的模型在域内测试数据上表现良好。 |
| [^69] | [Limits to Reservoir Learning.](http://arxiv.org/abs/2307.14474) | 这项工作限制了机器学习的能力，基于物理学所暗示的计算限制。储水库计算机在噪声下的性能下降意味着需要指数数量的样本来学习函数族，并讨论了没有噪声时的性能。 |
| [^70] | [What Kinds of Contracts Do ML APIs Need?.](http://arxiv.org/abs/2307.14465) | 这项研究研究了机器学习API的合同类型，以帮助API用户在早期阶段捕捉错误，并通过对四个ML库的帖子进行实证研究，提取了413个API规范，以了解ML合同违规的根本原因、常见模式和是否需要先进的ML软件专业知识。 |
| [^71] | [Training Quantum Boltzmann Machines with Coresets.](http://arxiv.org/abs/2307.14459) | 这篇论文研究了在近期量子设备上应用量子玻尔兹曼机算法的加速方法，利用核心集合技术代替完整数据集，最小化训练过程中的计算瓶颈并加速整体训练时间。 |
| [^72] | [Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches.](http://arxiv.org/abs/2307.14453) | 本研究提出了一种基于机器学习的预测性维护系统，通过分析传感器数据预测装甲车辆的维护需求，结果表明系统具有高准确率和稳定性。这种方法有助于减少车辆停机时间，提高运营效率。 |
| [^73] | [VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions.](http://arxiv.org/abs/2307.14448) | VISPUR是一个提供视觉分析和人本工作流程的系统，用于识别和解释数据驱动决策中的虚假关联。它包括混淆因素仪表盘和子群浏览器，可以帮助人们定位、推理和预防虚假关联。 |
| [^74] | [Fixed Integral Neural Networks.](http://arxiv.org/abs/2307.14439) | 本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。 |
| [^75] | [Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity.](http://arxiv.org/abs/2307.14403) | 这篇论文提出了一个基于无监督深度学习的Pansharpening模型，通过全分辨率训练和特定损失函数的使用，充分利用了这种方法的潜力，具有优秀的性能。 |
| [^76] | [A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot.](http://arxiv.org/abs/2307.14397) | 本文调查了在有限数据、少样本和零样本条件下学习生成模型的情况，并提出了关于任务和方法的分类体系，研究了它们之间的相互作用，并探讨了未来的研究方向。 |
| [^77] | [Learning to simulate partially known spatio-temporal dynamics with trainable difference operators.](http://arxiv.org/abs/2307.14395) | 本文提出了一种新的混合架构PDE-Net++，通过结合可训练差分算子和黑盒模型，明确嵌入了底层PDE的部分先验知识。数值实验证明，PDE-Net++具有比黑盒模型更高的预测准确性和更好的外推性能。 |
| [^78] | [Hypergraph Isomorphism Computation.](http://arxiv.org/abs/2307.14394) | 本文提出了用于超图同构计算的超图Weisfiler-Lehman测试算法以解决现有方法无法直接处理的高阶结构关系问题。 |
| [^79] | [Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG.](http://arxiv.org/abs/2307.14389) | 本研究提出了一种使用去噪的扩散概率模型和条件自编码器进行想象言语的脑电图解码的新颖方法。结果表明，与传统机器学习技术和基准模型相比，该方法显著提高了解码准确性，这对于发展通过想象言语进行交流的脑-机接口具有潜在的影响。 |
| [^80] | [HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning.](http://arxiv.org/abs/2307.14384) | 提出了HyperFed，用于解决联邦学习中非相同和独立数据分布造成的性能问题。该方法通过超球面原型探索、超球面原型学习和一致聚合等模块的结合，来解决类别统计的变化、层级信息利用不足和客户端聚合的不一致性问题。 |
| [^81] | [When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review.](http://arxiv.org/abs/2307.14382) | 本综述讨论了多任务学习如何在部分监督设置下应用，以解决由于复杂的优化方案和高标签需求而引入的挑战。 |
| [^82] | [EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence.](http://arxiv.org/abs/2307.14381) | EdgeConvEns是一种卷积集成学习方法，用于在边缘网络上训练和集成异构的弱模型，以满足计算能力有限和分布式数据处理的需求。 |
| [^83] | [Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations.](http://arxiv.org/abs/2307.14380) | 本文解决了主动学习中错误数据注释的问题，提出了两种利用未标记样本空间的新型标注统一算法。 |
| [^84] | [DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm.](http://arxiv.org/abs/2307.14375) | 本研究提出了一种新颖的数据自适应Bregman聚类算法（DBGSA），通过结合普适引力算法、引入Bregman分裂广义幂信息损失最小化方法和构建超参数识别优化模型，有效解决了传统聚类算法在初始质心选择敏感和处理非凸数据集的问题。 |
| [^85] | [Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design.](http://arxiv.org/abs/2307.14374) | 本研究综合应用时间序列分析、机器学习和材料设计，对欧洲国家和印度的不同行业CO2排放进行了全面预测和分析，发现电力、工业和陆地交通部门是数据集中变异最大的贡献者。 |
| [^86] | [Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks.](http://arxiv.org/abs/2307.14373) | 本文研究了使用无限宽度、有限成本的浅层ReLU神经网络以及修正线性单元作为激活函数来表示连续分段线性函数。通过将度量从参数空间映射到函数定义域中的超平面，证明了这种神经网络可以表示任意分段线性函数。 |
| [^87] | [Prediction of depression status in college students using a Naive Bayes classifier based machine learning model.](http://arxiv.org/abs/2307.14371) | 通过朴素贝叶斯分类器的机器学习模型可以预测大学生的抑郁状态，该模型在早期发现和治疗抑郁方面具有有效性，有助于改善学生群体的心理健康。 |
| [^88] | [Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers.](http://arxiv.org/abs/2307.14367) | 提出了一种名为Prot2Text的新方法，通过结合GNNs和Transformers，以自由文本样式预测蛋白质的功能。该方法能够综合蛋白质的序列、结构和文本注释等多种数据类型，超越传统的二进制或分类分类，实现了对蛋白质功能的全面表示。 |
| [^89] | [Explainable Disparity Compensation for Efficient Fair Ranking.](http://arxiv.org/abs/2307.14366) | 这项研究提出了解释性的数据驱动的排名函数补偿措施，通过给予低调群体成员奖励积分来解决公平排序中的不平等问题。 |
| [^90] | [Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis.](http://arxiv.org/abs/2307.14364) | 本文提出了一个名为ASPIRE算法的异步分布式算法，用于解决联邦分布鲁棒优化问题，并引入了约束的D-范数不确定性集合，以灵活控制鲁棒性的程度。 |
| [^91] | [Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields.](http://arxiv.org/abs/2307.14363) | 本文提出了一种无监督的方法，名为NF-cMRI，利用神经场表示重建加速心脏动态MRI，实验结果表明可以在较大的低采样率下获得良好的图像质量并改进时间描述。 |
| [^92] | [Learnable wavelet neural networks for cosmological inference.](http://arxiv.org/abs/2307.14362) | 本论文研究了在宇宙学推断中应用可学习的散射变换方法，与传统的两点统计相比，这种方法能够提取更多信息并很好地消除天体物理效应的影响。实验证明，在小样本训练数据集上，散射网络的性能优于卷积神经网络。 |
| [^93] | [A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe.](http://arxiv.org/abs/2307.14361) | 本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。 |
| [^94] | [A new derivative-free optimization method: Gaussian Crunching Search.](http://arxiv.org/abs/2307.14359) | 这篇论文介绍了一种新的无导数优化方法，高斯压缩搜索（GCS），通过模拟高斯分布中粒子的行为，该方法能够高效地探索解空间并找到全局最优解。通过实验评估和与其他方法的比较，突出了GCS的优势和潜力。 |
| [^95] | [Learned Gridification for Efficient Point Cloud Processing.](http://arxiv.org/abs/2307.14354) | 本论文提出了学习网格化方法，用于将点云转换为紧凑、规则的网格，在点云处理中提高了可扩展性，特别是对于大输入和大邻域。 |
| [^96] | [Multi-objective Deep Reinforcement Learning for Mobile Edge Computing.](http://arxiv.org/abs/2307.14346) | 本研究提出了一种多目标深度强化学习方法，以解决移动边缘计算中的离线问题。该方法通过考虑未知偏好参数，最小化能耗和传输延迟，并采用近端策略优化算法进行资源调度。引入了一种特征构建方法，用于处理MEC系统中的多个边缘。 |
| [^97] | [Pruning Distorted Images in MNIST Handwritten Digits.](http://arxiv.org/abs/2307.14343) | 本论文提出了一个两阶段的深度学习方法，通过识别和剔除扭曲和模糊图像，从而提高了MNIST数据集中手写数字的分类准确性和置信度。 |
| [^98] | [Pre-Training with Diffusion models for Dental Radiography segmentation.](http://arxiv.org/abs/2307.14066) | 本文提出了一种利用扩散模型进行预训练的方法，用于牙科放射学分割，实验结果表明该方法在标签效率方面具有显著性能，竞争力强。 |
| [^99] | [How to Scale Your EMA.](http://arxiv.org/abs/2307.13813) | 本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。 |
| [^100] | [Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items.](http://arxiv.org/abs/2307.13709) | 本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。 |
| [^101] | [Duet: efficient and scalable hybriD neUral rElation undersTanding.](http://arxiv.org/abs/2307.13494) | Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。 |
| [^102] | [Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations.](http://arxiv.org/abs/2307.13423) | 本研究将自学习语音表示应用于预测听力障碍个体的可理解性，并发现其作为非侵入式预测模型的输入特征具有竞争性能，需要更多数据才能推广到未知系统和个体。 |
| [^103] | [On the Vulnerability of Fairness Constrained Learning to Malicious Noise.](http://arxiv.org/abs/2307.11892) | 这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\Theta(\alpha)$和$O(\sqrt{\alpha})$，对应不同的公正约束要求。 |
| [^104] | [TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars.](http://arxiv.org/abs/2307.10705) | 本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。 |
| [^105] | [TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations.](http://arxiv.org/abs/2307.09916) | TimeTuner是一个新颖的可视化分析框架，旨在帮助分析人员理解时间序列预测中模型行为与时间表示的关系，并解决自动化特征学习方法的局限性。 |
| [^106] | [A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models.](http://arxiv.org/abs/2307.05946) | 本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。 |
| [^107] | [Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance.](http://arxiv.org/abs/2307.03811) | 该论文提出了一种深度学习模型，Formulation Graph Convolution Network（F-GCN），它可以将电池电解质的结构组成关系映射到整个液体配方的性能，从而加快新化合物的发现和应用。 |
| [^108] | [Differential Privacy for Clustering Under Continual Observation.](http://arxiv.org/abs/2307.03430) | 本文提出了一种在持续观察下的差分隐私聚类机制，用于在被删除和插入数据点的数据集中进行聚类，这是第一个具有仅以更新次数的对数依赖性的增加误差的近似算法，并且乘法误差几乎与非隐私情况相同。 |
| [^109] | [Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification.](http://arxiv.org/abs/2307.02377) | 本研究介绍了Fraunhofer SIT团队在CLEF-2023 CheckThat!英语实验室任务1B中使用模型混合技术解决分类不确定性的方法，该方法的目的是确定政治辩论中的文本片段是否值得进行事实检查评估，并在比赛中排名第二。 |
| [^110] | [Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation.](http://arxiv.org/abs/2307.01524) | 该论文提出了使用学习压缩表示进行语义分割的方法，以减少解压缩操作的延迟开销，并在实验证明了这种方法的有效性。 |
| [^111] | [Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets.](http://arxiv.org/abs/2307.00610) | 本文提出了一种混合单模分类器的方法，通过组合图像和文本分类器的结果，成功进行多模态推文的可靠性估计，并在CheckThat! 2023任务1A中取得了最佳表现。 |
| [^112] | [2D-Shapley: A Framework for Fragmented Data Valuation.](http://arxiv.org/abs/2306.10473) | 该论文提出了2D-Shapley框架，用于估值碎片化数据。该框架通过计算从聚合数据矩阵中移除一个碎片的对立现实，实现了对碎片化数据源的估值。而且，2D-Shapley满足碎片化数据环境下的一些重要公理，为选择有用的数据片段、解释样本数据值和进行精细化的数据问题诊断等提供了新的使用案例。 |
| [^113] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^114] | [Automating Model Comparison in Factor Graphs.](http://arxiv.org/abs/2306.05965) | 本文基于自定义混合节点 Forney 样式的因子图消息传递，实现了高效自动化贝叶斯模型平均、选择和组合，并缩短了模型设计周期。 |
| [^115] | [Group Equivariant Fourier Neural Operators for Partial Differential Equations.](http://arxiv.org/abs/2306.05697) | 本文扩展了群卷积到频率域，并设计了具有旋转、平移和镜像等变性质的傅里叶层的G-FNO架构，该架构在不同对称性水平的设置中表现良好。 |
| [^116] | [Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation.](http://arxiv.org/abs/2306.04169) | 本文提出了一种高效的求解加权低秩逼近问题的交替最小化框架，运行时间优化到了 n^2k，核心方法是一种高精度的多响应回归方法。 |
| [^117] | [PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning.](http://arxiv.org/abs/2305.19472) | PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法， |
| [^118] | [Exploring Weight Balancing on Long-Tailed Recognition Problem.](http://arxiv.org/abs/2305.16573) | 研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。 |
| [^119] | [Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation.](http://arxiv.org/abs/2305.13849) | 本文提出了一种自监督高斯正则化的深度分类器，可用于马氏距离不确定性评估，相比现有方法，该方法不需要对模型架构和训练程序做出大的改变，并在标准OOD基准测试上取得了最先进的性能。 |
| [^120] | [On Learning the Tail Quantiles of Driving Behavior Distributions via Quantile Regression and Flows.](http://arxiv.org/abs/2305.13106) | 该论文提出了两种学习驾驶行为分布尾部分位数的方法，分别是基于分位数回归和流量学习的框架，并在自动驾驶中进行了评估和验证。 |
| [^121] | [Differential Convolutional Fuzzy Time Series Forecasting.](http://arxiv.org/abs/2305.08890) | 本文提出了一种新的预测模型DFCNN，利用卷积神经网络实现具有可学习能力的FTSF，并能够处理非平稳时间序列。 |
| [^122] | [Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services.](http://arxiv.org/abs/2305.02109) | 本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。 |
| [^123] | [VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data.](http://arxiv.org/abs/2304.13037) | VeML是一种专门用于大规模高维数据的端到端机器学习生命周期的版本管理系统，在解决生命周期高成本问题、数据相似性计算和数据模式分析等关键问题方面表现出色。 |
| [^124] | [CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments.](http://arxiv.org/abs/2304.06848) | 本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。 |
| [^125] | [FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising.](http://arxiv.org/abs/2304.00570) | 本研究提出了FedFTN，一种个性化的联邦学习策略，用于解决多机构低计数PET去噪中的领域差异问题。该方法通过深度特征转换网络来改善低计数PET图像质量，同时避免了集中式数据集的隐私和安全问题。 |
| [^126] | [Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning.](http://arxiv.org/abs/2303.10135) | 本文提出了一种基于图表示学习的装配序列规划方法，通过GRACE模型可以从装配图中提取信息并预测可行的装配序列。 |
| [^127] | [Spectral learning of Bernoulli linear dynamical systems models.](http://arxiv.org/abs/2303.02060) | 该论文提出了一种用于快速、高效拟合概率波-伯努利潜在线性动力系统模型的谱学习方法。这种方法通过转换样本矩的方式将传统的子空间识别方法扩展到了伯努利设置中，得到了一个鲁棒的固定成本估计器。在数据有限的情况下，谱估计可以为Laplace-EM拟合提供良好的初始化。 |
| [^128] | [Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems.](http://arxiv.org/abs/2303.01669) | 本研究提出了一种学习常见理由的方法来改善细粒度视觉识别问题中的自监督表示。通过识别常见的区分性线索，这种方法能够有效克服自监督学习在细粒度视觉识别中的局限性。 |
| [^129] | [Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection.](http://arxiv.org/abs/2302.12012) | 本研究对使用不同降维和分类技术进行癫痫发作检测进行了实证分析，通过离散小波变换和机器学习分类器，结合主成分分析、独立成分分析和线性判别分析等降维算法，选择特征来提高检测准确性。 |
| [^130] | [Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales.](http://arxiv.org/abs/2302.08720) | 本论文将新兴的图像超分辨率技术应用于统计降尺度任务，具体探索了基于生成对抗网络的算法在模拟北美地区地表风中的应用。通过使用非理想化的低分辨率和高分辨率输入数据，该方法克服了共享尺度不匹配的问题，并通过评估空间功率谱等指标来评估模型的技能。 |
| [^131] | [Factor Fields: A Unified Framework for Neural Fields and Beyond.](http://arxiv.org/abs/2302.01226) | 因子场是一种统一的框架，将信号分解为因子的乘积，通过神经场或常规场表示进行处理。这个框架可以概括多种最近的信号表示方法，并且可以创建出更强大的新信号表示，如本文提出的系数基函数分解（CoBaFa）。实验结果表明，CoBaFa在逼近质量、紧凑性和效率方面相较于之前的方法有所改进。 |
| [^132] | [Causal Lifting and Link Prediction.](http://arxiv.org/abs/2302.01198) | 本文开发了第一个能够处理链路预测中路径依赖的因果模型，并介绍了因果提升的概念，通过有限的干预数据识别因果链路预测查询。 |
| [^133] | [Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver.](http://arxiv.org/abs/2301.01913) | 本论文提出了一种通用学习过程，用于在约束规划求解器内获取一个值选择启发式方法，以解决当前通用值选择启发式方法较为稀缺的问题。 |
| [^134] | [MixupE: Understanding and Improving Mixup from Directional Derivative Perspective.](http://arxiv.org/abs/2212.13381) | 本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。 |
| [^135] | [Scalable Bayesian Uncertainty Quantification for Neural Network Potentials: Promise and Pitfalls.](http://arxiv.org/abs/2212.07959) | 神经网络势的贝叶斯不确定性量化方法通过随机梯度MCMC实现可扩展的UQ，可准确估计MD可观测量，要求更少的训练数据和多个马尔科夫链。 |
| [^136] | [Contrastive Domain Adaptation for Time-Series via Temporal Mixup.](http://arxiv.org/abs/2212.01555) | 本文提出了一种基于时间混合的对比域自适应方法，用于解决时序数据中的领域偏移问题。 |
| [^137] | [Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality.](http://arxiv.org/abs/2211.08944) | 本文揭示了随机恢复算法相对于确定性恢复算法的优势，包括可以达到完美感知质量、对抗性攻击的鲁棒性和提高输出可变性。这些发现将进一步支持随机估计器的使用。 |
| [^138] | [Large Language Models Struggle to Learn Long-Tail Knowledge.](http://arxiv.org/abs/2211.08411) | 本文研究了大型语言模型记忆的知识与预训练数据集中信息之间的关系，并发现其回答基于事实的问题的能力与在预训练过程中接触到的相关文档数量之间存在强相关性和因果关系。 |
| [^139] | [Predicting Winning Regions in Parity Games via Graph Neural Networks (Extended Abstract).](http://arxiv.org/abs/2210.09924) | 通过图神经网络，我们提出了一种不完全多项式时间复杂度的方法来确定平价游戏的获胜区域，并且在900个随机生成的游戏中表现出了较高的有效性和效率。 |
| [^140] | [Learning Transfer Operators by Kernel Density Estimation.](http://arxiv.org/abs/2210.03124) | 本研究通过核密度估计方法，重新解释了传递算子的推断问题，并通过实例表明了该方法在估计Frobenius-Perron算子的特征向量上的有效性和有效性。 |
| [^141] | [Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization.](http://arxiv.org/abs/2209.10825) | 本论文提出了一种名为smoothed PLDA的算法来有效处理广泛的结构化非光滑非凸非凹极小极大问题，并证明了其具有全局收敛性，复杂度为O(epsilon^(-2/3))。 |
| [^142] | [Statistical process monitoring of artificial neural networks.](http://arxiv.org/abs/2209.07436) | 这篇论文提出了一种基于人工神经网络生成的数据潜在特征表示的监控方法，以确定数据流开始变得非平稳的时间。该方法通过应用基于数据深度计算和归一化排名的多元控制图进行监测，并与各种基准方法进行了比较。 |
| [^143] | [Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks.](http://arxiv.org/abs/2209.06589) | 本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。 |
| [^144] | [Learning Task Automata for Reinforcement Learning using Hidden Markov Models.](http://arxiv.org/abs/2208.11838) | 本文提出了一种学习非马尔可夫任务规范的新方法，通过从代理经验中学习，将其表示为有限状态的任务自动机。利用隐马尔可夫模型和新的提炼方法，使得代理能够解决稀疏和非马尔可夫奖励的强化学习任务。 |
| [^145] | [On the non-efficient PAC learnability of conjunctive queries.](http://arxiv.org/abs/2208.10255) | 这篇论文阐述了连词查询在PAC模型中的非高效可学习性，针对不同变种的连词查询提出了负面可学习性结果，并展示了通过成员查询可以高效学习连词查询和UCQs。 |
| [^146] | [Neural Networks for Scalar Input and Functional Output.](http://arxiv.org/abs/2208.05776) | 该论文提出了一种解决标量输入和函数输出之间回归问题的方法，使用前馈神经网络预测函数响应。该方法适用于大量预测变量或非线性关系，并可以控制预测曲线的平滑程度。在实验中验证了方法的有效性。 |
| [^147] | [Visual Pre-training for Navigation: What Can We Learn from Noise?.](http://arxiv.org/abs/2207.00052) | 本论文提出了一种使用随机裁剪预测进行自监督训练的视觉预训练方法，可以学习到对导航任务有用的表示，并通过自举学习有效地学习导航策略，减少对交互数据的需求。 |
| [^148] | [Trace Recovery from Stochastically Known Logs.](http://arxiv.org/abs/2206.12672) | 本文提出了一种从随机已知日志中恢复轨迹的算法，在两个公开数据集上平均恢复准确度达到90-97%。这一方法通过计算流程模型和随机已知轨迹的合规性，并恢复在该随机轨迹中的最佳对齐作为真实轨迹。对比其他轨迹恢复选项，使用了产品多图来分析成本模型对恢复准确性的影响。这一算法对于预测模型开发、错误排查和系统性能改进具有重要意义。 |
| [^149] | [Analyzing Explainer Robustness via Lipschitzness of Prediction Functions.](http://arxiv.org/abs/2206.12481) | 本文研究了通过评估预测函数的Lipschitz率来分析解释器的稳健性。通过引入解释器敏锐性的概念并与预测器的概率Lipschitz率相联系，我们提供了解释器敏锐性的下界保证。 |
| [^150] | [Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs.](http://arxiv.org/abs/2206.10291) | 该论文通过引入杠杆分数稀疏（LESS）嵌入的草图技术，提供了一种算法框架，实现了对数据分布的高斯化，从而能够高效地构建与次高斯随机设计几乎无法区分的数据草图。 |
| [^151] | [Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning.](http://arxiv.org/abs/2205.14704) | 本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。 |
| [^152] | [Deep learning of quantum entanglement from incomplete measurements.](http://arxiv.org/abs/2205.01462) | 该论文使用深度学习的方法，通过神经网络量化了量子系统中的纠缠程度，无需完全描述量子态，仅使用不完全的局部测量集即可实现比传统方法更低一个量级的量化误差，且仅使用模拟数据进行训练。 |
| [^153] | [RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes.](http://arxiv.org/abs/2112.13934) | RELDEC是一种基于强化学习的中等长度LDPC码的解码方法，通过训练智能体顺序调度CN集群，以优化解码策略。同时，通过改进MDP的状态空间表示，适用于更大块长的LDPC码。为了解决不同信道条件下的解码问题，提出了AM-RELDEC算法。 |
| [^154] | [Towards Out-Of-Distribution Generalization: A Survey.](http://arxiv.org/abs/2108.13624) | 这项研究调查了超出分布泛化的问题，该问题涉及到当测试数据的分布与训练数据不同时，模型性能下降的情况。这是对该问题的首次全面回顾，涵盖了问题定义、方法论发展、评估程序以及领域的意义和未来方向。 |
| [^155] | [Machine Learning with a Reject Option: A survey.](http://arxiv.org/abs/2107.11277) | 这项调查综述了机器学习中的拒绝选项。通过机器学习模型避免在可能犯错误时做出预测，可以在决策支持应用中避免严重后果。调查介绍了拒绝选项的条件、评估策略以及相关应用领域，并探讨了它与其他机器学习方法的关系。 |
| [^156] | [Compositional federated learning: Applications in distributionally robust averaging and meta learning.](http://arxiv.org/abs/2106.11264) | 本文提出了一种名为ComFedL的组合式联邦学习算法，用于解决分布鲁棒联邦学习和模型不可知元学习问题，通过使用KL散度正则化，将分布鲁棒联邦学习转化为简单的组合优化问题。已证明ComFedL算法具有收敛速度为O(1/√T)，创新性地将联邦学习与组合随机优化结合在一起。 |
| [^157] | [Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning.](http://arxiv.org/abs/2106.03328) | 该论文提出了一个名为Multi-RoundSecAgg的安全聚合框架，解决了联邦学习中多轮隐私泄漏的问题，通过介绍新的隐私指标和开发结构化的用户选择策略实现了多轮隐私保证。 |
| [^158] | [Dynamics of specialization in neural modules under resource constraints.](http://arxiv.org/abs/2106.02626) | 本研究使用人工神经网络模拟实验，发现结构模块化并不一定能够确保功能专业化，在特定环境和资源限制下，才能够出现专业化现象。 |
| [^159] | [Experimental Study on Reinforcement Learning-based Control of an Acrobot.](http://arxiv.org/abs/2011.09246) | 本研究通过强化学习算法控制了Acrobot的角速度和能量，实现了未驱动摆杆的摇摆或完全旋转，并深入探究了影响强化学习控制的参数。 |
| [^160] | [On the Generalization Effects of Linear Transformations in Data Augmentation.](http://arxiv.org/abs/2005.00695) | 这项研究考虑了一类线性转换，并研究了其在过参数化线性回归设置中对岭估计量的影响。研究发现，能够保持数据标签的转换可以通过扩大训练数据的张量来改善估计结果；而混合数据的转换则通过起到正则化作用来改善估计结果。此外，通过在MNIST数据集上进行验证，研究者提出了一个增强方案，该方案通过模型对转换后数据的不确定性进行搜索转换空间，并在图像和文本数据集上验证了其有效性。 |
| [^161] | [Declarative Mechanism Design.](http://arxiv.org/abs/1912.13122) | 本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。 |

# 详细

[^1]: 关于(Normalised) Discounted Cumulative Gain作为Top-n推荐的离线评估指标的论文翻译

    On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])

    [http://arxiv.org/abs/2307.15053](http://arxiv.org/abs/2307.15053)

    本文批判性审视了(Normalised) Discounted Cumulative Gain作为Top-n推荐离线评估指标的方法，并研究了何时可以期望这些指标逼近在线实验的金标准结果。

    

    推荐方法通常通过两种方式进行评估：(1) 通过(模拟)在线实验，通常被视为金标准，或者(2) 通过一些离线评估程序，目标是近似在线实验的结果。文献中采用了几种离线评估指标，受信息检索领域中常见的排名指标的启发。(Normalised) Discounted Cumulative Gain (nDCG)是其中一种广泛采用的度量标准，在很多年里，更高的(n)DCG值被用来展示新方法在Top-n推荐中的最新进展。我们的工作对这种方法进行了批判性的审视，并研究了我们何时可以期望这些指标逼近在线实验的金标准结果。我们从第一原理上正式提出了DCG被认为是在线奖励的无偏估计的假设，并给出了这个指标的推导。

    Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.  Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles
    
[^2]: 基于Transformer的阿拉伯离线手写文本识别方法

    A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])

    [http://arxiv.org/abs/2307.15045](http://arxiv.org/abs/2307.15045)

    本文提出了基于Transformer的阿拉伯离线手写文本识别方法，通过引入Transformer Transducer和标准的序列到序列Transformer架构，解决了循环神经网络的并行化和语言规则不考虑的问题，具有较高的准确性和速度。

    

    手写识别是模式识别和机器学习领域中具有挑战性和关键性的问题，其应用领域广泛。本文着重研究识别离线阿拉伯手写文本的特定问题。现有的方法通常利用卷积神经网络进行图像特征提取和循环神经网络进行时间建模，使用联结时序分类进行文本生成。然而，由于循环神经网络的序列性质，这些方法缺乏并行化。此外，这些模型无法考虑语言规则，因此需要在后处理阶段使用外部语言模型来提高准确性。为了解决这些问题，我们引入了两种替代架构，即Transformer Transducer和标准的序列到序列Transformer，并比较它们在准确性和速度方面的性能。

    Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and spee
    
[^3]: 对齐语言模型上的通用和可迁移对抗攻击

    Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])

    [http://arxiv.org/abs/2307.15043](http://arxiv.org/abs/2307.15043)

    这项研究提出了一种简单而有效的攻击方法，能够使对齐的语言模型生成不良行为，而不依赖于人工设计，通过自动化方法产生对抗性后缀，并在实践中取得改进。

    

    由于“开箱即用”的大型语言模型能够生成大量引起反感的内容，最新的研究专注于对齐这些模型，以防止产生不良生成。尽管在规避这些措施上取得了一些成功，所谓的对LLMs的“越狱”攻击，但这些攻击需要人为的巧思，实际上并不稳定。在本文中，我们提出了一种简单而有效的攻击方法，使对齐的语言模型生成不良行为。具体而言，我们的方法找到一个后缀，当附加到各种查询上，供LLM生成不良内容时，旨在最大化模型产生肯定回答（而不是拒绝回答）的概率。然而，与其依赖手工设计，我们的方法通过贪婪和基于梯度的搜索技术自动产生这些对抗性后缀，并且在过去的自动化方法上进行了改进。

    Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
    
[^4]: 通过混合精度加速傅里叶神经算子

    Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])

    [http://arxiv.org/abs/2307.15034](http://arxiv.org/abs/2307.15034)

    通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。

    

    傅里叶神经算子（FNO）是一种强大的技术，用于学习偏微分方程（PDE）解算器的代理映射。对于许多现实世界的应用，通常需要高分辨率的数据点，训练时间和内存使用量都是重要瓶颈。虽然对于标准神经网络有混合精度训练技术，但那些只适用于有限维度上的实值数据类型，因此不能直接应用于在复值（傅里叶）域和函数空间中重要操作的FNO。另一方面，由于傅里叶变换本身就是一次近似（由于离散化误差的存在），我们不需要以完全精度执行操作。在这项工作中，我们（i）对使用全精度和混合精度训练的FNO进行内存和运行时间剖析，（ii）对混合精度训练的数值稳定性进行研究，以及（iii）设计了一种训练过程，大大缩短了训练时间和内存使用率。

    The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor
    
[^5]: 自监督图变换器用于深度伪造检测

    Self-Supervised Graph Transformer for Deepfake Detection. (arXiv:2307.15019v1 [cs.CV])

    [http://arxiv.org/abs/2307.15019](http://arxiv.org/abs/2307.15019)

    这篇论文介绍了一种利用自监督预训练模型的深度伪造检测框架，具有出色的泛化能力，可以应对常见的破坏，并实现特征解释性。

    

    深度伪造检测方法在识别给定数据集中的伪造物方面取得了有希望的结果，其中训练和测试在内部分发的数据集上进行。然而，当面对未知样本时，它们的性能显著下降。因此，可靠的深度伪造检测系统必须对伪造类型、外观和质量保持公正，以确保可广泛应用的检测性能。尽管有各种尝试提高跨数据集泛化能力，但问题仍然具有挑战性，特别是当测试常见的后处理扰动时，如视频压缩或模糊。因此，本研究引入了一个深度伪造检测框架，利用自监督预训练模型，具有出色的泛化能力，可以承受常见的破坏，并实现特征解释性。该框架包括三个关键组件：基于视觉变换器架构的特征提取器，通过自监督方式进行预训练

    Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self
    
[^6]: 私有联邦数据分析的可采样匿名聚合

    Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])

    [http://arxiv.org/abs/2307.15017](http://arxiv.org/abs/2307.15017)

    本论文在解决每个设备持有私有数据情况下的私有统计和私有联邦学习设计中，提出了一个简单的原语，以实现高效的算法，并在不需要强信任假设的情况下保护隐私。

    

    在每个设备持有私有数据的情况下，我们重新审视设计可扩展的私有统计协议和私有联邦学习的问题。我们的第一个贡献是提出了一个简单的原语，可以有效地实现几种常用算法，并且可以在不需要强信任假设的情况下进行隐私账务，接近于集中设置中的隐私保护。其次，我们提出了一个实现该原语的系统架构，并对该系统进行了安全性分析。

    We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
    
[^7]: Google Bard的视觉理解能力如何？开放挑战的实证研究。

    How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])

    [http://arxiv.org/abs/2307.15016](http://arxiv.org/abs/2307.15016)

    本研究探索了Google Bard在理解和解释文本问题条件下的视觉数据方面的能力，并发现Bard在各种视觉场景中仍然存在困境，这凸显出在视觉理解方面存在重要的差距。

    

    Google的Bard在对话型人工智能领域与OpenAI的ChatGPT成为了强大的竞争对手。值得注意的是，Bard最近已经更新，可以在对话过程中处理文本提示和视觉输入。鉴于Bard在处理文本输入方面的出色表现，我们探索了其在理解和解释由文本问题条件下的视觉数据（图像）方面的能力。这种探索有潜力揭示Bard和其他即将发布的多模式生成模型在解决需要准确的视觉和语言理解的复杂计算机视觉问题时的新见解和挑战。具体而言，在这项研究中，我们专注于15个不同的任务场景，包括常规、伪装、医学、水下和遥感数据，全面评估了Bard的性能。我们的主要发现表明，Bard在这些视觉场景中仍然存在困境，突显了在基于视觉的理解方面存在的重要差距。

    Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs t
    
[^8]: 利用合成活性粒子进行物理储层计算

    Harnessing Synthetic Active Particles for Physical Reservoir Computing. (arXiv:2307.15010v1 [cond-mat.soft])

    [http://arxiv.org/abs/2307.15010](http://arxiv.org/abs/2307.15010)

    本研究展示了利用合成活性微粒系统进行物理储层计算，通过自组织和自耦合的特性，实现了具有噪声非线性动力学的预测任务。

    

    信息处理是生物系统不可或缺的特性，由具有巨大复杂性的活动过程网络实现。这些网络激发了许多现代机器学习的变体之一，即储层计算。通过在具有衰落记忆的节点网络中实现储层计算，可以进行计算和复杂预测。储层可在计算机硬件上实现，也可以在非传统的物理基质上实现，如机械振子、自旋或细菌，通常被称为物理储层计算。在这里，我们展示了一个自组织的合成活性微粒系统进行物理储层计算，该系统由活性和被动组成的成分形成固有的噪声非线性动力学单元。单元的自组织和动力响应是通过微游动器对被动目标的延迟推进来实现的。通过延迟响应的自耦合，这样的单元储层可以执行预测任务，尽管存在强烈的噪声和非线性特性。

    The processing of information is an indispensable property of living systems realized by networks of active processes with enormous complexity. They have inspired many variants of modern machine learning one of them being reservoir computing, in which stimulating a network of nodes with fading memory enables computations and complex predictions. Reservoirs are implemented on computer hardware, but also on unconventional physical substrates such as mechanical oscillators, spins, or bacteria often summarized as physical reservoir computing. Here we demonstrate physical reservoir computing with a synthetic active microparticle system that self-organizes from an active and passive component into inherently noisy nonlinear dynamical units. The self-organization and dynamical response of the unit is the result of a delayed propulsion of the microswimmer to a passive target. A reservoir of such units with a self-coupling via the delayed response can perform predictive tasks despite the strong
    
[^9]: 通过LLM辅助，对AI-Guardian的攻击研究

    A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])

    [http://arxiv.org/abs/2307.15008](http://arxiv.org/abs/2307.15008)

    本文研究了LLM是否能辅助进行对抗性机器学习研究，以AI-Guardian为案例评估了其鲁棒性。研究发现，我们成功破解了AI-Guardian的防御机制，并且通过指示和引导GPT-4实施攻击算法的方法非常有效和高效。

    

    如今，大型语言模型（LLMs）在各种任务上都能够表现出很高的能力。本文研究了一种名为GPT-4的LLM是否能够辅助进行对抗性机器学习研究。以AI-Guardian为案例，我们评估了这个最近在IEEE S&P 2023上发表的针对对抗样本的防御机制的鲁棒性。我们完全破解了这个防御机制：与未防御的基线相比，所提出的方案并没有增加鲁棒性。我们并没有编写攻击该模型的代码，而是指示和引导GPT-4按照我们的指令实施所有攻击算法。这个过程出奇地有效和高效，有时候语言模型在不明确的指令下产生的代码比本文作者还要快。最后，我们讨论了（1）评估中出现的警示信号表明AI-Guardian将被攻破，以及（2）我们在设计和实施攻击方案时的经验。

    Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.  We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing a
    
[^10]: 可验证的特征归因：后期解释性和内在可解释性的桥梁

    Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])

    [http://arxiv.org/abs/2307.15007](http://arxiv.org/abs/2307.15007)

    本文旨在搭建一座桥梁，将后期解释性和内在可解释性相结合，以解释复杂的黑盒模型的行为，并解决解释的忠实性和可验证性之间的问题。

    

    随着机器学习模型在各种实际应用中的部署增加，研究人员和从业者一直强调对模型行为的解释需求。为此，在以前的文献中概述了两种广泛的策略来解释模型。后期解释方法通过突出显示对模型预测至关重要的特征来解释复杂的黑盒模型的行为；然而，之前的工作表明这些解释可能不忠实，更令人担忧的是我们无法验证它们。另一方面，内在可解释模型通过将解释性信息明确编码到模型架构中来规避这些问题，这意味着它们的解释自然忠实且可验证，但由于其有限的表达能力，它们通常表现出较差的预测性能。在这项工作中，我们旨在搭建一座桥梁，将后期解释性和内在可解释性相结合。

    With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge t
    
[^11]: Thinker: 学习规划和行动

    Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])

    [http://arxiv.org/abs/2307.14993](http://arxiv.org/abs/2307.14993)

    Thinker算法通过引入世界模型和模型交互动作使强化学习代理实现自主规划，消除了手工设计规划算法的需求，并且在Sokoban游戏和Atari 2600基准测试中取得了state-of-the-art的性能。

    

    我们提出了Thinker算法，一种新颖的方法，使强化学习代理能够自主地与学习的世界模型进行交互并利用其。 Thinker算法通过给环境添加世界模型来改变环境，并引入了用于与世界模型交互的新动作。这些模型交互动作使代理能够通过在选择最终的环境动作之前向世界模型提出备选计划来进行规划。该方法通过使代理自主学习如何进行规划来消除了手工设计的规划算法的需求，并且允许对代理的计划进行易于解释的可视化。我们通过Sokoban游戏和Atari 2600基准测试的实验结果证明了该算法的有效性，其中Thinker算法分别实现了最先进的性能和有竞争力的结果。使用Thinker算法训练的代理的可视化结果表明，它们已经学到了优秀的规划策略。

    We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear
    
[^12]: 增量计算的神经网络：处理动态输入的高效推理方法

    Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])

    [http://arxiv.org/abs/2307.14988](http://arxiv.org/abs/2307.14988)

    本论文介绍了一种增量计算的神经网络方法，通过离散化中间值并过滤不必要的修改，实现了对动态输入的高效推理。

    

    深度学习在处理动态输入（例如传感器数据或用户输入）时常面临着高效处理的挑战。本论文提出了一种增量计算的方法，通过重复使用计算来适应输入变化，以解决这个问题。我们使用向量量化来离散化网络中的中间值，并过滤噪声和不必要的隐藏神经元修改，从而促进值的重用。我们将此方法应用于Transformer架构，创建了一个高效的增量推理算法。

    Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexi
    
[^13]: Take-A-Photo: 三维到二维的点云模型生成预训练

    Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])

    [http://arxiv.org/abs/2307.14971](http://arxiv.org/abs/2307.14971)

    本文提出了一种新的三维到二维的生成式预训练方法，通过生成视图图像作为预训练方案，帮助三维模型更好地理解点云的几何结构和立体关系，并在实验证明了其优越性。

    

    在MAE带领下，生成式预训练在2D视觉领域已经显示出显著的潜力来提升基本模型的性能。然而，在3D视觉领域，对Transformer为基础的骨干网络的过度依赖以及点云的无序性限制了生成式预训练的进一步发展。本文提出了一种新颖的适用于任何点云模型的三维到二维的生成式预训练方法。我们通过交叉注意机制从不同的姿势生成视图图像作为预训练方案。相比于其点云对应物，生成视图图像具有更精确的监督，从而帮助3D背骨更好地理解点云的几何结构和立体关系。实验结果证明了我们提出的三维到二维的生成式预训练方法优于先前的预训练方法。我们的方法还能有效地提升...

    With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
    
[^14]: 学习自组织活动粒子系统中局部主导力平衡

    Learning locally dominant force balances in active particle systems. (arXiv:2307.14970v1 [cond-mat.soft])

    [http://arxiv.org/abs/2307.14970](http://arxiv.org/abs/2307.14970)

    通过无监督聚类和稀疏推理算法，我们学习了自组织活动粒子系统中解释宏观模式形成的局部主导力平衡。分析结果表明，传播带是由密度梯度驱动的局部对齐相互作用形成的，而稳态星环是由扭曲引起的负压缩性机制塑造的。

    

    我们使用无监督聚类和稀疏推理算法的组合来学习解释自组织活动粒子系统中宏观模式形成的局部主导力平衡。自组织活动粒子系统中的宏观模式来自于自驱动粒子之间的微观相互作用，是一种广泛观察到的自然现象。尽管流体动力学理论帮助我们更好地理解这一现象的物理基础，但是在活动粒子系统中，识别出塑造、调节和维持自组织结构的局部相互作用的足够集合仍然具有挑战性。我们研究了一个经典的自驱动粒子流体动力学模型，它产生了各种各样的模式，如星环和移动密度带。我们的数据驱动分析显示，传播带是由密度梯度驱动的局部对齐相互作用形成的，而稳态星环是由一种扭曲引起的负压缩性机制塑造的。

    We use a combination of unsupervised clustering and sparsity-promoting inference algorithms to learn locally dominant force balances that explain macroscopic pattern formation in self-organized active particle systems. The self-organized emergence of macroscopic patterns from microscopic interactions between self-propelled particles can be widely observed nature. Although hydrodynamic theories help us better understand the physical basis of this phenomenon, identifying a sufficient set of local interactions that shape, regulate, and sustain self-organized structures in active particle systems remains challenging. We investigate a classic hydrodynamic model of self-propelled particles that produces a wide variety of patterns, like asters and moving density bands. Our data-driven analysis shows that propagating bands are formed by local alignment interactions driven by density gradients, while steady-state asters are shaped by a mechanism of splay-induced negative compressibility arising
    
[^15]: 通过自监督先验在高度不平衡的医学图像分类中实现联邦模型聚合

    Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])

    [http://arxiv.org/abs/2307.14959](http://arxiv.org/abs/2307.14959)

    本文研究了在高度不平衡的医学图像分类中通过自监督先验实现联邦模型聚合。根据利用公开可用的自监督辅助网络发现，在每个客户端上局部使用共享的预训练模型可以得到一致的发散测量结果。基于这些发现，提出了一个通过自监督先验引导全局模型优化的动态平衡模型聚合方法，用于实现高度鲁棒和无偏的全局模型。

    

    在医学领域，联邦学习通常处理高度不平衡的数据集，包括皮肤病变和胃肠图像。现有的联邦方法在高度不平衡的数据集上主要关注优化全局模型，而没有考虑到由于不同人群、发现和扫描仪导致的医学图像中可能出现的类内变化。在本文中，我们利用公开可用的自监督辅助网络研究了客户间的类内变化。具体而言，我们发现在每个客户端上局部使用共享的辅助预训练模型（如MoCo-V2）可以得到一致的发散测量结果。基于这些发现，我们提出了一个通过自监督先验（MAS）引导全局模型优化的动态平衡模型聚合方法。Fed-MAS可以与不同的局部学习方法结合使用，实现对高度鲁棒和无偏全局模型的有效聚合。

    In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \url{https://github.com
    
[^16]: 在Wasserstein空间中通过数据集字典学习进行多源域自适应

    Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])

    [http://arxiv.org/abs/2307.14953](http://arxiv.org/abs/2307.14953)

    本文提出了一种基于字典学习和最优传输的MSDA框架，通过将每个域表示为字典原子的Wasserstein重心来缓解数据分布偏移。根据该字典，提出了两种新的MSDA方法，分别基于目标域标记样本的重构和在原子分布上学习的分类器的集成。在多个基准测试集上进行的实验证明，这些方法在分类任务上取得了显著的改进效果。

    

    本文旨在解决多源域自适应（MSDA）问题，该问题旨在在从多个标记的源域转移知识到未标记的目标域时缓解数据分布偏移。我们提出了一种基于字典学习和最优传输的新型MSDA框架。我们将MSDA中的每个域解释为经验分布。因此，我们将每个域表达为字典原子的Wasserstein重心，这些原子是经验分布。我们提出了一种新的通过小批量学习的算法DaDiL：（i）原子分布；（ii）重心坐标矩阵。根据我们的字典，我们提出了两种新的MSDA方法：DaDiL-R，基于目标域标记样本的重构；DaDiL-E，基于在原子分布上学习的分类器的集成。我们在3个基准测试集中评估了我们的方法：Caltech-Office、Office 31和CRWU，在分类上改进了以前的最先进技术3.15％、2.29％和7.71％。

    This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
    
[^17]: 网络容错和拜占庭鲁棒的社交学习通过协作式分层非贝叶斯学习

    Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning. (arXiv:2307.14952v1 [cs.LG])

    [http://arxiv.org/abs/2307.14952](http://arxiv.org/abs/2307.14952)

    本文提出了一种分层的网络容错和拜占庭鲁棒的社交学习算法，解决了网络通信故障和敌对攻击的挑战，并设计了一种稀疏信息融合规则和具有可证明收敛性的算法。

    

    随着网络规模的增大，现有的完全分布式解决方案开始落后于现实世界的挑战，如（1）信息传播缓慢，（2）网络通信故障和（3）外部敌对攻击。本文关注分层系统架构，并解决了容易受到通信故障和敌对攻击的网络上的非贝叶斯学习问题。在网络通信方面，考虑包丢失的链路故障。我们首先提出了一种分层鲁棒的push-sum算法，可以在频繁的包丢失链路故障情况下实现平均一致性。我们为参数服务器和任意选择的网络代表之间提供了一种稀疏信息融合规则。然后，将一致性更新步骤与以KL散度作为邻近函数的双均值更新步骤交替进行，得到了一种具有可证明收敛性的容错非贝叶斯学习算法。

    As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks. In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks. On network communication, we consider packet-dropping link failures.  We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures. We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives. Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence gu
    
[^18]: 一种自适应惩罚方法用于将先验知识约束集成到神经常微分方程中

    A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])

    [http://arxiv.org/abs/2307.14940](http://arxiv.org/abs/2307.14940)

    本论文提出了一种自适应惩罚算法，用于将约束的自然系统集成到神经常微分方程中，通过引入先验知识提高了模型的可解释性，并通过数值实验证明了其有效性。

    

    使用神经常微分方程(Neural ODEs)，可以有效地对自然系统的连续动态进行建模。然而，为了实现准确而有意义的预测，模型必须遵循管理这些系统的基本规律或定律。在本论文中，我们提出了一种自适应惩罚算法，用于将约束的自然系统集成到神经常微分方程中。所提出的自适应惩罚函数可以动态调整惩罚参数。引入先验知识有助于增加基于神经常微分方程的模型的可解释性。我们通过模拟三个具有先验知识约束的自然系统(人口增长，化学反应演化和阻尼谐振运动)来验证所提出的方法。数值实验证明了所提出的自适应惩罚算法在神经常微分方程中的有效性，并与其他惩罚神经常微分方程方法和原始神经常微分方程进行了比较。

    The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural
    
[^19]: 高效互动感知神经网络反馈环的区间分析

    Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])

    [http://arxiv.org/abs/2307.14938](http://arxiv.org/abs/2307.14938)

    本文提出了一种计算效率高的神经网络控制系统区间可达性分析框架，通过引入包含函数和构建嵌入系统来捕捉系统和神经网络控制器之间的相互作用。

    

    本文提出了一种计算效率高的神经网络控制系统区间可达性分析框架。我们的方法基于神经网络控制器和开环系统的包含函数。我们观察到，许多最先进的神经网络验证器可以为神经网络生成包含函数。我们介绍并分析了一种基于函数雅可比边界的开环动力学包含函数的新类别，特别适用于捕捉系统和神经网络控制器之间的相互作用。接下来，对于任意动力系统，我们使用包含函数构建一个状态数是原系统两倍的嵌入系统。我们证明嵌入系统的单个轨迹可以提供可达集的超矩形近似。然后，我们提出了两种构建神经网络控制动力系统的闭环嵌入系统的方法，考虑系统之间的互动。

    In this paper, we propose a computationally efficient framework for interval reachability of neural network controlled systems. Our approach builds upon inclusion functions for the neural network controller and the open-loop system. We observe that many state-of-the-art neural network verifiers can produce inclusion functions for neural networks. We introduce and analyze a new class of inclusion functions for the open-loop dynamics based on bounds of the function Jacobian that is particularly suitable for capturing the interactions between systems and neural network controllers. Next, for any dynamical system, we use inclusion functions to construct an embedding system with twice the number of states as the original system. We show that a single trajectory of this embedding system provides hyper-rectangular over-approximations of reachable sets. We then propose two approaches for constructing a closed-loop embedding system for a neural network controlled dynamical system that accounts 
    
[^20]: PanGu-Coder2：利用排名反馈增强大型语言模型在代码生成方面的能力

    PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])

    [http://arxiv.org/abs/2307.14936](http://arxiv.org/abs/2307.14936)

    本文提出了一种新的框架RRTF，以增强预训练的大型语言模型在代码生成方面的能力。PanGu-Coder2是该框架的实现，在多个基准测试中均表现出色，优于其他先前的Code LLMs。

    

    大型语言模型（Code LLM）在代码生成任务上展现出了卓越的性能，每周都有新的强大模型发布。为了提高预训练的Code LLM的代码生成性能，提出了各种方法，如有监督的微调、指令微调、强化学习等。本文提出了一种新颖的RRTF（Rank Responses to align Test&Teacher Feedback）框架，可以有效、高效地提升预训练的大型语言模型在代码生成方面的能力。在该框架下，我们提出了PanGu-Coder2，在OpenAI HumanEval基准上取得了62.20%的一级通过率。此外，通过对CoderEval和LeetCode基准的广泛评估，我们展示了PanGu-Coder2始终优于所有先前的Code LLMs。

    Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
    
[^21]: 使用Desbordante解决数据质量问题：一项演示

    Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])

    [http://arxiv.org/abs/2307.14935](http://arxiv.org/abs/2307.14935)

    Desbordante是一个旨在解决数据质量问题的工具，通过发现和验证复杂统计信息来帮助现代数据科学家进行数据概要分析。它提供了与现有工具的适当集成，同时考虑到工业级工作负载，并提供描述性的解释来解释模式缺失的原因。

    

    数据概要分析是现代数据驱动行业中的重要过程。其中一个关键组成部分是发现和验证复杂统计信息，包括函数依赖、数据约束、关联规则等。然而，大多数现有的专注于复杂统计的数据概要分析系统在与现代数据科学家使用的工具进行适当集成方面存在问题。这在行业中对这些工具采用造成了重大障碍。此外，现有系统并不考虑工业级工作负载。最后，它们不旨在提供描述性的解释，即为什么找不到给定的模式。这是一个重要问题，因为了解特定模式缺失的根本原因对基于数据的明智决策至关重要。因此，这些模式实际上是消失在空中中：它们的应用范围相对有限，很少被广大公众使用。

    Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.  However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.  Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the
    
[^22]: 基于图的多音轨音乐生成

    Graph-based Polyphonic Multitrack Music Generation. (arXiv:2307.14928v1 [cs.SD])

    [http://arxiv.org/abs/2307.14928](http://arxiv.org/abs/2307.14928)

    这篇论文介绍了一种基于图的多音轨音乐生成方法，通过引入深度变分自动编码器，分别生成音乐图的结构和内容，实现了通过指定乐器来条件生成音乐，为音乐共创的人机互动提供了新的可能性。

    

    图可以用来建模多音轨的符号音乐，其中音符、和弦和整个片段可以按照音调和节奏的关系在不同层次上连接。然而，在音乐生成的深度学习系统中，缺乏考虑图表示的工作。本文通过引入一种新颖的图表示和深度变分自动编码器来弥补这一差距，该自动编码器分别生成音乐图的结构和内容，其层次结构与音乐的结构先验相匹配。通过将音乐图的结构和内容分离，可以通过指定特定时间播放的乐器来条件生成，为音乐共创的人机互动打开了新的可能性。实验表明，在现有的MIDI数据集上训练模型后，该模型能够生成...

    Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generat
    
[^23]: 在两个HPC系统上对深度学习模型进行材料分割性能基准测试

    Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems. (arXiv:2307.14921v1 [cs.PF])

    [http://arxiv.org/abs/2307.14921](http://arxiv.org/abs/2307.14921)

    该论文开发了一个基准测试工具，使用机器学习模型在两个HPC系统上进行材料分割性能分析，并发现Vulcanite在大多数测试中具有更快的模型时间，但容易受到环境因素影响，而Onyx的模型时间在所有测试中保持一致。

    

    HPC系统的性能基准测试是一个持续的工作，旨在提供信息，以增加性能并改善管理这些系统的作业调度程序。我们开发了一个基准测试工具，利用机器学习模型，在GPU加速节点上进行材料分割分析并收集性能数据。该基准测试使用了经过MMdnn工具包将Caffe转换为PyTorch的ML模型和MINC-2500数据集。在两个ERDC DSRC系统Onyx和Vulcanite上收集了性能数据。数据显示，尽管Vulcanite在大多数基准测试中具有更快的模型时间，但它也更容易受到一些环境因素的影响，导致性能低于Onyx。相比之下，Onyx的模型时间在所有基准测试中保持一致。

    Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems. We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis. The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx. In contrast the model times from Onyx are consistent across benchmarks.
    
[^24]: NSA: 自然支持性物件以提高网络的可信度

    NSA: Naturalistic Support Artifact to Boost Network Confidence. (arXiv:2307.14917v1 [cs.CV])

    [http://arxiv.org/abs/2307.14917](http://arxiv.org/abs/2307.14917)

    这项工作提出了自然支持性物件（NSA）的概念，用于训练稳健的视觉人工智能模型以抵御自然破坏。NSAs是通过使用DC-GAN进行物件训练生成的自然外观物件。

    

    视觉人工智能系统容易受到现实世界中自然和合成物理破坏的影响。这种破坏通常出现意外并改变模型的性能。近年来，主要关注的是对抗性攻击。然而，自然破坏（例如雪、雾、尘土）对于视觉人工智能系统来说是一个无处不在的威胁，同样应该被视为重要。许多现有的作品提出了有趣的解决方案，用于训练抗击自然破坏的强大模型。这些作品要么利用图像增强，这会增加模型培训的成本，要么在场景中放置可疑的补丁来设计非对抗性示例。在这项工作中，我们提出了自然支持性物件（NSA）的概念来进行稳健预测。NSAs在模型参数不可访问且在场景中添加物件可行的情况下显示出益处。NSAs是通过使用DC-GAN进行物件训练生成的自然外观物件，具有较高的可视真实性。

    Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world. Such corruption often arises unexpectedly and alters the model's performance. In recent years, the primary focus has been on adversarial attacks. However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important. Many existing works propose interesting solutions to train robust models against natural corruption. These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples. In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction. The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible. The NSAs are natural looking objects generated through artifact training using DC-GAN to have high v
    
[^25]: 使用优化的负采样和损失函数扩展基于会话的Transformer推荐系统

    Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])

    [http://arxiv.org/abs/2307.14906](http://arxiv.org/abs/2307.14906)

    本文介绍了一种使用优化的负采样和损失函数扩展基于会话的Transformer推荐系统，该系统在大规模电商数据集上通过集成负采样和列表损失函数实现了较高的推荐准确性，并在实践中表现出潜力。

    

    本文介绍了TRON，一种使用优化的负采样的可扩展的基于会话的Transformer推荐系统。受到SASRec和GRU4Rec+等现有模型在可扩展性和性能方面的限制，TRON集成了top-k负采样和列表损失函数，以提高其推荐准确性。在相关的大规模电子商务数据集上的评估结果表明，TRON在保持与SASRec类似的训练速度的同时，改进了当前方法的推荐质量。一项实时的A/B测试显示，相对于SASRec，TRON的点击率增加了18.14%，突显了其在实际环境中的潜力。

    This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
    
[^26]: CodeLens: 一种用于可视化代码表示的交互式工具

    CodeLens: An Interactive Tool for Visualizing Code Representations. (arXiv:2307.14902v1 [cs.SE])

    [http://arxiv.org/abs/2307.14902](http://arxiv.org/abs/2307.14902)

    CodeLens是一种可视化代码表示的交互式工具，支持多种表示方法和编程语言，开发人员能够快速理解和探索代码。

    

    将源代码以通用输入格式表示对于自动化软件工程任务至关重要，例如应用机器学习算法来提取信息。可视化代码表示可以进一步使人类专家直观地理解代码。然而，到目前为止，还没有一种通用工具能够同时可视化不同类型的代码表示。在本文中，我们介绍了一种名为CodeLens的工具，它提供了一个可视化交互环境，支持各种表示方法，并帮助开发人员理解和探索它们。CodeLens被设计成支持多种编程语言，如Java、Python和JavaScript，以及四种代码表示类型，包括标记序列、抽象语法树（AST）、数据流图（DFG）和控制流图（CFG）。通过使用CodeLens，开发人员可以快速可视化特定的代码表示，并获取代码模型的表示输入。

    Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The W
    
[^27]: 干燥大气边界层的生成对流参数化

    Generative convective parametrization of dry atmospheric boundary layer. (arXiv:2307.14857v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.14857](http://arxiv.org/abs/2307.14857)

    该研究提出了一种基于生成对抗网络的干燥对流边界层参数化方案，通过结合经典混合层理论中的自相似层生长物理学，显著改善了合成湍流场的统计预测性能。

    

    湍流参数化在公里尺度地球系统模型中仍然是必要的基础。在对流边界层中，平均垂直梯度的保守性质，如潜在温度和湿度，近似为零，标准的模型将湍流通量与平均垂直梯度之间的涡扩散系数延展为用质量通量参数化的模型， 用于大气边界层中的通常非对称的上升气流和下沉气流。在这项工作中，我们提出了基于生成对抗网络的干燥对流边界层参数化方案。该模型结合了Deardorff的经典混合层理论中的自相似层生长物理学，提高了生成机器学习算法的训练数据集，并显著改善了边界层内不同高度产生的合成湍流场的统计预测性能。

    Turbulence parametrizations will remain a necessary building block in kilometer-scale Earth system models. In convective boundary layers, where the mean vertical gradients of conserved properties such as potential temperature and moisture are approximately zero, the standard ansatz which relates turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be extended by mass flux parametrizations for the typically asymmetric up- and downdrafts in the atmospheric boundary layer. In this work, we present a parametrization for a dry convective boundary layer based on a generative adversarial network. The model incorporates the physics of self-similar layer growth following from the classical mixed layer theory by Deardorff. This enhances the training data base of the generative machine learning algorithm and thus significantly improves the predicted statistics of the synthetically generated turbulence fields at different heights inside the boundary layer. The algorithm train
    
[^28]: 通过密度视角进行图分类的反事实解释

    Counterfactual Explanations for Graph Classification Through the Lenses of Density. (arXiv:2307.14849v1 [cs.LG])

    [http://arxiv.org/abs/2307.14849](http://arxiv.org/abs/2307.14849)

    本文提出了一种基于密度的反事实搜索框架，利用图的主要特征生成图分类器的实例级反事实解释。

    

    反事实例子已经成为一种产生简单易懂的事后解释的有效方法。在图分类的背景下，先前的工作主要集中在通过改变图的最基本单元（即删除现有边或添加不存在的边）来生成反事实解释。本文认为这种解释方式可能太细致，转而关注真实世界复杂网络的一些主要特征，如闭合三角形的趋势、重复的模式以及组织成密集模块。因此，我们提出了一种基于密度的反事实搜索框架，用于为图分类器生成实例级反事实解释，该框架可以用不同的密集子结构概念实例化。特别地，我们展示了该通用框架的两个具体实例化方法：一种通过搜索反事实图来匹配密集子结构测度的方法和一种通过搜索反事实子图来匹配密集子图的方法。

    Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by 
    
[^29]: 核化归一化流

    Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])

    [http://arxiv.org/abs/2307.14839](http://arxiv.org/abs/2307.14839)

    本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。

    

    归一化流是以其可逆的架构而被描述的生成模型。然而，可逆性要求对其表达能力施加限制，需要大量的参数和创新的架构设计来达到满意的结果。虽然基于流的模型主要依赖于基于神经网络的转换来实现表达能力，但替代的转换方法却受到了有限的关注。在这项工作中，我们提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到框架中。我们的结果表明，相比于基于神经网络的流，核化流可以产生有竞争力或优越的结果，同时保持参数效率。核化流在低数据环境中表现出色，可以在数据稀缺的应用中进行灵活的非参数密度估计。

    Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
    
[^30]: 残差循环网络中的褪退记忆作为归纳偏差

    Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])

    [http://arxiv.org/abs/2307.14823](http://arxiv.org/abs/2307.14823)

    通过使用残差连接作为归纳偏差，我们提出了一种弱耦合残差循环网络，并研究了其对网络性能、动力学和记忆属性的影响。我们发现，几种不同形式的残差连接可以增加网络的表达能力，并产生有效的归纳偏差。

    

    残差连接被提出作为一种基于架构的归纳偏差，以解决反向传播算法训练的前馈和循环神经网络中梯度爆炸和消失的问题，并提高任务性能。然而，我们对残差连接如何影响循环神经网络的动态和褪退记忆属性知之甚少。在这里，我们引入了弱耦合残差循环网络(WCRNNs)，其中残差连接导致了明确定义的李雅普诺夫指数，并允许研究褪退记忆的属性。我们研究了WCRNNs的残差连接如何影响它们的性能、网络动力学和记忆属性在一组基准任务上。我们发现，几种不同形式的残差连接产生了有效的归纳偏差，从而增加了网络的表达能力。特别是，具有以下特点的残差连接：(i) 导致网络动态接近混沌的边缘，(ii) 允许网络在长时间内保持记忆。

    Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks
    
[^31]: 可能的，轻量的和准确的基于无环文法聚类的轨迹预测

    Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction. (arXiv:2307.14788v1 [cs.LG])

    [http://arxiv.org/abs/2307.14788](http://arxiv.org/abs/2307.14788)

    本文提出了一个多阶段概率方法用于轨迹预测，引入了一种新的深度特征聚类方法，并提出了一种新颖的基于距离的排名建议，该系统在处理分布转移时表现更好，同时在轨迹数据中表现优于无环深度生成模型。

    

    道路交通网络中的自主系统需要智能机制来应对不确定性以预测未来。本文提出了一个多阶段概率方法用于轨迹预测：轨迹转换成位移空间，位移时间序列的聚类，轨迹建议和排名建议。我们引入了一种新的深度特征聚类方法，基于自身条件的GAN，该方法在处理分布转移方面比传统方法更好。此外，我们提出了一种新颖的基于距离的排名建议，用于为生成的轨迹分配概率，比辅助神经网络更高效而准确。总体系统在人类和道路代理轨迹数据中超过了无环深度生成模型，而在比较最可能的轨迹时与点估计器的表现相似。

    Autonomous systems in the road transportation network require intelligent mechanisms that cope with uncertainty to foresee the future. In this paper, we propose a multi-stage probabilistic approach for trajectory forecasting: trajectory transformation to displacement space, clustering of displacement time series, trajectory proposals, and ranking proposals. We introduce a new deep feature clustering method, underlying self-conditioned GAN, which copes better with distribution shifts than traditional methods. Additionally, we propose novel distance-based ranking proposals to assign probabilities to the generated trajectories that are more efficient yet accurate than an auxiliary neural network. The overall system surpasses context-free deep generative models in human and road agents trajectory data while performing similarly to point estimators when comparing the most probable trajectory.
    
[^32]: Emotion4MIDI：基于歌词的带情感标签的符号音乐数据集

    Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])

    [http://arxiv.org/abs/2307.14783](http://arxiv.org/abs/2307.14783)

    Emotion4MIDI是一个包含12k个带情感标签的符号音乐数据集，通过在GoEmotions数据集上训练情感分类模型，并应用于两个大规模的MIDI数据集的歌词，提供了一个宝贵的资源来探索音乐和情感之间的联系，并开发可以根据特定情感生成音乐的模型。

    

    我们提出了一个新的大规模带情感标签的符号音乐数据集，包含12k个MIDI歌曲。为了创建这个数据集，我们首先在GoEmotions数据集上训练情感分类模型，使用一半大小的模型实现了最先进的结果。然后，我们将这些模型应用于两个大规模的MIDI数据集的歌词。我们的数据集涵盖了广泛的细粒度情感，为探索音乐和情感之间的联系，尤其是开发可以根据特定情感生成音乐的模型提供了宝贵的资源。我们的推断代码、训练模型和数据集都可以在线获取。

    We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
    
[^33]: MATNilm: 有限标注数据下多设备任务的无干扰负载监测

    MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data. (arXiv:2307.14778v1 [cs.LG])

    [http://arxiv.org/abs/2307.14778](http://arxiv.org/abs/2307.14778)

    本研究提出了MATNilm框架，能够通过样本增强和共享的分层拆分结构，在有限标记数据下提高非侵入式负载监测的性能。

    

    非侵入式负载监测 (NILM) 通过将整个房屋的总功耗信号分解来识别各种家用电器的状态和电能消耗。高效准确的负载监测有助于建立用户个人资料、智能家居能源管理和峰值负荷转移。这对于最终用户和公用事业都有益处，可以提高电力分配网络的整体效率。现有的方法主要集中在为每个电器开发单独的模型。这些方法通常依赖于大量难以收集的家庭标记数据。在本文中，我们提出了一种多设备任务框架，配合一个训练高效的样本增强 (SA) 方案，以有限的标记数据提高分解性能。对于每个电器，我们开发了一个共享的分层拆分结构，用于其回归和分类任务。此外，我们还提出了一个二维注意机制。

    Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mech
    
[^34]: 迈向可行的顺序变迁检测器

    Towards Practicable Sequential Shift Detectors. (arXiv:2307.14758v1 [cs.LG])

    [http://arxiv.org/abs/2307.14758](http://arxiv.org/abs/2307.14758)

    这项研究讨论了在机器学习模型中分布变迁的有害影响，提出了顺序变迁检测器的可行部署需求，并推荐了未来研究的重要方向。

    

    越来越多人意识到分布变迁对已部署的机器学习模型性能的有害影响。因此，在相关成本积累之前检测这些变迁引起了越来越多的兴趣。然而，现有的研究通常忽视了对于可行部署顺序变迁检测器至关重要的需求，从而限制了其广泛应用。我们确定了三个此类需求，并强调了与满足这些需求相关的现有研究，同时推荐了未来研究的有影响力的方向。

    There is a growing awareness of the harmful effects of distribution shift on the performance of deployed machine learning models. Consequently, there is a growing interest in detecting these shifts before associated costs have time to accumulate. However, desiderata of crucial importance to the practicable deployment of sequential shift detectors are typically overlooked by existing works, precluding their widespread adoption. We identify three such desiderata, highlight existing works relevant to their satisfaction, and recommend impactful directions for future research.
    
[^35]: 公平机器遗忘：在减少差异的同时删除数据

    Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])

    [http://arxiv.org/abs/2307.14754](http://arxiv.org/abs/2307.14754)

    本研究提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的机器学习方法。

    

    随着公众对企业收集和使用个人信息的意识增强，消费者积极参与企业数据集的管理变得越来越重要。为此，数据管理框架（如欧洲通用数据保护条例）已经提出了被遗忘的权利，允许个人请求将其个人数据从组织使用的数据库和模型中删除。为了实现遗忘，已经提出了几种机器学习遗忘方法，以解决每个遗忘请求重新训练模型的计算效率问题。虽然这些在线替代方案可以高效地进行遗忘，但它们对于其他关键的实际应用属性（如公平性）的影响尚不清楚。在这项工作中，我们提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的方法。

    As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
    
[^36]: FLARE: 使用通用对抗性掩码对指纹深度强化学习智能体进行识别

    FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])

    [http://arxiv.org/abs/2307.14751](http://arxiv.org/abs/2307.14751)

    FLARE是第一个用于验证疑似深度强化学习策略是否是另一个策略的非法副本的指纹机制，通过使用通用对抗性掩码作为指纹，并测量动作一致性值来验证被盗策略的真实所有权。

    

    我们提出了FLARE，这是第一个用于验证疑似深度强化学习(DRL)策略是否是另一个（受害）策略的非法副本的指纹机制。我们首先展示了通过找到不可传递的、通用的对抗性掩码，即扰动，可以生成成功地从受害策略传递到其修改版本但不能传递到独立训练的策略的对抗性样本。FLARE利用这些掩码作为指纹，通过对通过掩码扰动的状态上的动作一致性值进行测量来验证被盗的DRL策略的真实所有权。我们的实证评估表明，FLARE是有效的（对于被盗副本具有100%的动作一致性），并且不会错误地指控独立策略（无误报）。FLARE还对模型修改攻击具有鲁棒性，并且不容易被更明智的对手规避而对智能体性能产生负面影响。我们还表明，并非所有的通用对抗性掩码都是适用的。

    We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable 
    
[^37]: 使用GAN进行语义图像补全和增强

    Semantic Image Completion and Enhancement using GANs. (arXiv:2307.14748v1 [cs.CV])

    [http://arxiv.org/abs/2307.14748](http://arxiv.org/abs/2307.14748)

    本文介绍了使用生成对抗网络（GAN）进行语义图像补全和增强的方法。

    

    语义修复或图像补全是指根据图像语义推断出图像中任意大小的缺失区域的任务。由于预测图像像素需要高级上下文的指示，这使得它比图像补全更困难，后者通常更关注数据损坏的纠正和从输入图像中删除整个对象。另一方面，图像增强旨在消除图像中的噪声和模糊，并保留大部分图像细节。高效的图像补全和增强模型应能够恢复图像中的损坏和掩盖区域，然后进一步改善图像，提高输出图像的质量。生成对抗网络（GAN）在图像补全任务中已被证明是有用的。在本章中，我们将讨论底层的GAN架构以及它们如何用于图像补全任务。

    Semantic inpainting or image completion alludes to the task of inferring arbitrary large missing regions in images based on image semantics. Since the prediction of image pixels requires an indication of high-level context, this makes it significantly tougher than image completion, which is often more concerned with correcting data corruption and removing entire objects from the input image. On the other hand, image enhancement attempts to eliminate unwanted noise and blur from the image, along with sustaining most of the image details. Efficient image completion and enhancement model should be able to recover the corrupted and masked regions in images and then refine the image further to increase the quality of the output image. Generative Adversarial Networks (GAN), have turned out to be helpful in picture completion tasks. In this chapter, we will discuss the underlying GAN architecture and how they can be used used for image completion tasks.
    
[^38]: 战略框架：在足球1对1射门场景中做出最佳决策的综合方法——机器学习、理论模型和博弈论的整合

    A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory. (arXiv:2307.14732v1 [cs.LG])

    [http://arxiv.org/abs/2307.14732](http://arxiv.org/abs/2307.14732)

    这篇论文提出了一个综合方法来分析足球中的1对1射门情况，利用了机器学习、理论模型和博弈论。通过使用机器学习模型估计预期收益和特征提取，能够量化分析射门决策的策略，从而为决策提供客观依据。

    

    机器学习、博弈论和其他应用领域中，两个对立主体之间的复杂交互经常发生。量化分析涉及的策略可以为决策提供客观依据。足球中的一个关键场景就是射门，决策（例如进攻者是否应该射门或传球，防守者是否应该尝试封堵射门）在比赛结果中起着至关重要的作用。然而，目前还没有有效的基于数据或基于理论的方法来分析这样的情况。为了解决这个问题，我们提出了一个基于博弈论的新框架，使用机器学习模型估计预期收益，而基于理论的射门封堵模型则提取了用于机器学习模型的附加特征。传统上，成功或失败（1或0）被用作收益，而在足球中，成功的射门（进球）极为罕见。

    Complex interactions between two opposing agents frequently occur in domains of machine learning, game theory, and other application domains. Quantitatively analyzing the strategies involved can provide an objective basis for decision-making. One such critical scenario is shot-taking in football, where decisions, such as whether the attacker should shoot or pass the ball and whether the defender should attempt to block the shot, play a crucial role in the outcome of the game. However, there are currently no effective data-driven and/or theory-based approaches to analyzing such situations. To address this issue, we proposed a novel framework to analyze such scenarios based on game theory, where we estimate the expected payoff with machine learning (ML) models, and additional features for ML models were extracted with a theory-based shot block model. Conventionally, successes or failures (1 or 0) are used as payoffs, while a success shot (goal) is extremely rare in football. Therefore, w
    
[^39]: 理解医学图像分类中的隐性失败

    Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])

    [http://arxiv.org/abs/2307.14729](http://arxiv.org/abs/2307.14729)

    这项研究通过对医学应用中的分类系统进行分析，发现目前的置信度评分函数无法可靠地防止隐性失败，强调了对数据中失败根本原因的深入理解的重要性。引入了SF-Visuals，一个通过潜在空间聚类来可视化偏移和失败的交互式分析工具。

    

    为了确保分类系统在医学应用中的可靠性使用，防止隐性失败至关重要。可以通过设计足够稳健的分类器以避免首次失败，或通过使用置信度评分函数（CSFs）检测剩余的失败来实现。图像分类中失败的主要源头是训练数据和部署数据之间的分布偏移。为了理解医学图像中防止隐性失败的当前状态，我们进行了第一次全面分析，比较了四个生物医学任务和各种分布偏移下的各种CSFs。根据结果发现，没有一个被基准化的CSF能够可靠地防止隐性失败，我们认为需要对数据中失败的根本原因进行更深入的理解。为了促进这一点，我们引入了SF-Visuals，一个交互式分析工具，利用潜在空间聚类来可视化偏移和失败。

    To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of va
    
[^40]: TimeGNN: 时间动态图学习用于时间序列预测

    TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting. (arXiv:2307.14680v1 [cs.LG])

    [http://arxiv.org/abs/2307.14680](http://arxiv.org/abs/2307.14680)

    TimeGNN是一种学习动态时间图表示的方法，可以捕捉多个系列之间模式的演变以及多个系列之间的相关性。它在推断时间上比其他方法快4到80倍，并且达到了相当的预测性能。

    

    时间序列预测在许多科学和工程领域的重要实际应用中处于核心位置。大量包含复杂模式和长期依赖关系的大型时间序列数据集的存在，导致了各种神经网络架构的发展。最近，基于图神经网络的方法通过联合学习基于多元时间序列原始值之间的相关性的图结构来进行预测，在取得巨大成功。然而，这种解决方案往往训练成本高昂且难以扩展。在本文中，我们提出了一种名为TimeGNN的方法，它学习动态的时间图表示，可以捕捉多个系列之间模式的演变以及多个系列之间的相关性。TimeGNN的推断时间比其他最先进的基于图的方法快4到80倍，并且达到了相当的预测性能。

    Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures. Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success. However, such solutions are often costly to train and difficult to scale. In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance
    
[^41]: 使用物理信息神经网络和证据不确定性量化预测风力发电机功率

    Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])

    [http://arxiv.org/abs/2307.14675](http://arxiv.org/abs/2307.14675)

    本研究使用物理信息神经网络对风力涡轮机的历史数据进行建模，通过施加物理约束，成功地预测了涡轮机的功率、转矩和功率系数。这对于优化涡轮机运行和早期故障检测具有重要的实际应用价值。

    

    风能的不断增长使通过偏航角控制器来优化涡轮机运行，并通过早期故障检测来进行维护变得必要。准确而稳健的模型来模拟风力涡轮机的行为尤其关键，特别是预测生成的功率与风速的关系。现有的经验和基于物理的模型在捕捉输入变量和功率之间的复杂关系方面存在局限性，尤其是受风变性的加剧。数据驱动方法为通过提高准确性和效率来增强大数据集上的风力涡轮机建模提供了新的机会。在本研究中，我们使用物理信息神经网络以模拟来自一个风场的4台涡轮机的历史数据，同时对模型施加了一定的物理约束。开发的用于回归功率、转矩和功率系数的模型在真实数据和控制方程方面都展现了很好的准确性。

    The ever-growing use of wind energy makes necessary the optimization of turbine operations through pitch angle controllers and their maintenance with early fault detection. It is crucial to have accurate and robust models imitating the behavior of wind turbines, especially to predict the generated power as a function of the wind speed. Existing empirical and physics-based models have limitations in capturing the complex relations between the input variables and the power, aggravated by wind variability. Data-driven methods offer new opportunities to enhance wind turbine modeling of large datasets by improving accuracy and efficiency. In this study, we used physics-informed neural networks to reproduce historical data coming from 4 turbines in a wind farm, while imposing certain physical constraints to the model. The developed models for regression of the power, torque, and power coefficient as output variables showed great accuracy for both real data and physical equations governing th
    
[^42]: 通过模型不可知的排序调整实现二分排名公平性

    Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment. (arXiv:2307.14668v1 [cs.LG])

    [http://arxiv.org/abs/2307.14668](http://arxiv.org/abs/2307.14668)

    本文提出了一个模型不可知的后处理框架 xOrder，用于实现二分排名的公平性且能保持算法分类性能。它通过优化一个加权效用的和来确定不同受保护组之间的最佳路径，并兼容各种分类模型和排名公平性指标。实验证明，xOrder能够在保持分类性能的同时实现较好的公平性。

    

    算法公正性一直是机器学习社区关注的重点。本文针对二分排名场景，即实例来源于正类或负类，目标是学习一个排名函数，使得正实例排名高于负实例。虽然公平性和性能之间可能存在权衡，但我们提出了一个模型不可知的后处理框架 xOrder，用于实现二分排名的公平性并保持算法分类性能。具体而言，我们通过动态规划过程优化一个加权效用的和，以确定不同受保护组之间最佳弯曲路径。xOrder兼容各种分类模型和排名公平性指标，包括监督和无监督的公平性指标。除了二元组之外，xOrder还可以应用于多个受保护组。我们评估了我们提出的算法的性能，并与其他方法进行了比较。实验结果表明，xOrder在维持分类性能的同时实现了较好的公平性。

    Algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this paper, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed al
    
[^43]: 揭示恶意软件分类中的机器学习秘密：对数据集、特征提取和模型性能的深入研究

    Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance. (arXiv:2307.14657v1 [cs.CR])

    [http://arxiv.org/abs/2307.14657](http://arxiv.org/abs/2307.14657)

    本论文研究了恶意软件检测和分类中的机器学习模型，发现数据集的性质和分布以及训练数据集的家族和样本数量对性能有影响，并且静态和动态特征相互补充。

    

    许多研究已经提出了用于恶意软件检测和分类的机器学习（ML）模型，并报道了几乎完美的性能。然而，它们以不同的方式组装基准数据集，使用多样化的静态和动态分析技术进行特征提取，甚至在对恶意软件家族的定义上也存在差异。因此，我们的研究社区仍然缺乏对恶意软件分类结果的理解：它们是否与所收集数据集的性质和分布有关，训练数据集中的家族和样本数量在多大程度上影响性能，以及静态和动态特征如何相互补充。本研究通过调查影响基于ML的恶意软件检测和分类的关键因素来回答这些开放性问题。为此，我们收集了迄今为止最大的平衡恶意软件数据集，其中包含来自670个家族的67,000个样本（每个家族100个样本），并训练了最先进的恶意软件检测和家族分类模型。

    Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.  This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classificat
    
[^44]: 基于机器学习的区域气候模型参数敏感性研究--以WRF模型在澳大利亚东南部的热极端为例

    Machine Learning based Parameter Sensitivity of Regional Climate Models -- A Case Study of the WRF Model for Heat Extremes over Southeast Australia. (arXiv:2307.14654v1 [physics.ao-ph])

    [http://arxiv.org/abs/2307.14654](http://arxiv.org/abs/2307.14654)

    本研究通过基于机器学习的方法，探究了WRF模型在澳大利亚东南部对表面气象变量敏感性的研究，为热极端事件的准确模拟和评估提供了重要的帮助。

    

    热浪和森林火灾在全球范围内对社会和生态系统造成重大影响。准确的热极端信息对于支持可行的减灾和适应策略的制定至关重要。区域气候模型常用于更好地理解这些事件的动态。这些模型具有非常庞大的输入参数集，其中物理方案中的参数大大影响模型的性能。然而，对于热极端来说，区域模型的参数敏感性分析在很大程度上还未被探索。本研究侧重于澳大利亚东南部地区，这个地区是热极端的全球热点之一。在澳大利亚东南部，Weather Research and Forecasting (WRF) 模型是广泛使用的区域模型，用于模拟该地区的极端天气事件。因此，在本研究中，我们重点关注WRF模型参数对表面气象变量（如温度、相对湿度和风速）的敏感性。

    Heatwaves and bushfires cause substantial impacts on society and ecosystems across the globe. Accurate information of heat extremes is needed to support the development of actionable mitigation and adaptation strategies. Regional climate models are commonly used to better understand the dynamics of these events. These models have very large input parameter sets, and the parameters within the physics schemes substantially influence the model's performance. However, parameter sensitivity analysis (SA) of regional models for heat extremes is largely unexplored. Here, we focus on the southeast Australian region, one of the global hotspots of heat extremes. In southeast Australia Weather Research and Forecasting (WRF) model is the widely used regional model to simulate extreme weather events across the region. Hence in this study, we focus on the sensitivity of WRF model parameters to surface meteorological variables such as temperature, relative humidity, and wind speed during two extreme 
    
[^45]: 深度学习的速度限制

    Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])

    [http://arxiv.org/abs/2307.14653](http://arxiv.org/abs/2307.14653)

    研究使用随机热力学方法，根据权重分布间的Wasserstein-2距离和熵产生速率，提供了对深度学习网络从初始状态到完全训练的最大速度限制。通过应用于线性和可线性化的神经网络，结果表明，在某些缩放假设下，学习在某种程度上是最优的。

    

    现阶段的神经网络需要极大的计算能力才能进行训练。因此很自然地想知道它们是否被最优化地训练。在本文中，我们应用了最近在随机热力学中的一个进展，允许根据它们的Wasserstein-2距离的比率和连接它们的动态过程的熵产生速率，对从初始权重分布到完全训练的网络的最大速度进行界定。考虑了梯度流和Langevin训练动力学，我们为线性和可线性化的神经网络（例如神经切向核(NTK)）提供了这些速度限制的解析表达式。值得注意的是，如果对NTK谱和标签的谱分解做出一些合理的缩放假设，学习在某种程度上是最优化的。我们的结果与在CIFAR-10上使用卷积神经网络(CNNs)和全连接神经网络(FCNs)进行的小规模实验一致，显示了

    State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a
    
[^46]: 在去噪扩散概率模型中利用空间频率U-Net (SFUNet)

    Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models. (arXiv:2307.14648v1 [cs.CV])

    [http://arxiv.org/abs/2307.14648](http://arxiv.org/abs/2307.14648)

    本论文研究了将去噪扩散概率模型从像素空间转换到小波空间的方法，设计了SFUNet架构，能够同时捕捉到空间和频率领域中的相关性，从而生成质量更高的图像。

    

    本文研究了在可视合成中，将去噪扩散概率模型(DDPM)从像素空间转换到小波空间的方法。考虑到小波变换能同时表示图像的空间和频率信息，我们设计了一种新的SFUNet架构，能够有效地捕捉到两个领域中的相关性。具体来说，在对像素数据进行标准的去噪U-Net时，我们通过空间频率感知的卷积和注意力模块，来补充2D卷积和仅空间的注意力层，共同建模来自小波数据的空间和频率领域中的互补信息。我们的新架构可以作为像素网络的替代方案，并且与DDPM的训练过程兼容。通过明确地对小波信号建模，我们发现我们的模型在CIFAR-10、FFHQ、LSUN-Bedroom和LSUN-Church数据集上能够生成比基于像素的对应模型质量更高的图像。

    In this paper, we study the denoising diffusion probabilistic model (DDPM) in wavelet space, instead of pixel space, for visual synthesis. Considering the wavelet transform represents the image in spatial and frequency domains, we carefully design a novel architecture SFUNet to effectively capture the correlation for both domains. Specifically, in the standard denoising U-Net for pixel data, we supplement the 2D convolutions and spatial-only attention layers with our spatial frequency-aware convolution and attention modules to jointly model the complementary information from spatial and frequency domains in wavelet data. Our new architecture can be used as a drop-in replacement to the pixel-based network and is compatible with the vanilla DDPM training process. By explicitly modeling the wavelet signals, we find our model is able to generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and LSUN-Church datasets, than the pixel-based counterpart.
    
[^47]: MVMR-FS：基于最大类间差异和最小冗余的非参数特征选择算法

    MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])

    [http://arxiv.org/abs/2307.14643](http://arxiv.org/abs/2307.14643)

    本文提出了一种基于最大类间差异和最小冗余的非参数特征选择算法(MVMR-FS)，通过使用有监督和无监督核密度估计来度量特征之间的相关性和冗余，并提出了最大类间差异和最小冗余的准则(MVMR)，以辅助特征选择。

    

    如何准确度量特征的相关性和冗余是特征选择领域的一个古老挑战。然而，现有的基于过滤的特征选择方法无法直接度量连续数据的冗余。此外，大多数方法依赖于手动指定特征数量，这在没有专家知识的情况下可能导致错误。本文提出了一种基于最大类间差异和最小冗余的非参数特征选择算法，简称为MVMR-FS。我们首先引入了对特征的有监督和无监督核密度估计，以捕捉它们在类间和整体分布中的相似性和差异。随后，我们提出了最大类间差异和最小冗余（MVMR）的准则，其中利用类间概率分布来反映特征的相关性，利用整体概率分布之间的距离来量化冗余。

    How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundanc
    
[^48]: 黑盒变分推断的线性收敛性：我们应该坚持到底吗？

    Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])

    [http://arxiv.org/abs/2307.14642](http://arxiv.org/abs/2307.14642)

    本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。

    

    我们证明了带有控制变量的黑盒变分推断（BBVI），特别是着陆稳定（STL）估计器，在完美变分族规范下收敛于几何（传统上称为“线性”）速度。特别地，我们证明了STL估计器的梯度方差的二次界限，该界限包括了误指定的变分族。结合先前关于二次方差条件的工作，这直接暗示了在使用投影随机梯度下降的情况下BBVI的收敛性。我们还改进了现有对于正常封闭形式熵梯度估计器的分析，这使得我们能够将其与STL估计器进行比较，并为两者提供明确的非渐进复杂度保证。

    We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
    
[^49]: AI生成报告的事实核查

    Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])

    [http://arxiv.org/abs/2307.14634](http://arxiv.org/abs/2307.14634)

    本文提出了一种使用相关联的图像对AI生成报告进行事实核查的新方法，以区分报告中的真假句子。这对加快临床工作流程，提高准确性并降低总体成本具有重要意义。

    

    随着生成人工智能（AI）的进步，现在可以生成逼真的自动报告来对放射学图像进行初步阅读。这可以加快临床工作流程，提高准确性并降低总体成本。然而，众所周知，这种模型往往会产生幻觉，导致生成报告中出现错误的发现。在本文中，我们提出了一种使用相关联的图像对AI生成报告进行事实核查的新方法。具体而言，通过学习图像与描述真实或潜在虚假发现的句子之间的关联，开发的核查者区分报告中的真假句子。为了训练这样的核查者，我们首先通过扰动原始与图像相关的放射学报告中的发现，创建了一个新的伪造报告数据集。然后将来自这些报告的真假句子的文本编码与图像编码配对，学习映射到真/假标签的关系。

    With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility 
    
[^50]: 快速可扩展的贝叶斯AB测试

    Rapid and Scalable Bayesian AB Testing. (arXiv:2307.14628v1 [cs.LG])

    [http://arxiv.org/abs/2307.14628](http://arxiv.org/abs/2307.14628)

    该论文提出了一种快速可扩展的贝叶斯AB测试方法，通过应用分层贝叶斯估计来克服了常见的AB测试分析方法所存在的限制，包括多变量设计、相关性、顺序测试和汇总过去测试知识等问题。这种方法能够增加统计功效、实现顺序测试和渐进式早停止，同时降低错误判断的风险。

    

    AB测试帮助业务经营者进行决策，并被认为是从数据中学习以改进数字用户体验的标准方法。然而，实践者的要求通常与常用于AB测试分析的统计假设检验方法所施加的限制存在差距。这些限制包括多变量设计中缺乏统计功效、因素之间的相关性、需要顺序测试进行早停止，以及无法汇总过去测试的知识。在这里，我们提出了一种解决方案，通过应用分层贝叶斯估计来解决上述限制。与当前的顺序AB测试方法相比，我们通过利用因素之间的相关性来增加统计功效，实现了顺序测试和渐进式早停止，而不会产生过高的错误正例风险。我们还展示了如何将这种方法扩展到其他领域。

    AB testing aids business operators with their decision making, and is considered the gold standard method for learning from data to improve digital user experiences. However, there is usually a gap between the requirements of practitioners, and the constraints imposed by the statistical hypothesis testing methodologies commonly used for analysis of AB tests. These include the lack of statistical power in multivariate designs with many factors, correlations between these factors, the need of sequential testing for early stopping, and the inability to pool knowledge from past tests. Here, we propose a solution that applies hierarchical Bayesian estimation to address the above limitations. In comparison to current sequential AB testing methodology, we increase statistical power by exploiting correlations between factors, enabling sequential testing and progressive early stopping, without incurring excessive false positive risk. We also demonstrate how this methodology can be extended to e
    
[^51]: BubbleML: 用于机器学习的多物理数据集和基准

    BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])

    [http://arxiv.org/abs/2307.14623](http://arxiv.org/abs/2307.14623)

    BubbleML是一个用于机器学习的多物理数据集，通过物理驱动模拟获得准确的地面真实信息，并在各种沸腾场景中验证了其可靠性和潜力。

    

    在相变现象领域，缺乏适用于机器学习训练的可访问和多样化的数据集是一个重要挑战。现有的实验数据集通常受限，可用性有限且地面真实数据稀缺，阻碍了我们对这种复杂多物理现象的理解。为了弥补这一差距，我们提出了BubbleML数据集（https://github.com/HPCForge/BubbleML），它利用物理驱动的模拟为各种沸腾场景提供准确的地面真实信息，包括核泡池沸腾、流动沸腾和亚冷沸腾。这个广泛的数据集涵盖了各种参数，包括不同的重力条件、流量、亚冷水平和壁面过热，总共有51个模拟。BubbleML已经通过实验观察和趋势进行了验证，被确认为机器学习研究的宝贵资源。此外，我们展示了它促进多样化降低温度沸腾研究的潜力。

    In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
    
[^52]: 模仿复杂轨迹：桥接低层稳定性与高层行为

    Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])

    [http://arxiv.org/abs/2307.14619](http://arxiv.org/abs/2307.14619)

    本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。

    

    我们提出了一个理论框架来研究在非线性动态系统中模仿随机、非马尔可夫、潜在多模态（即“复杂”）专家演示的行为。我们的框架使用低层控制器（无论是学习的还是隐含的）来稳定围绕专家演示的模仿策略。我们证明，在（a）合适的低层稳定性保证和（b）学习策略的随机连续性属性（我们称之为“总变差连续性”）（TVC）的情况下，一个精确估计演示者状态分布上的行动的模仿者会与演示者对整个轨迹的分布相近。然后，我们证明可以通过将流行的数据增强规则与一种新颖的算法技巧相结合（即在执行时添加增强噪声）来确保TVC并且最小程度上降低精度。我们将我们的保证实例化为由扩散模型参数化的策略，并证明如果学习者准确地估计了演示者的分布，则最终完成这种实例化。

    We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
    
[^53]: 自对比图扩散网络

    Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])

    [http://arxiv.org/abs/2307.14613](http://arxiv.org/abs/2307.14613)

    提出了一种名为自对比图扩散网络（SCGDN）的新型框架，通过注意力模块和扩散模块实现对高阶结构和特征信息的优秀嵌入。与现有的方法不同，SCGDN是一种无增强的方法，避免了“采样偏差”和语义漂移问题。

    

    在对比学习中，增强技术和采样策略非常重要，但在大多数现有工作中，增强技术需要精心设计，而他们的采样策略只能捕捉到一小部分内在的监督信息。此外，现有的方法需要复杂的设计来获得数据的两个不同表示。为了克服这些限制，我们提出了一个新颖的框架，称为自对比图扩散网络（SCGDN）。我们的框架由两个主要组件组成：注意力模块（AttM）和扩散模块（DiFM）。AttM通过汇集高阶结构和特征信息来获得优秀的嵌入，而DiFM通过Laplacian扩散学习平衡图中每个节点的状态，并允许图中邻接和特征信息的协同演化。与现有的方法不同，SCGDN是一种无增强的方法，避免了“采样偏差”和语义漂移问题。

    Augmentation techniques and sampling strategies are crucial in contrastive learning, but in most existing works, augmentation techniques require careful design, and their sampling strategies can only capture a small amount of intrinsic supervision information. Additionally, the existing methods require complex designs to obtain two different representations of the data. To overcome these limitations, we propose a novel framework called the Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two main components: the Attentional Module (AttM) and the Diffusion Module (DiFM). AttM aggregates higher-order structure and feature information to get an excellent embedding, while DiFM balances the state of each node in the graph through Laplacian diffusion learning and allows the cooperative evolution of adjacency and feature information in the graph. Unlike existing methodologies, SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic drift, wit
    
[^54]: 完整且独立：具有缺失目标源属性补全的条件分离

    Complete and separate: Conditional separation with missing target source attribute completion. (arXiv:2307.14609v1 [cs.SD])

    [http://arxiv.org/abs/2307.14609](http://arxiv.org/abs/2307.14609)

    本论文提出了一种新的条件分离方法，通过训练一个模型来提取语义数据，然后利用这个模型来改进多条件分离网络的性能。实验证明，这个方法接近具有完整语义信息的预设模型的性能水平，并且与最佳性能的单条件模型相媲美。

    

    最近的源分离方法利用有关输入混合物和成分源的语义信息，在条件分离模型中使用可以实现令人印象深刻的性能。大多数这类方法专注于简单的描述，对于不同类型的输入混合物并不总是有用。在这项工作中，我们提出了一种方法，即给定输入混合物和有关目标源的部分语义信息，训练一个模型提取其他的语义数据。然后，我们利用这个预训练的模型来改进未耦合的多条件分离网络的分离性能。我们的实验证明，这个多条件模型的分离性能显著提高，接近具有完整语义信息的预设模型的性能。此外，我们的方法实现了与最佳性能的专门单条件模型相媲美的性能水平。

    Recent approaches in source separation leverage semantic information about their input mixtures and constituent sources that when used in conditional separation models can achieve impressive performance. Most approaches along these lines have focused on simple descriptions, which are not always useful for varying types of input mixtures. In this work, we present an approach in which a model, given an input mixture and partial semantic information about a target source, is trained to extract additional semantic data. We then leverage this pre-trained model to improve the separation performance of an uncoupled multi-conditional separation network. Our experiments demonstrate that the separation performance of this multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Furthermore, our approach achieves performance levels that are comparable to those of the best performing specialized single conditional models,
    
[^55]: HUTFormer：用于长期交通预测的分层U-Net Transformer

    HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting. (arXiv:2307.14596v1 [cs.LG])

    [http://arxiv.org/abs/2307.14596](http://arxiv.org/abs/2307.14596)

    HUTFormer是一种用于长期交通预测的分层U-Net Transformer模型，通过利用多尺度表示来解决长期预测中的挑战和问题。

    

    交通预测旨在基于历史观测数据预测交通状况，是智能交通领域中的重要研究课题。最近的空间-时间图神经网络（STGNNs）通过将顺序模型与图卷积网络相结合取得了显著的进展。然而，由于复杂性问题，STGNNs仅聚焦于短期交通预测，如1小时预测，而忽视了更实际的长期预测。本文首次尝试探索长期交通预测，例如1天的预测。为此，我们首先揭示了在利用多尺度表示方面的独特挑战。然后，我们提出了一种新颖的分层U-Net TransFormer（HUTFormer）来解决长期交通预测的问题。HUTFormer由分层编码器和解码器组成，共同生成和利用交通的多尺度表示。

    Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traff
    
[^56]: MCPA: 多尺度交叉感知器注意力网络用于二维医学图像分割

    MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation. (arXiv:2307.14588v1 [eess.IV])

    [http://arxiv.org/abs/2307.14588](http://arxiv.org/abs/2307.14588)

    提出了一种名为MCPA的多尺度交叉感知器注意力网络，用于二维医学图像分割。该网络通过多尺度交叉感知器模块捕捉局部相关性，并有效地融合全局特征，以克服UNet架构的局限性。

    

    基于卷积神经网络（CNN）的UNet架构在医学图像分析中显示出了卓越的性能。然而，由于有限的感受野和卷积操作固有的偏见，在捕捉长程依赖性方面面临挑战。最近，许多基于Transformer的技术已经被整合到UNet架构中，通过有效地捕捉全局特征相关性来克服这一限制。然而，Transformer模块的整合可能导致在全局特征融合过程中丢失局部上下文信息。为了克服这些挑战，我们提出了一种名为Multi-scale Cross Perceptron Attention Network（MCPA）的二维医学图像分割模型。MCPA由三个主要组件组成：编码器、解码器和交叉感知器。交叉感知器首先使用多个多尺度交叉感知器模块捕捉局部相关性，促进特征融合。

    The UNet architecture, based on Convolutional Neural Networks (CNN), has demonstrated its remarkable performance in medical image analysis. However, it faces challenges in capturing long-range dependencies due to the limited receptive fields and inherent bias of convolutional operations. Recently, numerous transformer-based techniques have been incorporated into the UNet architecture to overcome this limitation by effectively capturing global feature correlations. However, the integration of the Transformer modules may result in the loss of local contextual information during the global feature fusion process. To overcome these challenges, we propose a 2D medical image segmentation model called Multi-scale Cross Perceptron Attention Network (MCPA). The MCPA consists of three main components: an encoder, a decoder, and a Cross Perceptron. The Cross Perceptron first captures the local correlations using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of features ac
    
[^57]: 深度强化学习在自主导航中对安全约束的评估

    Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning. (arXiv:2307.14568v1 [cs.RO])

    [http://arxiv.org/abs/2307.14568](http://arxiv.org/abs/2307.14568)

    本研究评估了在深度强化学习中考虑安全约束的自主导航，在比较了安全和不安全两种学习策略后发现，安全策略能够生成更安全的轨迹，避免碰撞，而不影响整体性能。

    

    尽管强化学习算法在自主导航领域取得了巨大的成功，但在考虑到安全限制的情况下，它们不能直接应用于真实的自主系统中。这些限制对于避免自动驾驶车辆在道路上出现不安全行为至关重要。为了凸显这些限制的重要性，在本研究中，我们比较了两种可学习的导航策略：安全策略和不安全策略。安全策略考虑到了约束条件，而另一种策略则没有。我们展示了在训练过程中，安全策略能够生成具有更大安全间隙（与障碍物的距离）并且发生更少碰撞的轨迹，而不损害整体性能。

    While reinforcement learning algorithms have had great success in the field of autonomous navigation, they cannot be straightforwardly applied to the real autonomous systems without considering the safety constraints. The later are crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To highlight the importance of these constraints, in this study, we compare two learnable navigation policies: safe and unsafe. The safe policy takes the constraints into account, while the other does not. We show that the safe policy is able to generate trajectories with more clearance (distance to the obstacles) and makes less collisions while training without sacrificing the overall performance.
    
[^58]: Auto-Tables: 无需使用示例合成多步转换以使表格关系化

    Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples. (arXiv:2307.14565v1 [cs.DB])

    [http://arxiv.org/abs/2307.14565](http://arxiv.org/abs/2307.14565)

    Auto-Tables系统能自动合成多步转换的流水线，将非关系式表格转换为关系式表格，解决了非技术用户使用SQL分析工具的痛点。

    

    关系表格在关系数据库中是标准的，每一行对应一个实体，每一列对应一个属性。然而，在处理"野生"表格时，这样的标准无法保证。我们调查了真实的电子表格和网页表格，发现超过30%的表格不符合关系标准，这就需要进行复杂的表格重构转换，才能轻松地使用基于SQL的分析工具进行查询。然而，所需的转换编程并不简单，这已经成为技术和非技术用户的重要问题，从StackOverflow和Excel/Tableau论坛的大量问题可以证明。我们开发了一个Auto-Tables系统，可以自动合成具有多步转换（使用Python或其他语言）的流水线，将非关系型表转换为标准的关系形式。

    Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms fo
    
[^59]: 带有多个游戏的对抗睡眠赌博问题：算法和排名应用

    Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application. (arXiv:2307.14549v1 [cs.LG])

    [http://arxiv.org/abs/2307.14549](http://arxiv.org/abs/2307.14549)

    本论文提出了解决在线推荐系统中具有多个游戏的睡眠赌博问题的高效算法，该算法能够保证理论性能，并且后悔上界为$\bigO(kN^2\sqrt{T\log T})$。

    

    本论文介绍了在在线推荐系统中解决具有多个游戏的睡眠赌博问题的高效算法。该问题涉及有界的对抗性损失和未知的i.i.d.分布的武器可用性。该算法扩展了单武器选择的睡眠赌博算法，并保证达到理论性能，后悔上界为$\bigO(kN^2\sqrt{T\log T})$，其中$k$是每个时间步选择的武器数量，$N$是总武器数量，$T$是时间范围。

    This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.
    
[^60]: 通过修改核谱来控制宽神经网络的归纳偏差

    Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum. (arXiv:2307.14531v1 [cs.LG])

    [http://arxiv.org/abs/2307.14531](http://arxiv.org/abs/2307.14531)

    本文引入了Modified Spectrum Kernels（MSKs）构造核族，通过预条件梯度下降方法，实现了对宽神经网络归纳偏差的控制，并在不改变最终解的情况下，加速了训练速度。

    

    宽神经网络在学习特定函数方面存在偏差，影响梯度下降的收敛速度和有限训练时间内可达到的函数。因此，迫切需要一种可以根据任务修改这种偏差的方法。为此，我们引入了Modified Spectrum Kernels(MSKs)这一新颖的构造核族，可以用于近似没有已知闭合形式的期望特征值的核。我们利用宽神经网络和神经切向核之间的对偶性，提出了一种预条件梯度下降方法，改变了梯度下降的轨迹。结果是，这使得训练速度在某些情况下呈多项式甚至指数级加速，同时不改变最终解。我们的方法既计算高效又易于实现。

    Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.
    
[^61]: 混合成员随机块模型中的最优估计

    Optimal Estimation in Mixed-Membership Stochastic Block Models. (arXiv:2307.14530v1 [stat.ML])

    [http://arxiv.org/abs/2307.14530](http://arxiv.org/abs/2307.14530)

    本论文研究了重叠社区检测问题，在混合成员随机块模型的基础上提出了一个新的估计器，并建立了估计误差的极小下界。

    

    社区检测是现代网络科学中最关键的问题之一。其应用可以在各个领域找到，从蛋白质建模到社交网络分析。最近，出现了许多论文研究重叠社区检测问题，即网络中的每个节点可能属于多个社区。在本文中，我们考虑了由Airoldi等人（2008）首次提出的混合成员随机块模型（MMSB）。MMSB在图中对重叠社区结构提供了相当一般的设置。本文的核心问题是在观察到的网络中重建社区之间的关系。我们比较了不同的方法，并建立了估计误差的极小下界。然后，我们提出了一个与这个下界匹配的新估计器。理论结果在对所考虑的模型的相当普遍条件下得到证明。最后，我们通过一系列实验来说明这个理论。

    Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
    
[^62]: 函数值学习：基于Polyak步长和函数分割的自适应学习率的研究

    Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM. (arXiv:2307.14528v1 [cs.LG])

    [http://arxiv.org/abs/2307.14528](http://arxiv.org/abs/2307.14528)

    本论文提出了一种基于采样损失值和自适应步长的SGD变体，通过解决有限和问题，达到了较佳的收敛速度，并开发了一种逐渐学习损失值的算法。

    

    在本研究中，我们开发了一种具有自适应步长的SGD（随机梯度下降）变体，利用了采样损失值。特别地，我们专注于解决有限和问题，也称为经验风险最小化。我们首先详细介绍了一种理念上的自适应方法$\texttt{SPS}_+$，该方法利用了采样损失值，并假设对采样损失值在最优情况下有了解。这种$\texttt{SPS}_+$是SPS（随机Polyak步长）方法的一个小修改，其中步长被强制为正。然后，我们证明了$\texttt{SPS}_+$在Lipschitz非光滑中实现了SGD的最佳收敛速度。接下来，我们开发了$\texttt{FUVAL}$，这是$\texttt{SPS}_+$的一个变体，其中损失值在最优情况下逐渐学习，而不是给定的。我们给出了$\texttt{FUVAL}$的三个视角，作为基于投影的方法，作为近似线性方法的变体，以及作为特定的在线SGD方法。然后，我们继续研究了如何实现$\texttt{FUVAL}$。

    Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then pr
    
[^63]: 用于荒野SAR和寻找Patricia Wu-Murad的计算机视觉的开放问题

    Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])

    [http://arxiv.org/abs/2307.14527](http://arxiv.org/abs/2307.14527)

    该论文介绍了在荒野搜救中应用计算机视觉系统的挑战，提出了使用EfficientDET模型和无监督RX光谱分类器的方法，但在真实世界中存在假阳性的问题。

    

    本论文详细介绍了将两种计算机视觉系统，EfficientDET监督学习模型和无监督RX光谱分类器应用于来自日本Wu-Murad野外搜救（WSAR）努力的98.9 GB无人机图像的挑战，并确定了未来研究的3个方向。已经提出了至少19种方法和3个数据集，旨在在无人机图像中定位失踪人员，但只有3种方法（2种无监督和1种未知结构）在文献中被引用为实际WSAR操作中使用过。在这些提出的方法中，EfficientDET架构和无监督的RX光谱分类器被选择为最适合此情景的方法。EfficientDET模型应用于HERIDAL数据集，尽管在性能上达到了与最新技术相当的水平，但模型在假阳性方面无法在现实世界中有效识别（例如，识别树枝和岩石）

    This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks 
    
[^64]: 机器学习系统中的错误特征化

    Bug Characterization in Machine Learning-based Systems. (arXiv:2307.14512v1 [cs.SE])

    [http://arxiv.org/abs/2307.14512](http://arxiv.org/abs/2307.14512)

    该论文研究了机器学习系统中错误的特征和维护挑战，并比较了ML和非ML错误。通过对使用最流行的ML框架的GitHub仓库进行调查，作者发现了一些有关错误的见解。

    

    机器学习（ML）在不同领域的应用迅速增长，特别是在安全关键领域，增加了对可靠ML组件的需求，即基于ML操作的软件组件。了解ML系统中的错误特征和维护挑战可以帮助这些系统的开发人员确定维护和测试工作的重点，通过提供对最容易出错的组件、最常见的错误等方面的见解。在本文中，我们研究了ML软件系统中错误的特征以及从维护角度看ML和非ML错误的区别。我们提取了447,948个GitHub仓库，这些仓库使用了三个最流行的ML框架，即TensorFlow、Keras和PyTorch。经过多次过滤，我们选择了关闭问题数量最多的前300个仓库。我们手动调查提取的仓库，排除非ML系统。我们的调查涉及手动检查。

    Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Understanding the bugs characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspec
    
[^65]: 通过使用自监督语音表示损失函数研究口语对语音增强的影响

    The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])

    [http://arxiv.org/abs/2307.14502](http://arxiv.org/abs/2307.14502)

    本文研究了在语音增强中，使用与嘈杂数据语言完全匹配的自监督语音表示损失函数训练的模型的性能更好。与传统的频谱图或时间域损失函数相比，这些增强模型具有特定语言的特性。

    

    最近在语音增强领域的研究中，使用自监督语音表示（SSSRs）作为特征转换在损失函数中。然而，在之前的工作中，很少注意到用于训练自监督表示的音频语言与用于训练语音增强系统的语言之间的关系。使用将自监督表示与用于训练语音增强系统的嘈杂数据的语言完全匹配的损失函数训练的增强模型表现比不完全匹配的模型更好。这可能导致增强系统具有特定语言的特性，因此对未见语言的泛化能力不好，而使用传统的频谱图或时间域损失函数训练的模型则不会出现这个问题。本研究在多种不同语言上训练和测试了语音增强模型，使用了经过不同语言组合训练的自监督表示。

    Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinati
    
[^66]: 一个数字信息参与的预测模型：通过融合认知偏差、计算语言学和自然语言处理预测用户对英文字的参与度

    A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])

    [http://arxiv.org/abs/2307.14500](http://arxiv.org/abs/2307.14500)

    本研究提出了一个预测数字信息参与的新模型READ，通过融合认知偏差、计算语言学和自然语言处理，成功预测了英文字的参与水平，并区分了更具吸引力的词汇。

    

    本研究介绍并经过实证测试了一种新颖的数字信息参与（IE）的预测模型——READ模型，其代表了参与度高的信息的四个关键属性：代表性、易用性、情感和分布。在累积前景理论的理论框架下，该模型将关键的认知偏差与计算语言学和自然语言处理相结合，以发展出对信息参与的多维度视角。研究采用一个严格的测试协议，选取了WordNet数据库中的50对随机同义词（共100个词），通过大规模的在线调查（n = 80,500）评估了这些词的参与水平，得出了实证IE指标。然后对每个词的READ属性进行计算，并检查其预测效果。研究结果证实了READ模型的鲁棒性，准确预测了词的IE水平，并区分了更具吸引力的词汇。

    This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engagi
    
[^67]: HUGE: 使用TPU进行巨大无监督图嵌入

    HUGE: Huge Unsupervised Graph Embeddings with TPUs. (arXiv:2307.14490v1 [cs.LG])

    [http://arxiv.org/abs/2307.14490](http://arxiv.org/abs/2307.14490)

    本文提出了一种利用TPU进行高性能图嵌入的架构，能够处理具有数十亿节点和数万亿边的图。验证了嵌入空间质量。

    

    图是一种捕捉对象集合之间关系的结构化数据表示形式。随着可用网络数据的普及性，工业和学术界对于快速分析具有数十亿节点和数万亿边的图的需求越来越大。网络理解的常见首要步骤是图嵌入，即创建图中节点的连续表示的过程。连续表示通常更易于处理，尤其在规模上，用于解决下游的机器学习任务，如分类、链接预测和聚类。我们提出了一种利用可配置高带宽内存的张量处理单元（TPU）的高性能图嵌入架构，简化了图嵌入问题，并可以扩展到具有数十亿节点和数万亿边的图。我们在真实和合成的大规模数据集上验证了嵌入空间的质量。

    Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.
    
[^68]: 图像采集和患者表型变异在自动分割模型泛化中的作用

    Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization. (arXiv:2307.14482v1 [eess.IV])

    [http://arxiv.org/abs/2307.14482](http://arxiv.org/abs/2307.14482)

    本研究评估了自动医学图像分割模型在适应新的图像采集和疾病类型方面的泛化能力，结果显示使用多样化数据训练的模型在域内测试数据上表现良好。

    

    目的：本研究评估了自动医学图像分割模型在域外性能和泛化能力，特别关注对新的图像采集和疾病类型的适应性。材料：使用健康患者和多囊肾病（PKD）患者的非对比增强腹部CT扫描数据集。总共使用了400张图像（100张非对比对照组，100张对比对照组，100张非对比PKD组，100张对比PKD组）进行训练/验证，以分割肾脏、肝脏和脾脏，并将最终模型测试在受PKD影响的100张非对比CT图像上。使用Dice系数、Jaccard系数、TPR和Precision评估性能。结果：在域内测试数据上，使用多样化数据训练的模型表现不比仅使用域内数据训练的模型差。例如，对比25%数据集训练的模型的Dice相似度得分低于在域内数据上训练的模型。

    Purpose: This study evaluated the out-of-domain performance and generalization capabilities of automated medical image segmentation models, with a particular focus on adaptation to new image acquisitions and disease type.  Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT scans of healthy patients and those with polycystic kidney disease (PKD) were used. A total of 400 images (100 non-contrast controls, 100 contrast controls, 100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation of models to segment kidneys, livers, and spleens, and the final models were then tested on 100 non-contrast CT images of patients affected by PKD. Performance was evaluated using Dice, Jaccard, TPR, and Precision.  Results: Models trained on a diverse range of data showed no worse performance than models trained exclusively on in-domain data when tested on in-domain data. For instance, the Dice similarity of the model trained on 25% from each dataset was foun
    
[^69]: 河川学习的限制。

    Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])

    [http://arxiv.org/abs/2307.14474](http://arxiv.org/abs/2307.14474)

    这项工作限制了机器学习的能力，基于物理学所暗示的计算限制。储水库计算机在噪声下的性能下降意味着需要指数数量的样本来学习函数族，并讨论了没有噪声时的性能。

    

    在这项工作中，我们根据物理学所暗示的计算限制来限制机器学习的能力。我们首先考虑信息处理能力（IPC），这是一个对信号集合到完整函数基的期望平方误差进行归一化的指标。我们使用IPC来衡量噪声下储水库计算机（一种特殊的循环网络）的性能降低。首先，我们证明IPC在系统尺寸n上是一个多项式，即使考虑到n个输出信号的$2^n$个可能的逐点乘积。接下来，我们认为这种退化意味着在储水库噪声存在的情况下，储水库所表示的函数族需要指数数量的样本来进行学习。最后，我们讨论了在没有噪声的情况下，同一集合的$2^n$个函数在进行二元分类时的性能。

    In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
    
[^70]: ML API需要哪种类型的合同？

    What Kinds of Contracts Do ML APIs Need?. (arXiv:2307.14465v1 [cs.SE])

    [http://arxiv.org/abs/2307.14465](http://arxiv.org/abs/2307.14465)

    这项研究研究了机器学习API的合同类型，以帮助API用户在早期阶段捕捉错误，并通过对四个ML库的帖子进行实证研究，提取了413个API规范，以了解ML合同违规的根本原因、常见模式和是否需要先进的ML软件专业知识。

    

    最近的研究表明，机器学习（ML）程序容易出错，并呼吁为ML代码建立合同。合同，如设计合同方法论中所述，有助于记录API并帮助API用户编写正确的代码。问题是：哪种类型的合同能最大程度地帮助API用户？我们特别关注哪种类型的合同有助于API用户在ML流程的早期阶段捕捉错误。我们描述了一项针对Stack Overflow上最经常讨论的四个ML库（TensorFlow，Scikit-learn，Keras和PyTorch）的帖子的实证研究。对于这些库，我们的研究提取了413个非正式（英文）API规范。我们使用这些规范来回答以下问题。ML合同违规的根本原因和影响是什么？是否存在常见的ML合同违规模式？理解ML合同何时需要先进的ML软件专业知识？检查API级别的合同能否有助于检测合同违规？

    Recent work has shown that Machine Learning (ML) programs are error-prone and called for contracts for ML code. Contracts, as in the design by contract methodology, help document APIs and aid API users in writing correct code. The question is: what kinds of contracts would provide the most help to API users? We are especially interested in what kinds of contracts help API users catch errors at earlier stages in the ML pipeline. We describe an empirical study of posts on Stack Overflow of the four most often-discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study extracted 413 informal (English) API specifications. We used these specifications to understand the following questions. What are the root causes and effects behind ML contract violations? Are there common patterns of ML contract violations? When does understanding ML contracts require an advanced level of ML software expertise? Could checking contracts at the API level help detect t
    
[^71]: 使用核心集合训练量子玻尔兹曼机

    Training Quantum Boltzmann Machines with Coresets. (arXiv:2307.14459v1 [quant-ph])

    [http://arxiv.org/abs/2307.14459](http://arxiv.org/abs/2307.14459)

    这篇论文研究了在近期量子设备上应用量子玻尔兹曼机算法的加速方法，利用核心集合技术代替完整数据集，最小化训练过程中的计算瓶颈并加速整体训练时间。

    

    最近的研究提出并探索了使用核心集合技术来加速在近期量子设备上应用这些算法的可能性，这些算法是用于处理经典数据集的量子算法。我们将这些想法应用于量子玻尔兹曼机（QBM），其中基于梯度的步骤需要对Gibbs状态进行采样，这是训练过程中的主要计算瓶颈。通过使用核心集合代替完整的数据集，我们试图最小化所需的步骤数并加速整体训练时间。在量子计算时间非常宝贵的情况下，我们认为这可能会带来相当大的实际节省。我们使用QBM的36个可见单元和8个隐藏单元在一个增强的条纹图像数据集中的6x6二进制图像上评估了这种方法。通过使用受启发于Inception分数的度量标准，我们比较了使用和不使用核心集合的QBM训练时间。

    Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets.
    
[^72]: 使用机器学习方法进行装甲车辆的预测性维护

    Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches. (arXiv:2307.14453v1 [cs.LG])

    [http://arxiv.org/abs/2307.14453](http://arxiv.org/abs/2307.14453)

    本研究提出了一种基于机器学习的预测性维护系统，通过分析传感器数据预测装甲车辆的维护需求，结果表明系统具有高准确率和稳定性。这种方法有助于减少车辆停机时间，提高运营效率。

    

    装甲车辆是专业和复杂的机械设备，设计用于在高应力环境中运行，常常用于战斗或战术环境。本研究提出了一种基于预测性维护的集成系统，通过从这些车辆收集的传感器数据来预测潜在的维护需求。该模型的架构涉及各种模型，如轻梯度提升、随机森林、决策树、极端树分类器和梯度提升，以准确预测车辆的维护需求。此外，使用K折交叉验证和TOPSIS分析来评估提出的集成模型的稳定性。结果表明，该系统的准确率为98.93%，精确度为99.80%，召回率为99.03%。该算法可以有效预测维护需求，从而减少车辆停机时间，提高运营效率。通过对比各种算法和建议的比较

    Armoured vehicles are specialized and complex pieces of machinery designed to operate in high-stress environments, often in combat or tactical situations. This study proposes a predictive maintenance-based ensemble system that aids in predicting potential maintenance needs based on sensor data collected from these vehicles. The proposed model's architecture involves various models such as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier and Gradient Boosting to predict the maintenance requirements of the vehicles accurately. In addition, K-fold cross validation, along with TOPSIS analysis, is employed to evaluate the proposed ensemble model's stability. The results indicate that the proposed system achieves an accuracy of 98.93%, precision of 99.80% and recall of 99.03%. The algorithm can effectively predict maintenance needs, thereby reducing vehicle downtime and improving operational efficiency. Through comparisons between various algorithms and the sugges
    
[^73]: VISPUR: 用于识别和解释数据驱动决策中虚假关联的视觉辅助工具

    VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions. (arXiv:2307.14448v1 [cs.HC])

    [http://arxiv.org/abs/2307.14448](http://arxiv.org/abs/2307.14448)

    VISPUR是一个提供视觉分析和人本工作流程的系统，用于识别和解释数据驱动决策中的虚假关联。它包括混淆因素仪表盘和子群浏览器，可以帮助人们定位、推理和预防虚假关联。

    

    大数据和机器学习工具共同赋予人类在数据驱动决策方面的能力。然而，这些工具捕捉到的经验关联可能由于混淆因素和子群异质性而是虚假的。著名的辛普森悖论就是这样一个现象，聚合和子群级别的关联相互矛盾，给人类带来认知困惑和决策困难。现有的工具对于人类在实践中定位、推理和预防虚假关联提供的见解很少。我们提出VISPUR，一个提供因果分析框架和人本工作流程以应对虚假关联的可视化分析系统。其中包括一个混淆因素仪表盘，可以自动识别可能的混淆因素，以及一个子群浏览器，允许对可能导致对因果关系解释错误的多样子群模式进行可视化和比较。

    Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of ca
    
[^74]: 固定积分神经网络

    Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])

    [http://arxiv.org/abs/2307.14439](http://arxiv.org/abs/2307.14439)

    本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。

    

    将学习函数通过神经网络进行积分是非常有用的，但是这种积分通常是通过数值方法来计算的，因为解析计算积分过程复杂，尤其是对于神经网络这样的学习函数。本文介绍了一种表示学习函数 $f$ 解析积分的方法。这允许精确计算神经网络的积分，并且通过将约束直接应用于积分来对约束神经网络进行参数化。关键的是，我们还介绍了一种将 $f$ 约束为正的方法，这是许多应用（例如概率分布、距离度量等）所必需的条件。最后，我们介绍了几个可以利用我们的固定积分神经网络（FINN）的应用领域。

    It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
    
[^75]: 基于无监督深度学习的联合增强光谱和空间保真度的Pansharpening方法

    Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])

    [http://arxiv.org/abs/2307.14403](http://arxiv.org/abs/2307.14403)

    这篇论文提出了一个基于无监督深度学习的Pansharpening模型，通过全分辨率训练和特定损失函数的使用，充分利用了这种方法的潜力，具有优秀的性能。

    

    近年来，深度学习在多分辨率图像的Pansharpening中发挥重要作用。由于缺乏地面真实数据，大多数基于深度学习的方法在降低分辨率领域进行监督训练。然而，对于高分辨率目标图像，训练在降低分辨率图像上的模型往往表现不佳。因此，一些研究团队现在转向在全分辨率领域进行无监督训练，通过定义合适的损失函数和训练范例。在这个背景下，我们最近提出了一个可以应用于许多现有架构的全分辨率训练框架。在这里，我们提出了一个新的基于深度学习的Pansharpening模型，充分利用了这种方法的潜力并提供了前沿的性能。除了与先前工作相比的架构改进，如使用残余注意力模块，所提出的模型还具有一个新的损失函数，可以同时促进光谱和空间保真度。

    In latest years, deep learning has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most deep learning-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework which can be applied to many existing architectures.  Here, we propose a new deep learning-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes 
    
[^76]: 关于有限数据、少样本和零样本情况下生成建模的调查

    A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])

    [http://arxiv.org/abs/2307.14397](http://arxiv.org/abs/2307.14397)

    本文调查了在有限数据、少样本和零样本条件下学习生成模型的情况，并提出了关于任务和方法的分类体系，研究了它们之间的相互作用，并探讨了未来的研究方向。

    

    在机器学习中，生成建模旨在学习生成与训练数据分布统计相似的新数据。本文调查了在有限数据、少样本和零样本条件下学习生成模型的情况，称为数据约束下的生成建模（GM-DC）。这是一个重要的主题，当数据获取具有挑战性时，例如医疗应用。我们讨论了背景、挑战，并提出了两个分类体系：一个是GM-DC任务分类，另一个是GM-DC方法分类。重要的是，我们研究了不同GM-DC任务和方法之间的相互作用。此外，我们还强调了研究空白、研究趋势和未来探索的潜在途径。项目网站：https://gmdc-survey.github.io。

    In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.
    
[^77]: 学习使用可训练差分算子模拟部分已知的时空动态

    Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])

    [http://arxiv.org/abs/2307.14395](http://arxiv.org/abs/2307.14395)

    本文提出了一种新的混合架构PDE-Net++，通过结合可训练差分算子和黑盒模型，明确嵌入了底层PDE的部分先验知识。数值实验证明，PDE-Net++具有比黑盒模型更高的预测准确性和更好的外推性能。

    

    最近，使用神经网络来模拟时空动态引起了很大的关注。然而，大多数现有的方法采用纯数据驱动的黑盒模型，其准确性和可解释性有限。通过将可训练的差分算子与黑盒模型相结合，我们提出了一种新的混合架构，将底层PDE的部分先验知识明确嵌入到PDE-Net++中。此外，我们引入了两种不同的选择，称为可训练翻转差分层（TFDL）和可训练动态差分层（TDDL），用于差分算子。大量的数值实验证明，PDE-Net++具有比黑盒模型更高的预测准确性和更好的外推性能。

    Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.
    
[^78]: Hypergraph同构计算

    Hypergraph Isomorphism Computation. (arXiv:2307.14394v1 [cs.DS])

    [http://arxiv.org/abs/2307.14394](http://arxiv.org/abs/2307.14394)

    本文提出了用于超图同构计算的超图Weisfiler-Lehman测试算法以解决现有方法无法直接处理的高阶结构关系问题。

    

    同构问题是网络分析中的一个基本问题，它涉及捕捉低阶和高阶结构信息。从提取低阶结构信息的角度来看，图同构算法分析结构等价性以降低求解空间的维度，在蛋白质设计、化学途径和社区检测等许多应用中展示其强大的能力。对于现实场景中更常见的高阶关系，使用图同构方法无法直接解决超图同构问题，后者有效地捕捉这些高阶结构关系。此外，现有的超图内核方法可能存在内存消耗高或子结构识别不准确等问题，因此性能不佳。本文针对上述问题，首先提出了超图Weisfiler-Lehman测试算法用于超图同构的解决。

    The isomorphism problem is a fundamental problem in network analysis, which involves capturing both low-order and high-order structural information. In terms of extracting low-order structural information, graph isomorphism algorithms analyze the structural equivalence to reduce the solver space dimension, which demonstrates its power in many applications, such as protein design, chemical pathways, and community detection. For the more commonly occurring high-order relationships in real-life scenarios, the problem of hypergraph isomorphism, which effectively captures these high-order structural relationships, cannot be straightforwardly addressed using graph isomorphism methods. Besides, the existing hypergraph kernel methods may suffer from high memory consumption or inaccurate sub-structure identification, thus yielding sub-optimal performance. In this paper, to address the abovementioned problems, we first propose the hypergraph Weisfiler-Lehman test algorithm for the hypergraph iso
    
[^79]: 基于扩散的学习方法用于解码想象言语的脑电图

    Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])

    [http://arxiv.org/abs/2307.14389](http://arxiv.org/abs/2307.14389)

    本研究提出了一种使用去噪的扩散概率模型和条件自编码器进行想象言语的脑电图解码的新颖方法。结果表明，与传统机器学习技术和基准模型相比，该方法显著提高了解码准确性，这对于发展通过想象言语进行交流的脑-机接口具有潜在的影响。

    

    解码想象言语的脑电图是一项具有挑战性的任务，由于数据的高维特性和低信噪比。近年来，去噪扩散概率模型（DDPMs）已经成为各个领域中具有前景的表示学习方法。我们的研究提出了一种新颖的方法，使用DDPMs和条件自编码器Diff-E对想象言语的脑电图进行解码。结果表明，与传统机器学习技术和基准模型相比，Diff-E显著提高了对想象言语脑电图的解码准确性。我们的发现表明，DDPMs可以是一种有效的脑电图解码工具，对于通过想象言语进行交流的脑-机接口的发展具有潜在的影响。

    Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
    
[^80]: HyperFed: 非IID数据联邦学习中的超球面原型探索与一致聚合

    HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])

    [http://arxiv.org/abs/2307.14384](http://arxiv.org/abs/2307.14384)

    提出了HyperFed，用于解决联邦学习中非相同和独立数据分布造成的性能问题。该方法通过超球面原型探索、超球面原型学习和一致聚合等模块的结合，来解决类别统计的变化、层级信息利用不足和客户端聚合的不一致性问题。

    

    联邦学习（FL）以分散化的方式协同对用户数据建模。然而，在现实世界中，客户端之间的非相同和独立数据分布（非IID）阻碍了FL的性能，原因有三点，即（1）类别统计的变化，（2）层级信息利用不足和（3）客户端聚合的不一致性。为了解决上述问题，我们提出了HyperFed，其中包含三个主要模块，即超球面原型Tammes初始化（HPTI），超球面原型学习（HPL）和一致聚合（CA）。首先，服务器中的HPTI构造均匀分布且固定的类别原型，并与客户端分享以匹配类别统计，进一步指导本地客户端的一致特征表示。其次，每个客户端中的HPL在超球面模型空间中以共享的类别原型为监督，捕获本地数据中的层级信息。此外，在服务器中的CA进行一致聚合。

    Federated learning (FL) collaboratively models user data in a decentralized way. However, in the real world, non-identical and independent data distributions (non-IID) among clients hinder the performance of FL due to three issues, i.e., (1) the class statistics shifting, (2) the insufficient hierarchical information utilization, and (3) the inconsistency in aggregating clients. To address the above issues, we propose HyperFed which contains three main modules, i.e., hyperbolic prototype Tammes initialization (HPTI), hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly, HPTI in the server constructs uniformly distributed and fixed class prototypes, and shares them with clients to match class statistics, further guiding consistent feature representation for local clients. Secondly, HPL in each client captures the hierarchical information in local data with the supervision of shared class prototypes in the hyperbolic model space. Additionally, CA in the server mi
    
[^81]: 当多任务学习遇到部分监督：计算机视觉综述

    When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])

    [http://arxiv.org/abs/2307.14382](http://arxiv.org/abs/2307.14382)

    本综述讨论了多任务学习如何在部分监督设置下应用，以解决由于复杂的优化方案和高标签需求而引入的挑战。

    

    多任务学习(MTL)旨在同时学习多个任务，并利用它们之间的相互关系。通过使用共享资源同时计算多个输出，这种学习范式有潜力比传统方法在内存需求和推理时间方面更低。以往的MTL研究主要集中在完全监督方法上，因为任务之间的关系可以降低这些方法对数据的依赖性，并且可以提高性能。然而，MTL引入了一系列挑战，由于复杂的优化方案和更高的标签需求。本综述着重于MTL如何在不同的部分监督设置下应用，以解决这些挑战。首先，本综述分析了MTL传统上如何使用不同的参数共享技术在任务之间进行知识转移。其次，它介绍了不同的挑战。

    Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising fro
    
[^82]: EdgeConvEns: 基于卷积集成学习的边缘智能

    EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])

    [http://arxiv.org/abs/2307.14381](http://arxiv.org/abs/2307.14381)

    EdgeConvEns是一种卷积集成学习方法，用于在边缘网络上训练和集成异构的弱模型，以满足计算能力有限和分布式数据处理的需求。

    

    深度边缘智能旨在在计算能力有限的边缘网络上部署需要消耗计算资源的深度学习模型。此外，许多深度边缘智能应用程序需要处理分布式数据，由于隐私问题无法将其传输到中央服务器。分散式学习方法，如联邦学习，可以通过交换学到的权重来以集体方式学习模型。然而，它们通常需要复杂的模型，边缘设备可能无法处理，并且需要多轮网络通信才能达到最先进的性能。本研究提出了一种卷积集成学习方法，命名为EdgeConvEns，它可以在边缘上训练异构的弱模型，并学会将它们集成起来，在边缘上数据分布呈异质性。边缘模型采用不同计算能力的现场可编程门阵列（FPGA）设备进行独立实现和训练。

    Deep edge intelligence aims to deploy deep learning models that demand computationally expensive training in the edge network with limited computational power. Moreover, many deep edge intelligence applications require handling distributed data that cannot be transferred to a central server due to privacy concerns. Decentralized learning methods, such as federated learning, offer solutions where models are learned collectively by exchanging learned weights. However, they often require complex models that edge devices may not handle and multiple rounds of network communication to achieve state-of-the-art performances. This study proposes a convolutional ensemble learning approach, coined EdgeConvEns, that facilitates training heterogeneous weak models on edge and learning to ensemble them where data on edge are heterogeneously distributed. Edge models are implemented and trained independently on Field-Programmable Gate Array (FPGA) devices with various computational capacities. Learned 
    
[^83]: 用稀疏和嘈杂的标注进行主动学习的标签鲁棒分派

    Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations. (arXiv:2307.14380v1 [cs.LG])

    [http://arxiv.org/abs/2307.14380](http://arxiv.org/abs/2307.14380)

    本文解决了主动学习中错误数据注释的问题，提出了两种利用未标记样本空间的新型标注统一算法。

    

    监督分类算法用于解决全球越来越多的现实生活问题。它们的性能与训练中使用的标签质量密切相关。然而，对许多任务来说，获取高质量的注释是不可行的或者太昂贵以至于无法实际完成。为了解决这个挑战，通常使用主动学习算法仅选择最相关的数据进行标注。然而，这仅在从专家处获得的标签的质量和数量足够时才可能。不幸的是，在许多应用中，需要在为增加标签质量而多个注释人员注释个别样本与为增加标记实例的总数而注释新样本之间作出权衡。在本文中，我们针对主动学习中的错误数据注释问题进行了研究。具体而言，我们提出了两种利用未标记样本空间的新型标注统一算法。

    Supervised classification algorithms are used to solve a growing number of real-life problems around the globe. Their performance is strictly connected with the quality of labels used in training. Unfortunately, acquiring good-quality annotations for many tasks is infeasible or too expensive to be done in practice. To tackle this challenge, active learning algorithms are commonly employed to select only the most relevant data for labeling. However, this is possible only when the quality and quantity of labels acquired from experts are sufficient. Unfortunately, in many applications, a trade-off between annotating individual samples by multiple annotators to increase label quality vs. annotating new samples to increase the total number of labeled instances is necessary. In this paper, we address the issue of faulty data annotations in the context of active learning. In particular, we propose two novel annotation unification algorithms that utilize unlabeled parts of the sample space. Th
    
[^84]: DBGSA:一种新颖的数据自适应Bregman聚类算法

    DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm. (arXiv:2307.14375v1 [cs.LG])

    [http://arxiv.org/abs/2307.14375](http://arxiv.org/abs/2307.14375)

    本研究提出了一种新颖的数据自适应Bregman聚类算法（DBGSA），通过结合普适引力算法、引入Bregman分裂广义幂信息损失最小化方法和构建超参数识别优化模型，有效解决了传统聚类算法在初始质心选择敏感和处理非凸数据集的问题。

    

    随着大数据技术的发展，数据分析变得越来越重要。传统的聚类算法（如K-means）对初始质心的选择非常敏感，在非凸数据集上表现不佳。本文通过提出一种数据驱动的Bregman分裂参数优化聚类算法（DBGSA），解决了这些问题。该算法结合了普适引力算法，将相似点在数据集中靠近。我们构建了一个具有特殊属性的引力系数方程，随着迭代的进行逐渐降低影响因素。此外，我们引入了Bregman分裂广义幂信息损失最小化方法来识别聚类中心，并构建了一个超参数识别优化模型，有效解决了改进数据集中的手动调整和不确定性问题。对四个模拟数据集和六个真实数据集进行了大量实验。

    With the development of Big data technology, data analysis has become increasingly important. Traditional clustering algorithms such as K-means are highly sensitive to the initial centroid selection and perform poorly on non-convex datasets. In this paper, we address these problems by proposing a data-driven Bregman divergence parameter optimization clustering algorithm (DBGSA), which combines the Universal Gravitational Algorithm to bring similar points closer in the dataset. We construct a gravitational coefficient equation with a special property that gradually reduces the influence factor as the iteration progresses. Furthermore, we introduce the Bregman divergence generalized power mean information loss minimization to identify cluster centers and build a hyperparameter identification optimization model, which effectively solves the problems of manual adjustment and uncertainty in the improved dataset. Extensive experiments are conducted on four simulated datasets and six real dat
    
[^85]: 预测、捕获和活化二氧化碳（CO2）：时间序列分析、机器学习和材料设计的集成

    Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design. (arXiv:2307.14374v1 [cs.LG])

    [http://arxiv.org/abs/2307.14374](http://arxiv.org/abs/2307.14374)

    本研究综合应用时间序列分析、机器学习和材料设计，对欧洲国家和印度的不同行业CO2排放进行了全面预测和分析，发现电力、工业和陆地交通部门是数据集中变异最大的贡献者。

    

    本研究提供了从2019年1月至2023年2月的行业特定的每日二氧化碳（CO2）排放的全面时间序列分析。研究重点关注欧洲国家（EU27和英国、意大利、德国、西班牙）和印度的电力、工业、陆地交通、国内航空和国际航空部门，利用碳监测研究计划的近实时活动数据。为了识别常规排放模式，由于COVID-19大流行病造成的破坏性影响，该研究排除了2020年的数据。然后，研究通过主成分分析（PCA）确定了CO2排放的主要贡献者。分析结果显示，电力、工业和陆地交通部门在数据集中占据了相当大的变异。为了进一步的分析，使用了7天移动平均数据集来进行稳健的预测。该数据集捕捉到了短期和长期趋势，并增强了预测质量。

    This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO$_2$ emissions from January 2019 to February 2023. The research focuses on the Power, Industry, Ground Transport, Domestic Aviation, and International Aviation sectors in European countries (EU27 & UK, Italy, Germany, Spain) and India, utilizing near-real-time activity data from the Carbon Monitor research initiative. To identify regular emission patterns, the data from the year 2020 is excluded due to the disruptive effects caused by the COVID-19 pandemic. The study then performs a principal component analysis (PCA) to determine the key contributors to CO$_2$ emissions. The analysis reveals that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset. A 7-day moving averaged dataset is employed for further analysis to facilitate robust predictions. This dataset captures both short-term and long-term trends and enhances the quali
    
[^86]: 可由宽度无限、成本有限的浅层ReLU神经网络表示的分段线性函数

    Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks. (arXiv:2307.14373v1 [cs.LG])

    [http://arxiv.org/abs/2307.14373](http://arxiv.org/abs/2307.14373)

    本文研究了使用无限宽度、有限成本的浅层ReLU神经网络以及修正线性单元作为激活函数来表示连续分段线性函数。通过将度量从参数空间映射到函数定义域中的超平面，证明了这种神经网络可以表示任意分段线性函数。

    

    本文分析了使用修正线性单元（ReLU）作为激活函数的宽度无限、成本有限的浅层神经网络对连续的分段线性函数的表示。通过其积分表示，可以通过相应的有限符号度量在适当参数空间上识别出浅层神经网络。我们将这些度量映射到参数空间上的度量，并将参数空间中的点双射到函数定义域中的超平面。我们证明了Ongie等人的猜想，即每个可以由这种宽度无限神经网络表示的连续分段线性函数也可以由有限宽度的浅层ReLU神经网络表示。

    This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.
    
[^87]: 通过朴素贝叶斯分类器的机器学习模型预测大学生的抑郁状态

    Prediction of depression status in college students using a Naive Bayes classifier based machine learning model. (arXiv:2307.14371v1 [cs.LG])

    [http://arxiv.org/abs/2307.14371](http://arxiv.org/abs/2307.14371)

    通过朴素贝叶斯分类器的机器学习模型可以预测大学生的抑郁状态，该模型在早期发现和治疗抑郁方面具有有效性，有助于改善学生群体的心理健康。

    

    本研究提出了一个基于朴素贝叶斯分类器的机器学习模型，用于预测大学生的抑郁水平，目标是通过使用朴素贝叶斯分类器的机器学习模型，基于70%的训练数据和30%的验证数据，提高预测准确性，收集了来自519名大学生的与抑郁有关的因素，结果显示准确率为78.03%，在检测抑郁的阳性病例中具有较高的敏感性，尤其是在中度和重度水平上，以及在正确分类阴性病例方面具有显著的特异性，这些发现突显了该模型在早期发现和治疗抑郁方面的有效性，有益于弱势群体，并有助于改善学生群体的心理健康。

    This study presents a machine learning model based on the Naive Bayes classifier for predicting the level of depression in university students, the objective was to improve prediction accuracy using a machine learning model involving 70% training data and 30% validation data based on the Naive Bayes classifier, the collected data includes factors associated with depression from 519 university students, the results showed an accuracy of 78.03%, high sensitivity in detecting positive cases of depression, especially at moderate and severe levels, and significant specificity in correctly classifying negative cases, these findings highlight the effectiveness of the model in early detection and treatment of depression, benefiting vulnerable sectors and contributing to the improvement of mental health in the student population.
    
[^88]: Prot2Text: 基于GNNs和Transformers的多模态蛋白质功能生成

    Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.14367](http://arxiv.org/abs/2307.14367)

    提出了一种名为Prot2Text的新方法，通过结合GNNs和Transformers，以自由文本样式预测蛋白质的功能。该方法能够综合蛋白质的序列、结构和文本注释等多种数据类型，超越传统的二进制或分类分类，实现了对蛋白质功能的全面表示。

    

    大型生物系统的复杂性使某些科学家将其理解归类为难以想象的任务。不同级别的挑战使这项任务复杂化，其中之一是预测蛋白质的功能。近年来，通过开发各种机器学习方法，在这个领域取得了重大进展。然而，大多数现有的方法将任务表述为多分类问题，即将预定义标签分配给蛋白质。在这项工作中，我们提出了一种新的方法——Prot2Text，以自由文本样式预测蛋白质的功能，超越传统的二进制或分类分类。通过在编码器-解码器框架中结合图神经网络（GNNs）和大型语言模型（LLMs），我们的模型有效地整合了蛋白质序列、结构和文本注释等多种数据类型。这种多模态方法允许对蛋白质功能进行整体表示。

    The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
    
[^89]: 解释性差异补偿以实现有效的公平排序

    Explainable Disparity Compensation for Efficient Fair Ranking. (arXiv:2307.14366v1 [cs.LG])

    [http://arxiv.org/abs/2307.14366](http://arxiv.org/abs/2307.14366)

    这项研究提出了解释性的数据驱动的排名函数补偿措施，通过给予低调群体成员奖励积分来解决公平排序中的不平等问题。

    

    决策系统中使用的排名函数往往对不同人群产生不同的结果，因为基础数据存在偏见。解决和补偿这些不同的结果对于公平决策是一个关键问题。最近的补偿措施主要集中在对排名函数进行不透明的转换以满足公平保证，或者使用配额或留位来保证向代表低调群体的成员提供最少数量的积极结果。在本文中，我们提出了一种基于数据驱动的易解释的排名函数补偿措施。我们的措施依赖于给予低调群体成员的奖励积分来解决排名函数中的不平等性。奖励积分可以事先设置，并且可以进行组合，从而考虑到代表交叉，并为利益相关者提供更好的透明度。我们提出了高效的基于抽样的算法来计算数量

    Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number
    
[^90]: 具有非凸目标函数的联邦分布鲁棒优化：算法与分析

    Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])

    [http://arxiv.org/abs/2307.14364](http://arxiv.org/abs/2307.14364)

    本文提出了一个名为ASPIRE算法的异步分布式算法，用于解决联邦分布鲁棒优化问题，并引入了约束的D-范数不确定性集合，以灵活控制鲁棒性的程度。

    

    分布鲁棒优化 (DRO) 旨在找到一个最优决策，以在概率分布的模糊集合中最小化最坏情况成本，已在各种应用中广泛应用，例如网络行为分析、风险管理等。然而，现有的DRO技术面临三个关键挑战：1）如何处理分布环境中的异步更新；2）如何有效利用先验分布；3）如何根据不同场景适当调整鲁棒性的程度。为此，我们提出了一种名为Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE)算法的异步分布式算法，以处理联邦分布鲁棒优化 (FDRO) 问题。此外，我们还开发了一种新的不确定性集合，即约束的D-范数不确定性集合，以有效利用先验分布并灵活控制鲁棒性的程度。

    Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness.
    
[^91]: 无监督重建加速心脏动态MRI的神经场方法

    Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields. (arXiv:2307.14363v1 [eess.IV])

    [http://arxiv.org/abs/2307.14363](http://arxiv.org/abs/2307.14363)

    本文提出了一种无监督的方法，名为NF-cMRI，利用神经场表示重建加速心脏动态MRI，实验结果表明可以在较大的低采样率下获得良好的图像质量并改进时间描述。

    

    心脏动态MRI是心脏功能评估的黄金标准，但其本身较慢的采集过程使得对加速采样的重建方法成为必要。已经提出了几种利用时空冗余的正则化方法来重建加速采样的心脏动态MRI。最近，还提出了基于有监督深度学习的方法来进一步加速采集和重建。然而，这些技术通常依赖于大型数据集进行训练，而这些数据集并不总是可用的。在本研究中，我们提出了一种基于隐式神经场表示的无监督方法用于心脏动态MRI（称为NF-cMRI）。这种方法在体内低采样角度金属线圈多次获得中进行评估，得到了良好的图像质量，并且在空间和时间描述方面与最先进的重建技术相当或有所改进。

    Cardiac cine MRI is the gold standard for cardiac functional assessment, but the inherently slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions. Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction techn
    
[^92]: 可学习的小波神经网络在宇宙学推断中的应用

    Learnable wavelet neural networks for cosmological inference. (arXiv:2307.14362v1 [astro-ph.IM])

    [http://arxiv.org/abs/2307.14362](http://arxiv.org/abs/2307.14362)

    本论文研究了在宇宙学推断中应用可学习的散射变换方法，与传统的两点统计相比，这种方法能够提取更多信息并很好地消除天体物理效应的影响。实验证明，在小样本训练数据集上，散射网络的性能优于卷积神经网络。

    

    卷积神经网络(CNNs)已被证明可以从宇宙学领域中提取比传统的两点统计更多的信息，并且能够非常好地消除天体物理效应的影响。然而，CNNs需要大量的训练数据，在昂贵的宇宙学模拟领域可能存在问题，并且很难解释网络的工作原理。在本文中，我们将可学习的散射变换应用于宇宙学推断和天体物理效应边缘化的问题中，散射变换是一种使用可训练小波作为滤波器的卷积神经网络。我们提出了两个基于散射变换的模型，一个是为了性能而构建的，另一个是为了解释性而构建的，并与CNN进行了比较。我们发现，在训练数据样本较小的情况下，散射网络能够显著优于CNN。此外，我们还提出了一种轻量级的散射网络，它具有非常好的可解释性。

    Convolutional neural networks (CNNs) have been shown to both extract more information than the traditional two-point statistics from cosmological fields, and marginalise over astrophysical effects extremely well. However, CNNs require large amounts of training data, which is potentially problematic in the domain of expensive cosmological simulations, and it is difficult to interpret the network. In this work we apply the learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters, to the problem of cosmological inference and marginalisation over astrophysical effects. We present two models based on the scattering transform, one constructed for performance, and one constructed for interpretability, and perform a comparison with a CNN. We find that scattering architectures are able to outperform a CNN, significantly in the case of small training data samples. Additionally we present a lightweight scattering network that is highly interpr
    
[^93]: 一种基于LSTM、BiLSTM、CNN、GRU和GloVe的混合机器学习模型用于基因突变在癌症中的分类

    A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.14361](http://arxiv.org/abs/2307.14361)

    本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。

    

    本研究提出了一个集成模型，将LSTM、BiLSTM、CNN、GRU和GloVe结合起来，用于在Kaggle的“个性化医学：重新定义癌症治疗”数据集中对基因突变进行分类。通过与BERT、Electra、Roberta、XLNet、Distilbert以及它们的LSTM集成等知名转换器进行比较，结果显示我们的模型在准确率、精确率、召回率、F1分数和均方误差方面都优于其他模型。令人惊讶的是，它还需要较少的训练时间，实现了性能和效率的完美结合。该研究证明了集成模型在基因突变分类等困难任务中的实用性。

    This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
    
[^94]: 一种新的无导数优化方法：高斯压缩搜索

    A new derivative-free optimization method: Gaussian Crunching Search. (arXiv:2307.14359v1 [math.OC])

    [http://arxiv.org/abs/2307.14359](http://arxiv.org/abs/2307.14359)

    这篇论文介绍了一种新的无导数优化方法，高斯压缩搜索（GCS），通过模拟高斯分布中粒子的行为，该方法能够高效地探索解空间并找到全局最优解。通过实验评估和与其他方法的比较，突出了GCS的优势和潜力。

    

    优化方法在解决各个领域的复杂问题中至关重要。在这篇研究论文中，我们介绍了一种新颖的优化方法，称为高斯压缩搜索（GCS）。受到高斯分布中粒子行为的启发，GCS旨在高效地探索解空间，并收敛到全局最优解。我们对GCS进行了全面的分析，包括其工作机制和潜在应用。通过与现有优化方法的实验评估和比较，我们突出了GCS的优势和强项。这篇研究论文为对优化感兴趣的研究人员、实践者和学生提供了宝贵的资源，提供了关于高斯压缩搜索作为一种新的、有前景的方法的开发和潜力的见解。

    Optimization methods are essential in solving complex problems across various domains. In this research paper, we introduce a novel optimization method called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles in a Gaussian distribution, GCS aims to efficiently explore the solution space and converge towards the global optimum. We present a comprehensive analysis of GCS, including its working mechanism, and potential applications. Through experimental evaluations and comparisons with existing optimization methods, we highlight the advantages and strengths of GCS. This research paper serves as a valuable resource for researchers, practitioners, and students interested in optimization, providing insights into the development and potential of Gaussian Crunching Search as a new and promising approach.
    
[^95]: 学习网格化以提高点云处理效率

    Learned Gridification for Efficient Point Cloud Processing. (arXiv:2307.14354v1 [cs.CV])

    [http://arxiv.org/abs/2307.14354](http://arxiv.org/abs/2307.14354)

    本论文提出了学习网格化方法，用于将点云转换为紧凑、规则的网格，在点云处理中提高了可扩展性，特别是对于大输入和大邻域。

    

    与网格数据相比，点云上依赖于邻域信息的神经操作会更加昂贵，因为点云中点之间的距离是不规则的。另一方面，在网格上，我们可以仅计算一次卷积核，然后重用它来处理所有查询位置。因此，与网格数据相比，依赖于邻域信息的操作在点云上的可扩展性更差，特别是对于大输入和大邻域。在这项工作中，我们通过解决数据的不规则性来解决点云方法的可扩展性问题。我们提出了可学习的网格化作为点云处理流程的第一步，将点云转换为紧凑、规则的网格。通过网格化，后续的层可以使用在规则网格上定义的操作，例如Conv3D，这比原生的点云方法具有更好的可扩展性。然后，我们将网格化扩展到点云到点云的任务，例如分割，通过添加一个...

    Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.  In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a l
    
[^96]: 多目标深度强化学习用于移动边缘计算

    Multi-objective Deep Reinforcement Learning for Mobile Edge Computing. (arXiv:2307.14346v1 [cs.NI])

    [http://arxiv.org/abs/2307.14346](http://arxiv.org/abs/2307.14346)

    本研究提出了一种多目标深度强化学习方法，以解决移动边缘计算中的离线问题。该方法通过考虑未知偏好参数，最小化能耗和传输延迟，并采用近端策略优化算法进行资源调度。引入了一种特征构建方法，用于处理MEC系统中的多个边缘。

    

    移动边缘计算（MEC）对于下一代移动网络应用至关重要，这些应用优先考虑各种性能指标，包括延迟和能耗。然而，传统的单目标调度解决方案不能直接应用于实际系统，因为这些应用的偏好（即不同目标的权重）通常是未知的或难以事先指定。在这项研究中，我们通过制定一个多目标离线问题来解决这个问题，针对MEC中的多个边缘来最小化预期的长期能耗和传输延迟，同时考虑未知的偏好作为参数。为了解决未知偏好的挑战，我们设计了一种基于深度强化学习（MORL）和近端策略优化（PPO）的多目标资源调度方案。此外，我们还引入了一种精心设计的状态编码方法，用于构建MEC系统中多个边缘的特征。

    Mobile edge computing (MEC) is essential for next-generation mobile network applications that prioritize various performance metrics, including delays and energy consumption. However, conventional single-objective scheduling solutions cannot be directly applied to practical systems in which the preferences of these applications (i.e., the weights of different objectives) are often unknown or challenging to specify in advance. In this study, we address this issue by formulating a multi-objective offloading problem for MEC with multiple edges to minimize expected long-term energy consumption and transmission delay while considering unknown preferences as parameters. To address the challenge of unknown preferences, we design a multi-objective (deep) reinforcement learning (MORL)-based resource scheduling scheme with proximal policy optimization (PPO). In addition, we introduce a well-designed state encoding method for constructing features for multiple edges in MEC systems, a sophisticate
    
[^97]: MNIST手写数字中的扭曲图像剪枝

    Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])

    [http://arxiv.org/abs/2307.14343](http://arxiv.org/abs/2307.14343)

    本论文提出了一个两阶段的深度学习方法，通过识别和剔除扭曲和模糊图像，从而提高了MNIST数据集中手写数字的分类准确性和置信度。

    

    由于书写风格的多样性和噪声图像的存在，识别手写数字是一项具有挑战性的任务。广泛使用的MNIST数据集，作为这个任务的基准，包含具有不规则形状、不完整笔画和变异倾斜度的扭曲数字，同时存在于训练和测试数据集中。因此，这些因素导致了数字识别准确度的降低。为了克服这个挑战，我们提出了一个两阶段的深度学习方法。在第一阶段，我们使用一个简单的神经网络来识别训练集中的扭曲数字。这个模型用于检测和过滤出这些扭曲和模糊的图像。在第二阶段，我们将这些被识别出的图像从训练数据集中排除，并使用过滤后的数据集重新训练模型。这个过程旨在提高分类准确性和置信度，同时减轻欠拟合和过拟合的问题。我们的实验结果证明了这个方法的有效性。

    Recognizing handwritten digits is a challenging task primarily due to the diversity of writing styles and the presence of noisy images. The widely used MNIST dataset, which is commonly employed as a benchmark for this task, includes distorted digits with irregular shapes, incomplete strokes, and varying skew in both the training and testing datasets. Consequently, these factors contribute to reduced accuracy in digit recognition. To overcome this challenge, we propose a two-stage deep learning approach. In the first stage, we create a simple neural network to identify distorted digits within the training set. This model serves to detect and filter out such distorted and ambiguous images. In the second stage, we exclude these identified images from the training dataset and proceed to retrain the model using the filtered dataset. This process aims to improve the classification accuracy and confidence levels while mitigating issues of underfitting and overfitting. Our experimental results
    
[^98]: 牙科放射学分割的扩散模型预训练

    Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v1 [cs.CV])

    [http://arxiv.org/abs/2307.14066](http://arxiv.org/abs/2307.14066)

    本文提出了一种利用扩散模型进行预训练的方法，用于牙科放射学分割，实验结果表明该方法在标签效率方面具有显著性能，竞争力强。

    

    医学放射学分割，尤其是牙科放射学分割，受到标记成本的严重限制，需要具有特定专业知识和繁重的注释。本文提出了一种简单的预训练方法，利用去噪扩散概率模型（DDPM）进行语义分割，该模型在生成建模方面显示出令人印象深刻的结果。我们的简单方法在标签效率方面取得了显著的性能，并且在预训练和后续任务之间不需要架构修改。我们提议首先通过利用DDPM训练目标对Unet进行预训练，然后在分割任务上对得到的模型进行微调。我们对牙科放射片的分割的实验结果表明，所提出的方法与最先进的预训练方法相竞争。

    Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
    
[^99]: 如何扩展您的EMA（arXiv:2307.13813v1 [stat.ML]）

    How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])

    [http://arxiv.org/abs/2307.13813](http://arxiv.org/abs/2307.13813)

    本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。

    

    在实际机器学习中，保持训练动态在批量大小之间的一致性是一种重要工具，它能够在批量大小和墙钟时间之间进行权衡。这种权衡通常通过一个缩放规则来实现，例如，在随机梯度下降中，应该将学习率与批量大小呈线性关系。另一个实际机器学习的重要工具是模型指数移动平均（EMA），它是一个不接收梯度信息的模型副本，而是以一定的动量跟随其目标模型。这个模型EMA可以提高监督学习的稳健性和泛化性能，稳定伪标记，为自监督学习提供学习信号。之前的研究将模型EMA与优化分开处理，导致批量大小之间存在不同的训练动态和较低的模型性能。在这项工作中，我们提供了在存在模型EMA的情况下进行优化的缩放规则，并展示了其效果。

    Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
    
[^100]: 深度布拉德利-特里评分：在没有具体评价标准的情况下估计物品的属性

    Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])

    [http://arxiv.org/abs/2307.13709](http://arxiv.org/abs/2307.13709)

    本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。

    

    在现实世界中，许多属性，如竞争环境中的可取性或强度，无法直接观测，这使得它们难以评估。为了解决这个具有挑战性的问题，先前的研究主要集中在估计已知物品的这些属性，特别是出现在配对比较数据集中的运动员的实力。在本文中，我们介绍了深度布拉德利-特里评分（DBTR），这是一个新颖的机器学习框架，用于评估不一定存在于数据集中的未知物品的任何属性。我们的方法无缝地将传统的布拉德利-特里模型与神经网络结构相结合。我们还进一步推广了这个架构，用于具有不公平性的非对称环境，这在现实世界中更为常见。在我们的实验分析中，DBTR成功地学习了这些属性的预期量化。

    Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
    
[^101]: Duet: 高效且可扩展的混合神经关系理解

    Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])

    [http://arxiv.org/abs/2307.13494](http://arxiv.org/abs/2307.13494)

    Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。

    

    基于概率分布估计的基数估计方法相较于传统方法取得了高精度的估计结果。然而，最先进的方法由于在处理范围查询时使用的采样方法而导致估计成本较高。此外，这种采样方法也使得它们难以区分，因此来自查询工作负载的监督信号很难训练模型以提高基数估计的准确性。在本文中，我们提出了一种新的混合确定性建模方法（Duet）用于基数估计问题，与以前的方法相比，具有更好的效率和可扩展性。Duet可以以更低的时间和内存成本直接估计范围查询的基数，并且以可区分的形式呈现。由于此方法的预测过程是可微分的，我们可以将估计误差较大的查询纳入训练过程以进行改进。

    Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
    
[^102]: 非侵入式自学习语音表示对听力障碍个体的清晰度预测

    Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v1 [cs.SD])

    [http://arxiv.org/abs/2307.13423](http://arxiv.org/abs/2307.13423)

    本研究将自学习语音表示应用于预测听力障碍个体的可理解性，并发现其作为非侵入式预测模型的输入特征具有竞争性能，需要更多数据才能推广到未知系统和个体。

    

    自学习语音表示 (SSSRs) 已成功应用于多个语音处理任务，例如作为语音质量 (SQ) 预测的特征提取器，这对于评估和训练正常或有听力障碍的用户的语音增强系统至关重要。然而，为什么和如何将与质量相关的信息嵌入到这样的表示中仍然知之甚少。在这项工作中，非侵入式 SQ 评级预测技术被扩展到预测听力障碍用户的可理解性。发现自学习表示作为非侵入式预测模型的输入特征非常有用，其性能竞争力强于更复杂的系统。针对 Clarity Prediction Challenge 1 受试者和增强系统的性能进行了详细分析，结果表明可能需要更多数据才能推广到未知系统和（听力受损的）个体。

    Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
    
[^103]: 关于受恶意噪声影响的公正约束学习的脆弱性

    On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])

    [http://arxiv.org/abs/2307.11892](http://arxiv.org/abs/2307.11892)

    这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\Theta(\alpha)$和$O(\sqrt{\alpha})$，对应不同的公正约束要求。

    

    我们考虑了公正约束学习对训练数据中微小恶意噪声的脆弱性。Konstantinov和Lampert (2021)在这个问题上进行了研究，并展示了负面结果，表明在不平衡的群组大小下存在一些数据分布，任何适当的学习器都会表现出较高的脆弱性。在这里，我们展示了更乐观的观点，如果允许随机分类器，则情况更加细致。例如，对于人口统计学平等性，我们显示只会产生$\Theta(\alpha)$的精度损失，其中$\alpha$是恶意噪声率，甚至可以与没有公正约束的情况完全匹配。对于机会均等性，我们显示只会产生$O(\sqrt{\alpha})$的损失，并给出一个匹配的$\Omega(\sqrt{\alpha})$的下界。相比之下，Konstantinov和Lampert (2021)示范了对于适当的学习器，这两个概念的精度损失都是$\Omega(1)$。关键的技术创新是

    We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
    
[^104]: TwinLiteNet：自动驾驶汽车中可驱动区域和车道分割的高效轻量模型

    TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])

    [http://arxiv.org/abs/2307.10705](http://arxiv.org/abs/2307.10705)

    本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。

    

    语义分割是自动驾驶中一个常见的任务，用于理解周围环境。对于道路上的安全和高效导航来说，可驱动区域分割和车道检测尤为重要。然而，原始的语义分割模型计算开销大，需要高端硬件，这对于嵌入式系统的自动驾驶车辆来说是不可行的。本文提出了一种轻量级的可驱动区域和车道线分割模型。TwinLiteNet设计成成本低廉，但能够实现准确和高效的分割结果。我们在BDD100K数据集上评估了TwinLiteNet，并与现代模型进行了比较。实验结果表明，我们的TwinLiteNet与现有方法表现相似，但所需的计算资源显著减少。具体而言，TwinLiteNet在可驱动区域任务上实现了91.3%的mIoU评分，在车道检测任务上实现了31.08%的IoU评分，仅使用了40万个参数，在GPU RTX上实现了415 FPS。

    Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
    
[^105]: TimeTuner: 诊断时间序列预测中的时间表示的对照解释

    TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])

    [http://arxiv.org/abs/2307.09916](http://arxiv.org/abs/2307.09916)

    TimeTuner是一个新颖的可视化分析框架，旨在帮助分析人员理解时间序列预测中模型行为与时间表示的关系，并解决自动化特征学习方法的局限性。

    

    深度学习方法在时间序列预测中的应用越来越多，许多努力致力于设计复杂的深度学习模型。最近的研究表明，深度学习的成功往往归因于有效的数据表示，促进了特征工程和表示学习领域的发展。然而，自动化特征学习方法通常在融入先验知识、识别变量间相互作用和选择评估指标以确保模型可靠性方面存在局限性。为了改善这些限制，本文提出了一种新颖的可视化分析框架，即TimeTuner，旨在帮助分析人员理解模型行为与时间序列表示的局部相关性、平稳性和粒度之间的关联。该系统主要包括以下两个阶段技术：我们首先利用对照解释来建立时间序列表示之间的关系，

    Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations,
    
[^106]: 一种贝叶斯方法用于量化交通预测模型中的不确定性和改善泛化能力

    A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])

    [http://arxiv.org/abs/2307.05946](http://arxiv.org/abs/2307.05946)

    本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。

    

    交通数据预测的深度学习模型可以通过多层架构对复杂函数进行优化建模，但这些方法的一个主要缺点是大多数方法不提供带有不确定性估计的预测结果，而这对于交通运营和控制是必需的。本研究提出了一种贝叶斯循环神经网络框架，通过引入谱归一化到其隐藏层，实现交通预测中的不确定性量化和更高的泛化能力。我们的论文表明，归一化通过控制模型的复杂性并减少对训练数据的过度拟合风险，改善了深度神经网络的泛化性能。

    Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
    
[^107]: 将电池电解质的结构组成映射到器件性能的配方图

    Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.03811](http://arxiv.org/abs/2307.03811)

    该论文提出了一种深度学习模型，Formulation Graph Convolution Network（F-GCN），它可以将电池电解质的结构组成关系映射到整个液体配方的性能，从而加快新化合物的发现和应用。

    

    正在积极寻求先进的计算方法来解决发现和开发新的组合材料（如配方）所面临的挑战。一个广泛采用的方法是领域感知的高通量筛选，该方法可以将各个组分组合成配方。这种方法能够加速寻找目标应用的新化合物，但是在从精选化学空间中识别出合适的“配方”方面仍然主要是实验驱动的过程。我们报告了一种深度学习模型，称为Formulation Graph Convolution Network（F-GCN），可以将各个组分的结构组成关系映射到液体配方的性质。多个GCN并行组装，并根据配方中各个组成部分的摩尔百分比直观地对配方成分进行特征化。然后根据相应组成部分的摩尔百分比对所得的分子描述符进行缩放，接下来进行...

    Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed
    
[^108]: 在持续观察下的聚类问题中的差分隐私

    Differential Privacy for Clustering Under Continual Observation. (arXiv:2307.03430v1 [cs.DS])

    [http://arxiv.org/abs/2307.03430](http://arxiv.org/abs/2307.03430)

    本文提出了一种在持续观察下的差分隐私聚类机制，用于在被删除和插入数据点的数据集中进行聚类，这是第一个具有仅以更新次数的对数依赖性的增加误差的近似算法，并且乘法误差几乎与非隐私情况相同。

    

    我们考虑在$\mathbb{R}^d$中进行隐私聚类的问题，该问题在数据集中插入和删除点。具体来说，我们提供了一个在持续观察下的$\varepsilon$-差分隐私聚类机制，用于 $k$-means 目标。这是该问题的第一个近似算法，其增加的误差仅以更新次数 $T$ 的对数依赖性。乘法误差与非隐私情况几乎相同。为此，我们展示了如何在持续观察中进行维度缩减，并将其与用于 $k$-means 的差分隐私贪心逼近算法相结合。我们还部分地将我们的结果推广到 $k$-median 问题上。

    We consider the problem of clustering privately a dataset in $\mathbb{R}^d$ that undergoes both insertion and deletion of points. Specifically, we give an $\varepsilon$-differentially private clustering mechanism for the $k$-means objective under continual observation. This is the first approximation algorithm for that problem with an additive error that depends only logarithmically in the number $T$ of updates. The multiplicative error is almost the same as non privately. To do so we show how to perform dimension reduction under continual observation and combine it with a differentially private greedy approximation algorithm for $k$-means. We also partially extend our results to the $k$-median problem.
    
[^109]: Fraunhofer SIT在CheckThat! 2023中使用模型混合技术解决分类不确定性的研究：以可检查性分类为例

    Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02377](http://arxiv.org/abs/2307.02377)

    本研究介绍了Fraunhofer SIT团队在CLEF-2023 CheckThat!英语实验室任务1B中使用模型混合技术解决分类不确定性的方法，该方法的目的是确定政治辩论中的文本片段是否值得进行事实检查评估，并在比赛中排名第二。

    

    本文描述了Fraunhofer SIT团队在CLEF-2023 CheckThat!英语实验室任务1B中获得第二名的方法。给定一段政治辩论的文本片段，该任务的目标是确定是否应该对其进行检查价值评估。检测可检查声明旨在通过优先考虑事实检查人员应首先考虑的声明来简化手动事实检查工作。它还可以被视为事实检查系统的主要步骤。我们的最佳方法利用了以模型混合为中心的集成分类方案。在应用于英语数据集时，我们提交的模型在比赛中获得了0.878的整体F1得分，并被评为第二好的模型。

    This paper describes the second-placed approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text snippet from a political debate, the aim of this task is to determine whether it should be assessed for check-worthiness. Detecting check-worthy statements aims to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. It can also be considered as primary step of a fact-checking system. Our best-performing method took advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall F1 score of 0.878 and was ranked as the second-best model in the competition.
    
[^110]: 利用图像的学习压缩表示来进行语义分割的利用

    Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])

    [http://arxiv.org/abs/2307.01524](http://arxiv.org/abs/2307.01524)

    该论文提出了使用学习压缩表示进行语义分割的方法，以减少解压缩操作的延迟开销，并在实验证明了这种方法的有效性。

    

    自动驾驶车辆和先进驾驶辅助系统（ADAS）有可能彻底改变我们出行的方式。许多这样的车辆目前依赖于分割和目标检测算法来检测和跟踪周围的物体。从车辆收集的数据通常被发送到云服务器以便于对这些算法进行持续/终身学习。考虑到带宽限制，数据在发送到服务器之前被压缩，通常会在训练和分析时进行解压缩。在这项工作中，我们提出使用基于学习的压缩编解码器来减少标准流水线中解压缩操作所产生的时延开销。我们证明了学习的压缩表示还可以用于执行语义分割等任务，以获取图像。我们在Cityscapes数据集上进行了实验证实所提出的流水线，在其中实现了一个压缩因子。

    Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression facto
    
[^111]: Fraunhofer SIT在CheckThat! 2023中的贡献：混合单模分类器以估计多模态推文的可靠性

    Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00610](http://arxiv.org/abs/2307.00610)

    本文提出了一种混合单模分类器的方法，通过组合图像和文本分类器的结果，成功进行多模态推文的可靠性估计，并在CheckThat! 2023任务1A中取得了最佳表现。

    

    在社交媒体上分享图像、视频和音频文件的选项为区分网络上的虚假信息和假新闻提供了新的可能性。由于社交媒体每秒分享的海量数据，无法通过计算机或人类专家对所有数据进行验证。因此，可通过可靠性分析作为事实核查流程的第一步，以及作为提高效率的过滤机制。本文提出了一种新颖的方法来检测多模态推文的可靠性。它利用了两个在单模态上训练的分类器。对于图像数据，通过OCR分析提取嵌入的文本表现最佳。通过组合这两个分类器，该方法在CheckThat! 2023任务1A中达到了0.7297的F1分数，在私人测试集上排名第一。

    The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set.
    
[^112]: 2D-Shapley: 一个针对碎片化数据估值的框架

    2D-Shapley: A Framework for Fragmented Data Valuation. (arXiv:2306.10473v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10473](http://arxiv.org/abs/2306.10473)

    该论文提出了2D-Shapley框架，用于估值碎片化数据。该框架通过计算从聚合数据矩阵中移除一个碎片的对立现实，实现了对碎片化数据源的估值。而且，2D-Shapley满足碎片化数据环境下的一些重要公理，为选择有用的数据片段、解释样本数据值和进行精细化的数据问题诊断等提供了新的使用案例。

    

    数据估值——量化单个数据源对模型某些预测行为的贡献——对于增强机器学习的透明度和设计数据共享的激励系统至关重要。现有工作主要集中在评估具有共享特征或样本空间的数据源。如何估值碎片化数据源，其中每个只包含部分特征和样本，仍然是一个开放的问题。我们首先提出了一种计算从聚合数据矩阵中移除一个碎片的对立现实的方法。基于对立现实的计算，我们进一步提出了2D-Shapley，一个在碎片化数据估值环境中满足一些吸引人的公理的理论框架。 2D-Shapley能够实现一系列新的用例，比如选择有用的数据片段，为样本数据值提供解释，并进行精细化的数据问题诊断。

    Data valuation -- quantifying the contribution of individual data sources to certain predictive behaviors of a model -- is of great importance to enhancing the transparency of machine learning and designing incentive systems for data sharing. Existing work has focused on evaluating data sources with the shared feature or sample space. How to valuate fragmented data sources of which each only contains partial features and samples remains an open question. We start by presenting a method to calculate the counterfactual of removing a fragment from the aggregated data matrix. Based on the counterfactual calculation, we further propose 2D-Shapley, a theoretical framework for fragmented data valuation that uniquely satisfies some appealing axioms in the fragmented data context. 2D-Shapley empowers a range of new use cases, such as selecting useful data fragments, providing interpretation for sample-wise data values, and fine-grained data issue diagnosis.
    
[^113]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^114]: 在因子图中自动进行模型比较

    Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])

    [http://arxiv.org/abs/2306.05965](http://arxiv.org/abs/2306.05965)

    本文基于自定义混合节点 Forney 样式的因子图消息传递，实现了高效自动化贝叶斯模型平均、选择和组合，并缩短了模型设计周期。

    

    在文献中，贝叶斯状态和参数估计已经被有效自动化，但对于模型比较尚未如此，因此仍需要容易出错和耗时的手动推导。因此，模型比较经常被忽视和忽略，尽管它很重要。本文通过在Forney样式的因子图上使用自定义混合节点上的消息传递来高效地自动化贝叶斯模型平均、选择和组合。进而可使用缩放因子同时执行参数和状态推断以及模型比较。这种方法缩短了模型设计周期，同时允许简单地扩展到分层和时间模型先验，以适应建模复杂的时变过程。

    Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
    
[^115]: 基于群等变性的傅里叶神经算子求解偏微分方程

    Group Equivariant Fourier Neural Operators for Partial Differential Equations. (arXiv:2306.05697v1 [cs.LG])

    [http://arxiv.org/abs/2306.05697](http://arxiv.org/abs/2306.05697)

    本文扩展了群卷积到频率域，并设计了具有旋转、平移和镜像等变性质的傅里叶层的G-FNO架构，该架构在不同对称性水平的设置中表现良好。

    

    本研究考虑使用在频率域下操作的傅里叶神经算子（FNOs）求解偏微分方程（PDEs）。由于物理定律不依赖于用于描述它们的坐标系, 因此将这些对称性编码进神经算子架构以提高性能和更容易的学习是有益的。尽管使用群论编码物理域内的对称性已经被广泛研究，但如何在频率域内捕捉对称性仍未得到充分探讨。本文将群卷积扩展到频率域，并设计具有旋转、平移和镜像等变性质的傅里叶层，利用傅里叶变换的等变性质。产生的G-FNO架构跨输入分辨率具有良好的推广效果，并在具有不同对称性水平的设置中表现良好。我们的代码可作为AIRS库的一部分公开在Github（https://github.com/divelab/AIRS）上获得。

    We consider solving partial differential equations (PDEs) with Fourier neural operators (FNOs), which operate in the frequency domain. Since the laws of physics do not depend on the coordinate system used to describe them, it is desirable to encode such symmetries in the neural operator architecture for better performance and easier learning. While encoding symmetries in the physical domain using group theory has been studied extensively, how to capture symmetries in the frequency domain is under-explored. In this work, we extend group convolutions to the frequency domain and design Fourier layers that are equivariant to rotations, translations, and reflections by leveraging the equivariance property of the Fourier transform. The resulting $G$-FNO architecture generalizes well across input resolutions and performs well in settings with varying levels of symmetry. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS).
    
[^116]: 高效交替最小化及其在加权低秩逼近中的应用

    Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v1 [cs.LG])

    [http://arxiv.org/abs/2306.04169](http://arxiv.org/abs/2306.04169)

    本文提出了一种高效的求解加权低秩逼近问题的交替最小化框架，运行时间优化到了 n^2k，核心方法是一种高精度的多响应回归方法。

    

    加权低秩逼近是数值线性代数中的一个基本问题，在机器学习中有许多应用。本文提出了一种高效且鲁棒的交替最小化框架，用于求解该问题，并将运行时间从 n^2k^2 优化到了 n^2k。在我们的框架的核心是一种高精度的多响应回归方法。

    Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n \times n}$, a parameter $k$, the goal is to output two matrices $U, V \in \mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized, where $\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression
    
[^117]: PlaSma: 为 (反事实) 计划制定增强过程知识模型的小型语言模型

    PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])

    [http://arxiv.org/abs/2305.19472](http://arxiv.org/abs/2305.19472)

    PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法，

    

    过程规划是机器的一项重要而又复杂的任务，它将一个高级目标分解为一系列时间顺序的步骤。它需要整合常识知识以推理出常常是反事实的复杂情境，例如 "没有电话时安排医生的约会"。当前的方法使用大型语言模型 (LLM) 取得了令人鼓舞的结果，但受到昂贵的 API 调用和可复现性问题的限制。本文提出使用更小的语言模型来进行规划，我们介绍了 PlaSma，这是一种新的双重方法，使小型语言模型具有过程知识和 (反事实) 计划能力。更具体地说，我们开发了符号过程知识蒸馏来增强小型语言模型中的隐含知识，以及一种推理算法来促进更结构化和准确的推理。此外，我们还引入了一个新的任务，反事实规划。

    Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
    
[^118]: 探索长尾识别问题中的权重平衡

    Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])

    [http://arxiv.org/abs/2305.16573](http://arxiv.org/abs/2305.16573)

    研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。

    

    长尾数据中的识别问题最近变得越来越重要，因为数据集中每个类别的样本数量分布通常是指数分布，除非有意地调整样本数量。针对这些问题已经提出了各种方法。最近，提出了权重平衡方法，它结合了著名的经典正则化技术和两阶段训练。尽管其简单性，但已知其对现有各种不同方法具有高性能。然而，我们缺乏为什么这种方法对长尾数据有效的理解。在这项研究中，我们分析了该方法，并关注了神经崩溃和每个训练阶段的圆锥效应，并发现它可以分解为由权值衰减和交叉熵损失引起的特征提取器中Fisher判别比的增加以及由权重衰减和类平衡正则化引起的隐式逻辑调整。我们还证明了权重平衡方法成功缓解了神经崩溃和圆锥效应，从而提高了长尾数据的识别性能。

    Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
    
[^119]: 基于自监督高斯正则化的深度分类器马氏距离不确定性评估

    Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])

    [http://arxiv.org/abs/2305.13849](http://arxiv.org/abs/2305.13849)

    本文提出了一种自监督高斯正则化的深度分类器，可用于马氏距离不确定性评估，相比现有方法，该方法不需要对模型架构和训练程序做出大的改变，并在标准OOD基准测试上取得了最先进的性能。

    

    近期的工作表明，网络的潜在空间中的数据分布对于估计分类不确定性和检测超出分布范围（OOD）的样本非常有用。为了获得适用于不确定性估计的良好正则化潜在空间，现有方法对模型架构和训练程序进行了重大改变。在本文中，我们提出了一种用于马氏距离基础不确定性预测的轻量级、快速、高性能正则化方法，并且对网络架构的改动要求最小。为了得到适用于马氏距离计算的高斯潜在表示，我们引入了一种自监督表示学习方法，将类内表示分为多个高斯。具有非高斯表示的类别被自动识别并动态聚类为多个大概率是高斯分布的类别。在标准OOD基准测试上的评估显示出，我们的方法实现了最先进的性能，同时保持轻量级和高效的模型架构。

    Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a
    
[^120]: 学习通过分位数回归和流量学习驾驶行为分布的尾部分位数

    On Learning the Tail Quantiles of Driving Behavior Distributions via Quantile Regression and Flows. (arXiv:2305.13106v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13106](http://arxiv.org/abs/2305.13106)

    该论文提出了两种学习驾驶行为分布尾部分位数的方法，分别是基于分位数回归和流量学习的框架，并在自动驾驶中进行了评估和验证。

    

    针对安全的自动驾驶（AD），我们考虑学习模型以准确捕捉人类驾驶员行为概率分布的多样性和尾部分位数，与AD车辆进行交互。这些模型通过从状态预测驾驶员的连续动作，对于缩小AD代理模拟与现实之间的差距特别重要。为此，我们适应了两个灵活的分位数学习框架：（1）基于倾斜绝对损失的分位数回归，和（2）自回归分位数流（一种归一化流的版本）。训练采用行为克隆方式进行。我们使用了包含多条高速公路上驾驶员轨迹的高D数据集。我们在一步加速度预测任务和多步驾驶员模拟推理中评估了我们的方法。我们报告了使用倾斜绝对损失作为指标的定量结果，并给出了定性示例，显示出我们的方法的性能。

    Towards safe autonomous driving (AD), we consider the problem of learning models that accurately capture the diversity and tail quantiles of human driver behavior probability distributions, in interaction with an AD vehicle. Such models, which predict drivers' continuous actions from their states, are particularly relevant for closing the gap between AD agent simulations and reality. To this end, we adapt two flexible quantile learning frameworks for this setting that avoid strong distributional assumptions: (1) quantile regression (based on the titled absolute loss), and (2) autoregressive quantile flows (a version of normalizing flows). Training happens in a behavior cloning-fashion. We use the highD dataset consisting of driver trajectories on several highways. We evaluate our approach in a one-step acceleration prediction task, and in multi-step driver simulation rollouts. We report quantitative results using the tilted absolute loss as metric, give qualitative examples showing tha
    
[^121]: 差分卷积模糊时间序列预测

    Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])

    [http://arxiv.org/abs/2305.08890](http://arxiv.org/abs/2305.08890)

    本文提出了一种新的预测模型DFCNN，利用卷积神经网络实现具有可学习能力的FTSF，并能够处理非平稳时间序列。

    

    模糊时间序列预测（FTSF）是一种具有广泛应用的典型预测方法。传统的FTSF被认为是一种专家系统，导致失去了识别未定义特征的能力，这是FTSF预测不准确的主要原因。为了解决这个问题，提出了差分模糊卷积神经网络（DFCNN）模型，利用卷积神经网络实现具有可学习能力的FTSF。DFCNN能够识别潜在信息并改善预测精度。由于神经网络的可学习能力，FTSF建立的模糊规则的长度可以任意扩展，这是专家系统所无法处理的。同时，由于非平稳时间序列的趋势，FTSF通常无法实现令人满意的性能。非平稳时间序列的趋势会导致FTSF建立的模糊集失效，并导致预测失败。DFCNN利用卷积神经网络的学习能力，可以处理非平稳时间序列。

    Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
    
[^122]: 联邦学习与O-RAN的协同：面向多个分布式机器学习服务的弹性虚拟化架构

    Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])

    [http://arxiv.org/abs/2305.02109](http://arxiv.org/abs/2305.02109)

    本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。

    

    联邦学习是最流行的分布式机器学习技术，但是在现代无线网络中实现联邦学习面临着许多挑战，主要包括网络条件的动态性、系统中多个联邦学习服务/任务的并存以及联邦学习服务与其他网络服务的并行执行等。针对这些挑战，本文提出了一种名为动态多服务联邦学习（DMS-FL）的联邦学习泛型架构，并通过提出一种新的分布式机器学习架构——弹性虚拟化联邦学习（EV-FL）来解决DMS-FL中的三个未探索的设计问题。

    Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
    
[^123]: VeML：大规模高维数据的端到端机器学习生命周期

    VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v1 [cs.LG])

    [http://arxiv.org/abs/2304.13037](http://arxiv.org/abs/2304.13037)

    VeML是一种专门用于大规模高维数据的端到端机器学习生命周期的版本管理系统，在解决生命周期高成本问题、数据相似性计算和数据模式分析等关键问题方面表现出色。

    

    端到端的机器学习生命周期包含许多迭代过程，从数据准备和机器学习模型设计到模型训练，再到部署训练好的模型用于推理。当构建一个机器学习问题的端到端生命周期时，必须设计和执行许多机器学习管道，这会产生大量的生命周期版本。因此，本文介绍了VeML，一种专门用于端到端机器学习生命周期的版本管理系统。我们的系统解决了其他系统没有解决的几个关键问题。首先，我们解决了构建机器学习生命周期的高成本问题，特别是针对大规模和高维数据集。我们通过提议将在我们系统中管理的类似数据集的生命周期转移到新的训练数据来解决这个问题。我们设计了一种基于核心集的算法，可以有效地计算大规模高维数据的相似性。另一个关键问题是由于训练数据和测试数据的差异而导致模型准确性下降。我们开发了一种数据模式分析方法来检测先前使用的数据和新数据之间的差异。我们的系统使用户可以自定义机器学习生命周期工作流，并将生命周期的各个阶段与其API连接起来，作为用户运行自定义代码的桥梁。 VeML已应用于处理多个真实世界的机器学习问题，结果证明了我们的系统的有效性。

    An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data a
    
[^124]: CAR-DESPOT: 针对混杂环境下的机器人的因果关系在线POMDP规划

    CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])

    [http://arxiv.org/abs/2304.06848](http://arxiv.org/abs/2304.06848)

    本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。

    

    在现实环境中工作的机器人必须考虑随机行为的可能结果，并根据真实的世界状态的部分观察进行决策。因果混淆的问题是进行准确和强健的行为预测的主要挑战。部分可观察的马尔可夫决策过程(POMDP)是一种广泛使用的框架，用于模拟这些随机和部分可观测的决策问题。然而，由于缺乏明确的因果语义，POMDP规划方法容易受到混淆偏差的影响，在未观察到混杂变量的情况下，可能会产生表现不佳的策略。本文提出了一种新的因果关系在线POMDP规划方法，使用因果建模和推理来消除未测量混淆变量引起的错误。我们进一步提出了一种从观测数据中学习因果模型的方法，以在我们的方法中使用。实验结果表明，我们的方法CAR-DESPOT在混杂环境中比现有的最先进的POMDP规划程序表现显著更好。

    Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
    
[^125]: FedFTN: 多机构低计数PET去噪的个性化联邦学习与深度特征转换网络

    FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.00570](http://arxiv.org/abs/2304.00570)

    本研究提出了FedFTN，一种个性化的联邦学习策略，用于解决多机构低计数PET去噪中的领域差异问题。该方法通过深度特征转换网络来改善低计数PET图像质量，同时避免了集中式数据集的隐私和安全问题。

    

    低计数PET是降低辐射暴露和采集时间的有效方法，但重建图像往往受到信噪比低的影响，从而影响诊断和其他下游任务。深度学习的最新进展在改善低计数PET图像质量方面显示出巨大潜力，但由于患者数据的隐私和安全问题，从多个机构获取大型、集中和多样化的数据集以训练强大的模型是困难的。此外，不同机构的低计数PET数据可能具有不同的数据分布，因此需要个性化的模型。虽然之前的联邦学习算法可以在不需要聚合本地数据的情况下实现多机构的协作训练，但解决多机构低计数PET去噪应用中的领域差异仍然是一个挑战，并且还未完全探索。在这项工作中，我们提出了FedFTN，一种个性化的联邦学习策略。

    Low-count PET is an efficient way to reduce radiation exposure and acquisition time, but the reconstructed images often suffer from low signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream tasks. Recent advances in deep learning have shown great potential in improving low-count PET image quality, but acquiring a large, centralized, and diverse dataset from multiple institutions for training a robust model is difficult due to privacy and security concerns of patient data. Moreover, low-count PET data at different institutions may have different data distribution, thus requiring personalized models. While previous federated learning (FL) algorithms enable multi-institution collaborative training without the need of aggregating local data, addressing the large domain shift in the application of multi-institutional low-count PET denoising remains a challenge and is still highly under-explored. In this work, we propose FedFTN, a personalized federated learning strategy
    
[^126]: 基于图表示学习的高效可行的机器人装配序列规划

    Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])

    [http://arxiv.org/abs/2303.10135](http://arxiv.org/abs/2303.10135)

    本文提出了一种基于图表示学习的装配序列规划方法，通过GRACE模型可以从装配图中提取信息并预测可行的装配序列。

    

    自动机器人装配序列规划（RASP）可以显著提高现代制造业的生产力和适应力，随着对更大量化生产需求的不断增长。实现这种自动化的主要挑战之一在于从不断增加的潜在序列中高效地找到解决方案，进行越来越复杂的装配还需要成本昂贵的可行性检查。为了解决这个问题，我们提出了一种包括产品装配图的图形方法和一个名为GRACE的策略架构，用于装配序列生成。其次，我们使用GRACE从图形输入中提取有意义的信息，并逐步预测装配序列。在实验中，我们展示了我们的方法可以根据在模拟中收集的数据，预测铝型材产品变体的可行装配序列。

    Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
    
[^127]: Bernoulli线性动力系统模型的谱学习

    Spectral learning of Bernoulli linear dynamical systems models. (arXiv:2303.02060v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02060](http://arxiv.org/abs/2303.02060)

    该论文提出了一种用于快速、高效拟合概率波-伯努利潜在线性动力系统模型的谱学习方法。这种方法通过转换样本矩的方式将传统的子空间识别方法扩展到了伯努利设置中，得到了一个鲁棒的固定成本估计器。在数据有限的情况下，谱估计可以为Laplace-EM拟合提供良好的初始化。

    

    具有Bernoulli观测的潜在线性动力系统为识别二进制时间序列数据的时间动态提供了强大的建模框架，这些数据在二进制决策和离散随机过程（例如离散神经尖峰训练）等各种情况下产生。在这里，我们开发了一种快速有效的概率波/Bernoulli潜在线性动态系统（LDS）模型的谱学习方法。我们的方法通过对第一和第二个样本矩的转换将传统的子空间识别方法扩展到Bernoulli设置中。这导致了一个健壮的固定成本估计器，避免了局部最优解的危险以及期望最大化（EM）算法等迭代拟合过程的长时间计算。在数据有限或数据的统计结构不满足假设的情况下，我们证明了谱估计为Laplace-EM拟合提供了良好的初始化。

    Latent linear dynamical systems with Bernoulli observations provide a powerful modeling framework for identifying the temporal dynamics underlying binary time series data, which arise in a variety of contexts such as binary decision-making and discrete stochastic processes (e.g., binned neural spike trains). Here we develop a spectral learning method for fast, efficient fitting of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach extends traditional subspace identification methods to the Bernoulli setting via a transformation of the first and second sample moments. This results in a robust, fixed-cost estimator that avoids the hazards of local optima and the long computation time of iterative fitting procedures like the expectation-maximization (EM) algorithm. In regimes where data is limited or assumptions about the statistical structure of the data are not met, we demonstrate that the spectral estimate provides a good initialization for Laplace-EM fitting. Fi
    
[^128]: 学习常见理由以改进细粒度视觉识别问题的自监督表示

    Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems. (arXiv:2303.01669v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.01669](http://arxiv.org/abs/2303.01669)

    本研究提出了一种学习常见理由的方法来改善细粒度视觉识别问题中的自监督表示。通过识别常见的区分性线索，这种方法能够有效克服自监督学习在细粒度视觉识别中的局限性。

    

    自监督学习（SSL）策略在各种识别任务中展示出了卓越的性能。然而，我们的初步调查和最近的研究表明，它们在学习细粒度视觉识别（FGVR）的表示时可能不太有效，因为许多有助于优化SSL目标的特征不适合描述细粒度视觉识别中的细微差异。为了解决这个问题，我们提出了学习一个额外的筛选机制，以识别跨实例和类别中常见的具有区分性的线索，本文称之为常见理由。从直觉上讲，常见理由往往对应于前景物体的关键部位的区分模式。我们展示了一个常见理由检测器可以通过简单地利用从SSL目标导出的GradCAM来学习，而无需使用任何预训练的物体部位或显著性检测器，从而使其无缝集成到现有的SSL过程中。

    Self-supervised learning (SSL) strategies have demonstrated remarkable performance in various recognition tasks. However, both our preliminary investigation and recent studies suggest that they may be less effective in learning representations for fine-grained visual recognition (FGVR) since many features helpful for optimizing SSL objectives are not suitable for characterizing the subtle differences in FGVR. To overcome this issue, we propose learning an additional screening mechanism to identify discriminative clues commonly seen across instances and classes, dubbed as common rationales in this paper. Intuitively, common rationales tend to correspond to the discriminative patterns from the key parts of foreground objects. We show that a common rationale detector can be learned by simply exploiting the GradCAM induced from the SSL objective without using any pre-trained object parts or saliency detectors, making it seamlessly to be integrated with the existing SSL process. Specificall
    
[^129]: 使用不同降维和分类技术的癫痫发作检测的经验分析

    Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12012](http://arxiv.org/abs/2302.12012)

    本研究对使用不同降维和分类技术进行癫痫发作检测进行了实证分析，通过离散小波变换和机器学习分类器，结合主成分分析、独立成分分析和线性判别分析等降维算法，选择特征来提高检测准确性。

    

    脑电图（EEG）是一种非侵入性检查，记录大脑的电活动。该检查用于帮助诊断各种脑问题。通过离散小波变换（DWT）和机器学习分类器，可以进行癫痫检测。在癫痫发作检测中，主要使用机器学习分类器和统计特征。EEG信号中的隐藏信息对于检测影响大脑的疾病很有用。有时，在时间和频率域内识别EEG的最小变化是非常困难的。DWT可以在不同频带进行信号良好的分解和特征提取。我们使用三个降维算法：主成分分析（PCA）、独立成分分析（ICA）和线性判别分析（LDA）。最后，通过融合规则选择特征。

    An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
    
[^130]: 近地表风的算法幻觉：使用生成对抗网络进行统计降尺度的研究

    Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2302.08720](http://arxiv.org/abs/2302.08720)

    本论文将新兴的图像超分辨率技术应用于统计降尺度任务，具体探索了基于生成对抗网络的算法在模拟北美地区地表风中的应用。通过使用非理想化的低分辨率和高分辨率输入数据，该方法克服了共享尺度不匹配的问题，并通过评估空间功率谱等指标来评估模型的技能。

    

    本论文探讨了将图像超分辨率（SR）中新兴的机器学习方法应用于统计降尺度任务。我们特别关注卷积神经网络的生成对抗网络（GANs）。我们的GANs是通过对低分辨率（LR）输入进行条件训练来生成模拟北美地区 Weather Research and Forecasting（WRF）模型的高分辨率（HR）地表风。与传统的SR模型不同，LR输入在WRF模拟中使用了非理想化的LR和HR配对，导致由于内部变异引起的共享尺度不匹配。我们的研究基于当前基于SR的统计降尺度，并尝试了计算机视觉领域的新颖频率分离（FS）方法。为了评估SR模型的技能，我们精选评估指标，并关注基于空间功率谱的性能度量。我们的分析揭示了GAN配置如何影响模型的性能。

    This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
    
[^131]: 因子场：神经场和更多领域的统一框架

    Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01226](http://arxiv.org/abs/2302.01226)

    因子场是一种统一的框架，将信号分解为因子的乘积，通过神经场或常规场表示进行处理。这个框架可以概括多种最近的信号表示方法，并且可以创建出更强大的新信号表示，如本文提出的系数基函数分解（CoBaFa）。实验结果表明，CoBaFa在逼近质量、紧凑性和效率方面相较于之前的方法有所改进。

    

    我们提出了因子场，一种用于建模和表示信号的新框架。因子场将信号分解为因子的乘积，每个因子由神经场或常规场表示，并对输入信号进行坐标变换操作。我们展示了这种分解产生了一个统一的框架，可以概括多种最近的信号表示方法，包括NeRF，PlenOxels，EG3D，Instant-NGP和TensoRF。此外，这个框架还可以创建强大的新信号表示，例如本文提出的系数基函数分解（CoBaFa）。通过实验，我们证明CoBaFa相较于之前的快速重建方法在神经信号表示的三个关键目标：逼近质量，紧凑性和效率方面有所改进。在二维图像回归任务中，我们实验证明我们的表示可以获得更好的图像逼近质量，同时在几何质量方面也更高。

    We present Factor Fields, a novel framework for modeling and representing signals. Factor Fields decomposes a signal into a product of factors, each of which is represented by a neural or regular field representation operating on a coordinate transformed input signal. We show that this decomposition yields a unified framework that generalizes several recent signal representations including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the framework allows for the creation of powerful new signal representations, such as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper. As evidenced by our experiments, CoBaFa leads to improvements over previous fast reconstruction methods in terms of the three critical goals in neural signal representation: approximation quality, compactness and efficiency. Experimentally, we demonstrate that our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when r
    
[^132]: 因果提升与链路预测

    Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01198](http://arxiv.org/abs/2302.01198)

    本文开发了第一个能够处理链路预测中路径依赖的因果模型，并介绍了因果提升的概念，通过有限的干预数据识别因果链路预测查询。

    

    现有的链路预测因果模型假设存在一组固有的节点因子，即在节点出生时就定义的固有特征，它们控制着图中链路的因果演化。然而，在某些因果任务中，链路形成是路径依赖的：链路干预的结果取决于现有的链路。不幸的是，这些现有的因果方法并不适用于路径依赖的链路形成，因为链路之间的级联功能依赖（由路径依赖性产生）要么无法识别，要么需要大量不切实际的控制变量。为了克服这个问题，我们开发了第一个能够处理链路预测中路径依赖的因果模型。在这项工作中，我们引入了因果提升的概念，这是一种独立于图的因果模型的不变性，可以利用有限的干预数据来识别因果链路预测查询。此外，我们展示了结构对两个节点之间嵌入的低维表示的方式。

    Existing causal models for link prediction assume an underlying set of inherent node factors -- an innate characteristic defined at the node's birth -- that governs the causal evolution of links in the graph. In some causal tasks, however, link formation is path-dependent: The outcome of link interventions depends on existing links. Unfortunately, these existing causal methods are not designed for path-dependent link formation, as the cascading functional dependencies between links (arising from path dependence) are either unidentifiable or require an impractical number of control variables. To overcome this, we develop the first causal model capable of dealing with path dependencies in link prediction. In this work we introduce the concept of causal lifting, an invariance in causal models of independent interest that, on graphs, allows the identification of causal link prediction queries using limited interventional data. Further, we show how structural pairwise embeddings exhibit low
    
[^133]: 在约束规划求解器内学习通用的值选择启发式

    Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.01913](http://arxiv.org/abs/2301.01913)

    本论文提出了一种通用学习过程，用于在约束规划求解器内获取一个值选择启发式方法，以解决当前通用值选择启发式方法较为稀缺的问题。

    

    约束规划被认为是解决组合问题的一种高效方法。求解器中的重要设计选择是分支启发式，它们旨在在最短的时间内寻找最佳解决方案。然而，开发这些启发式需要耗费大量时间，并需要问题特定的专业知识。这一观察结果激发了许多使用机器学习自动学习高效启发式的努力，而无需专家干预。据我们所知，这仍然是一个开放的研究问题。尽管文献中有几种通用的变量选择启发式方法，但对于通用的值选择启发式方法的选择却较少。在本文中，我们提出通过引入一种通用的学习过程来解决这个问题，该过程可以用于在约束规划求解器内获得一个值选择启发式方法。这得益于深度Q学习算法和一个...

    Constraint programming is known for being an efficient approach for solving combinatorial problems. Important design choices in a solver are the branching heuristics, which are designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. To the best of our knowledge, it is still an open research question. Although several generic variable-selection heuristics are available in the literature, the options for a generic value-selection heuristic are more scarce. In this paper, we propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a t
    
[^134]: MixupE：从方向导数角度理解和改进Mixup技术

    MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13381](http://arxiv.org/abs/2212.13381)

    本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。

    

    Mixup是一种深度神经网络中流行的数据增强技术，通过线性插值输入和它们的标签生成额外的样本。该技术已被证实在许多学习范式和应用中提高了泛化性能。本文首先对Mixup进行分析，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一新的洞见，我们提出了一种改进版本的Mixup，理论上证明它可以比原始版本具有更好的泛化性能。为了证明这种方法的有效性，我们在各种领域进行了实验，例如图像、表格数据、语音和图形。我们的结果表明，所提出的方法改进了Mixup在多个数据集上的表现，在使用各种架构时都表现出比Mixup更好的性能，例如在ImageNet的top-1精度上比Mixup提高了0.8%。

    Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.
    
[^135]: 可扩展的贝叶斯不确定性量化方法用于神经网络势的研究：机遇与挑战

    Scalable Bayesian Uncertainty Quantification for Neural Network Potentials: Promise and Pitfalls. (arXiv:2212.07959v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2212.07959](http://arxiv.org/abs/2212.07959)

    神经网络势的贝叶斯不确定性量化方法通过随机梯度MCMC实现可扩展的UQ，可准确估计MD可观测量，要求更少的训练数据和多个马尔科夫链。

    

    神经网络（NN）势方法在计算复杂度可达到经典分子动力学力场的情况下，具有高精度的分子动力学（MD）模拟的潜力。然而，当应用于其训练领域之外时，NN势的预测可能不准确，因此增加了对不确定性量化（UQ）的需求。贝叶斯建模提供了UQ的数学框架，但基于马尔科夫链蒙特卡罗（MCMC）的传统贝叶斯方法在NN势中计算复杂性上存在问题。通过对粗粒化的液态水和丙氨酸二肽系统训练图NN势，我们在这里展示了通过随机梯度MCMC（SG-MCMC）实现可扩展贝叶斯UQ的可靠性估计MD可观测量。我们表明冷后验分布可以减少所需的训练数据大小，而为了可靠的UQ，需要多个马尔科夫链。此外，我们发现SG-MCMC和深度集合方法在结果上具有可比性，尽管训练时间较短且没有过度宣传。

    Neural network (NN) potentials promise highly accurate molecular dynamics (MD) simulations within the computational complexity of classical MD force fields. However, when applied outside their training domain, NN potential predictions can be inaccurate, increasing the need for Uncertainty Quantification (UQ). Bayesian modeling provides the mathematical framework for UQ, but classical Bayesian methods based on Markov chain Monte Carlo (MCMC) are computationally intractable for NN potentials. By training graph NN potentials for coarse-grained systems of liquid water and alanine dipeptide, we demonstrate here that scalable Bayesian UQ via stochastic gradient MCMC (SG-MCMC) yields reliable uncertainty estimates for MD observables. We show that cold posteriors can reduce the required training data size and that for reliable UQ, multiple Markov chains are needed. Additionally, we find that SG-MCMC and the Deep Ensemble method achieve comparable results, despite shorter training and less hype
    
[^136]: 基于时间混合的时序对比域自适应

    Contrastive Domain Adaptation for Time-Series via Temporal Mixup. (arXiv:2212.01555v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01555](http://arxiv.org/abs/2212.01555)

    本文提出了一种基于时间混合的对比域自适应方法，用于解决时序数据中的领域偏移问题。

    

    无监督域自适应（UDA）已经成为解决域偏移问题的强大解决方案，通过将有标签的源领域的知识转移到有转移的无标签的目标领域。尽管UDA在视觉应用中普遍存在，但在时序应用中相对较少探索。在这项工作中，我们提出了一种新颖且轻量级的对比域自适应框架CoTMix，用于时序数据。与现有方法不同，该方法仅利用对比学习来减轻不同域之间的分布偏移。具体来说，我们提出了一种新颖的时间混合策略，为源领域和目标领域生成两个中间增强视图。随后，我们利用对比学习来最大化每个领域与其相应增强视图之间的相似性。生成的视图在适应过程中考虑了时序数据的时间动态性。

    Unsupervised Domain Adaptation (UDA) has emerged as a powerful solution for the domain shift problem via transferring the knowledge from a labeled source domain to a shifted unlabeled target domain. Despite the prevalence of UDA for visual applications, it remains relatively less explored for time-series applications. In this work, we propose a novel lightweight contrastive domain adaptation framework called CoTMix for time-series data. Unlike existing approaches that either use statistical distances or adversarial techniques, we leverage contrastive learning solely to mitigate the distribution shift across the different domains. Specifically, we propose a novel temporal mixup strategy to generate two intermediate augmented views for the source and target domains. Subsequently, we leverage contrastive learning to maximize the similarity between each domain and its corresponding augmented view. The generated views consider the temporal dynamics of time-series data during the adaptation 
    
[^137]: 随机估计器优于确定性估计器的原因：鲁棒性，一致性和感知质量

    Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality. (arXiv:2211.08944v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.08944](http://arxiv.org/abs/2211.08944)

    本文揭示了随机恢复算法相对于确定性恢复算法的优势，包括可以达到完美感知质量、对抗性攻击的鲁棒性和提高输出可变性。这些发现将进一步支持随机估计器的使用。

    

    随机恢复算法允许探索与退化输入对应的解空间。本文揭示了随机方法相对于确定性方法的额外基本优势，并进一步激发了它们的使用动机。首先，我们证明了任何能达到完美感知质量且其输出与输入一致的恢复算法必须是一个后验采样器，因此必须是随机的。其次，我们说明了确定性恢复算法可能达到较高的感知质量，但只能通过使用极其敏感的映射填充所有可能的源图像空间来实现，这使它们非常容易受到对抗性攻击的影响。实际上，我们表明，要求确定性模型对这些攻击具有鲁棒性会严重阻碍其感知质量，而使随机模型具有鲁棒性几乎不会影响其感知质量，并改善其输出的可变性。这个发现将进一步支持随机估计器的使用。

    Stochastic restoration algorithms allow to explore the space of solutions that correspond to the degraded input. In this paper we reveal additional fundamental advantages of stochastic methods over deterministic ones, which further motivate their use. First, we prove that any restoration algorithm that attains perfect perceptual quality and whose outputs are consistent with the input must be a posterior sampler, and is thus required to be stochastic. Second, we illustrate that while deterministic restoration algorithms may attain high perceptual quality, this can be achieved only by filling up the space of all possible source images using an extremely sensitive mapping, which makes them highly vulnerable to adversarial attacks. Indeed, we show that enforcing deterministic models to be robust to such attacks profoundly hinders their perceptual quality, while robustifying stochastic models hardly influences their perceptual quality, and improves their output variability. These findings p
    
[^138]: 大型语言模型在学习长尾知识方面存在困难

    Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08411](http://arxiv.org/abs/2211.08411)

    本文研究了大型语言模型记忆的知识与预训练数据集中信息之间的关系，并发现其回答基于事实的问题的能力与在预训练过程中接触到的相关文档数量之间存在强相关性和因果关系。

    

    互联网中包含着丰富的知识，从历史人物的生日到编程教程等，所有这些都可以由语言模型学习。然而，尽管某些信息在网上无处不在，但其他信息的出现非常罕见。在本文中，我们研究了大型语言模型记忆的知识与从网络抓取的预训练数据集中的信息之间的关系。特别是，我们展示了语言模型回答基于事实的问题的能力与在预训练过程中看到与此问题相关的文档数量之间的关系。我们通过实体链接预训练数据集并计算包含与给定问题-答案对相同实体的文档数量来识别这些相关文档。我们的结果表明，在众多问答数据集（例如 TriviaQA）、预训练语料库（例如 ROOTS）和模型上，准确性与相关文档数量之间存在着强相关性和因果关系。

    The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model si
    
[^139]: 通过图神经网络预测平价游戏中的获胜区域 (扩展摘要)

    Predicting Winning Regions in Parity Games via Graph Neural Networks (Extended Abstract). (arXiv:2210.09924v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2210.09924](http://arxiv.org/abs/2210.09924)

    通过图神经网络，我们提出了一种不完全多项式时间复杂度的方法来确定平价游戏的获胜区域，并且在900个随机生成的游戏中表现出了较高的有效性和效率。

    

    解决平价游戏是反应式程序验证与合成中众多应用的重要基础。虽然它们可以在实践中高效地解决，但目前没有已知方法具有多项式的最坏情况运行时间复杂度。我们提出了一种通过图神经网络确定平价游戏的获胜区域的不完全多项式时间方法。我们在900个随机生成的平价游戏上进行评估，结果表明该方法在实践中是有效和高效的。它正确确定了数据集中约60%的游戏的获胜区域，并且在其余游戏中只产生了轻微的错误。我们相信这种方法还可以扩展以高效地解决平价游戏。

    Solving parity games is a major building block for numerous applications in reactive program verification and synthesis. While they can be solved efficiently in practice, no known approach has a polynomial worst-case runtime complexity. We present a incomplete polynomial-time approach to determining the winning regions of parity games via graph neural networks.  Our evaluation on 900 randomly generated parity games shows that this approach is effective and efficient in practice. It correctly determines the winning regions of $\sim$60\% of the games in our data set and only incurs minor errors in the remaining ones. We believe that this approach can be extended to efficiently solve parity games as well.
    
[^140]: 通过核密度估计学习传递算子

    Learning Transfer Operators by Kernel Density Estimation. (arXiv:2210.03124v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03124](http://arxiv.org/abs/2210.03124)

    本研究通过核密度估计方法，重新解释了传递算子的推断问题，并通过实例表明了该方法在估计Frobenius-Perron算子的特征向量上的有效性和有效性。

    

    从数据中推断传递算子经常被看作是一个基于乌拉姆方法的经典问题。传统描述中的乌拉姆-加勒金方法涉及到将基函数投影到以细网格的矩形支持的特征函数上。从这个角度来看，乌拉姆-加勒金方法可以被解释为使用直方图方法进行密度估计。在本研究中，我们把这个问题重新放到了统计密度估计的框架中。这种替代的观点允许对偏差和方差进行明确和严格的分析，从而促进对均方误差的讨论。通过使用逻辑映射和马尔可夫映射的综合示例，我们证明了该方法在估计Frobenius-Perron算子的特征向量上的有效性和有效性。我们比较了直方图密度估计（HDE）和核密度估计（KDE）方法的性能，并发现KDE方法在该问题上表现更好。

    Inference of transfer operators from data is often formulated as a classical problem that hinges on the Ulam method. The conventional description, known as the Ulam-Galerkin method, involves projecting onto basis functions represented as characteristic functions supported over a fine grid of rectangles. From this perspective, the Ulam-Galerkin approach can be interpreted as density estimation using the histogram method. In this study, we recast the problem within the framework of statistical density estimation. This alternative perspective allows for an explicit and rigorous analysis of bias and variance, thereby facilitating a discussion on the mean square error. Through comprehensive examples utilizing the logistic map and a Markov map, we demonstrate the validity and effectiveness of this approach in estimating the eigenvectors of the Frobenius-Perron operator. We compare the performance of Histogram Density Estimation(HDE) and Kernel Density Estimation(KDE) methods and find that KD
    
[^141]: 非光滑非凸非凹极小极大优化的全局收敛率分析

    Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization. (arXiv:2209.10825v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.10825](http://arxiv.org/abs/2209.10825)

    本论文提出了一种名为smoothed PLDA的算法来有效处理广泛的结构化非光滑非凸非凹极小极大问题，并证明了其具有全局收敛性，复杂度为O(epsilon^(-2/3))。

    

    在过去的十年中，非凸非凹极小极大优化引起了广泛关注。然而，大多数现有的工作集中在梯度下降-上升（GDA）算法的各种变体上，这些算法仅适用于平滑的非凸凹场景。为了解决这个局限性，我们提出了一种新算法，名为平滑的近端线性下降上升（smoothed PLDA），可以有效地处理广泛的结构化非光滑非凸非凹极小极大问题。具体而言，我们考虑原始函数具有非光滑复合结构，对偶函数具有Kurdyka-L{o}jasiewicz（K\L{}）性质的情况。我们引入了一种新的收敛分析框架来分析smoothed PLDA算法，其中关键组件是我们最新开发的非光滑原始误差界和对偶误差界属性。利用这个框架，我们证明了smoothed PLDA可以在具有非光滑复合原始函数和KL对偶函数的广泛极小极大问题中找到$\varepsilon$-game-stationary点和$\varepsilon$-最优化稳定点，其复杂度为$\mathcal{O}(\varepsilon^{-2/3})$。

    Nonconvex-nonconcave minimax optimization has gained widespread interest over the last decade. However, most existing work focuses on variants of gradient descent-ascent (GDA) algorithms, which are only applicable in smooth nonconvex-concave settings. To address this limitation, we propose a novel algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which can effectively handle a broad range of structured nonsmooth nonconvex-nonconcave minimax problems. Specifically, we consider the setting where the primal function has a nonsmooth composite structure and the dual function possesses the Kurdyka-\L{}ojasiewicz (K\L{}) property with exponent $\theta \in [0,1)$. We introduce a novel convergence analysis framework for smoothed PLDA, the key components of which are our newly developed nonsmooth primal error bound and dual error bound properties. Using this framework, we show that smoothed PLDA can find both $\epsilon$-game-stationary points and $\epsilon$-optimization-st
    
[^142]: 人工神经网络的统计过程监控

    Statistical process monitoring of artificial neural networks. (arXiv:2209.07436v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2209.07436](http://arxiv.org/abs/2209.07436)

    这篇论文提出了一种基于人工神经网络生成的数据潜在特征表示的监控方法，以确定数据流开始变得非平稳的时间。该方法通过应用基于数据深度计算和归一化排名的多元控制图进行监测，并与各种基准方法进行了比较。

    

    基于人工智能的模型的快速发展要求创新的监控技术，这些技术可以以低计算成本实时操作。在机器学习中，特别是考虑到人工神经网络（ANNs），模型通常是以监督方式进行训练的。因此，在模型部署期间，输入和输出之间学习到的关系必须保持有效。如果这个平稳性假设成立，我们可以得出结论，ANN提供准确的预测。否则，需要重新训练或重建模型。我们建议考虑由ANN生成的数据的潜在特征表示（称为“嵌入”），以确定数据流开始变得非平稳的时间。具体而言，我们通过应用基于数据深度计算和归一化排名的多元控制图来监测嵌入。我们将引入的方法与各种ANN基准方法进行了比较。

    The rapid advancement of models based on artificial intelligence demands innovative monitoring techniques which can operate in real time with low computational costs. In machine learning, especially if we consider artificial neural networks (ANNs), the models are often trained in a supervised manner. Consequently, the learned relationship between the input and the output must remain valid during the model's deployment. If this stationarity assumption holds, we can conclude that the ANN provides accurate predictions. Otherwise, the retraining or rebuilding of the model is required. We propose considering the latent feature representation of the data (called "embedding") generated by the ANN to determine the time when the data stream starts being nonstationary. In particular, we monitor embeddings by applying multivariate control charts based on the data depth calculation and normalized ranks. The performance of the introduced method is compared with benchmark approaches for various ANN 
    
[^143]: 多模块图神经网络的灵活表征促进更好的泛化能力

    Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06589](http://arxiv.org/abs/2209.06589)

    本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    

    图神经网络（GNN）已成为处理图结构数据的学习与推断的强大模型，但对于扩展到更大的图以及推广到从未见过的数据的基本限制的了解还不足。本文使用随机图生成器系统地研究了图的大小和结构属性如何影响GNN的预测性能，并提出多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
    
[^144]: 使用隐马尔可夫模型学习强化学习任务自动机

    Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11838](http://arxiv.org/abs/2208.11838)

    本文提出了一种学习非马尔可夫任务规范的新方法，通过从代理经验中学习，将其表示为有限状态的任务自动机。利用隐马尔可夫模型和新的提炼方法，使得代理能够解决稀疏和非马尔可夫奖励的强化学习任务。

    

    在环境具有稀疏和非马尔可夫奖励的情况下，使用标量奖励信号训练强化学习代理通常是不可行的。此外，在训练之前手工创建这些奖励函数往往容易出错，特别是当环境的动力学只有部分已知时。本文提出了一种新颖的流程，通过对未知环境中代理经验的 episodes 进行学习，从而学习非马尔可夫任务规范的简洁有限状态“任务自动机”。我们利用了两个关键算法的见解。首先，我们通过将产品 MDP 视为部分可观测 MDP 并使用众所周知的 Baum-Welch 算法来学习隐马尔可夫模型，从而学习到了规范自动机和环境的 MDP（初始都未知）所组成的模型。其次，我们提出了一种新颖的方法，从学到的产品 MDP 中提炼任务自动机（假设为确定性有限自动机）。我们学到的任务自动机使得代理可以解决非马尔可夫任务规范。

    Training reinforcement learning (RL) agents using scalar reward signals is often infeasible when an environment has sparse and non-Markovian rewards. Moreover, handcrafting these reward functions before training is prone to misspecification, especially when the environment's dynamics are only partially known. This paper proposes a novel pipeline for learning non-Markovian task specifications as succinct finite-state `task automata' from episodes of agent experience within unknown environments. We leverage two key algorithmic insights. First, we learn a product MDP, a model composed of the specification's automaton and the environment's MDP (both initially unknown), by treating the product MDP as a partially observable MDP and using the well-known Baum-Welch algorithm for learning hidden Markov models. Second, we propose a novel method for distilling the task automaton (assumed to be a deterministic finite automaton) from the learnt product MDP. Our learnt task automaton enables the dec
    
[^145]: 论连词查询的非高效PAC可学习性

    On the non-efficient PAC learnability of conjunctive queries. (arXiv:2208.10255v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2208.10255](http://arxiv.org/abs/2208.10255)

    这篇论文阐述了连词查询在PAC模型中的非高效可学习性，针对不同变种的连词查询提出了负面可学习性结果，并展示了通过成员查询可以高效学习连词查询和UCQs。

    

    这篇论文有三个目的：(i)我们提供了一个自包含的阐述，证明了连词查询在可能-近似正确(PAC)模型中没有高效可学习性，清楚地注意到这个概念类在很多计算学习理论文献中暗含的多项式规模适应性缺失；(ii)我们建立了一个强大的负面PAC可学习性结果，适用于许多限制类别的连词查询(CQs)，包括广义的“无环性”；(iii)我们展示了连词查询和UCQs通过成员查询是可以高效PAC可学习的。

    This note serves three purposes: (i) we provide a self-contained exposition of the fact that conjunctive queries are not efficiently learnable in the Probably-Approximately-Correct (PAC) model, paying clear attention to the complicating fact that this concept class lacks the polynomial-size fitting property, a property that is tacitly assumed in much of the computational learning theory literature; (ii) we establish a strong negative PAC learnability result that applies to many restricted classes of conjunctive queries (CQs), including acyclic CQs for a wide range of notions of "acyclicity"; (iii) we show that CQs (and UCQs) are efficiently PAC learnable with membership queries.
    
[^146]: 标量输入和函数输出的神经网络

    Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.05776](http://arxiv.org/abs/2208.05776)

    该论文提出了一种解决标量输入和函数输出之间回归问题的方法，使用前馈神经网络预测函数响应。该方法适用于大量预测变量或非线性关系，并可以控制预测曲线的平滑程度。在实验中验证了方法的有效性。

    

    在一组标量预测变量上回归函数响应可以是一项具有挑战性的任务，特别是当有大量预测变量或者预测变量与响应之间的关系是非线性的时候。在这项工作中，我们提出了一个解决方案：使用前馈神经网络（NN）预测标量输入下的函数响应。首先，我们将函数响应转化为有限维度表示，并构建一个输出该表示的神经网络。然后，我们提出通过目标函数修改神经网络的输出，并引入不同的目标函数来进行网络训练。所提出的模型适用于均匀和不均匀间隔的数据，并可以进一步应用平滑惩罚项来控制预测曲线的平滑程度。实现这些特性的困难在于定义可以进行反向传播的目标函数。在我们的实验中，我们展示了我们的方法在多个数据集上的有效性。

    The regression of a functional response on a set of scalar predictors can be a challenging task, especially if there is a large number of predictors, or the relationship between those predictors and the response is nonlinear. In this work, we propose a solution to this problem: a feed-forward neural network (NN) designed to predict a functional response using scalar inputs. First, we transform the functional response to a finite-dimensional representation and construct an NN that outputs this representation. Then, we propose to modify the output of an NN via the objective function and introduce different objective functions for network training. The proposed models are suited for both regularly and irregularly spaced data, and a roughness penalty can be further applied to control the smoothness of the predicted curve. The difficulty in implementing both those features lies in the definition of objective functions that can be back-propagated. In our experiments, we demonstrate that our 
    
[^147]: 视觉预训练用于导航：从噪声中我们能学到什么？

    Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.00052](http://arxiv.org/abs/2207.00052)

    本论文提出了一种使用随机裁剪预测进行自监督训练的视觉预训练方法，可以学习到对导航任务有用的表示，并通过自举学习有效地学习导航策略，减少对交互数据的需求。

    

    在视觉导航中，一种强大的范式是从观察中直接预测行为。训练这样一个端到端的系统可以自动产生对下游任务有用的表示。然而，缺乏归纳偏差使得该系统数据效率低下。我们假设通过预测与目标对应的当前视图裁剪的位置和大小，可以学习到导航策略所需的当前视图和目标视图的充分表示。我们进一步展示，在自监督的方式下，仅使用合成噪声图像进行随机裁剪预测的训练可以很好地迁移到自然家庭图像。然后，可以利用学到的表示有效地自举学习导航策略，减少交互数据的需求。

    One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz
    
[^148]: 从随机已知日志中恢复轨迹

    Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12672](http://arxiv.org/abs/2206.12672)

    本文提出了一种从随机已知日志中恢复轨迹的算法，在两个公开数据集上平均恢复准确度达到90-97%。这一方法通过计算流程模型和随机已知轨迹的合规性，并恢复在该随机轨迹中的最佳对齐作为真实轨迹。对比其他轨迹恢复选项，使用了产品多图来分析成本模型对恢复准确性的影响。这一算法对于预测模型开发、错误排查和系统性能改进具有重要意义。

    

    在这项工作中，我们提出了一种用于从随机已知日志中恢复轨迹的算法。随着传感器数量的增加和生成不确定数据的预测模型的增加，这种设置越来越常见。所提出的方法计算流程模型与随机已知轨迹之间的合规性，并在这个随机轨迹中恢复最佳对齐作为真实轨迹。论文对不同成本模型对轨迹恢复准确性的影响进行了分析，并利用产品多图来比较替代轨迹恢复选项。通过使用两个公开可用的数据集进行评估，我们的方法的平均准确性令人印象深刻，平均恢复准确度达到90-97%，显著改善了常见的启发式算法，该算法选择每个不确定活动的最可能值。我们相信，所提出的算法在从随机已知日志中恢复正确轨迹方面的有效性可能是开发预测模型、错误排查和改进系统性能的有力帮助。

    In this work we propose an algorithm for trace recovery from stochastically known logs, a setting that is becoming more common with the increasing number of sensors and predictive models that generate uncertain data. The suggested approach calculates the conformance between a process model and a stochastically known trace and recovers the best alignment within this stochastic trace as the true trace. The paper offers an analysis of the impact of various cost models on trace recovery accuracy and makes use of a product multi-graph to compare alternative trace recovery options. The average accuracy of our approach, evaluated using two publicly available datasets, is impressive, with an average recovery accuracy score of 90-97%, significantly improving a common heuristic that chooses the most likely value for each uncertain activity. We believe that the effectiveness of the proposed algorithm in recovering correct traces from stochastically known logs may be a powerful aid for developing 
    
[^149]: 分析通过预测函数的Lipschitz率来评估解释器的稳健性

    Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12481](http://arxiv.org/abs/2206.12481)

    本文研究了通过评估预测函数的Lipschitz率来分析解释器的稳健性。通过引入解释器敏锐性的概念并与预测器的概率Lipschitz率相联系，我们提供了解释器敏锐性的下界保证。

    

    机器学习方法在预测能力方面得到了显著提高，但与此同时，它们变得越来越复杂和不透明。因此，解释器总是被依赖于为这些黑盒预测模型提供可解释性。作为关键的诊断工具，重要的是这些解释器本身是稳健的。本文重点关注稳健性的一个特定方面，即解释器在相似的数据输入上应该给出类似的解释。我们通过引入和定义解释器敏锐性的概念来形式化这个观念，类似于预测函数的敏锐性。我们的形式化方法使我们能够将解释器的稳健性与预测器的概率Lipschitz率相联系，该率捕捉了函数局部平滑性的概率。我们根据预测函数的Lipschitz率提供对各种解释器（如SHAP，RISE，CXPlain）的敏锐性的下界保证。这些理论结果暗示了对于具有局部光滑性预测函数的解释器敏锐性。

    Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that lo
    
[^150]: 通过草图技术实现算法高斯化：将数据转换为次高斯随机设计

    Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs. (arXiv:2206.10291v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10291](http://arxiv.org/abs/2206.10291)

    该论文通过引入杠杆分数稀疏（LESS）嵌入的草图技术，提供了一种算法框架，实现了对数据分布的高斯化，从而能够高效地构建与次高斯随机设计几乎无法区分的数据草图。

    

    算法高斯化是使用随机草图方法或采样方法生成大型数据集的较小表示时可能出现的现象：对于某些任务，观察到的这些草图表示具有许多稳健性能特征，这些特征在数据样本来自次高斯随机设计时已被确认存在，而次高斯随机设计是数据分布的一个强大统计模型。然而，这种现象仅在特定任务和度量标准上进行了研究，或者依赖于计算昂贵的方法。我们通过提供一种通过平均来高斯化数据分布的算法框架来解决这个问题，并证明可以高效地构建与次高斯随机设计在总变异距离上几乎无法区分的数据草图。特别地，依赖于最近引入的一种称为杠杆分数稀疏（LESS）嵌入的草图技术，我们展示了可以构建一个n逼真的高斯化数据草图。

    Algorithmic Gaussianization is a phenomenon that can arise when using randomized sketching or sampling methods to produce smaller representations of large datasets: For certain tasks, these sketched representations have been observed to exhibit many robust performance characteristics that are known to occur when a data sample comes from a sub-gaussian random design, which is a powerful statistical model of data distributions. However, this phenomenon has only been studied for specific tasks and metrics, or by relying on computationally expensive methods. We address this by providing an algorithmic framework for gaussianizing data distributions via averaging, proving that it is possible to efficiently construct data sketches that are nearly indistinguishable (in terms of total variation distance) from sub-gaussian random designs. In particular, relying on a recently introduced sketching technique called Leverage Score Sparsified (LESS) embeddings, we show that one can construct an $n\ti
    
[^151]: 将知识从记忆中解耦：检索增强的提示学习

    Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14704](http://arxiv.org/abs/2205.14704)

    本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。

    

    提示学习方法在自然语言处理领域取得了显著的突破，提高了少样本学习的性能，但仍然遵循参数化学习范式；在学习过程中，遗忘和机械记忆问题可能导致不稳定的泛化问题。为了缓解这些限制，我们开发了RetroPrompt，旨在从记忆中将知识解耦，帮助模型在泛化和记忆之间取得平衡。与传统的提示学习方法相比，RetroPrompt从训练实例构建了一个开放式知识库，并在输入、训练和推断过程中实施检索机制，使模型具备了从训练语料库中检索相关上下文用于增强的能力。大量实验证明了RetroPrompt的效果。

    Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
    
[^152]: 量子纠缠的不完全测量深度学习

    Deep learning of quantum entanglement from incomplete measurements. (arXiv:2205.01462v6 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2205.01462](http://arxiv.org/abs/2205.01462)

    该论文使用深度学习的方法，通过神经网络量化了量子系统中的纠缠程度，无需完全描述量子态，仅使用不完全的局部测量集即可实现比传统方法更低一个量级的量化误差，且仅使用模拟数据进行训练。

    

    对于基础研究和许多前沿应用来说，量子系统中存在的纠缠程度的量化是非常重要的。目前，实现这一目标要么需要对系统具有先验知识，要么需要非常苛刻的实验过程，例如全态测量或集体测量。在这里，我们展示了通过使用神经网络，我们可以在不需要知道量子态的完全描述的情况下量化纠缠程度。我们的方法允许使用不完全的局部测量集对量子相关性进行直接量化。尽管使用了欠采样测量，我们实现了一个比最先进的量子态测量低一个量级的量化误差。此外，我们通过仅使用模拟数据训练的网络实现了这个结果。最后，我们推导出了基于卷积网络输入的方法，该方法可以接受来自各种测量场景的数据。

    The quantification of the entanglement present in a physical system is of para\-mount importance for fundamental research and many cutting-edge applications. Currently, achieving this goal requires either a priori knowledge on the system or very demanding experimental procedures such as full state tomography or collective measurements. Here, we demonstrate that by employing neural networks we can quantify the degree of entanglement without needing to know the full description of the quantum state. Our method allows for direct quantification of the quantum correlations using an incomplete set of local measurements. Despite using undersampled measurements, we achieve a quantification error of up to an order of magnitude lower than the state-of-the-art quantum tomography. Furthermore, we achieve this result employing networks trained using exclusively simulated data. Finally, we derive a method based on a convolutional network input that can accept data from various measurement scenarios 
    
[^153]: RELDEC: 强化学习基于的中等长度LDPC码的解码方法

    RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes. (arXiv:2112.13934v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2112.13934](http://arxiv.org/abs/2112.13934)

    RELDEC是一种基于强化学习的中等长度LDPC码的解码方法，通过训练智能体顺序调度CN集群，以优化解码策略。同时，通过改进MDP的状态空间表示，适用于更大块长的LDPC码。为了解决不同信道条件下的解码问题，提出了AM-RELDEC算法。

    

    在这项工作中，我们提出了一种名为RELDEC的新方法，用于中等长度低密度奇偶校验（LDPC）码的顺序解码。 RELDEC的主要思想是基于马尔可夫决策过程（MDP）通过强化学习获得优化的解码策略。与我们先前的工作不同，在先前的工作中，智能体仅学习一次迭代中集群内的单个检查节点（CN）的调度，而在这项工作中，我们训练智能体调度每个集群中的所有CN以及每次迭代中的所有集群。也就是说，在RELDEC的每个学习步骤中，智能体根据与调度特定集群相关联的奖励来顺序学习调度CN集群。我们还修改了MDP的状态空间表示，使RELDEC适用于比我们之前的工作研究的较大块长的LDPC码。此外，为了解决在不同信道条件下的解码问题，我们提出了灵活的元-RELDEC（AM-RELDEC）方法，它采用的是敏捷的元-RELDEC算法。

    In this work we propose RELDEC, a novel approach for sequential decoding of moderate length low-density parity-check (LDPC) codes. The main idea behind RELDEC is that an optimized decoding policy is subsequently obtained via reinforcement learning based on a Markov decision process (MDP). In contrast to our previous work, where an agent learns to schedule only a single check node (CN) within a group (cluster) of CNs per iteration, in this work we train the agent to schedule all CNs in a cluster, and all clusters in every iteration. That is, in each learning step of RELDEC an agent learns to schedule CN clusters sequentially depending on a reward associated with the outcome of scheduling a particular cluster. We also modify the state space representation of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes than those studied in our previous work. Furthermore, to address decoding under varying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) that empl
    
[^154]: 走向超出分布泛化：一项调查

    Towards Out-Of-Distribution Generalization: A Survey. (arXiv:2108.13624v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.13624](http://arxiv.org/abs/2108.13624)

    这项研究调查了超出分布泛化的问题，该问题涉及到当测试数据的分布与训练数据不同时，模型性能下降的情况。这是对该问题的首次全面回顾，涵盖了问题定义、方法论发展、评估程序以及领域的意义和未来方向。

    

    传统的机器学习范式基于训练数据和测试数据遵循相同的统计模式的假设，数学上称为独立同分布（$i.i.d.$）。然而，在真实世界的应用中，由于未预料到的分布转变，这种$i.i.d.$假设经常不成立，导致模型在部署时性能大大降低。这种观察到的差异表明了研究超出分布（OOD）泛化问题的重要性。OOD泛化是机器学习研究的一个新兴主题，重点关注测试数据的分布与训练数据不同的复杂场景。本文是对OOD泛化的首次全面、系统的回顾，涵盖了从问题定义、方法论发展和评估程序到领域的意义和未来方向的一系列方面。

    Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed ($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our di
    
[^155]: 机器学习中的拒绝选项：一项调查

    Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.11277](http://arxiv.org/abs/2107.11277)

    这项调查综述了机器学习中的拒绝选项。通过机器学习模型避免在可能犯错误时做出预测，可以在决策支持应用中避免严重后果。调查介绍了拒绝选项的条件、评估策略以及相关应用领域，并探讨了它与其他机器学习方法的关系。

    

    机器学习模型总是做出预测，即使可能是不准确的。在许多决策支持应用中，应避免这种行为，因为错误可能带来严重后果。尽管在1970年已经研究过，但近年来机器学习中的拒绝选项引起了人们的关注。这个机器学习子领域使得机器学习模型能够在可能犯错误时避免做出预测。本调查旨在提供机器学习中拒绝选项的概述。我们介绍了导致两种拒绝情况（模糊和新奇拒绝）的条件，并对其进行了仔细的形式化。此外，我们还回顾和分类了评估模型预测和拒绝质量的策略。此外，我们定义了现有的带有拒绝选项的模型架构，并描述了学习这些模型的标准技术。最后，我们提供了相关应用领域的示例，并展示了机器学习中的拒绝选项与其他机器学习方法之间的关系。

    Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.  This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machi
    
[^156]: 组合式联邦学习：在分布鲁棒平均和元学习中的应用

    Compositional federated learning: Applications in distributionally robust averaging and meta learning. (arXiv:2106.11264v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11264](http://arxiv.org/abs/2106.11264)

    本文提出了一种名为ComFedL的组合式联邦学习算法，用于解决分布鲁棒联邦学习和模型不可知元学习问题，通过使用KL散度正则化，将分布鲁棒联邦学习转化为简单的组合优化问题。已证明ComFedL算法具有收敛速度为O(1/√T)，创新性地将联邦学习与组合随机优化结合在一起。

    

    在本文中，我们提出了一种有效且高效的组合式联邦学习（ComFedL）算法，用于解决具有层次结构的许多数据挖掘和机器学习问题，如分布鲁棒联邦学习和模型不可知元学习（MAML）。此外，我们对我们的ComFedL算法进行了收敛性分析，在一些温和的条件下证明了它达到了$O(\frac{1}{\sqrt{T}})$的收敛速度，其中$T$表示迭代次数。据我们所知，我们的新的组合式联邦学习框架是首个将联邦学习与组合随机优化相结合的工作。特别地，我们首先通过使用KL散度正则化将分布鲁棒联邦学习（即极小极大优化问题）转化为简单的组合优化问题。同时，我们还首次将分布不可知的MAML问题（即一个

    In the paper, we propose an effective and efficient Compositional Federated Learning (ComFedL) algorithm for solving a new compositional Federated Learning (FL) framework, which frequently appears in many data mining and machine learning problems with a hierarchical structure such as distributionally robust FL and model-agnostic meta learning (MAML). Moreover, we study the convergence analysis of our ComFedL algorithm under some mild conditions, and prove that it achieves a convergence rate of $O(\frac{1}{\sqrt{T}})$, where $T$ denotes the number of iteration. To the best of our knowledge, our new Compositional FL framework is the first work to bridge federated learning with composition stochastic optimization. In particular, we first transform the distributionally robust FL (i.e., a minimax optimization problem) into a simple composition optimization problem by using KL divergence regularization. At the same time, we also first transform the distribution-agnostic MAML problem (i.e., a
    
[^157]: 保护安全聚合：减轻联邦学习中多轮隐私泄漏问题

    Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning. (arXiv:2106.03328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03328](http://arxiv.org/abs/2106.03328)

    该论文提出了一个名为Multi-RoundSecAgg的安全聚合框架，解决了联邦学习中多轮隐私泄漏的问题，通过介绍新的隐私指标和开发结构化的用户选择策略实现了多轮隐私保证。

    

    安全聚合是联邦学习中的一个关键组成部分，它使得服务器能够在不观察用户本地模型的情况下学习到用户的聚合模型。传统的安全聚合算法仅关注单轮训练中个体用户的隐私保护。我们认为，这样的设计可能导致在多轮训练中存在显著的隐私泄漏，因为每轮联邦学习都会选择/参与部分用户。事实上，我们证明了传统的随机用户选择策略导致了用户个体模型的泄漏，其泄漏的轮数与用户数量成线性关系。为了应对这一挑战，我们提出了一个安全聚合框架Multi-RoundSecAgg，具有多轮隐私保证。具体而言，我们引入了一个衡量联邦学习在多轮训练中隐私保证的新指标，并开发了一种结构化的用户选择策略，以确保每个用户的长期隐私保护。

    Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over 
    
[^158]: 约束资源下神经模块专业化的动力学研究

    Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2106.02626](http://arxiv.org/abs/2106.02626)

    本研究使用人工神经网络模拟实验，发现结构模块化并不一定能够确保功能专业化，在特定环境和资源限制下，才能够出现专业化现象。

    

    长期以来，人们一直认为大脑在结构和功能上高度模块化，但最近的证据使一些人对两种模块化的程度产生了怀疑。我们使用人工神经网络来测试结构模块化是否足以保证功能专业化，并发现一般情况下，并不一定成立，除非在极端水平上。然后，我们系统地测试了环境和网络的哪些特征会导致专业化的出现。我们使用了一个简单的玩具环境、任务和网络，以精确控制条件，并表明在这个设置中，几个不同的专业化度量指标给出了类似的结果。我们进一步发现，（1）专业化只能在环境中那些可以明确分离的特征存在的情况下出现，（2）专业化更容易在网络资源受到强烈限制的情况下出现，（3）这些发现在 qualitatively 上相似。

    It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
    
[^159]: 基于强化学习的Acrobot控制的实验研究

    Experimental Study on Reinforcement Learning-based Control of an Acrobot. (arXiv:2011.09246v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2011.09246](http://arxiv.org/abs/2011.09246)

    本研究通过强化学习算法控制了Acrobot的角速度和能量，实现了未驱动摆杆的摇摆或完全旋转，并深入探究了影响强化学习控制的参数。

    

    我们通过强化学习（RL）来研究人工智能（AI）如何学习控制Acrobot。我们设计了一个嵌入式系统作为实验装置，该系统对机器人和能源收集应用具有重要意义。具体地，我们研究了控制Acrobot角速度以及其总能量（即动能和势能之和）。通过RL算法，我们能够将Acrobot的第一个摆杆的角速度或能量驱动到期望值，从而实现Acrobot未驱动摆杆的摇摆或完全旋转。此外，我们进行了Acrobot控制的研究，通过这些研究我们对状态空间离散化、回合长度、动作空间以及驱动摆杆质量对RL控制的影响有了更深入的了解。通过更多的模拟和实验，我们还研究了参数的效应。

    We present computational and experimental results on how artificial intelligence (AI) learns to control an Acrobot using reinforcement learning (RL). Thereby the experimental setup is designed as an embedded system, which is of interest for robotics and energy harvesting applications. Specifically, we study the control of angular velocity of the Acrobot, as well as control of its total energy, which is the sum of the kinetic and the potential energy. By this means the RL algorithm is designed to drive the angular velocity or the energy of the first pendulum of the Acrobot towards a desired value. With this, libration or full rotation of the unactuated pendulum of the Acrobot is achieved. Moreover, investigations of the Acrobot control are carried out, which lead to insights about the influence of the state space discretization, the episode length, the action space or the mass of the driven pendulum on the RL control. By further numerous simulations and experiments the effects of parame
    
[^160]: 关于数据增强中线性转换的泛化效果的研究

    On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.00695](http://arxiv.org/abs/2005.00695)

    这项研究考虑了一类线性转换，并研究了其在过参数化线性回归设置中对岭估计量的影响。研究发现，能够保持数据标签的转换可以通过扩大训练数据的张量来改善估计结果；而混合数据的转换则通过起到正则化作用来改善估计结果。此外，通过在MNIST数据集上进行验证，研究者提出了一个增强方案，该方案通过模型对转换后数据的不确定性进行搜索转换空间，并在图像和文本数据集上验证了其有效性。

    

    数据增强是一种在图像和文本分类任务等应用中提高性能的强大技术。然而，对于各种增强方法为何有效以及其工作原理的严格理解还很有限。在本研究中，我们考虑了一类线性转换，并研究了其在过参数化线性回归设置中对岭估计量的影响。首先，我们通过扩大训练数据的张量来展示了能够保持数据标签的转换会改善估计结果。其次，我们通过混合数据的转换展示了对估计量起到了正则化的作用。最后，我们通过MNIST数据集验证了我们的理论洞见。基于这些洞见，我们提出了一个通过模型对转换后的数据的不确定性来搜索转换空间的增强方案。我们在图像和文本数据集上验证了我们提出的方案。例如，我们的方法在使用Wide-ResNet对CIFAR-100数据集上优于随机采样方法1.24%。

    Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet
    
[^161]: 声明性机制设计

    Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13122](http://arxiv.org/abs/1912.13122)

    本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。

    

    多智能体系统（MAS）和声明性电子机构（DEIs）的调控是过去十年涉及物理和软件智能体以及法律的多学科研究课题，但近年来逐渐演变为2016年起被称为新闻的机器律师。其中一种首次提出限制软件智能体行为的方案是电子机构。然而，随着人工神经网络（ANNs）被重新定义为深度学习（DL），有关DL使用的安全、隐私、伦理和法律问题引起了人工智能（AI）社区的关注。现在，MAS的规范几乎得到正确处理，我们提出将人工神经网络的规范作为一种特殊类型的受管制的人工神经网络，称之为机构神经网络（INN）。本文的主旨是引起人们对人工教学（AT）的关注，并给出一个初步的答案，展示了一种证明性的方法。

    Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
    

