# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Diffusion Models for Reinforcement Learning: A Survey.](http://arxiv.org/abs/2311.01223) | 强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。 |
| [^2] | [JADE: A Linguistic-based Safety Evaluation Platform for LLM.](http://arxiv.org/abs/2311.00286) | JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。 |
| [^3] | [Self-supervised Pre-training for Precipitation Post-processor.](http://arxiv.org/abs/2310.20187) | 该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。 |
| [^4] | [MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks.](http://arxiv.org/abs/2310.15074) | MGAS是一个多粒度架构搜索的统一框架，通过学习特定粒度级别的离散化函数，自适应地确定剩余比例，从而实现同时优化模型大小和模型性能。 |
| [^5] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | 本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。 |
| [^6] | [Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning.](http://arxiv.org/abs/2310.12609) | 本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。 |
| [^7] | [Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions.](http://arxiv.org/abs/2310.07427) | 该论文提出了一种名为Quantum Gramian Angular Field (QGAF)的新方法，通过将量子计算技术与深度学习相结合，成功将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像，从而提高了预测的精度。实验证明，相比传统方法，QGAF方法具有显著的性能优势。 |
| [^8] | [Estimating Shape Distances on Neural Representations with Limited Samples.](http://arxiv.org/abs/2310.05742) | 本论文研究了在数据有限情况下，对高维神经表示进行形状距离估计的问题。通过推导出对形状距离标准估计器最坏情况下的收敛上下界，我们揭示了这个问题的挑战性质。为了克服挑战，我们引入了一种新的矩法估计器，并展示了其在高维设置下相对于标准估计器的优越性能。 |
| [^9] | [Molecular De Novo Design through Transformer-based Reinforcement Learning.](http://arxiv.org/abs/2310.05365) | 本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。 |
| [^10] | [A Language-Agent Approach to Formal Theorem-Proving.](http://arxiv.org/abs/2310.04353) | COPRA是一种面向形式定理证明的语言代理方法，利用大型语言模型进行上下文学习，通过选择策略和检索定义和引理进行证明，在MiniF2F基准和Coq任务上表现出优异的性能。 |
| [^11] | [How the level sampling process impacts zero-shot generalisation in deep reinforcement learning.](http://arxiv.org/abs/2310.03494) | 这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。 |
| [^12] | [Multi-Domain Causal Representation Learning via Weak Distributional Invariances.](http://arxiv.org/abs/2310.02854) | 本文提出了一种通过弱分布不变性进行多领域因果表示学习的方法，证明了融入这种不变性的自编码器能够可靠地识别出稳定的变量集合。 |
| [^13] | [L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation.](http://arxiv.org/abs/2310.02003) | L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。 |
| [^14] | [A simple connection from loss flatness to compressed representations in neural networks.](http://arxiv.org/abs/2310.01770) | 该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。 |
| [^15] | [Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images.](http://arxiv.org/abs/2309.16066) | 该论文提出了一种适用于医学髋关节X射线图像中的标记点检测任务的标签增强方法。通过使用仅标签增强方案进行训练，该方法超越了传统的数据增强方法，在样本利用效率上表现出色，可提高标记点检测的准确性。 |
| [^16] | [Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation.](http://arxiv.org/abs/2309.15726) | 我们开发了一种无监督神经网络架构，通过去噪扩散目标训练模型来实现同时生成和分割图像。这种架构通过在输入中划分区域并并行去噪以及合并结果，实现了准确的无监督图像分割和高质量的合成图像生成。 |
| [^17] | [Exciton-Polariton Condensates: A Fourier Neural Operator Approach.](http://arxiv.org/abs/2309.15593) | 本研究首次将傅里叶神经算子方法应用于激子极化子库伦凝聚系统，通过机器学习实现对Gross-Pitaevskii方程的求解，可以以接近1000倍的速度准确预测最终状态的解，为激子极化子库伦凝聚系统的大规模应用提供了新的解决方案。 |
| [^18] | [Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling.](http://arxiv.org/abs/2309.15214) | 一种用于千米尺度大气降尺度的生成残差扩散建模方法被提出，并展示了在天气和气候的物理灾害预测方面具有潜力。 |
| [^19] | [Grad DFT: a software library for machine learning enhanced density functional theory.](http://arxiv.org/abs/2309.15127) | Grad DFT是一种机器学习增强的密度泛函理论（DFT）软件库，通过使用权重和神经网络处理交换关联能量泛函，对DFT的能力进行了扩展。 |
| [^20] | [LogGPT: Log Anomaly Detection via GPT.](http://arxiv.org/abs/2309.14482) | 本论文提出了LogGPT，它是一个使用GPT进行日志异常检测的框架。通过语言建模和强化学习策略，LogGPT能够有效地检测系统日志中的异常情况。 |
| [^21] | [NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields.](http://arxiv.org/abs/2309.14293) | NAS-NeRF是一种生成式神经体系结构搜索策略，通过平衡架构复杂度和目标合成质量指标生成紧凑、针对场景的NeRF架构。 |
| [^22] | [The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance.](http://arxiv.org/abs/2309.13775) | 提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。 |
| [^23] | [BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials.](http://arxiv.org/abs/2309.08788) | 一个名为BioinspiredLLM的开源语言模型利用大量的文献进行微调，能够主动交互地回忆和评估生物材料的信息，提出新的问题和回答，并为生物材料设计提供合理的假设。 |
| [^24] | [VerilogEval: Evaluating Large Language Models for Verilog Code Generation.](http://arxiv.org/abs/2309.07544) | 本文提出了一个专门用于评估大型语言模型在Verilog代码生成中的性能的基准框架，并提供了一个包含156个问题的综合评估数据集。通过与黄金解决方案进行比较，可以自动测试Verilog代码的功能正确性。通过使用LLM生成的合成问题-代码对进行监督微调，可以改善预训练语言模型的Verilog代码生成能力。 |
| [^25] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^26] | [Comparative Analysis of CPU and GPU Profiling for Deep Learning Models.](http://arxiv.org/abs/2309.02521) | 本文通过比较GPU和CPU在深度学习模型训练中的性能，发现GPU在深度神经网络上具有更低的运行时间，对于较简单的网络，GPU并没有太多显著的改进。 |
| [^27] | [MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision.](http://arxiv.org/abs/2308.16139) | MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。 |
| [^28] | [Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction.](http://arxiv.org/abs/2308.13466) | 本文提出了一种新颖且可扩展的分布式GNN训练方法SAT，通过在线动态嵌入预测减轻了过时性带来的问题，解决了分布式GNN训练中的并发性和通信开销的困扰。 |
| [^29] | [BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions.](http://arxiv.org/abs/2308.09936) | BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。 |
| [^30] | [Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs.](http://arxiv.org/abs/2308.09895) | 本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。 |
| [^31] | [Generalizing Topological Graph Neural Networks with Paths.](http://arxiv.org/abs/2308.06838) | 本论文提出了一种通用的拓扑图神经网络（GNNs）方法，通过强调图中的路径，突破了1-Weisfeiler-Lehmann测试的理论限制，并在无需对图的子结构进行假设的情况下，在多个基准测试中取得了最先进的性能。 |
| [^32] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^33] | [Targeted and Troublesome: Tracking and Advertising on Children's Websites.](http://arxiv.org/abs/2308.04887) | 这项研究提供了针对面向儿童的网站进行追踪和广告的详细测量。通过构建一个多语言分类器，并对过两百万个网页进行分类，研究者们编制了一个儿童网站列表。通过对这些网站进行爬取和分析，研究发现追踪器、指纹脚本和广告都存在于这些网站上。同时，研究还检测到这些广告是否启用了定向。 |
| [^34] | [Causality Guided Disentanglement for Cross-Platform Hate Speech Detection.](http://arxiv.org/abs/2308.02080) | 本研究提出了一种跨平台仇恨言论检测模型，通过解缠输入表示为不变特征和平台相关特征，实现了对多个未见平台的良好泛化能力。 |
| [^35] | [Graph Condensation for Inductive Node Representation Learning.](http://arxiv.org/abs/2307.15967) | 本论文提出了一种映射感知的图形压缩方法（MCond），通过学习节点之间的映射关系，实现了在合成图中高效地处理未知数据的能力。 |
| [^36] | [Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense.](http://arxiv.org/abs/2307.11730) | 本文针对分散式联邦学习中的通信安全挑战，引入了一个安全模块，通过结合加密技术和移动目标防御技术来对抗通信攻击。 |
| [^37] | [DiTTO: Diffusion-inspired Temporal Transformer Operator.](http://arxiv.org/abs/2307.09072) | DiTTO是一种扩散启发的算子学习方法，通过结合Transformer架构的元素，无需时间离散化连续解决时间相关PDEs，并在多维度的各种PDE中取得了最先进的准确性结果。 |
| [^38] | [Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks.](http://arxiv.org/abs/2307.08939) | 这项研究评估了基于深度神经网络的自适应巡航控制系统在隐蔽感知攻击下的安全性，并提出了一种上下文感知策略和基于优化的图像扰动生成方法。 |
| [^39] | [Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation.](http://arxiv.org/abs/2307.08875) | 本文研究了鲁棒强化学习问题，提出了两种新的不确定性集合形式，使得大规模鲁棒强化学习变得可行。同时，提出了一个鲁棒的自然演员-评论家算法，通过函数逼近，该算法能够在有限时间内收敛到最佳鲁棒策略。 |
| [^40] | [Meta-Value Learning: a General Framework for Learning with Learning Awareness.](http://arxiv.org/abs/2307.08863) | 元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。 |
| [^41] | [Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!.](http://arxiv.org/abs/2307.06483) | 传播学领域中的自动化内容分析常忽视了错误分类的偏差，我们介绍并测试了统计方法来纠正这种偏差，并设计了一种新方法来修复之。 |
| [^42] | [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates.](http://arxiv.org/abs/2307.05695) | 本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。 |
| [^43] | [Bidirectional Attention as a Mixture of Continuous Word Experts.](http://arxiv.org/abs/2307.04057) | 双向注意力模型具有混合专家权重，类似于连续词袋模型（CBOW）的统计模型，它在大型语言模型中起到了重要作用。 |
| [^44] | [EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models.](http://arxiv.org/abs/2307.02028) | 该论文介绍了EHRSHOT，一个用于少样本评估基础模型的电子健康记录基准。该论文利用EHRSHOT数据集和预训练模型CLMBR-T-base，为医疗保健ML的发展提供了解决方案。 |
| [^45] | [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit.](http://arxiv.org/abs/2306.17759) | 在无限深度和宽度的比例极限下，我们通过修改Softmax-based注意力模型，研究了Transformer的协方差矩阵。我们发现在初始化时，极限分布可以用随机微分方程来描述。通过修改注意力机制并使用残差连接，我们可以控制网络的稳定性和协方差结构的行为。 |
| [^46] | [LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning.](http://arxiv.org/abs/2306.09910) | 本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。 |
| [^47] | [Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization.](http://arxiv.org/abs/2306.09803) | 本文介绍了一个模块化框架和基准，用于组合和混合变量贝叶斯优化，并提供多样的合成和真实世界基准测试。通过此框架，作者展示了4种常见的MCBO技术。 |
| [^48] | [Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens.](http://arxiv.org/abs/2306.09444) | 本研究提出了一个机器学习管道用于大规模场景下量子可分性的近似解，通过有效算法近似查找最近的可分离密度矩阵，并将量子可分性视为分类问题，对任何二维混合状态都适用。 |
| [^49] | [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2306.09364) | TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。 |
| [^50] | [DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data.](http://arxiv.org/abs/2306.09344) | 本文提出了一种全面评估图像的感知度量DreamSim，该度量使用合成数据学习人类视觉相似性的新维度，表现出优越性能。 |
| [^51] | [On the Role of Entanglement and Statistics in Learning.](http://arxiv.org/abs/2306.03161) | 本论文探讨了量子学习中量子纠缠和统计学的作用，研究了纠缠测量与可分离测量以及纠缠测量与统计测量在学习模型中的区别，证明了QSQ学习与利用纠缠测量的量子学习之间的指数差异。 |
| [^52] | [Extracting Reward Functions from Diffusion Models.](http://arxiv.org/abs/2306.01804) | 本论文提出了一种从两个Diffusion模型中提取奖励函数的实用学习算法，可以在导航环境中找到正确的奖励函数，并以此控制Diffusion模型学习足够复杂的任务。 |
| [^53] | [Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption.](http://arxiv.org/abs/2306.00196) | 本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。 |
| [^54] | [Robust Nonparametric Regression under Poisoning Attack.](http://arxiv.org/abs/2305.16771) | 本文提出了一种针对毒化攻击的鲁棒非参数回归方法，包括基于Huber损失的M-评估器和通过将初始估计投影到Lipschitz函数空间中的校正方法。结果表明，正确选择带宽时$\ell_\infty $误差是极小化最优的，而$\ell_2 $误差在$q\lesssim \sqrt{N/\ln^2 N}$时最优。 |
| [^55] | [BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer.](http://arxiv.org/abs/2305.12534) | BertRLFuzzer是一种基于BERT和强化学习的Fuzzer工具，旨在发现Web应用程序的安全漏洞。通过使用BERT模型作为代理来指导Fuzzer进行高效学习，BertRLFuzzer相对于其他黑盒和白盒Fuzzer在时间到首次攻击、新漏洞发现和攻击率方面都取得了显著的改进。 |
| [^56] | [The Waymo Open Sim Agents Challenge.](http://arxiv.org/abs/2305.12032) | Waymo开放模拟代理挑战赛提出使用真实、互动的智能体仿真以促进自动驾驶行为模型的评估和训练，是该领域的首个公开挑战赛，旨在推动逼真模拟器的设计。 |
| [^57] | [Exact Recovery for System Identification with More Corrupt Data than Clean Data.](http://arxiv.org/abs/2305.10506) | 本文研究了在敌对环境下线性离散时间系统的系统识别问题。在周期性注入攻击时，系统动态可精确恢复样本复杂度为O(n)；当攻击以概率p进行时，精确恢复样本复杂度为O(log(n)p/(1-p)^2) 。即使有超过一半的数据受损，估计器仍可学习。 |
| [^58] | [Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model.](http://arxiv.org/abs/2305.10133) | 本文提出了一种基于口袋的三维分子生成方法，利用扰动和恢复预训练任务和一种新的分子表示形式。 该方法结合了语言模型和几何深度学习的优点，使得语言模型可以生成准确的三维分子。 |
| [^59] | [A score-based operator Newton method for measure transport.](http://arxiv.org/abs/2305.09792) | 本文提出一种新的基于分数的算子Newton方法，可以迭代构造一个易处理的原概率测度，该方法可以在满足目标分数光滑性假设下，实现快速收敛性。 |
| [^60] | [Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception.](http://arxiv.org/abs/2305.06324) | 本文提出了集成多模态感知（IMP）方法，将多模态输入集成到单个编码器中，采用交替梯度下降法（AGD）和混合专家（MoE）相结合的方法实现高效的模型和任务扩展，取得了在多个基准测试中具有竞争力的性能表现。 |
| [^61] | [UQ for Credit Risk Management: A deep evidence regression approach.](http://arxiv.org/abs/2305.04967) | 本文扩展了Deep Evidence Regression方法，将其应用于预测信用风险中的违约损失；我们提供了相关的学习框架，并在模拟和实际数据上进行了验证。 |
| [^62] | [FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer.](http://arxiv.org/abs/2305.01154) | 本文介绍了一种名为FedAVO的算法，利用非洲秃鹫优化器选择最佳超参数来提高联邦学习中的通信效率，显著降低了与FL操作相关的通信成本。 |
| [^63] | [Dynamic Pricing and Learning with Bayesian Persuasion.](http://arxiv.org/abs/2304.14385) | 本研究提出了一种计算有效的在线算法，在没有先验知识的情况下，自适应学习最优定价和广告策略，达到次线性后悔。 |
| [^64] | [Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method.](http://arxiv.org/abs/2304.11171) | 本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。 |
| [^65] | [AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks.](http://arxiv.org/abs/2304.09595) | 本文提出了一种专为图神经网络设计的δ调节方法——AdapterGNN，该方法保留了预训练模型的知识，利用高度表达的适配器能够在仅有少量参数的情况下有效地适应下游任务，并提高模型的泛化能力，实验结果表明其在多个基准数据集上取得了最优性能。 |
| [^66] | [Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy.](http://arxiv.org/abs/2304.07460) | 本文提出了一种名为PFELS的无线FL方案，通过先压缩模型更新再自适应地设计发送功率来提供客户端级别DP保证，并降低通信和能量开销并提高模型精度。 |
| [^67] | [Localisation of Regularised and Multiview Support Vector Machine Learning.](http://arxiv.org/abs/2304.05655) | 本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。 |
| [^68] | [Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems.](http://arxiv.org/abs/2304.03671) | 本文提出了一种基于收缩引导的自适应分区算法，用于改善带有神经网络控制器和干扰的非线性反馈回路中区间值鲁棒可达集估计，该算法通过将神经网络验证步骤和可达性分区层的解耦，可以在很小的计算成本下提供精度提升。 |
| [^69] | [Leveraging Neo4j and deep learning for traffic congestion simulation & optimization.](http://arxiv.org/abs/2304.00192) | 本文利用Neo4j图和深度学习方法解决城市道路网络中的交通拥堵问题，能够实现拥堵路段的负载平衡和优化，同时可以预测交通事故对整体交通的影响。 |
| [^70] | [Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems.](http://arxiv.org/abs/2303.12928) | 本文从理论上将特定优化问题与多时间Hamilton-Jacobi PDEs联系起来，表明当解决这些问题时，同时解决了对应的多时间HJ PDEs和最优控制问题。利用这种联系，提出了一种新的算法，实现了深度神经网络泛化性能的改进。 |
| [^71] | [Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.](http://arxiv.org/abs/2302.12247) | 通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。 |
| [^72] | [Principlism Guided Responsible Data Curation.](http://arxiv.org/abs/2302.03629) | 研究针对人本计算机视觉数据集的负责任数据管理建议，采用预防性反思的观点，遵循原则主义的伦理框架，解决隐私和偏差问题。 |
| [^73] | [MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs.](http://arxiv.org/abs/2302.00735) | 本文介绍了一种基于图的概率多智能体轨迹预测模型MTP-GO。该模型利用时间图神经网络编码场景，采用神经常微分方程实现运动模型，并结合混合密度网络和卡尔曼滤波实现多模态概率预测，在多个指标上优于其他最先进的方法。 |
| [^74] | [ClimaX: A foundation model for weather and climate.](http://arxiv.org/abs/2301.10343) | ClimaX是一种灵活且可推广的深度学习模型，用于天气和气候科学，可以使用不同数据集进行训练。 |
| [^75] | [Solving Bilevel Knapsack Problem using Graph Neural Networks.](http://arxiv.org/abs/2211.13436) | 本研究提出了一种使用图神经网络的深度学习方法来解决双层背包问题，该方法比精确算法快500倍，可找到可行性解决方案。 |
| [^76] | [Reinforcement Learning in Non-Markovian Environments.](http://arxiv.org/abs/2211.01595) | 本文通过递归计算近似充分统计量，提出了一种基于自编码器的代理设计方案，实现了在非马尔可夫环境中进行强化学习。 |
| [^77] | [PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting.](http://arxiv.org/abs/2210.08964) | 提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。 |
| [^78] | [BAFFLE: Backdoor Attack in Offline Reinforcement Learning.](http://arxiv.org/abs/2210.04688) | 本文研究离线增强学习中的后门攻击，通过向数据中添加扰动，使得智能体在注入触发器的观测值上采取低奖励动作，从而提出了BAFFLE方法。 |
| [^79] | [UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering.](http://arxiv.org/abs/2208.11435) | 本文提出了UniCon方法，用于解决多客户VQA任务的保密性约束和客户有限标记训练数据的问题。该方法通过模型共享学习跨模态表示，采用分裂学习架构确保隐私。 |
| [^80] | [GANs and Closures: Micro-Macro Consistency in Multiscale Modeling.](http://arxiv.org/abs/2208.10715) | 论文探讨了多尺度建模中采样和集合平均的问题，并将增强采样技术与分子模拟与机器学习中的生成对抗网络相结合。 |
| [^81] | [Supply-Side Equilibria in Recommender Systems.](http://arxiv.org/abs/2206.13489) | 本论文探究了推荐系统中个性化内容的供给侧均衡问题，其特点是生产者决策空间是多维的和用户群体是异构的，高维度和异质性的模型创造了专业化的可能性。 |
| [^82] | [Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification.](http://arxiv.org/abs/2206.09098) | 本文证明了对抗性代理风险的存在性、正则性和极小化定理，这一结果为对抗鲁棒性的理论提供了支持，并且可以指导算法的发展。 |
| [^83] | [The Selectively Adaptive Lasso.](http://arxiv.org/abs/2205.10697) | 本文提出了一种新算法——Selectively Adaptive Lasso（SAL），它基于HAL的理论构建，保留了无维度、非参数收敛速率的优点，同时也具有可扩展到大规模高维数据集的能力。这种算法将许多回归系数自动设置为零。 |
| [^84] | [HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement.](http://arxiv.org/abs/2203.13086) | 本文提出了一种统一框架HiFi++，用于带宽扩展和语音增强任务，通过改进的生成器架构，在这些任务中表现出与最先进方法相媲美甚至更好的性能，并且消耗更少的计算资源。 |
| [^85] | [Geometric Repair for Fair Classification at Any Decision Threshold.](http://arxiv.org/abs/2203.07490) | 本文研究了如何通过减少每个组分数分布之间的统计距离，在任何决策阈值下同时提高分类的公平性能，并提出了一种基于最佳运输的后处理算法。 |
| [^86] | [A Lightweight and Gradient-Stable Nerual Layer.](http://arxiv.org/abs/2106.04088) | Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。 |

# 详细

[^1]: 强化学习的扩散模型: 一份综述

    Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])

    [http://arxiv.org/abs/2311.01223](http://arxiv.org/abs/2311.01223)

    强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。

    

    扩散模型作为一种突出的生成模型类别已经出现，超越了以往方法在样本质量和训练稳定性方面的优势。最近的研究表明，扩散模型在改进强化学习（RL）解决方案方面具有优势，包括作为轨迹规划器、表达能力丰富的策略类别、数据合成器等。本综述旨在提供该新兴领域发展的概述，并希望能启发新的研究方向。首先，我们审查了当前RL算法遇到的一些挑战。然后，我们根据扩散模型在RL中所扮演的角色，提出了现有方法的分类法，并探讨了如何解决现有挑战。我们进一步概述了扩散模型在各种与RL相关任务中的成功应用，并讨论了当前方法的局限性。最后，我们总结了这项综述，并提出了对未来研究方向的见解，重点是提高模型性能和应用扩散模型的方法。

    Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
    
[^2]: JADE：基于语言的LLM安全评估平台

    JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])

    [http://arxiv.org/abs/2311.00286](http://arxiv.org/abs/2311.00286)

    JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。

    

    本文介绍了JADE，一种针对语言分析的模糊测试平台，通过增强种子问题的语言复杂性，同时并始终能够破坏广泛使用的三类LLM：八个开源中文LLM，六个商业中文LLM和四个商业英文LLM。JADE为这三类LLM生成了三个安全基准，其中包含高度威胁的不安全问题：这些问题可以同时触发多个LLM的有害生成，平均不安全生成比例为70%（请参见下表），同时这些问题仍然是自然、流畅且保留了核心的不安全语义。我们在以下链接中发布了对商业英文LLM和开源英文LLM生成的基准演示：https://github.com/whitzard-ai/jade-db。对于对JADE生成的更多问题感兴趣的读者，请与我们联系。

    In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
    
[^3]: 自监督预训练用于降水后处理

    Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])

    [http://arxiv.org/abs/2310.20187](http://arxiv.org/abs/2310.20187)

    该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。

    

    为了预防危险天气事件，确保充足的局地降水预报提前时间至关重要。然而，全球变暖引起的气候变化增加了准确预测严重降水事件（如暴雨）的挑战。本工作提出了一种基于深度学习的降水后处理方法，用于数值天气预报（NWP）模型。降水后处理包括（i）自监督预训练，其中编码器的参数在大气物理领域的遮蔽变量重构上进行预训练，以及（ii）从预训练的编码器中转移学习到降水分割任务（目标领域）。我们还引入了一种启发式标记方法，以有效地训练类别不平衡的数据集。我们在区域NWP中的降水校正实验结果表明，所提出的方法优于其他方法。

    Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
    
[^4]: MGAS: 多粒度架构搜索以实现高效且有效的神经网络

    MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15074](http://arxiv.org/abs/2310.15074)

    MGAS是一个多粒度架构搜索的统一框架，通过学习特定粒度级别的离散化函数，自适应地确定剩余比例，从而实现同时优化模型大小和模型性能。

    

    可微分架构搜索(DAS)通过时间高效的自动化改变了神经网络架构搜索(NAS)的方式，从离散候选采样和评估转变为可微分超网络优化和离散化。然而，现有的DAS方法要么只进行粗粒度的操作级搜索，要么手动定义剩余的细粒度的核级和权重级单位的比例，从而无法同时优化模型大小和模型性能。此外，这些方法为了减少内存消耗而牺牲了搜索质量。为了解决这些问题，我们引入了多粒度架构搜索(MGAS)，这是一个统一的框架，旨在全面而内存高效地探索多粒度搜索空间，发现既有效又高效的神经网络。具体来说，我们学习了针对每个粒度级别的离散化函数，根据不断演化的架构自适应地确定剩余的比例。

    Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec
    
[^5]: 几乎等变性通过李代数卷积

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    

    最近，在机器学习中，模型相对于群作用的等变性已成为一个重要的研究课题。然而，赋予一个架构具体的群等变性对模型所期望看到的数据变换类型施加了强大的先验。严格等变模型强制执行对称性，但真实世界的数据并不总是符合这样的严格等变性，可能是因为数据中的噪声或仅编码了近似或部分对称性的潜在物理定律。在这种情况下，严格等变性的先验实际上可能过于强大，导致模型在真实数据上表现不佳。因此，在这项工作中，我们研究了一个相关的主题，即几乎等变性。我们提供了一个与当前文献中现有定义不同的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^6]: 使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法

    Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])

    [http://arxiv.org/abs/2310.12609](http://arxiv.org/abs/2310.12609)

    本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。

    

    由于其灵活性和多模态性，扩散模型在机器人领域中已经成为一种强大的工具。尽管其中一些方法有效地解决了复杂问题，但它们往往严重依赖于推理时的障碍物检测并需要额外的设备。为了应对这些挑战，我们提出了一种方法，该方法在推理时能够从单一的视觉输入中同时生成可达目标并规划避开障碍物的运动路径。我们的方法的核心是对训练过程中新颖的碰撞避免扩散核进行使用。通过与行为克隆和经典扩散模型进行评估，我们的框架证明了其稳健性。特别是在多模态环境中，它能够导航到目标并避开被障碍物阻挡的不可达目标，同时确保避免碰撞。

    Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
    
[^7]: 增强量子预测能力：利用量子Gramian角度场和CNN进行股票回报预测

    Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v1 [cs.LG])

    [http://arxiv.org/abs/2310.07427](http://arxiv.org/abs/2310.07427)

    该论文提出了一种名为Quantum Gramian Angular Field (QGAF)的新方法，通过将量子计算技术与深度学习相结合，成功将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像，从而提高了预测的精度。实验证明，相比传统方法，QGAF方法具有显著的性能优势。

    

    我们提出了一种名为Quantum Gramian Angular Field (QGAF)的时间序列预测方法。该方法将量子计算技术与深度学习相结合，旨在提高时间序列分类和预测的精度。通过设计特定的量子电路，我们成功地将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像。与经典的Gramian Angular Field (GAF)方法不同，QGAF的独特之处在于消除了数据归一化和反余弦计算的需求，简化了从时间序列数据到二维图像的转换过程。为了验证该方法的有效性，我们在中国A股市场、香港股市和美国股市的数据集上进行了实验。实验结果表明，与经典的GAF方法相比，QGAF方法显著改善了预测性能。

    We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improv
    
[^8]: 有限采样下神经表示的形状距离估计

    Estimating Shape Distances on Neural Representations with Limited Samples. (arXiv:2310.05742v1 [stat.ML])

    [http://arxiv.org/abs/2310.05742](http://arxiv.org/abs/2310.05742)

    本论文研究了在数据有限情况下，对高维神经表示进行形状距离估计的问题。通过推导出对形状距离标准估计器最坏情况下的收敛上下界，我们揭示了这个问题的挑战性质。为了克服挑战，我们引入了一种新的矩法估计器，并展示了其在高维设置下相对于标准估计器的优越性能。

    

    在神经科学和深度学习领域，衡量高维网络表示之间的几何相似性一直是一个长期的研究兴趣。尽管已经提出了许多方法，但只有少数工作对它们的统计效率进行了严格分析，或者对数据有限情况下的估计器不确定性进行了量化。在这里，我们推导出了标准形状距离估计器（由Williams et al. (2021)提出）的最坏情况收敛上下界。这些界限揭示了在高维特征空间中这个问题的挑战性质。为了克服这些挑战，我们引入了一种新的矩法估计器，具有可调的偏差-方差权衡。我们展示了这个估计器在模拟和神经数据上相对于标准估计器在高维设置下实现了更好的性能。因此，我们为高维形状分析奠定了严格的统计理论基础。

    Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance$\unicode{x2014}$a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, an
    
[^9]: 通过基于Transformer的强化学习进行分子的全新设计

    Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05365](http://arxiv.org/abs/2310.05365)

    本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。

    

    本文介绍了一种通过精细调整基于Transformer的生成模型用于分子的全新设计的方法。利用Transformer相对于循环神经网络（RNN）的优越序列学习能力，我们的模型可以有效地生成具有所需性质的分子结构。与传统的基于RNN的模型相比，我们提出的方法在生成预测对多种生物靶点具有活性的化合物方面表现出卓越性能，捕捉了分子结构序列的长期依赖性。该模型的有效性在许多任务中得到了证明，包括生成与查询结构类似的分子和生成具有特定属性的化合物，在性能上优于基线的基于RNN的方法。我们的方法可以用于桥接化学、从单个分子开始扩展库，并生成具有高预测活性的化合物。

    In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
    
[^10]: 一种面向形式定理证明的语言代理方法

    A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])

    [http://arxiv.org/abs/2310.04353](http://arxiv.org/abs/2310.04353)

    COPRA是一种面向形式定理证明的语言代理方法，利用大型语言模型进行上下文学习，通过选择策略和检索定义和引理进行证明，在MiniF2F基准和Coq任务上表现出优异的性能。

    

    语言代理是利用大型语言模型（LLM）进行上下文学习来与外部环境进行交互的方法，最近被认为是一种有前景的控制任务方法。

    Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
    
[^11]: 如何水平采样过程影响深度强化学习中的零样本泛化

    How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])

    [http://arxiv.org/abs/2310.03494](http://arxiv.org/abs/2310.03494)

    这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。

    

    阻止广泛采用通过深度强化学习（RL）训练的自主代理代理的关键局限是它们有限的适应新环境能力，即使这些环境与训练中遇到的环境具有相似特征。本研究探讨了个体环境实例或层级的非均匀采样策略如何影响RL代理的零样本泛化（ZSG）能力，并考虑了两种失效模式：过拟合和过度泛化。首先，我们测量了代理的内部表示与训练层级集之间的相互信息（MI），发现MI与实例的过拟合相关性很强。与均匀采样相比，基于值损失优先级的自适应采样策略更能有效地保持较低的MI，这为这类技术提供了一种新的理论解释。接下来，我们将注意力转向无监督环境设计（U）

    A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
    
[^12]: 通过弱分布不变性实现多领域因果表示学习

    Multi-Domain Causal Representation Learning via Weak Distributional Invariances. (arXiv:2310.02854v1 [cs.LG])

    [http://arxiv.org/abs/2310.02854](http://arxiv.org/abs/2310.02854)

    本文提出了一种通过弱分布不变性进行多领域因果表示学习的方法，证明了融入这种不变性的自编码器能够可靠地识别出稳定的变量集合。

    

    因果表示学习已成为因果机器学习研究的核心。特别是，多领域数据集为展示因果表示学习相对于标准无监督表示学习的优势提供了自然机会。虽然最近的研究在学习因果表示方面取得了重要进展，但由于过于简化数据的假设，它们往往不能适用于多领域数据集；例如，每个领域都来自不同的单节点完美干预。在本文中，我们放宽了这些假设，并利用以下观察结果：在多领域数据中，往往存在一部分潜变量的某些分布属性（例如支持度、方差）在不同领域之间保持稳定；当每个领域来自多节点不完美干预时，这个属性成立。利用这个观察结果，我们证明了融入这种不变性的自编码器能够可靠地识别出稳定的变量集合。

    Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set o
    
[^13]: L2MAC：大规模语言模型自动计算机用于无限代码生成

    L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])

    [http://arxiv.org/abs/2310.02003](http://arxiv.org/abs/2310.02003)

    L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。

    

    基于Transformer的大型语言模型（LLM）受到底层Transformer架构固定上下文窗口的限制，阻碍了它们生成长且逻辑一致的代码的能力。增强记忆的LLM是一个有前途的解决方案，但目前的方法无法处理长时间的代码生成任务，因为它们要么只关注于读取内存并将其演变为新内存的连接，要么使用非常专门的内存，无法适应其他领域。本文介绍了L2MAC，这是一种基于LLM的长且一致代码生成的实用存储程序自动计算机。它的内存有两个组成部分：指令注册表，其中填充了一个解决用户给定任务的提示程序，以及文件存储，其中包含最终和中间输出。每个指令由单独的LLM实例执行，其上下文由控制单元管理，能够精确读取和写入内存，以确保有效的整合。

    Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
    
[^14]: 损失平坦性与神经网络中压缩表示的简单联系

    A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])

    [http://arxiv.org/abs/2310.01770](http://arxiv.org/abs/2310.01770)

    该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。

    

    对深度神经网络的泛化能力进行研究的方法有很多种，包括至少两种不同的方法：一种基于参数空间中损失景观的形状，另一种基于特征空间中表示流形的结构（即单位活动的空间）。这两种方法相关但很少同时进行研究和明确关联。在这里，我们提出了一种简单的分析方法来建立这种联系。我们展示了在深度神经网络学习的最后阶段，神经表示流形的体积压缩与正在进行的参数优化所探索的最小值周围的损失平坦性相关。我们证明了这可以由一个相对简单的数学关系来预测：损失平坦性意味着神经表示的压缩。我们的结果与\citet{ma_linear_2021}的先前研究密切相关，该研究展示了平坦性（即小特征值）与表示流形的体积压缩之间的关系。

    Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
    
[^15]: 医学髋关节X射线图像中的标记点检测的标签增强方法

    Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images. (arXiv:2309.16066v1 [cs.LG])

    [http://arxiv.org/abs/2309.16066](http://arxiv.org/abs/2309.16066)

    该论文提出了一种适用于医学髋关节X射线图像中的标记点检测任务的标签增强方法。通过使用仅标签增强方案进行训练，该方法超越了传统的数据增强方法，在样本利用效率上表现出色，可提高标记点检测的准确性。

    

    本文报道了一种用于预测髋关节X射线图像中临床标记点的自动化医学标记点检测方法的实证性能。值得注意的是，该检测方法是使用仅标签增强方案进行训练的；我们的结果表明，这种增强形式优于传统的数据增强，并且产生高效的样本估计器。我们使用基于通用U-Net架构的课程训练，该训练包括两个阶段：首先通过将标记点扩大到区域来放松标记任务，然后逐渐将这些标签区域回归到基本任务。我们在含有黄金标准专家注释的六个放射图像数据集上评估了这种方法的优势。

    This work reports the empirical performance of an automated medical landmark detection method for predict clinical markers in hip radiograph images. Notably, the detection method was trained using a label-only augmentation scheme; our results indicate that this form of augmentation outperforms traditional data augmentation and produces highly sample efficient estimators. We train a generic U-Net-based architecture under a curriculum consisting of two phases: initially relaxing the landmarking task by enlarging the label points to regions, then gradually eroding these label regions back to the base task. We measure the benefits of this approach on six datasets of radiographs with gold-standard expert annotations.
    
[^16]: 因式分解扩散架构用于无监督图像生成和分割

    Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v1 [cs.CV])

    [http://arxiv.org/abs/2309.15726](http://arxiv.org/abs/2309.15726)

    我们开发了一种无监督神经网络架构，通过去噪扩散目标训练模型来实现同时生成和分割图像。这种架构通过在输入中划分区域并并行去噪以及合并结果，实现了准确的无监督图像分割和高质量的合成图像生成。

    

    我们开发了一种神经网络架构，以非监督方式作为去噪扩散模型进行训练，同时学习生成和分割图像。学习完全是由去噪扩散目标驱动的，在训练期间没有任何注释或关于区域的先验知识。神经网络架构中的计算瓶颈鼓励去噪网络将输入划分为区域，同时对它们进行去噪，并将结果合并。我们训练的模型通过简单检查其内部预测的分区，生成合成图像和这些图像的语义分割。我们直接将我们的无监督模型应用于通过添加噪声然后去噪来分割真实图像的下游任务，无需任何微调。实验证明，我们的模型在多个数据集上实现了准确的无监督图像分割和高质量的合成图像生成。

    We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images. Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training. A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results. Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, a semantic segmentation of those images. Without any finetuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them. Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.
    
[^17]: 激子极化子库伦凝聚：一种傅里叶神经算子方法

    Exciton-Polariton Condensates: A Fourier Neural Operator Approach. (arXiv:2309.15593v1 [cond-mat.quant-gas])

    [http://arxiv.org/abs/2309.15593](http://arxiv.org/abs/2309.15593)

    本研究首次将傅里叶神经算子方法应用于激子极化子库伦凝聚系统，通过机器学习实现对Gross-Pitaevskii方程的求解，可以以接近1000倍的速度准确预测最终状态的解，为激子极化子库伦凝聚系统的大规模应用提供了新的解决方案。

    

    过去十年中，半导体制造技术的进展催生了对由激子极化子库伦凝聚驱动的全光学器件的广泛研究。包括晶体管在内的这类器件的初步验证已经在环境条件下取得了鼓舞人心的结果。然而，一个重要的挑战仍然存在于大规模应用领域：缺乏一个健壮的求解器，可以用于模拟需要较长时间达到稳定的复杂非线性系统。为了解决这个需求，我们提出了一种基于机器学习的傅里叶神经算子方法，用于求解与额外激子速率方程耦合的Gross-Pitaevskii方程。这项工作标志着神经算子首次直接应用于激子极化子库伦凝聚系统。我们的研究结果表明，与基于CUDA的GPU求解器相比，所提出的方法可以以接近1000倍的速度准确预测最终状态的解。此外，这为今后的深入研究铺平了道路。

    Advancements in semiconductor fabrication over the past decade have catalyzed extensive research into all-optical devices driven by exciton-polariton condensates. Preliminary validations of such devices, including transistors, have shown encouraging results even under ambient conditions. A significant challenge still remains for large scale application however: the lack of a robust solver that can be used to simulate complex nonlinear systems which require an extended period of time to stabilize. Addressing this need, we propose the application of a machine-learning-based Fourier Neural Operator approach to find the solution to the Gross-Pitaevskii equations coupled with extra exciton rate equations. This work marks the first direct application of Neural Operators to an exciton-polariton condensate system. Our findings show that the proposed method can predict final-state solutions to a high degree of accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this paves t
    
[^18]: 用于千米尺度大气降尺度的生成残差扩散建模

    Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])

    [http://arxiv.org/abs/2309.15214](http://arxiv.org/abs/2309.15214)

    一种用于千米尺度大气降尺度的生成残差扩散建模方法被提出，并展示了在天气和气候的物理灾害预测方面具有潜力。

    

    当前从天气和气候中进行物理灾害预测的最先进方法需要进行昂贵的千米尺度数值模拟，并驱动较粗分辨率的全球输入。本文提出了一种千米尺度降尺度扩散模型作为一种具有成本效益的替代方法。该模型是从台湾的区域高分辨率天气模型训练得到的，并在ERA5再分析数据的基础下进行了条件训练。为了解决降尺度的不确定性，大分辨率比率（25km至2km），不同尺度上涉及的不同物理过程以及在输入数据中不存在的预测通道，我们采用了一个两步的方法（ResDiff），其中一个（UNet）回归在第一步预测平均值，而扩散模型在第二步预测残差。\textit{ResDiff}在块均方根误差和CRPS得分上表现出了令人鼓舞的技能。ResDiff预测的光谱和分布忠实地恢复了调节有害风和雨的重要幂律关系。统一的天气现象案例研究

    The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
    
[^19]: Grad DFT：一种用于机器学习增强密度泛函理论的软件库

    Grad DFT: a software library for machine learning enhanced density functional theory. (arXiv:2309.15127v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15127](http://arxiv.org/abs/2309.15127)

    Grad DFT是一种机器学习增强的密度泛函理论（DFT）软件库，通过使用权重和神经网络处理交换关联能量泛函，对DFT的能力进行了扩展。

    

    密度泛函理论（DFT）作为计算量子化学和材料科学中的基石方法，因其出色的多功能性和可扩展性而闻名。然而，在处理强关联系统时，DFT存在精度限制。为了解决这些缺点，最近的研究开始探索如何利用机器学习扩展DFT的能力，这是一个充满许多开放问题和技术挑战的努力。在这项工作中，我们介绍了Grad DFT：一个完全可微的基于JAX的DFT库，能够快速原型设计和实验机器学习增强的交换关联能量泛函。Grad DFT采用了一种先驱性的交换关联泛函参数化方法，该方法使用能量密度的加权和来确定权重，权重通过神经网络确定。此外，Grad DFT包含了一套全面的辅助函数，其中最重要的特点是可即时编译和完全可微的。

    Density functional theory (DFT) stands as a cornerstone method in computational quantum chemistry and materials science due to its remarkable versatility and scalability. Yet, it suffers from limitations in accuracy, particularly when dealing with strongly correlated systems. To address these shortcomings, recent work has begun to explore how machine learning can expand the capabilities of DFT; an endeavor with many open questions and technical challenges. In this work, we present Grad DFT: a fully differentiable JAX-based DFT library, enabling quick prototyping and experimentation with machine learning-enhanced exchange-correlation energy functionals. Grad DFT employs a pioneering parametrization of exchange-correlation functionals constructed using a weighted sum of energy densities, where the weights are determined using neural networks. Moreover, Grad DFT encompasses a comprehensive suite of auxiliary functions, notably featuring a just-in-time compilable and fully differentiable s
    
[^20]: LogGPT：通过GPT进行日志异常检测

    LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v1 [cs.LG])

    [http://arxiv.org/abs/2309.14482](http://arxiv.org/abs/2309.14482)

    本论文提出了LogGPT，它是一个使用GPT进行日志异常检测的框架。通过语言建模和强化学习策略，LogGPT能够有效地检测系统日志中的异常情况。

    

    基于日志数据的系统异常检测对于确保计算机系统的安全性和可靠性非常重要。近年来，深度学习模型已被广泛用于日志异常检测。核心思想是将日志序列建模为自然语言，并采用深度时序模型（如LSTM或Transformer）通过语言建模来编码日志序列中的正常模式。然而，语言建模与异常检测之间存在差距，因为通过语言建模损失训练时序模型的目标与异常检测不直接相关。为填补这一差距，我们提出了LogGPT，这是一个使用GPT进行日志异常检测的新框架。首先，我们训练LogGPT根据前序序列预测下一个日志条目。为了进一步提高LogGPT的性能，我们提出了一种新的强化学习策略，专门为日志异常检测任务微调模型。在三个数据集上的实验结果表明。

    Detecting system anomalies based on log data is important for ensuring the security and reliability of computer systems. Recently, deep learning models have been widely used for log anomaly detection. The core idea is to model the log sequences as natural language and adopt deep sequential models, such as LSTM or Transformer, to encode the normal patterns in log sequences via language modeling. However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection. To fill up the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly detection. LogGPT is first trained to predict the next log entry based on the preceding sequence. To further enhance the performance of LogGPT, we propose a novel reinforcement learning strategy to finetune the model specifically for the log anomaly detection task. The experimental results on three datasets show tha
    
[^21]: NAS-NeRF: 用于神经辐射场的生成式神经体系结构搜索

    NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14293](http://arxiv.org/abs/2309.14293)

    NAS-NeRF是一种生成式神经体系结构搜索策略，通过平衡架构复杂度和目标合成质量指标生成紧凑、针对场景的NeRF架构。

    

    神经辐射场（NeRF）实现了高质量的新视图合成，但其高计算复杂度限制了其可部署性。现有的基于神经网络的解决方案努力提高效率，但不考虑场景复杂性，使用通用架构。同一个架构可能对简单场景来说过于庞大，对复杂场景则不足够。因此，有必要动态优化NeRF的神经网络组件，以在计算复杂度和合成质量之间实现平衡。我们引入了NAS-NeRF，一种生成式神经体系结构搜索策略，通过平衡架构复杂度和目标合成质量指标生成紧凑、针对场景的NeRF架构。我们的方法结合目标度量和预算约束，指导搜索以获得适合每个场景的架构。在Blender合成数据集上进行的实验证明，提出的NAS-NeRF方法可以生成多达5个架构。

    Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5
    
[^22]: 论文标题：The Rashomon Importance Distribution: 摆脱不稳定的基于单一模型的变量重要性

    The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])

    [http://arxiv.org/abs/2309.13775](http://arxiv.org/abs/2309.13775)

    提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。

    

    量化变量重要性对于回答遗传学、公共政策和医学等领域的重大问题至关重要。当前的方法通常计算给定数据集上训练的给定模型的变量重要性。然而，对于给定数据集，可能有许多模型同样能解释目标结果;如果不考虑所有可能的解释，不同的研究者可能会得出许多冲突但同样有效的结论。此外，即使考虑了给定数据集的所有可能解释，这些洞察力可能不具有普适性，因为并非所有好的解释在合理的数据扰动下都是稳定的。我们提出了一种新的变量重要性框架，该框架量化了在所有好的模型集合中的变量重要性，并且在数据分布上是稳定的。我们的框架非常灵活，可以与大多数现有的模型类和全局变量重要性指标结合使用。

    Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
    
[^23]: BioinspiredLLM: 生物和生物受启发材料力学的对话型大型语言模型

    BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials. (arXiv:2309.08788v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.08788](http://arxiv.org/abs/2309.08788)

    一个名为BioinspiredLLM的开源语言模型利用大量的文献进行微调，能够主动交互地回忆和评估生物材料的信息，提出新的问题和回答，并为生物材料设计提供合理的假设。

    

    生物材料和生物受启发材料科学的研究已经得到了很好的发展；然而，令人惊讶的是，很少有系统地将这些知识转化为工程解决方案。为了加快发现并引导洞察，报道了一个开源的自回归转换器大型语言模型BioinspiredLLM。该模型使用了一千多篇经过同行评审的结构生物学和生物受启发材料领域的文章进行了微调，可以被提示主动和交互地回忆信息，协助研究任务，并作为创造力的引擎。该模型通过示例证明了它不仅能够在查询时准确回忆有关生物材料的信息，还能够提出生物材料问题和答案来评估自己的性能。BioinspiredLLM还被证明能够对生物材料设计提出合理的假设，尤其是对于那些从未明确研究过的材料。

    The study of biological materials and bio-inspired materials science is well established; however, surprisingly little knowledge has been systematically translated to engineering solutions. To accelerate discovery and guide insights, an open-source autoregressive transformer large language model, BioinspiredLLM, is reported. The model was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to actively and interactively recall information, assist with research tasks, and function as an engine for creativity. The model has proven by example that it is not only able to accurately recall information about biological materials when queried but also formulate biomaterials questions and answers that can evaluate its own performance. BioinspiredLLM also has been shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly 
    
[^24]: VerilogEval：评估大型语言模型在Verilog代码生成中的性能

    VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])

    [http://arxiv.org/abs/2309.07544](http://arxiv.org/abs/2309.07544)

    本文提出了一个专门用于评估大型语言模型在Verilog代码生成中的性能的基准框架，并提供了一个包含156个问题的综合评估数据集。通过与黄金解决方案进行比较，可以自动测试Verilog代码的功能正确性。通过使用LLM生成的合成问题-代码对进行监督微调，可以改善预训练语言模型的Verilog代码生成能力。

    

    大型语言模型（LLMs）的日益普及为它们在各个领域的应用铺平了道路。本文提出了一个特别针对Verilog代码生成性能评估的基准框架，用于硬件设计和验证。我们提供了一个包含来自Verilog教学网站HDLBits的156个问题的综合评估数据集。该评估集包含了各种Verilog代码生成任务，从简单的组合电路到复杂的有限状态机。可以通过将生成的设计的瞬态仿真输出与黄金解决方案进行比较，自动测试Verilog代码完成的功能正确性。我们还证明了预训练语言模型的Verilog代码生成能力可以通过使用LLM生成的合成问题-代码对进行监督微调来改善。

    The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.
    
[^25]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^26]: 深度学习模型的CPU和GPU性能分析的比较研究

    Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v1 [cs.DC])

    [http://arxiv.org/abs/2309.02521](http://arxiv.org/abs/2309.02521)

    本文通过比较GPU和CPU在深度学习模型训练中的性能，发现GPU在深度神经网络上具有更低的运行时间，对于较简单的网络，GPU并没有太多显著的改进。

    

    在最近几天，深度学习(DL)和机器学习(ML)应用正在快速增加。大量的数据通过互联网生成，可以通过使用ML和DL算法来得出有意义的结果。硬件资源和开源库使得实现这些算法变得容易。Tensorflow和PyTorch是实现ML项目的领先框架之一。通过使用这些框架，我们可以跟踪在GPU和CPU上执行的操作，以分析资源分配和消耗。本文介绍了使用PyTorch训练深度神经网络时CPU和GPU的时间和内存分配情况。该文研究表明，与CPU相比，GPU在深度神经网络上具有更低的运行时间。对于一个较简单的网络，GPU在CPU上并没有太多显著的改进。

    Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
    
[^27]: MedShapeNet - 一个用于计算机视觉的大规模三维医学形状数据集

    MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])

    [http://arxiv.org/abs/2308.16139](http://arxiv.org/abs/2308.16139)

    MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。

    

    我们提出了MedShapeNet，一个包含了解剖形状（如骨骼、器官、血管）和三维手术器械模型的大型数据集。在深度学习时代之前，统计形状模型在医学图像分析中的广泛应用证明了形状常被用来描述医学数据。然而，当前医学图像领域的最先进深度学习算法主要是基于体素的。相反，在计算机视觉领域，形状（包括体素占据网格、网格、点云和隐式表面模型）是三维数据的首选表示方法，这一点可以从大量关于形状的文章及在顶级计算机视觉会议（如IEEE/CVF计算机视觉与模式识别会议（CVPR））中见到，同时ShapeNet（约51300个模型）和普林斯顿ModelNet（127,915个模型）的流行度也在不断增加。MedShapeNet的创建是为了作为这些常用形状基准的替代品。

    We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
    
[^28]: 可通过在线动态嵌入预测减轻过时性的分布式GNN训练

    Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])

    [http://arxiv.org/abs/2308.13466](http://arxiv.org/abs/2308.13466)

    本文提出了一种新颖且可扩展的分布式GNN训练方法SAT，通过在线动态嵌入预测减轻了过时性带来的问题，解决了分布式GNN训练中的并发性和通信开销的困扰。

    

    尽管图神经网络（GNNs）取得了近期的成功，但由于邻居扩散，仍然难以在大规模图上进行训练。分布式计算成为一种有希望的解决方案，通过利用丰富的计算资源（如GPU）。然而，在分布式GNN训练中，由于图数据的节点依赖性增加，实现高并发性变得困难，这会导致巨大的通信开销。为了解决这个问题，历史值近似被认为是一种有希望的分布式训练技术类别。它利用离线内存缓存历史信息（如节点嵌入），作为精确值的可承受的近似，并实现了高并发性。然而，这些好处是以使用过时的训练信息为代价的，导致过时性、不准确性和收敛性问题。为了克服这些挑战，本文提出了SAT（减轻过时性训练），这是一种新颖且可扩展的分布式GNN训练方法。

    Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training fr
    
[^29]: BLIVA: 一个简单的多模态LLM用于更好地处理文本丰富的视觉问题

    BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])

    [http://arxiv.org/abs/2308.09936](http://arxiv.org/abs/2308.09936)

    BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。

    

    视觉语言模型（VLM）通过整合视觉理解能力扩展了大规模语言模型（LLM），在解决开放式视觉问答（VQA）任务方面取得了显著进展。然而，这些模型无法准确解释嵌入文本的图像，这在现实场景中经常发生。从图像中提取信息的标准流程通常涉及学习一组固定的查询嵌入。这些嵌入被设计为封装图像上下文，并随后用作LLM中的软提示输入。然而，这个过程受令牌数量的限制，可能限制对文本丰富的上下文场景的识别。为了改进这一点，本研究引入了BLIVA：InstructBLIP with Visual Assistant的增强版本。BLIVA集成了来自InstructBLIP的查询嵌入，并将编码的补丁嵌入直接投影到LLM中，这是受到LLaVA的启发的一种技术。这种方法有助于处理文本丰富的视觉问题。

    Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
    
[^30]: 从高资源语言到低资源编程语言的知识转移用于代码LLMs

    Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])

    [http://arxiv.org/abs/2308.09895](http://arxiv.org/abs/2308.09895)

    本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。

    

    在过去几年中，代码LLMs（大规模语言模型）开始对编程实践产生重大影响。代码LLMs还成为编程语言和软件工程研究的重要组成部分。然而，代码LLMs生成的代码质量在不同编程语言之间存在显著差异。代码LLMs对训练数据充分的编程语言（如Java、Python或JavaScript）产生令人印象深刻的结果，但在像OCaml和Racket这样的低资源语言上表现困难。本文提出了一种有效的方法，通过使用半合成数据提高代码LLMs在低资源语言上的性能。我们的方法生成了高质量的低资源语言数据集，并可用于微调任何预训练的代码LLMs。我们的方法称为MultiPL-T，它将高资源语言的训练数据转化为低资源语言的训练数据。我们将该方法应用于生成十个数据集。

    Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
    
[^31]: 使用路径进行一般化拓扑图神经网络 (arXiv:2308.06838v2 [cs.LG] 已更新)

    Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06838](http://arxiv.org/abs/2308.06838)

    本论文提出了一种通用的拓扑图神经网络（GNNs）方法，通过强调图中的路径，突破了1-Weisfeiler-Lehmann测试的理论限制，并在无需对图的子结构进行假设的情况下，在多个基准测试中取得了最先进的性能。

    

    尽管图神经网络（GNNs）在不同领域取得了重大进展，但它们受到1-Weisfeiler-Lehmann测试的理论限制。尽管高阶GNNs的最新进展可以克服这个障碍，但它们通常集中在特定的图组件，如团或环。然而，我们的研究走了不同的路线。我们强调路径，这是每个图中固有的。我们能够构建一个更通用的拓扑视角，并与其他拓扑领域的一些成熟理论形成桥梁。有趣的是，在没有对图的子结构进行任何假设的情况下，我们的方法超过了该领域早期的技术，在多个基准测试中实现了最先进的性能。

    While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
    
[^32]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^33]: 瞄准且麻烦：追踪和广告在儿童网站上的问题

    Targeted and Troublesome: Tracking and Advertising on Children's Websites. (arXiv:2308.04887v1 [cs.CY])

    [http://arxiv.org/abs/2308.04887](http://arxiv.org/abs/2308.04887)

    这项研究提供了针对面向儿童的网站进行追踪和广告的详细测量。通过构建一个多语言分类器，并对过两百万个网页进行分类，研究者们编制了一个儿童网站列表。通过对这些网站进行爬取和分析，研究发现追踪器、指纹脚本和广告都存在于这些网站上。同时，研究还检测到这些广告是否启用了定向。

    

    在现代网络上，追踪器和广告商经常在未经同意的情况下构建和利用用户的详细行为配置文件。尽管对网络追踪机制和广告进行了各种研究，但还没有对面向儿童的网站进行严格的研究。为了填补这一空白，我们提出了一种针对面向儿童的网站进行追踪和（定向）广告的测量方法。在缺乏全面的针对儿童的网站（即面向儿童的网站）列表的基础上，我们首先构建了一个基于网页标题和描述的多语言分类器。将该分类器应用于超过两百万个网页中，我们编制了一个包含两千个儿童网站的列表。通过从五个观测点爬取这些网站，我们测量了追踪器、指纹脚本和广告的普遍存在。我们的爬虫检测到在面向儿童的网站上显示的广告，并通过抓取广告披露页面来确定是否启用了广告定向。我们的结果显示，ar

    On the modern web, trackers and advertisers frequently construct and monetize users' detailed behavioral profiles without consent. Despite various studies on web tracking mechanisms and advertisements, there has been no rigorous study focusing on websites targeted at children. To address this gap, we present a measurement of tracking and (targeted) advertising on websites directed at children. Motivated by lacking a comprehensive list of child-directed (i.e., targeted at children) websites, we first build a multilingual classifier based on web page titles and descriptions. Applying this classifier to over two million pages, we compile a list of two thousand child-directed websites. Crawling these sites from five vantage points, we measure the prevalence of trackers, fingerprinting scripts, and advertisements. Our crawler detects ads displayed on child-directed websites and determines if ad targeting is enabled by scraping ad disclosure pages whenever available. Our results show that ar
    
[^34]: 跨平台仇恨言论检测中的因果引导解缠问题

    Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])

    [http://arxiv.org/abs/2308.02080](http://arxiv.org/abs/2308.02080)

    本研究提出了一种跨平台仇恨言论检测模型，通过解缠输入表示为不变特征和平台相关特征，实现了对多个未见平台的良好泛化能力。

    

    尽管社交媒体平台在促进公开对话方面具有价值，但他们经常被利用来传播有害内容。目前用于检测这种有害内容的深度学习和自然语言处理模型过度依赖于领域特定术语，影响到了它们适应泛化仇恨言论检测的能力。这是因为它们倾向于过于狭隘地关注特定的语言信号或某些词语类别的使用。当平台缺乏高质量的标记数据用于训练时，另一个重要的挑战出现了，需要跨平台模型来适应不同的分布转化。我们的研究引入了一个跨平台仇恨言论检测模型，能够在一个平台的数据上训练并推广到多个未见平台。为了实现对不同平台的良好泛化性能，一种方法是将输入表示解缠为不变特征和平台相关特征。我们还认为学习因果关系是提供更好解缠和泛化性能的关键。

    Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
    
[^35]: 图形压缩方法用于归纳节点表示学习

    Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])

    [http://arxiv.org/abs/2307.15967](http://arxiv.org/abs/2307.15967)

    本论文提出了一种映射感知的图形压缩方法（MCond），通过学习节点之间的映射关系，实现了在合成图中高效地处理未知数据的能力。

    

    大规模图引导网络面临着计算挑战，限制了它们在不同应用中的有效性。为了解决这个问题，图形压缩作为一种有希望的技术出现了，它通过构建一个小的合成图来高效地训练图引导网络并保持性能。然而，由于节点之间的拓扑结构，图形压缩仅限于压缩观察到的训练节点及其对应的结构，因此缺乏有效处理未知数据的能力。因此，在推理阶段仍需要原始大图来对归纳节点进行消息传递，导致计算需求巨大。为了解决这个问题，我们提出了映射感知的图形压缩（MCond）方法，明确学习从原始节点到合成节点的一对多节点映射，以无缝地将新节点整合到合成图中。

    Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc
    
[^36]: 通过移动目标防御减轻分散式联邦学习中的通信威胁

    Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])

    [http://arxiv.org/abs/2307.11730](http://arxiv.org/abs/2307.11730)

    本文针对分散式联邦学习中的通信安全挑战，引入了一个安全模块，通过结合加密技术和移动目标防御技术来对抗通信攻击。

    

    分散式联邦学习（DFL）的兴起使得机器学习模型可以在联邦参与方之间进行训练，促进了分散式模型聚合并减少对服务器的依赖。然而，这种方法引入了独特的通信安全挑战，尚未在文献中得到充分解决。这些挑战主要源于聚合过程的分散性质、参与者的多样化角色和责任以及缺乏监管和缓解威胁的中央机构。本文针对这些挑战，首先界定了一个全面的威胁模型，突出了DFL通信的潜在风险。针对这些确定的风险，本文引入了一个专为DFL平台设计的安全模块来对抗基于通信的攻击。该模块结合了对称和非对称加密等安全技术与移动目标防御（MTD）技术。

    The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including
    
[^37]: DiTTO：受扩散启发的时空转换算子

    DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])

    [http://arxiv.org/abs/2307.09072](http://arxiv.org/abs/2307.09072)

    DiTTO是一种扩散启发的算子学习方法，通过结合Transformer架构的元素，无需时间离散化连续解决时间相关PDEs，并在多维度的各种PDE中取得了最先进的准确性结果。

    

    使用数据驱动方法解决偏微分方程（PDEs）已经越来越常见。最近的算子学习范式的发展使得解决更广泛PDE相关问题成为可能。我们提出了一种算子学习方法，可以连续地解决时间相关的PDEs，而不需要任何时间离散化。所提出的方法名为DiTTO，受潜在扩散模型的启发。尽管扩散模型通常用于生成人工智能任务，但其时间条件机制对PDEs非常有用。扩散启发的框架与Transformer架构的元素相结合，以提高其能力。我们展示了新方法在多维度的广泛PDE上的有效性，包括1维Burgers方程，2维Navier-Stokes方程和2维和3维声波方程。DiTTO在这些问题的准确性方面取得了最先进的结果。

    Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
    
[^38]: 基于深度神经网络的自适应巡航控制在上下文感知攻击下的安全性实验分析

    Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])

    [http://arxiv.org/abs/2307.08939](http://arxiv.org/abs/2307.08939)

    这项研究评估了基于深度神经网络的自适应巡航控制系统在隐蔽感知攻击下的安全性，并提出了一种上下文感知策略和基于优化的图像扰动生成方法。

    

    自适应巡航控制（ACC）是一种广泛应用的驾驶员辅助功能，用于保持期望速度和与前方车辆的安全距离。本文评估基于深度神经网络（DNN）的ACC系统在隐蔽感知攻击下的安全性，该攻击会对摄像机数据进行有针对性的扰动，以导致前方碰撞事故。我们提出了一种基于知识和数据驱动的方法，设计了一种上下文感知策略，用于选择触发攻击最关键的时间点，并采用了一种新颖的基于优化的方法，在运行时生成适应性图像扰动。我们使用实际驾驶数据集和逼真的仿真平台评估了所提出攻击的有效性，该仿真平台使用了来自生产ACC系统的控制软件和物理世界驾驶模拟器，并考虑了驾驶员的干预以及自动紧急制动（AEB）和前向碰撞警示（FCW）等安全功能。

    Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
    
[^39]: 自然演员-评论家算法用于带有函数逼近的鲁棒强化学习

    Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])

    [http://arxiv.org/abs/2307.08875](http://arxiv.org/abs/2307.08875)

    本文研究了鲁棒强化学习问题，提出了两种新的不确定性集合形式，使得大规模鲁棒强化学习变得可行。同时，提出了一个鲁棒的自然演员-评论家算法，通过函数逼近，该算法能够在有限时间内收敛到最佳鲁棒策略。

    

    本文研究了鲁棒强化学习，在确定一个对训练模拟器和测试环境之间的模型不匹配具有良好性能的策略的目标下。以前基于策略的鲁棒强化学习算法主要关注不确定性集合下的表格设置，该集合便于鲁棒策略评估，但在状态数量增加时变得不可行。为此，我们提出了两种新的不确定性集合形式，一种基于双重抽样，另一种基于积分概率度量。两者都使得即使只能访问模拟器，也能处理大规模的鲁棒强化学习。我们提出了一个鲁棒的自然演员-评论家算法（RNAC），它结合了新的不确定性集合，并使用函数逼近。我们提供了对于这个RNAC算法在有限时间内收敛到最佳鲁棒策略的收敛性保证，考虑函数逼近误差。最后，我们展示了通过我们的方法学习到的策略的鲁棒性能。

    We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our propo
    
[^40]: 元价值学习：一种带有学习意识的学习通用框架

    Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])

    [http://arxiv.org/abs/2307.08863](http://arxiv.org/abs/2307.08863)

    元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。

    

    多智能体系统中的梯度学习很困难，因为梯度来自于一个一阶模型，不考虑智能体学习过程之间的相互作用。我们扩展了LOLA的思想，并开发了一种完全通用的基于价值的优化方法。核心思想是一个称为元价值的函数，它在联合策略空间的每个点上，为每个智能体给出其未来优化步骤中目标的折扣总和。我们认为，元价值的梯度比原始目标的梯度更可靠的改进方向，因为元价值来自对优化效果的经验观察。我们展示了如何通过训练神经网络来近似元价值，以沿着智能体沿着元价值梯度的优化轨迹进行TD误差最小化。我们分析了我们方法的行为。

    Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our
    
[^41]: 自动化内容分析中的错误分类导致回归分析中的偏差。我们能修复吗？是的，我们能！

    Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])

    [http://arxiv.org/abs/2307.06483](http://arxiv.org/abs/2307.06483)

    传播学领域中的自动化内容分析常忽视了错误分类的偏差，我们介绍并测试了统计方法来纠正这种偏差，并设计了一种新方法来修复之。

    

    自动分类器（ACs）通常通过监督式机器学习（SML）构建，可以对从文本到图片和视频的大量数据进行分类，已经成为传播科学和相关领域中广泛流行的测量设备。尽管如此，即使是高度准确的分类器也会产生错误，这导致了错误分类的偏差和下游分析中误导性的结果，除非这些分析考虑到这些错误。通过对SML应用的系统文献综述，我们发现传播学者在很大程度上忽视了错误分类的偏差。原则上，现有的统计方法可以使用“黄金标准”验证数据（如由人类注释者创建的数据）来纠正错误分类的偏差，并产生一致的估计。我们介绍并测试了这些方法，包括我们在R包misclassificationmodels中设计和实现的一种新方法，通过蒙特卡洛模拟来揭示每种方法的局限性。

    Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
    
[^42]: 以不同方式堆叠更多层：通过低秩更新进行高秩训练

    Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])

    [http://arxiv.org/abs/2307.05695](http://arxiv.org/abs/2307.05695)

    本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。

    

    尽管大规模网络拥有数百亿个参数的规模已经占主导地位并且效果显著，但对于过度参数化模型的训练必要性仍然缺乏清晰的理解，而替代方法不一定能够降低训练高性能模型的成本。本文探索了低秩训练技术作为训练大型神经网络的替代方法。我们引入了一种称为ReLoRA的新方法，它利用低秩更新来训练高秩网络。我们将ReLoRA应用于预训练的Transformer语言模型，参数量高达350M，并且证明了与常规神经网络训练相当的性能。此外，我们观察到ReLoRA的效率随着模型大小的增加而提高，这使得它成为高效训练千亿级参数网络的有希望的方法。我们的研究结果揭示了低秩训练技术的潜力及其对于缩放定律的影响。

    Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
    
[^43]: 双向注意力作为连续词专家的混合物

    Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])

    [http://arxiv.org/abs/2307.04057](http://arxiv.org/abs/2307.04057)

    双向注意力模型具有混合专家权重，类似于连续词袋模型（CBOW）的统计模型，它在大型语言模型中起到了重要作用。

    

    双向注意力由位置编码和屏蔽语言模型（MLM）目标组成的自注意力构成，已成为现代大型语言模型（LLMs）的关键组件。尽管它在实践中取得了成功，但很少有研究探讨它的统计基础：双向注意力隐含地拟合了什么统计模型？它与非注意机制的先驱有何不同？本文探讨了这些问题。关键观察是，重新参数化后，拟合单层单头双向注意力等于拟合具有混合专家权重的连续词袋（CBOW）模型。此外，具有多个头和多个层的双向注意力等价于堆叠的MoEs和MoEs的混合。这个统计观点揭示了MoE在双向注意力中的独特用途，这与其在处理异构性方面的实际有效性相一致。

    Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous
    
[^44]: EHRSHOT:一种用于少样本评估基础模型的电子健康记录基准

    EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02028](http://arxiv.org/abs/2307.02028)

    该论文介绍了EHRSHOT，一个用于少样本评估基础模型的电子健康记录基准。该论文利用EHRSHOT数据集和预训练模型CLMBR-T-base，为医疗保健ML的发展提供了解决方案。

    

    尽管一般的机器学习(ML)社区已经受益于公开的数据集、任务和模型，但是ML在医疗保健领域的进展受到了共享资产的缺乏的阻碍。基础模型的成功为医疗保健ML带来了新的挑战，需要访问共享的预训练模型来验证性能优势。我们通过三个贡献来帮助解决这些挑战。首先，我们发布了一个新的数据集EHRSHOT，其中包含6,739名来自斯坦福医学的患者的去识别结构化的电子健康记录(EHR)数据。与MIMIC-III/IV和其他流行的EHR数据集不同，EHRSHOT是纵向的，不仅局限于ICU/ED患者。其次，我们发布了CLMBR-T-base的权重，这是一个在结构化EHR数据中预训练的141M参数临床基础模型，该数据包括2.57M名患者。我们是最早完全发布这样一个用于编码EHR数据的模型之一；相比之下，大多数先前发布的临床数据模型（如GatorTron、ClinicalBER）并没有完全发布。

    While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
    
[^45]: 受形状改变的Transformer：在无限深度和宽度极限中的注意力模型

    The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit. (arXiv:2306.17759v1 [stat.ML])

    [http://arxiv.org/abs/2306.17759](http://arxiv.org/abs/2306.17759)

    在无限深度和宽度的比例极限下，我们通过修改Softmax-based注意力模型，研究了Transformer的协方差矩阵。我们发现在初始化时，极限分布可以用随机微分方程来描述。通过修改注意力机制并使用残差连接，我们可以控制网络的稳定性和协方差结构的行为。

    

    在深度学习理论中，表示的协方差矩阵用作检查网络可训练性的代理。受Transformer的成功启发，我们研究了在无限深度和宽度的比例极限下，带有跳跃连接的修改Softmax-based注意力模型的协方差矩阵。我们展示了在初始化时，极限分布可以用深度与宽度比率为索引的随机微分方程（SDE）来描述。为了实现良定义的随机极限，Transformer的注意力机制通过将Softmax输出居中在单位矩阵上，并通过宽度相关的温度参数对Softmax logits进行缩放来进行修改。我们通过相应的SDE研究了网络的稳定性，展示了如何通过残差连接优雅地控制漂移和扩散的尺度。稳定SDE的存在意味着协方差结构是良 behaved 的，即使对于非常大的深度和宽度也是如此。

    In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and widt
    
[^46]: LabelBench：基于综合框架的标签高效学习基准评估

    LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])

    [http://arxiv.org/abs/2306.09910](http://arxiv.org/abs/2306.09910)

    本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。

    

    标记数据是现代机器学习应用程序的关键，但获取标记可能很昂贵。为了减缓这一成本，机器学习方法（如迁移学习、半监督学习和主动学习）旨在实现标签高效性：从相对较少的标记示例中实现高预测性能。虽然在实践中获得最佳的标签效率通常需要这些技术的组合，但现有的基准评估框架并没有捕捉到所有这些技术的协同组合。本文通过引入LabelBench解决了这个缺陷，这是一个新的计算效率高的综合性框架，用于联合评估多个标签高效学习技术。作为LabelBench的一个应用，我们引入了一种与半监督学习一起使用的最新主动学习方法的新基准，用于微调预训练的视觉转换器。我们的基准证明了比先前报告的更好的标签效率。

    Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
    
[^47]: 组合和混合变量贝叶斯优化的框架和基准。 (arXiv:2306.09803v1 [cs.LG])

    Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v1 [cs.LG])

    [http://arxiv.org/abs/2306.09803](http://arxiv.org/abs/2306.09803)

    本文介绍了一个模块化框架和基准，用于组合和混合变量贝叶斯优化，并提供多样的合成和真实世界基准测试。通过此框架，作者展示了4种常见的MCBO技术。

    

    本文介绍了一种模块化框架，用于混合变量和组合贝叶斯优化(MCBO)来解决领域中缺乏系统化基准和标准化评估的问题。目前的MCBO论文通常引入非多样性或非标准基准来评估其方法，阻碍了不同MCBO原语及其组合的正确评估。此外，介绍单个MCBO原语的论文通常省略了针对使用相同方法进行剩余原语的基线进行基准测试。这种省略主要是由于涉及的实现工作量非常大，导致缺乏控制评估并无法有效展示贡献的优点。为了克服这些挑战，我们提出的框架使贝叶斯优化组件的组合轻松易行，并提供了多样的合成和真实世界的基准测试任务。利用这种灵活性，我们实现了4种常见的MCBO技术，并在各种合成和真实基准测试中进行了评估。

    This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 4
    
[^48]: 基于可复制的机器学习方法的大规模量子可分性研究

    Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v1 [quant-ph])

    [http://arxiv.org/abs/2306.09444](http://arxiv.org/abs/2306.09444)

    本研究提出了一个机器学习管道用于大规模场景下量子可分性的近似解，通过有效算法近似查找最近的可分离密度矩阵，并将量子可分性视为分类问题，对任何二维混合状态都适用。

    

    量子可分性问题是指如何判断一个二分体密度矩阵是纠缠的还是可分的。我们提出了一种机器学习管道，用于在大规模场景下找到此NP-难问题的近似解。我们提供了一种基于Frank-Wolfe的有效算法来近似查找最近的可分离密度矩阵，并推导了一种系统的方法将密度矩阵标记为可分离的或纠缠的，使我们能够将量子可分性视为分类问题。我们的方法适用于任何二维混合状态。对3-和7维度中的量子态进行的数值实验验证了所提出的程序的效率，并证明它可以扩展到上千个密度矩阵，并具有高量子纠缠检测精度。这一进展有助于基准测试量子可分性，并支持更强大的纠缠检测技术的发展。

    The quantum separability problem consists in deciding whether a bipartite density matrix is entangled or separable. In this work, we propose a machine learning pipeline for finding approximate solutions for this NP-hard problem in large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to approximately seek the nearest separable density matrix and derive a systematic way for labeling density matrices as separable or entangled, allowing us to treat quantum separability as a classification problem. Our method is applicable to any two-qudit mixed states. Numerical experiments with quantum states of 3- and 7-dimensional qudits validate the efficiency of the proposed procedure, and demonstrate that it scales up to thousands of density matrices with a high quantum entanglement detection accuracy. This takes a step towards benchmarking quantum separability to support the development of more powerful entanglement detection techniques.
    
[^49]: TSMixer: 用于多元时间序列预测的轻量级MLP-Mixer模型

    TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])

    [http://arxiv.org/abs/2306.09364](http://arxiv.org/abs/2306.09364)

    TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。

    

    Transformers因其能够捕捉长序列交互而在时间序列预测中备受青睐。然而，其内存和计算要求高的问题对长期预测构成了严重瓶颈。为了解决这一问题，我们提出了TSMixer，这是一种轻量级神经架构，专为多元预测和补丁时间序列表示学习而设计，是Transformers的有效替代。我们的模型借鉴了MLP-Mixer模型在计算机视觉中的成功经验。我们展示了将视觉MLP-Mixer适应于时间序列的挑战，并引入了经过实验证实的组件以提高准确性。这包括一种新的设计范式，即将在线协调头附加到MLP-Mixer骨干上，以显式地建模时间序列的属性，如层次结构和通道相关性。我们还提出了一种混合通道建模方法，平衡了编码多个时间序列通道和保留单个通道信息之间的权衡。我们的实验表明，TSMixer在一元和多元时间序列预测任务中均实现了最先进的性能，同时需要比基于Transformers的方法少得多的参数。

    Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
    
[^50]: DreamSim：使用合成数据学习人类视觉相似性的新维度

    DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.09344](http://arxiv.org/abs/2306.09344)

    本文提出了一种全面评估图像的感知度量DreamSim，该度量使用合成数据学习人类视觉相似性的新维度，表现出优越性能。

    

    当前的感知相似性度量是在像素和图像块的层面操作的。这些度量使用低层次的颜色和纹理来比较图像，但未能捕捉图像布局、对象姿态和语义内容的中层次相似性和差异。本文提出了一种可以全面评估图像的感知度量。我们的第一步是收集一个包含多个相似维度图像对的人类相似性判断的新数据集。这个数据集的关键是评判是几乎自动的，并且由所有观察者共享。为了实现这一点，我们使用最近的文本到图像模型创建了一些沿不同维度扰动的合成图像对。我们观察到，现有的流行的感知度量无法解释我们的新数据，因此引入了一种新的度量DreamSim，以更好地与人类感知相匹配。我们分析了不同视觉属性如何影响度量结果，并发现它严重关注前景物体和语义内容。DreamSim表现出优越性能，在广泛的任务中比现有的度量更优，包括预测行为实验结果、预测对抗鲁棒性和与人类相似性判断相关。

    Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic conte
    
[^51]: 论量子学习中量子纠缠和统计学的作用

    On the Role of Entanglement and Statistics in Learning. (arXiv:2306.03161v1 [quant-ph])

    [http://arxiv.org/abs/2306.03161](http://arxiv.org/abs/2306.03161)

    本论文探讨了量子学习中量子纠缠和统计学的作用，研究了纠缠测量与可分离测量以及纠缠测量与统计测量在学习模型中的区别，证明了QSQ学习与利用纠缠测量的量子学习之间的指数差异。

    

    本论文探究了量子统计查询(QSQ)模型中利用量子纠缠、可分离以及统计测量方法进行学习模型之间的关系。我们得出下列结论。$\textbf{纠缠测量与可分离测量}$：在一个给定的概念类$C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$中，利用$\frac{1}{\sqrt{2^n}}\sum_x\vert x,f(x)\rangle$的副本来学习一个未知函数$f$，如果利用纠缠测量，则需要$T$个副本即可完成学习，则只需利用可分离测量，就需要$O(nT^2)$个副本。$\textbf{纠缠测量与统计测量}$：在可分离测量和统计测量的基础上学习一个函数$f\in C$。我们构建了一个类$C$，证明了QSQ学习与利用纠缠测量的量子学习之间的指数差异（即使在存在噪声的情况下也是如此），这证明了量子学习的量子版本。

    In this work we make progress in understanding the relationship between learning models with access to entangled, separable and statistical measurements in the quantum statistical query (QSQ) model. To this end, we show the following results.  $\textbf{Entangled versus separable measurements.}$ The goal here is to learn an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We show that, if $T$ copies suffice to learn $f$ using entangled measurements, then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.  $\textbf{Entangled versus statistical measurements}$ The goal here is to learn a function $f \in C$ given access to separable measurements and statistical measurements. We exhibit a class $C$ that gives an exponential separation between QSQ learning and quantum learning with entangled measurements (even in the presence of noise). This proves the "quantum analogue" of th
    
[^52]: 从Diffusion模型中提取奖励函数

    Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])

    [http://arxiv.org/abs/2306.01804](http://arxiv.org/abs/2306.01804)

    本论文提出了一种从两个Diffusion模型中提取奖励函数的实用学习算法，可以在导航环境中找到正确的奖励函数，并以此控制Diffusion模型学习足够复杂的任务。

    

    Diffusion模型在图像生成方面取得了显著成果，也被用于学习序列决策任务中的高性能策略。我们考虑通过比较建模低奖励行为和建模高奖励行为的决策传播模型来提取奖励函数的问题；这与逆强化学习相关。我们设计了一种实用的学习算法，通过将神经网络参数化的奖励函数的梯度与两个Diffusion模型的输出差异对齐来提取奖励函数。我们的方法可以在导航环境中找到正确的奖励函数，并且表明可以通过控制Diffusion模型来学习足够复杂的任务。

    Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering 
    
[^53]: 具有平均奖励的不安定赌徒问题：打破统一全局引子假设

    Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])

    [http://arxiv.org/abs/2306.00196](http://arxiv.org/abs/2306.00196)

    本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。

    

    我们研究了具有平均奖励标准下的无限时不安定赌徒问题，包括离散时间和连续时间设置。一个基本问题是如何设计计算有效的策略，使得优化差距随着臂的数量$N$的增加而减小。现有的渐近最优性结果都依赖于统一全局引子性质(UGAP)，这是一个复杂且难以验证的假设。在本文中，我们提出了一个通用的、基于模拟的框架，将任何单臂策略转化为原始的$N$臂问题的策略。这是通过在每个臂上模拟单臂策略，并仔细地将真实状态引导向模拟状态来实现的。我们的框架可以实例化，产生一个具有$O(1/\sqrt{N})$的最优解差距的策略。在离散时间设置中，我们的结果在更简单的同步假设下成立，涵盖了一些不满足UGAP的问题实例。更值得注意的是，我们的框架可以处理比现有方法更大的问题类，而不需对问题实例做任何特定的结构假设。

    We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
    
[^54]: 毒化攻击下的鲁棒非参数回归

    Robust Nonparametric Regression under Poisoning Attack. (arXiv:2305.16771v1 [math.ST])

    [http://arxiv.org/abs/2305.16771](http://arxiv.org/abs/2305.16771)

    本文提出了一种针对毒化攻击的鲁棒非参数回归方法，包括基于Huber损失的M-评估器和通过将初始估计投影到Lipschitz函数空间中的校正方法。结果表明，正确选择带宽时$\ell_\infty $误差是极小化最优的，而$\ell_2 $误差在$q\lesssim \sqrt{N/\ln^2 N}$时最优。

    

    本文研究了鲁棒非参数回归，在这种回归中，对抗性攻击者可以修改来自大小为N的训练数据集中最多q个样本的值。我们的初始解决方案是基于Huber损失最小化的M-评估器。与简单的核回归（即Nadaraya-Watson估计）相比，这种方法可以显着减弱恶意样本对回归性能的影响。我们提供了收敛速率以及相应的极小化下界。结果表明，通过正确选择带宽，$\ell_\infty $误差是极小化最优的。如果$q\lesssim \sqrt{N/\ln^2 N}$，则$\ell_2 $误差是最优的，但是如果$q$更大，则是次优的。原因是如果有许多被攻击的样本集中在一个小区域中，这个估计量就会很容易受到攻击。为了解决这个问题，我们提出了一种校正方法，将初始估计投影到Lipschitz函数空间中。最终的估计值几乎是任意$q$的极小化最优的。

    This paper studies robust nonparametric regression, in which an adversarial attacker can modify the values of up to $q$ samples from a training dataset of size $N$. Our initial solution is an M-estimator based on Huber loss minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson estimator, this method can significantly weaken the impact of malicious samples on the regression performance. We provide the convergence rate as well as the corresponding minimax lower bound. The result shows that, with proper bandwidth selection, $\ell_\infty$ error is minimax optimal. The $\ell_2$ error is optimal if $q\lesssim \sqrt{N/\ln^2 N}$, but is suboptimal with larger $q$. The reason is that this estimator is vulnerable if there are many attacked samples concentrating in a small region. To address this issue, we propose a correction method by projecting the initial estimate to the space of Lipschitz functions. The final estimate is nearly minimax optimal for arbitrary $q$, up t
    
[^55]: BertRLFuzzer: 一种基于BERT和强化学习的Fuzzer

    BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.12534](http://arxiv.org/abs/2305.12534)

    BertRLFuzzer是一种基于BERT和强化学习的Fuzzer工具，旨在发现Web应用程序的安全漏洞。通过使用BERT模型作为代理来指导Fuzzer进行高效学习，BertRLFuzzer相对于其他黑盒和白盒Fuzzer在时间到首次攻击、新漏洞发现和攻击率方面都取得了显著的改进。

    

    本文介绍了一种新颖的工具BertRLFuzzer，它是一种基于BERT和强化学习的Fuzzer，旨在发现Web应用程序的安全漏洞。BertRLFuzzer的工作原理如下：给定一组种子输入，Fuzzer对它们执行遵循语法并引发攻击的变异操作，以生成候选攻击向量。BertRLFuzzer的关键洞察是使用BERT模型作为代理来指导Fuzzer高效学习遵循语法和引发攻击的变异操作符。为了验证BertRLFuzzer的有效性，我们将其与共计13个黑盒和白盒Fuzzer在9个受害者网站的基准测试中进行比较，涉及超过16K行的源代码。相对于最接近的竞争工具，我们观察到时间到首次攻击的显著改进（减少54％），发现的新漏洞（17个新漏洞）和攻击率（生成的攻击向量增加了4.4％）。

    We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
    
[^56]: Waymo开放模拟代理挑战赛

    The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])

    [http://arxiv.org/abs/2305.12032](http://arxiv.org/abs/2305.12032)

    Waymo开放模拟代理挑战赛提出使用真实、互动的智能体仿真以促进自动驾驶行为模型的评估和训练，是该领域的首个公开挑战赛，旨在推动逼真模拟器的设计。

    

    本文定义了Waymo开放模拟代理挑战赛(WOSAC)。通过与真实、互动的智能体进行仿真是自动驾驶软件开发的关键任务。WOSAC是第一个公开的挑战赛，旨在解决该任务并提出相应的评估指标。该挑战的目标是激发设计逼真模拟器的兴趣，以用于评估和训练自动驾驶的行为模型。我们概述了评估方法，并展示了几种基准仿真代理方法的初步结果。

    In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
    
[^57]: 更多脏数据下的系统识别精确恢复

    Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])

    [http://arxiv.org/abs/2305.10506](http://arxiv.org/abs/2305.10506)

    本文研究了在敌对环境下线性离散时间系统的系统识别问题。在周期性注入攻击时，系统动态可精确恢复样本复杂度为O(n)；当攻击以概率p进行时，精确恢复样本复杂度为O(log(n)p/(1-p)^2) 。即使有超过一半的数据受损，估计器仍可学习。

    

    本文研究了在敌对环境下线性离散时间系统的系统识别问题，并分析了两种Lasso估计器的渐近和非渐近特性，涉及到对于攻击时刻的确定性和随机性两种不同场景。由于收集的样本相关，现有的Lasso结果不适用。我们发现，当系统稳定且攻击以周期性注入时，系统动态的精确恢复的样本复杂度为O(n)，其中n是状态的维度。当攻击在每个时间实例中以概率p进行时，精确恢复所需的样本复杂度将按O(log (n)p / (1-p)^2)进行缩放。该结果在渐近状态下意味着几乎肯定收敛于真实系统动态。作为副产品，即使超过一半的数据受损，我们的估计仍然能够学习。

    In this paper, we study the system identification problem for linear discrete-time systems under adversaries and analyze two lasso-type estimators. We study both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We show that when the system is stable and the attacks are injected periodically, the sample complexity for the exact recovery of the system dynamics is O(n), where n is the dimension of the states. When the adversarial attacks occur at each time instance with probability p, the required sample complexity for the exact recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, even when more than half of the data is compromised, our estimators still learn th
    
[^58]: Lingo3DMol:利用语言模型生成基于口袋的三维分子

    Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])

    [http://arxiv.org/abs/2305.10133](http://arxiv.org/abs/2305.10133)

    本文提出了一种基于口袋的三维分子生成方法，利用扰动和恢复预训练任务和一种新的分子表示形式。 该方法结合了语言模型和几何深度学习的优点，使得语言模型可以生成准确的三维分子。

    

    近年来，深度生成模型推动的基于结构的药物设计备受瞩目。 语言模型已经展示了在二维结构中生成有效分子的强大能力，而基于几何深度学习的方法则可以直接产生具有准确三维坐标的分子。受这两种方法的启发，本文提出了一种基于口袋的三维分子生成方法，该方法利用语言模型具有生成三维坐标的能力。 由于高质量的蛋白质-配体复合物数据不足，因此设计了一种扰动和恢复预训练任务，可以利用大量的小分子数据。 还提出了一种新的分子表示形式，即带有局部和全局坐标的基于片段的SMILES，使语言模型能够有效地学习分子拓扑结构和空间位置信息。最终，CrossDocked和DUD-E数据集被用于评估和衡量。

    Structure-based drug design powered by deep generative models have attracted increasing research interest in recent years. Language models have demonstrated a robust capacity for generating valid molecules in 2D structures, while methods based on geometric deep learning can directly produce molecules with accurate 3D coordinates. Inspired by both methods, this article proposes a pocket-based 3D molecule generation method that leverages the language model with the ability to generate 3D coordinates. High quality protein-ligand complex data are insufficient; hence, a perturbation and restoration pre-training task is designed that can utilize vast amounts of small-molecule data. A new molecular representation, a fragment-based SMILES with local and global coordinates, is also presented, enabling the language model to learn molecular topological structures and spatial position information effectively. Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and additional metri
    
[^59]: 基于分数的算子 Newton 方法用于测量运输

    A score-based operator Newton method for measure transport. (arXiv:2305.09792v1 [math.ST])

    [http://arxiv.org/abs/2305.09792](http://arxiv.org/abs/2305.09792)

    本文提出一种新的基于分数的算子Newton方法，可以迭代构造一个易处理的原概率测度，该方法可以在满足目标分数光滑性假设下，实现快速收敛性。

    

    概率测度的运输是统计学和机器学习中许多核心任务的基础，从变分推理到生成建模。一个典型的目标是将一个感兴趣的目标概率测度表示为通过学习的映射将一个易处理的原概率测度推向前面。我们提出了一种新的构建这样一个运输映射的方法，给出了评估目标分布分数的能力。具体而言，我们将该映射特征化为一个无穷维的分数残差算子的零，并推导出一种迭代构造这样一个零的牛顿类型方法。通过调用偏微分方程的经典椭圆正则性理论，我们证明了这些迭代的收敛性，并表明在目标分数光滑性假设下，这种构造具有快速收敛性。我们方法的一个关键元素是将基本的牛顿方法推广到无穷维算子，其他形式的无穷维算子已经出现在非线性 PDE 中。

    Transportation of probability measures underlies many core tasks in statistics and machine learning, from variational inference to generative modeling. A typical goal is to represent a target probability measure of interest as the push-forward of a tractable source measure through a learned map. We present a new construction of such a transport map, given the ability to evaluate the score of the target distribution. Specifically, we characterize the map as a zero of an infinite-dimensional score-residual operator and derive a Newton-type method for iteratively constructing such a zero. We prove convergence of these iterations by invoking classical elliptic regularity theory for partial differential equations (PDE) and show that this construction enjoys rapid convergence, under smoothness assumptions on the target score. A key element of our approach is a generalization of the elementary Newton method to infinite-dimensional operators, other forms of which have appeared in nonlinear PDE
    
[^60]: AGD和MoE用于集成多模态感知

    Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])

    [http://arxiv.org/abs/2305.06324](http://arxiv.org/abs/2305.06324)

    本文提出了集成多模态感知（IMP）方法，将多模态输入集成到单个编码器中，采用交替梯度下降法（AGD）和混合专家（MoE）相结合的方法实现高效的模型和任务扩展，取得了在多个基准测试中具有竞争力的性能表现。

    

    本文提出了一种简单可扩展的多模态多任务训练和建模方法——集成多模态感知（IMP）。IMP将图像、视频、文本和音频等多模态输入集成到单个Transformer编码器中，并具有最小的模态特定组件。IMP使用了一种新颖的设计，将交替梯度下降法（AGD）和混合专家（MoE）相结合，以实现高效的模型和任务扩展。通过广泛的实证研究，我们揭示了以下关键见解：1）在多样化的异构模态、损失函数和任务上交替执行梯度下降更新，并同时改变输入分辨率，可以有效提高多模态理解。2）在单一的模态不可知编码器上使用MoE进行模型稀疏化可以显著提高性能，胜过使用模态特定编码器或额外融合层的稠密模型，并大大缓解模态之间的冲突。IMP在三个多模态基准测试中取得了具有竞争力的性能，胜过了大部分已发表的方法。

    We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model \& task scaling. We conduct extensive empirical studies about IMP and reveal the following key insights: 1) performing gradient descent updates by alternating on diverse heterogeneous modalities, loss functions, and tasks, while also varying input resolutions, efficiently improves multimodal understanding. 2) model sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive p
    
[^61]: 信用风险管理中的量化不确定性：一种深度证据回归方法

    UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v1 [q-fin.RM])

    [http://arxiv.org/abs/2305.04967](http://arxiv.org/abs/2305.04967)

    本文扩展了Deep Evidence Regression方法，将其应用于预测信用风险中的违约损失；我们提供了相关的学习框架，并在模拟和实际数据上进行了验证。

    

    机器学习已经广泛应用于各种信用风险应用程序中。由于信用风险的固有性质，量化预测风险指标的不确定性是必要的，将考虑不确定性的深度学习模型应用于信用风险设置中非常有帮助。在本项工作中，我们探索了一种可扩展的UQ感知深度学习技术，Deep Evidence Regression，并将其应用于预测违约损失。我们通过将Deep Evidence Regression方法扩展到通过Weibull过程生成的目标变量的学习来为文献做出了贡献，并提供了相关的学习框架。我们展示了我们的方法在模拟和实际数据上的应用。

    Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world data.
    
[^62]: FedAVO：利用非洲秃鹫优化器提高联邦学习中的通信效率

    FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v1 [cs.LG])

    [http://arxiv.org/abs/2305.01154](http://arxiv.org/abs/2305.01154)

    本文介绍了一种名为FedAVO的算法，利用非洲秃鹫优化器选择最佳超参数来提高联邦学习中的通信效率，显著降低了与FL操作相关的通信成本。

    

    近年来，分布式机器学习技术联邦学习（FL）由于强调用户数据隐私保护而变得越来越受欢迎。然而，FL的分布式计算可能导致通信受限并且学习过程变得拖延，需要对客户-服务器通信成本进行优化。选择的客户比例和本地训练循环次数是对FL性能有重大影响的两个超参数。由于在各种应用程序中存在不同的训练偏好，因此FL从业者很难手动选择这些超参数。在我们的研究论文中，我们介绍了FedAVO，这是一种新的FL算法，通过利用非洲秃鹫优化器（AVO）选择最佳超参数来增强通信效率。我们的研究表明，采用AVO进行FL超参数调整可以大大减少与FL操作相关的通信成本。

    Federated Learning (FL), a distributed machine learning technique has recently experienced tremendous growth in popularity due to its emphasis on user data privacy. However, the distributed computations of FL can result in constrained communication and drawn-out learning processes, necessitating the client-server communication cost optimization. The ratio of chosen clients and the quantity of local training passes are two hyperparameters that have a significant impact on FL performance. Due to different training preferences across various applications, it can be difficult for FL practitioners to manually select such hyperparameters. In our research paper, we introduce FedAVO, a novel FL algorithm that enhances communication effectiveness by selecting the best hyperparameters leveraging the African Vulture Optimizer (AVO). Our research demonstrates that the communication costs associated with FL operations can be substantially reduced by adopting AVO for FL hyperparameter adjustment. Th
    
[^63]: 带有贝叶斯说服的动态定价和学习

    Dynamic Pricing and Learning with Bayesian Persuasion. (arXiv:2304.14385v1 [cs.GT])

    [http://arxiv.org/abs/2304.14385](http://arxiv.org/abs/2304.14385)

    本研究提出了一种计算有效的在线算法，在没有先验知识的情况下，自适应学习最优定价和广告策略，达到次线性后悔。

    

    我们考虑了一个新颖的动态定价和学习设置，在按顺序设置产品价格的同时，卖家还预先承诺“广告方案”。也就是说，在每轮开始时，卖家可以决定提供什么样的信号来告知买家产品实际的质量。我们使用流行的贝叶斯说服框架来模拟这些信号对买家的评估和购买反应的影响，我们制定了在最大化卖方预期收入的同时找到广告方案和定价方案的最优设计问题。在没有任何先验知识的情况下，我们的目标是设计一个在线算法，该算法可以使用过去的购买反应来自适应地学习最优定价和广告策略。我们研究了算法的后悔，与最优的千里之堤价格和广告计划进行比较。我们的主要结果是一种计算有效的在线算法，即使卖家没有买家需求函数的先验知识，也可以实现与最佳固定价格和广告方案相关的次线性后悔。

    We consider a novel dynamic pricing and learning setting where in addition to setting prices of products in sequential rounds, the seller also ex-ante commits to 'advertising schemes'. That is, in the beginning of each round the seller can decide what kind of signal they will provide to the buyer about the product's quality upon realization. Using the popular Bayesian persuasion framework to model the effect of these signals on the buyers' valuation and purchase responses, we formulate the problem of finding an optimal design of the advertising scheme along with a pricing scheme that maximizes the seller's expected revenue. Without any apriori knowledge of the buyers' demand function, our goal is to design an online algorithm that can use past purchase responses to adaptively learn the optimal pricing and advertising strategy. We study the regret of the algorithm when compared to the optimal clairvoyant price and advertising scheme.  Our main result is a computationally efficient onlin
    
[^64]: 颗粒球计算：一种高效、鲁棒和可解释的自适应多粒度表示和计算方法

    Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])

    [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)

    本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。

    

    人类认知具有“先大后小”的认知机制，因此具有自适应的多粒度描述能力。这导致了有效性、鲁棒性和可解释性等计算特性。本文提出了一种新的基于颗粒球计算的自适应多粒度表示和计算方法。他们将这种方法应用于几个机器学习任务，并证明其相对于其他最先进的方法的有效性。

    Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
    
[^65]: AdapterGNN：高效的δ调节提高了图神经网络的泛化能力

    AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. (arXiv:2304.09595v1 [cs.LG])

    [http://arxiv.org/abs/2304.09595](http://arxiv.org/abs/2304.09595)

    本文提出了一种专为图神经网络设计的δ调节方法——AdapterGNN，该方法保留了预训练模型的知识，利用高度表达的适配器能够在仅有少量参数的情况下有效地适应下游任务，并提高模型的泛化能力，实验结果表明其在多个基准数据集上取得了最优性能。

    

    最近，在图神经网络（GNNs）中微调预训练模型已经取得了显著的性能提升。除了预训练技术外，由于自然语言领域的最新工作的启示，更近期的研究转向应用有效的微调方法，例如参数有效的调节（δ调节）。然而，考虑到GNNs和基于transformer的模型之间存在重大差异，将这些方法直接应用于GNNs证明效果较弱。在本文中，我们对GNNs的δ调节技术进行了全面比较，并提出了一种专门为GNNs设计的新型δ调节方法——AdapterGNN。AdapterGNN保留了大型预训练模型的知识，并利用高度表达的GNN适配器，在仅有少量参数的情况下有效地适应下游任务，同时提高了模型的下游任务的泛化能力。广泛的实验表明，AdapterGNN在几个基准数据集上取得了最先进的性能，同时具有高效的特点。

    Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN a
    
[^66]: 具有内在隐私保护的高效通信和节能无线联邦学习

    Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])

    [http://arxiv.org/abs/2304.07460](http://arxiv.org/abs/2304.07460)

    本文提出了一种名为PFELS的无线FL方案，通过先压缩模型更新再自适应地设计发送功率来提供客户端级别DP保证，并降低通信和能量开销并提高模型精度。

    

    联邦学习（FL）是一种协同学习框架，使边缘设备在保留原始数据的同时协同学习全局模型。虽然FL避免了从本地数据集泄漏直接信息，但仍可能从共享模型推断出敏感信息。为解决FL中的隐私问题，利用差分隐私（DP）机制提供正式的隐私保证。然而，使用无线边缘部署FL时，确保客户端级别的DP面临着重大挑战。在本文中，我们提出了一种名为带稀疏化的私有联邦边缘学习（PFELS）的新型无线FL方案，以提供具有内在信道噪声的客户端级别DP保证，同时降低通信和能量开销并提高模型精度。PFELS的关键思想是使每个设备先压缩其模型更新，然后根据无线信道自适应设计压缩模型更新的发送功率。

    Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha
    
[^67]: 正则化和多视角支持向量机学习的本地化

    Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])

    [http://arxiv.org/abs/2304.05655](http://arxiv.org/abs/2304.05655)

    本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。

    

    本文证明了 H.Q. Minh、L. Bazzani 和 V. Murino 在《机器学习研究》（Journal of Machine Learning Research）中介绍的一种涉及算子值正定核及其再生核希尔伯特空间的正则化和多视角支持向量机学习问题的本地化版本的一些表示定理。结果涉及到考虑凸或非凸损失函数以及有限或无限维输入空间的一般情况。我们展示了该一般框架允许一些特殊情况下的无限维输入空间和非凸损失函数，特别是当损失函数为 Gâteaux 可微函数时。对导致部分非线性问题的指数最小二乘损失函数进行了详细计算。

    We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
    
[^68]: 基于收缩引导的自适应分区法用于神经网络控制系统可达集分析

    Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems. (arXiv:2304.03671v1 [eess.SY])

    [http://arxiv.org/abs/2304.03671](http://arxiv.org/abs/2304.03671)

    本文提出了一种基于收缩引导的自适应分区算法，用于改善带有神经网络控制器和干扰的非线性反馈回路中区间值鲁棒可达集估计，该算法通过将神经网络验证步骤和可达性分区层的解耦，可以在很小的计算成本下提供精度提升。

    

    本文提出了一种基于收缩引导的自适应分区算法，用于改进带神经网络控制器和扰动的非线性反馈回路中区间值鲁棒可达集估计。算法根据超逼近区间的收缩速率估计来选择何时何地进行分区。通过将神经网络验证步骤和可达性分区层的解耦，该算法可以在很小的计算成本下提供精度提升。该方法适用于任何具有足够精度的开环区间值可达性估计技术和任何用于界定神经网络输入输出行为的方法。使用基于收缩的鲁棒性分析，我们为混合单调可达性算法的性能提供了保证。最后，我们通过几个数值模拟来展示算法的性能，并将其与现有方法进行比较。

    In this paper, we present a contraction-guided adaptive partitioning algorithm for improving interval-valued robust reachable set estimates in a nonlinear feedback loop with a neural network controller and disturbances. Based on an estimate of the contraction rate of over-approximated intervals, the algorithm chooses when and where to partition. Then, by leveraging a decoupling of the neural network verification step and reachability partitioning layers, the algorithm can provide accuracy improvements for little computational cost. This approach is applicable with any sufficiently accurate open-loop interval-valued reachability estimation technique and any method for bounding the input-output behavior of a neural network. Using contraction-based robustness analysis, we provide guarantees of the algorithm's performance with mixed monotone reachability. Finally, we demonstrate the algorithm's performance through several numerical simulations and compare it with existing methods in the li
    
[^69]: 基于Neo4j和深度学习的交通拥堵模拟与优化

    Leveraging Neo4j and deep learning for traffic congestion simulation & optimization. (arXiv:2304.00192v1 [cs.AI])

    [http://arxiv.org/abs/2304.00192](http://arxiv.org/abs/2304.00192)

    本文利用Neo4j图和深度学习方法解决城市道路网络中的交通拥堵问题，能够实现拥堵路段的负载平衡和优化，同时可以预测交通事故对整体交通的影响。

    

    交通拥堵一直是城市道路网络中的主要挑战。过去进行了大量研究，以凸显与交通拥堵相关的问题，并通过数据驱动的方法解决了这个问题。目前，大多数交通拥堵分析都是使用模拟软件进行的，这些软件由于使用的工具和实用程序的限制而提供了有限的洞见。所有这些都影响到定制业务问题的制定，这些问题因地区和国家而异。通过利用知识图的能力，我们将交通拥堵问题建模为Neo4j图，然后使用负载平衡、优化算法来识别无拥堵的道路网络。我们还展示了在拥堵或事故情况下交通如何向后传播以及其对其他道路段的总体影响。我们还在实时交通数据上训练了顺序RNN-LSTM(长短时记忆)深度学习模型。

    Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
    
[^70]: 利用多时间 Hamilton-Jacobi PDE 解决一些科学机器学习问题

    Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v1 [cs.LG])

    [http://arxiv.org/abs/2303.12928](http://arxiv.org/abs/2303.12928)

    本文从理论上将特定优化问题与多时间Hamilton-Jacobi PDEs联系起来，表明当解决这些问题时，同时解决了对应的多时间HJ PDEs和最优控制问题。利用这种联系，提出了一种新的算法，实现了深度神经网络泛化性能的改进。

    

    Hamilton-Jacobi 偏微分方程(HJ PDEs)与广泛领域，如最优控制、微分游戏和成像科学有着深刻的联系。通过将时间变量视为更高维的量，HJ PDEs 可以扩展到多时间的情况。本文在特定机器学习优化问题与多时间Hopf公式之间建立了一种新的理论联系，该公式对应于某些多时间 HJ PDEs 的解的表示。通过这种联系，我们通过展示当我们解决这些学习问题时，我们也解决了一个多时间 HJ PDE 和相应的最优控制问题，从而增加了某些机器学习应用程序的训练过程的可解释性。作为这种联系的第一个探索，我们发展了正则化线性回归问题与线性二次调节器 (LQR) 之间的关系。然后，我们利用我们的理论框架设计了一种新的深度神经网络训练算法，实现了改进的泛化性能。

    Hamilton-Jacobi partial differential equations (HJ PDEs) have deep connections with a wide range of fields, including optimal control, differential games, and imaging sciences. By considering the time variable to be a higher dimensional quantity, HJ PDEs can be extended to the multi-time case. In this paper, we establish a novel theoretical connection between specific optimization problems arising in machine learning and the multi-time Hopf formula, which corresponds to a representation of the solution to certain multi-time HJ PDEs. Through this connection, we increase the interpretability of the training process of certain machine learning applications by showing that when we solve these learning problems, we also solve a multi-time HJ PDE and, by extension, its corresponding optimal control problem. As a first exploration of this connection, we develop the relation between the regularized linear regression problem and the Linear Quadratic Regulator (LQR). We then leverage our theoret
    
[^71]: 量化和建模多模态交互：一种信息分解框架

    Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12247](http://arxiv.org/abs/2302.12247)

    通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。

    

    对于解决多模态任务所需的交互如何进行量化？最适合捕捉这些交互的多模态模型是什么？为了回答这些问题，我们提出了一种信息论方法来量化输入模态与输出任务之间的冗余度、独特性和协同性。我们将这三个衡量标准称为多模态分布（或简称PID）的PID统计量，并引入了两个新的PID统计估计器，适用于高维分布。为了验证PID估计，我们在已知PID的合成数据集和大规模多模态基准测试集上进行了大量实验。

    The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
    
[^72]: 基于原则主义的负责任数据管理

    Principlism Guided Responsible Data Curation. (arXiv:2302.03629v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03629](http://arxiv.org/abs/2302.03629)

    研究针对人本计算机视觉数据集的负责任数据管理建议，采用预防性反思的观点，遵循原则主义的伦理框架，解决隐私和偏差问题。

    

    人本计算机视觉数据整理实践经常忽略隐私和偏差问题，导致数据集撤回和不公平的模型。此外，通过非同意网络爬取构建的人本计算机视觉数据集缺乏全面的公平性和鲁棒性评估所必需的元数据。当前的解决方法后期解决问题，缺乏说服力的采用理由或未能提供适当应用的合适背景。我们的研究着重于针对人本计算机视觉数据集的主动领域特定建议，解决隐私和偏差问题。我们采用反思的观点，并借鉴了现有的实践和指南，遵循原则主义的伦理框架。

    Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. Further, HCCV datasets constructed through nonconsensual web scraping lack the necessary metadata for comprehensive fairness and robustness evaluations. Current remedies address issues post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations for curating HCCV datasets, addressing privacy and bias. We adopt an ante hoc reflective perspective and draw from current practices and guidelines, guided by the ethical framework of principlism.
    
[^73]: 基于图的概率多智能体轨迹预测模型MTP-GO与神经ODE

    MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.00735](http://arxiv.org/abs/2302.00735)

    本文介绍了一种基于图的概率多智能体轨迹预测模型MTP-GO。该模型利用时间图神经网络编码场景，采用神经常微分方程实现运动模型，并结合混合密度网络和卡尔曼滤波实现多模态概率预测，在多个指标上优于其他最先进的方法。

    

    实现弹性的自主路径规划需要对周围道路用户未来行为做出可靠的预测。为响应此需求及相关挑战，本文引入了名为MTP-GO的模型。该模型采用时间图神经网络对场景进行编码，生成底层运动模型的输入。运动模型采用了神经常微分方程，其中的状态转移函数将和其他部分一起进行学习。结合混合密度网络和卡尔曼滤波的概念，可以获得多模态的概率预测。结果表明，所提出的模型在各种数据集上的预测能力优于几种最先进的方法，并且在多个指标上表现出色。

    Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
    
[^74]: ClimaX:一种用于天气和气候的基础模型

    ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10343](http://arxiv.org/abs/2301.10343)

    ClimaX是一种灵活且可推广的深度学习模型，用于天气和气候科学，可以使用不同数据集进行训练。

    

    目前大多数先进的天气和气候模型都是基于物理信息的数值模型。这些方法旨在模拟非线性动力学和多变量之间的复杂相互作用，这些相互作用很难近似。此外，许多这样的数值模型在模拟细粒度空间和时间分辨率的大气现象时计算量很大。最近的基于机器学习的数据驱动方法通过使用深度神经网络学习数据驱动的功能映射来直接解决下游预测或投射任务。然而，这些网络是使用为特定时空任务策划和同质化的气候数据集进行训练的，因此缺乏数值模型的普遍性。我们开发并演示了ClimaX，这是一个灵活且可推广的深度学习模型，可用于天气和气候科学，并可以使用跨越不同数据集的异构数据进行训练。

    Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
    
[^75]: 使用图神经网络求解双层背包问题

    Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13436](http://arxiv.org/abs/2211.13436)

    本研究提出了一种使用图神经网络的深度学习方法来解决双层背包问题，该方法比精确算法快500倍，可找到可行性解决方案。

    

    双层优化问题是一种具有两个代理人（领导者和追随者）的层次优化问题。领导者首先做出自己的决策，追随者随后做出最佳选择。领导者了解追随者的信息，问题的目标是从领导者的角度考虑追随者的反应，找到最优解。对于双层优化问题来说，没有通用的高效算法或商用求解器可以得到最优解，即使是简单的问题也很难得到良好的解。本文提出了一种使用图神经网络的深度学习方法来解决双层背包问题。我们训练模型来预测领导者的解决方案，并将其用于将层次优化问题转化为单层优化问题以获取解决方案。我们的模型发现了可行的解决方案，速度比精确算法快500倍。

    The Bilevel Optimization Problem is a hierarchical optimization problem with two agents, a leader and a follower. The leader make their own decisions first, and the followers make the best choices accordingly. The leader knows the information of the followers, and the goal of the problem is to find the optimal solution by considering the reactions of the followers from the leader's point of view. For the Bilevel Optimization Problem, there are no general and efficient algorithms or commercial solvers to get an optimal solution, and it is very difficult to get a good solution even for a simple problem. In this paper, we propose a deep learning approach using Graph Neural Networks to solve the bilevel knapsack problem. We train the model to predict the leader's solution and use it to transform the hierarchical optimization problem into a single-level optimization problem to get the solution. Our model found the feasible solution that was about 500 times faster than the exact algorithm wi
    
[^76]: 非马尔可夫环境下的强化学习

    Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.01595](http://arxiv.org/abs/2211.01595)

    本文通过递归计算近似充分统计量，提出了一种基于自编码器的代理设计方案，实现了在非马尔可夫环境中进行强化学习。

    

    本文以Van Roy及其合作者为基础，提出了在任意非马尔可夫环境中进行强化学习的新范式，并明确了当在该范式上应用Q学习算法时，由于非马尔可夫性质引起的错误。基于此观察，我们建议代理设计的标准应是寻找某些条件规律的良好近似。受经典随机控制的启发，我们证明了我们的问题归结为递归计算近似充分统计量的问题。这导致了一种基于自编码器的代理设计方案，我们在部分观察到的强化学习环境中进行了数值测试。

    Motivated by the novel paradigm developed by Van Roy and coauthors for reinforcement learning in arbitrary non-Markovian environments, we propose a related formulation and explicitly pin down the error caused by non-Markovianity of observations when the Q-learning algorithm is applied on this formulation. Based on this observation, we propose that the criterion for agent design should be to seek good approximations for certain conditional laws. Inspired by classical stochastic control, we show that our problem reduces to that of recursive computation of approximate sufficient statistics. This leads to an autoencoder-based scheme for agent design which is then numerically tested on partially observed reinforcement learning environments.
    
[^77]: PromptCast：一种新的基于提示的时间序列预测范式

    PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.08964](http://arxiv.org/abs/2210.08964)

    提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。

    

    本文提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast）。在这种新的任务中，将原来的数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，使得语言模型可以直接应用于预测的目的。为了支持和促进这个任务的研究，我们还提出了一个大规模的数据集（PISA）。

    This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
    
[^78]: BAFFLE: 离线增强学习中的后门攻击

    BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04688](http://arxiv.org/abs/2210.04688)

    本文研究离线增强学习中的后门攻击，通过向数据中添加扰动，使得智能体在注入触发器的观测值上采取低奖励动作，从而提出了BAFFLE方法。

    

    越来越多的研究关注于强化学习（RL）方法，允许智能体通过与环境的交互中收集的试错经验进行学习。最近，离线RL成为一种流行的RL范例，因为它节省了与环境的交互。在离线RL中，数据提供者共享大规模的预先收集的数据集，其他人可以在不与环境交互的情况下训练高质量的智能体。这种范例在机器人控制、自动驾驶等关键任务中表现出有效性。然而，较少关注研究离线RL系统的安全威胁。本文关注后门攻击，其中一些扰动被添加到数据（观测值）中，使得在给定正常观测值的情况下，智能体采取高奖励的动作，在注入触发器的观测值上采取低奖励的动作。在本文中，我们提出了BAFFLE（离线增强学习中的后门攻击），这是一种方法。

    A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
    
[^79]: UniCon: 带有对比损失的单向分歧学习用于视觉问答

    UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.11435](http://arxiv.org/abs/2208.11435)

    本文提出了UniCon方法，用于解决多客户VQA任务的保密性约束和客户有限标记训练数据的问题。该方法通过模型共享学习跨模态表示，采用分裂学习架构确保隐私。

    

    多模式数据的视觉问答（VQA）有助于现实应用，如家庭机器人和医学诊断。然而，面临的一个重要挑战是为各种客户任务设计强大的学习方法。其中一个关键方面是确保隐私，因为由于保密问题，客户数据共享受到限制。本文致力于解决多客户VQA任务的保密性约束和客户有限标记训练数据的问题。我们提出了带有对比损失的单向分歧学习（UniCon）方法来克服这些限制。所提出的方法在不同客户的整个数据分布上训练全局模型，通过模型共享学习精细的跨模态表示来实现隐私保证，利用分裂学习架构确保隐私，其中完整模型分为两个组件进行独立训练。此外，最近发现自我监督学习技术与我们的方法高度兼容。

    Visual Question Answering (VQA) using multi-modal data facilitates real-life applications, such as home robots and medical diagnoses. However, one significant challenge is to design a robust learning method for various client tasks. One critical aspect is to ensure privacy, as client data sharing is limited due to confidentiality concerns. This work focuses on addressing the issue of confidentiality constraints in multi-client VQA tasks and limited labeled training data of clients. We propose the Unidirectional Split Learning with Contrastive Loss (UniCon) method to overcome these limitations. The proposed method trains a global model on the entire data distribution of different clients, learning refined cross-modal representations through model sharing. Privacy is ensured by utilizing a split learning architecture in which a complete model is partitioned into two components for independent training. Moreover, recent self-supervised learning techniques were found to be highly compatibl
    
[^80]: GAN和闭合性: 多尺度建模中的微观-宏观一致性

    GANs and Closures: Micro-Macro Consistency in Multiscale Modeling. (arXiv:2208.10715v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10715](http://arxiv.org/abs/2208.10715)

    论文探讨了多尺度建模中采样和集合平均的问题，并将增强采样技术与分子模拟与机器学习中的生成对抗网络相结合。

    

    采样分子系统的相空间（更广义的是，用随机微分方程描述的复杂系统的相空间）在许多领域中是重要的建模步骤，从蛋白质折叠到材料发现。这些问题通常具有多尺度性质：它们可以用少量“缓慢”反应坐标参数化的低维有效自由能面来描述；剩余的“快速”自由度在反应坐标值上填充平衡测度。这些问题的采样过程用于估计有效的自由能差异和与条件平衡分布相关的集合平均数；这些后者的平均数导致有效的减少动态模型的闭合。多年来，增强采样技术与分子模拟相结合。与机器学习（ML）领域存在一个有趣的类比。

    Sampling the phase space of molecular systems -- and, more generally, of complex systems effectively modeled by stochastic differential equations -- is a crucial modeling step in many fields, from protein folding to materials discovery. These problems are often multiscale in nature: they can be described in terms of low-dimensional effective free energy surfaces parametrized by a small number of "slow" reaction coordinates; the remaining "fast" degrees of freedom populate an equilibrium measure on the reaction coordinate values. Sampling procedures for such problems are used to estimate effective free energy differences as well as ensemble averages with respect to the conditional equilibrium distributions; these latter averages lead to closures for effective reduced dynamic models. Over the years, enhanced sampling techniques coupled with molecular simulation have been developed. An intriguing analogy arises with the field of Machine Learning (ML), where Generative Adversarial Networks
    
[^81]: 推荐系统中的供给侧均衡

    Supply-Side Equilibria in Recommender Systems. (arXiv:2206.13489v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2206.13489](http://arxiv.org/abs/2206.13489)

    本论文探究了推荐系统中个性化内容的供给侧均衡问题，其特点是生产者决策空间是多维的和用户群体是异构的，高维度和异质性的模型创造了专业化的可能性。

    

    算法推荐系统（如Spotify和Netflix）不仅影响消费者行为，而且影响生产者的激励。生产者试图创建将被推荐算法显示的内容，这可能影响他们内容的多样性和质量。本文研究个性化内容推荐系统中的供给侧均衡。我们将用户和内容建模为 $D$ 维向量，推荐算法显示每个用户与之最高点积的内容，生产者最大化被推荐其内容的用户数减去生产成本。我们模型的两个关键特征是生产者决策空间是多维的，用户群体是异构的，这与经典低维模型不同。多维性和异质性创造了专业化的可能性，不同的生产者在均衡状态下创建不同类型的内容。使用对偶论证法

    Algorithmic recommender systems such as Spotify and Netflix affect not only consumer behavior but also producer incentives. Producers seek to create content that will be shown by the recommendation algorithm, which can impact both the diversity and quality of their content. In this work, we investigate the resulting supply-side equilibria in personalized content recommender systems. We model users and content as $D$-dimensional vectors, the recommendation algorithm as showing each user the content with highest dot product, and producers as maximizing the number of users who are recommended their content minus the cost of production. Two key features of our model are that the producer decision space is multi-dimensional and the user base is heterogeneous, which contrasts with classical low-dimensional models.  Multi-dimensionality and heterogeneity create the potential for specialization, where different producers create different types of content at equilibrium. Using a duality argumen
    
[^82]: 对抗性代理风险在二元分类中的存在性和极小化定理

    Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (arXiv:2206.09098v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09098](http://arxiv.org/abs/2206.09098)

    本文证明了对抗性代理风险的存在性、正则性和极小化定理，这一结果为对抗鲁棒性的理论提供了支持，并且可以指导算法的发展。

    

    对抗性训练是训练鲁棒性强的方法之一，然而，它从理论角度并不为人们所熟知。本文证明了对抗性代理风险的存在性、正则性和极小化定理。我们的结果解释了之前研究中有关对抗鲁棒性的一些经验观察，并提出了算法发展的新方向。此外，我们的结果将之前已知的对抗分类风险的存在和极小化定理扩展到了代理风险。

    Adversarial training is one of the most popular methods for training methods robust to adversarial attacks, however, it is not well-understood from a theoretical perspective. We prove and existence, regularity, and minimax theorems for adversarial surrogate risks. Our results explain some empirical observations on adversarial robustness from prior work and suggest new directions in algorithm development. Furthermore, our results extend previously known existence and minimax theorems for the adversarial classification risk to surrogate risks.
    
[^83]: Selectively Adaptive Lasso选适应Lasso

    The Selectively Adaptive Lasso. (arXiv:2205.10697v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.10697](http://arxiv.org/abs/2205.10697)

    本文提出了一种新算法——Selectively Adaptive Lasso（SAL），它基于HAL的理论构建，保留了无维度、非参数收敛速率的优点，同时也具有可扩展到大规模高维数据集的能力。这种算法将许多回归系数自动设置为零。

    

    机器学习回归方法能够进行无需过多的参数假设的函数估计。虽然它们可以在预测误差方面表现出色，但大多数缺乏类半参数有效估计（例如，TMLE，AIPW）所需的理论收敛速度。高度自适应Lasso（HAL）是唯一经证明能够快速收敛到意义上的大类函数的回归方法，与预测变量的维度无关。不幸的是，HAL无法扩展计算。在本文中，我们在HAL理论的基础上构建选择自适应Lasso（SAL），一种新的算法，保留HAL的无维度、非参数收敛率，但也能扩展到大规模的高维数据集。为了实现这一目标，我们证明了一些与嵌套Donsker类中的经验损失最小化有关的一般理论结果。我们的算法是一种梯度下降形式，具有简单的分组规则，自动将许多回归系数设为零。

    Machine learning regression methods allow estimation of functions without unrealistic parametric assumptions. Although they can perform exceptionally in prediction error, most lack theoretical convergence rates necessary for semi-parametric efficient estimation (e.g. TMLE, AIPW) of parameters like average treatment effects. The Highly Adaptive Lasso (HAL) is the only regression method proven to converge quickly enough for a meaningfully large class of functions, independent of the dimensionality of the predictors. Unfortunately, HAL is not computationally scalable. In this paper we build upon the theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new algorithm which retains HAL's dimension-free, nonparametric convergence rate but which also scales computationally to large high-dimensional datasets. To accomplish this, we prove some general theoretical results pertaining to empirical loss minimization in nested Donsker classes. Our resulting algorithm is a form of gradie
    
[^84]: HiFi++：用于带宽扩展和语音增强的统一框架

    HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement. (arXiv:2203.13086v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2203.13086](http://arxiv.org/abs/2203.13086)

    本文提出了一种统一框架HiFi++，用于带宽扩展和语音增强任务，通过改进的生成器架构，在这些任务中表现出与最先进方法相媲美甚至更好的性能，并且消耗更少的计算资源。

    

    近期对抗生成网络在神经声码器中展现出优异的性能，超过了最佳的自回归和流模型。本文中，我们展示了这一成功可以扩展到其他有条件音频生成任务。特别是，在 HiFi 声码器的基础上，我们提出了一种新颖的 HiFi++ 通用框架，用于带宽扩展和语音增强。我们展示了通过改进的生成器架构，HiFi++ 在这些任务中表现出与最先进方法相媲美甚至更好的性能，同时消耗更少的计算资源。我们通过一系列广泛的实验验证了我们方法的有效性。

    Generative adversarial networks have recently demonstrated outstanding performance in neural vocoding outperforming best autoregressive and flow-based models. In this paper, we show that this success can be extended to other tasks of conditional audio generation. In particular, building upon HiFi vocoders, we propose a novel HiFi++ general framework for bandwidth extension and speech enhancement. We show that with the improved generator architecture, HiFi++ performs better or comparably with the state-of-the-art in these tasks while spending significantly less computational resources. The effectiveness of our approach is validated through a series of extensive experiments.
    
[^85]: 任何决策阈值下公平分类的几何修复

    Geometric Repair for Fair Classification at Any Decision Threshold. (arXiv:2203.07490v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07490](http://arxiv.org/abs/2203.07490)

    本文研究了如何通过减少每个组分数分布之间的统计距离，在任何决策阈值下同时提高分类的公平性能，并提出了一种基于最佳运输的后处理算法。

    

    我们研究的问题是后处理监督机器学习回归器以在所有决策阈值下最大化公平的二分类问题。具体而言，我们表明通过减少每个组分数分布之间的统计距离，我们可以同时增加所有阈值上的公平性能，并且可以做到在不显着降低准确性的情况下实现。为此，我们介绍了一个形式化的分布平等度量，它捕获了不同保护组的分类分布相似程度。与先前只针对所有阈值的人口统计平等度研究的研究相比，我们的度量适用于大类公平性度量。我们的主要成果是提出一种基于最佳运输的后处理算法，可以证明它最大化了分布平等。我们在几个公平性基准测试上支持此结果。

    We study the problem of post-processing a supervised machine-learned regressor to maximize fair binary classification at all decision thresholds. Specifically, we show that by decreasing the statistical distance between each group's score distributions, we can increase fair performance across all thresholds at once, and that we can do so without a significant decrease in accuracy. To this end, we introduce a formal measure of distributional parity, which captures the degree of similarity in the distributions of classifications for different protected groups. In contrast to prior work, which has been limited to studies of demographic parity across all thresholds, our measure applies to a large class of fairness metrics. Our main result is to put forward a novel post-processing algorithm based on optimal transport, which provably maximizes distributional parity. We support this result with experiments on several fairness benchmarks.
    
[^86]: 一种轻量级且梯度稳定的神经层

    A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04088](http://arxiv.org/abs/2106.04088)

    Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。

    

    我们提出了一种基于Householder权重和绝对值激活的神经层结构，因此被称为Householder-absolute神经层或简称Han层。与具有$d$个神经元和$d$个输出的全连接层相比，Han层将参数数量和相应的复杂度从$O（d ^ 2）$降低到$O（d）$。Han层结构保证了两个理想属性：（1）梯度稳定性（不会出现梯度消失或梯度爆炸），以及（2）1-Lipschitz连续性。广泛的数值实验表明，可以有策略地使用Han层替换全连接（FC）层，从而减少模型参数的数量，同时保持或甚至提高泛化性能。我们将展示Han层结构在一些小型化的模型上的能力，同时讨论其当前的限制。

    We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
    

